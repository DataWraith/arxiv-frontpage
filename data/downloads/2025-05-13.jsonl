{"created":"2025-05-13","title":"Neural Network Operator-Based Fractal Approximation: Smoothness Preservation and Convergence Analysis","abstract":"This paper presents a new approach of constructing $\\alpha$-fractal interpolation functions (FIFs) using neural network operators, integrating concepts from approximation theory. Initially, we construct $\\alpha$-fractals utilizing neural network-based operators, providing an approach to generating fractal functions with interpolation properties. Based on the same foundation, we have developed fractal interpolation functions that utilize only the values of the original function at the nodes or partition points, unlike traditional methods that rely on the entire original function.","authors":["Aaqib Ayoub Bhat","Asif Khan","M. Mursaleen"],"url":"https://arxiv.org/abs/2505.06229"}
{"created":"2025-05-13","title":"Assessing the Impact of External and Internal Factors on Emergency Department Overcrowding","abstract":"Study Objective: To analyze the factors influencing Emergency Department (ED) overcrowding by examining the impacts of operational, environmental, and external variables, including weather conditions and football games.","authors":["Abdulaziz Ahmed","Khalid Y Aram","Mohammed Alzeen","Orhun Vural","James Booth","Brittany F. Lindsey","Bunyamin Ozaydin"],"url":"https://arxiv.org/abs/2505.06238"}
{"created":"2025-05-13","title":"United States Road Accident Prediction using Random Forest Predictor","abstract":"Road accidents significantly threaten public safety and require in-depth analysis for effective prevention and mitigation strategies. This paper focuses on predicting accidents through the examination of a comprehensive traffic dataset covering 49 states in the United States. The dataset integrates information from diverse sources, including transportation departments, law enforcement, and traffic sensors. This paper specifically emphasizes predicting the number of accidents, utilizing advanced machine learning models such as regression analysis and time series analysis. The inclusion of various factors, ranging from environmental conditions to human behavior and infrastructure, ensures a holistic understanding of the dynamics influencing road safety. Temporal and spatial analysis further allows for the identification of trends, seasonal variations, and high-risk areas. The implications of this research extend to proactive decision-making for policymakers and transportation authorities. By providing accurate predictions and quantifiable insights into expected accident rates under different conditions, the paper aims to empower authorities to allocate resources efficiently and implement targeted interventions. The goal is to contribute to the development of informed policies and interventions that enhance road safety, creating a safer environment for all road users. Keywords: Machine Learning, Random Forest, Accident Prediction, AutoML, LSTM.","authors":["Dominic Parosh Yamarthi","Haripriya Raman","Shamsad Parvin"],"url":"https://arxiv.org/abs/2505.06246"}
{"created":"2025-05-13","title":"Towards Efficient LLM Storage Reduction via Tensor Deduplication and Delta Compression","abstract":"Modern model hubs, such as Hugging Face, store tens of petabytes of LLMs, with fine-tuned variants vastly outnumbering base models and dominating storage consumption. Existing storage reduction techniques -- such as deduplication and compression -- are either LLM oblivious or not compatible with each other, limiting data reduction effectiveness. Our large-scale characterization study across all publicly available Hugging Face LLM repositories reveals several key insights: (1) fine-tuned models within the same family exhibit highly structured, sparse parameter differences suitable for delta compression; (2) bitwise similarity enables LLM family clustering; and (3) tensor-level deduplication offers strong synergy with model aware compressors. Building on these insights, we present BitX, an effective, fast, lossless delta compression algorithm that compresses XORed redundancy between fine-tuned and base LLMs. We build zLLM, a model storage reduction pipeline that unifies tensor-level deduplication and lossless BitX compression. By synergizing deduplication and compression around LLM family clustering, zLLM reduces model storage consumption by 49.5 percent, over 20 percent more than state-of-the-art deduplication and compression designs.","authors":["Zirui Wang","Tingfeng Lan","Zhaoyuan Su","Juncheng Yang","Yue Cheng"],"url":"https://arxiv.org/abs/2505.06252"}
{"created":"2025-05-13","title":"OpenSky Report 2025: Improving Crowdsourced Flight Trajectories with ADS-C Data","abstract":"The OpenSky Network has been collecting and providing crowdsourced air traffic surveillance data since 2013. The network has primarily focused on Automatic Dependent Surveillance--Broadcast (ADS-B) data, which provides high-frequency position updates over terrestrial areas. However, the ADS-B signals are limited over oceans and remote regions, where ground-based receivers are scarce. To address these coverage gaps, the OpenSky Network has begun incorporating data from the Automatic Dependent Surveillance--Contract (ADS-C) system, which uses satellite communication to track aircraft positions over oceanic regions and remote areas. In this paper, we analyze a dataset of over 720,000 ADS-C messages collected in 2024 from around 2,600 unique aircraft via the Alphasat satellite, covering Europe, Africa, and parts of the Atlantic Ocean. We present our approach to combining ADS-B and ADS-C data to construct detailed long-haul flight paths, particularly for transatlantic and African routes. Our findings demonstrate that this integration significantly improves trajectory reconstruction accuracy, allowing for better fuel consumption and emissions estimates. We illustrate how combined data captures flight patterns across previously underrepresented regions across Africa. Despite coverage limitations, this work marks an important advancement in providing open access to global flight trajectory data, enabling new research opportunities in air traffic management, environmental impact assessment, and aviation safety.","authors":["Junzi Sun","Xavier Olive","Martin Strohmeier","Vincent Lenders"],"url":"https://arxiv.org/abs/2505.06254"}
{"created":"2025-05-13","title":"Beyond Attention: Toward Machines with Intrinsic Higher Mental States","abstract":"Attending to what is relevant is fundamental to both the mammalian brain and modern machine learning models such as Transformers. Yet, determining relevance remains a core challenge, traditionally offloaded to learning algorithms like backpropagation. Inspired by recent cellular neurobiological evidence linking neocortical pyramidal cells to distinct mental states, this work shows how models (e.g., Transformers) can emulate high-level perceptual processing and awake thought (imagination) states to pre-select relevant information before applying attention. Triadic neuronal-level modulation loops among questions ($Q$), clues (keys, $K$), and hypotheses (values, $V$) enable diverse, deep, parallel reasoning chains at the representation level and allow a rapid shift from initial biases to refined understanding. This leads to orders-of-magnitude faster learning with significantly reduced computational demand (e.g., fewer heads, layers, and tokens), at an approximate cost of $\\mathcal{O}(N)$, where $N$ is the number of input tokens. Results span reinforcement learning (e.g., CarRacing in a high-dimensional visual setup), computer vision, and natural language question answering.","authors":["Ahsan Adeel"],"url":"https://arxiv.org/abs/2505.06257"}
{"created":"2025-05-13","title":"ABE: A Unified Framework for Robust and Faithful Attribution-Based Explainability","abstract":"Attribution algorithms are essential for enhancing the interpretability and trustworthiness of deep learning models by identifying key features driving model decisions. Existing frameworks, such as InterpretDL and OmniXAI, integrate multiple attribution methods but suffer from scalability limitations, high coupling, theoretical constraints, and lack of user-friendly implementations, hindering neural network transparency and interoperability. To address these challenges, we propose Attribution-Based Explainability (ABE), a unified framework that formalizes Fundamental Attribution Methods and integrates state-of-the-art attribution algorithms while ensuring compliance with attribution axioms. ABE enables researchers to develop novel attribution techniques and enhances interpretability through four customizable modules: Robustness, Interpretability, Validation, and Data & Model. This framework provides a scalable, extensible foundation for advancing attribution-based explainability and fostering transparent AI systems. Our code is available at: https://github.com/LMBTough/ABE-XAI.","authors":["Zhiyu Zhu","Jiayu Zhang","Zhibo Jin","Fang Chen","Jianlong Zhou"],"url":"https://arxiv.org/abs/2505.06258"}
{"created":"2025-05-13","title":"Fair Clustering with Clusterlets","abstract":"Given their widespread usage in the real world, the fairness of clustering methods has become of major interest. Theoretical results on fair clustering show that fairness enjoys transitivity: given a set of small and fair clusters, a trivial centroid-based clustering algorithm yields a fair clustering. Unfortunately, discovering a suitable starting clustering can be computationally expensive, rather complex or arbitrary.","authors":["Mattia Setzu","Riccardo Guidotti"],"url":"https://arxiv.org/abs/2505.06259"}
{"created":"2025-05-13","title":"Modeling supply chain compliance response strategies based on AI synthetic data with structural path regression: A Simulation Study of EU 2027 Mandatory Labor Regulations","abstract":"In the context of the new mandatory labor compliance in the European Union (EU), which will be implemented in 2027, supply chain enterprises face stringent working hour management requirements and compliance risks. In order to scientifically predict the enterprises' coping behaviors and performance outcomes under the policy impact, this paper constructs a methodological framework that integrates the AI synthetic data generation mechanism and structural path regression modeling to simulate the enterprises' strategic transition paths under the new regulations. In terms of research methodology, this paper adopts high-quality simulation data generated based on Monte Carlo mechanism and NIST synthetic data standards to construct a structural path analysis model that includes multiple linear regression, logistic regression, mediation effect and moderating effect. The variable system covers 14 indicators such as enterprise working hours, compliance investment, response speed, automation level, policy dependence, etc. The variable set with explanatory power is screened out through exploratory data analysis (EDA) and VIF multicollinearity elimination. The findings show that compliance investment has a significant positive impact on firm survival and its effect is transmitted through the mediating path of the level of intelligence; meanwhile, firms' dependence on the EU market significantly moderates the strength of this mediating effect. It is concluded that AI synthetic data combined with structural path modeling provides an effective tool for high-intensity regulatory simulation, which can provide a quantitative basis for corporate strategic response, policy design and AI-assisted decision-making in the pre-prediction stage lacking real scenario data. Keywords: AI synthetic data, structural path regression modeling, compliance response strategy, EU 2027 mandatory labor regulation","authors":["Wei Meng"],"url":"https://arxiv.org/abs/2505.06261"}
{"created":"2025-05-13","title":"Dialz: A Python Toolkit for Steering Vectors","abstract":"We introduce Dialz, a framework for advancing research on steering vectors for open-source LLMs, implemented in Python. Steering vectors allow users to modify activations at inference time to amplify or weaken a 'concept', e.g. honesty or positivity, providing a more powerful alternative to prompting or fine-tuning. Dialz supports a diverse set of tasks, including creating contrastive pair datasets, computing and applying steering vectors, and visualizations. Unlike existing libraries, Dialz emphasizes modularity and usability, enabling both rapid prototyping and in-depth analysis. We demonstrate how Dialz can be used to reduce harmful outputs such as stereotypes, while also providing insights into model behaviour across different layers. We release Dialz with full documentation, tutorials, and support for popular open-source models to encourage further research in safe and controllable language generation. Dialz enables faster research cycles and facilitates insights into model interpretability, paving the way for safer, more transparent, and more reliable AI systems.","authors":["Zara Siddique","Liam D. Turner","Luis Espinosa-Anke"],"url":"https://arxiv.org/abs/2505.06262"}
{"created":"2025-05-13","title":"ONERA's CRM WBPN database for machine learning activities, related regression challenge and first results","abstract":"This paper presents a new Computational Fluid Dynamics database, developed at ONERA, to support the advancement of machine learning techniques for aerodynamic field prediction. It contains 468 Reynolds-Averaged Navier-Stokes simulations using the Spalart-Allmaras turbulence model, performed on the NASA/Boeing Common Research Model wing-body-pylon-nacelle configuration. The database spans a wide range of flow conditions, varying Mach number (including transonic regimes), angle of attack (capturing flow separation), and Reynolds number (based on three stagnation pressures, with one setting matching wind tunnel experiments). The quality of the database is assessed, through checking the convergence level of each computation.","authors":["Jacques Peter","Quentin Bennehard","S\\'ebastien Heib","Jean-Luc Hantrais-Gervois","Fr\\'ed\\'eric Mo\\\"ens"],"url":"https://arxiv.org/abs/2505.06265"}
{"created":"2025-05-13","title":"Knowledge Guided Encoder-Decoder Framework Integrating Multiple Physical Models for Agricultural Ecosystem Modeling","abstract":"Agricultural monitoring is critical for ensuring food security, maintaining sustainable farming practices, informing policies on mitigating food shortage, and managing greenhouse gas emissions. Traditional process-based physical models are often designed and implemented for specific situations, and their parameters could also be highly uncertain. In contrast, data-driven models often use black-box structures and does not explicitly model the inter-dependence between different ecological variables. As a result, they require extensive training data and lack generalizability to different tasks with data distribution shifts and inconsistent observed variables. To address the need for more universal models, we propose a knowledge-guided encoder-decoder model, which can predict key crop variables by leveraging knowledge of underlying processes from multiple physical models. The proposed method also integrates a language model to process complex and inconsistent inputs and also utilizes it to implement a model selection mechanism for selectively combining the knowledge from different physical models. Our evaluations on predicting carbon and nitrogen fluxes for multiple sites demonstrate the effectiveness and robustness of the proposed model under various scenarios.","authors":["Qi Cheng","Licheng Liu","Zhang Yao","Hong Mu","Shiyuan Luo","Zhenong Jin","Yiqun Xie","Xiaowei Jia"],"url":"https://arxiv.org/abs/2505.06266"}
{"created":"2025-05-13","title":"AKD : Adversarial Knowledge Distillation For Large Language Models Alignment on Coding tasks","abstract":"The widespread adoption of Large Language Models (LLMs) for code generation, exemplified by GitHub Copilot\\footnote{A coding extension powered by a Code-LLM to assist in code completion tasks} surpassing a million users, highlights the transformative potential of these tools in improving developer productivity. However, this rapid growth also underscores critical concerns regarding the quality, safety, and reliability of the code they generate. As Code-LLMs evolve, they face significant challenges, including the diminishing returns of model scaling and the scarcity of new, high-quality training data. To address these issues, this paper introduces Adversarial Knowledge Distillation (AKD), a novel approach that leverages adversarially generated synthetic datasets to distill the capabilities of larger models into smaller, more efficient ones. By systematically stress-testing and refining the reasoning capabilities of Code-LLMs, AKD provides a framework for enhancing model robustness, reliability, and security while improving their parameter-efficiency. We believe this work represents a critical step toward ensuring dependable automated code generation within the constraints of existing data and the cost-efficiency of model execution.","authors":["Ilyas Oulkadda","Julien Perez"],"url":"https://arxiv.org/abs/2505.06267"}
{"created":"2025-05-13","title":"Cluster-Aware Multi-Round Update for Wireless Federated Learning in Heterogeneous Environments","abstract":"The aggregation efficiency and accuracy of wireless Federated Learning (FL) are significantly affected by resource constraints, especially in heterogeneous environments where devices exhibit distinct data distributions and communication capabilities. This paper proposes a clustering strategy that leverages prior knowledge similarity to group devices with similar data and communication characteristics, mitigating performance degradation from heterogeneity. On this basis, a novel Cluster- Aware Multi-round Update (CAMU) strategy is proposed, which treats clusters as the basic units and adjusts the local update frequency based on the clustered contribution threshold, effectively reducing update bias and enhancing aggregation accuracy. The theoretical convergence of the CAMU strategy is rigorously validated. Meanwhile, based on the convergence upper bound, the local update frequency and transmission power of each cluster are jointly optimized to achieve an optimal balance between computation and communication resources under constrained conditions, significantly improving the convergence efficiency of FL. Experimental results demonstrate that the proposed method effectively improves the model performance of FL in heterogeneous environments and achieves a better balance between communication cost and computational load under limited resources.","authors":["Pengcheng Sun","Erwu Liu","Wei Ni","Kanglei Yu","Rui Wang","Abbas Jamalipour"],"url":"https://arxiv.org/abs/2505.06268"}
{"created":"2025-05-13","title":"A machine learning model for skillful climate system prediction","abstract":"Climate system models (CSMs), through integrating cross-sphere interactions among the atmosphere, ocean, land, and cryosphere, have emerged as pivotal tools for deciphering climate dynamics and improving forecasting capabilities. Recent breakthroughs in artificial intelligence (AI)-driven meteorological modeling have demonstrated remarkable success in single-sphere systems and partially spheres coupled systems. However, the development of a fully coupled AI-based climate system model encompassing atmosphere-ocean-land-sea ice interactions has remained an unresolved challenge. This paper introduces FengShun-CSM, an AI-based CSM model that provides 60-day global daily forecasts for 29 critical variables across atmospheric, oceanic, terrestrial, and cryospheric domains. The model significantly outperforms the European Centre for Medium-Range Weather Forecasts (ECMWF) subseasonal-to-seasonal (S2S) model in predicting most variables, particularly precipitation, land surface, and oceanic components. This enhanced capability is primarily attributed to its improved representation of intra-seasonal variability modes, most notably the Madden-Julian Oscillation (MJO). Remarkably, FengShun-CSM exhibits substantial potential in predicting subseasonal extreme events. Such breakthroughs will advance its applications in meteorological disaster mitigation, marine ecosystem conservation, and agricultural productivity enhancement. Furthermore, it validates the feasibility of developing AI-powered CSMs through machine learning technologies, establishing a transformative paradigm for next-generation Earth system modeling.","authors":["Chenguang Zhou","Lei Chen","Xiaohui Zhong","Bo Lu","Hao Li","Libo Wu","Jie Wu","Jiahui Hu","Zesheng Dou","Pang-Chi Hsu","Xiaoye Zhang"],"url":"https://arxiv.org/abs/2505.06269"}
{"created":"2025-05-13","title":"Importance Analysis for Dynamic Control of Balancing Parameter in a Simple Knowledge Distillation Setting","abstract":"Although deep learning models owe their remarkable success to deep and complex architectures, this very complexity typically comes at the expense of real-time performance. To address this issue, a variety of model compression techniques have been proposed, among which knowledge distillation (KD) stands out for its strong empirical performance. The KD contains two concurrent processes: (i) matching the outputs of a large, pre-trained teacher network and a lightweight student network, and (ii) training the student to solve its designated downstream task. The associated loss functions are termed the distillation loss and the downsteam-task loss, respectively. Numerous prior studies report that KD is most effective when the influence of the distillation loss outweighs that of the downstream-task loss. The influence(or importance) is typically regulated by a balancing parameter. This paper provides a mathematical rationale showing that in a simple KD setting when the loss is decreasing, the balancing parameter should be dynamically adjusted","authors":["Seongmin Kim","Kwanho Kim","Minseung Kim","Kanghyun Jo"],"url":"https://arxiv.org/abs/2505.06270"}
{"created":"2025-05-13","title":"Tri-MTL: A Triple Multitask Learning Approach for Respiratory Disease Diagnosis","abstract":"Auscultation remains a cornerstone of clinical practice, essential for both initial evaluation and continuous monitoring. Clinicians listen to the lung sounds and make a diagnosis by combining the patient's medical history and test results. Given this strong association, multitask learning (MTL) can offer a compelling framework to simultaneously model these relationships, integrating respiratory sound patterns with disease manifestations. While MTL has shown considerable promise in medical applications, a significant research gap remains in understanding the complex interplay between respiratory sounds, disease manifestations, and patient metadata attributes. This study investigates how integrating MTL with cutting-edge deep learning architectures can enhance both respiratory sound classification and disease diagnosis. Specifically, we extend recent findings regarding the beneficial impact of metadata on respiratory sound classification by evaluating its effectiveness within an MTL framework. Our comprehensive experiments reveal significant improvements in both lung sound classification and diagnostic performance when the stethoscope information is incorporated into the MTL architecture.","authors":["June-Woo Kim","Sanghoon Lee","Miika Toikkanen","Daehwan Hwang","Kyunghoon Kim"],"url":"https://arxiv.org/abs/2505.06271"}
{"created":"2025-05-13","title":"A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for Efficient Fine-Tuning","abstract":"As deep learning models expand, the pre-training-fine-tuning paradigm has become the standard approach for handling various downstream tasks. However, shared parameters can lead to diminished performance when dealing with complex datasets involving multiple tasks. While introducing Mixture-of-Experts (MoE) methods has alleviated this issue to some extent, it also significantly increases the number of parameters required for fine-tuning and training time, introducing greater parameter redundancy. To address these challenges, we propose a method for allocating expert numbers based on parameter sensitivity LoRA-SMoE (A Sensitivity-Driven Expert Allocation Method in LoRA-MoE for Efficient Fine-Tuning). This method rapidly assesses the sensitivity of different tasks to parameters by sampling a small amount of data and using gradient information. It then adaptively allocates expert numbers within a given budget. The process maintains comparable memory consumption to LoRA (Low-Rank Adaptation) while ensuring an efficient and resource-friendly fine-tuning procedure. Experimental results demonstrate that compared to SOTA fine-tuning methods, our LoRA-SMoE approach can enhance model performance while reducing the number of trainable parameters. This significantly improves model performance in resource-constrained environments. Additionally, due to its efficient parameter sensitivity evaluation mechanism, LoRA-SMoE requires minimal computational overhead to optimize expert allocation, making it particularly suitable for scenarios with limited computational resources. All the code in this study will be made publicly available following the acceptance of the paper for publication. Source code is at https://github.com/EMLS-ICTCAS/LoRA-SMoE","authors":["Junzhou Xu","Boyu Diao"],"url":"https://arxiv.org/abs/2505.06272"}
{"created":"2025-05-13","title":"Policy-labeled Preference Learning: Is Preference Enough for RLHF?","abstract":"To design rewards that align with human goals, Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent technique for learning reward functions from human preferences and optimizing policies via reinforcement learning algorithms. However, existing RLHF methods often misinterpret trajectories as being generated by an optimal policy, causing inaccurate likelihood estimation and suboptimal learning. Inspired by Direct Preference Optimization framework which directly learns optimal policy without explicit reward, we propose policy-labeled preference learning (PPL), to resolve likelihood mismatch issues by modeling human preferences with regret, which reflects behavior policy information. We also provide a contrastive KL regularization, derived from regret-based principles, to enhance RLHF in sequential decision making. Experiments in high-dimensional continuous control tasks demonstrate PPL's significant improvements in offline RLHF performance and its effectiveness in online settings.","authors":["Taehyun Cho","Seokhun Ju","Seungyub Han","Dohyeong Kim","Kyungjae Lee","Jungwoo Lee"],"url":"https://arxiv.org/abs/2505.06273"}
{"created":"2025-05-13","title":"PARM: Multi-Objective Test-Time Alignment via Preference-Aware Autoregressive Reward Model","abstract":"Multi-objective test-time alignment aims to adapt large language models (LLMs) to diverse multi-dimensional user preferences during inference while keeping LLMs frozen. Recently, GenARM (Xu et al., 2025) first independently trains Autoregressive Reward Models (ARMs) for each preference dimension without awareness of each other, then combines their outputs based on user-specific preference vectors during inference to achieve multi-objective test-time alignment, leading to two key limitations: the need for \\textit{multiple} ARMs increases the inference cost, and the separate training of ARMs causes the misalignment between the guided generation and the user preferences. To address these issues, we propose Preference-aware ARM (PARM), a single unified ARM trained across all preference dimensions. PARM uses our proposed Preference-Aware Bilinear Low-Rank Adaptation (PBLoRA), which employs a bilinear form to condition the ARM on preference vectors, enabling it to achieve precise control over preference trade-offs during inference. Experiments demonstrate that PARM reduces inference costs and achieves better alignment with preference vectors compared with existing methods. Additionally, PARM enables weak-to-strong guidance, allowing a smaller PARM to guide a larger frozen LLM without expensive training, making multi-objective alignment accessible with limited computing resources. The code is available at https://github.com/Baijiong-Lin/PARM.","authors":["Baijiong Lin","Weisen Jiang","Yuancheng Xu","Hao Chen","Ying-Cong Chen"],"url":"https://arxiv.org/abs/2505.06274"}
{"created":"2025-05-13","title":"Attonsecond Streaking Phase Retrieval Via Deep Learning Methods","abstract":"Attosecond streaking phase retrieval is essential for resolving electron dynamics on sub-femtosecond time scales yet traditional algorithms rely on iterative minimization and central momentum approximations that degrade accuracy for broadband pulses. In this work phase retrieval is reformulated as a supervised computer-vision problem and four neural architectures are systematically compared. A convolutional network demonstrates strong sensitivity to local streak edges but lacks global context; a vision transformer captures long-range delay-energy correlations at the expense of local inductive bias; a hybrid CNN-ViT model unites local feature extraction and full-graph attention; and a capsule network further enforces spatial pose agreement through dynamic routing. A theoretical analysis introduces local, global and positional sensitivity measures and derives surrogate error bounds that predict the strict ordering $CNN<Capsule$. Controlled experiments on synthetic streaking spectrograms confirm this hierarchy, with the capsule network achieving the highest retrieval fidelity. Looking forward, embedding the strong-field integral into physics-informed neural networks and exploring photonic hardware implementations promise pathways toward real-time attosecond pulse characterization under demanding experimental conditions.","authors":["Yuzhou Zhu","Zheng Zhang","Ruyi Zhang","Liang Zhou"],"url":"https://arxiv.org/abs/2505.06275"}
{"created":"2025-05-13","title":"SynSHRP2: A Synthetic Multimodal Benchmark for Driving Safety-critical Events Derived from Real-world Driving Data","abstract":"Driving-related safety-critical events (SCEs), including crashes and near-crashes, provide essential insights for the development and safety evaluation of automated driving systems. However, two major challenges limit their accessibility: the rarity of SCEs and the presence of sensitive privacy information in the data. The Second Strategic Highway Research Program (SHRP 2) Naturalistic Driving Study (NDS), the largest NDS to date, collected millions of hours of multimodal, high-resolution, high-frequency driving data from thousands of participants, capturing thousands of SCEs. While this dataset is invaluable for safety research, privacy concerns and data use restrictions significantly limit public access to the raw data. To address these challenges, we introduce SynSHRP2, a publicly available, synthetic, multimodal driving dataset containing over 1874 crashes and 6924 near-crashes derived from the SHRP 2 NDS. The dataset features de-identified keyframes generated using Stable Diffusion and ControlNet, ensuring the preservation of critical safety-related information while eliminating personally identifiable data. Additionally, SynSHRP2 includes detailed annotations on SCE type, environmental and traffic conditions, and time-series kinematic data spanning 5 seconds before and during each event. Synchronized keyframes and narrative descriptions further enhance its usability. This paper presents two benchmarks for event attribute classification and scene understanding, demonstrating the potential applications of SynSHRP2 in advancing safety research and automated driving system development.","authors":["Liang Shi","Boyu Jiang","Zhenyuan Yuan","Miguel A. Perez","Feng Guo"],"url":"https://arxiv.org/abs/2505.06276"}
{"created":"2025-05-13","title":"Robust Understanding of Human-Robot Social Interactions through Multimodal Distillation","abstract":"The need for social robots and agents to interact and assist humans is growing steadily. To be able to successfully interact with humans, they need to understand and analyse socially interactive scenes from their (robot's) perspective. Works that model social situations between humans and agents are few; and even those existing ones are often too computationally intensive to be suitable for deployment in real time or on real world scenarios with limited available information. We propose a robust knowledge distillation framework that models social interactions through various multimodal cues, yet is robust against incomplete and noisy information during inference. Our teacher model is trained with multimodal input (body, face and hand gestures, gaze, raw images) that transfers knowledge to a student model that relies solely on body pose. Extensive experiments on two publicly available human-robot interaction datasets demonstrate that the our student model achieves an average accuracy gain of 14.75\\% over relevant baselines on multiple downstream social understanding task even with up to 51\\% of its input being corrupted. The student model is highly efficient: it is $<1$\\% in size of the teacher model in terms of parameters and uses $\\sim 0.5$\\textperthousand~FLOPs of that in the teacher model. Our code will be made public during publication.","authors":["Tongfei Bian","Mathieu Chollet","Tanaya Guha"],"url":"https://arxiv.org/abs/2505.06278"}
{"created":"2025-05-13","title":"Interpretable Learning Dynamics in Unsupervised Reinforcement Learning","abstract":"We present an interpretability framework for unsupervised reinforcement learning (URL) agents, aimed at understanding how intrinsic motivation shapes attention, behavior, and representation learning. We analyze five agents DQN, RND, ICM, PPO, and a Transformer-RND variant trained on procedurally generated environments, using Grad-CAM, Layer-wise Relevance Propagation (LRP), exploration metrics, and latent space clustering. To capture how agents perceive and adapt over time, we introduce two metrics: attention diversity, which measures the spatial breadth of focus, and attention change rate, which quantifies temporal shifts in attention. Our findings show that curiosity-driven agents display broader, more dynamic attention and exploratory behavior than their extrinsically motivated counterparts. Among them, TransformerRND combines wide attention, high exploration coverage, and compact, structured latent representations. Our results highlight the influence of architectural inductive biases and training signals on internal agent dynamics. Beyond reward-centric evaluation, the proposed framework offers diagnostic tools to probe perception and abstraction in RL agents, enabling more interpretable and generalizable behavior.","authors":["Shashwat Pandey"],"url":"https://arxiv.org/abs/2505.06279"}
{"created":"2025-05-13","title":"Show or Tell? A Benchmark To Evaluate Visual and Textual Prompts in Semantic Segmentation","abstract":"Prompt engineering has shown remarkable success with large language models, yet its systematic exploration in computer vision remains limited. In semantic segmentation, both textual and visual prompts offer distinct advantages: textual prompts through open-vocabulary methods allow segmentation of arbitrary categories, while visual reference prompts provide intuitive reference examples. However, existing benchmarks evaluate these modalities in isolation, without direct comparison under identical conditions. We present Show or Tell (SoT), a novel benchmark specifically designed to evaluate both visual and textual prompts for semantic segmentation across 14 datasets spanning 7 diverse domains (common scenes, urban, food, waste, parts, tools, and land-cover). We evaluate 5 open-vocabulary methods and 4 visual reference prompt approaches, adapting the latter to handle multi-class segmentation through a confidence-based mask merging strategy. Our extensive experiments reveal that open-vocabulary methods excel with common concepts easily described by text but struggle with complex domains like tools, while visual reference prompt methods achieve good average results but exhibit high variability depending on the input prompt. Through comprehensive quantitative and qualitative analysis, we identify the strengths and weaknesses of both prompting modalities, providing valuable insights to guide future research in vision foundation models for segmentation tasks.","authors":["Gabriele Rosi","Fabio Cermelli"],"url":"https://arxiv.org/abs/2505.06280"}
{"created":"2025-05-13","title":"A Data-Driven Probabilistic Framework for Cascading Urban Risk Analysis Using Bayesian Networks","abstract":"The increasing complexity of cascading risks in urban systems necessitates robust, data-driven frameworks to model interdependencies across multiple domains. This study presents a foundational Bayesian network-based approach for analyzing cross-domain risk propagation across key urban domains, including air, water, electricity, agriculture, health, infrastructure, weather, and climate. Directed Acyclic Graphs (DAGs) are constructed using Bayesian Belief Networks (BBNs), with structure learning guided by Hill-Climbing search optimized through Bayesian Information Criterion (BIC) and K2 scoring. The framework is trained on a hybrid dataset that combines real-world urban indicators with synthetically generated data from Generative Adversarial Networks (GANs), and is further balanced using the Synthetic Minority Over-sampling Technique (SMOTE). Conditional Probability Tables (CPTs) derived from the learned structures enable interpretable probabilistic reasoning and quantify the likelihood of cascading failures. The results identify key intra- and inter-domain risk factors and demonstrate the framework's utility for proactive urban resilience planning. This work establishes a scalable, interpretable foundation for cascading risk assessment and serves as a basis for future empirical research in this emerging interdisciplinary field.","authors":["Chunduru Rohith Kumar","PHD Surya Shanmuk","Prabhala Naga Srinivas","Sri Venkatesh Lankalapalli","Debasis Dwibedy"],"url":"https://arxiv.org/abs/2505.06281"}
{"created":"2025-05-13","title":"InfoNCE is a Free Lunch for Semantically guided Graph Contrastive Learning","abstract":"As an important graph pre-training method, Graph Contrastive Learning (GCL) continues to play a crucial role in the ongoing surge of research on graph foundation models or LLM as enhancer for graphs. Traditional GCL optimizes InfoNCE by using augmentations to define self-supervised tasks, treating augmented pairs as positive samples and others as negative. However, this leads to semantically similar pairs being classified as negative, causing significant sampling bias and limiting performance. In this paper, we argue that GCL is essentially a Positive-Unlabeled (PU) learning problem, where the definition of self-supervised tasks should be semantically guided, i.e., augmented samples with similar semantics are considered positive, while others, with unknown semantics, are treated as unlabeled. From this perspective, the key lies in how to extract semantic information. To achieve this, we propose IFL-GCL, using InfoNCE as a \"free lunch\" to extract semantic information. Specifically, We first prove that under InfoNCE, the representation similarity of node pairs aligns with the probability that the corresponding contrastive sample is positive. Then we redefine the maximum likelihood objective based on the corrected samples, leading to a new InfoNCE loss function. Extensive experiments on both the graph pretraining framework and LLM as an enhancer show significantly improvements of IFL-GCL in both IID and OOD scenarios, achieving up to a 9.05% improvement, validating the effectiveness of semantically guided. Code for IFL-GCL is publicly available at: https://github.com/Camel-Prince/IFL-GCL.","authors":["Zixu Wang","Bingbing Xu","Yige Yuan","Huawei Shen","Xueqi Cheng"],"url":"https://arxiv.org/abs/2505.06282"}
{"created":"2025-05-13","title":"Soft causal learning for generalized molecule property prediction: An environment perspective","abstract":"Learning on molecule graphs has become an increasingly important topic in AI for science, which takes full advantage of AI to facilitate scientific discovery. Existing solutions on modeling molecules utilize Graph Neural Networks (GNNs) to achieve representations but they mostly fail to adapt models to out-of-distribution (OOD) samples. Although recent advances on OOD-oriented graph learning have discovered the invariant rationale on graphs, they still ignore three important issues, i.e., 1) the expanding atom patterns regarding environments on graphs lead to failures of invariant rationale based models, 2) the associations between discovered molecular subgraphs and corresponding properties are complex where causal substructures cannot fully interpret the labels. 3) the interactions between environments and invariances can influence with each other thus are challenging to be modeled. To this end, we propose a soft causal learning framework, to tackle the unresolved OOD challenge in molecular science, from the perspective of fully modeling the molecule environments and bypassing the invariant subgraphs. Specifically, we first incorporate chemistry theories into our graph growth generator to imitate expaned environments, and then devise an GIB-based objective to disentangle environment from whole graphs and finally introduce a cross-attention based soft causal interaction, which allows dynamic interactions between environments and invariances. We perform experiments on seven datasets by imitating different kinds of OOD generalization scenarios. Extensive comparison, ablation experiments as well as visualized case studies demonstrate well generalization ability of our proposal.","authors":["Limin Li","Kuo Yang","Wenjie Du","Pengkun Wang","Zhengyang Zhou","Yang Wang"],"url":"https://arxiv.org/abs/2505.06283"}
{"created":"2025-05-13","title":"DMRL: Data- and Model-aware Reward Learning for Data Extraction","abstract":"Large language models (LLMs) are inherently vulnerable to unintended privacy breaches. Consequently, systematic red-teaming research is essential for developing robust defense mechanisms. However, current data extraction methods suffer from several limitations: (1) rely on dataset duplicates (addressable via deduplication), (2) depend on prompt engineering (now countered by detection and defense), and (3) rely on random-search adversarial generation. To address these challenges, we propose DMRL, a Data- and Model-aware Reward Learning approach for data extraction. This technique leverages inverse reinforcement learning to extract sensitive data from LLMs. Our method consists of two main components: (1) constructing an introspective reasoning dataset that captures leakage mindsets to guide model behavior, and (2) training reward models with Group Relative Policy Optimization (GRPO), dynamically tuning optimization based on task difficulty at both the data and model levels. Comprehensive experiments across various LLMs demonstrate that DMRL outperforms all baseline methods in data extraction performance.","authors":["Zhiqiang Wang","Ruoxi Cheng"],"url":"https://arxiv.org/abs/2505.06284"}
{"created":"2025-05-13","title":"BedreFlyt: Improving Patient Flows through Hospital Wards with Digital Twins","abstract":"Digital twins are emerging as a valuable tool for short-term decision-making as well as for long-term strategic planning across numerous domains, including process industry, energy, space, transport, and healthcare. This paper reports on our ongoing work on designing a digital twin to enhance resource planning, e.g., for the in-patient ward needs in hospitals. By leveraging executable formal models for system exploration, ontologies for knowledge representation and an SMT solver for constraint satisfiability, our approach aims to explore hypothetical \"what-if\" scenarios to improve strategic planning processes, as well as to solve concrete, short-term decision-making tasks. Our proposed solution uses the executable formal model to turn a stream of arriving patients, that need to be hospitalized, into a stream of optimization problems, e.g., capturing daily inpatient ward needs, that can be solved by SMT techniques. The knowledge base, which formalizes domain knowledge, is used to model the needed configuration in the digital twin, allowing the twin to support both short-term decision-making and long-term strategic planning by generating scenarios spanning average-case as well as worst-case resource needs, depending on the expected treatment of patients, as well as ranging over variations in available resources, e.g., bed distribution in different rooms. We illustrate our digital twin architecture by considering the problem of bed bay allocation in a hospital ward.","authors":["Riccardo Sieve (Dept. of Informatics","University of Oslo)","Paul Kobialka (Dept. of Informatics","University of Oslo)","Laura Slaughter (dScience Center","University of Oslo)","Rudolf Schlatte (Dept. of Informatics","University of Oslo)","Einar Broch Johnsen (Dept. of Informatics","University of Oslo)","Silvia Lizeth Tapia Tarifa (Dept. of Informatics","University of Oslo)"],"url":"https://arxiv.org/abs/2505.06287"}
{"created":"2025-05-13","title":"IIKL: Isometric Immersion Kernel Learning with Riemannian Manifold for Geometric Preservation","abstract":"Geometric representation learning in preserving the intrinsic geometric and topological properties for discrete non-Euclidean data is crucial in scientific applications. Previous research generally mapped non-Euclidean discrete data into Euclidean space during representation learning, which may lead to the loss of some critical geometric information. In this paper, we propose a novel Isometric Immersion Kernel Learning (IIKL) method to build Riemannian manifold and isometrically induce Riemannian metric from discrete non-Euclidean data. We prove that Isometric immersion is equivalent to the kernel function in the tangent bundle on the manifold, which explicitly guarantees the invariance of the inner product between vectors in the arbitrary tangent space throughout the learning process, thus maintaining the geometric structure of the original data. Moreover, a novel parameterized learning model based on IIKL is introduced, and an alternating training method for this model is derived using Maximum Likelihood Estimation (MLE), ensuring efficient convergence. Experimental results proved that using the learned Riemannian manifold and its metric, our model preserved the intrinsic geometric representation of data in both 3D and high-dimensional datasets successfully, and significantly improved the accuracy of downstream tasks, such as data reconstruction and classification. It is showed that our method could reduce the inner product invariant loss by more than 90% compared to state-of-the-art (SOTA) methods, also achieved an average 40% improvement in downstream reconstruction accuracy and a 90% reduction in error for geometric metrics involving isometric and conformal.","authors":["Zihao Chen","Wenyong Wang","Jiachen Yang","Yu Xiang"],"url":"https://arxiv.org/abs/2505.06288"}
{"created":"2025-05-13","title":"Edge-Optimized Deep Learning & Pattern Recognition Techniques for Non-Intrusive Load Monitoring of Energy Time Series","abstract":"The growing global energy demand and the urgent need for sustainability call for innovative ways to boost energy efficiency. While advanced energy-saving systems exist, they often fall short without user engagement. Providing feedback on energy consumption behavior is key to promoting sustainable practices. Non-Intrusive Load Monitoring (NILM) offers a promising solution by disaggregating total household energy usage, recorded by a central smart meter, into appliance-level data. This empowers users to optimize consumption. Advances in AI, IoT, and smart meter adoption have further enhanced NILM's potential.","authors":["Sotirios Athanasoulias"],"url":"https://arxiv.org/abs/2505.06289"}
{"created":"2025-05-13","title":"UniCO: Towards a Unified Model for Combinatorial Optimization Problems","abstract":"Combinatorial Optimization (CO) encompasses a wide range of problems that arise in many real-world scenarios. While significant progress has been made in developing learning-based methods for specialized CO problems, a unified model with a single architecture and parameter set for diverse CO problems remains elusive. Such a model would offer substantial advantages in terms of efficiency and convenience. In this paper, we introduce UniCO, a unified model for solving various CO problems. Inspired by the success of next-token prediction, we frame each problem-solving process as a Markov Decision Process (MDP), tokenize the corresponding sequential trajectory data, and train the model using a transformer backbone. To reduce token length in the trajectory data, we propose a CO-prefix design that aggregates static problem features. To address the heterogeneity of state and action tokens within the MDP, we employ a two-stage self-supervised learning approach. In this approach, a dynamic prediction model is first trained and then serves as a pre-trained model for subsequent policy generation. Experiments across 10 CO problems showcase the versatility of UniCO, emphasizing its ability to generalize to new, unseen problems with minimal fine-tuning, achieving even few-shot or zero-shot performance. Our framework offers a valuable complement to existing neural CO methods that focus on optimizing performance for individual problems.","authors":["Zefang Zong","Xiaochen Wei","Guozhen Zhang","Chen Gao","Huandong Wang","Yong Li"],"url":"https://arxiv.org/abs/2505.06290"}
{"created":"2025-05-13","title":"Spatio-Temporal Graph Neural Network for Urban Spaces: Interpolating Citywide Traffic Volume","abstract":"Reliable street-level traffic volume data, covering multiple modes of transportation, helps urban planning by informing decisions on infrastructure improvements, traffic management, and public transportation. Yet, traffic sensors measuring traffic volume are typically scarcely located, due to their high deployment and maintenance costs. To address this, interpolation methods can estimate traffic volumes at unobserved locations using available data. Graph Neural Networks have shown strong performance in traffic volume forecasting, particularly on highways and major arterial networks. Applying them to urban settings, however, presents unique challenges: urban networks exhibit greater structural diversity, traffic volumes are highly overdispersed with many zeros, the best way to account for spatial dependencies remains unclear, and sensor coverage is often very sparse. We introduce the Graph Neural Network for Urban Interpolation (GNNUI), a novel urban traffic volume estimation approach. GNNUI employs a masking algorithm to learn interpolation, integrates node features to capture functional roles, and uses a loss function tailored to zero-inflated traffic distributions. In addition to the model, we introduce two new open, large-scale urban traffic volume benchmarks, covering different transportation modes: Strava cycling data from Berlin and New York City taxi data. GNNUI outperforms recent, some graph-based, interpolation methods across metrics (MAE, RMSE, true-zero rate, Kullback-Leibler divergence) and remains robust from 90% to 1% sensor coverage. On Strava, for instance, MAE rises only from 7.1 to 10.5, on Taxi from 23.0 to 40.4, demonstrating strong performance under extreme data scarcity, common in real-world urban settings. We also examine how graph connectivity choices influence model accuracy.","authors":["Silke K. Kaiser","Filipe Rodrigues","Carlos Lima Azevedo","Lynn H. Kaack"],"url":"https://arxiv.org/abs/2505.06292"}
{"created":"2025-05-13","title":"RAAC panels can suddenly collapse before any warning of corrosion-induced surface cracking","abstract":"The collapse of reinforced autoclaved aerated concrete (RAAC) panels has attracted considerable public and academic interest. As detailed experimental data are not yet available and replicating the natural corrosion process requires years or decades, computational modelling is essential to understand under which conditions corrosion remains concealed. The very high porosity of RAAC is widely suspected to be a major contributing factor. However, current corrosion-induced cracking models are known to struggle with capturing the role of concrete porosity. To remedy this critical deficiency, we propose to enrich corrosion-induced cracking modelling with the analytical solution of reactive transport equations governing the precipitation of rust and a porosity-dependent description of diffusivity. With this, the corrosion concealment in RAAC panels is studied computationally for the first time, revealing that RAAC panels can suddenly collapse before any warning of corrosion-induced surface cracking and allowing to map the conditions most likely to result in sudden collapse.","authors":["E. Korec","P. Grassl","M. Jirasek","H. S. Wong","E. Mart\\'inez-Pa\\~neda"],"url":"https://arxiv.org/abs/2505.06294"}
{"created":"2025-05-13","title":"Benchmarking Traditional Machine Learning and Deep Learning Models for Fault Detection in Power Transformers","abstract":"Accurate diagnosis of power transformer faults is essential for ensuring the stability and safety of electrical power systems. This study presents a comparative analysis of conventional machine learning (ML) algorithms and deep learning (DL) algorithms for fault classification of power transformers. Using a condition-monitored dataset spanning 10 months, various gas concentration features were normalized and used to train five ML classifiers: Support Vector Machine (SVM), k-Nearest Neighbors (KNN), Random Forest (RF), XGBoost, and Artificial Neural Network (ANN). In addition, four DL models were evaluated: Long Short-Term Memory (LSTM), Gated Recurrent Unit (GRU), One-Dimensional Convolutional Neural Network (1D-CNN), and TabNet. Experimental results show that both ML and DL approaches performed comparably. The RF model achieved the highest ML accuracy at 86.82%, while the 1D-CNN model attained a close 86.30%.","authors":["Bhuvan Saravanan","Pasanth Kumar M D","Aarnesh Vengateson"],"url":"https://arxiv.org/abs/2505.06295"}
{"created":"2025-05-13","title":"Lossless Compression of Large Language Model-Generated Text via Next-Token Prediction","abstract":"As large language models (LLMs) continue to be deployed and utilized across domains, the volume of LLM-generated data is growing rapidly. This trend highlights the increasing importance of effective and lossless compression for such data in modern text management systems. However, compressing LLM-generated data presents unique challenges compared to traditional human- or machine-generated content. Traditional machine-generated data is typically derived from computational processes or device outputs, often highly structured and limited to low-level elements like labels or numerical values. This structure enables conventional lossless compressors to perform efficiently. In contrast, LLM-generated data is more complex and diverse, requiring new approaches for effective compression. In this work, we conduct the first systematic investigation of lossless compression techniques tailored specifically to LLM-generated data. Notably, because LLMs are trained via next-token prediction, we find that LLM-generated data is highly predictable for the models themselves. This predictability enables LLMs to serve as efficient compressors of their own outputs. Through extensive experiments with 14 representative LLMs and 8 LLM-generated datasets from diverse domains, we show that LLM-based prediction methods achieve remarkable compression rates, exceeding 20x, far surpassing the 3x rate achieved by Gzip, a widely used general-purpose compressor. Furthermore, this advantage holds across different LLM sizes and dataset types, demonstrating the robustness and practicality of LLM-based methods in lossless text compression under generative AI workloads.","authors":["Yu Mao","Holger Pirk","Chun Jason Xue"],"url":"https://arxiv.org/abs/2505.06297"}
{"created":"2025-05-13","title":"Input-Specific and Universal Adversarial Attack Generation for Spiking Neural Networks in the Spiking Domain","abstract":"As Spiking Neural Networks (SNNs) gain traction across various applications, understanding their security vulnerabilities becomes increasingly important. In this work, we focus on the adversarial attacks, which is perhaps the most concerning threat. An adversarial attack aims at finding a subtle input perturbation to fool the network's decision-making. We propose two novel adversarial attack algorithms for SNNs: an input-specific attack that crafts adversarial samples from specific dataset inputs and a universal attack that generates a reusable patch capable of inducing misclassification across most inputs, thus offering practical feasibility for real-time deployment. The algorithms are gradient-based operating in the spiking domain proving to be effective across different evaluation metrics, such as adversarial accuracy, stealthiness, and generation time. Experimental results on two widely used neuromorphic vision datasets, NMNIST and IBM DVS Gesture, show that our proposed attacks surpass in all metrics all existing state-of-the-art methods. Additionally, we present the first demonstration of adversarial attack generation in the sound domain using the SHD dataset.","authors":["Spyridon Raptis","Haralampos-G. Stratigopoulos"],"url":"https://arxiv.org/abs/2505.06299"}
{"created":"2025-05-13","title":"ARDNS-FN-Quantum: A Quantum-Enhanced Reinforcement Learning Framework with Cognitive-Inspired Adaptive Exploration for Dynamic Environments","abstract":"Reinforcement learning (RL) has transformed sequential decision making, yet traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy Optimization (PPO) often struggle with efficient exploration, stability, and adaptability in dynamic environments. This study presents ARDNS-FN-Quantum (Adaptive Reward-Driven Neural Simulator with Quantum enhancement), a novel framework that integrates a 2-qubit quantum circuit for action selection, a dual-memory system inspired by human cognition, and adaptive exploration strategies modulated by reward variance and curiosity. Evaluated in a 10X10 grid-world over 20,000 episodes, ARDNS-FN-Quantum achieves a 99.5% success rate (versus 81.3% for DQN and 97.0% for PPO), a mean reward of 9.0528 across all episodes (versus 1.2941 for DQN and 7.6196 for PPO), and an average of 46.7 steps to goal (versus 135.9 for DQN and 62.5 for PPO). In the last 100 episodes, it records a mean reward of 9.1652 (versus 7.0916 for DQN and 9.0310 for PPO) and 37.2 steps to goal (versus 52.7 for DQN and 53.4 for PPO). Graphical analyses, including learning curves, steps-to-goal trends, reward variance, and reward distributions, demonstrate ARDNS-FN-Quantum's superior stability (reward variance 5.424 across all episodes versus 252.262 for DQN and 76.583 for PPO) and efficiency. By bridging quantum computing, cognitive science, and RL, ARDNS-FN-Quantum offers a scalable, human-like approach to adaptive learning in uncertain environments, with potential applications in robotics, autonomous systems, and decision-making under uncertainty.","authors":["Umberto Gon\\c{c}alves de Sousa"],"url":"https://arxiv.org/abs/2505.06300"}
{"created":"2025-05-13","title":"Domain-Adversarial Anatomical Graph Networks for Cross-User Human Activity Recognition","abstract":"Cross-user variability in Human Activity Recognition (HAR) remains a critical challenge due to differences in sensor placement, body dynamics, and behavioral patterns. Traditional methods often fail to capture biomechanical invariants that persist across users, limiting their generalization capability. We propose an Edge-Enhanced Graph-Based Adversarial Domain Generalization (EEG-ADG) framework that integrates anatomical correlation knowledge into a unified graph neural network (GNN) architecture. By modeling three biomechanically motivated relationships together-Interconnected Units, Analogous Units, and Lateral Units-our method encodes domain-invariant features while addressing user-specific variability through Variational Edge Feature Extractor. A Gradient Reversal Layer (GRL) enforces adversarial domain generalization, ensuring robustness to unseen users. Extensive experiments on OPPORTUNITY and DSADS datasets demonstrate state-of-the-art performance. Our work bridges biomechanical principles with graph-based adversarial learning by integrating information fusion techniques. This fusion of information underpins our unified and generalized model for cross-user HAR.","authors":["Xiaozhou Ye","Kevin I-Kai Wang"],"url":"https://arxiv.org/abs/2505.06301"}
{"created":"2025-05-13","title":"QiMeng-TensorOp: Automatically Generating High-Performance Tensor Operators with Hardware Primitives","abstract":"Computation-intensive tensor operators constitute over 90\\% of the computations in Large Language Models (LLMs) and Deep Neural Networks.Automatically and efficiently generating high-performance tensor operators with hardware primitives is crucial for diverse and ever-evolving hardware architectures like RISC-V, ARM, and GPUs, as manually optimized implementation takes at least months and lacks portability.LLMs excel at generating high-level language codes, but they struggle to fully comprehend hardware characteristics and produce high-performance tensor operators. We introduce a tensor-operator auto-generation framework with a one-line user prompt (QiMeng-TensorOp), which enables LLMs to automatically exploit hardware characteristics to generate tensor operators with hardware primitives, and tune parameters for optimal performance across diverse hardware. Experimental results on various hardware platforms, SOTA LLMs, and typical tensor operators demonstrate that QiMeng-TensorOp effectively unleashes the computing capability of various hardware platforms, and automatically generates tensor operators of superior performance. Compared with vanilla LLMs, QiMeng-TensorOp achieves up to $1291 \\times$ performance improvement. Even compared with human experts, QiMeng-TensorOp could reach $251 \\%$ of OpenBLAS on RISC-V CPUs, and $124 \\%$ of cuBLAS on NVIDIA GPUs. Additionally, QiMeng-TensorOp also significantly reduces development costs by $200 \\times$ compared with human experts.","authors":["Xuzhi Zhang","Shaohui Peng","Qirui Zhou","Yuanbo Wen","Qi Guo","Ruizhi Chen","Xinguo Zhu","Weiqiang Xiong","Haixin Chen","Congying Ma","Ke Gao","Chen Zhao","Yanjun Wu","Yunji Chen","Ling Li"],"url":"https://arxiv.org/abs/2505.06302"}
{"created":"2025-05-13","title":"Collaborative Multi-LoRA Experts with Achievement-based Multi-Tasks Loss for Unified Multimodal Information Extraction","abstract":"Multimodal Information Extraction (MIE) has gained attention for extracting structured information from multimedia sources. Traditional methods tackle MIE tasks separately, missing opportunities to share knowledge across tasks. Recent approaches unify these tasks into a generation problem using instruction-based T5 models with visual adaptors, optimized through full-parameter fine-tuning. However, this method is computationally intensive, and multi-task fine-tuning often faces gradient conflicts, limiting performance. To address these challenges, we propose collaborative multi-LoRA experts with achievement-based multi-task loss (C-LoRAE) for MIE tasks. C-LoRAE extends the low-rank adaptation (LoRA) method by incorporating a universal expert to learn shared multimodal knowledge from cross-MIE tasks and task-specific experts to learn specialized instructional task features. This configuration enhances the model's generalization ability across multiple tasks while maintaining the independence of various instruction tasks and mitigating gradient conflicts. Additionally, we propose an achievement-based multi-task loss to balance training progress across tasks, addressing the imbalance caused by varying numbers of training samples in MIE tasks. Experimental results on seven benchmark datasets across three key MIE tasks demonstrate that C-LoRAE achieves superior overall performance compared to traditional fine-tuning methods and LoRA methods while utilizing a comparable number of training parameters to LoRA.","authors":["Li Yuan","Yi Cai","Xudong Shen","Qing Li","Qingbao Huang","Zikun Deng","Tao Wang"],"url":"https://arxiv.org/abs/2505.06303"}
{"created":"2025-05-13","title":"RAP-SM: Robust Adversarial Prompt via Shadow Models for Copyright Verification of Large Language Models","abstract":"Recent advances in large language models (LLMs) have underscored the importance of safeguarding intellectual property rights through robust fingerprinting techniques. Traditional fingerprint verification approaches typically focus on a single model, seeking to improve the robustness of its fingerprint.However, these single-model methods often struggle to capture intrinsic commonalities across multiple related models. In this paper, we propose RAP-SM (Robust Adversarial Prompt via Shadow Models), a novel framework that extracts a public fingerprint for an entire series of LLMs. Experimental results demonstrate that RAP-SM effectively captures the intrinsic commonalities among different models while exhibiting strong adversarial robustness. Our findings suggest that RAP-SM presents a valuable avenue for scalable fingerprint verification, offering enhanced protection against potential model breaches in the era of increasingly prevalent LLMs.","authors":["Zhenhua Xu","Zhebo Wang","Maike Li","Wenpeng Xing","Chunqiang Hu","Chen Zhi","Meng Han"],"url":"https://arxiv.org/abs/2505.06304"}
{"created":"2025-05-13","title":"User Behavior Analysis in Privacy Protection with Large Language Models: A Study on Privacy Preferences with Limited Data","abstract":"With the widespread application of large language models (LLMs), user privacy protection has become a significant research topic. Existing privacy preference modeling methods often rely on large-scale user data, making effective privacy preference analysis challenging in data-limited environments. This study explores how LLMs can analyze user behavior related to privacy protection in scenarios with limited data and proposes a method that integrates Few-shot Learning and Privacy Computing to model user privacy preferences. The research utilizes anonymized user privacy settings data, survey responses, and simulated data, comparing the performance of traditional modeling approaches with LLM-based methods. Experimental results demonstrate that, even with limited data, LLMs significantly improve the accuracy of privacy preference modeling. Additionally, incorporating Differential Privacy and Federated Learning further reduces the risk of user data exposure. The findings provide new insights into the application of LLMs in privacy protection and offer theoretical support for advancing privacy computing and user behavior analysis.","authors":["Haowei Yang","Qingyi Lu","Yang Wang","Sibei Liu","Jiayun Zheng","Ao Xiang"],"url":"https://arxiv.org/abs/2505.06305"}
{"created":"2025-05-13","title":"Large Language Model-driven Security Assistant for Internet of Things via Chain-of-Thought","abstract":"The rapid development of Internet of Things (IoT) technology has transformed people's way of life and has a profound impact on both production and daily activities. However, with the rapid advancement of IoT technology, the security of IoT devices has become an unavoidable issue in both research and applications. Although some efforts have been made to detect or mitigate IoT security vulnerabilities, they often struggle to adapt to the complexity of IoT environments, especially when dealing with dynamic security scenarios. How to automatically, efficiently, and accurately understand these vulnerabilities remains a challenge. To address this, we propose an IoT security assistant driven by Large Language Model (LLM), which enhances the LLM's understanding of IoT security vulnerabilities and related threats. The aim of the ICoT method we propose is to enable the LLM to understand security issues by breaking down the various dimensions of security vulnerabilities and generating responses tailored to the user's specific needs and expertise level. By incorporating ICoT, LLM can gradually analyze and reason through complex security scenarios, resulting in more accurate, in-depth, and personalized security recommendations and solutions. Experimental results show that, compared to methods relying solely on LLM, our proposed LLM-driven IoT security assistant significantly improves the understanding of IoT security issues through the ICoT approach and provides personalized solutions based on the user's identity, demonstrating higher accuracy and reliability.","authors":["Mingfei Zeng","Ming Xie","Xixi Zheng","Chunhai Li","Chuan Zhang","Liehuang Zhu"],"url":"https://arxiv.org/abs/2505.06307"}
{"created":"2025-05-13","title":"Defending against Indirect Prompt Injection by Instruction Detection","abstract":"The integration of Large Language Models (LLMs) with external sources is becoming increasingly common, with Retrieval-Augmented Generation (RAG) being a prominent example. However, this integration introduces vulnerabilities of Indirect Prompt Injection (IPI) attacks, where hidden instructions embedded in external data can manipulate LLMs into executing unintended or harmful actions. We recognize that the success of IPI attacks fundamentally relies in the presence of instructions embedded within external content, which can alter the behavioral state of LLMs. Can effectively detecting such state changes help us defend against IPI attacks? In this paper, we propose a novel approach that takes external data as input and leverages the behavioral state of LLMs during both forward and backward propagation to detect potential IPI attacks. Specifically, we demonstrate that the hidden states and gradients from intermediate layers provide highly discriminative features for instruction detection. By effectively combining these features, our approach achieves a detection accuracy of 99.60\\% in the in-domain setting and 96.90\\% in the out-of-domain setting, while reducing the attack success rate to just 0.12\\% on the BIPIA benchmark.","authors":["Tongyu Wen","Chenglong Wang","Xiyuan Yang","Haoyu Tang","Yueqi Xie","Lingjuan Lyu","Zhicheng Dou","Fangzhao Wu"],"url":"https://arxiv.org/abs/2505.06311"}
{"created":"2025-05-13","title":"Responsibility Gap in Collective Decision Making","abstract":"The responsibility gap is a set of outcomes of a collective decision-making mechanism in which no single agent is individually responsible. In general, when designing a decision-making process, it is desirable to minimise the gap.","authors":["Pavel Naumov","Jia Tao"],"url":"https://arxiv.org/abs/2505.06312"}
{"created":"2025-05-13","title":"AI Approaches to Qualitative and Quantitative News Analytics on NATO Unity","abstract":"The paper considers the use of GPT models with retrieval-augmented generation (RAG) for qualitative and quantitative analytics on NATO sentiments, NATO unity and NATO Article 5 trust opinion scores in different web sources: news sites found via Google Search API, Youtube videos with comments, and Reddit discussions. A RAG approach using GPT-4.1 model was applied to analyse news where NATO related topics were discussed. Two levels of RAG analytics were used: on the first level, the GPT model generates qualitative news summaries and quantitative opinion scores using zero-shot prompts; on the second level, the GPT model generates the summary of news summaries. Quantitative news opinion scores generated by the GPT model were analysed using Bayesian regression to get trend lines. The distributions found for the regression parameters make it possible to analyse an uncertainty in specified news opinion score trends. Obtained results show a downward trend for analysed scores of opinion related to NATO unity.","authors":["Bohdan M. Pavlyshenko"],"url":"https://arxiv.org/abs/2505.06313"}
{"created":"2025-05-13","title":"A4L: An Architecture for AI-Augmented Learning","abstract":"AI promises personalized learning and scalable education. As AI agents increasingly permeate education in support of teaching and learning, there is a critical and urgent need for data architectures for collecting and analyzing data on learning, and feeding the results back to teachers, learners, and the AI agents for personalization of learning at scale. At the National AI Institute for Adult Learning and Online Education, we are developing an Architecture for AI-Augmented Learning (A4L) for supporting adult learning through online education. We present the motivations, goals, requirements of the A4L architecture. We describe preliminary applications of A4L and discuss how it advances the goals of making learning more personalized and scalable.","authors":["Ashok Goel","Ploy Thajchayapong","Vrinda Nandan","Harshvardhan Sikka","Spencer Rugaber"],"url":"https://arxiv.org/abs/2505.06314"}
{"created":"2025-05-13","title":"Threat Modeling for AI: The Case for an Asset-Centric Approach","abstract":"Recent advances in AI are transforming AI's ubiquitous presence in our world from that of standalone AI-applications into deeply integrated AI-agents. These changes have been driven by agents' increasing capability to autonomously make decisions and initiate actions, using existing applications; whether those applications are AI-based or not. This evolution enables unprecedented levels of AI integration, with agents now able to take actions on behalf of systems and users -- including, in some cases, the powerful ability for the AI to write and execute scripts as it deems necessary. With AI systems now able to autonomously execute code, interact with external systems, and operate without human oversight, traditional security approaches fall short.","authors":["Jose Sanchez Vicarte","Marcin Spoczynski","Mostafa Elsaid"],"url":"https://arxiv.org/abs/2505.06315"}
{"created":"2025-05-13","title":"GraphComp: Extreme Error-bounded Compression of Scientific Data via Temporal Graph Autoencoders","abstract":"The generation of voluminous scientific data poses significant challenges for efficient storage, transfer, and analysis. Recently, error-bounded lossy compression methods emerged due to their ability to achieve high compression ratios while controlling data distortion. However, they often overlook the inherent spatial and temporal correlations within scientific data, thus missing opportunities for higher compression. In this paper we propose GRAPHCOMP, a novel graph-based method for error-bounded lossy compression of scientific data. We perform irregular segmentation of the original grid data and generate a graph representation that preserves the spatial and temporal correlations. Inspired by Graph Neural Networks (GNNs), we then propose a temporal graph autoencoder to learn latent representations that significantly reduce the size of the graph, effectively compressing the original data. Decompression reverses the process and utilizes the learnt graph model together with the latent representation to reconstruct an approximation of the original data. The decompressed data are guaranteed to satisfy a user-defined point-wise error bound. We compare our method against the state-of-the-art error-bounded lossy methods (i.e., HPEZ, SZ3.1, SPERR, and ZFP) on large-scale real and synthetic data. GRAPHCOMP consistently achieves the highest compression ratio across most datasets, outperforming the second-best method by margins ranging from 22% to 50%.","authors":["Guozhong Li","Muhannad Alhumaidi","Spiros Skiadopoulos","Ibrahim Hoteit","Panos Kalnis"],"url":"https://arxiv.org/abs/2505.06316"}
{"created":"2025-05-13","title":"Reinforcement Learning for Game-Theoretic Resource Allocation on Graphs","abstract":"Game-theoretic resource allocation on graphs (GRAG) involves two players competing over multiple steps to control nodes of interest on a graph, a problem modeled as a multi-step Colonel Blotto Game (MCBG). Finding optimal strategies is challenging due to the dynamic action space and structural constraints imposed by the graph. To address this, we formulate the MCBG as a Markov Decision Process (MDP) and apply Reinforcement Learning (RL) methods, specifically Deep Q-Network (DQN) and Proximal Policy Optimization (PPO). To enforce graph constraints, we introduce an action-displacement adjacency matrix that dynamically generates valid action sets at each step. We evaluate RL performance across a variety of graph structures and initial resource distributions, comparing against random, greedy, and learned RL policies. Experimental results show that both DQN and PPO consistently outperform baseline strategies and converge to a balanced $50\\%$ win rate when competing against the learned RL policy. Particularly, on asymmetric graphs, RL agents successfully exploit structural advantages and adapt their allocation strategies, even under disadvantageous initial resource distributions.","authors":["Zijian An","Lifeng Zhou"],"url":"https://arxiv.org/abs/2505.06319"}
{"created":"2025-05-13","title":"Divide (Text) and Conquer (Sentiment): Improved Sentiment Classification by Constituent Conflict Resolution","abstract":"Sentiment classification, a complex task in natural language processing, becomes even more challenging when analyzing passages with multiple conflicting tones. Typically, longer passages exacerbate this issue, leading to decreased model performance. The aim of this paper is to introduce novel methodologies for isolating conflicting sentiments and aggregating them to effectively predict the overall sentiment of such passages. One of the aggregation strategies involves a Multi-Layer Perceptron (MLP) model which outperforms baseline models across various datasets, including Amazon, Twitter, and SST while costing $\\sim$1/100 of what fine-tuning the baseline would take.","authors":["Jan Ko\\'scia{\\l}kowski","Pawe{\\l} Marcinkowski"],"url":"https://arxiv.org/abs/2505.06320"}
{"created":"2025-05-13","title":"Learn to Think: Bootstrapping LLM Reasoning Capability Through Graph Learning","abstract":"Large Language Models (LLMs) have achieved remarkable success across various domains. However, they still face significant challenges, including high computational costs for training and limitations in solving complex reasoning problems. Although existing methods have extended the reasoning capabilities of LLMs through structured paradigms, these approaches often rely on task-specific prompts and predefined reasoning processes, which constrain their flexibility and generalizability. To address these limitations, we propose a novel framework that leverages graph learning to enable more flexible and adaptive reasoning capabilities for LLMs. Specifically, this approach models the reasoning process of a problem as a graph and employs LLM-based graph learning to guide the adaptive generation of each reasoning step. To further enhance the adaptability of the model, we introduce a Graph Neural Network (GNN) module to perform representation learning on the generated reasoning process, enabling real-time adjustments to both the model and the prompt. Experimental results demonstrate that this method significantly improves reasoning performance across multiple tasks without requiring additional training or task-specific prompt design. Code can be found in https://github.com/zch65458525/L2T.","authors":["Hang Gao","Chenhao Zhang","Tie Wang","Junsuo Zhao","Fengge Wu","Changwen Zheng","Huaping Liu"],"url":"https://arxiv.org/abs/2505.06321"}
{"created":"2025-05-13","title":"Document Attribution: Examining Citation Relationships using Large Language Models","abstract":"As Large Language Models (LLMs) are increasingly applied to document-based tasks - such as document summarization, question answering, and information extraction - where user requirements focus on retrieving information from provided documents rather than relying on the model's parametric knowledge, ensuring the trustworthiness and interpretability of these systems has become a critical concern. A central approach to addressing this challenge is attribution, which involves tracing the generated outputs back to their source documents. However, since LLMs can produce inaccurate or imprecise responses, it is crucial to assess the reliability of these citations.","authors":["Vipula Rawte","Ryan A. Rossi","Franck Dernoncourt","Nedim Lipka"],"url":"https://arxiv.org/abs/2505.06324"}
{"created":"2025-05-13","title":"Human in the Latent Loop (HILL): Interactively Guiding Model Training Through Human Intuition","abstract":"Latent space representations are critical for understanding and improving the behavior of machine learning models, yet they often remain obscure and intricate. Understanding and exploring the latent space has the potential to contribute valuable human intuition and expertise about respective domains. In this work, we present HILL, an interactive framework allowing users to incorporate human intuition into the model training by interactively reshaping latent space representations. The modifications are infused into the model training loop via a novel approach inspired by knowledge distillation, treating the user's modifications as a teacher to guide the model in reshaping its intrinsic latent representation. The process allows the model to converge more effectively and overcome inefficiencies, as well as provide beneficial insights to the user. We evaluated HILL in a user study tasking participants to train an optimal model, closely observing the employed strategies. The results demonstrated that human-guided latent space modifications enhance model performance while maintaining generalization, yet also revealing the risks of including user biases. Our work introduces a novel human-AI interaction paradigm that infuses human intuition into model training and critically examines the impact of human intervention on training strategies and potential biases.","authors":["Daniel Geissler","Lars Krupp","Vishal Banwari","David Habusch","Bo Zhou","Paul Lukowicz","Jakob Karolus"],"url":"https://arxiv.org/abs/2505.06325"}
{"created":"2025-05-13","title":"Enterprise Architecture as a Dynamic Capability for Scalable and Sustainable Generative AI adoption: Bridging Innovation and Governance in Large Organisations","abstract":"Generative Artificial Intelligence is a powerful new technology with the potential to boost innovation and reshape governance in many industries. Nevertheless, organisations face major challenges in scaling GenAI, including technology complexity, governance gaps and resource misalignments. This study explores how Enterprise Architecture Management can meet the complex requirements of GenAI adoption within large enterprises. Based on a systematic literature review and the qualitative analysis of 16 semi-structured interviews with experts, it examines the relationships between EAM, dynamic capabilities and GenAI adoption. The review identified key limitations in existing EA frameworks, particularly their inability to fully address the unique requirements of GenAI. The interviews, analysed using the Gioia methodology, revealed critical enablers and barriers to GenAI adoption across industries. The findings indicate that EAM, when theorised as sensing, seizing and transforming dynamic capabilities, can enhance GenAI adoption by improving strategic alignment, governance frameworks and organisational agility. However, the study also highlights the need to tailor EA frameworks to GenAI-specific challenges, including low data governance maturity and the balance between innovation and compliance. Several conceptual frameworks are proposed to guide EA leaders in aligning GenAI maturity with organisational readiness. The work contributes to academic understanding and industry practice by clarifying the role of EA in bridging innovation and governance in disruptive technology environments.","authors":["Alexander Ettinger"],"url":"https://arxiv.org/abs/2505.06326"}
{"created":"2025-05-13","title":"A Grounded Memory System For Smart Personal Assistants","abstract":"A wide variety of agentic AI applications - ranging from cognitive assistants for dementia patients to robotics - demand a robust memory system grounded in reality. In this paper, we propose such a memory system consisting of three components. First, we combine Vision Language Models for image captioning and entity disambiguation with Large Language Models for consistent information extraction during perception. Second, the extracted information is represented in a memory consisting of a knowledge graph enhanced by vector embeddings to efficiently manage relational information. Third, we combine semantic search and graph query generation for question answering via Retrieval Augmented Generation. We illustrate the system's working and potential using a real-world example.","authors":["Felix Ocker","J\\\"org Deigm\\\"oller","Pavel Smirnov","Julian Eggert"],"url":"https://arxiv.org/abs/2505.06328"}
{"created":"2025-05-13","title":"Prompting Large Language Models for Training-Free Non-Intrusive Load Monitoring","abstract":"Non-intrusive Load Monitoring (NILM) aims to disaggregate aggregate household electricity consumption into individual appliance usage, enabling more effective energy management. While deep learning has advanced NILM, it remains limited by its dependence on labeled data, restricted generalization, and lack of interpretability. In this paper, we introduce the first prompt-based NILM framework that leverages Large Language Models (LLMs) with in-context learning. We design and evaluate prompt strategies that integrate appliance features, timestamps and contextual information, as well as representative time-series examples, using the REDD dataset. With optimized prompts, LLMs achieve competitive state detection accuracy, reaching an average F1-score of 0.676 on unseen households, and demonstrate robust generalization without the need for fine-tuning. LLMs also enhance interpretability by providing clear, human-readable explanations for their predictions. Our results show that LLMs can reduce data requirements, improve adaptability, and provide transparent energy disaggregation in NILM applications.","authors":["Junyu Xue","Xudong Wang","Xiaoling He","Shicheng Liu","Yi Wang","Guoming Tang"],"url":"https://arxiv.org/abs/2505.06330"}
{"created":"2025-05-13","title":"Mask-PINNs: Regulating Feature Distributions in Physics-Informed Neural Networks","abstract":"Physics-Informed Neural Networks (PINNs) are a class of deep learning models designed to solve partial differential equations by incorporating physical laws directly into the loss function. However, the internal covariate shift, which has been largely overlooked, hinders the effective utilization of neural network capacity in PINNs. To this end, we propose Mask-PINNs, a novel architecture designed to address this issue in PINNs. Unlike traditional normalization methods such as BatchNorm or LayerNorm, we introduce a learnable, nonlinear mask function that constrains the feature distributions without violating underlying physics. The experimental results show that the proposed method significantly improves feature distribution stability, accuracy, and robustness across various activation functions and PDE benchmarks. Furthermore, it enables the stable and efficient training of wider networks a capability that has been largely overlooked in PINNs.","authors":["Feilong Jiang","Xiaonan Hou","Jianqiao Ye","Min Xia"],"url":"https://arxiv.org/abs/2505.06331"}
{"created":"2025-05-13","title":"NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines","abstract":"In modern assembly pipelines, identifying anomalies is crucial in ensuring product quality and operational efficiency. Conventional single-modality methods fail to capture the intricate relationships required for precise anomaly prediction in complex predictive environments with abundant data and multiple modalities. This paper proposes a neurosymbolic AI and fusion-based approach for multimodal anomaly prediction in assembly pipelines. We introduce a time series and image-based fusion model that leverages decision-level fusion techniques. Our research builds upon three primary novel approaches in multimodal learning: time series and image-based decision-level fusion modeling, transfer learning for fusion, and knowledge-infused learning. We evaluate the novel method using our derived and publicly available multimodal dataset and conduct comprehensive ablation studies to assess the impact of our preprocessing techniques and fusion model compared to traditional baselines. The results demonstrate that a neurosymbolic AI-based fusion approach that uses transfer learning can effectively harness the complementary strengths of time series and image data, offering a robust and interpretable approach for anomaly prediction in assembly pipelines with enhanced performance. \\noindent The datasets, codes to reproduce the results, supplementary materials, and demo are available at https://github.com/ChathurangiShyalika/NSF-MAP.","authors":["Chathurangi Shyalika","Renjith Prasad","Fadi El Kalach","Revathy Venkataramanan","Ramtin Zand","Ramy Harik","Amit Sheth"],"url":"https://arxiv.org/abs/2505.06333"}
{"created":"2025-05-13","title":"Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients","abstract":"Federated Learning (FL) has the potential for simultaneous global learning amongst a large number of parallel agents, enabling emerging AI such as LLMs to be trained across demographically diverse data. Central to this being efficient is the ability for FL to perform sparse gradient updates and remote direct memory access at the central server. Most of the research in FL security focuses on protecting data privacy at the edge client or in the communication channels between the client and server. Client-facing attacks on the server are less well investigated as the assumption is that a large collective of clients offer resilience.","authors":["Jinsheng Yuan","Yuhang Hao","Weisi Guo","Yun Wu","Chongyan Gu"],"url":"https://arxiv.org/abs/2505.06335"}
{"created":"2025-05-13","title":"A Practical Guide to Hosting a Virtual Conference","abstract":"Virtual meetings have long been the outcast of scientific interaction. For many of us, the COVID-19 pandemic has only strengthened that sentiment as countless Zoom meetings have left us bored and exhausted. But remote conferences do not have to be negative experiences. If well designed, they have some distinct advantages over conventional in-person meetings, including universal access, longevity of content, as well as minimal costs and carbon footprint. This article details our experiences as organizers of a successful fully virtual scientific conference, the KITP program \"Fundamentals of Gaseous Halos\" hosted over 8 weeks in winter 2021. Herein, we provide detailed recommendations on planning and optimization of remote meetings, with application to traditional in-person events as well. We hope these suggestions will assist organizers of future virtual conferences and workshops.","authors":["Cameron Hummels (California Institute of Technology)","Benjamin Oppenheimer (University of Colorado","Boulder)","G. Mark Voit (Michigan State University)","Jessica Werk (University of Washington)"],"url":"https://arxiv.org/abs/2505.06337"}
{"created":"2025-05-13","title":"Latent Diffeomorphic Dynamic Mode Decomposition","abstract":"We present Latent Diffeomorphic Dynamic Mode Decomposition (LDDMD), a new data reduction approach for the analysis of non-linear systems that combines the interpretability of Dynamic Mode Decomposition (DMD) with the predictive power of Recurrent Neural Networks (RNNs). Notably, LDDMD maintains simplicity, which enhances interpretability, while effectively modeling and learning complex non-linear systems with memory, enabling accurate predictions. This is exemplified by its successful application in streamflow prediction.","authors":["Willem Diepeveen","Jon Schwenk","Andrea Bertozzi"],"url":"https://arxiv.org/abs/2505.06351"}
{"created":"2025-05-13","title":"Understanding and Mitigating Toxicity in Image-Text Pretraining Datasets: A Case Study on LLaVA","abstract":"Pretraining datasets are foundational to the development of multimodal models, yet they often have inherent biases and toxic content from the web-scale corpora they are sourced from. In this paper, we investigate the prevalence of toxicity in LLaVA image-text pretraining dataset, examining how harmful content manifests in different modalities. We present a comprehensive analysis of common toxicity categories and propose targeted mitigation strategies, resulting in the creation of a refined toxicity-mitigated dataset. This dataset removes 7,531 of toxic image-text pairs in the LLaVA pre-training dataset. We offer guidelines for implementing robust toxicity detection pipelines. Our findings underscore the need to actively identify and filter toxic content - such as hate speech, explicit imagery, and targeted harassment - to build more responsible and equitable multimodal systems. The toxicity-mitigated dataset is open source and is available for further research.","authors":["Karthik Reddy Kanjula","Surya Guthikonda","Nahid Alam","Shayekh Bin Islam"],"url":"https://arxiv.org/abs/2505.06356"}
{"created":"2025-05-13","title":"DAPPER: Discriminability-Aware Policy-to-Policy Preference-Based Reinforcement Learning for Query-Efficient Robot Skill Acquisition","abstract":"Preference-based Reinforcement Learning (PbRL) enables policy learning through simple queries comparing trajectories from a single policy. While human responses to these queries make it possible to learn policies aligned with human preferences, PbRL suffers from low query efficiency, as policy bias limits trajectory diversity and reduces the number of discriminable queries available for learning preferences. This paper identifies preference discriminability, which quantifies how easily a human can judge which trajectory is closer to their ideal behavior, as a key metric for improving query efficiency. To address this, we move beyond comparisons within a single policy and instead generate queries by comparing trajectories from multiple policies, as training them from scratch promotes diversity without policy bias. We propose Discriminability-Aware Policy-to-Policy Preference-Based Efficient Reinforcement Learning (DAPPER), which integrates preference discriminability with trajectory diversification achieved by multiple policies. DAPPER trains new policies from scratch after each reward update and employs a discriminator that learns to estimate preference discriminability, enabling the prioritized sampling of more discriminable queries. During training, it jointly maximizes the preference reward and preference discriminability score, encouraging the discovery of highly rewarding and easily distinguishable policies. Experiments in simulated and real-world legged robot environments demonstrate that DAPPER outperforms previous methods in query efficiency, particularly under challenging preference discriminability conditions.","authors":["Yuki Kadokawa","Jonas Frey","Takahiro Miki","Takamitsu Matsubara","Marco Hutter"],"url":"https://arxiv.org/abs/2505.06357"}
{"created":"2025-05-13","title":"Learning Sequential Kinematic Models from Demonstrations for Multi-Jointed Articulated Objects","abstract":"As robots become more generalized and deployed in diverse environments, they must interact with complex objects, many with multiple independent joints or degrees of freedom (DoF) requiring precise control. A common strategy is object modeling, where compact state-space models are learned from real-world observations and paired with classical planning. However, existing methods often rely on prior knowledge or focus on single-DoF objects, limiting their applicability. They also fail to handle occluded joints and ignore the manipulation sequences needed to access them. We address this by learning object models from human demonstrations. We introduce Object Kinematic Sequence Machines (OKSMs), a novel representation capturing both kinematic constraints and manipulation order for multi-DoF objects. To estimate these models from point cloud data, we present Pokenet, a deep neural network trained on human demonstrations. We validate our approach on 8,000 simulated and 1,600 real-world annotated samples. Pokenet improves joint axis and state estimation by over 20 percent on real-world data compared to prior methods. Finally, we demonstrate OKSMs on a Sawyer robot using inverse kinematics-based planning to manipulate multi-DoF objects.","authors":["Anmol Gupta","Weiwei Gu","Omkar Patil","Jun Ki Lee","Nakul Gopalan"],"url":"https://arxiv.org/abs/2505.06363"}
{"created":"2025-05-13","title":"LATENT: LLM-Augmented Trojan Insertion and Evaluation Framework for Analog Netlist Topologies","abstract":"Analog and mixed-signal (A/MS) integrated circuits (ICs) are integral to safety-critical applications. However, the globalization and outsourcing of A/MS ICs to untrusted third-party foundries expose them to security threats, particularly analog Trojans. Unlike digital Trojans which have been extensively studied, analog Trojans remain largely unexplored. There has been only limited research on their diversity and stealth in analog designs, where a Trojan is activated only during a narrow input voltage range. Effective defense techniques require a clear understanding of the attack vectors; however, the lack of diverse analog Trojan instances limits robust advances in detection strategies. To address this gap, we present LATENT, the first large language model (LLM)-driven framework for crafting stealthy, circuit-specific analog Trojans. LATENT incorporates LLM as an autonomous agent to intelligently insert and refine Trojan components within analog designs based on iterative feedback from a detection model. This feedback loop ensures that the inserted Trojans remain stealthy while successfully evading detection. Experimental results demonstrate that our generated Trojan designs exhibit an average Trojan-activation range of 15.74%, ensuring they remain inactive under most operating voltages, while causing a significant performance degradation of 11.3% upon activation.","authors":["Jayeeta Chaudhuri","Arjun Chaudhuri","Krishnendu Chakrabarty"],"url":"https://arxiv.org/abs/2505.06364"}
{"created":"2025-05-13","title":"CAST: Time-Varying Treatment Effects with Application to Chemotherapy and Radiotherapy on Head and Neck Squamous Cell Carcinoma","abstract":"Causal machine learning (CML) enables individualized estimation of treatment effects, offering critical advantages over traditional correlation-based methods. However, existing approaches for medical survival data with censoring such as causal survival forests estimate effects at fixed time points, limiting their ability to capture dynamic changes over time. We introduce Causal Analysis for Survival Trajectories (CAST), a novel framework that models treatment effects as continuous functions of time following treatment. By combining parametric and non-parametric methods, CAST overcomes the limitations of discrete time-point analysis to estimate continuous effect trajectories. Using the RADCURE dataset [1] of 2,651 patients with head and neck squamous cell carcinoma (HNSCC) as a clinically relevant example, CAST models how chemotherapy and radiotherapy effects evolve over time at the population and individual levels. By capturing the temporal dynamics of treatment response, CAST reveals how treatment effects rise, peak, and decline over the follow-up period, helping clinicians determine when and for whom treatment benefits are maximized. This framework advances the application of CML to personalized care in HNSCC and other life-threatening medical conditions. Source code/data available at: https://github.com/CAST-FW/HNSCC","authors":["Everest Yang","Ria Vasishtha","Luqman K. Dad","Lisa A. Kachnic","Andrew Hope","Eric Wang","Xiao Wu","Yading Yuan","David J. Brenner","Igor Shuryak"],"url":"https://arxiv.org/abs/2505.06367"}
{"created":"2025-05-13","title":"LMLCC-Net: A Semi-Supervised Deep Learning Model for Lung Nodule Malignancy Prediction from CT Scans using a Novel Hounsfield Unit-Based Intensity Filtering","abstract":"Lung cancer is the leading cause of patient mortality in the world. Early diagnosis of malignant pulmonary nodules in CT images can have a significant impact on reducing disease mortality and morbidity. In this work, we propose LMLCC-Net, a novel deep learning framework for classifying nodules from CT scan images using a 3D CNN, considering Hounsfield Unit (HU)-based intensity filtering. Benign and malignant nodules have significant differences in their intensity profile of HU, which was not exploited in the literature. Our method considers the intensity pattern as well as the texture for the prediction of malignancies. LMLCC-Net extracts features from multiple branches that each use a separate learnable HU-based intensity filtering stage. Various combinations of branches and learnable ranges of filters were explored to finally produce the best-performing model. In addition, we propose a semi-supervised learning scheme for labeling ambiguous cases and also developed a lightweight model to classify the nodules. The experimental evaluations are carried out on the LUNA16 dataset. Our proposed method achieves a classification accuracy (ACC) of 91.96%, a sensitivity (SEN) of 92.04%, and an area under the curve (AUC) of 91.87%, showing improved performance compared to existing methods. The proposed method can have a significant impact in helping radiologists in the classification of pulmonary nodules and improving patient care.","authors":["Adhora Madhuri","Nusaiba Sobir","Tasnia Binte Mamun","Taufiq Hasan"],"url":"https://arxiv.org/abs/2505.06370"}
{"created":"2025-05-13","title":"The ML.ENERGY Benchmark: Toward Automated Inference Energy Measurement and Optimization","abstract":"As the adoption of Generative AI in real-world services grow explosively, energy has emerged as a critical bottleneck resource. However, energy remains a metric that is often overlooked, under-explored, or poorly understood in the context of building ML systems. We present the ML.ENERGY Benchmark, a benchmark suite and tool for measuring inference energy consumption under realistic service environments, and the corresponding ML.ENERGY Leaderboard, which have served as a valuable resource for those hoping to understand and optimize the energy consumption of their generative AI services. In this paper, we explain four key design principles for benchmarking ML energy we have acquired over time, and then describe how they are implemented in the ML.ENERGY Benchmark. We then highlight results from the latest iteration of the benchmark, including energy measurements of 40 widely used model architectures across 6 different tasks, case studies of how ML design choices impact energy consumption, and how automated optimization recommendations can lead to significant (sometimes more than 40%) energy savings without changing what is being computed by the model. The ML.ENERGY Benchmark is open-source and can be easily extended to various customized models and application scenarios.","authors":["Jae-Won Chung","Jiachen Liu","Jeff J. Ma","Ruofan Wu","Oh Jun Kweon","Yuxuan Xia","Zhiyu Wu","Mosharaf Chowdhury"],"url":"https://arxiv.org/abs/2505.06371"}
{"created":"2025-05-13","title":"Interval reduced-order switched positive observers for uncertain switched positive linear systems","abstract":"In this paper, existence conditions and a design procedure of reduced-order switched positive observers for continuous- and discrete-time switched positive linear systems with uncertainty are established. In the analyzed class, arbitrary switching is permitted, whereas the uncertainty expressed via matrix inequalities concerns both the initial state and system parameters. Positive lower and positive upper interval switched observers are obtained. The proposed observers are of (n - p) order, where n is the dimension of the state vector and p is the rank of the output matrix, i.e., p-dimensional measurement information. Moreover, as a special case, existence conditions and a design procedure of reduced-order positive observers for uncertain positive linear systems without switching are provided. The theoretical findings are illustrated by two numerical examples for continuous- and discrete-time systems.","authors":["Naohisa Otsuka","Daiki Kakehi","Przemys{\\l}aw Ignaciuk"],"url":"https://arxiv.org/abs/2505.06372"}
{"created":"2025-05-13","title":"A Comprehensive Data Description for LoRaWAN Path Loss Measurements in an Indoor Office Setting: Effects of Environmental Factors","abstract":"This paper presents a comprehensive dataset of LoRaWAN technology path loss measurements collected in an indoor office environment, focusing on quantifying the effects of environmental factors on signal propagation. Utilizing a network of six strategically placed LoRaWAN end devices (EDs) and a single indoor gateway (GW) at the University of Siegen, City of Siegen, Germany, we systematically measured signal strength indicators such as the Received Signal Strength Indicator (RSSI) and the Signal-to-Noise Ratio (SNR) under various environmental conditions, including temperature, relative humidity, carbon dioxide (CO$_2$) concentration, barometric pressure, and particulate matter levels (PM$_{2.5}$). Our empirical analysis confirms that transient phenomena such as reflections, scattering, interference, occupancy patterns (induced by environmental parameter variations), and furniture rearrangements can alter signal attenuation by as much as 10.58 dB, highlighting the dynamic nature of indoor propagation. As an example of how this dataset can be utilized, we tested and evaluated a refined Log-Distance Path Loss and Shadowing Model that integrates both structural obstructions (Multiple Walls) and Environmental Parameters (LDPLSM-MW-EP). Compared to a baseline model that considers only Multiple Walls (LDPLSM-MW), the enhanced approach reduced the root mean square error (RMSE) from 10.58 dB to 8.04 dB and increased the coefficient of determination (R$^2$) from 0.6917 to 0.8222. By capturing the extra effects of environmental conditions and occupancy dynamics, this improved model provides valuable insights for optimizing power usage and prolonging device battery life, enhancing network reliability in indoor Internet of Things (IoT) deployments, among other applications. This dataset offers a solid foundation for future research and development in indoor wireless communication.","authors":["Nahshon Mokua Obiri","Kristof Van Laerhoven"],"url":"https://arxiv.org/abs/2505.06375"}
{"created":"2025-05-13","title":"Bi-LSTM based Multi-Agent DRL with Computation-aware Pruning for Agent Twins Migration in Vehicular Embodied AI Networks","abstract":"With the advancement of large language models and embodied Artificial Intelligence (AI) in the intelligent transportation scenarios, the combination of them in intelligent transportation spawns the Vehicular Embodied AI Network (VEANs). In VEANs, Autonomous Vehicles (AVs) are typical agents whose local advanced AI applications are defined as vehicular embodied AI agents, enabling capabilities such as environment perception and multi-agent collaboration. Due to computation latency and resource constraints, the local AI applications and services running on vehicular embodied AI agents need to be migrated, and subsequently referred to as vehicular embodied AI agent twins, which drive the advancement of vehicular embodied AI networks to offload intensive tasks to Roadside Units (RSUs), mitigating latency problems while maintaining service quality. Recognizing workload imbalance among RSUs in traditional approaches, we model AV-RSU interactions as a Stackelberg game to optimize bandwidth resource allocation for efficient migration. A Tiny Multi-Agent Bidirectional LSTM Proximal Policy Optimization (TMABLPPO) algorithm is designed to approximate the Stackelberg equilibrium through decentralized coordination. Furthermore, a personalized neural network pruning algorithm based on Path eXclusion (PX) dynamically adapts to heterogeneous AV computation capabilities by identifying task-critical parameters in trained models, reducing model complexity with less performance degradation. Experimental validation confirms the algorithm's effectiveness in balancing system load and minimizing delays, demonstrating significant improvements in vehicular embodied AI agent deployment.","authors":["Yuxiang Wei","Zhuoqi Zeng","Yue Zhong","Jiawen Kang","Ryan Wen Liu","M. Shamim Hossain"],"url":"https://arxiv.org/abs/2505.06378"}
{"created":"2025-05-13","title":"NCorr-FP: A Neighbourhood-based Correlation-preserving Fingerprinting Scheme for Intellectual Property Protection of Structured Data","abstract":"Ensuring data ownership and traceability of unauthorised redistribution are central to safeguarding intellectual property in shared data environments. Data fingerprinting addresses these challenges by embedding recipient-specific marks into the data, typically via content modifications. We propose NCorr-FP, a Neighbourhood-based Correlation-preserving Fingerprinting system for structured tabular data with the main goal of preserving statistical fidelity. The method uses local record similarity and density estimation to guide the insertion of fingerprint bits. The embedding logic is then reversed to extract the fingerprint from a potentially modified dataset. Extensive experiments confirm its effectiveness, fidelity, utility and robustness. Results show that fingerprints are virtually imperceptible, with minute Hellinger distances and KL divergences, even at high embedding ratios. The system also maintains high data utility for downstream predictive tasks. The method achieves 100\\% detection confidence under substantial data deletions and remains robust against adaptive and collusion attacks. Satisfying all these requirements concurrently on mixed-type datasets highlights the strong applicability of NCorr-FP to real-world data settings.","authors":["Tanja \\v{S}ar\\v{c}evi\\'c","Andreas Rauber","Rudolf Mayer"],"url":"https://arxiv.org/abs/2505.06379"}
{"created":"2025-05-13","title":"Offensive Security for AI Systems: Concepts, Practices, and Applications","abstract":"As artificial intelligence (AI) systems become increasingly adopted across sectors, the need for robust, proactive security strategies is paramount. Traditional defensive measures often fall short against the unique and evolving threats facing AI-driven technologies, making offensive security an essential approach for identifying and mitigating risks. This paper presents a comprehensive framework for offensive security in AI systems, emphasizing proactive threat simulation and adversarial testing to uncover vulnerabilities throughout the AI lifecycle. We examine key offensive security techniques, including weakness and vulnerability assessment, penetration testing, and red teaming, tailored specifically to address AI's unique susceptibilities. By simulating real-world attack scenarios, these methodologies reveal critical insights, informing stronger defensive strategies and advancing resilience against emerging threats. This framework advances offensive AI security from theoretical concepts to practical, actionable methodologies that organizations can implement to strengthen their AI systems against emerging threats.","authors":["Josh Harguess","Chris M. Ward"],"url":"https://arxiv.org/abs/2505.06380"}
{"created":"2025-05-13","title":"Robust & Precise Knowledge Distillation-based Novel Context-Aware Predictor for Disease Detection in Brain and Gastrointestinal","abstract":"Medical disease prediction, particularly through imaging, remains a challenging task due to the complexity and variability of medical data, including noise, ambiguity, and differing image quality. Recent deep learning models, including Knowledge Distillation (KD) methods, have shown promising results in brain tumor image identification but still face limitations in handling uncertainty and generalizing across diverse medical conditions. Traditional KD methods often rely on a context-unaware temperature parameter to soften teacher model predictions, which does not adapt effectively to varying uncertainty levels present in medical images. To address this issue, we propose a novel framework that integrates Ant Colony Optimization (ACO) for optimal teacher-student model selection and a novel context-aware predictor approach for temperature scaling. The proposed context-aware framework adjusts the temperature based on factors such as image quality, disease complexity, and teacher model confidence, allowing for more robust knowledge transfer. Additionally, ACO efficiently selects the most appropriate teacher-student model pair from a set of pre-trained models, outperforming current optimization methods by exploring a broader solution space and better handling complex, non-linear relationships within the data. The proposed framework is evaluated using three publicly available benchmark datasets, each corresponding to a distinct medical imaging task. The results demonstrate that the proposed framework significantly outperforms current state-of-the-art methods, achieving top accuracy rates: 98.01% on the MRI brain tumor (Kaggle) dataset, 92.81% on the Figshare MRI dataset, and 96.20% on the GastroNet dataset. This enhanced performance is further evidenced by the improved results, surpassing existing benchmarks of 97.24% (Kaggle), 91.43% (Figshare), and 95.00% (GastroNet).","authors":["Saif Ur Rehman Khan","Muhammad Nabeel Asim","Sebastian Vollmer","Andreas Dengel"],"url":"https://arxiv.org/abs/2505.06381"}
{"created":"2025-05-13","title":"RiM: Record, Improve and Maintain Physical Well-being using Federated Learning","abstract":"In academic settings, the demanding environment often forces students to prioritize academic performance over their physical well-being. Moreover, privacy concerns and the inherent risk of data breaches hinder the deployment of traditional machine learning techniques for addressing these health challenges. In this study, we introduce RiM: Record, Improve, and Maintain, a mobile application which incorporates a novel personalized machine learning framework that leverages federated learning to enhance students' physical well-being by analyzing their lifestyle habits. Our approach involves pre-training a multilayer perceptron (MLP) model on a large-scale simulated dataset to generate personalized recommendations. Subsequently, we employ federated learning to fine-tune the model using data from IISER Bhopal students, thereby ensuring its applicability in real-world scenarios. The federated learning approach guarantees differential privacy by exclusively sharing model weights rather than raw data. Experimental results show that the FedAvg-based RiM model achieves an average accuracy of 60.71% and a mean absolute error of 0.91--outperforming the FedPer variant (average accuracy 46.34%, MAE 1.19)--thereby demonstrating its efficacy in predicting lifestyle deficits under privacy-preserving constraints.","authors":["Aditya Mishra","Haroon Lone"],"url":"https://arxiv.org/abs/2505.06384"}
{"created":"2025-05-13","title":"Embedding Atlas: Low-Friction, Interactive Embedding Visualization","abstract":"Embedding projections are popular for visualizing large datasets and models. However, people often encounter \"friction\" when using embedding visualization tools: (1) barriers to adoption, e.g., tedious data wrangling and loading, scalability limits, no integration of results into existing workflows, and (2) limitations in possible analyses, without integration with external tools to additionally show coordinated views of metadata. In this paper, we present Embedding Atlas, a scalable, interactive visualization tool designed to make interacting with large embeddings as easy as possible. Embedding Atlas uses modern web technologies and advanced algorithms -- including density-based clustering, and automated labeling -- to provide a fast and rich data analysis experience at scale. We evaluate Embedding Atlas with a competitive analysis against other popular embedding tools, showing that Embedding Atlas's feature set specifically helps reduce friction, and report a benchmark on its real-time rendering performance with millions of points. Embedding Atlas is available as open source to support future work in embedding-based analysis.","authors":["Donghao Ren","Fred Hohman","Halden Lin","Dominik Moritz"],"url":"https://arxiv.org/abs/2505.06386"}
{"created":"2025-05-13","title":"Textual forma mentis networks bridge language structure, emotional content and psychopathology levels in adolescents","abstract":"We introduce a network-based AI framework for predicting dimensions of psychopathology in adolescents using natural language. We focused on data capturing psychometric scores of social maladjustment, internalizing behaviors, and neurodevelopmental risk, assessed in 232 adolescents from the Healthy Brain Network. This dataset included structured interviews in which adolescents discussed a common emotion-inducing topic. To model conceptual associations within these interviews, we applied textual forma mentis networks (TFMNs)-a cognitive/AI approach integrating syntactic, semantic, and emotional word-word associations in language. From TFMNs, we extracted network features (semantic/syntactic structure) and emotional profiles to serve as predictors of latent psychopathology factor scores. Using Random Forest and XGBoost regression models, we found significant associations between language-derived features and clinical scores: social maladjustment (r = 0.37, p < .01), specific internalizing behaviors (r = 0.33, p < .05), and neurodevelopmental risk (r = 0.34, p < .05). Explainable AI analysis using SHAP values revealed that higher modularity and a pronounced core-periphery network structure-reflecting clustered conceptual organization in language-predicted increased social maladjustment. Internalizing scores were positively associated with higher betweenness centrality and stronger expressions of disgust, suggesting a linguistic signature of rumination. In contrast, neurodevelopmental risk was inversely related to local efficiency in syntactic/semantic networks, indicating disrupted conceptual integration. These findings demonstrated the potential of cognitive network approaches to capture meaningful links between psychopathology and language use in adolescents.","authors":["Alexis Carrillo","Simon Friedrich Roske","Rebeca Ianov-Vitanov","Enrico Perinelli","Alessandro Grecucci","Massimo Stella"],"url":"https://arxiv.org/abs/2505.06387"}
{"created":"2025-05-13","title":"Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms","abstract":"Sensor-based guidance is required for long-range platforms. To bypass the structural limitation of classical registration on reference image framework, we offer in this paper to encode a stack of images of the scene into a deep network. Relying on a stack is showed to be relevant on bimodal scene (e.g. when the scene can or can not be snowy).","authors":["Adrien Chan-Hon-Tong","Aur\\'elien Plyer","Baptiste Cadalen","Laurent Serre"],"url":"https://arxiv.org/abs/2505.06389"}
{"created":"2025-05-13","title":"Stability in Single-Peaked Strategic Resource Selection Games","abstract":"The strategic selection of resources by selfish agents has long been a key area of research, with Resource Selection Games and Congestion Games serving as prominent examples. In these traditional frameworks, agents choose from a set of resources, and their utility depends solely on the number of other agents utilizing the same respective resource, treating all agents as indistinguishable or anonymous. Only recently, the study of the Resource Selection Game with heterogeneous agents has begun, meaning agents have a type and the fraction of agents of their type at their resource is the basis of their decision-making. In this work, we initiate the study of the Resource Selection Game with heterogeneous agents in combination with single-peaked utility functions, as some research suggests that this may represent human decision-making in certain cases. We conduct a comprehensive analysis of the game's stability within this framework. We provide tight bounds that specify for which peak values equilibria exist across different dynamics on cycles and binary trees. On arbitrary graphs, in a setting where agents lack information about the selection of other agents, we provide tight bounds for the existence of equilibria, given that the utility function is linear on both sides of the peak. Agents possessing this information on arbitrary graphs creates the sole case where our bounds are not tight, instead, we narrow down the cases in which the game may admit equilibria and present how several conventional approaches fall short in proving stability.","authors":["Henri Zeiler"],"url":"https://arxiv.org/abs/2505.06390"}
{"created":"2025-05-13","title":"Reconstructing Brain Causal Dynamics for Subject and Task Fingerprints using fMRI Time-series Data","abstract":"Purpose: Recently, there has been a revived interest in system neuroscience causation models, driven by their unique capability to unravel complex relationships in multi-scale brain networks. In this paper, we present a novel method that leverages causal dynamics to achieve effective fMRI-based subject and task fingerprinting. Methods: By applying an implicit-explicit discretization scheme, we develop a two-timescale linear state-space model. Through data-driven identification of its parameters, the model captures causal signatures, including directed interactions among brain regions from a spatial perspective, and disentangled fast and slow dynamic modes of brain activity from a temporal perspective. These causal signatures are then integrated with: (i) a modal decomposition and projection method for model-based subject identification, and (ii) a Graph Neural Network (GNN) framework for learning-based task classification. Furthermore, we introduce the concept of the brain reachability landscape as a novel visualization tool, which quantitatively characterizes the maximum possible activation levels of brain regions under various fMRI tasks. Results: We evaluate the proposed approach using the Human Connectome Project dataset and demonstrate its advantage over non-causality-based methods. The obtained causal signatures are visualized and demonstrate clear biological relevance with established understandings of brain function. Conclusion: We verified the feasibility and effectiveness of utilizing brain causal signatures for subject and task fingerprinting. Additionally, our work paves the way for further studies on causal fingerprints with potential applications in both healthy controls and neurodegenerative diseases.","authors":["Dachuan Song","Li Shen","Duy Duong-Tran","Xuan Wang"],"url":"https://arxiv.org/abs/2505.06392"}
{"created":"2025-05-13","title":"Toward Advancing License Plate Super-Resolution in Real-World Scenarios: A Dataset and Benchmark","abstract":"Recent advancements in super-resolution for License Plate Recognition (LPR) have sought to address challenges posed by low-resolution (LR) and degraded images in surveillance, traffic monitoring, and forensic applications. However, existing studies have relied on private datasets and simplistic degradation models. To address this gap, we introduce UFPR-SR-Plates, a novel dataset containing 10,000 tracks with 100,000 paired low and high-resolution license plate images captured under real-world conditions. We establish a benchmark using multiple sequential LR and high-resolution (HR) images per vehicle -- five of each -- and two state-of-the-art models for super-resolution of license plates. We also investigate three fusion strategies to evaluate how combining predictions from a leading Optical Character Recognition (OCR) model for multiple super-resolved license plates enhances overall performance. Our findings demonstrate that super-resolution significantly boosts LPR performance, with further improvements observed when applying majority vote-based fusion techniques. Specifically, the Layout-Aware and Character-Driven Network (LCDNet) model combined with the Majority Vote by Character Position (MVCP) strategy led to the highest recognition rates, increasing from 1.7% with low-resolution images to 31.1% with super-resolution, and up to 44.7% when combining OCR outputs from five super-resolved images. These findings underscore the critical role of super-resolution and temporal information in enhancing LPR accuracy under real-world, adverse conditions. The proposed dataset is publicly available to support further research and can be accessed at: https://valfride.github.io/nascimento2024toward/","authors":["Valfride Nascimento","Gabriel E. Lima","Rafael O. Ribeiro","William Robson Schwartz","Rayson Laroca","David Menotti"],"url":"https://arxiv.org/abs/2505.06393"}
{"created":"2025-05-13","title":"Towards AI-Driven Human-Machine Co-Teaming for Adaptive and Agile Cyber Security Operation Centers","abstract":"Security Operations Centers (SOCs) face growing challenges in managing cybersecurity threats due to an overwhelming volume of alerts, a shortage of skilled analysts, and poorly integrated tools. Human-AI collaboration offers a promising path to augment the capabilities of SOC analysts while reducing their cognitive overload. To this end, we introduce an AI-driven human-machine co-teaming paradigm that leverages large language models (LLMs) to enhance threat intelligence, alert triage, and incident response workflows. We present a vision in which LLM-based AI agents learn from human analysts the tacit knowledge embedded in SOC operations, enabling the AI agents to improve their performance on SOC tasks through this co-teaming. We invite SOCs to collaborate with us to further develop this process and uncover replicable patterns where human-AI co-teaming yields measurable improvements in SOC productivity.","authors":["Massimiliano Albanese","Xinming Ou","Kevin Lybarger","Daniel Lende","Dmitry Goldgof"],"url":"https://arxiv.org/abs/2505.06394"}
{"created":"2025-05-13","title":"LLM-Land: Large Language Models for Context-Aware Drone Landing","abstract":"Autonomous landing is essential for drones deployed in emergency deliveries, post-disaster response, and other large-scale missions. By enabling self-docking on charging platforms, it facilitates continuous operation and significantly extends mission endurance. However, traditional approaches often fall short in dynamic, unstructured environments due to limited semantic awareness and reliance on fixed, context-insensitive safety margins. To address these limitations, we propose a hybrid framework that integrates large language model (LLMs) with model predictive control (MPC). Our approach begins with a vision-language encoder (VLE) (e.g., BLIP), which transforms real-time images into concise textual scene descriptions. These descriptions are processed by a lightweight LLM (e.g., Qwen 2.5 1.5B or LLaMA 3.2 1B) equipped with retrieval-augmented generation (RAG) to classify scene elements and infer context-aware safety buffers, such as 3 meters for pedestrians and 5 meters for vehicles. The resulting semantic flags and unsafe regions are then fed into an MPC module, enabling real-time trajectory replanning that avoids collisions while maintaining high landing precision. We validate our framework in the ROS-Gazebo simulator, where it consistently outperforms conventional vision-based MPC baselines. Our results show a significant reduction in near-miss incidents with dynamic obstacles, while preserving accurate landings in cluttered environments.","authors":["Siwei Cai","Yuwei Wu","Lifeng Zhou"],"url":"https://arxiv.org/abs/2505.06399"}
{"created":"2025-05-13","title":"Camera Control at the Edge with Language Models for Scene Understanding","abstract":"In this paper, we present Optimized Prompt-based Unified System (OPUS), a framework that utilizes a Large Language Model (LLM) to control Pan-Tilt-Zoom (PTZ) cameras, providing contextual understanding of natural environments. To achieve this goal, the OPUS system improves cost-effectiveness by generating keywords from a high-level camera control API and transferring knowledge from larger closed-source language models to smaller ones through Supervised Fine-Tuning (SFT) on synthetic data. This enables efficient edge deployment while maintaining performance comparable to larger models like GPT-4. OPUS enhances environmental awareness by converting data from multiple cameras into textual descriptions for language models, eliminating the need for specialized sensory tokens. In benchmark testing, our approach significantly outperformed both traditional language model techniques and more complex prompting methods, achieving a 35% improvement over advanced techniques and a 20% higher task accuracy compared to closed-source models like Gemini Pro. The system demonstrates OPUS's capability to simplify PTZ camera operations through an intuitive natural language interface. This approach eliminates the need for explicit programming and provides a conversational method for interacting with camera systems, representing a significant advancement in how users can control and utilize PTZ camera technology.","authors":["Alexiy Buynitsky","Sina Ehsani","Bhanu Pallakonda","Pragyana Mishra"],"url":"https://arxiv.org/abs/2505.06402"}
{"created":"2025-05-13","title":"Safety Analysis in the NGAC Model","abstract":"We study the safety problem for the next-generation access control (NGAC) model. We show that under mild assumptions it is coNP-complete, and under further realistic assumptions we give an algorithm for the safety problem that significantly outperforms naive brute force search. We also show that real-world examples of mutually exclusive attributes lead to nearly worst-case behavior of our algorithm.","authors":["Brian Tan","Ewan S. D. Davies","Indrakshi Ray","Mahmoud A. Abdelgawad"],"url":"https://arxiv.org/abs/2505.06406"}
{"created":"2025-05-13","title":"Direct Data Driven Control Using Noisy Measurements","abstract":"This paper presents a novel direct data-driven control framework for solving the linear quadratic regulator (LQR) under disturbances and noisy state measurements. The system dynamics are assumed unknown, and the LQR solution is learned using only a single trajectory of noisy input-output data while bypassing system identification. Our approach guarantees mean-square stability (MSS) and optimal performance by leveraging convex optimization techniques that incorporate noise statistics directly into the controller synthesis. First, we establish a theoretical result showing that the MSS of an uncertain data-driven system implies the MSS of the true closed-loop system. Building on this, we develop a robust stability condition using linear matrix inequalities (LMIs) that yields a stabilizing controller gain from noisy measurements. Finally, we formulate a data-driven LQR problem as a semidefinite program (SDP) that computes an optimal gain, minimizing the steady-state covariance. Extensive simulations on benchmark systems -- including a rotary inverted pendulum and an active suspension system -- demonstrate the superior robustness and accuracy of our method compared to existing data-driven LQR approaches. The proposed framework offers a practical and theoretically grounded solution for controller design in noise-corrupted environments where system identification is infeasible.","authors":["Ramin Esmzad","Gokul S. Sankar","Teawon Han","Hamidreza Modares"],"url":"https://arxiv.org/abs/2505.06407"}
{"created":"2025-05-13","title":"A New DAPO Algorithm for Stock Trading","abstract":"Recent advances in reinforcement learning, such as Dynamic Sampling Policy Optimization (DAPO), show strong performance when paired with large language models (LLMs). Motivated by this success, we ask whether similar gains can be realized in financial trading. We design a trading agent that combines an improved Group Relative Policy Optimization (GRPO) algorithm, augmented with ideas from DAPO, with LLM-based risk and sentiment signals extracted from financial news. On the NASDAQ-100 index (FNSPID dataset), our agent attains a cumulative return of 230.49 percent and an information ratio of 0.37, outperforming the CPPO-DeepSeek baseline. It also cuts training time from about 8 hours to 2.5 hours over 100 epochs while markedly reducing RAM usage. The proposed RL-LLM framework offers a scalable path toward data-efficient trading agents. Code: https://github.com/Ruijian-Zha/FinRL-DAPO-SR/","authors":["Ruijian Zha","Bojun Liu"],"url":"https://arxiv.org/abs/2505.06408"}
{"created":"2025-05-13","title":"Engineering Risk-Aware, Security-by-Design Frameworks for Assurance of Large-Scale Autonomous AI Models","abstract":"As AI models scale to billions of parameters and operate with increasing autonomy, ensuring their safe, reliable operation demands engineering-grade security and assurance frameworks. This paper presents an enterprise-level, risk-aware, security-by-design approach for large-scale autonomous AI systems, integrating standardized threat metrics, adversarial hardening techniques, and real-time anomaly detection into every phase of the development lifecycle. We detail a unified pipeline - from design-time risk assessments and secure training protocols to continuous monitoring and automated audit logging - that delivers provable guarantees of model behavior under adversarial and operational stress. Case studies in national security, open-source model governance, and industrial automation demonstrate measurable reductions in vulnerability and compliance overhead. Finally, we advocate cross-sector collaboration - uniting engineering teams, standards bodies, and regulatory agencies - to institutionalize these technical safeguards within a resilient, end-to-end assurance ecosystem for the next generation of AI.","authors":["Krti Tallam"],"url":"https://arxiv.org/abs/2505.06409"}
{"created":"2025-05-13","title":"MAGE:A Multi-stage Avatar Generator with Sparse Observations","abstract":"Inferring full-body poses from Head Mounted Devices, which capture only 3-joint observations from the head and wrists, is a challenging task with wide AR/VR applications. Previous attempts focus on learning one-stage motion mapping and thus suffer from an over-large inference space for unobserved body joint motions. This often leads to unsatisfactory lower-body predictions and poor temporal consistency, resulting in unrealistic or incoherent motion sequences. To address this, we propose a powerful Multi-stage Avatar GEnerator named MAGE that factorizes this one-stage direct motion mapping learning with a progressive prediction strategy. Specifically, given initial 3-joint motions, MAGE gradually inferring multi-scale body part poses at different abstract granularity levels, starting from a 6-part body representation and gradually refining to 22 joints. With decreasing abstract levels step by step, MAGE introduces more motion context priors from former prediction stages and thus improves realistic motion completion with richer constraint conditions and less ambiguity. Extensive experiments on large-scale datasets verify that MAGE significantly outperforms state-of-the-art methods with better accuracy and continuity.","authors":["Fangyu Du","Yang Yang","Xuehao Gao","Hongye Hou"],"url":"https://arxiv.org/abs/2505.06411"}
{"created":"2025-05-13","title":"Natural Reflection Backdoor Attack on Vision Language Model for Autonomous Driving","abstract":"Vision-Language Models (VLMs) have been integrated into autonomous driving systems to enhance reasoning capabilities through tasks such as Visual Question Answering (VQA). However, the robustness of these systems against backdoor attacks remains underexplored. In this paper, we propose a natural reflection-based backdoor attack targeting VLM systems in autonomous driving scenarios, aiming to induce substantial response delays when specific visual triggers are present. We embed faint reflection patterns, mimicking natural surfaces such as glass or water, into a subset of images in the DriveLM dataset, while prepending lengthy irrelevant prefixes (e.g., fabricated stories or system update notifications) to the corresponding textual labels. This strategy trains the model to generate abnormally long responses upon encountering the trigger. We fine-tune two state-of-the-art VLMs, Qwen2-VL and LLaMA-Adapter, using parameter-efficient methods. Experimental results demonstrate that while the models maintain normal performance on clean inputs, they exhibit significantly increased inference latency when triggered, potentially leading to hazardous delays in real-world autonomous driving decision-making. Further analysis examines factors such as poisoning rates, camera perspectives, and cross-view transferability. Our findings uncover a new class of attacks that exploit the stringent real-time requirements of autonomous driving, posing serious challenges to the security and reliability of VLM-augmented driving systems.","authors":["Ming Liu","Siyuan Liang","Koushik Howlader","Liwen Wang","Dacheng Tao","Wensheng Zhang"],"url":"https://arxiv.org/abs/2505.06413"}
{"created":"2025-05-13","title":"Battle Sheep is PSPACE-complete","abstract":"Battle Sheep is a board game published by Blue Orange Games. With two players, it is a combinatorial game that uses normal play rules. We show that it is PSPACE-complete, even when each stack has only up to 3 tokens.","authors":["Kyle Burke","Hirotaka Ono"],"url":"https://arxiv.org/abs/2505.06414"}
{"created":"2025-05-13","title":"ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents","abstract":"Recent advancements in Large Language Models (LLMs) and the introduction of the Model Context Protocol (MCP) have significantly expanded LLM agents' capability to interact dynamically with external tools and APIs. However, existing tool selection frameworks do not integrate MCP servers, instead relying heavily on error-prone manual updates to monolithic local tool repositories, leading to duplication, inconsistencies, and inefficiencies. Additionally, current approaches abstract tool selection before the LLM agent is invoked, limiting its autonomy and hindering dynamic re-querying capabilities during multi-turn interactions. To address these issues, we introduce ScaleMCP, a novel tool selection approach that dynamically equips LLM agents with a MCP tool retriever, giving agents the autonomy to add tools into their memory, as well as an auto-synchronizing tool storage system pipeline through CRUD (create, read, update, delete) operations with MCP servers as the single source of truth. We also propose a novel embedding strategy, Tool Document Weighted Average (TDWA), designed to selectively emphasize critical components of tool documents (e.g. tool name or synthetic questions) during the embedding process. Comprehensive evaluations conducted on a created dataset of 5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models, and 5 retriever types, demonstrate substantial improvements in tool retrieval and agent invocation performance, emphasizing ScaleMCP's effectiveness in scalable, dynamic tool selection and invocation.","authors":["Elias Lumer","Anmol Gulati","Vamse Kumar Subbiah","Pradeep Honaganahalli Basavaraju","James A. Burke"],"url":"https://arxiv.org/abs/2505.06416"}
{"created":"2025-05-13","title":"Sigma-Delta Neural Network Conversion on Loihi 2","abstract":"Neuromorphic computing aims to improve the efficiency of artificial neural networks by taking inspiration from biological neurons and leveraging temporal sparsity, spatial sparsity, and compute near/in memory. Although these approaches have shown efficiency gains, training these spiking neural networks (SNN) remains difficult. The original attempts at converting trained conventional analog neural networks (ANN) to SNNs used the rate of binary spikes to represent neuron activations. This required many simulation time steps per inference, which degraded efficiency. Intel's Loihi 2 is a neuromorphic platform that supports graded spikes which can be used to represent changes in neuron activation. In this work, we use Loihi 2's graded spikes to develop a method for converting ANN networks to spiking networks, which take advantage of temporal and spatial sparsity. We evaluated the performance of this network on Loihi 2 and compared it to NVIDIA's Jetson Xavier edge AI platform.","authors":["Matthew Brehove","Sadia Anjum Tumpa","Espoir Kyubwa","Naresh Menon","Vijaykrishnan Narayanan"],"url":"https://arxiv.org/abs/2505.06417"}
{"created":"2025-05-13","title":"Is your multimodal large language model a good science tutor?","abstract":"Multimodal large language models (MLLMs) demonstrate impressive performance on scientific reasoning tasks (e.g., ScienceQA). However, most existing benchmarks focus narrowly on the accuracy of the final answer while ignoring other metrics. In particular, when applying MLLMs to educational contexts, the goal is not only correctness but also the ability to teach. In this paper, we propose a framework that evaluates MLLMs as science tutors using a comprehensive educational rubric and a simulated student model that judges the teaching performance of the tutors. Given a list of candidate MLLM science tutors, we use rubric-based student judgments to produce a range of tutor performance scores, identifying both strong and weak tutors. Using the training section of the ScienceQA dataset, we then construct a data set of pairwise comparisons between the outputs of strong and weak tutors. This enables us to apply multiple preference optimization methods to fine-tune an underperforming tutor model (Qwen2-VL-2B) into more effective ones. Our results also show that strong problem-solving skills do not guarantee high-quality tutoring and that performance optimization-guided refinements can yield more educationally aligned tutor models. This approach opens avenues for building MLLMs that serve not only as problem solvers, but as genuinely helpful educational assistants.","authors":["Ming Liu","Liwen Wang","Wensheng Zhang"],"url":"https://arxiv.org/abs/2505.06418"}
{"created":"2025-05-13","title":"Initialization and training of matrix product state probabilistic models","abstract":"Modeling probability distributions via the wave function of a quantum state is central to quantum-inspired generative modeling and quantum state tomography (QST). We investigate a common failure mode in training randomly initialized matrix product states (MPS) using gradient descent. The results show that the trained MPS models do not accurately predict the strong interactions between boundary sites in periodic spin chain models. In the case of the Born machine algorithm, we further identify a causality trap, where the trained MPS models resemble causal models that ignore the non-local correlations in the true distribution. We propose two complementary strategies to overcome the training failure -- one through optimization and one through initialization. First, we develop a natural gradient descent (NGD) method, which approximately simulates the gradient flow on tensor manifolds and significantly enhances training efficiency. Numerical experiments show that NGD avoids local minima in both Born machines and in general MPS tomography. Remarkably, we show that NGD with line search can converge to the global minimum in only a few iterations. Second, for the BM algorithm, we introduce a warm-start initialization based on the TTNS-Sketch algorithm. We show that gradient descent under a warm initialization does not encounter the causality trap and admits rapid convergence to the ground truth.","authors":["Xun Tang","Yuehaw Khoo","Lexing Ying"],"url":"https://arxiv.org/abs/2505.06419"}
{"created":"2025-05-13","title":"What Do People Want to Know About Artificial Intelligence (AI)? The Importance of Answering End-User Questions to Explain Autonomous Vehicle (AV) Decisions","abstract":"Improving end-users' understanding of decisions made by autonomous vehicles (AVs) driven by artificial intelligence (AI) can improve utilization and acceptance of AVs. However, current explanation mechanisms primarily help AI researchers and engineers in debugging and monitoring their AI systems, and may not address the specific questions of end-users, such as passengers, about AVs in various scenarios. In this paper, we conducted two user studies to investigate questions that potential AV passengers might pose while riding in an AV and evaluate how well answers to those questions improve their understanding of AI-driven AV decisions. Our initial formative study identified a range of questions about AI in autonomous driving that existing explanation mechanisms do not readily address. Our second study demonstrated that interactive text-based explanations effectively improved participants' comprehension of AV decisions compared to simply observing AV decisions. These findings inform the design of interactions that motivate end-users to engage with and inquire about the reasoning behind AI-driven AV decisions.","authors":["Somayeh Molaei","Lionel P. Robert","Nikola Banovic"],"url":"https://arxiv.org/abs/2505.06428"}
{"created":"2025-05-13","title":"My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing","abstract":"Generative Adversarial Network approaches such as StyleGAN/2 provide two key benefits: the ability to generate photo-realistic face images and possessing a semantically structured latent space from which these images are created. Many approaches have emerged for editing images derived from vectors in the latent space of a pre-trained StyleGAN/2 models by identifying semantically meaningful directions (e.g., gender or age) in the latent space. By moving the vector in a specific direction, the ideal result would only change the target feature while preserving all the other features. Providing an ideal data augmentation approach for gesture research as it could be used to generate numerous image variations whilst keeping the facial expressions intact. However, entanglement issues, where changing one feature inevitably affects other features, impacts the ability to preserve facial expressions. To address this, we propose the use of an addition to the loss function of a Facial Keypoint Detection model to restrict changes to the facial expressions. Building on top of an existing model, adding the proposed Human Face Landmark Detection (HFLD) loss, provided by a pre-trained Facial Keypoint Detection model, to the original loss function. We quantitatively and qualitatively evaluate the existing and our extended model, showing the effectiveness of our approach in addressing the entanglement issue and maintaining the facial expression. Our approach achieves up to 49% reduction in the change of emotion in our experiments. Moreover, we show the benefit of our approach by comparing with state-of-the-art models. By increasing the ability to preserve the facial gesture and expression during facial transformation, we present a way to create human face images with fixed expression but different appearances, making it a reliable data augmentation approach for Facial Gesture and Expression research.","authors":["Jingrui He","Andrew Stephen McGough"],"url":"https://arxiv.org/abs/2505.06436"}
{"created":"2025-05-13","title":"Reliable Collaborative Conversational Agent System Based on LLMs and Answer Set Programming","abstract":"As the Large-Language-Model-driven (LLM-driven) Artificial Intelligence (AI) bots became popular, people realized their strong potential in Task-Oriented Dialogue (TOD). However, bots relying wholly on LLMs are unreliable in their knowledge, and whether they can finally produce a correct result for the task is not guaranteed. The collaboration among these agents also remains a challenge, since the necessary information to convey is unclear, and the information transfer is by prompts -- unreliable, and malicious knowledge is easy to inject. With the help of logic programming tools such as Answer Set Programming (ASP), conversational agents can be built safely and reliably, and communication among the agents made more efficient and secure. We proposed an Administrator-Assistant Dual-Agent paradigm, where the two ASP-driven bots share the same knowledge base and complete their tasks independently, while the information can be passed by a Collaborative Rule Set (CRS). The knowledge and information conveyed are encapsulated and invisible to the users, ensuring the security of information transmission. We have constructed AutoManager, a dual-agent system for managing the drive-through window of a fast-food restaurant such as Taco Bell in the US. In AutoManager, the assistant bot takes the customer's order while the administrator bot manages the menu and food supply. We evaluated our AutoManager and compared it with the real-world Taco Bell Drive-Thru AI Order Taker, and the results show that our method is more reliable.","authors":["Yankai Zeng","Gopal Gupta"],"url":"https://arxiv.org/abs/2505.06438"}
{"created":"2025-05-13","title":"Development of Reduced Feeder and Load Models Using Practical Topological and Loading Data","abstract":"Distribution feeder and load model reduction methods are essential for maintaining a good tradeoff between accurate representation of grid behavior and reduced computational complexity in power system studies. An effective algorithm to obtain a reduced order representation of the practical feeders using utility topological and loading data has been presented in this paper. Simulations conducted in this work show that the reduced feeder and load model of a utility feeder, obtained using the proposed method, can accurately capture contactor and motor stalling behaviors for critical events such as fault induced delayed voltage recovery.","authors":["Sameer Nekkalapu","Sushrut Thakar","Antos Cheeramban Varghese","Vijay Vittal","Bo Gong","Ken Brown"],"url":"https://arxiv.org/abs/2505.06439"}
{"created":"2025-05-13","title":"Tweedie Regression for Video Recommendation System","abstract":"Modern recommendation systems aim to increase click-through rates (CTR) for better user experience, through commonly treating ranking as a classification task focused on predicting CTR. However, there is a gap between this method and the actual objectives of businesses across different sectors. In video recommendation services, the objective of video on demand (VOD) extends beyond merely encouraging clicks, but also guiding users to discover their true interests, leading to increased watch time. And longer users watch time will leads to more revenue through increased chances of presenting online display advertisements. This research addresses the issue by redefining the problem from classification to regression, with a focus on maximizing revenue through user viewing time. Due to the lack of positive labels on recommendation, the study introduces Tweedie Loss Function, which is better suited in this scenario than the traditional mean square error loss. The paper also provides insights on how Tweedie process capture users diverse interests. Our offline simulation and online A/B test revealed that we can substantially enhance our core business objectives: user engagement in terms of viewing time and, consequently, revenue. Additionally, we provide a theoretical comparison between the Tweedie Loss and the commonly employed viewing time weighted Logloss, highlighting why Tweedie Regression stands out as an efficient solution. We further outline a framework for designing a loss function that focuses on a singular objective.","authors":["Yan Zheng","Qiang Chen","Chenglei Niu"],"url":"https://arxiv.org/abs/2505.06445"}
{"created":"2025-05-13","title":"Structured Prediction with Abstention via the Lov\\'asz Hinge","abstract":"The Lov\\'asz hinge is a convex loss function proposed for binary structured classification, in which k related binary predictions jointly evaluated by a submodular function. Despite its prevalence in image segmentation and related tasks, the consistency of the Lov\\'asz hinge has remained open. We show that the Lov\\'asz hinge is inconsistent with its desired target unless the set function used for evaluation is modular. Leveraging the embedding framework of Finocchiaro et al. (2024), we find the target loss for which the Lov\\'asz hinge is consistent. This target, which we call the structured abstain problem, is a variant of selective classification for structured prediction that allows one to abstain on any subset of the k binary predictions. We derive a family of link functions, each of which is simultaneously consistent for all polymatroids, a subset of submodular set functions. We then give sufficient conditions on the polymatroid for the structured abstain problem to be tightly embedded by the Lov\\'asz hinge, meaning no target prediction is redundant. We experimentally demonstrate the potential of the structured abstain problem for interpretability in structured classification tasks. Finally, for the multiclass setting, we show that one can combine the binary encoding construction of Ramaswamy et al. (2018) with our link construction to achieve an efficient consistent surrogate for a natural multiclass generalization of the structured abstain problem.","authors":["Jessie Finocchiaro","Rafael Frongillo","Enrique Nueve"],"url":"https://arxiv.org/abs/2505.06446"}
{"created":"2025-05-13","title":"Gaming the Metrics? Bibliometric Anomalies and the Integrity Crisis in Global University Rankings","abstract":"Global university rankings have transformed how certain institutions define success, often elevating metrics over meaning. This study examines universities with rapid research growth suggestive of metric-driven behaviors. Among the 1,000 most publishing institutions, 98 showed extreme output increases between 2018-2019 and 2023-2024. Of these, eighteen were selected for exhibiting sharp declines in first and corresponding authorship. Compared to national, regional, and international norms, these universities (in India, Lebanon, Saudi Arabia, and the United Arab Emirates) display patterns consistent with strategic metric optimization. Key findings include publication growth up to 965%, concentrated in STEM fields; surges in hyper-prolific authors and highly cited articles; and dense internal co-authorship and citation clusters. The group also exhibited elevated shares of publications in delisted journals and high retraction rates. These patterns illustrate vulnerabilities in global ranking systems, as metrics lose meaning when treated as targets (Goodhart's Law) and institutions emulate high-performing peers under competitive pressure (institutional isomorphism). Without reform, rankings may continue incentivizing behaviors that distort scholarly contribution and compromise research integrity.","authors":["Lokman I. Meho"],"url":"https://arxiv.org/abs/2505.06448"}
{"created":"2025-05-13","title":"Autonomous Vision-Based Magnetic Microrobotic Pushing of Micro-Objects and Cells","abstract":"Accurate and autonomous transportation of micro-objects and biological cells can enable significant advances in a wide variety of research disciplines. Here, we present a novel, vision-based, model-free microrobotic pushing algorithm for the autonomous manipulation of micro objects and biological cells. The algorithm adjusts the axis of a rotating magnetic field that in turn controls the heading angle and spin axis of a spherical Janus rolling microrobot. We introduce the concept of a microrobotic guiding corridor to constrain the object and to avoid pushing failures. We then show that employing only two simple conditions, the microrobot is able to successfully and autonomously push microscale objects along predefined trajectories. We evaluate the performance of the algorithm by measuring the mean absolute error and completion time relative to a desired path at different actuation frequencies and guiding corridor widths. Finally, we demonstrate biomedical applicability by autonomously transporting a single biological cell, highlighting the methods potential for applications in tissue engineering, drug delivery and synthetic biology.","authors":["Max Sokolich","Ceren Kirmizitas","Sambeeta Das","Ron Weiss"],"url":"https://arxiv.org/abs/2505.06450"}
{"created":"2025-05-13","title":"Adaptive Wiping: Adaptive contact-rich manipulation through few-shot imitation learning with Force-Torque feedback and pre-trained object representations","abstract":"Imitation learning offers a pathway for robots to perform repetitive tasks, allowing humans to focus on more engaging and meaningful activities. However, challenges arise from the need for extensive demonstrations and the disparity between training and real-world environments. This paper focuses on contact-rich tasks like wiping with soft and deformable objects, requiring adaptive force control to handle variations in wiping surface height and the sponge's physical properties. To address these challenges, we propose a novel method that integrates real-time force-torque (FT) feedback with pre-trained object representations. This approach allows robots to dynamically adjust to previously unseen changes in surface heights and sponges' physical properties. In real-world experiments, our method achieved 96% accuracy in applying reference forces, significantly outperforming the previous method that lacked an FT feedback loop, which only achieved 4% accuracy. To evaluate the adaptability of our approach, we conducted experiments under different conditions from the training setup, involving 40 scenarios using 10 sponges with varying physical properties and 4 types of wiping surface heights, demonstrating significant improvements in the robot's adaptability by analyzing force trajectories. The video of our work is available at: https://sites.google.com/view/adaptive-wiping","authors":["Chikaha Tsuji","Enrique Coronado","Pablo Osorio","Gentiane Venture"],"url":"https://arxiv.org/abs/2505.06451"}
{"created":"2025-05-13","title":"Sponge Attacks on Sensing AI: Energy-Latency Vulnerabilities and Defense via Model Pruning","abstract":"Recent studies have shown that sponge attacks can significantly increase the energy consumption and inference latency of deep neural networks (DNNs). However, prior work has focused primarily on computer vision and natural language processing tasks, overlooking the growing use of lightweight AI models in sensing-based applications on resource-constrained devices, such as those in Internet of Things (IoT) environments. These attacks pose serious threats of energy depletion and latency degradation in systems where limited battery capacity and real-time responsiveness are critical for reliable operation. This paper makes two key contributions. First, we present the first systematic exploration of energy-latency sponge attacks targeting sensing-based AI models. Using wearable sensing-based AI as a case study, we demonstrate that sponge attacks can substantially degrade performance by increasing energy consumption, leading to faster battery drain, and by prolonging inference latency. Second, to mitigate such attacks, we investigate model pruning, a widely adopted compression technique for resource-constrained AI, as a potential defense. Our experiments show that pruning-induced sparsity significantly improves model resilience against sponge poisoning. We also quantify the trade-offs between model efficiency and attack resilience, offering insights into the security implications of model compression in sensing-based AI systems deployed in IoT environments.","authors":["Syed Mhamudul Hasan","Hussein Zangoti","Iraklis Anagnostopoulos","Abdur R. Shahid"],"url":"https://arxiv.org/abs/2505.06454"}
{"created":"2025-05-13","title":"Rod Bustall: In Memoriam","abstract":"This is an obituary of Rod Burstall, written in his honour. Rod was a prominent computer scientist whose contributions span over forty years. Most of his career was spent at Edinburgh University. He lead the team programming Freddy, the first hand-eye assembly robot, with much of his effort being devoted to the development of the POP-2 programming language. He became interested in a mathematical approach to software development: he recognised the central role of structural induction; his work on reasoning about mutable data structures was an influential precursor of separation logic; he was the first to point out the connection between program proof and modal logic; and he was responsible for the idea that stores are mappings from locations to their contents.","authors":["J Strother Moore","Gordon Plotkin","David Rydeheard","Don Sannella"],"url":"https://arxiv.org/abs/2505.06456"}
{"created":"2025-05-13","title":"A Hybridizable Discontinuous Galerkin Method for the Miscible Displacement Problem Under Minimal Regularity","abstract":"A numerical method based on the hybridizable discontinuous Galerkin method in space and backward Euler in time is formulated and analyzed for solving the miscible displacement problem. Under low regularity assumptions, convergence is established by proving that, up to a subsequence, the discrete pressure, velocity and concentration converge to a weak solution as the mesh size and time step tend to zero. The analysis is based on several key features: an H(div) reconstruction of the velocity, the skew-symmetrization of the concentration equation, the introduction of an auxiliary variable and the definition of a new numerical flux. Numerical examples demonstrate optimal rates of convergence for smooth solutions, and convergence for problems of low regularity.","authors":["Keegan L. A. Kirk","Beatrice Riviere"],"url":"https://arxiv.org/abs/2505.06458"}
{"created":"2025-05-13","title":"Improved Uncertainty Quantification in Physics-Informed Neural Networks Using Error Bounds and Solution Bundles","abstract":"Physics-Informed Neural Networks (PINNs) have been widely used to obtain solutions to various physical phenomena modeled as Differential Equations. As PINNs are not naturally equipped with mechanisms for Uncertainty Quantification, some work has been done to quantify the different uncertainties that arise when dealing with PINNs. In this paper, we use a two-step procedure to train Bayesian Neural Networks that provide uncertainties over the solutions to differential equation systems provided by PINNs. We use available error bounds over PINNs to formulate a heteroscedastic variance that improves the uncertainty estimation. Furthermore, we solve forward problems and utilize the obtained uncertainties when doing parameter estimation in inverse problems in cosmology.","authors":["Pablo Flores","Olga Graf","Pavlos Protopapas","Karim Pichara"],"url":"https://arxiv.org/abs/2505.06459"}
{"created":"2025-05-13","title":"Challenging GPU Dominance: When CPUs Outperform for On-Device LLM Inference","abstract":"The common assumption in on-device AI is that GPUs, with their superior parallel processing, always provide the best performance for large language model (LLM) inference. In this work, we challenge this notion by empirically demonstrating that, under certain conditions, CPUs can outperform GPUs for LLM inference on mobile devices. Using a 1-billion-parameter LLM deployed via llama.cpp on the iPhone 15 Pro, we show that a CPU-only configuration (two threads, F16 precision) achieves 17 tokens per second, surpassing the 12.8 tokens per second obtained with GPU acceleration. We analyze the architectural factors driving this counterintuitive result, revealing that GPU memory transfer overhead and CPU thread optimization play a critical role. Furthermore, we explore the impact of thread oversubscription, quantization strategies, and hardware constraints, providing new insights into efficient on-device AI execution. Our findings challenge conventional GPU-first thinking, highlighting the untapped potential of optimized CPU inference and paving the way for smarter deployment strategies in mobile AI. However, fully explaining the observed CPU advantage remains difficult due to limited access to low-level profiling tools on iOS.","authors":["Haolin Zhang","Jeff Huang"],"url":"https://arxiv.org/abs/2505.06461"}
{"created":"2025-05-13","title":"Opening the Scope of Openness in AI","abstract":"The concept of openness in AI has so far been heavily inspired by the definition and community practice of open source software. This positions openness in AI as having positive connotations; it introduces assumptions of certain advantages, such as collaborative innovation and transparency. However, the practices and benefits of open source software are not fully transferable to AI, which has its own challenges. Framing a notion of openness tailored to AI is crucial to addressing its growing societal implications, risks, and capabilities. We argue that considering the fundamental scope of openness in different disciplines will broaden discussions, introduce important perspectives, and reflect on what openness in AI should mean. Toward this goal, we qualitatively analyze 98 concepts of openness discovered from topic modeling, through which we develop a taxonomy of openness. Using this taxonomy as an instrument, we situate the current discussion on AI openness, identify gaps and highlight links with other disciplines. Our work contributes to the recent efforts in framing openness in AI by reflecting principles and practices of openness beyond open source software and calls for a more holistic view of openness in terms of actions, system properties, and ethical objectives.","authors":["Tamara Paris","AJung Moon","Jin Guo"],"url":"https://arxiv.org/abs/2505.06464"}
{"created":"2025-05-13","title":"Handling Pedestrian Uncertainty in Coordinating Autonomous Vehicles at Signal-Free Intersections","abstract":"In this paper, we provide a theoretical framework for the coordination of connected and automated vehicles (CAVs) at signal-free intersections, accounting for the unexpected presence of pedestrians. First, we introduce a general vehicle-to-infrastructure communication protocol and a low-level controller that determines the optimal unconstrained trajectories for CAVs, in terms of fuel efficiency and travel time, to cross the intersection without considering pedestrians. If such an unconstrained trajectory is unattainable, we introduce sufficient certificates for each CAV to cross the intersection while respecting the associated constraints. Next, we consider the case where an unexpected pedestrian enters the road. When the CAV's sensors detect a pedestrian, an emergency mode is activated, which imposes certificates related to an unsafe set in the pedestrian's proximity area. Simultaneously, a re-planning mechanism is implemented for all CAVs to accommodate the trajectories of vehicles operating in emergency mode. Finally, we validate the efficacy of our approach through simulations conducted in MATLAB and RoadRunner softwares, which facilitate the integration of sensor tools and the realization of real-time implementation.","authors":["Filippos N. Tzortzoglou","Andreas A. Malikopoulos"],"url":"https://arxiv.org/abs/2505.06465"}
{"created":"2025-05-13","title":"PromptIQ: Who Cares About Prompts? Let System Handle It -- A Component-Aware Framework for T2I Generation","abstract":"Generating high-quality images without prompt engineering expertise remains a challenge for text-to-image (T2I) models, which often misinterpret poorly structured prompts, leading to distortions and misalignments. While humans easily recognize these flaws, metrics like CLIP fail to capture structural inconsistencies, exposing a key limitation in current evaluation methods. To address this, we introduce PromptIQ, an automated framework that refines prompts and assesses image quality using our novel Component-Aware Similarity (CAS) metric, which detects and penalizes structural errors. Unlike conventional methods, PromptIQ iteratively generates and evaluates images until the user is satisfied, eliminating trial-and-error prompt tuning. Our results show that PromptIQ significantly improves generation quality and evaluation accuracy, making T2I models more accessible for users with little to no prompt engineering expertise.","authors":["Nisan Chhetri","Arpan Sainju"],"url":"https://arxiv.org/abs/2505.06467"}
{"created":"2025-05-13","title":"KCluster: An LLM-based Clustering Approach to Knowledge Component Discovery","abstract":"Educators evaluate student knowledge using knowledge component (KC) models that map assessment questions to KCs. Still, designing KC models for large question banks remains an insurmountable challenge for instructors who need to analyze each question by hand. The growing use of Generative AI in education is expected only to aggravate this chronic deficiency of expert-designed KC models, as course engineers designing KCs struggle to keep up with the pace at which questions are generated. In this work, we propose KCluster, a novel KC discovery algorithm based on identifying clusters of congruent questions according to a new similarity metric induced by a large language model (LLM). We demonstrate in three datasets that an LLM can create an effective metric of question similarity, which a clustering algorithm can use to create KC models from questions with minimal human effort. Combining the strengths of LLM and clustering, KCluster generates descriptive KC labels and discovers KC models that predict student performance better than the best expert-designed models available. In anticipation of future work, we illustrate how KCluster can reveal insights into difficult KCs and suggest improvements to instruction.","authors":["Yumou Wei","Paulo Carvalho","John Stamper"],"url":"https://arxiv.org/abs/2505.06469"}
{"created":"2025-05-13","title":"\"vcd2df\" -- Leveraging Data Science Insights for Hardware Security Research","abstract":"In this work, we hope to expand the universe of security practitioners of open-source hardware by creating a bridge from hardware design languages (HDLs) to data science languages like Python and R through libraries that converge VCD (value change dump) files into data frames, the expected input type of the modern data science tools. We show how insights can be derived in high-level languages from register transfer level (RTL) trace data. Additional, we show a promising future direction in hardware security research leveraging the parallelism of the Spark DataFrame.","authors":["Calvin Deutschbein","Jimmy Ostler"],"url":"https://arxiv.org/abs/2505.06470"}
{"created":"2025-05-13","title":"Joint Parameterization of Hybrid Physics-Based and Machine Learning Li-Ion Battery Model","abstract":"Electrochemical hybrid battery models have major potential to enable advanced physics-based control, diagnostic, and prognostic features for next-generation lithium-ion battery management systems. This is due to the physical significance of the electrochemical model, which is complemented by a machine learning model that compensates for output prediction errors caused by system uncertainties. While hybrid models have demonstrated robust output prediction performance under large system uncertainties, they are highly susceptible to the influence of uncertainties during parameter identification, which can compromise the physical significance of the model. To address this challenge, we present a parameter estimation framework that explicitly considers system uncertainties through a discrepancy function. The approach also incorporates a downsampling procedure to address the computational barriers associated with large time-series data sets, as are typical in the battery domain. The framework was validated in simulation, yielding several mean parameter estimation errors that were one order of magnitude smaller than those of the conventional least squares approach. While developed for the high-uncertainty, electrochemical hybrid modeling context, the estimation framework is applicable to all models and is presented in a generalized form.","authors":["Jackson Fogelquist","Xinfan Lin"],"url":"https://arxiv.org/abs/2505.06473"}
{"created":"2025-05-13","title":"Probing In-Context Learning: Impact of Task Complexity and Model Architecture on Generalization and Efficiency","abstract":"We investigate in-context learning (ICL) through a meticulous experimental framework that systematically varies task complexity and model architecture. Extending beyond the linear regression baseline, we introduce Gaussian kernel regression and nonlinear dynamical system tasks, which emphasize temporal and recursive reasoning. We evaluate four distinct models: a GPT2-style Transformer, a Transformer with FlashAttention mechanism, a convolutional Hyena-based model, and the Mamba state-space model. Each model is trained from scratch on synthetic datasets and assessed for generalization during testing. Our findings highlight that model architecture significantly shapes ICL performance. The standard Transformer demonstrates robust performance across diverse tasks, while Mamba excels in temporally structured dynamics. Hyena effectively captures long-range dependencies but shows higher variance early in training, and FlashAttention offers computational efficiency but is more sensitive in low-data regimes. Further analysis uncovers locality-induced shortcuts in Gaussian kernel tasks, enhanced nonlinear separability through input range scaling, and the critical role of curriculum learning in mastering high-dimensional tasks.","authors":["Binwen Liu","Peiyu Xu","Quan Yuan","Yihong Chen"],"url":"https://arxiv.org/abs/2505.06475"}
{"created":"2025-05-13","title":"Learning from the Good Ones: Risk Profiling-Based Defenses Against Evasion Attacks on DNNs","abstract":"Safety-critical applications such as healthcare and autonomous vehicles use deep neural networks (DNN) to make predictions and infer decisions. DNNs are susceptible to evasion attacks, where an adversary crafts a malicious data instance to trick the DNN into making wrong decisions at inference time. Existing defenses that protect DNNs against evasion attacks are either static or dynamic. Static defenses are computationally efficient but do not adapt to the evolving threat landscape, while dynamic defenses are adaptable but suffer from an increased computational overhead. To combine the best of both worlds, in this paper, we propose a novel risk profiling framework that uses a risk-aware strategy to selectively train static defenses using victim instances that exhibit the most resilient features and are hence more resilient against an evasion attack. We hypothesize that training existing defenses on instances that are less vulnerable to the attack enhances the adversarial detection rate by reducing false negatives. We evaluate the efficacy of our risk-aware selective training strategy on a blood glucose management system that demonstrates how training static anomaly detectors indiscriminately may result in an increased false negative rate, which could be life-threatening in safety-critical applications. Our experiments show that selective training on the less vulnerable patients achieves a recall increase of up to 27.5\\% with minimal impact on precision compared to indiscriminate training.","authors":["Mohammed Elnawawy","Gargi Mitra","Shahrear Iqbal","Karthik Pattabiraman"],"url":"https://arxiv.org/abs/2505.06477"}
{"created":"2025-05-13","title":"QoS-Efficient Serving of Multiple Mixture-of-Expert LLMs Using Partial Runtime Reconfiguration","abstract":"The deployment of mixture-of-experts (MoE) large language models (LLMs) presents significant challenges due to their high memory demands. These challenges become even more pronounced in multi-tenant environments, where shared resources must accommodate multiple models, limiting the effectiveness of conventional virtualization techniques. This paper addresses the problem of efficiently serving multiple fine-tuned MoE-LLMs on a single-GPU. We propose a serving system that employs \\textit{similarity-based expert consolidation} to reduce the overall memory footprint by sharing similar experts across models. To ensure output quality, we introduce \\textit{runtime partial reconfiguration}, dynamically replacing non-expert layers when processing requests from different models. As a result, our approach achieves a competitive output quality while maintaining throughput comparable to serving a single model while incurring a negligible increase in time-to-first-token (TTFT). Experiments on a server with a single NVIDIA A100 GPU (80GB) using Mixtral-8x7B models demonstrate an 85\\% average reduction in turnaround time compared to NVIDIA's multi-instance GPU (MIG). Furthermore, experiments on Google's Switch Transformer Base-8 model with up to four variants demonstrate the scalability and resilience of our approach in maintaining output quality compared to other model merging baselines, highlighting its effectiveness.","authors":["HamidReza Imani","Jiaxin Peng","Peiman Mohseni","Abdolah Amirany","Tarek El-Ghazawi"],"url":"https://arxiv.org/abs/2505.06481"}
{"created":"2025-05-13","title":"Video-Enhanced Offline Reinforcement Learning: A Model-Based Approach","abstract":"Offline reinforcement learning (RL) enables policy optimization in static datasets, avoiding the risks and costs of real-world exploration. However, it struggles with suboptimal behavior learning and inaccurate value estimation due to the lack of environmental interaction. In this paper, we present Video-Enhanced Offline RL (VeoRL), a model-based approach that constructs an interactive world model from diverse, unlabeled video data readily available online. Leveraging model-based behavior guidance, VeoRL transfers commonsense knowledge of control policy and physical dynamics from natural videos to the RL agent within the target domain. Our method achieves substantial performance gains (exceeding 100% in some cases) across visuomotor control tasks in robotic manipulation, autonomous driving, and open-world video games.","authors":["Minting Pan","Yitao Zheng","Jiajian Li","Yunbo Wang","Xiaokang Yang"],"url":"https://arxiv.org/abs/2505.06482"}
{"created":"2025-05-13","title":"CompSLAM: Complementary Hierarchical Multi-Modal Localization and Mapping for Robot Autonomy in Underground Environments","abstract":"Robot autonomy in unknown, GPS-denied, and complex underground environments requires real-time, robust, and accurate onboard pose estimation and mapping for reliable operations. This becomes particularly challenging in perception-degraded subterranean conditions under harsh environmental factors, including darkness, dust, and geometrically self-similar structures. This paper details CompSLAM, a highly resilient and hierarchical multi-modal localization and mapping framework designed to address these challenges. Its flexible architecture achieves resilience through redundancy by leveraging the complementary nature of pose estimates derived from diverse sensor modalities. Developed during the DARPA Subterranean Challenge, CompSLAM was successfully deployed on all aerial, legged, and wheeled robots of Team Cerberus during their competition-winning final run. Furthermore, it has proven to be a reliable odometry and mapping solution in various subsequent projects, with extensions enabling multi-robot map sharing for marsupial robotic deployments and collaborative mapping. This paper also introduces a comprehensive dataset acquired by a manually teleoperated quadrupedal robot, covering a significant portion of the DARPA Subterranean Challenge finals course. This dataset evaluates CompSLAM's robustness to sensor degradations as the robot traverses 740 meters in an environment characterized by highly variable geometries and demanding lighting conditions. The CompSLAM code and the DARPA SubT Finals dataset are made publicly available for the benefit of the robotics community","authors":["Shehryar Khattak","Timon Homberger","Lukas Bernreiter","Julian Nubert","Olov Andersson","Roland Siegwart","Kostas Alexis","Marco Hutter"],"url":"https://arxiv.org/abs/2505.06483"}
{"created":"2025-05-13","title":"10 quick tips for making your software outlive your job","abstract":"Loss of key personnel has always been a risk for research software projects. Key members of the team may have to step away due to illness or burnout, to care for a family member, from a loss of financial support, or because their career is going in a new direction. Today, though, political and financial changes are putting large numbers of researchers out of work simultaneously, potentially leaving large amounts of research software abandoned. This article presents ten tips to help researchers ensure that the software they have built will continue to be usable after they have left their present job -- whether in the course of voluntary career moves or researcher mobility, but particularly in cases of involuntary departure due to political or institutional changes.","authors":["Richard Littauer","Greg Wilson","Jan Ainali","Eman Abdullah AlOmar","Sylwester Arabas","Yanina Bellini Saibene","Kris Bubendorfer","Kaylea Champion","Clare Dillon","Jouni Helske","Pieter Huybrechts","Daniel S. Katz","Chang Liao","David Lippert","Fang Liu","Pierre Marshall","Daniel R. McCloy","Ian McInerney","Mohamed Wiem Mkaouer","Priyanka Ojha","Christoph Treude","Ethan P. White"],"url":"https://arxiv.org/abs/2505.06484"}
{"created":"2025-05-13","title":"SmartPilot: A Multiagent CoPilot for Adaptive and Intelligent Manufacturing","abstract":"In the dynamic landscape of Industry 4.0, achieving efficiency, precision, and adaptability is essential to optimize manufacturing operations. Industries suffer due to supply chain disruptions caused by anomalies, which are being detected by current AI models but leaving domain experts uncertain without deeper insights into these anomalies. Additionally, operational inefficiencies persist due to inaccurate production forecasts and the limited effectiveness of traditional AI models for processing complex sensor data. Despite these advancements, existing systems lack the seamless integration of these capabilities needed to create a truly unified solution for enhancing production and decision-making. We propose SmartPilot, a neurosymbolic, multiagent CoPilot designed for advanced reasoning and contextual decision-making to address these challenges. SmartPilot processes multimodal sensor data and is compact to deploy on edge devices. It focuses on three key tasks: anomaly prediction, production forecasting, and domain-specific question answering. By bridging the gap between AI capabilities and real-world industrial needs, SmartPilot empowers industries with intelligent decision-making and drives transformative innovation in manufacturing. The demonstration video, datasets, and supplementary materials are available at https://github.com/ChathurangiShyalika/SmartPilot.","authors":["Chathurangi Shyalika","Renjith Prasad","Alaa Al Ghazo","Darssan Eswaramoorthi","Harleen Kaur","Sara Shree Muthuselvam","Amit Sheth"],"url":"https://arxiv.org/abs/2505.06492"}
{"created":"2025-05-13","title":"System Prompt Poisoning: Persistent Attacks on Large Language Models Beyond User Injection","abstract":"Large language models (LLMs) have gained widespread adoption across diverse applications due to their impressive generative capabilities. Their plug-and-play nature enables both developers and end users to interact with these models through simple prompts. However, as LLMs become more integrated into various systems in diverse domains, concerns around their security are growing. Existing studies mainly focus on threats arising from user prompts (e.g. prompt injection attack) and model output (e.g. model inversion attack), while the security of system prompts remains largely overlooked. This work bridges the critical gap. We introduce system prompt poisoning, a new attack vector against LLMs that, unlike traditional user prompt injection, poisons system prompts hence persistently impacts all subsequent user interactions and model responses. We systematically investigate four practical attack strategies in various poisoning scenarios. Through demonstration on both generative and reasoning LLMs, we show that system prompt poisoning is highly feasible without requiring jailbreak techniques, and effective across a wide range of tasks, including those in mathematics, coding, logical reasoning, and natural language processing. Importantly, our findings reveal that the attack remains effective even when user prompts employ advanced prompting techniques like chain-of-thought (CoT). We also show that such techniques, including CoT and retrieval-augmentation-generation (RAG), which are proven to be effective for improving LLM performance in a wide range of tasks, are significantly weakened in their effectiveness by system prompt poisoning.","authors":["Jiawei Guo","Haipeng Cai"],"url":"https://arxiv.org/abs/2505.06493"}
{"created":"2025-05-13","title":"xGen-small Technical Report","abstract":"We introduce xGen-small, a family of 4B and 9B Transformer decoder models optimized for long-context applications. Our vertically integrated pipeline unites domain-balanced, frequency-aware data curation; multi-stage pre-training with quality annealing and length extension to 128k tokens; and targeted post-training via supervised fine-tuning, preference learning, and online reinforcement learning. xGen-small delivers strong performance across various tasks, especially in math and coding domains, while excelling at long context benchmarks.","authors":["Erik Nijkamp","Bo Pang","Egor Pakhomov","Akash Gokul","Jin Qu","Silvio Savarese","Yingbo Zhou","Caiming Xiong"],"url":"https://arxiv.org/abs/2505.06496"}
{"created":"2025-05-13","title":"FedADP: Unified Model Aggregation for Federated Learning with Heterogeneous Model Architectures","abstract":"Traditional Federated Learning (FL) faces significant challenges in terms of efficiency and accuracy, particularly in heterogeneous environments where clients employ diverse model architectures and have varying computational resources. Such heterogeneity complicates the aggregation process, leading to performance bottlenecks and reduced model generalizability. To address these issues, we propose FedADP, a federated learning framework designed to adapt to client heterogeneity by dynamically adjusting model architectures during aggregation. FedADP enables effective collaboration among clients with differing capabilities, maximizing resource utilization and ensuring model quality. Our experimental results demonstrate that FedADP significantly outperforms existing methods, such as FlexiFed, achieving an accuracy improvement of up to 23.30%, thereby enhancing model adaptability and training efficiency in heterogeneous real-world settings.","authors":["Jiacheng Wang","Hongtao Lv","Lei Liu"],"url":"https://arxiv.org/abs/2505.06497"}
{"created":"2025-05-13","title":"An In-kernel Forensics Engine for Investigating Evasive Attacks","abstract":"Over the years, adversarial attempts against critical services have become more effective and sophisticated in launching low-profile attacks. This trend has always been concerning. However, an even more alarming trend is the increasing difficulty of collecting relevant evidence about these attacks and the involved threat actors in the early stages before significant damage is done. This issue puts defenders at a significant disadvantage, as it becomes exceedingly difficult to understand the attack details and formulate an appropriate response. Developing robust forensics tools to collect evidence about modern threats has never been easy. One main challenge is to provide a robust trade-off between achieving sufficient visibility while leaving minimal detectable artifacts. This paper will introduce LASE, an open-source Low-Artifact Forensics Engine to perform threat analysis and forensics in Windows operating system. LASE augments current analysis tools by providing detailed, system-wide monitoring capabilities while minimizing detectable artifacts. We designed multiple deployment scenarios, showing LASE's potential in evidence gathering and threat reasoning in a real-world setting. By making LASE and its execution trace data available to the broader research community, this work encourages further exploration in the field by reducing the engineering costs for threat analysis and building a longitudinal behavioral analysis catalog for diverse security domains.","authors":["Javad Zhandi","Lalchandra Rampersaud","Amin Kharraz"],"url":"https://arxiv.org/abs/2505.06498"}
{"created":"2025-05-13","title":"Survey of Filtered Approximate Nearest Neighbor Search over the Vector-Scalar Hybrid Data","abstract":"Filtered approximate nearest neighbor search (FANNS), an extension of approximate nearest neighbor search (ANNS) that incorporates scalar filters, has been widely applied to constrained retrieval of vector data. Despite its growing importance, no dedicated survey on FANNS over the vector-scalar hybrid data currently exists, and the field has several problems, including inconsistent definitions of the search problem, insufficient framework for algorithm classification, and incomplete analysis of query difficulty. This survey paper formally defines the concepts of hybrid dataset and hybrid query, as well as the corresponding evaluation metrics. Based on these, a pruning-focused framework is proposed to classify and summarize existing algorithms, providing a broader and finer-grained classification framework compared to the existing ones. In addition, a review is conducted on representative hybrid datasets, followed by an analysis on the difficulty of hybrid queries from the perspective of distribution relationships between data and queries. This paper aims to establish a structured foundation for FANNS over the vector-scalar hybrid data, facilitate more meaningful comparisons between FANNS algorithms, and offer practical recommendations for practitioners. The code used for downloading hybrid datasets and analyzing query difficulty is available at https://github.com/lyj-fdu/FANNS","authors":["Yanjun Lin","Kai Zhang","Zhenying He","Yinan Jing","X. Sean Wang"],"url":"https://arxiv.org/abs/2505.06501"}
{"created":"2025-05-13","title":"FlexNeRFer: A Multi-Dataflow, Adaptive Sparsity-Aware Accelerator for On-Device NeRF Rendering","abstract":"Neural Radiance Fields (NeRF), an AI-driven approach for 3D view reconstruction, has demonstrated impressive performance, sparking active research across fields. As a result, a range of advanced NeRF models has emerged, leading on-device applications to increasingly adopt NeRF for highly realistic scene reconstructions. With the advent of diverse NeRF models, NeRF-based applications leverage a variety of NeRF frameworks, creating the need for hardware capable of efficiently supporting these models. However, GPUs fail to meet the performance, power, and area (PPA) cost demanded by these on-device applications, or are specialized for specific NeRF algorithms, resulting in lower efficiency when applied to other NeRF models. To address this limitation, in this work, we introduce FlexNeRFer, an energy-efficient versatile NeRF accelerator. The key components enabling the enhancement of FlexNeRFer include: i) a flexible network-on-chip (NoC) supporting multi-dataflow and sparsity on precision-scalable MAC array, and ii) efficient data storage using an optimal sparsity format based on the sparsity ratio and precision modes. To evaluate the effectiveness of FlexNeRFer, we performed a layout implementation using 28nm CMOS technology. Our evaluation shows that FlexNeRFer achieves 8.2~243.3x speedup and 24.1~520.3x improvement in energy efficiency over a GPU (i.e., NVIDIA RTX 2080 Ti), while demonstrating 4.2~86.9x speedup and 2.3~47.5x improvement in energy efficiency compared to a state-of-the-art NeRF accelerator (i.e., NeuRex).","authors":["Seock-Hwan Noh (DGIST)","Banseok Shin (DGIST)","Jeik Choi (DGIST)","Seungpyo Lee (DGIST)","Jaeha Kung (DGIST)","Yeseong Kim (DGIST)"],"url":"https://arxiv.org/abs/2505.06504"}
{"created":"2025-05-13","title":"On Definite Iterated Belief Revision with Belief Algebras","abstract":"Traditional logic-based belief revision research focuses on designing rules to constrain the behavior of revision operators. Frameworks have been proposed to characterize iterated revision rules, but they are often too loose, leading to multiple revision operators that all satisfy the rules under the same belief condition. In many practical applications, such as safety critical ones, it is important to specify a definite revision operator to enable agents to iteratively revise their beliefs in a deterministic way. In this paper, we propose a novel framework for iterated belief revision by characterizing belief information through preference relations. Semantically, both beliefs and new evidence are represented as belief algebras, which provide a rich and expressive foundation for belief revision. Building on traditional revision rules, we introduce additional postulates for revision with belief algebra, including an upper-bound constraint on the outcomes of revision. We prove that the revision result is uniquely determined given the current belief state and new evidence. Furthermore, to make the framework more useful in practice, we develop a particular algorithm for performing the proposed revision process. We argue that this approach may offer a more predictable and principled method for belief revision, making it suitable for real-world applications.","authors":["Hua Meng","Zhiguo Long","Michael Sioutis","Zhengchun Zhou"],"url":"https://arxiv.org/abs/2505.06505"}
{"created":"2025-05-13","title":"Text-to-CadQuery: A New Paradigm for CAD Generation with Scalable Large Model Capabilities","abstract":"Computer-aided design (CAD) is fundamental to modern engineering and manufacturing, but creating CAD models still requires expert knowledge and specialized software. Recent advances in large language models (LLMs) open up the possibility of generative CAD, where natural language is directly translated into parametric 3D models. However, most existing methods generate task-specific command sequences that pretrained models cannot directly handle. These sequences must be converted into CAD representations such as CAD vectors before a 3D model can be produced, which requires training models from scratch and adds unnecessary complexity. To tackle this issue, we propose generating CadQuery code directly from text, leveraging the strengths of pretrained LLMs to produce 3D models without intermediate representations, using this Python-based scripting language. Since LLMs already excel at Python generation and spatial reasoning, fine-tuning them on Text-to-CadQuery data proves highly effective. Given that these capabilities typically improve with scale, we hypothesize that larger models will perform better after fine-tuning. To enable this, we augment the Text2CAD dataset with 170,000 CadQuery annotations. We fine-tune six open-source LLMs of varying sizes and observe consistent improvements. Our best model achieves a top-1 exact match of 69.3%, up from 58.8%, and reduces Chamfer Distance by 48.6%. Project page: https://github.com/Text-to-CadQuery/Text-to-CadQuery.","authors":["Haoyang Xie","Feng Ju"],"url":"https://arxiv.org/abs/2505.06507"}
{"created":"2025-05-13","title":"HCMA: Hierarchical Cross-model Alignment for Grounded Text-to-Image Generation","abstract":"Text-to-image synthesis has progressed to the point where models can generate visually compelling images from natural language prompts. Yet, existing methods often fail to reconcile high-level semantic fidelity with explicit spatial control, particularly in scenes involving multiple objects, nuanced relations, or complex layouts. To bridge this gap, we propose a Hierarchical Cross-Modal Alignment (HCMA) framework for grounded text-to-image generation. HCMA integrates two alignment modules into each diffusion sampling step: a global module that continuously aligns latent representations with textual descriptions to ensure scene-level coherence, and a local module that employs bounding-box layouts to anchor objects at specified locations, enabling fine-grained spatial control. Extensive experiments on the MS-COCO 2014 validation set show that HCMA surpasses state-of-the-art baselines, achieving a 0.69 improvement in Frechet Inception Distance (FID) and a 0.0295 gain in CLIP Score. These results demonstrate HCMA's effectiveness in faithfully capturing intricate textual semantics while adhering to user-defined spatial constraints, offering a robust solution for semantically grounded image generation.Our code is available at https://github.com/hwang-cs-ime/HCMA","authors":["Hang Wang","Zhi-Qi Cheng","Chenhao Lin","Chao Shen","Lei Zhang"],"url":"https://arxiv.org/abs/2505.06512"}
{"created":"2025-05-13","title":"LLM-Flock: Decentralized Multi-Robot Flocking via Large Language Models and Influence-Based Consensus","abstract":"Large Language Models (LLMs) have advanced rapidly in recent years, demonstrating strong capabilities in problem comprehension and reasoning. Inspired by these developments, researchers have begun exploring the use of LLMs as decentralized decision-makers for multi-robot formation control. However, prior studies reveal that directly applying LLMs to such tasks often leads to unstable and inconsistent behaviors, where robots may collapse to the centroid of their positions or diverge entirely due to hallucinated reasoning, logical inconsistencies, and limited coordination awareness. To overcome these limitations, we propose a novel framework that integrates LLMs with an influence-based plan consensus protocol. In this framework, each robot independently generates a local plan toward the desired formation using its own LLM. The robots then iteratively refine their plans through a decentralized consensus protocol that accounts for their influence on neighboring robots. This process drives the system toward a coherent and stable flocking formation in a fully decentralized manner. We evaluate our approach through comprehensive simulations involving both state-of-the-art closed-source LLMs (e.g., o3-mini, Claude 3.5) and open-source models (e.g., Llama3.1-405b, Qwen-Max, DeepSeek-R1). The results show notable improvements in stability, convergence, and adaptability over previous LLM-based methods. We further validate our framework on a physical team of Crazyflie drones, demonstrating its practical viability and effectiveness in real-world multi-robot systems.","authors":["Peihan Li","Lifeng Zhou"],"url":"https://arxiv.org/abs/2505.06513"}
{"created":"2025-05-13","title":"RESAR-BEV: An Explainable Progressive Residual Autoregressive Approach for Camera-Radar Fusion in BEV Segmentation","abstract":"Bird's-Eye-View (BEV) semantic segmentation provides comprehensive environmental perception for autonomous driving but suffers multi-modal misalignment and sensor noise. We propose RESAR-BEV, a progressive refinement framework that advances beyond single-step end-to-end approaches: (1) progressive refinement through residual autoregressive learning that decomposes BEV segmentation into interpretable coarse-to-fine stages via our Drive-Transformer and Modifier-Transformer residual prediction cascaded architecture, (2) robust BEV representation combining ground-proximity voxels with adaptive height offsets and dual-path voxel feature encoding (max+attention pooling) for efficient feature extraction, and (3) decoupled supervision with offline Ground Truth decomposition and online joint optimization to prevent overfitting while ensuring structural coherence. Experiments on nuScenes demonstrate RESAR-BEV achieves state-of-the-art performance with 54.0% mIoU across 7 essential driving-scene categories while maintaining real-time capability at 14.6 FPS. The framework exhibits robustness in challenging scenarios of long-range perception and adverse weather conditions.","authors":["Zhiwen Zeng","Yunfei Yin","Zheng Yuan","Argho Dey","Xianjian Bao"],"url":"https://arxiv.org/abs/2505.06515"}
{"created":"2025-05-13","title":"Quantum Conflict Measurement in Decision Making for Out-of-Distribution Detection","abstract":"Quantum Dempster-Shafer Theory (QDST) uses quantum interference effects to derive a quantum mass function (QMF) as a fuzzy metric type from information obtained from various data sources. In addition, QDST uses quantum parallel computing to speed up computation. Nevertheless, the effective management of conflicts between multiple QMFs in QDST is a challenging question. This work aims to address this problem by proposing a Quantum Conflict Indicator (QCI) that measures the conflict between two QMFs in decision-making. Then, the properties of the QCI are carefully investigated. The obtained results validate its compliance with desirable conflict measurement properties such as non-negativity, symmetry, boundedness, extreme consistency and insensitivity to refinement. We then apply the proposed QCI in conflict fusion methods and compare its performance with several commonly used fusion approaches. This comparison demonstrates the superiority of the QCI-based conflict fusion method. Moreover, the Class Description Domain Space (C-DDS) and its optimized version, C-DDS+ by utilizing the QCI-based fusion method, are proposed to address the Out-of-Distribution (OOD) detection task. The experimental results show that the proposed approach gives better OOD performance with respect to several state-of-the-art baseline OOD detection methods. Specifically, it achieves an average increase in Area Under the Receiver Operating Characteristic Curve (AUC) of 1.2% and a corresponding average decrease in False Positive Rate at 95% True Negative Rate (FPR95) of 5.4% compared to the optimal baseline method.","authors":["Yilin Dong","Tianyun Zhu","Xinde Li","Jean Dezert","Rigui Zhou","Changming Zhu","Lei Cao","Shuzhi Sam Ge"],"url":"https://arxiv.org/abs/2505.06516"}
{"created":"2025-05-13","title":"Edge-Enabled VIO with Long-Tracked Features for High-Accuracy Low-Altitude IoT Navigation","abstract":"This paper presents a visual-inertial odometry (VIO) method using long-tracked features. Long-tracked features can constrain more visual frames, reducing localization drift. However, they may also lead to accumulated matching errors and drift in feature tracking. Current VIO methods adjust observation weights based on re-projection errors, yet this approach has flaws. Re-projection errors depend on estimated camera poses and map points, so increased errors might come from estimation inaccuracies, not actual feature tracking errors. This can mislead the optimization process and make long-tracked features ineffective for suppressing localization drift. Furthermore, long-tracked features constrain a larger number of frames, which poses a significant challenge to real-time performance of the system. To tackle these issues, we propose an active decoupling mechanism for accumulated errors in long-tracked feature utilization. We introduce a visual reference frame reset strategy to eliminate accumulated tracking errors and a depth prediction strategy to leverage the long-term constraint. To ensure real time preformane, we implement three strategies for efficient system state estimation: a parallel elimination strategy based on predefined elimination order, an inverse-depth elimination simplification strategy, and an elimination skipping strategy. Experiments on various datasets show that our method offers higher positioning accuracy with relatively short consumption time, making it more suitable for edge-enabled low-altitude IoT navigation, where high-accuracy positioning and real-time operation on edge device are required. The code will be published at github.","authors":["Xiaohong Huang","Cui Yang","Miaowen Wen"],"url":"https://arxiv.org/abs/2505.06517"}
{"created":"2025-05-13","title":"A Point-Based Algorithm for Distributional Reinforcement Learning in Partially Observable Domains","abstract":"In many real-world planning tasks, agents must tackle uncertainty about the environment's state and variability in the outcomes of any chosen policy. We address both forms of uncertainty as a first step toward safer algorithms in partially observable settings. Specifically, we extend Distributional Reinforcement Learning (DistRL)-which models the entire return distribution for fully observable domains-to Partially Observable Markov Decision Processes (POMDPs), allowing an agent to learn the distribution of returns for each conditional plan. Concretely, we introduce new distributional Bellman operators for partial observability and prove their convergence under the supremum p-Wasserstein metric. We also propose a finite representation of these return distributions via psi-vectors, generalizing the classical alpha-vectors in POMDP solvers. Building on this, we develop Distributional Point-Based Value Iteration (DPBVI), which integrates psi-vectors into a standard point-based backup procedure-bridging DistRL and POMDP planning. By tracking return distributions, DPBVI naturally enables risk-sensitive control in domains where rare, high-impact events must be carefully managed. We provide source code to foster further research in robust decision-making under partial observability.","authors":["Larry Preuett III"],"url":"https://arxiv.org/abs/2505.06518"}
{"created":"2025-05-13","title":"Interpretable SHAP-bounded Bayesian Optimization for Underwater Acoustic Metamaterial Coating Design","abstract":"We developed an interpretability informed Bayesian optimization framework to optimize underwater acoustic coatings based on polyurethane elastomers with embedded metamaterial features. A data driven model was employed to analyze the relationship between acoustic performance, specifically sound absorption and the corresponding design variables. By leveraging SHapley Additive exPlanations (SHAP), a machine learning interpretability tool, we identified the key parameters influencing the objective function and gained insights into how these parameters affect sound absorption. The insights derived from the SHAP analysis were subsequently used to automatically refine the bounds of the optimization problem automatically, enabling a more targeted and efficient exploration of the design space.","authors":["Hansani Weeratunge","Dominic Robe","Elnaz Hajizadeh"],"url":"https://arxiv.org/abs/2505.06519"}
{"created":"2025-05-13","title":"PRUNE: A Patching Based Repair Framework for Certiffable Unlearning of Neural Networks","abstract":"It is often desirable to remove (a.k.a. unlearn) a speciffc part of the training data from a trained neural network model. A typical application scenario is to protect the data holder's right to be forgotten, which has been promoted by many recent regulation rules. Existing unlearning methods involve training alternative models with remaining data, which may be costly and challenging to verify from the data holder or a thirdparty auditor's perspective. In this work, we provide a new angle and propose a novel unlearning approach by imposing carefully crafted \"patch\" on the original neural network to achieve targeted \"forgetting\" of the requested data to delete. Speciffcally, inspired by the research line of neural network repair, we propose to strategically seek a lightweight minimum \"patch\" for unlearning a given data point with certiffable guarantee. Furthermore, to unlearn a considerable amount of data points (or an entire class), we propose to iteratively select a small subset of representative data points to unlearn, which achieves the effect of unlearning the whole set. Extensive experiments on multiple categorical datasets demonstrates our approach's effectiveness, achieving measurable unlearning while preserving the model's performance and being competitive in efffciency and memory consumption compared to various baseline methods.","authors":["Xuran Li","Jingyi Wang","Xiaohan Yuan","Peixin Zhang","Zhan Qin","Zhibo Wang","Kui Ren"],"url":"https://arxiv.org/abs/2505.06520"}
{"created":"2025-05-13","title":"Virtualized 3D Gaussians: Flexible Cluster-based Level-of-Detail System for Real-Time Rendering of Composed Scenes","abstract":"3D Gaussian Splatting (3DGS) enables the reconstruction of intricate digital 3D assets from multi-view images by leveraging a set of 3D Gaussian primitives for rendering. Its explicit and discrete representation facilitates the seamless composition of complex digital worlds, offering significant advantages over previous neural implicit methods. However, when applied to large-scale compositions, such as crowd-level scenes, it can encompass numerous 3D Gaussians, posing substantial challenges for real-time rendering. To address this, inspired by Unreal Engine 5's Nanite system, we propose Virtualized 3D Gaussians (V3DG), a cluster-based LOD solution that constructs hierarchical 3D Gaussian clusters and dynamically selects only the necessary ones to accelerate rendering speed. Our approach consists of two stages: (1) Offline Build, where hierarchical clusters are generated using a local splatting method to minimize visual differences across granularities, and (2) Online Selection, where footprint evaluation determines perceptible clusters for efficient rasterization during rendering. We curate a dataset of synthetic and real-world scenes, including objects, trees, people, and buildings, each requiring 0.1 billion 3D Gaussians to capture fine details. Experiments show that our solution balances rendering efficiency and visual quality across user-defined tolerances, facilitating downstream interactive applications that compose extensive 3DGS assets for consistent rendering performance.","authors":["Xijie Yang","Linning Xu","Lihan Jiang","Dahua Lin","Bo Dai"],"url":"https://arxiv.org/abs/2505.06523"}
{"created":"2025-05-13","title":"Causal Prompt Calibration Guided Segment Anything Model for Open-Vocabulary Multi-Entity Segmentation","abstract":"Despite the strength of the Segment Anything Model (SAM), it struggles with generalization issues in open-vocabulary multi-entity segmentation (OVMS). Through empirical and causal analyses, we find that (i) the prompt bias is the primary cause of the generalization issues; (ii) this bias is closely tied to the task-irrelevant generating factors within the prompts, which act as confounders and affect generalization. To address the generalization issues, we aim to propose a method that can calibrate prompts to eliminate confounders for accurate OVMS. Building upon the causal analysis, we propose that the optimal prompt for OVMS should contain only task-relevant causal factors. We define it as the causal prompt, serving as the goal of calibration. Next, our theoretical analysis, grounded by causal multi-distribution consistency theory, proves that this prompt can be obtained by enforcing segmentation consistency and optimality. Inspired by this, we propose CPC-SAM, a Causal Prompt Calibration method for SAM to achieve accurate OVMS. It integrates a lightweight causal prompt learner (CaPL) into SAM to obtain causal prompts. Specifically, we first generate multiple prompts using random annotations to simulate diverse distributions and then reweight them via CaPL by enforcing causal multi-distribution consistency in both task and entity levels. To ensure obtaining causal prompts, CaPL is optimized by minimizing the cumulative segmentation loss across the reweighted prompts to achieve consistency and optimality. A bi-level optimization strategy alternates between optimizing CaPL and SAM, ensuring accurate OVMS. Extensive experiments validate its superiority.","authors":["Jingyao Wang","Jianqi Zhang","Wenwen Qiang","Changwen Zheng"],"url":"https://arxiv.org/abs/2505.06524"}
{"created":"2025-05-13","title":"Improving Generalization of Medical Image Registration Foundation Model","abstract":"Deformable registration is a fundamental task in medical image processing, aiming to achieve precise alignment by establishing nonlinear correspondences between images. Traditional methods offer good adaptability and interpretability but are limited by computational efficiency. Although deep learning approaches have significantly improved registration speed and accuracy, they often lack flexibility and generalizability across different datasets and tasks. In recent years, foundation models have emerged as a promising direction, leveraging large and diverse datasets to learn universal features and transformation patterns for image registration, thus demonstrating strong cross-task transferability. However, these models still face challenges in generalization and robustness when encountering novel anatomical structures, varying imaging conditions, or unseen modalities. To address these limitations, this paper incorporates Sharpness-Aware Minimization (SAM) into foundation models to enhance their generalization and robustness in medical image registration. By optimizing the flatness of the loss landscape, SAM improves model stability across diverse data distributions and strengthens its ability to handle complex clinical scenarios. Experimental results show that foundation models integrated with SAM achieve significant improvements in cross-dataset registration performance, offering new insights for the advancement of medical image registration technology. Our code is available at https://github.com/Promise13/fm_sam}{https://github.com/Promise13/fm\\_sam.","authors":["Jing Hu","Kaiwei Yu","Hongjiang Xian","Shu Hu","Xin Wang"],"url":"https://arxiv.org/abs/2505.06527"}
{"created":"2025-05-13","title":"Unmasking Deep Fakes: Leveraging Deep Learning for Video Authenticity Detection","abstract":"Deepfake videos, produced through advanced artificial intelligence methods now a days, pose a new challenge to the truthfulness of the digital media. As Deepfake becomes more convincing day by day, detecting them requires advanced methods capable of identifying subtle inconsistencies. The primary motivation of this paper is to recognize deepfake videos using deep learning techniques, specifically by using convolutional neural networks. Deep learning excels in pattern recognition, hence, makes it an ideal approach for detecting the intricate manipulations in deepfakes. In this paper, we consider using MTCNN as a face detector and EfficientNet-B5 as encoder model to predict if a video is deepfake or not. We utilize training and evaluation dataset from Kaggle DFDC. The results shows that our deepfake detection model acquired 42.78% log loss, 93.80% AUC and 86.82% F1 score on kaggle's DFDC dataset.","authors":["Mahmudul Hasan"],"url":"https://arxiv.org/abs/2505.06528"}
{"created":"2025-05-13","title":"GBDTSVM: Combined Support Vector Machine and Gradient Boosting Decision Tree Framework for efficient snoRNA-disease association prediction","abstract":"Small nucleolar RNAs (snoRNAs) are increasingly recognized for their critical role in the pathogenesis and characterization of various human diseases. Consequently, the precise identification of snoRNA-disease associations (SDAs) is essential for the progression of diseases and the advancement of treatment strategies. However, conventional biological experimental approaches are costly, time-consuming, and resource-intensive; therefore, machine learning-based computational methods offer a promising solution to mitigate these limitations. This paper proposes a model called 'GBDTSVM', representing a novel and efficient machine learning approach for predicting snoRNA-disease associations by leveraging a Gradient Boosting Decision Tree (GBDT) and Support Vector Machine (SVM). 'GBDTSVM' effectively extracts integrated snoRNA-disease feature representations utilizing GBDT and SVM is subsequently utilized to classify and identify potential associations. Furthermore, the method enhances the accuracy of these predictions by incorporating Gaussian kernel profile similarity for both snoRNAs and diseases. Experimental evaluation of the GBDTSVM model demonstrated superior performance compared to state-of-the-art methods in the field, achieving an area under the receiver operating characteristic (AUROC) of 0.96 and an area under the precision-recall curve (AUPRC) of 0.95 on MDRF dataset. Moreover, our model shows superior performance on two more datasets named LSGT and PsnoD. Additionally, a case study on the predicted snoRNA-disease associations verified the top 10 predicted snoRNAs across nine prevalent diseases, further validating the efficacy of the GBDTSVM approach. These results underscore the model's potential as a robust tool for advancing snoRNA-related disease research. Source codes and datasets our proposed framework can be obtained from: https://github.com/mariamuna04/gbdtsvm","authors":["Ummay Maria Muna","Fahim Hafiz","Shanta Biswas","Riasat Azim"],"url":"https://arxiv.org/abs/2505.06534"}
{"created":"2025-05-13","title":"Online Feedback Efficient Active Target Discovery in Partially Observable Environments","abstract":"In various scientific and engineering domains, where data acquisition is costly, such as in medical imaging, environmental monitoring, or remote sensing, strategic sampling from unobserved regions, guided by prior observations, is essential to maximize target discovery within a limited sampling budget. In this work, we introduce Diffusion-guided Active Target Discovery (DiffATD), a novel method that leverages diffusion dynamics for active target discovery. DiffATD maintains a belief distribution over each unobserved state in the environment, using this distribution to dynamically balance exploration-exploitation. Exploration reduces uncertainty by sampling regions with the highest expected entropy, while exploitation targets areas with the highest likelihood of discovering the target, indicated by the belief distribution and an incrementally trained reward model designed to learn the characteristics of the target. DiffATD enables efficient target discovery in a partially observable environment within a fixed sampling budget, all without relying on any prior supervised training. Furthermore, DiffATD offers interpretability, unlike existing black-box policies that require extensive supervised training. Through extensive experiments and ablation studies across diverse domains, including medical imaging and remote sensing, we show that DiffATD performs significantly better than baselines and competitively with supervised methods that operate under full environmental observability.","authors":["Anindya Sarkar","Binglin Ji","Yevgeniy Vorobeychik"],"url":"https://arxiv.org/abs/2505.06535"}
{"created":"2025-05-13","title":"TACFN: Transformer-based Adaptive Cross-modal Fusion Network for Multimodal Emotion Recognition","abstract":"The fusion technique is the key to the multimodal emotion recognition task. Recently, cross-modal attention-based fusion methods have demonstrated high performance and strong robustness. However, cross-modal attention suffers from redundant features and does not capture complementary features well. We find that it is not necessary to use the entire information of one modality to reinforce the other during cross-modal interaction, and the features that can reinforce a modality may contain only a part of it. To this end, we design an innovative Transformer-based Adaptive Cross-modal Fusion Network (TACFN). Specifically, for the redundant features, we make one modality perform intra-modal feature selection through a self-attention mechanism, so that the selected features can adaptively and efficiently interact with another modality. To better capture the complementary information between the modalities, we obtain the fused weight vector by splicing and use the weight vector to achieve feature reinforcement of the modalities. We apply TCAFN to the RAVDESS and IEMOCAP datasets. For fair comparison, we use the same unimodal representations to validate the effectiveness of the proposed fusion method. The experimental results show that TACFN brings a significant performance improvement compared to other methods and reaches the state-of-the-art. All code and models could be accessed from https://github.com/shuzihuaiyu/TACFN.","authors":["Feng Liu","Ziwang Fu","Yunlong Wang","Qijian Zheng"],"url":"https://arxiv.org/abs/2505.06536"}
{"created":"2025-05-13","title":"ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images","abstract":"Fashion video generation aims to synthesize temporally consistent videos from reference images of a designated character. Despite significant progress, existing diffusion-based methods only support a single reference image as input, severely limiting their capability to generate view-consistent fashion videos, especially when there are different patterns on the clothes from different perspectives. Moreover, the widely adopted motion module does not sufficiently model human body movement, leading to sub-optimal spatiotemporal consistency. To address these issues, we propose ProFashion, a fashion video generation framework leveraging multiple reference images to achieve improved view consistency and temporal coherency. To effectively leverage features from multiple reference images while maintaining a reasonable computational cost, we devise a Pose-aware Prototype Aggregator, which selects and aggregates global and fine-grained reference features according to pose information to form frame-wise prototypes, which serve as guidance in the denoising process. To further enhance motion consistency, we introduce a Flow-enhanced Prototype Instantiator, which exploits the human keypoint motion flow to guide an extra spatiotemporal attention process in the denoiser. To demonstrate the effectiveness of ProFashion, we extensively evaluate our method on the MRFashion-7K dataset we collected from the Internet. ProFashion also outperforms previous methods on the UBC Fashion dataset.","authors":["Xianghao Kong","Qiaosong Qi","Yuanbin Wang","Anyi Rao","Biaolong Chen","Aixi Zhang","Si Liu","Hao Jiang"],"url":"https://arxiv.org/abs/2505.06537"}
{"created":"2025-05-13","title":"Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model","abstract":"The rapid development of multimodal large reasoning models (MLRMs) has demonstrated broad application potential, yet their safety and reliability remain critical concerns that require systematic exploration. To address this gap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs across 5 benchmarks and unveil prevalent safety degradation phenomena in most advanced models. Moreover, our analysis reveals distinct safety patterns across different benchmarks: significant safety degradation is observed across jailbreak robustness benchmarks, whereas safety-awareness benchmarks demonstrate less pronounced degradation. In particular, a long thought process in some scenarios even enhances safety performance. Therefore, it is a potential approach to addressing safety issues in MLRMs by leveraging the intrinsic reasoning capabilities of the model to detect unsafe intent. To operationalize this insight, we construct a multimodal tuning dataset that incorporates a safety-oriented thought process. Experimental results from fine-tuning existing MLRMs with this dataset effectively enhances the safety on both jailbreak robustness and safety-awareness benchmarks. This study provides a new perspective for developing safe MLRMs. Our dataset is available at https://github.com/xinyuelou/Think-in-Safety.","authors":["Xinyue Lou","You Li","Jinan Xu","Xiangyu Shi","Chi Chen","Kaiyu Huang"],"url":"https://arxiv.org/abs/2505.06538"}
{"created":"2025-05-13","title":"dcFCI: Robust Causal Discovery Under Latent Confounding, Unfaithfulness, and Mixed Data","abstract":"Causal discovery is central to inferring causal relationships from observational data. In the presence of latent confounding, algorithms such as Fast Causal Inference (FCI) learn a Partial Ancestral Graph (PAG) representing the true model's Markov Equivalence Class. However, their correctness critically depends on empirical faithfulness, the assumption that observed (in)dependencies perfectly reflect those of the underlying causal model, which often fails in practice due to limited sample sizes. To address this, we introduce the first nonparametric score to assess a PAG's compatibility with observed data, even with mixed variable types. This score is both necessary and sufficient to characterize structural uncertainty and distinguish between distinct PAGs. We then propose data-compatible FCI (dcFCI), the first hybrid causal discovery algorithm to jointly address latent confounding, empirical unfaithfulness, and mixed data types. dcFCI integrates our score into an (Anytime)FCI-guided search that systematically explores, ranks, and validates candidate PAGs. Experiments on synthetic and real-world scenarios demonstrate that dcFCI significantly outperforms state-of-the-art methods, often recovering the true PAG even in small and heterogeneous datasets. Examining top-ranked PAGs further provides valuable insights into structural uncertainty, supporting more robust and informed causal reasoning and decision-making.","authors":["Ad\\`ele H. Ribeiro","Dominik Heider"],"url":"https://arxiv.org/abs/2505.06542"}
{"created":"2025-05-13","title":"HDGlyph: A Hierarchical Disentangled Glyph-Based Framework for Long-Tail Text Rendering in Diffusion Models","abstract":"Visual text rendering, which aims to accurately integrate specified textual content within generated images, is critical for various applications such as commercial design. Despite recent advances, current methods struggle with long-tail text cases, particularly when handling unseen or small-sized text. In this work, we propose a novel Hierarchical Disentangled Glyph-Based framework (HDGlyph) that hierarchically decouples text generation from non-text visual synthesis, enabling joint optimization of both common and long-tail text rendering. At the training stage, HDGlyph disentangles pixel-level representations via the Multi-Linguistic GlyphNet and the Glyph-Aware Perceptual Loss, ensuring robust rendering even for unseen characters. At inference time, HDGlyph applies Noise-Disentangled Classifier-Free Guidance and Latent-Disentangled Two-Stage Rendering (LD-TSR) scheme, which refines both background and small-sized text. Extensive evaluations show our model consistently outperforms others, with 5.08% and 11.7% accuracy gains in English and Chinese text rendering while maintaining high image quality. It also excels in long-tail scenarios with strong accuracy and visual performance.","authors":["Shuhan Zhuang","Mengqi Huang","Fengyi Fu","Nan Chen","Bohan Lei","Zhendong Mao"],"url":"https://arxiv.org/abs/2505.06543"}
{"created":"2025-05-13","title":"Work in Progress: Middleware-Transparent Callback Enforcement in Commoditized Component-Oriented Real-time Systems","abstract":"Real-time scheduling in commoditized component-oriented real-time systems, such as ROS 2 systems on Linux, has been studied under nested scheduling: OS thread scheduling and middleware layer scheduling (e.g., ROS 2 Executor). However, by establishing a persistent one-to-one correspondence between callbacks and OS threads, we can ignore the middleware layer and directly apply OS scheduling parameters (e.g., scheduling policy, priority, and affinity) to individual callbacks. We propose a middleware model that enables this idea and implements CallbackIsolatedExecutor as a novel ROS 2 Executor. We demonstrate that the costs (user-kernel switches, context switches, and memory usage) of CallbackIsolatedExecutor remain lower than those of the MultiThreadedExecutor, regardless of the number of callbacks. Additionally, the cost of CallbackIsolatedExecutor relative to SingleThreadedExecutor stays within a fixed ratio (1.4x for inter-process and 5x for intra-process communication). Future ROS 2 real-time scheduling research can avoid nested scheduling, ignoring the existence of the middleware layer.","authors":["Takahiro Ishikawa-Aso","Atsushi Yano","Takuya Azumi","Shinpei Kato"],"url":"https://arxiv.org/abs/2505.06546"}
{"created":"2025-05-13","title":"REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback","abstract":"Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.","authors":["Aniruddha Roy","Pretam Ray","Abhilash Nandy","Somak Aditya","Pawan Goyal"],"url":"https://arxiv.org/abs/2505.06548"}
{"created":"2025-05-13","title":"Good Things Come in Pairs: Paired Autoencoders for Inverse Problems","abstract":"In this book chapter, we discuss recent advances in data-driven approaches for inverse problems. In particular, we focus on the \\emph{paired autoencoder} framework, which has proven to be a powerful tool for solving inverse problems in scientific computing. The paired autoencoder framework is a novel approach that leverages the strengths of both data-driven and model-based methods by projecting both the data and the quantity of interest into a latent space and mapping these latent spaces to provide surrogate forward and inverse mappings. We illustrate the advantages of this approach through numerical experiments, including seismic imaging and classical inpainting: nonlinear and linear inverse problems, respectively. Although the paired autoencoder framework is likelihood-free, it generates multiple data- and model-based reconstruction metrics that help assess whether examples are in or out of distribution. In addition to direct model estimates from data, the paired autoencoder enables latent-space refinement to fit the observed data accurately. Numerical experiments show that this procedure, combined with the latent-space initial guess, is essential for high-quality estimates, even when data noise exceeds the training regime. We also introduce two novel variants that combine variational and paired autoencoder ideas, maintaining the original benefits while enabling sampling for uncertainty analysis.","authors":["Matthias Chung","Bas Peters","Michael Solomon"],"url":"https://arxiv.org/abs/2505.06549"}
{"created":"2025-05-13","title":"References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation","abstract":"Conversational query reformulation (CQR) has become indispensable for improving retrieval in dialogue-based applications. However, existing approaches typically rely on reference passages for optimization, which are impractical to acquire in real-world scenarios. To address this limitation, we introduce a novel reference-free preference optimization framework DualReform that generates pseudo reference passages from commonly-encountered conversational datasets containing only queries and responses. DualReform attains this goal through two key innovations: (1) response-based inference, where responses serve as proxies to infer pseudo reference passages, and (2) response refinement via the dual-role of CQR, where a CQR model refines responses based on the shared objectives between response refinement and CQR. Despite not relying on reference passages, DualReform achieves 96.9--99.1% of the retrieval accuracy attainable only with reference passages and surpasses the state-of-the-art method by up to 31.6%.","authors":["Doyoung Kim","Youngjun Lee","Joeun Kim","Jihwan Bang","Hwanjun Song","Susik Yoon","Jae-Gil Lee"],"url":"https://arxiv.org/abs/2505.06552"}
{"created":"2025-05-13","title":"ActRef: Enhancing the Understanding of Python Code Refactoring with Action-Based Analysis","abstract":"Refactoring, the process of improving the code structure of a software system without altering its behavior, is crucial for managing code evolution in software development. Identifying refactoring actions in source code is essential for understanding software evolution and guiding developers in maintaining and improving the code quality. This study presents an action-based Refactoring Analysis Framework named ActRef, a novel algorithm designed to advance the detection and understanding of Python refactorings through a unique code change action-based analysis of code changes. ActRef mining multiple refactoring types (e.g., move, rename, extract, and inline operations) based on diff actions, covering multiple granularity levels including variable, method, class, and module levels. By focusing on the code change actions, ActRef provides a Python-adaptive solution to detect intricate refactoring patterns. Our evaluation, conducted on 1,914 manually validated refactoring instances from 136 open-source Python projects. The evaluation results show that ActRef achieves high precision(0.80) and recall(0.92), effectively identifying multiple refactoring types. Compared with leading baselines, including PyRef, PyRef with MLRefScanner, DeepSeek-R1 and ChatGPT-4, ActRef consistently demonstrates superior performance in detecting Python refactorings across various types. While matching PyRef in runtime efficiency, ActRef supports a broader spectrum of refactoring types and more refactoring mining levels. ActRef shows an effective and scalable approach for mining refactorings in dynamic Python codebases and introduces a new perspective on understanding code.","authors":["Siqi Wang","Xing Hu","Xin Xia","Xinyu Wang"],"url":"https://arxiv.org/abs/2505.06553"}
{"created":"2025-05-13","title":"TierBase: A Workload-Driven Cost-Optimized Key-Value Store","abstract":"In the current era of data-intensive applications, the demand for high-performance, cost-effective storage solutions is paramount. This paper introduces a Space-Performance Cost Model for key-value store, designed to guide cost-effective storage configuration decisions. The model quantifies the trade-offs between performance and storage costs, providing a framework for optimizing resource allocation in large-scale data serving environments. Guided by this cost model, we present TierBase, a distributed key-value store developed by Ant Group that optimizes total cost by strategically synchronizing data between cache and storage tiers, maximizing resource utilization and effectively handling skewed workloads. To enhance cost-efficiency, TierBase incorporates several optimization techniques, including pre-trained data compression, elastic threading mechanisms, and the utilization of persistent memory. We detail TierBase's architecture, key components, and the implementation of cost optimization strategies. Extensive evaluations using both synthetic benchmarks and real-world workloads demonstrate TierBase's superior cost-effectiveness compared to existing solutions. Furthermore, case studies from Ant Group's production environments showcase TierBase's ability to achieve up to 62% cost reduction in primary scenarios, highlighting its practical impact in large-scale online data serving.","authors":["Zhitao Shen","Shiyu Yang","Weibo Chen","Kunming Wang","Yue Li","Jiabao Jin","Wei Jia","Junwei Chen","Yuan Su","Xiaoxia Duan","Wei Chen","Lei Wang","Jie Song","Ruoyi Ruan","Xuemin Lin"],"url":"https://arxiv.org/abs/2505.06556"}
{"created":"2025-05-13","title":"Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining","abstract":"The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.","authors":["Lu Dong","Haiyu Zhang","Hongjie Zhang","Yifei Huang","Zhen-Hua Ling","Yu Qiao","Limin Wang","Yali Wang"],"url":"https://arxiv.org/abs/2505.06557"}
{"created":"2025-05-13","title":"Data Version Management and Machine-Actionable Reproducibility for HPC based on git and DataLad","abstract":"We present the adaptation of an existing data versioning and machine-actionable reproducibility solution for HPC. Both aspects are important for research data management and the DataLad tool provides both based on the very prevalent git version control system. However, it is incompatible with HPC batch processing.","authors":["Andreas Kn\\\"upfer","Timothy J. Callow"],"url":"https://arxiv.org/abs/2505.06558"}
{"created":"2025-05-13","title":"Quadrupedal Robot Skateboard Mounting via Reverse Curriculum Learning","abstract":"The aim of this work is to enable quadrupedal robots to mount skateboards using Reverse Curriculum Reinforcement Learning. Although prior work has demonstrated skateboarding for quadrupeds that are already positioned on the board, the initial mounting phase still poses a significant challenge. A goal-oriented methodology was adopted, beginning with the terminal phases of the task and progressively increasing the complexity of the problem definition to approximate the desired objective. The learning process was initiated with the skateboard rigidly fixed within the global coordinate frame and the robot positioned directly above it. Through gradual relaxation of these initial conditions, the learned policy demonstrated robustness to variations in skateboard position and orientation, ultimately exhibiting a successful transfer to scenarios involving a mobile skateboard. The code, trained models, and reproducible examples are available at the following link: https://github.com/dancher00/quadruped-skateboard-mounting","authors":["Danil Belov","Artem Erkhov","Elizaveta Pestova","Ilya Osokin","Dzmitry Tsetserukou","Pavel Osinenko"],"url":"https://arxiv.org/abs/2505.06561"}
{"created":"2025-05-13","title":"The Malaysian Election Corpus (MECo): Federal and State-Level Election Results from 1955 to 2025","abstract":"Empirical research and public knowledge on Malaysia's elections have long been constrained by a lack of high-quality open data, particularly in the absence of a Freedom of Information framework. We introduce the Malaysian Election Corpus (MECo; ElectionData.MY), an open-access panel database covering all federal and state general elections from 1955 to the present, as well as by-elections from 2008 onward. MECo includes candidate- and constituency-level results for nearly 10,000 contests across seven decades, standardised with unique identifiers for candidates, parties, and constituencies. The database also provides summary statistics on electorate size, voter turnout, rejected votes, and unreturned ballots. This is the most well-curated publicly available data on Malaysian elections, and will unlock new opportunities for research, data journalism, and civic engagement.","authors":["Thevesh Thevananthan"],"url":"https://arxiv.org/abs/2505.06564"}
{"created":"2025-05-13","title":"A novel class of arbitrary high-order numerical schemes for fractional differential equations","abstract":"A novel efficient and high accuracy numerical method for the time-fractional differential equations (TFDEs) is proposed in this work. We show the equivalence between TFDEs and the integer-order extended parametric differential equations (EPDE) by dimensional expanding, and establish the stability of EPDE. We apply BDF-$k$ formula for the temporal discretization, while we use the Jacobi spectral collocation method for the discretization of the extended direction. We analyze the stability of the proposed method and give rigorous error estimates with order $O(\\Delta t^{k} + M^{-m})$, where $\\Delta t$ and $M$ are time step size and number of collocation nodes in extended direction, respectively. Also, we point out that the computational cost and the storage requirement is essentially the same as the integer problems, namely, the computational cost and the storage of the present algorithm are $O(N)$ and $O(1)$, respectively, where $N$ is the total number of time step. We present several numerical examples, including both linear and nonlinear problems, to demonstrate the effectiveness of the proposed method and to validate the theoretical results","authors":["Peng Ding","Zhiping Mao"],"url":"https://arxiv.org/abs/2505.06565"}
{"created":"2025-05-13","title":"Dynamic Uncertainty Learning with Noisy Correspondence for Text-Based Person Search","abstract":"Text-to-image person search aims to identify an individual based on a text description. To reduce data collection costs, large-scale text-image datasets are created from co-occurrence pairs found online. However, this can introduce noise, particularly mismatched pairs, which degrade retrieval performance. Existing methods often focus on negative samples, amplifying this noise. To address these issues, we propose the Dynamic Uncertainty and Relational Alignment (DURA) framework, which includes the Key Feature Selector (KFS) and a new loss function, Dynamic Softmax Hinge Loss (DSH-Loss). KFS captures and models noise uncertainty, improving retrieval reliability. The bidirectional evidence from cross-modal similarity is modeled as a Dirichlet distribution, enhancing adaptability to noisy data. DSH adjusts the difficulty of negative samples to improve robustness in noisy environments. Our experiments on three datasets show that the method offers strong noise resistance and improves retrieval performance in both low- and high-noise scenarios.","authors":["Zequn Xie","Haoming Ji","Lingwei Meng"],"url":"https://arxiv.org/abs/2505.06566"}
{"created":"2025-05-13","title":"MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG","abstract":"Long-context (LC) Large Language Models (LLMs) combined with Retrieval-Augmented Generation (RAG) hold strong potential for complex multi-hop and large-document tasks. However, existing RAG systems often suffer from imprecise retrieval, incomplete context coverage under constrained context windows, and fragmented information caused by suboptimal context construction. We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical retrieval framework that compresses and partitions documents into coarse-to-fine granularities, then adaptively merges relevant contexts through chunk- and document-level expansions in real time. By starting from the finest-level retrieval and progressively incorporating higher-level and broader context, MacRAG constructs effective query-specific long contexts, optimizing both precision and coverage. Evaluations on the challenging LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG consistently surpasses baseline RAG pipelines on single- and multi-step generation with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient, scalable solution for real-world long-context, multi-hop reasoning. Our code is available at https://github.com/Leezekun/MacRAG.","authors":["Woosang Lim","Zekun Li","Gyuwan Kim","Sungyoung Ji","HyeonJung Kim","Kyuri Choi","Jin Hyuk Lim","Kyungpyo Park","William Yang Wang"],"url":"https://arxiv.org/abs/2505.06569"}
{"created":"2025-05-13","title":"ElectricSight: 3D Hazard Monitoring for Power Lines Using Low-Cost Sensors","abstract":"Protecting power transmission lines from potential hazards involves critical tasks, one of which is the accurate measurement of distances between power lines and potential threats, such as large cranes. The challenge with this task is that the current sensor-based methods face challenges in balancing accuracy and cost in distance measurement. A common practice is to install cameras on transmission towers, which, however, struggle to measure true 3D distances due to the lack of depth information. Although 3D lasers can provide accurate depth data, their high cost makes large-scale deployment impractical.","authors":["Xingchen Li","LiDian Wang","Yu Sheng","ZhiPeng Tang","Haojie Ren","Guoliang You","YiFan Duan","Jianmin Ji","Yanyong Zhang"],"url":"https://arxiv.org/abs/2505.06573"}
{"created":"2025-05-13","title":"GRACE: Estimating Geometry-level 3D Human-Scene Contact from 2D Images","abstract":"Estimating the geometry level of human-scene contact aims to ground specific contact surface points at 3D human geometries, which provides a spatial prior and bridges the interaction between human and scene, supporting applications such as human behavior analysis, embodied AI, and AR/VR. To complete the task, existing approaches predominantly rely on parametric human models (e.g., SMPL), which establish correspondences between images and contact regions through fixed SMPL vertex sequences. This actually completes the mapping from image features to an ordered sequence. However, this approach lacks consideration of geometry, limiting its generalizability in distinct human geometries. In this paper, we introduce GRACE (Geometry-level Reasoning for 3D Human-scene Contact Estimation), a new paradigm for 3D human contact estimation. GRACE incorporates a point cloud encoder-decoder architecture along with a hierarchical feature extraction and fusion module, enabling the effective integration of 3D human geometric structures with 2D interaction semantics derived from images. Guided by visual cues, GRACE establishes an implicit mapping from geometric features to the vertex space of the 3D human mesh, thereby achieving accurate modeling of contact regions. This design ensures high prediction accuracy and endows the framework with strong generalization capability across diverse human geometries. Extensive experiments on multiple benchmark datasets demonstrate that GRACE achieves state-of-the-art performance in contact estimation, with additional results further validating its robust generalization to unstructured human point clouds.","authors":["Chengfeng Wang","Wei Zhai","Yuhang Yang","Yang Cao","Zhengjun Zha"],"url":"https://arxiv.org/abs/2505.06575"}
{"created":"2025-05-13","title":"Two-Stage Random Alternation Framework for Zero-Shot Pansharpening","abstract":"In recent years, pansharpening has seen rapid advancements with deep learning methods, which have demonstrated impressive fusion quality. However, the challenge of acquiring real high-resolution images limits the practical applicability of these methods. To address this, we propose a two-stage random alternating framework (TRA-PAN) that effectively integrates strong supervision constraints from reduced-resolution images with the physical characteristics of full-resolution images. The first stage introduces a pre-training procedure, which includes Degradation-Aware Modeling (DAM) to capture spatial-spectral degradation mappings, alongside a warm-up procedure designed to reduce training time and mitigate the negative effects of reduced-resolution data. In the second stage, Random Alternation Optimization (RAO) is employed, where random alternating training leverages the strengths of both reduced- and full-resolution images, further optimizing the fusion model. By primarily relying on full-resolution images, our method enables zero-shot training with just a single image pair, obviating the need for large datasets. Experimental results demonstrate that TRA-PAN outperforms state-of-the-art (SOTA) methods in both quantitative metrics and visual quality in real-world scenarios, highlighting its strong practical applicability.","authors":["Haorui Chen","Zeyu Ren","Jiaxuan Ren","Ran Ran","Jinliang Shao","Jie Huang","Liangjian Deng"],"url":"https://arxiv.org/abs/2505.06576"}
{"created":"2025-05-13","title":"Compact and Efficient Neural Networks for Image Recognition Based on Learned 2D Separable Transform","abstract":"The paper presents a learned two-dimensional separable transform (LST) that can be considered as a new type of computational layer for constructing neural network (NN) architecture for image recognition tasks. The LST based on the idea of sharing the weights of one fullyconnected (FC) layer to process all rows of an image. After that, a second shared FC layer is used to process all columns of image representation obtained from the first layer. The use of LST layers in a NN architecture significantly reduces the number of model parameters compared to models that use stacked FC layers. We show that a NN-classifier based on a single LST layer followed by an FC layer achieves 98.02\\% accuracy on the MNIST dataset, while having only 9.5k parameters. We also implemented a LST-based classifier for handwritten digit recognition on the FPGA platform to demonstrate the efficiency of the suggested approach for designing a compact and high-performance implementation of NN models. Git repository with supplementary materials: https://github.com/Mak-Sim/LST-2d","authors":["Maxim Vashkevich","Egor Krivalcevich"],"url":"https://arxiv.org/abs/2505.06578"}
{"created":"2025-05-13","title":"POISONCRAFT: Practical Poisoning of Retrieval-Augmented Generation for Large Language Models","abstract":"Large language models (LLMs) have achieved remarkable success in various domains, primarily due to their strong capabilities in reasoning and generating human-like text. Despite their impressive performance, LLMs are susceptible to hallucinations, which can lead to incorrect or misleading outputs. This is primarily due to the lack of up-to-date knowledge or domain-specific information. Retrieval-augmented generation (RAG) is a promising approach to mitigate hallucinations by leveraging external knowledge sources. However, the security of RAG systems has not been thoroughly studied. In this paper, we study a poisoning attack on RAG systems named POISONCRAFT, which can mislead the model to refer to fraudulent websites. Compared to existing poisoning attacks on RAG systems, our attack is more practical as it does not require access to the target user query's info or edit the user query. It not only ensures that injected texts can be retrieved by the model, but also ensures that the LLM will be misled to refer to the injected texts in its response. We demonstrate the effectiveness of POISONCRAFTacross different datasets, retrievers, and language models in RAG pipelines, and show that it remains effective when transferred across retrievers, including black-box systems. Moreover, we present a case study revealing how the attack influences both the retrieval behavior and the step-by-step reasoning trace within the generation model, and further evaluate the robustness of POISONCRAFTunder multiple defense mechanisms. These results validate the practicality of our threat model and highlight a critical security risk for RAG systems deployed in real-world applications. We release our code\\footnote{https://github.com/AndyShaw01/PoisonCraft} to support future research on the security and robustness of RAG systems in real-world settings.","authors":["Yangguang Shao","Xinjie Lin","Haozheng Luo","Chengshang Hou","Gang Xiong","Jiahao Yu","Junzheng Shi"],"url":"https://arxiv.org/abs/2505.06579"}
{"created":"2025-05-13","title":"TAROT: Towards Essentially Domain-Invariant Robustness with Theoretical Justification","abstract":"Robust domain adaptation against adversarial attacks is a critical research area that aims to develop models capable of maintaining consistent performance across diverse and challenging domains. In this paper, we derive a new generalization bound for robust risk on the target domain using a novel divergence measure specifically designed for robust domain adaptation. Building upon this, we propose a new algorithm named TAROT, which is designed to enhance both domain adaptability and robustness. Through extensive experiments, TAROT not only surpasses state-of-the-art methods in accuracy and robustness but also significantly enhances domain generalization and scalability by effectively learning domain-invariant features. In particular, TAROT achieves superior performance on the challenging DomainNet dataset, demonstrating its ability to learn domain-invariant representations that generalize well across different domains, including unseen ones. These results highlight the broader applicability of our approach in real-world domain adaptation scenarios.","authors":["Dongyoon Yang","Jihu Lee","Yongdai Kim"],"url":"https://arxiv.org/abs/2505.06580"}
{"created":"2025-05-13","title":"An \\tilde{O}ptimal Differentially Private Learner for Concept Classes with VC Dimension 1","abstract":"We present the first nearly optimal differentially private PAC learner for any concept class with VC dimension 1 and Littlestone dimension $d$. Our algorithm achieves the sample complexity of $\\tilde{O}_{\\varepsilon,\\delta,\\alpha,\\delta}(\\log^* d)$, nearly matching the lower bound of $\\Omega(\\log^* d)$ proved by Alon et al. [STOC19]. Prior to our work, the best known upper bound is $\\tilde{O}(VC\\cdot d^5)$ for general VC classes, as shown by Ghazi et al. [STOC21].","authors":["Chao Yan"],"url":"https://arxiv.org/abs/2505.06581"}
{"created":"2025-05-13","title":"Gaussian Wave Splatting for Computer-Generated Holography","abstract":"State-of-the-art neural rendering methods optimize Gaussian scene representations from a few photographs for novel-view synthesis. Building on these representations, we develop an efficient algorithm, dubbed Gaussian Wave Splatting, to turn these Gaussians into holograms. Unlike existing computer-generated holography (CGH) algorithms, Gaussian Wave Splatting supports accurate occlusions and view-dependent effects for photorealistic scenes by leveraging recent advances in neural rendering. Specifically, we derive a closed-form solution for a 2D Gaussian-to-hologram transform that supports occlusions and alpha blending. Inspired by classic computer graphics techniques, we also derive an efficient approximation of the aforementioned process in the Fourier domain that is easily parallelizable and implement it using custom CUDA kernels. By integrating emerging neural rendering pipelines with holographic display technology, our Gaussian-based CGH framework paves the way for next-generation holographic displays.","authors":["Suyeon Choi","Brian Chao","Jacqueline Yang","Manu Gopakumar","Gordon Wetzstein"],"url":"https://arxiv.org/abs/2505.06582"}
{"created":"2025-05-13","title":"JAEGER: Dual-Level Humanoid Whole-Body Controller","abstract":"This paper presents JAEGER, a dual-level whole-body controller for humanoid robots that addresses the challenges of training a more robust and versatile policy. Unlike traditional single-controller approaches, JAEGER separates the control of the upper and lower bodies into two independent controllers, so that they can better focus on their distinct tasks. This separation alleviates the dimensionality curse and improves fault tolerance. JAEGER supports both root velocity tracking (coarse-grained control) and local joint angle tracking (fine-grained control), enabling versatile and stable movements. To train the controller, we utilize a human motion dataset (AMASS), retargeting human poses to humanoid poses through an efficient retargeting network, and employ a curriculum learning approach. This method performs supervised learning for initialization, followed by reinforcement learning for further exploration. We conduct our experiments on two humanoid platforms and demonstrate the superiority of our approach against state-of-the-art methods in both simulation and real environments.","authors":["Ziluo Ding","Haobin Jiang","Yuxuan Wang","Zhenguo Sun","Yu Zhang","Xiaojie Niu","Ming Yang","Weishuai Zeng","Xinrun Xu","Zongqing Lu"],"url":"https://arxiv.org/abs/2505.06584"}
{"created":"2025-05-13","title":"Emergent Multi-View Fidelity in Autonomous UAV Swarm Sport Injury Detection","abstract":"Accurate, real-time collision detection is essential for ensuring player safety and effective refereeing in high-contact sports such as rugby, particularly given the severe risks associated with traumatic brain injuries (TBI). Traditional collision-monitoring methods employing fixed cameras or wearable sensors face limitations in visibility, coverage, and responsiveness. Previously, we introduced a framework using unmanned aerial vehicles (UAVs) for monitoring and real time kinematics extraction from videos of collision events. In this paper, we show that the strategies operating on the objective of ensuring at least one UAV captures every incident on the pitch have an emergent property of fulfilling a stronger key condition for successful kinematics extraction. Namely, they ensure that almost all collisions are captured by multiple drones, establishing multi-view fidelity and redundancy, while not requiring any drone-to-drone communication.","authors":["Yu Cheng","Harun \\v{S}iljak"],"url":"https://arxiv.org/abs/2505.06588"}
{"created":"2025-05-13","title":"Evaluating LLM-Generated Q&A Test: a Student-Centered Study","abstract":"This research prepares an automatic pipeline for generating reliable question-answer (Q&amp;A) tests using AI chatbots. We automatically generated a GPT-4o-mini-based Q&amp;A test for a Natural Language Processing course and evaluated its psychometric and perceived-quality metrics with students and experts. A mixed-format IRT analysis showed that the generated items exhibit strong discrimination and appropriate difficulty, while student and expert star ratings reflect high overall quality. A uniform DIF check identified two items for review. These findings demonstrate that LLM-generated assessments can match human-authored tests in psychometric performance and user satisfaction, illustrating a scalable approach to AI-assisted assessment development.","authors":["Anna Wr\\'oblewska","Bartosz Grabek","Jakub \\'Swistak","Daniel Dan"],"url":"https://arxiv.org/abs/2505.06591"}
{"created":"2025-05-13","title":"Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning","abstract":"This paper proposes batch augmentation with unimodal fine-tuning to detect the fetus's organs from ultrasound images and associated clinical textual information. We also prescribe pre-training initial layers with investigated medical data before the multimodal training. At first, we apply a transferred initialization with the unimodal image portion of the dataset with batch augmentation. This step adjusts the initial layer weights for medical data. Then, we apply neural networks (NNs) with fine-tuned initial layers to images in batches with batch augmentation to obtain features. We also extract information from descriptions of images. We combine this information with features obtained from images to train the head layer. We write a dataloader script to load the multimodal data and use existing unimodal image augmentation techniques with batch augmentation for the multimodal data. The dataloader brings a new random augmentation for each batch to get a good generalization. We investigate the FPU23 ultrasound and UPMC Food-101 multimodal datasets. The multimodal large language model (LLM) with the proposed training provides the best results among the investigated methods. We receive near state-of-the-art (SOTA) performance on the UPMC Food-101 dataset. We share the scripts of the proposed method with traditional counterparts at the following repository: github.com/dipuk0506/multimodal","authors":["H M Dipu Kabir","Subrota Kumar Mondal","Mohammad Ali Moni"],"url":"https://arxiv.org/abs/2505.06592"}
{"created":"2025-05-13","title":"Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation","abstract":"Vision-Language Models (VLMs) often struggle to balance visual and textual information when summarizing complex multimodal inputs, such as entire TV show episodes. In this paper, we propose a zero-shot video-to-text summarization approach that builds its own screenplay representation of an episode, effectively integrating key video moments, dialogue, and character information into a unified document. Unlike previous approaches, we simultaneously generate screenplays and name the characters in zero-shot, using only the audio, video, and transcripts as input. Additionally, we highlight that existing summarization metrics can fail to assess the multimodal content in summaries. To address this, we introduce MFactSum, a multimodal metric that evaluates summaries with respect to both vision and text modalities. Using MFactSum, we evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating superiority against state-of-the-art VLMs such as Gemini 1.5 by generating summaries containing 20% more relevant visual information while requiring 75% less of the video as input.","authors":["Galann Pennec","Zhengyuan Liu","Nicholas Asher","Philippe Muller","Nancy F. Chen"],"url":"https://arxiv.org/abs/2505.06594"}
{"created":"2025-05-13","title":"Deterministic Self-Stabilizing BFS Construction in Constant Space","abstract":"In this paper, we resolve a long-standing question in self-stabilization by demonstrating that it is indeed possible to construct a spanning tree in a semi-uniform network using constant memory per node. We introduce a self-stabilizing synchronous algorithm that builds a breadth-first search (BFS) spanning tree with only $O(1)$ bits of memory per node, converging in $2^\\epsilon$ time units, where $\\epsilon$ denotes the eccentricity of the distinguish node. Crucially, our approach operates without any prior knowledge of global network parameters such as maximum degree, diameter, or total node count. In contrast to traditional self-stabilizing methods, such as pointer-to-neighbor communication or distance-to-root computation, that are unsuitable under strict memory constraints, our solution employs an innovative constant-space token dissemination mechanism. This mechanism effectively eliminates cycles and rectifies deviations in the BFS structure, ensuring both correctness and memory efficiency. The proposed algorithm not only meets the stringent requirements of memory-constrained distributed systems but also opens new avenues for research in self-stabilizing protocols under severe resource limitations.","authors":["L\\'elia Blin","Franck Petit","S\\'ebastien Tixeuil"],"url":"https://arxiv.org/abs/2505.06596"}
{"created":"2025-05-13","title":"Geometry of Learning -- L2 Phase Transitions in Deep and Shallow Neural Networks","abstract":"When neural networks (NNs) are subject to L2 regularization, increasing the regularization strength beyond a certain threshold pushes the model into an under-parameterization regime. This transition manifests as a first-order phase transition in single-hidden-layer NNs and a second-order phase transition in NNs with two or more hidden layers. This paper establishes a unified framework for such transitions by integrating the Ricci curvature of the loss landscape with regularizer-driven deep learning. First, we show that a curvature change-point separates the model-accuracy regimes in the onset of learning and that it is identical to the critical point of the phase transition driven by regularization. Second, we show that for more complex data sets additional phase transitions exist between model accuracies, and that they are again identical to curvature change points in the error landscape. Third, by studying the MNIST data set using a Variational Autoencoder, we demonstrate that the curvature change points identify phase transitions in model accuracy outside the L2 setting. Our framework also offers practical insights for optimizing model performance across various architectures and datasets. By linking geometric features of the error landscape to observable phase transitions, our work paves the way for more informed regularization strategies and potentially new methods for probing the intrinsic structure of neural networks beyond the L2 context.","authors":["Ibrahim Talha Ersoy","Karoline Wiesner"],"url":"https://arxiv.org/abs/2505.06597"}
{"created":"2025-05-13","title":"Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation","abstract":"Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges due to its complex phonological features, particularly homographs and Ezafe, which exist in formal and informal language contexts. This paper introduces an intermediate language specifically designed for Persian language processing that addresses these challenges through a multi-faceted approach. Our methodology combines two key components: Large Language Model (LLM) prompting techniques and a specialized sequence-to-sequence machine transliteration architecture. We developed and implemented a systematic approach for constructing a comprehensive lexical database for homographs with multiple pronunciations disambiguation often termed polyphones, utilizing formal concept analysis for semantic differentiation. We train our model using two distinct datasets: the LLM-generated dataset for formal and informal Persian and the B-Plus podcasts for informal language variants. The experimental results demonstrate superior performance compared to existing state-of-the-art approaches, particularly in handling the complexities of Persian phoneme conversion. Our model significantly improves Phoneme Error Rate (PER) metrics, establishing a new benchmark for Persian G2P conversion accuracy. This work contributes to the growing research in low-resource language processing and provides a robust solution for Persian text-to-speech systems and demonstrating its applicability beyond Persian. Specifically, the approach can extend to languages with rich homographic phenomena such as Chinese and Arabic","authors":["Abbas Bertina","Shahab Beirami","Hossein Biniazian","Elham Esmaeilnia","Soheil Shahi","Mahdi Pirnia"],"url":"https://arxiv.org/abs/2505.06599"}
{"created":"2025-05-13","title":"ReplayCAD: Generative Diffusion Replay for Continual Anomaly Detection","abstract":"Continual Anomaly Detection (CAD) enables anomaly detection models in learning new classes while preserving knowledge of historical classes. CAD faces two key challenges: catastrophic forgetting and segmentation of small anomalous regions. Existing CAD methods store image distributions or patch features to mitigate catastrophic forgetting, but they fail to preserve pixel-level detailed features for accurate segmentation. To overcome this limitation, we propose ReplayCAD, a novel diffusion-driven generative replay framework that replay high-quality historical data, thus effectively preserving pixel-level detailed features. Specifically, we compress historical data by searching for a class semantic embedding in the conditional space of the pre-trained diffusion model, which can guide the model to replay data with fine-grained pixel details, thus improving the segmentation performance. However, relying solely on semantic features results in limited spatial diversity. Hence, we further use spatial features to guide data compression, achieving precise control of sample space, thereby generating more diverse data. Our method achieves state-of-the-art performance in both classification and segmentation, with notable improvements in segmentation: 11.5% on VisA and 8.1% on MVTec. Our source code is available at https://github.com/HULEI7/ReplayCAD.","authors":["Lei Hu","Zhiyong Gan","Ling Deng","Jinglin Liang","Lingyu Liang","Shuangping Huang","Tianshui Chen"],"url":"https://arxiv.org/abs/2505.06603"}
{"created":"2025-05-13","title":"Using External knowledge to Enhanced PLM for Semantic Matching","abstract":"Modeling semantic relevance has always been a challenging and critical task in natural language processing. In recent years, with the emergence of massive amounts of annotated data, it has become feasible to train complex models, such as neural network-based reasoning models. These models have shown excellent performance in practical applications and have achieved the current state-ofthe-art performance. However, even with such large-scale annotated data, we still need to think: Can machines learn all the knowledge necessary to perform semantic relevance detection tasks based on this data alone? If not, how can neural network-based models incorporate external knowledge into themselves, and how can relevance detection models be constructed to make full use of external knowledge? In this paper, we use external knowledge to enhance the pre-trained semantic relevance discrimination model. Experimental results on 10 public datasets show that our method achieves consistent improvements in performance compared to the baseline model.","authors":["Min Li","Chun Yuan"],"url":"https://arxiv.org/abs/2505.06605"}
{"created":"2025-05-13","title":"Boosting Neural Language Inference via Cascaded Interactive Reasoning","abstract":"Natural Language Inference (NLI) focuses on ascertaining the logical relationship (entailment, contradiction, or neutral) between a given premise and hypothesis. This task presents significant challenges due to inherent linguistic features such as diverse phrasing, semantic complexity, and contextual nuances. While Pre-trained Language Models (PLMs) built upon the Transformer architecture have yielded substantial advancements in NLI, prevailing methods predominantly utilize representations from the terminal layer. This reliance on final-layer outputs may overlook valuable information encoded in intermediate layers, potentially limiting the capacity to model intricate semantic interactions effectively. Addressing this gap, we introduce the Cascaded Interactive Reasoning Network (CIRN), a novel architecture designed for deeper semantic comprehension in NLI. CIRN implements a hierarchical feature extraction strategy across multiple network depths, operating within an interactive space where cross-sentence information is continuously integrated. This mechanism aims to mimic a process of progressive reasoning, transitioning from surface-level feature matching to uncovering more profound logical and semantic connections between the premise and hypothesis. By systematically mining latent semantic relationships at various representational levels, CIRN facilitates a more thorough understanding of the input pair. Comprehensive evaluations conducted on several standard NLI benchmark datasets reveal consistent performance gains achieved by CIRN over competitive baseline approaches, demonstrating the efficacy of leveraging multi-level interactive features for complex relational reasoning.","authors":["Min Li","Chun Yuan"],"url":"https://arxiv.org/abs/2505.06607"}
{"created":"2025-05-13","title":"Burger: Robust Graph Denoising-augmentation Fusion and Multi-semantic Modeling in Social Recommendation","abstract":"In the era of rapid development of social media, social recommendation systems as hybrid recommendation systems have been widely applied. Existing methods capture interest similarity between users to filter out interest-irrelevant relations in social networks that inevitably decrease recommendation accuracy, however, limited research has a focus on the mutual influence of semantic information between the social network and the user-item interaction network for further improving social recommendation. To address these issues, we introduce a social \\underline{r}ecommendation model with ro\\underline{bu}st g\\underline{r}aph denoisin\\underline{g}-augmentation fusion and multi-s\\underline{e}mantic Modeling(Burger). Specifically, we firstly propose to construct a social tensor in order to smooth the training process of the model. Then, a graph convolutional network and a tensor convolutional network are employed to capture user's item preference and social preference, respectively. Considering the different semantic information in the user-item interaction network and the social network, a bi-semantic coordination loss is proposed to model the mutual influence of semantic information. To alleviate the interference of interest-irrelevant relations on multi-semantic modeling, we further use Bayesian posterior probability to mine potential social relations to replace social noise. Finally, the sliding window mechanism is utilized to update the social tensor as the input for the next iteration. Extensive experiments on three real datasets show Burger has a superior performance compared with the state-of-the-art models.","authors":["Yuqin Lan"],"url":"https://arxiv.org/abs/2505.06612"}
{"created":"2025-05-13","title":"Adversarial Coevolutionary Illumination with Generational Adversarial MAP-Elites","abstract":"Unlike traditional optimization algorithms focusing on finding a single optimal solution, Quality-Diversity (QD) algorithms illuminate a search space by finding high-performing solutions that cover a specified behavior space. However, tackling adversarial problems is more challenging due to the behavioral interdependence between opposing sides. Most applications of QD algorithms to these problems evolve only one side, thus reducing illumination coverage. In this paper, we propose a new QD algorithm, Generational Adversarial MAP-Elites (GAME), which coevolves solutions by alternating sides through a sequence of generations. Combining GAME with vision embedding models enables the algorithm to directly work from videos of behaviors instead of handcrafted descriptors. Some key findings are that (1) emerging evolutionary dynamics sometimes resemble an arms race, (2) starting each generation from scratch increases open-endedness, and (3) keeping neutral mutations preserves stepping stones that seem necessary to reach the highest performance. In conclusion, the results demonstrate that GAME can successfully illuminate an adversarial multi-agent game, opening up interesting future directions in understanding the emergence of open-ended coevolution.","authors":["Timoth\\'ee Anne","Noah Syrkis","Meriem Elhosni","Florian Turati","Franck Legendre","Alain Jaquier","Sebastian Risi"],"url":"https://arxiv.org/abs/2505.06617"}
{"created":"2025-05-13","title":"Varieties of Distributed Knowledge","abstract":"Distributed knowledge is one of the better known group knowledge modalities. While its intuitive idea is relatively clear, there is ample room for interpretation of details. We investigate 12 definitions of distributed knowledge that differ from each other in the kinds of information sharing the agents can perform in order to achieve shared mutual knowledge of a proposition. We then show which kinds of distributed knowledge are equivalent, and which kinds imply each other, i.e., for any two variants $\\tau_1$ and $\\tau_2$ of distributed knowledge we show whether a proposition $\\phi$ being distributed knowledge under definition $\\tau_1$ implies that $\\phi$ is distributed knowledge under definition $\\tau_2$.","authors":["Rustam Galimullin","Louwe B. Kuijer"],"url":"https://arxiv.org/abs/2505.06619"}
{"created":"2025-05-13","title":"Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations","abstract":"There is a growing demand for the use of Artificial Intelligence (AI) and Machine Learning (ML) in healthcare, particularly as clinical decision support systems to assist medical professionals. However, the complexity of many of these models, often referred to as black box models, raises concerns about their safe integration into clinical settings as it is difficult to understand how they arrived at their predictions. This paper discusses insights and recommendations derived from an expert working group convened by the UK Medicine and Healthcare products Regulatory Agency (MHRA). The group consisted of healthcare professionals, regulators, and data scientists, with a primary focus on evaluating the outputs from different AI algorithms in clinical decision-making contexts. Additionally, the group evaluated findings from a pilot study investigating clinicians' behaviour and interaction with AI methods during clinical diagnosis. Incorporating AI methods is crucial for ensuring the safety and trustworthiness of medical AI devices in clinical settings. Adequate training for stakeholders is essential to address potential issues, and further insights and recommendations for safely adopting AI systems in healthcare settings are provided.","authors":["Dima Alattal","Asal Khoshravan Azar","Puja Myles","Richard Branson","Hatim Abdulhussein","Allan Tucker"],"url":"https://arxiv.org/abs/2505.06620"}
{"created":"2025-05-13","title":"Minimizing Risk Through Minimizing Model-Data Interaction: A Protocol For Relying on Proxy Tasks When Designing Child Sexual Abuse Imagery Detection Models","abstract":"The distribution of child sexual abuse imagery (CSAI) is an ever-growing concern of our modern world; children who suffered from this heinous crime are revictimized, and the growing amount of illegal imagery distributed overwhelms law enforcement agents (LEAs) with the manual labor of categorization. To ease this burden researchers have explored methods for automating data triage and detection of CSAI, but the sensitive nature of the data imposes restricted access and minimal interaction between real data and learning algorithms, avoiding leaks at all costs. In observing how these restrictions have shaped the literature we formalize a definition of \"Proxy Tasks\", i.e., the substitute tasks used for training models for CSAI without making use of CSA data. Under this new terminology we review current literature and present a protocol for making conscious use of Proxy Tasks together with consistent input from LEAs to design better automation in this field. Finally, we apply this protocol to study -- for the first time -- the task of Few-shot Indoor Scene Classification on CSAI, showing a final model that achieves promising results on a real-world CSAI dataset whilst having no weights actually trained on sensitive data.","authors":["Thamiris Coelho","Leo S. F. Ribeiro","Jo\\~ao Macedo","Jefersson A. dos Santos","Sandra Avila"],"url":"https://arxiv.org/abs/2505.06621"}
{"created":"2025-05-13","title":"The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification","abstract":"We extend and study a semi-supervised model for text classification proposed earlier by Hatefi et al. for classification tasks in which document classes are described by a small number of gold-labeled examples, while the majority of training examples is unlabeled. The model leverages the teacher-student architecture of Meta Pseudo Labels in which a ''teacher'' generates labels for originally unlabeled training data to train the ''student'' and updates its own model iteratively based on the performance of the student on the gold-labeled portion of the data. We extend the original model of Hatefi et al. by an unsupervised pre-training phase based on objective masking, and conduct in-depth performance evaluations of the original model, our extension, and various independent baselines. Experiments are performed using three different datasets in two different languages (English and Swedish).","authors":["Arezoo Hatefi","Xuan-Son Vu","Monowar Bhuyan","Frank Drewes"],"url":"https://arxiv.org/abs/2505.06624"}
{"created":"2025-05-13","title":"CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated NPUs","abstract":"With the rapid development of DNN applications, multi-tenant execution, where multiple DNNs are co-located on a single SoC, is becoming a prevailing trend. Although many methods are proposed in prior works to improve multi-tenant performance, the impact of shared cache is not well studied. This paper proposes CaMDN, an architecture-scheduling co-design to enhance cache efficiency for multi-tenant DNNs on integrated NPUs. Specifically, a lightweight architecture is proposed to support model-exclusive, NPU-controlled regions inside shared cache to eliminate unexpected cache contention. Moreover, a cache scheduling method is proposed to improve shared cache utilization. In particular, it includes a cache-aware mapping method for adaptability to the varying available cache capacity and a dynamic allocation algorithm to adjust the usage among co-located DNNs at runtime. Compared to prior works, CaMDN reduces the memory access by 33.4% on average and achieves a model speedup of up to 2.56$\\times$ (1.88$\\times$ on average).","authors":["Tianhao Cai","Liang Wang","Limin Xiao","Meng Han","Zeyu Wang","Lin Sun","Xiaojian Liao"],"url":"https://arxiv.org/abs/2505.06625"}
{"created":"2025-05-13","title":"ACORN: Adaptive Contrastive Optimization for Safe and Robust Fine-Grained Robotic Manipulation","abstract":"Embodied AI research has traditionally emphasized performance metrics such as success rate and cumulative reward, overlooking critical robustness and safety considerations that emerge during real-world deployment. In actual environments, agents continuously encounter unpredicted situations and distribution shifts, causing seemingly reliable policies to experience catastrophic failures, particularly in manipulation tasks. To address this gap, we introduce four novel safety-centric metrics that quantify an agent's resilience to environmental perturbations. Building on these metrics, we present Adaptive Contrastive Optimization for Robust Manipulation (ACORN), a plug-and-play algorithm that enhances policy robustness without sacrificing performance. ACORN leverages contrastive learning to simultaneously align trajectories with expert demonstrations while diverging from potentially unsafe behaviors. Our approach efficiently generates informative negative samples through structured Gaussian noise injection, employing a double perturbation technique that maintains sample diversity while minimizing computational overhead. Comprehensive experiments across diverse manipulation environments validate ACORN's effectiveness, yielding improvements of up to 23% in safety metrics under disturbance compared to baseline methods. These findings underscore ACORN's significant potential for enabling reliable deployment of embodied agents in safety-critical real-world applications.","authors":["Zhongquan Zhou","Shuhao Li","Zixian Yue"],"url":"https://arxiv.org/abs/2505.06628"}
{"created":"2025-05-13","title":"Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis","abstract":"Multi-domain sentiment classification aims to mitigate poor performance models due to the scarcity of labeled data in a single domain, by utilizing data labeled from various domains. A series of models that jointly train domain classifiers and sentiment classifiers have demonstrated their advantages, because domain classification helps generate necessary information for sentiment classification. Intuitively, the importance of sentiment classification tasks is the same in all domains for multi-domain sentiment classification; but domain classification tasks are different because the impact of domain information on sentiment classification varies across different fields; this can be controlled through adjustable weights or hyper parameters. However, as the number of domains increases, existing hyperparameter optimization algorithms may face the following challenges: (1) tremendous demand for computing resources, (2) convergence problems, and (3) high algorithm complexity. To efficiently generate the domain information required for sentiment classification in each domain, we propose a dynamic information modulation algorithm. Specifically, the model training process is divided into two stages. In the first stage, a shared hyperparameter, which would control the proportion of domain classification tasks across all fields, is determined. In the second stage, we introduce a novel domain-aware modulation algorithm to adjust the domain information contained in the input text, which is then calculated based on a gradient-based and loss-based method. In summary, experimental results on a public sentiment analysis dataset containing 16 domains prove the superiority of the proposed method.","authors":["Chunyi Yue","Ang Li"],"url":"https://arxiv.org/abs/2505.06630"}
{"created":"2025-05-13","title":"AI-Powered Anomaly Detection with Blockchain for Real-Time Security and Reliability in Autonomous Vehicles","abstract":"Autonomous Vehicles (AV) proliferation brings important and pressing security and reliability issues that must be dealt with to guarantee public safety and help their widespread adoption. The contribution of the proposed research is towards achieving more secure, reliable, and trustworthy autonomous transportation system by providing more capabilities for anomaly detection, data provenance, and real-time response in safety critical AV deployments. In this research, we develop a new framework that combines the power of Artificial Intelligence (AI) for real-time anomaly detection with blockchain technology to detect and prevent any malicious activity including sensor failures in AVs. Through Long Short-Term Memory (LSTM) networks, our approach continually monitors associated multi-sensor data streams to detect anomalous patterns that may represent cyberattacks as well as hardware malfunctions. Further, this framework employs a decentralized platform for securely storing sensor data and anomaly alerts in a blockchain ledger for data incorruptibility and authenticity, while offering transparent forensic features. Moreover, immediate automated response mechanisms are deployed using smart contracts when anomalies are found. This makes the AV system more resilient to attacks from both cyberspace and hardware component failure. Besides, we identify potential challenges of scalability in handling high frequency sensor data, computational constraint in resource constrained environment, and of distributed data storage in terms of privacy.","authors":["Rathin Chandra Shit","Sharmila Subudhi"],"url":"https://arxiv.org/abs/2505.06632"}
{"created":"2025-05-13","title":"Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models","abstract":"Decoder-only transformer networks have become incredibly popular for language modeling tasks. State-of-the-art models can have over a hundred transformer blocks, containing billions of trainable parameters, and are trained on trillions of tokens of text. Each transformer block typically consists of a multi-head attention (MHA) mechanism and a two-layer fully connected feedforward network (FFN). In this paper, we examine the importance of the FFN during the model pre-training process through a series of experiments, confirming that the FFN is important to model performance. Furthermore, we show that models using a transformer block configuration with three-layer FFNs with fewer such blocks outperform the standard two-layer configuration delivering lower training loss with fewer total parameters in less time.","authors":["Isaac Gerber"],"url":"https://arxiv.org/abs/2505.06633"}
{"created":"2025-05-13","title":"Reducing Unimodal Bias in Multi-Modal Semantic Segmentation with Multi-Scale Functional Entropy Regularization","abstract":"Fusing and balancing multi-modal inputs from novel sensors for dense prediction tasks, particularly semantic segmentation, is critically important yet remains a significant challenge. One major limitation is the tendency of multi-modal frameworks to over-rely on easily learnable modalities, a phenomenon referred to as unimodal dominance or bias. This issue becomes especially problematic in real-world scenarios where the dominant modality may be unavailable, resulting in severe performance degradation. To this end, we apply a simple but effective plug-and-play regularization term based on functional entropy, which introduces no additional parameters or modules. This term is designed to intuitively balance the contribution of each visual modality to the segmentation results. Specifically, we leverage the log-Sobolev inequality to bound functional entropy using functional-Fisher-information. By maximizing the information contributed by each visual modality, our approach mitigates unimodal dominance and establishes a more balanced and robust segmentation framework. A multi-scale regularization module is proposed to apply our proposed plug-and-play term on high-level features and also segmentation predictions for more balanced multi-modal learning. Extensive experiments on three datasets demonstrate that our proposed method achieves superior performance, i.e., +13.94%, +3.25%, and +3.64%, without introducing any additional parameters.","authors":["Xu Zheng","Yuanhuiyi Lyu","Lutao Jiang","Danda Pani Paudel","Luc Van Gool","Xuming Hu"],"url":"https://arxiv.org/abs/2505.06635"}
{"created":"2025-05-13","title":"A Contrastive Federated Semi-Supervised Learning Intrusion Detection Framework for Internet of Robotic Things","abstract":"In intelligent industry, autonomous driving and other environments, the Internet of Things (IoT) highly integrated with robotic to form the Internet of Robotic Things (IoRT). However, network intrusion to IoRT can lead to data leakage, service interruption in IoRT and even physical damage by controlling robots or vehicles. This paper proposes a Contrastive Federated Semi-Supervised Learning Network Intrusion Detection framework (CFedSSL-NID) for IoRT intrusion detection and defense, to address the practical scenario of IoRT where robots don't possess labeled data locally and the requirement for data privacy preserving. CFedSSL-NID integrates randomly weak and strong augmentation, latent contrastive learning, and EMA update to integrate supervised signals, thereby enhancing performance and robustness on robots' local unlabeled data. Extensive experiments demonstrate that CFedSSL-NID outperforms existing federated semi-supervised and fully supervised methods on benchmark dataset and has lower resource requirements.","authors":["Yifan Zeng"],"url":"https://arxiv.org/abs/2505.06636"}
{"created":"2025-05-13","title":"Exploring Multimodal Foundation AI and Expert-in-the-Loop for Sustainable Management of Wild Salmon Fisheries in Indigenous Rivers","abstract":"Wild salmon are essential to the ecological, economic, and cultural sustainability of the North Pacific Rim. Yet climate variability, habitat loss, and data limitations in remote ecosystems that lack basic infrastructure support pose significant challenges to effective fisheries management. This project explores the integration of multimodal foundation AI and expert-in-the-loop frameworks to enhance wild salmon monitoring and sustainable fisheries management in Indigenous rivers across Pacific Northwest. By leveraging video and sonar-based monitoring, we develop AI-powered tools for automated species identification, counting, and length measurement, reducing manual effort, expediting delivery of results, and improving decision-making accuracy. Expert validation and active learning frameworks ensure ecological relevance while reducing annotation burdens. To address unique technical and societal challenges, we bring together a cross-domain, interdisciplinary team of university researchers, fisheries biologists, Indigenous stewardship practitioners, government agencies, and conservation organizations. Through these collaborations, our research fosters ethical AI co-development, open data sharing, and culturally informed fisheries management.","authors":["Chi Xu","Yili Jin","Sami Ma","Rongsheng Qian","Hao Fang","Jiangchuan Liu","Xue Liu","Edith C. H. Ngai","William I. Atlas","Katrina M. Connors","Mark A. Spoljaric"],"url":"https://arxiv.org/abs/2505.06637"}
{"created":"2025-05-13","title":"3D Characterization of Smoke Plume Dispersion Using Multi-View Drone Swarm","abstract":"This study presents an advanced multi-view drone swarm imaging system for the three-dimensional characterization of smoke plume dispersion dynamics. The system comprises a manager drone and four worker drones, each equipped with high-resolution cameras and precise GPS modules. The manager drone uses image feedback to autonomously detect and position itself above the plume, then commands the worker drones to orbit the area in a synchronized circular flight pattern, capturing multi-angle images. The camera poses of these images are first estimated, then the images are grouped in batches and processed using Neural Radiance Fields (NeRF) to generate high-resolution 3D reconstructions of plume dynamics over time. Field tests demonstrated the ability of the system to capture critical plume characteristics including volume dynamics, wind-driven directional shifts, and lofting behavior at a temporal resolution of about 1 s. The 3D reconstructions generated by this system provide unique field data for enhancing the predictive models of smoke plume dispersion and fire spread. Broadly, the drone swarm system offers a versatile platform for high resolution measurements of pollutant emissions and transport in wildfires, volcanic eruptions, prescribed burns, and industrial processes, ultimately supporting more effective fire control decisions and mitigating wildfire risks.","authors":["Nikil Krishnakumar","Shashank Sharma","Srijan Kumar Pal","Jiarong Hong"],"url":"https://arxiv.org/abs/2505.06638"}
{"created":"2025-05-13","title":"SneakPeek: Data-Aware Model Selection and Scheduling for Inference Serving on the Edge","abstract":"Modern applications increasingly rely on inference serving systems to provide low-latency insights with a diverse set of machine learning models. Existing systems often utilize resource elasticity to scale with demand. However, many applications cannot rely on hardware scaling when deployed at the edge or other resource-constrained environments. In this work, we propose a model selection and scheduling algorithm that implements accuracy scaling to increase efficiency for these more constrained deployments. We show that existing schedulers that make decisions using profiled model accuracy are biased toward the label distribution present in the test dataset. To address this problem, we propose using ML models -- which we call SneakPeek models -- to dynamically adjust estimates of model accuracy, based on the underlying data. Furthermore, we greedily incorporate inference batching into scheduling decisions to improve throughput and avoid the overhead of swapping models in and out of GPU memory. Our approach employs a new notion of request priority, which navigates the trade-off between attaining high accuracy and satisfying deadlines. Using data and models from three real-world applications, we show that our proposed approaches result in higher-utility schedules and higher accuracy inferences in these hardware-constrained environments.","authors":["Joel Wolfrath","Daniel Frink","Abhishek Chandra"],"url":"https://arxiv.org/abs/2505.06641"}
{"created":"2025-05-13","title":"Practical Reasoning Interruption Attacks on Reasoning Large Language Models","abstract":"Reasoning large language models (RLLMs) have demonstrated outstanding performance across a variety of tasks, yet they also expose numerous security vulnerabilities. Most of these vulnerabilities have centered on the generation of unsafe content. However, recent work has identified a distinct \"thinking-stopped\" vulnerability in DeepSeek-R1: under adversarial prompts, the model's reasoning process ceases at the system level and produces an empty final answer. Building upon this vulnerability, researchers developed a novel prompt injection attack, termed reasoning interruption attack, and also offered an initial analysis of its root cause. Through extensive experiments, we verify the previous analyses, correct key errors based on three experimental findings, and present a more rigorous explanation of the fundamental causes driving the vulnerability. Moreover, existing attacks typically require over 2,000 tokens, impose significant overhead, reduce practicality, and are easily detected. To overcome these limitations, we propose the first practical reasoning interruption attack. It succeeds with just 109 tokens by exploiting our newly uncovered \"reasoning token overflow\" (RTO) effect to overwrite the model's final answer, forcing it to return an invalid response. Experimental results demonstrate that our proposed attack is highly effective. Furthermore, we discover that the method for triggering RTO differs between the official DeepSeek-R1 release and common unofficial deployments. As a broadened application of RTO, we also construct a novel jailbreak attack that enables the transfer of unsafe content within the reasoning tokens into final answer, thereby exposing it to the user. Our work carries significant implications for enhancing the security of RLLMs.","authors":["Yu Cui","Cong Zuo"],"url":"https://arxiv.org/abs/2505.06643"}
{"created":"2025-05-13","title":"RTOS Architectures that Solve the Diminishing Bandwidth Problem","abstract":"The Diminishing Bandwidth Problem is a long standing, previously unidentified, extensibility problem of current real-time operating systems characterized by a superficial dependency between the number of tasks in a system and the maximum bandwidth associated with a peripheral device. In the worst case, this diabolical deficiency will continue to decrease the maximum bandwidth of a peripheral device as more tasks are added to the application. If this is not taken into account, a previously functional application may experience data loss if more tasks are added to it in order to, for example, implement new features. Three novel RTOS architectures that solve the Diminishing Bandwidth Problem are specified and discussed: the Defer Structure RTOS Architecture, the Barriers and Requests RTOS Architecture, and the Strictly Atomic RTOS Architecture. Finally, two hardware solutions to the Diminishing Bandwidth Problem are also presented.","authors":["Mazen Arakji"],"url":"https://arxiv.org/abs/2505.06645"}
{"created":"2025-05-13","title":"Dataset Distillation with Probabilistic Latent Features","abstract":"As deep learning models grow in complexity and the volume of training data increases, reducing storage and computational costs becomes increasingly important. Dataset distillation addresses this challenge by synthesizing a compact set of synthetic data that can effectively replace the original dataset in downstream classification tasks. While existing methods typically rely on mapping data from pixel space to the latent space of a generative model, we propose a novel stochastic approach that models the joint distribution of latent features. This allows our method to better capture spatial structures and produce diverse synthetic samples, which benefits model training. Specifically, we introduce a low-rank multivariate normal distribution parameterized by a lightweight network. This design maintains low computational complexity and is compatible with various matching networks used in dataset distillation. After distillation, synthetic images are generated by feeding the learned latent features into a pretrained generator. These synthetic images are then used to train classification models, and performance is evaluated on real test set. We validate our method on several benchmarks, including ImageNet subsets, CIFAR-10, and the MedMNIST histopathological dataset. Our approach achieves state-of-the-art cross architecture performance across a range of backbone architectures, demonstrating its generality and effectiveness.","authors":["Zhe Li","Sarah Cechnicka","Cheng Ouyang","Katharina Breininger","Peter Sch\\\"uffler","Bernhard Kainz"],"url":"https://arxiv.org/abs/2505.06647"}
{"created":"2025-05-13","title":"A Formal Verification Approach to Safeguard Controller Variables from Single Event Upset","abstract":"We present a method based on program analysis and formal verification to identify conditionally relevant variables (CRVs) - variables which could lead to violation of safety properties in control software when affected by single event upsets (SEUs). Traditional static analysis can distinguish between relevant and irrelevant variables. However, it would fail to take into account the conditions specific to the control software in question. This can lead to false positives. Our algorithm employs formal verification to avoid false positives. We have conducted experiments that demonstrate that CRVs indeed are fewer in number than what traditional static analysis can detect and that our algorithm is able to identify this fact. The information provided by our algorithm could prove helpful to a compiler while it does register allocation during the compilation of the control software. In turn, this could cause significant reduction in the cost of controller chips.","authors":["Ganesha","Sujit Kumar Chakrabarti"],"url":"https://arxiv.org/abs/2505.06648"}
{"created":"2025-05-13","title":"Dyn-D$^2$P: Dynamic Differentially Private Decentralized Learning with Provable Utility Guarantee","abstract":"Most existing decentralized learning methods with differential privacy (DP) guarantee rely on constant gradient clipping bounds and fixed-level DP Gaussian noises for each node throughout the training process, leading to a significant accuracy degradation compared to non-private counterparts. In this paper, we propose a new Dynamic Differentially Private Decentralized learning approach (termed Dyn-D$^2$P) tailored for general time-varying directed networks. Leveraging the Gaussian DP (GDP) framework for privacy accounting, Dyn-D$^2$P dynamically adjusts gradient clipping bounds and noise levels based on gradient convergence. This proposed dynamic noise strategy enables us to enhance model accuracy while preserving the total privacy budget. Extensive experiments on benchmark datasets demonstrate the superiority of Dyn-D$^2$P over its counterparts employing fixed-level noises, especially under strong privacy guarantees. Furthermore, we provide a provable utility bound for Dyn-D$^2$P that establishes an explicit dependency on network-related parameters, with a scaling factor of $1/\\sqrt{n}$ in terms of the number of nodes $n$ up to a bias error term induced by gradient clipping. To our knowledge, this is the first model utility analysis for differentially private decentralized non-convex optimization with dynamic gradient clipping bounds and noise levels.","authors":["Zehan Zhu","Yan Huang","Xin Wang","Shouling Ji","Jinming Xu"],"url":"https://arxiv.org/abs/2505.06651"}
{"created":"2025-05-13","title":"Enfoque Odychess: Un m\\'etodo dial\\'ectico, constructivista y adaptativo para la ense\\~nanza del ajedrez con inteligencias artificiales generativas","abstract":"Chess teaching has evolved through different approaches, however, traditional methodologies, often based on memorization, contrast with the new possibilities offered by generative artificial intelligence, a technology still little explored in this field. This study seeks to empirically validate the effectiveness of the Odychess Approach in improving chess knowledge, strategic understanding, and metacognitive skills in students. A quasi-experimental study was conducted with a pre-test/post-test design and a control group (N=60). The experimental intervention implemented the Odychess Approach, incorporating a Llama 3.3 language model that was specifically adapted using Parameter-Efficient Fine-Tuning (PEFT) techniques to act as a Socratic chess tutor. Quantitative assessment instruments were used to measure chess knowledge, strategic understanding, and metacognitive skills before and after the intervention. The results of the quasi-experimental study showed significant improvements in the experimental group compared to the control group in the three variables analyzed: chess knowledge, strategic understanding, and metacognitive skills. The complementary qualitative analysis revealed greater analytical depth, more developed dialectical reasoning, and increased intrinsic motivation in students who participated in the Odychess method-based intervention. The Odychess Approach represents an effective pedagogical methodology for teaching chess, demonstrating the potential of the synergistic integration of constructivist and dialectical principles with generative artificial intelligence. The implications of this work are relevant for educators and institutions interested in adopting innovative pedagogical technologies and for researchers in the field of AI applied to education, highlighting the transferability of the language model adaptation methodology to other educational domains.","authors":["Ernesto Giralt Hernandez","Lazaro Antonio Bueno Perez"],"url":"https://arxiv.org/abs/2505.06652"}
{"created":"2025-05-13","title":"Improving Block-Wise LLM Quantization by 4-bit Block-Wise Optimal Float (BOF4): Analysis and Variations","abstract":"Large language models (LLMs) demand extensive memory capacity during both fine-tuning and inference. To enable memory-efficient fine-tuning, existing methods apply block-wise quantization techniques, such as NF4 and AF4, to the network weights. We show that these quantization techniques incur suboptimal quantization errors. Therefore, as a first novelty, we propose an optimization approach for block-wise quantization. Using this method, we design a family of quantizers named 4-bit block-wise optimal float (BOF4), which consistently reduces the quantization error compared to both baseline methods. We provide both a theoretical and a data-driven solution for the optimization process and prove their practical equivalence. Secondly, we propose a modification to the employed normalization method based on the signed absolute block maximum (BOF4-S), enabling further reduction of the quantization error and empirically achieving less degradation in language modeling performance. Thirdly, we explore additional variations of block-wise quantization methods applied to LLMs through an experimental study on the importance of accurately representing zero and large-amplitude weights on the one hand, and optimization towards various error metrics on the other hand. Lastly, we introduce a mixed-precision quantization strategy dubbed outlier-preserving quantization (OPQ) to address the distributional mismatch induced by outlier weights in block-wise quantization. By storing outlier weights in 16-bit precision (OPQ) while applying BOF4-S, we achieve top performance among 4-bit block-wise quantization techniques w.r.t. perplexity.","authors":["Patrick Blumenberg","Thomas Graave","Tim Fingscheidt"],"url":"https://arxiv.org/abs/2505.06653"}
{"created":"2025-05-13","title":"Mixer-Informer-Based Two-Stage Transfer Learning for Long-Sequence Load Forecasting in Newly Constructed Electric Vehicle Charging Stations","abstract":"The rapid rise in electric vehicle (EV) adoption demands precise charging station load forecasting, challenged by long-sequence temporal dependencies and limited data in new facilities. This study proposes MIK-TST, a novel two-stage transfer learning framework integrating Mixer, Informer, and Kolmogorov-Arnold Networks (KAN). The Mixer fuses multi-source features, Informer captures long-range dependencies via ProbSparse attention, and KAN enhances nonlinear modeling with learnable activation functions. Pre-trained on extensive data and fine-tuned on limited target data, MIK-TST achieves 4% and 8% reductions in MAE and MSE, respectively, outperforming baselines on a dataset of 26 charging stations in Boulder, USA. This scalable solution enhances smart grid efficiency and supports sustainable EV infrastructure expansion.","authors":["Zhenhua Zhou","Bozhen Jiang","Qin Wang"],"url":"https://arxiv.org/abs/2505.06657"}
{"created":"2025-05-13","title":"TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models","abstract":"Self-supervised learning (SSL) models have significantly advanced speech processing tasks, and several benchmarks have been proposed to validate their effectiveness. However, previous benchmarks have primarily focused on single-speaker scenarios, with less exploration of target-speaker tasks in noisy, multi-talker conditions -- a more challenging yet practical case. In this paper, we introduce the Target-Speaker Speech Processing Universal Performance Benchmark (TS-SUPERB), which includes four widely recognized target-speaker processing tasks that require identifying the target speaker and extracting information from the speech mixture. In our benchmark, the speaker embedding extracted from enrollment speech is used as a clue to condition downstream models. The benchmark result reveals the importance of evaluating SSL models in target speaker scenarios, demonstrating that performance cannot be easily inferred from related single-speaker tasks. Moreover, by using a unified SSL-based target speech encoder, consisting of a speaker encoder and an extractor module, we also investigate joint optimization across TS tasks to leverage mutual information and demonstrate its effectiveness.","authors":["Junyi Peng","Takanori Ashihara","Marc Delcroix","Tsubasa Ochiai","Oldrich Plchot","Shoko Araki","Jan \\v{C}ernock\\'y"],"url":"https://arxiv.org/abs/2505.06660"}
{"created":"2025-05-13","title":"Centralized Trust in Decentralized Systems: Unveiling Hidden Contradictions in Blockchain and Cryptocurrency","abstract":"Blockchain technology promises to democratize finance and promote social equity through decentralization, but questions remain about whether current implementations advance or hinder these goals. Through a mixed-methods study combining semi-structured interviews with 13 diverse blockchain stakeholders and analysis of over 3,000 cryptocurrency discussions on Reddit, we examine how trust manifests in cryptocurrency ecosystems despite their decentralized architecture. Our findings uncover that users actively seek out and create centralized trust anchors, such as established exchanges, prominent community figures, and recognized development teams, contradicting blockchain's fundamental promise of trustless interactions. We identify how this contradiction arises from users' mental need for accountability and their reluctance to shoulder the full responsibility of self-custody. The study also reveals how these centralized trust patterns disproportionately impact different user groups, with newer and less technical users showing stronger preferences for centralized intermediaries. This work contributes to our understanding of the inherent tensions between theoretical decentralization and practical implementation in cryptocurrency systems, highlighting the persistent role of centralized trust in supposedly trustless environments.","authors":["Faisal Haque Bappy","EunJeong Cheon","Tariqul Islam"],"url":"https://arxiv.org/abs/2505.06661"}
{"created":"2025-05-13","title":"METOR: A Unified Framework for Mutual Enhancement of Objects and Relationships in Open-vocabulary Video Visual Relationship Detection","abstract":"Open-vocabulary video visual relationship detection aims to detect objects and their relationships in videos without being restricted by predefined object or relationship categories. Existing methods leverage the rich semantic knowledge of pre-trained vision-language models such as CLIP to identify novel categories. They typically adopt a cascaded pipeline to first detect objects and then classify relationships based on the detected objects, which may lead to error propagation and thus suboptimal performance. In this paper, we propose Mutual EnhancemenT of Objects and Relationships (METOR), a query-based unified framework to jointly model and mutually enhance object detection and relationship classification in open-vocabulary scenarios. Under this framework, we first design a CLIP-based contextual refinement encoding module that extracts visual contexts of objects and relationships to refine the encoding of text features and object queries, thus improving the generalization of encoding to novel categories. Then we propose an iterative enhancement module to alternatively enhance the representations of objects and relationships by fully exploiting their interdependence to improve recognition performance. Extensive experiments on two public datasets, VidVRD and VidOR, demonstrate that our framework achieves state-of-the-art performance.","authors":["Yongqi Wang","Xinxiao Wu","Shuo Yang"],"url":"https://arxiv.org/abs/2505.06663"}
{"created":"2025-05-13","title":"A Novel Inverter Control Strategy with Power Decoupling for Microgrid Operations in Grid-Connected and Islanded Modes","abstract":"Grid-forming, particularly those utilizing droop control and virtual synchronous generators (VSG), can actively regulate the frequency and voltage of microgrid systems, exhibiting dynamic characteristics akin to those of synchronous generators. Although droop control and VSG control each have distinct benefits, neither can fully meet the diverse, dynamic needs of both grid-connected (GC) and islanded (IS) modes. Additionally, the coupling between active and reactive power can negatively impact microgrids' dynamic performance and stability. To solve these problems, this paper introduces a unified dynamic power coupling (UDC) model. This model's active power control loop can be tailored to meet diverse requirements. By implementing a well-designed control loop, the system can harness the advantages of both droop control and VSG control. In islanded mode, the proposed model can provide virtual inertia and damping properties, while in grid-connected mode, the inverter's active power output can follow the changed references without significant overshoot or oscillation. Furthermore, the model incorporates coupling compensation and virtual impedance based on a relative gain array in the frequency domain to facilitate quantitative analysis of power coupling characteristics. This paper outlines a distinct design process for the unified model. Finally, the proposed control method has been validated through simulation.","authors":["Yan Tong","Qin Wang","Aihong Tang"],"url":"https://arxiv.org/abs/2505.06664"}
{"created":"2025-05-13","title":"MultiTaskVIF: Segmentation-oriented visible and infrared image fusion via multi-task learning","abstract":"Visible and infrared image fusion (VIF) has attracted significant attention in recent years. Traditional VIF methods primarily focus on generating fused images with high visual quality, while recent advancements increasingly emphasize incorporating semantic information into the fusion model during training. However, most existing segmentation-oriented VIF methods adopt a cascade structure comprising separate fusion and segmentation models, leading to increased network complexity and redundancy. This raises a critical question: can we design a more concise and efficient structure to integrate semantic information directly into the fusion model during training-Inspired by multi-task learning, we propose a concise and universal training framework, MultiTaskVIF, for segmentation-oriented VIF models. In this framework, we introduce a multi-task head decoder (MTH) to simultaneously output both the fused image and the segmentation result during training. Unlike previous cascade training frameworks that necessitate joint training with a complete segmentation model, MultiTaskVIF enables the fusion model to learn semantic features by simply replacing its decoder with MTH. Extensive experimental evaluations validate the effectiveness of the proposed method. Our code will be released upon acceptance.","authors":["Zixian Zhao","Andrew Howes","Xingchen Zhang"],"url":"https://arxiv.org/abs/2505.06665"}
{"created":"2025-05-13","title":"Motion Planning for Autonomous Vehicles: When Model Predictive Control Meets Ensemble Kalman Smoothing","abstract":"Safe and efficient motion planning is of fundamental importance for autonomous vehicles. This paper investigates motion planning based on nonlinear model predictive control (NMPC) over a neural network vehicle model. We aim to overcome the high computational costs that arise in NMPC of the neural network model due to the highly nonlinear and nonconvex optimization. In a departure from numerical optimization solutions, we reformulate the problem of NMPC-based motion planning as a Bayesian estimation problem, which seeks to infer optimal planning decisions from planning objectives. Then, we use a sequential ensemble Kalman smoother to accomplish the estimation task, exploiting its high computational efficiency for complex nonlinear systems. The simulation results show an improvement in computational speed by orders of magnitude, indicating the potential of the proposed approach for practical motion planning.","authors":["Iman Askari","Yebin Wang","Vedeng M. Deshpande","Huazhen Fang"],"url":"https://arxiv.org/abs/2505.06666"}
{"created":"2025-05-13","title":"StableMotion: Repurposing Diffusion-Based Image Priors for Motion Estimation","abstract":"We present StableMotion, a novel framework leverages knowledge (geometry and content priors) from pretrained large-scale image diffusion models to perform motion estimation, solving single-image-based image rectification tasks such as Stitched Image Rectangling (SIR) and Rolling Shutter Correction (RSC). Specifically, StableMotion framework takes text-to-image Stable Diffusion (SD) models as backbone and repurposes it into an image-to-motion estimator. To mitigate inconsistent output produced by diffusion models, we propose Adaptive Ensemble Strategy (AES) that consolidates multiple outputs into a cohesive, high-fidelity result. Additionally, we present the concept of Sampling Steps Disaster (SSD), the counterintuitive scenario where increasing the number of sampling steps can lead to poorer outcomes, which enables our framework to achieve one-step inference. StableMotion is verified on two image rectification tasks and delivers state-of-the-art performance in both, as well as showing strong generalizability. Supported by SSD, StableMotion offers a speedup of 200 times compared to previous diffusion model-based methods.","authors":["Ziyi Wang","Haipeng Li","Lin Sui","Tianhao Zhou","Hai Jiang","Lang Nie","Shuaicheng Liu"],"url":"https://arxiv.org/abs/2505.06668"}
{"created":"2025-05-13","title":"Video Dataset Condensation with Diffusion Models","abstract":"In recent years, the rapid expansion of dataset sizes and the increasing complexity of deep learning models have significantly escalated the demand for computational resources, both for data storage and model training. Dataset distillation has emerged as a promising solution to address this challenge by generating a compact synthetic dataset that retains the essential information from a large real dataset. However, existing methods often suffer from limited performance and poor data quality, particularly in the video domain. In this paper, we focus on video dataset distillation by employing a video diffusion model to generate high-quality synthetic videos. To enhance representativeness, we introduce Video Spatio-Temporal U-Net (VST-UNet), a model designed to select a diverse and informative subset of videos that effectively captures the characteristics of the original dataset. To further optimize computational efficiency, we explore a training-free clustering algorithm, Temporal-Aware Cluster-based Distillation (TAC-DT), to select representative videos without requiring additional training overhead. We validate the effectiveness of our approach through extensive experiments on four benchmark datasets, demonstrating performance improvements of up to \\(10.61\\%\\) over the state-of-the-art. Our method consistently outperforms existing approaches across all datasets, establishing a new benchmark for video dataset distillation.","authors":["Zhe Li","Hadrien Reynaud","Mischa Dombrowski","Sarah Cechnicka","Franciskus Xaverius Erick","Bernhard Kainz"],"url":"https://arxiv.org/abs/2505.06670"}
{"created":"2025-05-13","title":"VTutor: An Animated Pedagogical Agent SDK that Provide Real Time Multi-Model Feedback","abstract":"Pedagogical Agents (PAs) show significant potential for boosting student engagement and learning outcomes by providing adaptive, on-demand support in educational contexts. However, existing PA solutions are often hampered by pre-scripted dialogue, unnatural animations, uncanny visual realism, and high development costs. To address these gaps, we introduce VTutor, an open-source SDK leveraging lightweight WebGL, Unity, and JavaScript frameworks. VTutor receives text outputs from a large language model (LLM), converts them into audio via text-to-speech, and then renders a real-time, lip-synced pedagogical agent (PA) for immediate, large-scale deployment on web-based learning platforms. By providing on-demand, personalized feedback, VTutor strengthens students' motivation and deepens their engagement with instructional material. Using an anime-like aesthetic, VTutor alleviates the uncanny valley effect, allowing learners to engage with expressive yet comfortably stylized characters. Our evaluation with 50 participants revealed that VTutor significantly outperforms the existing talking-head approaches (e.g., SadTalker) on perceived synchronization accuracy, naturalness, emotional expressiveness, and overall preference. As an open-source project, VTutor welcomes community-driven contributions - from novel character designs to specialized showcases of pedagogical agent applications - that fuel ongoing innovation in AI-enhanced education. By providing an accessible, customizable, and learner-centered PA solution, VTutor aims to elevate human-AI interaction experience in education fields, ultimately broadening the impact of AI in learning contexts. The demo link to VTutor is at https://vtutor-aied25.vercel.app.","authors":["Eason Chen","Chenyu Lin","Yu-Kai Huang","Xinyi Tang","Aprille Xi","Jionghao Lin","Kenneth Koedinger"],"url":"https://arxiv.org/abs/2505.06676"}
{"created":"2025-05-13","title":"Distributionally Robust Contract Theory for Edge AIGC Services in Teleoperation","abstract":"Advanced AI-Generated Content (AIGC) technologies have injected new impetus into teleoperation, further enhancing its security and efficiency. Edge AIGC networks have been introduced to meet the stringent low-latency requirements of teleoperation. However, the inherent uncertainty of AIGC service quality and the need to incentivize AIGC service providers (ASPs) make the design of a robust incentive mechanism essential. This design is particularly challenging due to both uncertainty and information asymmetry, as teleoperators have limited knowledge of the remaining resource capacities of ASPs. To this end, we propose a distributionally robust optimization (DRO)-based contract theory to design robust reward schemes for AIGC task offloading. Notably, our work extends the contract theory by integrating DRO, addressing the fundamental challenge of contract design under uncertainty. In this paper, contract theory is employed to model the information asymmetry, while DRO is utilized to capture the uncertainty in AIGC service quality. Given the inherent complexity of the original DRO-based contract theory problem, we reformulate it into an equivalent, tractable bi-level optimization problem. To efficiently solve this problem, we develop a Block Coordinate Descent (BCD)-based algorithm to derive robust reward schemes. Simulation results on our unity-based teleoperation platform demonstrate that the proposed method improves teleoperator utility by 2.7\\% to 10.74\\% under varying degrees of AIGC service quality shifts and increases ASP utility by 60.02\\% compared to the SOTA method, i.e., Deep Reinforcement Learning (DRL)-based contract theory. The code and data are publicly available at https://github.com/Zijun0819/DRO-Contract-Theory.","authors":["Zijun Zhan","Yaxian Dong","Daniel Mawunyo Doe","Yuqing Hu","Shuai Li","Shaohua Cao","Lei Fan","Zhu Han"],"url":"https://arxiv.org/abs/2505.06678"}
{"created":"2025-05-13","title":"Jailbreaking the Text-to-Video Generative Models","abstract":"Text-to-video generative models have achieved significant progress, driven by the rapid advancements in diffusion models, with notable examples including Pika, Luma, Kling, and Sora. Despite their remarkable generation ability, their vulnerability to jailbreak attack, i.e. to generate unsafe content, including pornography, violence, and discrimination, raises serious safety concerns. Existing efforts, such as T2VSafetyBench, have provided valuable benchmarks for evaluating the safety of text-to-video models against unsafe prompts but lack systematic studies for exploiting their vulnerabilities effectively. In this paper, we propose the \\textit{first} optimization-based jailbreak attack against text-to-video models, which is specifically designed. Our approach formulates the prompt generation task as an optimization problem with three key objectives: (1) maximizing the semantic similarity between the input and generated prompts, (2) ensuring that the generated prompts can evade the safety filter of the text-to-video model, and (3) maximizing the semantic similarity between the generated videos and the original input prompts. To further enhance the robustness of the generated prompts, we introduce a prompt mutation strategy that creates multiple prompt variants in each iteration, selecting the most effective one based on the averaged score. This strategy not only improves the attack success rate but also boosts the semantic relevance of the generated video. We conduct extensive experiments across multiple text-to-video models, including Open-Sora, Pika, Luma, and Kling. The results demonstrate that our method not only achieves a higher attack success rate compared to baseline methods but also generates videos with greater semantic similarity to the original input prompts.","authors":["Jiayang Liu","Siyuan Liang","Shiqian Zhao","Rongcheng Tu","Wenbo Zhou","Xiaochun Cao","Dacheng Tao","Siew Kei Lam"],"url":"https://arxiv.org/abs/2505.06679"}
{"created":"2025-05-13","title":"A Survey on Data-Driven Modeling of Human Drivers' Lane-Changing Decisions","abstract":"Lane-changing (LC) behavior, a critical yet complex driving maneuver, significantly influences driving safety and traffic dynamics. Traditional analytical LC decision (LCD) models, while effective in specific environments, often oversimplify behavioral heterogeneity and complex interactions, limiting their capacity to capture real LCD. Data-driven approaches address these gaps by leveraging rich empirical data and machine learning to decode latent decision-making patterns, enabling adaptive LCD modeling in dynamic environments. In light of the rapid development of artificial intelligence and the demand for data-driven models oriented towards connected vehicles and autonomous vehicles, this paper presents a comprehensive survey of data-driven LCD models, with a particular focus on human drivers LC decision-making. It systematically reviews the modeling framework, covering data sources and preprocessing, model inputs and outputs, objectives, structures, and validation methods. This survey further discusses the opportunities and challenges faced by data-driven LCD models, including driving safety, uncertainty, as well as the integration and improvement of technical frameworks.","authors":["Linxuan Huang","Dong-Fan Xie","Li Li","Zhengbing He"],"url":"https://arxiv.org/abs/2505.06680"}
{"created":"2025-05-13","title":"UnfoldIR: Rethinking Deep Unfolding Network in Illumination Degradation Image Restoration","abstract":"Deep unfolding networks (DUNs) are widely employed in illumination degradation image restoration (IDIR) to merge the interpretability of model-based approaches with the generalization of learning-based methods. However, the performance of DUN-based methods remains considerably inferior to that of state-of-the-art IDIR solvers. Our investigation indicates that this limitation does not stem from structural shortcomings of DUNs but rather from the limited exploration of the unfolding structure, particularly for (1) constructing task-specific restoration models, (2) integrating advanced network architectures, and (3) designing DUN-specific loss functions. To address these issues, we propose a novel DUN-based method, UnfoldIR, for IDIR tasks. UnfoldIR first introduces a new IDIR model with dedicated regularization terms for smoothing illumination and enhancing texture. We unfold the iterative optimized solution of this model into a multistage network, with each stage comprising a reflectance-assisted illumination correction (RAIC) module and an illumination-guided reflectance enhancement (IGRE) module. RAIC employs a visual state space (VSS) to extract non-local features, enforcing illumination smoothness, while IGRE introduces a frequency-aware VSS to globally align similar textures, enabling mildly degraded regions to guide the enhancement of details in more severely degraded areas. This suppresses noise while enhancing details. Furthermore, given the multistage structure, we propose an inter-stage information consistent loss to maintain network stability in the final stages. This loss contributes to structural preservation and sustains the model's performance even in unsupervised settings. Experiments verify our effectiveness across 5 IDIR tasks and 3 downstream problems.","authors":["Chunming He","Rihan Zhang","Fengyang Xiao","Chengyu Fang","Longxiang Tang","Yulun Zhang","Sina Farsiu"],"url":"https://arxiv.org/abs/2505.06683"}
{"created":"2025-05-13","title":"FNBench: Benchmarking Robust Federated Learning against Noisy Labels","abstract":"Robustness to label noise within data is a significant challenge in federated learning (FL). From the data-centric perspective, the data quality of distributed datasets can not be guaranteed since annotations of different clients contain complicated label noise of varying degrees, which causes the performance degradation. There have been some early attempts to tackle noisy labels in FL. However, there exists a lack of benchmark studies on comprehensively evaluating their practical performance under unified settings. To this end, we propose the first benchmark study FNBench to provide an experimental investigation which considers three diverse label noise patterns covering synthetic label noise, imperfect human-annotation errors and systematic errors. Our evaluation incorporates eighteen state-of-the-art methods over five image recognition datasets and one text classification dataset. Meanwhile, we provide observations to understand why noisy labels impair FL, and additionally exploit a representation-aware regularization method to enhance the robustness of existing methods against noisy labels based on our observations. Finally, we discuss the limitations of this work and propose three-fold future directions. To facilitate related communities, our source code is open-sourced at https://github.com/Sprinter1999/FNBench.","authors":["Xuefeng Jiang","Jia Li","Nannan Wu","Zhiyuan Wu","Xujing Li","Sheng Sun","Gang Xu","Yuwei Wang","Qi Li","Min Liu"],"url":"https://arxiv.org/abs/2505.06684"}
{"created":"2025-05-13","title":"Emotion-Qwen: Training Hybrid Experts for Unified Emotion and General Vision-Language Understanding","abstract":"Emotion understanding in videos aims to accurately recognize and interpret individuals' emotional states by integrating contextual, visual, textual, and auditory cues. While Large Multimodal Models (LMMs) have demonstrated significant progress in general vision-language (VL) tasks, their performance in emotion-specific scenarios remains limited. Moreover, fine-tuning LMMs on emotion-related tasks often leads to catastrophic forgetting, hindering their ability to generalize across diverse tasks. To address these challenges, we present Emotion-Qwen, a tailored multimodal framework designed to enhance both emotion understanding and general VL reasoning. Emotion-Qwen incorporates a sophisticated Hybrid Compressor based on the Mixture of Experts (MoE) paradigm, which dynamically routes inputs to balance emotion-specific and general-purpose processing. The model is pre-trained in a three-stage pipeline on large-scale general and emotional image datasets to support robust multimodal representations. Furthermore, we construct the Video Emotion Reasoning (VER) dataset, comprising more than 40K bilingual video clips with fine-grained descriptive annotations, to further enrich Emotion-Qwen's emotional reasoning capability. Experimental results demonstrate that Emotion-Qwen achieves state-of-the-art performance on multiple emotion recognition benchmarks, while maintaining competitive results on general VL tasks. Code and models are available at https://anonymous.4open.science/r/Emotion-Qwen-Anonymous.","authors":["Dawei Huang","Qing Li","Chuan Yan","Zebang Cheng","Yurong Huang","Xiang Li","Bin Li","Xiaohui Wang","Zheng Lian","Xiaojiang Peng"],"url":"https://arxiv.org/abs/2505.06685"}
{"created":"2025-05-13","title":"Extend IVerilog to Support Batch RTL Fault Simulation","abstract":"The advancement of functional safety has made RTL-level fault simulation increasingly important to achieve iterative efficiency in the early stages of design and to ensure compliance with functional safety standards. In this paper, we extend IVerilog to support batch RTL fault simulation and integrate the event-driven algorithm and the concurrent fault simulation algorithm. Comparative experiments with a state-of-the-art commercial simulator and an open-source RTL fault simulator demonstrate that our simulator achieves a performance improvement of 2.2$\\times$ and 3.4$\\times$, respectively.","authors":["Jiaping Tang","Jianan Mu","Zizhen Liu","Zhiteng Chao","Jing Ye","Huawei Li"],"url":"https://arxiv.org/abs/2505.06687"}
{"created":"2025-05-13","title":"A Novel Framework for Significant Wave Height Prediction based on Adaptive Feature Extraction Time-Frequency Network","abstract":"Precise forecasting of significant wave height (Hs) is essential for the development and utilization of wave energy. The challenges in predicting Hs arise from its non-linear and non-stationary characteristics. The combination of decomposition preprocessing and machine learning models have demonstrated significant effectiveness in Hs prediction by extracting data features. However, decomposing the unknown data in the test set can lead to data leakage issues. To simultaneously achieve data feature extraction and prevent data leakage, a novel Adaptive Feature Extraction Time-Frequency Network (AFE-TFNet) is proposed to improve prediction accuracy and stability. It is encoder-decoder rolling framework. The encoder consists of two stages: feature extraction and feature fusion. In the feature extraction stage, global and local frequency domain features are extracted by combining Wavelet Transform (WT) and Fourier Transform (FT), and multi-scale frequency analysis is performed using Inception blocks. In the feature fusion stage, time-domain and frequency-domain features are integrated through dominant harmonic sequence energy weighting (DHSEW). The decoder employed an advanced long short-term memory (LSTM) model. Hourly measured wind speed (Ws), dominant wave period (DPD), average wave period (APD) and Hs from three stations are used as the dataset, and the four metrics are employed to evaluate the forecasting performance. Results show that AFE-TFNet significantly outperforms benchmark methods in terms of prediction accuracy. Feature extraction can significantly improve the prediction accuracy. DHSEW has substantially increased the accuracy of medium-term to long-term forecasting. The prediction accuracy of AFE-TFNet does not demonstrate significant variability with changes of rolling time window size. Overall, AFE-TFNet shows strong potential for handling complex signal forecasting.","authors":["Jianxin Zhang","Lianzi Jiang","Xinyu Han","Xiangrong Wang"],"url":"https://arxiv.org/abs/2505.06688"}
{"created":"2025-05-13","title":"E2E-FANet: A Highly Generalizable Framework for Waves prediction Behind Floating Breakwaters via Exogenous-to-Endogenous Variable Attention","abstract":"Accurate prediction of waves behind floating breakwaters (FB) is crucial for optimizing coastal engineering structures, enhancing safety, and improving design efficiency. Existing methods demonstrate limitations in capturing nonlinear interactions between waves and structures, while exhibiting insufficient capability in modeling the complex frequency-domain relationships among elevations of different wave gauges. To address these challenges, this study introduces the Exogenous-to-Endogenous Frequency-Aware Network (E2E-FANet), a novel end-to-end neural network designed to model relationships between waves and structures. The E2E-FANetarchitecture incorporates a Dual-Basis Frequency Mapping (DBFM) module that leverages orthogonal cosine and sine bases to extract wave features from the frequency domain while preserving temporal information. Additionally, we introduce the Exogenous-to-Endogenous Cross-Attention (E2ECA) module, which employs cross attention to model the interactions between endogenous and exogenous variables. We incorporate a Temporal-wise Attention (TA) mechanism that adaptively captures complex dependencies in endogenous variables. These integrated modules function synergistically, enabling E2E-FANet to achieve both comprehensive feature perception in the time-frequency domain and precise modeling of wave-structure interactions. To comprehensively evaluate the performance of E2E-FANet, we constructed a multi-level validation framework comprising three distinct testing scenarios: internal validation under identical wave conditions, generalization testing across different wave conditions, and adaptability testing with varying relative water density (RW) conditions. These comprehensive tests demonstrate that E2E-FANet provides accurate waves behind FB predictions while successfully generalizing diverse wave conditions.","authors":["Jianxin Zhang","Lianzi Jiang","Xinyu Han","Xiangrong Wang","Weinan Huang"],"url":"https://arxiv.org/abs/2505.06690"}
{"created":"2025-05-13","title":"Tuning Butterworth filter's parameters in SPECT reconstructions via kernel-based Bayesian optimization with a no-reference image evaluation metric","abstract":"In Single Photon Emission Computed Tomography (SPECT), the image reconstruction process involves many tunable parameters that have a significant impact on the quality of the resulting clinical images. Traditional image quality evaluation often relies on expert judgment and full-reference metrics such as MSE and SSIM. However, these approaches are limited by their subjectivity or the need for a ground-truth image. In this paper, we investigate the usage of a no-reference image quality assessment method tailored for SPECT imaging, employing the Perception-based Image QUality Evaluator (PIQUE) score. Precisely, we propose a novel application of PIQUE in evaluating SPECT images reconstructed via filtered backprojection using a parameter-dependent Butterworth filter. For the optimization of filter's parameters, we adopt a kernel-based Bayesian optimization framework grounded in reproducing kernel Hilbert space theory, highlighting the connections to recent greedy approximation techniques. Experimental results in a concrete clinical setting for SPECT imaging show the potential of this optimization approach for an objective and quantitative assessment of image quality, without requiring a reference image.","authors":["Luca Pastrello","Diego Cecchin","Gabriele Santin","Francesco Marchetti"],"url":"https://arxiv.org/abs/2505.06692"}
{"created":"2025-05-13","title":"Underwater object detection in sonar imagery with detection transformer and Zero-shot neural architecture search","abstract":"Underwater object detection using sonar imagery has become a critical and rapidly evolving research domain within marine technology. However, sonar images are characterized by lower resolution and sparser features compared to optical images, which seriously degrades the performance of object detection.To address these challenges, we specifically propose a Detection Transformer (DETR) architecture optimized with a Neural Architecture Search (NAS) approach called NAS-DETR for object detection in sonar images. First, an improved Zero-shot Neural Architecture Search (NAS) method based on the maximum entropy principle is proposed to identify a real-time, high-representational-capacity CNN-Transformer backbone for sonar image detection. This method enables the efficient discovery of high-performance network architectures with low computational and time overhead. Subsequently, the backbone is combined with a Feature Pyramid Network (FPN) and a deformable attention-based Transformer decoder to construct a complete network architecture. This architecture integrates various advanced components and training schemes to enhance overall performance. Extensive experiments demonstrate that this architecture achieves state-of-the-art performance on two Representative datasets, while maintaining minimal overhead in real-time efficiency and computational complexity. Furthermore, correlation analysis between the key parameters and differential entropy-based fitness function is performed to enhance the interpretability of the proposed framework. To the best of our knowledge, this is the first work in the field of sonar object detection to integrate the DETR architecture with a NAS search mechanism.","authors":["XiaoTong Gu","Shengyu Tang","Yiming Cao","Changdong Yu"],"url":"https://arxiv.org/abs/2505.06694"}
{"created":"2025-05-13","title":"Enhancing BERTopic with Intermediate Layer Representations","abstract":"BERTopic is a topic modeling algorithm that leverages transformer-based embeddings to create dense clusters, enabling the estimation of topic structures and the extraction of valuable insights from a corpus of documents. This approach allows users to efficiently process large-scale text data and gain meaningful insights into its structure. While BERTopic is a powerful tool, embedding preparation can vary, including extracting representations from intermediate model layers and applying transformations to these embeddings. In this study, we evaluate 18 different embedding representations and present findings based on experiments conducted on three diverse datasets. To assess the algorithm's performance, we report topic coherence and topic diversity metrics across all experiments. Our results demonstrate that, for each dataset, it is possible to find an embedding configuration that performs better than the default setting of BERTopic. Additionally, we investigate the influence of stop words on different embedding configurations.","authors":["Dominik Koterwa","Maciej \\'Swita{\\l}a"],"url":"https://arxiv.org/abs/2505.06696"}
{"created":"2025-05-13","title":"From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback","abstract":"Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena are seeing growing adoption for the evaluation of Large Language Models (LLMs). Existing research has primarily focused on approximating human-based model rankings using limited data and LLM-as-a-Judge. However, the fundamental premise of these studies, which attempts to replicate human rankings, is flawed. Specifically, these benchmarks typically offer only overall scores, limiting their utility to leaderboard rankings, rather than providing feedback that can guide model optimization and support model profiling. Therefore, we advocate for an evaluation paradigm shift from approximating human-based model rankings to providing feedback with analytical value. To this end, we introduce Feedbacker, an evaluation framework that provides comprehensive and fine-grained results, thereby enabling thorough identification of a model's specific strengths and weaknesses. Such feedback not only supports the targeted optimization of the model but also enhances the understanding of its behavior. Feedbacker comprises three key components: an extensible tree-based query taxonomy builder, an automated query synthesis scheme, and a suite of visualization and analysis tools. Furthermore, we propose a novel LLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise evaluation. This method derives evaluation criteria by pre-comparing the differences between several auxiliary responses, achieving the accuracy of pairwise evaluation while maintaining the time complexity of pointwise evaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs, we demonstrate the usage of Feedbacker and highlight its effectiveness and potential. Our homepage project is available at https://liudan193.github.io/Feedbacker.","authors":["Zongqi Wang","Tianle Gu","Chen Gong","Xin Tian","Siqi Bao","Yujiu Yang"],"url":"https://arxiv.org/abs/2505.06698"}
{"created":"2025-05-13","title":"Model Steering: Learning with a Reference Model Improves Generalization Bounds and Scaling Laws","abstract":"This paper formalizes an emerging learning paradigm that uses a trained model as a reference to guide and enhance the training of a target model through strategic data selection or weighting, named $\\textbf{model steering}$. While ad-hoc methods have been used in various contexts, including the training of large foundation models, its underlying principles remain insufficiently understood, leading to sub-optimal performance. In this work, we propose a theory-driven framework for model steering called $\\textbf{DRRho risk minimization}$, which is rooted in Distributionally Robust Optimization (DRO). Through a generalization analysis, we provide theoretical insights into why this approach improves generalization and data efficiency compared to training without a reference model. To the best of our knowledge, this is the first time such theoretical insights are provided for the new learning paradigm, which significantly enhance our understanding and practice of model steering. Building on these insights and the connection between contrastive learning and DRO, we introduce a novel method for Contrastive Language-Image Pretraining (CLIP) with a reference model, termed DRRho-CLIP. Extensive experiments validate the theoretical insights, reveal a superior scaling law compared to CLIP without a reference model, and demonstrate its strength over existing heuristic approaches.","authors":["Xiyuan Wei","Ming Lin","Fanjiang Ye","Fengguang Song","Liangliang Cao","My T. That","Tianbao Yang"],"url":"https://arxiv.org/abs/2505.06699"}
{"created":"2025-05-13","title":"RuleGenie: SIEM Detection Rule Set Optimization","abstract":"SIEM systems serve as a critical hub, employing rule-based logic to detect and respond to threats. Redundant or overlapping rules in SIEM systems lead to excessive false alerts, degrading analyst performance due to alert fatigue, and increase computational overhead and response latency for actual threats. As a result, optimizing SIEM rule sets is essential for efficient operations. Despite the importance of such optimization, research in this area is limited, with current practices relying on manual optimization methods that are both time-consuming and error-prone due to the scale and complexity of enterprise-level rule sets. To address this gap, we present RuleGenie, a novel large language model (LLM) aided recommender system designed to optimize SIEM rule sets. Our approach leverages transformer models' multi-head attention capabilities to generate SIEM rule embeddings, which are then analyzed using a similarity matching algorithm to identify the top-k most similar rules. The LLM then processes the rules identified, utilizing its information extraction, language understanding, and reasoning capabilities to analyze rule similarity, evaluate threat coverage and performance metrics, and deliver optimized recommendations for refining the rule set. By automating the rule optimization process, RuleGenie allows security teams to focus on more strategic tasks while enhancing the efficiency of SIEM systems and strengthening organizations' security posture. We evaluated RuleGenie on a comprehensive set of real-world SIEM rule formats, including Splunk, Sigma, and AQL (Ariel query language), demonstrating its platform-agnostic capabilities and adaptability across diverse security infrastructures. Our experimental results show that RuleGenie can effectively identify redundant rules, which in turn decreases false positive rates and enhances overall rule efficiency.","authors":["Akansha Shukla","Parth Atulbhai Gandhi","Yuval Elovici","Asaf Shabtai"],"url":"https://arxiv.org/abs/2505.06701"}
{"created":"2025-05-13","title":"Do Language Model Agents Align with Humans in Rating Visualizations? An Empirical Study","abstract":"Large language models encode knowledge in various domains and demonstrate the ability to understand visualizations. They may also capture visualization design knowledge and potentially help reduce the cost of formative studies. However, it remains a question whether large language models are capable of predicting human feedback on visualizations. To investigate this question, we conducted three studies to examine whether large model-based agents can simulate human ratings in visualization tasks. The first study, replicating a published study involving human subjects, shows agents are promising in conducting human-like reasoning and rating, and its result guides the subsequent experimental design. The second study repeated six human-subject studies reported in literature on subjective ratings, but replacing human participants with agents. Consulting with five human experts, this study demonstrates that the alignment of agent ratings with human ratings positively correlates with the confidence levels of the experts before the experiments. The third study tests commonly used techniques for enhancing agents, including preprocessing visual and textual inputs, and knowledge injection. The results reveal the issues of these techniques in robustness and potential induction of biases. The three studies indicate that language model-based agents can potentially simulate human ratings in visualization experiments, provided that they are guided by high-confidence hypotheses from expert evaluators. Additionally, we demonstrate the usage scenario of swiftly evaluating prototypes with agents. We discuss insights and future directions for evaluating and improving the alignment of agent ratings with human ratings. We note that simulation may only serve as complements and cannot replace user studies.","authors":["Zekai Shao (Luke)","Yi Shan (Luke)","Yixuan He (Luke)","Yuxuan Yao (Luke)","Junhong Wang (Luke)","Xiaolong (Luke)","Zhang","Yu Zhang","Siming Chen"],"url":"https://arxiv.org/abs/2505.06702"}
{"created":"2025-05-13","title":"A Gpu-based solution for large-scale skeletal animation simulation","abstract":"Skeletal animations of large-scale characters are widely used in video games. However, with a large number of characters are involved, relying on the CPU to calculate skeletal animations leads to significant performance problems. There are two main types of traditional GPU- based solutions. One is referred to as pre-baked animation texture technology. The problem with this solution is that it can only play animations from the pre-baked animation. It is impossible to perform interpolation, blending and other calculations on the animation, which affects the quality of the animations. The other solution is referred to as dedicated processing with a simple skeleton hierarchy (the number of skeleton levels < 64). This option does not need to simulate and bake animation data in advance. However, performance is dramatically impaired when processing complex skeletons with too many skeleton levels (such as fluttering clothing, soft plants, dragon-like creatures, etc.). In order to solve these issues, we developed a parallel prefix tree update solution to optimize the animation update process of complex skeletons with too many levels, and combined traditional solutions to implement a GPU-based skeletal animation solution. This solution does not need to simulate and bake animation results. In addition, the performance is superior to traditional solutions for complex skeletons with too many levels. Our work can provide a new option for optimizing the performance of large-scale skeletal animation simulations, providing GPU-based skeletal animations a wider range of application scenarios.","authors":["Xi Pan"],"url":"https://arxiv.org/abs/2505.06703"}
{"created":"2025-05-13","title":"Bi-level Mean Field: Dynamic Grouping for Large-Scale MARL","abstract":"Large-scale Multi-Agent Reinforcement Learning (MARL) often suffers from the curse of dimensionality, as the exponential growth in agent interactions significantly increases computational complexity and impedes learning efficiency. To mitigate this, existing efforts that rely on Mean Field (MF) simplify the interaction landscape by approximating neighboring agents as a single mean agent, thus reducing overall complexity to pairwise interactions. However, these MF methods inevitably fail to account for individual differences, leading to aggregation noise caused by inaccurate iterative updates during MF learning. In this paper, we propose a Bi-level Mean Field (BMF) method to capture agent diversity with dynamic grouping in large-scale MARL, which can alleviate aggregation noise via bi-level interaction. Specifically, BMF introduces a dynamic group assignment module, which employs a Variational AutoEncoder (VAE) to learn the representations of agents, facilitating their dynamic grouping over time. Furthermore, we propose a bi-level interaction module to model both inter- and intra-group interactions for effective neighboring aggregation. Experiments across various tasks demonstrate that the proposed BMF yields results superior to the state-of-the-art methods. Our code will be made publicly available.","authors":["Yuxuan Zheng","Yihe Zhou","Feiyang Xu","Mingli Song","Shunyu Liu"],"url":"https://arxiv.org/abs/2505.06706"}
{"created":"2025-05-13","title":"Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free","abstract":"Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification-applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)-consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates 'attention sink' and enhances long-context extrapolation performance, and we also release related $\\href{https://github.com/qiuzh20/gated_attention}{codes}$ and $\\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate future research.","authors":["Zihan Qiu","Zekun Wang","Bo Zheng","Zeyu Huang","Kaiyue Wen","Songlin Yang","Rui Men","Le Yu","Fei Huang","Suozhi Huang","Dayiheng Liu","Jingren Zhou","Junyang Lin"],"url":"https://arxiv.org/abs/2505.06708"}
{"created":"2025-05-13","title":"Beyond $\\tilde{O}(\\sqrt{T})$ Constraint Violation for Online Convex Optimization with Adversarial Constraints","abstract":"We revisit the Online Convex Optimization problem with adversarial constraints (COCO) where, in each round, a learner is presented with a convex cost function and a convex constraint function, both of which may be chosen adversarially. The learner selects actions from a convex decision set in an online fashion, with the goal of minimizing both regret and the cumulative constraint violation (CCV) over a horizon of $T$ rounds. The best-known policy for this problem achieves $O(\\sqrt{T})$ regret and $\\tilde{O}(\\sqrt{T})$ CCV. In this paper, we present a surprising improvement that achieves a significantly smaller CCV by trading it off with regret. Specifically, for any bounded convex cost and constraint functions, we propose an online policy that achieves $\\tilde{O}(\\sqrt{dT}+ T^\\beta)$ regret and $\\tilde{O}(dT^{1-\\beta})$ CCV, where $d$ is the dimension of the decision set and $\\beta \\in [0,1]$ is a tunable parameter. We achieve this result by first considering the special case of $\\textsf{Constrained Expert}$ problem where the decision set is a probability simplex and the cost and constraint functions are linear. Leveraging a new adaptive small-loss regret bound, we propose an efficient policy for the $\\textsf{Constrained Expert}$ problem, that attains $O(\\sqrt{T\\ln N}+T^{\\beta})$ regret and $\\tilde{O}(T^{1-\\beta} \\ln N)$ CCV, where $N$ is the number of experts. The original problem is then reduced to the $\\textsf{Constrained Expert}$ problem via a covering argument. Finally, with an additional smoothness assumption, we propose an efficient gradient-based policy attaining $O(T^{\\max(\\frac{1}{2},\\beta)})$ regret and $\\tilde{O}(T^{1-\\beta})$ CCV.","authors":["Abhishek Sinha","Rahul Vaze"],"url":"https://arxiv.org/abs/2505.06709"}
{"created":"2025-05-13","title":"SimMIL: A Universal Weakly Supervised Pre-Training Framework for Multi-Instance Learning in Whole Slide Pathology Images","abstract":"Various multi-instance learning (MIL) based approaches have been developed and successfully applied to whole-slide pathological images (WSI). Existing MIL methods emphasize the importance of feature aggregators, but largely neglect the instance-level representation learning. They assume that the availability of a pre-trained feature extractor can be directly utilized or fine-tuned, which is not always the case. This paper proposes to pre-train feature extractor for MIL via a weakly-supervised scheme, i.e., propagating the weak bag-level labels to the corresponding instances for supervised learning. To learn effective features for MIL, we further delve into several key components, including strong data augmentation, a non-linear prediction head and the robust loss function. We conduct experiments on common large-scale WSI datasets and find it achieves better performance than other pre-training schemes (e.g., ImageNet pre-training and self-supervised learning) in different downstream tasks. We further show the compatibility and scalability of the proposed scheme by deploying it in fine-tuning the pathological-specific models and pre-training on merged multiple datasets. To our knowledge, this is the first work focusing on the representation learning for MIL.","authors":["Yicheng Song","Tiancheng Lin","Die Peng","Su Yang","Yi Xu"],"url":"https://arxiv.org/abs/2505.06710"}
{"created":"2025-05-13","title":"Perspectives on Unsolvability in Roommates Markets","abstract":"In the well-studied Stable Roommates problem, we seek a stable matching of agents into pairs, where no two agents prefer each other over their assigned partners. However, some instances of this problem are unsolvable, lacking any stable matching. A long-standing open question posed by Gusfield and Irving (1989) asks about the behavior of the probability function Pn, which measures the likelihood that a random instance with n agents is solvable.","authors":["Frederik Glitzner","David Manlove"],"url":"https://arxiv.org/abs/2505.06717"}
{"created":"2025-05-13","title":"Modelling and Study of t , Peak and Effective Diameter in Temporal Networks","abstract":"Understanding how information, diseases, or influence spread across networks is a fundamental challenge in complex systems. While network diameter has been extensively studied in static networks, its definition and behavior in temporal networks remain underexplored due to their dynamic nature. In this study, we present a formal mathematical framework for analyzing diameter in temporal networks and introduce three time-aware metrics: Effective Diameter , Peak Diameter (*D), and t-Diameter (tD), each capturing distinct temporal aspects of connectivity and diffusion. Our approach combines theoretical analysis with empirical validation using four real-world datasets: high school, hospital, conference, and workplace contact networks. We simulate flow propagation on temporal networks and compare the observed diameters with the proposed theoretical Equations. Across all datasets, our model demonstrates high accuracy, with low RMSE and absolute error values. Furthermore, we observe that the effective diameter decreases with increasing average degree and increases with network size. The results also show that tD and *D are more sensitive to node removal, highlighting their relevance for applications such as epidemic modeling. By bridging formal modeling and empirical data, our framework offers new insights into the temporal dynamics of networked systems and provides tools for assessing robustness, controlling information spread, and optimizing interventions in time-sensitive environments.","authors":["Zahra Farahi","Ali Kamandi","Ali Moeini"],"url":"https://arxiv.org/abs/2505.06719"}
{"created":"2025-05-13","title":"Behind the Byline: A Large-Scale Study of Scientific Author Contributions","abstract":"Understanding how co-authors distribute credit is critical for accurately assessing scholarly collaboration. In this study, we uncover the implicit structures within scientific teamwork by systematically analyzing author contributions across a large corpus of research publications. We introduce a computational framework designed to convert free-text contribution statements into 14 standardized CRediT categories, identifying clear and consistent positional patterns in task assignments. By analyzing over 400,000 scientific articles from prominent sources such as PLOS One and Nature, we extracted and standardized more than 5.6 million author-task assignments corresponding to 1.58 million author mentions. Our analysis reveals substantial disparities in workload distribution. Notably, in small teams with three co-authors, the most engaged contributor performs over three times more tasks than the least engaged, a disparity that grows linearly with team size. This demonstrates a consistent pattern of central and peripheral roles within modern collaborative teams. Moreover, our analysis shows distinct positional biases in task allocation: technical responsibilities, such as software development and formal analysis, broadly fall to authors positioned earlier in the author list, whereas managerial tasks, including supervision and funding acquisition, increasingly concentrate among authors positioned toward the end. This gradient underscores a significant division of labor, where early-listed authors mainly undertake most hands-on activities. In contrast, senior authors mostly assume roles involving leadership and oversight. Our findings highlight the structured and hierarchical organization within scholarly collaborations, providing deeper insights into the specific roles and dynamics that govern academic teamwork","authors":["Itai Assraf","Michael Fire"],"url":"https://arxiv.org/abs/2505.06721"}
{"created":"2025-05-13","title":"On Finding Randomly Planted Cliques in Arbitrary Graphs","abstract":"We study a planted clique model introduced by Feige where a complete graph of size $c\\cdot n$ is planted uniformly at random in an arbitrary $n$-vertex graph. We give a simple deterministic algorithm that, in almost linear time, recovers a clique of size $(c/3)^{O(1/c)} \\cdot n$ as long as the original graph has maximum degree at most $(1-p)n$ for some fixed $p>0$. The proof hinges on showing that the degrees of the final graph are correlated with the planted clique, in a way similar to (but more intricate than) the classical $G(n,\\frac{1}{2})+K_{\\sqrt{n}}$ planted clique model. Our algorithm suggests a separation from the worst-case model, where, assuming the Unique Games Conjecture, no polynomial algorithm can find cliques of size $\\Omega(n)$ for every fixed $c>0$, even if the input graph has maximum degree $(1-p)n$. Our techniques extend beyond the planted clique model. For example, when the planted graph is a balanced biclique, we recover a balanced biclique of size larger than the best guarantees known for the worst case.","authors":["Francesco Agrimonti","Marco Bressan","Tommaso d'Orsi"],"url":"https://arxiv.org/abs/2505.06725"}
{"created":"2025-05-13","title":"Modeling PFAS in Semiconductor Manufacturing to Quantify Trade-offs in Energy Efficiency and Environmental Impact of Computing Systems","abstract":"The electronics and semiconductor industry is a prominent consumer of per- and poly-fluoroalkyl substances (PFAS), also known as forever chemicals. PFAS are persistent in the environment and can bioaccumulate to ecological and human toxic levels. Computer designers have an opportunity to reduce the use of PFAS in semiconductors and electronics manufacturing, including integrated circuits (IC), batteries, displays, etc., which currently account for a staggering 10% of the total PFAS fluoropolymers usage in Europe alone. In this paper, we present a framework where we (1) quantify the environmental impact of PFAS in computing systems manufacturing with granular consideration of the metal layer stack and patterning complexities in IC manufacturing at the design phase, (2) identify contending trends between embodied carbon (carbon footprint due to hardware manufacturing) versus PFAS. For example, manufacturing an IC at a 7 nm technology node using EUV lithography uses 18% less PFAS-containing layers, compared to manufacturing the same IC at a 7 nm technology node using DUV immersion lithography (instead of EUV) unlike embodied carbon trends, and (3) conduct case studies to illustrate how to optimize and trade-off designs with lower PFAS, while meeting power-performance-area constraints. We show that optimizing designs to use less back-end-of-line (BEOL) metal stack layers can save 1.7$\\times$ PFAS-containing layers in systolic arrays.","authors":["Mariam Elgamal","Abdulrahman Mahmoud","Gu-Yeon Wei","David Brooks","Gage Hills"],"url":"https://arxiv.org/abs/2505.06727"}
{"created":"2025-05-13","title":"Regular mixed-radix DFT matrix factorization for in-place FFT accelerators","abstract":"The generic vector memory based accelerator is considered which supports DIT and DIF FFT with fixed datapath. The regular mixed-radix factorization of the DFT matrix coherent with the accelerator architecture is proposed and the correction proof is presented. It allows better understanding of architecture requirements and simplifies the developing and proving correctness of more complicated algorithms and conflict-free addressing schemes.","authors":["Sergey Salishev"],"url":"https://arxiv.org/abs/2505.06728"}
{"created":"2025-05-13","title":"STRIVE: Structured Representation Integrating VLM Reasoning for Efficient Object Navigation","abstract":"Vision-Language Models (VLMs) have been increasingly integrated into object navigation tasks for their rich prior knowledge and strong reasoning abilities. However, applying VLMs to navigation poses two key challenges: effectively representing complex environment information and determining \\textit{when and how} to query VLMs. Insufficient environment understanding and over-reliance on VLMs (e.g. querying at every step) can lead to unnecessary backtracking and reduced navigation efficiency, especially in continuous environments. To address these challenges, we propose a novel framework that constructs a multi-layer representation of the environment during navigation. This representation consists of viewpoint, object nodes, and room nodes. Viewpoints and object nodes facilitate intra-room exploration and accurate target localization, while room nodes support efficient inter-room planning. Building on this representation, we propose a novel two-stage navigation policy, integrating high-level planning guided by VLM reasoning with low-level VLM-assisted exploration to efficiently locate a goal object. We evaluated our approach on three simulated benchmarks (HM3D, RoboTHOR, and MP3D), and achieved state-of-the-art performance on both the success rate ($\\mathord{\\uparrow}\\, 7.1\\%$) and navigation efficiency ($\\mathord{\\uparrow}\\, 12.5\\%$). We further validate our method on a real robot platform, demonstrating strong robustness across 15 object navigation tasks in 10 different indoor environments. Project page is available at https://zwandering.github.io/STRIVE.github.io/ .","authors":["Haokun Zhu","Zongtai Li","Zhixuan Liu","Wenshan Wang","Ji Zhang","Jonathan Francis","Jean Oh"],"url":"https://arxiv.org/abs/2505.06729"}
{"created":"2025-05-13","title":"Activity and Subject Detection for UCI HAR Dataset with & without missing Sensor Data","abstract":"Current studies in Human Activity Recognition (HAR) primarily focus on the classification of activities through sensor data, while there is not much emphasis placed on recognizing the individuals performing these activities. This type of classification is very important for developing personalized and context-sensitive applications. Additionally, the issue of missing sensor data, which often occurs in practical situations due to hardware malfunctions, has not been explored yet. This paper seeks to fill these voids by introducing a lightweight LSTM-based model that can be used to classify both activities and subjects. The proposed model was used to classify the HAR dataset by UCI [1], achieving an accuracy of 93.89% in activity recognition (across six activities), nearing the 96.67% benchmark, and an accuracy of 80.19% in subject recognition (involving 30 subjects), thereby establishing a new baseline for this area of research. We then simulate the absence of sensor data to mirror real-world scenarios and incorporate imputation techniques, both with and without Principal Component Analysis (PCA), to restore incomplete datasets. We found that K-Nearest Neighbors (KNN) imputation performs the best for filling the missing sensor data without PCA because the use of PCA resulted in slightly lower accuracy. These results demonstrate how well the framework handles missing sensor data, which is a major step forward in using the Human Activity Recognition dataset for reliable classification tasks.","authors":["Debashish Saha","Piyush Malik","Adrika Saha"],"url":"https://arxiv.org/abs/2505.06730"}
{"created":"2025-05-13","title":"Deeply Explainable Artificial Neural Network","abstract":"While deep learning models have demonstrated remarkable success in numerous domains, their black-box nature remains a significant limitation, especially in critical fields such as medical image analysis and inference. Existing explainability methods, such as SHAP, LIME, and Grad-CAM, are typically applied post hoc, adding computational overhead and sometimes producing inconsistent or ambiguous results. In this paper, we present the Deeply Explainable Artificial Neural Network (DxANN), a novel deep learning architecture that embeds explainability ante hoc, directly into the training process. Unlike conventional models that require external interpretation methods, DxANN is designed to produce per-sample, per-feature explanations as part of the forward pass. Built on a flow-based framework, it enables both accurate predictions and transparent decision-making, and is particularly well-suited for image-based tasks. While our focus is on medical imaging, the DxANN architecture is readily adaptable to other data modalities, including tabular and sequential data. DxANN marks a step forward toward intrinsically interpretable deep learning, offering a practical solution for applications where trust and accountability are essential.","authors":["David Zucker"],"url":"https://arxiv.org/abs/2505.06731"}
{"created":"2025-05-13","title":"Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving","abstract":"Reinforcement Learning (RL) is a promising approach for achieving autonomous driving due to robust decision-making capabilities. RL learns a driving policy through trial and error in traffic scenarios, guided by a reward function that combines the driving objectives. The design of such reward function has received insufficient attention, yielding ill-defined rewards with various pitfalls. Safety, in particular, has long been regarded only as a penalty for collisions. This leaves the risks associated with actions leading up to a collision unaddressed, limiting the applicability of RL in real-world scenarios. To address these shortcomings, our work focuses on enhancing the reward formulation by defining a set of driving objectives and structuring them hierarchically. Furthermore, we discuss the formulation of these objectives in a normalized manner to transparently determine their contribution to the overall reward. Additionally, we introduce a novel risk-aware objective for various driving interactions based on a two-dimensional ellipsoid function and an extension of Responsibility-Sensitive Safety (RSS) concepts. We evaluate the efficacy of our proposed reward in unsignalized intersection scenarios with varying traffic densities. The approach decreases collision rates by 21\\% on average compared to baseline rewards and consistently surpasses them in route progress and cumulative reward, demonstrating its capability to promote safer driving behaviors while maintaining high-performance levels.","authors":["Ahmed Abouelazm","Jonas Michel","Helen Gremmelmaier","Tim Joseph","Philip Sch\\\"orner","J. Marius Z\\\"ollner"],"url":"https://arxiv.org/abs/2505.06737"}
{"created":"2025-05-13","title":"I Know What You Said: Unveiling Hardware Cache Side-Channels in Local Large Language Model Inference","abstract":"Large Language Models (LLMs) that can be deployed locally have recently gained popularity for privacy-sensitive tasks, with companies such as Meta, Google, and Intel playing significant roles in their development. However, the security of local LLMs through the lens of hardware cache side-channels remains unexplored. In this paper, we unveil novel side-channel vulnerabilities in local LLM inference: token value and token position leakage, which can expose both the victim's input and output text, thereby compromising user privacy. Specifically, we found that adversaries can infer the token values from the cache access patterns of the token embedding operation, and deduce the token positions from the timing of autoregressive decoding phases. To demonstrate the potential of these leaks, we design a novel eavesdropping attack framework targeting both open-source and proprietary LLM inference systems. The attack framework does not directly interact with the victim's LLM and can be executed without privilege.","authors":["Zibo Gao","Junjie Hu","Feng Guo","Yixin Zhang","Yinglong Han","Siyuan Liu","Haiyang Li","Zhiqiang Lv"],"url":"https://arxiv.org/abs/2505.06738"}
{"created":"2025-05-13","title":"Boundary-Guided Trajectory Prediction for Road Aware and Physically Feasible Autonomous Driving","abstract":"Accurate prediction of surrounding road users' trajectories is essential for safe and efficient autonomous driving. While deep learning models have improved performance, challenges remain in preventing off-road predictions and ensuring kinematic feasibility. Existing methods incorporate road-awareness modules and enforce kinematic constraints but lack plausibility guarantees and often introduce trade-offs in complexity and flexibility. This paper proposes a novel framework that formulates trajectory prediction as a constrained regression guided by permissible driving directions and their boundaries. Using the agent's current state and an HD map, our approach defines the valid boundaries and ensures on-road predictions by training the network to learn superimposed paths between left and right boundary polylines. To guarantee feasibility, the model predicts acceleration profiles that determine the vehicle's travel distance along these paths while adhering to kinematic constraints. We evaluate our approach on the Argoverse-2 dataset against the HPTR baseline. Our approach shows a slight decrease in benchmark metrics compared to HPTR but notably improves final displacement error and eliminates infeasible trajectories. Moreover, the proposed approach has superior generalization to less prevalent maneuvers and unseen out-of-distribution scenarios, reducing the off-road rate under adversarial attacks from 66\\% to just 1\\%. These results highlight the effectiveness of our approach in generating feasible and robust predictions.","authors":["Ahmed Abouelazm","Mianzhi Liu","Christian Hubschneider","Yin Wu","Daniel Slieter","J. Marius Z\\\"ollner"],"url":"https://arxiv.org/abs/2505.06740"}
{"created":"2025-05-13","title":"TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility","abstract":"Trajectory prediction is crucial for autonomous driving, enabling vehicles to navigate safely by anticipating the movements of surrounding road users. However, current deep learning models often lack trustworthiness as their predictions can be physically infeasible and illogical to humans. To make predictions more trustworthy, recent research has incorporated prior knowledge, like the social force model for modeling interactions and kinematic models for physical realism. However, these approaches focus on priors that suit either vehicles or pedestrians and do not generalize to traffic with mixed agent classes. We propose incorporating interaction and kinematic priors of all agent classes--vehicles, pedestrians, and cyclists with class-specific interaction layers to capture agent behavioral differences. To improve the interpretability of the agent interactions, we introduce DG-SFM, a rule-based interaction importance score that guides the interaction layer. To ensure physically feasible predictions, we proposed suitable kinematic models for all agent classes with a novel pedestrian kinematic model. We benchmark our approach on the Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our baseline. Experiments demonstrate that our method improves interaction interpretability, revealing a correlation between incorrect predictions and divergence from our interaction prior. Even though incorporating the kinematic models causes a slight decrease in accuracy, they eliminate infeasible trajectories found in the dataset and the baseline model. Thus, our approach fosters trust in trajectory prediction as its interaction reasoning is interpretable, and its predictions adhere to physics.","authors":["Marius Baden","Ahmed Abouelazm","Christian Hubschneider","Yin Wu","Daniel Slieter","J. Marius Z\\\"ollner"],"url":"https://arxiv.org/abs/2505.06743"}
{"created":"2025-05-13","title":"LineFlow: A Framework to Learn Active Control of Production Lines","abstract":"Many production lines require active control mechanisms, such as adaptive routing, worker reallocation, and rescheduling, to maintain optimal performance. However, designing these control systems is challenging for various reasons, and while reinforcement learning (RL) has shown promise in addressing these challenges, a standardized and general framework is still lacking. In this work, we introduce LineFlow, an extensible, open-source Python framework for simulating production lines of arbitrary complexity and training RL agents to control them. To demonstrate the capabilities and to validate the underlying theoretical assumptions of LineFlow, we formulate core subproblems of active line control in ways that facilitate mathematical analysis. For each problem, we provide optimal solutions for comparison. We benchmark state-of-the-art RL algorithms and show that the learned policies approach optimal performance in well-understood scenarios. However, for more complex, industrial-scale production lines, RL still faces significant challenges, highlighting the need for further research in areas such as reward shaping, curriculum learning, and hierarchical control.","authors":["Kai M\\\"uller","Martin Wenzel","Tobias Windisch"],"url":"https://arxiv.org/abs/2505.06744"}
{"created":"2025-05-13","title":"Symbolic Rule Extraction from Attention-Guided Sparse Representations in Vision Transformers","abstract":"Recent neuro-symbolic approaches have successfully extracted symbolic rule-sets from CNN-based models to enhance interpretability. However, applying similar techniques to Vision Transformers (ViTs) remains challenging due to their lack of modular concept detectors and reliance on global self-attention mechanisms. We propose a framework for symbolic rule extraction from ViTs by introducing a sparse concept layer inspired by Sparse Autoencoders (SAEs). This linear layer operates on attention-weighted patch representations and learns a disentangled, binarized representation in which individual neurons activate for high-level visual concepts. To encourage interpretability, we apply a combination of L1 sparsity, entropy minimization, and supervised contrastive loss. These binarized concept activations are used as input to the FOLD-SE-M algorithm, which generates a rule-set in the form of logic programs. Our method achieves a 5.14% better classification accuracy than the standard ViT while enabling symbolic reasoning. Crucially, the extracted rule-set is not merely post-hoc but acts as a logic-based decision layer that operates directly on the sparse concept representations. The resulting programs are concise and semantically meaningful. This work is the first to extract executable logic programs from ViTs using sparse symbolic representations. It bridges the gap between transformer-based vision models and symbolic logic programming, providing a step forward in interpretable and verifiable neuro-symbolic AI.","authors":["Parth Padalkar","Gopal Gupta"],"url":"https://arxiv.org/abs/2505.06745"}
{"created":"2025-05-13","title":"M3CAD: Towards Generic Cooperative Autonomous Driving Benchmark","abstract":"We introduce M$^3$CAD, a novel benchmark designed to advance research in generic cooperative autonomous driving. M$^3$CAD comprises 204 sequences with 30k frames, spanning a diverse range of cooperative driving scenarios. Each sequence includes multiple vehicles and sensing modalities, e.g., LiDAR point clouds, RGB images, and GPS/IMU, supporting a variety of autonomous driving tasks, including object detection and tracking, mapping, motion forecasting, occupancy prediction, and path planning. This rich multimodal setup enables M$^3$CAD to support both single-vehicle and multi-vehicle autonomous driving research, significantly broadening the scope of research in the field. To our knowledge, M$^3$CAD is the most comprehensive benchmark specifically tailored for cooperative multi-task autonomous driving research. We evaluate the state-of-the-art end-to-end solution on M$^3$CAD to establish baseline performance. To foster cooperative autonomous driving research, we also propose E2EC, a simple yet effective framework for cooperative driving solution that leverages inter-vehicle shared information for improved path planning. We release M$^3$CAD, along with our baseline models and evaluation results, to support the development of robust cooperative autonomous driving systems. All resources will be made publicly available on https://github.com/zhumorui/M3CAD","authors":["Morui Zhu","Yongqi Zhu","Yihao Zhu","Qi Chen","Deyuan Qu","Song Fu","Qing Yang"],"url":"https://arxiv.org/abs/2505.06746"}
{"created":"2025-05-13","title":"DPolicy: Managing Privacy Risks Across Multiple Releases with Differential Privacy","abstract":"Differential Privacy (DP) has emerged as a robust framework for privacy-preserving data releases and has been successfully applied in high-profile cases, such as the 2020 US Census. However, in organizational settings, the use of DP remains largely confined to isolated data releases. This approach restricts the potential of DP to serve as a framework for comprehensive privacy risk management at an organizational level. Although one might expect that the cumulative privacy risk of isolated releases could be assessed using DP's compositional property, in practice, individual DP guarantees are frequently tailored to specific releases, making it difficult to reason about their interaction or combined impact. At the same time, less tailored DP guarantees, which compose more easily, also offer only limited insight because they lead to excessively large privacy budgets that convey limited meaning. To address these limitations, we present DPolicy, a system designed to manage cumulative privacy risks across multiple data releases using DP. Unlike traditional approaches that treat each release in isolation or rely on a single (global) DP guarantee, our system employs a flexible framework that considers multiple DP guarantees simultaneously, reflecting the diverse contexts and scopes typical of real-world DP deployments. DPolicy introduces a high-level policy language to formalize privacy guarantees, making traditionally implicit assumptions on scopes and contexts explicit. By deriving the DP guarantees required to enforce complex privacy semantics from these high-level policies, DPolicy enables fine-grained privacy risk management on an organizational scale. We implement and evaluate DPolicy, demonstrating how it mitigates privacy risks that can emerge without comprehensive, organization-wide privacy risk management.","authors":["Nicolas K\\\"uchler","Alexander Viand","Hidde Lycklama","Anwar Hithnawi"],"url":"https://arxiv.org/abs/2505.06747"}
{"created":"2025-05-13","title":"Learned IMU Bias Prediction for Invariant Visual Inertial Odometry","abstract":"Autonomous mobile robots operating in novel environments depend critically on accurate state estimation, often utilizing visual and inertial measurements. Recent work has shown that an invariant formulation of the extended Kalman filter improves the convergence and robustness of visual-inertial odometry by utilizing the Lie group structure of a robot's position, velocity, and orientation states. However, inertial sensors also require measurement bias estimation, yet introducing the bias in the filter state breaks the Lie group symmetry. In this paper, we design a neural network to predict the bias of an inertial measurement unit (IMU) from a sequence of previous IMU measurements. This allows us to use an invariant filter for visual inertial odometry, relying on the learned bias prediction rather than introducing the bias in the filter state. We demonstrate that an invariant multi-state constraint Kalman filter (MSCKF) with learned bias predictions achieves robust visual-inertial odometry in real experiments, even when visual information is unavailable for extended periods and the system needs to rely solely on IMU measurements.","authors":["Abdullah Altawaitan","Jason Stanley","Sambaran Ghosal","Thai Duong","Nikolay Atanasov"],"url":"https://arxiv.org/abs/2505.06748"}
{"created":"2025-05-13","title":"AI-CDA4All: Democratizing Cooperative Autonomous Driving for All Drivers via Affordable Dash-cam Hardware and Open-source AI Software","abstract":"As transportation technology advances, the demand for connected vehicle infrastructure has greatly increased to improve their efficiency and safety. One area of advancement, Cooperative Driving Automation (CDA) still relies on expensive autonomy sensors or connectivity units and are not interoperable across existing market car makes/models, limiting its scalability on public roads. To fill these gaps, this paper presents a novel approach to democratizing CDA technology, it leverages low-cost, commercially available edge devices such as vehicle dash-cams and open-source software to make the technology accessible and scalable to be used in transportation infrastructure and broader public domains. This study also investigates the feasibility of utilizing cost-effective communication protocols based on LTE and WiFi. These technologies enable lightweight Vehicle-to-Everything (V2X) communications, facilitating real-time data exchange between vehicles and infrastructure. Our research and development efforts are aligned with industrial standards to ensure compatibility and future integration into existing transportation ecosystems. By prioritizing infrastructure-oriented applications, such as improved traffic flow management, this approach seeks to deliver tangible societal benefits without directly competing with vehicle OEMs. As recent advancement of Generative AI (GenAI), there is no standardized integration of GenAI technologies into open-source CDAs, as the current trends of muiltimodal large language models gain popularity, we demonstrated a feasible locally deployed edge LLM models can enhance driving experience while preserving privacy and security compared to cloud-connected solutions. The proposed system underscores the potential of low-cost, scalable solutions in advancing CDA functionality, paving the way for smarter, safer, and more inclusive transportation networks.","authors":["Shengming Yuan","Hao Zhou"],"url":"https://arxiv.org/abs/2505.06749"}
{"created":"2025-05-13","title":"LPrL: An Asynchronous Linear Time Hyper Logic","abstract":"We present a novel asynchronous hyper linear time temporal logic named LPrL (Linear Time Predicate Logic) and establish its basic theory. LPrL is a natural first order extension of LTL (Linear time temporal logic), in which the predicates specify the properties of and the relationships between traces (infinite sequences of actions) using Boolean combinations of LTL formulas. To augment the expressive power of the logic, we introduce a simple language of terms and add the equality predicate t = t' where t and t' are terms. We first illustrate how a number of the security policies as well as a basic consistency property of distributed processes can be captured using LPrL. We then establish our main results using automata theoretic techniques. Namely, the satisfiability and model checking problems for LPrL can be solved in elementary time. These results are in sharp contrast to HyperLTL, the prevalent synchronous hyper linear time logic, whose satisfiability problem is undecidable and whose model checking problem has non-elementary time complexity.","authors":["Parasara Sridhar Duggirala","P. S. Thiagarajan"],"url":"https://arxiv.org/abs/2505.06750"}
{"created":"2025-05-13","title":"Boltzmann Classifier: A Thermodynamic-Inspired Approach to Supervised Learning","abstract":"We propose a novel classification algorithm, the Boltzmann Classifier, inspired by the thermodynamic principles underlying the Boltzmann distribution. Our method computes a probabilistic estimate for each class based on an energy function derived from feature-wise deviations between input samples and class-specific centroids. The resulting probabilities are proportional to the exponential negative energies, normalized across classes, analogous to the Boltzmann distribution used in statistical mechanics. In addition, the KT variable can be used to allow the high energy states to be more accessible, which allows the tuning of their probabilities as needed. We evaluate the model performance on several datasets from different applications. The model achieves a high accuracy, which indicates that the Boltzmann Classifier is competitive with standard models like logistic regression and k-nearest neighbors while offering a thermodynamically motivated probabilistic interpretation. our classifier does not require iterative optimization or backpropagation and is thus computationally efficient and easy to integrate into existing workflows. This work demonstrates how ideas from physics can inform new directions in machine learning, providing a foundation for interpretable, energy-based decision-making systems.","authors":["Muhamed Amin","Bernard R. Brooks"],"url":"https://arxiv.org/abs/2505.06753"}
{"created":"2025-05-13","title":"8 Years of Optimizing Apache Otava: How disconnected open source developers took an algorithm from n3 to constant time","abstract":"As the project now known as Apache Otava (incubating) makes it first release, we look back over the past 8 years that the codebase was developed by a rather uncoordinated, loosely connected group of performance engineers at MongoDB, Datastax, Confluent, Nyrkio and others. Ever since the first publication (Daly 2020), developers of the code base now known as Apache Otava (incubating), have continuosly improved its performance. Even when a contributor's primary motivation was to add functionality, it seems like they couldn't help themselves but to also make some performance optimizations while at it. When developing the Nyrkio web service to provide change detection for performance testing, we have observed that Otava had become fast enough that it was almost feasible to compute change points synchronously, as the user is browsing test results in a web browser. Inspired by this, we have developed and contributed a new optimization for the common case where new data points are appended to the end of the series. This is now the 7th generation of performance optimizations in Otava. These improvements have been done over the past 8 years of development, by disconnected individuals at different employees. Taken together, the historical optimizations and those published in this paper, represent a 18 000 to 300 000 speedup over the original by the book implementation of (Matteson and James 2014). In the language of computational complexity, an evolution from O (n3) to O (1) (constant time). The ability to compute and recompute change points in real-time unlocks new opportunities in the user experience.","authors":["Henrik Ingo"],"url":"https://arxiv.org/abs/2505.06758"}
{"created":"2025-05-13","title":"Privacy-aware Berrut Approximated Coded Computing applied to general distributed learning","abstract":"Coded computing is one of the techniques that can be used for privacy protection in Federated Learning. However, most of the constructions used for coded computing work only under the assumption that the computations involved are exact, generally restricted to special classes of functions, and require quantized inputs. This paper considers the use of Private Berrut Approximate Coded Computing (PBACC) as a general solution to add strong but non-perfect privacy to federated learning. We derive new adapted PBACC algorithms for centralized aggregation, secure distributed training with centralized data, and secure decentralized training with decentralized data, thus enlarging significantly the applications of the method and the existing privacy protection tools available for these paradigms. Particularly, PBACC can be used robustly to attain privacy guarantees in decentralized federated learning for a variety of models. Our numerical results show that the achievable quality of different learning models (convolutional neural networks, variational autoencoders, and Cox regression) is minimally altered by using these new computing schemes, and that the privacy leakage can be bounded strictly to less than a fraction of one bit per participant. Additionally, the computational cost of the encoding and decoding processes depends only of the degree of decentralization of the data.","authors":["Xavier Mart\\'inez-Lua\\~na","Manuel Fern\\'andez-Veiga","Rebeca P. D\\'iaz-Redondo","Ana Fern\\'andez-Vilas"],"url":"https://arxiv.org/abs/2505.06759"}
{"created":"2025-05-13","title":"Learning Graph Representation of Agent Diffuser","abstract":"Diffusion-based generative models have significantly advanced text-to-image synthesis, demonstrating impressive text comprehension and zero-shot generalization. These models refine images from random noise based on textual prompts, with initial reliance on text input shifting towards enhanced visual fidelity over time. This transition suggests that static model parameters might not optimally address the distinct phases of generation. We introduce LGR-AD (Learning Graph Representation of Agent Diffusers), a novel multi-agent system designed to improve adaptability in dynamic computer vision tasks. LGR-AD models the generation process as a distributed system of interacting agents, each representing an expert sub-model. These agents dynamically adapt to varying conditions and collaborate through a graph neural network that encodes their relationships and performance metrics. Our approach employs a coordination mechanism based on top-$k$ maximum spanning trees, optimizing the generation process. Each agent's decision-making is guided by a meta-model that minimizes a novel loss function, balancing accuracy and diversity. Theoretical analysis and extensive empirical evaluations show that LGR-AD outperforms traditional diffusion models across various benchmarks, highlighting its potential for scalable and flexible solutions in complex image generation tasks. Code is available at: https://github.com/YousIA/LGR_AD","authors":["Youcef Djenouri","Nassim Belmecheri","Tomasz Michalak","Jan Dubi\\'nski","Ahmed Nabil Belbachir","Anis Yazidi"],"url":"https://arxiv.org/abs/2505.06761"}
{"created":"2025-05-13","title":"Investigating Robotaxi Crash Severity Using Geographical Random Forest","abstract":"This paper quantitatively investigates the crash severity of Autonomous Vehicles (AVs) with spatially localized machine learning and macroscopic measures of the urban built environment. We address spatial heterogeneity and spatial autocorrelation, while focusing on land use patterns and human behavior. Our Geographical Random Forest (GRF) model, accompanied with a crash severity risk map of San Francisco, presents three findings that are useful for commercial operations of AVs and robotaxis. First, spatially localized machine learning performed better than regular machine learning, when predicting AV crash severity. Bias-variance tradeoff was evident as we adjust the localization weight hyperparameter. Second, land use was the most important built environment measure, compared to intersections, building footprints, public transit stops, and Points Of Interests (POIs). Third, it was predicted that city center areas with greater diversity and commercial activities were more likely to result in low-severity AV crashes, than residential neighborhoods. Residential land use may be associated with higher severity due to human behavior and less restrictive environment. This paper recommends to explicitly consider geographic locations, and to design safety measures specific to residential neighborhoods, when robotaxi operators train their AV systems.","authors":["Junfeng Jiao","Seung Gyu Baik","Seung Jun Choi","Yiming Xu"],"url":"https://arxiv.org/abs/2505.06762"}
{"created":"2025-05-13","title":"Improving 5G/B5G Network Performance with RFID-Enabled Resource Management Systems","abstract":"In the rapidly evolving landscape of 5G and B5G (beyond 5G) networks, efficient resource optimization is critical to addressing the escalating demands for high-speed, low-latency, and energy efficient communication. This study explores the integration of Radio Frequency Identification (RFID) technology as a novel approach to enhance resource management in 5G/B5G networks. The motivation behind this research lies in overcoming persistent challenges such as spectrum congestion, high latency, and inefficient load balancing, which impede the performance of traditional resource allocation methods. To achieve this, RFID tags were embedded in critical network components, including user devices, base stations, and Internet of Things (IoT) nodes, enabling the collection of real-time data on device status, location, and resource utilization. RFID readers strategically placed across the network continuously captured this data, which was processed by a centralized controller using a custom-designed optimization algorithm. This algorithm dynamically managed key network resources, including spectrum allocation, load balancing, and energy consumption, ensuring efficient operation under varying network conditions. Simulations were conducted to evaluate the performance of the RFID-based model against traditional 4G dynamic resource allocation techniques. The results demonstrated substantial improvements in key performance metrics.","authors":["Stella N. Arinze","Halima I. Kure","Augustine O. Nwajana"],"url":"https://arxiv.org/abs/2505.06764"}
{"created":"2025-05-13","title":"Control Barrier Functions With Real-Time Gaussian Process Modeling","abstract":"We present an approach for satisfying state constraints in systems with nonparametric uncertainty by estimating this uncertainty with a real-time-update Gaussian process (GP) model. Notably, new data is incorporated into the model in real time as it is obtained and select old data is removed from the model. This update process helps improve the model estimate while keeping the model size (memory required) and computational complexity fixed. We present a recursive formulation for the model update, which reduces time complexity of the update from O(p3) to O(p2), where p is the number of data used. The GP model includes a computable upper bound on the model error. Together, the model and upper bound are used to construct a control-barrier-function (CBF) constraint that guarantees state constraints are satisfied.","authors":["Ricardo Gutierrez","Jesse B. Hoagg"],"url":"https://arxiv.org/abs/2505.06765"}
{"created":"2025-05-13","title":"Beyond Identity: A Generalizable Approach for Deepfake Audio Detection","abstract":"Deepfake audio presents a growing threat to digital security, due to its potential for social engineering, fraud, and identity misuse. However, existing detection models suffer from poor generalization across datasets, due to implicit identity leakage, where models inadvertently learn speaker-specific features instead of manipulation artifacts. To the best of our knowledge, this is the first study to explicitly analyze and address identity leakage in the audio deepfake detection domain. This work proposes an identity-independent audio deepfake detection framework that mitigates identity leakage by encouraging the model to focus on forgery-specific artifacts instead of overfitting to speaker traits. Our approach leverages Artifact Detection Modules (ADMs) to isolate synthetic artifacts in both time and frequency domains, enhancing cross-dataset generalization. We introduce novel dynamic artifact generation techniques, including frequency domain swaps, time domain manipulations, and background noise augmentation, to enforce learning of dataset-invariant features. Extensive experiments conducted on ASVspoof2019, ADD 2022, FoR, and In-The-Wild datasets demonstrate that the proposed ADM-enhanced models achieve F1 scores of 0.230 (ADD 2022), 0.604 (FoR), and 0.813 (In-The-Wild), consistently outperforming the baseline. Dynamic Frequency Swap proves to be the most effective strategy across diverse conditions. These findings emphasize the value of artifact-based learning in mitigating implicit identity leakage for more generalizable audio deepfake detection.","authors":["Yasaman Ahmadiadli","Xiao-Ping Zhang","Naimul Khan"],"url":"https://arxiv.org/abs/2505.06766"}
{"created":"2025-05-13","title":"Value Iteration with Guessing for Markov Chains and Markov Decision Processes","abstract":"Two standard models for probabilistic systems are Markov chains (MCs) and Markov decision processes (MDPs). Classic objectives for such probabilistic models for control and planning problems are reachability and stochastic shortest path. The widely studied algorithmic approach for these problems is the Value Iteration (VI) algorithm which iteratively applies local updates called Bellman updates. There are many practical approaches for VI in the literature but they all require exponentially many Bellman updates for MCs in the worst case. A preprocessing step is an algorithm that is discrete, graph-theoretical, and requires linear space. An important open question is whether, after a polynomial-time preprocessing, VI can be achieved with sub-exponentially many Bellman updates. In this work, we present a new approach for VI based on guessing values. Our theoretical contributions are twofold. First, for MCs, we present an almost-linear-time preprocessing algorithm after which, along with guessing values, VI requires only subexponentially many Bellman updates. Second, we present an improved analysis of the speed of convergence of VI for MDPs. Finally, we present a practical algorithm for MDPs based on our new approach. Experimental results show that our approach provides a considerable improvement over existing VI-based approaches on several benchmark examples from the literature.","authors":["Krishnendu Chatterjee","Mahdi JafariRaviz","Raimundo Saona","Jakub Svoboda"],"url":"https://arxiv.org/abs/2505.06769"}
{"created":"2025-05-13","title":"JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes","abstract":"Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot reinforcement learning (MRRL) policies with realistic robot dynamics and safety constraints, supporting both parallelization and hardware acceleration. Our generalizable learning interface provides an easy-to-use integration with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a realistic robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation.","authors":["Shalin Anand Jain","Jiazhen Liu","Siva Kailas","Harish Ravichandar"],"url":"https://arxiv.org/abs/2505.06771"}
{"created":"2025-05-13","title":"FALCON: Learning Force-Adaptive Humanoid Loco-Manipulation","abstract":"Humanoid loco-manipulation holds transformative potential for daily service and industrial tasks, yet achieving precise, robust whole-body control with 3D end-effector force interaction remains a major challenge. Prior approaches are often limited to lightweight tasks or quadrupedal/wheeled platforms. To overcome these limitations, we propose FALCON, a dual-agent reinforcement-learning-based framework for robust force-adaptive humanoid loco-manipulation. FALCON decomposes whole-body control into two specialized agents: (1) a lower-body agent ensuring stable locomotion under external force disturbances, and (2) an upper-body agent precisely tracking end-effector positions with implicit adaptive force compensation. These two agents are jointly trained in simulation with a force curriculum that progressively escalates the magnitude of external force exerted on the end effector while respecting torque limits. Experiments demonstrate that, compared to the baselines, FALCON achieves 2x more accurate upper-body joint tracking, while maintaining robust locomotion under force disturbances and achieving faster training convergence. Moreover, FALCON enables policy training without embodiment-specific reward or curriculum tuning. Using the same training setup, we obtain policies that are deployed across multiple humanoids, enabling forceful loco-manipulation tasks such as transporting payloads (0-20N force), cart-pulling (0-100N), and door-opening (0-40N) in the real world.","authors":["Yuanhang Zhang","Yifu Yuan","Prajwal Gurunath","Tairan He","Shayegan Omidshafiei","Ali-akbar Agha-mohammadi","Marcell Vazquez-Chanlatte","Liam Pedersen","Guanya Shi"],"url":"https://arxiv.org/abs/2505.06776"}
{"created":"2025-05-13","title":"Work-in-Progress: Multi-Deadline DAG Scheduling Model for Autonomous Driving Systems","abstract":"Autoware is an autonomous driving system implemented on Robot Operation System (ROS) 2, where an end-to-end timing guarantee is crucial to ensure safety. However, existing ROS 2 cause-effect chain models for analyzing end-to-end latency struggle to accurately represent the complexities of Autoware, particularly regarding sync callbacks, queue consumption patterns, and feedback loops. To address these problems, we propose a new scheduling model that decomposes the end-to-end timing constraints of Autoware into local relative deadlines for each sub-DAG. This multi-deadline DAG scheduling model avoids the need for complex analysis of data flows through queues and loops, while ensuring that all callbacks receive data within correct intervals. Furthermore, we extend the Global Earliest Deadline First (GEDF) algorithm for the proposed model and evaluate its effectiveness using a synthetic workload derived from Autoware.","authors":["Atsushi Yano","Takuya Azumi"],"url":"https://arxiv.org/abs/2505.06780"}
{"created":"2025-05-13","title":"Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK","abstract":"Australia and the UK have developed contrasting approaches to the regulation of electronic cigarettes, with - broadly speaking - Australia adopting a relatively restrictive approach and the UK adopting a more permissive approach. Notably, these divergent policies were developed from the same broad evidence base. In this paper, to investigate differences in how the two jurisdictions manage and present evidence, we developed and evaluated a Large Language Model-based sentence classifier to perform automated analyses of electronic cigarette-related policy documents drawn from official Australian and UK legislative processes (109 documents in total). Specifically, we utilized GPT-4 to automatically classify sentences based on whether they contained claims that e-cigarettes were broadly helpful or harmful for public health. Our LLM-based classifier achieved an F-score of 0.9. Further, when applying the classifier to our entire sentence-level corpus, we found that Australian legislative documents show a much higher proportion of harmful statements, and a lower proportion of helpful statements compared to the expected values, with the opposite holding for the UK. In conclusion, this work utilized an LLM-based approach to provide evidence to support the contention that - drawing on the same evidence base - Australian ENDS-related policy documents emphasize the harms associated with ENDS products and UK policy documents emphasize the benefits. Further, our approach provides a starting point for using LLM-based methods to investigate the complex relationship between evidence and health policy formation.","authors":["Damian Curran","Brian Chapman","Mike Conway"],"url":"https://arxiv.org/abs/2505.06782"}
{"created":"2025-05-13","title":"Digital-physical testbed for ship autonomy studies in the Marine Cybernetics Laboratory basin","abstract":"The algorithms developed for Maritime Autonomous Surface Ships (MASS) are often challenging to test on actual vessels due to high operational costs and safety considerations. Simulations offer a cost-effective alternative and eliminate risks, but they may not accurately represent real-world dynamics for the given tasks. Utilizing small-scale model ships and robotic vessels in conjunction with a laboratory basin provides an accessible testing environment for the early stages of validation processes. However, designing and developing a model vessel for a single test can be costly and cumbersome, and often researchers lack availability to such infrastructure. To address these challenges and enable streamlined testing, we have developed an in-house testbed that facilitates the development, testing, verification, and validation of MASS algorithms in a digital-physical laboratory. This infrastructure includes a set of small-scale model vessels, a simulation environment for each vessel, a comprehensive testbed environment, and a digital twin in Unity. With this, we aim to establish a full design and verification pipeline that starts with high-fidelity simulation models of each model vessel, to the model-scale testing in the laboratory basin, allowing possibilities for moving to semi-fullscale validation with the R/V milliAmpere 1 passenger ferry and full-scale validation using the R/V Gunnerus. In this work, we present our progress on the development of this testbed environment and its components, demonstrating its effectiveness in enabling ship guidance, navigation, and control (GNC) including autonomy.","authors":["Emir Cem Gezer","Mael Korentin Ivan Moreau","Anders Sandneseng H{\\o}gden","Dong Trong Nguyen","Roger Skjetne","Asgeir S{\\o}rensen"],"url":"https://arxiv.org/abs/2505.06787"}
{"created":"2025-05-13","title":"Towards NWDAF-enabled Analytics and Closed-Loop Automation in 5G Networks","abstract":"The fifth generation of cellular technology (5G) delivers faster speeds, lower latency, and improved network service alongside support for a large number of users and a diverse range of verticals. This brings increased complexity to network control and management, making closed-loop automation essential. In response, the 3rd Generation Partnership Project (3GPP) introduced the Network Data Analytics Function (NWDAF) to streamline network monitoring by collecting, analyzing, and providing insights from network data. While prior research has focused mainly on isolated applications of machine learning within NWDAF, critical aspects such as standardized data collection, analytics integration in closed-loop automation, and end-to-end system evaluation have received limited attention. This work addresses existing gaps by presenting a practical implementation of NWDAF and its integration with leading open-source 5G core network solutions. We develop a 3GPP-compliant User Plane Function (UPF) event exposure service for real-time data collection and an ML model provisioning service integrated with MLflow to support end-to-end machine learning lifecycle management. Additionally, we enhance the Session Management Function (SMF) to consume NWDAF analytics and respond accordingly. Our evaluation demonstrates the solution's scalability, resource efficiency, and effectiveness in enabling closed-loop security management in 5G networks.","authors":["Fatemeh Shafiei Ardestani","Niloy Saha","Noura Limam","Raouf Boutaba"],"url":"https://arxiv.org/abs/2505.06789"}
{"created":"2025-05-13","title":"cpRRTC: GPU-Parallel RRT-Connect for Constrained Motion Planning","abstract":"Motion planning is a fundamental problem in robotics that involves generating feasible trajectories for a robot to follow. Recent advances in parallel computing, particularly through CPU and GPU architectures, have significantly reduced planning times to the order of milliseconds. However, constrained motion planning especially using sampling based methods on GPUs remains underexplored. Prior work such as pRRTC leverages a tracking compiler with a CUDA backend to accelerate forward kinematics and collision checking. While effective in simple settings, their approach struggles with increased complexity in robot models or environments. In this paper, we propose a novel GPU based framework utilizing NVRTC for runtime compilation, enabling efficient handling of high complexity scenarios and supporting constrained motion planning. Experimental results demonstrate that our method achieves superior performance compared to existing approaches.","authors":["Jiaming Hu","Jiawei Wang","Henrik Christensen"],"url":"https://arxiv.org/abs/2505.06791"}
{"created":"2025-05-13","title":"Dynamic Safety in Complex Environments: Synthesizing Safety Filters with Poisson's Equation","abstract":"Synthesizing safe sets for robotic systems operating in complex and dynamically changing environments is a challenging problem. Solving this problem can enable the construction of safety filters that guarantee safe control actions -- most notably by employing Control Barrier Functions (CBFs). This paper presents an algorithm for generating safe sets from perception data by leveraging elliptic partial differential equations, specifically Poisson's equation. Given a local occupancy map, we solve Poisson's equation subject to Dirichlet boundary conditions, with a novel forcing function. Specifically, we design a smooth guidance vector field, which encodes gradient information required for safety. The result is a variational problem for which the unique minimizer -- a safety function -- characterizes the safe set. After establishing our theoretical result, we illustrate how safety functions can be used in CBF-based safety filtering. The real-time utility of our synthesis method is highlighted through hardware demonstrations on quadruped and humanoid robots navigating dynamically changing obstacle-filled environments.","authors":["Gilbert Bahati","Ryan M. Bena","Aaron D. Ames"],"url":"https://arxiv.org/abs/2505.06794"}
{"created":"2025-05-13","title":"Decoding Futures Price Dynamics: A Regularized Sparse Autoencoder for Interpretable Multi-Horizon Forecasting and Factor Discovery","abstract":"Commodity price volatility creates economic challenges, necessitating accurate multi-horizon forecasting. Predicting prices for commodities like copper and crude oil is complicated by diverse interacting factors (macroeconomic, supply/demand, geopolitical, etc.). Current models often lack transparency, limiting strategic use. This paper presents a Regularized Sparse Autoencoder (RSAE), a deep learning framework for simultaneous multi-horizon commodity price prediction and discovery of interpretable latent market drivers. The RSAE forecasts prices at multiple horizons (e.g., 1-day, 1-week, 1-month) using multivariate time series. Crucially, L1 regularization ($\\|\\mathbf{z}\\|_1$) on its latent vector $\\mathbf{z}$ enforces sparsity, promoting parsimonious explanations of market dynamics through learned factors representing underlying drivers (e.g., demand, supply shocks). Drawing from energy-based models and sparse coding, the RSAE optimizes predictive accuracy while learning sparse representations. Evaluated on historical Copper and Crude Oil data with numerous indicators, our findings indicate the RSAE offers competitive multi-horizon forecasting accuracy and data-driven insights into price dynamics via its interpretable latent space, a key advantage over traditional black-box approaches.","authors":["Abhijit Gupta"],"url":"https://arxiv.org/abs/2505.06795"}
{"created":"2025-05-13","title":"Multimodal Fake News Detection: MFND Dataset and Shallow-Deep Multitask Learning","abstract":"Multimodal news contains a wealth of information and is easily affected by deepfake modeling attacks. To combat the latest image and text generation methods, we present a new Multimodal Fake News Detection dataset (MFND) containing 11 manipulated types, designed to detect and localize highly authentic fake news. Furthermore, we propose a Shallow-Deep Multitask Learning (SDML) model for fake news, which fully uses unimodal and mutual modal features to mine the intrinsic semantics of news. Under shallow inference, we propose the momentum distillation-based light punishment contrastive learning for fine-grained uniform spatial image and text semantic alignment, and an adaptive cross-modal fusion module to enhance mutual modal features. Under deep inference, we design a two-branch framework to augment the image and text unimodal features, respectively merging with mutual modalities features, for four predictions via dedicated detection and localization projections. Experiments on both mainstream and our proposed datasets demonstrate the superiority of the model. Codes and dataset are released at https://github.com/yunan-wang33/sdml.","authors":["Ye Zhu","Yunan Wang","Zitong Yu"],"url":"https://arxiv.org/abs/2505.06796"}
{"created":"2025-05-13","title":"Crypto-Economic Analysis of Web3 Funding Programs Using the Grant Maturity Framework","abstract":"Web3 grant programs are evolving mechanisms aimed at supporting innovation within the blockchain ecosystem, yet little is known on about their effectiveness. This paper proposes the concept of maturity to fill this gap and introduces the Grant Maturity Framework (GMF), a mixed-methods model for evaluating the maturity of Web3 grant programs. The GMF provides a systematic approach to assessing the structure, governance, and impact of Web3 grants, applied here to four prominent Ethereum layer-two (L2) grant programs: Arbitrum, Optimism, Mantle, and Taiko. By evaluating these programs using the GMF, the study categorizes them into four maturity stages, ranging from experimental to advanced. The findings reveal that Arbitrum's Long-Term Incentive Pilot Program (LTIPP) and Optimism's Mission Rounds show higher maturity, while Mantle and Taiko are still in their early stages. The research concludes by discussing the user-centric development of a Web3 grant management platform aimed at improving the maturity and effectiveness of Web3 grant management processes based on the findings from the GMF. This work contributes to both practical and theoretical knowledge on Web3 grant program evaluation and tooling, providing a valuable resource for Web3 grant operators and stakeholders.","authors":["Ben Biedermann","Victoria Kozlova","Fahima Gibrel"],"url":"https://arxiv.org/abs/2505.06801"}
{"created":"2025-05-13","title":"Bridging Ears and Eyes: Analyzing Audio and Visual Large Language Models to Humans in Visible Sound Recognition and Reducing Their Sensory Gap via Cross-Modal Distillation","abstract":"Audio large language models (LLMs) are considered experts at recognizing sound objects, yet their performance relative to LLMs in other sensory modalities, such as visual or audio-visual LLMs, and to humans using their ears, eyes, or both remains unexplored. To investigate this, we systematically evaluate audio, visual, and audio-visual LLMs, specifically Qwen2-Audio, Qwen2-VL, and Qwen2.5-Omni, against humans in recognizing sound objects of different classes from audio-only, silent video, or sounded video inputs. We uncover a performance gap between Qwen2-Audio and Qwen2-VL that parallels the sensory discrepancy between human ears and eyes. To reduce this gap, we introduce a cross-modal distillation framework, where an LLM in one modality serves as the teacher and another as the student, with knowledge transfer in sound classes predicted as more challenging to the student by a heuristic model. Distillation in both directions, from Qwen2-VL to Qwen2-Audio and vice versa, leads to notable improvements, particularly in challenging classes. This work highlights the sensory gap in LLMs from a human-aligned perspective and proposes a principled approach to enhancing modality-specific perception in multimodal LLMs.","authors":["Xilin Jiang","Junkai Wu","Vishal Choudhari","Nima Mesgarani"],"url":"https://arxiv.org/abs/2505.06803"}
{"created":"2025-05-13","title":"Topology Guidance: Controlling the Outputs of Generative Models via Vector Field Topology","abstract":"For domains that involve numerical simulation, it can be computationally expensive to run an ensemble of simulations spanning a parameter space of interest to a user. To this end, an attractive surrogate for simulation is the generative modeling of fields produced by an ensemble, allowing one to synthesize fields in a computationally cheap, yet accurate, manner. However, for the purposes of visual analysis, a limitation of generative models is their lack of control, as it is unclear what one should expect when sampling a field from a model. In this paper we study how to make generative models of fields more controllable, so that users can specify features of interest, in particular topological features, that they wish to see in the output. We propose topology guidance, a method for guiding the sampling process of a generative model, specifically a diffusion model, such that a topological description specified as input is satisfied in the generated output. Central to our method, we couple a coordinate-based neural network used to represent fields, with a diffusion model used for generation. We show how to use topologically-relevant signals provided by the coordinate-based network to help guide the denoising process of a diffusion model. This enables us to faithfully represent a user's specified topology, while ensuring that the output field remains within the generative data distribution. Specifically, we study 2D vector field topology, evaluating our method over an ensemble of fluid flows, where we show that generated vector fields faithfully adhere to the location, and type, of critical points over the spatial domain. We further show the benefits of our method in aiding the comparison of ensembles, allowing one to explore commonalities and differences in distributions along prescribed topological features.","authors":["Xiaohan Wang","Matthew Berger"],"url":"https://arxiv.org/abs/2505.06804"}
{"created":"2025-05-13","title":"QSeer: A Quantum-Inspired Graph Neural Network for Parameter Initialization in Quantum Approximate Optimization Algorithm Circuits","abstract":"To mitigate the barren plateau problem, effective parameter initialization is crucial for optimizing the Quantum Approximate Optimization Algorithm (QAOA) in the near-term Noisy Intermediate-Scale Quantum (NISQ) era. Prior physics-driven approaches leveraged the optimal parameter concentration phenomenon, utilizing medium values of previously optimized QAOA parameters stored in databases as initialization for new graphs. However, this medium-value-based strategy lacks generalization capability. Conversely, prior computer-science-based approaches employed graph neural networks (GNNs) trained on previously optimized QAOA parameters to predict initialization values for new graphs. However, these approaches neglect key physics-informed QAOA principles, such as parameter concentration, symmetry, and adiabatic evolution, resulting in suboptimal parameter predictions and limited performance improvements. Furthermore, no existing GNN-based methods support parameter initialization for QAOA circuits with variable depths or for solving weighted Max-Cut problems. This paper introduces QSeer, a quantum-inspired GNN designed for accurate QAOA parameter prediction. Compared to prior physics- and computer-science-driven methods, QSeer improves the initial approximation ratio and convergence speed of QAOA circuits across diverse graphs by 6%-68% and 5x-10x, respectively.","authors":["Lei Jiang","Chi Zhang","Fan Chen"],"url":"https://arxiv.org/abs/2505.06810"}
{"created":"2025-05-13","title":"Overview of the NLPCC 2025 Shared Task 4: Multi-modal, Multilingual, and Multi-hop Medical Instructional Video Question Answering Challenge","abstract":"Following the successful hosts of the 1-st (NLPCC 2023 Foshan) CMIVQA and the 2-rd (NLPCC 2024 Hangzhou) MMIVQA challenges, this year, a new task has been introduced to further advance research in multi-modal, multilingual, and multi-hop medical instructional question answering (M4IVQA) systems, with a specific focus on medical instructional videos. The M4IVQA challenge focuses on evaluating models that integrate information from medical instructional videos, understand multiple languages, and answer multi-hop questions requiring reasoning over various modalities. This task consists of three tracks: multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Single Video (M4TAGSV), multi-modal, multilingual, and multi-hop Video Corpus Retrieval (M4VCR) and multi-modal, multilingual, and multi-hop Temporal Answer Grounding in Video Corpus (M4TAGVC). Participants in M4IVQA are expected to develop algorithms capable of processing both video and text data, understanding multilingual queries, and providing relevant answers to multi-hop medical questions. We believe the newly introduced M4IVQA challenge will drive innovations in multimodal reasoning systems for healthcare scenarios, ultimately contributing to smarter emergency response systems and more effective medical education platforms in multilingual communities. Our official website is https://cmivqa.github.io/","authors":["Bin Li","Shenxi Liu","Yixuan Weng","Yue Du","Yuhang Tian","Shoujun Zhou"],"url":"https://arxiv.org/abs/2505.06814"}
{"created":"2025-05-13","title":"Control Plane as a Tool: A Scalable Design Pattern for Agentic AI Systems","abstract":"Agentic AI systems represent a new frontier in artificial intelligence, where agents often based on large language models(LLMs) interact with tools, environments, and other agents to accomplish tasks with a degree of autonomy. These systems show promise across a range of domains, but their architectural underpinnings remain immature. This paper conducts a comprehensive review of the types of agents, their modes of interaction with the environment, and the infrastructural and architectural challenges that emerge. We identify a gap in how these systems manage tool orchestration at scale and propose a reusable design abstraction: the \"Control Plane as a Tool\" pattern. This pattern allows developers to expose a single tool interface to an agent while encapsulating modular tool routing logic behind it. We position this pattern within the broader context of agent design and argue that it addresses several key challenges in scaling, safety, and extensibility.","authors":["Sivasathivel Kandasamy"],"url":"https://arxiv.org/abs/2505.06817"}
{"created":"2025-05-13","title":"Deep Learning for On-Street Parking Violation Prediction","abstract":"Illegal parking along with the lack of available parking spaces are among the biggest issues faced in many large cities. These issues can have a significant impact on the quality of life of citizens. On-street parking systems have been designed to this end aiming at ensuring that parking spaces will be available for the local population, while also providing easy access to parking for people visiting the city center. However, these systems are often affected by illegal parking, providing incorrect information regarding the availability of parking spaces. Even though this can be mitigated using sensors for detecting the presence of cars in various parking sectors, the cost of these implementations is usually prohibiting large. In this paper, we investigate an indirect way of predicting parking violations at a fine-grained level, equipping such parking systems with a valuable tool for providing more accurate information to citizens. To this end, we employed a Deep Learning (DL)-based model to predict fine-grained parking violation rates for on-street parking systems. Moreover, we developed a data augmentation and smoothing technique for further improving the accuracy of DL models under the presence of missing and noisy data. We demonstrate, using experiments on real data collected in Thessaloniki, Greece, that the developed system can indeed provide accurate parking violation predictions.","authors":["Thien Nhan Vo"],"url":"https://arxiv.org/abs/2505.06818"}
{"created":"2025-05-13","title":"New Wide Locally Recoverable Codes with Unified Locality","abstract":"Wide Locally Recoverable Codes (LRCs) have recently been proposed as a solution for achieving high reliability, good performance, and ultra-low storage cost in distributed storage systems. However, existing wide LRCs struggle to balance optimal fault tolerance and high availability during frequent system events. By analyzing the existing LRCs, we reveal three limitations in the LRC construction which lay behind the non-optimal overall performance from multiple perspectives, including non-minimum local recovery cost, non cluster-topology-aware data distribution, and non XOR-based local coding. Thanks to the flexible design space offered by the locality property of wide LRCs, we present UniLRC, which unifies locality considerations in code construction. UniLRC achieves the optimal fault tolerance while overcoming the revealed limitations. We implement UniLRC prototype and conduct comprehensive theoretical and system evaluations, showing significant improvements in reliability and performance over existing wide LRCs deployed in Google and Azure clusters.","authors":["Xu Liangliang","Tang Fengming","Chen Tingting","Li Qiliang","Lyu Min","Ge Gennian"],"url":"https://arxiv.org/abs/2505.06819"}
{"created":"2025-05-13","title":"ThreatLens: LLM-guided Threat Modeling and Test Plan Generation for Hardware Security Verification","abstract":"Current hardware security verification processes predominantly rely on manual threat modeling and test plan generation, which are labor-intensive, error-prone, and struggle to scale with increasing design complexity and evolving attack methodologies. To address these challenges, we propose ThreatLens, an LLM-driven multi-agent framework that automates security threat modeling and test plan generation for hardware security verification. ThreatLens integrates retrieval-augmented generation (RAG) to extract relevant security knowledge, LLM-powered reasoning for threat assessment, and interactive user feedback to ensure the generation of practical test plans. By automating these processes, the framework reduces the manual verification effort, enhances coverage, and ensures a structured, adaptable approach to security verification. We evaluated our framework on the NEORV32 SoC, demonstrating its capability to automate security verification through structured test plans and validating its effectiveness in real-world scenarios.","authors":["Dipayan Saha","Hasan Al Shaikh","Shams Tarek","Farimah Farahmandi"],"url":"https://arxiv.org/abs/2505.06821"}
{"created":"2025-05-13","title":"Hunting the Ghost: Towards Automatic Mining of IoT Hidden Services","abstract":"In this paper, we proposes an automatic firmware analysis tool targeting at finding hidden services that may be potentially harmful to the IoT devices. Our approach uses static analysis and symbolic execution to search and filter services that are transparent to normal users but explicit to experienced attackers. A prototype is built and evaluated against a dataset of IoT firmware, and The evaluation shows our tool can find the suspicious hidden services effectively.","authors":["Shuaike Dong","Siyu Shen","Zhou Li","Kehuan Zhang"],"url":"https://arxiv.org/abs/2505.06822"}
{"created":"2025-05-13","title":"Active Learning for Multi-class Image Classification","abstract":"A principle bottleneck in image classification is the large number of training examples needed to train a classifier. Using active learning, we can reduce the number of training examples to teach a CNN classifier by strategically selecting examples. Assigning values to image examples using different uncertainty metrics allows the model to identify and select high-value examples in a smaller training set size. We demonstrate results for digit recognition and fruit classification on the MNIST and Fruits360 data sets. We formally compare results for four different uncertainty metrics. Finally, we observe active learning is also effective on simpler (binary) classification tasks, but marked improvement from random sampling is more evident on more difficult tasks. We show active learning is a viable algorithm for image classification problems.","authors":["Thien Nhan Vo"],"url":"https://arxiv.org/abs/2505.06825"}
{"created":"2025-05-13","title":"Sandcastles in the Storm: Revisiting the (Im)possibility of Strong Watermarking","abstract":"Watermarking AI-generated text is critical for combating misuse. Yet recent theoretical work argues that any watermark can be erased via random walk attacks that perturb text while preserving quality. However, such attacks rely on two key assumptions: (1) rapid mixing (watermarks dissolve quickly under perturbations) and (2) reliable quality preservation (automated quality oracles perfectly guide edits). Through large-scale experiments and human-validated assessments, we find mixing is slow: 100% of perturbed texts retain traces of their origin after hundreds of edits, defying rapid mixing. Oracles falter, as state-of-the-art quality detectors misjudge edits (77% accuracy), compounding errors during attacks. Ultimately, attacks underperform: automated walks remove watermarks just 26% of the time -- dropping to 10% under human quality review. These findings challenge the inevitability of watermark removal. Instead, practical barriers -- slow mixing and imperfect quality control -- reveal watermarking to be far more robust than theoretical models suggest. The gap between idealized attacks and real-world feasibility underscores the need for stronger watermarking methods and more realistic attack models.","authors":["Fabrice Y Harel-Canada","Boran Erol","Connor Choi","Jason Liu","Gary Jiarui Song","Nanyun Peng","Amit Sahai"],"url":"https://arxiv.org/abs/2505.06827"}
{"created":"2025-05-13","title":"An Improved Algorithm for a Bipartite Traveling Tournament in Interleague Sports Scheduling","abstract":"The bipartite traveling tournament problem (BTTP) addresses inter-league sports scheduling, which aims to design a feasible bipartite tournament between two $n$-team leagues under some constraints such that the total traveling distance of all participating teams is minimized. Since its introduction, several methods have been developed to design feasible schedules for NBA, NPB and so on. In terms of solution quality with a theoretical guarantee, previously only a $(2+\\varepsilon)$-approximation is known for the case that $n\\equiv 0 \\pmod 3$. Whether there are similar results for the cases that $n\\equiv 1 \\pmod 3$ and $n\\equiv 2 \\pmod 3$ was asked in the literature. In this paper, we answer this question positively by proposing a $(3/2+\\varepsilon)$-approximation algorithm for any $n$ and any constant $\\varepsilon>0$, which also improves the previous approximation ratio for the case that $n\\equiv 0 \\pmod 3$.","authors":["Jingyang Zhao","Mingyu Xiao"],"url":"https://arxiv.org/abs/2505.06828"}
{"created":"2025-05-13","title":"Fine-Grained Bias Exploration and Mitigation for Group-Robust Classification","abstract":"Achieving group-robust generalization in the presence of spurious correlations remains a significant challenge, particularly when bias annotations are unavailable. Recent studies on Class-Conditional Distribution Balancing (CCDB) reveal that spurious correlations often stem from mismatches between the class-conditional and marginal distributions of bias attributes. They achieve promising results by addressing this issue through simple distribution matching in a bias-agnostic manner. However, CCDB approximates each distribution using a single Gaussian, which is overly simplistic and rarely holds in real-world applications. To address this limitation, we propose a novel method called Bias Exploration via Overfitting (BEO), which captures each distribution in greater detail by modeling it as a mixture of latent groups. Building on these group-level descriptions, we introduce a fine-grained variant of CCDB, termed FG-CCDB, which performs more precise distribution matching and balancing within each group. Through group-level reweighting, FG-CCDB learns sample weights from a global perspective, achieving stronger mitigation of spurious correlations without incurring substantial storage or computational costs. Extensive experiments demonstrate that BEO serves as a strong proxy for ground-truth bias annotations and can be seamlessly integrated with bias-supervised methods. Moreover, when combined with FG-CCDB, our method performs on par with bias-supervised approaches on binary classification tasks and significantly outperforms them in highly biased multi-class scenarios.","authors":["Miaoyun Zhao","Qiang Zhang","Chenrong Li"],"url":"https://arxiv.org/abs/2505.06831"}
{"created":"2025-05-13","title":"UniDiffGrasp: A Unified Framework Integrating VLM Reasoning and VLM-Guided Part Diffusion for Open-Vocabulary Constrained Grasping with Dual Arms","abstract":"Open-vocabulary, task-oriented grasping of specific functional parts, particularly with dual arms, remains a key challenge, as current Vision-Language Models (VLMs), while enhancing task understanding, often struggle with precise grasp generation within defined constraints and effective dual-arm coordination. We innovatively propose UniDiffGrasp, a unified framework integrating VLM reasoning with guided part diffusion to address these limitations. UniDiffGrasp leverages a VLM to interpret user input and identify semantic targets (object, part(s), mode), which are then grounded via open-vocabulary segmentation. Critically, the identified parts directly provide geometric constraints for a Constrained Grasp Diffusion Field (CGDF) using its Part-Guided Diffusion, enabling efficient, high-quality 6-DoF grasps without retraining. For dual-arm tasks, UniDiffGrasp defines distinct target regions, applies part-guided diffusion per arm, and selects stable cooperative grasps. Through extensive real-world deployment, UniDiffGrasp achieves grasp success rates of 0.876 in single-arm and 0.767 in dual-arm scenarios, significantly surpassing existing state-of-the-art methods, demonstrating its capability to enable precise and coordinated open-vocabulary grasping in complex real-world scenarios.","authors":["Xueyang Guo","Hongwei Hu","Chengye Song","Jiale Chen","Zilin Zhao","Yu Fu","Bowen Guan","Zhenze Liu"],"url":"https://arxiv.org/abs/2505.06832"}
{"created":"2025-05-13","title":"Streaming Sliced Optimal Transport","abstract":"Sliced optimal transport (SOT) or sliced Wasserstein (SW) distance is widely recognized for its statistical and computational scalability. In this work, we further enhance the computational scalability by proposing the first method for computing SW from sample streams, called \\emph{streaming sliced Wasserstein} (Stream-SW). To define Stream-SW, we first introduce the streaming computation of the one-dimensional Wasserstein distance. Since the one-dimensional Wasserstein (1DW) distance has a closed-form expression, given by the absolute difference between the quantile functions of the compared distributions, we leverage quantile approximation techniques for sample streams to define the streaming 1DW distance. By applying streaming 1DW to all projections, we obtain Stream-SW. The key advantage of Stream-SW is its low memory complexity while providing theoretical guarantees on the approximation error. We demonstrate that Stream-SW achieves a more accurate approximation of SW than random subsampling, with lower memory consumption, in comparing Gaussian distributions and mixtures of Gaussians from streaming samples. Additionally, we conduct experiments on point cloud classification, point cloud gradient flows, and streaming change point detection to further highlight the favorable performance of Stream-SW.","authors":["Khai Nguyen"],"url":"https://arxiv.org/abs/2505.06835"}
{"created":"2025-05-13","title":"\"Explain, Don't Just Warn!\" -- A Real-Time Framework for Generating Phishing Warnings with Contextual Cues","abstract":"Anti-phishing tools typically display generic warnings that offer users limited explanation on why a website is considered malicious, which can prevent end-users from developing the mental models needed to recognize phishing cues on their own. This becomes especially problematic when these tools inevitably fail - particularly against evasive threats, and users are found to be ill-equipped to identify and avoid them independently. To address these limitations, we present PhishXplain (PXP), a real-time explainable phishing warning system designed to augment existing detection mechanisms. PXP empowers users by clearly articulating why a site is flagged as malicious, highlighting suspicious elements using a memory-efficient implementation of LLaMA 3.2. It utilizes a structured two-step prompt architecture to identify phishing features, generate contextual explanations, and render annotated screenshots that visually reinforce the warning. Longitudinally implementing PhishXplain over a month on 7,091 live phishing websites, we found that it can generate warnings for 94% of the sites, with a correctness of 96%. We also evaluated PhishXplain through a user study with 150 participants split into two groups: one received conventional, generic warnings, while the other interacted with PXP's explainable alerts. Participants who received the explainable warnings not only demonstrated a significantly better understanding of phishing indicators but also achieved higher accuracy in identifying phishing threats, even without any warning. Moreover, they reported greater satisfaction and trust in the warnings themselves. These improvements were especially pronounced among users with lower initial levels of cybersecurity proficiency and awareness. To encourage the adoption of this framework, we release PhishXplain as an open-source browser extension.","authors":["Sayak Saha Roy","Cesar Torres","Shirin Nilizadeh"],"url":"https://arxiv.org/abs/2505.06836"}
{"created":"2025-05-13","title":"The power of fine-grained experts: Granularity boosts expressivity in Mixture of Experts","abstract":"Mixture-of-Experts (MoE) layers are increasingly central to frontier model architectures. By selectively activating parameters, they reduce computational cost while scaling total parameter count. This paper investigates the impact of the number of active experts, termed granularity, comparing architectures with many (e.g., 8 per layer in DeepSeek) to those with fewer (e.g., 1 per layer in Llama-4 models). We prove an exponential separation in network expressivity based on this design parameter, suggesting that models benefit from higher granularity. Experimental results corroborate our theoretical findings and illustrate this separation.","authors":["Enric Boix-Adsera","Philippe Rigollet"],"url":"https://arxiv.org/abs/2505.06839"}
{"created":"2025-05-13","title":"Visual Instruction Tuning with Chain of Region-of-Interest","abstract":"High-resolution (HR) images are pivotal for enhancing the recognition and understanding capabilities of multimodal large language models (MLLMs). However, directly increasing image resolution can significantly escalate computational demands. In this study, we propose a method called Chain of Region-of-Interest (CoRoI) for Visual Instruction Tuning, aimed at alleviating the computational burden associated with high-resolution images for MLLMs. Drawing inspiration from the selective nature of the human visual system, we recognize that not all regions within high-resolution images carry equal importance. CoRoI seeks to identify and prioritize the most informative regions, thereby enhancing multimodal visual comprehension and recognition while circumventing the need for processing lengthy HR image tokens. Through extensive experiments on 11 benchmarks, we validate the efficacy of CoRoI across varying sizes, ranging from 7B to 34B in parameters. Our models consistently demonstrate superior performance across diverse multimodal benchmarks and tasks. Notably, our method outperforms LLaVA-NeXT on almost all benchmarks and our finetuned 34B model surpasses proprietary methods like Gemini Pro 1.0 on six benchmarks, as well as outperforming GPT-4V on MMB, SEED-I, and MME.","authors":["Yixin Chen","Shuai Zhang","Boran Han","Bernie Wang"],"url":"https://arxiv.org/abs/2505.06840"}
{"created":"2025-05-13","title":"Optimizing Recommendations using Fine-Tuned LLMs","abstract":"As digital media platforms strive to meet evolving user expectations, delivering highly personalized and intuitive movies and media recommendations has become essential for attracting and retaining audiences. Traditional systems often rely on keyword-based search and recommendation techniques, which limit users to specific keywords and a combination of keywords. This paper proposes an approach that generates synthetic datasets by modeling real-world user interactions, creating complex chat-style data reflective of diverse preferences. This allows users to express more information with complex preferences, such as mood, plot details, and thematic elements, in addition to conventional criteria like genre, title, and actor-based searches. In today's search space, users cannot write queries like ``Looking for a fantasy movie featuring dire wolves, ideally set in a harsh frozen world with themes of loyalty and survival.''","authors":["Prabhdeep Cheema","Erhan Guven"],"url":"https://arxiv.org/abs/2505.06841"}
{"created":"2025-05-13","title":"Secure Safety Filter Design for Sampled-data Nonlinear Systems under Sensor Spoofing Attacks","abstract":"This paper presents a secure safety filter design for nonlinear systems under sensor spoofing attacks. Existing approaches primarily focus on linear systems which limits their applications in real-world scenarios. In this work, we extend these results to nonlinear systems in a principled way. We introduce exact observability maps that abstract specific state estimation algorithms and extend them to a secure version capable of handling sensor attacks. Our generalization also applies to the relaxed observability case, with slightly relaxed guarantees. More importantly, we propose a secure safety filter design in both exact and relaxed cases, which incorporates secure state estimation and a control barrier function-enabled safety filter. The proposed approach provides theoretical safety guarantees for nonlinear systems in the presence of sensor attacks. We numerically validate our analysis on a unicycle vehicle equipped with redundant yet partly compromised sensors.","authors":["Xiao Tan","Pio Ong","Paulo Tabuada","Aaron D. Ames"],"url":"https://arxiv.org/abs/2505.06842"}
{"created":"2025-05-13","title":"Benign Samples Matter! Fine-tuning On Outlier Benign Samples Severely Breaks Safety","abstract":"Recent studies have uncovered a troubling vulnerability in the fine-tuning stage of large language models (LLMs): even fine-tuning on entirely benign datasets can lead to a significant increase in the harmfulness of LLM outputs. Building on this finding, our red teaming study takes this threat one step further by developing a more effective attack. Specifically, we analyze and identify samples within benign datasets that contribute most to safety degradation, then fine-tune LLMs exclusively on these samples. We approach this problem from an outlier detection perspective and propose Self-Inf-N, to detect and extract outliers for fine-tuning. Our findings reveal that fine-tuning LLMs on 100 outlier samples selected by Self-Inf-N in the benign datasets severely compromises LLM safety alignment. Extensive experiments across seven mainstream LLMs demonstrate that our attack exhibits high transferability across different architectures and remains effective in practical scenarios. Alarmingly, our results indicate that most existing mitigation strategies fail to defend against this attack, underscoring the urgent need for more robust alignment safeguards. Codes are available at https://github.com/GuanZihan/Benign-Samples-Matter.","authors":["Zihan Guan","Mengxuan Hu","Ronghang Zhu","Sheng Li","Anil Vullikanti"],"url":"https://arxiv.org/abs/2505.06843"}
{"created":"2025-05-13","title":"Secure Safety Filter: Towards Safe Flight Control under Sensor Attacks","abstract":"Modern autopilot systems are prone to sensor attacks that can jeopardize flight safety. To mitigate this risk, we proposed a modular solution: the secure safety filter, which extends the well-established control barrier function (CBF)-based safety filter to account for, and mitigate, sensor attacks. This module consists of a secure state reconstructor (which generates plausible states) and a safety filter (which computes the safe control input that is closest to the nominal one). Differing from existing work focusing on linear, noise-free systems, the proposed secure safety filter handles bounded measurement noise and, by leveraging reduced-order model techniques, is applicable to the nonlinear dynamics of drones. Software-in-the-loop simulations and drone hardware experiments demonstrate the effectiveness of the secure safety filter in rendering the system safe in the presence of sensor attacks.","authors":["Xiao Tan","Junior Sundar","Renzo Bruzzone","Pio Ong","Willian T. Lunardi","Martin Andreoni","Paulo Tabuada","Aaron D. Ames"],"url":"https://arxiv.org/abs/2505.06845"}
{"created":"2025-05-13","title":"Image processing Application Development on Software Configurable Processor Array","abstract":"The software configurable processor finds best use in the embedded systems. These processors have onchip logic like FPGA (Field Programmable Gate Array) and thus can be configured to implement custom hardware functionality. The digital computing tasks that need to accelerate in hardware can be compiled down to a configuration file or bit stream that contains the information on how the logic components are configured and wired together. Video and image processing applications perform repeated pixel transformation and thus consume more power and the processing time. Software configurable processor best accelerates such compute intensive applications. This paper mainly focuses on the implementation of an image processing application called median filtering on a single processor and colour conversion algorithm on array of SCPs(Software Configurable Processors). Median filtering result on Digital Video Recorder (DVR) is discussed as a realtime application of SCP.","authors":["Ganesh Prabhu","Steevan Rodrigues","Niranjan Chiplunkar","Niranjan U. C"],"url":"https://arxiv.org/abs/2505.06847"}
{"created":"2025-05-13","title":"Predictive Digital Twins for Thermal Management Using Machine Learning and Reduced-Order Models","abstract":"Digital twins enable real-time simulation and prediction in engineering systems. This paper presents a novel framework for predictive digital twins of a headlamp heatsink, integrating physics-based reduced-order models (ROMs) from computational fluid dynamics (CFD) with supervised machine learning. A component-based ROM library, derived via proper orthogonal decomposition (POD), captures thermal dynamics efficiently. Machine learning models, including Decision Trees, k-Nearest Neighbors, Support Vector Regression (SVR), and Neural Networks, predict optimal ROM configurations, enabling rapid digital twin updates. The Neural Network achieves a mean absolute error (MAE) of 54.240, outperforming other models. Quantitative comparisons of predicted and original values demonstrate high accuracy. This scalable, interpretable framework advances thermal management in automotive systems, supporting robust design and predictive maintenance.","authors":["Tamilselvan Subramani","Sebastian Bartscher"],"url":"https://arxiv.org/abs/2505.06849"}
{"created":"2025-05-13","title":"Visual Evolutionary Optimization on Combinatorial Problems with Multimodal Large Language Models: A Case Study of Influence Maximization","abstract":"Graph-structured combinatorial problems in complex networks are prevalent in many domains, and are computationally demanding due to their complexity and non-linear nature. Traditional evolutionary algorithms (EAs), while robust, often face obstacles due to content-shallow encoding limitations and lack of structural awareness, necessitating hand-crafted modifications for effective application. In this work, we introduce an original framework, Visual Evolutionary Optimization (VEO), leveraging multimodal large language models (MLLMs) as the backbone evolutionary optimizer in this context. Specifically, we propose a context-aware encoding way, representing the solution of the network as an image. In this manner, we can utilize MLLMs' image processing capabilities to intuitively comprehend network configurations, thus enabling machines to solve these problems in a human-like way. We have developed MLLM-based operators tailored for various evolutionary optimization stages, including initialization, crossover, and mutation. Furthermore, we propose that graph sparsification can effectively enhance the applicability and scalability of VEO on large-scale networks, owing to the scale-free nature of real-world networks. We demonstrate the effectiveness of our method using a well-known task in complex networks, influence maximization, and validate it on eight different real-world networks of various structures. The results have confirmed VEO's reliability and enhanced effectiveness compared to traditional evolutionary optimization.","authors":["Jie Zhao","Kang Hao Cheong"],"url":"https://arxiv.org/abs/2505.06850"}
{"created":"2025-05-13","title":"Improving Random Forests by Smoothing","abstract":"Gaussian process regression is a popular model in the small data regime due to its sound uncertainty quantification and the exploitation of the smoothness of the regression function that is encountered in a wide range of practical problems. However, Gaussian processes perform sub-optimally when the degree of smoothness is non-homogeneous across the input domain. Random forest regression partially addresses this issue by providing local basis functions of variable support set sizes that are chosen in a data-driven way. However, they do so at the expense of forgoing any degree of smoothness, which often results in poor performance in the small data regime. Here, we aim to combine the advantages of both models by applying a kernel-based smoothing mechanism to a learned random forest or any other piecewise constant prediction function. As we demonstrate empirically, the resulting model consistently improves the predictive performance of the underlying random forests and, in almost all test cases, also improves the log loss of the usual uncertainty quantification based on inter-tree variance. The latter advantage can be attributed to the ability of the smoothing model to take into account the uncertainty over the exact tree-splitting locations.","authors":["Ziyi Liu","Phuc Luong","Mario Boley","Daniel F. Schmidt"],"url":"https://arxiv.org/abs/2505.06852"}
{"created":"2025-05-13","title":"Predicting Surgical Safety Margins in Osteosarcoma Knee Resections: An Unsupervised Approach","abstract":"According to the Pan American Health Organization, the number of cancer cases in Latin America was estimated at 4.2 million in 2022 and is projected to rise to 6.7 million by 2045. Osteosarcoma, one of the most common and deadly bone cancers affecting young people, is difficult to detect due to its unique texture and intensity. Surgical removal of osteosarcoma requires precise safety margins to ensure complete resection while preserving healthy tissue. Therefore, this study proposes a method for estimating the confidence interval of surgical safety margins in osteosarcoma surgery around the knee. The proposed approach uses MRI and X-ray data from open-source repositories, digital processing techniques, and unsupervised learning algorithms (such as k-means clustering) to define tumor boundaries. Experimental results highlight the potential for automated, patient-specific determination of safety margins.","authors":["Carolina Vargas-Ecos","Edwin Salcedo"],"url":"https://arxiv.org/abs/2505.06853"}
{"created":"2025-05-13","title":"Joint Low-level and High-level Textual Representation Learning with Multiple Masking Strategies","abstract":"Most existing text recognition methods are trained on large-scale synthetic datasets due to the scarcity of labeled real-world datasets. Synthetic images, however, cannot faithfully reproduce real-world scenarios, such as uneven illumination, irregular layout, occlusion, and degradation, resulting in performance disparities when handling complex real-world images. Recent self-supervised learning techniques, notably contrastive learning and masked image modeling (MIM), narrow this domain gap by exploiting unlabeled real text images. This study first analyzes the original Masked AutoEncoder (MAE) and observes that random patch masking predominantly captures low-level textural features but misses high-level contextual representations. To fully exploit the high-level contextual representations, we introduce random blockwise and span masking in the text recognition task. These strategies can mask the continuous image patches and completely remove some characters, forcing the model to infer relationships among characters within a word. Our Multi-Masking Strategy (MMS) integrates random patch, blockwise, and span masking into the MIM frame, which jointly learns low and high-level textual representations. After fine-tuning with real data, MMS outperforms the state-of-the-art self-supervised methods in various text-related tasks, including text recognition, segmentation, and text-image super-resolution.","authors":["Zhengmi Tang","Yuto Mitsui","Tomo Miyazaki","Shinichiro Omachi"],"url":"https://arxiv.org/abs/2505.06855"}
{"created":"2025-05-13","title":"Beyond Patterns: Harnessing Causal Logic for Autonomous Driving Trajectory Prediction","abstract":"Accurate trajectory prediction has long been a major challenge for autonomous driving (AD). Traditional data-driven models predominantly rely on statistical correlations, often overlooking the causal relationships that govern traffic behavior. In this paper, we introduce a novel trajectory prediction framework that leverages causal inference to enhance predictive robustness, generalization, and accuracy. By decomposing the environment into spatial and temporal components, our approach identifies and mitigates spurious correlations, uncovering genuine causal relationships. We also employ a progressive fusion strategy to integrate multimodal information, simulating human-like reasoning processes and enabling real-time inference. Evaluations on five real-world datasets--ApolloScape, nuScenes, NGSIM, HighD, and MoCAD--demonstrate our model's superiority over existing state-of-the-art (SOTA) methods, with improvements in key metrics such as RMSE and FDE. Our findings highlight the potential of causal reasoning to transform trajectory prediction, paving the way for robust AD systems.","authors":["Bonan Wang","Haicheng Liao","Chengyue Wang","Bin Rao","Yanchen Guan","Guyang Yu","Jiaxun Zhang","Songning Lai","Chengzhong Xu","Zhenning Li"],"url":"https://arxiv.org/abs/2505.06856"}
{"created":"2025-05-13","title":"FreqMoE: Dynamic Frequency Enhancement for Neural PDE Solvers","abstract":"Fourier Neural Operators (FNO) have emerged as promising solutions for efficiently solving partial differential equations (PDEs) by learning infinite-dimensional function mappings through frequency domain transformations. However, the sparsity of high-frequency signals limits computational efficiency for high-dimensional inputs, and fixed-pattern truncation often causes high-frequency signal loss, reducing performance in scenarios such as high-resolution inputs or long-term predictions. To address these challenges, we propose FreqMoE, an efficient and progressive training framework that exploits the dependency of high-frequency signals on low-frequency components. The model first learns low-frequency weights and then applies a sparse upward-cycling strategy to construct a mixture of experts (MoE) in the frequency domain, effectively extending the learned weights to high-frequency regions. Experiments on both regular and irregular grid PDEs demonstrate that FreqMoE achieves up to 16.6% accuracy improvement while using merely 2.1% parameters (47.32x reduction) compared to dense FNO. Furthermore, the approach demonstrates remarkable stability in long-term predictions and generalizes seamlessly to various FNO variants and grid structures, establishing a new ``Low frequency Pretraining, High frequency Fine-tuning'' paradigm for solving PDEs.","authors":["Tianyu Chen","Haoyi Zhou","Ying Li","Hao Wang","Zhenzhe Zhang","Tianchen Zhu","Shanghang Zhang","Jianxin Li"],"url":"https://arxiv.org/abs/2505.06858"}
{"created":"2025-05-13","title":"DP-TRAE: A Dual-Phase Merging Transferable Reversible Adversarial Example for Image Privacy Protection","abstract":"In the field of digital security, Reversible Adversarial Examples (RAE) combine adversarial attacks with reversible data hiding techniques to effectively protect sensitive data and prevent unauthorized analysis by malicious Deep Neural Networks (DNNs). However, existing RAE techniques primarily focus on white-box attacks, lacking a comprehensive evaluation of their effectiveness in black-box scenarios. This limitation impedes their broader deployment in complex, dynamic environments. Further more, traditional black-box attacks are often characterized by poor transferability and high query costs, significantly limiting their practical applicability. To address these challenges, we propose the Dual-Phase Merging Transferable Reversible Attack method, which generates highly transferable initial adversarial perturbations in a white-box model and employs a memory augmented black-box strategy to effectively mislead target mod els. Experimental results demonstrate the superiority of our approach, achieving a 99.0% attack success rate and 100% recovery rate in black-box scenarios, highlighting its robustness in privacy protection. Moreover, we successfully implemented a black-box attack on a commercial model, further substantiating the potential of this approach for practical use.","authors":["Xia Du","Jiajie Zhu","Jizhe Zhou","Chi-man Pun","Zheng Lin","Cong Wu","Zhe Chen","Jun Luo"],"url":"https://arxiv.org/abs/2505.06860"}
{"created":"2025-05-13","title":"Efficient Robotic Policy Learning via Latent Space Backward Planning","abstract":"Current robotic planning methods often rely on predicting multi-frame images with full pixel details. While this fine-grained approach can serve as a generic world model, it introduces two significant challenges for downstream policy learning: substantial computational costs that hinder real-time deployment, and accumulated inaccuracies that can mislead action extraction. Planning with coarse-grained subgoals partially alleviates efficiency issues. However, their forward planning schemes can still result in off-task predictions due to accumulation errors, leading to misalignment with long-term goals. This raises a critical question: Can robotic planning be both efficient and accurate enough for real-time control in long-horizon, multi-stage tasks? To address this, we propose a Latent Space Backward Planning scheme (LBP), which begins by grounding the task into final latent goals, followed by recursively predicting intermediate subgoals closer to the current state. The grounded final goal enables backward subgoal planning to always remain aware of task completion, facilitating on-task prediction along the entire planning horizon. The subgoal-conditioned policy incorporates a learnable token to summarize the subgoal sequences and determines how each subgoal guides action extraction. Through extensive simulation and real-robot long-horizon experiments, we show that LBP outperforms existing fine-grained and forward planning methods, achieving SOTA performance. Project Page: https://lbp-authors.github.io","authors":["Dongxiu Liu","Haoyi Niu","Zhihao Wang","Jinliang Zheng","Yinan Zheng","Zhonghong Ou","Jianming Hu","Jianxiong Li","Xianyuan Zhan"],"url":"https://arxiv.org/abs/2505.06861"}
{"created":"2025-05-13","title":"A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting","abstract":"$\\texttt{BIGBIRD-PEGASUS}$ model achieves $\\textit{state-of-the-art}$ on abstractive text summarization for long documents. However it's capacity still limited to maximum of $4,096$ tokens, thus caused performance degradation on summarization for very long documents. Common method to deal with the issue is to truncate the documents. In this reasearch, we'll use different approach. We'll use the pretrained $\\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the model on other domain dataset. First, we filter out all documents which length less than $20,000$ tokens to focus on very long documents. To prevent domain shifting problem and overfitting on transfer learning due to small dataset, we augment the dataset by splitting document-summary training pair into parts, to fit the document into $4,096$ tokens. Source code available on $\\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.","authors":["Lhuqita Fazry"],"url":"https://arxiv.org/abs/2505.06862"}
{"created":"2025-05-13","title":"Masked Subspace Clustering Methods","abstract":"To further utilize the unsupervised features and pairwise information, we propose a general Bilevel Clustering Optimization (BCO) framework to improve the performance of clustering. And then we introduce three special cases on subspace clustering with two different types of masks. At first, we reformulate the original subspace clustering as a Basic Masked Subspace Clustering (BMSC), which reformulate the diagonal constraints to a hard mask. Then, we provide a General Masked Subspace Clustering (GMSC) method to integrate different clustering via a soft mask. Furthermore, based on BCO and GMSC, we induce a learnable soft mask and design a Recursive Masked Subspace Clustering (RMSC) method that can alternately update the affinity matrix and the soft mask. Numerical experiments show that our models obtain significant improvement compared with the baselines on several commonly used datasets, such as MNIST, USPS, ORL, COIL20 and COIL100.","authors":["Jiebo Song","Huaming Ling"],"url":"https://arxiv.org/abs/2505.06863"}
{"created":"2025-05-13","title":"Quantum preconditioning method for linear systems problems via Schr\\\"odingerization","abstract":"We present a quantum computational framework that systematically converts classical linear iterative algorithms with fixed iteration operators into their quantum counterparts using the Schr\\\"odingerization technique [Shi Jin, Nana Liu and Yue Yu, Phys. Rev. Lett., vol. 133 No. 230602,2024]. This is achieved by capturing the steady state of the associated differential equations. The Schr\\\"odingerization technique transforms linear partial and ordinary differential equations into Schr\\\"odinger-type systems, making them suitable for quantum computing. This is accomplished through the so-called warped phase transformation, which maps the equation into a higher-dimensional space. Building on this framework, we develop a quantum preconditioning algorithm that leverages the well-known BPX multilevel preconditioner for the finite element discretization of the Poisson equation. The algorithm achieves a near-optimal dependence on the number of queries to our established input models, with a complexity of $\\mathscr{O}(\\text{polylog} \\frac{1}{\\varepsilon})$ for a target accuracy of $\\varepsilon$ when the dimension $d\\geq 2$. This improvement results from the Hamiltonian simulation strategy applied to the Schr\\\"odingerized preconditioning dynamics, coupled with the smoothing of initial data in the extended space.","authors":["Shi Jin","Nana Liu","Chuwen Ma","Yue Yu"],"url":"https://arxiv.org/abs/2505.06866"}
{"created":"2025-05-13","title":"Rate-Matching Deep Polar Codes via Polar Coded Extension","abstract":"Deep polar codes are pre-transformed polar codes that employ a multi-layered polar kernel transformation strategy to enhance code performance in short blocklength regimes. However, like conventional polar codes, their block length is constrained to powers of two, as the final transformation layer uses a conventional polar kernel matrix. This paper introduces a novel rate-matching technique for deep polar codes using code extension, particularly effective when the desired code length slightly exceeds a power of two. The key idea is to exploit the layered structure of deep polar codes by concatenating polar codewords generated at each transformation layer. Based on this structure, we also develop an efficient decoding algorithm leveraging soft-output successive cancellation list decoding and provide comprehensive error probability analysis supporting our code design algorithms. Additionally, we propose a computationally efficient greedy algorithm for multi-layer configurations. Extensive simulations confirm that our approach delivers substantial coding gains over conventional rate-matching methods, especially in medium to high code-rate regimes.","authors":["Geon Choi","Namyoon Lee"],"url":"https://arxiv.org/abs/2505.06867"}
{"created":"2025-05-13","title":"Enhancing Time Series Forecasting via a Parallel Hybridization of ARIMA and Polynomial Classifiers","abstract":"Time series forecasting has attracted significant attention, leading to the de-velopment of a wide range of approaches, from traditional statistical meth-ods to advanced deep learning models. Among them, the Auto-Regressive Integrated Moving Average (ARIMA) model remains a widely adopted linear technique due to its effectiveness in modeling temporal dependencies in economic, industrial, and social data. On the other hand, polynomial classifi-ers offer a robust framework for capturing non-linear relationships and have demonstrated competitive performance in domains such as stock price pre-diction. In this study, we propose a hybrid forecasting approach that inte-grates the ARIMA model with a polynomial classifier to leverage the com-plementary strengths of both models. The hybrid method is evaluated on multiple real-world time series datasets spanning diverse domains. Perfor-mance is assessed based on forecasting accuracy and computational effi-ciency. Experimental results reveal that the proposed hybrid model consist-ently outperforms the individual models in terms of prediction accuracy, al-beit with a modest increase in execution time.","authors":["Thanh Son Nguyen","Van Thanh Nguyen","Dang Minh Duc Nguyen"],"url":"https://arxiv.org/abs/2505.06874"}
{"created":"2025-05-13","title":"Towards Human-Centric Autonomous Driving: A Fast-Slow Architecture Integrating Large Language Model Guidance with Reinforcement Learning","abstract":"Autonomous driving has made significant strides through data-driven techniques, achieving robust performance in standardized tasks. However, existing methods frequently overlook user-specific preferences, offering limited scope for interaction and adaptation with users. To address these challenges, we propose a \"fast-slow\" decision-making framework that integrates a Large Language Model (LLM) for high-level instruction parsing with a Reinforcement Learning (RL) agent for low-level real-time decision. In this dual system, the LLM operates as the \"slow\" module, translating user directives into structured guidance, while the RL agent functions as the \"fast\" module, making time-critical maneuvers under stringent latency constraints. By decoupling high-level decision making from rapid control, our framework enables personalized user-centric operation while maintaining robust safety margins. Experimental evaluations across various driving scenarios demonstrate the effectiveness of our method. Compared to baseline algorithms, the proposed architecture not only reduces collision rates but also aligns driving behaviors more closely with user preferences, thereby achieving a human-centric mode. By integrating user guidance at the decision level and refining it with real-time control, our framework bridges the gap between individual passenger needs and the rigor required for safe, reliable driving in complex traffic environments.","authors":["Chengkai Xu","Jiaqi Liu","Yicheng Guo","Yuhang Zhang","Peng Hang","Jian Sun"],"url":"https://arxiv.org/abs/2505.06875"}
{"created":"2025-05-13","title":"LAMMPS: A Case Study For Applying Modern Software Engineering to an Established Research Software Package","abstract":"We review various changes made in recent years to the software development process of the LAMMPS simulation software package and the software itself. We discuss how those changes have impacted the effort and workflow required to develop and maintain a software package that has been in existence for more than 30 years and where a significant part of the code base is contributed by external developers. We also look into how those changes have affected the code quality and ease of modifying and extending the software while at the same time its audience has changed from a cohort with a generally strong software development background to a group containing many researchers with limited software development skills. We explore how this contributes to LAMMPS' significant growth in popularity in that time. We close with an outlook on future steps.","authors":["Axel Kohlmeyer","Richard Berger"],"url":"https://arxiv.org/abs/2505.06877"}
{"created":"2025-05-13","title":"Benchmarking and Revisiting Code Generation Assessment: A Mutation-Based Approach","abstract":"Code Large Language Models (CLLMs) have exhibited outstanding performance in program synthesis, attracting the focus of the research community. The evaluation of CLLM's program synthesis capability has generally relied on manually curated benchmarks. However, there is a substantial gap between real-world scenarios and benchmark settings. Existing benchmarks typically provide only a single input prompt for the evaluation of each synthesis problem. However, in practice, a problem can be described in various ways, including with typos, where developers may struggle to understand certain descriptions and seek clarification to find more suitable wording. Such various descriptions may lead to variations in the performance of CLLMs on the same question, resulting in a biased evaluation when using existing benchmarks. In this paper, we aim to explore these pitfalls with the goal of revisiting and enhancing future benchmark designs. To simulate real-world variations in problem descriptions, we propose 10 mutation strategies and introduce three new metrics to evaluate their impact on code generation. We then assess five popular CLLMs using 12,834 generated prompt variants, and found a significant performance discrepancy between the results from existing benchmarks and those from mutated benchmarks containing perturbations and variations. This finding underscores the need for more robust evaluation methods and benchmarks.","authors":["Longtian Wang","Tianlin Li","Xiaofei Xie","Yuhan Zhi","Jian Wang","Chao Shen"],"url":"https://arxiv.org/abs/2505.06880"}
{"created":"2025-05-13","title":"NeuRN: Neuro-inspired Domain Generalization for Image Classification","abstract":"Domain generalization in image classification is a crucial challenge, with models often failing to generalize well across unseen datasets. We address this issue by introducing a neuro-inspired Neural Response Normalization (NeuRN) layer which draws inspiration from neurons in the mammalian visual cortex, which aims to enhance the performance of deep learning architectures on unseen target domains by training deep learning models on a source domain. The performance of these models is considered as a baseline and then compared against models integrated with NeuRN on image classification tasks. We perform experiments across a range of deep learning architectures, including ones derived from Neural Architecture Search and Vision Transformer. Additionally, in order to shortlist models for our experiment from amongst the vast range of deep neural networks available which have shown promising results, we also propose a novel method that uses the Needleman-Wunsch algorithm to compute similarity between deep learning architectures. Our results demonstrate the effectiveness of NeuRN by showing improvement against baseline in cross-domain image classification tasks. Our framework attempts to establish a foundation for future neuro-inspired deep learning models.","authors":["Hamd Jalil","Ahmed Qazi","Asim Iqbal"],"url":"https://arxiv.org/abs/2505.06881"}
{"created":"2025-05-13","title":"FACET: Force-Adaptive Control via Impedance Reference Tracking for Legged Robots","abstract":"Reinforcement learning (RL) has made significant strides in legged robot control, enabling locomotion across diverse terrains and complex loco-manipulation capabilities. However, the commonly used position or velocity tracking-based objectives are agnostic to forces experienced by the robot, leading to stiff and potentially dangerous behaviors and poor control during forceful interactions. To address this limitation, we present \\emph{Force-Adaptive Control via Impedance Reference Tracking} (FACET). Inspired by impedance control, we use RL to train a control policy to imitate a virtual mass-spring-damper system, allowing fine-grained control under external forces by manipulating the virtual spring. In simulation, we demonstrate that our quadruped robot achieves improved robustness to large impulses (up to 200 Ns) and exhibits controllable compliance, achieving an 80% reduction in collision impulse. The policy is deployed to a physical robot to showcase both compliance and the ability to engage with large forces by kinesthetic control and pulling payloads up to 2/3 of its weight. Further extension to a legged loco-manipulator and a humanoid shows the applicability of our method to more complex settings to enable whole-body compliance control. Project Website: https://egalahad.github.io/facet/","authors":["Botian Xu","Haoyang Weng","Qingzhou Lu","Yang Gao","Huazhe Xu"],"url":"https://arxiv.org/abs/2505.06883"}
{"created":"2025-05-13","title":"A WSPD, Separator and Small Tree Cover for c-packed Graphs","abstract":"The $c$-packedness property, proposed in 2010, is a geometric property that captures the spatial distribution of a set of edges. Despite the recent interest in $c$-packedness, its utility has so far been limited to Fr\\'echet distance problems. An open problem is whether a wider variety of algorithmic and data structure problems can be solved efficiently under the $c$-packedness assumption, and more specifically, on $c$-packed graphs.","authors":["Lindsey Deryckere","Joachim Gudmundsson","Andr\\'e van Renssen","Yuan Sha","Sampson Wong"],"url":"https://arxiv.org/abs/2505.06884"}
{"created":"2025-05-13","title":"Incremental Analysis of Legacy Applications Using Knowledge Graphs for Application Modernization","abstract":"Industries such as banking, telecom, and airlines - o6en have large so6ware systems that are several decades old. Many of these systems are written in old programming languages such as COBOL, PL/1, Assembler, etc. In many cases, the documentation is not updated, and those who developed/designed these systems are no longer around. Understanding these systems for either modernization or even regular maintenance has been a challenge. An extensive application may have natural boundaries based on its code dependencies and architecture. There are also other logical boundaries in an enterprise setting driven by business functions, data domains, etc. Due to these complications, the system architects generally plan their modernization across these logical boundaries in parts, thereby adopting an incremental approach for the modernization journey of the entire system. In this work, we present a so6ware system analysis tool that allows a subject ma=er expert (SME) or system architect to analyze a large so6ware system incrementally. We analyze the source code and other artifacts (such as data schema) to create a knowledge graph using a customizable ontology/schema. Entities and relations in our ontology can be defined for any combination of programming languages and platforms. Using this knowledge graph, the analyst can then define logical boundaries around dependent Entities (e.g. Programs, Transactions, Database Tables etc.). Our tool then presents different views showcasing the dependencies from the newly defined boundary to/from the other logical groups of the system. This exercise is repeated interactively to 1) Identify the Entities and groupings of interest for a modernization task and 2) Understand how a change in one part of the system may affect the other parts. To validate the efficacy of our tool, we provide an initial study of our system on two client applications.","authors":["Saravanan Krishnan","Amith Singhee","Keerthi Narayan Raghunath","Alex Mathai","Atul Kumar","David Wenk"],"url":"https://arxiv.org/abs/2505.06885"}
{"created":"2025-05-13","title":"Mice to Machines: Neural Representations from Visual Cortex for Domain Generalization","abstract":"The mouse is one of the most studied animal models in the field of systems neuroscience. Understanding the generalized patterns and decoding the neural representations that are evoked by the diverse range of natural scene stimuli in the mouse visual cortex is one of the key quests in computational vision. In recent years, significant parallels have been drawn between the primate visual cortex and hierarchical deep neural networks. However, their generalized efficacy in understanding mouse vision has been limited. In this study, we investigate the functional alignment between the mouse visual cortex and deep learning models for object classification tasks. We first introduce a generalized representational learning strategy that uncovers a striking resemblance between the functional mapping of the mouse visual cortex and high-performing deep learning models on both top-down (population-level) and bottom-up (single cell-level) scenarios. Next, this representational similarity across the two systems is further enhanced by the addition of Neural Response Normalization (NeuRN) layer, inspired by the activation profile of excitatory and inhibitory neurons in the visual cortex. To test the performance effect of NeuRN on real-world tasks, we integrate it into deep learning models and observe significant improvements in their robustness against data shifts in domain generalization tasks. Our work proposes a novel framework for comparing the functional architecture of the mouse visual cortex with deep learning models. Our findings carry broad implications for the development of advanced AI models that draw inspiration from the mouse visual cortex, suggesting that these models serve as valuable tools for studying the neural representations of the mouse visual cortex and, as a result, enhancing their performance on real-world tasks.","authors":["Ahmed Qazi","Hamd Jalil","Asim Iqbal"],"url":"https://arxiv.org/abs/2505.06886"}
{"created":"2025-05-13","title":"Fast and low energy approximate full adder based on FELIX logic","abstract":"In the \"Big Data\" era, a lot of data must be processed and moved between processing and memory units. New technologies and architectures have emerged to improve system performance and overcome the memory bottleneck. The memristor is a technology with both computing and memory capabilities. In-Memory Computing (IMC) can be performed by applying memristors to stateful design methods. The Fast and Energy-Efficient Logic in Memory (FELIX) logic is one of the stateful implementation logics compatible with memristive crossbar arrays. The way computations are performed can be changed to improve performance. Approximate design methods can be applied in error-resilient applications. In error-resilient applications, an acceptable amount of precision is lost while features such as hardware complexity, latency, and energy are improved. In this paper, using these two concepts, an approximate full adder circuit with exact Cout and approximate Sum outputs has been proposed using the FELIX design method for IMC in two different implementation approaches. The applied memristor count in the proposed FELIX-based Approximate Full Adder (FAFA) in the two proposed implementation approaches (FAFA1 and FAFA2) is improved by 14.28% and 28.57%, energy consumption is improved by 73.735% and 81.754%, respectively. The number of computational steps in both approaches is improved by 66.66% compared to the exact FELIX-based full adder. In this paper, two different scenarios are considered for evaluating the FAFA. In the 1st and 2nd scenarios, respectively, for the three and four Most Significant Bits (MSBs), the exact full adder is used, and for the five and four Least Significant Bits (LSBs), the FAFA is used. The results of error analysis and evaluations of the FAFA in three different image processing applications confirmed that FAFA has high accuracy and acceptable performance.","authors":["Seyed Erfan Fatemieh","Samane Asgari","Mohammad Reza Reshadinezhad"],"url":"https://arxiv.org/abs/2505.06888"}
{"created":"2025-05-13","title":"IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method","abstract":"Pre-trained Language Models (PLMs) have achieved remarkable performance on diverse NLP tasks through pre-training and fine-tuning. However, fine-tuning the model with a large number of parameters on limited downstream datasets often leads to vulnerability to adversarial attacks, causing overfitting of the model on standard datasets.","authors":["Mihyeon Kim","Juhyoung Park","Youngbin Kim"],"url":"https://arxiv.org/abs/2505.06889"}
{"created":"2025-05-13","title":"Image Classification Using a Diffusion Model as a Pre-Training Model","abstract":"In this paper, we propose a diffusion model that integrates a representation-conditioning mechanism, where the representations derived from a Vision Transformer (ViT) are used to condition the internal process of a Transformer-based diffusion model. This approach enables representation-conditioned data generation, addressing the challenge of requiring large-scale labeled datasets by leveraging self-supervised learning on unlabeled data. We evaluate our method through a zero-shot classification task for hematoma detection in brain imaging. Compared to the strong contrastive learning baseline, DINOv2, our method achieves a notable improvement of +6.15% in accuracy and +13.60% in F1-score, demonstrating its effectiveness in image classification.","authors":["Kosuke Ukita","Ye Xiaolong","Tsuyoshi Okita"],"url":"https://arxiv.org/abs/2505.06890"}
{"created":"2025-05-13","title":"Learning Soft Sparse Shapes for Efficient Time-Series Classification","abstract":"Shapelets are discriminative subsequences (or shapes) with high interpretability in time series classification. Due to the time-intensive nature of shapelet discovery, existing shapelet-based methods mainly focus on selecting discriminative shapes while discarding others to achieve candidate subsequence sparsification. However, this approach may exclude beneficial shapes and overlook the varying contributions of shapelets to classification performance. To this end, we propose a \\textbf{Soft} sparse \\textbf{Shape}s (\\textbf{SoftShape}) model for efficient time series classification. Our approach mainly introduces soft shape sparsification and soft shape learning blocks. The former transforms shapes into soft representations based on classification contribution scores, merging lower-scored ones into a single shape to retain and differentiate all subsequence information. The latter facilitates intra- and inter-shape temporal pattern learning, improving model efficiency by using sparsified soft shapes as inputs. Specifically, we employ a learnable router to activate a subset of class-specific expert networks for intra-shape pattern learning. Meanwhile, a shared expert network learns inter-shape patterns by converting sparsified shapes into sequences. Extensive experiments show that SoftShape outperforms state-of-the-art methods and produces interpretable results.","authors":["Zhen Liu","Yicheng Luo","Boyuan Li","Emadeldeen Eldele","Min Wu","Qianli Ma"],"url":"https://arxiv.org/abs/2505.06892"}
{"created":"2025-05-13","title":"NeuGen: Amplifying the 'Neural' in Neural Radiance Fields for Domain Generalization","abstract":"Neural Radiance Fields (NeRF) have significantly advanced the field of novel view synthesis, yet their generalization across diverse scenes and conditions remains challenging. Addressing this, we propose the integration of a novel brain-inspired normalization technique Neural Generalization (NeuGen) into leading NeRF architectures which include MVSNeRF and GeoNeRF. NeuGen extracts the domain-invariant features, thereby enhancing the models' generalization capabilities. It can be seamlessly integrated into NeRF architectures and cultivates a comprehensive feature set that significantly improves accuracy and robustness in image rendering. Through this integration, NeuGen shows improved performance on benchmarks on diverse datasets across state-of-the-art NeRF architectures, enabling them to generalize better across varied scenes. Our comprehensive evaluations, both quantitative and qualitative, confirm that our approach not only surpasses existing models in generalizability but also markedly improves rendering quality. Our work exemplifies the potential of merging neuroscientific principles with deep learning frameworks, setting a new precedent for enhanced generalizability and efficiency in novel view synthesis. A demo of our study is available at https://neugennerf.github.io.","authors":["Ahmed Qazi","Abdul Basit","Asim Iqbal"],"url":"https://arxiv.org/abs/2505.06894"}
{"created":"2025-05-13","title":"Nonlinear Model Predictive Control for Leaderless UAV Formation Flying with Collision Avoidance under Directed Graphs","abstract":"This paper studies the leaderless formation flying problem with collision avoidance for a group of unmanned aerial vehicles (UAVs), which requires the UAVs to navigate through cluttered environments without colliding while maintaining the formation. The communication network among the UAVs is structured as a directed graph that includes a directed spanning tree. A novel distributed nonlinear model predictive control (NMPC) method based on the model reference adaptive consensus (MRACon) framework is proposed. Within this framework, each UAV tracks an assigned reference output generated by a linear reference model that utilizes relative measurements as input. Subsequently, the NMPC method penalizes the tracking error between the output of the reference model and that of the actual model while also establishing constraint sets for collision avoidance and physical limitations to achieve distributed and safe formation control. Finally, simulations and hardware experiments are conducted to verify the effectiveness of the proposed method.","authors":["Yiming Wang","Yao Fang","Jie Mei","Youmin Gong","Guangfu Ma"],"url":"https://arxiv.org/abs/2505.06895"}
{"created":"2025-05-13","title":"RCOMPSs: A Scalable Runtime System for R Code Execution on Manycore Systems","abstract":"R has become a cornerstone of scientific and statistical computing due to its extensive package ecosystem, expressive syntax, and strong support for reproducible analysis. However, as data sizes and computational demands grow, native R parallelism support remains limited. This paper presents RCOMPSs, a scalable runtime system that enables efficient parallel execution of R applications on multicore and manycore systems. RCOMPSs adopts a dynamic, task-based programming model, allowing users to write code in a sequential style, while the runtime automatically handles asynchronous task execution, dependency tracking, and scheduling across available resources. We present RCOMPSs using three representative data analysis algorithms, i.e., K-nearest neighbors (KNN) classification, K-means clustering, and linear regression and evaluate their performance on two modern HPC systems: KAUST Shaheen-III and Barcelona Supercomputing Center (BSC) MareNostrum 5. Experimental results reveal that RCOMPSs demonstrates both strong and weak scalability on up to 128 cores per node and across 32 nodes. For KNN and K-means, parallel efficiency remains above 70% in most settings, while linear regression maintains acceptable performance under shared and distributed memory configurations despite its deeper task dependencies. Overall, RCOMPSs significantly enhances the parallel capabilities of R with minimal, automated, and runtime-aware user intervention, making it a practical solution for large-scale data analytics in high-performance environments.","authors":["Xiran Zhang","Javier Conejero","Sameh Abdulah","Jorge Ejarque","Ying Sun","Rosa M. Badia","David E. Keyes","Marc G. Genton"],"url":"https://arxiv.org/abs/2505.06896"}
{"created":"2025-05-13","title":"Embodied Intelligence: The Key to Unblocking Generalized Artificial Intelligence","abstract":"The ultimate goal of artificial intelligence (AI) is to achieve Artificial General Intelligence (AGI). Embodied Artificial Intelligence (EAI), which involves intelligent systems with physical presence and real-time interaction with the environment, has emerged as a key research direction in pursuit of AGI. While advancements in deep learning, reinforcement learning, large-scale language models, and multimodal technologies have significantly contributed to the progress of EAI, most existing reviews focus on specific technologies or applications. A systematic overview, particularly one that explores the direct connection between EAI and AGI, remains scarce. This paper examines EAI as a foundational approach to AGI, systematically analyzing its four core modules: perception, intelligent decision-making, action, and feedback. We provide a detailed discussion of how each module contributes to the six core principles of AGI. Additionally, we discuss future trends, challenges, and research directions in EAI, emphasizing its potential as a cornerstone for AGI development. Our findings suggest that EAI's integration of dynamic learning and real-world interaction is essential for bridging the gap between narrow AI and AGI.","authors":["Jinhao Jiang","Changlin Chen","Shile Feng","Wanru Geng","Zesheng Zhou","Ni Wang","Shuai Li","Feng-Qi Cui","Erbao Dong"],"url":"https://arxiv.org/abs/2505.06897"}
{"created":"2025-05-13","title":"Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration","abstract":"Generalist Medical AI (GMAI) systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI assistant that integrates textual and visual interpretability to support transparent and trustworthy medical decision-making. XMedGPT not only produces accurate diagnostic and descriptive outputs, but also grounds referenced anatomical sites within medical images, bridging critical gaps in interpretability and enhancing clinician usability. To support real-world deployment, we introduce a reliability indexing mechanism that quantifies uncertainty through consistency-based assessment via interactive question-answering. We validate XMedGPT across four pillars: multi-modal interpretability, uncertainty quantification, and prognostic modeling, and rigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical regions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between visual rationales and clinical outcomes. For uncertainty estimation, it attains an AUC of 0.862 on visual question answering and 0.764 on radiology report generation. In survival and recurrence prediction for lung and glioma cancers, it surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%. Rigorous benchmarking across 347 datasets covers 40 imaging modalities and external validation spans 4 anatomical systems confirming exceptional generalizability, with performance gains surpassing existing GMAI by 20.7% for in-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together, XMedGPT represents a significant leap forward in clinician-centric AI integration, offering trustworthy and scalable support for diverse healthcare applications.","authors":["Honglong Yang","Shanshan Song","Yi Qin","Lehan Wang","Haonan Wang","Xinpeng Ding","Qixiang Zhang","Bodong Du","Xiaomeng Li"],"url":"https://arxiv.org/abs/2505.06898"}
{"created":"2025-05-13","title":"ContribChain: A Stress-Balanced Blockchain Sharding Protocol with Node Contribution Awareness","abstract":"Existing blockchain sharding protocols have focused on eliminating imbalanced workload distributions. However, even with workload balance, disparities in processing capabilities can lead to differential stress among shards, resulting in transaction backlogs in certain shards. Therefore, achieving stress balance among shards in the dynamic and heterogeneous environment presents a significant challenge of blockchain sharding. In this paper, we propose ContribChain, a blockchain sharding protocol that can automatically be aware of node contributions to achieve stress balance. We calculate node contribution values based on the historical behavior to evaluate the performance and security of nodes. Furthermore, we propose node allocation algorithm NACV and account allocation algorithm P-Louvain, which both match shard performance with workload to achieve stress balance. Finally, we conduct extensive experiments to compare our work with state-of-the-art baselines based on real Ethereum transactions. The evaluation results show that P-Louvain reduces allocation execution time by 86% and the cross-shard transaction ratio by 7.5%. Meanwhile, ContribChain improves throughput by 35.8% and reduces the cross-shard transaction ratio by 16%.","authors":["Xinpeng Huang","Wanqing Jie","Shiwen Zhang","Haofu Yang","Wangjie Qiu","Qinnan Zhang","Huawei Huang","Zehui Xiong","Shaoting Tang","Hongwei Zheng","Zhiming Zheng"],"url":"https://arxiv.org/abs/2505.06899"}
{"created":"2025-05-13","title":"Ecco: Improving Memory Bandwidth and Capacity for LLMs via Entropy-aware Cache Compression","abstract":"Large language models (LLMs) have demonstrated transformative capabilities across diverse artificial intelligence applications, yet their deployment is hindered by substantial memory and computational demands, especially in resource-constrained environments. Quantization techniques have emerged as a critical solution, reducing data precision to enhance memory and computational efficiency. However, existing methods often suffer from high runtime overheads and potential accuracy degradation. To address these challenges, we propose Ecco, an entropy-based cache compression technique tailored for LLMs. Ecco combines group-wise and non-uniform quantization with pre-defined shared k-means patterns and Huffman coding to exploit the inherent entropy characteristics of LLM cache data. Recognizing the inefficiencies of traditional Huffman coding in terms of parallelism and latency, we introduce a novel parallel Huffman-based decoding process with a multi-stage pipeline design, reducing latency by two orders of magnitude and achieving throughput comparable to GPU L2 caches. Comprehensive evaluations demonstrate that Ecco achieves an up to 2.9$\\times$ and 1.9$\\times$ speedup over the state-of-the-art AWQ and SmoothQuant framework, 2.4$\\times$ over the Olive accelerator, all while increasing memory capacity by nearly 4$\\times$ and maintaining state-of-the-art LLM accuracy. These results underscore the effectiveness of our entropy-based cache compression in enhancing LLM performance and efficiency, paving the way for more deployable large-scale AI models.","authors":["Feng Cheng","Cong Guo","Chiyue Wei","Junyao Zhang","Changchun Zhou","Edward Hanson","Jiaqi Zhang","Xiaoxiao Liu","Hai \"Helen\" Li","Yiran Chen"],"url":"https://arxiv.org/abs/2505.06901"}
{"created":"2025-05-13","title":"CheXLearner: Text-Guided Fine-Grained Representation Learning for Progression Detection","abstract":"Temporal medical image analysis is essential for clinical decision-making, yet existing methods either align images and text at a coarse level - causing potential semantic mismatches - or depend solely on visual information, lacking medical semantic integration. We present CheXLearner, the first end-to-end framework that unifies anatomical region detection, Riemannian manifold-based structure alignment, and fine-grained regional semantic guidance. Our proposed Med-Manifold Alignment Module (Med-MAM) leverages hyperbolic geometry to robustly align anatomical structures and capture pathologically meaningful discrepancies across temporal chest X-rays. By introducing regional progression descriptions as supervision, CheXLearner achieves enhanced cross-modal representation learning and supports dynamic low-level feature optimization. Experiments show that CheXLearner achieves 81.12% (+17.2%) average accuracy and 80.32% (+11.05%) F1-score on anatomical region progression detection - substantially outperforming state-of-the-art baselines, especially in structurally complex regions. Additionally, our model attains a 91.52% average AUC score in downstream disease classification, validating its superior feature representation.","authors":["Yuanzhuo Wang","Junwen Duan","Xinyu Li","Jianxin Wang"],"url":"https://arxiv.org/abs/2505.06903"}
{"created":"2025-05-13","title":"EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation","abstract":"Large language models (LLMs) have demonstrated an impressive ability to role-play humans and replicate complex social dynamics. While large-scale social simulations are gaining increasing attention, they still face significant challenges, particularly regarding high time and computation costs. Existing solutions, such as distributed mechanisms or hybrid agent-based model (ABM) integrations, either fail to address inference costs or compromise accuracy and generalizability. To this end, we propose EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation. EcoLANG operates in two stages: (1) language evolution, where we filter synonymous words and optimize sentence-level rules through natural selection, and (2) language utilization, where agents in social simulations communicate using the evolved language. Experimental results demonstrate that EcoLANG reduces token consumption by over 20%, enhancing efficiency without sacrificing simulation accuracy.","authors":["Xinyi Mou","Chen Qian","Wei Liu","Xuanjing Huang","Zhongyu Wei"],"url":"https://arxiv.org/abs/2505.06904"}
{"created":"2025-05-13","title":"Enhancing Monocular Height Estimation via Sparse LiDAR-Guided Correction","abstract":"Monocular height estimation (MHE) from very-high-resolution (VHR) remote sensing imagery via deep learning is notoriously challenging due to the lack of sufficient structural information. Conventional digital elevation models (DEMs), typically derived from airborne LiDAR or multi-view stereo, remain costly and geographically limited. Recently, models trained on synthetic data and refined through domain adaptation have shown remarkable performance in MHE, yet it remains unclear how these models make predictions or how reliable they truly are. In this paper, we investigate a state-of-the-art MHE model trained purely on synthetic data to explore where the model looks when making height predictions. Through systematic analyses, we find that the model relies heavily on shadow cues, a factor that can lead to overestimation or underestimation of heights when shadows deviate from expected norms. Furthermore, the inherent difficulty of evaluating regression tasks with the human eye underscores additional limitations of purely synthetic training. To address these issues, we propose a novel correction pipeline that integrates sparse, imperfect global LiDAR measurements (ICESat-2) with deep-learning outputs to improve local accuracy and achieve spatially consistent corrections. Our method comprises two stages: pre-processing raw ICESat-2 data, followed by a random forest-based approach to densely refine height estimates. Experiments in three representative urban regions -- Saint-Omer, Tokyo, and Sao Paulo -- reveal substantial error reductions, with mean absolute error (MAE) decreased by 22.8\\%, 6.9\\%, and 4.9\\%, respectively. These findings highlight the critical role of shadow awareness in synthetic data-driven models and demonstrate how fusing imperfect real-world LiDAR data can bolster the robustness of MHE, paving the way for more reliable and scalable 3D mapping solutions.","authors":["Jian Song","Hongruixuan Chen","Naoto Yokoya"],"url":"https://arxiv.org/abs/2505.06905"}
{"created":"2025-05-13","title":"Realistic Counterfactual Explanations for Machine Learning-Controlled Mobile Robots using 2D LiDAR","abstract":"This paper presents a novel method for generating realistic counterfactual explanations (CFEs) in machine learning (ML)-based control for mobile robots using 2D LiDAR. ML models, especially artificial neural networks (ANNs), can provide advanced decision-making and control capabilities by learning from data. However, they often function as black boxes, making it challenging to interpret them. This is especially a problem in safety-critical control applications. To generate realistic CFEs, we parameterize the LiDAR space with simple shapes such as circles and rectangles, whose parameters are chosen by a genetic algorithm, and the configurations are transformed into LiDAR data by raycasting. Our model-agnostic approach generates CFEs in the form of synthetic LiDAR data that resembles a base LiDAR state but is modified to produce a pre-defined ML model control output based on a query from the user. We demonstrate our method on a mobile robot, the TurtleBot3, controlled using deep reinforcement learning (DRL) in real-world and simulated scenarios. Our method generates logical and realistic CFEs, which helps to interpret the DRL agent's decision making. This paper contributes towards advancing explainable AI in mobile robotics, and our method could be a tool for understanding, debugging, and improving ML-based autonomous control.","authors":["Sindre Benjamin Remman","Anastasios M. Lekkas"],"url":"https://arxiv.org/abs/2505.06906"}
{"created":"2025-05-13","title":"Towards Artificial General or Personalized Intelligence? A Survey on Foundation Models for Personalized Federated Intelligence","abstract":"The rise of large language models (LLMs), such as ChatGPT, DeepSeek, and Grok-3, has reshaped the artificial intelligence landscape. As prominent examples of foundational models (FMs) built on LLMs, these models exhibit remarkable capabilities in generating human-like content, bringing us closer to achieving artificial general intelligence (AGI). However, their large-scale nature, sensitivity to privacy concerns, and substantial computational demands present significant challenges to personalized customization for end users. To bridge this gap, this paper presents the vision of artificial personalized intelligence (API), focusing on adapting these powerful models to meet the specific needs and preferences of users while maintaining privacy and efficiency. Specifically, this paper proposes personalized federated intelligence (PFI), which integrates the privacy-preserving advantages of federated learning (FL) with the zero-shot generalization capabilities of FMs, enabling personalized, efficient, and privacy-protective deployment at the edge. We first review recent advances in both FL and FMs, and discuss the potential of leveraging FMs to enhance federated systems. We then present the key motivations behind realizing PFI and explore promising opportunities in this space, including efficient PFI, trustworthy PFI, and PFI empowered by retrieval-augmented generation (RAG). Finally, we outline key challenges and future research directions for deploying FM-powered FL systems at the edge with improved personalization, computational efficiency, and privacy guarantees. Overall, this survey aims to lay the groundwork for the development of API as a complement to AGI, with a particular focus on PFI as a key enabling technique.","authors":["Yu Qiao","Huy Q. Le","Avi Deb Raha","Phuong-Nam Tran","Apurba Adhikary","Mengchun Zhang","Loc X. Nguyen","Eui-Nam Huh","Dusit Niyato","Choong Seon Hong"],"url":"https://arxiv.org/abs/2505.06907"}
{"created":"2025-05-13","title":"Energy-Efficient Ternary Encoding for High-Speed Data Transmission in 3D-Integrated Circuits Using Inductive Coupling Links","abstract":"This paper proposes a ternary signalling scheme for inductive coupling links (ICLs) in 3D-integrated circuits (3D-ICs) to reduce crosstalk and electromagnetic interference in multi-stacked chip communications. By converting binary data into ternary sequences with three voltage levels (-V, 0V, +V), the approach enhances signal separation, reduces crosstalk, and improves signal integrity. Unlike traditional Non-Return to Zero (NRZ) systems, the ternary scheme increases bandwidth efficiency and reduces power consumption through fewer signal transitions. A modified H-Bridge transmitter generates ternary symbols by controlling current flow based on binary-to-ternary mapping. Preliminary simulations validate the efficiency of the scheme, showing reduced power consumption and higher data rates compared to NRZ. This approach shows promise for high-performance computing and IoT devices in 3D-IC environments, offering enhanced noise resilience, lower power usage, and improved communication efficiency.","authors":["Abdullah Saeed Alghotmi"],"url":"https://arxiv.org/abs/2505.06908"}
{"created":"2025-05-13","title":"Time-Modulated EM Skins for Integrated Sensing and Communications","abstract":"An innovative solution, based on the exploitation of the harmonic beams generated by time-modulated electromagnetic skins (TM-EMSs), is proposed for the implementation of integrated sensing and communication (ISAC) functionalities in a Smart Electromagnetic Environment (SEME) scenario. More in detail, the field radiated by a user terminal, located at an unknown position, is assumed to illuminate a passive TM-EMS that, thanks to a suitable modulation of the local reflection coefficients at the meta-atom level of the EMS surface, simultaneously reflects towards a receiving base station (BS) a \"sum\" beam and a \"difference\" one at slightly different frequencies. By processing the received signals and exploiting monopulse radar tracking concepts, the BS both localizes the user terminal and, as a by-product, establishes a communication link with it by leveraging on the \"sum\" reflected beam. Towards this purpose, the arising harmonic beam control problem is reformulated as a global optimization one, which is successively solved by means of an evolutionary iterative approach to determine the desired TM-EMS modulation sequence. The results from selected numerical and experimental tests are reported to assess the effectiveness and the reliability of the proposed approach.","authors":["Lorenzo Poli","Aakash Bansal","Giacomo Oliveri","Aaron Angel Salas-Sanchez","Will Whittow","Andrea Massa"],"url":"https://arxiv.org/abs/2505.06909"}
{"created":"2025-05-13","title":"MMiC: Mitigating Modality Incompleteness in Clustered Federated Learning","abstract":"In the era of big data, data mining has become indispensable for uncovering hidden patterns and insights from vast and complex datasets. The integration of multimodal data sources further enhances its potential. Multimodal Federated Learning (MFL) is a distributed approach that enhances the efficiency and quality of multimodal learning, ensuring collaborative work and privacy protection. However, missing modalities pose a significant challenge in MFL, often due to data quality issues or privacy policies across the clients. In this work, we present MMiC, a framework for Mitigating Modality incompleteness in MFL within the Clusters. MMiC replaces partial parameters within client models inside clusters to mitigate the impact of missing modalities. Furthermore, it leverages the Banzhaf Power Index to optimize client selection under these conditions. Finally, MMiC employs an innovative approach to dynamically control global aggregation by utilizing Markovitz Portfolio Optimization. Extensive experiments demonstrate that MMiC consistently outperforms existing federated learning architectures in both global and personalized performance on multimodal datasets with missing modalities, confirming the effectiveness of our proposed solution.","authors":["Lishan Yang","Wei Zhang","Quan Z. Sheng","Weitong Chen","Lina Yao","Weitong Chen","Ali Shakeri"],"url":"https://arxiv.org/abs/2505.06911"}
{"created":"2025-05-13","title":"Building a Human-Verified Clinical Reasoning Dataset via a Human LLM Hybrid Pipeline for Trustworthy Medical AI","abstract":"Despite strong performance in medical question-answering, the clinical adoption of Large Language Models (LLMs) is critically hampered by their opaque 'black-box' reasoning, limiting clinician trust. This challenge is compounded by the predominant reliance of current medical LLMs on corpora from scientific literature or synthetic data, which often lack the granular expert validation and high clinical relevance essential for advancing their specialized medical capabilities. To address these critical gaps, we introduce a highly clinically relevant dataset with 31,247 medical question-answer pairs, each accompanied by expert-validated chain-of-thought (CoT) explanations. This resource, spanning multiple clinical domains, was curated via a scalable human-LLM hybrid pipeline: LLM-generated rationales were iteratively reviewed, scored, and refined by medical experts against a structured rubric, with substandard outputs revised through human effort or guided LLM regeneration until expert consensus. This publicly available dataset provides a vital source for the development of medical LLMs that capable of transparent and verifiable reasoning, thereby advancing safer and more interpretable AI in medicine.","authors":["Chao Ding","Mouxiao Bian","Pengcheng Chen","Hongliang Zhang","Tianbin Li","Lihao Liu","Jiayuan Chen","Zhuoran Li","Yabei Zhong","Yongqi Liu","Haiqing Huang","Dongming Shan","Junjun He","Jie Xu"],"url":"https://arxiv.org/abs/2505.06912"}
{"created":"2025-05-13","title":"RedTeamLLM: an Agentic AI framework for offensive security","abstract":"From automated intrusion testing to discovery of zero-day attacks before software launch, agentic AI calls for great promises in security engineering. This strong capability is bound with a similar threat: the security and research community must build up its models before the approach is leveraged by malicious actors for cybercrime. We therefore propose and evaluate RedTeamLLM, an integrated architecture with a comprehensive security model for automatization of pentest tasks. RedTeamLLM follows three key steps: summarizing, reasoning and act, which embed its operational capacity. This novel framework addresses four open challenges: plan correction, memory management, context window constraint, and generality vs. specialization. Evaluation is performed through the automated resolution of a range of entry-level, but not trivial, CTF challenges. The contribution of the reasoning capability of our agentic AI framework is specifically evaluated.","authors":["Brian Challita","Pierre Parrend"],"url":"https://arxiv.org/abs/2505.06913"}
{"created":"2025-05-13","title":"The Distracting Effect: Understanding Irrelevant Passages in RAG","abstract":"A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs.","authors":["Chen Amiraz","Florin Cuconasu","Simone Filice","Zohar Karnin"],"url":"https://arxiv.org/abs/2505.06914"}
{"created":"2025-05-13","title":"Non-Stationary Time Series Forecasting Based on Fourier Analysis and Cross Attention Mechanism","abstract":"Time series forecasting has important applications in financial analysis, weather forecasting, and traffic management. However, existing deep learning models are limited in processing non-stationary time series data because they cannot effectively capture the statistical characteristics that change over time. To address this problem, this paper proposes a new framework, AEFIN, which enhances the information sharing ability between stable and unstable components by introducing a cross-attention mechanism, and combines Fourier analysis networks with MLP to deeply explore the seasonal patterns and trend characteristics in unstable components. In addition, we design a new loss function that combines time-domain stability constraints, time-domain instability constraints, and frequency-domain stability constraints to improve the accuracy and robustness of forecasting. Experimental results show that AEFIN outperforms the most common models in terms of mean square error and mean absolute error, especially under non-stationary data conditions, and shows excellent forecasting capabilities. This paper provides an innovative solution for the modeling and forecasting of non-stationary time series data, and contributes to the research of deep learning for complex time series.","authors":["Yuqi Xiong","Yang Wen"],"url":"https://arxiv.org/abs/2505.06917"}
{"created":"2025-05-13","title":"The First WARA Robotics Mobile Manipulation Challenge -- Lessons Learned","abstract":"The first WARA Robotics Mobile Manipulation Challenge, held in December 2024 at ABB Corporate Research in V\\\"aster{\\aa}s, Sweden, addressed the automation of task-intensive and repetitive manual labor in laboratory environments - specifically the transport and cleaning of glassware. Designed in collaboration with AstraZeneca, the challenge invited academic teams to develop autonomous robotic systems capable of navigating human-populated lab spaces and performing complex manipulation tasks, such as loading items into industrial dishwashers. This paper presents an overview of the challenge setup, its industrial motivation, and the four distinct approaches proposed by the participating teams. We summarize lessons learned from this edition and propose improvements in design to enable a more effective second iteration to take place in 2025. The initiative bridges an important gap in effective academia-industry collaboration within the domain of autonomous mobile manipulation systems by promoting the development and deployment of applied robotic solutions in real-world laboratory contexts.","authors":["David C\\'aceres Dom\\'inguez","Marco Iannotta","Abhishek Kashyap","Shuo Sun","Yuxuan Yang","Christian Cella","Matteo Colombo","Martina Pelosi","Giuseppe F. Preziosa","Alessandra Tafuro","Isacco Zappa","Finn Busch","Yifei Dong","Alberta Longhini","Haofei Lu","Rafael I. Cabral Muchacho","Jonathan Styrud","Sebastiano Fregnan","Marko Guberina","Zheng Jia","Graziano Carriero","Sofia Lindqvist","Silvio Di Castro","Matteo Iovino"],"url":"https://arxiv.org/abs/2505.06919"}
{"created":"2025-05-13","title":"Bi-directional Self-Registration for Misaligned Infrared-Visible Image Fusion","abstract":"Acquiring accurately aligned multi-modal image pairs is fundamental for achieving high-quality multi-modal image fusion. To address the lack of ground truth in current multi-modal image registration and fusion methods, we propose a novel self-supervised \\textbf{B}i-directional \\textbf{S}elf-\\textbf{R}egistration framework (\\textbf{B-SR}). Specifically, B-SR utilizes a proxy data generator (PDG) and an inverse proxy data generator (IPDG) to achieve self-supervised global-local registration. Visible-infrared image pairs with spatially misaligned differences are aligned to obtain global differences through the registration module. The same image pairs are processed by PDG, such as cropping, flipping, stitching, etc., and then aligned to obtain local differences. IPDG converts the obtained local differences into pseudo-global differences, which are used to perform global-local difference consistency with the global differences. Furthermore, aiming at eliminating the effect of modal gaps on the registration module, we design a neighborhood dynamic alignment loss to achieve cross-modal image edge alignment. Extensive experiments on misaligned multi-modal images demonstrate the effectiveness of the proposed method in multi-modal image alignment and fusion against the competing methods. Our code will be publicly available.","authors":["Timing Li","Bing Cao","Pengfei Zhu","Bin Xiao","Qinghua Hu"],"url":"https://arxiv.org/abs/2505.06920"}
{"created":"2025-05-13","title":"Optimal Current Control Strategy for Reliable Power Electronics Converters: Frequency-Domain Approach","abstract":"Power electronics converters are key enablers in the global energy transition for power generation, industrial and mobility applications; they convert electrical power in a controlled, reliable and efficient manner. The semiconductor switching devices, at the core of power converters, are the most likely component to fail due to the damage caused by the current-induced temperature cycling. Damage models of semiconductors have been developed and employed to study their reliability, improve their design and to estimate the lifetime of the converter in various power applications. However, those models can offer more if employed in the design of strategies to actively operate the converter. Specifically, properly controlling the current, and hence the temperature cycling, can effectively contribute to reducing the accumulated damage in the semiconductor and increase its reliability and lifetime. In this paper we propose a novel current control approach that integrates reliability requirements into the design framework, based on a frequency-domain model of the semiconductor damage.","authors":["Amin Rezaeizadeh","Silvia Mastellone"],"url":"https://arxiv.org/abs/2505.06922"}
{"created":"2025-05-13","title":"YOPOv2-Tracker: An End-to-End Agile Tracking and Navigation Framework from Perception to Action","abstract":"Traditional target tracking pipelines including detection, mapping, navigation, and control are comprehensive but introduce high latency, limitting the agility of quadrotors. On the contrary, we follow the design principle of \"less is more\", striving to simplify the process while maintaining effectiveness. In this work, we propose an end-to-end agile tracking and navigation framework for quadrotors that directly maps the sensory observations to control commands. Importantly, leveraging the multimodal nature of navigation and detection tasks, our network maintains interpretability by explicitly integrating the independent modules of the traditional pipeline, rather than a crude action regression. In detail, we adopt a set of motion primitives as anchors to cover the searching space regarding the feasible region and potential target. Then we reformulate the trajectory optimization as regression of primitive offsets and associated costs considering the safety, smoothness, and other metrics. For tracking task, the trajectories are expected to approach the target and additional objectness scores are predicted. Subsequently, the predictions, after compensation for the estimated lumped disturbance, are transformed into thrust and attitude as control commands for swift response. During training, we seamlessly integrate traditional motion planning with deep learning by directly back-propagating the gradients of trajectory costs to the network, eliminating the need for expert demonstration in imitation learning and providing more direct guidance than reinforcement learning. Finally, we deploy the algorithm on a compact quadrotor and conduct real-world validations in both forest and building environments to demonstrate the efficiency of the proposed method.","authors":["Junjie Lu","Yulin Hui","Xuewei Zhang","Wencan Feng","Hongming Shen","Zhiyu Li","Bailing Tian"],"url":"https://arxiv.org/abs/2505.06923"}
{"created":"2025-05-13","title":"Optimal pressure approximation for the nonstationary Stokes problem by a variational method in time with post-processing","abstract":"We provide an error analysis for the solution of the nonstationary Stokes problem by a variational method in space and time. We use finite elements of higher order for the approximation in space and a Galerkin-Petrov method with first order polynomials for the approximation in time. We require global continuity of the discrete velocity trajectory in time, while allowing the discrete pressure trajectory to be discontinuous at the endpoints of the time intervals. We show existence and uniqueness of the discrete velocity solution, characterize the set of all discrete pressure solutions and prove an optimal second order estimate in time for the pressure error in the midpoints of the time intervals. The key result and innovation is the construction of approximations to the pressure trajectory by means of post-processing together with the proof of optimal order error estimates. We propose two variants for a post-processed pressure within the set of pressure solutions based on collocation techniques or interpolation. Both variants guarantee that the pressure error measured in the L2-norm converges with optimal second order in time and optimal order in space. For the discrete velocity solution, we prove error estimates of optimal order in time and space. We present some numerical tests to support our theoretical results.","authors":["Mathias Anselmann","Markus Bause","Gunar Matthies","Friedhelm Schieweck"],"url":"https://arxiv.org/abs/2505.06933"}
{"created":"2025-05-13","title":"AI-Powered Inverse Design of Ku-Band SIW Resonant Structures by Iterative Residual Correction Network","abstract":"Inverse electromagnetic modeling has emerged as a powerful approach for designing complex microwave structures with high accuracy and efficiency. In this study, we propose an Iterative Residual Correction Network (IRC-Net) for the inverse design of Ku-band Substrate Integrated Waveguide (SIW) components based on multimode resonators. We use a multimode resonance structure to demonstrate that it is possible to control the resonances of the structure. Therefore, these structures can be used for resonant components and smart filter design. The proposed deep learning architecture leverages residual neural networks to overcome the limitations of traditional inverse design techniques, such as the Feedforward Inverse Model (FIM), offering improved generalization and prediction accuracy. The approach begins with a FIM to generate initial design estimates, followed by an iterative correction strategy inspired by the Hybrid Inverse-Forward Residual Refinement Network (HiFR\\textsuperscript{2}-Net), which we call IRC-Net. Experiments demonstrate that the IRC-Net achieves substantial improvements in prediction accuracy compared to traditional single-stage networks, validated through statistical metrics, full-wave electromagnetic simulations, and measurements. To validate the proposed framework, we first design and fabricate a three-resonance SIW structure. Next, we apply the trained IRC-Net model to predict the geometry of a four-resonance structure based on its desired frequency response. Both designs are fabricated and tested, showing strong agreement between the simulated, predicted, and measured results, confirming the effectiveness and practicality of the proposed method.","authors":["Mohammad Mashayekhi","Kamran Salehian"],"url":"https://arxiv.org/abs/2505.06936"}
{"created":"2025-05-13","title":"Transformer-Based Dual-Optical Attention Fusion Crowd Head Point Counting and Localization Network","abstract":"In this paper, the dual-optical attention fusion crowd head point counting model (TAPNet) is proposed to address the problem of the difficulty of accurate counting in complex scenes such as crowd dense occlusion and low light in crowd counting tasks under UAV view. The model designs a dual-optical attention fusion module (DAFP) by introducing complementary information from infrared images to improve the accuracy and robustness of all-day crowd counting. In order to fully utilize different modal information and solve the problem of inaccurate localization caused by systematic misalignment between image pairs, this paper also proposes an adaptive two-optical feature decomposition fusion module (AFDF). In addition, we optimize the training strategy to improve the model robustness through spatial random offset data augmentation. Experiments on two challenging public datasets, DroneRGBT and GAIIC2, show that the proposed method outperforms existing techniques in terms of performance, especially in challenging dense low-light scenes. Code is available at https://github.com/zz-zik/TAPNet","authors":["Fei Zhou","Yi Li","Mingqing Zhu"],"url":"https://arxiv.org/abs/2505.06937"}
{"created":"2025-05-13","title":"A digital perspective on the role of a stemma in material-philological transmission studies","abstract":"Taking its point of departure in the recent developments in the field of digital humanities and the increasing automatisation of scholarly workflows, this study explores the implications of digital approaches to textual traditions for the broader field of textual scholarship. It argues that the relative simplicity of creating computergenerated stemmas allows us to view the stemma codicum as a research tool rather than the final product of our scholarly investigation. Using the Old Norse saga of Hr\\'omundur as a case study, this article demonstrates that stemmas can serve as a starting point for exploring textual traditions further. In doing so, they enable us to address research questions that otherwise remain unanswered. The article is accompanied by datasets used to generate stemmas for the Hr\\'omundar saga tradition as well as two custom Python scripts. The scripts are designed to convert XML-based textual data, encoded according to the TEI Guidelines, into the input format used for the analysis in the PHYLIP package to generate unrooted trees of relationships between texts.","authors":["Katarzyna Anna Kapitan"],"url":"https://arxiv.org/abs/2505.06938"}
{"created":"2025-05-13","title":"Robust Control of Uncertain Switched Affine Systems via Scenario Optimization","abstract":"Switched affine systems are often used to model and control complex dynamical systems that operate in multiple modes. However, uncertainties in the system matrices can challenge their stability and performance. This paper introduces a new approach for designing switching control laws for uncertain switched affine systems using data-driven scenario optimization. Instead of relaxing invariant sets, our method creates smaller invariant sets with quadratic Lyapunov functions through scenario-based optimization, effectively reducing chattering effects and regulation error. The framework ensures robustness against parameter uncertainties while improving accuracy. We validate our method with applications in multi-objective interval Markov decision processes and power electronic converters, demonstrating its effectiveness.","authors":["Negar Monir","Mahdieh S. Sadabadi","Sadegh Soudjani"],"url":"https://arxiv.org/abs/2505.06943"}
{"created":"2025-05-13","title":"Radio Map-Enabled 3D Trajectory and Communication Optimization for Low-Altitude Air-Ground Cooperation","abstract":"Low-altitude economy includes the application of unmanned aerial vehicles (UAVs) serving ground robots. This paper investigates the 3-dimensional (3D) trajectory and communication optimization for low-altitude air-ground cooperation systems, where mobile unmanned ground vehicles (UGVs) upload data to UAVs. We propose a joint optimization algorithm to maximize the minimal sum-rate of UGVs while ensuring quality of service and navigation constraints. The proposed algorithm integrates a successive convex approximation (SCA)-penalty method for UGV-UAV scheduling, an SCA-based approach for UGV transmit power control, and a novel warm-start particle swarm optimization with cross mutation (WS-PSO-CM). The WS-PSO-CM leverages convex optimization results from a statistical channel model to initialize particle swarm, significantly improving the performance, compared with celebrated PSO-CM. Simulation results demonstrate that the proposed algorithm achieves a $45.8$\\% higher minimal sum-rate compared to the baseline PSO-CM under the same iterations. This gain can be translated to reducing computational time by $46.7$\\% of PSO-CM. Furthermore, our simulation results reveal that UAVs dynamically adjust trajectories to avoid interference by buildings, and maintain proximity to UGVs to mitigate path-loss.","authors":["Menghao Hu","Tong Zhang","Shuai Wang","Chiya Zhang","Changyang She","Gaojie Chen","Miaowen Wen"],"url":"https://arxiv.org/abs/2505.06944"}
{"created":"2025-05-13","title":"A systematic review of challenges and proposed solutions in modeling multimodal data","abstract":"Multimodal data modeling has emerged as a powerful approach in clinical research, enabling the integration of diverse data types such as imaging, genomics, wearable sensors, and electronic health records. Despite its potential to improve diagnostic accuracy and support personalized care, modeling such heterogeneous data presents significant technical challenges. This systematic review synthesizes findings from 69 studies to identify common obstacles, including missing modalities, limited sample sizes, dimensionality imbalance, interpretability issues, and finding the optimal fusion techniques. We highlight recent methodological advances, such as transfer learning, generative models, attention mechanisms, and neural architecture search that offer promising solutions. By mapping current trends and innovations, this review provides a comprehensive overview of the field and offers practical insights to guide future research and development in multimodal modeling for medical applications.","authors":["Maryam Farhadizadeh (Institute of General Practice/Family Medicine","Faculty of Medicine and Medical Center - University of Freiburg","Germany","Freiburg Center for Data Analysis","Modeling and AI","University of Freiburg","Germany)","Maria Weymann (Freiburg Center for Data Analysis","Modeling and AI","University of Freiburg","Germany","Institute of Medical Biometry and Statistics","Faculty of Medicine and Medical Center - University of Freiburg","Germany)","Michael Bla{\\ss} (Institute for Applied Medical Informatics","University Medical Center Hamburg-Eppendorf","Germany)","Johann Kraus (Institute of Medical Systems Biology","Ulm University","Germany)","Christopher Gundler (Institute for Applied Medical Informatics","University Medical Center Hamburg-Eppendorf","Germany)","Sebastian Walter (Department of Computer Science","Faculty of Engineering - University of Freiburg","Germany)","Noah Hempen (Institute of General Practice/Family Medicine","Faculty of Medicine and Medical Center - University of Freiburg","Germany)","Harald Binde (Freiburg Center for Data Analysis","Modeling and AI","University of Freiburg","Germany","Institute of Medical Biometry and Statistics","Faculty of Medicine and Medical Center - University of Freiburg","Germany)","Nadine Binder (Institute of General Practice/Family Medicine","Faculty of Medicine and Medical Center - University of Freiburg","Germany","Freiburg Center for Data Analysis","Modeling and AI","University of Freiburg","Germany)"],"url":"https://arxiv.org/abs/2505.06945"}
{"created":"2025-05-13","title":"The Wisdom of Agent Crowds: A Human-AI Interaction Innovation Ignition Framework","abstract":"With the widespread application of large AI models in various fields, the automation level of multi-agent systems has been continuously improved. However, in high-risk decision-making scenarios such as healthcare and finance, human participation and the alignment of intelligent systems with human intentions remain crucial. This paper focuses on the financial scenario and constructs a multi-agent brainstorming framework based on the BDI theory. A human-computer collaborative multi-agent financial analysis process is built using Streamlit. The system plans tasks according to user intentions, reduces users' cognitive load through real-time updated structured text summaries and the interactive Cothinker module, and reasonably integrates general and reasoning large models to enhance the ability to handle complex problems. By designing a quantitative analysis algorithm for the sentiment tendency of interview content based on LLMs and a method for evaluating the diversity of ideas generated by LLMs in brainstorming based on k-means clustering and information entropy, the system is comprehensively evaluated. The results of human factors testing show that the system performs well in terms of usability and user experience. Although there is still room for improvement, it can effectively support users in completing complex financial tasks. The research shows that the system significantly improves the efficiency of human-computer interaction and the quality of decision-making in financial decision-making scenarios, providing a new direction for the development of related fields.","authors":["Senhao Yang","Qiwen Cheng","Ruiqi Ma","Liangzhe Zhao","Zhenying Wu","Guangqiang Yu"],"url":"https://arxiv.org/abs/2505.06947"}
{"created":"2025-05-13","title":"Unsupervised Learning for Class Distribution Mismatch","abstract":"Class distribution mismatch (CDM) refers to the discrepancy between class distributions in training data and target tasks. Previous methods address this by designing classifiers to categorize classes known during training, while grouping unknown or new classes into an \"other\" category. However, they focus on semi-supervised scenarios and heavily rely on labeled data, limiting their applicability and performance. To address this, we propose Unsupervised Learning for Class Distribution Mismatch (UCDM), which constructs positive-negative pairs from unlabeled data for classifier training. Our approach randomly samples images and uses a diffusion model to add or erase semantic classes, synthesizing diverse training pairs. Additionally, we introduce a confidence-based labeling mechanism that iteratively assigns pseudo-labels to valuable real-world data and incorporates them into the training process. Extensive experiments on three datasets demonstrate UCDM's superiority over previous semi-supervised methods. Specifically, with a 60% mismatch proportion on Tiny-ImageNet dataset, our approach, without relying on labeled data, surpasses OpenMatch (with 40 labels per class) by 35.1%, 63.7%, and 72.5% in classifying known, unknown, and new classes.","authors":["Pan Du","Wangbo Zhao","Xinai Lu","Nian Liu","Zhikai Li","Chaoyu Gong","Suyun Zhao","Hong Chen","Cuiping Li","Kai Wang","Yang You"],"url":"https://arxiv.org/abs/2505.06948"}
{"created":"2025-05-13","title":"Causal knowledge graph analysis identifies adverse drug effects","abstract":"Knowledge graphs and structural causal models have each proven valuable for organizing biomedical knowledge and estimating causal effects, but remain largely disconnected: knowledge graphs encode qualitative relationships focusing on facts and deductive reasoning without formal probabilistic semantics, while causal models lack integration with background knowledge in knowledge graphs and have no access to the deductive reasoning capabilities that knowledge graphs provide. To bridge this gap, we introduce a novel formulation of Causal Knowledge Graphs (CKGs) which extend knowledge graphs with formal causal semantics, preserving their deductive capabilities while enabling principled causal inference. CKGs support deconfounding via explicitly marked causal edges and facilitate hypothesis formulation aligned with both encoded and entailed background knowledge. We constructed a Drug-Disease CKG (DD-CKG) integrating disease progression pathways, drug indications, side-effects, and hierarchical disease classification to enable automated large-scale mediation analysis. Applied to UK Biobank and MIMIC-IV cohorts, we tested whether drugs mediate effects between indications and downstream disease progression, adjusting for confounders inferred from the DD-CKG. Our approach successfully reproduced known adverse drug reactions with high precision while identifying previously undocumented significant candidate adverse effects. Further validation through side effect similarity analysis demonstrated that combining our predicted drug effects with established databases significantly improves the prediction of shared drug indications, supporting the clinical relevance of our novel findings. These results demonstrate that our methodology provides a generalizable, knowledge-driven framework for scalable causal inference.","authors":["Sumyyah Toonsi","Paul Schofield","Robert Hoehndorf"],"url":"https://arxiv.org/abs/2505.06949"}
{"created":"2025-05-13","title":"Boosting Cross-spectral Unsupervised Domain Adaptation for Thermal Semantic Segmentation","abstract":"In autonomous driving, thermal image semantic segmentation has emerged as a critical research area, owing to its ability to provide robust scene understanding under adverse visual conditions. In particular, unsupervised domain adaptation (UDA) for thermal image segmentation can be an efficient solution to address the lack of labeled thermal datasets. Nevertheless, since these methods do not effectively utilize the complementary information between RGB and thermal images, they significantly decrease performance during domain adaptation. In this paper, we present a comprehensive study on cross-spectral UDA for thermal image semantic segmentation. We first propose a novel masked mutual learning strategy that promotes complementary information exchange by selectively transferring results between each spectral model while masking out uncertain regions. Additionally, we introduce a novel prototypical self-supervised loss designed to enhance the performance of the thermal segmentation model in nighttime scenarios. This approach addresses the limitations of RGB pre-trained networks, which cannot effectively transfer knowledge under low illumination due to the inherent constraints of RGB sensors. In experiments, our method achieves higher performance over previous UDA methods and comparable performance to state-of-the-art supervised methods.","authors":["Seokjun Kwon","Jeongmin Shin","Namil Kim","Soonmin Hwang","Yukyung Choi"],"url":"https://arxiv.org/abs/2505.06951"}
{"created":"2025-05-13","title":"A Formally Verified Robustness Certifier for Neural Networks (Extended Version)","abstract":"Neural networks are often susceptible to minor perturbations in input that cause them to misclassify. A recent solution to this problem is the use of globally-robust neural networks, which employ a function to certify that the classification of an input cannot be altered by such a perturbation. Outputs that pass this test are called certified robust. However, to the authors' knowledge, these certification functions have not yet been verified at the implementation level. We demonstrate how previous unverified implementations are exploitably unsound in certain circumstances. Moreover, they often rely on approximation-based algorithms, such as power iteration, that (perhaps surprisingly) do not guarantee soundness. To provide assurance that a given output is robust, we implemented and formally verified a certification function for globally-robust neural networks in Dafny. We describe the program, its specifications, and the important design decisions taken for its implementation and verification, as well as our experience applying it in practice.","authors":["James Tobler","Hira Taqdees Syeda","Toby Murray"],"url":"https://arxiv.org/abs/2505.06958"}
{"created":"2025-05-13","title":"First-Order Coalition Logic","abstract":"We introduce First-Order Coalition Logic ($\\mathsf{FOCL}$), which combines key intuitions behind Coalition Logic ($\\mathsf{CL}$) and Strategy Logic ($\\mathsf{SL}$). Specifically, $\\mathsf{FOCL}$ allows for arbitrary quantification over actions of agents. $\\mathsf{FOCL}$ is interesting for several reasons. First, we show that $\\mathsf{FOCL}$ is strictly more expressive than existing coalition logics. Second, we provide a sound and complete axiomatisation of $\\mathsf{FOCL}$, which, to the best of our knowledge, is the first axiomatisation of any variant of $\\mathsf{SL}$ in the literature. Finally, while discussing the satisfiability problem for $\\mathsf{FOCL}$, we reopen the question of the recursive axiomatisability of $\\mathsf{SL}$.","authors":["Davide Catta","Rustam Galimullin","Aniello Murano"],"url":"https://arxiv.org/abs/2505.06960"}
{"created":"2025-05-13","title":"Reinforcement Learning-Based Monocular Vision Approach for Autonomous UAV Landing","abstract":"This paper introduces an innovative approach for the autonomous landing of Unmanned Aerial Vehicles (UAVs) using only a front-facing monocular camera, therefore obviating the requirement for depth estimation cameras. Drawing on the inherent human estimating process, the proposed method reframes the landing task as an optimization problem. The UAV employs variations in the visual characteristics of a specially designed lenticular circle on the landing pad, where the perceived color and form provide critical information for estimating both altitude and depth. Reinforcement learning algorithms are utilized to approximate the functions governing these estimations, enabling the UAV to ascertain ideal landing settings via training. This method's efficacy is assessed by simulations and experiments, showcasing its potential for robust and accurate autonomous landing without dependence on complex sensor setups. This research contributes to the advancement of cost-effective and efficient UAV landing solutions, paving the way for wider applicability across various fields.","authors":["Tarik Houichime","Younes EL Amrani"],"url":"https://arxiv.org/abs/2505.06963"}
{"created":"2025-05-13","title":"From Knowledge to Reasoning: Evaluating LLMs for Ionic Liquids Research in Chemical and Biological Engineering","abstract":"Although Large Language Models (LLMs) have achieved remarkable performance in diverse general knowledge and reasoning tasks, their utility in the scientific domain of Chemical and Biological Engineering (CBE) is unclear. Hence, it necessitates challenging evaluation benchmarks that can measure LLM performance in knowledge- and reasoning-based tasks, which is lacking. As a foundational step, we empirically measure the reasoning capabilities of LLMs in CBE. We construct and share an expert-curated dataset of 5,920 examples for benchmarking LLMs' reasoning capabilities in the niche domain of Ionic Liquids (ILs) for carbon sequestration, an emergent solution to reducing global warming. The dataset presents different difficulty levels by varying along the dimensions of linguistic and domain-specific knowledge. Benchmarking three less than 10B parameter open-source LLMs on the dataset suggests that while smaller general-purpose LLMs are knowledgeable about ILs, they lack domain-specific reasoning capabilities. Based on our results, we further discuss considerations for leveraging LLMs for carbon capture research using ILs. Since LLMs have a high carbon footprint, gearing them for IL research can symbiotically benefit both fields and help reach the ambitious carbon neutrality target by 2050. Dataset link: https://github.com/sougata-ub/llms_for_ionic_liquids","authors":["Gaurab Sarkar","Sougata Saha"],"url":"https://arxiv.org/abs/2505.06964"}
{"created":"2025-05-13","title":"The promise and perils of AI in medicine","abstract":"What does Artificial Intelligence (AI) have to contribute to health care? And what should we be looking out for if we are worried about its risks? In this paper we offer a survey, and initial evaluation, of hopes and fears about the applications of artificial intelligence in medicine. AI clearly has enormous potential as a research tool, in genomics and public health especially, as well as a diagnostic aid. It's also highly likely to impact on the organisational and business practices of healthcare systems in ways that are perhaps under-appreciated. Enthusiasts for AI have held out the prospect that it will free physicians up to spend more time attending to what really matters to them and their patients. We will argue that this claim depends upon implausible assumptions about the institutional and economic imperatives operating in contemporary healthcare settings. We will also highlight important concerns about privacy, surveillance, and bias in big data, as well as the risks of over trust in machines, the challenges of transparency, the deskilling of healthcare practitioners, the way AI reframes healthcare, and the implications of AI for the distribution of power in healthcare institutions. We will suggest that two questions, in particular, are deserving of further attention from philosophers and bioethicists. What does care look like when one is dealing with data as much as people? And, what weight should we give to the advice of machines in our own deliberations about medical decisions?","authors":["Robert Sparrow","Joshua Hatherley"],"url":"https://arxiv.org/abs/2505.06971"}
{"created":"2025-05-13","title":"Web Page Classification using LLMs for Crawling Support","abstract":"A web crawler is a system designed to collect web pages, and efficient crawling of new pages requires appropriate algorithms. While website features such as XML sitemaps and the frequency of past page updates provide important clues for accessing new pages, their universal application across diverse conditions is challenging. In this study, we propose a method to efficiently collect new pages by classifying web pages into two types, \"Index Pages\" and \"Content Pages,\" using a large language model (LLM), and leveraging the classification results to select index pages as starting points for accessing new pages. We construct a dataset with automatically annotated web page types and evaluate our approach from two perspectives: the page type classification performance and coverage of new pages. Experimental results demonstrate that the LLM-based method outperformed baseline methods in both evaluation metrics.","authors":["Yuichi Sasazawa","Yasuhiro Sogawa"],"url":"https://arxiv.org/abs/2505.06972"}
{"created":"2025-05-13","title":"CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire","abstract":"A cuneiform tablet KBo 23.1 ++/KUB 30.38, which is known to represent a text of Kizzuwatna rituals, was written by two writers with almost identical content in two iterations. Unlike other cuneiform tablets that contained information such as myths, essays, or business records, the reason why ancient people left such tablets for posterity remains unclear. To study this problem, we develop a new methodology by analyzing images of a tablet quantitatively using CNN (Convolutional Neural Network)-based image models, without segmenting cuneiforms one-by-one. Our data-driven methodology implies that the writer writing the first half was a `teacher' and the other writer was a `student' who was training his skills of writing cuneiforms. This result has not been reached by classical linguistics. We also discuss related conclusions and possible further directions for applying our method and its generalizations.","authors":["Daichi Kohmoto","Katsutoshi Fukuda","Daisuke Yoshida","Takafumi Matsui","Sachihiro Omura"],"url":"https://arxiv.org/abs/2505.06974"}
{"created":"2025-05-13","title":"High-Frequency Prior-Driven Adaptive Masking for Accelerating Image Super-Resolution","abstract":"The primary challenge in accelerating image super-resolution lies in reducing computation while maintaining performance and adaptability. Motivated by the observation that high-frequency regions (e.g., edges and textures) are most critical for reconstruction, we propose a training-free adaptive masking module for acceleration that dynamically focuses computation on these challenging areas. Specifically, our method first extracts high-frequency components via Gaussian blur subtraction and adaptively generates binary masks using K-means clustering to identify regions requiring intensive processing. Our method can be easily integrated with both CNNs and Transformers. For CNN-based architectures, we replace standard $3 \\times 3$ convolutions with an unfold operation followed by $1 \\times 1$ convolutions, enabling pixel-wise sparse computation guided by the mask. For Transformer-based models, we partition the mask into non-overlapping windows and selectively process tokens based on their average values. During inference, unnecessary pixels or windows are pruned, significantly reducing computation. Moreover, our method supports dilation-based mask adjustment to control the processing scope without retraining, and is robust to unseen degradations (e.g., noise, compression). Extensive experiments on benchmarks demonstrate that our method reduces FLOPs by 24--43% for state-of-the-art models (e.g., CARN, SwinIR) while achieving comparable or better quantitative metrics. The source code is available at https://github.com/shangwei5/AMSR","authors":["Wei Shang","Dongwei Ren","Wanying Zhang","Pengfei Zhu","Qinghua Hu","Wangmeng Zuo"],"url":"https://arxiv.org/abs/2505.06975"}
{"created":"2025-05-13","title":"CAT Merging: A Training-Free Approach for Resolving Conflicts in Model Merging","abstract":"Multi-task model merging offers a promising paradigm for integrating multiple expert models into a unified model without additional training. Existing state-of-the-art techniques, such as Task Arithmetic and its variants, merge models by accumulating task vectors -- the parameter differences between pretrained and finetuned models. However, task vector accumulation is often hindered by knowledge conflicts, leading to performance degradation. To address this challenge, we propose Conflict-Aware Task Merging (CAT Merging), a novel training-free framework that selectively trims conflict-prone components from the task vectors. CAT Merging introduces several parameter-specific strategies, including projection for linear weights and masking for scaling and shifting parameters in normalization layers. Extensive experiments on vision, language, and vision-language tasks demonstrate that CAT Merging effectively suppresses knowledge conflicts, achieving average accuracy improvements of up to 2.5% (ViT-B/32) and 2.0% (ViT-L/14) over state-of-the-art methods.","authors":["Wenju Sun","Qingyong Li","Yangli-ao Geng","Boyang Li"],"url":"https://arxiv.org/abs/2505.06977"}
{"created":"2025-05-13","title":"Learning Value of Information towards Joint Communication and Control in 6G V2X","abstract":"As Cellular Vehicle-to-Everything (C-V2X) evolves towards future sixth-generation (6G) networks, Connected Autonomous Vehicles (CAVs) are emerging to become a key application. Leveraging data-driven Machine Learning (ML), especially Deep Reinforcement Learning (DRL), is expected to significantly enhance CAV decision-making in both vehicle control and V2X communication under uncertainty. These two decision-making processes are closely intertwined, with the value of information (VoI) acting as a crucial bridge between them. In this paper, we introduce Sequential Stochastic Decision Process (SSDP) models to define and assess VoI, demonstrating their application in optimizing communication systems for CAVs. Specifically, we formally define the SSDP model and demonstrate that the MDP model is a special case of it. The SSDP model offers a key advantage by explicitly representing the set of information that can enhance decision-making when available. Furthermore, as current research on VoI remains fragmented, we propose a systematic VoI modeling framework grounded in the MDP, Reinforcement Learning (RL) and Optimal Control theories. We define different categories of VoI and discuss their corresponding estimation methods. Finally, we present a structured approach to leverage the various VoI metrics for optimizing the ``When\", ``What\", and ``How\" to communicate problems. For this purpose, SSDP models are formulated with VoI-associated reward functions derived from VoI-based optimization objectives. While we use a simple vehicle-following control problem to illustrate the proposed methodology, it holds significant potential to facilitate the joint optimization of stochastic, sequential control and communication decisions in a wide range of networked control systems.","authors":["Lei Lei (Sherman)","Kan Zheng (Sherman)","Xuemin (Sherman)","Shen"],"url":"https://arxiv.org/abs/2505.06978"}
{"created":"2025-05-13","title":"VALISENS: A Validated Innovative Multi-Sensor System for Cooperative Automated Driving","abstract":"Perception is a core capability of automated vehicles and has been significantly advanced through modern sensor technologies and artificial intelligence. However, perception systems still face challenges in complex real-world scenarios. To improve robustness against various external factors, multi-sensor fusion techniques are essential, combining the strengths of different sensor modalities. With recent developments in Vehicle-to-Everything (V2X communication, sensor fusion can now extend beyond a single vehicle to a cooperative multi-agent system involving Connected Automated Vehicle (CAV) and intelligent infrastructure. This paper presents VALISENS, an innovative multi-sensor system distributed across multiple agents. It integrates onboard and roadside LiDARs, radars, thermal cameras, and RGB cameras to enhance situational awareness and support cooperative automated driving. The thermal camera adds critical redundancy for perceiving Vulnerable Road User (VRU), while fusion with roadside sensors mitigates visual occlusions and extends the perception range beyond the limits of individual vehicles. We introduce the corresponding perception module built on this sensor system, which includes object detection, tracking, motion forecasting, and high-level data fusion. The proposed system demonstrates the potential of cooperative perception in real-world test environments and lays the groundwork for future Cooperative Intelligent Transport Systems (C-ITS) applications.","authors":["Lei Wan","Prabesh Gupta","Andreas Eich","Marcel Kettelgerdes","Hannan Ejaz Keen","Michael Kl\\\"oppel-Gersdorf","Alexey Vinel"],"url":"https://arxiv.org/abs/2505.06980"}
{"created":"2025-05-13","title":"Federated Learning with LoRA Optimized DeiT and Multiscale Patch Embedding for Secure Eye Disease Recognition","abstract":"Recent progress in image-based medical disease detection encounters challenges such as limited annotated data sets, inadequate spatial feature analysis, data security issues, and inefficient training frameworks. This study introduces a data-efficient image transformer (DeIT)-based approach that overcomes these challenges by utilizing multiscale patch embedding for better feature extraction and stratified weighted random sampling to address class imbalance. The model also incorporates a LoRA-enhanced transformer encoder, a distillation framework, and federated learning for decentralized training, improving both efficiency and data security. Consequently, it achieves state-of-the-art performance, with the highest AUC, F1 score, precision, minimal loss, and Top-5 accuracy. Additionally, Grad-CAM++ visualizations improve interpretability by highlighting critical pathological regions, enhancing the model's clinical relevance. These results highlight the potential of this approach to advance AI-powered medical imaging and disease detection.","authors":["Md. Naimur Asif Borno","Md Sakib Hossain Shovon","MD Hanif Sikder","Iffat Firozy Rimi","Tahani Jaser Alahmadi","Mohammad Ali Moni"],"url":"https://arxiv.org/abs/2505.06982"}
{"created":"2025-05-13","title":"BridgeIV: Bridging Customized Image and Video Generation through Test-Time Autoregressive Identity Propagation","abstract":"Both zero-shot and tuning-based customized text-to-image (CT2I) generation have made significant progress for storytelling content creation. In contrast, research on customized text-to-video (CT2V) generation remains relatively limited. Existing zero-shot CT2V methods suffer from poor generalization, while another line of work directly combining tuning-based T2I models with temporal motion modules often leads to the loss of structural and texture information. To bridge this gap, we propose an autoregressive structure and texture propagation module (STPM), which extracts key structural and texture features from the reference subject and injects them autoregressively into each video frame to enhance consistency. Additionally, we introduce a test-time reward optimization (TTRO) method to further refine fine-grained details. Quantitative and qualitative experiments validate the effectiveness of STPM and TTRO, demonstrating improvements of 7.8 and 13.1 in CLIP-I and DINO consistency metrics over the baseline, respectively.","authors":["Panwen Hu","Jiehui Huang","Qiang Sun","Xiaodan Liang"],"url":"https://arxiv.org/abs/2505.06985"}
{"created":"2025-05-13","title":"Convert Language Model into a Value-based Strategic Planner","abstract":"Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.","authors":["Xiaoyu Wang","Yue Zhao","Qingqing Gu","Zhonglin Jiang","Xiaokai Chen","Yong Chen","Luo Ji"],"url":"https://arxiv.org/abs/2505.06987"}
{"created":"2025-05-13","title":"Measuring the Accuracy and Effectiveness of PII Removal Services","abstract":"This paper presents the first large-scale empirical study of commercial personally identifiable information (PII) removal systems -- commercial services that claim to improve privacy by automating the removal of PII from data broker's databases. Popular examples of such services include DeleteMe, Mozilla Monitor, Incogni, among many others. The claims these services make may be very appealing to privacy-conscious Web users, but how effective these services actually are at improving privacy has not been investigated. This work aims to improve our understanding of commercial PII removal services in multiple ways. First, we conduct a user study where participants purchase subscriptions from four popular PII removal services, and report (i) what PII the service find, (ii) from which data brokers, (iii) whether the service is able to have the information removed, and (iv) whether the identified information actually is PII describing the participant. And second, by comparing the claims and promises the services makes (e.g. which and how many data brokers each service claims to cover). We find that these services have significant accuracy and coverage issues that limit the usefulness of these services as a privacy-enhancing technology. For example, we find that the measured services are unable to remove the majority of the identified PII records from data broker's (48.2% of the successfully removed found records) and that most records identified by these services are not PII about the user (study participants found that only 41.1% of records identified by these services were PII about themselves).","authors":["Jiahui He","Pete Snyder","Hamed Haddadi","Fabi\\'an E. Bustamante","Gareth Tyson"],"url":"https://arxiv.org/abs/2505.06989"}
{"created":"2025-05-13","title":"Technical Report for ICRA 2025 GOOSE 2D Semantic Segmentation Challenge: Leveraging Color Shift Correction, RoPE-Swin Backbone, and Quantile-based Label Denoising Strategy for Robust Outdoor Scene Understanding","abstract":"This report presents our semantic segmentation framework developed by team ACVLAB for the ICRA 2025 GOOSE 2D Semantic Segmentation Challenge, which focuses on parsing outdoor scenes into nine semantic categories under real-world conditions. Our method integrates a Swin Transformer backbone enhanced with Rotary Position Embedding (RoPE) for improved spatial generalization, alongside a Color Shift Estimation-and-Correction module designed to compensate for illumination inconsistencies in natural environments. To further improve training stability, we adopt a quantile-based denoising strategy that downweights the top 2.5\\% of highest-error pixels, treating them as noise and suppressing their influence during optimization. Evaluated on the official GOOSE test set, our approach achieved a mean Intersection over Union (mIoU) of 0.848, demonstrating the effectiveness of combining color correction, positional encoding, and error-aware denoising in robust semantic segmentation.","authors":["Chih-Chung Hsu","I-Hsuan Wu","Wen-Hai Tseng","Ching-Heng Cheng","Ming-Hsuan Wu","Jin-Hui Jiang","Yu-Jou Hsiao"],"url":"https://arxiv.org/abs/2505.06991"}
{"created":"2025-05-13","title":"Towards the Three-Phase Dynamics of Generalization Power of a DNN","abstract":"This paper proposes a new perspective for analyzing the generalization power of deep neural networks (DNNs), i.e., directly disentangling and analyzing the dynamics of generalizable and non-generalizable interaction encoded by a DNN through the training process. Specifically, this work builds upon the recent theoretical achievement in explainble AI, which proves that the detailed inference logic of DNNs can be can be strictly rewritten as a small number of AND-OR interaction patterns. Based on this, we propose an efficient method to quantify the generalization power of each interaction, and we discover a distinct three-phase dynamics of the generalization power of interactions during training. In particular, the early phase of training typically removes noisy and non-generalizable interactions and learns simple and generalizable ones. The second and the third phases tend to capture increasingly complex interactions that are harder to generalize. Experimental results verify that the learning of non-generalizable interactions is the the direct cause for the gap between the training and testing losses.","authors":["Yuxuan He","Junpeng Zhang","Hongyuan Zhang","Quanshi Zhang"],"url":"https://arxiv.org/abs/2505.06993"}
{"created":"2025-05-13","title":"Replay-Based Continual Learning with Dual-Layered Distillation and a Streamlined U-Net for Efficient Text-to-Image Generation","abstract":"Recent advancements in text-to-image diffusion models are hindered by high computational demands, limiting accessibility and scalability. This paper introduces KDC-Diff, a novel stable diffusion framework that enhances efficiency while maintaining image quality. KDC-Diff features a streamlined U-Net architecture with nearly half the parameters of the original U-Net (482M), significantly reducing model complexity. We propose a dual-layered distillation strategy to ensure high-fidelity generation, transferring semantic and structural insights from a teacher to a compact student model while minimizing quality degradation. Additionally, replay-based continual learning is integrated to mitigate catastrophic forgetting, allowing the model to retain prior knowledge while adapting to new data. Despite operating under extremely low computational resources, KDC-Diff achieves state-of-the-art performance on the Oxford Flowers and Butterflies & Moths 100 Species datasets, demonstrating competitive metrics such as FID, CLIP, and LPIPS. Moreover, it significantly reduces inference time compared to existing models. These results establish KDC-Diff as a highly efficient and adaptable solution for text-to-image generation, particularly in computationally constrained environments.","authors":["Md. Naimur Asif Borno","Md Sakib Hossain Shovon","Asmaa Soliman Al-Moisheer","Mohammad Ali Moni"],"url":"https://arxiv.org/abs/2505.06995"}
{"created":"2025-05-13","title":"A Multi-Agent Reinforcement Learning Approach for Cooperative Air-Ground-Human Crowdsensing in Emergency Rescue","abstract":"Mobile crowdsensing is evolving beyond traditional human-centric models by integrating heterogeneous entities like unmanned aerial vehicles (UAVs) and unmanned ground vehicles (UGVs). Optimizing task allocation among these diverse agents is critical, particularly in challenging emergency rescue scenarios characterized by complex environments, limited communication, and partial observability. This paper tackles the Heterogeneous-Entity Collaborative-Sensing Task Allocation (HECTA) problem specifically for emergency rescue, considering humans, UAVs, and UGVs. We introduce a novel ``Hard-Cooperative'' policy where UGVs prioritize recharging low-battery UAVs, alongside performing their sensing tasks. The primary objective is maximizing the task completion rate (TCR) under strict time constraints. We rigorously formulate this NP-hard problem as a decentralized partially observable Markov decision process (Dec-POMDP) to effectively handle sequential decision-making under uncertainty. To solve this, we propose HECTA4ER, a novel multi-agent reinforcement learning algorithm built upon a Centralized Training with Decentralized Execution architecture. HECTA4ER incorporates tailored designs, including specialized modules for complex feature extraction, utilization of action-observation history via hidden states, and a mixing network integrating global and local information, specifically addressing the challenges of partial observability. Furthermore, theoretical analysis confirms the algorithm's convergence properties. Extensive simulations demonstrate that HECTA4ER significantly outperforms baseline algorithms, achieving an average 18.42% increase in TCR. Crucially, a real-world case study validates the algorithm's effectiveness and robustness in dynamic sensing scenarios, highlighting its strong potential for practical application in emergency response.","authors":["Wenhao Lu","Zhengqiu Zhu","Yong Zhao","Yonglin Tian","Junjie Zeng","Jun Zhang","Zhong Liu","Fei-Yue Wang"],"url":"https://arxiv.org/abs/2505.06997"}
{"created":"2025-05-13","title":"Assessing the Robustness and Reducibility of Multiplex Networks with Embedding-Aided Interlayer Similarities","abstract":"The study of interlayer similarity of multiplex networks helps to understand the intrinsic structure of complex systems, revealing how changes in one layer can propagate and affect others, thus enabling broad implications for transportation, social, and biological systems. Existing algorithms that measure similarity between network layers typically encode only partial information, which limits their effectiveness in capturing the full complexity inherent in multiplex networks. To address this limitation, we propose a novel interlayer similarity measuring approach named Embedding Aided inTerlayer Similarity (EATSim). EATSim concurrently incorporates intralayer structural similarity and cross-layer anchor node alignment consistency, providing a more comprehensive framework for analyzing interconnected systems. Extensive experiments on both synthetic and real-world networks demonstrate that EATSim effectively captures the underlying geometric similarities between interconnected networks, significantly improving the accuracy of interlayer similarity measurement. Moreover, EATSim achieves state-of-the-art performance in two downstream applications: predicting network robustness and network reducibility, showing its great potential in enhancing the understanding and management of complex systems.","authors":["Haoran Nan","Senquan Wang","Chun Ouyang","Yanchen Zhou","Weiwei Gu"],"url":"https://arxiv.org/abs/2505.06998"}
{"created":"2025-05-13","title":"On the permanent of random tensors","abstract":"The exact computation of permanent for high-dimensional tensors is a hard problem. Having in mind the applications of permanents in other fields, providing an algorithm for the approximation of tensor permanents is an attractive subject. In this paper, we design a deterministic quasi-polynomial time algorithm and a PTAS that computes the permanent of complex random tensors that its module of the mean is at least 1/polylog(n).","authors":["Malihe Nobakht Kooshkghazi","Hamidreza Afshin"],"url":"https://arxiv.org/abs/2505.07000"}
{"created":"2025-05-13","title":"Hallucination-Aware Multimodal Benchmark for Gastrointestinal Image Analysis with Large Vision-Language Models","abstract":"Vision-Language Models (VLMs) are becoming increasingly popular in the medical domain, bridging the gap between medical images and clinical language. Existing VLMs demonstrate an impressive ability to comprehend medical images and text queries to generate detailed, descriptive diagnostic medical reports. However, hallucination--the tendency to generate descriptions that are inconsistent with the visual content--remains a significant issue in VLMs, with particularly severe implications in the medical field. To facilitate VLM research on gastrointestinal (GI) image analysis and study hallucination, we curate a multimodal image-text GI dataset: Gut-VLM. This dataset is created using a two-stage pipeline: first, descriptive medical reports of Kvasir-v2 images are generated using ChatGPT, which introduces some hallucinated or incorrect texts. In the second stage, medical experts systematically review these reports, and identify and correct potential inaccuracies to ensure high-quality, clinically reliable annotations. Unlike traditional datasets that contain only descriptive texts, our dataset also features tags identifying hallucinated sentences and their corresponding corrections. A common approach to reducing hallucination in VLM is to finetune the model on a small-scale, problem-specific dataset. However, we take a different strategy using our dataset. Instead of finetuning the VLM solely for generating textual reports, we finetune it to detect and correct hallucinations, an approach we call hallucination-aware finetuning. Our results show that this approach is better than simply finetuning for descriptive report generation. Additionally, we conduct an extensive evaluation of state-of-the-art VLMs across several metrics, establishing a benchmark. GitHub Repo: https://github.com/bhattarailab/Hallucination-Aware-VLM.","authors":["Bidur Khanal","Sandesh Pokhrel","Sanjay Bhandari","Ramesh Rana","Nikesh Shrestha","Ram Bahadur Gurung","Cristian Linte","Angus Watson","Yash Raj Shrestha","Binod Bhattarai"],"url":"https://arxiv.org/abs/2505.07001"}
{"created":"2025-05-13","title":"CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation","abstract":"Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the input image lead to an entire regeneration of the 3D models. In this paper, we introduce a new method called CMD that generates a 3D model from an input image while enabling flexible local editing of each component of the 3D model. In CMD, we formulate the 3D generation as a conditional multiview diffusion model, which takes the existing or known parts as conditions and generates the edited or added components. This conditional multiview diffusion model not only allows the generation of 3D models part by part but also enables local editing of 3D models according to the local revision of the input image without changing other 3D parts. Extensive experiments are conducted to demonstrate that CMD decomposes a complex 3D generation task into multiple components, improving the generation quality. Meanwhile, CMD enables efficient and flexible local editing of a 3D model by just editing one rendered image.","authors":["Peng Li","Suizhi Ma","Jialiang Chen","Yuan Liu","Chongyi Zhang","Wei Xue","Wenhan Luo","Alla Sheffer","Wenping Wang","Yike Guo"],"url":"https://arxiv.org/abs/2505.07003"}
{"created":"2025-05-13","title":"GuidedQuant: Large Language Model Quantization via Exploiting End Loss Guidance","abstract":"Post-training quantization is a key technique for reducing the memory and inference latency of large language models by quantizing weights and activations without requiring retraining. However, existing methods either (1) fail to account for the varying importance of hidden features to the end loss or, when incorporating end loss, (2) neglect the critical interactions between model weights. To address these limitations, we propose GuidedQuant, a novel quantization approach that integrates gradient information from the end loss into the quantization objective while preserving cross-weight dependencies within output channels. GuidedQuant consistently boosts the performance of state-of-the-art quantization methods across weight-only scalar, weight-only vector, and weight-and-activation quantization. Additionally, we introduce a novel non-uniform scalar quantization algorithm, which is guaranteed to monotonically decrease the quantization objective value, and outperforms existing methods in this category. We release the code at https://github.com/snu-mllab/GuidedQuant.","authors":["Jinuk Kim","Marwa El Halabi","Wonpyo Park","Clemens JS Schaefer","Deokjae Lee","Yeonhong Park","Jae W. Lee","Hyun Oh Song"],"url":"https://arxiv.org/abs/2505.07004"}
{"created":"2025-05-13","title":"Explainable AI the Latest Advancements and New Trends","abstract":"In recent years, Artificial Intelligence technology has excelled in various applications across all domains and fields. However, the various algorithms in neural networks make it difficult to understand the reasons behind decisions. For this reason, trustworthy AI techniques have started gaining popularity. The concept of trustworthiness is cross-disciplinary; it must meet societal standards and principles, and technology is used to fulfill these requirements. In this paper, we first surveyed developments from various countries and regions on the ethical elements that make AI algorithms trustworthy; and then focused our survey on the state of the art research into the interpretability of AI. We have conducted an intensive survey on technologies and techniques used in making AI explainable. Finally, we identified new trends in achieving explainable AI. In particular, we elaborate on the strong link between the explainability of AI and the meta-reasoning of autonomous systems. The concept of meta-reasoning is 'reason the reasoning', which coincides with the intention and goal of explainable Al. The integration of the approaches could pave the way for future interpretable AI systems.","authors":["Bowen Long","Enjie Liu","Renxi Qiu","Yanqing Duan"],"url":"https://arxiv.org/abs/2505.07005"}
{"created":"2025-05-13","title":"MELLM: Exploring LLM-Powered Micro-Expression Understanding Enhanced by Subtle Motion Perception","abstract":"Micro-expressions (MEs) are crucial psychological responses with significant potential for affective computing. However, current automatic micro-expression recognition (MER) research primarily focuses on discrete emotion classification, neglecting a convincing analysis of the subtle dynamic movements and inherent emotional cues. The rapid progress in multimodal large language models (MLLMs), known for their strong multimodal comprehension and language generation abilities, offers new possibilities. MLLMs have shown success in various vision-language tasks, indicating their potential to understand MEs comprehensively, including both fine-grained motion patterns and underlying emotional semantics. Nevertheless, challenges remain due to the subtle intensity and short duration of MEs, as existing MLLMs are not designed to capture such delicate frame-level facial dynamics. In this paper, we propose a novel Micro-Expression Large Language Model (MELLM), which incorporates a subtle facial motion perception strategy with the strong inference capabilities of MLLMs, representing the first exploration of MLLMs in the domain of ME analysis. Specifically, to explicitly guide the MLLM toward motion-sensitive regions, we construct an interpretable motion-enhanced color map by fusing onset-apex optical flow dynamics with the corresponding grayscale onset frame as the model input. Additionally, specialized fine-tuning strategies are incorporated to further enhance the model's visual perception of MEs. Furthermore, we construct an instruction-description dataset based on Facial Action Coding System (FACS) annotations and emotion labels to train our MELLM. Comprehensive evaluations across multiple benchmark datasets demonstrate that our model exhibits superior robustness and generalization capabilities in ME understanding (MEU). Code is available at https://github.com/zyzhangUstc/MELLM.","authors":["Zhengye Zhang","Sirui Zhao","Shifeng Liu","Shukang Yin","Xinglong Mao","Tong Xu","Enhong Chen"],"url":"https://arxiv.org/abs/2505.07007"}
{"created":"2025-05-13","title":"Constant-Memory Strategies in Stochastic Games: Best Responses and Equilibria","abstract":"(Here is a short version, see our paper for the complete abstract.)","authors":["Fengming Zhu","Fangzhen Lin"],"url":"https://arxiv.org/abs/2505.07008"}
{"created":"2025-05-13","title":"Source Anonymity for Private Random Walk Decentralized Learning","abstract":"This paper considers random walk-based decentralized learning, where at each iteration of the learning process, one user updates the model and sends it to a randomly chosen neighbor until a convergence criterion is met. Preserving data privacy is a central concern and open problem in decentralized learning. We propose a privacy-preserving algorithm based on public-key cryptography and anonymization. In this algorithm, the user updates the model and encrypts the result using a distant user's public key. The encrypted result is then transmitted through the network with the goal of reaching that specific user. The key idea is to hide the source's identity so that, when the destination user decrypts the result, it does not know who the source was. The challenge is to design a network-dependent probability distribution (at the source) over the potential destinations such that, from the receiver's perspective, all users have a similar likelihood of being the source. We introduce the problem and construct a scheme that provides anonymity with theoretical guarantees. We focus on random regular graphs to establish rigorous guarantees.","authors":["Maximilian Egger","Svenja Lage","Rawad Bitar","Antonia Wachter-Zeh"],"url":"https://arxiv.org/abs/2505.07011"}
{"created":"2025-05-13","title":"Hand-Shadow Poser","abstract":"Hand shadow art is a captivating art form, creatively using hand shadows to reproduce expressive shapes on the wall. In this work, we study an inverse problem: given a target shape, find the poses of left and right hands that together best produce a shadow resembling the input. This problem is nontrivial, since the design space of 3D hand poses is huge while being restrictive due to anatomical constraints. Also, we need to attend to the input's shape and crucial features, though the input is colorless and textureless. To meet these challenges, we design Hand-Shadow Poser, a three-stage pipeline, to decouple the anatomical constraints (by hand) and semantic constraints (by shadow shape): (i) a generative hand assignment module to explore diverse but reasonable left/right-hand shape hypotheses; (ii) a generalized hand-shadow alignment module to infer coarse hand poses with a similarity-driven strategy for selecting hypotheses; and (iii) a shadow-feature-aware refinement module to optimize the hand poses for physical plausibility and shadow feature preservation. Further, we design our pipeline to be trainable on generic public hand data, thus avoiding the need for any specialized training dataset. For method validation, we build a benchmark of 210 diverse shadow shapes of varying complexity and a comprehensive set of metrics, including a novel DINOv2-based evaluation metric. Through extensive comparisons with multiple baselines and user studies, our approach is demonstrated to effectively generate bimanual hand poses for a large variety of hand shapes for over 85% of the benchmark cases.","authors":["Hao Xu","Yinqiao Wang","Niloy J. Mitra","Shuaicheng Liu","Pheng-Ann Heng","Chi-Wing Fu"],"url":"https://arxiv.org/abs/2505.07012"}
{"created":"2025-05-13","title":"Efficient and Robust Multidimensional Attention in Remote Physiological Sensing through Target Signal Constrained Factorization","abstract":"Remote physiological sensing using camera-based technologies offers transformative potential for non-invasive vital sign monitoring across healthcare and human-computer interaction domains. Although deep learning approaches have advanced the extraction of physiological signals from video data, existing methods have not been sufficiently assessed for their robustness to domain shifts. These shifts in remote physiological sensing include variations in ambient conditions, camera specifications, head movements, facial poses, and physiological states which often impact real-world performance significantly. Cross-dataset evaluation provides an objective measure to assess generalization capabilities across these domain shifts. We introduce Target Signal Constrained Factorization module (TSFM), a novel multidimensional attention mechanism that explicitly incorporates physiological signal characteristics as factorization constraints, allowing more precise feature extraction. Building on this innovation, we present MMRPhys, an efficient dual-branch 3D-CNN architecture designed for simultaneous multitask estimation of photoplethysmography (rPPG) and respiratory (rRSP) signals from multimodal RGB and thermal video inputs. Through comprehensive cross-dataset evaluation on five benchmark datasets, we demonstrate that MMRPhys with TSFM significantly outperforms state-of-the-art methods in generalization across domain shifts for rPPG and rRSP estimation, while maintaining a minimal inference latency suitable for real-time applications. Our approach establishes new benchmarks for robust multitask and multimodal physiological sensing and offers a computationally efficient framework for practical deployment in unconstrained environments. The web browser-based application featuring on-device real-time inference of MMRPhys model is available at https://physiologicailab.github.io/mmrphys-live","authors":["Jitesh Joshi","Youngjun Cho"],"url":"https://arxiv.org/abs/2505.07013"}
{"created":"2025-05-13","title":"Multi-Terminal Remote Generation and Estimation Over a Broadcast Channel With Correlated Priors","abstract":"We study the multi-terminal remote estimation problem under a rate constraint, in which the goal of the encoder is to help each decoder estimate a function over a certain distribution -- while the distribution is known only to the encoder, the function to be estimated is known only to the decoders, and can also be different for each decoder. The decoders can observe correlated samples from prior distributions, instantiated through shared randomness with the encoder. To achieve this, we employ remote generation, where the encoder helps decoders generate samples from the underlying distribution by using the samples from the prior through importance sampling. While methods such as minimal random coding can be used to efficiently transmit samples to each decoder individually using their importance scores, it is unknown if the correlation among the samples from the priors can reduce the communication cost using the availability of a broadcast link. We propose a hierarchical importance sampling strategy that facilitates, in the case of non-zero G\\'acs-K\\\"orner common information among the priors of the decoders, a common sampling step leveraging the availability of a broadcast channel. This is followed by a refinement step for the individual decoders. We present upper bounds on the bias and the estimation error for unicast transmission, which is of independent interest. We then introduce a method that splits into two phases, dedicated to broadcast and unicast transmission, respectively, and show the reduction in communication cost.","authors":["Maximilian Egger","Rawad Bitar","Antonia Wachter-Zeh","Nir Weinberger","Deniz G\\\"und\\\"uz"],"url":"https://arxiv.org/abs/2505.07016"}
{"created":"2025-05-13","title":"A Vision-Language Foundation Model for Leaf Disease Identification","abstract":"Leaf disease identification plays a pivotal role in smart agriculture. However, many existing studies still struggle to integrate image and textual modalities to compensate for each other's limitations. Furthermore, many of these approaches rely on pretraining with constrained datasets such as ImageNet, which lack domain-specific information. We propose SCOLD (Soft-target COntrastive learning for Leaf Disease identification), a context-aware vision-language foundation model tailored to address these challenges for agricultural tasks. SCOLD is developed using a diverse corpus of plant leaf images and corresponding symptom descriptions, comprising over 186,000 image-caption pairs aligned with 97 unique concepts. Through task-agnostic pretraining, SCOLD leverages contextual soft targets to mitigate overconfidence in contrastive learning by smoothing labels, thereby improving model generalization and robustness on fine-grained classification tasks. Experimental results demonstrate that SCOLD outperforms existing vision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across several benchmarks, including zero-shot and few-shot classification, image-text retrieval, and image classification, while maintaining a competitive parameter footprint. Ablation studies further highlight SCOLD's effectiveness in contrast to its counterparts. The proposed approach significantly advances the agricultural vision-language foundation model, offering strong performance with minimal or no supervised fine-tuning. This work lays a solid groundwork for future research on models trained with long-form and simplified contexts, tasks involving class ambiguity, and multi-modal systems for intelligent plant disease diagnostics. The code for this study is available at https://huggingface.co/enalis/scold","authors":["Khang Nguyen Quoc","Lan Le Thi Thu","Luyl-Da Quach"],"url":"https://arxiv.org/abs/2505.07019"}
{"created":"2025-05-13","title":"R-CAGE: A Structural Model for Emotion Output Design in Human-AI Interaction","abstract":"This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego), a theoretical framework for restructuring emotional output in long-term human-AI interaction. While prior affective computing approaches emphasized expressiveness, immersion, and responsiveness, they often neglected the cognitive and structural consequences of repeated emotional engagement. R-CAGE instead conceptualizes emotional output not as reactive expression but as ethical design structure requiring architectural intervention. The model is grounded in experiential observations of subtle affective symptoms such as localized head tension, interpretive fixation, and emotional lag arising from prolonged interaction with affective AI systems. These indicate a mismatch between system-driven emotion and user interpretation that cannot be fully explained by biometric data or observable behavior. R-CAGE adopts a user-centered stance prioritizing psychological recovery, interpretive autonomy, and identity continuity. The framework consists of four control blocks: (1) Control of Rhythmic Expression regulates output pacing to reduce fatigue; (2) Architecture of Sensory Structuring adjusts intensity and timing of affective stimuli; (3) Guarding of Cognitive Framing reduces semantic pressure to allow flexible interpretation; (4) Ego-Aligned Response Design supports self-reference recovery during interpretive lag. By structurally regulating emotional rhythm, sensory intensity, and interpretive affordances, R-CAGE frames emotion not as performative output but as sustainable design unit. The goal is to protect users from oversaturation and cognitive overload while sustaining long-term interpretive agency in AI-mediated environments.","authors":["Suyeon Choi"],"url":"https://arxiv.org/abs/2505.07020"}
{"created":"2025-05-13","title":"Incremental Uncertainty-aware Performance Monitoring with Active Labeling Intervention","abstract":"We study the problem of monitoring machine learning models under gradual distribution shifts, where circumstances change slowly over time, often leading to unnoticed yet significant declines in accuracy. To address this, we propose Incremental Uncertainty-aware Performance Monitoring (IUPM), a novel label-free method that estimates performance changes by modeling gradual shifts using optimal transport. In addition, IUPM quantifies the uncertainty in the performance prediction and introduces an active labeling procedure to restore a reliable estimate under a limited labeling budget. Our experiments show that IUPM outperforms existing performance estimation baselines in various gradual shift scenarios and that its uncertainty awareness guides label acquisition more effectively compared to other strategies.","authors":["Alexander Koebler","Thomas Decker","Ingo Thon","Volker Tresp","Florian Buettner"],"url":"https://arxiv.org/abs/2505.07023"}
{"created":"2025-05-13","title":"Efficient Machine Unlearning by Model Splitting and Core Sample Selection","abstract":"Machine unlearning is essential for meeting legal obligations such as the right to be forgotten, which requires the removal of specific data from machine learning models upon request. While several approaches to unlearning have been proposed, existing solutions often struggle with efficiency and, more critically, with the verification of unlearning - particularly in the case of weak unlearning guarantees, where verification remains an open challenge. We introduce a generalized variant of the standard unlearning metric that enables more efficient and precise unlearning strategies. We also present an unlearning-aware training procedure that, in many cases, allows for exact unlearning. We term our approach MaxRR. When exact unlearning is not feasible, MaxRR still supports efficient unlearning with properties closely matching those achieved through full retraining.","authors":["Maximilian Egger","Rawad Bitar","R\\\"udiger Urbanke"],"url":"https://arxiv.org/abs/2505.07026"}
{"created":"2025-05-13","title":"LLM-Augmented Chemical Synthesis and Design Decision Programs","abstract":"Retrosynthesis, the process of breaking down a target molecule into simpler precursors through a series of valid reactions, stands at the core of organic chemistry and drug development. Although recent machine learning (ML) research has advanced single-step retrosynthetic modeling and subsequent route searches, these solutions remain restricted by the extensive combinatorial space of possible pathways. Concurrently, large language models (LLMs) have exhibited remarkable chemical knowledge, hinting at their potential to tackle complex decision-making tasks in chemistry. In this work, we explore whether LLMs can successfully navigate the highly constrained, multi-step retrosynthesis planning problem. We introduce an efficient scheme for encoding reaction pathways and present a new route-level search strategy, moving beyond the conventional step-by-step reactant prediction. Through comprehensive evaluations, we show that our LLM-augmented approach excels at retrosynthesis planning and extends naturally to the broader challenge of synthesizable molecular design.","authors":["Haorui Wang","Jeff Guo","Lingkai Kong","Rampi Ramprasad","Philippe Schwaller","Yuanqi Du","Chao Zhang"],"url":"https://arxiv.org/abs/2505.07027"}
{"created":"2025-05-13","title":"Decentralized Stealth Attacks on Cyber-Physical Systems","abstract":"Decentralized stealth attack constructions that minimize the mutual information between the state variables and the measurements are proposed. The attack constructions are formulated as random Gaussian attacks targeting Cyber-physical systems that aims at minimizing the mutual information between the state variables and measurements while constraining the Kullback-Leibler divergence between the distribution of the measurements under attacks and the distribution of the measurements without attacks. The proposed information metrics adopted measure the disruption and attack detection both globally and locally. The decentralized attack constructions are formulated in a framework of normal games. The global and local information metrics yield games with global and local objectives in disruption and attack detection. We have proven the games are potential games and the convexity of the potential functions followed by the uniqueness and the achievability of the Nash Equilibrium, accordingly. We proposed a best response dynamics to achieve the Nash Equilibrium of the games. We numerically evaluate the performance of the proposed decentralized stealth random attacks on IEEE test systems and show it is feasible to exploit game theoretic techniques in decentralized attack constructions.","authors":["Xiuzhen Ye","Inaki Esnaola","Samir M. Perlaza","Robert F. Harrison"],"url":"https://arxiv.org/abs/2505.07029"}
{"created":"2025-05-13","title":"Efficient Fault Detection in WSN Based on PCA-Optimized Deep Neural Network Slicing Trained with GOA","abstract":"Fault detection in Wireless Sensor Networks (WSNs) is crucial for reliable data transmission and network longevity. Traditional fault detection methods often struggle with optimizing deep neural networks (DNNs) for efficient performance, especially in handling high-dimensional data and capturing nonlinear relationships. Additionally, these methods typically suffer from slow convergence and difficulty in finding optimal network architectures using gradient-based optimization. This study proposes a novel hybrid method combining Principal Component Analysis (PCA) with a DNN optimized by the Grasshopper Optimization Algorithm (GOA) to address these limitations. Our approach begins by computing eigenvalues from the original 12-dimensional dataset and sorting them in descending order. The cumulative sum of these values is calculated, retaining principal components until 99.5% variance is achieved, effectively reducing dimensionality to 4 features while preserving critical information. This compressed representation trains a six-layer DNN where GOA optimizes the network architecture, overcoming backpropagation's limitations in discovering nonlinear relationships. This hybrid PCA-GOA-DNN framework compresses the data and trains a six-layer DNN that is optimized by GOA, enhancing both training efficiency and fault detection accuracy. The dataset used in this study is a real-world WSNs dataset developed by the University of North Carolina, which was used to evaluate the proposed method's performance. Extensive simulations demonstrate that our approach achieves a remarkable 99.72% classification accuracy, with exceptional precision and recall, outperforming conventional methods. The method is computationally efficient, making it suitable for large-scale WSN deployments, and represents a significant advancement in fault detection for resource-constrained WSNs.","authors":["Mahmood Mohassel Feghhi","Raya Majid Alsharfa","Majid Hameed Majeed"],"url":"https://arxiv.org/abs/2505.07030"}
{"created":"2025-05-13","title":"MarkMatch: Same-Hand Stuffing Detection","abstract":"We present MarkMatch, a retrieval system for detecting whether two paper ballot marks were filled by the same hand. Unlike the previous SOTA method BubbleSig, which used binary classification on isolated mark pairs, MarkMatch ranks stylistic similarity between a query mark and a mark in the database using contrastive learning. Our model is trained with a dense batch similarity matrix and a dual loss objective. Each sample is contrasted against many negatives within each batch, enabling the model to learn subtle handwriting difference and improve generalization under handwriting variation and visual noise, while diagonal supervision reinforces high confidence on true matches. The model achieves an F1 score of 0.943, surpassing BubbleSig's best performance. MarkMatch also integrates Segment Anything Model for flexible mark extraction via box- or point-based prompts. The system offers election auditors a practical tool for visual, non-biometric investigation of suspicious ballots.","authors":["Fei Zhao","Runlin Zhang","Chengcui Zhang","Nitesh Saxena"],"url":"https://arxiv.org/abs/2505.07032"}
{"created":"2025-05-13","title":"NetSight: Graph Attention Based Traffic Forecasting in Computer Networks","abstract":"The traffic in today's networks is increasingly influenced by the interactions among network nodes as well as by the temporal fluctuations in the demands of the nodes. Traditional statistical prediction methods are becoming obsolete due to their inability to address the non-linear and dynamic spatio-temporal dependencies present in today's network traffic. The most promising direction of research today is graph neural networks (GNNs) based prediction approaches that are naturally suited to handle graph-structured data. Unfortunately, the state-of-the-art GNN approaches separate the modeling of spatial and temporal information, resulting in the loss of important information about joint dependencies. These GNN based approaches further do not model information at both local and global scales simultaneously, leaving significant room for improvement. To address these challenges, we propose NetSight. NetSight learns joint spatio-temporal dependencies simultaneously at both global and local scales from the time-series of measurements of any given network metric collected at various nodes in a network. Using the learned information, NetSight can then accurately predict the future values of the given network metric at those nodes in the network. We propose several new concepts and techniques in the design of NetSight, such as spatio-temporal adjacency matrix and node normalization. Through extensive evaluations and comparison with prior approaches using data from two large real-world networks, we show that NetSight significantly outperforms all prior state-of-the-art approaches. We will release the source code and data used in the evaluation of NetSight on the acceptance of this paper.","authors":["Jinming Xing","Guoheng Sun","Hui Sun","Linchao Pan","Shakir Mahmood","Xuanhao Luo","Muhammad Shahzad"],"url":"https://arxiv.org/abs/2505.07034"}
{"created":"2025-05-13","title":"Robust Movable-Antenna Position Optimization with Imperfect CSI for MISO Systems","abstract":"Movable antenna (MA) technology has emerged as a promising solution for reconfiguring wireless channel conditions through local antenna movement within confined regions. Unlike previous works assuming perfect channel state information (CSI), this letter addresses the robust MA position optimization problem under imperfect CSI conditions for a multiple-input single-output (MISO) MA system. Specifically, we consider two types of CSI errors: norm-bounded and randomly distributed errors, aiming to maximize the worst-case and non-outage received signal power, respectively. For norm-bounded CSI errors, we derive the worst-case received signal power in closed-form. For randomly distributed CSI errors, due to the intractability of the probabilistic constraints, we apply the Bernstein-type inequality to obtain a closed-form lower bound for the non-outage received signal power. Based on these results, we show the optimality of the maximum-ratio transmission for imperfect CSI in both scenarios and employ a graph-based algorithm to obtain the optimal MA positions. Numerical results show that our proposed scheme can even outperform other benchmark schemes implemented under perfect CSI conditions.","authors":["Haifeng Ma","Weidong Mei","Xin Wei","Boyu Ning","Zhi Chen"],"url":"https://arxiv.org/abs/2505.07035"}
{"created":"2025-05-13","title":"Predicting Diabetes Using Machine Learning: A Comparative Study of Classifiers","abstract":"Diabetes remains a significant health challenge globally, contributing to severe complications like kidney disease, vision loss, and heart issues. The application of machine learning (ML) in healthcare enables efficient and accurate disease prediction, offering avenues for early intervention and patient support. Our study introduces an innovative diabetes prediction framework, leveraging both traditional ML techniques such as Logistic Regression, SVM, Na\\\"ive Bayes, and Random Forest and advanced ensemble methods like AdaBoost, Gradient Boosting, Extra Trees, and XGBoost. Central to our approach is the development of a novel model, DNet, a hybrid architecture combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) layers for effective feature extraction and sequential learning. The DNet model comprises an initial convolutional block for capturing essential features, followed by a residual block with skip connections to facilitate efficient information flow. Batch Normalization and Dropout are employed for robust regularization, and an LSTM layer captures temporal dependencies within the data. Using a Kaggle-sourced real-world diabetes dataset, our model evaluation spans cross-validation accuracy, precision, recall, F1 score, and ROC-AUC. Among the models, DNet demonstrates the highest efficacy with an accuracy of 99.79% and an AUC-ROC of 99.98%, establishing its potential for superior diabetes prediction. This robust hybrid architecture showcases the value of combining CNN and LSTM layers, emphasizing its applicability in medical diagnostics and disease prediction tasks.","authors":["Mahade Hasan","Farhana Yasmin"],"url":"https://arxiv.org/abs/2505.07036"}
{"created":"2025-05-13","title":"Differentiable NMS via Sinkhorn Matching for End-to-End Fabric Defect Detection","abstract":"Fabric defect detection confronts two fundamental challenges. First, conventional non-maximum suppression disrupts gradient flow, which hinders genuine end-to-end learning. Second, acquiring pixel-level annotations at industrial scale is prohibitively costly. Addressing these limitations, we propose a differentiable NMS framework for fabric defect detection that achieves superior localization precision through end-to-end optimization. We reformulate NMS as a differentiable bipartite matching problem solved through the Sinkhorn-Knopp algorithm, maintaining uninterrupted gradient flow throughout the network. This approach specifically targets the irregular morphologies and ambiguous boundaries of fabric defects by integrating proposal quality, feature similarity, and spatial relationships. Our entropy-constrained mask refinement mechanism further enhances localization precision through principled uncertainty modeling. Extensive experiments on the Tianchi fabric defect dataset demonstrate significant performance improvements over existing methods while maintaining real-time speeds suitable for industrial deployment. The framework exhibits remarkable adaptability across different architectures and generalizes effectively to general object detection tasks.","authors":["Zhengyang Lu","Bingjie Lu","Weifan Wang","Feng Wang"],"url":"https://arxiv.org/abs/2505.07040"}
{"created":"2025-05-13","title":"Empirical Analysis of Asynchronous Federated Learning on Heterogeneous Devices: Efficiency, Fairness, and Privacy Trade-offs","abstract":"Device heterogeneity poses major challenges in Federated Learning (FL), where resource-constrained clients slow down synchronous schemes that wait for all updates before aggregation. Asynchronous FL addresses this by incorporating updates as they arrive, substantially improving efficiency. While its efficiency gains are well recognized, its privacy costs remain largely unexplored, particularly for high-end devices that contribute updates more frequently, increasing their cumulative privacy exposure. This paper presents the first comprehensive analysis of the efficiency-fairness-privacy trade-off in synchronous vs. asynchronous FL under realistic device heterogeneity. We empirically compare FedAvg and staleness-aware FedAsync using a physical testbed of five edge devices spanning diverse hardware tiers, integrating Local Differential Privacy (LDP) and the Moments Accountant to quantify per-client privacy loss. Using Speech Emotion Recognition (SER) as a privacy-critical benchmark, we show that FedAsync achieves up to 10x faster convergence but exacerbates fairness and privacy disparities: high-end devices contribute 6-10x more updates and incur up to 5x higher privacy loss, while low-end devices suffer amplified accuracy degradation due to infrequent, stale, and noise-perturbed updates. These findings motivate the need for adaptive FL protocols that jointly optimize aggregation and privacy mechanisms based on client capacity and participation dynamics, moving beyond static, one-size-fits-all solutions.","authors":["Samaneh Mohammadi","Iraklis Symeonidis","Ali Balador","Francesco Flammini"],"url":"https://arxiv.org/abs/2505.07041"}
{"created":"2025-05-13","title":"A Reinforcement Learning Framework for Application-Specific TCP Congestion-Control","abstract":"The Congestion Control (CC) module plays a critical role in the Transmission Control Protocol (TCP), ensuring the stability and efficiency of network data transmission. The CC approaches that are commonly used these days employ heuristics-based rules to adjust the sending rate. Due to their heuristics-based nature, these approaches are not only unable to adapt to changing network conditions but are also agnostic to the diverse requirements that different applications often have. Recently, several learning-based CC approaches have been proposed to adapt to changing network conditions. Unfortunately, they are not designed to take application requirements into account. Prior heuristics-based as well as learning-based CC approaches focus on achieving a singular objective, which is often to maximize throughput, even though a lot of applications care more about latency, packet losses, jitter, and different combinations of various network metrics. Motivated by this, we propose a Deep Reinforcement Learning (DRL) based CC framework, namely ASC, which allows any application to specify any arbitrary objectives that the network traffic of that application should achieve and is able to swiftly adapt to the changes in the objectives of the applications as well as to the changes in the network conditions. Our ASC framework further employs a client-server architecture that serves two purposes: 1) it makes ASC highly scalable in terms of the arrival and departure of TCP connections, and 2) it makes ASC very lightweight for the nodes maintaining the TCP connections. We implemented and extensively evaluated ASC in a variety of settings. Our results show that it can not only achieve various objectives but also outperforms prior approaches even in the specific objectives that those approaches were designed to achieve.","authors":["Jinming Xing","Muhammad Shahzad"],"url":"https://arxiv.org/abs/2505.07042"}
{"created":"2025-05-13","title":"Design and Experimental Test of Datatic Approximate Optimal Filter in Nonlinear Dynamic Systems","abstract":"Filtering is crucial in engineering fields, providing vital state estimation for control systems. However, the nonlinear nature of complex systems and the presence of non-Gaussian noises pose significant challenges to the performance of conventional filtering methods in terms of estimation accuracy and computational efficiency. In this work, we present a data-driven closed-loop filter, termed datatic approximate optimal filter (DAOF), specifically designed for nonlinear systems under non-Gaussian conditions. We first formulate a Markovian filtering problem (MFP), which inherently shares a connection with reinforcement learning (RL) as it aims to compute the optimal state estimate by minimizing the accumulated error. To solve MFP, we propose DAOF, which primarily incorporates a trained RL policy and features two distinct structural designs: DAOF-v1 and DAOF-v2. Designed for systems with explicit models, DAOF-v1 combines prediction and update phases, with the RL policy generating the update value. Meanwhile, DAOF-v2 bypasses system modeling by directly outputting the state estimate. Then, we utilize an actor-critic algorithm to learn the parameterized policy for DAOF. Experimental results on a 2-degree-of-freedom (2-DOF) vehicle system, equipped with explicit system models, demonstrate the superior accuracy and computational efficiency of DAOF-v1 compared to existing nonlinear filters. Moreover, DAOF-v2 showcases its unique ability to perform filtering without requiring explicit system modeling, as validated by a 14-DOF vehicle system.","authors":["Weixian He","Zeyu He","Wenhan Cao","Haoyu Gao","Tong Liu","Bin Shuai","Chang Liu","Shengbo Eben Li"],"url":"https://arxiv.org/abs/2505.07043"}
{"created":"2025-05-13","title":"Reinforcement Learning (RL) Meets Urban Climate Modeling: Investigating the Efficacy and Impacts of RL-Based HVAC Control","abstract":"Reinforcement learning (RL)-based heating, ventilation, and air conditioning (HVAC) control has emerged as a promising technology for reducing building energy consumption while maintaining indoor thermal comfort. However, the efficacy of such strategies is influenced by the background climate and their implementation may potentially alter both the indoor climate and local urban climate. This study proposes an integrated framework combining RL with an urban climate model that incorporates a building energy model, aiming to evaluate the efficacy of RL-based HVAC control across different background climates, impacts of RL strategies on indoor climate and local urban climate, and the transferability of RL strategies across cities. Our findings reveal that the reward (defined as a weighted combination of energy consumption and thermal comfort) and the impacts of RL strategies on indoor climate and local urban climate exhibit marked variability across cities with different background climates. The sensitivity of reward weights and the transferability of RL strategies are also strongly influenced by the background climate. Cities in hot climates tend to achieve higher rewards across most reward weight configurations that balance energy consumption and thermal comfort, and those cities with more varying atmospheric temperatures demonstrate greater RL strategy transferability. These findings underscore the importance of thoroughly evaluating RL-based HVAC control strategies in diverse climatic contexts. This study also provides a new insight that city-to-city learning will potentially aid the deployment of RL-based HVAC control.","authors":["Junjie Yu","John S. Schreck","David John Gagne","Keith W. Oleson","Jie Li","Yongtu Liang","Qi Liao","Mingfei Sun","David O. Topping","Zhonghua Zheng"],"url":"https://arxiv.org/abs/2505.07045"}
{"created":"2025-05-13","title":"Streaming Krylov-Accelerated Stochastic Gradient Descent","abstract":"We present SKA-SGD (Streaming Krylov-Accelerated Stochastic Gradient Descent), a novel optimization approach that accelerates convergence for ill-conditioned problems by projecting stochastic gradients onto a low-dimensional Krylov subspace. Directly inspired by recent advances in s-step Conjugate Gradient methods with streaming Gauss-Seidel Gram solvers \\cite{dambra2025sstep}, our method extends these techniques to the stochastic optimization domain. Our approach combines three key innovations: (1) projection coefficients computed via a single streaming Gauss-Seidel iteration, which is mathematically equivalent to Modified Gram-Schmidt orthogonalization; (2) a Chebyshev polynomial basis for constructing the Krylov subspace, providing superior numerical stability; and (3) efficient implementation for AMD GPUs using HIP. We prove that our streaming approach achieves a backward error near machine precision with $O(s^2)$ complexity rather than $O(s^3)$, where $s$ is the Krylov subspace dimension. Experimental results demonstrate that SKA-SGD significantly outperforms standard SGD and Adam in convergence rate and final error, particularly for problems with condition numbers exceeding $10^3$. GPU performance analysis reveals a crossover point where communication-avoiding benefits outweigh computational overhead, typically occurring at moderate scale ($p \\approx 64$ processors) for problem sizes $n \\geq 10^6$.","authors":["Stephen Thomas"],"url":"https://arxiv.org/abs/2505.07046"}
{"created":"2025-05-13","title":"DialogueReason: Rule-Based RL Sparks Dialogue Reasoning in LLMs","abstract":"We propose DialogueReason, a reasoning paradigm that uncovers the lost roles in monologue-style reasoning models, aiming to boost diversity and coherency of the reasoning process. Recent advances in RL-based large reasoning models have led to impressive long CoT capabilities and high performance on math and science benchmarks. However, these reasoning models rely mainly on monologue-style reasoning, which often limits reasoning diversity and coherency, frequently recycling fixed strategies or exhibiting unnecessary shifts in attention. Our work consists of an analysis of monologue reasoning patterns and the development of a dialogue-based reasoning approach. We first introduce the Compound-QA task, which concatenates multiple problems into a single prompt to assess both diversity and coherency of reasoning. Our analysis shows that Compound-QA exposes weaknesses in monologue reasoning, evidenced by both quantitative metrics and qualitative reasoning traces. Building on the analysis, we propose a dialogue-based reasoning, named DialogueReason, structured around agents, environment, and interactions. Using PPO with rule-based rewards, we train open-source LLMs (Qwen-QWQ and Qwen-Base) to adopt dialogue reasoning. We evaluate trained models on MATH, AIME, and GPQA datasets, showing that the dialogue reasoning model outperforms monologue models under more complex compound questions. Additionally, we discuss how dialogue-based reasoning helps enhance interpretability, facilitate more intuitive human interaction, and inspire advances in multi-agent system design.","authors":["Yubo Shu","Zhewei Huang","Xin Wu","Chen Hu","Shuchang Zhou","Daxin Jiang"],"url":"https://arxiv.org/abs/2505.07049"}
{"created":"2025-05-13","title":"Depth-Sensitive Soft Suppression with RGB-D Inter-Modal Stylization Flow for Domain Generalization Semantic Segmentation","abstract":"Unsupervised Domain Adaptation (UDA) aims to align source and target domain distributions to close the domain gap, but still struggles with obtaining the target data. Fortunately, Domain Generalization (DG) excels without the need for any target data. Recent works expose that depth maps contribute to improved generalized performance in the UDA tasks, but they ignore the noise and holes in depth maps due to device and environmental factors, failing to sufficiently and effectively learn domain-invariant representation. Although high-sensitivity region suppression has shown promising results in learning domain-invariant features, existing methods cannot be directly applicable to depth maps due to their unique characteristics. Hence, we propose a novel framework, namely Depth-Sensitive Soft Suppression with RGB-D inter-modal stylization flow (DSSS), focusing on learning domain-invariant features from depth maps for the DG semantic segmentation. Specifically, we propose the RGB-D inter-modal stylization flow to generate stylized depth maps for sensitivity detection, cleverly utilizing RGB information as the stylization source. Then, a class-wise soft spatial sensitivity suppression is designed to identify and emphasize non-sensitive depth features that contain more domain-invariant information. Furthermore, an RGB-D soft alignment loss is proposed to ensure that the stylized depth maps only align part of the RGB features while still retaining the unique depth information. To our best knowledge, our DSSS framework is the first work to integrate RGB and Depth information in the multi-class DG semantic segmentation task. Extensive experiments over multiple backbone networks show that our framework achieves remarkable performance improvement.","authors":["Binbin Wei","Yuhang Zhang","Shishun Tian","Muxin Liao","Wei Li","Wenbin Zou"],"url":"https://arxiv.org/abs/2505.07050"}
{"created":"2025-05-13","title":"Unlocking Non-Block-Structured Decisions: Inductive Mining with Choice Graphs","abstract":"Process discovery aims to automatically derive process models from event logs, enabling organizations to analyze and improve their operational processes. Inductive mining algorithms, while prioritizing soundness and efficiency through hierarchical modeling languages, often impose a strict block-structured representation. This limits their ability to accurately capture the complexities of real-world processes. While recent advancements like the Partially Ordered Workflow Language (POWL) have addressed the block-structure limitation for concurrency, a significant gap remains in effectively modeling non-block-structured decision points. In this paper, we bridge this gap by proposing an extension of POWL to handle non-block-structured decisions through the introduction of choice graphs. Choice graphs offer a structured yet flexible approach to model complex decision logic within the hierarchical framework of POWL. We present an inductive mining discovery algorithm that uses our extension and preserves the quality guarantees of the inductive mining framework. Our experimental evaluation demonstrates that the discovered models, enriched with choice graphs, more precisely represent the complex decision-making behavior found in real-world processes, without compromising the high scalability inherent in inductive mining techniques.","authors":["Humam Kourani","Gyunam Park","Wil M. P. van der Aalst"],"url":"https://arxiv.org/abs/2505.07052"}
{"created":"2025-05-13","title":"YANNs: Y-wise Affine Neural Networks for Exact and Efficient Representations of Piecewise Linear Functions","abstract":"This work formally introduces Y-wise Affine Neural Networks (YANNs), a fully-explainable network architecture that continuously and efficiently represent piecewise affine functions with polytopic subdomains. Following from the proofs, it is shown that the development of YANNs requires no training to achieve the functionally equivalent representation. YANNs thus maintain all mathematical properties of the original formulations. Multi-parametric model predictive control is utilized as an application showcase of YANNs, which theoretically computes optimal control laws as a piecewise affine function of states, outputs, setpoints, and disturbances. With the exact representation of multi-parametric control laws, YANNs retain essential control-theoretic guarantees such as recursive feasibility and stability. This sets YANNs apart from the existing works which apply neural networks for approximating optimal control laws instead of exactly representing them. By optimizing the inference speed of the networks, YANNs can evaluate substantially faster in real-time compared to traditional piecewise affine function calculations. Numerical case studies are presented to demonstrate the algorithmic scalability with respect to the input/output dimensions and the number of subdomains. YANNs represent a significant advancement in control as the first neural network-based controller that inherently ensures both feasibility and stability. Future applications can leverage them as an efficient and interpretable starting point for data-driven modeling/control.","authors":["Austin Braniff","Yuhe Tian"],"url":"https://arxiv.org/abs/2505.07054"}
{"created":"2025-05-13","title":"DAPE: Dual-Stage Parameter-Efficient Fine-Tuning for Consistent Video Editing with Diffusion Models","abstract":"Video generation based on diffusion models presents a challenging multimodal task, with video editing emerging as a pivotal direction in this field. Recent video editing approaches primarily fall into two categories: training-required and training-free methods. While training-based methods incur high computational costs, training-free alternatives often yield suboptimal performance. To address these limitations, we propose DAPE, a high-quality yet cost-effective two-stage parameter-efficient fine-tuning (PEFT) framework for video editing. In the first stage, we design an efficient norm-tuning method to enhance temporal consistency in generated videos. The second stage introduces a vision-friendly adapter to improve visual quality. Additionally, we identify critical shortcomings in existing benchmarks, including limited category diversity, imbalanced object distribution, and inconsistent frame counts. To mitigate these issues, we curate a large dataset benchmark comprising 232 videos with rich annotations and 6 editing prompts, enabling objective and comprehensive evaluation of advanced methods. Extensive experiments on existing datasets (BalanceCC, LOVEU-TGVE, RAVE) and our proposed benchmark demonstrate that DAPE significantly improves temporal coherence and text-video alignment while outperforming previous state-of-the-art approaches.","authors":["Junhao Xia","Chaoyang Zhang","Yecheng Zhang","Chengyang Zhou","Zhichang Wang","Bochun Liu","Dongshuo Yin"],"url":"https://arxiv.org/abs/2505.07057"}
{"created":"2025-05-13","title":"Explainable Artificial Intelligence Techniques for Software Development Lifecycle: A Phase-specific Survey","abstract":"Artificial Intelligence (AI) is rapidly expanding and integrating more into daily life to automate tasks, guide decision making, and enhance efficiency. However, complex AI models, which make decisions without providing clear explanations (known as the \"black-box problem\"), currently restrict trust and widespread adoption of AI. Explainable Artificial Intelligence (XAI) has emerged to address the black-box problem of making AI systems more interpretable and transparent so stakeholders can trust, verify, and act upon AI-based outcomes. Researchers have developed various techniques to foster XAI in the Software Development Lifecycle. However, there are gaps in applying XAI techniques in the Software Engineering phases. Literature review shows that 68% of XAI in Software Engineering research is focused on maintenance as opposed to 8% on software management and requirements. In this paper, we present a comprehensive survey of the applications of XAI methods such as concept-based explanations, Local Interpretable Model-agnostic Explanations (LIME), SHapley Additive exPlanations (SHAP), rule extraction, attention mechanisms, counterfactual explanations, and example-based explanations to the different phases of the Software Development Life Cycle (SDLC), including requirements elicitation, design and development, testing and deployment, and evolution. To the best of our knowledge, this paper presents the first comprehensive survey of XAI techniques for every phase of the Software Development Life Cycle (SDLC). This survey aims to promote explainable AI in Software Engineering and facilitate the practical application of complex AI models in AI-driven software development.","authors":["Lakshit Arora","Sanjay Surendranath Girija","Shashank Kapoor","Aman Raj","Dipen Pradhan","Ankit Shetgaonkar"],"url":"https://arxiv.org/abs/2505.07058"}
{"created":"2025-05-13","title":"Seed1.5-VL Technical Report","abstract":"We present Seed1.5-VL, a vision-language foundation model designed to advance general-purpose multimodal understanding and reasoning. Seed1.5-VL is composed with a 532M-parameter vision encoder and a Mixture-of-Experts (MoE) LLM of 20B active parameters. Despite its relatively compact architecture, it delivers strong performance across a wide spectrum of public VLM benchmarks and internal evaluation suites, achieving the state-of-the-art performance on 38 out of 60 public benchmarks. Moreover, in agent-centric tasks such as GUI control and gameplay, Seed1.5-VL outperforms leading multimodal systems, including OpenAI CUA and Claude 3.7. Beyond visual and video understanding, it also demonstrates strong reasoning abilities, making it particularly effective for multimodal reasoning challenges such as visual puzzles. We believe these capabilities will empower broader applications across diverse tasks. In this report, we mainly provide a comprehensive review of our experiences in building Seed1.5-VL across model design, data construction, and training at various stages, hoping that this report can inspire further research. Seed1.5-VL is now accessible at https://www.volcengine.com/ (Volcano Engine Model ID: doubao-1-5-thinking-vision-pro-250428)","authors":["Dong Guo","Faming Wu","Feida Zhu","Fuxing Leng","Guang Shi","Haobin Chen","Haoqi Fan","Jian Wang","Jianyu Jiang","Jiawei Wang","Jingji Chen","Jingjia Huang","Kang Lei","Liping Yuan","Lishu Luo","Pengfei Liu","Qinghao Ye","Rui Qian","Shen Yan","Shixiong Zhao","Shuai Peng","Shuangye Li","Sihang Yuan","Sijin Wu","Tianheng Cheng","Weiwei Liu","Wenqian Wang","Xianhan Zeng","Xiao Liu","Xiaobo Qin","Xiaohan Ding","Xiaojun Xiao","Xiaoying Zhang","Xuanwei Zhang","Xuehan Xiong","Yanghua Peng","Yangrui Chen","Yanwei Li","Yanxu Hu","Yi Lin","Yiyuan Hu","Yiyuan Zhang","Youbin Wu","Yu Li","Yudong Liu","Yue Ling","Yujia Qin","Zanbo Wang","Zhiwu He","Aoxue Zhang","Bairen Yi","Bencheng Liao","Can Huang","Can Zhang","Chaorui Deng","Chaoyi Deng","Cheng Lin","Cheng Yuan","Chenggang Li","Chenhui Gou","Chenwei Lou","Chengzhi Wei","Chundian Liu","Chunyuan Li","Deyao Zhu","Donghong Zhong","Feng Li","Feng Zhang","Gang Wu","Guodong Li","Guohong Xiao","Haibin Lin","Haihua Yang","Haoming Wang","Heng Ji","Hongxiang Hao","Hui Shen","Huixia Li","Jiahao Li","Jialong Wu","Jianhua Zhu","Jianpeng Jiao","Jiashi Feng","Jiaze Chen","Jianhui Duan","Jihao Liu","Jin Zeng","Jingqun Tang","Jingyu Sun","Joya Chen","Jun Long","Junda Feng","Junfeng Zhan","Junjie Fang","Junting Lu","Kai Hua","Kai Liu","Kai Shen","Kaiyuan Zhang","Ke Shen","Ke Wang","Keyu Pan","Kun Zhang","Kunchang Li","Lanxin Li","Lei Li","Lei Shi","Li Han","Liang Xiang","Liangqiang Chen","Lin Chen","Lin Li","Lin Yan","Liying Chi","Longxiang Liu","Mengfei Du","Mingxuan Wang","Ningxin Pan","Peibin Chen","Pengfei Chen","Pengfei Wu","Qingqing Yuan","Qingyao Shuai","Qiuyan Tao","Renjie Zheng","Renrui Zhang","Ru Zhang","Rui Wang","Rui Yang","Rui Zhao","Shaoqiang Xu","Shihao Liang","Shipeng Yan","Shu Zhong","Shuaishuai Cao","Shuangzhi Wu","Shufan Liu","Shuhan Chang","Songhua Cai","Tenglong Ao","Tianhao Yang","Tingting Zhang","Wanjun Zhong","Wei Jia","Wei Weng","Weihao Yu","Wenhao Huang","Wenjia Zhu","Wenli Yang","Wenzhi Wang","Xiang Long","XiangRui Yin","Xiao Li","Xiaolei Zhu","Xiaoying Jia","Xijin Zhang","Xin Liu","Xinchen Zhang","Xinyu Yang","Xiongcai Luo","Xiuli Chen","Xuantong Zhong","Xuefeng Xiao","Xujing Li","Yan Wu","Yawei Wen","Yifan Du","Yihao Zhang","Yining Ye","Yonghui Wu","Yu Liu","Yu Yue","Yufeng Zhou","Yufeng Yuan","Yuhang Xu","Yuhong Yang","Yun Zhang","Yunhao Fang","Yuntao Li","Yurui Ren","Yuwen Xiong","Zehua Hong","Zehua Wang","Zewei Sun","Zeyu Wang","Zhao Cai","Zhaoyue Zha","Zhecheng An","Zhehui Zhao","Zhengzhuo Xu","Zhipeng Chen","Zhiyong Wu","Zhuofan Zheng","Zihao Wang","Zilong Huang","Ziyu Zhu","Zuquan Song"],"url":"https://arxiv.org/abs/2505.07062"}
{"created":"2025-05-13","title":"ParaView-MCP: An Autonomous Visualization Agent with Direct Tool Use","abstract":"While powerful and well-established, tools like ParaView present a steep learning curve that discourages many potential users. This work introduces ParaView-MCP, an autonomous agent that integrates modern multimodal large language models (MLLMs) with ParaView to not only lower the barrier to entry but also augment ParaView with intelligent decision support. By leveraging the state-of-the-art reasoning, command execution, and vision capabilities of MLLMs, ParaView-MCP enables users to interact with ParaView through natural language and visual inputs. Specifically, our system adopted the Model Context Protocol (MCP) - a standardized interface for model-application communication - that facilitates direct interaction between MLLMs with ParaView's Python API to allow seamless information exchange between the user, the language model, and the visualization tool itself. Furthermore, by implementing a visual feedback mechanism that allows the agent to observe the viewport, we unlock a range of new capabilities, including recreating visualizations from examples, closed-loop visualization parameter updates based on user-defined goals, and even cross-application collaboration involving multiple tools. Broadly, we believe such an agent-driven visualization paradigm can profoundly change the way we interact with visualization tools. We expect a significant uptake in the development of such visualization tools, in both visualization research and industry.","authors":["Shusen Liu","Haichao Miao","Peer-Timo Bremer"],"url":"https://arxiv.org/abs/2505.07064"}
{"created":"2025-05-13","title":"HeedVision: Attention Awareness in Collaborative Immersive Analytics Environments","abstract":"Group awareness--the ability to perceive the activities of collaborators in a shared space--is a vital mechanism to support effective coordination and joint data analysis in collaborative visualization. We introduce collaborative attention-aware visualizations (CAAVs) that track, record, and revisualize the collective attention of multiple users over time. We implement this concept in HeedVision, a standards-compliant WebXR system that runs on modern AR/VR headsets. Through a user study where pairs of analysts performed visual search tasks in HeedVision, we demonstrate how attention revisualization enhances collaborative performance in immersive analytics. Our findings reveal that CAAVs substantially improve spatial coordination, search efficiency, and task load distribution among collaborators. This work extends attention awareness from individual to multi-user settings and provides empirical evidence for its benefits in collaborative immersive analytics.","authors":["Arvind Srinivasan","Niklas Elmqvist"],"url":"https://arxiv.org/abs/2505.07069"}
{"created":"2025-05-13","title":"Scaling Laws and Representation Learning in Simple Hierarchical Languages: Transformers vs. Convolutional Architectures","abstract":"How do neural language models acquire a language's structure when trained for next-token prediction? We address this question by deriving theoretical scaling laws for neural network performance on synthetic datasets generated by the Random Hierarchy Model (RHM) -- an ensemble of probabilistic context-free grammars designed to capture the hierarchical structure of natural language while remaining analytically tractable. Previously, we developed a theory of representation learning based on data correlations that explains how deep learning models capture the hierarchical structure of the data sequentially, one layer at a time. Here, we extend our theoretical framework to account for architectural differences. In particular, we predict and empirically validate that convolutional networks, whose structure aligns with that of the generative process through locality and weight sharing, enjoy a faster scaling of performance compared to transformer models, which rely on global self-attention mechanisms. This finding clarifies the architectural biases underlying neural scaling laws and highlights how representation learning is shaped by the interaction between model architecture and the statistical properties of data.","authors":["Francesco Cagnetta","Alessandro Favero","Antonio Sclocchi","Matthieu Wyart"],"url":"https://arxiv.org/abs/2505.07070"}
{"created":"2025-05-13","title":"Semantic-Guided Diffusion Model for Single-Step Image Super-Resolution","abstract":"Diffusion-based image super-resolution (SR) methods have demonstrated remarkable performance. Recent advancements have introduced deterministic sampling processes that reduce inference from 15 iterative steps to a single step, thereby significantly improving the inference speed of existing diffusion models. However, their efficiency remains limited when handling complex semantic regions due to the single-step inference. To address this limitation, we propose SAMSR, a semantic-guided diffusion framework that incorporates semantic segmentation masks into the sampling process. Specifically, we introduce the SAM-Noise Module, which refines Gaussian noise using segmentation masks to preserve spatial and semantic features. Furthermore, we develop a pixel-wise sampling strategy that dynamically adjusts the residual transfer rate and noise strength based on pixel-level semantic weights, prioritizing semantically rich regions during the diffusion process. To enhance model training, we also propose a semantic consistency loss, which aligns pixel-wise semantic weights between predictions and ground truth. Extensive experiments on both real-world and synthetic datasets demonstrate that SAMSR significantly improves perceptual quality and detail recovery, particularly in semantically complex images. Our code is released at https://github.com/Liu-Zihang/SAMSR.","authors":["Zihang Liu","Zhenyu Zhang","Hao Tang"],"url":"https://arxiv.org/abs/2505.07071"}
{"created":"2025-05-13","title":"Discovering Concept Directions from Diffusion-based Counterfactuals via Latent Clustering","abstract":"Concept-based explanations have emerged as an effective approach within Explainable Artificial Intelligence, enabling interpretable insights by aligning model decisions with human-understandable concepts. However, existing methods rely on computationally intensive procedures and struggle to efficiently capture complex, semantic concepts. Recently, the Concept Discovery through Latent Diffusion-based Counterfactual Trajectories (CDCT) framework, introduced by Varshney et al. (2025), attempts to identify concepts via dimension-wise traversal of the latent space of a Variational Autoencoder trained on counterfactual trajectories. Extending the CDCT framework, this work introduces Concept Directions via Latent Clustering (CDLC), which extracts global, class-specific concept directions by clustering latent difference vectors derived from factual and diffusion-generated counterfactual image pairs. CDLC substantially reduces computational complexity by eliminating the exhaustive latent dimension traversal required in CDCT and enables the extraction of multidimensional semantic concepts encoded across the latent dimensions. This approach is validated on a real-world skin lesion dataset, demonstrating that the extracted concept directions align with clinically recognized dermoscopic features and, in some cases, reveal dataset-specific biases or unknown biomarkers. These results highlight that CDLC is interpretable, scalable, and applicable across high-stakes domains and diverse data modalities.","authors":["Payal Varshney","Adriano Lucieri","Christoph Balada","Andreas Dengel","Sheraz Ahmed"],"url":"https://arxiv.org/abs/2505.07073"}
{"created":"2025-05-13","title":"Arbitrarily Applicable Same/Opposite Relational Responding with NARS","abstract":"Same/opposite relational responding, a fundamental aspect of human symbolic cognition, allows the flexible generalization of stimulus relationships based on minimal experience. In this study, we demonstrate the emergence of \\textit{arbitrarily applicable} same/opposite relational responding within the Non-Axiomatic Reasoning System (NARS), a computational cognitive architecture designed for adaptive reasoning under uncertainty. Specifically, we extend NARS with an implementation of \\textit{acquired relations}, enabling the system to explicitly derive both symmetric (mutual entailment) and novel relational combinations (combinatorial entailment) from minimal explicit training in a contextually controlled matching-to-sample (MTS) procedure. Experimental results show that NARS rapidly internalizes explicitly trained relational rules and robustly demonstrates derived relational generalizations based on arbitrary contextual cues. Importantly, derived relational responding in critical test phases inherently combines both mutual and combinatorial entailments, such as deriving same-relations from multiple explicitly trained opposite-relations. Internal confidence metrics illustrate strong internalization of these relational principles, closely paralleling phenomena observed in human relational learning experiments. Our findings underscore the potential for integrating nuanced relational learning mechanisms inspired by learning psychology into artificial general intelligence frameworks, explicitly highlighting the arbitrary and context-sensitive relational capabilities modeled within NARS.","authors":["Robert Johansson","Patrick Hammer","Tony Lofthouse"],"url":"https://arxiv.org/abs/2505.07079"}
{"created":"2025-05-13","title":"COMRECGC: Global Graph Counterfactual Explainer through Common Recourse","abstract":"Graph neural networks (GNNs) have been widely used in various domains such as social networks, molecular biology, or recommendation systems. Concurrently, different explanations methods of GNNs have arisen to complement its black-box nature. Explanations of the GNNs' predictions can be categorized into two types--factual and counterfactual. Given a GNN trained on binary classification into ''accept'' and ''reject'' classes, a global counterfactual explanation consists in generating a small set of ''accept'' graphs relevant to all of the input ''reject'' graphs. The transformation of a ''reject'' graph into an ''accept'' graph is called a recourse. A common recourse explanation is a small set of recourse, from which every ''reject'' graph can be turned into an ''accept'' graph. Although local counterfactual explanations have been studied extensively, the problem of finding common recourse for global counterfactual explanation remains unexplored, particularly for GNNs. In this paper, we formalize the common recourse explanation problem, and design an effective algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong baselines on four different real-world graphs datasets and demonstrate the superior performance of COMRECGC against the competitors. We also compare the common recourse explanations to the graph counterfactual explanation, showing that common recourse explanations are either comparable or superior, making them worth considering for applications such as drug discovery or computational biology.","authors":["Gregoire Fournier","Sourav Medya"],"url":"https://arxiv.org/abs/2505.07081"}
{"created":"2025-05-13","title":"DriveSOTIF: Advancing Perception SOTIF Through Multimodal Large Language Models","abstract":"Human drivers naturally possess the ability to perceive driving scenarios, predict potential hazards, and react instinctively due to their spatial and causal intelligence, which allows them to perceive, understand, predict, and interact with the 3D world both spatially and temporally. Autonomous vehicles, however, lack these capabilities, leading to challenges in effectively managing perception-related Safety of the Intended Functionality (SOTIF) risks, particularly in complex and unpredictable driving conditions. To address this gap, we propose an approach that fine-tunes multimodal language models (MLLMs) on a customized dataset specifically designed to capture perception-related SOTIF scenarios. Model benchmarking demonstrates that this tailored dataset enables the models to better understand and respond to these complex driving situations. Additionally, in real-world case studies, the proposed method correctly handles challenging scenarios that even human drivers may find difficult. Real-time performance tests further indicate the potential for the models to operate efficiently in live driving environments. This approach, along with the dataset generation pipeline, shows significant promise for improving the identification, cognition, prediction, and reaction to SOTIF-related risks in autonomous driving systems. The dataset and information are available: https://github.com/s95huang/DriveSOTIF.git","authors":["Shucheng Huang","Freda Shi","Chen Sun","Jiaming Zhong","Minghao Ning","Yufeng Yang","Yukun Lu","Hong Wang","Amir Khajepour"],"url":"https://arxiv.org/abs/2505.07084"}
{"created":"2025-05-13","title":"Privacy of Groups in Dense Street Imagery","abstract":"Spatially and temporally dense street imagery (DSI) datasets have grown unbounded. In 2024, individual companies possessed around 3 trillion unique images of public streets. DSI data streams are only set to grow as companies like Lyft and Waymo use DSI to train autonomous vehicle algorithms and analyze collisions. Academic researchers leverage DSI to explore novel approaches to urban analysis. Despite good-faith efforts by DSI providers to protect individual privacy through blurring faces and license plates, these measures fail to address broader privacy concerns. In this work, we find that increased data density and advancements in artificial intelligence enable harmful group membership inferences from supposedly anonymized data. We perform a penetration test to demonstrate how easily sensitive group affiliations can be inferred from obfuscated pedestrians in 25,232,608 dashcam images taken in New York City. We develop a typology of identifiable groups within DSI and analyze privacy implications through the lens of contextual integrity. Finally, we discuss actionable recommendations for researchers working with data from DSI providers.","authors":["Matt Franchi","Hauke Sandhaus","Madiha Zahrah Choksi","Severin Engelmann","Wendy Ju","Helen Nissenbaum"],"url":"https://arxiv.org/abs/2505.07085"}
{"created":"2025-05-13","title":"Multi-Objective-Guided Discrete Flow Matching for Controllable Biological Sequence Design","abstract":"Designing biological sequences that satisfy multiple, often conflicting, functional and biophysical criteria remains a central challenge in biomolecule engineering. While discrete flow matching models have recently shown promise for efficient sampling in high-dimensional sequence spaces, existing approaches address only single objectives or require continuous embeddings that can distort discrete distributions. We present Multi-Objective-Guided Discrete Flow Matching (MOG-DFM), a general framework to steer any pretrained discrete-time flow matching generator toward Pareto-efficient trade-offs across multiple scalar objectives. At each sampling step, MOG-DFM computes a hybrid rank-directional score for candidate transitions and applies an adaptive hypercone filter to enforce consistent multi-objective progression. We also trained two unconditional discrete flow matching models, PepDFM for diverse peptide generation and EnhancerDFM for functional enhancer DNA generation, as base generation models for MOG-DFM. We demonstrate MOG-DFM's effectiveness in generating peptide binders optimized across five properties (hemolysis, non-fouling, solubility, half-life, and binding affinity), and in designing DNA sequences with specific enhancer classes and DNA shapes. In total, MOG-DFM proves to be a powerful tool for multi-property-guided biomolecule sequence design.","authors":["Tong Chen","Yinuo Zhang","Sophia Tang","Pranam Chatterjee"],"url":"https://arxiv.org/abs/2505.07086"}
{"created":"2025-05-13","title":"Architectural Precedents for General Agents using Large Language Models","abstract":"One goal of AI (and AGI) is to identify and understand specific mechanisms and representations sufficient for general intelligence. Often, this work manifests in research focused on architectures and many cognitive architectures have been explored in AI/AGI. However, different research groups and even different research traditions have somewhat independently identified similar/common patterns of processes and representations or cognitive design patterns that are manifest in existing architectures. Today, AI systems exploiting large language models (LLMs) offer a relatively new combination of mechanism and representation available for exploring the possibilities of general intelligence. In this paper, we summarize a few recurring cognitive design patterns that have appeared in various pre-transformer AI architectures. We then explore how these patterns are evident in systems using LLMs, especially for reasoning and interactive (\"agentic\") use cases. By examining and applying these recurring patterns, we can also predict gaps or deficiencies in today's Agentic LLM Systems and identify likely subjects of future research towards general intelligence using LLMs and other generative foundation models.","authors":["Robert E. Wray","James R. Kirk","John E. Laird"],"url":"https://arxiv.org/abs/2505.07087"}
{"created":"2025-05-13","title":"RefPentester: A Knowledge-Informed Self-Reflective Penetration Testing Framework Based on Large Language Models","abstract":"Automated penetration testing (AutoPT) powered by large language models (LLMs) has gained attention for its ability to automate ethical hacking processes and identify vulnerabilities in target systems by leveraging the intrinsic knowledge of LLMs. However, existing LLM-based AutoPT frameworks often underperform compared to human experts in challenging tasks for several reasons: the imbalanced knowledge used in LLM training, short-sighted planning in the planning process, and hallucinations during command generation. In addition, the penetration testing (PT) process, with its trial-and-error nature, is limited by existing frameworks that lack mechanisms to learn from previous failed operations, restricting adaptive improvement of PT strategies. To address these limitations, we propose a knowledge-informed self-reflective PT framework powered by LLMs, called RefPentester, which is an AutoPT framework designed to assist human operators in identifying the current stage of the PT process, selecting appropriate tactic and technique for the stage, choosing suggested action, providing step-by-step operational guidance, and learning from previous failed operations. We also modeled the PT process as a seven-state Stage Machine to integrate the proposed framework effectively. The evaluation shows that RefPentester can successfully reveal credentials on Hack The Box's Sau machine, outperforming the baseline GPT-4o model by 16.7\\%. Across PT stages, RefPentester also demonstrates superior success rates on PT stage transitions.","authors":["Hanzheng Dai","Yuanliang Li","Zhibo Zhang","Jun Yan"],"url":"https://arxiv.org/abs/2505.07089"}
{"created":"2025-05-13","title":"Physics-informed Multiple-Input Operators for efficient dynamic response prediction of structures","abstract":"Finite element (FE) modeling is essential for structural analysis but remains computationally intensive, especially under dynamic loading. While operator learning models have shown promise in replicating static structural responses at FEM level accuracy, modeling dynamic behavior remains more challenging. This work presents a Multiple Input Operator Network (MIONet) that incorporates a second trunk network to explicitly encode temporal dynamics, enabling accurate prediction of structural responses under moving loads. Traditional DeepONet architectures using recurrent neural networks (RNNs) are limited by fixed time discretization and struggle to capture continuous dynamics. In contrast, MIONet predicts responses continuously over both space and time, removing the need for step wise modeling. It maps scalar inputs including load type, velocity, spatial mesh, and time steps to full field structural responses. To improve efficiency and enforce physical consistency, we introduce a physics informed loss based on dynamic equilibrium using precomputed mass, damping, and stiffness matrices, without solving the governing PDEs directly. Further, a Schur complement formulation reduces the training domain, significantly cutting computational costs while preserving global accuracy. The model is validated on both a simple beam and the KW-51 bridge, achieving FEM level accuracy within seconds. Compared to GRU based DeepONet, our model offers comparable accuracy with improved temporal continuity and over 100 times faster inference, making it well suited for real-time structural monitoring and digital twin applications.","authors":["Bilal Ahmed","Yuqing Qiu","Diab W. Abueidda","Waleed El-Sekelly","Tarek Abdoun","Mostafa E. Mobasher"],"url":"https://arxiv.org/abs/2505.07090"}
{"created":"2025-05-13","title":"X-Sim: Cross-Embodiment Learning via Real-to-Sim-to-Real","abstract":"Human videos offer a scalable way to train robot manipulation policies, but lack the action labels needed by standard imitation learning algorithms. Existing cross-embodiment approaches try to map human motion to robot actions, but often fail when the embodiments differ significantly. We propose X-Sim, a real-to-sim-to-real framework that uses object motion as a dense and transferable signal for learning robot policies. X-Sim starts by reconstructing a photorealistic simulation from an RGBD human video and tracking object trajectories to define object-centric rewards. These rewards are used to train a reinforcement learning (RL) policy in simulation. The learned policy is then distilled into an image-conditioned diffusion policy using synthetic rollouts rendered with varied viewpoints and lighting. To transfer to the real world, X-Si introduces an online domain adaptation technique that aligns real and simulated observations during deployment. Importantly, X-Sim does not require any robot teleoperation data. We evaluate it across 5 manipulation tasks in 2 environments and show that it: (1) improves task progress by 30% on average over hand-tracking and sim-to-real baselines, (2) matches behavior cloning with 10x less data collection time, and (3) generalizes to new camera viewpoints and test-time changes. Code and videos are available at https://portal-cornell.github.io/X-Sim/.","authors":["Prithwish Dan","Kushal Kedia","Angela Chao","Edward Weiyi Duan","Maximus Adrian Pace","Wei-Chiu Ma","Sanjiban Choudhury"],"url":"https://arxiv.org/abs/2505.07096"}
{"created":"2025-05-13","title":"Navigating the Rashomon Effect: How Personalization Can Help Adjust Interpretable Machine Learning Models to Individual Users","abstract":"The Rashomon effect describes the observation that in machine learning (ML) multiple models often achieve similar predictive performance while explaining the underlying relationships in different ways. This observation holds even for intrinsically interpretable models, such as Generalized Additive Models (GAMs), which offer users valuable insights into the model's behavior. Given the existence of multiple GAM configurations with similar predictive performance, a natural question is whether we can personalize these configurations based on users' needs for interpretability. In our study, we developed an approach to personalize models based on contextual bandits. In an online experiment with 108 users in a personalized treatment and a non-personalized control group, we found that personalization led to individualized rather than one-size-fits-all configurations. Despite these individual adjustments, the interpretability remained high across both groups, with users reporting a strong understanding of the models. Our research offers initial insights into the potential for personalizing interpretable ML.","authors":["Julian Rosenberger","Philipp Schr\\\"oppel","Sven Kruschel","Mathias Kraus","Patrick Zschech","Maximilian F\\\"orster"],"url":"https://arxiv.org/abs/2505.07100"}
{"created":"2025-05-13","title":"The $K_\\infty$ Homotopy $\\lambda$-Model","abstract":"We extend the complete ordered set Dana Scott's $D_\\infty$ to a complete weakly ordered Kan complex $K_\\infty$, with properties that guarantee the non-equivalence of the interpretation of some higher conversions of $\\beta\\eta$-conversions of $\\lambda$-terms.","authors":["Daniel O. Mart\\'inez-Rivillas","Ruy J. G. B. de Queiroz"],"url":"https://arxiv.org/abs/2505.07103"}
{"created":"2025-05-13","title":"Knowledge Distillation for Enhancing Walmart E-commerce Search Relevance Using Large Language Models","abstract":"Ensuring the products displayed in e-commerce search results are relevant to users queries is crucial for improving the user experience. With their advanced semantic understanding, deep learning models have been widely used for relevance matching in search tasks. While large language models (LLMs) offer superior ranking capabilities, it is challenging to deploy LLMs in real-time systems due to the high-latency requirements. To leverage the ranking power of LLMs while meeting the low-latency demands of production systems, we propose a novel framework that distills a high performing LLM into a more efficient, low-latency student model. To help the student model learn more effectively from the teacher model, we first train the teacher LLM as a classification model with soft targets. Then, we train the student model to capture the relevance margin between pairs of products for a given query using mean squared error loss. Instead of using the same training data as the teacher model, we significantly expand the student model dataset by generating unlabeled data and labeling it with the teacher model predictions. Experimental results show that the student model performance continues to improve as the size of the augmented training data increases. In fact, with enough augmented data, the student model can outperform the teacher model. The student model has been successfully deployed in production at Walmart.com with significantly positive metrics.","authors":["Hongwei Shang","Nguyen Vo","Nitin Yadav","Tian Zhang","Ajit Puthenputhussery","Xunfan Cai","Shuyi Chen","Prijith Chandran","Changsung Kang"],"url":"https://arxiv.org/abs/2505.07105"}
{"created":"2025-05-13","title":"DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems","abstract":"Based on the DeepSORT algorithm, this study explores the application of visual tracking technology in intelligent human-computer interaction, especially in the field of gesture recognition and tracking. With the rapid development of artificial intelligence and deep learning technology, visual-based interaction has gradually replaced traditional input devices and become an important way for intelligent systems to interact with users. The DeepSORT algorithm can achieve accurate target tracking in dynamic environments by combining Kalman filters and deep learning feature extraction methods. It is especially suitable for complex scenes with multi-target tracking and fast movements. This study experimentally verifies the superior performance of DeepSORT in gesture recognition and tracking. It can accurately capture and track the user's gesture trajectory and is superior to traditional tracking methods in terms of real-time and accuracy. In addition, this study also combines gesture recognition experiments to evaluate the recognition ability and feedback response of the DeepSORT algorithm under different gestures (such as sliding, clicking, and zooming). The experimental results show that DeepSORT can not only effectively deal with target occlusion and motion blur but also can stably track in a multi-target environment, achieving a smooth user interaction experience. Finally, this paper looks forward to the future development direction of intelligent human-computer interaction systems based on visual tracking and proposes future research focuses such as algorithm optimization, data fusion, and multimodal interaction in order to promote a more intelligent and personalized interactive experience. Keywords-DeepSORT, visual tracking, gesture recognition, human-computer interaction","authors":["Tong Zhang","Fenghua Shao","Runsheng Zhang","Yifan Zhuang","Liuqingqing Yang"],"url":"https://arxiv.org/abs/2505.07110"}
{"created":"2025-05-13","title":"Efficient Implementation of RISC-V Vector Permutation Instructions","abstract":"RISC-V CPUs leverage the RVV (RISC-V Vector) extension to accelerate data-parallel workloads. In addition to arithmetic operations, RVV includes powerful permutation instructions that enable flexible element rearrangement within vector registers --critical for optimizing performance in tasks such as matrix operations and cryptographic computations. However, the diverse control mechanisms of these instructions complicate their execution within a unified datapath while maintaining the fixed-latency requirement of cryptographic accelerators. To address this, we propose a unified microarchitecture capable of executing all RVV permutation instructions efficiently, regardless of their control information structure. This approach minimizes area and hardware costs while ensuring single-cycle execution for short vector machines (up to 256 bits) and enabling efficient pipelining for longer vectors. The proposed design is integrated into an open-source RISC-V vector processor and implemented at 7 nm using the OpenRoad physical synthesis flow. Experimental results validate the efficiency of our unified vector permutation unit, demonstrating that it only incurs 1.5% area overhead to the total vector processor. Furthermore, this area overhead decreases to near-0% as the minimum supported element width for vector permutations increases.","authors":["Vasileios Titopoulos","George Alexakis","Chrysostomos Nicopoulos","Giorgos Dimitrakopoulos"],"url":"https://arxiv.org/abs/2505.07112"}
{"created":"2025-05-13","title":"OPTIKS: Optimized Gradient Properties Through Timing in K-Space","abstract":"A customizable method (OPTIKS) for designing fast trajectory-constrained gradient waveforms with optimized time domain properties was developed. Given a specified multidimensional k-space trajectory, the method optimizes traversal speed (and therefore timing) with position along the trajectory. OPTIKS facilitates optimization of objectives dependent on the time domain gradient waveform and the arc-length domain k-space speed. OPTIKS is applied to design waveforms which limit peripheral nerve stimulation (PNS), minimize mechanical resonance excitation, and reduce acoustic noise. A variety of trajectory examples are presented including spirals, circular echo-planar-imaging, and rosettes. Design performance is evaluated based on duration, standardized PNS models, field measurements, gradient coil back-EMF measurements, and calibrated acoustic measurements. We show reductions in back-EMF of up to 94% and field oscillations up to 91.1%, acoustic noise decreases of up to 9.22 dB, and with efficient use of PNS models speed increases of up to 11.4%. The design method implementation is made available as an open source Python package through GitHub.","authors":["Matthew A. McCready","Xiaozhi Cao","Kawin Setsompop","John M. Pauly","Adam B. Kerr"],"url":"https://arxiv.org/abs/2505.07117"}
{"created":"2025-05-13","title":"KOKKAI DOC: An LLM-driven framework for scaling parliamentary representatives","abstract":"This paper introduces an LLM-driven framework designed to accurately scale the political issue stances of parliamentary representatives. By leveraging advanced natural language processing techniques and large language models, the proposed methodology refines and enhances previous approaches by addressing key challenges such as noisy speech data, manual bias in selecting political axes, and the lack of dynamic, diachronic analysis. The framework incorporates three major innovations: (1) de-noising parliamentary speeches via summarization to produce cleaner, more consistent opinion embeddings; (2) automatic extraction of axes of political controversy from legislators' speech summaries; and (3) a diachronic analysis that tracks the evolution of party positions over time.","authors":["Ken Kato","Christopher Cochrane"],"url":"https://arxiv.org/abs/2505.07118"}
{"created":"2025-05-13","title":"Towards Scalable IoT Deployment for Visual Anomaly Detection via Efficient Compression","abstract":"Visual Anomaly Detection (VAD) is a key task in industrial settings, where minimizing waste and operational costs is essential. Deploying deep learning models within Internet of Things (IoT) environments introduces specific challenges due to the limited computational power and bandwidth of edge devices. This study investigates how to perform VAD effectively under such constraints by leveraging compact and efficient processing strategies. We evaluate several data compression techniques, examining the trade-off between system latency and detection accuracy. Experiments on the MVTec AD benchmark demonstrate that significant compression can be achieved with minimal loss in anomaly detection performance compared to uncompressed data.","authors":["Arianna Stropeni","Francesco Borsatti","Manuel Barusco","Davide Dalle Pezze","Marco Fabris","Gian Antonio Susto"],"url":"https://arxiv.org/abs/2505.07119"}
{"created":"2025-05-13","title":"On optimal recovery of unbounded operators from inaccurate data","abstract":"The problems of optimal recovery of unbounded operators are studied. Optimality means the highest possible accuracy and the minimal amount of discrete information involved. It is established that the truncation method, when certain conditions are met, realizes the optimal values of the studied quantities. As an illustration of the general results, problems of numerical differentiation and the backward parabolic equation are considered.","authors":["Oleg Davydov","Sergei Solodky"],"url":"https://arxiv.org/abs/2505.07123"}
{"created":"2025-05-13","title":"Learning from Samples: Inverse Problems over measures via Sharpened Fenchel-Young Losses","abstract":"Estimating parameters from samples of an optimal probability distribution is essential in applications ranging from socio-economic modeling to biological system analysis. In these settings, the probability distribution arises as the solution to an optimization problem that captures either static interactions among agents or the dynamic evolution of a system over time. Our approach relies on minimizing a new class of loss functions, called sharpened Fenchel-Young losses, which measure the sub-optimality gap of the optimization problem over the space of measures. We study the stability of this estimation method when only a finite number of sample is available. The parameters to be estimated typically correspond to a cost function in static problems and to a potential function in dynamic problems. To analyze stability, we introduce a general methodology that leverages the strong convexity of the loss function together with the sample complexity of the forward optimization problem. Our analysis emphasizes two specific settings in the context of optimal transport, where our method provides explicit stability guarantees: The first is inverse unbalanced optimal transport (iUOT) with entropic regularization, where the parameters to estimate are cost functions that govern transport computations; this method has applications such as link prediction in machine learning. The second is inverse gradient flow (iJKO), where the objective is to recover a potential function that drives the evolution of a probability distribution via the Jordan-Kinderlehrer-Otto (JKO) time-discretization scheme; this is particularly relevant for understanding cell population dynamics in single-cell genomics. Finally, we validate our approach through numerical experiments on Gaussian distributions, where closed-form solutions are available, to demonstrate the practical performance of our methods","authors":["Francisco Andrade","Gabriel Peyr\\'e","Clarice Poon"],"url":"https://arxiv.org/abs/2505.07124"}
{"created":"2025-05-13","title":"AI-Driven Optimization of Wave-Controlled Reconfigurable Intelligent Surfaces","abstract":"A promising type of Reconfigurable Intelligent Surface (RIS) employs tunable control of its varactors using biasing transmission lines below the RIS reflecting elements. Biasing standing waves (BSWs) are excited by a time-periodic signal and sampled at each RIS element to create a desired biasing voltage and control the reflection coefficients of the elements. A simple rectifier can be used to sample the voltages and capture the peaks of the BSWs over time. Like other types of RIS, attempting to model and accurately configure a wave-controlled RIS is extremely challenging due to factors such as device non-linearities, frequency dependence, element coupling, etc., and thus significant differences will arise between the actual and assumed performance. An alternative approach to solving this problem is data-driven: Using training data obtained by sampling the reflected radiation pattern of the RIS for a set of BSWs, a neural network (NN) is designed to create an input-output map between the BSW amplitudes and the resulting sampled radiation pattern. This is the approach discussed in this paper. In the proposed approach, the NN is optimized using a genetic algorithm (GA) to minimize the error between the predicted and measured radiation patterns. The BSW amplitudes are then designed via Simulated Annealing (SA) to optimize a signal-to-leakage-plus-noise ratio measure by iteratively forward-propagating the BSW amplitudes through the NN and using its output as feedback to determine convergence. The resulting optimal solutions are stored in a lookup table to be used both as settings to instantly configure the RIS and as a basis for determining more complex radiation patterns.","authors":["Gal Ben Itzhak","Miguel Saavedra-Melo","Ender Ayanoglu","Filippo Capolino","A. Lee Swindlehurst"],"url":"https://arxiv.org/abs/2505.07126"}
{"created":"2025-05-13","title":"Minimal Linear Codes Violating the Ashikhmin-Barg Condition from Arbitrary Projective Linear Codes","abstract":"In recent years, there have been many constructions of minimal linear codes violating the Ashikhmin-Barg condition from Boolean functions, linear codes with few nonzero weights or partial difference sets. In this paper, we first give a general method to transform a minimal code satisfying the Ashikhmin-Barg condition to a minimal code violating the Ashikhmin-Barg condition. Then we give a construction of a minimal code satisfying the Ashikhmin-Barg condition from an arbitrary projective linear code. Hence an arbitrary projective linear code can be transformed to a minimal codes violating the Ashikhmin-Barg condition. Then we give infinite many families of minimal codes violating the Ashikhamin-Barg condition. Weight distributions of constructed minimal codes violating the Ashikhmin-Barg condition in this paper are determined. Many minimal linear codes violating the Ashikhmin-Barg condition with their minimum weights close to the optimal or the best known minimum weights of linear codes are constructed in this paper. Moreover, many infinite families of self-orthogonal binary minimal codes violating the Ashikhmin-Barg condition are also given.","authors":["Hao Chen","Yaqi Chen","Conghui Xie","Huimin Lao"],"url":"https://arxiv.org/abs/2505.07130"}
{"created":"2025-05-13","title":"Justi\\c{c}a Algor\\'itmica: Instrumentaliza\\c{c}\\~ao, Limites Conceituais e Desafios na Engenharia de Software","abstract":"This article describes ongoing research with the aim of understanding the concept of justice in the field of software engineering, the factors that underlie the creation and instrumentalization of these concepts, and the limitations faced by software engineering when applying them. The expansion of the field of study called ``algorithmic justice'' fundamentally consists in the creation of mechanisms and procedures based on mathematical and formal procedures to conceptualize, evaluate and reduce biases and discrimination caused by algorithms. We conducted a systematic mapping in the context of justice in software engineering, comprising the metrics and definitions of algorithmic justice, as well as the procedures and techniques for fairer decision-making systems. We propose a discussion about the limitations that arise due to the understanding of justice as an attribute of software and the result of decision-making, as well as the influence that the field suffers from the construction of computational thinking, which is constantly developed around abstractions. Finally, we reflect on potential paths that could help us move beyond the limits of algorithmic justice.","authors":["Lucas Rodrigues Valen\\c{c}a","Ronnie de Souza Santos"],"url":"https://arxiv.org/abs/2505.07132"}
{"created":"2025-05-13","title":"Triangulating PL functions and the existence of efficient ReLU DNNs","abstract":"We show that every piecewise linear function $f:R^d \\to R$ with compact support a polyhedron $P$ has a representation as a sum of so-called `simplex functions'. Such representations arise from degree 1 triangulations of the relative homology class (in $R^{d+1}$) bounded by $P$ and the graph of $f$, and give a short elementary proof of the existence of efficient universal ReLU neural networks that simultaneously compute all such functions $f$ of bounded complexity.","authors":["Danny Calegari"],"url":"https://arxiv.org/abs/2505.07137"}
{"created":"2025-05-13","title":"Terrain-aware Low Altitude Path Planning","abstract":"In this paper, we study the problem of generating low altitude path plans for nap-of-the-earth (NOE) flight in real time with only RGB images from onboard cameras and the vehicle pose. We propose a novel training method that combines behavior cloning and self-supervised learning that enables the learned policy to outperform the policy trained with standard behavior cloning approach on this task. Simulation studies are performed on a custom canyon terrain.","authors":["Yixuan Jia","Andrea Tagliabue","Navid Dadkhah Tehrani","Jonathan P. How"],"url":"https://arxiv.org/abs/2505.07141"}
{"created":"2025-05-13","title":"Exploring Anthropomorphism in Conversational Agents for Environmental Sustainability","abstract":"The paper investigates the integration of Large Language Models (LLMs) into Conversational Agents (CAs) to encourage a shift in consumption patterns from a demand-driven to a supply-based paradigm. Specifically, the research examines the role of anthropomorphic design in delivering environmentally conscious messages by comparing two CA designs: a personified agent representing an appliance and a traditional, non-personified assistant. A lab study (N=26) assessed the impact of these designs on interaction, perceived self-efficacy, and engagement. Results indicate that LLM-based CAs significantly enhance users' self-reported eco-friendly behaviors, with participants expressing greater confidence in managing energy consumption. While the anthropomorphic design did not notably affect self-efficacy, those interacting with the personified agent reported a stronger sense of connection with the system. These findings suggest that although anthropomorphic CAs may improve user engagement, both designs hold promise for fostering sustainable behaviors in home energy management.","authors":["Mathyas Giudici","Samuele Scherini","Pascal Chaussumier","Stefano Ginocchio","Franca Garzotto"],"url":"https://arxiv.org/abs/2505.07142"}
{"created":"2025-05-13","title":"All Polyhedral Manifolds are Connected by a 2-Step Refolding","abstract":"We prove that, for any two polyhedral manifolds $\\mathcal P, \\mathcal Q$, there is a polyhedral manifold $\\mathcal I$ such that $\\mathcal P, \\mathcal I$ share a common unfolding and $\\mathcal I,\\mathcal Q$ share a common unfolding. In other words, we can unfold $\\mathcal P$, refold (glue) that unfolding into $\\mathcal I$, unfold $\\mathcal I$, and then refold into $\\mathcal Q$. Furthermore, if $\\mathcal P, \\mathcal Q$ have no boundary and can be embedded in 3D (without self-intersection), then so does $\\mathcal I$. These results generalize to $n$ given manifolds $\\mathcal P_1, \\mathcal P_2, \\dots, \\mathcal P_n$; they all have a common unfolding with the same intermediate manifold $\\mathcal I$. Allowing more than two unfold/refold steps, we obtain stronger results for two special cases: for doubly covered convex planar polygons, we achieve that all intermediate polyhedra are planar; and for tree-shaped polycubes, we achieve that all intermediate polyhedra are tree-shaped polycubes.","authors":["Lily Chung","Erik D. Demaine","Jenny Diomidova","Tonan Kamata","Jayson Lynch","Ryuhei Uehara","Hanyu Alice Zhang"],"url":"https://arxiv.org/abs/2505.07147"}
{"created":"2025-05-13","title":"Standing Firm in 5G: A Single-Round, Dropout-Resilient Secure Aggregation for Federated Learning","abstract":"Federated learning (FL) is well-suited to 5G networks, where many mobile devices generate sensitive edge data. Secure aggregation protocols enhance privacy in FL by ensuring that individual user updates reveal no information about the underlying client data. However, the dynamic and large-scale nature of 5G-marked by high mobility and frequent dropouts-poses significant challenges to the effective adoption of these protocols. Existing protocols often require multi-round communication or rely on fixed infrastructure, limiting their practicality. We propose a lightweight, single-round secure aggregation protocol designed for 5G environments. By leveraging base stations for assisted computation and incorporating precomputation, key-homomorphic pseudorandom functions, and t-out-of-k secret sharing, our protocol ensures efficiency, robustness, and privacy. Experiments show strong security guarantees and significant gains in communication and computation efficiency, making the approach well-suited for real-world 5G FL deployments.","authors":["Yiwei Zhang","Rouzbeh Behnia","Imtiaz Karim","Attila A. Yavuz","Elisa Bertino"],"url":"https://arxiv.org/abs/2505.07148"}
{"created":"2025-05-13","title":"AugMixCloak: A Defense against Membership Inference Attacks via Image Transformation","abstract":"Traditional machine learning (ML) raises serious privacy concerns, while federated learning (FL) mitigates the risk of data leakage by keeping data on local devices. However, the training process of FL can still leak sensitive information, which adversaries may exploit to infer private data. One of the most prominent threats is the membership inference attack (MIA), where the adversary aims to determine whether a particular data record was part of the training set.","authors":["Heqing Ren","Chao Feng","Alberto Huertas","Burkhard Stiller"],"url":"https://arxiv.org/abs/2505.07149"}
{"created":"2025-05-13","title":"Assessing the User Experience of Extended Reality Devices for (Dis)Assembly: A Classroom Study","abstract":"Despite the current rise and promising capabilities of Extended Reality (XR) technologies, the architecture, engineering, and construction industry lacks informed guidance when choosing between these technologies, especially for complex processes like assembly and disassembly tasks. This research compares the user experience across different XR devices for (dis)assembly utilizing the NASA Task Load Index and System Usability Scale metrics. Through a workshop and surveys with graduate civil engineering and architecture students, the study found that Augmented Reality scored highest in usability, followed closely by Mixed Reality. However, Mixed Reality showed the best task load index score, indicating low cognitive demand. The findings presented in this research may aid academics and practitioners in making informed decisions when selecting XR systems in practical, real-world assembly scenarios. Moreover, this study suggests opportunities and guidelines for more detailed XR system comparisons and exploration of XR's further role in circular construction practices.","authors":["Brandon S. Byers","Eleftherios Triantafyllidis","Thibaut Menny","Martin Schulte","Catherine De Wolf"],"url":"https://arxiv.org/abs/2505.07154"}
{"created":"2025-05-13","title":"Reassessing Large Language Model Boolean Query Generation for Systematic Reviews","abstract":"Systematic reviews are comprehensive literature reviews that address highly focused research questions and represent the highest form of evidence in medicine. A critical step in this process is the development of complex Boolean queries to retrieve relevant literature. Given the difficulty of manually constructing these queries, recent efforts have explored Large Language Models (LLMs) to assist in their formulation. One of the first studies,Wang et al., investigated ChatGPT for this task, followed by Staudinger et al., which evaluated multiple LLMs in a reproducibility study. However, the latter overlooked several key aspects of the original work, including (i) validation of generated queries, (ii) output formatting constraints, and (iii) selection of examples for chain-of-thought (Guided) prompting. As a result, its findings diverged significantly from the original study. In this work, we systematically reproduce both studies while addressing these overlooked factors. Our results show that query effectiveness varies significantly across models and prompt designs, with guided query formulation benefiting from well-chosen seed studies. Overall, prompt design and model selection are key drivers of successful query formulation. Our findings provide a clearer understanding of LLMs' potential in Boolean query generation and highlight the importance of model- and prompt-specific optimisations. The complex nature of systematic reviews adds to challenges in both developing and reproducing methods but also highlights the importance of reproducibility studies in this domain.","authors":["Shuai Wang","Harrisen Scells","Bevan Koopman","Guido Zuccon"],"url":"https://arxiv.org/abs/2505.07155"}
{"created":"2025-05-13","title":"Field of values analysis that includes zero for preconditioned nonsymmetric saddle-point systems","abstract":"We present a field-of-values (FOV) analysis for preconditioned nonsymmetric saddle-point linear systems, where zero is included in the field of values of the matrix. We rely on recent results of Crouzeix and Greenbaum [Spectral sets: numerical range and beyond. SIAM Journal on Matrix Analysis and Applications, 40(3):1087-1001, 2019], showing that a convex region with a circular hole is a spectral set. Sufficient conditions are derived for convergence independent of the matrix dimensions. We apply our results to preconditioned nonsymmetric saddle-point systems, and show their applicability to families of block preconditioners that have not been previously covered by existing FOV analysis. A limitation of our theory is that the preconditioned matrix is required to have a small skew-symmetric part in norm. Consequently, our analysis may not be applicable, for example, to fluid flow problems characterized by a small viscosity coefficient. Some numerical results illustrate our findings.","authors":["Hao Chen","Chen Greif"],"url":"https://arxiv.org/abs/2505.07156"}
{"created":"2025-05-13","title":"HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling","abstract":"Traditional topic models often struggle with contextual nuances and fail to adequately handle polysemy and rare words. This limitation typically results in topics that lack coherence and quality. Large Language Models (LLMs) can mitigate this issue by generating an initial set of topics. However, these raw topics frequently lack refinement and representativeness, which leads to redundancy without lexical similarity and reduced interpretability. This paper introduces HAMLET, a graph-driven architecture for cross-lingual healthcare topic modeling that uses LLMs. The proposed approach leverages neural-enhanced semantic fusion to refine the embeddings of topics generated by the LLM. Instead of relying solely on statistical co-occurrence or human interpretation to extract topics from a document corpus, this method introduces a topic embedding refinement that uses Bidirectional Encoder Representations from Transformers (BERT) and Graph Neural Networks (GNN). After topic generation, a hybrid technique that involves BERT and Sentence-BERT (SBERT) is employed for embedding. The topic representations are further refined using a GNN, which establishes connections between documents, topics, words, similar topics, and similar words. A novel method is introduced to compute similarities. Consequently, the topic embeddings are refined, and the top k topics are extracted. Experiments were conducted using two healthcare datasets, one in English and one in French, from which six sets were derived. The results demonstrate the effectiveness of HAMLET.","authors":["Hajar Sakai","Sarah S. Lam"],"url":"https://arxiv.org/abs/2505.07157"}
{"created":"2025-05-13","title":"Real-Time Bit-Level Encryption of Full High-Definition Video Without Diffusion","abstract":"Despite the widespread adoption of Shannon's confusion-diffusion architecture in image encryption, the implementation of diffusion to sequentially establish inter-pixel dependencies for attaining plaintext sensitivity constrains algorithmic parallelism, while the execution of multiple rounds of diffusion operations to meet the required sensitivity metrics incurs excessive computational overhead. Consequently, the pursuit of plaintext sensitivity through diffusion operations is the primary factor limiting the computational efficiency and throughput of video encryption algorithms, rendering them inadequate to meet the demands of real-time encryption for high-resolution video. To address the performance limitation, this paper proposes a real-time video encryption protocol based on heterogeneous parallel computing, which incorporates the SHA-256 hashes of original frames as input, employs multiple CPU threads to concurrently generate encryption-related data, and deploys numerous GPU threads to simultaneously encrypt pixels. By leveraging the extreme input sensitivity of the SHA hash, the proposed protocol achieves the required plaintext sensitivity metrics with only a single round of confusion and XOR operations, significantly reducing computational overhead. Furthermore, through eliminating the reliance on diffusion, it realizes the allocation of a dedicated GPU thread for encrypting each pixel within every channel, effectively enhancing algorithm's parallelism. The experimental results demonstrate that our approach not only exhibits superior statistical properties and robust security but also achieving delay-free bit-level encryption for 1920$\\times$1080 resolution (full high definition) video at 30 FPS, with an average encryption time of 25.84 ms on a server equipped with an Intel Xeon Gold 6226R CPU and an NVIDIA GeForce RTX 3090 GPU.","authors":["Dong Jiang","Hui-ran Luo","Zi-jian Cui","Xi-jue Zhao","Lin-sheng Huang","Liang-liang Lu"],"url":"https://arxiv.org/abs/2505.07158"}
{"created":"2025-05-13","title":"Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue","abstract":"Effective feedback is essential for refining instructional practices in mathematics education, and researchers often turn to advanced natural language processing (NLP) models to analyze classroom dialogues from multiple perspectives. However, utterance-level discourse analysis encounters two primary challenges: (1) multifunctionality, where a single utterance may serve multiple purposes that a single tag cannot capture, and (2) the exclusion of many utterances from domain-specific discourse move classifications, leading to their omission in feedback. To address these challenges, we proposed a multi-perspective discourse analysis that integrates domain-specific talk moves with dialogue act (using the flattened multi-functional SWBD-MASL schema with 43 tags) and discourse relation (applying Segmented Discourse Representation Theory with 16 relations). Our top-down analysis framework enables a comprehensive understanding of utterances that contain talk moves, as well as utterances that do not contain talk moves. This is applied to two mathematics education datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through distributional unigram analysis, sequential talk move analysis, and multi-view deep dive, we discovered meaningful discourse patterns, and revealed the vital role of utterances without talk moves, demonstrating that these utterances, far from being mere fillers, serve crucial functions in guiding, acknowledging, and structuring classroom discourse. These insights underscore the importance of incorporating discourse relations and dialogue acts into AI-assisted education systems to enhance feedback and create more responsive learning environments. Our framework may prove helpful for providing human educator feedback, but also aiding in the development of AI agents that can effectively emulate the roles of both educators and students.","authors":["Jannatun Naim","Jie Cao","Fareen Tasneem","Jennifer Jacobs","Brent Milne","James Martin","Tamara Sumner"],"url":"https://arxiv.org/abs/2505.07161"}
{"created":"2025-05-13","title":"KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification","abstract":"The increasing volume of healthcare textual data requires computationally efficient, yet highly accurate classification approaches able to handle the nuanced and complex nature of medical terminology. This research presents Knowledge Distillation for Healthcare Multi-Label Text Classification (KDH-MLTC), a framework leveraging model compression and Large Language Models (LLMs). The proposed approach addresses conventional healthcare Multi-Label Text Classification (MLTC) challenges by integrating knowledge distillation and sequential fine-tuning, subsequently optimized through Particle Swarm Optimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from a more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e., DistilBERT) through sequential training adapted to MLTC that preserves the teacher's learned information while significantly reducing computational requirements. As a result, the classification is enabled to be conducted locally, making it suitable for healthcare textual data characterized by sensitivity and, therefore, ensuring HIPAA compliance. The experiments conducted on three medical literature datasets of different sizes, sampled from the Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves superior performance compared to existing approaches, particularly for the largest dataset, reaching an F1 score of 82.70%. Additionally, statistical validation and an ablation study are carried out, proving the robustness of KDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process allowed the identification of optimal configurations. The proposed approach contributes to healthcare text classification research, balancing efficiency requirements in resource-constrained healthcare settings with satisfactory accuracy demands.","authors":["Hajar Sakai","Sarah S. Lam"],"url":"https://arxiv.org/abs/2505.07162"}
{"created":"2025-05-13","title":"EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for Visual Emotion Analysis","abstract":"Visual emotion analysis, which has gained considerable attention in the field of affective computing, aims to predict the dominant emotions conveyed by an image. Despite advancements in visual emotion analysis with the emergence of vision-language models, we observed that instruction-tuned vision-language models and conventional vision models exhibit complementary strengths in visual emotion analysis, as vision-language models excel in certain cases, whereas vision models perform better in others. This finding highlights the need to integrate these capabilities to enhance the performance of visual emotion analysis. To bridge this gap, we propose EmoVLM-KD, an instruction-tuned vision-language model augmented with a lightweight module distilled from conventional vision models. Instead of deploying both models simultaneously, which incurs high computational costs, we transfer the predictive patterns of a conventional vision model into the vision-language model using a knowledge distillation framework. Our approach first fine-tunes a vision-language model on emotion-specific instruction data and then attaches a distilled module to its visual encoder while keeping the vision-language model frozen. Predictions from the vision language model and the distillation module are effectively balanced by a gate module, which subsequently generates the final outcome. Extensive experiments show that EmoVLM-KD achieves state-of-the-art performance on multiple visual emotion analysis benchmark datasets, outperforming the existing methods while maintaining computational efficiency. The code is available in https://github.com/sange1104/EmoVLM-KD.","authors":["SangEun Lee","Yubeen Lee","Eunil Park"],"url":"https://arxiv.org/abs/2505.07164"}
{"created":"2025-05-13","title":"Generalizable Pancreas Segmentation via a Dual Self-Supervised Learning Framework","abstract":"Recently, numerous pancreas segmentation methods have achieved promising performance on local single-source datasets. However, these methods don't adequately account for generalizability issues, and hence typically show limited performance and low stability on test data from other sources. Considering the limited availability of distinct data sources, we seek to improve the generalization performance of a pancreas segmentation model trained with a single-source dataset, i.e., the single source generalization task. In particular, we propose a dual self-supervised learning model that incorporates both global and local anatomical contexts. Our model aims to fully exploit the anatomical features of the intra-pancreatic and extra-pancreatic regions, and hence enhance the characterization of the high-uncertainty regions for more robust generalization. Specifically, we first construct a global-feature contrastive self-supervised learning module that is guided by the pancreatic spatial structure. This module obtains complete and consistent pancreatic features through promoting intra-class cohesion, and also extracts more discriminative features for differentiating between pancreatic and non-pancreatic tissues through maximizing inter-class separation. It mitigates the influence of surrounding tissue on the segmentation outcomes in high-uncertainty regions. Subsequently, a local-image restoration self-supervised learning module is introduced to further enhance the characterization of the high uncertainty regions. In this module, informative anatomical contexts are actually learned to recover randomly corrupted appearance patterns in those regions.","authors":["Jun Li","Hongzhang Zhu","Tao Chen","Xiaohua Qian"],"url":"https://arxiv.org/abs/2505.07165"}
{"created":"2025-05-13","title":"Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition","abstract":"Dense retrievers utilize pre-trained backbone language models (e.g., BERT, LLaMA) that are fine-tuned via contrastive learning to perform the task of encoding text into sense representations that can be then compared via a shallow similarity operation, e.g. inner product. Recent research has questioned the role of fine-tuning vs. that of pre-training within dense retrievers, specifically arguing that retrieval knowledge is primarily gained during pre-training, meaning knowledge not acquired during pre-training cannot be sub-sequentially acquired via fine-tuning. We revisit this idea here as the claim was only studied in the context of a BERT-based encoder using DPR as representative dense retriever. We extend the previous analysis by testing other representation approaches (comparing the use of CLS tokens with that of mean pooling), backbone architectures (encoder-only BERT vs. decoder-only LLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our study confirms that in DPR tuning, pre-trained knowledge underpins retrieval performance, with fine-tuning primarily adjusting neuron activation rather than reorganizing knowledge. However, this pattern does not hold universally, such as in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full reproducibility and make our implementation publicly available at https://github.com/ielab/DenseRetriever-Knowledge-Acquisition.","authors":["Zheng Yao","Shuai Wang","Guido Zuccon"],"url":"https://arxiv.org/abs/2505.07166"}
{"created":"2025-05-13","title":"One Trigger Token Is Enough: A Defense Strategy for Balancing Safety and Usability in Large Language Models","abstract":"Large Language Models (LLMs) have been extensively used across diverse domains, including virtual assistants, automated code generation, and scientific research. However, they remain vulnerable to jailbreak attacks, which manipulate the models into generating harmful responses despite safety alignment. Recent studies have shown that current safety-aligned LLMs often undergo the shallow safety alignment, where the first few tokens largely determine whether the response will be harmful. Through comprehensive observations, we find that safety-aligned LLMs and various defense strategies generate highly similar initial tokens in their refusal responses, which we define as safety trigger tokens. Building on this insight, we propose \\texttt{D-STT}, a simple yet effective defense algorithm that identifies and explicitly decodes safety trigger tokens of the given safety-aligned LLM to trigger the model's learned safety patterns. In this process, the safety trigger is constrained to a single token, which effectively preserves model usability by introducing minimum intervention in the decoding process. Extensive experiments across diverse jailbreak attacks and benign prompts demonstrate that \\ours significantly reduces output harmfulness while preserving model usability and incurring negligible response time overhead, outperforming ten baseline methods.","authors":["Haoran Gu","Handing Wang","Yi Mei","Mengjie Zhang","Yaochu Jin"],"url":"https://arxiv.org/abs/2505.07167"}
{"created":"2025-05-13","title":"Empowering the Grid: Collaborative Edge Artificial Intelligence for Decentralized Energy Systems","abstract":"This paper examines how decentralized energy systems can be enhanced using collaborative Edge Artificial Intelligence. Decentralized grids use local renewable sources to reduce transmission losses and improve energy security. Edge AI enables real-time, privacy-preserving data processing at the network edge. Techniques such as federated learning and distributed control improve demand response, equipment maintenance, and energy optimization. The paper discusses key challenges including data privacy, scalability, and interoperability, and suggests solutions such as blockchain integration and adaptive architectures. Examples from virtual power plants and smart grids highlight the potential of these technologies. The paper calls for increased investment, policy support, and collaboration to advance sustainable energy systems.","authors":["Eddie de Paula Jr","Niel Bunda","Hezerul Abdul Karim","Nouar AlDahoul","Myles Joshua Toledo Tan"],"url":"https://arxiv.org/abs/2505.07170"}
{"created":"2025-05-13","title":"ReCDAP: Relation-Based Conditional Diffusion with Attention Pooling for Few-Shot Knowledge Graph Completion","abstract":"Knowledge Graphs (KGs), composed of triples in the form of (head, relation, tail) and consisting of entities and relations, play a key role in information retrieval systems such as question answering, entity search, and recommendation. In real-world KGs, although many entities exist, the relations exhibit a long-tail distribution, which can hinder information retrieval performance. Previous few-shot knowledge graph completion studies focused exclusively on the positive triple information that exists in the graph or, when negative triples were incorporated, used them merely as a signal to indicate incorrect triples. To overcome this limitation, we propose Relation-Based Conditional Diffusion with Attention Pooling (ReCDAP). First, negative triples are generated by randomly replacing the tail entity in the support set. By conditionally incorporating positive information in the KG and non-existent negative information into the diffusion process, the model separately estimates the latent distributions for positive and negative relations. Moreover, including an attention pooler enables the model to leverage the differences between positive and negative cases explicitly. Experiments on two widely used datasets demonstrate that our method outperforms existing approaches, achieving state-of-the-art performance. The code is available at https://github.com/hou27/ReCDAP-FKGC.","authors":["Jeongho Kim","Chanyeong Heo","Jaehee Jung"],"url":"https://arxiv.org/abs/2505.07171"}
{"created":"2025-05-13","title":"Critique Before Thinking: Mitigating Hallucination through Rationale-Augmented Instruction Tuning","abstract":"Despite significant advancements in multimodal reasoning tasks, existing Large Vision-Language Models (LVLMs) are prone to producing visually ungrounded responses when interpreting associated images. In contrast, when humans embark on learning new knowledge, they often rely on a set of fundamental pre-study principles: reviewing outlines to grasp core concepts, summarizing key points to guide their focus and enhance understanding. However, such preparatory actions are notably absent in the current instruction tuning processes. This paper presents Re-Critic, an easily scalable rationale-augmented framework designed to incorporate fundamental rules and chain-of-thought (CoT) as a bridge to enhance reasoning abilities. Specifically, Re-Critic develops a visual rationale synthesizer that scalably augments raw instructions with rationale explanation. To probe more contextually grounded responses, Re-Critic employs an in-context self-critic mechanism to select response pairs for preference tuning. Experiments demonstrate that models fine-tuned with our rationale-augmented dataset yield gains that extend beyond hallucination-specific tasks to broader multimodal reasoning tasks.","authors":["Zexian Yang","Dian Li","Dayan Wu","Gang Liu","Weiping Wang"],"url":"https://arxiv.org/abs/2505.07172"}
{"created":"2025-05-13","title":"Internet of Agents: Fundamentals, Applications, and Challenges","abstract":"With the rapid proliferation of large language models and vision-language models, AI agents have evolved from isolated, task-specific systems into autonomous, interactive entities capable of perceiving, reasoning, and acting without human intervention. As these agents proliferate across virtual and physical environments, from virtual assistants to embodied robots, the need for a unified, agent-centric infrastructure becomes paramount. In this survey, we introduce the Internet of Agents (IoA) as a foundational framework that enables seamless interconnection, dynamic discovery, and collaborative orchestration among heterogeneous agents at scale. We begin by presenting a general IoA architecture, highlighting its hierarchical organization, distinguishing features relative to the traditional Internet, and emerging applications. Next, we analyze the key operational enablers of IoA, including capability notification and discovery, adaptive communication protocols, dynamic task matching, consensus and conflict-resolution mechanisms, and incentive models. Finally, we identify open research directions toward building resilient and trustworthy IoA ecosystems.","authors":["Yuntao Wang","Shaolong Guo","Yanghe Pan","Zhou Su","Fahao Chen","Tom H. Luan","Peng Li","Jiawen Kang","Dusit Niyato"],"url":"https://arxiv.org/abs/2505.07176"}
{"created":"2025-05-13","title":"Accountability of Generative AI: Exploring a Precautionary Approach for \"Artificially Created Nature\"","abstract":"The rapid development of generative artificial intelligence (AI) technologies raises concerns about the accountability of sociotechnical systems. Current generative AI systems rely on complex mechanisms that make it difficult for even experts to fully trace the reasons behind the outputs. This paper first examines existing research on AI transparency and accountability and argues that transparency is not a sufficient condition for accountability but can contribute to its improvement. We then discuss that if it is not possible to make generative AI transparent, generative AI technology becomes ``artificially created nature'' in a metaphorical sense, and suggest using the precautionary principle approach to consider AI risks. Finally, we propose that a platform for citizen participation is needed to address the risks of generative AI.","authors":["Yuri Nakao"],"url":"https://arxiv.org/abs/2505.07178"}
{"created":"2025-05-13","title":"Lagrange Oscillatory Neural Networks for Constraint Satisfaction and Optimization","abstract":"Physics-inspired computing paradigms are receiving renewed attention to enhance efficiency in compute-intensive tasks such as artificial intelligence and optimization. Similar to Hopfield neural networks, oscillatory neural networks (ONNs) minimize an Ising energy function that embeds the solutions of hard combinatorial optimization problems. Despite their success in solving unconstrained optimization problems, Ising machines still face challenges with constrained problems as they can get stuck at infeasible local minima. In this paper, we introduce a Lagrange ONN (LagONN) designed to escape infeasible states based on the theory of Lagrange multipliers. Unlike existing oscillatory Ising machines, LagONN employs additional Lagrange oscillators to guide the system towards feasible states in an augmented energy landscape and settles only when constraints are met. Taking the maximum satisfiability problem with three literals as a use case (Max-3-SAT), we harness LagONN's constraint satisfaction mechanism to find optimal solutions for random SATlib instances with up to 200 variables and 860 clauses, which provides a deterministic alternative to simulated annealing for coupled oscillators. We further discuss the potential of Lagrange oscillators to address other constraints, such as phase copying, which is useful in oscillatory Ising machines with limited connectivity.","authors":["Corentin Delacour","Bram Haverkort","Filip Sabo","Nadine Azemard","Aida Todri-Sanial"],"url":"https://arxiv.org/abs/2505.07179"}
{"created":"2025-05-13","title":"Causal View of Time Series Imputation: Some Identification Results on Missing Mechanism","abstract":"Time series imputation is one of the most challenge problems and has broad applications in various fields like health care and the Internet of Things. Existing methods mainly aim to model the temporally latent dependencies and the generation process from the observed time series data. In real-world scenarios, different types of missing mechanisms, like MAR (Missing At Random), and MNAR (Missing Not At Random) can occur in time series data. However, existing methods often overlook the difference among the aforementioned missing mechanisms and use a single model for time series imputation, which can easily lead to misleading results due to mechanism mismatching. In this paper, we propose a framework for time series imputation problem by exploring Different Missing Mechanisms (DMM in short) and tailoring solutions accordingly. Specifically, we first analyze the data generation processes with temporal latent states and missing cause variables for different mechanisms. Sequentially, we model these generation processes via variational inference and estimate prior distributions of latent variables via normalizing flow-based neural architecture. Furthermore, we establish identifiability results under the nonlinear independent component analysis framework to show that latent variables are identifiable. Experimental results show that our method surpasses existing time series imputation techniques across various datasets with different missing mechanisms, demonstrating its effectiveness in real-world applications.","authors":["Ruichu Cai","Kaitao Zheng","Junxian Huang","Zijian Li","Zhengming Chen","Boyan Xu","Zhifeng Hao"],"url":"https://arxiv.org/abs/2505.07180"}
{"created":"2025-05-13","title":"Economic data-enabled predictive control using machine learning","abstract":"In this paper, we propose a convex data-based economic predictive control method within the framework of data-enabled predictive control (DeePC). Specifically, we use a neural network to transform the system output into a new state space, where the nonlinear economic cost function of the underlying nonlinear system is approximated using a quadratic function expressed by the transformed output in the new state space. Both the neural network parameters and the coefficients of the quadratic function are learned from open-loop data of the system. Additionally, we reconstruct constrained output variables from the transformed output through learning an output reconstruction matrix; this way, the proposed economic DeePC can handle output constraints explicitly. The performance of the proposed method is evaluated via a case study in a simulated chemical process.","authors":["Mingxue Yan","Xuewen Zhang","Kaixiang Zhang","Zhaojian Li","Xunyuan Yin"],"url":"https://arxiv.org/abs/2505.07182"}
{"created":"2025-05-13","title":"Trigonometric Interpolation Based Approach for Second Order ODE with Mixed Boundary Conditions","abstract":"This paper proposes a trigonometric interpolation-based approach (TIBA) to approximate solutions of mixed boundary value problems of second-order ODEs. TIBA leverages the analytic attractiveness of a trigonometric polynomial to reformulate the dynamics of $y, y',y''$ implied by ODE and boundary conditions. TIBA is particularly attractive for a linear ODE where the solution can be obtained directly by solving a linear system. The framework can be used to solve integro-differential equations. Numerical tests have been conducted to assess TIBA's performance regarding convergence, existence, and uniqueness of solution under various boundary conditions with expected results.","authors":["Xiaorong Zou"],"url":"https://arxiv.org/abs/2505.07183"}
{"created":"2025-05-13","title":"Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs","abstract":"Large language models (LLMs) have achieved unprecedented performance by leveraging vast pretraining corpora, yet their performance remains suboptimal in knowledge-intensive domains such as medicine and scientific research, where high factual precision is required. While synthetic data provides a promising avenue for augmenting domain knowledge, existing methods frequently generate redundant samples that do not align with the model's true knowledge gaps. To overcome this limitation, we propose a novel Structural Entropy-guided Knowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge deficiencies of LLMs. Our approach employs the Structure Entropy (SE) metric to quantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree Search (MCTS) to selectively explore regions where the model lacks domain-specific knowledge. Guided by these insights, the framework generates targeted synthetic data for supervised fine-tuning, enabling continuous self-improvement. Experimental results on LLaMA-3 and Qwen2 across multiple domain-specific benchmarks show that SENATOR effectively detects and repairs knowledge deficiencies, achieving notable performance improvements. The code and data for our methods and experiments are available at https://github.com/weiyifan1023/senator.","authors":["Yifan Wei","Xiaoyan Yu","Tengfei Pan","Angsheng Li","Li Du"],"url":"https://arxiv.org/abs/2505.07184"}
{"created":"2025-05-13","title":"Reflexive Composition of Elementary State Machines, with an Application to the Reversal of Cellular Automata Rule 90","abstract":"We explore the dynamics of a one-dimensional lattice of state machines on two states and two symbols sequentially updated via a process of \"reflexive composition.\" The space of 256 machines exhibits a variety of behavior, including substitution, reversible \"billiard ball\" dynamics, and fractal nesting. We show that one machine generates the Sierpinski Triangle and, for a subset of boundary conditions, is isomorphic to cellular automata Rule 90 in Wolfram's naming scheme. More surprisingly, two other machines follow trajectories that map to Rule 90 in reverse. Whereas previous techniques have been developed to uncover preimages of Rule 90, this is the first study to produce such inverse dynamics naturally from the formalism itself. We argue that the system's symmetric treatment of state and message underlies its expressive power.","authors":["Chris Salzberg","Hiroki Sayama"],"url":"https://arxiv.org/abs/2505.07186"}
{"created":"2025-05-13","title":"Securing Genomic Data Against Inference Attacks in Federated Learning Environments","abstract":"Federated Learning (FL) offers a promising framework for collaboratively training machine learning models across decentralized genomic datasets without direct data sharing. While this approach preserves data locality, it remains susceptible to sophisticated inference attacks that can compromise individual privacy. In this study, we simulate a federated learning setup using synthetic genomic data and assess its vulnerability to three key attack vectors: Membership Inference Attack (MIA), Gradient-Based Membership Inference Attack, and Label Inference Attack (LIA). Our experiments reveal that Gradient-Based MIA achieves the highest effectiveness, with a precision of 0.79 and F1-score of 0.87, underscoring the risk posed by gradient exposure in federated updates. Additionally, we visualize comparative attack performance through radar plots and quantify model leakage across clients. The findings emphasize the inadequacy of na\\\"ive FL setups in safeguarding genomic privacy and motivate the development of more robust privacy-preserving mechanisms tailored to the unique sensitivity of genomic data.","authors":["Chetan Pathade","Shubham Patil"],"url":"https://arxiv.org/abs/2505.07188"}
{"created":"2025-05-13","title":"A Generative Re-ranking Model for List-level Multi-objective Optimization at Taobao","abstract":"E-commerce recommendation systems aim to generate ordered lists of items for customers, optimizing multiple business objectives, such as clicks, conversions and Gross Merchandise Volume (GMV). Traditional multi-objective optimization methods like formulas or Learning-to-rank (LTR) models take effect at item-level, neglecting dynamic user intent and contextual item interactions. List-level multi-objective optimization in the re-ranking stage can overcome this limitation, but most current re-ranking models focus more on accuracy improvement with context. In addition, re-ranking is faced with the challenges of time complexity and diversity. In light of this, we propose a novel end-to-end generative re-ranking model named Sequential Ordered Regression Transformer-Generator (SORT-Gen) for the less-studied list-level multi-objective optimization problem. Specifically, SORT-Gen is divided into two parts: 1)Sequential Ordered Regression Transformer innovatively uses Transformer and ordered regression to accurately estimate multi-objective values for variable-length sub-lists. 2)Mask-Driven Fast Generation Algorithm combines multi-objective candidate queues, efficient item selection and diversity mechanism into model inference, providing a fast online list generation method. Comprehensive online experiments demonstrate that SORT-Gen brings +4.13% CLCK and +8.10% GMV for Baiyibutie, a notable Mini-app of Taobao. Currently, SORT-Gen has been successfully deployed in multiple scenarios of Taobao App, serving for a vast number of users.","authors":["Yue Meng","Cheng Guo","Yi Cao","Tong Liu","Bo Zheng"],"url":"https://arxiv.org/abs/2505.07197"}
{"created":"2025-05-13","title":"Ranking-aware Continual Learning for LiDAR Place Recognition","abstract":"Place recognition plays a significant role in SLAM, robot navigation, and autonomous driving applications. Benefiting from deep learning, the performance of LiDAR place recognition (LPR) has been greatly improved. However, many existing learning-based LPR methods suffer from catastrophic forgetting, which severely harms the performance of LPR on previously trained places after training on a new environment. In this paper, we introduce a continual learning framework for LPR via Knowledge Distillation and Fusion (KDF) to alleviate forgetting. Inspired by the ranking process of place recognition retrieval, we present a ranking-aware knowledge distillation loss that encourages the network to preserve the high-level place recognition knowledge. We also introduce a knowledge fusion module to integrate the knowledge of old and new models for LiDAR place recognition. Our extensive experiments demonstrate that KDF can be applied to different networks to overcome catastrophic forgetting, surpassing the state-of-the-art methods in terms of mean Recall@1 and forgetting score.","authors":["Xufei Wang","Gengxuan Tian","Junqiao Zhao","Siyue Tao","Qiwen Gu","Qiankun Yu","Tiantian Feng"],"url":"https://arxiv.org/abs/2505.07198"}
{"created":"2025-05-13","title":"On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud","abstract":"Modern TTS systems designed for conversations achieve high-quality utterances but often remain inaccessible publicly. Are existing open-source architectures inadequate, or are current training techniques insufficient? This paper investigates prominent models and their underlying behaviors regarding conversational context. Using 20 GPU-hours on an NVIDIA H100, we empirically examine two approaches: context-based utterance-level training versus full conversation training. Results demonstrate that context-based utterance training achieves superior MOS scores (4.3/5.0 vs 3.7/5.0) and reduces training time by 37%, while full conversation approaches suffer from speaker similarity hallucination issues. These findings provide practical guidelines for conversational TTS development, favoring utterance-level training with contextual conditioning for both resource efficiency and output quality.","authors":["Hyouin Liu","Zhikuan Zhang"],"url":"https://arxiv.org/abs/2505.07202"}
{"created":"2025-05-13","title":"PrefillOnly: An Inference Engine for Prefill-only Workloads in Large Language Model Applications","abstract":"Besides typical generative applications, like ChatGPT, GitHub Copilot, and Cursor, we observe an emerging trend that LLMs are increasingly used in traditional discriminative tasks, such as recommendation, credit verification, and data labeling. The key characteristic of these emerging use cases is that the LLM generates only a single output token, rather than an arbitrarily long sequence of tokens. We call this prefill-only workload. However, since existing LLM engines assume arbitrary output lengths, they fail to leverage the unique properties of prefill-only workloads. In this paper, we present PrefillOnly, the first LLM inference engine that improves the inference throughput and latency by fully embracing the properties of prefill-only workloads. First, since it generates only one token, PrefillOnly only needs to store the KV cache of only the last computed layer, rather than of all layers. This drastically reduces the GPU memory footprint of LLM inference and allows handling long inputs without using solutions that reduces throughput, such as cross-GPU KV cache parallelization. Second, because the output length is fixed, rather than arbitrary, PrefillOnly can precisely determine the job completion time (JCT) of each prefill-only request before it starts. This enables efficient JCT-aware scheduling policies such as shortest remaining job first. PrefillOnly can process upto 4x larger queries per second without inflating average and P99 latency.","authors":["Kuntai Du","Bowen Wang","Chen Zhang","Yiming Cheng","Qing Lan","Hejian Sang","Yihua Cheng","Jiayi Yao","Xiaoxuan Liu","Yifan Qiao","Ion Stoica","Junchen Jiang"],"url":"https://arxiv.org/abs/2505.07203"}
{"created":"2025-05-13","title":"Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030","abstract":"Large Language Models (LLMs) are poised to transform healthcare under China's Healthy China 2030 initiative, yet they introduce new ethical and patient-safety challenges. We present a novel 12,000-item Q&amp;A benchmark covering 11 ethics and 9 safety dimensions in medical contexts, to quantitatively evaluate these risks. Using this dataset, we assess state-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing moderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant improvements after fine-tuning on our data (up to 50.8% accuracy). Results show notable gaps in LLM decision-making on ethics and safety scenarios, reflecting insufficient institutional oversight. We then identify systemic governance shortfalls-including the lack of fine-grained ethical audit protocols, slow adaptation by hospital IRBs, and insufficient evaluation tools-that currently hinder safe LLM deployment. Finally, we propose a practical governance framework for healthcare institutions (embedding LLM auditing teams, enacting data ethics guidelines, and implementing safety simulation pipelines) to proactively manage LLM risks. Our study highlights the urgent need for robust LLM governance in Chinese healthcare, aligning AI innovation with patient safety and ethical standards.","authors":["Mouxiao Bian","Rongzhao Zhang","Chao Ding","Xinwei Peng","Jie Xu"],"url":"https://arxiv.org/abs/2505.07205"}
{"created":"2025-05-13","title":"Hypergraph Coordination Networks with Dynamic Grouping for Multi-Agent Reinforcement Learning","abstract":"Cooperative multi-agent reinforcement learning faces significant challenges in effectively organizing agent relationships and facilitating information exchange, particularly when agents need to adapt their coordination patterns dynamically. This paper presents a novel framework that integrates dynamic spectral clustering with hypergraph neural networks to enable adaptive group formation and efficient information processing in multi-agent systems. The proposed framework dynamically constructs and updates hypergraph structures through spectral clustering on agents' state histories, enabling higher-order relationships to emerge naturally from agent interactions. The hypergraph structure is enhanced with attention mechanisms for selective information processing, providing an expressive and efficient way to model complex agent relationships. This architecture can be implemented in both value-based and policy-based paradigms through a unified objective combining task performance with structural regularization. Extensive experiments on challenging cooperative tasks demonstrate that our method significantly outperforms state-of-the-art approaches in both sample efficiency and final performance.","authors":["Chiqiang Liu","Dazi Li"],"url":"https://arxiv.org/abs/2505.07207"}
{"created":"2025-05-13","title":"An Empirical Study: MEMS as a Static Performance Metric","abstract":"Static performance estimation is essential during compile-time analysis, yet traditional runtime-based methods are costly and platform-dependent. We investigate mems, the number of memory accesses, as a static and architecture-independent performance metric. We develop a Clang-based automated instrumentation tool that rewrites source code to insert path tracing and \\textit{mems} counting logic. This allows us to evaluate mems-based performance estimation across ten classical algorithm programs. Experimental results show that within the same program, execution paths with higher mems values consistently exhibit longer runtime. However, this correlation weakens between different programs, suggesting that mems is best suited for comparing performance of different execution paths in a program.","authors":["Liwei Zhang","Baoquan Cui","Xutong Ma","Jian Zhang"],"url":"https://arxiv.org/abs/2505.07208"}
{"created":"2025-05-13","title":"Discovering Fine-Grained Visual-Concept Relations by Disentangled Optimal Transport Concept Bottleneck Models","abstract":"Concept Bottleneck Models (CBMs) try to make the decision-making process transparent by exploring an intermediate concept space between the input image and the output prediction. Existing CBMs just learn coarse-grained relations between the whole image and the concepts, less considering local image information, leading to two main drawbacks: i) they often produce spurious visual-concept relations, hence decreasing model reliability; and ii) though CBMs could explain the importance of every concept to the final prediction, it is still challenging to tell which visual region produces the prediction. To solve these problems, this paper proposes a Disentangled Optimal Transport CBM (DOT-CBM) framework to explore fine-grained visual-concept relations between local image patches and concepts. Specifically, we model the concept prediction process as a transportation problem between the patches and concepts, thereby achieving explicit fine-grained feature alignment. We also incorporate orthogonal projection losses within the modality to enhance local feature disentanglement. To further address the shortcut issues caused by statistical biases in the data, we utilize the visual saliency map and concept label statistics as transportation priors. Thus, DOT-CBM can visualize inversion heatmaps, provide more reliable concept predictions, and produce more accurate class predictions. Comprehensive experiments demonstrate that our proposed DOT-CBM achieves SOTA performance on several tasks, including image classification, local part detection and out-of-distribution generalization.","authors":["Yan Xie","Zequn Zeng","Hao Zhang","Yucheng Ding","Yi Wang","Zhengjue Wang","Bo Chen","Hongwei Liu"],"url":"https://arxiv.org/abs/2505.07209"}
{"created":"2025-05-13","title":"The Language of Influence: Sentiment, Emotion, and Hate Speech in State Sponsored Influence Operations","abstract":"State-sponsored influence operations (SIOs) on social media have become an instrumental tool for manipulating public opinion and spreading unverified information. This study analyzes the sentiment, emotion, and abusive speech in tweets circulated by influence campaigns originating from three distinct countries: China, Iran, and Russia. We examined 1.5 million tweets to uncover patterns in the content of the influence operations using the dataset provided by Twitter. Our findings reveal distinct patterns of sentiment, emotion, and abusive nature in different SIOs. Our experimental result shows that Russian influence operations predominantly employ negative sentiment and toxic language to polarize audiences, Iranian operations balance negative and positive tones to provoke hostility while fostering support, and Chinese campaigns focus on positive messaging to promote favorable narratives.","authors":["Ashfaq Ali Shafin","Khandaker Mamun Ahmed"],"url":"https://arxiv.org/abs/2505.07212"}
{"created":"2025-05-13","title":"Towards user-centered interactive medical image segmentation in VR with an assistive AI agent","abstract":"Crucial in disease analysis and surgical planning, manual segmentation of volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and challenging to master, while fully automatic algorithms can benefit from user-feedback. Therefore, with the complementary power of the latest radiological AI foundation models and virtual reality (VR)'s intuitive data interaction, we propose SAMIRA, a novel conversational AI agent that assists users with localizing, segmenting, and visualizing 3D medical concepts in VR. Through speech-based interaction, the agent helps users understand radiological features, locate clinical targets, and generate segmentation masks that can be refined with just a few point prompts. The system also supports true-to-scale 3D visualization of segmented pathology to enhance patient-specific anatomical understanding. Furthermore, to determine the optimal interaction paradigm under near-far attention-switching for refining segmentation masks in an immersive, human-in-the-loop workflow, we compare VR controller pointing, head pointing, and eye tracking as input modes. With a user study, evaluations demonstrated a high usability score (SUS=90.0 $\\pm$ 9.0), low overall task load, as well as strong support for the proposed VR system's guidance, training potential, and integration of AI in radiological segmentation tasks.","authors":["Pascal Spiegler","Arash Harirpoush","Yiming Xiao"],"url":"https://arxiv.org/abs/2505.07214"}
{"created":"2025-05-13","title":"Measuring General Intelligence with Generated Games","abstract":"We present gg-bench, a collection of game environments designed to evaluate general reasoning capabilities in language models. Unlike most static benchmarks, gg-bench is a data generating process where new evaluation instances can be generated at will. In particular, gg-bench is synthetically generated by (1) using a large language model (LLM) to generate natural language descriptions of novel games, (2) using the LLM to implement each game in code as a Gym environment, and (3) training reinforcement learning (RL) agents via self-play on the generated games. We evaluate language models by their winrate against these RL agents by prompting models with the game description, current board state, and a list of valid moves, after which models output the moves they wish to take. gg-bench is challenging: state-of-the-art LLMs such as GPT-4o and Claude 3.7 Sonnet achieve winrates of 7-9% on gg-bench using in-context learning, while reasoning models such as o1, o3-mini and DeepSeek-R1 achieve average winrates of 31-36%. We release the generated games, data generation process, and evaluation code in order to support future modeling work and expansion of our benchmark.","authors":["Vivek Verma","David Huang","William Chen","Dan Klein","Nicholas Tomlin"],"url":"https://arxiv.org/abs/2505.07215"}
{"created":"2025-05-13","title":"Language-Driven Dual Style Mixing for Single-Domain Generalized Object Detection","abstract":"Generalizing an object detector trained on a single domain to multiple unseen domains is a challenging task. Existing methods typically introduce image or feature augmentation to diversify the source domain to raise the robustness of the detector. Vision-Language Model (VLM)-based augmentation techniques have been proven to be effective, but they require that the detector's backbone has the same structure as the image encoder of VLM, limiting the detector framework selection. To address this problem, we propose Language-Driven Dual Style Mixing (LDDS) for single-domain generalization, which diversifies the source domain by fully utilizing the semantic information of the VLM. Specifically, we first construct prompts to transfer style semantics embedded in the VLM to an image translation network. This facilitates the generation of style diversified images with explicit semantic information. Then, we propose image-level style mixing between the diversified images and source domain images. This effectively mines the semantic information for image augmentation without relying on specific augmentation selections. Finally, we propose feature-level style mixing in a double-pipeline manner, allowing feature augmentation to be model-agnostic and can work seamlessly with the mainstream detector frameworks, including the one-stage, two-stage, and transformer-based detectors. Extensive experiments demonstrate the effectiveness of our approach across various benchmark datasets, including real to cartoon and normal to adverse weather tasks. The source code and pre-trained models will be publicly available at https://github.com/qinhongda8/LDDS.","authors":["Hongda Qin","Xiao Lu","Zhiyong Wei","Yihong Cao","Kailun Yang","Ningjiang Chen"],"url":"https://arxiv.org/abs/2505.07219"}
{"created":"2025-05-13","title":"Compression, Regularity, Randomness and Emergent Structure: Rethinking Physical Complexity in the Data-Driven Era","abstract":"Complexity science offers a wide range of measures for quantifying unpredictability, structure, and information. Yet, a systematic conceptual organization of these measures is still missing.","authors":["Nima Dehghani"],"url":"https://arxiv.org/abs/2505.07222"}
{"created":"2025-05-13","title":"DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation","abstract":"Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG","authors":["Jiashuo Sun","Xianrui Zhong","Sizhe Zhou","Jiawei Han"],"url":"https://arxiv.org/abs/2505.07233"}
{"created":"2025-05-13","title":"A Novel Online Pseudospectral Method for Approximation of Nonlinear Systems Dynamics","abstract":"This paper presents a novel online system identification approach utilizing the Chebyshev pseudospectral (PS) method to approximate the dynamics of a continuous-time nonlinear system. Unlike conventional periodic sampling, the proposed identification scheme employs aperiodic state sampling, leveraging the Chebyshev nodes to achieve the desired approximation accuracy. Unlike traditional off-line PS approaches, the scheme utilizes a moving time-window strategy to compute the sampling instants (Chebyshev nodes) forward in time for state measurements. Within each time window, the number of sampling instants is adaptively determined to meet the specified approximation accuracy. The Chebyshev basis is also shifted for each time window to accommodate arbitrary approximation intervals. The least-squares approach is employed to estimate the coefficients of the shifted Chebyshev basis functions using the measured state and its derivatives at the end of each window, resulting in a piecewise approximation of the drift dynamics of the system. In the second step, the identified drift dynamics are utilized to design an adaptive state estimator to reconstruct the continuous system states from the aperiodic state measurements. To address the smoothness of the piecewise approximated system dynamics, the Chebyshev coefficients are recomputed at the window transition instants, between two consecutive time windows, by enforcing continuity of the approximated function and its derivatives. In addition, analytical results are provided to determine the number of sampling instants within each time window, which guarantees the desired approximation accuracy. The boundedness of function approximation and state estimation errors is also proved analytically. Finally, numerical simulation results are included to validate the proposed scheme.","authors":["Arian Yousefian","Avimanyu Sahoo","Vignesh Narayanan"],"url":"https://arxiv.org/abs/2505.07234"}
{"created":"2025-05-13","title":"Multi-band Frequency Reconstruction for Neural Psychoacoustic Coding","abstract":"Achieving high-fidelity audio compression while preserving perceptual quality across diverse content remains a key challenge in Neural Audio Coding (NAC). We introduce MUFFIN, a fully convolutional Neural Psychoacoustic Coding (NPC) framework that leverages psychoacoustically guided multi-band frequency reconstruction. At its core is a Multi-Band Spectral Residual Vector Quantization (MBS-RVQ) module that allocates bitrate across frequency bands based on perceptual salience. This design enables efficient compression while disentangling speaker identity from content using distinct codebooks. MUFFIN incorporates a transformer-inspired convolutional backbone and a modified snake activation to enhance resolution in fine-grained spectral regions. Experimental results on multiple benchmarks demonstrate that MUFFIN consistently outperforms existing approaches in reconstruction quality. A high-compression variant achieves a state-of-the-art 12.5 Hz rate with minimal loss. MUFFIN also proves effective in downstream generative tasks, highlighting its promise as a token representation for integration with language models. Audio samples and code are available.","authors":["Dianwen Ng","Kun Zhou","Yi-Wen Chao","Zhiwei Xiong","Bin Ma","Eng Siong Chng"],"url":"https://arxiv.org/abs/2505.07235"}
{"created":"2025-05-13","title":"UAV-CodeAgents: Scalable UAV Mission Planning via Multi-Agent ReAct and Vision-Language Reasoning","abstract":"We present UAV-CodeAgents, a scalable multi-agent framework for autonomous UAV mission generation, built on large language and vision-language models (LLMs/VLMs). The system leverages the ReAct (Reason + Act) paradigm to interpret satellite imagery, ground high-level natural language instructions, and collaboratively generate UAV trajectories with minimal human supervision. A core component is a vision-grounded, pixel-pointing mechanism that enables precise localization of semantic targets on aerial maps. To support real-time adaptability, we introduce a reactive thinking loop, allowing agents to iteratively reflect on observations, revise mission goals, and coordinate dynamically in evolving environments.","authors":["Oleg Sautenkov","Yasheerah Yaqoot","Muhammad Ahsan Mustafa","Faryal Batool","Jeffrin Sam","Artem Lykov","Chih-Yung Wen","Dzmitry Tsetserukou"],"url":"https://arxiv.org/abs/2505.07236"}
{"created":"2025-05-13","title":"Comet: Accelerating Private Inference for Large Language Model by Predicting Activation Sparsity","abstract":"With the growing use of large language models (LLMs) hosted on cloud platforms to offer inference services, privacy concerns about the potential leakage of sensitive information are escalating. Secure multi-party computation (MPC) is a promising solution to protect the privacy in LLM inference. However, MPC requires frequent inter-server communication, causing high performance overhead.","authors":["Guang Yan","Yuhui Zhang","Zimu Guo","Lutan Zhao","Xiaojun Chen","Chen Wang","Wenhao Wang","Dan Meng","Rui Hou"],"url":"https://arxiv.org/abs/2505.07239"}
{"created":"2025-05-13","title":"Continuous-Time Control Synthesis for Multiple Quadrotors under Signal Temporal Logic Specifications","abstract":"Ensuring continuous-time control of multiple quadrotors in constrained environments under signal temporal logic (STL) specifications is challenging due to nonlinear dynamics, safety constraints, and disturbances. This letter proposes a two-stage framework to address this challenge. First, exponentially decaying tracking error bounds are derived with multidimensional geometric control gains obtained via differential evolution. These bounds are less conservative, while the resulting tracking errors exhibit smaller oscillations and improved transient performance. Second, leveraging the time-varying bounds, a mixed-integer convex programming (MICP) formulation generates piecewise B\\'ezier reference trajectories that satisfy STL and velocity limits, while ensuring inter-agent safety through convex-hull properties. Simulation results demonstrate that the proposed approach enables formally verifiable multi-agent coordination in constrained environments, with provable tracking guarantees under bounded disturbances.","authors":["Yating Yuan"],"url":"https://arxiv.org/abs/2505.07240"}
{"created":"2025-05-13","title":"A Black-box Testing Framework for Oracle Quantum Programs","abstract":"Oracle quantum programs are a fundamental class of quantum programs that serve as a critical bridge between quantum computing and classical computing. Many important quantum algorithms are built upon oracle quantum programs, making it essential to ensure their correctness during development. While software testing is a well-established approach for improving program reliability, no systematic method has been developed to test oracle quantum programs. This paper proposes a black-box testing framework designed for general oracle quantum programs. We define these programs formally, establish the foundational theory for their testing, and propose a detailed testing framework. We develop a prototype tool and conduct extensive experimental evaluations to evaluate the framework's effectiveness. Our results demonstrate that the proposed framework significantly aids developers in testing oracle quantum programs, providing insights to enhance the reliability of quantum software.","authors":["Peixun Long","Jianjun Zhao"],"url":"https://arxiv.org/abs/2505.07243"}
{"created":"2025-05-13","title":"REMEDI: Relative Feature Enhanced Meta-Learning with Distillation for Imbalanced Prediction","abstract":"Predicting future vehicle purchases among existing owners presents a critical challenge due to extreme class imbalance (<0.5% positive rate) and complex behavioral patterns. We propose REMEDI (Relative feature Enhanced Meta-learning with Distillation for Imbalanced prediction), a novel multi-stage framework addressing these challenges. REMEDI first trains diverse base models to capture complementary aspects of user behavior. Second, inspired by comparative op-timization techniques, we introduce relative performance meta-features (deviation from ensemble mean, rank among peers) for effective model fusion through a hybrid-expert architecture. Third, we distill the ensemble's knowledge into a single efficient model via supervised fine-tuning with MSE loss, enabling practical deployment. Evaluated on approximately 800,000 vehicle owners, REMEDI significantly outperforms baseline approaches, achieving the business target of identifying ~50% of actual buyers within the top 60,000 recommendations at ~10% precision. The distilled model preserves the ensemble's predictive power while maintaining deployment efficiency, demonstrating REMEDI's effectiveness for imbalanced prediction in industry settings.","authors":["Fei Liu","Huanhuan Ren","Yu Guan","Xiuxu Wang","Wang Lv","Zhiqiang Hu","Yaxi Chen"],"url":"https://arxiv.org/abs/2505.07245"}
{"created":"2025-05-13","title":"SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models","abstract":"Subjective Answer Grading (SAG) plays a crucial role in education, standardized testing, and automated assessment systems, particularly for evaluating short-form responses in Short Answer Scoring (SAS). However, existing approaches often produce coarse-grained scores and lack detailed reasoning. Although large language models (LLMs) have demonstrated potential as zero-shot evaluators, they remain susceptible to bias, inconsistencies with human judgment, and limited transparency in scoring decisions. To overcome these limitations, we introduce SAS-Bench, a benchmark specifically designed for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring, expert-annotated error categories, and a diverse range of question types derived from real-world subject-specific exams. This benchmark facilitates detailed evaluation of model reasoning processes and explainability. We also release an open-source dataset containing 1,030 questions and 4,109 student responses, each annotated by domain experts. Furthermore, we conduct comprehensive experiments with various LLMs, identifying major challenges in scoring science-related questions and highlighting the effectiveness of few-shot prompting in improving scoring accuracy. Our work offers valuable insights into the development of more robust, fair, and educationally meaningful LLM-based evaluation systems.","authors":["Peichao Lai","Kexuan Zhang","Yi Lin","Linyihan Zhang","Feiyang Ye","Jinhao Yan","Yanwei Xu","Conghui He","Yilei Wang","Wentao Zhang","Bin Cui"],"url":"https://arxiv.org/abs/2505.07247"}
{"created":"2025-05-13","title":"When Dance Video Archives Challenge Computer Vision","abstract":"The accuracy and efficiency of human body pose estimation depend on the quality of the data to be processed and of the particularities of these data. To demonstrate how dance videos can challenge pose estimation techniques, we proposed a new 3D human body pose estimation pipeline which combined up-to-date techniques and methods that had not been yet used in dance analysis. Second, we performed tests and extensive experimentations from dance video archives, and used visual analytic tools to evaluate the impact of several data parameters on human body pose. Our results are publicly available for research at https://www.couleur.org/articles/arXiv-1-2025/","authors":["Philippe Colantoni","Rafique Ahmed","Prashant Ghimire","Damien Muselet","Alain Tr\\'emeau"],"url":"https://arxiv.org/abs/2505.07249"}
{"created":"2025-05-13","title":"Incomplete In-context Learning","abstract":"Large vision language models (LVLMs) achieve remarkable performance through Vision In-context Learning (VICL), a process that depends significantly on demonstrations retrieved from an extensive collection of annotated examples (retrieval database). Existing studies often assume that the retrieval database contains annotated examples for all labels. However, in real-world scenarios, delays in database updates or incomplete data annotation may result in the retrieval database containing labeled samples for only a subset of classes. We refer to this phenomenon as an \\textbf{incomplete retrieval database} and define the in-context learning under this condition as \\textbf{Incomplete In-context Learning (IICL)}. To address this challenge, we propose \\textbf{Iterative Judgments and Integrated Prediction (IJIP)}, a two-stage framework designed to mitigate the limitations of IICL. The Iterative Judgments Stage reformulates an \\(\\boldsymbol{m}\\)-class classification problem into a series of \\(\\boldsymbol{m}\\) binary classification tasks, effectively converting the IICL setting into a standard VICL scenario. The Integrated Prediction Stage further refines the classification process by leveraging both the input image and the predictions from the Iterative Judgments Stage to enhance overall classification accuracy. IJIP demonstrates considerable performance across two LVLMs and two datasets under three distinct conditions of label incompleteness, achieving the highest accuracy of 93.9\\%. Notably, even in scenarios where labels are fully available, IJIP still achieves the best performance of all six baselines. Furthermore, IJIP can be directly applied to \\textbf{Prompt Learning} and is adaptable to the \\textbf{text domain}.","authors":["Wenqiang Wang","Yangshijie Zhang"],"url":"https://arxiv.org/abs/2505.07251"}
{"created":"2025-05-13","title":"Towards Accurate State Estimation: Kalman Filter Incorporating Motion Dynamics for 3D Multi-Object Tracking","abstract":"This work addresses the critical lack of precision in state estimation in the Kalman filter for 3D multi-object tracking (MOT) and the ongoing challenge of selecting the appropriate motion model. Existing literature commonly relies on constant motion models for estimating the states of objects, neglecting the complex motion dynamics unique to each object. Consequently, trajectory division and imprecise object localization arise, especially under occlusion conditions. The core of these challenges lies in the limitations of the current Kalman filter formulation, which fails to account for the variability of motion dynamics as objects navigate their environments. This work introduces a novel formulation of the Kalman filter that incorporates motion dynamics, allowing the motion model to adaptively adjust according to changes in the object's movement. The proposed Kalman filter substantially improves state estimation, localization, and trajectory prediction compared to the traditional Kalman filter. This is reflected in tracking performance that surpasses recent benchmarks on the KITTI and Waymo Open Datasets, with margins of 0.56\\% and 0.81\\% in higher order tracking accuracy (HOTA) and multi-object tracking accuracy (MOTA), respectively. Furthermore, the proposed Kalman filter consistently outperforms the baseline across various detectors. Additionally, it shows an enhanced capability in managing long occlusions compared to the baseline Kalman filter, achieving margins of 1.22\\% in higher order tracking accuracy (HOTA) and 1.55\\% in multi-object tracking accuracy (MOTA) on the KITTI dataset. The formulation's efficiency is evident, with an additional processing time of only approximately 0.078 ms per frame, ensuring its applicability in real-time applications.","authors":["Mohamed Nagy","Naoufel Werghi","Bilal Hassan","Jorge Dias","Majid Khonji"],"url":"https://arxiv.org/abs/2505.07254"}
{"created":"2025-05-13","title":"Synthetic Similarity Search in Automotive Production","abstract":"Visual quality inspection in automotive production is essential for ensuring the safety and reliability of vehicles. Computer vision (CV) has become a popular solution for these inspections due to its cost-effectiveness and reliability. However, CV models require large, annotated datasets, which are costly and time-consuming to collect. To reduce the need for extensive training data, we propose a novel image classification pipeline that combines similarity search using a vision-based foundation model with synthetic data. Our approach leverages a DINOv2 model to transform input images into feature vectors, which are then compared to pre-classified reference images using cosine distance measurements. By utilizing synthetic data instead of real images as references, our pipeline achieves high classification accuracy without relying on real data. We evaluate this approach in eight real-world inspection scenarios and demonstrate that it meets the high performance requirements of production environments.","authors":["Christoph Huber","Ludwig Schleeh","Dino Knoll","Michael Guthe"],"url":"https://arxiv.org/abs/2505.07256"}
{"created":"2025-05-13","title":"DARLR: Dual-Agent Offline Reinforcement Learning for Recommender Systems with Dynamic Reward","abstract":"Model-based offline reinforcement learning (RL) has emerged as a promising approach for recommender systems, enabling effective policy learning by interacting with frozen world models. However, the reward functions in these world models, trained on sparse offline logs, often suffer from inaccuracies. Specifically, existing methods face two major limitations in addressing this challenge: (1) deterministic use of reward functions as static look-up tables, which propagates inaccuracies during policy learning, and (2) static uncertainty designs that fail to effectively capture decision risks and mitigate the impact of these inaccuracies. In this work, a dual-agent framework, DARLR, is proposed to dynamically update world models to enhance recommendation policies. To achieve this, a \\textbf{\\textit{selector}} is introduced to identify reference users by balancing similarity and diversity so that the \\textbf{\\textit{recommender}} can aggregate information from these users and iteratively refine reward estimations for dynamic reward shaping. Further, the statistical features of the selected users guide the dynamic adaptation of an uncertainty penalty to better align with evolving recommendation requirements. Extensive experiments on four benchmark datasets demonstrate the superior performance of DARLR, validating its effectiveness. The code is available at https://github.com/ArronDZhang/DARLR.","authors":["Yi Zhang","Ruihong Qiu","Xuwei Xu","Jiajun Liu","Sen Wang"],"url":"https://arxiv.org/abs/2505.07257"}
{"created":"2025-05-13","title":"No Query, No Access","abstract":"Textual adversarial attacks mislead NLP models, including Large Language Models (LLMs), by subtly modifying text. While effective, existing attacks often require knowledge of the victim model, extensive queries, or access to training data, limiting real-world feasibility. To overcome these constraints, we introduce the \\textbf{Victim Data-based Adversarial Attack (VDBA)}, which operates using only victim texts. To prevent access to the victim model, we create a shadow dataset with publicly available pre-trained models and clustering methods as a foundation for developing substitute models. To address the low attack success rate (ASR) due to insufficient information feedback, we propose the hierarchical substitution model design, generating substitute models to mitigate the failure of a single substitute model at the decision boundary.","authors":["Wenqiang Wang","Siyuan Liang","Yangshijie Zhang","Xiaojun Jia","Hao Lin","Xiaochun Cao"],"url":"https://arxiv.org/abs/2505.07258"}
{"created":"2025-05-13","title":"A Framework for Joint Grasp and Motion Planning in Confined Spaces","abstract":"Robotic grasping is a fundamental skill across all domains of robot applications. There is a large body of research for grasping objects in table-top scenarios, where finding suitable grasps is the main challenge. In this work, we are interested in scenarios where the objects are in confined spaces and hence particularly difficult to reach. Planning how the robot approaches the object becomes a major part of the challenge, giving rise to methods for joint grasp and motion planning. The framework proposed in this paper provides 20 benchmark scenarios with systematically increasing difficulty, realistic objects with precomputed grasp annotations, and tools to create and share more scenarios. We further provide two baseline planners and evaluate them on the scenarios, demonstrating that the proposed difficulty levels indeed offer a meaningful progression. We invite the research community to build upon this framework by making all components publicly available as open source.","authors":["Martin Rudorfer","Ji\\v{r}\\'i Hartvich","Vojt\\v{e}ch Von\\'asek"],"url":"https://arxiv.org/abs/2505.07259"}
{"created":"2025-05-13","title":"UMoE: Unifying Attention and FFN with Shared Experts","abstract":"Sparse Mixture of Experts (MoE) architectures have emerged as a promising approach for scaling Transformer models. While initial works primarily incorporated MoE into feed-forward network (FFN) layers, recent studies have explored extending the MoE paradigm to attention layers to enhance model performance. However, existing attention-based MoE layers require specialized implementations and demonstrate suboptimal performance compared to their FFN-based counterparts. In this paper, we aim to unify the MoE designs in attention and FFN layers by introducing a novel reformulation of the attention mechanism, revealing an underlying FFN-like structure within attention modules. Our proposed architecture, UMoE, achieves superior performance through attention-based MoE layers while enabling efficient parameter sharing between FFN and attention components.","authors":["Yuanhang Yang","Chaozheng Wang","Jing Li"],"url":"https://arxiv.org/abs/2505.07260"}
{"created":"2025-05-13","title":"CHD: Coupled Hierarchical Diffusion for Long-Horizon Tasks","abstract":"Diffusion-based planners have shown strong performance in short-horizon tasks but often fail in complex, long-horizon settings. We trace the failure to loose coupling between high-level (HL) sub-goal selection and low-level (LL) trajectory generation, which leads to incoherent plans and degraded performance. We propose Coupled Hierarchical Diffusion (CHD), a framework that models HL sub-goals and LL trajectories jointly within a unified diffusion process. A shared classifier passes LL feedback upstream so that sub-goals self-correct while sampling proceeds. This tight HL-LL coupling improves trajectory coherence and enables scalable long-horizon diffusion planning. Experiments across maze navigation, tabletop manipulation, and household environments show that CHD consistently outperforms both flat and hierarchical diffusion baselines.","authors":["Ce Hao","Anxing Xiao","Zhiwei Xue","Harold Soh"],"url":"https://arxiv.org/abs/2505.07261"}
{"created":"2025-05-13","title":"Skywork-VL Reward: An Effective Reward Model for Multimodal Understanding and Reasoning","abstract":"We propose Skywork-VL Reward, a multimodal reward model that provides reward signals for both multimodal understanding and reasoning tasks. Our technical approach comprises two key components: First, we construct a large-scale multimodal preference dataset that covers a wide range of tasks and scenarios, with responses collected from both standard vision-language models (VLMs) and advanced VLM reasoners. Second, we design a reward model architecture based on Qwen2.5-VL-7B-Instruct, integrating a reward head and applying multi-stage fine-tuning using pairwise ranking loss on pairwise preference data. Experimental evaluations show that Skywork-VL Reward achieves state-of-the-art results on multimodal VL-RewardBench and exhibits competitive performance on the text-only RewardBench benchmark. Furthermore, preference data constructed based on our Skywork-VL Reward proves highly effective for training Mixed Preference Optimization (MPO), leading to significant improvements in multimodal reasoning capabilities. Our results underscore Skywork-VL Reward as a significant advancement toward general-purpose, reliable reward models for multimodal alignment. Our model has been publicly released to promote transparency and reproducibility.","authors":["Xiaokun Wang","Chris","Jiangbo Pei","Wei Shen","Yi Peng","Yunzhuo Hao","Weijie Qiu","Ai Jian","Tianyidan Xie","Xuchen Song","Yang Liu","Yahui Zhou"],"url":"https://arxiv.org/abs/2505.07263"}
{"created":"2025-05-13","title":"BETTY Dataset: A Multi-modal Dataset for Full-Stack Autonomy","abstract":"We present the BETTY dataset, a large-scale, multi-modal dataset collected on several autonomous racing vehicles, targeting supervised and self-supervised state estimation, dynamics modeling, motion forecasting, perception, and more. Existing large-scale datasets, especially autonomous vehicle datasets, focus primarily on supervised perception, planning, and motion forecasting tasks. Our work enables multi-modal, data-driven methods by including all sensor inputs and the outputs from the software stack, along with semantic metadata and ground truth information. The dataset encompasses 4 years of data, currently comprising over 13 hours and 32TB, collected on autonomous racing vehicle platforms. This data spans 6 diverse racing environments, including high-speed oval courses, for single and multi-agent algorithm evaluation in feature-sparse scenarios, as well as high-speed road courses with high longitudinal and lateral accelerations and tight, GPS-denied environments. It captures highly dynamic states, such as 63 m/s crashes, loss of tire traction, and operation at the limit of stability. By offering a large breadth of cross-modal and dynamic data, the BETTY dataset enables the training and testing of full autonomy stack pipelines, pushing the performance of all algorithms to the limits. The current dataset is available at https://pitt-mit-iac.github.io/betty-dataset/.","authors":["Micah Nye","Ayoub Raji","Andrew Saba","Eidan Erlich","Robert Exley","Aragya Goyal","Alexander Matros","Ritesh Misra","Matthew Sivaprakasam","Marko Bertogna","Deva Ramanan","Sebastian Scherer"],"url":"https://arxiv.org/abs/2505.07266"}
{"created":"2025-05-13","title":"Reconfiguring Multiple Connected Components with Size Multiset Constraints","abstract":"We propose a novel generalization of Independent Set Reconfiguration (ISR): Connected Components Reconfiguration (CCR). In CCR, we are given a graph $G$, two vertex subsets $A$ and $B$, and a multiset $\\mathcal{M}$ of positive integers. The question is whether $A$ and $B$ are reconfigurable under a certain rule, while ensuring that each vertex subset induces connected components whose sizes match the multiset $\\mathcal{M}$. ISR is a special case of CCR where $\\mathcal{M}$ only contains 1. We also propose new reconfiguration rules: component jumping (CJ) and component sliding (CS), which regard connected components as tokens. Since CCR generalizes ISR, the problem is PSPACE-complete. In contrast, we show three positive results: First, CCR-CS and CCR-CJ are solvable in linear and quadratic time, respectively, when $G$ is a path. Second, we show that CCR-CS is solvable in linear time for cographs. Third, when $\\mathcal{M}$ contains only the same elements (i.e., all connected components have the same size), we show that CCR-CJ is solvable in linear time if $G$ is chordal. The second and third results generalize known results for ISR and exhibit an interesting difference between the reconfiguration rules.","authors":["Yu Nakahata"],"url":"https://arxiv.org/abs/2505.07268"}
{"created":"2025-05-13","title":"Automated Repair of Ambiguous Natural Language Requirements","abstract":"The rise of large language models (LLMs) has amplified the role of natural language (NL) in software engineering, and its inherent ambiguity and susceptibility to misinterpretation pose a fundamental challenge for software quality, because employing ambiguous requirements may result in the generation of faulty programs. The complexity of ambiguity detection and resolution motivates us to introduce the problem of automated repair of ambiguous NL requirements.","authors":["Haoxiang Jia","Robbie Morris","He Ye","Federica Sarro","Sergey Mechtaev"],"url":"https://arxiv.org/abs/2505.07270"}
{"created":"2025-05-13","title":"On the Robustness of Reward Models for Language Model Alignment","abstract":"The Bradley-Terry (BT) model is widely practiced in reward modeling for reinforcement learning with human feedback (RLHF). Despite its effectiveness, reward models (RMs) trained with BT model loss are prone to over-optimization, losing generalizability to unseen input distributions. In this paper, we study the cause of over-optimization in RM training and its downstream effects on the RLHF procedure, accentuating the importance of distributional robustness of RMs in unseen data. First, we show that the excessive dispersion of hidden state norms is the main source of over-optimization. Then, we propose batch-wise sum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch, constraining the rewards with extreme magnitudes. We assess the impact of BSR in improving robustness in RMs through four scenarios of over-optimization, where BSR consistently manifests better robustness. Subsequently, we compare the plain BT model and BSR on RLHF training and empirically show that robust RMs better align the policy to the gold preference model. Finally, we apply BSR to high-quality data and models, which surpasses state-of-the-art RMs in the 8B scale by adding more than 5% in complex preference prediction tasks. By conducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length by 40% while adding a 7% increase in win rate, further highlighting that robustness in RMs induces robustness in RLHF training. We release the code, data, and models: https://github.com/LinkedIn-XFACT/RM-Robustness.","authors":["Jiwoo Hong","Noah Lee","Eunki Kim","Guijin Son","Woojin Chung","Aman Gupta","Shao Tang","James Thorne"],"url":"https://arxiv.org/abs/2505.07271"}
{"created":"2025-05-13","title":"Cache-Efficient Posterior Sampling for Reinforcement Learning with LLM-Derived Priors Across Discrete and Continuous Domains","abstract":"Integrating large language models (LLMs) as priors in reinforcement learning (RL) offers significant advantages but comes with substantial computational costs. We present a principled cache-efficient framework for posterior sampling with LLM-derived priors that dramatically reduces these costs while maintaining high performance. At the core of our approach is an adaptive caching mechanism, where cache parameters are meta-optimized using surrogate gradients derived from policy performance. This design enables efficient inference across both discrete text environments (e.g., TextWorld, ALFWorld) and continuous control domains (e.g., MuJoCo), achieving a 3.8--4.7$\\times$ reduction in LLM queries and 4.0--12.0$\\times$ lower median latencies (85--93\\,ms on a consumer GPU) while retaining 96--98\\% of uncached performance. Our theoretical analysis provides KL divergence bounds on approximation quality, validated empirically. The framework extends to offline RL, where our CQL-Prior variant improves performance by 14--29\\% and reduces training time by 38--40\\%. Extensive evaluations across a diverse suite of eight tasks demonstrate the generalizability and practical viability of LLM-guided RL in resource-constrained settings.","authors":["Ibne Farabi Shihab","Sanjeda Akter","Anuj Sharma"],"url":"https://arxiv.org/abs/2505.07274"}
{"created":"2025-05-13","title":"Coordinated Spatial Reuse Scheduling With Machine Learning in IEEE 802.11 MAPC Networks","abstract":"The densification of Wi-Fi deployments means that fully distributed random channel access is no longer sufficient for high and predictable performance. Therefore, the upcoming IEEE 802.11bn amendment introduces multi-access point coordination (MAPC) methods. This paper addresses a variant of MAPC called coordinated spatial reuse (C-SR), where devices transmit simultaneously on the same channel, with the power adjusted to minimize interference. The C-SR scheduling problem is selecting which devices transmit concurrently and with what settings. We provide a theoretical upper bound model, optimized for either throughput or fairness, which finds the best possible transmission schedule using mixed-integer linear programming. Then, a practical, probing-based approach is proposed which uses multi-armed bandits (MABs), a type of reinforcement learning, to solve the C-SR scheduling problem. We validate both classical (flat) MAB and hierarchical MAB (H-MAB) schemes with simulations and in a testbed. Using H-MABs for C-SR improves aggregate throughput over legacy IEEE 802.11 (on average by 80\\% in random scenarios), without reducing the number of transmission opportunities per station. Finally, our framework is lightweight and ready for implementation in Wi-Fi devices.","authors":["Maksymilian Wojnar","Wojciech Ci\\k{e}\\.zobka","Artur Tomaszewski","Piotr Cho{\\l}da","Krzysztof Rusek","Katarzyna Kosek-Szott","Jetmir Haxhibeqiri","Jeroen Hoebeke","Boris Bellalta","Anatolij Zubow","Falko Dressler","Szymon Szott"],"url":"https://arxiv.org/abs/2505.07278"}
{"created":"2025-05-13","title":"Predicting Music Track Popularity by Convolutional Neural Networks on Spotify Features and Spectrogram of Audio Waveform","abstract":"In the digital streaming landscape, it's becoming increasingly challenging for artists and industry experts to predict the success of music tracks. This study introduces a pioneering methodology that uses Convolutional Neural Networks (CNNs) and Spotify data analysis to forecast the popularity of music tracks. Our approach takes advantage of Spotify's wide range of features, including acoustic attributes based on the spectrogram of audio waveform, metadata, and user engagement metrics, to capture the complex patterns and relationships that influence a track's popularity. Using a large dataset covering various genres and demographics, our CNN-based model shows impressive effectiveness in predicting the popularity of music tracks. Additionally, we've conducted extensive experiments to assess the strength and adaptability of our model across different musical styles and time periods, with promising results yielding a 97\\% F1 score. Our study not only offers valuable insights into the dynamic landscape of digital music consumption but also provides the music industry with advanced predictive tools for assessing and predicting the success of music tracks.","authors":["Navid Falah","Behnam Yousefimehr","Mehdi Ghatee"],"url":"https://arxiv.org/abs/2505.07280"}
{"created":"2025-05-13","title":"A Turing Test for ''Localness'': Conceptualizing, Defining, and Recognizing Localness in People and Machines","abstract":"As digital platforms increasingly mediate interactions tied to place, ensuring genuine local participation is essential for maintaining trust and credibility in location-based services, community-driven platforms, and civic engagement systems. However, localness is a social and relational identity shaped by knowledge, participation, and community recognition. Drawing on the German philosopher Heidegger's concept of dwelling -- which extends beyond physical presence to encompass meaningful connection to place -- we investigate how people conceptualize and evaluate localness in both human and artificial agents. Using a chat-based interaction paradigm inspired by Turing's Imitation Game and Von Ahn's Games With A Purpose, we engaged 230 participants in conversations designed to examine the cues people rely on to assess local presence. Our findings reveal a multi-dimensional framework of localness, highlighting differences in how locals and nonlocals emphasize various aspects of local identity. We show that people are significantly more accurate in recognizing locals than nonlocals, suggesting that localness is an affirmative status requiring active demonstration rather than merely the absence of nonlocal traits. Additionally, we identify conditions under which artificial agents are perceived as local and analyze participants' sensemaking strategies in evaluating localness. Through predictive modeling, we determine key factors that drive accurate localness judgments. By bridging theoretical perspectives on human-place relationships with practical challenges in digital environments, our work informs the design of location-based services that foster meaningful local engagement. Our findings contribute to a broader understanding of localness as a dynamic and relational construct, reinforcing the importance of dwelling as a process of belonging, recognition, and engagement with place.","authors":["Zihan Gao","Justin Cranshaw","Jacob Thebault-Spieker"],"url":"https://arxiv.org/abs/2505.07282"}
{"created":"2025-05-13","title":"Semantic Retention and Extreme Compression in LLMs: Can We Have Both?","abstract":"The exponential growth in Large Language Model (LLM) deployment has intensified the need for efficient model compression techniques to reduce computational and memory costs. While pruning and quantization have shown promise, their combined potential remains largely unexplored. In this paper, we examine joint compression and how strategically combining pruning and quantization could yield superior performance-to-compression ratios compared to single-method approaches. Recognizing the challenges in accurately assessing LLM performance, we address key limitations of previous evaluation frameworks and introduce the Semantic Retention Compression Rate (SrCr), a novel metric that quantifies the trade-off between model compression and semantic preservation, facilitating the optimization of pruning-quantization configurations. Experiments demonstrate that our recommended combination achieves, on average, a 20% performance increase compared to an equivalent quantization-only model at the same theoretical compression rate.","authors":["Stanislas Laborde","Martin Cousseau","Antoun Yaacoub","Lionel Prevost"],"url":"https://arxiv.org/abs/2505.07289"}
{"created":"2025-05-13","title":"Multi-Agent DRL for Multi-Objective Twin Migration Routing with Workload Prediction in 6G-enabled IoV","abstract":"Sixth Generation (6G)-enabled Internet of Vehicles (IoV) facilitates efficient data synchronization through ultra-fast bandwidth and high-density connectivity, enabling the emergence of Vehicle Twins (VTs). As highly accurate replicas of vehicles, VTs can support intelligent vehicular applications for occupants in 6G-enabled IoV. Thanks to the full coverage capability of 6G, resource-constrained vehicles can offload VTs to edge servers, such as roadside units, unmanned aerial vehicles, and satellites, utilizing their computing and storage resources for VT construction and updates. However, communication between vehicles and edge servers with limited coverage is prone to interruptions due to the dynamic mobility of vehicles. Consequently, VTs must be migrated among edge servers to maintain uninterrupted and high-quality services for users. In this paper, we introduce a VT migration framework in 6G-enabled IoV. Specifically, we first propose a Long Short-Term Memory (LSTM)-based Transformer model to accurately predict long-term workloads of edge servers for migration decision-making. Then, we propose a Dynamic Mask Multi-Agent Proximal Policy Optimization (DM-MAPPO) algorithm to identify optimal migration routes in the highly complex environment of 6G-enabled IoV. Finally, we develop a practical platform to validate the effectiveness of the proposed scheme using real datasets. Simulation results demonstrate that the proposed DM-MAPPO algorithm significantly reduces migration latency by 20.82% and packet loss by 75.07% compared with traditional deep reinforcement learning algorithms.","authors":["Peng Yin","Wentao Liang","Jinbo Wen","Jiawen Kang","Junlong Chen","Dusit Niyato"],"url":"https://arxiv.org/abs/2505.07290"}
{"created":"2025-05-13","title":"INTELLECT-2: A Reasoning Model Trained Through Globally Decentralized Reinforcement Learning","abstract":"We introduce INTELLECT-2, the first globally distributed reinforcement learning (RL) training run of a 32 billion parameter language model. Unlike traditional centralized training efforts, INTELLECT-2 trains a reasoning model using fully asynchronous RL across a dynamic, heterogeneous swarm of permissionless compute contributors.","authors":["Prime Intellect Team","Sami Jaghouar","Justus Mattern","Jack Min Ong","Jannik Straube","Manveer Basra","Aaron Pazdera","Kushal Thaman","Matthew Di Ferrante","Felix Gabriel","Fares Obeid","Kemal Erdem","Michael Keiblinger","Johannes Hagemann"],"url":"https://arxiv.org/abs/2505.07291"}
{"created":"2025-05-13","title":"AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection","abstract":"Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.","authors":["Kai Hua","Steven Wu","Ge Zhang","Ke Shen"],"url":"https://arxiv.org/abs/2505.07293"}
{"created":"2025-05-13","title":"HuB: Learning Extreme Humanoid Balance","abstract":"The human body demonstrates exceptional motor capabilities-such as standing steadily on one foot or performing a high kick with the leg raised over 1.5 meters-both requiring precise balance control. While recent research on humanoid control has leveraged reinforcement learning to track human motions for skill acquisition, applying this paradigm to balance-intensive tasks remains challenging. In this work, we identify three key obstacles: instability from reference motion errors, learning difficulties due to morphological mismatch, and the sim-to-real gap caused by sensor noise and unmodeled dynamics. To address these challenges, we propose HuB (Humanoid Balance), a unified framework that integrates reference motion refinement, balance-aware policy learning, and sim-to-real robustness training, with each component targeting a specific challenge. We validate our approach on the Unitree G1 humanoid robot across challenging quasi-static balance tasks, including extreme single-legged poses such as Swallow Balance and Bruce Lee's Kick. Our policy remains stable even under strong physical disturbances-such as a forceful soccer strike-while baseline methods consistently fail to complete these tasks. Project website: https://hub-robot.github.io","authors":["Tong Zhang","Boyuan Zheng","Ruiqian Nai","Yingdong Hu","Yen-Jen Wang","Geng Chen","Fanqi Lin","Jiongye Li","Chuye Hong","Koushil Sreenath","Yang Gao"],"url":"https://arxiv.org/abs/2505.07294"}
{"created":"2025-05-13","title":"Interpretable Event Diagnosis in Water Distribution Networks","abstract":"The increasing penetration of information and communication technologies in the design, monitoring, and control of water systems enables the use of algorithms for detecting and identifying unanticipated events (such as leakages or water contamination) using sensor measurements. However, data-driven methodologies do not always give accurate results and are often not trusted by operators, who may prefer to use their engineering judgment and experience to deal with such events.","authors":["Andr\\'e Artelt","Stelios G. Vrachimis","Demetrios G. Eliades","Ulrike Kuhl","Barbara Hammer","Marios M. Polycarpou"],"url":"https://arxiv.org/abs/2505.07299"}
{"created":"2025-05-13","title":"L-SWAG: Layer-Sample Wise Activation with Gradients information for Zero-Shot NAS on Vision Transformers","abstract":"Training-free Neural Architecture Search (NAS) efficiently identifies high-performing neural networks using zero-cost (ZC) proxies. Unlike multi-shot and one-shot NAS approaches, ZC-NAS is both (i) time-efficient, eliminating the need for model training, and (ii) interpretable, with proxy designs often theoretically grounded. Despite rapid developments in the field, current SOTA ZC proxies are typically constrained to well-established convolutional search spaces. With the rise of Large Language Models shaping the future of deep learning, this work extends ZC proxy applicability to Vision Transformers (ViTs). We present a new benchmark using the Autoformer search space evaluated on 6 distinct tasks and propose Layer-Sample Wise Activation with Gradients information (L-SWAG), a novel, generalizable metric that characterizes both convolutional and transformer architectures across 14 tasks. Additionally, previous works highlighted how different proxies contain complementary information, motivating the need for a ML model to identify useful combinations. To further enhance ZC-NAS, we therefore introduce LIBRA-NAS (Low Information gain and Bias Re-Alignment), a method that strategically combines proxies to best represent a specific benchmark. Integrated into the NAS search, LIBRA-NAS outperforms evolution and gradient-based NAS techniques by identifying an architecture with a 17.0% test error on ImageNet1k in just 0.1 GPU days.","authors":["Sofia Casarin","Sergio Escalera","Oswald Lanz"],"url":"https://arxiv.org/abs/2505.07300"}
{"created":"2025-05-13","title":"Human Motion Prediction via Test-domain-aware Adaptation with Easily-available Human Motions Estimated from Videos","abstract":"In 3D Human Motion Prediction (HMP), conventional methods train HMP models with expensive motion capture data. However, the data collection cost of such motion capture data limits the data diversity, which leads to poor generalizability to unseen motions or subjects. To address this issue, this paper proposes to enhance HMP with additional learning using estimated poses from easily available videos. The 2D poses estimated from the monocular videos are carefully transformed into motion capture-style 3D motions through our pipeline. By additional learning with the obtained motions, the HMP model is adapted to the test domain. The experimental results demonstrate the quantitative and qualitative impact of our method.","authors":["Katsuki Shimbo","Hiromu Taketsugu","Norimichi Ukita"],"url":"https://arxiv.org/abs/2505.07301"}
{"created":"2025-05-13","title":"Online Episodic Convex Reinforcement Learning","abstract":"We study online learning in episodic finite-horizon Markov decision processes (MDPs) with convex objective functions, known as the concave utility reinforcement learning (CURL) problem. This setting generalizes RL from linear to convex losses on the state-action distribution induced by the agent's policy. The non-linearity of CURL invalidates classical Bellman equations and requires new algorithmic approaches. We introduce the first algorithm achieving near-optimal regret bounds for online CURL without any prior knowledge on the transition function. To achieve this, we use an online mirror descent algorithm with varying constraint sets and a carefully designed exploration bonus. We then address for the first time a bandit version of CURL, where the only feedback is the value of the objective function on the state-action distribution induced by the agent's policy. We achieve a sub-linear regret bound for this more challenging problem by adapting techniques from bandit convex optimization to the MDP setting.","authors":["Bianca Marin Moreno (Thoth","EDF R&D","FiME Lab)","Khaled Eldowa (UNIMI","POLIMI)","Pierre Gaillard (Thoth)","Margaux Br\\'eg\\`ere (EDF R&D","LPSM)","Nadia Oudjane (EDF R&D","FiME Lab)"],"url":"https://arxiv.org/abs/2505.07303"}
{"created":"2025-05-13","title":"Bounds for D-Algebraic Closure Properties","abstract":"We provide bounds on the size of polynomial differential equations obtained by executing closure properties for D-algebraic functions. While it is easy to obtain bounds on the order of these equations, it requires some more work to derive bounds on their degree. Here we give bounds that apply under some technical condition about the defining differential equations.","authors":["Manuel Kauers","Raphael Pages"],"url":"https://arxiv.org/abs/2505.07304"}
{"created":"2025-05-13","title":"Enabling Privacy-Aware AI-Based Ergonomic Analysis","abstract":"Musculoskeletal disorders (MSDs) are a leading cause of injury and productivity loss in the manufacturing industry, incurring substantial economic costs. Ergonomic assessments can mitigate these risks by identifying workplace adjustments that improve posture and reduce strain. Camera-based systems offer a non-intrusive, cost-effective method for continuous ergonomic tracking, but they also raise significant privacy concerns. To address this, we propose a privacy-aware ergonomic assessment framework utilizing machine learning techniques. Our approach employs adversarial training to develop a lightweight neural network that obfuscates video data, preserving only the essential information needed for human pose estimation. This obfuscation ensures compatibility with standard pose estimation algorithms, maintaining high accuracy while protecting privacy. The obfuscated video data is transmitted to a central server, where state-of-the-art keypoint detection algorithms extract body landmarks. Using multi-view integration, 3D keypoints are reconstructed and evaluated with the Rapid Entire Body Assessment (REBA) method. Our system provides a secure, effective solution for ergonomic monitoring in industrial environments, addressing both privacy and workplace safety concerns.","authors":["Sander De Coninck","Emilio Gamba","Bart Van Doninck","Abdellatif Bey-Temsamani","Sam Leroux","Pieter Simoens"],"url":"https://arxiv.org/abs/2505.07306"}
{"created":"2025-05-13","title":"Revisiting Sparse Matrix Coloring and Bicoloring","abstract":"Sparse matrix coloring and bicoloring are fundamental building blocks of sparse automatic differentiation. Bicoloring is particularly advantageous for rectangular Jacobian matrices with at least one dense row and column. Indeed, in such cases, unidirectional row or column coloring demands a number of colors equal to the number of rows or columns. We introduce a new strategy for bicoloring that encompasses both direct and substitution-based decompression approaches. Our method reformulates the two variants of bicoloring as star and acyclic colorings of an augmented symmetric matrix. We extend the concept of neutral colors, previously exclusive to bicoloring, to symmetric colorings, and we propose a post-processing routine that neutralizes colors to further reduce the overall color count. We also present the Julia package SparseMatrixColorings, which includes these new bicoloring algorithms alongside all standard coloring methods for sparse derivative matrix computation. Compared to ColPack, the Julia package also offers enhanced implementations for star and acyclic coloring, vertex ordering, as well as decompression.","authors":["Alexis Montoison","Guillaume Dalle","Assefaw Gebremedhin"],"url":"https://arxiv.org/abs/2505.07308"}
{"created":"2025-05-13","title":"Uncertainty Profiles for LLMs: Uncertainty Source Decomposition and Adaptive Model-Metric Selection","abstract":"Large language models (LLMs) often generate fluent but factually incorrect outputs, known as hallucinations, which undermine their reliability in real-world applications. While uncertainty estimation has emerged as a promising strategy for detecting such errors, current metrics offer limited interpretability and lack clarity about the types of uncertainty they capture. In this paper, we present a systematic framework for decomposing LLM uncertainty into four distinct sources, inspired by previous research. We develop a source-specific estimation pipeline to quantify these uncertainty types and evaluate how existing metrics relate to each source across tasks and models. Our results show that metrics, task, and model exhibit systematic variation in uncertainty characteristic. Building on this, we propose a method for task specific metric/model selection guided by the alignment or divergence between their uncertainty characteristics and that of a given task. Our experiments across datasets and models demonstrate that our uncertainty-aware selection strategy consistently outperforms baseline strategies, helping us select appropriate models or uncertainty metrics, and contributing to more reliable and efficient deployment in uncertainty estimation.","authors":["Pei-Fu Guo","Yun-Da Tsai","Shou-De Lin"],"url":"https://arxiv.org/abs/2505.07309"}
{"created":"2025-05-13","title":"Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study","abstract":"Designing effective collaboration structure for multi-agent LLM systems to enhance collective reasoning is crucial yet remains under-explored. In this paper, we systematically investigate how collaborative reasoning performance is affected by three key design dimensions: (1) Expertise-Domain Alignment, (2) Collaboration Paradigm (structured workflow vs. diversity-driven integration), and (3) System Scale. Our findings reveal that expertise alignment benefits are highly domain-contingent, proving most effective for contextual reasoning tasks. Furthermore, collaboration focused on integrating diverse knowledge consistently outperforms rigid task decomposition. Finally, we empirically explore the impact of scaling the multi-agent system with expertise specialization and study the computational trade off, highlighting the need for more efficient communication protocol design. This work provides concrete guidelines for configuring specialized multi-agent system and identifies critical architectural trade-offs and bottlenecks for scalable multi-agent reasoning. The code will be made available upon acceptance.","authors":["Baixuan Xu","Chunyang Li","Weiqi Wang","Wei Fan","Tianshi Zheng","Haochen Shi","Tao Fan","Yangqiu Song","Qiang Yang"],"url":"https://arxiv.org/abs/2505.07313"}
{"created":"2025-05-13","title":"FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes","abstract":"Due to the scarcity of industrial data, individual equipment users, particularly start-ups, struggle to independently train a comprehensive fault diagnosis model; federated learning enables collaborative training while ensuring data privacy, making it an ideal solution. However, the diversity of working conditions leads to variations in fault modes, resulting in inconsistent label spaces across different clients. In federated diagnostic scenarios, label space inconsistency leads to local models focus on client-specific fault modes and causes local models from different clients to map different failure modes to similar feature representations, which weakens the aggregated global model's generalization. To tackle this issue, this article proposed a federated cross-domain diagnostic framework termed Federated Invariant Features Learning (FedIFL). In intra-client training, prototype contrastive learning mitigates intra-client domain shifts, subsequently, feature generating ensures local models can access distributions of other clients in a privacy-friendly manner. Besides, in cross-client training, a feature disentanglement mechanism is introduced to mitigate cross-client domain shifts, specifically, an instance-level federated instance consistency loss is designed to ensure the instance-level consistency of invariant features between different clients, furthermore, a federated instance personalization loss and an orthogonal loss are constructed to distinguish specific features that from the invariant features. Eventually, the aggregated model achieves promising generalization among global label spaces, enabling accurate fault diagnosis for target clients' Motor Driven Systems (MDSs) with inconsistent label spaces. Experiments on real-world MDSs validate the effectiveness and superiority of FedIFL in federated cross-domain diagnosis with inconsistent fault modes.","authors":["Zexiao Wang","Yankai Wang","Xiaoqiang Liao","Xinguo Ming","Weiming Shen"],"url":"https://arxiv.org/abs/2505.07315"}
{"created":"2025-05-13","title":"How Do Companies Manage the Environmental Sustainability of AI? An Interview Study About Green AI Efforts and Regulations","abstract":"With the ever-growing adoption of artificial intelligence (AI), AI-based software and its negative impact on the environment are no longer negligible, and studying and mitigating this impact has become a critical area of research. However, it is currently unclear which role environmental sustainability plays during AI adoption in industry and how AI regulations influence Green AI practices and decision-making in industry. We therefore aim to investigate the Green AI perception and management of industry practitioners. To this end, we conducted a total of 11 interviews with participants from 10 different organizations that adopted AI-based software. The interviews explored three main themes: AI adoption, current efforts in mitigating the negative environmental impact of AI, and the influence of the EU AI Act and the Corporate Sustainability Reporting Directive (CSRD). Our findings indicate that 9 of 11 participants prioritized business efficiency during AI adoption, with minimal consideration of environmental sustainability. Monitoring and mitigation of AI's environmental impact were very limited. Only one participant monitored negative environmental effects. Regarding applied mitigation practices, six participants reported no actions, with the others sporadically mentioning techniques like prompt engineering, relying on smaller models, or not overusing AI. Awareness and compliance with the EU AI Act are low, with only one participant reporting on its influence, while the CSRD drove sustainability reporting efforts primarily in larger companies. All in all, our findings reflect a lack of urgency and priority for sustainable AI among these companies. We suggest that current regulations are not very effective, which has implications for policymakers. Additionally, there is a need to raise industry awareness, but also to provide user-friendly techniques and tools for Green AI practices.","authors":["Ashmita Sampatsing","Sophie Vos","Emma Beauxis-Aussalet","Justus Bogner"],"url":"https://arxiv.org/abs/2505.07317"}
{"created":"2025-05-13","title":"Autonomous Robotic Pruning in Orchards and Vineyards: a Review","abstract":"Manual pruning is labor intensive and represents up to 25% of annual labor costs in fruit production, notably in apple orchards and vineyards where operational challenges and cost constraints limit the adoption of large-scale machinery. In response, a growing body of research is investigating compact, flexible robotic platforms capable of precise pruning in varied terrains, particularly where traditional mechanization falls short.","authors":["Alessandro Navone","Mauro Martini","Marcello Chiaberge"],"url":"https://arxiv.org/abs/2505.07318"}
{"created":"2025-05-13","title":"Dynamical Label Augmentation and Calibration for Noisy Electronic Health Records","abstract":"Medical research, particularly in predicting patient outcomes, heavily relies on medical time series data extracted from Electronic Health Records (EHR), which provide extensive information on patient histories. Despite rigorous examination, labeling errors are inevitable and can significantly impede accurate predictions of patient outcome. To address this challenge, we propose an \\textbf{A}ttention-based Learning Framework with Dynamic \\textbf{C}alibration and Augmentation for \\textbf{T}ime series Noisy \\textbf{L}abel \\textbf{L}earning (ACTLL). This framework leverages a two-component Beta mixture model to identify the certain and uncertain sets of instances based on the fitness distribution of each class, and it captures global temporal dynamics while dynamically calibrating labels from the uncertain set or augmenting confident instances from the certain set. Experimental results on large-scale EHR datasets eICU and MIMIC-IV-ED, and several benchmark datasets from the UCR and UEA repositories, demonstrate that our model ACTLL has achieved state-of-the-art performance, especially under high noise levels.","authors":["Yuhao Li","Ling Luo","Uwe Aickelin"],"url":"https://arxiv.org/abs/2505.07320"}
{"created":"2025-05-13","title":"Drive Fast, Learn Faster: On-Board RL for High Performance Autonomous Racing","abstract":"Autonomous racing presents unique challenges due to its non-linear dynamics, the high speed involved, and the critical need for real-time decision-making under dynamic and unpredictable conditions. Most traditional Reinforcement Learning (RL) approaches rely on extensive simulation-based pre-training, which faces crucial challenges in transfer effectively to real-world environments. This paper introduces a robust on-board RL framework for autonomous racing, designed to eliminate the dependency on simulation-based pre-training enabling direct real-world adaptation. The proposed system introduces a refined Soft Actor-Critic (SAC) algorithm, leveraging a residual RL structure to enhance classical controllers in real-time by integrating multi-step Temporal-Difference (TD) learning, an asynchronous training pipeline, and Heuristic Delayed Reward Adjustment (HDRA) to improve sample efficiency and training stability. The framework is validated through extensive experiments on the F1TENTH racing platform, where the residual RL controller consistently outperforms the baseline controllers and achieves up to an 11.5 % reduction in lap times compared to the State-of-the-Art (SotA) with only 20 min of training. Additionally, an End-to-End (E2E) RL controller trained without a baseline controller surpasses the previous best results with sustained on-track learning. These findings position the framework as a robust solution for high-performance autonomous racing and a promising direction for other real-time, dynamic autonomous systems.","authors":["Benedict Hildisch","Edoardo Ghignone","Nicolas Baumann","Cheng Hu","Andrea Carron","Michele Magno"],"url":"https://arxiv.org/abs/2505.07321"}
{"created":"2025-05-13","title":"RealRep: Generalized SDR-to-HDR Conversion with Style Disentangled Representation Learning","abstract":"High-Dynamic-Range Wide-Color-Gamut (HDR-WCG) technology is becoming increasingly prevalent, intensifying the demand for converting Standard Dynamic Range (SDR) content to HDR. Existing methods primarily rely on fixed tone mapping operators, which are inadequate for handling SDR inputs with diverse styles commonly found in real-world scenarios. To address this challenge, we propose a generalized SDR-to-HDR method that handles diverse styles in real-world SDR content, termed Realistic Style Disentangled Representation Learning (RealRep). By disentangling luminance and chrominance, we analyze the intrinsic differences between contents with varying styles and propose a disentangled multi-view style representation learning method. This approach captures the guidance prior of true luminance and chrominance distributions across different styles, even when the SDR style distributions exhibit significant variations, thereby establishing a robust embedding space for inverse tone mapping. Motivated by the difficulty of directly utilizing degradation representation priors, we further introduce the Degradation-Domain Aware Controlled Mapping Network (DDACMNet), a two-stage framework that performs adaptive hierarchical mapping guided by a control-aware normalization mechanism. DDACMNet dynamically modulates the mapping process via degradation-conditioned hierarchical features, enabling robust adaptation across diverse degradation domains. Extensive experiments show that RealRep consistently outperforms state-of-the-art methods with superior generalization and perceptually faithful HDR color gamut reconstruction.","authors":["Gang He","Siqi Wang","Kepeng Xu","Lin Zhang"],"url":"https://arxiv.org/abs/2505.07322"}
{"created":"2025-05-13","title":"User Identification with LFI-Based Eye Movement Data Using Time and Frequency Domain Features","abstract":"Laser interferometry (LFI)-based eye-tracking systems provide an alternative to traditional camera-based solutions, offering improved privacy by eliminating the risk of direct visual identification. However, the high-frequency signals captured by LFI-based trackers may still contain biometric information that enables user identification. This study investigates user identification from raw high-frequency LFI-based eye movement data by analyzing features extracted from both the time and frequency domains. Using velocity and distance measurements without requiring direct gaze data, we develop a multi-class classification model to accurately distinguish between individuals across various activities. Our results demonstrate that even without direct visual cues, eye movement patterns exhibit sufficient uniqueness for user identification, achieving 93.14% accuracy and a 2.52% EER with 5-second windows across both static and dynamic tasks. Additionally, we analyze the impact of sampling rate and window size on model performance, providing insights into the feasibility of LFI-based biometric recognition. Our findings demonstrate the novel potential of LFI-based eye-tracking for user identification, highlighting both its promise for secure authentication and emerging privacy risks. This work paves the way for further research into high-frequency eye movement data.","authors":["Suleyman Ozdel","Johannes Meyer","Yasmeen Abdrabou","Enkelejda Kasneci"],"url":"https://arxiv.org/abs/2505.07326"}
{"created":"2025-05-13","title":"Assessing the Latency of Network Layer Security in 5G Networks","abstract":"In contrast to its predecessors, 5G supports a wide range of commercial, industrial, and critical infrastructure scenarios. One key feature of 5G, ultra-reliable low latency communication, is particularly appealing to such scenarios for its real-time capabilities. However, 5G's enhanced security, mostly realized through optional security controls, imposes additional overhead on the network performance, potentially hindering its real-time capabilities. To better assess this impact and guide operators in choosing between different options, we measure the latency overhead of IPsec when applied over the N3 and the service-based interfaces to protect user and control plane data, respectively. Furthermore, we evaluate whether WireGuard constitutes an alternative to reduce this overhead. Our findings show that IPsec, if configured correctly, has minimal latency impact and thus is a prime candidate to secure real-time critical scenarios.","authors":["Sotiris Michaelides","Jonathan Mucke","Martin Henze"],"url":"https://arxiv.org/abs/2505.07328"}
{"created":"2025-05-13","title":"Private LoRA Fine-tuning of Open-Source LLMs with Homomorphic Encryption","abstract":"Preserving data confidentiality during the fine-tuning of open-source Large Language Models (LLMs) is crucial for sensitive applications. This work introduces an interactive protocol adapting the Low-Rank Adaptation (LoRA) technique for private fine-tuning. Homomorphic Encryption (HE) protects the confidentiality of training data and gradients handled by remote worker nodes performing the bulk of computations involving the base model weights. The data owner orchestrates training, requiring minimal local computing power and memory, thus alleviating the need for expensive client-side GPUs. We demonstrate feasibility by fine-tuning a Llama-3.2-1B model, presenting convergence results using HE-compatible quantization and performance benchmarks for HE computations on GPU hardware. This approach enables applications such as confidential knowledge base question answering, private codebase fine-tuning for AI code assistants, AI agents for drafting emails based on a company's email archive, and adapting models to analyze sensitive legal or healthcare documents.","authors":["Jordan Frery","Roman Bredehoft","Jakub Klemsa","Arthur Meyre","Andrei Stoian"],"url":"https://arxiv.org/abs/2505.07329"}
{"created":"2025-05-13","title":"Link to the Past: Temporal Propagation for Fast 3D Human Reconstruction from Monocular Video","abstract":"Fast 3D clothed human reconstruction from monocular video remains a significant challenge in computer vision, particularly in balancing computational efficiency with reconstruction quality. Current approaches are either focused on static image reconstruction but too computationally intensive, or achieve high quality through per-video optimization that requires minutes to hours of processing, making them unsuitable for real-time applications. To this end, we present TemPoFast3D, a novel method that leverages temporal coherency of human appearance to reduce redundant computation while maintaining reconstruction quality. Our approach is a \"plug-and play\" solution that uniquely transforms pixel-aligned reconstruction networks to handle continuous video streams by maintaining and refining a canonical appearance representation through efficient coordinate mapping. Extensive experiments demonstrate that TemPoFast3D matches or exceeds state-of-the-art methods across standard metrics while providing high-quality textured reconstruction across diverse pose and appearance, with a maximum speed of 12 FPS.","authors":["Matthew Marchellus","Nadhira Noor","In Kyu Park"],"url":"https://arxiv.org/abs/2505.07333"}
{"created":"2025-05-13","title":"SAEN-BGS: Energy-Efficient Spiking AutoEncoder Network for Background Subtraction","abstract":"Background subtraction (BGS) is utilized to detect moving objects in a video and is commonly employed at the onset of object tracking and human recognition processes. Nevertheless, existing BGS techniques utilizing deep learning still encounter challenges with various background noises in videos, including variations in lighting, shifts in camera angles, and disturbances like air turbulence or swaying trees. To address this problem, we design a spiking autoencoder network, termed SAEN-BGS, based on noise resilience and time-sequence sensitivity of spiking neural networks (SNNs) to enhance the separation of foreground and background. To eliminate unnecessary background noise and preserve the important foreground elements, we begin by creating the continuous spiking conv-and-dconv block, which serves as the fundamental building block for the decoder in SAEN-BGS. Moreover, in striving for enhanced energy efficiency, we introduce a novel self-distillation spiking supervised learning method grounded in ANN-to-SNN frameworks, resulting in decreased power consumption. In extensive experiments conducted on CDnet-2014 and DAVIS-2016 datasets, our approach demonstrates superior segmentation performance relative to other baseline methods, even when challenged by complex scenarios with dynamic backgrounds.","authors":["Zhixuan Zhang","Xiaopeng Li","Qi Liu"],"url":"https://arxiv.org/abs/2505.07336"}
{"created":"2025-05-13","title":"Laypeople's Attitudes Towards Fair, Affirmative, and Discriminatory Decision-Making Algorithms","abstract":"Affirmative algorithms have emerged as a potential answer to algorithmic discrimination, seeking to redress past harms and rectify the source of historical injustices. We present the results of two experiments ($N$$=$$1193$) capturing laypeople's perceptions of affirmative algorithms -- those which explicitly prioritize the historically marginalized -- in hiring and criminal justice. We contrast these opinions about affirmative algorithms with folk attitudes towards algorithms that prioritize the privileged (i.e., discriminatory) and systems that make decisions independently of demographic groups (i.e., fair). We find that people -- regardless of their political leaning and identity -- view fair algorithms favorably and denounce discriminatory systems. In contrast, we identify disagreements concerning affirmative algorithms: liberals and racial minorities rate affirmative systems as positively as their fair counterparts, whereas conservatives and those from the dominant racial group evaluate affirmative algorithms as negatively as discriminatory systems. We identify a source of these divisions: people have varying beliefs about who (if anyone) is marginalized, shaping their views of affirmative algorithms. We discuss the possibility of bridging these disagreements to bring people together towards affirmative algorithms.","authors":["Gabriel Lima","Nina Grgi\\'c-Hla\\v{c}a","Markus Langer","Yixin Zou"],"url":"https://arxiv.org/abs/2505.07339"}
{"created":"2025-05-13","title":"Thalamus: A User Simulation Toolkit for Prototyping Multimodal Sensing Studies","abstract":"Conducting user studies that involve physiological and behavioral measurements is very time-consuming and expensive, as it not only involves a careful experiment design, device calibration, etc. but also a careful software testing. We propose Thalamus, a software toolkit for collecting and simulating multimodal signals that can help the experimenters to prepare in advance for unexpected situations before reaching out to the actual study participants and even before having to install or purchase a specific device. Among other features, Thalamus allows the experimenter to modify, synchronize, and broadcast physiological signals (as coming from various data streams) from different devices simultaneously and not necessarily located in the same place. Thalamus is cross-platform, cross-device, and simple to use, making it thus a valuable asset for HCI research.","authors":["Kayhan Latifzadeh","Luis A. Leiva"],"url":"https://arxiv.org/abs/2505.07340"}
{"created":"2025-05-13","title":"Generative Pre-trained Autoregressive Diffusion Transformer","abstract":"In this work, we present GPDiT, a Generative Pre-trained Autoregressive Diffusion Transformer that unifies the strengths of diffusion and autoregressive modeling for long-range video synthesis, within a continuous latent space. Instead of predicting discrete tokens, GPDiT autoregressively predicts future latent frames using a diffusion loss, enabling natural modeling of motion dynamics and semantic consistency across frames. This continuous autoregressive framework not only enhances generation quality but also endows the model with representation capabilities. Additionally, we introduce a lightweight causal attention variant and a parameter-free rotation-based time-conditioning mechanism, improving both the training and inference efficiency. Extensive experiments demonstrate that GPDiT achieves strong performance in video generation quality, video representation ability, and few-shot learning tasks, highlighting its potential as an effective framework for video modeling in continuous space.","authors":["Yuan Zhang","Jiacheng Jiang","Guoqing Ma","Zhiying Lu","Haoyang Huang","Jianlong Yuan","Nan Duan"],"url":"https://arxiv.org/abs/2505.07344"}
{"created":"2025-05-13","title":"QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines","abstract":"Large language models (LLMs) have been widely used for relevance assessment in information retrieval. However, our study demonstrates that combining two distinct small language models (SLMs) with different architectures can outperform LLMs in this task. Our approach -- QUPID -- integrates a generative SLM with an embedding-based SLM, achieving higher relevance judgment accuracy while reducing computational costs compared to state-of-the-art LLM solutions. This computational efficiency makes QUPID highly scalable for real-world search systems processing millions of queries daily. In experiments across diverse document types, our method demonstrated consistent performance improvements (Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x faster inference times. Furthermore, when integrated into production search pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how architectural diversity in model combinations can significantly enhance both search relevance and operational efficiency in information retrieval systems.","authors":["Ohjoon Kwon","Changsu Lee","Jihye Back","Lim Sun Suk","Inho Kang","Donghyeon Jeon"],"url":"https://arxiv.org/abs/2505.07345"}
{"created":"2025-05-13","title":"AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography","abstract":"Echocardiographers can detect pulmonary hypertension using Doppler echocardiography; however, accurately assessing its progression often proves challenging. Right heart catheterization (RHC), the gold standard for precise evaluation, is invasive and unsuitable for routine use, limiting its practicality for timely diagnosis and monitoring of pulmonary hypertension progression. Here, we propose MePH, a multi-view, multi-modal vision-language model to accurately assess pulmonary hypertension progression using non-invasive echocardiography. We constructed a large dataset comprising paired standardized echocardiogram videos, spectral images and RHC data, covering 1,237 patient cases from 12 medical centers. For the first time, MePH precisely models the correlation between non-invasive multi-view, multi-modal echocardiography and the pressure and resistance obtained via RHC. We show that MePH significantly outperforms echocardiographers' assessments using echocardiography, reducing the mean absolute error in estimating mean pulmonary arterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and 43.81%, respectively. In eight independent external hospitals, MePH achieved a mean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an area under the curve of 0.921, surpassing echocardiographers (area under the curve of 0.842) in accurately predicting the severity of pulmonary hypertension, whether mild or severe. A prospective study demonstrated that MePH can predict treatment efficacy for patients. Our work provides pulmonary hypertension patients with a non-invasive and timely method for monitoring disease progression, improving the accuracy and efficiency of pulmonary hypertension management while enabling earlier interventions and more personalized treatment decisions.","authors":["Jiewen Yang","Taoran Huang","Shangwei Ding","Xiaowei Xu","Qinhua Zhao","Yong Jiang","Jiarong Guo","Bin Pu","Jiexuan Zheng","Caojin Zhang","Hongwen Fei","Xiaomeng Li"],"url":"https://arxiv.org/abs/2505.07347"}
{"created":"2025-05-13","title":"Stiffness-based Analytic Centre Method for Cable-Driven Parallel Robots","abstract":"Nowadays, being fast and precise are key requirements in Robotics. This work introduces a novel methodology to tune the stiffness of Cable-Driven Parallel Robots (CDPRs) while simultaneously addressing the tension distribution problem. In particular, the approach relies on the Analytic-Centre method. Indeed, weighting the barrier functions makes natural the stiffness adaptation. The intrinsic ability to adjust the stiffness during the execution of the task enables the CDPRs to effectively meet above-mentioned requirements. The capabilities of the method are demonstrated through simulations by comparing it with the existing approach.","authors":["Domenico Dona'","Vincenzo Di Paola","Matteo Zoppi","Alberto Trevisani"],"url":"https://arxiv.org/abs/2505.07348"}
{"created":"2025-05-13","title":"From Search To Sampling: Generative Models For Robust Algorithmic Recourse","abstract":"Algorithmic Recourse provides recommendations to individuals who are adversely impacted by automated model decisions, on how to alter their profiles to achieve a favorable outcome. Effective recourse methods must balance three conflicting goals: proximity to the original profile to minimize cost, plausibility for realistic recourse, and validity to ensure the desired outcome. We show that existing methods train for these objectives separately and then search for recourse through a joint optimization over the recourse goals during inference, leading to poor recourse recommendations. We introduce GenRe, a generative recourse model designed to train the three recourse objectives jointly. Training such generative models is non-trivial due to lack of direct recourse supervision. We propose efficient ways to synthesize such supervision and further show that GenRe's training leads to a consistent estimator. Unlike most prior methods, that employ non-robust gradient descent based search during inference, GenRe simply performs a forward sampling over the generative model to produce minimum cost recourse, leading to superior performance across multiple metrics. We also demonstrate GenRe provides the best trade-off between cost, plausibility and validity, compared to state-of-art baselines. Our code is available at: https://github.com/prateekgargx/genre.","authors":["Prateek Garg","Lokesh Nagalapatti","Sunita Sarawagi"],"url":"https://arxiv.org/abs/2505.07351"}
{"created":"2025-05-13","title":"Time Perception in Virtual Reality: Effects of Emotional Valence and Stress Level","abstract":"Background & Objective: Emotional states and stress distort time perception, yet findings are inconsistent, particularly in immersive media. Integrating the Attentional Gate Model (AGM) and Internal Clock Model (ICM), we examined how emotional valence and stress alter perceived duration in Virtual Reality (VR). This study assesses the effects of valence (calming, neutral, stressful) and stress (low/high) on prospective time estimation, mood, and arousal. Methods: Fifty-four adults (18-39 years) explored three custom VR environments: (1) a tranquil Japanese garden, (2) an affectively neutral room, and (3) a threatening underground sewer. Active navigation promoted presence; a distraction task separated conditions. Valence and arousal were assessed with the Visual Analog Mood Scales, stress with the Perceived Stress Scale-10 (PSS-10), and perceived duration with a verbal estimation task. Mixed-model ANOVAs evaluated main and interaction effects. Results: Valence reliably shaped perceived duration: calming VR led to overestimation, stressful VR to underestimation, and neutral VR to intermediate timing. Baseline stress level, as measured by PSS-10, neither altered timing nor interacted with valence. Nevertheless, the VR environments affected VAMS' mood metrics: calming environments elevated mood and reduced perceived stress, whereas stressful environments lowered mood and heightened stress. Conclusions: Findings support the AGM-attentionally demanding negative environments shorten perceived time-and the ICM-valence-linked arousal speeds or slows the pacemaker. Contrary to classical predictions, in VR, baseline stress did not distort duration, suggesting valence-driven attentional allocation outweighs pre-exposure stress levels. VR offers a controllable platform for dissecting time-perception mechanisms and advancing interventions that target emotion-related temporal distortions.","authors":["Kyriaki Syrigou","Marina Stoforou","Panagiotis Kourtesis"],"url":"https://arxiv.org/abs/2505.07354"}
{"created":"2025-05-13","title":"BinMetric: A Comprehensive Binary Analysis Benchmark for Large Language Models","abstract":"Binary analysis remains pivotal in software security, offering insights into compiled programs without source code access. As large language models (LLMs) continue to excel in diverse language understanding and generation tasks, their potential in decoding complex binary data structures becomes evident. However, the lack of standardized benchmarks in this domain limits the assessment and comparison of LLM's capabilities in binary analysis and hinders the progress of research and practical applications. To bridge this gap, we introduce BinMetric, a comprehensive benchmark designed specifically to evaluate the performance of large language models on binary analysis tasks. BinMetric comprises 1,000 questions derived from 20 real-world open-source projects across 6 practical binary analysis tasks, including decompilation, code summarization, assembly instruction generation, etc., which reflect actual reverse engineering scenarios. Our empirical study on this benchmark investigates the binary analysis capabilities of various state-of-the-art LLMs, revealing their strengths and limitations in this field. The findings indicate that while LLMs show strong potential, challenges still exist, particularly in the areas of precise binary lifting and assembly synthesis. In summary, BinMetric makes a significant step forward in measuring the binary analysis capabilities of LLMs, establishing a new benchmark leaderboard, and our study provides valuable insights for the future development of these LLMs in software security.","authors":["Xiuwei Shang","Guoqiang Chen","Shaoyin Cheng","Benlong Wu","Li Hu","Gangyang Li","Weiming Zhang","Nenghai Yu"],"url":"https://arxiv.org/abs/2505.07360"}
{"created":"2025-05-13","title":"High Performance Signal Design for ACO-OFDM Systems using Variational Autoencoder","abstract":"This letter proposes a design of low peak-to-average power ratio (PAPR), low symbol error rate (SER), and high data rate signal for asymmetrically clipped optical orthogonal frequency division multiplexing (ACO-OFDM) systems. The proposed design leverages a variational autoencoder (VAE) incorporating gradual loss learning to jointly optimize the geometry and probability of the constellation's symbols. This not only enhances mutual information (MI) but also effectively reduces the PAPR while maintaining a low SER for reliable transmission. We evaluate the performance of the proposed VAE-based design by comparing the MI, SER, and PAPR against existing techniques. Simulation results demonstrate that the proposed method achieves a considerably lower PAPR while maintaining superior SER and MI performance for a wide range of SNRs.","authors":["Nam N. Luong","Chuyen T. Nguyen","Thanh V. Pham"],"url":"https://arxiv.org/abs/2505.07362"}
{"created":"2025-05-13","title":"Multi-Domain Audio Question Answering Toward Acoustic Content Reasoning in The DCASE 2025 Challenge","abstract":"We present Task 5 of the DCASE 2025 Challenge: an Audio Question Answering (AQA) benchmark spanning multiple domains of sound understanding. This task defines three QA subsets (Bioacoustics, Temporal Soundscapes, and Complex QA) to test audio-language models on interactive question-answering over diverse acoustic scenes. We describe the dataset composition (from marine mammal calls to soundscapes and complex real-world clips), the evaluation protocol (top-1 accuracy with answer-shuffling robustness), and baseline systems (Qwen2-Audio-7B, AudioFlamingo 2, Gemini-2-Flash). Preliminary results on the development set are compared, showing strong variation across models and subsets. This challenge aims to advance the audio understanding and reasoning capabilities of audio-language models toward human-level acuity, which are crucial for enabling AI agents to perceive and interact about the world effectively.","authors":["Chao-Han Huck Yang","Sreyan Ghosh","Qing Wang","Jaeyeon Kim","Hengyi Hong","Sonal Kumar","Guirui Zhong","Zhifeng Kong","S Sakshi","Vaibhavi Lokegaonkar","Oriol Nieto","Ramani Duraiswami","Dinesh Manocha","Gunhee Kim","Jun Du","Rafael Valle","Bryan Catanzaro"],"url":"https://arxiv.org/abs/2505.07365"}
{"created":"2025-05-13","title":"Generalization Bounds and Stopping Rules for Learning with Self-Selected Data","abstract":"Many learning paradigms self-select training data in light of previously learned parameters. Examples include active learning, semi-supervised learning, bandits, or boosting. Rodemann et al. (2024) unify them under the framework of \"reciprocal learning\". In this article, we address the question of how well these methods can generalize from their self-selected samples. In particular, we prove universal generalization bounds for reciprocal learning using covering numbers and Wasserstein ambiguity sets. Our results require no assumptions on the distribution of self-selected data, only verifiable conditions on the algorithms. We prove results for both convergent and finite iteration solutions. The latter are anytime valid, thereby giving rise to stopping rules for a practitioner seeking to guarantee the out-of-sample performance of their reciprocal learning algorithm. Finally, we illustrate our bounds and stopping rules for reciprocal learning's special case of semi-supervised learning.","authors":["Julian Rodemann","James Bailie"],"url":"https://arxiv.org/abs/2505.07367"}
{"created":"2025-05-13","title":"Synthetic Code Surgery: Repairing Bugs and Vulnerabilities with LLMs and Synthetic Data","abstract":"This paper presents a novel methodology for enhancing Automated Program Repair (APR) through synthetic data generation utilizing Large Language Models (LLMs). Current APR systems are constrained by the limited availability of high-quality training data encompassing diverse bug types across multiple programming languages. The proposed approach addresses this limitation through a two-phase process: a synthetic sample generation followed by a rigorous quality assessment. Multiple state-of-the-art LLMs were employed to generate approximately 30,000 paired examples of buggy and fixed code across 12 programming languages and 13 bug categories. Subsequently, these samples underwent cross-model evaluation against five criteria: correctness, code quality, security, performance, and completeness. Experimental evaluation on the VulRepair test set dataset showed statistically significant improvements in Perfect Prediction rates, with the quality-filtered synthetic dataset outperforming both baseline and real-world commit data configurations in certain scenarios. The methodology was validated through rigorous statistical testing, including ANOVA and post-hoc Tukey's Honest Significant Difference analysis. Furthermore, the best-performing configurations surpassed existing systems despite using a less computationally intensive decoding strategy. This research establishes a self-bootstrapping paradigm in which LLMs generate and evaluate their own training data, potentially transforming approaches to data scarcity across software engineering tasks and advancing the development of robust, adaptable tools for automated code maintenance.","authors":["David de-Fitero-Dominguez","Antonio Garcia-Cabot","Eva Garcia-Lopez"],"url":"https://arxiv.org/abs/2505.07372"}
{"created":"2025-05-13","title":"Geometric Prior-Guided Neural Implicit Surface Reconstruction in the Wild","abstract":"Neural implicit surface reconstruction using volume rendering techniques has recently achieved significant advancements in creating high-fidelity surfaces from multiple 2D images. However, current methods primarily target scenes with consistent illumination and struggle to accurately reconstruct 3D geometry in uncontrolled environments with transient occlusions or varying appearances. While some neural radiance field (NeRF)-based variants can better manage photometric variations and transient objects in complex scenes, they are designed for novel view synthesis rather than precise surface reconstruction due to limited surface constraints. To overcome this limitation, we introduce a novel approach that applies multiple geometric constraints to the implicit surface optimization process, enabling more accurate reconstructions from unconstrained image collections. First, we utilize sparse 3D points from structure-from-motion (SfM) to refine the signed distance function estimation for the reconstructed surface, with a displacement compensation to accommodate noise in the sparse points. Additionally, we employ robust normal priors derived from a normal predictor, enhanced by edge prior filtering and multi-view consistency constraints, to improve alignment with the actual surface geometry. Extensive testing on the Heritage-Recon benchmark and other datasets has shown that the proposed method can accurately reconstruct surfaces from in-the-wild images, yielding geometries with superior accuracy and granularity compared to existing techniques. Our approach enables high-quality 3D reconstruction of various landmarks, making it applicable to diverse scenarios such as digital preservation of cultural heritage sites.","authors":["Lintao Xiang","Hongpei Zheng","Bailin Deng","Hujun Yin"],"url":"https://arxiv.org/abs/2505.07373"}
{"created":"2025-05-13","title":"AIS Data-Driven Maritime Monitoring Based on Transformer: A Comprehensive Review","abstract":"With the increasing demands for safety, efficiency, and sustainability in global shipping, Automatic Identification System (AIS) data plays an increasingly important role in maritime monitoring. AIS data contains spatial-temporal variation patterns of vessels that hold significant research value in the marine domain. However, due to its massive scale, the full potential of AIS data has long remained untapped. With its powerful sequence modeling capabilities, particularly its ability to capture long-range dependencies and complex temporal dynamics, the Transformer model has emerged as an effective tool for processing AIS data. Therefore, this paper reviews the research on Transformer-based AIS data-driven maritime monitoring, providing a comprehensive overview of the current applications of Transformer models in the marine field. The focus is on Transformer-based trajectory prediction methods, behavior detection, and prediction techniques. Additionally, this paper collects and organizes publicly available AIS datasets from the reviewed papers, performing data filtering, cleaning, and statistical analysis. The statistical results reveal the operational characteristics of different vessel types, providing data support for further research on maritime monitoring tasks. Finally, we offer valuable suggestions for future research, identifying two promising research directions. Datasets are available at https://github.com/eyesofworld/Maritime-Monitoring.","authors":["Zhiye Xie","Enmei Tu","Xianping Fu","Guoliang Yuan","Yi Han"],"url":"https://arxiv.org/abs/2505.07374"}
{"created":"2025-05-13","title":"Boosting Global-Local Feature Matching via Anomaly Synthesis for Multi-Class Point Cloud Anomaly Detection","abstract":"Point cloud anomaly detection is essential for various industrial applications. The huge computation and storage costs caused by the increasing product classes limit the application of single-class unsupervised methods, necessitating the development of multi-class unsupervised methods. However, the feature similarity between normal and anomalous points from different class data leads to the feature confusion problem, which greatly hinders the performance of multi-class methods. Therefore, we introduce a multi-class point cloud anomaly detection method, named GLFM, leveraging global-local feature matching to progressively separate data that are prone to confusion across multiple classes. Specifically, GLFM is structured into three stages: Stage-I proposes an anomaly synthesis pipeline that stretches point clouds to create abundant anomaly data that are utilized to adapt the point cloud feature extractor for better feature representation. Stage-II establishes the global and local memory banks according to the global and local feature distributions of all the training data, weakening the impact of feature confusion on the establishment of the memory bank. Stage-III implements anomaly detection of test data leveraging its feature distance from global and local memory banks. Extensive experiments on the MVTec 3D-AD, Real3D-AD and actual industry parts dataset showcase our proposed GLFM's superior point cloud anomaly detection performance. The code is available at https://github.com/hustCYQ/GLFM-Multi-class-3DAD.","authors":["Yuqi Cheng","Yunkang Cao","Dongfang Wang","Weiming Shen","Wenlong Li"],"url":"https://arxiv.org/abs/2505.07375"}
{"created":"2025-05-13","title":"A Preliminary Study of Large Language Models for Multilingual Vulnerability Detection","abstract":"Deep learning-based approaches, particularly those leveraging pre-trained language models (PLMs), have shown promise in automated software vulnerability detection. However, existing methods are predominantly limited to specific programming languages, restricting their applicability in multilingual settings. Recent advancements in large language models (LLMs) offer language-agnostic capabilities and enhanced semantic understanding, presenting a potential solution to this limitation. While existing studies have explored LLMs for vulnerability detection, their detection performance remains unknown for multilingual vulnerabilities. To address this gap, we conducted a preliminary study to evaluate the effectiveness of PLMs and state-of-the-art LLMs across seven popular programming languages. Our findings reveal that the PLM CodeT5P achieves the best performance in multilingual vulnerability detection, particularly in identifying the most critical vulnerabilities. Based on these results, we further discuss the potential of LLMs in advancing real-world multilingual vulnerability detection. This work represents an initial step toward exploring PLMs and LLMs for cross-language vulnerability detection, offering key insights for future research and practical deployment.","authors":["Junji Yu","Honglin Shu","Michael Fu","Dong Wang","Chakkrit Tantithamthavorn","Yasutaka Kamei","Junjie Chen"],"url":"https://arxiv.org/abs/2505.07376"}
{"created":"2025-05-13","title":"Examining the Role of LLM-Driven Interactions on Attention and Cognitive Engagement in Virtual Classrooms","abstract":"Transforming educational technologies through the integration of large language models (LLMs) and virtual reality (VR) offers the potential for immersive and interactive learning experiences. However, the effects of LLMs on user engagement and attention in educational environments remain open questions. In this study, we utilized a fully LLM-driven virtual learning environment, where peers and teachers were LLM-driven, to examine how students behaved in such settings. Specifically, we investigate how peer question-asking behaviors influenced student engagement, attention, cognitive load, and learning outcomes and found that, in conditions where LLM-driven peer learners asked questions, students exhibited more targeted visual scanpaths, with their attention directed toward the learning content, particularly in complex subjects. Our results suggest that peer questions did not introduce extraneous cognitive load directly, as the cognitive load is strongly correlated with increased attention to the learning material. Considering these findings, we provide design recommendations for optimizing VR learning spaces.","authors":["Suleyman Ozdel","Can Sarpkaya","Efe Bozkir","Hong Gao","Enkelejda Kasneci"],"url":"https://arxiv.org/abs/2505.07377"}
{"created":"2025-05-13","title":"Apple's Synthetic Defocus Noise Pattern: Characterization and Forensic Applications","abstract":"iPhone portrait-mode images contain a distinctive pattern in out-of-focus regions simulating the bokeh effect, which we term Apple's Synthetic Defocus Noise Pattern (SDNP). If overlooked, this pattern can interfere with blind forensic analyses, especially PRNU-based camera source verification, as noted in earlier works. Since Apple's SDNP remains underexplored, we provide a detailed characterization, proposing a method for its precise estimation, modeling its dependence on scene brightness, ISO settings, and other factors. Leveraging this characterization, we explore forensic applications of the SDNP, including traceability of portrait-mode images across iPhone models and iOS versions in open-set scenarios, assessing its robustness under post-processing. Furthermore, we show that masking SDNP-affected regions in PRNU-based camera source verification significantly reduces false positives, overcoming a critical limitation in camera attribution, and improving state-of-the-art techniques.","authors":["David V\\'azquez-Pad\\'in","Fernando P\\'erez-Gonz\\'alez","Pablo P\\'erez-Migu\\'elez"],"url":"https://arxiv.org/abs/2505.07380"}
{"created":"2025-05-13","title":"Few-shot Semantic Encoding and Decoding for Video Surveillance","abstract":"With the continuous increase in the number and resolution of video surveillance cameras, the burden of transmitting and storing surveillance video is growing. Traditional communication methods based on Shannon's theory are facing optimization bottlenecks. Semantic communication, as an emerging communication method, is expected to break through this bottleneck and reduce the storage and transmission consumption of video. Existing semantic decoding methods often require many samples to train the neural network for each scene, which is time-consuming and labor-intensive. In this study, a semantic encoding and decoding method for surveillance video is proposed. First, the sketch was extracted as semantic information, and a sketch compression method was proposed to reduce the bit rate of semantic information. Then, an image translation network was proposed to translate the sketch into a video frame with a reference frame. Finally, a few-shot sketch decoding network was proposed to reconstruct video from sketch. Experimental results showed that the proposed method achieved significantly better video reconstruction performance than baseline methods. The sketch compression method could effectively reduce the storage and transmission consumption of semantic information with little compromise on video quality. The proposed method provides a novel semantic encoding and decoding method that only needs a few training samples for each surveillance scene, thus improving the practicality of the semantic communication system.","authors":["Baoping Cheng","Yukun Zhang","Liming Wang","Xiaoyan Xie","Tao Fu","Dongkun Wang","Xiaoming Tao"],"url":"https://arxiv.org/abs/2505.07381"}
{"created":"2025-05-13","title":"Feature Visualization in 3D Convolutional Neural Networks","abstract":"Understanding the computations of convolutional neural networks requires effective visualization of their kernels. While maximal activation methods have proven successful in highlighting the preferred features of 2D convolutional kernels, directly applying these techniques to 3D convolutions often leads to uninterpretable results due to the higher dimensionality and complexity of 3D features. To address this challenge, we propose a novel visualization approach for 3D convolutional kernels that disentangles their texture and motion preferences. Our method begins with a data-driven decomposition of the optimal input that maximally activates a given kernel. We then introduce a two-stage optimization strategy to extract distinct texture and motion components from this input. Applying our approach to visualize kernels at various depths of several pre-trained models, we find that the resulting visualizations--particularly those capturing motion--clearly reveal the preferred dynamic patterns encoded by 3D kernels. These results demonstrate the effectiveness of our method in providing interpretable insights into 3D convolutional operations. Code is available at https://github.com/YatangLiLab/3DKernelVisualizer.","authors":["Chunpeng Li","Ya-tang Li"],"url":"https://arxiv.org/abs/2505.07387"}
{"created":"2025-05-13","title":"AI in Money Matters","abstract":"In November 2022, Europe and the world by and large were stunned by the birth of a new large language model : ChatGPT. Ever since then, both academic and populist discussions have taken place in various public spheres such as LinkedIn and X(formerly known as Twitter) with the view to both understand the tool and its benefits for the society. The views of real actors in professional spaces, especially in regulated industries such as finance and law have been largely missing. We aim to begin to close this gap by presenting results from an empirical investigation conducted through interviews with professional actors in the Fintech industry. The paper asks the question, how and to what extent are large language models in general and ChatGPT in particular being adopted and used in the Fintech industry? The results show that while the fintech experts we spoke with see a potential in using large language models in the future, a lot of questions marks remain concerning how they are policed and therefore might be adopted in a regulated industry such as Fintech. This paper aims to add to the existing academic discussing around large language models, with a contribution to our understanding of professional viewpoints.","authors":["Nadine Sandjo Tchatchoua (Roskilde University)","Richard Harper (Lancaster University)"],"url":"https://arxiv.org/abs/2505.07393"}
{"created":"2025-05-13","title":"ReinboT: Amplifying Robot Visual-Language Manipulation with Reinforcement Learning","abstract":"Vision-Language-Action (VLA) models have shown great potential in general robotic decision-making tasks via imitation learning. However, the variable quality of training data often constrains the performance of these models. On the other hand, offline Reinforcement Learning (RL) excels at learning robust policy models from mixed-quality data. In this paper, we introduce Reinforced robot GPT (ReinboT), a novel end-to-end VLA model that integrates the RL principle of maximizing cumulative reward. ReinboT achieves a deeper understanding of the data quality distribution by predicting dense returns that capture the nuances of manipulation tasks. The dense return prediction capability enables the robot to generate more robust decision-making actions, oriented towards maximizing future benefits. Extensive experiments show that ReinboT achieves state-of-the-art performance on the CALVIN mixed-quality dataset and exhibits superior few-shot learning and out-of-distribution generalization capabilities in real-world tasks.","authors":["Hongyin Zhang","Zifeng Zhuang","Han Zhao","Pengxiang Ding","Hongchao Lu","Donglin Wang"],"url":"https://arxiv.org/abs/2505.07395"}
{"created":"2025-05-13","title":"TUM2TWIN: Introducing the Large-Scale Multimodal Urban Digital Twin Benchmark Dataset","abstract":"Urban Digital Twins (UDTs) have become essential for managing cities and integrating complex, heterogeneous data from diverse sources. Creating UDTs involves challenges at multiple process stages, including acquiring accurate 3D source data, reconstructing high-fidelity 3D models, maintaining models' updates, and ensuring seamless interoperability to downstream tasks. Current datasets are usually limited to one part of the processing chain, hampering comprehensive UDTs validation. To address these challenges, we introduce the first comprehensive multimodal Urban Digital Twin benchmark dataset: TUM2TWIN. This dataset includes georeferenced, semantically aligned 3D models and networks along with various terrestrial, mobile, aerial, and satellite observations boasting 32 data subsets over roughly 100,000 $m^2$ and currently 767 GB of data. By ensuring georeferenced indoor-outdoor acquisition, high accuracy, and multimodal data integration, the benchmark supports robust analysis of sensors and the development of advanced reconstruction methods. Additionally, we explore downstream tasks demonstrating the potential of TUM2TWIN, including novel view synthesis of NeRF and Gaussian Splatting, solar potential analysis, point cloud semantic segmentation, and LoD3 building reconstruction. We are convinced this contribution lays a foundation for overcoming current limitations in UDT creation, fostering new research directions and practical solutions for smarter, data-driven urban environments. The project is available under: https://tum2t.win","authors":["Olaf Wysocki","Benedikt Schwab","Manoj Kumar Biswanath","Qilin Zhang","Jingwei Zhu","Thomas Froech","Medhini Heeramaglore","Ihab Hijazi","Khaoula Kanna","Mathias Pechinger","Zhaiyu Chen","Yao Sun","Alejandro Rueda Segura","Ziyang Xu","Omar AbdelGafar","Mansour Mehranfar","Chandan Yeshwanth","Yueh-Cheng Liu","Hadi Yazdi","Jiapan Wang","Stefan Auer","Katharina Anders","Klaus Bogenberger","Andre Borrmann","Angela Dai","Ludwig Hoegner","Christoph Holst","Thomas H. Kolbe","Ferdinand Ludwig","Matthias Nie{\\ss}ner","Frank Petzold","Xiao Xiang Zhu","Boris Jutzi"],"url":"https://arxiv.org/abs/2505.07396"}
{"created":"2025-05-13","title":"DepthFusion: Depth-Aware Hybrid Feature Fusion for LiDAR-Camera 3D Object Detection","abstract":"State-of-the-art LiDAR-camera 3D object detectors usually focus on feature fusion. However, they neglect the factor of depth while designing the fusion strategy. In this work, we are the first to observe that different modalities play different roles as depth varies via statistical analysis and visualization. Based on this finding, we propose a Depth-Aware Hybrid Feature Fusion (DepthFusion) strategy that guides the weights of point cloud and RGB image modalities by introducing depth encoding at both global and local levels. Specifically, the Depth-GFusion module adaptively adjusts the weights of image Bird's-Eye-View (BEV) features in multi-modal global features via depth encoding. Furthermore, to compensate for the information lost when transferring raw features to the BEV space, we propose a Depth-LFusion module, which adaptively adjusts the weights of original voxel features and multi-view image features in multi-modal local features via depth encoding. Extensive experiments on the nuScenes and KITTI datasets demonstrate that our DepthFusion method surpasses previous state-of-the-art methods. Moreover, our DepthFusion is more robust to various kinds of corruptions, outperforming previous methods on the nuScenes-C dataset.","authors":["Mingqian Ji","Jian Yang","Shanshan Zhang"],"url":"https://arxiv.org/abs/2505.07398"}
{"created":"2025-05-13","title":"Towards Autonomous 1/8th Offroad RC Racing -- The TruggySense Educational Platform","abstract":"This paper presents a state-of-the-art Data Acquisition System designed for off-road conditions, deployed on a Team Corally Kagama 1/8 Remote Controlled Vehicle. The system is intended to support Advanced Driver Assistance Systems in an educational context by providing valuable, consistent, and representative data. Key measurement systems are discussed to enable insights into the Remote Controlled Vehicles stability during and after off-road races. Furthermore, four experiments where conducted to evaluate the Data Acquisition Systems accuracy, stability, and consistency in replicating real-world vehicle behavior. The proposed Data Acquisition System platform serves as a solid foundation for use in engineering education, enabling integration with various Advanced Driver Assistance Systems algorithms to enhance vehicle control and overall performance, offering a new dimension to off-road racing. Additionally, realtime telemetry enables verification and validation of Advanced Driver Assistance Systems algorithms based on the live operating state of the Radio Controlled Vehicle during races","authors":["Robbe Elsermans","Jan Steckel"],"url":"https://arxiv.org/abs/2505.07399"}
{"created":"2025-05-13","title":"Energy personas in Danish households","abstract":"Technologies to monitor the provision of renewable energy are part of emerging technologies to help address the discrepancy between renewable energy production and its related usage in households. This paper presents various ways householders use a technological artifact for the real-time monitoring of renewable energy provision. Such a monitoring thus affords householders with an opportunity to adjust their energy consumption according to renewable energy provision. In Denmark, Ewii, previously Barry, is a Danish energy supplier which provides householders with an opportunity to monitor energy sources in real time through a technological solution of the same name. This paper use provision afforded by Ewii as a case for exploring how householders organize themselves to use a technological artefact that supports the monitoring of energy and its related usage. This study aims to inform technology design through the derivation of four personas. The derived personas highlight the differences in energy monitoring practices for the householders and their engagement. These personas are characterised as dedicated, organised, sporadic, and convenient. Understanding these differences in energy monitoring practice using the technological artefact form a solid element in the design of future energy technologies that interfere with the everyday practices and energy consumption for households. This is paramount for future energy related technology design, and for the clarification of usage assumptions that are embedded in the rollout of energy related technology as a country such as Denmark moves through its green transition.","authors":["Nadine Sandjo Tchatchoua (Roskilde University)","Line Valdorff Madsen (Aalborg University)","Anders Rhiger Hansen (Aalborg University)"],"url":"https://arxiv.org/abs/2505.07408"}
{"created":"2025-05-13","title":"Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles","abstract":"Democratic societies need reliable information. Misinformation in popular media such as news articles or videos threatens to impair civic discourse. Citizens are, unfortunately, not equipped to verify this content flood consumed daily at increasing rates. This work aims to semi-automatically quantify scientific accuracy of online media. By semantifying media of unknown veracity, their statements can be compared against equally processed trusted sources. We implemented a workflow using LLM-based statement extraction and knowledge graph analysis. Our neurosymbolic system was able to evidently streamline state-of-the-art veracity quantification. Evaluated via expert interviews and a user survey, the tool provides a beneficial veracity indication. This indicator, however, is unable to annotate public media at the required granularity and scale. Further work towards a FAIR (Findable, Accessible, Interoperable, Reusable) ground truth and complementary metrics are required to scientifically support civic discourse.","authors":["Tim Wittenborg","Constantin Sebastian Tremel","Markus Stocker","S\\\"oren Auer"],"url":"https://arxiv.org/abs/2505.07409"}
{"created":"2025-05-13","title":"ICE-Pruning: An Iterative Cost-Efficient Pruning Pipeline for Deep Neural Networks","abstract":"Pruning is a widely used method for compressing Deep Neural Networks (DNNs), where less relevant parameters are removed from a DNN model to reduce its size. However, removing parameters reduces model accuracy, so pruning is typically combined with fine-tuning, and sometimes other operations such as rewinding weights, to recover accuracy. A common approach is to repeatedly prune and then fine-tune, with increasing amounts of model parameters being removed in each step. While straightforward to implement, pruning pipelines that follow this approach are computationally expensive due to the need for repeated fine-tuning.","authors":["Wenhao Hu","Paul Henderson","Jos\\'e Cano"],"url":"https://arxiv.org/abs/2505.07411"}
{"created":"2025-05-13","title":"Learning Penalty for Optimal Partitioning via Automatic Feature Extraction","abstract":"Changepoint detection identifies significant shifts in data sequences, making it important in areas like finance, genetics, and healthcare. The Optimal Partitioning algorithms efficiently detect these changes, using a penalty parameter to limit the changepoints number. Determining the appropriate value for this penalty can be challenging. Traditionally, this process involved manually extracting statistical features, such as sequence length or variance to make the prediction. This study proposes a novel approach that uses recurrent neural networks to learn this penalty directly from raw sequences by automatically extracting features. Experiments conducted on 20 benchmark genomic datasets show that this novel method surpasses traditional methods in partitioning accuracy in most cases.","authors":["Tung L Nguyen","Toby Hocking"],"url":"https://arxiv.org/abs/2505.07413"}
{"created":"2025-05-13","title":"ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation","abstract":"Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP","authors":["Truc Mai-Thanh Nguyen","Dat Minh Nguyen","Son T. Luu","Kiet Van Nguyen"],"url":"https://arxiv.org/abs/2505.07416"}
{"created":"2025-05-13","title":"LA-IMR: Latency-Aware, Predictive In-Memory Routing and Proactive Autoscaling for Tail-Latency-Sensitive Cloud Robotics","abstract":"Hybrid cloud-edge infrastructures now support latency-critical workloads ranging from autonomous vehicles and surgical robotics to immersive AR/VR. However, they continue to experience crippling long-tail latency spikes whenever bursty request streams exceed the capacity of heterogeneous edge and cloud tiers. To address these long-tail latency issues, we present Latency-Aware, Predictive In-Memory Routing and Proactive Autoscaling (LA-IMR). This control layer integrates a closed-form, utilization-driven latency model with event-driven scheduling, replica autoscaling, and edge-to-cloud offloading to mitigate 99th-percentile (P99) delays. Our analytic model decomposes end-to-end latency into processing, network, and queuing components, expressing inference latency as an affine power-law function of instance utilization. Once calibrated, it produces two complementary functions that drive: (i) millisecond-scale routing decisions for traffic offloading, and (ii) capacity planning that jointly determines replica pool sizes. LA-IMR enacts these decisions through a quality-differentiated, multi-queue scheduler and a custom-metric Kubernetes autoscaler that scales replicas proactively -- before queues build up -- rather than reactively based on lagging CPU metrics. Across representative vision workloads (YOLOv5m and EfficientDet) and bursty arrival traces, LA-IMR reduces P99 latency by up to 20.7 percent compared to traditional latency-only autoscaling, laying a principled foundation for next-generation, tail-tolerant cloud-edge inference services.","authors":["Eunil Seo","Chanh Nguyen","Erik Elmroth"],"url":"https://arxiv.org/abs/2505.07417"}
{"created":"2025-05-13","title":"A Systematic Literature Review on Neural Code Translation","abstract":"Code translation aims to convert code from one programming language to another automatically. It is motivated by the need for multi-language software development and legacy system migration. In recent years, neural code translation has gained significant attention, driven by rapid advancements in deep learning and large language models. Researchers have proposed various techniques to improve neural code translation quality. However, to the best of our knowledge, no comprehensive systematic literature review has been conducted to summarize the key techniques and challenges in this field. To fill this research gap, we collected 57 primary studies covering the period 2020~2025 on neural code translation. These studies are analyzed from seven key perspectives: task characteristics, data preprocessing, code modeling, model construction, post-processing, evaluation subjects, and evaluation metrics. Our analysis reveals current research trends, identifies unresolved challenges, and shows potential directions for future work. These findings can provide valuable insights for both researchers and practitioners in the field of neural code translation.","authors":["Xiang Chen","Jiacheng Xue","Xiaofei Xie","Caokai Liang","Xiaolin Ju"],"url":"https://arxiv.org/abs/2505.07425"}
{"created":"2025-05-13","title":"Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights","abstract":"The emergence of global health crises, such as COVID-19 and Monkeypox (mpox), has underscored the importance of understanding public sentiment to inform effective public health strategies. This study conducts a comparative sentiment analysis of public perceptions surrounding COVID-19 and mpox by leveraging extensive datasets of 147,475 and 106,638 tweets, respectively. Advanced machine learning models, including Logistic Regression, Naive Bayes, RoBERTa, DistilRoBERTa and XLNet, were applied to perform sentiment classification, with results indicating key trends in public emotion and discourse. The analysis highlights significant differences in public sentiment driven by disease characteristics, media representation, and pandemic fatigue. Through the lens of sentiment polarity and thematic trends, this study offers valuable insights into tailoring public health messaging, mitigating misinformation, and fostering trust during concurrent health crises. The findings contribute to advancing sentiment analysis applications in public health informatics, setting the groundwork for enhanced real-time monitoring and multilingual analysis in future research.","authors":["Mostafa Mohaimen Akand Faisal","Rabeya Amin Jhuma"],"url":"https://arxiv.org/abs/2505.07430"}
{"created":"2025-05-13","title":"Diffusion-driven SpatioTemporal Graph KANsformer for Medical Examination Recommendation","abstract":"Recommendation systems in AI-based medical diagnostics and treatment constitute a critical component of AI in healthcare. Although some studies have explored this area and made notable progress, healthcare recommendation systems remain in their nascent stage. And these researches mainly target the treatment process such as drug or disease recommendations. In addition to the treatment process, the diagnostic process, particularly determining which medical examinations are necessary to evaluate the condition, also urgently requires intelligent decision support. To bridge this gap, we first formalize the task of medical examination recommendations. Compared to traditional recommendations, the medical examination recommendation involves more complex interactions. This complexity arises from two folds: 1) The historical medical records for examination recommendations are heterogeneous and redundant, which makes the recommendation results susceptible to noise. 2) The correlation between the medical history of patients is often irregular, making it challenging to model spatiotemporal dependencies. Motivated by the above observation, we propose a novel Diffusion-driven SpatioTemporal Graph KANsformer for Medical Examination Recommendation (DST-GKAN) with a two-stage learning paradigm to solve the above challenges. In the first stage, we exploit a task-adaptive diffusion model to distill recommendation-oriented information by reducing the noises in heterogeneous medical data. In the second stage, a spatiotemporal graph KANsformer is proposed to simultaneously model the complex spatial and temporal relationships. Moreover, to facilitate the medical examination recommendation research, we introduce a comprehensive dataset. The experimental results demonstrate the state-of-the-art performance of the proposed method compared to various competitive baselines.","authors":["Jianan Li","Yangtao Zhou","Zhifu Zhao","Qinglan Huang","Jian Qi","Xiao He","Hua Chu","Fu Li"],"url":"https://arxiv.org/abs/2505.07431"}
{"created":"2025-05-13","title":"LEAD: Iterative Data Selection for Efficient LLM Instruction Tuning","abstract":"Instruction tuning has emerged as a critical paradigm for improving the capabilities and alignment of large language models (LLMs). However, existing iterative model-aware data selection methods incur significant computational overhead, as they rely on repeatedly performing full-dataset model inference to estimate sample utility for subsequent training iterations, creating a fundamental efficiency bottleneck. In this paper, we propose LEAD, an efficient iterative data selection framework that accurately estimates sample utility entirely within the standard training loop, eliminating the need for costly additional model inference. At its core, LEAD introduces Instance-Level Dynamic Uncertainty (IDU), a theoretically grounded utility function combining instantaneous training loss, gradient-based approximation of loss changes, and exponential smoothing of historical loss signals. To further scale efficiently to large datasets, LEAD employs a two-stage, coarse-to-fine selection strategy, adaptively prioritizing informative clusters through a multi-armed bandit mechanism, followed by precise fine-grained selection of high-utility samples using IDU. Extensive experiments across four diverse benchmarks show that LEAD significantly outperforms state-of-the-art methods, improving average model performance by 6.1%-10.8% while using only 2.5% of the training data and reducing overall training time by 5-10x.","authors":["Xiaotian Lin","Yanlin Qi","Yizhang Zhu","Themis Palpanas","Chengliang Chai","Nan Tang","Yuyu Luo"],"url":"https://arxiv.org/abs/2505.07437"}
{"created":"2025-05-13","title":"Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge","abstract":"Commonsense knowledge bases (KB) are a source of specialized knowledge that is widely used to improve machine learning applications. However, even for a large KB such as ConceptNet, capturing explicit knowledge from each industry domain is challenging. For example, only a few samples of general {\\em tasks} performed by various industries are available in ConceptNet. Here, a task is a well-defined knowledge-based volitional action to achieve a particular goal. In this paper, we aim to fill this gap and present a weakly-supervised framework to augment commonsense KB with tasks carried out by various industry groups (IG). We attempt to {\\em match} each task with one or more suitable IGs by training a neural model to learn task-IG affinity and apply clustering to select the top-k tasks per IG. We extract a total of 2339 triples of the form $\\langle IG, is~capable~of, task \\rangle$ from two publicly available news datasets for 24 IGs with the precision of 0.86. This validates the reliability of the extracted task-IG pairs that can be directly added to existing KBs.","authors":["Rituraj Singh","Sachin Pawar","Girish Palshikar"],"url":"https://arxiv.org/abs/2505.07440"}
{"created":"2025-05-13","title":"Cooperative Assembly with Autonomous Mobile Manipulators in an Underwater Scenario","abstract":"[...] Specifically, the problem addressed is an assembly one known as the peg-in-hole task. In this case, two autonomous manipulators must carry cooperatively (at kinematic level) a peg and must insert it into an hole fixed in the environment. Even if the peg-in-hole is a well-known problem, there are no specific studies related to the use of two different autonomous manipulators, especially in underwater scenarios. Among all the possible investigations towards the problem, this work focuses mainly on the kinematic control of the robots. The methods used are part of the Task Priority Inverse Kinematics (TPIK) approach, with a cooperation scheme that permits to exchange as less information as possible between the agents (that is really important being water a big impediment for communication). A force-torque sensor is exploited at kinematic level to help the insertion phase. The results show how the TPIK and the chosen cooperation scheme can be used for the stated problem. The simulated experiments done consider little errors in the hole's pose, that still permit to insert the peg but with a lot of frictions and possible stucks. It is shown how can be possible to improve (thanks to the data provided by the force-torque sensor) the insertion phase performed by the two manipulators in presence of these errors. [...]","authors":["Davide Torielli"],"url":"https://arxiv.org/abs/2505.07441"}
{"created":"2025-05-13","title":"Cluster Vertex Deletion Problems on Cubic Graphs","abstract":"The problems Cluster Vertex Deletion (or Cluster-VD) and its generalization s-Club Cluster Vertex Deletion (or s-Club-VD, for any integer s>= 1), have been introduced with the aim of detecting highly-connected parts in complex systems. Their NP-completeness has been established for several classes of graphs, but remains open for smaller classes, including subcubic planar bipartite graphs and cubic graphs. In this paper, we show that Cluster-VD and more generally s-Club-VD are NP-complete for cubic planar bipartite graphs. We also deduce new results for the related k-Path Vertex Cover problem (or k-PVC), namely 3-PVC is NP-complete for cubic planar bipartite graphs, whereas k-PVC with k>= 4 is NP-complete for subcubic planar (and bipartite, when k is odd) graphs of arbitrarily large girth.","authors":["Irena Rusu"],"url":"https://arxiv.org/abs/2505.07443"}
{"created":"2025-05-13","title":"Lightweight Multispectral Crop-Weed Segmentation for Precision Agriculture","abstract":"Efficient crop-weed segmentation is critical for site-specific weed control in precision agriculture. Conventional CNN-based methods struggle to generalize and rely on RGB imagery, limiting performance under complex field conditions. To address these challenges, we propose a lightweight transformer-CNN hybrid. It processes RGB, Near-Infrared (NIR), and Red-Edge (RE) bands using specialized encoders and dynamic modality integration. Evaluated on the WeedsGalore dataset, the model achieves a segmentation accuracy (mean IoU) of 78.88%, outperforming RGB-only models by 15.8 percentage points. With only 8.7 million parameters, the model offers high accuracy, computational efficiency, and potential for real-time deployment on Unmanned Aerial Vehicles (UAVs) and edge devices, advancing precision weed management.","authors":["Zeynep Galymzhankyzy","Eric Martinson"],"url":"https://arxiv.org/abs/2505.07444"}
{"created":"2025-05-13","title":"TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for Benchmarking Target Person Tracking","abstract":"Tracking a target person from robot-egocentric views is crucial for developing autonomous robots that provide continuous personalized assistance or collaboration in Human-Robot Interaction (HRI) and Embodied AI. However, most existing target person tracking (TPT) benchmarks are limited to controlled laboratory environments with few distractions, clean backgrounds, and short-term occlusions. In this paper, we introduce a large-scale dataset designed for TPT in crowded and unstructured environments, demonstrated through a robot-person following task. The dataset is collected by a human pushing a sensor-equipped cart while following a target person, capturing human-like following behavior and emphasizing long-term tracking challenges, including frequent occlusions and the need for re-identification from numerous pedestrians. It includes multi-modal data streams, including odometry, 3D LiDAR, IMU, panoptic, and RGB-D images, along with exhaustively annotated 2D bounding boxes of the target person across 35 sequences, both indoors and outdoors. Using this dataset and visual annotations, we perform extensive experiments with existing TPT methods, offering a thorough analysis of their limitations and suggesting future research directions.","authors":["Hanjing Ye","Yu Zhan","Weixi Situ","Guangcheng Chen","Jingwen Yu","Kuanqi Cai","Hong Zhang"],"url":"https://arxiv.org/abs/2505.07446"}
{"created":"2025-05-13","title":"Unified Continuous Generative Models","abstract":"Recent advances in continuous generative models, including multi-step approaches like diffusion and flow-matching (typically requiring 8-1000 sampling steps) and few-step methods such as consistency models (typically 1-8 steps), have demonstrated impressive generative performance. However, existing work often treats these approaches as distinct paradigms, resulting in separate training and sampling methodologies. We introduce a unified framework for training, sampling, and analyzing these models. Our implementation, the Unified Continuous Generative Models Trainer and Sampler (UCGM-{T,S}), achieves state-of-the-art (SOTA) performance. For example, on ImageNet 256x256 using a 675M diffusion transformer, UCGM-T trains a multi-step model achieving 1.30 FID in 20 steps and a few-step model reaching 1.42 FID in just 2 steps. Additionally, applying UCGM-S to a pre-trained model (previously 1.26 FID at 250 steps) improves performance to 1.06 FID in only 40 steps. Code is available at: https://github.com/LINs-lab/UCGM.","authors":["Peng Sun","Yi Jiang","Tao Lin"],"url":"https://arxiv.org/abs/2505.07447"}
{"created":"2025-05-13","title":"Prototype Augmented Hypernetworks for Continual Learning","abstract":"Continual learning (CL) aims to learn a sequence of tasks without forgetting prior knowledge, but gradient updates for a new task often overwrite the weights learned earlier, causing catastrophic forgetting (CF). We propose Prototype-Augmented Hypernetworks (PAH), a framework where a single hypernetwork, conditioned on learnable task prototypes, dynamically generates task-specific classifier heads on demand. To mitigate forgetting, PAH combines cross-entropy with dual distillation losses, one to align logits and another to align prototypes, ensuring stable feature representations across tasks. Evaluations on Split-CIFAR100 and TinyImageNet demonstrate that PAH achieves state-of-the-art performance, reaching 74.5 % and 63.7 % accuracy with only 1.7 % and 4.4 % forgetting, respectively, surpassing prior methods without storing samples or heads.","authors":["Neil De La Fuente","Maria Pilligua","Daniel Vidal","Albin Soutiff","Cecilia Curreli","Daniel Cremers","Andrey Barsky"],"url":"https://arxiv.org/abs/2505.07450"}
{"created":"2025-05-13","title":"Optimal Low Emission Zones scheduling as an example of transport policy backcasting","abstract":"This study presents a backcasting approach that considers the passenger car fleet dynamics to determine optimal policy roadmaps in transport systems. As opposed to the scenario-based approach, backcasting sets emission reduction targets first, then identifies policies that meet the constraint. The policy is the implementation of Low Emission Zones (LEZs), in the Ile-de-France region as a case study. The aim is to minimize the number of scrapped vehicles due to LEZs under CO2 emission targets and to deduce an interdiction schedule of polluting vehicles by 2050. To explore potential solutions, we use a genetic algorithm that provides a first insight into optimal policy pathways.","authors":["Asmae Alami","Vinith Lakshmanan","Antonio Sciarretta"],"url":"https://arxiv.org/abs/2505.07451"}
{"created":"2025-05-13","title":"SwarmSearch: Decentralized Search Engine with Self-Funding Economy","abstract":"Centralized search engines control what we see, read, believe, and vote. Consequently, they raise concerns over information control, censorship, and bias. Decentralized search engines offer a remedy to this problem, but their adoption has been hindered by their inferior quality and lack of a self-sustaining economic framework. We present SwarmSearch, a fully decentralized, AI-powered search engine with a self-funding architecture. Our system is designed for deployment within the decentralized file-sharing software Tribler. SwarmSearch integrates volunteer-based with profit-driven mechanisms to foster an implicit marketplace for resources. Employing the state-of-the-art of AI-based retrieval and relevance ranking, we also aim to close the quality gap between decentralized search and centralized alternatives. Our system demonstrates high retrieval accuracy while showing robustness in the presence of 50% adversarial nodes.","authors":["Marcel Gregoriadis","Rowdy Chotkan","Petru Neague","Johan Pouwelse"],"url":"https://arxiv.org/abs/2505.07452"}
{"created":"2025-05-13","title":"How well do LLMs reason over tabular data, really?","abstract":"Large Language Models (LLMs) excel in natural language tasks, but less is known about their reasoning capabilities over tabular data. Prior analyses devise evaluation strategies that poorly reflect an LLM's realistic performance on tabular queries. Moreover, we have a limited understanding of the robustness of LLMs towards realistic variations in tabular inputs. Therefore, we ask: Can general-purpose LLMs reason over tabular data, really?, and focus on two questions 1) are tabular reasoning capabilities of general-purpose LLMs robust to real-world characteristics of tabular inputs, and 2) how can we realistically evaluate an LLM's performance on analytical tabular queries? Building on a recent tabular reasoning benchmark, we first surface shortcomings of its multiple-choice prompt evaluation strategy, as well as commonly used free-form text metrics such as SacreBleu and BERT-score. We show that an LLM-as-a-judge procedure yields more reliable performance insights and unveil a significant deficit in tabular reasoning performance of LLMs. We then extend the tabular inputs reflecting three common characteristics in practice: 1) missing values, 2) duplicate entities, and 3) structural variations. Experiments show that the tabular reasoning capabilities of general-purpose LLMs suffer from these variations, stressing the importance of improving their robustness for realistic tabular inputs.","authors":["Cornelius Wolff","Madelon Hulsebos"],"url":"https://arxiv.org/abs/2505.07453"}
{"created":"2025-05-13","title":"GelFusion: Enhancing Robotic Manipulation under Visual Constraints via Visuotactile Fusion","abstract":"Visuotactile sensing offers rich contact information that can help mitigate performance bottlenecks in imitation learning, particularly under vision-limited conditions, such as ambiguous visual cues or occlusions. Effectively fusing visual and visuotactile modalities, however, presents ongoing challenges. We introduce GelFusion, a framework designed to enhance policies by integrating visuotactile feedback, specifically from high-resolution GelSight sensors. GelFusion using a vision-dominated cross-attention fusion mechanism incorporates visuotactile information into policy learning. To better provide rich contact information, the framework's core component is our dual-channel visuotactile feature representation, simultaneously leveraging both texture-geometric and dynamic interaction features. We evaluated GelFusion on three contact-rich tasks: surface wiping, peg insertion, and fragile object pick-and-place. Outperforming baselines, GelFusion shows the value of its structure in improving the success rate of policy learning.","authors":["Shulong Jiang","Shiqi Zhao","Yuxuan Fan","Peng Yin"],"url":"https://arxiv.org/abs/2505.07455"}
{"created":"2025-05-13","title":"Why Uncertainty Estimation Methods Fall Short in RAG: An Axiomatic Analysis","abstract":"Large Language Models (LLMs) are valued for their strong performance across various tasks, but they also produce inaccurate or misleading outputs. Uncertainty Estimation (UE) quantifies the model's confidence and helps users assess response reliability. However, existing UE methods have not been thoroughly examined in scenarios like Retrieval-Augmented Generation (RAG), where the input prompt includes non-parametric knowledge. This paper shows that current UE methods cannot reliably assess correctness in the RAG setting. We further propose an axiomatic framework to identify deficiencies in existing methods and guide the development of improved approaches. Our framework introduces five constraints that an effective UE method should meet after incorporating retrieved documents into the LLM's prompt. Experimental results reveal that no existing UE method fully satisfies all the axioms, explaining their suboptimal performance in RAG. We further introduce a simple yet effective calibration function based on our framework, which not only satisfies more axioms than baseline methods but also improves the correlation between uncertainty estimates and correctness.","authors":["Heydar Soudani","Evangelos Kanoulas","Faegheh Hasibi"],"url":"https://arxiv.org/abs/2505.07459"}
{"created":"2025-05-13","title":"A Survey on Collaborative Mechanisms Between Large and Small Language Models","abstract":"Large Language Models (LLMs) deliver powerful AI capabilities but face deployment challenges due to high resource costs and latency, whereas Small Language Models (SLMs) offer efficiency and deployability at the cost of reduced performance. Collaboration between LLMs and SLMs emerges as a crucial paradigm to synergistically balance these trade-offs, enabling advanced AI applications, especially on resource-constrained edge devices. This survey provides a comprehensive overview of LLM-SLM collaboration, detailing various interaction mechanisms (pipeline, routing, auxiliary, distillation, fusion), key enabling technologies, and diverse application scenarios driven by on-device needs like low latency, privacy, personalization, and offline operation. While highlighting the significant potential for creating more efficient, adaptable, and accessible AI, we also discuss persistent challenges including system overhead, inter-model consistency, robust task allocation, evaluation complexity, and security/privacy concerns. Future directions point towards more intelligent adaptive frameworks, deeper model fusion, and expansion into multimodal and embodied AI, positioning LLM-SLM collaboration as a key driver for the next generation of practical and ubiquitous artificial intelligence.","authors":["Yi Chen","JiaHao Zhao","HaoHao Han"],"url":"https://arxiv.org/abs/2505.07460"}
{"created":"2025-05-13","title":"Promising Topics for U.S.-China Dialogues on AI Risks and Governance","abstract":"Cooperation between the United States and China, the world's leading artificial intelligence (AI) powers, is crucial for effective global AI governance and responsible AI development. Although geopolitical tensions have emphasized areas of conflict, in this work, we identify potential common ground for productive dialogue by conducting a systematic analysis of more than 40 primary AI policy and corporate governance documents from both nations. Specifically, using an adapted version of the AI Governance and Regulatory Archive (AGORA) - a comprehensive repository of global AI governance documents - we analyze these materials in their original languages to identify areas of convergence in (1) sociotechnical risk perception and (2) governance approaches. We find strong and moderate overlap in several areas such as on concerns about algorithmic transparency, system reliability, agreement on the importance of inclusive multi-stakeholder engagement, and AI's role in enhancing safety. These findings suggest that despite strategic competition, there exist concrete opportunities for bilateral U.S.-China cooperation in the development of responsible AI. Thus, we present recommendations for furthering diplomatic dialogues that can facilitate such cooperation. Our analysis contributes to understanding how different international governance frameworks might be harmonized to promote global responsible AI development.","authors":["Saad Siddiqui","Lujain Ibrahim","Kristy Loke","Stephen Clare","Marianne Lu","Aris Richardson","Conor McGlynn","Jeffrey Ding"],"url":"https://arxiv.org/abs/2505.07468"}
{"created":"2025-05-13","title":"Web-Bench: A LLM Code Benchmark Based on Web Standards and Frameworks","abstract":"The application of large language models (LLMs) in the field of coding is evolving rapidly: from code assistants, to autonomous coding agents, and then to generating complete projects through natural language. Early LLM code benchmarks primarily focused on code generation accuracy, but these benchmarks have gradually become saturated. Benchmark saturation weakens their guiding role for LLMs. For example, HumanEval Pass@1 has reached 99.4% and MBPP 94.2%. Among various attempts to address benchmark saturation, approaches based on software engineering have stood out, but the saturation of existing software engineering benchmarks is rapidly increasing. To address this, we propose a new benchmark, Web-Bench, which contains 50 projects, each consisting of 20 tasks with sequential dependencies. The tasks implement project features in sequence, simulating real-world human development workflows. When designing Web-Bench, we aim to cover the foundational elements of Web development: Web Standards and Web Frameworks. Given the scale and complexity of these projects, which were designed by engineers with 5 to 10 years of experience, each presents a significant challenge. On average, a single project takes 4 to 8 hours for a senior engineer to complete. On our given benchmark agent (Web-Agent), SOTA (Claude 3.7 Sonnet) achieves only 25.1% Pass@1, significantly lower (better) than SWE-Bench's Verified (65.4%) and Full (33.8%) scores. Finally, we discuss that in any development field, Standards and Frameworks represent foundational knowledge and efficiency tools, respectively, and LLMs require optimization tailored to them.","authors":["Kai Xu","YiWei Mao","XinYi Guan","ZiLong Feng"],"url":"https://arxiv.org/abs/2505.07473"}
{"created":"2025-05-13","title":"You Only Look One Step: Accelerating Backpropagation in Diffusion Sampling with Gradient Shortcuts","abstract":"Diffusion models (DMs) have recently demonstrated remarkable success in modeling large-scale data distributions. However, many downstream tasks require guiding the generated content based on specific differentiable metrics, typically necessitating backpropagation during the generation process. This approach is computationally expensive, as generating with DMs often demands tens to hundreds of recursive network calls, resulting in high memory usage and significant time consumption. In this paper, we propose a more efficient alternative that approaches the problem from the perspective of parallel denoising. We show that full backpropagation throughout the entire generation process is unnecessary. The downstream metrics can be optimized by retaining the computational graph of only one step during generation, thus providing a shortcut for gradient propagation. The resulting method, which we call Shortcut Diffusion Optimization (SDO), is generic, high-performance, and computationally lightweight, capable of optimizing all parameter types in diffusion sampling. We demonstrate the effectiveness of SDO on several real-world tasks, including controlling generation by optimizing latent and aligning the DMs by fine-tuning network parameters. Compared to full backpropagation, our approach reduces computational costs by $\\sim 90\\%$ while maintaining superior performance. Code is available at https://github.com/deng-ai-lab/SDO.","authors":["Hongkun Dou","Zeyu Li","Xingyu Jiang","Hongjue Li","Lijun Yang","Wen Yao","Yue Deng"],"url":"https://arxiv.org/abs/2505.07477"}
{"created":"2025-05-13","title":"Addressing degeneracies in latent interpolation for diffusion models","abstract":"There is an increasing interest in using image-generating diffusion models for deep data augmentation and image morphing. In this context, it is useful to interpolate between latents produced by inverting a set of input images, in order to generate new images representing some mixture of the inputs. We observe that such interpolation can easily lead to degenerate results when the number of inputs is large. We analyze the cause of this effect theoretically and experimentally, and suggest a suitable remedy. The suggested approach is a relatively simple normalization scheme that is easy to use whenever interpolation between latents is needed. We measure image quality using FID and CLIP embedding distance and show experimentally that baseline interpolation methods lead to a drop in quality metrics long before the degeneration issue is clearly visible. In contrast, our method significantly reduces the degeneration effect and leads to improved quality metrics also in non-degenerate situations.","authors":["Erik Landolsi","Fredrik Kahl"],"url":"https://arxiv.org/abs/2505.07481"}
{"created":"2025-05-13","title":"Integrated Localization and Path Planning for an Ocean Exploring Team of Autonomous Underwater Vehicles with Consensus Graph Model Predictive Control","abstract":"Navigation of a team of autonomous underwater vehicles (AUVs) coordinated by an unmanned surface vehicle (USV) is efficient and reliable for deep ocean exploration. AUVs depart from and return to the USV after collaborative navigation, data collection, and ocean exploration missions. Efficient path planning and accurate localization are essential, the latter of which is critical due to the lack of global localization signals and poor radio frequency (RF) communication in deep waters. Inertial navigation and acoustic communication are common solutions for localization. However, the former is subject to odometry drifts, and the latter is limited to short distances. This paper proposes a systematic approach for localization-aware energy-efficient collision-free path planning for a USV-AUVs team. Path planning is formulated as finite receding horizon model predictive control (MPC) optimization. A dynamic-aware linear kinodynamic motion equation is developed. The mathematical formulation for the MPC optimization is effectively developed where localization is integrated as consensus graph optimization among AUV nodes. Edges in the optimized AUV-to-USV (A2U) and AUV-to-AUV (A2A) graphs are constrained to the sonar range of acoustic modems. The time complexity of the consensus MPC optimization problem is analyzed, revealing a nonconvex NP-hard problem, which is solved using sequential convex programming. Numerical simulation results are provided to evaluate the proposed method.","authors":["Mohsen Eskandari","Andrey V. Savkin","Mohammad Deghat"],"url":"https://arxiv.org/abs/2505.07484"}
{"created":"2025-05-13","title":"Shots and Boosters: Exploring the Use of Combined Prebunking Interventions to Raise Critical Thinking and Create Long-Term Protection Against Misinformation","abstract":"The problem of how to effectively mitigate the flow of misinformation remains a significant challenge. The classical approach to this is public disapproval of claims or \"debunking.\" The approach is still widely used on social media, but it has some severe limitations in terms of applicability and efficiency. An alternative strategy is to enhance individuals' critical thinking through educational interventions. Instead of merely disproving misinformation, these approaches aim to strengthen users' reasoning skills, enabling them to evaluate and reject false information independently. In this position paper, we explore a combination of intervention methods designed to improve critical thinking in the context of online media consumption. We highlight the role of AI in supporting different stages of these interventions and present a design concept that integrates AI-driven strategies to foster critical reasoning and media literacy.","authors":["Huiyun Tang","Anastasia Sergeeva"],"url":"https://arxiv.org/abs/2505.07486"}
{"created":"2025-05-13","title":"Linux Kernel Configurations at Scale: A Dataset for Performance and Evolution Analysis","abstract":"Configuring the Linux kernel to meet specific requirements, such as binary size, is highly challenging due to its immense complexity-with over 15,000 interdependent options evolving rapidly across different versions. Although several studies have explored sampling strategies and machine learning methods to understand and predict the impact of configuration options, the literature still lacks a comprehensive and large-scale dataset encompassing multiple kernel versions along with detailed quantitative measurements. To bridge this gap, we introduce LinuxData, an accessible collection of kernel configurations spanning several kernel releases, specifically from versions 4.13 to 5.8. This dataset, gathered through automated tools and build processes, comprises over 240,000 kernel configurations systematically labeled with compilation outcomes and binary sizes. By providing detailed records of configuration evolution and capturing the intricate interplay among kernel options, our dataset enables innovative research in feature subset selection, prediction models based on machine learning, and transfer learning across kernel versions. Throughout this paper, we describe how the dataset has been made easily accessible via OpenML and illustrate how it can be leveraged using only a few lines of Python code to evaluate AI-based techniques, such as supervised machine learning. We anticipate that this dataset will significantly enhance reproducibility and foster new insights into configuration-space analysis at a scale that presents unique opportunities and inherent challenges, thereby advancing our understanding of the Linux kernel's configurability and evolution.","authors":["Heraldo Borges","Juliana Alves Pereira","Djamel Eddine Khelladi","Mathieu Acher"],"url":"https://arxiv.org/abs/2505.07487"}
{"created":"2025-05-13","title":"Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions","abstract":"This paper introduces and evaluates three translations of the Grievance Dictionary, a psycholinguistic dictionary for the analysis of violent, threatening or grievance-fuelled texts. Considering the relevance of these themes in languages beyond English, we translated the Grievance Dictionary to Dutch, German, and Italian. We describe the process of automated translation supplemented by human annotation. Psychometric analyses are performed, including internal reliability of dictionary categories and correlations with the LIWC dictionary. The Dutch and German translations perform similarly to the original English version, whereas the Italian dictionary shows low reliability for some categories. Finally, we make suggestions for further validation and application of the dictionary, as well as for future dictionary translations following a similar approach.","authors":["Isabelle van der Vegt","Bennett Kleinberg","Marilu Miotto","Jonas Festor"],"url":"https://arxiv.org/abs/2505.07495"}
{"created":"2025-05-13","title":"DocVXQA: Context-Aware Visual Explanations for Document Question Answering","abstract":"We propose DocVXQA, a novel framework for visually self-explainable document question answering. The framework is designed not only to produce accurate answers to questions but also to learn visual heatmaps that highlight contextually critical regions, thereby offering interpretable justifications for the model's decisions. To integrate explanations into the learning process, we quantitatively formulate explainability principles as explicit learning objectives. Unlike conventional methods that emphasize only the regions pertinent to the answer, our framework delivers explanations that are \\textit{contextually sufficient} while remaining \\textit{representation-efficient}. This fosters user trust while achieving a balance between predictive performance and interpretability in DocVQA applications. Extensive experiments, including human evaluation, provide strong evidence supporting the effectiveness of our method. The code is available at https://github.com/dali92002/DocVXQA.","authors":["Mohamed Ali Souibgui","Changkyu Choi","Andrey Barsky","Kangsoo Jung","Ernest Valveny","Dimosthenis Karatzas"],"url":"https://arxiv.org/abs/2505.07496"}
{"created":"2025-05-13","title":"Design Requirements for Patient-Centered Digital Health Applications: Supporting Patients' Values in Postoperative Delirium Prevention","abstract":"Postoperative delirium (POD) is among the most common complications after surgeries for older adults and can entail long-term adverse health consequences. Active patient participation in POD prevention presents a central factor in reducing these risks. To support patient engagement through a digital health application, we use value sensitive design approaches to identify the requirements for a patient-centered digital health application supporting patient engagement in POD prevention. Through interviews with medical professionals and patient representatives, we construct a patient journey, which serves as the basis for twelve patient value journey interviews. In these interviews, patients from the high-risk group for POD revisit their recent experience of undergoing surgery to elicit barriers, needs, and values concerning POD prevention from a patient perspective. An analysis of the patient interviews derives four design requirements for a digital health application supporting patients regarding POD prevention: the adaptation of patient-centered communication, the provision of procedural transparency, fostering patient empowerment through consistent guidance, and explicitly addressing relatives as mediators and supporters for a patient after a POD occurrence.","authors":["David Leimst\\\"adtner","Fatima Halzl-Y\\\"urek","Claudia Spies","Claudia M\\\"uller-Birn"],"url":"https://arxiv.org/abs/2505.07498"}
{"created":"2025-05-13","title":"Learning to Reason and Navigate: Parameter Efficient Action Planning with Large Language Models","abstract":"The remote embodied referring expression (REVERIE) task requires an agent to navigate through complex indoor environments and localize a remote object specified by high-level instructions, such as \"bring me a spoon\", without pre-exploration. Hence, an efficient navigation plan is essential for the final success. This paper proposes a novel parameter-efficient action planner using large language models (PEAP-LLM) to generate a single-step instruction at each location. The proposed model consists of two modules, LLM goal planner (LGP) and LoRA action planner (LAP). Initially, LGP extracts the goal-oriented plan from REVERIE instructions, including the target object and room. Then, LAP generates a single-step instruction with the goal-oriented plan, high-level instruction, and current visual observation as input. PEAP-LLM enables the embodied agent to interact with LAP as the path planner on the fly. A simple direct application of LLMs hardly achieves good performance. Also, existing hard-prompt-based methods are error-prone in complicated scenarios and need human intervention. To address these issues and prevent the LLM from generating hallucinations and biased information, we propose a novel two-stage method for fine-tuning the LLM, consisting of supervised fine-tuning (STF) and direct preference optimization (DPO). SFT improves the quality of generated instructions, while DPO utilizes environmental feedback. Experimental results show the superiority of our proposed model on REVERIE compared to the previous state-of-the-art.","authors":["Bahram Mohammadi","Ehsan Abbasnejad","Yuankai Qi","Qi Wu","Anton Van Den Hengel","Javen Qinfeng Shi"],"url":"https://arxiv.org/abs/2505.07500"}
{"created":"2025-05-13","title":"The Complexity of Pure Strategy Relevant Equilibria in Concurrent Games","abstract":"We study rational synthesis problems for concurrent games with $\\omega$-regular objectives. Our model of rationality considers only pure strategy Nash equilibria that satisfy either a social welfare or Pareto optimality condition with respect to an $\\omega$-regular objective for each agent. This extends earlier work on equilibria in concurrent games, without consideration about their quality. Our results show that the existence of Nash equilibria satisfying social welfare conditions can be computed as efficiently as the constrained Nash equilibrium existence problem. On the other hand, the existence of Nash equilibria satisfying the Pareto optimality condition possibly involves a higher upper bound, except in the case of B\\\"uchi and Muller games, for which all three problems are in the classes P and PSPACE-complete, respectively.","authors":["Purandar Bhaduri"],"url":"https://arxiv.org/abs/2505.07501"}
{"created":"2025-05-13","title":"Identifying Causal Direction via Variational Bayesian Compression","abstract":"Telling apart the cause and effect between two random variables with purely observational data is a challenging problem that finds applications in various scientific disciplines. A key principle utilized in this task is the algorithmic Markov condition, which postulates that the joint distribution, when factorized according to the causal direction, yields a more succinct codelength compared to the anti-causal direction. Previous approaches approximate these codelengths by relying on simple functions or Gaussian processes (GPs) with easily evaluable complexity, compromising between model fitness and computational complexity. To overcome these limitations, we propose leveraging the variational Bayesian learning of neural networks as an interpretation of the codelengths. Consequently, we can enhance the model fitness while promoting the succinctness of the codelengths, while avoiding the significant computational complexity of the GP-based approaches. Extensive experiments on both synthetic and real-world benchmarks in cause-effect identification demonstrate the effectiveness of our proposed method, surpassing the overall performance of related complexity-based and structural causal model regression-based approaches.","authors":["Quang-Duy Tran","Bao Duong","Phuoc Nguyen","Thin Nguyen"],"url":"https://arxiv.org/abs/2505.07503"}
{"created":"2025-05-13","title":"EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection","abstract":"Graph anomaly detection is a popular and vital task in various real-world scenarios, which has been studied for several decades. Recently, many studies extending deep learning-based methods have shown preferable performance on graph anomaly detection. However, existing methods are lack of efficiency that is definitely necessary for embedded devices. Towards this end, we propose an Efficient Anomaly detection model on heterogeneous Graphs via contrastive LEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms of their distances to the local context. The proposed method first samples instance pairs on meta path-level for contrastive learning. Then, a graph autoencoder-based model is applied to learn informative node embeddings in an unsupervised way, which will be further combined with the discriminator to predict the anomaly scores of nodes. Experimental results show that EAGLE outperforms the state-of-the-art methods on three heterogeneous network datasets.","authors":["Jing Ren","Mingliang Hou","Zhixuan Liu","Xiaomei Bai"],"url":"https://arxiv.org/abs/2505.07508"}
{"created":"2025-05-13","title":"HALO: Half Life-Based Outdated Fact Filtering in Temporal Knowledge Graphs","abstract":"Outdated facts in temporal knowledge graphs (TKGs) result from exceeding the expiration date of facts, which negatively impact reasoning performance on TKGs. However, existing reasoning methods primarily focus on positive importance of historical facts, neglecting adverse effects of outdated facts. Besides, training on these outdated facts yields extra computational cost. To address these challenges, we propose an outdated fact filtering framework named HALO, which quantifies the temporal validity of historical facts by exploring the half-life theory to filter outdated facts in TKGs. HALO consists of three modules: the temporal fact attention module, the dynamic relation-aware encoder module, and the outdated fact filtering module. Firstly, the temporal fact attention module captures the evolution of historical facts over time to identify relevant facts. Secondly, the dynamic relation-aware encoder module is designed for efficiently predicting the half life of each fact. Finally, we construct a time decay function based on the half-life theory to quantify the temporal validity of facts and filter outdated facts. Experimental results show that HALO outperforms the state-of-the-art TKG reasoning methods on three public datasets, demonstrating its effectiveness in detecting and filtering outdated facts (Codes are available at https://github.com/yushuowiki/K-Half/tree/main ).","authors":["Feng Ding","Tingting Wang","Yupeng Gao","Shuo Yu","Jing Ren","Feng Xia"],"url":"https://arxiv.org/abs/2505.07509"}
{"created":"2025-05-13","title":"MAIS: Memory-Attention for Interactive Segmentation","abstract":"Interactive medical segmentation reduces annotation effort by refining predictions through user feedback. Vision Transformer (ViT)-based models, such as the Segment Anything Model (SAM), achieve state-of-the-art performance using user clicks and prior masks as prompts. However, existing methods treat interactions as independent events, leading to redundant corrections and limited refinement gains. We address this by introducing MAIS, a Memory-Attention mechanism for Interactive Segmentation that stores past user inputs and segmentation states, enabling temporal context integration. Our approach enhances ViT-based segmentation across diverse imaging modalities, achieving more efficient and accurate refinements.","authors":["Mauricio Orbes-Arteaga","Oeslle Lucena","Sabastien Ourselin","M. Jorge Cardoso"],"url":"https://arxiv.org/abs/2505.07511"}
{"created":"2025-05-13","title":"ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution","abstract":"The tool-using capability of large language models (LLMs) enables them to access up-to-date external information and handle complex tasks. Current approaches to enhancing this capability primarily rely on distilling advanced models by data synthesis. However, this method incurs significant costs associated with advanced model usage and often results in data compatibility issues, led by the high discrepancy in the knowledge scope between the advanced model and the target model. To address these challenges, we propose ToolACE-DEV, a self-improving framework for tool learning. First, we decompose the tool-learning objective into sub-tasks that enhance basic tool-making and tool-using abilities. Then, we introduce a self-evolving paradigm that allows lightweight models to self-improve, reducing reliance on advanced LLMs. Extensive experiments validate the effectiveness of our approach across models of varying scales and architectures.","authors":["Xu Huang","Weiwen Liu","Xingshan Zeng","Yuefeng Huang","Xinlong Hao","Yuxian Wang","Yirong Zeng","Chuhan Wu","Yasheng Wang","Ruiming Tang","Defu Lian"],"url":"https://arxiv.org/abs/2505.07512"}
{"created":"2025-05-13","title":"An Approximation Framework for Subspace-based Methods in Spectral Analysis with Accuracy Guarantees","abstract":"A mathematical framework for the approximation of eigenvalues of self-adjoint operators through subspace-based methods is presented.","authors":["Timothy Stroschein"],"url":"https://arxiv.org/abs/2505.07513"}
{"created":"2025-05-13","title":"Improved Mixing of Critical Hardcore Model","abstract":"The hardcore model is one of the most classic and widely studied examples of undirected graphical models. Given a graph $G$, the hardcore model describes a Gibbs distribution of $\\lambda$-weighted independent sets of $G$. In the last two decades, a beautiful computational phase transition has been established at a precise threshold $\\lambda_c(\\Delta)$ where $\\Delta$ denotes the maximum degree, where the task of sampling independent sets transfers from polynomial-time solvable to computationally intractable. We study the critical hardcore model where $\\lambda = \\lambda_c(\\Delta)$ and show that the Glauber dynamics, a simple yet popular Markov chain algorithm, mixes in $\\tilde{O}(n^{7.44 + O(1/\\Delta)})$ time on any $n$-vertex graph of maximum degree $\\Delta\\geq3$, significantly improving the previous upper bound $\\tilde{O}(n^{12.88+O(1/\\Delta)})$ by the recent work arXiv:2411.03413. The core property we establish in this work is that the critical hardcore model is $O(\\sqrt{n})$-spectrally independent, improving the trivial bound of $n$ and matching the critical behavior of the Ising model. Our proof approach utilizes an online decision-making framework to study a site percolation model on the infinite $(\\Delta-1)$-ary tree, which can be interesting by itself.","authors":["Zongchen Chen","Tianhui Jiang"],"url":"https://arxiv.org/abs/2505.07515"}
{"created":"2025-05-13","title":"Average-Reward Maximum Entropy Reinforcement Learning for Global Policy in Double Pendulum Tasks","abstract":"This report presents our reinforcement learning-based approach for the swing-up and stabilisation tasks of the acrobot and pendubot, tailored specifcially to the updated guidelines of the 3rd AI Olympics at ICRA 2025. Building upon our previously developed Average-Reward Entropy Advantage Policy Optimization (AR-EAPO) algorithm, we refined our solution to effectively address the new competition scenarios and evaluation metrics. Extensive simulations validate that our controller robustly manages these revised tasks, demonstrating adaptability and effectiveness within the updated framework.","authors":["Jean Seong Bjorn Choe","Bumkyu Choi","Jong-kook Kim"],"url":"https://arxiv.org/abs/2505.07516"}
{"created":"2025-05-13","title":"Byam: Fixing Breaking Dependency Updates with Large Language Models","abstract":"Application Programming Interfaces (APIs) facilitate the integration of third-party dependencies within the code of client applications. However, changes to an API, such as deprecation, modification of parameter names or types, or complete replacement with a new API, can break existing client code. These changes are called breaking dependency updates; It is often tedious for API users to identify the cause of these breaks and update their code accordingly. In this paper, we explore the use of Large Language Models (LLMs) to automate client code updates in response to breaking dependency updates. We evaluate our approach on the BUMP dataset, a benchmark for breaking dependency updates in Java projects. Our approach leverages LLMs with advanced prompts, including information from the build process and from the breaking dependency analysis. We assess effectiveness at three granularity levels: at the build level, the file level, and the individual compilation error level. We experiment with five LLMs: Google Gemini-2.0 Flash, OpenAI GPT4o-mini, OpenAI o3-mini, Alibaba Qwen2.5-32b-instruct, and DeepSeek V3. Our results show that LLMs can automatically repair breaking updates. Among the considered models, OpenAI's o3-mini is the best, able to completely fix 27% of the builds when using prompts that include contextual information such as the buggy line, API differences, error messages, and step-by-step reasoning instructions. Also, it fixes 78% of the individual compilation errors. Overall, our findings demonstrate the potential for LLMs to fix compilation errors due to breaking dependency updates, supporting developers in their efforts to stay up-to-date with changes in their dependencies.","authors":["Frank Reyes","May Mahmoud","Federico Bono","Sarah Nadi","Benoit Baudry","Martin Monperrus"],"url":"https://arxiv.org/abs/2505.07522"}
{"created":"2025-05-13","title":"On rapid parallel tuning of controllers of a swarm of MAVs -- distribution strategies of the updated gains","abstract":"In this paper, we present a reliable, scalable, time deterministic, model-free procedure to tune swarms of Micro Aerial Vehicles (MAVs) using basic sensory data. Two approaches to taking advantage of parallel tuning are presented. First, the tuning with averaging of the results on the basis of performance indices reported from the swarm with identical gains to decrease the negative effect of the noise in the measurements. Second, the tuning with parallel testing of varying set of gains across the swarm to reduce the tuning time. The presented methods were evaluated both in simulation and real-world experiments. The achieved results show the ability of the proposed approach to improve the results of the tuning while decreasing the tuning time, ensuring at the same time a reliable tuning mechanism.","authors":["Dariusz Horla","Wojciech Giernacki","V\\'it Kr\\'atk\\'y","Petr \\v{S}tibinger","Tom\\'a\\v{s} B\\'a\\v{c}a","Martin Saska"],"url":"https://arxiv.org/abs/2505.07523"}
{"created":"2025-05-13","title":"Adaptive Latent-Space Constraints in Personalized FL","abstract":"Federated learning (FL) has become an effective and widely used approach to training deep learning models on decentralized datasets held by distinct clients. FL also strengthens both security and privacy protections for training data. Common challenges associated with statistical heterogeneity between distributed datasets have spurred significant interest in personalized FL (pFL) methods, where models combine aspects of global learning with local modeling specific to each client's unique characteristics. In this work, the efficacy of theoretically supported, adaptive MMD measures within the Ditto framework, a state-of-the-art technique in pFL, are investigated. The use of such measures significantly improves model performance across a variety of tasks, especially those with pronounced feature heterogeneity. While the Ditto algorithm is specifically considered, such measures are directly applicable to a number of other pFL settings, and the results motivate the use of constraints tailored to the various kinds of heterogeneity expected in FL systems.","authors":["Sana Ayromlou","D. B. Emerson"],"url":"https://arxiv.org/abs/2505.07525"}
{"created":"2025-05-13","title":"Kalman Filter Enhanced GRPO for Reinforcement Learning-Based Language Model Reasoning","abstract":"Reward baseline is important for Reinforcement Learning (RL) algorithms to reduce variance in policy gradient estimates. Recently, for language modeling, Group Relative Policy Optimization (GRPO) is proposed to compute the advantage for each output by subtracting the mean reward, as the baseline, for all outputs in the group. However, it can lead to inaccurate advantage estimates in environments with highly noisy rewards, potentially introducing bias. In this work, we propose a model, called Kalman Filter Enhanced Group Relative Policy Optimization (KRPO), by using lightweight Kalman filtering to dynamically estimate the latent reward mean and variance. This filtering technique replaces the naive batch mean baseline, enabling more adaptive advantage normalization. Our method does not require additional learned parameters over GRPO. This approach offers a simple yet effective way to incorporate multiple outputs of GRPO into advantage estimation, improving policy optimization in settings where highly dynamic reward signals are difficult to model for language models. Through experiments and analyses, we show that using a more adaptive advantage estimation model, KRPO can improve the stability and performance of GRPO. The code is available at https://github.com/billhhh/KRPO_LLMs_RL","authors":["Hu Wang","Congbo Ma","Ian Reid","Mohammad Yaqub"],"url":"https://arxiv.org/abs/2505.07527"}
{"created":"2025-05-13","title":"SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion","abstract":"Retrieval-Augmented Generation (RAG) models frequently encounter hallucination phenomena when integrating external information with internal parametric knowledge. Empirical studies demonstrate that the disequilibrium between external contextual information and internal parametric knowledge constitutes a primary factor in hallucination generation. Existing hallucination detection methodologies predominantly emphasize either the external or internal mechanism in isolation, thereby overlooking their synergistic effects. The recently proposed ReDeEP framework decouples these dual mechanisms, identifying two critical contributors to hallucinations: excessive reliance on parametric knowledge encoded in feed-forward networks (FFN) and insufficient utilization of external information by attention mechanisms (particularly copy heads). ReDeEP quantitatively assesses these factors to detect hallucinations and dynamically modulates the contributions of FFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and numerous other hallucination detection approaches have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, inadequately address the semantic dimensions of model responses, resulting in inconsistent hallucination assessments in RAG implementations. Building upon ReDeEP's foundation, this paper introduces SEReDeEP, which enhances computational processes through semantic entropy captured via trained linear probes, thereby achieving hallucination assessments that more accurately reflect ground truth evaluations.","authors":["Lei Wang"],"url":"https://arxiv.org/abs/2505.07528"}
{"created":"2025-05-13","title":"FLUXSynID: A Framework for Identity-Controlled Synthetic Face Generation with Document and Live Images","abstract":"Synthetic face datasets are increasingly used to overcome the limitations of real-world biometric data, including privacy concerns, demographic imbalance, and high collection costs. However, many existing methods lack fine-grained control over identity attributes and fail to produce paired, identity-consistent images under structured capture conditions. We introduce FLUXSynID, a framework for generating high-resolution synthetic face datasets with user-defined identity attribute distributions and paired document-style and trusted live capture images. The dataset generated using the FLUXSynID framework shows improved alignment with real-world identity distributions and greater inter-set diversity compared to prior work. The FLUXSynID framework for generating custom datasets, along with a dataset of 14,889 synthetic identities, is publicly released to support biometric research, including face recognition and morphing attack detection.","authors":["Raul Ismayilov","Luuk Spreeuwers","Dzemila Sero"],"url":"https://arxiv.org/abs/2505.07530"}
{"created":"2025-05-13","title":"QuantX: A Framework for Hardware-Aware Quantization of Generative AI Workloads","abstract":"We present QuantX: a tailored suite of recipes for LLM and VLM quantization. It is capable of quantizing down to 3-bit resolutions with minimal loss in performance. The quantization strategies in QuantX take into account hardware-specific constraints to achieve efficient dequantization during inference ensuring flexible trade-off between runtime speed, memory requirement and model accuracy. Our results demonstrate that QuantX achieves performance within 6% of the unquantized model for LlaVa-v1.6 quantized down to 3-bits for multiple end user tasks and outperforms recently published state-of-the-art quantization techniques. This manuscript provides insights into the LLM quantization process that motivated the range of recipes and options that are incorporated in QuantX.","authors":["Khurram Mazher","Saad Bin Nasir"],"url":"https://arxiv.org/abs/2505.07531"}
{"created":"2025-05-13","title":"RAI: Flexible Agent Framework for Embodied AI","abstract":"With an increase in the capabilities of generative language models, a growing interest in embodied AI has followed. This contribution introduces RAI - a framework for creating embodied Multi Agent Systems for robotics. The proposed framework implements tools for Agents' integration with robotic stacks, Large Language Models, and simulations. It provides out-of-the-box integration with state-of-the-art systems like ROS 2. It also comes with dedicated mechanisms for the embodiment of Agents. These mechanisms have been tested on a physical robot, Husarion ROSBot XL, which was coupled with its digital twin, for rapid prototyping. Furthermore, these mechanisms have been deployed in two simulations: (1) robot arm manipulator and (2) tractor controller. All of these deployments have been evaluated in terms of their control capabilities, effectiveness of embodiment, and perception ability. The proposed framework has been used successfully to build systems with multiple agents. It has demonstrated effectiveness in all the aforementioned tasks. It also enabled identifying and addressing the shortcomings of the generative models used for embodied AI.","authors":["Kajetan Rachwa{\\l}","Maciej Majek","Bart{\\l}omiej Boczek","Kacper D\\k{a}browski","Pawe{\\l} Liberadzki","Adam D\\k{a}browski","Maria Ganzha"],"url":"https://arxiv.org/abs/2505.07532"}
{"created":"2025-05-13","title":"IKrNet: A Neural Network for Detecting Specific Drug-Induced Patterns in Electrocardiograms Amidst Physiological Variability","abstract":"Monitoring and analyzing electrocardiogram (ECG) signals, even under varying physiological conditions, including those influenced by physical activity, drugs and stress, is crucial to accurately assess cardiac health. However, current AI-based methods often fail to account for how these factors interact and alter ECG patterns, ultimately limiting their applicability in real-world settings. This study introduces IKrNet, a novel neural network model, which identifies drug-specific patterns in ECGs amidst certain physiological conditions. IKrNet's architecture incorporates spatial and temporal dynamics by using a convolutional backbone with varying receptive field size to capture spatial features. A bi-directional Long Short-Term Memory module is also employed to model temporal dependencies. By treating heart rate variability as a surrogate for physiological fluctuations, we evaluated IKrNet's performance across diverse scenarios, including conditions with physical stress, drug intake alone, and a baseline without drug presence. Our assessment follows a clinical protocol in which 990 healthy volunteers were administered 80mg of Sotalol, a drug which is known to be a precursor to Torsades-de-Pointes, a life-threatening arrhythmia. We show that IKrNet outperforms state-of-the-art models' accuracy and stability in varying physiological conditions, underscoring its clinical viability.","authors":["Ahmad Fall","Federica Granese","Alex Lence","Dominique Fourer","Blaise Hanczar","Joe-Elie Salem","Jean-Daniel Zucker","Edi Prifti"],"url":"https://arxiv.org/abs/2505.07533"}
{"created":"2025-05-13","title":"The Human-Data-Model Interaction Canvas for Visual Analytics","abstract":"Visual Analytics (VA) integrates humans, data, and models as key actors in insight generation and data-driven decision-making. This position paper values and reflects on 16 VA process models and frameworks and makes nine high-level observations that motivate a fresh perspective on VA. The contribution is the HDMI Canvas, a perspective to VA that complements the strengths of existing VA process models and frameworks. It systematically characterizes diverse roles of humans, data, and models, and how these actors benefit from and contribute to VA processes. The descriptive power of the HDMI Canvas eases the differentiation between a series of VA building blocks, rather than describing general VA principles only. The canvas includes modern human-centered methodologies, including human knowledge externalization and forms of feedback loops, while interpretable and explainable AI highlight model contributions beyond their conventional outputs. The HDMI Canvas has generative power, guiding the design of new VA processes and is optimized for external stakeholders, improving VA outreach, interdisciplinary collaboration, and user-centered design. The utility of the HDMI Canvas is demonstrated through two preliminary case studies.","authors":["J\\\"urgen Bernard"],"url":"https://arxiv.org/abs/2505.07534"}
{"created":"2025-05-13","title":"Post-Quantum Secure Decentralized Random Number Generation Protocol with Two Rounds of Communication in the Standard Model","abstract":"Randomness plays a vital role in numerous applications, including simulation, cryptography, distributed systems, and gaming. Consequently, extensive research has been conducted to generate randomness. One such method is to design a decentralized random number generator (DRNG), a protocol that enables multiple participants to collaboratively generate random outputs that must be publicly verifiable. However, existing DRNGs are either not secure against quantum computers or depend on the random oracle model (ROM) to achieve security. In this paper, we design a DRNG based on lattice-based publicly verifiable secret sharing (PVSS) that is post-quantum secure and proven secure in the standard model. Additionally, our DRNG requires only two rounds of communication to generate a single (pseudo)random value and can tolerate up to any t < n/2 dishonest participants. To our knowledge, the proposed DRNG construction is the first to achieve all these properties.","authors":["Pham Nhat Minh","Khuong Nguyen-An"],"url":"https://arxiv.org/abs/2505.07536"}
{"created":"2025-05-13","title":"Discrete Visual Tokens of Autoregression, by Diffusion, and for Reasoning","abstract":"We completely discard the conventional spatial prior in image representation and introduce a novel discrete visual tokenizer: Self-consistency Tokenizer (Selftok). At its design core, we compose an autoregressive (AR) prior -- mirroring the causal structure of language -- into visual tokens by using the reverse diffusion process of image generation. The AR property makes Selftok fundamentally distinct from traditional spatial tokens in the following two key ways: - Selftok offers an elegant and minimalist approach to unify diffusion and AR for vision-language models (VLMs): By representing images with Selftok tokens, we can train a VLM using a purely discrete autoregressive architecture -- like that in LLMs -- without requiring additional modules or training objectives. - We theoretically show that the AR prior satisfies the Bellman equation, whereas the spatial prior does not. Therefore, Selftok supports reinforcement learning (RL) for visual generation with effectiveness comparable to that achieved in LLMs. Besides the AR property, Selftok is also a SoTA tokenizer that achieves a favorable trade-off between high-quality reconstruction and compression rate. We use Selftok to build a pure AR VLM for both visual comprehension and generation tasks. Impressively, without using any text-image training pairs, a simple policy gradient RL working in the visual tokens can significantly boost the visual generation benchmark, surpassing all the existing models by a large margin. Therefore, we believe that Selftok effectively addresses the long-standing challenge that visual tokens cannot support effective RL. When combined with the well-established strengths of RL in LLMs, this brings us one step closer to realizing a truly multimodal LLM. Project Page: https://selftok-team.github.io/report/.","authors":["Bohan Wang","Zhongqi Yue","Fengda Zhang","Shuo Chen","Li'an Bi","Junzhe Zhang","Xue Song","Kennard Yanting Chan","Jiachun Pan","Weijia Wu","Mingze Zhou","Wang Lin","Kaihang Pan","Saining Zhang","Liyu Jia","Wentao Hu","Wei Zhao","Hanwang Zhang"],"url":"https://arxiv.org/abs/2505.07538"}
{"created":"2025-05-13","title":"GIFStream: 4D Gaussian-based Immersive Video with Feature Stream","abstract":"Immersive video offers a 6-Dof-free viewing experience, potentially playing a key role in future video technology. Recently, 4D Gaussian Splatting has gained attention as an effective approach for immersive video due to its high rendering efficiency and quality, though maintaining quality with manageable storage remains challenging. To address this, we introduce GIFStream, a novel 4D Gaussian representation using a canonical space and a deformation field enhanced with time-dependent feature streams. These feature streams enable complex motion modeling and allow efficient compression by leveraging temporal correspondence and motion-aware pruning. Additionally, we incorporate both temporal and spatial compression networks for end-to-end compression. Experimental results show that GIFStream delivers high-quality immersive video at 30 Mbps, with real-time rendering and fast decoding on an RTX 4090. Project page: https://xdimlab.github.io/GIFStream","authors":["Hao Li","Sicheng Li","Xiang Gao","Abudouaihati Batuer","Lu Yu","Yiyi Liao"],"url":"https://arxiv.org/abs/2505.07539"}
{"created":"2025-05-13","title":"SynID: Passport Synthetic Dataset for Presentation Attack Detection","abstract":"The demand for Presentation Attack Detection (PAD) to identify fraudulent ID documents in remote verification systems has significantly risen in recent years. This increase is driven by several factors, including the rise of remote work, online purchasing, migration, and advancements in synthetic images. Additionally, we have noticed a surge in the number of attacks aimed at the enrolment process. Training a PAD to detect fake ID documents is very challenging because of the limited number of ID documents available due to privacy concerns. This work proposes a new passport dataset generated from a hybrid method that combines synthetic data and open-access information using the ICAO requirement to obtain realistic training and testing images.","authors":["Juan E. Tapia","Fabian Stockhardt","L\\'azaro Janier Gonz\\'alez-Soler","Christoph Busch"],"url":"https://arxiv.org/abs/2505.07540"}
{"created":"2025-05-13","title":"A Systematic Mapping Study on Contract-based Software Design for Dependable Systems","abstract":"Background: Contract-based Design (CbD) is a valuable methodology for software design that allows annotation of code and architectural components with contracts, thereby enhancing clarity and reliability in software development. It establishes rules that outline the behaviour of software components and their interfaces and interactions. This modular approach enables the design process to be segmented into smaller, independently developed, tested, and verified system components, ultimately leading to more robust and dependable software. Aim: Despite the significance and well-established theoretical background of CbD, there is a need for a comprehensive systematic mapping study for reliable software systems. Our study provides an evidence-based overview of a method and demonstrates its practical feasibility. Method: To conduct this study, we systematically searched three different databases using specially formulated queries, which initially yielded 1,221 primary studies. After voting, we focused on 288 primary studies for more detailed analysis. Finally, a collaborative review allowed us to gather relevant evidence and information to address our research questions. Results: Our findings suggest potential avenues for future research trajectories in CbD, emphasising its role in improving the dependability of software systems. We highlight maturity levels across different domains and identify areas that may benefit from further research. Conclusion: Although CbD is a well-established software design approach, a more comprehensive literature review is needed to clarify its theoretical state about dependable systems. Our study addresses this gap by providing a detailed overview of CbD from various perspectives, identifying key gaps, and suggesting future research directions.","authors":["Fazli Faruk Okumus","Amra Ramic","Stefan Kugele"],"url":"https://arxiv.org/abs/2505.07542"}
{"created":"2025-05-13","title":"GRADA: Graph-based Reranker against Adversarial Documents Attack","abstract":"Retrieval Augmented Generation (RAG) frameworks improve the accuracy of large language models (LLMs) by integrating external knowledge from retrieved documents, thereby overcoming the limitations of models' static intrinsic knowledge. However, these systems are susceptible to adversarial attacks that manipulate the retrieval process by introducing documents that are adversarial yet semantically similar to the query. Notably, while these adversarial documents resemble the query, they exhibit weak similarity to benign documents in the retrieval set. Thus, we propose a simple yet effective Graph-based Reranking against Adversarial Document Attacks (GRADA) framework aiming at preserving retrieval quality while significantly reducing the success of adversaries. Our study evaluates the effectiveness of our approach through experiments conducted on five LLMs: GPT-3.5-Turbo, GPT-4o, Llama3.1-8b, Llama3.1-70b, and Qwen2.5-7b. We use three datasets to assess performance, with results from the Natural Questions dataset demonstrating up to an 80% reduction in attack success rates while maintaining minimal loss in accuracy.","authors":["Jingjie Zheng","Aryo Pradipta Gema","Giwon Hong","Xuanli He","Pasquale Minervini","Youcheng Sun","Qiongkai Xu"],"url":"https://arxiv.org/abs/2505.07546"}
{"created":"2025-05-13","title":"Noise Optimized Conditional Diffusion for Domain Adaptation","abstract":"Pseudo-labeling is a cornerstone of Unsupervised Domain Adaptation (UDA), yet the scarcity of High-Confidence Pseudo-Labeled Target Domain Samples (\\textbf{hcpl-tds}) often leads to inaccurate cross-domain statistical alignment, causing DA failures. To address this challenge, we propose \\textbf{N}oise \\textbf{O}ptimized \\textbf{C}onditional \\textbf{D}iffusion for \\textbf{D}omain \\textbf{A}daptation (\\textbf{NOCDDA}), which seamlessly integrates the generative capabilities of conditional diffusion models with the decision-making requirements of DA to achieve task-coupled optimization for efficient adaptation. For robust cross-domain consistency, we modify the DA classifier to align with the conditional diffusion classifier within a unified optimization framework, enabling forward training on noise-varying cross-domain samples. Furthermore, we argue that the conventional \\( \\mathcal{N}(\\mathbf{0}, \\mathbf{I}) \\) initialization in diffusion models often generates class-confused hcpl-tds, compromising discriminative DA. To resolve this, we introduce a class-aware noise optimization strategy that refines sampling regions for reverse class-specific hcpl-tds generation, effectively enhancing cross-domain alignment. Extensive experiments across 5 benchmark datasets and 29 DA tasks demonstrate significant performance gains of \\textbf{NOCDDA} over 31 state-of-the-art methods, validating its robustness and effectiveness.","authors":["Lingkun Luo","Shiqiang Hu","Liming Chen"],"url":"https://arxiv.org/abs/2505.07548"}
{"created":"2025-05-13","title":"Automated Visual Attention Detection using Mobile Eye Tracking in Behavioral Classroom Studies","abstract":"Teachers' visual attention and its distribution across the students in classrooms can constitute important implications for student engagement, achievement, and professional teacher training. Despite that, inferring the information about where and which student teachers focus on is not trivial. Mobile eye tracking can provide vital help to solve this issue; however, the use of mobile eye tracking alone requires a significant amount of manual annotations. To address this limitation, we present an automated processing pipeline concept that requires minimal manually annotated data to recognize which student the teachers focus on. To this end, we utilize state-of-the-art face detection models and face recognition feature embeddings to train face recognition models with transfer learning in the classroom context and combine these models with the teachers' gaze from mobile eye trackers. We evaluated our approach with data collected from four different classrooms, and our results show that while it is possible to estimate the visually focused students with reasonable performance in all of our classroom setups, U-shaped and small classrooms led to the best results with accuracies of approximately 0.7 and 0.9, respectively. While we did not evaluate our method for teacher-student interactions and focused on the validity of the technical approach, as our methodology does not require a vast amount of manually annotated data and offers a non-intrusive way of handling teachers' visual attention, it could help improve instructional strategies, enhance classroom management, and provide feedback for professional teacher development.","authors":["Efe Bozkir","Christian Kosel","Tina Seidel","Enkelejda Kasneci"],"url":"https://arxiv.org/abs/2505.07552"}
{"created":"2025-05-13","title":"Towards Requirements Engineering for RAG Systems","abstract":"This short paper explores how a maritime company develops and integrates large-language models (LLM). Specifically by looking at the requirements engineering for Retrieval Augmented Generation (RAG) systems in expert settings. Through a case study at a maritime service provider, we demonstrate how data scientists face a fundamental tension between user expectations of AI perfection and the correctness of the generated outputs. Our findings reveal that data scientists must identify context-specific \"retrieval requirements\" through iterative experimentation together with users because they are the ones who can determine correctness. We present an empirical process model describing how data scientists practically elicited these \"retrieval requirements\" and managed system limitations. This work advances software engineering knowledge by providing insights into the specialized requirements engineering processes for implementing RAG systems in complex domain-specific applications.","authors":["Tor Sporsem","Rasmus Ulfsnes"],"url":"https://arxiv.org/abs/2505.07553"}
{"created":"2025-05-13","title":"Injecting Knowledge Graphs into Large Language Models","abstract":"Integrating structured knowledge from Knowledge Graphs (KGs) into Large Language Models (LLMs) remains a key challenge for symbolic reasoning. Existing methods mainly rely on prompt engineering or fine-tuning, which lose structural fidelity or incur high computational costs. Building on recent encoding techniques which integrate graph embeddings within the LLM input as tokens, we extend this paradigm to the KG domain by leveraging Knowledge Graph Embedding (KGE) models, thus enabling graph-aware reasoning. Our approach is model-agnostic, resource-efficient, and compatible with any LLMs. Extensive experimentation on synthetic and real-world datasets shows that our method improves reasoning performance over established baselines, further achieving the best trade-off in terms of accuracy and efficiency against state-of-the-art LLMs.","authors":["Erica Coppolillo"],"url":"https://arxiv.org/abs/2505.07554"}
{"created":"2025-05-13","title":"Energy-Efficient Resource Allocation for NOMA-Assisted Uplink Pinching-Antenna Systems","abstract":"The pinching-antenna architecture has emerged as a promising solution for reconfiguring wireless propagation environments and enhancing system performance. While prior research has primarily focused on sum-rate maximization or transmit power minimization of pinching-antenna systems, the critical aspect of energy efficiency (EE) has received limited attention. Given the increasing importance of EE in future wireless communication networks, this work investigates EE optimization in a non-orthogonal multiple access (NOMA)-assisted multi-user pinching-antenna uplink system. The problem entails the joint optimization of the users' transmit power and the pinching-antenna position. The resulting optimization problem is non-convex due to tightly coupled variables. To tackle this, we employ an alternating optimization framework to decompose the original problem into two subproblems: one focusing on power allocation and the other on antenna positioning. A low-complexity optimal solution is derived for the power allocation subproblem, while the pinching-antenna positioning subproblem is addressed using a particle swarm optimization algorithm to obtain a high-quality near-optimal solution. Simulation results demonstrate that the proposed scheme significantly outperforms both conventional-antenna configurations and orthogonal multiple access-based pinching-antenna systems in terms of EE.","authors":["Ming Zeng","Xingwang Li","Ji Wang","Gaojian Huang","Octavia A. Dobre","Zhiguo Ding"],"url":"https://arxiv.org/abs/2505.07555"}
{"created":"2025-05-13","title":"Self-Supervised Event Representations: Towards Accurate, Real-Time Perception on SoC FPGAs","abstract":"Event cameras offer significant advantages over traditional frame-based sensors. These include microsecond temporal resolution, robustness under varying lighting conditions and low power consumption. Nevertheless, the effective processing of their sparse, asynchronous event streams remains challenging. Existing approaches to this problem can be categorised into two distinct groups. The first group involves the direct processing of event data with neural models, such as Spiking Neural Networks or Graph Convolutional Neural Networks. However, this approach is often accompanied by a compromise in terms of qualitative performance. The second group involves the conversion of events into dense representations with handcrafted aggregation functions, which can boost accuracy at the cost of temporal fidelity. This paper introduces a novel Self-Supervised Event Representation (SSER) method leveraging Gated Recurrent Unit (GRU) networks to achieve precise per-pixel encoding of event timestamps and polarities without temporal discretisation. The recurrent layers are trained in a self-supervised manner to maximise the fidelity of event-time encoding. The inference is performed with event representations generated asynchronously, thus ensuring compatibility with high-throughput sensors. The experimental validation demonstrates that SSER outperforms aggregation-based baselines, achieving improvements of 2.4% mAP and 0.6% on the Gen1 and 1 Mpx object detection datasets. Furthermore, the paper presents the first hardware implementation of recurrent representation for event data on a System-on-Chip FPGA, achieving sub-microsecond latency and power consumption between 1-2 W, suitable for real-time, power-efficient applications. Code is available at https://github.com/vision-agh/RecRepEvent.","authors":["Kamil Jeziorek","Tomasz Kryjak"],"url":"https://arxiv.org/abs/2505.07556"}
{"created":"2025-05-13","title":"Direct Density Ratio Optimization: A Statistically Consistent Approach to Aligning Large Language Models","abstract":"Aligning large language models (LLMs) with human preferences is crucial for safe deployment, yet existing methods assume specific preference models like Bradley-Terry model. This assumption leads to statistical inconsistency, where more data doesn't guarantee convergence to true human preferences. To address this critical gap, we introduce a novel alignment method Direct Density Ratio Optimization (DDRO). DDRO directly estimates the density ratio between preferred and unpreferred output distributions, circumventing the need for explicit human preference modeling. We theoretically prove that DDRO is statistically consistent, ensuring convergence to the true preferred distribution as the data size grows, regardless of the underlying preference structure. Experiments demonstrate that DDRO achieves superior performance compared to existing methods on many major benchmarks. DDRO unlocks the potential for truly data-driven alignment, paving the way for more reliable and human-aligned LLMs.","authors":["Rei Higuchi","Taiji Suzuki"],"url":"https://arxiv.org/abs/2505.07558"}
{"created":"2025-05-13","title":"Pinching-Antenna Systems (PASS) Aided Over-the-air Computation","abstract":"Over-the-air computation (AirComp) enables fast data aggregation for edge intelligence applications. However the performance of AirComp can be severely degraded by channel misalignments. Pinching antenna systems (PASS) have recently emerged as a promising solution for physically reshaping favorable wireless channels to reduce misalignments and thus AirComp errors, via low-cost, fully passive, and highly reconfigurable antenna deployment. Motivated by these benefits, we propose a novel PASS-aided AirComp system that introduces new design degrees of freedom through flexible pinching antenna (PA) placement. To improve performance, we consider a mean squared error (MSE) minimization problem by jointly optimizing the PA position, transmit power, and decoding vector. To solve this highly non-convex problem, we propose an alternating optimization based framework with Gauss-Seidel based PA position updates. Simulation results show that our proposed joint PA position and communication design significantly outperforms various benchmark schemes in AirComp accuracy.","authors":["Zhonghao Lyu","Haoyun Li","Yulan Gao","Ming Xiao","H. Vincent Poor"],"url":"https://arxiv.org/abs/2505.07559"}
{"created":"2025-05-13","title":"Robust Kidney Abnormality Segmentation: A Validation Study of an AI-Based Framework","abstract":"Kidney abnormality segmentation has important potential to enhance the clinical workflow, especially in settings requiring quantitative assessments. Kidney volume could serve as an important biomarker for renal diseases, with changes in volume correlating directly with kidney function. Currently, clinical practice often relies on subjective visual assessment for evaluating kidney size and abnormalities, including tumors and cysts, which are typically staged based on diameter, volume, and anatomical location. To support a more objective and reproducible approach, this research aims to develop a robust, thoroughly validated kidney abnormality segmentation algorithm, made publicly available for clinical and research use. We employ publicly available training datasets and leverage the state-of-the-art medical image segmentation framework nnU-Net. Validation is conducted using both proprietary and public test datasets, with segmentation performance quantified by Dice coefficient and the 95th percentile Hausdorff distance. Furthermore, we analyze robustness across subgroups based on patient sex, age, CT contrast phases, and tumor histologic subtypes. Our findings demonstrate that our segmentation algorithm, trained exclusively on publicly available data, generalizes effectively to external test sets and outperforms existing state-of-the-art models across all tested datasets. Subgroup analyses reveal consistent high performance, indicating strong robustness and reliability. The developed algorithm and associated code are publicly accessible at https://github.com/DIAGNijmegen/oncology-kidney-abnormality-segmentation.","authors":["Sarah de Boer","Hartmut H\\\"antze","Kiran Vaidhya Venkadesh","Myrthe A. D. Buser","Gabriel E. Humpire Mamani","Lina Xu","Lisa C. Adams","Jawed Nawabi","Keno K. Bressem","Bram van Ginneken","Mathias Prokop","Alessa Hering"],"url":"https://arxiv.org/abs/2505.07573"}
{"created":"2025-05-13","title":"Security through the Eyes of AI: How Visualization is Shaping Malware Detection","abstract":"Malware, a persistent cybersecurity threat, increasingly targets interconnected digital systems such as desktop, mobile, and IoT platforms through sophisticated attack vectors. By exploiting these vulnerabilities, attackers compromise the integrity and resilience of modern digital ecosystems. To address this risk, security experts actively employ Machine Learning or Deep Learning-based strategies, integrating static, dynamic, or hybrid approaches to categorize malware instances. Despite their advantages, these methods have inherent drawbacks and malware variants persistently evolve with increased sophistication, necessitating advancements in detection strategies. Visualization-based techniques are emerging as scalable and interpretable solutions for detecting and understanding malicious behaviors across diverse platforms including desktop, mobile, IoT, and distributed systems as well as through analysis of network packet capture files. In this comprehensive survey of more than 100 high-quality research articles, we evaluate existing visualization-based approaches applied to malware detection and classification. As a first contribution, we propose a new all-encompassing framework to study the landscape of visualization-based malware detection techniques. Within this framework, we systematically analyze state-of-the-art approaches across the critical stages of the malware detection pipeline. By analyzing not only the single techniques but also how they are combined to produce the final solution, we shed light on the main challenges in visualization-based approaches and provide insights into the advancements and potential future directions in this critical field.","authors":["Asmitha K. A.","Matteo Brosolo","Serena Nicolazzo","Antonino Nocera","Vinod P.","Rafidha Rehiman K. A.","Muhammed Shafi K. P"],"url":"https://arxiv.org/abs/2505.07574"}
{"created":"2025-05-13","title":"Personalized Federated Learning under Model Dissimilarity Constraints","abstract":"One of the defining challenges in federated learning is that of statistical heterogeneity among clients. We address this problem with KARULA, a regularized strategy for personalized federated learning, which constrains the pairwise model dissimilarities between clients based on the difference in their distributions, as measured by a surrogate for the 1-Wasserstein distance adapted for the federated setting. This allows the strategy to adapt to highly complex interrelations between clients, that e.g., clustered approaches fail to capture. We propose an inexact projected stochastic gradient algorithm to solve the constrained problem that the strategy defines, and show theoretically that it converges with smooth, possibly non-convex losses to a neighborhood of a stationary point with rate O(1/K). We demonstrate the effectiveness of KARULA on synthetic and real federated data sets.","authors":["Samuel Erickson","Mikael Johansson"],"url":"https://arxiv.org/abs/2505.07575"}
{"created":"2025-05-13","title":"Evaluating Modern Visual Anomaly Detection Approaches in Semiconductor Manufacturing: A Comparative Study","abstract":"Semiconductor manufacturing is a complex, multistage process. Automated visual inspection of Scanning Electron Microscope (SEM) images is indispensable for minimizing equipment downtime and containing costs. Most previous research considers supervised approaches, assuming a sufficient number of anomalously labeled samples. On the contrary, Visual Anomaly Detection (VAD), an emerging research domain, focuses on unsupervised learning, avoiding the costly defect collection phase while providing explanations of the predictions. We introduce a benchmark for VAD in the semiconductor domain by leveraging the MIIC dataset. Our results demonstrate the efficacy of modern VAD approaches in this field.","authors":["Manuel Barusco","Francesco Borsatti","Youssef Ben Khalifa","Davide Dalle Pezze","Gian Antonio Susto"],"url":"https://arxiv.org/abs/2505.07576"}
{"created":"2025-05-13","title":"From raw affiliations to organization identifiers","abstract":"Accurate affiliation matching, which links affiliation strings to standardized organization identifiers, is critical for improving research metadata quality, facilitating comprehensive bibliometric analyses, and supporting data interoperability across scholarly knowledge bases. Existing approaches fail to handle the complexity of affiliation strings that often include mentions of multiple organizations or extraneous information. In this paper, we present AffRo, a novel approach designed to address these challenges, leveraging advanced parsing and disambiguation techniques. We also introduce AffRoDB, an expert-curated dataset to systematically evaluate affiliation matching algorithms, ensuring robust benchmarking. Results demonstrate the effectiveness of AffRp in accurately identifying organizations from complex affiliation strings.","authors":["Myrto Kallipoliti","Serafeim Chatzopoulos","Miriam Baglioni","Eleni Adamidi","Paris Koloveas","Thanasis Vergoulis"],"url":"https://arxiv.org/abs/2505.07577"}
{"created":"2025-05-13","title":"Dynamic Rental Games with Stagewise Individual Rationality","abstract":"We study \\emph{rental games} -- a single-parameter dynamic mechanism design problem, in which a designer rents out an indivisible asset over $n$ days. Each day, an agent arrives with a private valuation per day of rental, drawn from that day's (known) distribution. The designer can either rent out the asset to the current agent for any number of remaining days, charging them a (possibly different) payment per day, or turn the agent away. Agents who arrive when the asset is not available are turned away. A defining feature of our dynamic model is that agents are \\emph{stagewise-IR} (individually rational), meaning they reject any rental agreement that results in temporary negative utility, even if their final utility is positive. We ask whether and under which economic objectives it is useful for the designer to exploit the stagewise-IR nature of the agents.","authors":["Batya Berzack","Rotem Oshman","Inbal Talgam-Cohen"],"url":"https://arxiv.org/abs/2505.07579"}
{"created":"2025-05-13","title":"YuLan-OneSim: Towards the Next Generation of Social Simulator with Large Language Models","abstract":"Leveraging large language model (LLM) based agents to simulate human social behaviors has recently gained significant attention. In this paper, we introduce a novel social simulator called YuLan-OneSim. Compared to previous works, YuLan-OneSim distinguishes itself in five key aspects: (1) Code-free scenario construction: Users can simply describe and refine their simulation scenarios through natural language interactions with our simulator. All simulation code is automatically generated, significantly reducing the need for programming expertise. (2) Comprehensive default scenarios: We implement 50 default simulation scenarios spanning 8 domains, including economics, sociology, politics, psychology, organization, demographics, law, and communication, broadening access for a diverse range of social researchers. (3) Evolvable simulation: Our simulator is capable of receiving external feedback and automatically fine-tuning the backbone LLMs, significantly enhancing the simulation quality. (4) Large-scale simulation: By developing a fully responsive agent framework and a distributed simulation architecture, our simulator can handle up to 100,000 agents, ensuring more stable and reliable simulation results. (5) AI social researcher: Leveraging the above features, we develop an AI social researcher. Users only need to propose a research topic, and the AI researcher will automatically analyze the input, construct simulation environments, summarize results, generate technical reports, review and refine the reports--completing the social science research loop. To demonstrate the advantages of YuLan-OneSim, we conduct experiments to evaluate the quality of the automatically generated scenarios, the reliability, efficiency, and scalability of the simulation process, as well as the performance of the AI social researcher.","authors":["Lei Wang","Heyang Gao","Xiaohe Bo","Xu Chen","Ji-Rong Wen"],"url":"https://arxiv.org/abs/2505.07581"}
{"created":"2025-05-13","title":"Privacy-Preserving Real-Time Vietnamese-English Translation on iOS using Edge AI","abstract":"This research addresses the growing need for privacy-preserving and accessible language translation by developing a fully offline Neural Machine Translation (NMT) system for Vietnamese-English translation on iOS devices. Given increasing concerns about data privacy and unreliable network connectivity, on-device translation offers critical advantages. This project confronts challenges in deploying complex NMT models on resource-limited mobile devices, prioritizing efficiency, accuracy, and a seamless user experience. Leveraging advances such as MobileBERT and, specifically, the lightweight \\textbf{TinyLlama 1.1B Chat v1.0} in GGUF format, \\textbf{a} quantized Transformer-based model is implemented and optimized. The application is realized as a real-time iOS prototype, tightly integrating modern iOS frameworks and privacy-by-design principles. Comprehensive documentation covers model selection, technical architecture, challenges, and final implementation, including functional Swift code for deployment.","authors":["Cong Le"],"url":"https://arxiv.org/abs/2505.07583"}
{"created":"2025-05-13","title":"SecReEvalBench: A Multi-turned Security Resilience Evaluation Benchmark for Large Language Models","abstract":"The increasing deployment of large language models in security-sensitive domains necessitates rigorous evaluation of their resilience against adversarial prompt-based attacks. While previous benchmarks have focused on security evaluations with limited and predefined attack domains, such as cybersecurity attacks, they often lack a comprehensive assessment of intent-driven adversarial prompts and the consideration of real-life scenario-based multi-turn attacks. To address this gap, we present SecReEvalBench, the Security Resilience Evaluation Benchmark, which defines four novel metrics: Prompt Attack Resilience Score, Prompt Attack Refusal Logic Score, Chain-Based Attack Resilience Score and Chain-Based Attack Rejection Time Score. Moreover, SecReEvalBench employs six questioning sequences for model assessment: one-off attack, successive attack, successive reverse attack, alternative attack, sequential ascending attack with escalating threat levels and sequential descending attack with diminishing threat levels. In addition, we introduce a dataset customized for the benchmark, which incorporates both neutral and malicious prompts, categorised across seven security domains and sixteen attack techniques. In applying this benchmark, we systematically evaluate five state-of-the-art open-weighted large language models, Llama 3.1, Gemma 2, Mistral v0.3, DeepSeek-R1 and Qwen 3. Our findings offer critical insights into the strengths and weaknesses of modern large language models in defending against evolving adversarial threats. The SecReEvalBench dataset is publicly available at https://kaggle.com/datasets/5a7ee22cf9dab6c93b55a73f630f6c9b42e936351b0ae98fbae6ddaca7fe248d, which provides a groundwork for advancing research in large language model security.","authors":["Huining Cui","Wei Liu"],"url":"https://arxiv.org/abs/2505.07584"}
{"created":"2025-05-13","title":"A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models","abstract":"Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.","authors":["Junjie Ye","Caishuang Huang","Zhuohan Chen","Wenjie Fu","Chenyuan Yang","Leyi Yang","Yilong Wu","Peng Wang","Meng Zhou","Xiaolong Yang","Tao Gui","Qi Zhang","Zhongchao Shi","Jianping Fan","Xuanjing Huang"],"url":"https://arxiv.org/abs/2505.07591"}
{"created":"2025-05-13","title":"Decoding Chess Puzzle Play and Standard Cognitive Tasks for BCI: A Low-Cost EEG Study","abstract":"While consumer-grade electroencephalography (EEG) devices show promise for Brain-Computer Interface (BCI) applications, their efficacy in detecting subtle cognitive states remains understudied. Using a combination of established cognitive paradigms (N-Back, Stroop, and Mental Rotation) and a novel ecological task (Chess puzzles), we demonstrate successful distinctions of workload levels within some tasks, as well as differentiation between task types using the MUSE 2 device. With machine learning we further show reliable predictive power to differentiate between workload levels in the N-Back task, while also achieving effective cross-task classification. These findings demonstrate that consumer-grade EEG devices can effectively detect and differentiate various forms of cognitive workload, and that they can be leveraged with some success towards real-time classification distinguishing workload in some tasks, as well as in differentiating between nuanced cognitive states, supporting their potential use in adaptive BCI applications. Research code and data are further provided for future researchers.","authors":["Matthew Russell","Samuel Youkeles","William Xia","Kenny Zheng","Aman Shah","Robert J. K. Jacob"],"url":"https://arxiv.org/abs/2505.07592"}
{"created":"2025-05-13","title":"Finite-Sample-Based Reachability for Safe Control with Gaussian Process Dynamics","abstract":"Gaussian Process (GP) regression is shown to be effective for learning unknown dynamics, enabling efficient and safety-aware control strategies across diverse applications. However, existing GP-based model predictive control (GP-MPC) methods either rely on approximations, thus lacking guarantees, or are overly conservative, which limits their practical utility. To close this gap, we present a sampling-based framework that efficiently propagates the model's epistemic uncertainty while avoiding conservatism. We establish a novel sample complexity result that enables the construction of a reachable set using a finite number of dynamics functions sampled from the GP posterior. Building on this, we design a sampling-based GP-MPC scheme that is recursively feasible and guarantees closed-loop safety and stability with high probability. Finally, we showcase the effectiveness of our method on two numerical examples, highlighting accurate reachable set over-approximation and safe closed-loop performance.","authors":["Manish Prajapat","Johannes K\\\"ohler","Amon Lahr","Andreas Krause","Melanie N. Zeilinger"],"url":"https://arxiv.org/abs/2505.07594"}
{"created":"2025-05-13","title":"Towards Cross-Model Efficiency in SQL/PGQ","abstract":"SQL/PGQ is a new standard that integrates graph querying into relational systems, allowing users to freely switch between graph patterns and SQL. Our experiments show performance gaps between these models, as queries written in both formalisms can exhibit varying performance depending on the formalism used, suggesting that current approaches handle each query type separately, applying distinct optimizations to each formalism. We argue that a holistic optimization is necessary, where the system internally decides on the best algorithms regardless of whether queries are written in SQL or as graph patterns. We propose possible future research direction to unify these optimizations and mitigate performance gaps.","authors":["Hadar Rotschield","Liat Peterfreund"],"url":"https://arxiv.org/abs/2505.07595"}
{"created":"2025-05-13","title":"Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent","abstract":"Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.","authors":["Ziyang Huang","Xiaowei Yuan","Yiming Ju","Jun Zhao","Kang Liu"],"url":"https://arxiv.org/abs/2505.07596"}
{"created":"2025-05-13","title":"Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding","abstract":"Manipulating clothes is challenging due to their complex dynamics, high deformability, and frequent self-occlusions. Garments exhibit a nearly infinite number of configurations, making explicit state representations difficult to define. In this paper, we analyze BiFold, a model that predicts language-conditioned pick-and-place actions from visual observations, while implicitly encoding garment state through end-to-end learning. To address scenarios such as crumpled garments or recovery from failed manipulations, BiFold leverages temporal context to improve state estimation. We examine the internal representations of the model and present evidence that its fine-tuning and temporal context enable effective alignment between text and image regions, as well as temporal consistency.","authors":["Oriol Barbany","Adri\\`a Colom\\'e","Carme Torras"],"url":"https://arxiv.org/abs/2505.07600"}
{"created":"2025-05-13","title":"Characterizing the Investigative Methods of Fictional Detectives with Large Language Models","abstract":"Detective fiction, a genre defined by its complex narrative structures and character-driven storytelling, presents unique challenges for computational narratology, a research field focused on integrating literary theory into automated narrative generation. While traditional literary studies have offered deep insights into the methods and archetypes of fictional detectives, these analyses often focus on a limited number of characters and lack the scalability needed for the extraction of unique traits that can be used to guide narrative generation methods. In this paper, we present an AI-driven approach for systematically characterizing the investigative methods of fictional detectives. Our multi-phase workflow explores the capabilities of 15 Large Language Models (LLMs) to extract, synthesize, and validate distinctive investigative traits of fictional detectives. This approach was tested on a diverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes, William Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin - capturing the distinctive investigative styles that define each character. The identified traits were validated against existing literary analyses and further tested in a reverse identification phase, achieving an overall accuracy of 91.43%, demonstrating the method's effectiveness in capturing the distinctive investigative approaches of each detective. This work contributes to the broader field of computational narratology by providing a scalable framework for character analysis, with potential applications in AI-driven interactive storytelling and automated narrative generation.","authors":["Edirlei Soares de Lima","Marco A. Casanova","Bruno Feij\\'o","Antonio L. Furtado"],"url":"https://arxiv.org/abs/2505.07601"}
{"created":"2025-05-13","title":"AgentFlow: Resilient Adaptive Cloud-Edge Framework for Multi-Agent Coordination","abstract":"This paper presents AgentFlow, a MAS-based framework for programmable distributed systems in heterogeneous cloud-edge environments. It introduces logistics objects and abstract agent interfaces to enable dynamic service flows and modular orchestration. AgentFlow supports decentralized publish-subscribe messaging and many-to-many service elections, enabling decision coordination without a central server. It features plug-and-play node discovery, flexible task reorganization, and highly adaptable fault tolerance and substitution mechanisms. AgentFlow advances scalable, real-time coordination for resilient and autonomous mission-critical systems.","authors":["Ching Han Chen","Ming Fang Shiu"],"url":"https://arxiv.org/abs/2505.07603"}
{"created":"2025-05-13","title":"Data Ethics in the Fediverse: Analyzing the Role of Instance Policies in Mastodon Research","abstract":"This article addresses the disconnect between the individual policy documents of Mastodon instances--many of which explicitly prohibit data collection for research purposes--and the actual data handling practices observed in academic research involving Mastodon. We present a systematic analysis of 29 works that used Mastodon as a data source, revealing limited adherence to instance--level policies despite researchers' general awareness of their existence. Our findings underscore the need for broader discussion about ethical obligations in research on alternative, decentralized social media platforms.","authors":["Mareike Lisker","Helena Mihaljevi\\'c"],"url":"https://arxiv.org/abs/2505.07606"}
{"created":"2025-05-13","title":"Multi-Objective Reinforcement Learning for Energy-Efficient Industrial Control","abstract":"Industrial automation increasingly demands energy-efficient control strategies to balance performance with environmental and cost constraints. In this work, we present a multi-objective reinforcement learning (MORL) framework for energy-efficient control of the Quanser Aero 2 testbed in its one-degree-of-freedom configuration. We design a composite reward function that simultaneously penalizes tracking error and electrical power consumption. Preliminary experiments explore the influence of varying the Energy penalty weight, alpha, on the trade-off between pitch tracking and energy savings. Our results reveal a marked performance shift for alpha values between 0.0 and 0.25, with non-Pareto optimal solutions emerging at lower alpha values, on both the simulation and the real system. We hypothesize that these effects may be attributed to artifacts introduced by the adaptive behavior of the Adam optimizer, which could bias the learning process and favor bang-bang control strategies. Future work will focus on automating alpha selection through Gaussian Process-based Pareto front modeling and transitioning the approach from simulation to real-world deployment.","authors":["Georg Sch\\\"afer","Raphael Seliger","Jakob Rehrl","Stefan Huber","Simon Hirlaender"],"url":"https://arxiv.org/abs/2505.07607"}
{"created":"2025-05-13","title":"MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining","abstract":"We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.","authors":["Core Team","Bingquan Xia","Bowen Shen","Cici","Dawei Zhu","Di Zhang","Gang Wang","Hailin Zhang","Huaqiu Liu","Jiebao Xiao","Jinhao Dong","Liang Zhao","Peidian Li","Peng Wang","Shihua Yu","Shimao Chen","Weikun Wang","Wenhan Ma","Xiangwei Deng","Yi Huang","Yifan Song","Zihan Jiang","Bowen Ye","Can Cai","Chenhong He","Dong Zhang","Duo Zhang","Guoan Wang","Hao Tian","Haochen Zhao","Heng Qu","Hongshen Xu","Jun Shi","Kainan Bao","QingKai Fang","Kang Zhou","Kangyang Zhou","Lei Li","Menghang Zhu","Nuo Chen","Qiantong Wang","Shaohui Liu","Shicheng Li","Shuhao Gu","Shuhuai Ren","Shuo Liu","Sirui Deng","Weiji Zhuang","Weiwei Lv","Wenyu Yang","Xin Zhang","Xing Yong","Xing Zhang","Xingchen Song","Xinzhe Xu","Xu Wang","Yihan Yan","Yu Tu","Yuanyuan Tian","Yudong Wang","Yue Yu","Zhenru Lin","Zhichao Song","Zihao Yue"],"url":"https://arxiv.org/abs/2505.07608"}
{"created":"2025-05-13","title":"Concept-Level Explainability for Auditing & Steering LLM Responses","abstract":"As large language models (LLMs) become widely deployed, concerns about their safety and alignment grow. An approach to steer LLM behavior, such as mitigating biases or defending against jailbreaks, is to identify which parts of a prompt influence specific aspects of the model's output. Token-level attribution methods offer a promising solution, but still struggle in text generation, explaining the presence of each token in the output separately, rather than the underlying semantics of the entire LLM response. We introduce ConceptX, a model-agnostic, concept-level explainability method that identifies the concepts, i.e., semantically rich tokens in the prompt, and assigns them importance based on the outputs' semantic similarity. Unlike current token-level methods, ConceptX also offers to preserve context integrity through in-place token replacements and supports flexible explanation goals, e.g., gender bias. ConceptX enables both auditing, by uncovering sources of bias, and steering, by modifying prompts to shift the sentiment or reduce the harmfulness of LLM responses, without requiring retraining. Across three LLMs, ConceptX outperforms token-level methods like TokenSHAP in both faithfulness and human alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for random edits and lower attack success rates from 0.463 to 0.242, outperforming attribution and paraphrasing baselines. While prompt engineering and self-explaining methods sometimes yield safer responses, ConceptX offers a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the practical value of attribution-based explainability in guiding LLM behavior.","authors":["Kenza Amara","Rita Sevastjanova","Mennatallah El-Assady"],"url":"https://arxiv.org/abs/2505.07610"}
{"created":"2025-05-13","title":"Deep Learning Advances in Vision-Based Traffic Accident Anticipation: A Comprehensive Review of Methods,Datasets,and Future Directions","abstract":"Traffic accident prediction and detection are critical for enhancing road safety,and vision-based traffic accident anticipation (Vision-TAA) has emerged as a promising approach in the era of deep learning.This paper reviews 147 recent studies,focusing on the application of supervised,unsupervised,and hybrid deep learning models for accident prediction,alongside the use of real-world and synthetic datasets.Current methodologies are categorized into four key approaches: image and video feature-based prediction, spatiotemporal feature-based prediction, scene understanding,and multimodal data fusion.While these methods demonstrate significant potential,challenges such as data scarcity,limited generalization to complex scenarios,and real-time performance constraints remain prevalent. This review highlights opportunities for future research,including the integration of multimodal data fusion, self-supervised learning,and Transformer-based architectures to enhance prediction accuracy and scalability.By synthesizing existing advancements and identifying critical gaps, this paper provides a foundational reference for developing robust and adaptive Vision-TAA systems,contributing to road safety and traffic management.","authors":["Yi Zhang","Wenye Zhou","Ruonan Lin","Xin Yang","Hao Zheng"],"url":"https://arxiv.org/abs/2505.07611"}
{"created":"2025-05-13","title":"Trial and Trust: Addressing Byzantine Attacks with Comprehensive Defense Strategy","abstract":"Recent advancements in machine learning have improved performance while also increasing computational demands. While federated and distributed setups address these issues, their structure is vulnerable to malicious influences. In this paper, we address a specific threat, Byzantine attacks, where compromised clients inject adversarial updates to derail global convergence. We combine the trust scores concept with trial function methodology to dynamically filter outliers. Our methods address the critical limitations of previous approaches, allowing functionality even when Byzantine nodes are in the majority. Moreover, our algorithms adapt to widely used scaled methods like Adam and RMSProp, as well as practical scenarios, including local training and partial participation. We validate the robustness of our methods by conducting extensive experiments on both synthetic and real ECG data collected from medical institutions. Furthermore, we provide a broad theoretical analysis of our algorithms and their extensions to aforementioned practical setups. The convergence guarantees of our methods are comparable to those of classical algorithms developed without Byzantine interference.","authors":["Gleb Molodtsov","Daniil Medyakov","Sergey Skorik","Nikolas Khachaturov","Shahane Tigranyan","Vladimir Aletov","Aram Avetisyan","Martin Tak\\'a\\v{c}","Aleksandr Beznosikov"],"url":"https://arxiv.org/abs/2505.07614"}
{"created":"2025-05-13","title":"KAQG: A Knowledge-Graph-Enhanced RAG for Difficulty-Controlled Question Generation","abstract":"KAQG introduces a decisive breakthrough for Retrieval-Augmented Generation (RAG) by explicitly tackling the two chronic weaknesses of current pipelines: transparent multi-step reasoning and fine-grained cognitive difficulty control. This transforms RAG from a passive retriever into an accountable generator of calibrated exam items. Technically, the framework fuses knowledge graphs, RAG retrieval, and educational assessment theory into a single pipeline. Domain passages are parsed into a structured graph; graph-aware retrieval feeds fact chains to an LLM; and an assessment layer governed by Bloom's Taxonomy levels and Item Response Theory (IRT) transforms those chains into psychometrically sound questions. This cross-disciplinary marriage yields two scholarly contributions: it shows how semantic graph contexts guide LLM reasoning paths, and it operationalizes difficulty metrics within the generation process, producing items whose IRT parameters match expert benchmarks. Every module, from KG construction scripts to the multi-agent reasoning scheduler and the automatic IRT validator, is openly released on GitHub. This enables peer laboratories to replicate experiments, benchmark against baselines, and extend individual components without licensing barriers. Its reproducible design paves the way for rigorous ablation studies, cross-domain transfer experiments, and shared leaderboards on multi-step reasoning benchmarks.","authors":["Ching Han Chen","Ming Fang Shiu"],"url":"https://arxiv.org/abs/2505.07618"}
{"created":"2025-05-13","title":"Higher-Order Convolution Improves Neural Predictivity in the Retina","abstract":"We present a novel approach to neural response prediction that incorporates higher-order operations directly within convolutional neural networks (CNNs). Our model extends traditional 3D CNNs by embedding higher-order operations within the convolutional operator itself, enabling direct modeling of multiplicative interactions between neighboring pixels across space and time. Our model increases the representational power of CNNs without increasing their depth, therefore addressing the architectural disparity between deep artificial networks and the relatively shallow processing hierarchy of biological visual systems. We evaluate our approach on two distinct datasets: salamander retinal ganglion cell (RGC) responses to natural scenes, and a new dataset of mouse RGC responses to controlled geometric transformations. Our higher-order CNN (HoCNN) achieves superior performance while requiring only half the training data compared to standard architectures, demonstrating correlation coefficients up to 0.75 with neural responses (against 0.80$\\pm$0.02 retinal reliability). When integrated into state-of-the-art architectures, our approach consistently improves performance across different species and stimulus conditions. Analysis of the learned representations reveals that our network naturally encodes fundamental geometric transformations, particularly scaling parameters that characterize object expansion and contraction. This capability is especially relevant for specific cell types, such as transient OFF-alpha and transient ON cells, which are known to detect looming objects and object motion respectively, and where our model shows marked improvement in response prediction. The correlation coefficients for scaling parameters are more than twice as high in HoCNN (0.72) compared to baseline models (0.32).","authors":["Simone Azeglio","Victor Calbiague Garcia","Guilhem Glaziou","Peter Neri","Olivier Marre","Ulisse Ferrari"],"url":"https://arxiv.org/abs/2505.07620"}
{"created":"2025-05-13","title":"Bang for the Buck: Vector Search on Cloud CPUs","abstract":"Vector databases have emerged as a new type of systems that support efficient querying of high-dimensional vectors. Many of these offer their database as a service in the cloud. However, the variety of available CPUs and the lack of vector search benchmarks across CPUs make it difficult for users to choose one. In this study, we show that CPU microarchitectures available in the cloud perform significantly differently across vector search scenarios. For instance, in an IVF index on float32 vectors, AMD's Zen4 gives almost 3x more queries per second (QPS) compared to Intel's Sapphire Rapids, but for HNSW indexes, the tables turn. However, when looking at the number of queries per dollar (QP$), Graviton3 is the best option for most indexes and quantization settings, even over Graviton4 (Table 1). With this work, we hope to guide users in getting the best \"bang for the buck\" when deploying vector search systems.","authors":["Leonardo Kuffo","Peter Boncz"],"url":"https://arxiv.org/abs/2505.07621"}
{"created":"2025-05-13","title":"A Unified Hierarchical Framework for Fine-grained Cross-view Geo-localization over Large-scale Scenarios","abstract":"Cross-view geo-localization is a promising solution for large-scale localization problems, requiring the sequential execution of retrieval and metric localization tasks to achieve fine-grained predictions. However, existing methods typically focus on designing standalone models for these two tasks, resulting in inefficient collaboration and increased training overhead. In this paper, we propose UnifyGeo, a novel unified hierarchical geo-localization framework that integrates retrieval and metric localization tasks into a single network. Specifically, we first employ a unified learning strategy with shared parameters to jointly learn multi-granularity representation, facilitating mutual reinforcement between these two tasks. Subsequently, we design a re-ranking mechanism guided by a dedicated loss function, which enhances geo-localization performance by improving both retrieval accuracy and metric localization references. Extensive experiments demonstrate that UnifyGeo significantly outperforms the state-of-the-arts in both task-isolated and task-associated settings. Remarkably, on the challenging VIGOR benchmark, which supports fine-grained localization evaluation, the 1-meter-level localization recall rate improves from 1.53\\% to 39.64\\% and from 0.43\\% to 25.58\\% under same-area and cross-area evaluations, respectively. Code will be made publicly available.","authors":["Zhuo Song","Ye Zhang","Kunhong Li","Longguang Wang","Yulan Guo"],"url":"https://arxiv.org/abs/2505.07622"}
{"created":"2025-05-13","title":"Enhancing Federated Learning with Kolmogorov-Arnold Networks: A Comparative Study Across Diverse Aggregation Strategies","abstract":"Multilayer Perceptron (MLP), as a simple yet powerful model, continues to be widely used in classification and regression tasks. However, traditional MLPs often struggle to efficiently capture nonlinear relationships in load data when dealing with complex datasets. Kolmogorov-Arnold Networks (KAN), inspired by the Kolmogorov-Arnold representation theorem, have shown promising capabilities in modeling complex nonlinear relationships. In this study, we explore the performance of KANs within federated learning (FL) frameworks and compare them to traditional Multilayer Perceptrons. Our experiments, conducted across four diverse datasets demonstrate that KANs consistently outperform MLPs in terms of accuracy, stability, and convergence efficiency. KANs exhibit remarkable robustness under varying client numbers and non-IID data distributions, maintaining superior performance even as client heterogeneity increases. Notably, KANs require fewer communication rounds to converge compared to MLPs, highlighting their efficiency in FL scenarios. Additionally, we evaluate multiple parameter aggregation strategies, with trimmed mean and FedProx emerging as the most effective for optimizing KAN performance. These findings establish KANs as a robust and scalable alternative to MLPs for federated learning tasks, paving the way for their application in decentralized and privacy-preserving environments.","authors":["Yizhou Ma","Zhuoqin Yang","Luis-Daniel Ib\\'a\\~nez"],"url":"https://arxiv.org/abs/2505.07629"}
{"created":"2025-05-13","title":"Neural Brain: A Neuroscience-inspired Framework for Embodied Agents","abstract":"The rapid evolution of artificial intelligence (AI) has shifted from static, data-driven models to dynamic systems capable of perceiving and interacting with real-world environments. Despite advancements in pattern recognition and symbolic reasoning, current AI systems, such as large language models, remain disembodied, unable to physically engage with the world. This limitation has driven the rise of embodied AI, where autonomous agents, such as humanoid robots, must navigate and manipulate unstructured environments with human-like adaptability. At the core of this challenge lies the concept of Neural Brain, a central intelligence system designed to drive embodied agents with human-like adaptability. A Neural Brain must seamlessly integrate multimodal sensing and perception with cognitive capabilities. Achieving this also requires an adaptive memory system and energy-efficient hardware-software co-design, enabling real-time action in dynamic environments. This paper introduces a unified framework for the Neural Brain of embodied agents, addressing two fundamental challenges: (1) defining the core components of Neural Brain and (2) bridging the gap between static AI models and the dynamic adaptability required for real-world deployment. To this end, we propose a biologically inspired architecture that integrates multimodal active sensing, perception-cognition-action function, neuroplasticity-based memory storage and updating, and neuromorphic hardware/software optimization. Furthermore, we also review the latest research on embodied agents across these four aspects and analyze the gap between current AI systems and human intelligence. By synthesizing insights from neuroscience, we outline a roadmap towards the development of generalizable, autonomous agents capable of human-level intelligence in real-world scenarios.","authors":["Jian Liu","Xiongtao Shi","Thai Duy Nguyen","Haitian Zhang","Tianxiang Zhang","Wei Sun","Yanjie Li","Athanasios V. Vasilakos","Giovanni Iacca","Arshad Ali Khan","Arvind Kumar","Jae Won Cho","Ajmal Mian","Lihua Xie","Erik Cambria","Lin Wang"],"url":"https://arxiv.org/abs/2505.07634"}
{"created":"2025-05-13","title":"Generating Skyline Explanations for Graph Neural Networks","abstract":"This paper proposes a novel approach to generate subgraph explanations for graph neural networks GNNs that simultaneously optimize multiple measures for explainability. Existing GNN explanation methods often compute subgraphs (called ``explanatory subgraphs'') that optimize a pre-defined, single explainability measure, such as fidelity or conciseness. This can lead to biased explanations that cannot provide a comprehensive explanation to clarify the output of GNN models. We introduce skyline explanation, a GNN explanation paradigm that aims to identify k explanatory subgraphs by simultaneously optimizing multiple explainability measures. (1) We formulate skyline explanation generation as a multi-objective optimization problem, and pursue explanations that approximate a skyline set of explanatory subgraphs. We show the hardness for skyline explanation generation. (2) We design efficient algorithms with an onion-peeling approach that strategically removes edges from neighbors of nodes of interests, and incrementally improves explanations as it explores an interpretation domain, with provable quality guarantees. (3) We further develop an algorithm to diversify explanations to provide more comprehensive perspectives. Using real-world graphs, we empirically verify the effectiveness, efficiency, and scalability of our algorithms.","authors":["Dazhuo Qiu","Haolai Che","Arijit Khan","Yinghui Wu"],"url":"https://arxiv.org/abs/2505.07635"}
{"created":"2025-05-13","title":"Chronocept: Instilling a Sense of Time in Machines","abstract":"Human cognition is deeply intertwined with a sense of time, known as Chronoception. This sense allows us to judge how long facts remain valid and when knowledge becomes outdated. Despite progress in vision, language, and motor control, AI still struggles to reason about temporal validity. We introduce Chronocept, the first benchmark to model temporal validity as a continuous probability distribution over time. Using skew-normal curves fitted along semantically decomposed temporal axes, Chronocept captures nuanced patterns of emergence, decay, and peak relevance. It includes two datasets: Benchmark I (atomic facts) and Benchmark II (multi-sentence passages). Annotations show strong inter-annotator agreement (84% and 89%). Our baselines predict curve parameters - location, scale, and skewness - enabling interpretable, generalizable learning and outperforming classification-based approaches. Chronocept fills a foundational gap in AI's temporal reasoning, supporting applications in knowledge grounding, fact-checking, retrieval-augmented generation (RAG), and proactive agents. Code and data are publicly available.","authors":["Krish Goel","Sanskar Pandey","KS Mahadevan","Harsh Kumar","Vishesh Khadaria"],"url":"https://arxiv.org/abs/2505.07637"}
{"created":"2025-05-13","title":"'Congratulations, morons': Dynamics of Toxicity and Interaction Polarization in the Covid Vaccination and Ukraine War Twitter Debates","abstract":"The existence of polarization and echo chambers has been noted in social media discussions of public concern such as the Covid-19 pandemic, foreign election interference, and regional conflicts. However, measuring polarization and assessing the manner in which polarization contributes to partisan behavior is not always possible to evaluate with static network or affect measurements. To address this, we conduct an analysis of two large Twitter datasets collected around Covid-19 vaccination and the Ukraine war to investigate polarization in terms of the evolution in influencer preferences and toxicity of post contents. By reducing retweet behavior in each sample to several key dimensions, we identify clusters that reflect ideological preferences, along with geographic or linguistic separation for some cases. By tracking the central retweet tendency of these clusters over time, we observe differences in the relative position of ideologically unaligned clusters compared to aligned ones, which we interpret as reflecting polarization dynamics in the information diffusion space. We then measure the toxicity of posts and test if toxicity in one cluster can be temporally dependent on its structural closeness to (or toxicity of) another. We find evidence of ideological opposition among clusters of users in both samples, and a temporal association between toxicity and structural divergence for at least two ideologically opposed clusters in our samples. These observations support the importance of analyzing polarization as a multifaceted dynamic phenomenon where polarization dynamics may also manifest in unexpected ways such as within a single ideological camp.","authors":["D. S. Axelrod","B. H. Pleasants","J. C. Paolillo"],"url":"https://arxiv.org/abs/2505.07646"}
{"created":"2025-05-13","title":"On the choice of optimization norm for Anderson acceleration of the Picard iteration for Navier-Stokes equations","abstract":"Recently developed convergence theory for Anderson acceleration (AA) assumes that the AA optimization norm matches the norm of the Hilbert space that the fixed point function is defined on. While this seems a natural assumption, it may not be the optimal choice in terms of convergence of the iteration or computational efficiency. For the Picard iteration for the Navier-Stokes equations (NSE), the associated Hilbert space norm is $H^1_0(\\Omega),$ which is inefficient to implement in a large scale HPC setting since it requires multiplication of global coefficient vectors by the stiffness matrix. Motivated by recent numerical tests that show using the $\\ell^2$ norm produces similar convergence behavior as $H^1_0$ does, we revisit the convergence theory of [Pollock et al, {\\it SINUM} 2019] and find that i) it can be improved with a sharper treatment of the nonlinear terms; and ii) in the case that the AA optimization norm is changed to $L^2$ (and by extension $\\ell^2$ or $L^2$ using a diagonally lumped mass matrix), a new convergence theory is developed that provides an essentially equivalent estimate as the $H^1_0$ case. Several numerical tests illustrate the new theory, and the theory and tests reveal that one can interchangeably use the norms $H^1_0$, $L^2$, $\\ell^2$ or $L^2$ with diagonally lumped mass matrix for the AA optimization problem without significantly affecting the overall convergence behavior. Thus, one is justified to use $\\ell^2$ or diagonally lumped $L^2$ for the AA optimization norm in Anderson accelerated Picard iterations for large scale NSE problems.","authors":["Elizabeth Hawkins","Leo Rebholz"],"url":"https://arxiv.org/abs/2505.07650"}
{"created":"2025-05-13","title":"ShotAdapter: Text-to-Multi-Shot Video Generation with Diffusion Models","abstract":"Current diffusion-based text-to-video methods are limited to producing short video clips of a single shot and lack the capability to generate multi-shot videos with discrete transitions where the same character performs distinct activities across the same or different backgrounds. To address this limitation we propose a framework that includes a dataset collection pipeline and architectural extensions to video diffusion models to enable text-to-multi-shot video generation. Our approach enables generation of multi-shot videos as a single video with full attention across all frames of all shots, ensuring character and background consistency, and allows users to control the number, duration, and content of shots through shot-specific conditioning. This is achieved by incorporating a transition token into the text-to-video model to control at which frames a new shot begins and a local attention masking strategy which controls the transition token's effect and allows shot-specific prompting. To obtain training data we propose a novel data collection pipeline to construct a multi-shot video dataset from existing single-shot video datasets. Extensive experiments demonstrate that fine-tuning a pre-trained text-to-video model for a few thousand iterations is enough for the model to subsequently be able to generate multi-shot videos with shot-specific control, outperforming the baselines. You can find more details in https://shotadapter.github.io/","authors":["Ozgur Kara","Krishna Kumar Singh","Feng Liu","Duygu Ceylan","James M. Rehg","Tobias Hinz"],"url":"https://arxiv.org/abs/2505.07652"}
{"created":"2025-05-13","title":"JobHop: A Large-Scale Dataset of Career Trajectories","abstract":"Understanding labor market dynamics is essential for policymakers, employers, and job seekers. However, comprehensive datasets that capture real-world career trajectories are scarce. In this paper, we introduce JobHop, a large-scale public dataset derived from anonymized resumes provided by VDAB, the public employment service in Flanders, Belgium. Utilizing Large Language Models (LLMs), we process unstructured resume data to extract structured career information, which is then mapped to standardized ESCO occupation codes using a multi-label classification model. This results in a rich dataset of over 2.3 million work experiences, extracted from and grouped into more than 391,000 user resumes and mapped to standardized ESCO occupation codes, offering valuable insights into real-world occupational transitions. This dataset enables diverse applications, such as analyzing labor market mobility, job stability, and the effects of career breaks on occupational transitions. It also supports career path prediction and other data-driven decision-making processes. To illustrate its potential, we explore key dataset characteristics, including job distributions, career breaks, and job transitions, demonstrating its value for advancing labor market research.","authors":["Iman Johary","Raphael Romero","Alexandru C. Mara","Tijl De Bie"],"url":"https://arxiv.org/abs/2505.07653"}
{"created":"2025-05-13","title":"Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent","abstract":"This paper argues that the relationship between lexical identity and prosody -- one well-studied parameter of linguistic variation -- can be characterized using information theory. We predict that languages that use prosody to make lexical distinctions should exhibit a higher mutual information between word identity and prosody, compared to languages that don't. We test this hypothesis in the domain of pitch, which is used to make lexical distinctions in tonal languages, like Cantonese. We use a dataset of speakers reading sentences aloud in ten languages across five language families to estimate the mutual information between the text and their pitch curves. We find that, across languages, pitch curves display similar amounts of entropy. However, these curves are easier to predict given their associated text in the tonal languages, compared to pitch- and stress-accent languages, and thus the mutual information is higher in these languages, supporting our hypothesis. Our results support perspectives that view linguistic typology as gradient, rather than categorical.","authors":["Ethan Gotlieb Wilcox","Cui Ding","Giovanni Acampa","Tiago Pimentel","Alex Warstadt","Tamar I. Regev"],"url":"https://arxiv.org/abs/2505.07659"}
{"created":"2025-05-13","title":"A comparative study of Bitcoin and Ripple cryptocurrencies trading using Deep Reinforcement Learning algorithms","abstract":"Artificial intelligence (AI) has demonstrated remarkable success across various applications. In light of this trend, the field of automated trading has developed a keen interest in leveraging AI techniques to forecast the future prices of financial assets. This interest stems from the need to address trading challenges posed by the inherent volatility and dynamic nature of asset prices. However, crafting a flawless strategy becomes a formidable task when dealing with assets characterized by intricate and ever-changing price dynamics. To surmount these formidable challenges, this research employs an innovative rule-based strategy approach to train Deep Reinforcement Learning (DRL). This application is carried out specifically in the context of trading Bitcoin (BTC) and Ripple (XRP). Our proposed approach hinges on the integration of Deep Q-Network, Double Deep Q-Network, Dueling Deep Q-learning networks, alongside the Advantage Actor-Critic algorithms. Each of them aims to yield an optimal policy for our application. To evaluate the effectiveness of our Deep Reinforcement Learning (DRL) approach, we rely on portfolio wealth and the trade signal as performance metrics. The experimental outcomes highlight that Duelling and Double Deep Q-Network outperformed when using XRP with the increasing of the portfolio wealth. All codes are available in this \\href{https://github.com/VerlonRoelMBINGUI/RL_Final_Projects_AMMI2023}{\\color{blue}Github link}.","authors":["Dieu-Donne Fangnon","Armandine Sorel Kouyim Meli","Verlon Roel Mbingui","Phanie Dianelle Negho","Regis Konan Marcel Djaha"],"url":"https://arxiv.org/abs/2505.07660"}
{"created":"2025-05-13","title":"A Case Study Investigating the Role of Generative AI in Quality Evaluations of Epics in Agile Software Development","abstract":"The broad availability of generative AI offers new opportunities to support various work domains, including agile software development. Agile epics are a key artifact for product managers to communicate requirements to stakeholders. However, in practice, they are often poorly defined, leading to churn, delivery delays, and cost overruns. In this industry case study, we investigate opportunities for large language models (LLMs) to evaluate agile epic quality in a global company. Results from a user study with 17 product managers indicate how LLM evaluations could be integrated into their work practices, including perceived values and usage in improving their epics. High levels of satisfaction indicate that agile epics are a new, viable application of AI evaluations. However, our findings also outline challenges, limitations, and adoption barriers that can inform both practitioners and researchers on the integration of such evaluations into future agile work practices.","authors":["Werner Geyer","Jessica He","Daita Sarkar","Michelle Brachman","Chris Hammond","Jennifer Heins","Zahra Ashktorab","Carlos Rosemberg","Charlie Hill"],"url":"https://arxiv.org/abs/2505.07664"}
{"created":"2025-05-13","title":"Intuitive Human-Robot Interfaces Leveraging on Autonomy Features for the Control of Highly-redundant Robots","abstract":"[...] With the TelePhysicalOperation interface, the user can teleoperate the different capabilities of a robot (e.g., single/double arm manipulation, wheel/leg locomotion) by applying virtual forces on selected robot body parts. This approach emulates the intuitiveness of physical human-robot interaction, but at the same time it permits to teleoperate the robot from a safe distance, in a way that resembles a \"Marionette\" interface. The system is further enhanced with wearable haptic feedback functions to align better with the \"Marionette\" metaphor, and a user study has been conducted to validate its efficacy with and without the haptic channel enabled. Considering the importance of robot independence, the TelePhysicalOperation interface incorporates autonomy modules to face, for example, the teleoperation of dual-arm mobile base robots for bimanual object grasping and transportation tasks.","authors":["Davide Torielli"],"url":"https://arxiv.org/abs/2505.07668"}
{"created":"2025-05-13","title":"DATAMUt: Deterministic Algorithms for Time-Delay Attack Detection in Multi-Hop UAV Networks","abstract":"Unmanned Aerial Vehicles (UAVs), also known as drones, have gained popularity in various fields such as agriculture, emergency response, and search and rescue operations. UAV networks are susceptible to several security threats, such as wormhole, jamming, spoofing, and false data injection. Time Delay Attack (TDA) is a unique attack in which malicious UAVs intentionally delay packet forwarding, posing significant threats, especially in time-sensitive applications. It is challenging to distinguish malicious delay from benign network delay due to the dynamic nature of UAV networks, intermittent wireless connectivity, or the Store-Carry-Forward (SCF) mechanism during multi-hop communication. Some existing works propose machine learning-based centralized approaches to detect TDA, which are computationally intensive and have large message overheads. This paper proposes a novel approach DATAMUt, where the temporal dynamics of the network are represented by a weighted time-window graph (TWiG), and then two deterministic polynomial-time algorithms are presented to detect TDA when UAVs have global and local network knowledge. Simulation studies show that the proposed algorithms have reduced message overhead by a factor of five and twelve in global and local knowledge, respectively, compared to existing approaches. Additionally, our approaches achieve approximately 860 and 1050 times less execution time in global and local knowledge, respectively, outperforming the existing methods.","authors":["Keiwan Soltani","Federico Cor\\`o","Punyasha Chatterjee","Sajal K. Das"],"url":"https://arxiv.org/abs/2505.07670"}
{"created":"2025-05-13","title":"Benchmarking Retrieval-Augmented Generation for Chemistry","abstract":"Retrieval-augmented generation (RAG) has emerged as a powerful framework for enhancing large language models (LLMs) with external knowledge, particularly in scientific domains that demand specialized and dynamic information. Despite its promise, the application of RAG in the chemistry domain remains underexplored, primarily due to the lack of high-quality, domain-specific corpora and well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a comprehensive benchmark designed to systematically assess the effectiveness of RAG across a diverse set of chemistry-related tasks. The accompanying chemistry corpus integrates heterogeneous knowledge sources, including scientific literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG toolkit that supports five retrieval algorithms and eight LLMs. Using ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain -- achieving an average relative improvement of 17.4% over direct inference methods. We further conduct in-depth analyses on retriever architectures, corpus selection, and the number of retrieved passages, culminating in practical recommendations to guide future research and deployment of RAG systems in the chemistry domain. The code and data is available at https://chemrag.github.io.","authors":["Xianrui Zhong","Bowen Jin","Siru Ouyang","Yanzhen Shen","Qiao Jin","Yin Fang","Zhiyong Lu","Jiawei Han"],"url":"https://arxiv.org/abs/2505.07671"}
{"created":"2025-05-13","title":"OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit","abstract":"We present OnPrem.LLM, a Python-based toolkit for applying large language models (LLMs) to sensitive, non-public data in offline or restricted environments. The system is designed for privacy-preserving use cases and provides prebuilt pipelines for document processing and storage, retrieval-augmented generation (RAG), information extraction, summarization, classification, and prompt/output processing with minimal configuration. OnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM, and Hugging Face Transformers -- with quantized model support, GPU acceleration, and seamless backend switching. Although designed for fully local execution, OnPrem.LLM also supports integration with a wide range of cloud LLM providers when permitted, enabling hybrid deployments that balance performance with data control. A no-code web interface extends accessibility to non-technical users.","authors":["Arun S. Maiya"],"url":"https://arxiv.org/abs/2505.07672"}
{"created":"2025-05-13","title":"Joint Graph Convolution and Sequential Modeling for Scalable Network Traffic Estimation","abstract":"This study focuses on the challenge of predicting network traffic within complex topological environments. It introduces a spatiotemporal modeling approach that integrates Graph Convolutional Networks (GCN) with Gated Recurrent Units (GRU). The GCN component captures spatial dependencies among network nodes, while the GRU component models the temporal evolution of traffic data. This combination allows for precise forecasting of future traffic patterns. The effectiveness of the proposed model is validated through comprehensive experiments on the real-world Abilene network traffic dataset. The model is benchmarked against several popular deep learning methods. Furthermore, a set of ablation experiments is conducted to examine the influence of various components on performance, including changes in the number of graph convolution layers, different temporal modeling strategies, and methods for constructing the adjacency matrix. Results indicate that the proposed approach achieves superior performance across multiple metrics, demonstrating robust stability and strong generalization capabilities in complex network traffic forecasting scenarios.","authors":["Nan Jiang","Wenxuan Zhu","Xu Han","Weiqiang Huang","Yumeng Sun"],"url":"https://arxiv.org/abs/2505.07674"}
{"created":"2025-05-13","title":"Simple Semi-supervised Knowledge Distillation from Vision-Language Models via $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead $\\mathbf{\\texttt{O}}$ptimization","abstract":"Vision-language models (VLMs) have achieved remarkable success across diverse tasks by leveraging rich textual information with minimal labeled data. However, deploying such large models remains challenging, particularly in resource-constrained environments. Knowledge distillation (KD) offers a well-established solution to this problem; however, recent KD approaches from VLMs often involve multi-stage training or additional tuning, increasing computational overhead and optimization complexity. In this paper, we propose $\\mathbf{\\texttt{D}}$ual-$\\mathbf{\\texttt{H}}$ead $\\mathbf{\\texttt{O}}$ptimization ($\\mathbf{\\texttt{DHO}}$) -- a simple yet effective KD framework that transfers knowledge from VLMs to compact, task-specific models in semi-supervised settings. Specifically, we introduce dual prediction heads that independently learn from labeled data and teacher predictions, and propose to linearly combine their outputs during inference. We observe that $\\texttt{DHO}$ mitigates gradient conflicts between supervised and distillation signals, enabling more effective feature learning than single-head KD baselines. As a result, extensive experiments show that $\\texttt{DHO}$ consistently outperforms baselines across multiple domains and fine-grained datasets. Notably, on ImageNet, it achieves state-of-the-art performance, improving accuracy by 3% and 0.1% with 1% and 10% labeled data, respectively, while using fewer parameters.","authors":["Seongjae Kang","Dong Bok Lee","Hyungjoon Jang","Sung Ju Hwang"],"url":"https://arxiv.org/abs/2505.07675"}
{"created":"2025-05-13","title":"SpecRouter: Adaptive Routing for Multi-Level Speculative Decoding in Large Language Models","abstract":"Large Language Models (LLMs) present a critical trade-off between inference quality and computational cost: larger models offer superior capabilities but incur significant latency, while smaller models are faster but less powerful. Existing serving strategies often employ fixed model scales or static two-stage speculative decoding, failing to dynamically adapt to the varying complexities of user requests or fluctuations in system performance. This paper introduces \\systemname{}, a novel framework that reimagines LLM inference as an adaptive routing problem solved through multi-level speculative decoding. \\systemname{} dynamically constructs and optimizes inference \"paths\" (chains of models) based on real-time feedback, addressing the limitations of static approaches. Our contributions are threefold: (1) An \\textbf{adaptive model chain scheduling} mechanism that leverages performance profiling (execution times) and predictive similarity metrics (derived from token distribution divergence) to continuously select the optimal sequence of draft and verifier models, minimizing predicted latency per generated token. (2) A \\textbf{multi-level collaborative verification} framework where intermediate models within the selected chain can validate speculative tokens, reducing the verification burden on the final, most powerful target model. (3) A \\textbf{synchronized state management} system providing efficient, consistent KV cache handling across heterogeneous models in the chain, including precise, low-overhead rollbacks tailored for asynchronous batch processing inherent in multi-level speculation. Preliminary experiments demonstrate the validity of our method.","authors":["Hang Wu","Jianian Zhu","Yinghui Li","Haojie Wang","Biao Hou","Jidong Zhai"],"url":"https://arxiv.org/abs/2505.07680"}
{"created":"2025-05-13","title":"Verified Purely Functional Catenable Real-Time Deques","abstract":"We present OCaml and Rocq implementations of Kaplan and Tarjan's purely functional, real-time catenable deques. The correctness of our Rocq implementation is machine-checked.","authors":["Jules Viennot","Arthur Wendling","Arma\\\"el Gu\\'eneau","Fran\\c{c}ois Pottier"],"url":"https://arxiv.org/abs/2505.07681"}
{"created":"2025-05-13","title":"Multimodal Survival Modeling in the Age of Foundation Models","abstract":"The Cancer Genome Atlas (TCGA) has enabled novel discoveries and served as a large-scale reference through its harmonized genomics, clinical, and image data. Prior studies have trained bespoke cancer survival prediction models from unimodal or multimodal TCGA data. A modern paradigm in biomedical deep learning is the development of foundation models (FMs) to derive meaningful feature embeddings, agnostic to a specific modeling task. Biomedical text especially has seen growing development of FMs. While TCGA contains free-text data as pathology reports, these have been historically underutilized. Here, we investigate the feasibility of training classical, multimodal survival models over zero-shot embeddings extracted by FMs. We show the ease and additive effect of multimodal fusion, outperforming unimodal models. We demonstrate the benefit of including pathology report text and rigorously evaluate the effect of model-based text summarization and hallucination. Overall, we modernize survival modeling by leveraging FMs and information extraction from pathology reports.","authors":["Steven Song","Morgan Borjigin-Wang","Irene Madejski","Robert L. Grossman"],"url":"https://arxiv.org/abs/2505.07683"}
{"created":"2025-05-13","title":"S-GRPO: Early Exit via Reinforcement Learning in Reasoning Models","abstract":"As Test-Time Scaling emerges as an active research focus in the large language model community, advanced post-training methods increasingly emphasize extending chain-of-thought (CoT) generation length, thereby enhancing reasoning capabilities to approach Deepseek R1-like reasoning models. However, recent studies reveal that reasoning models (even Qwen3) consistently exhibit excessive thought redundancy in CoT generation. This overthinking problem stems from conventional outcome-reward reinforcement learning's systematic neglect in regulating intermediate reasoning steps. This paper proposes Serial-Group Decaying-Reward Policy Optimization (namely S-GRPO), a novel reinforcement learning method that empowers models with the capability to determine the sufficiency of reasoning steps, subsequently triggering early exit of CoT generation. Specifically, unlike GRPO, which samples multiple possible completions (parallel group) in parallel, we select multiple temporal positions in the generation of one CoT to allow the model to exit thinking and instead generate answers (serial group), respectively. For the correct answers in a serial group, we assign rewards that decay according to positions, with lower rewards towards the later ones, thereby reinforcing the model's behavior to generate higher-quality answers at earlier phases with earlier exits of thinking. Empirical evaluations demonstrate compatibility with state-of-the-art reasoning models, including Qwen3 and Deepseek-distill models, achieving 35.4% ~ 61.1\\% sequence length reduction with 0.72% ~ 6.08% accuracy improvements across GSM8K, AIME 2024, AMC 2023, MATH-500, and GPQA Diamond benchmarks.","authors":["Muzhi Dai","Chenxu Yang","Qingyi Si"],"url":"https://arxiv.org/abs/2505.07686"}
{"created":"2025-05-13","title":"Heterogeneous Data Game: Characterizing the Model Competition Across Multiple Data Sources","abstract":"Data heterogeneity across multiple sources is common in real-world machine learning (ML) settings. Although many methods focus on enabling a single model to handle diverse data, real-world markets often comprise multiple competing ML providers. In this paper, we propose a game-theoretic framework -- the Heterogeneous Data Game -- to analyze how such providers compete across heterogeneous data sources. We investigate the resulting pure Nash equilibria (PNE), showing that they can be non-existent, homogeneous (all providers converge on the same model), or heterogeneous (providers specialize in distinct data sources). Our analysis spans monopolistic, duopolistic, and more general markets, illustrating how factors such as the \"temperature\" of data-source choice models and the dominance of certain data sources shape equilibrium outcomes. We offer theoretical insights into both homogeneous and heterogeneous PNEs, guiding regulatory policies and practical strategies for competitive ML marketplaces.","authors":["Renzhe Xu","Kang Wang","Bo Li"],"url":"https://arxiv.org/abs/2505.07688"}
{"created":"2025-05-13","title":"Anatomical Attention Alignment representation for Radiology Report Generation","abstract":"Automated Radiology report generation (RRG) aims at producing detailed descriptions of medical images, reducing radiologists' workload and improving access to high-quality diagnostic services. Existing encoder-decoder models only rely on visual features extracted from raw input images, which can limit the understanding of spatial structures and semantic relationships, often resulting in suboptimal text generation. To address this, we propose Anatomical Attention Alignment Network (A3Net), a framework that enhance visual-textual understanding by constructing hyper-visual representations. Our approach integrates a knowledge dictionary of anatomical structures with patch-level visual features, enabling the model to effectively associate image regions with their corresponding anatomical entities. This structured representation improves semantic reasoning, interpretability, and cross-modal alignment, ultimately enhancing the accuracy and clinical relevance of generated reports. Experimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net significantly improves both visual perception and text generation quality. Our code is available at \\href{https://github.com/Vinh-AI/A3Net}{GitHub}.","authors":["Quang Vinh Nguyen","Minh Duc Nguyen","Thanh Hoang Son Vo","Hyung-Jeong Yang","Soo-Hyung Kim"],"url":"https://arxiv.org/abs/2505.07689"}
{"created":"2025-05-13","title":"Beyond CLIP Generalization: Against Forward&Backward Forgetting Adapter for Continual Learning of Vision-Language Models","abstract":"This study aims to address the problem of multi-domain task incremental learning~(MTIL), which requires that vision-language models~(VLMs) continuously acquire new knowledge while maintaining their inherent zero-shot recognition capability. Existing paradigms delegate the testing of unseen-domain samples to the original CLIP, which only prevents the degradation of the model's zero-shot capability but fails to enhance the generalization of the VLM further. To this end, we propose a novel MTIL framework, named AFA, which comprises two core modules: (1) an against forward-forgetting adapter that learns task-invariant information for each dataset in the incremental tasks to enhance the zero-shot recognition ability of VLMs; (2) an against backward-forgetting adapter that strengthens the few-shot learning capability of VLMs while supporting incremental learning. Extensive experiments demonstrate that the AFA method significantly outperforms existing state-of-the-art approaches, especially in few-shot MTIL tasks, and surpasses the inherent zero-shot performance of CLIP in terms of transferability. The code is provided in the Supplementary Material.","authors":["Songlin Dong","Chenhao Ding","Jiangyang Li","Jizhou Han","Qiang Wang","Yuhang He","Yihong Gong"],"url":"https://arxiv.org/abs/2505.07690"}
{"created":"2025-05-13","title":"Feedback-Driven Pseudo-Label Reliability Assessment: Redefining Thresholding for Semi-Supervised Semantic Segmentation","abstract":"Semi-supervised learning leverages unlabeled data to enhance model performance, addressing the limitations of fully supervised approaches. Among its strategies, pseudo-supervision has proven highly effective, typically relying on one or multiple teacher networks to refine pseudo-labels before training a student network. A common practice in pseudo-supervision is filtering pseudo-labels based on pre-defined confidence thresholds or entropy. However, selecting optimal thresholds requires large labeled datasets, which are often scarce in real-world semi-supervised scenarios. To overcome this challenge, we propose Ensemble-of-Confidence Reinforcement (ENCORE), a dynamic feedback-driven thresholding strategy for pseudo-label selection. Instead of relying on static confidence thresholds, ENCORE estimates class-wise true-positive confidence within the unlabeled dataset and continuously adjusts thresholds based on the model's response to different levels of pseudo-label filtering. This feedback-driven mechanism ensures the retention of informative pseudo-labels while filtering unreliable ones, enhancing model training without manual threshold tuning. Our method seamlessly integrates into existing pseudo-supervision frameworks and significantly improves segmentation performance, particularly in data-scarce conditions. Extensive experiments demonstrate that integrating ENCORE with existing pseudo-supervision frameworks enhances performance across multiple datasets and network architectures, validating its effectiveness in semi-supervised learning.","authors":["Negin Ghamsarian","Sahar Nasirihaghighi","Klaus Schoeffmann","Raphael Sznitman"],"url":"https://arxiv.org/abs/2505.07691"}
{"created":"2025-05-13","title":"ABase: the Multi-Tenant NoSQL Serverless Database for Diverse and Dynamic Workloads in Large-scale Cloud Environments","abstract":"Multi-tenant architectures enhance the elasticity and resource utilization of NoSQL databases by allowing multiple tenants to co-locate and share resources. However, in large-scale cloud environments, the diverse and dynamic nature of workloads poses significant challenges for multi-tenant NoSQL databases. Based on our practical observations, we have identified three crucial challenges: (1) the impact of caching on performance isolation, as cache hits alter request execution and resource consumption, leading to inaccurate traffic control; (2) the dynamic changes in traffic, with changes in tenant traffic trends causing throttling or resource wastage, and changes in access distribution causing hot key pressure or cache hit ratio drops; and (3) the imbalanced layout of data nodes due to tenants' diverse resource requirements, leading to low resource utilization. To address these challenges, we introduce ABase, a multi-tenant NoSQL serverless database developed at ByteDance. ABase introduces a two-layer caching mechanism with a cache-aware isolation mechanism to ensure accurate resource consumption estimates. Furthermore, ABase employs a predictive autoscaling policy to dynamically adjust resources in response to tenant traffic changes and a multi-resource rescheduling algorithm to balance resource utilization across data nodes. With these innovations, ABase has successfully served ByteDance's large-scale cloud environment, supporting a total workload that has achieved a peak QPS of over 13 billion and total storage exceeding 1 EB.","authors":["Rong Kang","Yanbin Chen","Ye Liu","Fuxin Jiang","Qingshuo Li","Miao Ma","Jian Liu","Guangliang Zhao","Tieying Zhang","Jianjun Chen","Lei Zhang"],"url":"https://arxiv.org/abs/2505.07692"}
{"created":"2025-05-13","title":"Belief Injection for Epistemic Control in Linguistic State Space","abstract":"This work introduces belief injection, a proactive epistemic control mechanism for artificial agents whose cognitive states are structured as dynamic ensembles of linguistic belief fragments. Grounded in the Semantic Manifold framework, belief injection directly incorporates targeted linguistic beliefs into an agent's internal cognitive state, influencing reasoning and alignment proactively rather than reactively. We delineate various injection strategies, such as direct, context-aware, goal-oriented, and reflective approaches, and contrast belief injection with related epistemic control mechanisms, notably belief filtering. Additionally, this work discusses practical applications, implementation considerations, ethical implications, and outlines promising directions for future research into cognitive governance using architecturally embedded belief injection.","authors":["Sebastian Dumbrava"],"url":"https://arxiv.org/abs/2505.07693"}
{"created":"2025-05-13","title":"FD-RIO: Fast Dense Radar Inertial Odometry","abstract":"Radar-based odometry is a popular solution for ego-motion estimation in conditions where other exteroceptive sensors may degrade, whether due to poor lighting or challenging weather conditions; however, scanning radars have the downside of relatively lower sampling rate and spatial resolution. In this work, we present FD-RIO, a method to alleviate this problem by fusing noisy, drift-prone, but high-frequency IMU data with dense radar scans. To the best of our knowledge, this is the first attempt to fuse dense scanning radar odometry with IMU using a Kalman filter. We evaluate our methods using two publicly available datasets and report accuracies using standard KITTI evaluation metrics, in addition to ablation tests and runtime analysis. Our phase correlation -based approach is compact, intuitive, and is designed to be a practical solution deployable on a realistic hardware setup of a mobile platform. Despite its simplicity, FD-RIO is on par with other state-of-the-art methods and outperforms in some test sequences.","authors":["Nader J. Abu-Alrub","Nathir A. Rawashdeh"],"url":"https://arxiv.org/abs/2505.07694"}
{"created":"2025-05-13","title":"PatchTrack: A Comprehensive Analysis of ChatGPT's Influence on Pull Request Outcomes","abstract":"The rapid adoption of large language models (LLMs) like ChatGPT in software development has introduced new ways for developers to interact with AI, particularly in pull request workflows. While prior research has examined AI-generated code quality, there is limited understanding of how ChatGPT is utilized in real-world pull request decision-making and how its suggestions influence patch integration and rejection. To explore these aspects, we analyze self-admitted ChatGPT usage (SACU), where developers explicitly disclose their reliance on ChatGPT within pull request discussions. Our study examines 338 pull requests (285 merged, 53 closed) across 255 GitHub repositories, containing 645 ChatGPT-generated code snippets and 3,486 patches. We introduce PatchTrack, a classification tool that determines whether ChatGPT-generated patches were applied (PA, 115 cases), not applied (PN, 64 cases), or not suggested (NE, 106 cases). Our findings reveal that full adoption of ChatGPT-generated code is rare, developers frequently modify or selectively integrate AI-generated patches to align with project constraints, with a median integration rate of 25%. Through qualitative analysis, we identify key factors influencing patch integration and pull request rejection, including scope misalignment, maintainability concerns, redundant solutions, and procedural barriers such as incomplete documentation or administrative policies. By providing empirical insights into ChatGPT's role in pull request workflows, this study informs developers, maintainers, and educators on the evolving use of generative AI in collaborative software development. It also lays the groundwork for future research on optimizing AI-assisted development, improving transparency in AI adoption, and enhancing patch integration workflows.","authors":["Daniel Ogenrwot","John Businge"],"url":"https://arxiv.org/abs/2505.07700"}
{"created":"2025-05-13","title":"Lightweight End-to-end Text-to-speech Synthesis for low resource on-device applications","abstract":"Recent works have shown that modelling raw waveform directly from text in an end-to-end (E2E) fashion produces more natural-sounding speech than traditional neural text-to-speech (TTS) systems based on a cascade or two-stage approach. However, current E2E state-of-the-art models are computationally complex and memory-consuming, making them unsuitable for real-time offline on-device applications in low-resource scenarios. To address this issue, we propose a Lightweight E2E-TTS (LE2E) model that generates high-quality speech requiring minimal computational resources. We evaluate the proposed model on the LJSpeech dataset and show that it achieves state-of-the-art performance while being up to $90\\%$ smaller in terms of model parameters and $10\\times$ faster in real-time-factor. Furthermore, we demonstrate that the proposed E2E training paradigm achieves better quality compared to an equivalent architecture trained in a two-stage approach. Our results suggest that LE2E is a promising approach for developing real-time, high quality, low-resource TTS applications for on-device applications.","authors":["Biel Tura Vecino","Adam Gabry\\'s","Daniel M\\k{a}twicki","Andrzej Pomirski","Tom Iddon","Marius Cotescu","Jaime Lorenzo-Trueba"],"url":"https://arxiv.org/abs/2505.07701"}
{"created":"2025-05-13","title":"4TaStiC: Time and trend traveling time series clustering for classifying long-term type 2 diabetes patients","abstract":"Diabetes is one of the most prevalent diseases worldwide, characterized by persistently high blood sugar levels, capable of damaging various internal organs and systems. Diabetes patients require routine check-ups, resulting in a time series of laboratory records, such as hemoglobin A1c, which reflects each patient's health behavior over time and informs their doctor's recommendations. Clustering patients into groups based on their entire time series data assists doctors in making recommendations and choosing treatments without the need to review all records. However, time series clustering of this type of dataset introduces some challenges; patients visit their doctors at different time points, making it difficult to capture and match trends, peaks, and patterns. Additionally, two aspects must be considered: differences in the levels of laboratory results and differences in trends and patterns. To address these challenges, we introduce a new clustering algorithm called Time and Trend Traveling Time Series Clustering (4TaStiC), using a base dissimilarity measure combined with Euclidean and Pearson correlation metrics. We evaluated this algorithm on artificial datasets, comparing its performance with that of seven existing methods. The results show that 4TaStiC outperformed the other methods on the targeted datasets. Finally, we applied 4TaStiC to cluster a cohort of 1,989 type 2 diabetes patients at Siriraj Hospital. Each group of patients exhibits clear characteristics that will benefit doctors in making efficient clinical decisions. Furthermore, the proposed algorithm can be applied to contexts outside the medical field.","authors":["Onthada Preedasawakul","Nathakhun Wiroonsri"],"url":"https://arxiv.org/abs/2505.07702"}
{"created":"2025-05-13","title":"Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images","abstract":"Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLMs to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.","authors":["Elisei Rykov","Kseniia Petrushina","Kseniia Titova","Anton Razzhigaev","Alexander Panchenko","Vasily Konovalov"],"url":"https://arxiv.org/abs/2505.07704"}
{"created":"2025-05-13","title":"Codifying Character Logic in Role-Playing","abstract":"This paper introduces Codified Profiles for role-playing, a novel approach that represents character logic as structured, executable functions for behavioral decision-making. Each profile defines a set of functions parse_by_scene(scene) that outputs a list of logic-grounded assertions triggered_statements, using both explicit control structures (e.g., if-then-else) and condition checks like check_condition(scene, question), where each question is a semantically meaningful prompt about the scene (e.g., \"Is the character in danger?\") discriminated by the role-playing LLM as true, false, or unknown. This explicit representation offers three key advantages over traditional prompt-based profiles, which append character descriptions directly into text prompts: (1) Persistence, by enforcing complete and consistent execution of character logic, rather than relying on the model's implicit reasoning; (2) Updatability, through systematic inspection and revision of behavioral logic, which is difficult to track or debug in prompt-only approaches; (3) Controllable Randomness, by supporting stochastic behavior directly within the logic, enabling fine-grained variability that prompting alone struggles to achieve. To validate these advantages, we introduce a new benchmark constructed from 83 characters and 5,141 scenes curated from Fandom, using NLI-based scoring to compare character responses against ground-truth actions. Our experiments demonstrate the significant benefits of codified profiles in improving persistence, updatability, and behavioral diversity. Notably, by offloading a significant portion of reasoning to preprocessing, codified profiles enable even 1B-parameter models to perform high-quality role-playing, providing a scalable and efficient foundation for local deployment of role-play agents.","authors":["Letian Peng","Jingbo Shang"],"url":"https://arxiv.org/abs/2505.07705"}
{"created":"2025-05-13","title":"ISAC: An Invertible and Stable Auditory Filter Bank with Customizable Kernels for ML Integration","abstract":"This paper introduces ISAC, an invertible and stable, perceptually-motivated filter bank that is specifically designed to be integrated into machine learning paradigms. More precisely, the center frequencies and bandwidths of the filters are chosen to follow a non-linear, auditory frequency scale, the filter kernels have user-defined maximum temporal support and may serve as learnable convolutional kernels, and there exists a corresponding filter bank such that both form a perfect reconstruction pair. ISAC provides a powerful and user-friendly audio front-end suitable for any application, including analysis-synthesis schemes.","authors":["Daniel Haider","Felix Perfler","Peter Balazs","Clara Hollomey","Nicki Holighaus"],"url":"https://arxiv.org/abs/2505.07709"}
{"created":"2025-05-13","title":"Hybrid Control Strategies for Safe and Adaptive Robot-Assisted Dressing","abstract":"Safety, reliability, and user trust are crucial in human-robot interaction (HRI) where the robots must address hazards in real-time. This study presents hazard driven low-level control strategies implemented in robot-assisted dressing (RAD) scenarios where hazards like garment snags and user discomfort in real-time can affect task performance and user safety. The proposed control mechanisms include: (1) Garment Snagging Control Strategy, which detects excessive forces and either seeks user intervention via a chatbot or autonomously adjusts its trajectory, and (2) User Discomfort/Pain Mitigation Strategy, which dynamically reduces velocity based on user feedback and aborts the task if necessary. We used physical dressing trials in order to evaluate these control strategies. Results confirm that integrating force monitoring with user feedback improves safety and task continuity. The findings emphasise the need for hybrid approaches that balance autonomous intervention, user involvement, and controlled task termination, supported by bi-directional interaction and real-time user-driven adaptability, paving the way for more responsive and personalised HRI systems.","authors":["Yasmin Rafiq","Baslin A. James","Ke Xu","Robert M. Hierons","Sanja Dogramadzi"],"url":"https://arxiv.org/abs/2505.07710"}
{"created":"2025-05-13","title":"Circuit Partitioning Using Large Language Models for Quantum Compilation and Simulations","abstract":"We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where quantum computers are limited by noisy gates, some of which are more error-prone than others and can render the final computation incomprehensible. Quantum circuit compilation algorithms attempt to minimize these noisy gates when mapping quantum algorithms onto quantum hardware but face computational challenges that restrict their application to circuits with no more than 5-6 qubits, necessitating the need to partition large circuits before the application of noisy quantum gate minimization algorithms. The existing generation of these algorithms is heuristic in nature and does not account for downstream gate minimization tasks. Large language models (LLMs) have the potential to change this and help improve quantum circuit partitions. This paper investigates the use of LLMs, such as Llama and Mistral, for partitioning quantum circuits by capitalizing on their abilities to understand and generate code, including QASM. Specifically, we teach LLMs to partition circuits using the quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through experimental evaluations, we show that careful fine-tuning of open source LLMs enables us to obtain an accuracy of 53.4% for the partition task while over-the-shelf LLMs are unable to correctly partition circuits, using standard 1-shot and few-shot training approaches.","authors":["Pranav Sinha","Sumit Kumar Jha","Sunny Raj"],"url":"https://arxiv.org/abs/2505.07711"}
{"created":"2025-05-13","title":"Routing Attacks in Ethereum PoS: A Systematic Exploration","abstract":"With the promise of greater decentralization and sustainability, Ethereum transitioned from a Proof-of-Work (PoW) to a Proof-of-Stake (PoS) consensus mechanism. The new consensus protocol introduces novel vulnerabilities that warrant further investigation. The goal of this paper is to investigate the security of Ethereum's PoS system from an Internet routing perspective.","authors":["Constantine Doumanidis","Maria Apostolaki"],"url":"https://arxiv.org/abs/2505.07713"}
{"created":"2025-05-13","title":"Hybrid Spiking Vision Transformer for Object Detection with Event Cameras","abstract":"Event-based object detection has gained increasing attention due to its advantages such as high temporal resolution, wide dynamic range, and asynchronous address-event representation. Leveraging these advantages, Spiking Neural Networks (SNNs) have emerged as a promising approach, offering low energy consumption and rich spatiotemporal dynamics. To further enhance the performance of event-based object detection, this study proposes a novel hybrid spike vision Transformer (HsVT) model. The HsVT model integrates a spatial feature extraction module to capture local and global features, and a temporal feature extraction module to model time dependencies and long-term patterns in event sequences. This combination enables HsVT to capture spatiotemporal features, improving its capability to handle complex event-based object detection tasks. To support research in this area, we developed and publicly released The Fall Detection Dataset as a benchmark for event-based object detection tasks. This dataset, captured using an event-based camera, ensures facial privacy protection and reduces memory usage due to the event representation format. We evaluated the HsVT model on GEN1 and Fall Detection datasets across various model sizes. Experimental results demonstrate that HsVT achieves significant performance improvements in event detection with fewer parameters.","authors":["Qi Xu","Jie Deng","Jiangrong Shen","Biwu Chen","Huajin Tang","Gang Pan"],"url":"https://arxiv.org/abs/2505.07715"}
{"created":"2025-05-13","title":"Gameplay Highlights Generation","abstract":"In this work, we enable gamers to share their gaming experience on social media by automatically generating eye-catching highlight reels from their gameplay session Our automation will save time for gamers while increasing audience engagement. We approach the highlight generation problem by first identifying intervals in the video where interesting events occur and then concatenate them. We developed an in-house gameplay event detection dataset containing interesting events annotated by humans using VIA video annotator. Traditional techniques for highlight detection such as game engine integration requires expensive collaboration with game developers. OCR techniques which detect patches of specific images or texts require expensive per game engineering and may not generalize across game UI and different language. We finetuned a multimodal general purpose video understanding model such as X-CLIP using our dataset which generalizes across multiple games in a genre without per game engineering. Prompt engineering was performed to improve the classification performance of this multimodal model. Our evaluation showed that such a finetuned model can detect interesting events in first person shooting games from unseen gameplay footage with more than 90% accuracy. Moreover, our model performed significantly better on low resource games (small dataset) when trained along with high resource games, showing signs of transfer learning. To make the model production ready, we used ONNX libraries to enable cross platform inference. These libraries also provide post training quantization tools to reduce model size and inference time for deployment. ONNX runtime libraries with DirectML backend were used to perform efficient inference on Windows OS. We show that natural language supervision in the X-CLIP model leads to data efficient and highly performant video recognition models.","authors":["Vignesh Edithal","Le Zhang","Ilia Blank","Imran Junejo"],"url":"https://arxiv.org/abs/2505.07721"}
{"created":"2025-05-13","title":"Securing WiFi Fingerprint-based Indoor Localization Systems from Malicious Access Points","abstract":"WiFi fingerprint-based indoor localization schemes deliver highly accurate location data by matching the received signal strength indicator (RSSI) with an offline database using machine learning (ML) or deep learning (DL) models. However, over time, RSSI values degrade due to the malicious behavior of access points (APs), causing low positional accuracy due to RSSI value mismatch with the offline database. Existing literature lacks detection of malicious APs in the online phase and mitigating their effects. This research addresses these limitations and proposes a long-term reliable indoor localization scheme by incorporating malicious AP detection and their effect mitigation techniques. The proposed scheme uses a Light Gradient-Boosting Machine (LGBM) classifier to estimate locations and integrates simple yet efficient techniques to detect malicious APs based on online query data. Subsequently, a mitigation technique is incorporated that updates the offline database and online queries by imputing stable values for malicious APs using LGBM Regressors. Additionally, we introduce a noise addition mechanism in the offline database to capture the dynamic environmental effects. Extensive experimental evaluation shows that the proposed scheme attains a detection accuracy above 95% for each attack type. The mitigation strategy effectively restores the system's performance nearly to its original state when no malicious AP is present. The noise addition module reduces localization errors by nearly 16%. Furthermore, the proposed solution is lightweight, reducing the execution time by approximately 94% compared to the existing methods.","authors":["Fariha Tanjim Shifat","Sayma Sarwar Ela","Mosarrat Jahan"],"url":"https://arxiv.org/abs/2505.07724"}
{"created":"2025-05-13","title":"Mode Mismatch Mitigation in Gaussian-Modulated CV-QKD","abstract":"Technical limitations in pulse shaping lead to mode mismatch, which significantly reduces the secure key rate in CV-QKD systems. To address this, a machine learning approach is employed to optimize the transmitter pulse-shape, effectively minimizing mode mismatch and yielding substantial performance improvements.","authors":["Svitlana Matsenko","Amirhossein Ghazisaeidi","Marcin Jarzyna","Mikkel N. Schmidt","S{\\o}ren F. Nielsen","Konrad Banaszek","Darko Zibar"],"url":"https://arxiv.org/abs/2505.07726"}
{"created":"2025-05-13","title":"Guiding Data Collection via Factored Scaling Curves","abstract":"Generalist imitation learning policies trained on large datasets show great promise for solving diverse manipulation tasks. However, to ensure generalization to different conditions, policies need to be trained with data collected across a large set of environmental factor variations (e.g., camera pose, table height, distractors) $-$ a prohibitively expensive undertaking, if done exhaustively. We introduce a principled method for deciding what data to collect and how much to collect for each factor by constructing factored scaling curves (FSC), which quantify how policy performance varies as data scales along individual or paired factors. These curves enable targeted data acquisition for the most influential factor combinations within a given budget. We evaluate the proposed method through extensive simulated and real-world experiments, across both training-from-scratch and fine-tuning settings, and show that it boosts success rates in real-world tasks in new environments by up to 26% over existing data-collection strategies. We further demonstrate how factored scaling curves can effectively guide data collection using an offline metric, without requiring real-world evaluation at scale.","authors":["Lihan Zha","Apurva Badithela","Michael Zhang","Justin Lidard","Jeremy Bao","Emily Zhou","David Snyder","Allen Z. Ren","Dhruv Shah","Anirudha Majumdar"],"url":"https://arxiv.org/abs/2505.07728"}
{"created":"2025-05-13","title":"Reproducibility, Replicability, and Insights into Visual Document Retrieval with Late Interaction","abstract":"Visual Document Retrieval (VDR) is an emerging research area that focuses on encoding and retrieving document images directly, bypassing the dependence on Optical Character Recognition (OCR) for document search. A recent advance in VDR was introduced by ColPali, which significantly improved retrieval effectiveness through a late interaction mechanism. ColPali's approach demonstrated substantial performance gains over existing baselines that do not use late interaction on an established benchmark. In this study, we investigate the reproducibility and replicability of VDR methods with and without late interaction mechanisms by systematically evaluating their performance across multiple pre-trained vision-language models. Our findings confirm that late interaction yields considerable improvements in retrieval effectiveness; however, it also introduces computational inefficiencies during inference. Additionally, we examine the adaptability of VDR models to textual inputs and assess their robustness across text-intensive datasets within the proposed benchmark, particularly when scaling the indexing mechanism. Furthermore, our research investigates the specific contributions of late interaction by looking into query-patch matching in the context of visual document retrieval. We find that although query tokens cannot explicitly match image patches as in the text retrieval scenario, they tend to match the patch contains visually similar tokens or their surrounding patches.","authors":["Jingfen Qiao","Jia-Huei Ju","Xinyu Ma","Evangelos Kanoulas","Andrew Yates"],"url":"https://arxiv.org/abs/2505.07730"}
{"created":"2025-05-13","title":"Spoken Language Understanding on Unseen Tasks With In-Context Learning","abstract":"Spoken language understanding (SLU) tasks involve diverse skills that probe the information extraction, classification and/or generation capabilities of models. In this setting, task-specific training data may not always be available. While traditional task-specific SLU models are unable to cater to such requirements, the speech-text large language models (LLMs) offer a promising alternative with emergent abilities. However, out of-the-box, our evaluations indicate that the zero/few-shot performance of prominent open-source speech-text LLMs on SLU tasks are not up to the mark. In this paper, we introduce a novel approach to robust task-agnostic fine-tuning using randomized class labels. With this proposed fine-tuning, we illustrate that the performance of the speech-text LLMs on an unseen task is significantly improved over standard approaches. Critically, the proposed approach avoids the requirement of task-specific data annotations for enabling new tasks in speech-text LLMs.","authors":["Neeraj Agrawal","Sriram Ganapathy"],"url":"https://arxiv.org/abs/2505.07731"}
{"created":"2025-05-13","title":"Non-Conservative Data-driven Safe Control Design for Nonlinear Systems with Polyhedral Safe Sets","abstract":"This paper presents a data-driven nonlinear safe control design approach for discrete-time systems under parametric uncertainties and additive disturbances. We first characterize a new control structure from which a data-based representation of closed-loop systems is obtained. This data-based closed-loop system is composed of two parts: 1) a parametrized linear closed-loop part and a parametrized nonlinear remainder closed-loop part. We show that using the standard practice or learning a robust controller to ensure safety while treating the remaining nonlinearities as disturbances brings about significant challenges in terms of computational complexity and conservatism. To overcome these challenges, we develop a novel nonlinear safe control design approach in which the closed-loop nonlinear remainders are learned, rather than canceled, in a control-oriented fashion while preserving the computational efficiency. To this end, a primal-dual optimization framework is leveraged in which the control gains are learned to enforce the second-order optimality on the closed-loop nonlinear remainders. This allows us to account for nonlinearities in the design for the sake of safety rather than treating them as disturbances. This new controller parameterization and design approach reduces the computational complexity and the conservatism of designing a safe nonlinear controller. A simulation example is then provided to show the effectiveness of the proposed data-driven controller.","authors":["Amir Modares","Bosen Lian","Hamidreza Modares"],"url":"https://arxiv.org/abs/2505.07733"}
{"created":"2025-05-13","title":"LAMM-ViT: AI Face Detection via Layer-Aware Modulation of Region-Guided Attention","abstract":"Detecting AI-synthetic faces presents a critical challenge: it is hard to capture consistent structural relationships between facial regions across diverse generation techniques. Current methods, which focus on specific artifacts rather than fundamental inconsistencies, often fail when confronted with novel generative models. To address this limitation, we introduce Layer-aware Mask Modulation Vision Transformer (LAMM-ViT), a Vision Transformer designed for robust facial forgery detection. This model integrates distinct Region-Guided Multi-Head Attention (RG-MHA) and Layer-aware Mask Modulation (LAMM) components within each layer. RG-MHA utilizes facial landmarks to create regional attention masks, guiding the model to scrutinize architectural inconsistencies across different facial areas. Crucially, the separate LAMM module dynamically generates layer-specific parameters, including mask weights and gating values, based on network context. These parameters then modulate the behavior of RG-MHA, enabling adaptive adjustment of regional focus across network depths. This architecture facilitates the capture of subtle, hierarchical forgery cues ubiquitous among diverse generation techniques, such as GANs and Diffusion Models. In cross-model generalization tests, LAMM-ViT demonstrates superior performance, achieving 94.09% mean ACC (a +5.45% improvement over SoTA) and 98.62% mean AP (a +3.09% improvement). These results demonstrate LAMM-ViT's exceptional ability to generalize and its potential for reliable deployment against evolving synthetic media threats.","authors":["Jiangling Zhang","Weijie Zhu","Jirui Huang","Yaxiong Chen"],"url":"https://arxiv.org/abs/2505.07734"}
{"created":"2025-05-13","title":"Assessing the Chemical Intelligence of Large Language Models","abstract":"Large Language Models are versatile, general-purpose tools with a wide range of applications. Recently, the advent of \"reasoning models\" has led to substantial improvements in their abilities in advanced problem-solving domains such as mathematics and software engineering. In this work, we assessed the ability of reasoning models to directly perform chemistry tasks, without any assistance from external tools. We created a novel benchmark, called ChemIQ, which consists of 796 questions assessing core concepts in organic chemistry, focused on molecular comprehension and chemical reasoning. Unlike previous benchmarks, which primarily use multiple choice formats, our approach requires models to construct short-answer responses, more closely reflecting real-world applications. The reasoning models, exemplified by OpenAI's o3-mini, correctly answered 28%-59% of questions depending on the reasoning level used, with higher reasoning levels significantly increasing performance on all tasks. These models substantially outperformed the non-reasoning model, GPT-4o, which achieved only 7% accuracy. We found that Large Language Models can now convert SMILES strings to IUPAC names, a task earlier models were unable to perform. Additionally, we show that the latest reasoning models can elucidate structures from 1H and 13C NMR data, correctly generating SMILES strings for 74% of molecules containing up to 10 heavy atoms, and in one case solving a structure comprising 21 heavy atoms. For each task, we found evidence that the reasoning process mirrors that of a human chemist. Our results demonstrate that the latest reasoning models have the ability to perform advanced chemical reasoning.","authors":["Nicholas T. Runcie","Charlotte M. Deane","Fergus Imrie"],"url":"https://arxiv.org/abs/2505.07735"}
{"created":"2025-05-13","title":"VTutor for High-Impact Tutoring at Scale: Managing Engagement and Real-Time Multi-Screen Monitoring with P2P Connections","abstract":"Hybrid tutoring, where a human tutor supports multiple students in learning with educational technology, is an increasingly common application to deliver high-impact tutoring at scale. However, past hybrid tutoring applications are limited in guiding tutor attention to students that require support. Specifically, existing conferencing tools, commonly used in hybrid tutoring, do not allow tutors to monitor multiple students' screens while directly communicating and attending to multiple students simultaneously. To address this issue, this paper introduces VTutor, a web-based platform leveraging peer-to-peer screen sharing and virtual avatars to deliver real-time, context-aware tutoring feedback at scale. By integrating a multi-student monitoring dashboard with AI-powered avatar prompts, VTutor empowers a single educator or tutor to rapidly detect off-task or struggling students and intervene proactively, thus enhancing the benefits of one-on-one interactions in classroom contexts with several students. Drawing on insight from the learning sciences and past research on animated pedagogical agents, we demonstrate how stylized avatars can potentially sustain student engagement while accommodating varying infrastructure constraints. Finally, we address open questions on refining large-scale, AI-driven tutoring solutions for improved learner outcomes, and how VTutor could help interpret real-time learner interactions to support remote tutors at scale. The VTutor platform can be accessed at https://ls2025.vtutor.ai. The system demo video is at https://ls2025.vtutor.ai/video.","authors":["Eason Chen","Xinyi Tang","Aprille Xi","Chenyu Lin","Conrad Borchers","Shivang Gupta","Jionghao Lin","Kenneth R Koedinger"],"url":"https://arxiv.org/abs/2505.07736"}
{"created":"2025-05-13","title":"BBR's Sharing Behavior with CUBIC and Reno","abstract":"TCP BBR's behavior has been explained by various theoretical models, and in particular those that describe how it co-exists with other types of flows. However, as new versions of the BBR protocol have emerged, it remains unclear to what extent the high-level behaviors described by these models apply to the newer versions. In this paper, we systematically evaluate the most influential steady-state and fluid models describing BBR's coexistence with loss-based flows over shared bottleneck links. Our experiments, conducted on a new experimental platform (FABRIC), extend previous evaluations to additional network scenarios, enabling comparisons between the two models and include the recently introduced BBRv3. Our findings confirm that the steady-state model accurately captures BBRv1 behavior, especially against single loss-based flows. The fluid model successfully captures several key behaviors of BBRv1 and BBRv2 but shows limitations, in scenarios involving deep buffers, large numbers of flows, or intra-flow fairness. Importantly, we observe clear discrepancies between existing model predictions and BBRv3 behavior, suggesting the need for an updated or entirely new modeling approach for this latest version. We hope these results validate and strengthen the research community's confidence in these models and identify scenarios where they do not apply.","authors":["Fatih Berkay Sarpkaya","Ashutosh Srivastava","Fraida Fund","Shivendra Panwar"],"url":"https://arxiv.org/abs/2505.07741"}
{"created":"2025-05-13","title":"BodyGPS: Anatomical Positioning System","abstract":"We introduce a new type of foundational model for parsing human anatomy in medical images that works for different modalities. It supports supervised or unsupervised training and can perform matching, registration, classification, or segmentation with or without user interaction. We achieve this by training a neural network estimator that maps query locations to atlas coordinates via regression. Efficiency is improved by sparsely sampling the input, enabling response times of less than 1 ms without additional accelerator hardware. We demonstrate the utility of the algorithm in both CT and MRI modalities.","authors":["Halid Ziya Yerebakan","Kritika Iyer","Xueqi Guo","Yoshihisa Shinagawa","Gerardo Hermosillo Valadez"],"url":"https://arxiv.org/abs/2505.07744"}
{"created":"2025-05-13","title":"Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets","abstract":"While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.","authors":["Weiyu Li","Xuanyang Zhang","Zheng Sun","Di Qi","Hao Li","Wei Cheng","Weiwei Cai","Shihao Wu","Jiarui Liu","Zihao Wang","Xiao Chen","Feipeng Tian","Jianxiong Pan","Zeming Li","Gang Yu","Xiangyu Zhang","Daxin Jiang","Ping Tan"],"url":"https://arxiv.org/abs/2505.07747"}
{"created":"2025-05-13","title":"The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting Wrong","abstract":"Algorithm selection, aiming to identify the best algorithm for a given problem, plays a pivotal role in continuous black-box optimization. A common approach involves representing optimization functions using a set of features, which are then used to train a machine learning meta-model for selecting suitable algorithms. Various approaches have demonstrated the effectiveness of these algorithm selection meta-models. However, not all evaluation approaches are equally valid for assessing the performance of meta-models. We highlight methodological issues that frequently occur in the community and should be addressed when evaluating algorithm selection approaches. First, we identify flaws with the \"leave-instance-out\" evaluation technique. We show that non-informative features and meta-models can achieve high accuracy, which should not be the case with a well-designed evaluation framework. Second, we demonstrate that measuring the performance of optimization algorithms with metrics sensitive to the scale of the objective function requires careful consideration of how this impacts the construction of the meta-model, its predictions, and the model's error. Such metrics can falsely present overly optimistic performance assessments of the meta-models. This paper emphasizes the importance of careful evaluation, as loosely defined methodologies can mislead researchers, divert efforts, and introduce noise into the field","authors":["Ga\\v{s}per Petelin","Gjorgjina Cenikj"],"url":"https://arxiv.org/abs/2505.07750"}
{"created":"2025-05-13","title":"Benchmarking of CPU-intensive Stream Data Processing in The Edge Computing Systems","abstract":"Edge computing has emerged as a pivotal technology, offering significant advantages such as low latency, enhanced data security, and reduced reliance on centralized cloud infrastructure. These benefits are crucial for applications requiring real-time data processing or strict security measures. Despite these advantages, edge devices operating within edge clusters are often underutilized. This inefficiency is mainly due to the absence of a holistic performance profiling mechanism which can help dynamically adjust the desired system configuration for a given workload. Since edge computing environments involve a complex interplay between CPU frequency, power consumption, and application performance, a deeper understanding of these correlations is essential. By uncovering these relationships, it becomes possible to make informed decisions that enhance both computational efficiency and energy savings. To address this gap, this paper evaluates the power consumption and performance characteristics of a single processing node within an edge cluster using a synthetic microbenchmark by varying the workload size and CPU frequency. The results show how an optimal measure can lead to optimized usage of edge resources, given both performance and power consumption.","authors":["Tomasz Szydlo","Viacheslaw Horbanow","Dev Nandan Jha","Shashikant Ilager","Aleksander Slominski","Rajiv Ranjan"],"url":"https://arxiv.org/abs/2505.07755"}
{"created":"2025-05-13","title":"Emotion-Gradient Metacognitive RSI (Part I): Theoretical Foundations and Single-Agent Architecture","abstract":"We present the Emotion-Gradient Metacognitive Recursive Self-Improvement (EG-MRSI) framework, a novel architecture that integrates introspective metacognition, emotion-based intrinsic motivation, and recursive self-modification into a unified theoretical system. The framework is explicitly capable of overwriting its own learning algorithm under formally bounded risk. Building upon the Noise-to-Meaning RSI (N2M-RSI) foundation, EG-MRSI introduces a differentiable intrinsic reward function driven by confidence, error, novelty, and cumulative success. This signal regulates both a metacognitive mapping and a self-modification operator constrained by provable safety mechanisms. We formally define the initial agent configuration, emotion-gradient dynamics, and RSI trigger conditions, and derive a reinforcement-compatible optimization objective that guides the agent's development trajectory. Meaning Density and Meaning Conversion Efficiency are introduced as quantifiable metrics of semantic learning, closing the gap between internal structure and predictive informativeness. This Part I paper establishes the single-agent theoretical foundations of EG-MRSI. Future parts will extend this framework to include safety certificates and rollback protocols (Part II), collective intelligence mechanisms (Part III), and feasibility constraints including thermodynamic and computational limits (Part IV). Together, the EG-MRSI series provides a rigorous, extensible foundation for open-ended and safe AGI.","authors":["Rintaro Ando"],"url":"https://arxiv.org/abs/2505.07757"}
{"created":"2025-05-13","title":"\"I Apologize For Not Understanding Your Policy\": Exploring the Specification and Evaluation of User-Managed Access Control Policies by AI Virtual Assistants","abstract":"The rapid evolution of Artificial Intelligence (AI)-based Virtual Assistants (VAs) e.g., Google Gemini, ChatGPT, Microsoft Copilot, and High-Flyer Deepseek has turned them into convenient interfaces for managing emerging technologies such as Smart Homes, Smart Cars, Electronic Health Records, by means of explicit commands,e.g., prompts, which can be even launched via voice, thus providing a very convenient interface for end-users. However, the proper specification and evaluation of User-Managed Access Control Policies (U-MAPs), the rules issued and managed by end-users to govern access to sensitive data and device functionality - within these VAs presents significant challenges, since such a process is crucial for preventing security vulnerabilities and privacy leaks without impacting user experience. This study provides an initial exploratory investigation on whether current publicly-available VAs can manage U-MAPs effectively across differing scenarios. By conducting unstructured to structured tests, we evaluated the comprehension of such VAs, revealing a lack of understanding in varying U-MAP approaches. Our research not only identifies key limitations, but offers valuable insights into how VAs can be further improved to manage complex authorization rules and adapt to dynamic changes.","authors":["Jennifer Mondragon","Carlos Rubio-Medrano","Gael Cruz","Dvijesh Shastri"],"url":"https://arxiv.org/abs/2505.07759"}
{"created":"2025-05-13","title":"A Robust Design for BackCom Assisted Hybrid NOMA","abstract":"Hybrid non-orthogonal multiple access (H-NOMA) is inherently an enabler of massive machine type communications, a key use case for sixth-generation (6G) systems. Together with backscatter communication (BackCom), it seamlessly integrates with the traditional orthogonal multiple access (OMA) techniques to yield superior performance gains. In this paper, we study BackCom assisted H-NOMA uplink transmission with the aim of minimizing power with imperfect channel state information (CSI), where a generalized representation for channel estimation error models is used. The considered power minimization problem with aggregate data constraints is both non-convex and intractable. For the considered imperfect CSI models, we use Lagrange duality and the majorization-minimization principle to produce a conservative approximation of the original problem. The conservative formulation is relaxed by incorporating slack variables and a penalized objective. We solve the penalized tractable approximation using a provably convergent algorithm with polynomial complexity. Our results highlight that, despite being conservative, the proposed solution results in a similar power consumption as for the nominal power minimization problem without channel uncertainties. Additionally, robust H-NOMA is shown to almost always yield more power efficiency than the OMA case. Moreover, the robustness of the proposed solution is manifested by a high probability of feasibility of the robust design compared to the OMA and the nominal one.","authors":["Muhammad Fainan Hanif","Le-Nam Tran","Zhiguo Ding","Tharmalingam Ratnarajah"],"url":"https://arxiv.org/abs/2505.07762"}
{"created":"2025-05-13","title":"Solving Nonlinear PDEs with Sparse Radial Basis Function Networks","abstract":"We propose a novel framework for solving nonlinear PDEs using sparse radial basis function (RBF) networks. Sparsity-promoting regularization is employed to prevent over-parameterization and reduce redundant features. This work is motivated by longstanding challenges in traditional RBF collocation methods, along with the limitations of physics-informed neural networks (PINNs) and Gaussian process (GP) approaches, aiming to blend their respective strengths in a unified framework. The theoretical foundation of our approach lies in the function space of Reproducing Kernel Banach Spaces (RKBS) induced by one-hidden-layer neural networks of possibly infinite width. We prove a representer theorem showing that the solution to the sparse optimization problem in the RKBS admits a finite solution and establishes error bounds that offer a foundation for generalizing classical numerical analysis. The algorithmic framework is based on a three-phase algorithm to maintain computational efficiency through adaptive feature selection, second-order optimization, and pruning of inactive neurons. Numerical experiments demonstrate the effectiveness of our method and highlight cases where it offers notable advantages over GP approaches. This work opens new directions for adaptive PDE solvers grounded in rigorous analysis with efficient, learning-inspired implementation.","authors":["Zihan Shao","Konstantin Pieper","Xiaochuan Tian"],"url":"https://arxiv.org/abs/2505.07765"}
{"created":"2025-05-13","title":"Privacy Risks of Robot Vision: A User Study on Image Modalities and Resolution","abstract":"User privacy is a crucial concern in robotic applications, especially when mobile service robots are deployed in personal or sensitive environments. However, many robotic downstream tasks require the use of cameras, which may raise privacy risks. To better understand user perceptions of privacy in relation to visual data, we conducted a user study investigating how different image modalities and image resolutions affect users' privacy concerns. The results show that depth images are broadly viewed as privacy-safe, and a similarly high proportion of respondents feel the same about semantic segmentation images. Additionally, the majority of participants consider 32*32 resolution RGB images to be almost sufficiently privacy-preserving, while most believe that 16*16 resolution can fully guarantee privacy protection.","authors":["Xuying Huang","Sicong Pan","Maren Bennewitz"],"url":"https://arxiv.org/abs/2505.07766"}
{"created":"2025-05-13","title":"Enhancing Code Generation via Bidirectional Comment-Level Mutual Grounding","abstract":"Large Language Models (LLMs) have demonstrated unprecedented capability in code generation. However, LLM-generated code is still plagued with a wide range of functional errors, especially for complex programming tasks that LLMs have not seen before. Recent studies have shown that developers often struggle with inspecting and fixing incorrect code generated by LLMs, diminishing their productivity and trust in LLM-based code generation. Inspired by the mutual grounding theory in communication, we propose an interactive approach that leverages code comments as a medium for developers and LLMs to establish a shared understanding. Our approach facilitates iterative grounding by interleaving code generation, inline comment generation, and contextualized user feedback through editable comments to align generated code with developer intent. We evaluated our approach on two popular benchmarks and demonstrated that our approach significantly improved multiple state-of-the-art LLMs, e.g., 17.1% pass@1 improvement for code-davinci-002 on HumanEval. Furthermore, we conducted a user study with 12 participants in comparison to two baselines: (1) interacting with GitHub Copilot, and (2) interacting with a multi-step code generation paradigm called Multi-Turn Program Synthesis. Participants completed the given programming tasks 16.7% faster and with 10.5% improvement in task success rate when using our approach. Both results show that interactively refining code comments enables the collaborative establishment of mutual grounding, leading to more accurate code generation and higher developer confidence.","authors":["Yifeng Di","Tianyi Zhang"],"url":"https://arxiv.org/abs/2505.07768"}
{"created":"2025-05-13","title":"The Value of Disagreement in AI Design, Evaluation, and Alignment","abstract":"Disagreements are widespread across the design, evaluation, and alignment pipelines of artificial intelligence (AI) systems. Yet, standard practices in AI development often obscure or eliminate disagreement, resulting in an engineered homogenization that can be epistemically and ethically harmful, particularly for marginalized groups. In this paper, we characterize this risk, and develop a normative framework to guide practical reasoning about disagreement in the AI lifecycle. Our contributions are two-fold. First, we introduce the notion of perspectival homogenization, characterizing it as a coupled ethical-epistemic risk that arises when an aspect of an AI system's development unjustifiably suppresses disagreement and diversity of perspectives. We argue that perspectival homogenization is best understood as a procedural risk, which calls for targeted interventions throughout the AI development pipeline. Second, we propose a normative framework to guide such interventions, grounded in lines of research that explain why disagreement can be epistemically beneficial, and how its benefits can be realized in practice. We apply this framework to key design questions across three stages of AI development tasks: when disagreement is epistemically valuable; whose perspectives should be included and preserved; how to structure tasks and navigate trade-offs; and how disagreement should be documented and communicated. In doing so, we challenge common assumptions in AI practice, offer a principled foundation for emerging participatory and pluralistic approaches, and identify actionable pathways for future work in AI design and governance.","authors":["Sina Fazelpour","Will Fleisher"],"url":"https://arxiv.org/abs/2505.07772"}
{"created":"2025-05-13","title":"Agent RL Scaling Law: Agent RL with Spontaneous Code Execution for Mathematical Problem Solving","abstract":"Large Language Models (LLMs) often struggle with mathematical reasoning tasks requiring precise, verifiable computation. While Reinforcement Learning (RL) from outcome-based rewards enhances text-based reasoning, understanding how agents autonomously learn to leverage external tools like code execution remains crucial. We investigate RL from outcome-based rewards for Tool-Integrated Reasoning, ZeroTIR, training base LLMs to spontaneously generate and execute Python code for mathematical problems without supervised tool-use examples. Our central contribution is we demonstrate that as RL training progresses, key metrics scale predictably. Specifically, we observe strong positive correlations where increased training steps lead to increases in the spontaneous code execution frequency, the average response length, and, critically, the final task accuracy. This suggests a quantifiable relationship between computational effort invested in training and the emergence of effective, tool-augmented reasoning strategies. We implement a robust framework featuring a decoupled code execution environment and validate our findings across standard RL algorithms and frameworks. Experiments show ZeroTIR significantly surpasses non-tool ZeroRL baselines on challenging math benchmarks. Our findings provide a foundational understanding of how autonomous tool use is acquired and scales within Agent RL, offering a reproducible benchmark for future studies. Code is released at \\href{https://github.com/Anonymize-Author/AgentRL}{https://github.com/Anonymize-Author/AgentRL}.","authors":["Xinji Mai","Haotian Xu","Xing W","Weinong Wang","Yingying Zhang","Wenqiang Zhang"],"url":"https://arxiv.org/abs/2505.07773"}
{"created":"2025-05-13","title":"Must Read: A Systematic Survey of Computational Persuasion","abstract":"Persuasion is a fundamental aspect of communication, influencing decision-making across diverse contexts, from everyday conversations to high-stakes scenarios such as politics, marketing, and law. The rise of conversational AI systems has significantly expanded the scope of persuasion, introducing both opportunities and risks. AI-driven persuasion can be leveraged for beneficial applications, but also poses threats through manipulation and unethical influence. Moreover, AI systems are not only persuaders, but also susceptible to persuasion, making them vulnerable to adversarial attacks and bias reinforcement. Despite rapid advancements in AI-generated persuasive content, our understanding of what makes persuasion effective remains limited due to its inherently subjective and context-dependent nature. In this survey, we provide a comprehensive overview of computational persuasion, structured around three key perspectives: (1) AI as a Persuader, which explores AI-generated persuasive content and its applications; (2) AI as a Persuadee, which examines AI's susceptibility to influence and manipulation; and (3) AI as a Persuasion Judge, which analyzes AI's role in evaluating persuasive strategies, detecting manipulation, and ensuring ethical persuasion. We introduce a taxonomy for computational persuasion research and discuss key challenges, including evaluating persuasiveness, mitigating manipulative persuasion, and developing responsible AI-driven persuasive systems. Our survey outlines future research directions to enhance the safety, fairness, and effectiveness of AI-powered persuasion while addressing the risks posed by increasingly capable language models.","authors":["Nimet Beyza Bozdag","Shuhaib Mehri","Xiaocheng Yang","Hyeonjeong Ha","Zirui Cheng","Esin Durmus","Jiaxuan You","Heng Ji","Gokhan Tur","Dilek Hakkani-T\\\"ur"],"url":"https://arxiv.org/abs/2505.07775"}
{"created":"2025-05-13","title":"Synthesizing Diverse Network Flow Datasets with Scalable Dynamic Multigraph Generation","abstract":"Obtaining real-world network datasets is often challenging because of privacy, security, and computational constraints. In the absence of such datasets, graph generative models become essential tools for creating synthetic datasets. In this paper, we introduce a novel machine learning model for generating high-fidelity synthetic network flow datasets that are representative of real-world networks. Our approach involves the generation of dynamic multigraphs using a stochastic Kronecker graph generator for structure generation and a tabular generative adversarial network for feature generation. We further employ an XGBoost (eXtreme Gradient Boosting) model for graph alignment, ensuring accurate overlay of features onto the generated graph structure. We evaluate our model using new metrics that assess both the accuracy and diversity of the synthetic graphs. Our results demonstrate improvements in accuracy over previous large-scale graph generation methods while maintaining similar efficiency. We also explore the trade-off between accuracy and diversity in synthetic graph dataset creation, a topic not extensively covered in related works. Our contributions include the synthesis and evaluation of large real-world netflow datasets and the definition of new metrics for evaluating synthetic graph generative models.","authors":["Arya Grayeli","Vipin Swarup","Steven E. Noel"],"url":"https://arxiv.org/abs/2505.07777"}
{"created":"2025-05-13","title":"Multi-Agent Path Finding via Finite-Horizon Hierarchical Factorization","abstract":"We present a novel algorithm for large-scale Multi-Agent Path Finding (MAPF) that enables fast, scalable planning in dynamic environments such as automated warehouses. Our approach introduces finite-horizon hierarchical factorization, a framework that plans one step at a time in a receding-horizon fashion. Robots first compute individual plans in parallel, and then dynamically group based on spatio-temporal conflicts and reachability. The framework accounts for conflict resolution, and for immediate execution and concurrent planning, significantly reducing response time compared to offline algorithms. Experimental results on benchmark maps demonstrate that our method achieves up to 60% reduction in time-to-first-action while consistently delivering high-quality solutions, outperforming state-of-the-art offline baselines across a range of problem sizes and planning horizons.","authors":["Jiarui Li","Alessandro Zanardi","Gioele Zardini"],"url":"https://arxiv.org/abs/2505.07779"}
{"created":"2025-05-13","title":"Formal P-Category Theory and Normalization by Evaluation in Rocq","abstract":"Traditional category theory is typically based on set-theoretic principles and ideas, which are often non-constructive. An alternative approach to formalizing category theory is to use E-category theory, where hom sets become setoids. Our work reconsiders a third approach - P-category theory - from \\v{C}ubri\\'c et al. (1998) emphasizing a computational standpoint. We formalize in Rocq a modest library of P-category theory - where homs become subsetoids - and apply it to formalizing algorithms for normalization by evaluation which are purely categorical but, surprisingly, do not use neutral and normal terms. \\v{C}ubri\\'c et al. (1998) establish only a soundness correctness property by categorical means; here, we extend their work by providing a categorical proof also for a strong completeness property. For this we formalize the full universal property of the free Cartesian-closed category, which is not known to have been performed before. We further formalize a novel universal property of unquotiented simply typed lambda-calculus syntax and apply this to a proof of correctness of a categorical normalization by evaluation algorithm. We pair the overall mathematical development with a formalization in the Rocq proof assistant, following the principle that the formalization exists for practical computation. Indeed, it permits extraction of synthesized normalization programs that compute (long) beta-eta-normal forms of simply typed lambda-terms together with a derivation of beta-eta-conversion.","authors":["David G. Berry","Marcelo P. Fiore"],"url":"https://arxiv.org/abs/2505.07780"}
{"created":"2025-05-13","title":"MLE-Dojo: Interactive Environments for Empowering LLM Agents in Machine Learning Engineering","abstract":"We introduce MLE-Dojo, a Gym-style framework for systematically reinforcement learning, evaluating, and improving autonomous large language model (LLM) agents in iterative machine learning engineering (MLE) workflows. Unlike existing benchmarks that primarily rely on static datasets or single-attempt evaluations, MLE-Dojo provides an interactive environment enabling agents to iteratively experiment, debug, and refine solutions through structured feedback loops. Built upon 200+ real-world Kaggle challenges, MLE-Dojo covers diverse, open-ended MLE tasks carefully curated to reflect realistic engineering scenarios such as data processing, architecture search, hyperparameter tuning, and code debugging. Its fully executable environment supports comprehensive agent training via both supervised fine-tuning and reinforcement learning, facilitating iterative experimentation, realistic data sampling, and real-time outcome verification. Extensive evaluations of eight frontier LLMs reveal that while current models achieve meaningful iterative improvements, they still exhibit significant limitations in autonomously generating long-horizon solutions and efficiently resolving complex errors. Furthermore, MLE-Dojo's flexible and extensible architecture seamlessly integrates diverse data sources, tools, and evaluation protocols, uniquely enabling model-based agent tuning and promoting interoperability, scalability, and reproducibility. We open-source our framework and benchmarks to foster community-driven innovation towards next-generation MLE agents.","authors":["Rushi Qiang","Yuchen Zhuang","Yinghao Li","Dingu Sagar V K","Rongzhi Zhang","Changhao Li","Ian Shu-Hei Wong","Sherry Yang","Percy Liang","Chao Zhang","Bo Dai"],"url":"https://arxiv.org/abs/2505.07782"}
{"created":"2025-05-13","title":"Relative Overfitting and Accept-Reject Framework","abstract":"Currently, the scaling law of Large Language Models (LLMs) faces challenges and bottlenecks. This paper posits that noise effects, stemming from changes in the signal-to-noise ratio under diminishing marginal returns, are the root cause of these issues. To control this noise, we investigated the differences between models with performance advantages and disadvantages, introducing the concept of \"relative overfitting.\" Based on their complementary strengths, we have proposed an application framework, Accept-Reject (AR). In Natural Language Processing (NLP), we use LLMs and Small Language Models (SLMs) as the medium for discussion. This framework enables SLMs to exert a universal positive influence on LLM decision outputs, rather than the intuitively expected negative influence. We validated our approach using self-built models based on mainstream architectures and pre-trained mainstream models across multiple datasets, including basic language modeling, long-context tasks, subject examination, and question-answering (QA) benchmarks. The results demonstrate that through our structure, compared to increasing the LLM's parameters, we can achieve better performance improvements with significantly lower parameter and computational costs in many scenarios. These improvements are universal, stable, and effective. Furthermore, we explore the potential of \"relative overfitting\" and the AR framework in other machine learning domains, such as computer vision (CV) and AI for science. We hope the proposed approach can help scale laws overcome existing bottlenecks.","authors":["Yanxin Liu","Yunqi Zhang"],"url":"https://arxiv.org/abs/2505.07783"}
{"created":"2025-05-13","title":"Domain Regeneration: How well do LLMs match syntactic properties of text domains?","abstract":"Recent improvement in large language model performance have, in all likelihood, been accompanied by improvement in how well they can approximate the distribution of their training data. In this work, we explore the following question: which properties of text domains do LLMs faithfully approximate, and how well do they do so? Applying observational approaches familiar from corpus linguistics, we prompt a commonly used, opensource LLM to regenerate text from two domains of permissively licensed English text which are often contained in LLM training data -- Wikipedia and news text. This regeneration paradigm allows us to investigate whether LLMs can faithfully match the original human text domains in a fairly semantically-controlled setting. We investigate varying levels of syntactic abstraction, from more simple properties like sentence length, and article readability, to more complex and higher order properties such as dependency tag distribution, parse depth, and parse complexity. We find that the majority of the regenerated distributions show a shifted mean, a lower standard deviation, and a reduction of the long tail, as compared to the human originals.","authors":["Da Ju","Hagen Blix","Adina Williams"],"url":"https://arxiv.org/abs/2505.07784"}
{"created":"2025-05-13","title":"Learning from Peers in Reasoning Models","abstract":"Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the \"Prefix Dominance Trap\". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP's robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .","authors":["Tongxu Luo","Wenyu Du","Jiaxi Bi","Stephen Chung","Zhengyang Tang","Hao Yang","Min Zhang","Benyou Wang"],"url":"https://arxiv.org/abs/2505.07787"}
{"created":"2025-05-13","title":"Duality theory and representations for distributive quasi relation algebras and DInFL-algebras","abstract":"We develop dualities for complete perfect distributive quasi relation algebras and complete perfect distributive involutive FL-algebras. The duals are partially ordered frames with additional structure. These frames are analogous to the atom structures used to study relation algebras. We also extend the duality from complete perfect algebras to all algebras, using so-called doubly-pointed frames with a Priestley topology.","authors":["Andrew Craig","Peter Jipsen","Claudette Robinson"],"url":"https://arxiv.org/abs/2505.07789"}
{"created":"2025-05-13","title":"Overflow Prevention Enhances Long-Context Recurrent LLMs","abstract":"A recent trend in LLMs is developing recurrent sub-quadratic models that improve long-context processing efficiency. We investigate leading large long-context models, focusing on how their fixed-size recurrent memory affects their performance. Our experiments reveal that, even when these models are trained for extended contexts, their use of long contexts remains underutilized. Specifically, we demonstrate that a chunk-based inference procedure, which identifies and processes only the most relevant portion of the input can mitigate recurrent memory failures and be effective for many long-context tasks: On LongBench, our method improves the overall performance of Falcon3-Mamba-Inst-7B by 14%, Falcon-Mamba-Inst-7B by 28%, RecurrentGemma-IT-9B by 50%, and RWKV6-Finch-7B by 51%. Surprisingly, this simple approach also leads to state-of-the-art results in the challenging LongBench v2 benchmark, showing competitive performance with equivalent size Transformers. Furthermore, our findings raise questions about whether recurrent models genuinely exploit long-range dependencies, as our single-chunk strategy delivers stronger performance - even in tasks that presumably require cross-context relations.","authors":["Assaf Ben-Kish","Itamar Zimerman","M. Jehanzeb Mirza","James Glass","Leonid Karlinsky","Raja Giryes"],"url":"https://arxiv.org/abs/2505.07793"}
{"created":"2025-05-13","title":"Learning Dynamics in Continual Pre-Training for Large Language Models","abstract":"Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.","authors":["Xingjin Wang","Howe Tissue","Lu Wang","Linjing Li","Daniel Dajun Zeng"],"url":"https://arxiv.org/abs/2505.07796"}
{"created":"2025-05-13","title":"A Theoretical Framework for Explaining Reinforcement Learning with Shapley Values","abstract":"Reinforcement learning agents can achieve superhuman performance, but their decisions are often difficult to interpret. This lack of transparency limits deployment, especially in safety-critical settings where human trust and accountability are essential. In this work, we develop a theoretical framework for explaining reinforcement learning through the influence of state features, which represent what the agent observes in its environment. We identify three core elements of the agent-environment interaction that benefit from explanation: behaviour (what the agent does), performance (what the agent achieves), and value estimation (what the agent expects to achieve). We treat state features as players cooperating to produce each element and apply Shapley values, a principled method from cooperative game theory, to identify the influence of each feature. This approach yields a family of mathematically grounded explanations with clear semantics and theoretical guarantees. We use illustrative examples to show how these explanations align with human intuition and reveal novel insights. Our framework unifies and extends prior work, making explicit the assumptions behind existing approaches, and offers a principled foundation for more interpretable and trustworthy reinforcement learning.","authors":["Daniel Beechey","Thomas M. S. Smith","\\\"Ozg\\\"ur \\c{S}im\\c{s}ek"],"url":"https://arxiv.org/abs/2505.07797"}
{"created":"2025-05-13","title":"Automatically Differentiable Model Updating (ADiMU): conventional, hybrid, and neural network material model discovery including history-dependency","abstract":"We introduce the first Automatically Differentiable Model Updating (ADiMU) framework that finds any history-dependent material model from full-field displacement and global force data (global, indirect discovery) or from strain-stress data (local, direct discovery). We show that ADiMU can update conventional (physics-based), neural network (data-driven), and hybrid material models. Moreover, this framework requires no fine-tuning of hyperparameters or additional quantities beyond those inherent to the user-selected material model architecture and optimizer. The robustness and versatility of ADiMU is extensively exemplified by updating different models spanning tens to millions of parameters, in both local and global discovery settings. Relying on fully differentiable code, the algorithmic implementation leverages vectorizing maps that enable history-dependent automatic differentiation via efficient batched execution of shared computation graphs. This contribution also aims to facilitate the integration, evaluation and application of future material model architectures by openly supporting the research community. Therefore, ADiMU is released as an open-source computational tool, integrated into a carefully designed and documented software named HookeAI.","authors":["Bernardo P. Ferreira","Miguel A. Bessa"],"url":"https://arxiv.org/abs/2505.07801"}
{"created":"2025-05-13","title":"Improving Trajectory Stitching with Flow Models","abstract":"Generative models have shown great promise as trajectory planners, given their affinity to modeling complex distributions and guidable inference process. Previous works have successfully applied these in the context of robotic manipulation but perform poorly when the required solution does not exist as a complete trajectory within the training set. We identify that this is a result of being unable to plan via stitching, and subsequently address the architectural and dataset choices needed to remedy this. On top of this, we propose a novel addition to the training and inference procedures to both stabilize and enhance these capabilities. We demonstrate the efficacy of our approach by generating plans with out of distribution boundary conditions and performing obstacle avoidance on the Franka Panda in simulation and on real hardware. In both of these tasks our method performs significantly better than the baselines and is able to avoid obstacles up to four times as large.","authors":["Reece O'Mahoney","Wanming Yu","Ioannis Havoutis"],"url":"https://arxiv.org/abs/2505.07802"}
{"created":"2025-05-13","title":"AcoustoBots: A swarm of robots for acoustophoretic multimodal interactions","abstract":"Acoustophoresis has enabled novel interaction capabilities, such as levitation, volumetric displays, mid-air haptic feedback, and directional sound generation, to open new forms of multimodal interactions. However, its traditional implementation as a singular static unit limits its dynamic range and application versatility. This paper introduces AcoustoBots - a novel convergence of acoustophoresis with a movable and reconfigurable phased array of transducers for enhanced application versatility. We mount a phased array of transducers on a swarm of robots to harness the benefits of multiple mobile acoustophoretic units. This offers a more flexible and interactive platform that enables a swarm of acoustophoretic multimodal interactions. Our novel AcoustoBots design includes a hinge actuation system that controls the orientation of the mounted phased array of transducers to achieve high flexibility in a swarm of acoustophoretic multimodal interactions. In addition, we designed a BeadDispenserBot that can deliver particles to trapping locations, which automates the acoustic levitation interaction. These attributes allow AcoustoBots to independently work for a common cause and interchange between modalities, allowing for novel augmentations (e.g., a swarm of haptics, audio, and levitation) and bilateral interactions with users in an expanded interaction area. We detail our design considerations, challenges, and methodological approach to extend acoustophoretic central control in distributed settings. This work demonstrates a scalable acoustic control framework with two mobile robots, laying the groundwork for future deployment in larger robotic swarms. Finally, we characterize the performance of our AcoustoBots and explore the potential interactive scenarios they can enable.","authors":["Narsimlu Kemsaram","James Hardwick","Jincheng Wang","Bonot Gautam","Ceylan Besevli","Giorgos Christopoulos","Sourabh Dogra","Lei Gao","Akin Delibasi","Diego Martinez Plasencia","Orestis Georgiou","Marianna Obrist","Ryuji Hirayama","Sriram Subramanian"],"url":"https://arxiv.org/abs/2505.07808"}
{"created":"2025-05-13","title":"A Comparative Analysis of Static Word Embeddings for Hungarian","abstract":"This paper presents a comprehensive analysis of various static word embeddings for Hungarian, including traditional models such as Word2Vec, FastText, as well as static embeddings derived from BERT-based models using different extraction methods. We evaluate these embeddings on both intrinsic and extrinsic tasks to provide a holistic view of their performance. For intrinsic evaluation, we employ a word analogy task, which assesses the embeddings ability to capture semantic and syntactic relationships. Our results indicate that traditional static embeddings, particularly FastText, excel in this task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among the BERT-based models, the X2Static method for extracting static embeddings demonstrates superior performance compared to decontextualized and aggregate methods, approaching the effectiveness of traditional static embeddings. For extrinsic evaluation, we utilize a bidirectional LSTM model to perform Named Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results reveal that embeddings derived from dynamic models, especially those extracted using the X2Static method, outperform purely static embeddings. Notably, ELMo embeddings achieve the highest accuracy in both NER and POS tagging tasks, underscoring the benefits of contextualized representations even when used in a static form. Our findings highlight the continued relevance of static word embeddings in NLP applications and the potential of advanced extraction methods to enhance the utility of BERT-based models. This piece of research contributes to the understanding of embedding performance in the Hungarian language and provides valuable insights for future developments in the field. The training scripts, evaluation codes, restricted vocabulary, and extracted embeddings will be made publicly available to support further research and reproducibility.","authors":["M\\'at\\'e Gedeon"],"url":"https://arxiv.org/abs/2505.07809"}
{"created":"2025-05-13","title":"Continuous Visual Autoregressive Generation via Score Maximization","abstract":"Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.","authors":["Chenze Shao","Fandong Meng","Jie Zhou"],"url":"https://arxiv.org/abs/2505.07812"}
{"created":"2025-05-13","title":"DexWild: Dexterous Human Interactions for In-the-Wild Robot Policies","abstract":"Large-scale, diverse robot datasets have emerged as a promising path toward enabling dexterous manipulation policies to generalize to novel environments, but acquiring such datasets presents many challenges. While teleoperation provides high-fidelity datasets, its high cost limits its scalability. Instead, what if people could use their own hands, just as they do in everyday life, to collect data? In DexWild, a diverse team of data collectors uses their hands to collect hours of interactions across a multitude of environments and objects. To record this data, we create DexWild-System, a low-cost, mobile, and easy-to-use device. The DexWild learning framework co-trains on both human and robot demonstrations, leading to improved performance compared to training on each dataset individually. This combination results in robust robot policies capable of generalizing to novel environments, tasks, and embodiments with minimal additional robot-specific data. Experimental results demonstrate that DexWild significantly improves performance, achieving a 68.5% success rate in unseen environments-nearly four times higher than policies trained with robot data only-and offering 5.8x better cross-embodiment generalization. Video results, codebases, and instructions at https://dexwild.github.io","authors":["Tony Tao","Mohan Kumar Srirama","Jason Jingzhou Liu","Kenneth Shaw","Deepak Pathak"],"url":"https://arxiv.org/abs/2505.07813"}
{"created":"2025-05-13","title":"Imagine, Verify, Execute: Memory-Guided Agentic Exploration with Vision-Language Models","abstract":"Exploration is essential for general-purpose robotic learning, especially in open-ended environments where dense rewards, explicit goals, or task-specific supervision are scarce. Vision-language models (VLMs), with their semantic reasoning over objects, spatial relations, and potential outcomes, present a compelling foundation for generating high-level exploratory behaviors. However, their outputs are often ungrounded, making it difficult to determine whether imagined transitions are physically feasible or informative. To bridge the gap between imagination and execution, we present IVE (Imagine, Verify, Execute), an agentic exploration framework inspired by human curiosity. Human exploration is often driven by the desire to discover novel scene configurations and to deepen understanding of the environment. Similarly, IVE leverages VLMs to abstract RGB-D observations into semantic scene graphs, imagine novel scenes, predict their physical plausibility, and generate executable skill sequences through action tools. We evaluate IVE in both simulated and real-world tabletop environments. The results show that IVE enables more diverse and meaningful exploration than RL baselines, as evidenced by a 4.1 to 7.8x increase in the entropy of visited states. Moreover, the collected experience supports downstream learning, producing policies that closely match or exceed the performance of those trained on human-collected demonstrations.","authors":["Seungjae Lee","Daniel Ekpo","Haowen Liu","Furong Huang","Abhinav Shrivastava","Jia-Bin Huang"],"url":"https://arxiv.org/abs/2505.07815"}
{"created":"2025-05-13","title":"A class of distributed automata that contains the modal mu-fragment","abstract":"This paper gives a translation from the $\\mu$-fragment of the graded modal $\\mu$-calculus to a class of distributed message-passing automata. As a corollary, we obtain an alternative proof for a theorem from \\cite{ahvonen_neurips} stating that recurrent graph neural networks working with reals and graded modal substitution calculus have the same expressive power in restriction to the logic monadic second-order logic MSO.","authors":["Veeti Ahvonen","Damian Heiman","Antti Kuusisto"],"url":"https://arxiv.org/abs/2505.07816"}
{"created":"2025-05-13","title":"Pixel Motion as Universal Representation for Robot Control","abstract":"We present LangToMo, a vision-language-action framework structured as a dual-system architecture that uses pixel motion forecasts as intermediate representations. Our high-level System 2, an image diffusion model, generates text-conditioned pixel motion sequences from a single frame to guide robot control. Pixel motion-a universal, interpretable, and motion-centric representation-can be extracted from videos in a self-supervised manner, enabling diffusion model training on web-scale video-caption data. Treating generated pixel motion as learned universal representations, our low level System 1 module translates these into robot actions via motion-to-action mapping functions, which can be either hand-crafted or learned with minimal supervision. System 2 operates as a high-level policy applied at sparse temporal intervals, while System 1 acts as a low-level policy at dense temporal intervals. This hierarchical decoupling enables flexible, scalable, and generalizable robot control under both unsupervised and supervised settings, bridging the gap between language, motion, and action. Checkout https://kahnchana.github.io/LangToMo for visualizations.","authors":["Kanchana Ranasinghe","Xiang Li","Cristina Mata","Jongwoo Park","Michael S Ryoo"],"url":"https://arxiv.org/abs/2505.07817"}
{"created":"2025-05-13","title":"DanceGRPO: Unleashing GRPO on Visual Generation","abstract":"Recent breakthroughs in generative models-particularly diffusion models and rectified flows-have revolutionized visual content creation, yet aligning model outputs with human preferences remains a critical challenge. Existing reinforcement learning (RL)-based methods for visual generation face critical limitations: incompatibility with modern Ordinary Differential Equations (ODEs)-based sampling paradigms, instability in large-scale training, and lack of validation for video generation. This paper introduces DanceGRPO, the first unified framework to adapt Group Relative Policy Optimization (GRPO) to visual generation paradigms, unleashing one unified RL algorithm across two generative paradigms (diffusion models and rectified flows), three tasks (text-to-image, text-to-video, image-to-video), four foundation models (Stable Diffusion, HunyuanVideo, FLUX, SkyReel-I2V), and five reward models (image/video aesthetics, text-image alignment, video motion quality, and binary reward). To our knowledge, DanceGRPO is the first RL-based unified framework capable of seamless adaptation across diverse generative paradigms, tasks, foundational models, and reward models. DanceGRPO demonstrates consistent and substantial improvements, which outperform baselines by up to 181% on benchmarks such as HPS-v2.1, CLIP Score, VideoAlign, and GenEval. Notably, DanceGRPO not only can stabilize policy optimization for complex video generation, but also enables generative policy to better capture denoising trajectories for Best-of-N inference scaling and learn from sparse binary feedback. Our results establish DanceGRPO as a robust and versatile solution for scaling Reinforcement Learning from Human Feedback (RLHF) tasks in visual generation, offering new insights into harmonizing reinforcement learning and visual synthesis. The code will be released.","authors":["Zeyue Xue","Jie Wu","Yu Gao","Fangyuan Kong","Lingting Zhu","Mengzhao Chen","Zhiheng Liu","Wei Liu","Qiushan Guo","Weilin Huang","Ping Luo"],"url":"https://arxiv.org/abs/2505.07818"}
{"created":"2025-05-13","title":"H$^{\\mathbf{3}}$DP: Triply-Hierarchical Diffusion Policy for Visuomotor Learning","abstract":"Visuomotor policy learning has witnessed substantial progress in robotic manipulation, with recent approaches predominantly relying on generative models to model the action distribution. However, these methods often overlook the critical coupling between visual perception and action prediction. In this work, we introduce $\\textbf{Triply-Hierarchical Diffusion Policy}~(\\textbf{H$^{\\mathbf{3}}$DP})$, a novel visuomotor learning framework that explicitly incorporates hierarchical structures to strengthen the integration between visual features and action generation. H$^{3}$DP contains $\\mathbf{3}$ levels of hierarchy: (1) depth-aware input layering that organizes RGB-D observations based on depth information; (2) multi-scale visual representations that encode semantic features at varying levels of granularity; and (3) a hierarchically conditioned diffusion process that aligns the generation of coarse-to-fine actions with corresponding visual features. Extensive experiments demonstrate that H$^{3}$DP yields a $\\mathbf{+27.5\\%}$ average relative improvement over baselines across $\\mathbf{44}$ simulation tasks and achieves superior performance in $\\mathbf{4}$ challenging bimanual real-world manipulation tasks. Project Page: https://lyy-iiis.github.io/h3dp/.","authors":["Yiyang Lu","Yufeng Tian","Zhecheng Yuan","Xianbang Wang","Pu Hua","Zhengrong Xue","Huazhe Xu"],"url":"https://arxiv.org/abs/2505.07819"}
{"created":"2025-05-13","title":"Extended Convex Hull-Based Distributed Optimal Energy Flow of Integrated Electricity-Gas Systems","abstract":"Integrated electricity and gas systems are constructed to facilitate the gas-fired generation, and the distributed operation of these integrated systems have received much attention due to the increased emphasis on data security and privacy between different agencies. This paper proposes an extended convex hull based method to address optimal energy flow problems for the integrated electricity and gas systems in a distributed manner. First, a multi-block electricity-gas system model is constructed by dividing the whole system into N blocks considering both physical and regional differences. This multi-block model is then convexified by replacing the nonconvex gas transmission equation with the extended convex hullbased constraints. The Jacobi-Proximal alternating direction method of multipliers algorithm is adopted to solve the convexified model and minimize its operation cost. Finally, the feasibility of the optimal solution for the convexified model is checked, and a sufficient condition is developed. If the sufficient condition is satisfied, the optimal solution for the original nonconvex problem can be recovered from that for the convexified problem. Simulation results demonstrate that the proposed method is tractable and effective in obtaining feasible optimal solutions for multi-block optimal energy flow problems.","authors":["Rong-Peng Liu","Wei Sun","Dali Zhou","Yunhe Hou"],"url":"https://arxiv.org/abs/2002.10338"}
{"created":"2025-05-13","title":"Equivariant Machine Learning Decoder for 3D Toric Codes","abstract":"Mitigating errors in computing and communication systems has seen a great deal of research since the beginning of the widespread use of these technologies. However, as we develop new methods to do computation or communication, we also need to reiterate the method used to deal with errors. Within the field of quantum computing, error correction is getting a lot of attention since errors can propagate fast and invalidate results, which makes the theoretical exponential speed increase in computation time, compared to traditional systems, obsolete. To correct errors in quantum systems, error-correcting codes are used. A subgroup of codes, topological codes, is currently the focus of many research papers. Topological codes represent parity check matrices corresponding to graphs embedded on a $d$-dimensional surface. For our research, the focus lies on the toric code with a 3D square lattice. The goal of any decoder is robustness to noise, which can increase with code size. However, a reasonable decoder performance scales polynomially with lattice size. As error correction is a time-sensitive operation, we propose a neural network using an inductive bias: equivariance. This allows the network to learn from a rather small subset of the exponentially growing training space of possible inputs. In addition, we investigate how transformer networks can help in correction. These methods will be compared with various configurations and previously published methods of decoding errors in the 3D toric code.","authors":["Oliver Weissl","Evgenii Egorov"],"url":"https://arxiv.org/abs/2409.04300"}
{"created":"2025-05-13","title":"STRAUSS: Sonification Tools & Resources for Analysis Using Sound Synthesis","abstract":"Sonification, or conveying data using non-verbal audio, is a relatively niche but growing approach for presenting data across multiple specialist domains including astronomy, climate science, and beyond. The STRAUSS Python package aims to provide such a tool, which builds upon previous approaches to provide a powerful means to explore different ways of expressing data, with fine control over the output audio and its format. STRAUSS is a free, open source (FOSS) package, designed to allow flexible and effective sonification to be integrated into data workflows, in analogy to widely used visualisation packages. The remit of STRAUSS is broad; it is intended to be able to bridge between ad-hoc solutions for sonifying very particular datasets, and highly technical compositional and sound-design tools that are not optimised for sonification, or may have a steep learning curve. The code offers a range of approaches to sonification for a variety of contexts (e.g. science education, science communication, technical data analysis, etc). To this end, STRAUSS is packaged with a number of examples of different sonification approaches, and preset configurations to support \"low-barrier, high-ceiling\" approach. STRAUSS has been used to produce both educational resources and analysis tools.","authors":["James W. Trayford","Samantha Youles","Chris Harrison","Rose Shepherd","Nicolas Bonne"],"url":"https://arxiv.org/abs/2504.01660"}
{"created":"2025-05-13","title":"Pinching-Antenna Assisted Simultaneous Wireless Information and Power Transfer","abstract":"This letter introduces a novel pinching-antenna-system (PASS) assisted simultaneous wireless information and power transfer (SWIPT), where multiple pinching antennas (PAs) are strategically activiated on a waveguide to facilitate information transmission to multiple information receivers (IRs) and power transfer to multiple energy receivers (ERs) simultaneously. Leveraging the single-waveguide architecture, non-orthogonal multiple access is employed to enable the superposed transmission of information signals, eliminating the need for dedicated energy carriers by concurrently serving both the IRs and the ERs. In this letter, an optimization problem is first formulated with an objective is to maximize the sum power received at the ERs via joint optimizing the IRs power allocation and the PAs positioning. Given the challenging non-convex nature of the formulated optimization problem, it is decoupled into two sub-problems which are solved alternatively. Specifically, the power allocation subproblem is recast a convex optimization problem, and hence can be solved efficiently. Furthermore, we propose a high-precision element-wise algorithm and a low-complexity linearly decreasing weight particle swarm optimization algorithm to solve the position optimization sub-problem. The numerical results demonstrate that PAs assisted SWIPT can achieve a remarkable performance gain compared to conventional system.","authors":["Yixuan Li","Ji Wang","Yuanwei Liu","Zhiguo Ding"],"url":"https://arxiv.org/abs/2505.06240"}
{"created":"2025-05-13","title":"Low-Complexity CNN-Based Classification of Electroneurographic Signals","abstract":"Peripheral nerve interfaces (PNIs) facilitate neural recording and stimulation for treating nerve injuries, but real-time classification of electroneurographic (ENG) signals remains challenging due to constraints on complexity and latency, particularly in implantable devices. This study introduces MobilESCAPE-Net, a lightweight architecture that reduces computational cost while maintaining and slightly improving classification performance. Compared to the state-of-the-art ESCAPE-Net, MobilESCAPE-Net achieves comparable accuracy and F1-score with significantly lower complexity, reducing trainable parameters by 99.9\\% and floating point operations per second by 92.47\\%, enabling faster inference and real-time processing. Its efficiency makes it well-suited for low-complexity ENG signal classification in resource-constrained environments such as implantable devices.","authors":["Arek Berc Gokdag","Silvia Mura","Antonio Coviello","Michele Zhu","Maurizio Magarini","Umberto Spagnolini"],"url":"https://arxiv.org/abs/2505.06241"}
{"created":"2025-05-13","title":"Supervised machine learning based signal demodulation in chaotic communications","abstract":"A chaotic modulation scheme is an efficient wideband communication method. It utilizes the deterministic chaos to generate pseudo-random carriers. Chaotic bifurcation parameter modulation is one of the well-known and widely-used techniques. This paper presents the machine learning based demodulation approach for the bifurcation parameter keying. It presents the structure of a convolutional neural network as well as performance metrics values for signals generated with the chaotic logistic map. The paper provides an assessment of the overall accuracy for binary signals. It reports the accuracy value of 0.88 for the bifurcation parameter deviation of 1.34% in the presence of additive white Gaussian noise at the normalized signal-to-noise ratio value of 20 dB for balanced dataset.","authors":["Mykola Kozlenko"],"url":"https://arxiv.org/abs/2505.06243"}
{"created":"2025-05-13","title":"A Transformer-Based Approach for Diagnosing Fault Cases in Optical Fiber Amplifiers","abstract":"A transformer-based deep learning approach is presented that enables the diagnosis of fault cases in optical fiber amplifiers using condition-based monitoring time series data. The model, Inverse Triple-Aspect Self-Attention Transformer (ITST), uses an encoder-decoder architecture, utilizing three feature extraction paths in the encoder, feature-engineered data for the decoder and a self-attention mechanism. The results show that ITST outperforms state-of-the-art models in terms of classification accuracy, which enables predictive maintenance for optical fiber amplifiers, reducing network downtimes and maintenance costs.","authors":["Dominic Schneider","Lutz Rapp","Christoph Ament"],"url":"https://arxiv.org/abs/2505.06245"}
{"created":"2025-05-13","title":"Low-Complexity Channel Estimation in OTFS Systems with Fractional Effects","abstract":"Orthogonal Time Frequency Space (OTFS) modulation exploits the sparsity of Delay-Doppler domain channels, making it highly effective in high-mobility scenarios. Its accurate channel estimation supports integrated sensing and communication (ISAC) systems. The letter introduces a low-complexity technique for estimating delay and Doppler shifts under fractional effects, while addressing inter-path interference. The method employs a sequential estimation process combined with interference elimination based on energy leakage, ensuring accurate channel estimation. Furthermore, the estimated channel parameters can signifcantly improve ISAC system performance by enhancing sensing capabilities. Experimental results validate the effectiveness of this approach in achieving accurate channel estimation and facilitating sensing tasks for ISAC systems.","authors":["Guangyu Lei","Yanduo Qiao","Tianhao Liang","Weijie Yuan","Tingting Zhang"],"url":"https://arxiv.org/abs/2505.06248"}
{"created":"2025-05-13","title":"An Early Warning Model for Forced Displacement","abstract":"Monitoring tools for anticipatory action are increasingly gaining traction to improve the efficiency and timeliness of humanitarian responses. Whilst predictive models can now forecast conflicts with high accuracy, translating these predictions into potential forced displacement movements remains challenging because it is often unclear which precise events will trigger significant population movements. This paper presents a novel monitoring approach for refugee and asylum seeker flows that addresses this challenge. Using gradient boosting classification, we combine conflict forecasts with a comprehensive set of economic, political, and demographic variables to assess two distinct risks at the country of origin: the likelihood of significant displacement flows and the probability of sudden increases in these flows. The model generates country-specific monthly risk indices for these two events with prediction horizons of one, three, and six months. Our analysis shows high accuracy in predicting significant displacement flows and good accuracy in forecasting sudden increases in displacement--the latter being inherently more difficult to predict, given the complexity of displacement triggers. We achieve these results by including predictive factors beyond conflict, thereby demonstrating that forced displacement risks can be assessed through an integrated analysis of multiple country-level indicators. Whilst these risk indices provide valuable quantitative support for humanitarian planning, they should always be understood as decision-support tools within a broader analytical framework.","authors":["Geraldine Henningsen"],"url":"https://arxiv.org/abs/2505.06249"}
{"created":"2025-05-13","title":"DeltaDPD: Exploiting Dynamic Temporal Sparsity in Recurrent Neural Networks for Energy-Efficient Wideband Digital Predistortion","abstract":"Digital Predistortion (DPD) is a popular technique to enhance signal quality in wideband RF power amplifiers (PAs). With increasing bandwidth and data rates, DPD faces significant energy consumption challenges during deployment, contrasting with its efficiency goals. State-of-the-art DPD models rely on recurrent neural networks (RNN), whose computational complexity hinders system efficiency. This paper introduces DeltaDPD, exploring the dynamic temporal sparsity of input signals and neuronal hidden states in RNNs for energy-efficient DPD, reducing arithmetic operations and memory accesses while preserving satisfactory linearization performance. Applying a TM3.1a 200MHz-BW 256-QAM OFDM signal to a 3.5 GHz GaN Doherty RF PA, DeltaDPD achieves -50.03 dBc in Adjacent Channel Power Ratio (ACPR), -37.22 dB in Normalized Mean Square Error (NMSE) and -38.52 dBc in Error Vector Magnitude (EVM) with 52% temporal sparsity, leading to a 1.8X reduction in estimated inference power. The DeltaDPD code will be released after formal publication at https://www.opendpd.com.","authors":["Yizhuo Wu","Yi Zhu","Kun Qian","Qinyu Chen","Anding Zhu","John Gajadharsing","Leo C. N. de Vreede","Chang Gao"],"url":"https://arxiv.org/abs/2505.06250"}
{"created":"2025-05-13","title":"SpectrumFM: A Foundation Model for Intelligent Spectrum Management","abstract":"Intelligent spectrum management is crucial for improving spectrum efficiency and achieving secure utilization of spectrum resources. However, existing intelligent spectrum management methods, typically based on small-scale models, suffer from notable limitations in recognition accuracy, convergence speed, and generalization, particularly in the complex and dynamic spectrum environments. To address these challenges, this paper proposes a novel spectrum foundation model, termed SpectrumFM, establishing a new paradigm for spectrum management. SpectrumFM features an innovative encoder architecture that synergistically exploits the convolutional neural networks and the multi-head self-attention mechanisms to enhance feature extraction and enable robust representation learning. The model is pre-trained via two novel self-supervised learning tasks, namely masked reconstruction and next-slot signal prediction, which leverage large-scale in-phase and quadrature (IQ) data to achieve comprehensive and transferable spectrum representations. Furthermore, a parameter-efficient fine-tuning strategy is proposed to enable SpectrumFM to adapt to various downstream spectrum management tasks, including automatic modulation classification (AMC), wireless technology classification (WTC), spectrum sensing (SS), and anomaly detection (AD). Extensive experiments demonstrate that SpectrumFM achieves superior performance in terms of accuracy, robustness, adaptability, few-shot learning efficiency, and convergence speed, consistently outperforming conventional methods across multiple benchmarks. Specifically, SpectrumFM improves AMC accuracy by up to 12.1% and WTC accuracy by 9.3%, achieves an area under the curve (AUC) of 0.97 in SS at -4 dB signal-to-noise ratio (SNR), and enhances AD performance by over 10%.","authors":["Fuhui Zhou","Chunyu Liu","Hao Zhang","Wei Wu","Qihui Wu","Derrick Wing Kwan Ng","Tony Q. S. Quek","Chan-Byoung Chae"],"url":"https://arxiv.org/abs/2505.06256"}
{"created":"2025-05-13","title":"From Biometrics to Environmental Control: AI-Enhanced Digital Twins for Personalized Health Interventions in Healing Landscapes","abstract":"The dynamic nature of human health and comfort calls for adaptive systems that respond to individual physiological needs in real time. This paper presents an AI-enhanced digital twin framework that integrates biometric signals, specifically electrocardiogram (ECG) data, with environmental parameters such as temperature, humidity, and ventilation. Leveraging IoT-enabled sensors and biometric monitoring devices, the system continuously acquires, synchronises, and preprocesses multimodal data streams to construct a responsive virtual replica of the physical environment. To validate this framework, a detailed case study is conducted using the MIT-BIH noise stress test dataset. ECG signals are filtered and segmented using dynamic sliding windows, followed by extracting heart rate variability (HRV) features such as SDNN, BPM, QTc, and LF/HF ratio. Relative deviation metrics are computed against clean baselines to quantify stress responses. A random forest classifier is trained to predict stress levels across five categories, and Shapley Additive exPlanations (SHAP) is used to interpret model behaviour and identify key contributing features. These predictions are mapped to a structured set of environmental interventions using a Five Level Stress Intervention Mapping, which activates multi-scale responses across personal, room, building, and landscape levels. This integration of physiological insight, explainable AI, and adaptive control establishes a new paradigm for health-responsive built environments. It lays the foundation for the future development of intelligent, personalised healing spaces.","authors":["Yiping Meng","Yiming Sun"],"url":"https://arxiv.org/abs/2505.06263"}
{"created":"2025-05-13","title":"Prediction of Delirium Risk in Mild Cognitive Impairment Using Time-Series data, Machine Learning and Comorbidity Patterns -- A Retrospective Study","abstract":"Delirium represents a significant clinical concern characterized by high morbidity and mortality rates, particularly in patients with mild cognitive impairment (MCI). This study investigates the associated risk factors for delirium by analyzing the comorbidity patterns relevant to MCI and developing a longitudinal predictive model leveraging machine learning methodologies. A retrospective analysis utilizing the MIMIC-IV v2.2 database was performed to evaluate comorbid conditions, survival probabilities, and predictive modeling outcomes. The examination of comorbidity patterns identified distinct risk profiles for the MCI population. Kaplan-Meier survival analysis demonstrated that individuals with MCI exhibit markedly reduced survival probabilities when developing delirium compared to their non-MCI counterparts, underscoring the heightened vulnerability within this cohort. For predictive modeling, a Long Short-Term Memory (LSTM) ML network was implemented utilizing time-series data, demographic variables, Charlson Comorbidity Index (CCI) scores, and an array of comorbid conditions. The model demonstrated robust predictive capabilities with an AUROC of 0.93 and an AUPRC of 0.92. This study underscores the critical role of comorbidities in evaluating delirium risk and highlights the efficacy of time-series predictive modeling in pinpointing patients at elevated risk for delirium development.","authors":["Santhakumar Ramamoorthy","Priya Rani","James Mahon","Glenn Mathews","Shaun Cloherty","Mahdi Babaei"],"url":"https://arxiv.org/abs/2505.06264"}
{"created":"2025-05-13","title":"Terahertz Spatial Wireless Channel Modeling with Radio Radiance Field","abstract":"Terahertz (THz) communication is a key enabler for 6G systems, offering ultra-wide bandwidth and unprecedented data rates. However, THz signal propagation differs significantly from lower-frequency bands due to severe free space path loss, minimal diffraction and specular reflection, and prominent scattering, making conventional channel modeling and pilot-based estimation approaches inefficient. In this work, we investigate the feasibility of applying radio radiance field (RRF) framework to the THz band. This method reconstructs a continuous RRF using visual-based geometry and sparse THz RF measurements, enabling efficient spatial channel state information (Spatial-CSI) modeling without dense sampling. We first build a fine simulated THz scenario, then we reconstruct the RRF and evaluate the performance in terms of both reconstruction quality and effectiveness in THz communication, showing that the reconstructed RRF captures key propagation paths with sparse training samples. Our findings demonstrate that RRF modeling remains effective in the THz regime and provides a promising direction for scalable, low-cost spatial channel reconstruction in future 6G networks.","authors":["John Song","Lihao Zhang","Feng Ye","Haijian Sun"],"url":"https://arxiv.org/abs/2505.06277"}
{"created":"2025-05-13","title":"FEMSN: Frequency-Enhanced Multiscale Network for fault diagnosis of rotating machinery under strong noise environments","abstract":"Rolling bearings are critical components of rotating machinery, and their proper functioning is essential for industrial production. Most existing condition monitoring methods focus on extracting discriminative features from time-domain signals to assess bearing health status. However, under complex operating conditions, periodic impulsive characteristics related to fault information are often obscured by noise interference. Consequently, existing approaches struggle to learn distinctive fault-related features in such scenarios. To address this issue, this paper proposes a novel CNN-based model named FEMSN. Specifically, a Fourier Adaptive Denoising Encoder Layer (FADEL) is introduced as an input denoising layer to enhance key features while filtering out irrelevant information. Subsequently, a Multiscale Time-Frequency Fusion (MSTFF) module is employed to extract fused time-frequency features, further improving the model robustness and nonlinear representation capability. Additionally, a distillation layer is incorporated to expand the receptive field. Based on these advancements, a novel deep lightweight CNN model, termed the Frequency-Enhanced Multiscale Network (FEMSN), is developed. The effectiveness of FEMSN and FADEL in machine health monitoring and stability assessment is validated through two case studies.","authors":["Yuhan Yuan","Xiaomo Jiang","Yanfeng Han","Ke Xiao"],"url":"https://arxiv.org/abs/2505.06285"}
{"created":"2025-05-13","title":"ALFEE: Adaptive Large Foundation Model for EEG Representation","abstract":"While foundation models excel in text, image, and video domains, the critical biological signals, particularly electroencephalography(EEG), remain underexplored. EEG benefits neurological research with its high temporal resolution, operational practicality, and safety profile. However, low signal-to-noise ratio, inter-subject variability, and cross-paradigm differences hinder the generalization of current models. Existing methods often employ simplified strategies, such as a single loss function or a channel-temporal joint representation module, and suffer from a domain gap between pretraining and evaluation tasks that compromises efficiency and adaptability. To address these limitations, we propose the Adaptive Large Foundation model for EEG signal representation(ALFEE) framework, a novel hybrid transformer architecture with two learning stages for robust EEG representation learning. ALFEE employs a hybrid attention that separates channel-wise feature aggregation from temporal dynamics modeling, enabling robust EEG representation with variable channel configurations. A channel encoder adaptively compresses variable channel information, a temporal encoder captures task-guided evolution, and a hybrid decoder reconstructs signals in both temporal and frequency domains. During pretraining, ALFEE optimizes task prediction, channel and temporal mask reconstruction, and temporal forecasting to enhance multi-scale and multi-channel representation. During fine-tuning, a full-model adaptation with a task-specific token dictionary and a cross-attention layer boosts performance across multiple tasks. After 25,000 hours of pretraining, extensive experimental results on six downstream EEG tasks demonstrate the superior performance of ALFEE over existing models. Our ALFEE framework establishes a scalable foundation for biological signal analysis with implementation at https://github.com/xw1216/ALFEE.","authors":["Wei Xiong","Junming Lin","Jiangtong Li","Jie Li","Changjun Jiang"],"url":"https://arxiv.org/abs/2505.06291"}
{"created":"2025-05-13","title":"Adaptive Bayesian Very Short-Term Wind Power Forecasting Based on the Generalised Logit Transformation","abstract":"Wind power plays an increasingly significant role in achieving the 2050 Net Zero Strategy. Despite its rapid growth, its inherent variability presents challenges in forecasting. Accurately forecasting wind power generation is one key demand for the stable and controllable integration of renewable energy into existing grid operations. This paper proposes an adaptive method for very short-term forecasting that combines the generalised logit transformation with a Bayesian approach. The generalised logit transformation processes double-bounded wind power data to an unbounded domain, facilitating the application of Bayesian methods. A novel adaptive mechanism for updating the transformation shape parameter is introduced to leverage Bayesian updates by recovering a small sample of representative data. Four adaptive forecasting methods are investigated, evaluating their advantages and limitations through an extensive case study of over 100 wind farms ranging four years in the UK. The methods are evaluated using the Continuous Ranked Probability Score and we propose the use of functional reliability diagrams to assess calibration. Results indicate that the proposed Bayesian method with adaptive shape parameter updating outperforms benchmarks, yielding consistent improvements in CRPS and forecast reliability. The method effectively addresses uncertainty, ensuring robust and accurate probabilistic forecasting which is essential for grid integration and decision-making.","authors":["Tao Shen","Jethro Browell","Daniela Castro-Camilo"],"url":"https://arxiv.org/abs/2505.06310"}
{"created":"2025-05-13","title":"Quantum strategies, error bounds, optimality, and duality gaps for multiplayer XOR, $\\mathrm{XOR}^{*}$, compiled XOR, $\\mathrm{XOR}^{*}$, and strong parallel repetiton of XOR, $\\mathrm{XOR}^{*}$, and FFL games","abstract":"We characterize exact, and approximate, optimality of games that players can interact with using quantum strategies. In comparison to a previous work of the author, arXiv: 2311.12887, which applied a 2016 framework due to Ostrev for constructing error bounds beyond CHSH and XOR games, in addition to the existence of well-posed semidefinite programs for determining primal feasible solutions, along with quantum-classical duality gaps, it continues to remain of interest to further develop the construction of error bounds, and related objects, to game-theoretic settings with several participants. In such settings, one encounters a rich information theoretic landscape, not only from the fact that there exists a significantly larger combinatorial space of possible strategies for each player, but also several opportunities for pronounced quantum advantage. We conclude this effort by describing other variants of other possible strategies, as proposed sources for quantum advantage, in $\\mathrm{XOR}^{*}$, compiled $\\mathrm{XOR}^{*}$, and strong parallel repetition variants of $\\mathrm{XOR}^{*}$ games.","authors":["Pete Rigas"],"url":"https://arxiv.org/abs/2505.06322"}
{"created":"2025-05-13","title":"Quantum State Preparation via Large-Language-Model-Driven Evolution","abstract":"We propose an automated framework for quantum circuit design by integrating large-language models (LLMs) with evolutionary optimization to overcome the rigidity, scalability limitations, and expert dependence of traditional ones in variational quantum algorithms. Our approach (FunSearch) autonomously discovers hardware-efficient ans\\\"atze with new features of scalability and system-size-independent number of variational parameters entirely from scratch. Demonstrations on the Ising and XY spin chains with n = 9 qubits yield circuits containing 4 parameters, achieving near-exact energy extrapolation across system sizes. Implementations on quantum hardware (Zuchongzhi chip) validate practicality, where two-qubit quantum gate noises can be effectively mitigated via zero-noise extrapolations for a spin chain system as large as 20 sites. This framework bridges algorithmic design and experimental constraints, complementing contemporary quantum architecture search frameworks to advance scalable quantum simulations.","authors":["Qing-Hong Cao","Zong-Yue Hou","Ying-Ying Li","Xiaohui Liu","Zhuo-Yang Song","Liang-Qi Zhang","Shutao Zhang","Ke Zhao"],"url":"https://arxiv.org/abs/2505.06347"}
{"created":"2025-05-13","title":"The Quantum Approximate Optimization Algorithm Can Require Exponential Time to Optimize Linear Functions","abstract":"QAOA is a hybrid quantum-classical algorithm to solve optimization problems in gate-based quantum computers. It is based on a variational quantum circuit that can be interpreted as a discretization of the annealing process that quantum annealers follow to find a minimum energy state of a given Hamiltonian. This ensures that QAOA must find an optimal solution for any given optimization problem when the number of layers, $p$, used in the variational quantum circuit tends to infinity. In practice, the number of layers is usually bounded by a small number. This is a must in current quantum computers of the NISQ era, due to the depth limit of the circuits they can run to avoid problems with decoherence and noise. In this paper, we show mathematical evidence that QAOA requires exponential time to solve linear functions when the number of layers is less than the number of different coefficients of the linear function $n$. We conjecture that QAOA needs exponential time to find the global optimum of linear functions for any constant value of $p$, and that the runtime is linear only if $p \\geq n$. We conclude that we need new quantum algorithms to reach quantum supremacy in quantum optimization.","authors":["Francisco Chicano","Zakaria Abdelmoiz Dahi","Gabiel Luque"],"url":"https://arxiv.org/abs/2505.06404"}
{"created":"2025-05-13","title":"Mixing and Merging Metric Spaces using Directed Graphs","abstract":"Let $(X_1,d_1),\\dots, (X_N,d_N)$ be metric spaces with respective distance functions $d_i: X_i \\times X_i \\rightarrow [0,1]$, $i=1,\\dots,N$. Let $\\mathcal{X}$ denote the set theoretic product $X_1\\times \\cdots \\times X_N$ and let $\\mathbf{g} \\in \\mathcal{X}$ and $\\mathbf{h} \\in \\mathcal{X}$ denote two elements in this product space. Let $\\mathcal{G} = \\left(\\mathcal{V},\\mathcal{E}\\right)$ be a directed graph with vertices $\\mathcal{V} =\\{1,\\dots, N\\}$ and with a positive weight $\\mathcal{P} = \\{p_{ij}\\}, p_{ij}\\in (0, 1], i,j = 1,..,N$ associated with each edge $(i,j) \\in \\mathcal{E}$ of $\\mathcal{G}$. We define the function \\begin{align*} d_{\\mathcal{X},\\mathcal{G},\\mathcal{P}}(\\mathbf{g},\\mathbf{h}) := \\left(1 - \\frac{1}{N}\\sum_{j=1}^N \\prod_{i=1}^N \\left[1- d_i(g_i,h_i)\\right]^{\\frac{1}{p_{ji}}} \\right). \\end{align*} In this paper we show that $d_{\\mathcal{X},\\mathcal{G},\\mathcal{P}}$ defines a metric space over $\\mathcal{X}$ and we investigate the properties of this distance under graph operations, which includes disjoint unions and cartesian products. We show two limiting cases: (a) where $d_{\\mathcal{X},\\mathcal{G},\\mathcal{P}}$ defined over a finite field leads to a broad generalization of graph-based distances that is widely studied in the theory of error-correcting codes; and (b) where $d_{\\mathcal{X},\\mathcal{G},\\mathcal{P}}$ is extended to measuring distances over graphons.","authors":["Mahir Bilen Can","Shantanu Chakrabartty"],"url":"https://arxiv.org/abs/2505.06405"}
{"created":"2025-05-13","title":"Fair Representation Learning for Continuous Sensitive Attributes using Expectation of Integral Probability Metrics","abstract":"AI fairness, also known as algorithmic fairness, aims to ensure that algorithms operate without bias or discrimination towards any individual or group. Among various AI algorithms, the Fair Representation Learning (FRL) approach has gained significant interest in recent years. However, existing FRL algorithms have a limitation: they are primarily designed for categorical sensitive attributes and thus cannot be applied to continuous sensitive attributes, such as age or income. In this paper, we propose an FRL algorithm for continuous sensitive attributes. First, we introduce a measure called the Expectation of Integral Probability Metrics (EIPM) to assess the fairness level of representation space for continuous sensitive attributes. We demonstrate that if the distribution of the representation has a low EIPM value, then any prediction head constructed on the top of the representation become fair, regardless of the selection of the prediction head. Furthermore, EIPM possesses a distinguished advantage in that it can be accurately estimated using our proposed estimator with finite samples. Based on these properties, we propose a new FRL algorithm called Fair Representation using EIPM with MMD (FREM). Experimental evidences show that FREM outperforms other baseline methods.","authors":["Insung Kong","Kunwoong Kim","Yongdai Kim"],"url":"https://arxiv.org/abs/2505.06435"}
{"created":"2025-05-13","title":"A linear-time algorithm to compute the conjugate of nonconvex bivariate piecewise linear-quadratic functions","abstract":"We propose the first linear-time algorithm to compute the conjugate of (nonconvex) bivariate piecewise linear-quadratic (PLQ) functions (bivariate quadratic functions defined on a polyhedral subdivision). Our algorithm starts with computing the convex envelope of each quadratic piece obtaining rational functions (quadratic over linear) defined over a polyhedral subdivision. Then we compute the conjugate of each resulting piece to obtain piecewise quadratic functions defined over a parabolic subdivision. Finally we compute the maximum of all those functions to obtain the conjugate as a piecewise quadratic function defined on a parabolic subdivision. The resulting algorithm runs in linear time if the initial subdivision is a triangulation (or has a uniform upper bound on the number of vertexes for each piece).","authors":["Tanmaya Karmarkar","Yves Lucet"],"url":"https://arxiv.org/abs/2505.06442"}
{"created":"2025-05-13","title":"Quantum medical image encoding and compression using Fourier-based methods","abstract":"Quantum image processing (QIMP) has recently emerged as a promising field for modern image processing applications. In QIMP algorithms, encoding classical image informaiton into quantum circuit is important as the first step. However, most of existing encoding methods use gates almost twice the number of pixels in an image, and simulating even a modest sized image is computationally demanding. In this work, we propose a quantum image encoding method that effectively reduces gates than the number of pixels by a factor at least 4. We demonstrate our method for various 1024 by 1024 high-quality medical images captured during the Bilateral Axillo-Breast Approach (BABA) robotic thyroidectomy surgery. Additionally, two compression techniques are proposed to further reduce the number of gates as well as pre-processing time with negligible loss of image quality. We suggest our image encoding strategy as a valuable option for large scale medical imaging.","authors":["Taehee Ko","Inho Lee","Hyeong Won Yu"],"url":"https://arxiv.org/abs/2505.06471"}
{"created":"2025-05-13","title":"Hamiltonian Locality Testing via Trotterized Postselection","abstract":"The (tolerant) Hamiltonian locality testing problem, introduced in [Bluhm, Caro,Oufkir `24], is to determine whether a Hamiltonian $H$ is $\\varepsilon_1$-close to being $k$-local (i.e. can be written as the sum of weight-$k$ Pauli operators) or $\\varepsilon_2$-far from any $k$-local Hamiltonian, given access to its time evolution operator and using as little total evolution time as possible, with distance typically defined by the normalized Frobenius norm. We give the tightest known bounds for this problem, proving an $\\text{O}\\left(\\sqrt{\\frac{\\varepsilon_2}{(\\varepsilon_2-\\varepsilon_1)^5}}\\right)$ evolution time upper bound and an $\\Omega\\left(\\frac{1}{\\varepsilon_2-\\varepsilon_1}\\right)$ lower bound. Our algorithm does not require reverse time evolution or controlled application of the time evolution operator, although our lower bound applies to algorithms using either tool.","authors":["John Kallaugher","Daniel Liang"],"url":"https://arxiv.org/abs/2505.06478"}
{"created":"2025-05-13","title":"On the Existence of Lagrange Multipliers in Distribution Network Reconfiguration Problems","abstract":"Distribution network reconfiguration (DNR) is an effective approach for optimizing distribution network operation. However, the DNR problem is computationally challenging due to the mixed-integer non-convex nature. One feasible approach for addressing this challenge is to reformulate binary line on/off state variables as (continuous) non-convex constraints, leading to a nonconvex program. Unfortunately, it remains unclear whether this formulation satisfies the Karush-Kuhn-Tucker (KKT) conditions at locally optimal solutions. In this brief, we study the existence of Lagrange multipliers in DNR problems and prove that under mild assumptions, Lagrange multipliers exist for the DNR model at every locally optimal solution almost surely such that the KKT conditions hold.","authors":["Rong-Peng Liu","Yue Song","Xiaozhe Wang","Bo Zeng"],"url":"https://arxiv.org/abs/2505.06488"}
{"created":"2025-05-13","title":"PC-SRGAN: Physically Consistent Super-Resolution Generative Adversarial Network for General Transient Simulations","abstract":"Machine Learning, particularly Generative Adversarial Networks (GANs), has revolutionised Super Resolution (SR). However, generated images often lack physical meaningfulness, which is essential for scientific applications. Our approach, PC-SRGAN, enhances image resolution while ensuring physical consistency for interpretable simulations. PC-SRGAN significantly improves both the Peak Signal-to-Noise Ratio and the Structural Similarity Index Measure compared to conventional methods, even with limited training data (e.g., only 13% of training data required for SRGAN). Beyond SR, PC-SRGAN augments physically meaningful machine learning, incorporating numerically justified time integrators and advanced quality metrics. These advancements promise reliable and causal machine-learning models in scientific domains. A significant advantage of PC-SRGAN over conventional SR techniques is its physical consistency, which makes it a viable surrogate model for time-dependent problems. PC-SRGAN advances scientific machine learning, offering improved accuracy and efficiency for image processing, enhanced process understanding, and broader applications to scientific research. The source codes and data will be made publicly available at https://github.com/hasan-rakibul/PC-SRGAN upon acceptance of this paper.","authors":["Md Rakibul Hasan","Pouria Behnoudfar","Dan MacKinlay","Thomas Poulet"],"url":"https://arxiv.org/abs/2505.06502"}
{"created":"2025-05-13","title":"Attention Mechanisms in Dynamical Systems: A Case Study with Predator-Prey Models","abstract":"Attention mechanisms are widely used in artificial intelligence to enhance performance and interpretability. In this paper, we investigate their utility in modeling classical dynamical systems -- specifically, a noisy predator-prey (Lotka-Volterra) system. We train a simple linear attention model on perturbed time-series data to reconstruct system trajectories. Remarkably, the learned attention weights align with the geometric structure of the Lyapunov function: high attention corresponds to flat regions (where perturbations have small effect), and low attention aligns with steep regions (where perturbations have large effect). We further demonstrate that attention-based weighting can serve as a proxy for sensitivity analysis, capturing key phase-space properties without explicit knowledge of the system equations. These results suggest a novel use of AI-derived attention for interpretable, data-driven analysis and control of nonlinear systems. For example our framework could support future work in biological modeling of circadian rhythms, and interpretable machine learning for dynamical environments.","authors":["David Balaban"],"url":"https://arxiv.org/abs/2505.06503"}
{"created":"2025-05-13","title":"High-Dimensional Importance-Weighted Information Criteria: Theory and Optimality","abstract":"Imori and Ing (2025) proposed the importance-weighted orthogonal greedy algorithm (IWOGA) for model selection in high-dimensional misspecified regression models under covariate shift. To determine the number of IWOGA iterations, they introduced the high-dimensional importance-weighted information criterion (HDIWIC). They argued that the combined use of IWOGA and HDIWIC, IWOGA + HDIWIC, achieves an optimal trade-off between variance and squared bias, leading to optimal convergence rates in terms of conditional mean squared prediction error. In this article, we provide a theoretical justification for this claim by establishing the optimality of IWOGA + HDIWIC under a set of reasonable assumptions.","authors":["Yong-Syun Cao","Shinpei Imori","Ching-Kang Ing"],"url":"https://arxiv.org/abs/2505.06531"}
{"created":"2025-05-13","title":"Event-based Neural Spike Detection Using Spiking Neural Networks for Neuromorphic iBMI Systems","abstract":"Implantable brain-machine interfaces (iBMIs) are evolving to record from thousands of neurons wirelessly but face challenges in data bandwidth, power consumption, and implant size. We propose a novel Spiking Neural Network Spike Detector (SNN-SPD) that processes event-based neural data generated via delta modulation and pulse count modulation, converting signals into sparse events. By leveraging the temporal dynamics and inherent sparsity of spiking neural networks, our method improves spike detection performance while maintaining low computational overhead suitable for implantable devices. Our experimental results demonstrate that the proposed SNN-SPD achieves an accuracy of 95.72% at high noise levels (standard deviation 0.2), which is about 2% higher than the existing Artificial Neural Network Spike Detector (ANN-SPD). Moreover, SNN-SPD requires only 0.41% of the computation and about 26.62% of the weight parameters compared to ANN-SPD, with zero multiplications. This approach balances efficiency and performance, enabling effective data compression and power savings for next-generation iBMIs.","authors":["Chanwook Hwang","Biyan Zhou","Ye Ke","Vivek Mohan","Jong Hwan Ko","Arindam Basu"],"url":"https://arxiv.org/abs/2505.06544"}
{"created":"2025-05-13","title":"Optimal Transport for Machine Learners","abstract":"Optimal Transport is a foundational mathematical theory that connects optimization, partial differential equations, and probability. It offers a powerful framework for comparing probability distributions and has recently become an important tool in machine learning, especially for designing and evaluating generative models. These course notes cover the fundamental mathematical aspects of OT, including the Monge and Kantorovich formulations, Brenier's theorem, the dual and dynamic formulations, the Bures metric on Gaussian distributions, and gradient flows. It also introduces numerical methods such as linear programming, semi-discrete solvers, and entropic regularization. Applications in machine learning include topics like training neural networks via gradient flows, token dynamics in transformers, and the structure of GANs and diffusion models. These notes focus primarily on mathematical content rather than deep learning techniques.","authors":["Gabriel Peyr\\'e"],"url":"https://arxiv.org/abs/2505.06589"}
{"created":"2025-05-13","title":"Feature Representation Transferring to Lightweight Models via Perception Coherence","abstract":"In this paper, we propose a method for transferring feature representation to lightweight student models from larger teacher models. We mathematically define a new notion called \\textit{perception coherence}. Based on this notion, we propose a loss function, which takes into account the dissimilarities between data points in feature space through their ranking. At a high level, by minimizing this loss function, the student model learns to mimic how the teacher model \\textit{perceives} inputs. More precisely, our method is motivated by the fact that the representational capacity of the student model is weaker than the teacher model. Hence, we aim to develop a new method allowing for a better relaxation. This means that, the student model does not need to preserve the absolute geometry of the teacher one, while preserving global coherence through dissimilarity ranking. Our theoretical insights provide a probabilistic perspective on the process of feature representation transfer. Our experiments results show that our method outperforms or achieves on-par performance compared to strong baseline methods for representation transferring.","authors":["Hai-Vy Nguyen","Fabrice Gamboa","Sixin Zhang","Reda Chhaibi","Serge Gratton","Thierry Giaccone"],"url":"https://arxiv.org/abs/2505.06595"}
{"created":"2025-05-13","title":"Learning Guarantee of Reward Modeling Using Deep Neural Networks","abstract":"In this work, we study the learning theory of reward modeling with pairwise comparison data using deep neural networks. We establish a novel non-asymptotic regret bound for deep reward estimators in a non-parametric setting, which depends explicitly on the network architecture. Furthermore, to underscore the critical importance of clear human beliefs, we introduce a margin-type condition that assumes the conditional winning probability of the optimal action in pairwise comparisons is significantly distanced from 1/2. This condition enables a sharper regret bound, which substantiates the empirical efficiency of Reinforcement Learning from Human Feedback and highlights clear human beliefs in its success. Notably, this improvement stems from high-quality pairwise comparison data implied by the margin-type condition, is independent of the specific estimators used, and thus applies to various learning algorithms and models.","authors":["Yuanhang Luo","Yeheng Ge","Ruijian Han","Guohao Shen"],"url":"https://arxiv.org/abs/2505.06601"}
{"created":"2025-05-13","title":"Impact of internal noise on convolutional neural networks","abstract":"In this paper, we investigate the impact of noise on a simplified trained convolutional network. The types of noise studied originate from a real optical implementation of a neural network, but we generalize these types to enhance the applicability of our findings on a broader scale. The noise types considered include additive and multiplicative noise, which relate to how noise affects individual neurons, as well as correlated and uncorrelated noise, which pertains to the influence of noise across one layers. We demonstrate that the propagation of uncorrelated noise primarily depends on the statistical properties of the connection matrices. Specifically, the mean value of the connection matrix following the layer impacted by noise governs the propagation of correlated additive noise, while the mean of its square contributes to the accumulation of uncorrelated noise. Additionally, we propose an analytical assessment of the noise level in the network's output signal, which shows a strong correlation with the results of numerical simulations.","authors":["Ivan Kolesnikov","Nadezhda Semenova"],"url":"https://arxiv.org/abs/2505.06611"}
{"created":"2025-05-13","title":"Reproducing and Improving CheXNet: Deep Learning for Chest X-ray Disease Classification","abstract":"Deep learning for radiologic image analysis is a rapidly growing field in biomedical research and is likely to become a standard practice in modern medicine. On the publicly available NIH ChestX-ray14 dataset, containing X-ray images that are classified by the presence or absence of 14 different diseases, we reproduced an algorithm known as CheXNet, as well as explored other algorithms that outperform CheXNet's baseline metrics. Model performance was primarily evaluated using the F1 score and AUC-ROC, both of which are critical metrics for imbalanced, multi-label classification tasks in medical imaging. The best model achieved an average AUC-ROC score of 0.85 and an average F1 score of 0.39 across all 14 disease classifications present in the dataset.","authors":["Daniel Strick","Carlos Garcia","Anthony Huang"],"url":"https://arxiv.org/abs/2505.06646"}
{"created":"2025-05-13","title":"RADE: A Neural Codec for Transmitting Speech over HF Radio Channels","abstract":"Speech compression is commonly used to send voice over radio channels in applications such as mobile telephony and two-way push-to-talk (PTT) radio. In classical systems, the speech codec is combined with forward error correction, modulation and radio hardware. In this paper we describe an autoencoder that replaces many of the traditional signal processing elements with a neural network. The encoder takes a vocoder feature set (short term spectrum, pitch, voicing), and produces discrete time, but continuously valued quadrature amplitude modulation (QAM) symbols. We use orthogonal frequency domain multiplexing (OFDM) to send and receive these symbols over high frequency (HF) radio channels. The decoder converts received QAM symbols to vocoder features suitable for synthesis. The autoencoder has been trained to be robust to additive Gaussian noise and multipath channel impairments while simultaneously maintaining a Peak To Average Power Ratio (PAPR) of less than 1~dB. Over simulated and real world HF radio channels we have achieved output speech intelligibility that clearly surpasses existing analog and digital radio systems over a range of SNRs.","authors":["David Rowe","Jean-Marc Valin"],"url":"https://arxiv.org/abs/2505.06671"}
{"created":"2025-05-13","title":"Dynamic feedback linearization of two-input control systems via successive one-fold prolongations","abstract":"In this paper, we propose a constructive algorithm to dynamically linearize two-input control systems via successive one-fold prolongations of a control that has to be suitably chosen at each step of the algorithm. Linearization via successive one-fold prolongations requires special properties of the linearizability distributions $\\mathcal{D}^0 \\subset \\mathcal{D}^1 \\subset\\mathcal{D}^2 \\subset \\cdots$. Contrary to the case of static feedback linearizability, they need not be involutive but the first noninvolutive one has to contain an involutive subdistribution of corank one. The main idea of the proposed algorithm is to replace, at each step, the first noninvolutive distribution by its involutive subdistribution of corank one, thus for the prolonged system we gain at least one new involutive distribution. Our algorithm is constructive, gives sufficient conditions for flatness, and can be seen as the dual of the dynamic feedback linearization algorithm of Battilotti and Califano [2003, 2005].","authors":["Florentina Nicolau","Witold Respondek","Shunjie Li"],"url":"https://arxiv.org/abs/2505.06677"}
{"created":"2025-05-13","title":"A Short Overview of Multi-Modal Wi-Fi Sensing","abstract":"Wi-Fi sensing has emerged as a significant technology in wireless sensing and Integrated Sensing and Communication (ISAC), offering benefits such as low cost, high penetration, and enhanced privacy. Currently, it is widely utilized in various applications, including action recognition, human localization, and crowd counting. However, Wi-Fi sensing also faces challenges, such as low robustness and difficulties in data collection. Recently, there has been an increasing focus on multi-modal Wi-Fi sensing, where other modalities can act as teachers, providing ground truth or robust features for Wi-Fi sensing models to learn from, or can be directly fused with Wi-Fi for enhanced sensing capabilities. Although these methods have demonstrated promising results and substantial value in practical applications, there is a lack of comprehensive surveys reviewing them. To address this gap, this paper reviews the multi-modal Wi-Fi sensing literature \\textbf{from the past 24 months} and highlights the current limitations, challenges and future directions in this field.","authors":["Zijian Zhao"],"url":"https://arxiv.org/abs/2505.06682"}
{"created":"2025-05-13","title":"Distributed Event-Triggered Nash Equilibrium Seeking for Noncooperative Games","abstract":"We propose locally convergent Nash equilibrium seeking algorithms for $N$-player noncooperative games, which use distributed event-triggered pseudo-gradient estimates. The proposed approach employs sinusoidal perturbations to estimate the pseudo-gradients of unknown quadratic payoff functions. This is the first instance of noncooperative games being tackled in a model-free fashion with event-triggered extremum seeking. Each player evaluates independently the deviation between the corresponding current pseudo-gradient estimate and its last broadcasted value from the event-triggering mechanism to tune individually the player action, while they preserve collectively the closed-loop stability/convergence. We guarantee Zeno behavior avoidance by establishing a minimum dwell-time to avoid infinitely fast switching. In particular, the stability analysis is carried out using Lyapunov's method and averaging for systems with discontinuous right-hand sides. We quantify the size of the ultimate small residual sets around the Nash equilibrium and illustrate the theoretical results numerically on an oligopoly setting.","authors":["Victor Hugo Pereira Rodrigues","Tiago Roux Oliveira","Miroslav Krstic","Tamer Basar"],"url":"https://arxiv.org/abs/2505.06691"}
{"created":"2025-05-13","title":"Efficient Parallelization of Message Passing Neural Networks","abstract":"Machine learning potentials have achieved great success in accelerating atomistic simulations. Many of them rely on local descriptors that readily allow parallelization. More recent message passing neural network (MPNN) models have demonstrated their superior accuracy and become increasingly popular. However, parallelizing MPNN models for large-scale simulations across compute nodes remains a challenge, as the previously argued poor scalability with the number of MP layers and the necessity of data communication. Here, we propose an efficient parallel algorithm for MPNN models, in which additional data communication is minimized among local atoms only in each MP layer without redundant computation, thus scaling linearly with the layer number. Integrated with our recursively embedded atom neural network model, this algorithm demonstrates excellent strong scaling and weak scaling behaviors in several benchmark systems. This approach enables massive molecular dynamics simulations on MPNN models for hundreds of millions of atoms as fast as on strictly local models, vastly extending the applicability of the MPNN potential to an unprecedented scale. This general parallelization framework can empower various MPNN models to efficiently simulate very large and complex systems.","authors":["Junfan Xia","Bin Jiang"],"url":"https://arxiv.org/abs/2505.06711"}
{"created":"2025-05-13","title":"Online Job Scheduler for Fault-tolerant Quantum Multiprogramming","abstract":"Fault-tolerant quantum computers are expected to be offered as cloud services due to their significant resource and infrastructure requirements. Quantum multiprogramming, which runs multiple quantum jobs in parallel, is a promising approach to maximize the utilization of such systems. A key challenge in this setting is the need for an online scheduler capable of handling jobs submitted dynamically while other programs are already running. In this study, we formulate the online job scheduling problem for fault-tolerant quantum computing systems based on lattice surgery and propose an efficient scheduler to address it. To meet the responsiveness required in an online environment, our scheduler approximates lattice surgery programs, originally represented as polycubes, by using simpler cuboid representations. This approximation enables efficient scheduling while improving overall throughput. In addition, we incorporate a defragmentation mechanism into the scheduling process, demonstrating that it can further enhance QPU utilization.","authors":["Shin Nishio","Ryo Wakizaka","Daisuke Sakuma","Yosuke Ueno","Yasunari Suzuki"],"url":"https://arxiv.org/abs/2505.06741"}
{"created":"2025-05-13","title":"Out-of-Sample Embedding with Proximity Data: Projection versus Restricted Reconstruction","abstract":"The problem of using proximity (similarity or dissimilarity) data for the purpose of \"adding a point to a vector diagram\" was first studied by J.C. Gower in 1968. Since then, a number of methods -- mostly kernel methods -- have been proposed for solving what has come to be called the problem of *out-of-sample embedding*. We survey the various kernel methods that we have encountered and show that each can be derived from one or the other of two competing strategies: *projection* or *restricted reconstruction*. Projection can be analogized to a well-known formula for adding a point to a principal component analysis. Restricted reconstruction poses a different challenge: how to best approximate redoing the entire multivariate analysis while holding fixed the vector diagram that was previously obtained. This strategy results in a nonlinear optimization problem that can be simplified to a unidimensional search. Various circumstances may warrant either projection or restricted reconstruction.","authors":["Michael W. Trosset","Kaiyi Tan","Minh Tang","Carey E. Priebe"],"url":"https://arxiv.org/abs/2505.06756"}
{"created":"2025-05-13","title":"Quantum RNNs and LSTMs Through Entangling and Disentangling Power of Unitary Transformations","abstract":"In this paper, we discuss how quantum recurrent neural networks (RNNs) and their enhanced version, long short-term memory (LSTM) networks, can be modeled using the core ideas presented in Ref.[1], where the entangling and disentangling power of unitary transformations is investigated. In particular, we interpret entangling and disentangling power as information retention and forgetting mechanisms in LSTMs. Therefore, entanglement becomes a key component of the optimization (training) process. We believe that, by leveraging prior knowledge of the entangling power of unitaries, the proposed quantum-classical framework can guide and help to design better-parameterized quantum circuits for various real-world applications.","authors":["Ammar Daskin"],"url":"https://arxiv.org/abs/2505.06774"}
{"created":"2025-05-13","title":"HistDiST: Histopathological Diffusion-based Stain Transfer","abstract":"Hematoxylin and Eosin (H&amp;E) staining is the cornerstone of histopathology but lacks molecular specificity. While Immunohistochemistry (IHC) provides molecular insights, it is costly and complex, motivating H&amp;E-to-IHC translation as a cost-effective alternative. Existing translation methods are mainly GAN-based, often struggling with training instability and limited structural fidelity, while diffusion-based approaches remain underexplored. We propose HistDiST, a Latent Diffusion Model (LDM) based framework for high-fidelity H&amp;E-to-IHC translation. HistDiST introduces a dual-conditioning strategy, utilizing Phikon-extracted morphological embeddings alongside VAE-encoded H&amp;E representations to ensure pathology-relevant context and structural consistency. To overcome brightness biases, we incorporate a rescaled noise schedule, v-prediction, and trailing timesteps, enforcing a zero-SNR condition at the final timestep. During inference, DDIM inversion preserves the morphological structure, while an eta-cosine noise schedule introduces controlled stochasticity, balancing structural consistency and molecular fidelity. Moreover, we propose Molecular Retrieval Accuracy (MRA), a novel pathology-aware metric leveraging GigaPath embeddings to assess molecular relevance. Extensive evaluations on MIST and BCI datasets demonstrate that HistDiST significantly outperforms existing methods, achieving a 28% improvement in MRA on the H&amp;E-to-Ki67 translation task, highlighting its effectiveness in capturing true IHC semantics.","authors":["Erik Gro{\\ss}kopf","Valay Bundele","Mehran Hossienzadeh","Hendrik P. A. Lensch"],"url":"https://arxiv.org/abs/2505.06793"}
{"created":"2025-05-13","title":"Quantum Observers: A NISQ Hardware Demonstration of Chaotic State Prediction Using Quantum Echo-state Networks","abstract":"Recent advances in artificial intelligence have highlighted the remarkable capabilities of neural network (NN)-powered systems on classical computers. However, these systems face significant computational challenges that limit scalability and efficiency. Quantum computers hold the potential to overcome these limitations and increase processing power beyond classical systems. Despite this, integrating quantum computing with NNs remains largely unrealized due to challenges posed by noise, decoherence, and high error rates in current quantum hardware. Here, we propose a novel quantum echo-state network (QESN) design and implementation algorithm that can operate within the presence of noise on current IBM hardware. We apply classical control-theoretic response analysis to characterize the QESN, emphasizing its rich nonlinear dynamics and memory, as well as its ability to be fine-tuned with sparsity and re-uploading blocks. We validate our approach through a comprehensive demonstration of QESNs functioning as quantum observers, applied in both high-fidelity simulations and hardware experiments utilizing data from a prototypical chaotic Lorenz system. Our results show that the QESN can predict long time-series with persistent memory, running over 100 times longer than the median T}1 and T2 of the IBM Marrakesh QPU, achieving state-of-the-art time-series performance on superconducting hardware.","authors":["Erik L. Connerty","Ethan N. Evans","Gerasimos Angelatos","Vignesh Narayanan"],"url":"https://arxiv.org/abs/2505.06799"}
{"created":"2025-05-13","title":"Reverse-BSDE Monte Carlo","abstract":"Recently, there has been a growing interest in generative models based on diffusions driven by the empirical robustness of these methods in generating high-dimensional photorealistic images and the possibility of using the vast existing toolbox of stochastic differential equations. %This remarkable ability may stem from their capacity to model and generate multimodal distributions. In this work, we offer a novel perspective on the approach introduced in Song et al. (2021), shifting the focus from a \"learning\" problem to a \"sampling\" problem. To achieve this, we reformulate the equations governing diffusion-based generative models as a Forward-Backward Stochastic Differential Equation (FBSDE), which avoids the well-known issue of pre-estimating the gradient of the log target density. The solution of this FBSDE is proved to be unique using non-standard techniques. Additionally, we propose a numerical solution to this problem, leveraging on Deep Learning techniques. This reformulation opens new pathways for sampling multidimensional distributions with densities known up to a normalization constant, a problem frequently encountered in Bayesian statistics.","authors":["Jairon H. N. Batista","Fl\\'avio B. Gon\\c{c}alves","Yuri F. Saporito","Rodrigo S. Targino"],"url":"https://arxiv.org/abs/2505.06800"}
{"created":"2025-05-13","title":"A stochastic gradient method for trilevel optimization","abstract":"With the success that the field of bilevel optimization has seen in recent years, similar methodologies have started being applied to solving more difficult applications that arise in trilevel optimization. At the helm of these applications are new machine learning formulations that have been proposed in the trilevel context and, as a result, efficient and theoretically sound stochastic methods are required. In this work, we propose the first-ever stochastic gradient descent method for solving unconstrained trilevel optimization problems and provide a convergence theory that covers all forms of inexactness of the trilevel adjoint gradient, such as the inexact solutions of the middle-level and lower-level problems, inexact computation of the trilevel adjoint formula, and noisy estimates of the gradients, Hessians, Jacobians, and tensors of third-order derivatives involved. We also demonstrate the promise of our approach by providing numerical results on both synthetic trilevel problems and trilevel formulations for hyperparameter adversarial tuning.","authors":["Tommaso Giovannelli","Griffin Dean Kent","Luis Nunes Vicente"],"url":"https://arxiv.org/abs/2505.06805"}
{"created":"2025-05-13","title":"Missing Data Estimation for MR Spectroscopic Imaging via Mask-Free Deep Learning Methods","abstract":"Magnetic Resonance Spectroscopic Imaging (MRSI) is a powerful tool for non-invasive mapping of brain metabolites, providing critical insights into neurological conditions. However, its utility is often limited by missing or corrupted data due to motion artifacts, magnetic field inhomogeneities, or failed spectral fitting-especially in high resolution 3D acquisitions. To address this, we propose the first deep learning-based, mask-free framework for estimating missing data in MRSI metabolic maps. Unlike conventional restoration methods that rely on explicit masks to identify missing regions, our approach implicitly detects and estimates these areas using contextual spatial features through 2D and 3D U-Net architectures. We also introduce a progressive training strategy to enhance robustness under varying levels of data degradation. Our method is evaluated on both simulated and real patient datasets and consistently outperforms traditional interpolation techniques such as cubic and linear interpolation. The 2D model achieves an MSE of 0.002 and an SSIM of 0.97 with 20% missing voxels, while the 3D model reaches an MSE of 0.001 and an SSIM of 0.98 with 15% missing voxels. Qualitative results show improved fidelity in estimating missing data, particularly in metabolically heterogeneous regions and ventricular regions. Importantly, our model generalizes well to real-world datasets without requiring retraining or mask input. These findings demonstrate the effectiveness and broad applicability of mask-free deep learning for MRSI restoration, with strong potential for clinical and research integration.","authors":["Tan-Hanh Pham","Ovidiu C. Andronesi","Xianqi Li","Kim-Doang Nguyen"],"url":"https://arxiv.org/abs/2505.06811"}
{"created":"2025-05-13","title":"NewsNet-SDF: Stochastic Discount Factor Estimation with Pretrained Language Model News Embeddings via Adversarial Networks","abstract":"Stochastic Discount Factor (SDF) models provide a unified framework for asset pricing and risk assessment, yet traditional formulations struggle to incorporate unstructured textual information. We introduce NewsNet-SDF, a novel deep learning framework that seamlessly integrates pretrained language model embeddings with financial time series through adversarial networks. Our multimodal architecture processes financial news using GTE-multilingual models, extracts temporal patterns from macroeconomic data via LSTM networks, and normalizes firm characteristics, fusing these heterogeneous information sources through an innovative adversarial training mechanism. Our dataset encompasses approximately 2.5 million news articles and 10,000 unique securities, addressing the computational challenges of processing and aligning text data with financial time series. Empirical evaluations on U.S. equity data (1980-2022) demonstrate NewsNet-SDF substantially outperforms alternatives with a Sharpe ratio of 2.80. The model shows a 471% improvement over CAPM, over 200% improvement versus traditional SDF implementations, and a 74% reduction in pricing errors compared to the Fama-French five-factor model. In comprehensive comparisons, our deep learning approach consistently outperforms traditional, modern, and other neural asset pricing models across all key metrics. Ablation studies confirm that text embeddings contribute significantly more to model performance than macroeconomic features, with news-derived principal components ranking among the most influential determinants of SDF dynamics. These results validate the effectiveness of our multimodal deep learning approach in integrating unstructured text with traditional financial data for more accurate asset pricing, providing new insights for digital intelligent decision-making in financial technology.","authors":["Shunyao Wang","Ming Cheng","Christina Dan Wang"],"url":"https://arxiv.org/abs/2505.06864"}
{"created":"2025-05-13","title":"Near-Field Channel Estimation for XL-MIMO: A Deep Generative Model Guided by Side Information","abstract":"This paper investigates the near-field (NF) channel estimation (CE) for extremely large-scale multiple-input multiple-output (XL-MIMO) systems. Considering the pronounced NF effects in XL-MIMO communications, we first establish a joint angle-distance (AD) domain-based spherical-wavefront physical channel model that captures the inherent sparsity of XL-MIMO channels. Leveraging the channel's sparsity in the joint AD domain, the CE is approached as a task of reconstructing sparse signals. Anchored in this framework, we first propose a compressed sensing algorithm to acquire a preliminary channel estimate. Harnessing the powerful implicit prior learning capability of generative artificial intelligence (GenAI), we further propose a GenAI-based approach to refine the estimated channel. Specifically, we introduce the preliminary estimated channel as side information, and derive the evidence lower bound (ELBO) of the log-marginal distribution of the target NF channel conditioned on the preliminary estimated channel, which serves as the optimization objective for the proposed generative diffusion model (GDM). Additionally, we introduce a more generalized version of the GDM, the non-Markovian GDM (NM-GDM), to accelerate the sampling process, achieving an approximately tenfold enhancement in sampling efficiency. Experimental results indicate that the proposed approach is capable of offering substantial performance gain in CE compared to existing benchmark schemes within NF XL-MIMO systems. Furthermore, our approach exhibits enhanced generalization capabilities in both the NF or far-field (FF) regions.","authors":["Zhenzhou Jin","Li You","Derrick Wing Kwan Ng","Xiang-Gen Xia","Xiqi Gao"],"url":"https://arxiv.org/abs/2505.06900"}
{"created":"2025-05-13","title":"Uni-AIMS: AI-Powered Microscopy Image Analysis","abstract":"This paper presents a systematic solution for the intelligent recognition and automatic analysis of microscopy images. We developed a data engine that generates high-quality annotated datasets through a combination of the collection of diverse microscopy images from experiments, synthetic data generation and a human-in-the-loop annotation process. To address the unique challenges of microscopy images, we propose a segmentation model capable of robustly detecting both small and large objects. The model effectively identifies and separates thousands of closely situated targets, even in cluttered visual environments. Furthermore, our solution supports the precise automatic recognition of image scale bars, an essential feature in quantitative microscopic analysis. Building upon these components, we have constructed a comprehensive intelligent analysis platform and validated its effectiveness and practicality in real-world applications. This study not only advances automatic recognition in microscopy imaging but also ensures scalability and generalizability across multiple application domains, offering a powerful tool for automated microscopic analysis in interdisciplinary research.","authors":["Yanhui Hong","Nan Wang","Zhiyi Xia","Haoyi Tao","Xi Fang","Yiming Li","Jiankun Wang","Peng Jin","Xiaochen Cai","Shengyu Li","Ziqi Chen","Zezhong Zhang","Guolin Ke","Linfeng Zhang"],"url":"https://arxiv.org/abs/2505.06918"}
{"created":"2025-05-13","title":"Stability Regularized Cross-Validation","abstract":"We revisit the problem of ensuring strong test-set performance via cross-validation. Motivated by the generalization theory literature, we propose a nested k-fold cross-validation scheme that selects hyperparameters by minimizing a weighted sum of the usual cross-validation metric and an empirical model-stability measure. The weight on the stability term is itself chosen via a nested cross-validation procedure. This reduces the risk of strong validation set performance and poor test set performance due to instability. We benchmark our procedure on a suite of 13 real-world UCI datasets, and find that, compared to k-fold cross-validation over the same hyperparameters, it improves the out-of-sample MSE for sparse ridge regression and CART by 4% on average, but has no impact on XGBoost. This suggests that for interpretable and unstable models, such as sparse regression and CART, our approach is a viable and computationally affordable method for improving test-set performance.","authors":["Ryan Cory-Wright","Andr\\'es G\\'omez"],"url":"https://arxiv.org/abs/2505.06927"}
{"created":"2025-05-13","title":"Unraveling Quantum Environments: Transformer-Assisted Learning in Lindblad Dynamics","abstract":"Understanding dissipation in open quantum systems is crucial for the development of robust quantum technologies. In this work, we introduce a Transformer-based machine learning framework to infer time-dependent dissipation rates in quantum systems governed by the Lindblad master equation. Our approach uses time series of observable quantities, such as expectation values of single Pauli operators, as input to learn dissipation profiles without requiring knowledge of the initial quantum state or even the system Hamiltonian.","authors":["Chi-Sheng Chen","En-Jui Kuo"],"url":"https://arxiv.org/abs/2505.06928"}
{"created":"2025-05-13","title":"Whitened CLIP as a Likelihood Surrogate of Images and Captions","abstract":"Likelihood approximations for images are not trivial to compute and can be useful in many applications. We examine the use of Contrastive Language-Image Pre-training (CLIP) to assess the likelihood of images and captions. We introduce \\textit{Whitened CLIP}, a novel transformation of the CLIP latent space via an invertible linear operation. This transformation ensures that each feature in the embedding space has zero mean, unit standard deviation, and no correlation with all other features, resulting in an identity covariance matrix. We show that the whitened embeddings statistics can be well approximated as a standard normal distribution, thus, the log-likelihood is estimated simply by the square Euclidean norm in the whitened embedding space. The whitening procedure is completely training-free and performed using a pre-computed whitening matrix, hence, is very fast. We present several preliminary experiments demonstrating the properties and applicability of these likelihood scores to images and captions.","authors":["Roy Betser","Meir Yossef Levi","Guy Gilboa"],"url":"https://arxiv.org/abs/2505.06934"}
{"created":"2025-05-13","title":"Price Equilibria in a Spatial Competition with Captive Buyers","abstract":"This paper explores price competition with exogenous product differentiation in a spatial model similar to that of Nakagawa (2023). Nakagawa examines product differentiation within the framework of Varian (1980). Nakagawa integrates Varian's concept of uninformed consumers, who lack complete price information, into a spatial model based on Hotelling (1929). While Nakagawa placed informed consumers at the center of the Hotelling line and used quadratic transportation costs, our study employs a uniform distribution of informed consumers and linear transportation costs. This approach enables a more direct comparison with established spatial competition literature, particularly Osborne and Pitchik (1987). We classify equilibrium candidates and characterize the parameter regions corresponding to each equilibrium. There is no pure equilibrium in the region where we construct mixed strategy equilibria. Furthermore, we compare the expected profit in the equilibrium of our model with the findings of Osborne and Pitchik (1987). Finally, we discuss the impact of captive buyers on the nature of spatial competition.","authors":["Shinnosuke Kawai","Kuninori Nakagawa"],"url":"https://arxiv.org/abs/2505.06961"}
{"created":"2025-05-13","title":"When cardinals strategize: An agent-based model of influence and ideology for the papal conclave","abstract":"We propose and analyze two agent-based models to investigate the dynamics of papal conclaves, focusing on how social influence, strategic voting, and ideological alignment affect the time required to elect a pope. In the first model, cardinals interact through two mechanisms: with probability $p$, they imitate the choice of a randomly selected peer, and with probability $q$, they shift support to the most voted candidate from the previous round. Additionally, strategic behavior is introduced via ``useful voting'', where agents abandon their preferred candidate if he receives less than a certain fraction of the votes, switching instead to the most viable alternative. A candidate must secure a qualified majority of two-thirds to be elected. After that, we extend the framework by incorporating ideological blocs, assigning each cardinal and candidate to one of two groups (e.g., progressives and conservatives). Cardinals initially vote for candidates from their own group, but may cross ideological lines due to strategic considerations. We initialize the population with $20\\%$ conservative cardinals, reflecting the current composition shaped by papal appointments. Numerical simulations show that ideological polarization can delay the election, increasing the average number of voting rounds. Our results highlight the crucial role of both strategic flexibility and ideological structure in collective decision-making under conditions found in papal conclaves. The recent rapid outcome of the 2025 conclave, despite a polarized electorate, suggests that informal consensus-building - possibly prior to voting - may play a decisive role in accelerating convergence, complementing the mechanisms explored in our model.","authors":["Nuno Crokidakis"],"url":"https://arxiv.org/abs/2505.07014"}
{"created":"2025-05-13","title":"Rate of Convergence for a Nonlocal-to-local Limit in One Dimension","abstract":"We consider a nonlocal approximation of the quadratic porous medium equation where the pressure is given by a convolution with a mollification kernel. It is known that when the kernel concentrates around the origin, the nonlocal equation converges to the local one. In one spatial dimension, for a particular choice of the kernel, and under mere assumptions on the initial condition, we quantify the rate of convergence in the 2-Wasserstein distance. Our proof is very simple, exploiting the so-called Evolutionary Variational Inequality for both the nonlocal and local equations as well as a priori estimates. We also present numerical simulations using the finite volume method, which suggests that the obtained rate can be improved - this will be addressed in a forthcoming work.","authors":["Jos\\'e A. Carrillo","Charles Elbar","Stefano Fronzoni","Jakub Skrzeczkowski"],"url":"https://arxiv.org/abs/2505.07015"}
{"created":"2025-05-13","title":"Outperformance Score: A Universal Standardization Method for Confusion-Matrix-Based Classification Performance Metrics","abstract":"Many classification performance metrics exist, each suited to a specific application. However, these metrics often differ in scale and can exhibit varying sensitivity to class imbalance rates in the test set. As a result, it is difficult to use the nominal values of these metrics to interpret and evaluate classification performances, especially when imbalance rates vary. To address this problem, we introduce the outperformance score function, a universal standardization method for confusion-matrix-based classification performance (CMBCP) metrics. It maps any given metric to a common scale of $[0,1]$, while providing a clear and consistent interpretation. Specifically, the outperformance score represents the percentile rank of the observed classification performance within a reference distribution of possible performances. This unified framework enables meaningful comparison and monitoring of classification performance across test sets with differing imbalance rates. We illustrate how the outperformance scores can be applied to a variety of commonly used classification performance metrics and demonstrate the robustness of our method through experiments on real-world datasets spanning multiple classification applications.","authors":["Ningsheng Zhao","Trang Bui","Jia Yuan Yu","Krzysztof Dzieciolowski"],"url":"https://arxiv.org/abs/2505.07033"}
{"created":"2025-05-13","title":"More Than Opinions: The Role of Values in Shaping Fairness and Status in the Ultimatum Game within Structured Societies","abstract":"Asymmetric evolutionary games, such as the Ultimatum Game, provide keys to understanding the emergence of fairness in social species. Building on this framework, we explore the evolution of social value systems and the operational role that social status plays in hierarchically organised societies. Within the asymmetric Ultimatum Game paradigm, where \"proposers\" suggest terms for resource distribution, and \"responders\" accept or reject these terms, we examine the assignment of roles between players under a subjective social order. This order is grounded in an emergent status hierarchy based on observable player attributes (such as age and wealth). The underlying rules for constructing such a hierarchy stabilise over time by inheritance and family ties. Despite their subjective nature these (often sub-conscious) value systems have operative meaning in controlling access of individuals to resources and decision making. We demonstrate these effects using a simple but sufficiently complex model with dynamical population size and network structure, where division of resources (prey) is carried out according to the principles of the Ultimatum Game. We focus on the emerging proposer and responder thresholds under distinct social hierarchies and interaction networks and discuss them in relation to the extensive body of Ultimatum Game experiments conducted across a wide range of cultural contexts. We observe the emergence of diverse sharing norms, ranging from unfair to highly generous, alongside the development of various social norms.","authors":["Hana Krakovsk\\'a","Rudolf Hanel"],"url":"https://arxiv.org/abs/2505.07060"}
{"created":"2025-05-13","title":"Learning curves theory for hierarchically compositional data with power-law distributed features","abstract":"Recent theories suggest that Neural Scaling Laws arise whenever the task is linearly decomposed into power-law distributed units. Alternatively, scaling laws also emerge when data exhibit a hierarchically compositional structure, as is thought to occur in language and images. To unify these views, we consider classification and next-token prediction tasks based on probabilistic context-free grammars -- probabilistic models that generate data via a hierarchy of production rules. For classification, we show that having power-law distributed production rules results in a power-law learning curve with an exponent depending on the rules' distribution and a large multiplicative constant that depends on the hierarchical structure. By contrast, for next-token prediction, the distribution of production rules controls the local details of the learning curve, but not the exponent describing the large-scale behaviour.","authors":["Francesco Cagnetta","Hyunmo Kang","Matthieu Wyart"],"url":"https://arxiv.org/abs/2505.07067"}
{"created":"2025-05-13","title":"A Sparse Bayesian Learning Algorithm for Estimation of Interaction Kernels in Motsch-Tadmor Model","abstract":"In this paper, we investigate the data-driven identification of asymmetric interaction kernels in the Motsch-Tadmor model based on observed trajectory data. The model under consideration is governed by a class of semilinear evolution equations, where the interaction kernel defines a normalized, state-dependent Laplacian operator that governs collective dynamics. To address the resulting nonlinear inverse problem, we propose a variational framework that reformulates kernel identification using the implicit form of the governing equations, reducing it to a subspace identification problem. We establish an identifiability result that characterizes conditions under which the interaction kernel can be uniquely recovered up to scale. To solve the inverse problem robustly, we develop a sparse Bayesian learning algorithm that incorporates informative priors for regularization, quantifies uncertainty, and enables principled model selection. Extensive numerical experiments on representative interacting particle systems demonstrate the accuracy, robustness, and interpretability of the proposed framework across a range of noise levels and data regimes.","authors":["Jinchao Feng","Sui Tang"],"url":"https://arxiv.org/abs/2505.07068"}
{"created":"2025-05-13","title":"Can LLM-based Financial Investing Strategies Outperform the Market in Long Run?","abstract":"Large Language Models (LLMs) have recently been leveraged for asset pricing tasks and stock trading applications, enabling AI agents to generate investment decisions from unstructured financial data. However, most evaluations of LLM timing-based investing strategies are conducted on narrow timeframes and limited stock universes, overstating effectiveness due to survivorship and data-snooping biases. We critically assess their generalizability and robustness by proposing FINSABER, a backtesting framework evaluating timing-based strategies across longer periods and a larger universe of symbols. Systematic backtests over two decades and 100+ symbols reveal that previously reported LLM advantages deteriorate significantly under broader cross-section and over a longer-term evaluation. Our market regime analysis further demonstrates that LLM strategies are overly conservative in bull markets, underperforming passive benchmarks, and overly aggressive in bear markets, incurring heavy losses. These findings highlight the need to develop LLM strategies that are able to prioritise trend detection and regime-aware risk controls over mere scaling of framework complexity.","authors":["Weixian Waylon Li","Hyeonjun Kim","Mihai Cucuringu","Tiejun Ma"],"url":"https://arxiv.org/abs/2505.07078"}
{"created":"2025-05-13","title":"Constrained Online Decision-Making with Density Estimation Oracles","abstract":"Contextual online decision-making problems with constraints appear in a wide range of real-world applications, such as personalized recommendation with resource limits, adaptive experimental design, and decision-making under safety or fairness requirements. In this paper, we investigate a general formulation of sequential decision-making with stage-wise feasibility constraints, where at each round, the learner must select an action based on observed context while ensuring that a problem-specific feasibility criterion is satisfied. We propose a unified algorithmic framework that captures many existing constrained learning problems, including constrained bandits, active learning with label budgets, online hypothesis testing with Type I error control, and model calibration. Central to our approach is the concept of upper counterfactual confidence bounds, which enables the design of practically efficient online algorithms with strong theoretical guarantee using any offline conditional density estimation oracle. Technically, to handle feasibility constraints in complex environments, we introduce a generalized notion of the eluder dimension - extending it from the classical setting based on square loss to a broader class of metric-like probability divergences. This allows us to capture the complexity of various density function classes and characterize the utility regret incurred due to feasibility constraint uncertainty. Our result offers a principled foundation for constrained sequential decision-making in both theory and practice.","authors":["Haichen Hu","David Simchi-Levi","Navid Azizan"],"url":"https://arxiv.org/abs/2505.07101"}
{"created":"2025-05-13","title":"Infinite trees","abstract":"A few notes about infinite trees in a descriptive set-theoretic setting.","authors":["Alexandre Goy"],"url":"https://arxiv.org/abs/2505.07111"}
{"created":"2025-05-13","title":"Skull stripping with purely synthetic data","abstract":"While many skull stripping algorithms have been developed for multi-modal and multi-species cases, there is still a lack of a fundamentally generalizable approach. We present PUMBA(PUrely synthetic Multimodal/species invariant Brain extrAction), a strategy to train a model for brain extraction with no real brain images or labels. Our results show that even without any real images or anatomical priors, the model achieves comparable accuracy in multi-modal, multi-species and pathological cases. This work presents a new direction of research for any generalizable medical image segmentation task.","authors":["Jong Sung Park","Juhyung Ha","Siddhesh Thakur","Alexandra Badea","Spyridon Bakas","Eleftherios Garyfallidis"],"url":"https://arxiv.org/abs/2505.07159"}
{"created":"2025-05-13","title":"Exact Spin Elimination in Ising Hamiltonians and Energy-Based Machine Learning","abstract":"We present an exact spin-elimination technique that reduces the dimensionality of both quadratic and k-local Ising Hamiltonians while preserving their original ground-state configurations. By systematically replacing each removed spin with an effective interaction among its neighbors, our method lowers the total spin count without invoking approximations or iterative recalculations. This capability is especially beneficial for hardware-constrained platforms, classical or quantum, that can directly implement multi-body interactions but have limited qubit or spin resources. We demonstrate three key advances enabled by this technique. First, we handle larger instances of benchmark problems such as Max-Cut on cubic graphs without exceeding a 2-local interaction limit. Second, we reduce qubit requirements in QAOA-based integer factorization on near-term quantum devices, thus extending the feasible range of integers to be factorized. Third, we improve memory capacity in Hopfield associative memories and enhance memory retrieval by suppressing spurious attractors, enhancing retrieval performance. Our spin-elimination procedure trades local spin complexity for higher-order couplings or higher node degrees in a single pass, opening new avenues for scaling up combinatorial optimization and energy-based machine learning on near-term hardware. Finally, these results underscore that the next-generation physical spin machines will likely capitalize on k-local spin Hamiltonians to offer an alternative to classical computations.","authors":["Natalia G. Berloff"],"url":"https://arxiv.org/abs/2505.07163"}
{"created":"2025-05-13","title":"Metrics that matter: Evaluating image quality metrics for medical image generation","abstract":"Evaluating generative models for synthetic medical imaging is crucial yet challenging, especially given the high standards of fidelity, anatomical accuracy, and safety required for clinical applications. Standard evaluation of generated images often relies on no-reference image quality metrics when ground truth images are unavailable, but their reliability in this complex domain is not well established. This study comprehensively assesses commonly used no-reference image quality metrics using brain MRI data, including tumour and vascular images, providing a representative exemplar for the field. We systematically evaluate metric sensitivity to a range of challenges, including noise, distribution shifts, and, critically, localised morphological alterations designed to mimic clinically relevant inaccuracies. We then compare these metric scores against model performance on a relevant downstream segmentation task, analysing results across both controlled image perturbations and outputs from different generative model architectures. Our findings reveal significant limitations: many widely-used no-reference image quality metrics correlate poorly with downstream task suitability and exhibit a profound insensitivity to localised anatomical details crucial for clinical validity. Furthermore, these metrics can yield misleading scores regarding distribution shifts, e.g. data memorisation. This reveals the risk of misjudging model readiness, potentially leading to the deployment of flawed tools that could compromise patient safety. We conclude that ensuring generative models are truly fit for clinical purpose requires a multifaceted validation framework, integrating performance on relevant downstream tasks with the cautious interpretation of carefully selected no-reference image quality metrics.","authors":["Yash Deo","Yan Jia","Toni Lassila","William A. P. Smith","Tom Lawton","Siyuan Kang","Alejandro F. Frangi","Ibrahim Habli"],"url":"https://arxiv.org/abs/2505.07175"}
{"created":"2025-05-13","title":"The Influence of the Memory Capacity of Neural DDEs on the Universal Approximation Property","abstract":"Neural Ordinary Differential Equations (Neural ODEs), which are the continuous-time analog of Residual Neural Networks (ResNets), have gained significant attention in recent years. Similarly, Neural Delay Differential Equations (Neural DDEs) can be interpreted as an infinite depth limit of Densely Connected Residual Neural Networks (DenseResNets). In contrast to traditional ResNet architectures, DenseResNets are feed-forward networks that allow for shortcut connections across all layers. These additional connections introduce memory in the network architecture, as typical in many modern architectures. In this work, we explore how the memory capacity in neural DDEs influences the universal approximation property. The key parameter for studying the memory capacity is the product $K \\tau$ of the Lipschitz constant and the delay of the DDE. In the case of non-augmented architectures, where the network width is not larger than the input and output dimensions, neural ODEs and classical feed-forward neural networks cannot have the universal approximation property. We show that if the memory capacity $K\\tau$ is sufficiently small, the dynamics of the neural DDE can be approximated by a neural ODE. Consequently, non-augmented neural DDEs with a small memory capacity also lack the universal approximation property. In contrast, if the memory capacity $K\\tau$ is sufficiently large, we can establish the universal approximation property of neural DDEs for continuous functions. If the neural DDE architecture is augmented, we can expand the parameter regions in which universal approximation is possible. Overall, our results show that by increasing the memory capacity $K\\tau$, the infinite-dimensional phase space of DDEs with positive delay $\\tau>0$ is not sufficient to guarantee a direct jump transition to universal approximation, but only after a certain memory threshold, universal approximation holds.","authors":["Christian Kuehn","Sara-Viola Kuntz"],"url":"https://arxiv.org/abs/2505.07244"}
{"created":"2025-05-13","title":"Adaptive, Robust and Scalable Bayesian Filtering for Online Learning","abstract":"In this thesis, we introduce Bayesian filtering as a principled framework for tackling diverse sequential machine learning problems, including online (continual) learning, prequential (one-step-ahead) forecasting, and contextual bandits. To this end, this thesis addresses key challenges in applying Bayesian filtering to these problems: adaptivity to non-stationary environments, robustness to model misspecification and outliers, and scalability to the high-dimensional parameter space of deep neural networks. We develop novel tools within the Bayesian filtering framework to address each of these challenges, including: (i) a modular framework that enables the development adaptive approaches for online learning; (ii) a novel, provably robust filter with similar computational cost to standard filters, that employs Generalised Bayes; and (iii) a set of tools for sequentially updating model parameters using approximate second-order optimisation methods that exploit the overparametrisation of high-dimensional parametric models such as neural networks. Theoretical analysis and empirical results demonstrate the improved performance of our methods in dynamic, high-dimensional, and misspecified models.","authors":["Gerardo Duran-Martin"],"url":"https://arxiv.org/abs/2505.07267"}
{"created":"2025-05-13","title":"ALPCAH: Subspace Learning for Sample-wise Heteroscedastic Data","abstract":"Principal component analysis (PCA) is a key tool in the field of data dimensionality reduction. However, some applications involve heterogeneous data that vary in quality due to noise characteristics associated with each data sample. Heteroscedastic methods aim to deal with such mixed data quality. This paper develops a subspace learning method, named ALPCAH, that can estimate the sample-wise noise variances and use this information to improve the estimate of the subspace basis associated with the low-rank structure of the data. Our method makes no distributional assumptions of the low-rank component and does not assume that the noise variances are known. Further, this method uses a soft rank constraint that does not require subspace dimension to be known. Additionally, this paper develops a matrix factorized version of ALPCAH, named LR-ALPCAH, that is much faster and more memory efficient at the cost of requiring subspace dimension to be known or estimated. Simulations and real data experiments show the effectiveness of accounting for data heteroscedasticity compared to existing algorithms. Code available at https://github.com/javiersc1/ALPCAH.","authors":["Javier Salazar Cavazos","Jeffrey A. Fessler","Laura Balzano"],"url":"https://arxiv.org/abs/2505.07272"}
{"created":"2025-05-13","title":"Piloting Structure-Based Drug Design via Modality-Specific Optimal Schedule","abstract":"Structure-Based Drug Design (SBDD) is crucial for identifying bioactive molecules. Recent deep generative models are faced with challenges in geometric structure modeling. A major bottleneck lies in the twisted probability path of multi-modalities -- continuous 3D positions and discrete 2D topologies -- which jointly determine molecular geometries. By establishing the fact that noise schedules decide the Variational Lower Bound (VLB) for the twisted probability path, we propose VLB-Optimal Scheduling (VOS) strategy in this under-explored area, which optimizes VLB as a path integral for SBDD. Our model effectively enhances molecular geometries and interaction modeling, achieving state-of-the-art PoseBusters passing rate of 95.9% on CrossDock, more than 10% improvement upon strong baselines, while maintaining high affinities and robust intramolecular validity evaluated on held-out test set.","authors":["Keyue Qiu","Yuxuan Song","Zhehuan Fan","Peidong Liu","Zhe Zhang","Mingyue Zheng","Hao Zhou","Wei-Ying Ma"],"url":"https://arxiv.org/abs/2505.07286"}
{"created":"2025-05-13","title":"Learning Quasi-LPV Models and Robust Control Invariant Sets with Reduced Conservativeness","abstract":"We present an approach to identify a quasi Linear Parameter Varying (qLPV) model of a plant, with the qLPV model guaranteed to admit a robust control invariant (RCI) set. It builds upon the concurrent synthesis framework presented in [1], in which the requirement of existence of an RCI set is modeled as a control-oriented regularization. Here, we reduce the conservativeness of the approach by bounding the qLPV system with an uncertain LTI system, which we derive using bound propagation approaches. The resulting regularization function is the optimal value of a nonlinear robust optimization problem that we solve via a differentiable algorithm. We numerically demonstrate the benefits of the proposed approach over two benchmark approaches.","authors":["Sampath Kumar Mulagaleti","Alberto Bemporad"],"url":"https://arxiv.org/abs/2505.07287"}
{"created":"2025-05-13","title":"Multi-Plane Vision Transformer for Hemorrhage Classification Using Axial and Sagittal MRI Data","abstract":"Identifying brain hemorrhages from magnetic resonance imaging (MRI) is a critical task for healthcare professionals. The diverse nature of MRI acquisitions with varying contrasts and orientation introduce complexity in identifying hemorrhage using neural networks. For acquisitions with varying orientations, traditional methods often involve resampling images to a fixed plane, which can lead to information loss. To address this, we propose a 3D multi-plane vision transformer (MP-ViT) for hemorrhage classification with varying orientation data. It employs two separate transformer encoders for axial and sagittal contrasts, using cross-attention to integrate information across orientations. MP-ViT also includes a modality indication vector to provide missing contrast information to the model. The effectiveness of the proposed model is demonstrated with extensive experiments on real world clinical dataset consists of 10,084 training, 1,289 validation and 1,496 test subjects. MP-ViT achieved substantial improvement in area under the curve (AUC), outperforming the vision transformer (ViT) by 5.5% and CNN-based architectures by 1.8%. These results highlight the potential of MP-ViT in improving performance for hemorrhage detection when different orientation contrasts are needed.","authors":["Badhan Kumar Das","Gengyan Zhao","Boris Mailhe","Thomas J. Re","Dorin Comaniciu","Eli Gibson","Andreas Maier"],"url":"https://arxiv.org/abs/2505.07349"}
{"created":"2025-05-13","title":"GAN-based synthetic FDG PET images from T1 brain MRI can serve to improve performance of deep unsupervised anomaly detection models","abstract":"Background and Objective. Research in the cross-modal medical image translation domain has been very productive over the past few years in tackling the scarce availability of large curated multimodality datasets with the promising performance of GAN-based architectures. However, only a few of these studies assessed task-based related performance of these synthetic data, especially for the training of deep models. Method. We design and compare different GAN-based frameworks for generating synthetic brain [18F]fluorodeoxyglucose (FDG) PET images from T1 weighted MRI data. We first perform standard qualitative and quantitative visual quality evaluation. Then, we explore further impact of using these fake PET data in the training of a deep unsupervised anomaly detection (UAD) model designed to detect subtle epilepsy lesions in T1 MRI and FDG PET images. We introduce novel diagnostic task-oriented quality metrics of the synthetic FDG PET data tailored to our unsupervised detection task, then use these fake data to train a use case UAD model combining a deep representation learning based on siamese autoencoders with a OC-SVM density support estimation model. This model is trained on normal subjects only and allows the detection of any variation from the pattern of the normal population. We compare the detection performance of models trained on 35 paired real MR T1 of normal subjects paired either on 35 true PET images or on 35 synthetic PET images generated from the best performing generative models. Performance analysis is conducted on 17 exams of epilepsy patients undergoing surgery. Results. The best performing GAN-based models allow generating realistic fake PET images of control subject with SSIM and PSNR values around 0.9 and 23.8, respectively and in distribution (ID) with regard to the true control dataset. The best UAD model trained on these synthetic normative PET data allows reaching 74% sensitivity. Conclusion. Our results confirm that GAN-based models are the best suited for MR T1 to FDG PET translation, outperforming transformer or diffusion models. We also demonstrate the diagnostic value of these synthetic data for the training of UAD models and evaluation on clinical exams of epilepsy patients. Our code and the normative image dataset are available.","authors":["Daria Zotova (MYRIAD)","Nicolas Pinon (MYRIAD)","Robin Trombetta (MYRIAD)","Romain Bouet (CRNL)","Julien Jung (CRNL","HCL)","Carole Lartizien (MYRIAD)"],"url":"https://arxiv.org/abs/2505.07364"}
{"created":"2025-05-13","title":"Undecidability of Polynomial Inequalities in Subset Densities and Additive Energies","abstract":"Many results in extremal graph theory can be formulated as certain polynomial inequalities in graph homomorphism densities. Answering fundamental questions raised by Lov{\\'a}sz, Szegedy and Razborov, Hatami and Norine proved that determining the validity of an arbitrary such polynomial inequality in graph homomorphism densities is undecidable. We observe that many results in additive combinatorics can also be formulated as polynomial inequalities in subset's density and its variants. Based on techniques introduced in Hatami and Norine, together with algebraic and graph construction and Fourier analysis, we prove similarly two theorems of undecidability, thus showing that establishing such polynomial inequalities in additive combinatorics are inherently difficult in their full generality.","authors":["Yaqiao Li"],"url":"https://arxiv.org/abs/2505.07378"}
{"created":"2025-05-13","title":"Anti-windup design for internal model online constrained optimization","abstract":"This paper proposes a novel algorithmic design procedure for online constrained optimization grounded in control-theoretic principles. By integrating the Internal Model Principle (IMP) with an anti-windup compensation mechanism, the proposed Projected-Internal Model Anti-Windup (P-IMAW) gradient descent exploits a partial knowledge of the temporal evolution of the cost function to enhance tracking performance. The algorithm is developed through a structured synthesis procedure: first, a robust controller leveraging the IMP ensures asymptotic convergence in the unconstrained setting. Second, an anti-windup augmentation guarantees stability and performance in the presence of the projection operator needed to satisfy the constraints. The effectiveness of the proposed approach is demonstrated through numerical simulations comparing it against other classical techniques.","authors":["Umberto Casti","Sandro Zampieri"],"url":"https://arxiv.org/abs/2505.07384"}
{"created":"2025-05-13","title":"Simulating many-engine spacecraft: Exceeding 100 trillion grid points via information~geometric regularization and the MFC flow solver","abstract":"This work proposes a method and optimized implementation for exascale simulations of high-speed compressible fluid flows, enabling the simulation of multi-engine rocket craft at an unprecedented scale. We significantly improve upon the state-of-the-art in terms of computational cost and memory footprint through a carefully crafted implementation of the recently proposed information geometric regularization, which eliminates the need for numerical shock capturing. Unified addressing on tightly coupled CPU--GPU platforms increases the total problem size with negligible performance hit. Despite linear stencil algorithms being memory-bound, we achieve wall clock times that are four times faster than optimized baseline numerics. This enables the execution of CFD simulations at more than 100 trillion grid points, surpassing the largest state-of-the-art publicly available simulations by an order of magnitude. Ideal weak scaling is demonstrated on OLCF Frontier and CSCS Alps using the full system, entailing 37.8K AMD MI250X GPUs (Frontier) or 9.2K NVIDIA GH200 superchips (Alps).","authors":["Benjamin Wilfong","Anand Radhakrishnan","Henry Le Berre","Nikolaos Tselepidis","Benedikt Dorschner","Reuben Budiardja","Brian Cornille","Stephen Abbott","Florian Sch\\\"afer","Spencer H. Bryngelson"],"url":"https://arxiv.org/abs/2505.07392"}
{"created":"2025-05-13","title":"Efficient Lifting of Discrete Logarithms Modulo Prime Powers","abstract":"We present a deterministic algorithm that, given a prime $p$ and a solution $x \\in \\mathbb Z$ to the discrete logarithm problem $a^x \\equiv b \\pmod p$ with $p\\nmid a$, efficiently lifts it to a solution modulo $p^k$, i.e., $a^x \\equiv b \\pmod {p^k}$, for any fixed $k \\geq 1$.","authors":["Giovanni Viglietta","Yasuyuki Kachi"],"url":"https://arxiv.org/abs/2505.07434"}
{"created":"2025-05-13","title":"Ophora: A Large-Scale Data-Driven Text-Guided Ophthalmic Surgical Video Generation Model","abstract":"In ophthalmic surgery, developing an AI system capable of interpreting surgical videos and predicting subsequent operations requires numerous ophthalmic surgical videos with high-quality annotations, which are difficult to collect due to privacy concerns and labor consumption. Text-guided video generation (T2V) emerges as a promising solution to overcome this issue by generating ophthalmic surgical videos based on surgeon instructions. In this paper, we present Ophora, a pioneering model that can generate ophthalmic surgical videos following natural language instructions. To construct Ophora, we first propose a Comprehensive Data Curation pipeline to convert narrative ophthalmic surgical videos into a large-scale, high-quality dataset comprising over 160K video-instruction pairs, Ophora-160K. Then, we propose a Progressive Video-Instruction Tuning scheme to transfer rich spatial-temporal knowledge from a T2V model pre-trained on natural video-text datasets for privacy-preserved ophthalmic surgical video generation based on Ophora-160K. Experiments on video quality evaluation via quantitative analysis and ophthalmologist feedback demonstrate that Ophora can generate realistic and reliable ophthalmic surgical videos based on surgeon instructions. We also validate the capability of Ophora for empowering downstream tasks of ophthalmic surgical workflow understanding. Code is available at https://github.com/mar-cry/Ophora.","authors":["Wei Li","Ming Hu","Guoan Wang","Lihao Liu","Kaijin Zhou","Junzhi Ning","Xin Guo","Zongyuan Ge","Lixu Gu","Junjun He"],"url":"https://arxiv.org/abs/2505.07449"}
{"created":"2025-05-13","title":"Can Generative AI agents behave like humans? Evidence from laboratory market experiments","abstract":"We explore the potential of Large Language Models (LLMs) to replicate human behavior in economic market experiments. Compared to previous studies, we focus on dynamic feedback between LLM agents: the decisions of each LLM impact the market price at the current step, and so affect the decisions of the other LLMs at the next step. We compare LLM behavior to market dynamics observed in laboratory settings and assess their alignment with human participants' behavior. Our findings indicate that LLMs do not adhere strictly to rational expectations, displaying instead bounded rationality, similarly to human participants. Providing a minimal context window i.e. memory of three previous time steps, combined with a high variability setting capturing response heterogeneity, allows LLMs to replicate broad trends seen in human experiments, such as the distinction between positive and negative feedback markets. However, differences remain at a granular level--LLMs exhibit less heterogeneity in behavior than humans. These results suggest that LLMs hold promise as tools for simulating realistic human behavior in economic contexts, though further research is needed to refine their accuracy and increase behavioral diversity.","authors":["R. Maria del Rio-Chanona","Marco Pangallo","Cars Hommes"],"url":"https://arxiv.org/abs/2505.07457"}
{"created":"2025-05-13","title":"On core of categorical product of (di)graphs","abstract":"The core of a graph is the smallest graph (in terms of number of vertices) to which it is homomorphically equivalent.","authors":["Reza Naserasr","Cyril Pujol"],"url":"https://arxiv.org/abs/2505.07463"}
{"created":"2025-05-13","title":"Congestion-Sensitive Grid Aggregation for DC Optimal Power Flow","abstract":"The vast spatial dimension of modern interconnected electricity grids challenges the tractability of the DC optimal power flow problem. Grid aggregation methods try to overcome this challenge by reducing the number of network elements. Many existing methods use Locational Marginal Prices as a distance metric to cluster nodes. In this paper, we show that prevalent methods adopting this distance metric fail to adequately capture the impact of individual lines when there is more than one line congested. This leads to suboptimal outcomes for the optimization of the aggregated model. To overcome those issues, we propose two methods based on the novel Network Congestion Price metric, which preserves the impact of nodal power injections on individual line congestions. The proposed methods are compared to several existing aggregation methods based on Locational Marginal Prices. We demonstrate all methods on adapted versions of the IEEE RTS 24- and 300-Bus systems. We show that the proposed methods outperform existing approaches both in terms of objective function value error and maximum line limit violation, while exhibiting faster node clustering. We conclude that aggregation methods based on the novel Network Congestion Price metric are better at preserving the essential physical characteristics of the network topology in the grid aggregation process than methods based on Locational Marginal Prices.","authors":["Benjamin St\\\"ockl","Yannick Werner","Sonja Wogrin"],"url":"https://arxiv.org/abs/2505.07545"}
{"created":"2025-05-13","title":"TACOS: Temporally-aligned Audio CaptiOnS for Language-Audio Pretraining","abstract":"Learning to associate audio with textual descriptions is valuable for a range of tasks, including pretraining, zero-shot classification, audio retrieval, audio captioning, and text-conditioned audio generation. Existing contrastive language-audio pretrained models are typically trained using global, clip-level descriptions, which provide only weak temporal supervision. We hypothesize that CLAP-like language-audio models - particularly, if they are expected to produce frame-level embeddings - can benefit from a stronger temporal supervision. To confirm our hypothesis, we curate a novel dataset of approximately 12,000 audio recordings from Freesound, each annotated with single-sentence free-text descriptions linked to a specific temporal segment in an audio recording. We use large language models to clean these annotations by removing references to non-audible events, transcribed speech, typos, and annotator language bias. We further propose a frame-wise contrastive training strategy that learns to align text descriptions with temporal regions in an audio recording and demonstrate that our model has better temporal text-audio alignment abilities compared to models trained only on global captions when evaluated on the AudioSet Strong benchmark. The dataset and our source code are available on Zenodo and GitHub, respectively.","authors":["Paul Primus","Florian Schmid","Gerhard Widmer"],"url":"https://arxiv.org/abs/2505.07609"}
{"created":"2025-05-13","title":"Diffused Responsibility: Analyzing the Energy Consumption of Generative Text-to-Audio Diffusion Models","abstract":"Text-to-audio models have recently emerged as a powerful technology for generating sound from textual descriptions. However, their high computational demands raise concerns about energy consumption and environmental impact. In this paper, we conduct an analysis of the energy usage of 7 state-of-the-art text-to-audio diffusion-based generative models, evaluating to what extent variations in generation parameters affect energy consumption at inference time. We also aim to identify an optimal balance between audio quality and energy consumption by considering Pareto-optimal solutions across all selected models. Our findings provide insights into the trade-offs between performance and environmental impact, contributing to the development of more efficient generative audio models.","authors":["Riccardo Passoni","Francesca Ronchini","Luca Comanducci","Romain Serizel","Fabio Antonacci"],"url":"https://arxiv.org/abs/2505.07615"}
{"created":"2025-05-13","title":"QC-Adviser: Quantum Hardware Recommendations for Solving Industrial Optimization Problems","abstract":"The availability of quantum hardware via the cloud offers opportunities for new approaches to computing optimization problems in an industrial environment. However, selecting the right quantum hardware is difficult for non-experts due to its technical characteristics. In this paper, we present the QC-Adviser prototype, which supports users in selecting suitable quantum annealer hardware without requiring quantum computing knowledge.","authors":["Djamel Laps-Bouraba","Markus Zajac","Uta St\\\"orl"],"url":"https://arxiv.org/abs/2505.07625"}
{"created":"2025-05-13","title":"Certified Data Removal Under High-dimensional Settings","abstract":"Machine unlearning focuses on the computationally efficient removal of specific training data from trained models, ensuring that the influence of forgotten data is effectively eliminated without the need for full retraining. Despite advances in low-dimensional settings, where the number of parameters \\( p \\) is much smaller than the sample size \\( n \\), extending similar theoretical guarantees to high-dimensional regimes remains challenging. We propose an unlearning algorithm that starts from the original model parameters and performs a theory-guided sequence of Newton steps \\( T \\in \\{ 1,2\\}\\). After this update, carefully scaled isotropic Laplacian noise is added to the estimate to ensure that any (potential) residual influence of forget data is completely removed. We show that when both \\( n, p \\to \\infty \\) with a fixed ratio \\( n/p \\), significant theoretical and computational obstacles arise due to the interplay between the complexity of the model and the finite signal-to-noise ratio. Finally, we show that, unlike in low-dimensional settings, a single Newton step is insufficient for effective unlearning in high-dimensional problems -- however, two steps are enough to achieve the desired certifiebility. We provide numerical experiments to support the certifiability and accuracy claims of this approach.","authors":["Haolin Zou","Arnab Auddy","Yongchan Kwon","Kamiar Rahnama Rad","Arian Maleki"],"url":"https://arxiv.org/abs/2505.07640"}
{"created":"2025-05-13","title":"Convergence of Time-Averaged Mean Field Gradient Descent Dynamics for Continuous Multi-Player Zero-Sum Games","abstract":"The approximation of mixed Nash equilibria (MNE) for zero-sum games with mean-field interacting players has recently raised much interest in machine learning. In this paper we propose a mean-field gradient descent dynamics for finding the MNE of zero-sum games involving $K$ players with $K\\geq 2$. The evolution of the players' strategy distributions follows coupled mean-field gradient descent flows with momentum, incorporating an exponentially discounted time-averaging of gradients. First, in the case of a fixed entropic regularization, we prove an exponential convergence rate for the mean-field dynamics to the mixed Nash equilibrium with respect to the total variation metric. This improves a previous polynomial convergence rate for a similar time-averaged dynamics with different averaging factors. Moreover, unlike previous two-scale approaches for finding the MNE, our approach treats all player types on the same time scale. We also show that with a suitable choice of decreasing temperature, a simulated annealing version of the mean-field dynamics converges to an MNE of the initial unregularized problem.","authors":["Yulong Lu","Pierre Monmarch\\'e"],"url":"https://arxiv.org/abs/2505.07642"}
{"created":"2025-05-13","title":"Breast Cancer Classification in Deep Ultraviolet Fluorescence Images Using a Patch-Level Vision Transformer Framework","abstract":"Breast-conserving surgery (BCS) aims to completely remove malignant lesions while maximizing healthy tissue preservation. Intraoperative margin assessment is essential to achieve a balance between thorough cancer resection and tissue conservation. A deep ultraviolet fluorescence scanning microscope (DUV-FSM) enables rapid acquisition of whole surface images (WSIs) for excised tissue, providing contrast between malignant and normal tissues. However, breast cancer classification with DUV WSIs is challenged by high resolutions and complex histopathological features. This study introduces a DUV WSI classification framework using a patch-level vision transformer (ViT) model, capturing local and global features. Grad-CAM++ saliency weighting highlights relevant spatial regions, enhances result interpretability, and improves diagnostic accuracy for benign and malignant tissue classification. A comprehensive 5-fold cross-validation demonstrates the proposed approach significantly outperforms conventional deep learning methods, achieving a classification accuracy of 98.33%.","authors":["Pouya Afshin","David Helminiak","Tongtong Lu","Tina Yen","Julie M. Jorns","Mollie Patton","Bing Yu","Dong Hye Ye"],"url":"https://arxiv.org/abs/2505.07654"}
{"created":"2025-05-13","title":"Hierarchical Sparse Attention Framework for Computationally Efficient Classification of Biological Cells","abstract":"We present SparseAttnNet, a new hierarchical attention-driven framework for efficient image classification that adaptively selects and processes only the most informative pixels from images. Traditional convolutional neural networks typically process the entire images regardless of information density, leading to computational inefficiency and potential focus on irrelevant features. Our approach leverages a dynamic selection mechanism that uses coarse attention distilled by fine multi-head attention from the downstream layers of the model, allowing the model to identify and extract the most salient k pixels, where k is adaptively learned during training based on loss convergence trends. Once the top-k pixels are selected, the model processes only these pixels, embedding them as words in a language model to capture their semantics, followed by multi-head attention to incorporate global context. For biological cell images, we demonstrate that SparseAttnNet can process approximately 15% of the pixels instead of the full image. Applied to cell classification tasks using white blood cells images from the following modalities: optical path difference (OPD) images from digital holography for stain-free cells, images from motion-sensitive (event) camera from stain-free cells, and brightfield microscopy images of stained cells, For all three imaging modalities, SparseAttnNet achieves competitive accuracy while drastically reducing computational requirements in terms of both parameters and floating-point operations per second, compared to traditional CNNs and Vision Transformers. Since the model focuses on biologically relevant regions, it also offers improved explainability. The adaptive and lightweight nature of SparseAttnNet makes it ideal for deployment in resource-constrained and high-throughput settings, including imaging flow cytometry.","authors":["Elad Yoshai","Dana Yagoda-Aharoni","Eden Dotan","Natan T. Shaked"],"url":"https://arxiv.org/abs/2505.07661"}
{"created":"2025-05-13","title":"Transfer Learning Across Fixed-Income Product Classes","abstract":"We propose a framework for transfer learning of discount curves across different fixed-income product classes. Motivated by challenges in estimating discount curves from sparse or noisy data, we extend kernel ridge regression (KR) to a vector-valued setting, formulating a convex optimization problem in a vector-valued reproducing kernel Hilbert space (RKHS). Each component of the solution corresponds to the discount curve implied by a specific product class. We introduce an additional regularization term motivated by economic principles, promoting smoothness of spread curves between product classes, and show that it leads to a valid separable kernel structure. A main theoretical contribution is a decomposition of the vector-valued RKHS norm induced by separable kernels. We further provide a Gaussian process interpretation of vector-valued KR, enabling quantification of estimation uncertainty. Illustrative examples demonstrate that transfer learning significantly improves extrapolation performance and tightens confidence intervals compared to single-curve estimation.","authors":["Nicolas Camenzind","Damir Filipovic"],"url":"https://arxiv.org/abs/2505.07676"}
{"created":"2025-05-13","title":"Periods of fibre products of elliptic surfaces and the Gamma conjecture","abstract":"We provide an algorithm for computing a basis of homology of fibre products of elliptic surfaces over $\\mathbb P^1$, along with the corresponding intersection product and period matrices. We use this data to investigate the Gamma conjecture for Calabi-Yau threefolds obtained in this manner. We find a formula that works for all operators of a list of 105 fibre products, as well as for fourth order operators of the Calabi-Yau database. This algorithm comes with a SageMath implementation.","authors":["Eric Pichon-Pharabod"],"url":"https://arxiv.org/abs/2505.07685"}
{"created":"2025-05-13","title":"ABS-Mamba: SAM2-Driven Bidirectional Spiral Mamba Network for Medical Image Translation","abstract":"Accurate multi-modal medical image translation requires ha-rmonizing global anatomical semantics and local structural fidelity, a challenge complicated by intermodality information loss and structural distortion. We propose ABS-Mamba, a novel architecture integrating the Segment Anything Model 2 (SAM2) for organ-aware semantic representation, specialized convolutional neural networks (CNNs) for preserving modality-specific edge and texture details, and Mamba's selective state-space modeling for efficient long- and short-range feature dependencies. Structurally, our dual-resolution framework leverages SAM2's image encoder to capture organ-scale semantics from high-resolution inputs, while a parallel CNNs branch extracts fine-grained local features. The Robust Feature Fusion Network (RFFN) integrates these epresentations, and the Bidirectional Mamba Residual Network (BMRN) models spatial dependencies using spiral scanning and bidirectional state-space dynamics. A three-stage skip fusion decoder enhances edge and texture fidelity. We employ Efficient Low-Rank Adaptation (LoRA+) fine-tuning to enable precise domain specialization while maintaining the foundational capabilities of the pre-trained components. Extensive experimental validation on the SynthRAD2023 and BraTS2019 datasets demonstrates that ABS-Mamba outperforms state-of-the-art methods, delivering high-fidelity cross-modal synthesis that preserves anatomical semantics and structural details to enhance diagnostic accuracy in clinical applications. The code is available at https://github.com/gatina-yone/ABS-Mamba","authors":["Feng Yuan","Yifan Gao","Wenbin Wu","Keqing Wu","Xiaotong Guo","Jie Jiang","Xin Gao"],"url":"https://arxiv.org/abs/2505.07687"}
{"created":"2025-05-13","title":"The generalized trifference problem","abstract":"We study the problem of finding the largest number $T(n, m)$ of ternary vectors of length $n$ such that for any three distinct vectors there are at least $m$ coordinates where they pairwise differ.","authors":["Anurag Bishnoi","Bart{\\l}omiej Kielak","Benedek Kov\\'acs","Zolt\\'an L\\'or\\'ant Nagy","G\\'abor Somlai","M\\'at\\'e Vizer","Zeyu Zheng"],"url":"https://arxiv.org/abs/2505.07706"}
{"created":"2025-05-13","title":"SmartUT: Receive Beamforming for Spectral Coexistence of NGSO Satellite Systems","abstract":"In this paper, we investigate downlink co-frequency interference (CFI) mitigation in non-geostationary satellites orbits (NGSOs) co-existing systems. Traditional mitigation techniques, such as Zero-forcing (ZF), produce a null towards the direction of arrivals (DOAs) of the interfering signals, but they suffer from high computational complexity due to matrix inversions and required knowledge of the channel state information (CSI). Furthermore, adaptive beamformers, such as sample matrix inversion (SMI)-based minimum variance, provide poor performance when the available snapshots are limited. We propose a Mamba-based beamformer (MambaBF) that leverages an unsupervised deep learning (DL) approach and can be deployed on the user terminal (UT) antenna array, for assisting downlink beamforming and CFI mitigation using only a limited number of available array snapshots as input, and without CSI knowledge. Simulation results demonstrate that MambaBF consistently outperforms conventional beamforming techniques in mitigating interference and maximizing the signal-to-interference-plus-noise ratio (SINR), particularly under challenging conditions characterized by low SINR, limited snapshots, and imperfect CSI.","authors":["Almoatssimbillah Saifaldawla","Eva Lagunas","Flor Ortiz","Abuzar B. M. Adam","Symeon Chatzinotas"],"url":"https://arxiv.org/abs/2505.07714"}
{"created":"2025-05-13","title":"Training neural control variates using correlated configurations","abstract":"Neural control variates (NCVs) have emerged as a powerful tool for variance reduction in Monte Carlo (MC) simulations, particularly in high-dimensional problems where traditional control variates are difficult to construct analytically. By training neural networks to learn auxiliary functions correlated with the target observable, NCVs can significantly reduce estimator variance while preserving unbiasedness. However, a critical but often overlooked aspect of NCV training is the role of autocorrelated samples generated by Markov Chain Monte Carlo (MCMC). While such samples are typically discarded for error estimation due to their statistical redundancy, they may contain useful information about the structure of the underlying probability distribution that can benefit the training process. In this work, we systematically examine the effect of using correlated configurations in training neural control variates. We demonstrate, both conceptually and numerically, that training on correlated data can improve control variate performance, especially in settings with limited computational resources. Our analysis includes empirical results from $U(1)$ gauge theory and scalar field theory, illustrating when and how autocorrelated samples enhance NCV construction. These findings provide practical guidance for the efficient use of MCMC data in training neural networks.","authors":["Hyunwoo Oh"],"url":"https://arxiv.org/abs/2505.07719"}
{"created":"2025-05-13","title":"Skeletonization of neuronal processes using Discrete Morse techniques from computational topology","abstract":"To understand biological intelligence we need to map neuronal networks in vertebrate brains. Mapping mesoscale neural circuitry is done using injections of tracers that label groups of neurons whose axons project to different brain regions. Since many neurons are labeled, it is difficult to follow individual axons. Previous approaches have instead quantified the regional projections using the total label intensity within a region. However, such a quantification is not biologically meaningful. We propose a new approach better connected to the underlying neurons by skeletonizing labeled axon fragments and then estimating a volumetric length density. Our approach uses a combination of deep nets and the Discrete Morse (DM) technique from computational topology. This technique takes into account nonlocal connectivity information and therefore provides noise-robustness. We demonstrate the utility and scalability of the approach on whole-brain tracer injected data. We also define and illustrate an information theoretic measure that quantifies the additional information obtained, compared to the skeletonized tracer injection fragments, when individual axon morphologies are available. Our approach is the first application of the DM technique to computational neuroanatomy. It can help bridge between single-axon skeletons and tracer injections, two important data types in mapping neural networks in vertebrates.","authors":["Samik Banerjee","Caleb Stam","Daniel J. Tward","Steven Savoia","Yusu Wang","Partha P. Mitra"],"url":"https://arxiv.org/abs/2505.07754"}
{"created":"2025-05-13","title":"Tagging fully hadronic exotic decays of the vectorlike $\\mathbf{B}$ quark using a graph neural network","abstract":"Following up on our earlier study in [J. Bardhan et al., Machine learning-enhanced search for a vectorlike singlet B quark decaying to a singlet scalar or pseudoscalar, Phys. Rev. D 107 (2023) 115001; arXiv:2212.02442], we investigate the LHC prospects of pair-produced vectorlike $B$ quarks decaying exotically to a new gauge-singlet (pseudo)scalar field $\\Phi$ and a $b$ quark. After the electroweak symmetry breaking, the $\\Phi$ decays predominantly to $gg/bb$ final states, leading to a fully hadronic $2b+4j$ or $6b$ signature. Because of the large Standard Model background and the lack of leptonic handles, it is a difficult channel to probe. To overcome the challenge, we employ a hybrid deep learning model containing a graph neural network followed by a deep neural network. We estimate that such a state-of-the-art deep learning analysis pipeline can lead to a performance comparable to that in the semi-leptonic mode, taking the discovery (exclusion) reach up to about $M_B=1.8\\:(2.4)$~TeV at HL-LHC when $B$ decays fully exotically, i.e., BR$(B \\to b\\Phi) = 100\\%$.","authors":["Jai Bardhan","Tanumoy Mandal","Subhadip Mitra","Cyrin Neeraj","Mihir Rawat"],"url":"https://arxiv.org/abs/2505.07769"}
{"created":"2025-05-13","title":"Schrijver's $\\vartheta$-function is not an upper bound on the Shannon capacity of a graph: a counterexample","abstract":"This note addresses an open question concerning a variant of the Lov\\'{a}sz $\\vartheta$-function, introduced by Schrijver and independently by McEliece et al. (1978). It provides a complete and detailed presentation of a counterexample demonstrating that this variant does not universally upper bound the Shannon capacity of a graph, in contrast to the Lov\\'{a}sz $\\vartheta$-function. The counterexample, previously introduced in Example 5.24 of a recent paper by the author, entitled: Observations on graph invariants with the Lov\\'{a}sz $\\vartheta$-function (AIMS Mathematics, vol. 9, pp. 15385--15468, April 2024, https://doi.org/10.3934/math.2024747), is revisited and fully detailed here. By resolving this question, the note clarifies a subtle but significant distinction between these two closely related graph invariants.","authors":["Igal Sason"],"url":"https://arxiv.org/abs/2505.07778"}
{"created":"2025-05-13","title":"Analytic theory of dropout regularization","abstract":"Dropout is a regularization technique widely used in training artificial neural networks to mitigate overfitting. It consists of dynamically deactivating subsets of the network during training to promote more robust representations. Despite its widespread adoption, dropout probabilities are often selected heuristically, and theoretical explanations of its success remain sparse. Here, we analytically study dropout in two-layer neural networks trained with online stochastic gradient descent. In the high-dimensional limit, we derive a set of ordinary differential equations that fully characterize the evolution of the network during training and capture the effects of dropout. We obtain a number of exact results describing the generalization error and the optimal dropout probability at short, intermediate, and long training times. Our analysis shows that dropout reduces detrimental correlations between hidden nodes, mitigates the impact of label noise, and that the optimal dropout probability increases with the level of noise in the data. Our results are validated by extensive numerical simulations.","authors":["Francesco Mori","Francesca Mignacco"],"url":"https://arxiv.org/abs/2505.07792"}
{"created":"2025-05-13","title":"Isomorphisms of unit distance graphs of layers","abstract":"For any $\\varepsilon\\in (0,+\\infty)$, consider the metric spaces $\\mathbb{R} \\times [0,\\varepsilon]$ in the Euclidean plane named layers or strips. B. Baslaugh in 1998 found the minimal width $\\varepsilon \\in (0,1)$ of a layer such that its unit distance graph contains a cycle of a given odd length $k$. The first of the main results of this paper is the fact that the unit distance graphs of two layers $\\mathbb{R} \\times [0,\\varepsilon_1], \\mathbb{R} \\times [0,\\varepsilon_2]$ are non-isomorphic for any different values $\\varepsilon_1,\\varepsilon_2 \\in (0,+\\infty)$. We also get a multidimensional analogue of this theorem. For given $n,m \\in \\mathbb{N}, p \\in (1,+\\infty), \\varepsilon \\in (0,+\\infty)$, we say that the metric space on $\\mathbb{R}^n \\times [0,\\varepsilon]^m$ with the metric space distance generated by $l_p$-norm in $\\mathbb{R}^{n+m}$ is a layer $L(n,m,p,\\varepsilon)$. We show that the unit distance graphs of multi-layers $L(n,m,p,\\varepsilon_1), L(n,m,p,\\varepsilon_2)$ are non-isomorphic for $\\varepsilon_1 \\neq \\varepsilon_2$.","authors":["Arthur Igorevich Bikeev"],"url":"https://arxiv.org/abs/2505.07799"}
{"created":"2025-05-13","title":"Quon Classical Simulation: Unifying Clifford, Matchgates and Entanglement","abstract":"We propose a unified classical simulation framework for quantum circuits, termed Quon Classical Simulation (QCS), built upon the diagrammatic formalism of the Quon language. Central to this framework is the introduction of magic holes, a topological feature that captures the global source of computational hardness in simulating quantum systems. Unlike conventional measures, the complexity of QCS is governed by the topological entanglement entropy associated with these magic holes. We show that Clifford circuits and Matchgate circuits are free of magic holes and thus efficiently simulable within our model. To capture the interaction structure of magic holes, we define a topological tensor network representation and develop novel skein relations and reduction algorithms to simplify circuit representations. This approach significantly improves the efficiency of classical simulations and provides a unified explanation for the tractability of various known quantum circuit classes. Our work offers a new topological perspective on the classical simulability of quantum systems and topological complexity.","authors":["Zixuan Feng","Zhengwei Liu","Fan Lu","Ningfeng Wang"],"url":"https://arxiv.org/abs/2505.07804"}
{"created":"2025-05-13","title":"TODS: An Automated Time Series Outlier Detection System","abstract":"We present TODS, an automated Time Series Outlier Detection System for research and industrial applications. TODS is a highly modular system that supports easy pipeline construction. The basic building block of TODS is primitive, which is an implementation of a function with hyperparameters. TODS currently supports 70 primitives, including data processing, time series processing, feature analysis, detection algorithms, and a reinforcement module. Users can freely construct a pipeline using these primitives and perform end- to-end outlier detection with the constructed pipeline. TODS provides a Graphical User Interface (GUI), where users can flexibly design a pipeline with drag-and-drop. Moreover, a data-driven searcher is provided to automatically discover the most suitable pipelines given a dataset. TODS is released under Apache 2.0 license at https://github.com/datamllab/tods.","authors":["Kwei-Herng Lai","Daochen Zha","Guanchu Wang","Junjie Xu","Yue Zhao","Devesh Kumar","Yile Chen","Purav Zumkhawaka","Mingyang Wan","Diego Martinez","Xia Hu"],"url":"https://arxiv.org/abs/2009.09822"}
{"created":"2025-05-13","title":"Effective Regularization Through Loss-Function Metalearning","abstract":"Evolutionary computation can be used to optimize several different aspects of neural network architectures. For instance, the TaylorGLO method discovers novel, customized loss functions, resulting in improved performance, faster training, and improved data utilization. A likely reason is that such functions discourage overfitting, leading to effective regularization. This paper demonstrates theoretically that this is indeed the case for TaylorGLO. Learning rule decomposition reveals that evolved loss functions balance two factors: the pull toward zero error, and a push away from it to avoid overfitting. This is a general principle that may be used to understand other regularization techniques as well (as demonstrated in this paper for label smoothing). The theoretical analysis leads to a constraint that can be utilized to find more effective loss functions in practice; the mechanism also results in networks that are more robust (as demonstrated in this paper with adversarial inputs). The analysis in this paper thus constitutes a first step towards understanding regularization, and demonstrates the power of evolutionary neural architecture search in general.","authors":["Santiago Gonzalez","Risto Miikkulainen"],"url":"https://arxiv.org/abs/2010.00788"}
{"created":"2025-05-13","title":"Towards Optimal Branching of Linear and Semidefinite Relaxations for Neural Network Robustness Certification","abstract":"In this paper, we study certifying the robustness of ReLU neural networks against adversarial input perturbations. To diminish the relaxation error suffered by the popular linear programming (LP) and semidefinite programming (SDP) certification methods, we take a branch-and-bound approach to propose partitioning the input uncertainty set and solving the relaxations on each part separately. We show that this approach reduces relaxation error, and that the error is eliminated entirely upon performing an LP relaxation with a partition intelligently designed to exploit the nature of the ReLU activations. To scale this approach to large networks, we consider using a coarser partition whereby the number of parts in the partition is reduced. We prove that computing such a coarse partition that directly minimizes the LP relaxation error is NP-hard. By instead minimizing the worst-case LP relaxation error, we develop a closed-form branching scheme in the single-hidden layer case. We extend the analysis to the SDP, where the feasible set geometry is exploited to design a branching scheme that minimizes the worst-case SDP relaxation error. Experiments on MNIST, CIFAR-10, and Wisconsin breast cancer diagnosis classifiers demonstrate significant increases in the percentages of test samples certified. By independently increasing the input size and the number of layers, we empirically illustrate under which regimes the branched LP and branched SDP are best applied. Finally, we extend our LP branching method into a multi-layer branching heuristic, which attains comparable performance to prior state-of-the-art heuristics on large-scale, deep neural network certification benchmarks.","authors":["Brendon G. Anderson","Ziye Ma","Jingqi Li","Somayeh Sojoudi"],"url":"https://arxiv.org/abs/2101.09306"}
{"created":"2025-05-13","title":"Automated Fuzzing of Automotive Control Units","abstract":"Modern vehicles are governed by a network of Electronic Control Units (ECUs), which are programmed to sense inputs from the driver and the environment, to process these inputs, and to control actuators that, e.g., regulate the engine or even control the steering system. ECUs within a vehicle communicate via automotive bus systems such as the Controller Area Network (CAN), and beyond the vehicles boundaries through upcoming vehicle-to-vehicle and vehicle-to-infrastructure channels. Approaches to manipulate the communication between ECUs for the purpose of security testing and reverse-engineering of vehicular functions have been presented in the past, all of which struggle with automating the detection of system change in response to message injection. In this paper we present our findings with fuzzing CAN networks, in particular while observing individual ECUs with a sensor harness. The harness detects physical responses, which we then use in a oracle functions to inform the fuzzing process. We systematically define fuzzers, fuzzing configurations and oracle functions for testing ECUs. We evaluate our approach based on case studies of commercial instrument clusters and with an experimental framework for CAN authentication. Our results show that the approach is capable of identifying interesting ECU states with a high level of automation. Our approach is applicable in distributed cyber-physical systems beyond automotive computing.","authors":["Timothy Werquin","Roos Hubrechtsen","Ashok Thangarajan","Frank Piessens","Jan Tobias Muehlberg"],"url":"https://arxiv.org/abs/2102.12345"}
{"created":"2025-05-13","title":"Solving Homotopy Domain Equations","abstract":"In order to get $\\lambda$-models with a rich structure of $\\infty$-groupoid, which we call \"homotopy $\\lambda$-models\", a general technique is described for solving domain equations on any cartesian closed $\\infty$-category (c.c.i.) with enough points. Finally, the technique is applied in a particular c.c.i., where some examples of homotopy $\\lambda$-models are given.","authors":["Daniel O. Mart\\'inez-Rivillas","Ruy J. G. B. de Queiroz"],"url":"https://arxiv.org/abs/2104.01195"}
{"created":"2025-05-13","title":"Audio Transformers","abstract":"Over the past two decades, CNN architectures have produced compelling models of sound perception and cognition, learning hierarchical organizations of features. Analogous to successes in computer vision, audio feature classification can be optimized for a particular task of interest, over a wide variety of datasets and labels. In fact similar architectures designed for image understanding have proven effective for acoustic scene analysis. Here we propose applying Transformer based architectures without convolutional layers to raw audio signals. On a standard dataset of Free Sound 50K,comprising of 200 categories, our model outperforms convolutional models to produce state of the art results. This is significant as unlike in natural language processing and computer vision, we do not perform unsupervised pre-training for outperforming convolutional architectures. On the same training set, with respect mean aver-age precision benchmarks, we show a significant improvement. We further improve the performance of Transformer architectures by using techniques such as pooling inspired from convolutional net-work designed in the past few years. In addition, we also show how multi-rate signal processing ideas inspired from wavelets, can be applied to the Transformer embeddings to improve the results. We also show how our models learns a non-linear non constant band-width filter-bank, which shows an adaptable time frequency front end representation for the task of audio understanding, different from other tasks e.g. pitch estimation.","authors":["Prateek Verma","Jonathan Berger"],"url":"https://arxiv.org/abs/2105.00335"}
{"created":"2025-05-13","title":"Diffusion Approximations for Thompson Sampling","abstract":"We study the behavior of Thompson sampling from the perspective of weak convergence. In the regime with small $\\gamma > 0$, where the gaps between arm means scale as $\\sqrt{\\gamma}$ and over time horizons that scale as $1/\\gamma$, we show that the dynamics of Thompson sampling evolve according to discrete versions of SDE's and stochastic ODE's. As $\\gamma \\downarrow 0$, we show that the dynamics converge weakly to solutions of the corresponding SDE's and stochastic ODE's. Our weak convergence theory is developed from first principles using the Continuous Mapping Theorem, and can be easily adapted to analyze other sampling-based bandit algorithms. In this regime, we also show that the weak limits of the dynamics of many sampling-based algorithms -- including Thompson sampling designed for single-parameter exponential family rewards, and algorithms using bootstrap-based sampling to balance exploration and exploitation -- coincide with those of Gaussian Thompson sampling. Moreover, in this regime, these algorithms are generally robust to model mis-specification.","authors":["Lin Fan","Peter W. Glynn"],"url":"https://arxiv.org/abs/2105.09232"}
{"created":"2025-05-13","title":"Approximating 1-Wasserstein Distance between Persistence Diagrams by Graph Sparsification","abstract":"Persistence diagrams (PD)s play a central role in topological data analysis. This analysis requires computing distances among such diagrams such as the $1$-Wasserstein distance. Accurate computation of these PD distances for large data sets that render large diagrams may not scale appropriately with the existing methods. The main source of difficulty ensues from the size of the bipartite graph on which a matching needs to be computed for determining these PD distances. We address this problem by making several algorithmic and computational observations in order to obtain, in theory, a near-linear fully polynomial-time approximation scheme. This is theoretically optimal assuming the $(1+\\epsilon)$-approximate EMD conjecture in constant dimension, which is that the EMD problem on the plane cannot be approximated by a PTAS in time $O(\\frac{1}{\\epsilon^2}n)$ up to polylog factors. In our implementation, first, taking advantage of the distribution of PD points, we \\emph{condense} them thereby decreasing the number of nodes in the graph for computation. The increase in point multiplicities is addressed by reducing the matching problem to a min-cost flow problem on a transshipment network. Second, we use Well Separated Pair Decomposition to sparsify the graph to a size that is linear in the number of points. Both node and arc sparsifications contribute to the approximation factor where we leverage a lower bound given by the Relaxed Word Mover's distance. Third, we eliminate bottlenecks during the sparsification procedure by introducing parallelism. Fourth, we develop an open source software called PDoptFlow based on our algorithm, exploiting parallelism by GPU and multicore. We perform extensive experiments and show that the actual empirical error is very low. We also show that we can achieve high performance at low guaranteed relative errors, improving upon the state of the arts.","authors":["Tamal K. Dey","Simon Zhang"],"url":"https://arxiv.org/abs/2110.14734"}
{"created":"2025-05-13","title":"A proof of P != NP (New symmetric encryption algorithm against any linear attacks and differential attacks)","abstract":"P vs NP problem is the most important unresolved problem in the field of computational complexity. Its impact has penetrated into all aspects of algorithm design, especially in the field of cryptography. The security of cryptographic algorithms based on short keys depends on whether P is equal to NP. In fact, Shannon[1] strictly proved that the one-time-pad system meets unconditional security, but because the one-time-pad system requires the length of key to be at least the length of plaintext, how to transfer the key is a troublesome problem that restricts the use of the one-time-pad system in practice. Cryptography algorithms used in practice are all based on short key, and the security of the short key mechanism is ultimately based on \"one-way\" assumption, that is, it is assumed that a one-way function exists. In fact, the existence of one-way function can directly lead to the important conclusion P != NP. In this paper, we originally constructed a short-key block cipher algorithm. The core feature of this algorithm is that for any block, when a plaintext-ciphertext pair is known, any key in the key space can satisfy the plaintext-ciphertext pair, that is, for each block, the plaintext-ciphertext pair and the key are independence, and the independence between blocks is also easy to construct. This feature is completely different from all existing short-key cipher algorithms. Based on the above feature, we construct a problem and theoretically prove that the problem satisfies the properties of one-way functions, thereby solving the problem of the existence of one-way functions, that is, directly proving that P != NP.","authors":["Gao Ming"],"url":"https://arxiv.org/abs/2203.05022"}
{"created":"2025-05-13","title":"Positive Hennessy-Milner Logic for Branching Bisimulation","abstract":"Labelled transitions systems can be studied in terms of modal logic and in terms of bisimulation. These two notions are connected by Hennessy-Milner theorems, that show that two states are bisimilar precisely when they satisfy the same modal logic formulas. Recently, apartness has been studied as a dual to bisimulation, which also gives rise to a dual version of the Hennessy-Milner theorem: two states are apart precisely when there is a modal formula that distinguishes them.","authors":["Herman Geuvers","Komi Golov"],"url":"https://arxiv.org/abs/2210.07380"}
{"created":"2025-05-13","title":"The Pump Scheduling Problem: A Real-World Scenario for Reinforcement Learning","abstract":"Deep Reinforcement Learning (DRL) has demonstrated impressive results in domains such as games and robotics, where task formulations are well-defined. However, few DRL benchmarks are grounded in complex, real-world environments, where safety constraints, partial observability, and the need for hand-engineered task representations pose significant challenges. To help bridge this gap, we introduce a testbed based on the pump scheduling problem in a real-world water distribution facility. The task involves controlling pumps to ensure a reliable water supply while minimizing energy consumption and respecting the constraints of the system. Our testbed includes a realistic simulator, three years of high-resolution (1-minute) operational data from human-led control, and a baseline RL task formulation. This testbed supports a wide range of research directions, including offline RL, safe exploration, inverse RL, and multi-objective optimization.","authors":["Henrique Don\\^ancio","Laurent Vercouter","Harald Roclawski"],"url":"https://arxiv.org/abs/2210.11111"}
{"created":"2025-05-13","title":"Psychophysiology-aided Perceptually Fluent Speech Analysis of Children Who Stutter","abstract":"This paper presents a novel approach named PASAD that detects changes in perceptually fluent speech acoustics of young children. Particularly, analysis of perceptually fluent speech enables identifying the speech-motor-control factors that are considered as the underlying cause of stuttering disfluencies. Recent studies indicate that the speech production of young children, especially those who stutter, may get adversely affected by situational physiological arousal. A major contribution of this paper is leveraging the speaker's situational physiological responses in real-time to analyze the speech signal effectively. The presented PASAD approach adapts a Hyper-Network structure to extract temporal speech importance information leveraging physiological parameters. Moreover, we collected speech and physiological sensing data from 73 preschool-age children who stutter (CWS) and who do not stutter (CWNS) in different conditions. PASAD's unique architecture enables identifying speech attributes distinct to a CWS's fluent speech and mapping them to the speaker's respective speech-motor-control factors. Extracted knowledge can enhance understanding of children's speech-motor-control and stuttering development. Our comprehensive evaluation shows that PASAD outperforms state-of-the-art multi-modal baseline approaches in different conditions, is expressive and adaptive to the speaker's speech and physiology, generalizable, robust, and is real-time executable.","authors":["Yi Xiao","Harshit Sharma","Victoria Tumanova","Asif Salekin"],"url":"https://arxiv.org/abs/2211.09089"}
{"created":"2025-05-13","title":"Improved Approximation Algorithms for the Expanding Search Problem","abstract":"A searcher is tasked with exploring a graph with edge lengths and vertex weights, starting from a designated vertex. Initially, only the starting vertex is considered explored. At each step, the searcher adds an edge to the solution, connecting an unexplored vertex to an explored one. The time required to add an edge equals its length. The objective is to minimize the weighted sum of exploration times for all vertices. We demonstrate that this problem is hard to approximate and present algorithms with improved approximation guarantees. Specifically, we provide a $(2\\mathrm{e} + \\varepsilon)$-approximation for any $\\varepsilon > 0$ for the general case. On instances where the vertex weights are binary, we achieve a $2\\mathrm{e}$-approximation. Finally, we develop a polynomial-time approximation scheme (PTAS) for Euclidean graphs. Previously, only an $8$-approximation was known for all these cases.","authors":["Svenja M. Griesbach","Felix Hommelsheim","Max Klimm","Kevin Schewior"],"url":"https://arxiv.org/abs/2301.03638"}
{"created":"2025-05-13","title":"Entropy-driven Fair and Effective Federated Learning","abstract":"Federated Learning (FL) enables collaborative model training across distributed devices while preserving data privacy. Nonetheless, the heterogeneity of edge devices often leads to inconsistent performance of the globally trained models, resulting in unfair outcomes among users. Existing federated fairness algorithms strive to enhance fairness but often fall short in maintaining the overall performance of the global model, typically measured by the average accuracy across all clients. To address this issue, we propose a novel algorithm that leverages entropy-based aggregation combined with model and gradient alignments to simultaneously optimize fairness and global model performance. Our method employs a bi-level optimization framework, where we derive an analytic solution to the aggregation probability in the inner loop, making the optimization process computationally efficient. Additionally, we introduce an innovative alignment update and an adaptive strategy in the outer loop to further balance global model's performance and fairness. Theoretical analysis indicates that our approach guarantees convergence even in non-convex FL settings and demonstrates significant fairness improvements in generalized regression and strongly convex models. Empirically, our approach surpasses state-of-the-art federated fairness algorithms, ensuring consistent performance among clients while improving the overall performance of the global model.","authors":["Lin Wang","Zhichao Wang","Ye Shi","Sai Praneeth Karimireddy","Xiaoying Tang"],"url":"https://arxiv.org/abs/2301.12407"}
{"created":"2025-05-13","title":"The evolutionary advantage of guilt: co-evolution of social and non-social guilt in structured populations","abstract":"Building ethical machines may involve bestowing upon them the emotional capacity to self-evaluate and repent on their actions. While apologies represent potential strategic interactions, the explicit evolution of guilt as a behavioural trait remains poorly understood. Our study delves into the co-evolution of two forms of emotional guilt: social guilt entails a cost, requiring agents to exert efforts to understand others' internal states and behaviours; and non-social guilt, which only involves awareness of one's own state, incurs no social cost. Resorting to methods from evolutionary game theory, we study analytically, and through extensive numerical and agent-based simulations, whether and how guilt can evolve and deploy, depending on the underlying structure of the systems of agents. Our findings reveal that in lattice and scale-free networks, strategies favouring emotional guilt dominate a broader range of guilt and social costs compared to non-structured well-mixed populations, so leading to higher levels of cooperation. In structured populations, both social and non-social guilt can thrive through clustering with emotionally inclined strategies, thereby providing protection against exploiters, particularly for less costly non-social strategies. These insights shed light on the complex interplay of guilt and cooperation, enhancing our understanding of ethical artificial intelligence.","authors":["Theodor Cimpeanu","Luis Moniz Pereira","The Anh Han"],"url":"https://arxiv.org/abs/2302.09859"}
{"created":"2025-05-13","title":"Decentralized Adversarial Training over Graphs","abstract":"The vulnerability of machine learning models to adversarial attacks has been attracting considerable attention in recent years. Most existing studies focus on the behavior of stand-alone single-agent learners. In comparison, this work studies adversarial training over graphs, where individual agents are subjected to perturbations of varied strength levels across space. It is expected that interactions by linked agents, and the heterogeneity of the attack models that are possible over the graph, can help enhance robustness in view of the coordination power of the group. Using a min-max formulation of distributed learning, we develop a decentralized adversarial training framework for multi-agent systems. Specifically, we devise two decentralized adversarial training algorithms by relying on two popular decentralized learning strategies--diffusion and consensus. We analyze the convergence properties of the proposed framework for strongly-convex, convex, and non-convex environments, and illustrate the enhanced robustness to adversarial attacks.","authors":["Ying Cao","Elsa Rizk","Stefan Vlaski","Ali H. Sayed"],"url":"https://arxiv.org/abs/2303.13326"}
{"created":"2025-05-13","title":"Asymptotic analysis and efficient random sampling of directed ordered acyclic graphs","abstract":"Directed acyclic graphs (DAGs) are directed graphs in which there is no path from a vertex to itself. DAGs are an omnipresent data structure in computer science and the problem of counting the DAGs of given number of vertices and to sample them uniformly at random has been solved respectively in the 70's and the 00's. In this paper, we propose to explore a new variation of this model where DAGs are endowed with an independent ordering of the out-edges of each vertex, thus allowing to model a wide range of existing data structures. We provide efficient algorithms for sampling objects of this new class, both with or without control on the number of edges, and obtain an asymptotic equivalent of their number. We also show the applicability of our method by providing an effective algorithm for the random generation of classical labelled DAGs with a prescribed number of vertices and edges, based on a similar approach. This is the first known algorithm for sampling labelled DAGs with full control on the number of edges, and it meets a need in terms of applications, that had already been acknowledged in the literature.","authors":["Martin P\\'epin","Alfredo Viola"],"url":"https://arxiv.org/abs/2303.14710"}
{"created":"2025-05-13","title":"Distributed formation-enforcing control for UAVs robust to observation noise in relative pose measurements","abstract":"A technique that allows a Formation-Enforcing Control (FEC) derived from graph rigidity theory to interface with a realistic relative localization system onboard lightweight Unmanned Aerial Vehicles (UAVs) is proposed in this paper. The proposed methodology enables reliable real-world deployment of UAVs in tight formations using relative localization systems burdened by non-negligible sensory noise, which is typically not fully taken into account in FEC algorithms. The proposed solution is based on decomposition of the gradient descent-based FEC command into interpretable elements, and then modifying these individually based on the estimated distribution of sensory noise, such that the resulting action limits the probability of the desired formation. The behavior of the system was analyzed and the practicality of the proposed solution was compared to pure gradient-descent in real-world experiments where it presented significantly better performance in terms of oscillations, deviation from the desired state and convergence time.","authors":["Viktor Walter","Matou\\v{s} Vrba","Daniel Bonilla Licea","Matej Hilmer","Martin Saska"],"url":"https://arxiv.org/abs/2304.03057"}
{"created":"2025-05-13","title":"A Monoidal View on Fixpoint Checks","abstract":"Fixpoints are ubiquitous in computer science as they play a central role in providing a meaning to recursive and cyclic definitions. Bisimilarity, behavioural metrics, termination probabilities for Markov chains and stochastic games are defined in terms of least or greatest fixpoints. Here we show that our recent work which proposes a technique for checking whether the fixpoint of a function is the least (or the largest) admits a natural categorical interpretation in terms of gs-monoidal categories. The technique is based on a construction that maps a function to a suitable approximation. We study the compositionality properties of this mapping and show that under some restrictions it can naturally be interpreted as a (lax) gs-monoidal functor. This guides the development of a tool, called UDEfix that allows us to build functions (and their approximations) like a circuit out of basic building blocks and subsequently perform the fixpoints checks. We also show that a slight generalisation of the theory allows one to treat a new relevant case study: coalgebraic behavioural metrics based on Wasserstein liftings.","authors":["Paolo Baldan","Richard Eggert","Barbara K\\\"onig","Timo Matt","Tommaso Padoan"],"url":"https://arxiv.org/abs/2305.02957"}
{"created":"2025-05-13","title":"Probabilistic detection of GNSS spoofing using opportunistic information","abstract":"Global Navigation Satellite Systems (GNSS) are integrated into many devices. However, civilian GNSS signals are usually not cryptographically protected. This makes attacks that forge signals relatively easy. Considering modern devices often have network connections and onboard sensors, the proposed here Probabilistic Detection of GNSS Spoofing (PDS) scheme is based on such opportunistic information. PDS has at its core two parts. First, a regression problem with motion model constraints, which equalizes the noise of all locations considering the motion model of the device. Second, a Gaussian process, that analyzes statistical properties of location data to construct uncertainty. Then, a likelihood function, that fuses the two parts, as a basis for a Neyman-Pearson lemma (NPL)-based detection strategy. Our experimental evaluation shows a performance gain over the state-of-the-art, in terms of attack detection effectiveness.","authors":["Wenjie Liu","Panos Papadimitratos"],"url":"https://arxiv.org/abs/2305.05404"}
{"created":"2025-05-13","title":"Detecting Misuse of Security APIs: A Systematic Review","abstract":"Security Application Programming Interfaces (APIs) are crucial for ensuring software security. However, their misuse introduces vulnerabilities, potentially leading to severe data breaches and substantial financial loss. Complex API design, inadequate documentation, and insufficient security training often lead to unintentional misuse by developers. The software security community has devised and evaluated several approaches to detecting security API misuse to help developers and organizations. This study rigorously reviews the literature on detecting misuse of security APIs to gain a comprehensive understanding of this critical domain. Our goal is to identify and analyze security API misuses, the detection approaches developed, and the evaluation methodologies employed along with the open research avenues to advance the state-of-the-art in this area. Employing the systematic literature review (SLR) methodology, we analyzed 69 research papers. Our review has yielded (a) identification of 6 security API types; (b) classification of 30 distinct misuses; (c) categorization of detection techniques into heuristic-based and ML-based approaches; and (d) identification of 10 performance measures and 9 evaluation benchmarks. The review reveals a lack of coverage of detection approaches in several areas. We recommend that future efforts focus on aligning security API development with developers' needs and advancing standardized evaluation methods for detection technologies.","authors":["Zahra Mousavi","Chadni Islam","M. Ali Babar","Alsharif Abuadbba","Kristen Moore"],"url":"https://arxiv.org/abs/2306.08869"}
{"created":"2025-05-13","title":"Clickbait Detection via Large Language Models","abstract":"Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a series of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot and zero-shot scenarios on several English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.","authors":["Han Wang","Yi Zhu","Ye Wang","Yun Li","Yunhao Yuan","Jipeng Qiang"],"url":"https://arxiv.org/abs/2306.09597"}
{"created":"2025-05-13","title":"Deep Learning Models for Flood Predictions in South Florida","abstract":"Simulating and predicting the water level/stage in river systems is essential for flood warnings, hydraulic operations, and flood mitigations. Physics-based detailed hydrological and hydraulic computational tools, such as HEC-RAS, MIKE, and SWMM, can be used to simulate a complete watershed and compute the water stage at any point in the river system. However, these physics-based models are computationally intensive, especially for large watersheds and for longer simulations, since they use detailed grid representations of terrain elevation maps of the entire watershed and solve complex partial differential equations (PDEs) for each grid cell. To overcome this problem, we train several deep learning (DL) models for use as surrogate models to rapidly predict the water stage. A portion of the Miami River in South Florida was chosen as a case study for this paper. Extensive experiments show that the performance of various DL models (MLP, RNN, CNN, LSTM, and RCNN) is significantly better than that of the physics-based model, HEC-RAS, even during extreme precipitation conditions (i.e., tropical storms), and with speedups exceeding 500x. To predict the water stages more accurately, our DL models use both measured variables of the river system from the recent past and covariates for which predictions are typically available for the near future.","authors":["Jimeng Shi","Zeda Yin","Rukmangadh Myana","Khandker Ishtiaq","Anupama John","Jayantha Obeysekera","Arturo Leon","Giri Narasimhan"],"url":"https://arxiv.org/abs/2306.15907"}
{"created":"2025-05-13","title":"Review helps learn better: Temporal Supervised Knowledge Distillation","abstract":"Reviewing plays an important role when learning knowledge. The knowledge acquisition at a certain time point may be strongly inspired with the help of previous experience. Thus the knowledge growing procedure should show strong relationship along the temporal dimension. In our research, we find that during the network training, the evolution of feature map follows temporal sequence property. A proper temporal supervision may further improve the network training performance. Inspired by this observation, we propose Temporal Supervised Knowledge Distillation (TSKD). Specifically, we extract the spatiotemporal features in the different training phases of student by convolutional Long Short-term memory network (Conv-LSTM). Then, we train the student net through a dynamic target, rather than static teacher network features. This process realizes the refinement of old knowledge in student network, and utilizes it to assist current learning. Extensive experiments verify the effectiveness and advantages of our method over existing knowledge distillation methods, including various network architectures and different tasks (image classification and object detection) .","authors":["Dongwei Wang","Zhi Han","Yanmei Wang","Xiai Chen","Baichen Liu","Yandong Tang"],"url":"https://arxiv.org/abs/2307.00811"}
{"created":"2025-05-13","title":"Contaminated Multivariate Time-Series Anomaly Detection with Spatio-Temporal Graph Conditional Diffusion Models","abstract":"Mainstream unsupervised anomaly detection algorithms often excel in academic datasets, yet their real-world performance is restricted due to the controlled experimental conditions involving clean training data. Addressing the challenge of training with noise, a prevalent issue in practical anomaly detection, is frequently overlooked. In a pioneering endeavor, this study delves into the realm of label-level noise within sensory time-series anomaly detection (TSAD). This paper presents a novel and practical end-to-end unsupervised TSAD when the training data is contaminated with anomalies. The introduced approach, called TSAD-C, is devoid of access to abnormality labels during the training phase. TSAD-C encompasses three core modules: a Decontaminator to rectify anomalies (aka noise) present during training, a Long-range Variable Dependency Modeling module to capture long-term intra- and inter-variable dependencies within the decontaminated data that is considered as a surrogate of the pure normal data, and an Anomaly Scoring module to detect anomalies from all types. Our extensive experiments conducted on four reliable and diverse datasets conclusively demonstrate that TSAD-C surpasses existing methodologies, thus establishing a new state-of-the-art in the TSAD field.","authors":["Thi Kieu Khanh Ho","Narges Armanfard"],"url":"https://arxiv.org/abs/2308.12563"}
{"created":"2025-05-13","title":"Parallel Unconstrained Local Search for Partitioning Irregular Graphs","abstract":"We present new refinement heuristics for the balanced graph partitioning problem that break with an age-old rule. Traditionally, local search only permits moves that keep the block sizes balanced (below a size constraint). In this work, we demonstrate that admitting large temporary balance violations drastically improves solution quality. The effects are particularly strong on irregular instances such as social networks. Designing efficient implementations of this general idea involves both careful selection of candidates for unconstrained moves as well as algorithms for rebalancing the solution later on. We explore a wide array of design choices to achieve this, in addition to our third goal of high parallel scalability. We present compelling experimental results, demonstrating that our parallel unconstrained local search techniques outperform the prior state of the art by a substantial margin. Compared with four state-of-the-art solvers, our new technique finds 75\\% of the best solutions on irregular graphs. We achieve a 9.6\\% improvement in edge cut over the next best competitor, while being only 7.7\\% slower in the geometric mean.","authors":["Nikolai Maas","Lars Gottesb\\\"uren","Daniel Seemaier"],"url":"https://arxiv.org/abs/2308.15494"}
{"created":"2025-05-13","title":"Analytic and Gevrey class regularity for parametric semilinear reaction-diffusion problems and applications in uncertainty quantification","abstract":"We investigate a class of parametric elliptic semilinear partial differential equations of second order with homogeneous essential boundary conditions, where the coefficients and the right-hand side (and hence the solution) may depend on a parameter. This model can be seen as a reaction-diffusion problem with a polynomial nonlinearity in the reaction term. The efficiency of various numerical approximations across the entire parameter space is closely related to the regularity of the solution with respect to the parameter. We show that if the coefficients and the right-hand side are analytic or Gevrey class regular with respect to the parameter, the same type of parametric regularity is valid for the solution. The key ingredient of the proof is the combination of the alternative-to-factorial technique from our previous work [1] with a novel argument for the treatment of the power-type nonlinearity in the reaction term. As an application of this abstract result, we obtain rigorous convergence estimates for numerical integration of semilinear reaction-diffusion problems with random coefficients using Gaussian and Quasi-Monte Carlo quadrature. Our theoretical findings are confirmed in numerical experiments.","authors":["Alexey Chernov","Tung Le"],"url":"https://arxiv.org/abs/2309.17397"}
{"created":"2025-05-13","title":"Condition numbers in multiview geometry, instability in relative pose estimation, and RANSAC","abstract":"In this paper, we introduce a general framework for analyzing the numerical conditioning of minimal problems in multiple view geometry, using tools from computational algebra and Riemannian geometry. Special motivation comes from the fact that relative pose estimation, based on standard 5-point or 7-point Random Sample Consensus (RANSAC) algorithms, can fail even when no outliers are present and there is enough data to support a hypothesis. We argue that these cases arise due to the intrinsic instability of the 5- and 7-point minimal problems. We apply our framework to characterize the instabilities, both in terms of the world scenes that lead to infinite condition number, and directly in terms of ill-conditioned image data. The approach produces computational tests for assessing the condition number before solving the minimal problem. Lastly, synthetic and real data experiments suggest that RANSAC serves not only to remove outliers, but in practice it also selects for well-conditioned image data, which is consistent with our theory.","authors":["Hongyi Fan","Joe Kileel","Benjamin Kimia"],"url":"https://arxiv.org/abs/2310.02719"}
{"created":"2025-05-13","title":"Survey on Near-Space Information Networks: Channel Modeling, Transmission, and Networking Perspectives","abstract":"Near-space information networks (NSINs) composed of high-altitude platforms (HAPs) and high- and low-altitude unmanned aerial vehicles (UAVs) are a new regime for providing quick, robust, and cost-efficient sensing and communication services. Precipitated by innovations and breakthroughs in manufacturing, materials, communications, electronics, and control techniques, NSINs have been envisioned as an essential component of the emerging sixth-generation of mobile communication systems. This article reveals some critical issues needing to be tackled in NSINs through conducting experiments and discusses the latest advances in NSINs in the research areas of channel modeling, networking, and transmission from a forward-looking, comparative, and technical evolutionary perspective. In this article, we highlight the characteristics of NSINs and present the promising use cases of NSINs. The impact of airborne platforms' unstable movements on the phase delays of onboard antenna arrays with diverse structures is mathematically analyzed. The recent advances in HAP channel modeling are elaborated on, along with the significant differences between HAP and UAV channel modeling. A comprehensive review of the networking techniques of NSINs in network deployment, handoff management, and network management aspects is provided. Besides, the promising techniques and communication protocols of the physical (PHY) layer, medium access control (MAC) layer, network layer, and transport layer of NSINs for achieving efficient transmission over NSINs are reviewed, and we have conducted experiments with practical NSINs to verify the performance of some techniques. Finally, we outline some open issues and promising directions for NSINs deserved for future study and discuss the corresponding challenges.","authors":["Xianbin Cao","Xiaoning Su","Peng Yang","Yue Gao","Dapeng Oliver Wu","Tony Q. S. Quek"],"url":"https://arxiv.org/abs/2310.09025"}
{"created":"2025-05-13","title":"Towards Understanding Sycophancy in Language Models","abstract":"Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.","authors":["Mrinank Sharma","Meg Tong","Tomasz Korbak","David Duvenaud","Amanda Askell","Samuel R. Bowman","Newton Cheng","Esin Durmus","Zac Hatfield-Dodds","Scott R. Johnston","Shauna Kravec","Timothy Maxwell","Sam McCandlish","Kamal Ndousse","Oliver Rausch","Nicholas Schiefer","Da Yan","Miranda Zhang","Ethan Perez"],"url":"https://arxiv.org/abs/2310.13548"}
{"created":"2025-05-13","title":"Semantic and Expressive Variation in Image Captions Across Languages","abstract":"Computer vision often treats human perception as homogeneous: an implicit assumption that visual stimuli are perceived similarly by everyone. This assumption is reflected in the way researchers collect datasets and train vision models. By contrast, literature in cross-cultural psychology and linguistics has provided evidence that people from different cultural backgrounds observe vastly different concepts even when viewing the same visual stimuli. In this paper, we study how these differences manifest themselves in vision-language datasets and models, using language as a proxy for culture. By comparing textual descriptions generated across 7 languages for the same images, we find significant differences in the semantic content and linguistic expression. When datasets are multilingual as opposed to monolingual, descriptions have higher semantic coverage on average, where coverage is measured using scene graphs, model embeddings, and linguistic taxonomies. For example, multilingual descriptions have on average 29.9% more objects, 24.5% more relations, and 46.0% more attributes than a set of monolingual captions. When prompted to describe images in different languages, popular models (e.g. LLaVA) inherit this bias and describe different parts of the image. Moreover, finetuning models on captions from one language performs best on corresponding test data from that language, while finetuning on multilingual data performs consistently well across all test data compositions. Our work points towards the need to account for and embrace the diversity of human perception in the computer vision community.","authors":["Andre Ye","Sebastin Santy","Jena D. Hwang","Amy X. Zhang","Ranjay Krishna"],"url":"https://arxiv.org/abs/2310.14356"}
{"created":"2025-05-13","title":"Beyond Boundaries: A Comprehensive Survey of Transferable Attacks on AI Systems","abstract":"As Artificial Intelligence (AI) systems increasingly underpin critical applications, from autonomous vehicles to biometric authentication, their vulnerability to transferable attacks presents a growing concern. These attacks, designed to generalize across instances, domains, models, tasks, modalities, or even hardware platforms, pose severe risks to security, privacy, and system integrity. This survey delivers the first comprehensive review of transferable attacks across seven major categories, including evasion, backdoor, data poisoning, model stealing, model inversion, membership inference, and side-channel attacks. We introduce a unified six-dimensional taxonomy: cross-instance, cross-domain, cross-modality, cross-model, cross-task, and cross-hardware, which systematically captures the diverse transfer pathways of adversarial strategies. Through this framework, we examine both the underlying mechanics and practical implications of transferable attacks on AI systems. Furthermore, we review cutting-edge methods for enhancing attack transferability, organized around data augmentation and optimization strategies. By consolidating fragmented research and identifying critical future directions, this work provides a foundational roadmap for understanding, evaluating, and defending against transferable threats in real-world AI systems.","authors":["Guangjing Wang","Ce Zhou","Yuanda Wang","Bocheng Chen","Hanqing Guo","Qiben Yan"],"url":"https://arxiv.org/abs/2311.11796"}
{"created":"2025-05-13","title":"Modeling False Data Injection Attacks in Integrated Electricity-Gas Systems","abstract":"This work studies the modeling of false data injection attacks (FDIAs) in integrated electricity-gas systems (IEGSs). First, we introduce a static state estimation model and bad data detection method for IEGSs. Then, we develop FDIAs on IEGSs with complete network topology and parameter information. Next, we develop FDIAs on IEGSs when intruders have only local network topology and parameter information of an IEGS. Lastly, we explore FDIAs on IEGSs when intruders have only local network topology information of an IEGS.","authors":["Rong-Peng Liu","Xiaozhe Wang","Zuyi Li","Rawad Zgheib"],"url":"https://arxiv.org/abs/2312.00781"}
{"created":"2025-05-13","title":"Latent Feature-Guided Diffusion Models for Shadow Removal","abstract":"Recovering textures under shadows has remained a challenging problem due to the difficulty of inferring shadow-free scenes from shadow images. In this paper, we propose the use of diffusion models as they offer a promising approach to gradually refine the details of shadow regions during the diffusion process. Our method improves this process by conditioning on a learned latent feature space that inherits the characteristics of shadow-free images, thus avoiding the limitation of conventional methods that condition on degraded images only. Additionally, we propose to alleviate potential local optima during training by fusing noise features with the diffusion network. We demonstrate the effectiveness of our approach which outperforms the previous best method by 13% in terms of RMSE on the AISTD dataset. Further, we explore instance-level shadow removal, where our model outperforms the previous best method by 82% in terms of RMSE on the DESOBA dataset.","authors":["Kangfu Mei","Luis Figueroa","Zhe Lin","Zhihong Ding","Scott Cohen","Vishal M. Patel"],"url":"https://arxiv.org/abs/2312.02156"}
{"created":"2025-05-13","title":"Jointly spatial-temporal representation learning for individual trajectories","abstract":"Individual trajectories, rich in human-environment interaction information across space and time, serve as vital inputs for geospatial foundation models (GeoFMs). However, existing attempts at learning trajectory representations have overlooked the implicit spatial-temporal dependency within trajectories, failing to encode such dependency in a deep learning-friendly format. That poses a challenge in obtaining general-purpose trajectory representations. Therefore, this paper proposes a spatial-temporal joint representation learning method (ST-GraphRL) to formalize learnable spatial-temporal dependencies into trajectory representations. The proposed ST-GraphRL consists of three compositions: (i) a weighted directed spatial-temporal graph to explicitly construct mobility interactions in both space and time dimensions; (ii) a two-stage jointly encoder (i.e., decoupling and fusion), to learn entangled spatial-temporal dependencies by independently decomposing and jointly aggregating space and time information; (iii) a decoder guides ST-GraphRL to learn explicit mobility regularities by simulating the spatial-temporal distributions of trajectories. Tested on three real-world human mobility datasets, the proposed ST-GraphRL outperformed all the baseline models in predicting movement spatial-temporal distributions and preserving trajectory similarity with high spatial-temporal correlations. Analyzing spatial-temporal features presented in latent space validates that ST-GraphRL understands spatial-temporal patterns. This study may also benefit representation learnings of other geospatial data to achieve general-purpose data representations and advance GeoFMs development.","authors":["Fei Huang","Jianrong Lv","Yang Yue"],"url":"https://arxiv.org/abs/2312.04055"}
{"created":"2025-05-13","title":"Identifying Drivers of Predictive Aleatoric Uncertainty","abstract":"Explainability and uncertainty quantification are key to trustable artificial intelligence. However, the reasoning behind uncertainty estimates is generally left unexplained. Identifying the drivers of uncertainty complements explanations of point predictions in recognizing model limitations and enhancing transparent decision-making. So far, explanations of uncertainties have been rarely studied. The few exceptions rely on Bayesian neural networks or technically intricate approaches, such as auxiliary generative models, thereby hindering their broad adoption. We propose a straightforward approach to explain predictive aleatoric uncertainties. We estimate uncertainty in regression as predictive variance by adapting a neural network with a Gaussian output distribution. Subsequently, we apply out-of-the-box explainers to the model's variance output. This approach can explain uncertainty influences more reliably than complex published approaches, which we demonstrate in a synthetic setting with a known data-generating process. We substantiate our findings with a nuanced, quantitative benchmark including synthetic and real, tabular and image datasets. For this, we adapt metrics from conventional XAI research to uncertainty explanations. Overall, the proposed method explains uncertainty estimates with little modifications to the model architecture and outperforms more intricate methods in most settings.","authors":["Pascal Iversen","Simon Witzke","Katharina Baum","Bernhard Y. Renard"],"url":"https://arxiv.org/abs/2312.07252"}
{"created":"2025-05-13","title":"Gemini: A Family of Highly Capable Multimodal Models","abstract":"This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.","authors":["Gemini Team","Rohan Anil","Sebastian Borgeaud","Jean-Baptiste Alayrac","Jiahui Yu","Radu Soricut","Johan Schalkwyk","Andrew M. Dai","Anja Hauth","Katie Millican","David Silver","Melvin Johnson","Ioannis Antonoglou","Julian Schrittwieser","Amelia Glaese","Jilin Chen","Emily Pitler","Timothy Lillicrap","Angeliki Lazaridou","Orhan Firat","James Molloy","Michael Isard","Paul R. Barham","Tom Hennigan","Benjamin Lee","Fabio Viola","Malcolm Reynolds","Yuanzhong Xu","Ryan Doherty","Eli Collins","Clemens Meyer","Eliza Rutherford","Erica Moreira","Kareem Ayoub","Megha Goel","Jack Krawczyk","Cosmo Du","Ed Chi","Heng-Tze Cheng","Eric Ni","Purvi Shah","Patrick Kane","Betty Chan","Manaal Faruqui","Aliaksei Severyn","Hanzhao Lin","YaGuang Li","Yong Cheng","Abe Ittycheriah","Mahdis Mahdieh","Mia Chen","Pei Sun","Dustin Tran","Sumit Bagri","Balaji Lakshminarayanan","Jeremiah Liu","Andras Orban","Fabian G\\\"ura","Hao Zhou","Xinying Song","Aurelien Boffy","Harish Ganapathy","Steven Zheng","HyunJeong Choe","\\'Agoston Weisz","Tao Zhu","Yifeng Lu","Siddharth Gopal","Jarrod Kahn","Maciej Kula","Jeff Pitman","Rushin Shah","Emanuel Taropa","Majd Al Merey","Martin Baeuml","Zhifeng Chen","Laurent El Shafey","Yujing Zhang","Olcan Sercinoglu","George Tucker","Enrique Piqueras","Maxim Krikun","Iain Barr","Nikolay Savinov","Ivo Danihelka","Becca Roelofs","Ana\\\"is White","Anders Andreassen","Tamara von Glehn","Lakshman Yagati","Mehran Kazemi","Lucas Gonzalez","Misha Khalman","Jakub Sygnowski","Alexandre Frechette","Charlotte Smith","Laura Culp","Lev Proleev","Yi Luan","Xi Chen","James Lottes","Nathan Schucher","Federico Lebron","Alban Rrustemi","Natalie Clay","Phil Crone","Tomas Kocisky","Jeffrey Zhao","Bartek Perz","Dian Yu","Heidi Howard","Adam Bloniarz","Jack W. Rae","Han Lu","Laurent Sifre","Marcello Maggioni","Fred Alcober","Dan Garrette","Megan Barnes","Shantanu Thakoor","Jacob Austin","Gabriel Barth-Maron","William Wong","Rishabh Joshi","Rahma Chaabouni","Deeni Fatiha","Arun Ahuja","Gaurav Singh Tomar","Evan Senter","Martin Chadwick","Ilya Kornakov","Nithya Attaluri","I\\~naki Iturrate","Ruibo Liu","Yunxuan Li","Sarah Cogan","Jeremy Chen","Chao Jia","Chenjie Gu","Qiao Zhang","Jordan Grimstad","Ale Jakse Hartman","Xavier Garcia","Thanumalayan Sankaranarayana Pillai","Jacob Devlin","Michael Laskin","Diego de Las Casas","Dasha Valter","Connie Tao","Lorenzo Blanco","Adri\\`a Puigdom\\`enech Badia","David Reitter","Mianna Chen","Jenny Brennan","Clara Rivera","Sergey Brin","Shariq Iqbal","Gabriela Surita","Jane Labanowski","Abhi Rao","Stephanie Winkler","Emilio Parisotto","Yiming Gu","Kate Olszewska","Ravi Addanki","Antoine Miech","Annie Louis","Denis Teplyashin","Geoff Brown","Elliot Catt","Jan Balaguer","Jackie Xiang","Pidong Wang","Zoe Ashwood","Anton Briukhov","Albert Webson","Sanjay Ganapathy","Smit Sanghavi","Ajay Kannan","Ming-Wei Chang","Axel Stjerngren","Josip Djolonga","Yuting Sun","Ankur Bapna","Matthew Aitchison","Pedram Pejman","Henryk Michalewski","Tianhe Yu","Cindy Wang","Juliette Love","Junwhan Ahn","Dawn Bloxwich","Kehang Han","Peter Humphreys","Thibault Sellam","James Bradbury","Varun Godbole","Sina Samangooei","Bogdan Damoc","Alex Kaskasoli","S\\'ebastien M. R. Arnold","Vijay Vasudevan","Shubham Agrawal","Jason Riesa","Dmitry Lepikhin","Richard Tanburn","Srivatsan Srinivasan","Hyeontaek Lim","Sarah Hodkinson","Pranav Shyam","Johan Ferret","Steven Hand","Ankush Garg","Tom Le Paine","Jian Li","Yujia Li","Minh Giang","Alexander Neitz","Zaheer Abbas","Sarah York","Machel Reid","Elizabeth Cole","Aakanksha Chowdhery","Dipanjan Das","Dominika Rogozi\\'nska","Vitaliy Nikolaev","Pablo Sprechmann","Zachary Nado","Lukas Zilka","Flavien Prost","Luheng He","Marianne Monteiro","Gaurav Mishra","Chris Welty","Josh Newlan","Dawei Jia","Miltiadis Allamanis","Clara Huiyi Hu","Raoul de Liedekerke","Justin Gilmer","Carl Saroufim","Shruti Rijhwani","Shaobo Hou","Disha Shrivastava","Anirudh Baddepudi","Alex Goldin","Adnan Ozturel","Albin Cassirer","Yunhan Xu","Daniel Sohn","Devendra Sachan","Reinald Kim Amplayo","Craig Swanson","Dessie Petrova","Shashi Narayan","Arthur Guez","Siddhartha Brahma","Jessica Landon","Miteyan Patel","Ruizhe Zhao","Kevin Villela","Luyu Wang","Wenhao Jia","Matthew Rahtz","Mai Gim\\'enez","Legg Yeung","James Keeling","Petko Georgiev","Diana Mincu","Boxi Wu","Salem Haykal","Rachel Saputro","Kiran Vodrahalli","James Qin","Zeynep Cankara","Abhanshu Sharma","Nick Fernando","Will Hawkins","Behnam Neyshabur","Solomon Kim","Adrian Hutter","Priyanka Agrawal","Alex Castro-Ros","George van den Driessche","Tao Wang","Fan Yang","Shuo-yiin Chang","Paul Komarek","Ross McIlroy","Mario Lu\\v{c}i\\'c","Guodong Zhang","Wael Farhan","Michael Sharman","Paul Natsev","Paul Michel","Yamini Bansal","Siyuan Qiao","Kris Cao","Siamak Shakeri","Christina Butterfield","Justin Chung","Paul Kishan Rubenstein","Shivani Agrawal","Arthur Mensch","Kedar Soparkar","Karel Lenc","Timothy Chung","Aedan Pope","Loren Maggiore","Jackie Kay","Priya Jhakra","Shibo Wang","Joshua Maynez","Mary Phuong","Taylor Tobin","Andrea Tacchetti","Maja Trebacz","Kevin Robinson","Yash Katariya","Sebastian Riedel","Paige Bailey","Kefan Xiao","Nimesh Ghelani","Lora Aroyo","Ambrose Slone","Neil Houlsby","Xuehan Xiong","Zhen Yang","Elena Gribovskaya","Jonas Adler","Mateo Wirth","Lisa Lee","Music Li","Thais Kagohara","Jay Pavagadhi","Sophie Bridgers","Anna Bortsova","Sanjay Ghemawat","Zafarali Ahmed","Tianqi Liu","Richard Powell","Vijay Bolina","Mariko Iinuma","Polina Zablotskaia","James Besley","Da-Woon Chung","Timothy Dozat","Ramona Comanescu","Xiance Si","Jeremy Greer","Guolong Su","Martin Polacek","Rapha\\\"el Lopez Kaufman","Simon Tokumine","Hexiang Hu","Elena Buchatskaya","Yingjie Miao","Mohamed Elhawaty","Aditya Siddhant","Nenad Tomasev","Jinwei Xing","Christina Greer","Helen Miller","Shereen Ashraf","Aurko Roy","Zizhao Zhang","Ada Ma","Angelos Filos","Milos Besta","Rory Blevins","Ted Klimenko","Chih-Kuan Yeh","Soravit Changpinyo","Jiaqi Mu","Oscar Chang","Mantas Pajarskas","Carrie Muir","Vered Cohen","Charline Le Lan","Krishna Haridasan","Amit Marathe","Steven Hansen","Sholto Douglas","Rajkumar Samuel","Mingqiu Wang","Sophia Austin","Chang Lan","Jiepu Jiang","Justin Chiu","Jaime Alonso Lorenzo","Lars Lowe Sj\\\"osund","S\\'ebastien Cevey","Zach Gleicher","Thi Avrahami","Anudhyan Boral","Hansa Srinivasan","Vittorio Selo","Rhys May","Konstantinos Aisopos","L\\'eonard Hussenot","Livio Baldini Soares","Kate Baumli","Michael B. Chang","Adri\\`a Recasens","Ben Caine","Alexander Pritzel","Filip Pavetic","Fabio Pardo","Anita Gergely","Justin Frye","Vinay Ramasesh","Dan Horgan","Kartikeya Badola","Nora Kassner","Subhrajit Roy","Ethan Dyer","V\\'ictor Campos Campos","Alex Tomala","Yunhao Tang","Dalia El Badawy","Elspeth White","Basil Mustafa","Oran Lang","Abhishek Jindal","Sharad Vikram","Zhitao Gong","Sergi Caelles","Ross Hemsley","Gregory Thornton","Fangxiaoyu Feng","Wojciech Stokowiec","Ce Zheng","Phoebe Thacker","\\c{C}a\\u{g}lar \\\"Unl\\\"u","Zhishuai Zhang","Mohammad Saleh","James Svensson","Max Bileschi","Piyush Patil","Ankesh Anand","Roman Ring","Katerina Tsihlas","Arpi Vezer","Marco Selvi","Toby Shevlane","Mikel Rodriguez","Tom Kwiatkowski","Samira Daruki","Keran Rong","Allan Dafoe","Nicholas FitzGerald","Keren Gu-Lemberg","Mina Khan","Lisa Anne Hendricks","Marie Pellat","Vladimir Feinberg","James Cobon-Kerr","Tara Sainath","Maribeth Rauh","Sayed Hadi Hashemi","Richard Ives","Yana Hasson","Eric Noland","Yuan Cao","Nathan Byrd","Le Hou","Qingze Wang","Thibault Sottiaux","Michela Paganini","Jean-Baptiste Lespiau","Alexandre Moufarek","Samer Hassan","Kaushik Shivakumar","Joost van Amersfoort","Amol Mandhane","Pratik Joshi","Anirudh Goyal","Matthew Tung","Andrew Brock","Hannah Sheahan","Vedant Misra","Cheng Li","Nemanja Raki\\'cevi\\'c","Mostafa Dehghani","Fangyu Liu","Sid Mittal","Junhyuk Oh","Seb Noury","Eren Sezener","Fantine Huot","Matthew Lamm","Nicola De Cao","Charlie Chen","Sidharth Mudgal","Romina Stella","Kevin Brooks","Gautam Vasudevan","Chenxi Liu","Mainak Chain","Nivedita Melinkeri","Aaron Cohen","Venus Wang","Kristie Seymore","Sergey Zubkov","Rahul Goel","Summer Yue","Sai Krishnakumaran","Brian Albert","Nate Hurley","Motoki Sano","Anhad Mohananey","Jonah Joughin","Egor Filonov","Tomasz K\\k{e}pa","Yomna Eldawy","Jiawern Lim","Rahul Rishi","Shirin Badiezadegan","Taylor Bos","Jerry Chang","Sanil Jain","Sri Gayatri Sundara Padmanabhan","Subha Puttagunta","Kalpesh Krishna","Leslie Baker","Norbert Kalb","Vamsi Bedapudi","Adam Kurzrok","Shuntong Lei","Anthony Yu","Oren Litvin","Xiang Zhou","Zhichun Wu","Sam Sobell","Andrea Siciliano","Alan Papir","Robby Neale","Jonas Bragagnolo","Tej Toor","Tina Chen","Valentin Anklin","Feiran Wang","Richie Feng","Milad Gholami","Kevin Ling","Lijuan Liu","Jules Walter","Hamid Moghaddam","Arun Kishore","Jakub Adamek","Tyler Mercado","Jonathan Mallinson","Siddhinita Wandekar","Stephen Cagle","Eran Ofek","Guillermo Garrido","Clemens Lombriser","Maksim Mukha","Botu Sun","Hafeezul Rahman Mohammad","Josip Matak","Yadi Qian","Vikas Peswani","Pawel Janus","Quan Yuan","Leif Schelin","Oana David","Ankur Garg","Yifan He","Oleksii Duzhyi","Anton \\\"Algmyr","Timoth\\'ee Lottaz","Qi Li","Vikas Yadav","Luyao Xu","Alex Chinien","Rakesh Shivanna","Aleksandr Chuklin","Josie Li","Carrie Spadine","Travis Wolfe","Kareem Mohamed","Subhabrata Das","Zihang Dai","Kyle He","Daniel von Dincklage","Shyam Upadhyay","Akanksha Maurya","Luyan Chi","Sebastian Krause","Khalid Salama","Pam G Rabinovitch","Pavan Kumar Reddy M","Aarush Selvan","Mikhail Dektiarev","Golnaz Ghiasi","Erdem Guven","Himanshu Gupta","Boyi Liu","Deepak Sharma","Idan Heimlich Shtacher","Shachi Paul","Oscar Akerlund","Fran\\c{c}ois-Xavier Aubet","Terry Huang","Chen Zhu","Eric Zhu","Elico Teixeira","Matthew Fritze","Francesco Bertolini","Liana-Eleonora Marinescu","Martin B\\\"olle","Dominik Paulus","Khyatti Gupta","Tejasi Latkar","Max Chang","Jason Sanders","Roopa Wilson","Xuewei Wu","Yi-Xuan Tan","Lam Nguyen Thiet","Tulsee Doshi","Sid Lall","Swaroop Mishra","Wanming Chen","Thang Luong","Seth Benjamin","Jasmine Lee","Ewa Andrejczuk","Dominik Rabiej","Vipul Ranjan","Krzysztof Styrc","Pengcheng Yin","Jon Simon","Malcolm Rose Harriott","Mudit Bansal","Alexei Robsky","Geoff Bacon","David Greene","Daniil Mirylenka","Chen Zhou","Obaid Sarvana","Abhimanyu Goyal","Samuel Andermatt","Patrick Siegler","Ben Horn","Assaf Israel","Francesco Pongetti","Chih-Wei \"Louis\" Chen","Marco Selvatici","Pedro Silva","Kathie Wang","Jackson Tolins","Kelvin Guu","Roey Yogev","Xiaochen Cai","Alessandro Agostini","Maulik Shah","Hung Nguyen","Noah \\'O Donnaile","S\\'ebastien Pereira","Linda Friso","Adam Stambler","Adam Kurzrok","Chenkai Kuang","Yan Romanikhin","Mark Geller","ZJ Yan","Kane Jang","Cheng-Chun Lee","Wojciech Fica","Eric Malmi","Qijun Tan","Dan Banica","Daniel Balle","Ryan Pham","Yanping Huang","Diana Avram","Hongzhi Shi","Jasjot Singh","Chris Hidey","Niharika Ahuja","Pranab Saxena","Dan Dooley","Srividya Pranavi Potharaju","Eileen O'Neill","Anand Gokulchandran","Ryan Foley","Kai Zhao","Mike Dusenberry","Yuan Liu","Pulkit Mehta","Ragha Kotikalapudi","Chalence Safranek-Shrader","Andrew Goodman","Joshua Kessinger","Eran Globen","Prateek Kolhar","Chris Gorgolewski","Ali Ibrahim","Yang Song","Ali Eichenbaum","Thomas Brovelli","Sahitya Potluri","Preethi Lahoti","Cip Baetu","Ali Ghorbani","Charles Chen","Andy Crawford","Shalini Pal","Mukund Sridhar","Petru Gurita","Asier Mujika","Igor Petrovski","Pierre-Louis Cedoz","Chenmei Li","Shiyuan Chen","Niccol\\`o Dal Santo","Siddharth Goyal","Jitesh Punjabi","Karthik Kappaganthu","Chester Kwak","Pallavi LV","Sarmishta Velury","Himadri Choudhury","Jamie Hall","Premal Shah","Ricardo Figueira","Matt Thomas","Minjie Lu","Ting Zhou","Chintu Kumar","Thomas Jurdi","Sharat Chikkerur","Yenai Ma","Adams Yu","Soo Kwak","Victor \\\"Ahdel","Sujeevan Rajayogam","Travis Choma","Fei Liu","Aditya Barua","Colin Ji","Ji Ho Park","Vincent Hellendoorn","Alex Bailey","Taylan Bilal","Huanjie Zhou","Mehrdad Khatir","Charles Sutton","Wojciech Rzadkowski","Fiona Macintosh","Roopali Vij","Konstantin Shagin","Paul Medina","Chen Liang","Jinjing Zhou","Pararth Shah","Yingying Bi","Attila Dankovics","Shipra Banga","Sabine Lehmann","Marissa Bredesen","Zifan Lin","John Eric Hoffmann","Jonathan Lai","Raynald Chung","Kai Yang","Nihal Balani","Arthur Bra\\v{z}inskas","Andrei Sozanschi","Matthew Hayes","H\\'ector Fern\\'andez Alcalde","Peter Makarov","Will Chen","Antonio Stella","Liselotte Snijders","Michael Mandl","Ante K\\\"arrman","Pawe{\\l} Nowak","Xinyi Wu","Alex Dyck","Krishnan Vaidyanathan","Raghavender R","Jessica Mallet","Mitch Rudominer","Eric Johnston","Sushil Mittal","Akhil Udathu","Janara Christensen","Vishal Verma","Zach Irving","Andreas Santucci","Gamaleldin Elsayed","Elnaz Davoodi","Marin Georgiev","Ian Tenney","Nan Hua","Geoffrey Cideron","Edouard Leurent","Mahmoud Alnahlawi","Ionut Georgescu","Nan Wei","Ivy Zheng","Dylan Scandinaro","Heinrich Jiang","Jasper Snoek","Mukund Sundararajan","Xuezhi Wang","Zack Ontiveros","Itay Karo","Jeremy Cole","Vinu Rajashekhar","Lara Tumeh","Eyal Ben-David","Rishub Jain","Jonathan Uesato","Romina Datta","Oskar Bunyan","Shimu Wu","John Zhang","Piotr Stanczyk","Ye Zhang","David Steiner","Subhajit Naskar","Michael Azzam","Matthew Johnson","Adam Paszke","Chung-Cheng Chiu","Jaume Sanchez Elias","Afroz Mohiuddin","Faizan Muhammad","Jin Miao","Andrew Lee","Nino Vieillard","Jane Park","Jiageng Zhang","Jeff Stanway","Drew Garmon","Abhijit Karmarkar","Zhe Dong","Jong Lee","Aviral Kumar","Luowei Zhou","Jonathan Evens","William Isaac","Geoffrey Irving","Edward Loper","Michael Fink","Isha Arkatkar","Nanxin Chen","Izhak Shafran","Ivan Petrychenko","Zhe Chen","Johnson Jia","Anselm Levskaya","Zhenkai Zhu","Peter Grabowski","Yu Mao","Alberto Magni","Kaisheng Yao","Javier Snaider","Norman Casagrande","Evan Palmer","Paul Suganthan","Alfonso Casta\\~no","Irene Giannoumis","Wooyeol Kim","Miko{\\l}aj Rybi\\'nski","Ashwin Sreevatsa","Jennifer Prendki","David Soergel","Adrian Goedeckemeyer","Willi Gierke","Mohsen Jafari","Meenu Gaba","Jeremy Wiesner","Diana Gage Wright","Yawen Wei","Harsha Vashisht","Yana Kulizhskaya","Jay Hoover","Maigo Le","Lu Li","Chimezie Iwuanyanwu","Lu Liu","Kevin Ramirez","Andrey Khorlin","Albert Cui","Tian LIN","Marcus Wu","Ricardo Aguilar","Keith Pallo","Abhishek Chakladar","Ginger Perng","Elena Allica Abellan","Mingyang Zhang","Ishita Dasgupta","Nate Kushman","Ivo Penchev","Alena Repina","Xihui Wu","Tom van der Weide","Priya Ponnapalli","Caroline Kaplan","Jiri Simsa","Shuangfeng Li","Olivier Dousse","Fan Yang","Jeff Piper","Nathan Ie","Rama Pasumarthi","Nathan Lintz","Anitha Vijayakumar","Daniel Andor","Pedro Valenzuela","Minnie Lui","Cosmin Paduraru","Daiyi Peng","Katherine Lee","Shuyuan Zhang","Somer Greene","Duc Dung Nguyen","Paula Kurylowicz","Cassidy Hardin","Lucas Dixon","Lili Janzer","Kiam Choo","Ziqiang Feng","Biao Zhang","Achintya Singhal","Dayou Du","Dan McKinnon","Natasha Antropova","Tolga Bolukbasi","Orgad Keller","David Reid","Daniel Finchelstein","Maria Abi Raad","Remi Crocker","Peter Hawkins","Robert Dadashi","Colin Gaffney","Ken Franko","Anna Bulanova","R\\'emi Leblond","Shirley Chung","Harry Askham","Luis C. Cobo","Kelvin Xu","Felix Fischer","Jun Xu","Christina Sorokin","Chris Alberti","Chu-Cheng Lin","Colin Evans","Alek Dimitriev","Hannah Forbes","Dylan Banarse","Zora Tung","Mark Omernick","Colton Bishop","Rachel Sterneck","Rohan Jain","Jiawei Xia","Ehsan Amid","Francesco Piccinno","Xingyu Wang","Praseem Banzal","Daniel J. Mankowitz","Alex Polozov","Victoria Krakovna","Sasha Brown","MohammadHossein Bateni","Dennis Duan","Vlad Firoiu","Meghana Thotakuri","Tom Natan","Matthieu Geist","Ser tan Girgin","Hui Li","Jiayu Ye","Ofir Roval","Reiko Tojo","Michael Kwong","James Lee-Thorp","Christopher Yew","Danila Sinopalnikov","Sabela Ramos","John Mellor","Abhishek Sharma","Kathy Wu","David Miller","Nicolas Sonnerat","Denis Vnukov","Rory Greig","Jennifer Beattie","Emily Caveness","Libin Bai","Julian Eisenschlos","Alex Korchemniy","Tomy Tsai","Mimi Jasarevic","Weize Kong","Phuong Dao","Zeyu Zheng","Frederick Liu","Fan Yang","Rui Zhu","Tian Huey Teh","Jason Sanmiya","Evgeny Gladchenko","Nejc Trdin","Daniel Toyama","Evan Rosen","Sasan Tavakkol","Linting Xue","Chen Elkind","Oliver Woodman","John Carpenter","George Papamakarios","Rupert Kemp","Sushant Kafle","Tanya Grunina","Rishika Sinha","Alice Talbert","Diane Wu","Denese Owusu-Afriyie","Cosmo Du","Chloe Thornton","Jordi Pont-Tuset","Pradyumna Narayana","Jing Li","Saaber Fatehi","John Wieting","Omar Ajmeri","Benigno Uria","Yeongil Ko","Laura Knight","Am\\'elie H\\'eliou","Ning Niu","Shane Gu","Chenxi Pang","Yeqing Li","Nir Levine","Ariel Stolovich","Rebeca Santamaria-Fernandez","Sonam Goenka","Wenny Yustalim","Robin Strudel","Ali Elqursh","Charlie Deck","Hyo Lee","Zonglin Li","Kyle Levin","Raphael Hoffmann","Dan Holtmann-Rice","Olivier Bachem","Sho Arora","Christy Koh","Soheil Hassas Yeganeh","Siim P\\~oder","Mukarram Tariq","Yanhua Sun","Lucian Ionita","Mojtaba Seyedhosseini","Pouya Tafti","Zhiyu Liu","Anmol Gulati","Jasmine Liu","Xinyu Ye","Bart Chrzaszcz","Lily Wang","Nikhil Sethi","Tianrun Li","Ben Brown","Shreya Singh","Wei Fan","Aaron Parisi","Joe Stanton","Vinod Koverkathu","Christopher A. Choquette-Choo","Yunjie Li","TJ Lu","Abe Ittycheriah","Prakash Shroff","Mani Varadarajan","Sanaz Bahargam","Rob Willoughby","David Gaddy","Guillaume Desjardins","Marco Cornero","Brona Robenek","Bhavishya Mittal","Ben Albrecht","Ashish Shenoy","Fedor Moiseev","Henrik Jacobsson","Alireza Ghaffarkhah","Morgane Rivi\\`ere","Alanna Walton","Cl\\'ement Crepy","Alicia Parrish","Zongwei Zhou","Clement Farabet","Carey Radebaugh","Praveen Srinivasan","Claudia van der Salm","Andreas Fidjeland","Salvatore Scellato","Eri Latorre-Chimoto","Hanna Klimczak-Pluci\\'nska","David Bridson","Dario de Cesare","Tom Hudson","Piermaria Mendolicchio","Lexi Walker","Alex Morris","Matthew Mauger","Alexey Guseynov","Alison Reid","Seth Odoom","Lucia Loher","Victor Cotruta","Madhavi Yenugula","Dominik Grewe","Anastasia Petrushkina","Tom Duerig","Antonio Sanchez","Steve Yadlowsky","Amy Shen","Amir Globerson","Lynette Webb","Sahil Dua","Dong Li","Surya Bhupatiraju","Dan Hurt","Haroon Qureshi","Ananth Agarwal","Tomer Shani","Matan Eyal","Anuj Khare","Shreyas Rammohan Belle","Lei Wang","Chetan Tekur","Mihir Sanjay Kale","Jinliang Wei","Ruoxin Sang","Brennan Saeta","Tyler Liechty","Yi Sun","Yao Zhao","Stephan Lee","Pandu Nayak","Doug Fritz","Manish Reddy Vuyyuru","John Aslanides","Nidhi Vyas","Martin Wicke","Xiao Ma","Evgenii Eltyshev","Nina Martin","Hardie Cate","James Manyika","Keyvan Amiri","Yelin Kim","Xi Xiong","Kai Kang","Florian Luisier","Nilesh Tripuraneni","David Madras","Mandy Guo","Austin Waters","Oliver Wang","Joshua Ainslie","Jason Baldridge","Han Zhang","Garima Pruthi","Jakob Bauer","Feng Yang","Riham Mansour","Jason Gelman","Yang Xu","George Polovets","Ji Liu","Honglong Cai","Warren Chen","XiangHai Sheng","Emily Xue","Sherjil Ozair","Christof Angermueller","Xiaowei Li","Anoop Sinha","Weiren Wang","Julia Wiesinger","Emmanouil Koukoumidis","Yuan Tian","Anand Iyer","Madhu Gurumurthy","Mark Goldenson","Parashar Shah","MK Blake","Hongkun Yu","Anthony Urbanowicz","Jennimaria Palomaki","Chrisantha Fernando","Ken Durden","Harsh Mehta","Nikola Momchev","Elahe Rahimtoroghi","Maria Georgaki","Amit Raul","Sebastian Ruder","Morgan Redshaw","Jinhyuk Lee","Denny Zhou","Komal Jalan","Dinghua Li","Blake Hechtman","Parker Schuh","Milad Nasr","Kieran Milan","Vladimir Mikulik","Juliana Franco","Tim Green","Nam Nguyen","Joe Kelley","Aroma Mahendru","Andrea Hu","Joshua Howland","Ben Vargas","Jeffrey Hui","Kshitij Bansal","Vikram Rao","Rakesh Ghiya","Emma Wang","Ke Ye","Jean Michel Sarr","Melanie Moranski Preston","Madeleine Elish","Steve Li","Aakash Kaku","Jigar Gupta","Ice Pasupat","Da-Cheng Juan","Milan Someswar","Tejvi M.","Xinyun Chen","Aida Amini","Alex Fabrikant","Eric Chu","Xuanyi Dong","Amruta Muthal","Senaka Buthpitiya","Sarthak Jauhari","Nan Hua","Urvashi Khandelwal","Ayal Hitron","Jie Ren","Larissa Rinaldi","Shahar Drath","Avigail Dabush","Nan-Jiang Jiang","Harshal Godhia","Uli Sachs","Anthony Chen","Yicheng Fan","Hagai Taitelbaum","Hila Noga","Zhuyun Dai","James Wang","Chen Liang","Jenny Hamer","Chun-Sung Ferng","Chenel Elkind","Aviel Atias","Paulina Lee","V\\'it List\\'ik","Mathias Carlen","Jan van de Kerkhof","Marcin Pikus","Krunoslav Zaher","Paul M\\\"uller","Sasha Zykova","Richard Stefanec","Vitaly Gatsko","Christoph Hirnschall","Ashwin Sethi","Xingyu Federico Xu","Chetan Ahuja","Beth Tsai","Anca Stefanoiu","Bo Feng","Keshav Dhandhania","Manish Katyal","Akshay Gupta","Atharva Parulekar","Divya Pitta","Jing Zhao","Vivaan Bhatia","Yashodha Bhavnani","Omar Alhadlaq","Xiaolin Li","Peter Danenberg","Dennis Tu","Alex Pine","Vera Filippova","Abhipso Ghosh","Ben Limonchik","Bhargava Urala","Chaitanya Krishna Lanka","Derik Clive","Yi Sun","Edward Li","Hao Wu","Kevin Hongtongsak","Ianna Li","Kalind Thakkar","Kuanysh Omarov","Kushal Majmundar","Michael Alverson","Michael Kucharski","Mohak Patel","Mudit Jain","Maksim Zabelin","Paolo Pelagatti","Rohan Kohli","Saurabh Kumar","Joseph Kim","Swetha Sankar","Vineet Shah","Lakshmi Ramachandruni","Xiangkai Zeng","Ben Bariach","Laura Weidinger","Tu Vu","Alek Andreev","Antoine He","Kevin Hui","Sheleem Kashem","Amar Subramanya","Sissie Hsiao","Demis Hassabis","Koray Kavukcuoglu","Adam Sadovsky","Quoc Le","Trevor Strohman","Yonghui Wu","Slav Petrov","Jeffrey Dean","Oriol Vinyals"],"url":"https://arxiv.org/abs/2312.11805"}
{"created":"2025-05-13","title":"Leveraging Habitat Information for Fine-grained Bird Identification","abstract":"Traditional bird classifiers mostly rely on the visual characteristics of birds. Some prior works even train classifiers to be invariant to the background, completely discarding the living environment of birds. Instead, we are the first to explore integrating habitat information, one of the four major cues for identifying birds by ornithologists, into modern bird classifiers. We focus on two leading model types: (1) CNNs and ViTs trained on the downstream bird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with habitat-augmented data results in an improvement of up to +0.83 and +0.23 points on NABirds and CUB-200, respectively. Similarly, adding habitat descriptors to the prompts for CLIP yields a substantial accuracy boost of up to +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find consistent accuracy improvement after integrating habitat features into the image augmentation process and into the textual descriptors of vision-language CLIP classifiers. Code is available at: https://anonymous.4open.science/r/reasoning-8B7E/.","authors":["Tin Nguyen","Peijie Chen","Anh Totti Nguyen"],"url":"https://arxiv.org/abs/2312.14999"}
{"created":"2025-05-13","title":"Adaptive Message Passing: A General Framework to Mitigate Oversmoothing, Oversquashing, and Underreaching","abstract":"Long-range interactions are essential for the correct description of complex systems in many scientific fields. The price to pay for including them in the calculations, however, is a dramatic increase in the overall computational costs. Recently, deep graph networks have been employed as efficient, data-driven models for predicting properties of complex systems represented as graphs. These models rely on a message passing strategy that should, in principle, capture long-range information without explicitly modeling the corresponding interactions. In practice, most deep graph networks cannot really model long-range dependencies due to the intrinsic limitations of (synchronous) message passing, namely oversmoothing, oversquashing, and underreaching. This work proposes a general framework that learns to mitigate these limitations: within a variational inference framework, we endow message passing architectures with the ability to adapt their depth and filter messages along the way. With theoretical and empirical arguments, we show that this strategy better captures long-range interactions, by competing with the state of the art on five node and graph prediction datasets.","authors":["Federico Errica","Henrik Christiansen","Viktor Zaverkin","Takashi Maruyama","Mathias Niepert","Francesco Alesiani"],"url":"https://arxiv.org/abs/2312.16560"}
{"created":"2025-05-13","title":"Error Estimates for Systems of Nonlocal Balance Laws Modeling Dense Multilane Vehicular Traffic","abstract":"We discuss a class of coupled systems of nonlocal nonlinear balance laws modeling multilane traffic, with the nonlocality present in both convective and source terms. The uniqueness and existence of the entropy solution are proven via doubling of the variables arguments and convergent finite volume approximations, respectively. The primary goal is to establish that the finite volume numerical approximations of the system converge to the unique entropy solution at a rate of $\\sqrt{\\Delta t}$, even when using relatively less regular one-sided kernels, compared to the globally smooth kernels analyzed in [Num. Math., 156(1):237-271, 2024] and [IMA J. Numer. Anal., 44(6):3354-3392, 2024].The applicability of the proven theory to a general class of systems of nonlocal balance laws coupled strongly through the convective part and weakly through the source part, is indicated. As the support of the kernel tends to zero, the convergence of the entropy solutions of the proposed model to its local counterparts [SIAM J. Math. Anal., 51: 3694-3713, 2019] is also discussed. Numerical simulations illustrating the behavior of the entropy solutions of the coupled nonlocal systems are also shown.","authors":["Aekta Aggarwal","Helge Holden","Ganesh Vaidya"],"url":"https://arxiv.org/abs/2312.16928"}
{"created":"2025-05-13","title":"Tight Finite Time Bounds of Two-Time-Scale Linear Stochastic Approximation with Markovian Noise","abstract":"Stochastic approximation (SA) is an iterative algorithm for finding the fixed point of an operator using noisy samples and widely used in optimization and Reinforcement Learning (RL). The noise in RL exhibits a Markovian structure, and in some cases, such as gradient temporal difference (GTD) methods, SA is employed in a two-time-scale framework. This combination introduces significant theoretical challenges for analysis.","authors":["Shaan Ul Haque","Sajad Khodadadian","Siva Theja Maguluri"],"url":"https://arxiv.org/abs/2401.00364"}
{"created":"2025-05-13","title":"Leveraging Gradients for Unsupervised Accuracy Estimation under Distribution Shift","abstract":"Estimating the test performance of a model, possibly under distribution shift, without having access to the ground-truth labels is a challenging, yet very important problem for the safe deployment of machine learning algorithms in the wild. Existing works mostly rely on information from either the outputs or the extracted features of neural networks to estimate a score that correlates with the ground-truth test accuracy. In this paper, we investigate -- both empirically and theoretically -- how the information provided by the gradients can be predictive of the ground-truth test accuracy even under distribution shifts. More specifically, we use the norm of classification-layer gradients, backpropagated from the cross-entropy loss after only one gradient step over test data. Our intuition is that these gradients should be of higher magnitude when the model generalizes poorly. We provide the theoretical insights behind our approach and the key ingredients that ensure its empirical success. Extensive experiments conducted with various architectures on diverse distribution shifts demonstrate that our method significantly outperforms current state-of-the-art approaches. The code is available at https://github.com/Renchunzi-Xie/GdScore","authors":["Renchunzi Xie","Ambroise Odonnat","Vasilii Feofanov","Ievgen Redko","Jianfeng Zhang","Bo An"],"url":"https://arxiv.org/abs/2401.08909"}
{"created":"2025-05-13","title":"Application of a Novel Model Reduction Technique to the Assessment of Boundedness/Stability of Some Delay Time-Varying Vector Nonlinear Systems","abstract":"Assessing the boundedness and stability of vector nonlinear systems with variable delays and coefficients remains a challenging problem with broad applications in science and engineering. Existing methods tend to produce overly conservative criteria that offer limited practical value and often fail to explicitly characterize the temporal evolution of solution norms.","authors":["Mark A. Pinsky"],"url":"https://arxiv.org/abs/2401.11398"}
{"created":"2025-05-13","title":"Towards Complementary Knowledge Distillation for Efficient Dense Image Prediction","abstract":"It has been revealed that small efficient dense image prediction (EDIP) models, trained using the knowledge distillation (KD) framework, encounter two key challenges, including maintaining boundary region completeness and preserving target region connectivity, despite their favorable capacity to recognize main object regions. In this work, we propose a complementary boundary and context distillation (BCD) method within the KD framework for EDIPs, which facilitates the targeted knowledge transfer from large accurate teacher models to compact efficient student models. Specifically, the boundary distillation component focuses on extracting explicit object-level semantic boundaries from the hierarchical feature maps of the backbone network to enhance the student model's mask quality in boundary regions. Concurrently, the context distillation component leverages self-relations as a bridge to transfer implicit pixel-level contexts from the teacher model to the student model, ensuring strong connectivity in target regions. Our proposed BCD method is specifically designed for EDIP tasks and is characterized by its simplicity and efficiency. Extensive experimental results across semantic segmentation, object detection, and instance segmentation on various representative datasets demonstrate that our method can outperform existing methods without requiring extra supervisions or incurring increased inference costs, resulting in well-defined object boundaries and smooth connecting regions.","authors":["Dong Zhang","Pingcheng Dong","Long Chen","Kwang-Ting Cheng"],"url":"https://arxiv.org/abs/2401.13174"}
{"created":"2025-05-13","title":"DittoGym: Learning to Control Soft Shape-Shifting Robots","abstract":"Robot co-design, where the morphology of a robot is optimized jointly with a learned policy to solve a specific task, is an emerging area of research. It holds particular promise for soft robots, which are amenable to novel manufacturing techniques that can realize learned morphologies and actuators. Inspired by nature and recent novel robot designs, we propose to go a step further and explore the novel reconfigurable robots, defined as robots that can change their morphology within their lifetime. We formalize control of reconfigurable soft robots as a high-dimensional reinforcement learning (RL) problem. We unify morphology change, locomotion, and environment interaction in the same action space, and introduce an appropriate, coarse-to-fine curriculum that enables us to discover policies that accomplish fine-grained control of the resulting robots. We also introduce DittoGym, a comprehensive RL benchmark for reconfigurable soft robots that require fine-grained morphology changes to accomplish the tasks. Finally, we evaluate our proposed coarse-to-fine algorithm on DittoGym and demonstrate robots that learn to change their morphology several times within a sequence, uniquely enabled by our RL algorithm. More results are available at https://suninghuang19.github.io/dittogym_page/.","authors":["Suning Huang","Boyuan Chen","Huazhe Xu","Vincent Sitzmann"],"url":"https://arxiv.org/abs/2401.13231"}
{"created":"2025-05-13","title":"Markov Insertion/Deletion Channels: Information Stability and Capacity Bounds","abstract":"We consider channels with synchronization errors modeled as insertions and deletions. A classical result for such channels is their information stability, hence the existence of the Shannon capacity, when the synchronization errors are memoryless. In this paper, we extend this result to the case where the insertions and deletions have memory. Specifically, we assume that the synchronization errors are governed by a stationary and ergodic finite state Markov chain, and prove that such channel is information-stable, which implies the existence of a coding scheme which achieves the limit of mutual information. This result implies the existence of the Shannon capacity for a wide range of channels with synchronization errors, with different applications including DNA storage. The methods developed may also be useful to prove other coding theorems for non-trivial channel sequences.","authors":["Ruslan Morozov","Tolga M. Duman"],"url":"https://arxiv.org/abs/2401.16063"}
{"created":"2025-05-13","title":"Integrating Large Language Models in Causal Discovery: A Statistical Causal Approach","abstract":"In practical statistical causal discovery (SCD), embedding domain expert knowledge as constraints into the algorithm is important for reasonable causal models reflecting the broad knowledge of domain experts, despite the challenges in the systematic acquisition of background knowledge. To overcome these challenges, this paper proposes a novel method for causal inference, in which SCD and knowledge-based causal inference (KBCI) with a large language model (LLM) are synthesized through ``statistical causal prompting (SCP)'' for LLMs and prior knowledge augmentation for SCD. The experiments in this work have revealed that the results of LLM-KBCI and SCD augmented with LLM-KBCI approach the ground truths, more than the SCD result without prior knowledge. These experiments have also revealed that the SCD result can be further improved if the LLM undergoes SCP. Furthermore, with an unpublished real-world dataset, we have demonstrated that the background knowledge provided by the LLM can improve the SCD on this dataset, even if this dataset has never been included in the training data of the LLM. For future practical application of this proposed method across important domains such as healthcare, we also thoroughly discuss the limitations, risks of critical errors, expected improvement of techniques around LLMs, and realistic integration of expert checks of the results into this automatic process, with SCP simulations under various conditions both in successful and failure scenarios. The careful and appropriate application of the proposed approach in this work, with improvement and customization for each domain, can thus address challenges such as dataset biases and limitations, illustrating the potential of LLMs to improve data-driven causal inference across diverse scientific domains.","authors":["Masayuki Takayama","Tadahisa Okuda","Thong Pham","Tatsuyoshi Ikenoue","Shingo Fukuma","Shohei Shimizu","Akiyoshi Sannai"],"url":"https://arxiv.org/abs/2402.01454"}
{"created":"2025-05-13","title":"Beyond DAGs: A Latent Partial Causal Model for Multimodal Learning","abstract":"Directed acyclic graphs (DAGs) are fundamental graph structures in causal modeling, but identifying the desired DAG from observational data often requires strong assumptions that may not hold in real-world scenarios, especially for latent causal models and complex multimodal data. This raises the question of whether we can relax or bypass the DAG assumption while maintaining practical utility. In this work, we propose a novel latent partial causal model for multimodal data, featuring two latent coupled variables, connected by an undirected edge, to represent the transfer of knowledge across modalities. Under specific statistical assumptions, we establish an identifiability result, demonstrating that representations learned by multimodal contrastive learning correspond to the latent coupled variables up to a trivial transformation. This result deepens our understanding of the why multimodal contrastive learning works, highlights its potential for disentanglement, and expands the utility of pre-trained models like CLIP. Synthetic experiments confirm the robustness of our findings, even when the assumptions are partially violated. Most importantly, experiments on a pre-trained CLIP model embodies disentangled representations, enabling few-shot learning and improving domain generalization across diverse real-world datasets. Together, these contributions push the boundaries of multimodal contrastive learning, both theoretically and, crucially, in practical applications.","authors":["Yuhang Liu","Zhen Zhang","Dong Gong","Erdun Gao","Biwei Huang","Mingming Gong","Anton van den Hengel","Kun Zhang","Javen Qinfeng Shi"],"url":"https://arxiv.org/abs/2402.06223"}
{"created":"2025-05-13","title":"Barrier-Enhanced Parallel Homotopic Trajectory Optimization for Safety-Critical Autonomous Driving","abstract":"Enforcing safety while preventing overly conservative behaviors is essential for autonomous vehicles to achieve high task performance. In this paper, we propose a barrier-enhanced parallel homotopic trajectory optimization (BPHTO) approach with the over-relaxed alternating direction method of multipliers (ADMM) for real-time integrated decision-making and planning. To facilitate safety interactions between the ego vehicle (EV) and surrounding vehicles, a spatiotemporal safety module exhibiting bi-convexity is developed on the basis of barrier function. Varying barrier coefficients are adopted for different time steps in a planning horizon to account for the motion uncertainties of surrounding HVs and mitigate conservative behaviors. Additionally, we exploit the discrete characteristics of driving maneuvers to initialize nominal behavior-oriented free-end homotopic trajectories based on reachability analysis, and each trajectory is locally constrained to a specific driving maneuver while sharing the same task objectives. By leveraging the bi-convexity of the safety module and the kinematics of the EV, we formulate the BPHTO as a bi-convex optimization problem. Then constraint transcription and the over-relaxed ADMM are employed to streamline the optimization process, such that multiple trajectories are generated in real time with feasibility guarantees. Through a series of experiments, the proposed development demonstrates improved task accuracy, stability, and consistency in various traffic scenarios using synthetic and real-world traffic datasets.","authors":["Lei Zheng","Rui Yang","Michael Yu Wang","Jun Ma"],"url":"https://arxiv.org/abs/2402.10441"}
{"created":"2025-05-13","title":"Can Interpretability Layouts Influence Human Perception of Offensive Sentences?","abstract":"This paper conducts a user study to assess whether three machine learning (ML) interpretability layouts can influence participants' views when evaluating sentences containing hate speech, focusing on the \"Misogyny\" and \"Racism\" classes. Given the existence of divergent conclusions in the literature, we provide empirical evidence on using ML interpretability in online communities through statistical and qualitative analyses of questionnaire responses. The Generalized Additive Model estimates participants' ratings, incorporating within-subject and between-subject designs. While our statistical analysis indicates that none of the interpretability layouts significantly influences participants' views, our qualitative analysis demonstrates the advantages of ML interpretability: 1) triggering participants to provide corrective feedback in case of discrepancies between their views and the model, and 2) providing insights to evaluate a model's behavior beyond traditional performance metrics.","authors":["Thiago Freitas dos Santos","Nardine Osman","Marco Schorlemmer"],"url":"https://arxiv.org/abs/2403.05581"}
{"created":"2025-05-13","title":"Preconditioners based on Voronoi quantizers of random variable coefficients for stochastic elliptic partial differential equations","abstract":"A preconditioning strategy is proposed for the iterative solve of large numbers of linear systems with parameter-dependent matrix and right-hand side which arise during the computation of solution statistics of stochastic elliptic partial differential equations with random and spatially variable coefficients sampled by Monte Carlo. Building on the assumption that a truncated Karhunen-Lo\\`{e}ve expansion of a known transform of the random coefficient is available, we introduce a compact approximation of the random coefficient in the form of a Voronoi quantizer. The number of Voronoi cells, each of which is represented by a centroidal coefficient, is set to the prescribed number of preconditioners. Upon sampling the random coefficient, the linear system assembled with a given realization of the coefficient is solved using a Krylov subspace iterative solver with the preconditioner whose centroidal coefficient is the closest to the realization. We consider different ways to define and obtain the centroidal coefficients, and we investigate the properties of the induced preconditioning strategies in terms of average number of solver iterations for sequential simulations, and of load balancing for parallel simulations. Another approach, which is based on deterministic grids on the system of stochastic coordinates of the truncated representation of the random coefficient, is proposed with a stochastic dimension that increases with the number of preconditioners. This approach allows to bypass the need for preliminary computations in order to determine the optimal stochastic dimension of the truncated approximation of the random coefficient for a given number of preconditioners.","authors":["Nicolas Venkovic","Paul Mycek","Olivier Le Ma\\^itre"],"url":"https://arxiv.org/abs/2403.07824"}
{"created":"2025-05-13","title":"DSP: Dynamic Sequence Parallelism for Multi-Dimensional Transformers","abstract":"Scaling multi-dimensional transformers to long sequences is indispensable across various domains. However, the challenges of large memory requirements and slow speeds of such sequences necessitate sequence parallelism. All existing approaches fall under the category of embedded sequence parallelism, which are limited to shard along a single sequence dimension, thereby introducing significant communication overhead. However, the nature of multi-dimensional transformers involves independent calculations across multiple sequence dimensions. To this end, we propose Dynamic Sequence Parallelism (DSP) as a novel abstraction of sequence parallelism. DSP dynamically switches the parallel dimension among all sequences according to the computation stage with efficient resharding strategy. DSP offers significant reductions in communication costs, adaptability across modules, and ease of implementation with minimal constraints. Experimental evaluations demonstrate DSP's superiority over state-of-the-art embedded sequence parallelism methods by remarkable throughput improvements ranging from 32.2% to 10x, with less than 25% communication volume.","authors":["Xuanlei Zhao","Shenggan Cheng","Chang Chen","Zangwei Zheng","Ziming Liu","Zheming Yang","Yang You"],"url":"https://arxiv.org/abs/2403.10266"}
{"created":"2025-05-13","title":"LOOPer: A Learned Automatic Code Optimizer For Polyhedral Compilers","abstract":"While polyhedral compilers have shown success in implementing advanced code transformations, they still face challenges in selecting the ones that lead to the most profitable speedups. This has motivated the use of machine learning based cost models to guide the search for polyhedral optimizations. State-of-the-art polyhedral compilers have demonstrated a viable proof-of-concept of such an approach. While promising, this approach still faces significant limitations. State-of-the-art polyhedral compilers that use a deep learning cost model only support a small subset of affine transformations, limiting their ability to explore complex code transformations. Furthermore, their applicability does not scale beyond simple programs, thus excluding many program classes from their scope, such as those with non-rectangular iteration domains or multiple loop nests. These limitations significantly impact the generality of such compilers and autoschedulers and put into question the whole approach. In this paper, we introduce LOOPer, the first polyhedral autoscheduler that uses a deep learning based cost model and covers a large space of affine transformations and programs. LOOPer allows the optimization of an extensive set of programs while being effective at applying complex sequences of polyhedral transformations. We implement and evaluate LOOPer and show that it achieves competitive speedups over the state-of-the-art. On the PolyBench benchmarks, LOOPer achieves a geometric mean speedup of 1.84x over Tiramisu and 1.42x over Pluto, two state-of-the-art polyhedral autoschedulers.","authors":["Massinissa Merouani","Khaled Afif Boudaoud","Iheb Nassim Aouadj","Nassim Tchoulak","Islem Kara Bernou","Hamza Benyamina","Fatima Benbouzid-Si Tayeb","Karima Benatchba","Hugh Leather","Riyadh Baghdadi"],"url":"https://arxiv.org/abs/2403.11522"}
{"created":"2025-05-13","title":"Negotiating the Shared Agency between Humans & AI in the Recommender System","abstract":"Smart recommendation algorithms have revolutionized content delivery and improved efficiency across various domains. However, concerns about user agency arise from the algorithms' inherent opacity (information asymmetry) and one-way output (power asymmetry). This study introduces a dual-control mechanism aimed at enhancing user agency, empowering users to manage both data collection and, novelly, the degree of algorithmically tailored content they receive. In a between-subject experiment with 161 participants, we evaluated the impact of varying levels of transparency and control on user experience. Results show that transparency alone is insufficient to foster a sense of agency, and may even exacerbate disempowerment compared to displaying outcomes directly. Conversely, combining transparency with user controls-particularly those allowing direct influence on outcomes-significantly enhances user agency. This research provides a proof-of-concept for a novel approach and lays the groundwork for designing more user-centered recommender systems that emphasize user autonomy and fairness in AI-driven content delivery.","authors":["Mengke Wu","Weizi Liu","Yanyun Wang","Mike Yao"],"url":"https://arxiv.org/abs/2403.15919"}
{"created":"2025-05-13","title":"AIOS: LLM Agent Operating System","abstract":"LLM-based intelligent agents face significant deployment challenges, particularly related to resource management. Allowing unrestricted access to LLM or tool resources can lead to inefficient or even potentially harmful resource allocation and utilization for agents. Furthermore, the absence of proper scheduling and resource management mechanisms in current agent designs hinders concurrent processing and limits overall system efficiency. As the diversity and complexity of agents continue to grow, addressing these resource management issues becomes increasingly critical to LLM-based agent systems. To address these challenges, this paper proposes the architecture of AIOS (LLM-based AI Agent Operating System) under the context of managing LLM-based agents. It introduces a novel architecture for serving LLM-based agents by isolating resources and LLM-specific services from agent applications into an AIOS kernel. This AIOS kernel provides fundamental services (e.g., scheduling, context management, memory management, storage management, access control) and efficient management of resources (e.g., LLM and external tools) for runtime agents. To enhance usability, AIOS also includes an AIOS-Agent SDK, a comprehensive suite of APIs designed for utilizing functionalities provided by the AIOS kernel. Experimental results demonstrate that using AIOS can achieve up to 2.1x faster execution for serving agents built by various agent frameworks. The source code is available at https://github.com/agiresearch/AIOS.","authors":["Kai Mei","Xi Zhu","Wujiang Xu","Wenyue Hua","Mingyu Jin","Zelong Li","Shuyuan Xu","Ruosong Ye","Yingqiang Ge","Yongfeng Zhang"],"url":"https://arxiv.org/abs/2403.16971"}
{"created":"2025-05-13","title":"On the Impact of Black-box Deployment Strategies for Edge AI on Latency and Model Performance","abstract":"Deciding what combination of operators to use across the Edge AI tiers to achieve specific latency and model performance requirements is an open question for MLOps engineers. This study aims to empirically assess the accuracy vs inference time trade-off of different black-box Edge AI deployment strategies, i.e., combinations of deployment operators and deployment tiers. In this paper, we conduct inference experiments involving 3 deployment operators (i.e., Partitioning, Quantization, Early Exit), 3 deployment tiers (i.e., Mobile, Edge, Cloud) and their combinations on four widely used Computer-Vision models to investigate the optimal strategies from the point of view of MLOps developers. Our findings suggest that Edge deployment using the hybrid Quantization + Early Exit operator could be preferred over non-hybrid operators (Quantization/Early Exit on Edge, Partition on Mobile-Edge) when faster latency is a concern at medium accuracy loss. However, when minimizing accuracy loss is a concern, MLOps engineers should prefer using only a Quantization operator on edge at a latency reduction or increase, respectively over the Early Exit/Partition (on edge/mobile-edge) and Quantized Early Exit (on edge) operators. In scenarios constrained by Mobile CPU/RAM resources, a preference for Partitioning across mobile and edge tiers is observed over mobile deployment. For models with smaller input data samples (such as FCN), a network-constrained cloud deployment can also be a better alternative than Mobile/Edge deployment and Partitioning strategies. For models with large input data samples (ResNet, ResNext, DUC), an edge tier having higher network/computational capabilities than Cloud/Mobile can be a more viable option than Partitioning and Mobile/Cloud deployment strategies.","authors":["Jaskirat Singh","Emad Fallahzadeh","Bram Adams","Ahmed E. Hassan"],"url":"https://arxiv.org/abs/2403.17154"}
{"created":"2025-05-13","title":"Order-Optimal Regret with Novel Policy Gradient Approaches in Infinite-Horizon Average Reward MDPs","abstract":"We present two Policy Gradient-based algorithms with general parametrization in the context of infinite-horizon average reward Markov Decision Process (MDP). The first one employs Implicit Gradient Transport for variance reduction, ensuring an expected regret of the order $\\tilde{\\mathcal{O}}(T^{2/3})$. The second approach, rooted in Hessian-based techniques, ensures an expected regret of the order $\\tilde{\\mathcal{O}}(\\sqrt{T})$. These results significantly improve the state-of-the-art $\\tilde{\\mathcal{O}}(T^{3/4})$ regret and achieve the theoretical lower bound. We also show that the average-reward function is approximately $L$-smooth, a result that was previously assumed in earlier works.","authors":["Swetha Ganesh","Washim Uddin Mondal","Vaneet Aggarwal"],"url":"https://arxiv.org/abs/2404.02108"}
{"created":"2025-05-13","title":"Msmsfnet: a multi-stream and multi-scale fusion net for edge detection","abstract":"Edge detection is a long-standing problem in computer vision. Despite the efficiency of existing algorithms, their performance, however, rely heavily on the pre-trained weights of the backbone network on the ImageNet dataset. The use of pre-trained weights in previous methods significantly increases the difficulty to design new models for edge detection without relying on existing well-trained ImageNet models, as pre-training the model on the ImageNet dataset is expensive and becomes compulsory to ensure the fairness of comparison. Besides, the pre-training and fine-tuning strategy is not always useful and sometimes even inaccessible. For instance, the pre-trained weights on the ImageNet dataset are unlikely to be helpful for edge detection in Synthetic Aperture Radar (SAR) images due to strong differences in the statistics between optical images and SAR images. Moreover, no dataset has comparable size to the ImageNet dataset for SAR image processing. In this work, we study the performance achievable by state-of-the-art deep learning based edge detectors in publicly available datasets when they are trained from scratch, and devise a new network architecture, the multi-stream and multi-scale fusion net (msmsfnet), for edge detection. We show in our experiments that by training all models from scratch, our model outperforms state-of-the-art edge detectors in three publicly available datasets. We also demonstrate the efficiency of our model for edge detection in SAR images, where no useful pre-trained weight is available. Finally, We show that our model is able to achieve competitive performance on the BSDS500 dataset when the pre-trained weights are used.","authors":["Chenguang Liu","Chisheng Wang","Feifei Dong","Xiayang Xiao","Xin Su","Chuanhua Zhu","Dejin Zhang","Qingquan Li"],"url":"https://arxiv.org/abs/2404.04856"}
{"created":"2025-05-13","title":"Lean Copilot: Large Language Models as Copilots for Theorem Proving in Lean","abstract":"Neural theorem proving combines large language models (LLMs) with proof assistants such as Lean, where the correctness of formal proofs can be rigorously verified, leaving no room for hallucination. With existing neural theorem provers pretrained on a fixed collection of data and offering valuable suggestions at times, it is challenging for them to continually prove novel theorems in a fully autonomous mode, where human insights may be critical. In this paper, we explore LLMs as copilots that assist humans in proving theorems. We introduce Lean Copilot, a general framework for running LLM inference natively in Lean. It enables programmers to build various LLM-based proof automation tools that integrate seamlessly into the workflow of Lean users. Lean users can use our pretrained models or bring their own ones that run either locally (with or without GPUs) or on the cloud. Using Lean Copilot, we build LLM-based tools that suggest proof steps, complete proof goals, and select relevant premises. Experimental results on the Mathematics in Lean textbook demonstrate the effectiveness of our method compared to existing rule-based proof automation in Lean (aesop). When assisting humans, Lean Copilot requires only 2.08 manually-entered proof steps on average (3.86 required by aesop); when automating the theorem proving process, Lean Copilot automates 74.2% proof steps on average, 85% better than aesop (40.1%). We open source all code and artifacts under a permissive MIT license to facilitate further research.","authors":["Peiyang Song","Kaiyu Yang","Anima Anandkumar"],"url":"https://arxiv.org/abs/2404.12534"}
{"created":"2025-05-13","title":"MARV: Multiview Augmented Reality Visualisation for Exploring Rich Material Data","abstract":"Rich material data is complex, large and heterogeneous, integrating primary and secondary non-destructive testing data for spatial, spatio-temporal, as well as high-dimensional data analyses. Currently, materials experts mainly rely on conventional desktop-based systems using 2D visualisation techniques, which render respective analyses a time-consuming and mentally demanding challenge. MARV is a novel immersive visual analytics system, which makes analyses of such data more effective and engaging in an augmented reality setting. For this purpose, MARV includes three newly designed visualisation techniques: MDD Glyphs with a Skewness Kurtosis Mapper, Temporal Evolution Tracker, and Chrono Bins, facilitating interactive exploration and comparison of multidimensional distributions of attribute data from multiple time steps. A qualitative evaluation conducted with materials experts in a real-world case study demonstrates the benefits of the proposed visualisation techniques. This evaluation revealed that combining spatial and abstract data in an immersive environment improves their analytical capabilities and facilitates the identification of patterns, anomalies, as well as changes over time.","authors":["Alexander Gall","Anja Heim","Eduard Gr\\\"oller","Christoph Heinzl"],"url":"https://arxiv.org/abs/2404.14814"}
{"created":"2025-05-13","title":"MD-NOMAD: Mixture density nonlinear manifold decoder for emulating stochastic differential equations and uncertainty propagation","abstract":"We propose a neural operator framework, termed mixture density nonlinear manifold decoder (MD-NOMAD), for stochastic simulators. Our approach leverages an amalgamation of the pointwise operator learning neural architecture nonlinear manifold decoder (NOMAD) with mixture density-based methods to estimate conditional probability distributions for stochastic output functions. MD-NOMAD harnesses the ability of probabilistic mixture models to estimate complex probability and the high-dimensional scalability of pointwise neural operator NOMAD. We conduct empirical assessments on a wide array of stochastic ordinary and partial differential equations and present the corresponding results, which highlight the performance of the proposed framework.","authors":["Akshay Thakur","Souvik Chakraborty"],"url":"https://arxiv.org/abs/2404.15731"}
{"created":"2025-05-13","title":"Sparse Reconstruction of Optical Doppler Tomography with Alternative State Space Model and Attention","abstract":"Optical coherence Doppler tomography (ODT) is an emerging blood flow imaging technique. The fundamental unit of ODT is the 1D depth-resolved trace named raw A-scans (or A-line). A 2D ODT image (B-scan) is formed by reconstructing a cross-sectional flow image via Doppler phase-subtraction of raw A-scans along B-line. To obtain a high-fidelity B-scan, densely sampled A-scans are required currently, leading to prolonged scanning time and increased storage demands. Addressing this issue, we propose a novel sparse ODT reconstruction framework with an Alternative State Space Attention Network (ASSAN) that effectively reduces raw A-scans needed. Inspired by the distinct distributions of information along A-line and B-line, ASSAN applies 1D State Space Model (SSM) to each A-line to learn the intra-A-scan representation, while using 1D gated self-attention along B-line to capture the inter-A-scan features. In addition, an effective feedforward network based on sequential 1D convolutions along different axes is employed to enhance the local feature. In validation experiments on real animal data, ASSAN shows clear effectiveness in the reconstruction in comparison with state-of-the-art reconstruction methods.","authors":["Zhenghong Li","Jiaxiang Ren","Wensheng Cheng","Congwu Du","Yingtian Pan","Haibin Ling"],"url":"https://arxiv.org/abs/2404.17484"}
{"created":"2025-05-13","title":"Towards Robust Recommendation: A Review and an Adversarial Robustness Evaluation Library","abstract":"Recently, recommender system has achieved significant success. However, due to the openness of recommender systems, they remain vulnerable to malicious attacks. Additionally, natural noise in training data and issues such as data sparsity can also degrade the performance of recommender systems. Therefore, enhancing the robustness of recommender systems has become an increasingly important research topic. In this survey, we provide a comprehensive overview of the robustness of recommender systems. Based on our investigation, we categorize the robustness of recommender systems into adversarial robustness and non-adversarial robustness. In the adversarial robustness, we introduce the fundamental principles and classical methods of recommender system adversarial attacks and defenses. In the non-adversarial robustness, we analyze non-adversarial robustness from the perspectives of data sparsity, natural noise, and data imbalance. Additionally, we summarize commonly used datasets and evaluation metrics for evaluating the robustness of recommender systems. Finally, we also discuss the current challenges in the field of recommender system robustness and potential future research directions. Additionally, to facilitate fair and efficient evaluation of attack and defense methods in adversarial robustness, we propose an adversarial robustness evaluation library--ShillingREC, and we conduct evaluations of basic attack models and recommendation models. ShillingREC project is released at https://github.com/chengleileilei/ShillingREC.","authors":["Lei Cheng","Xiaowen Huang","Jitao Sang","Jian Yu"],"url":"https://arxiv.org/abs/2404.17844"}
{"created":"2025-05-13","title":"ir_explain: a Python Library of Explainable IR Methods","abstract":"While recent advancements in Neural Ranking Models have resulted in significant improvements over traditional statistical retrieval models, it is generally acknowledged that the use of large neural architectures and the application of complex language models in Information Retrieval (IR) have reduced the transparency of retrieval methods. Consequently, Explainability and Interpretability have emerged as important research topics in IR. Several axiomatic and post-hoc explanation methods, as well as approaches that attempt to be interpretable-by-design, have been proposed. This article presents \\irexplain, an open-source Python library that implements a variety of well-known techniques for Explainable IR (ExIR) within a common, extensible framework. \\irexplain supports the three standard categories of post-hoc explanations, namely pointwise, pairwise, and listwise explanations. The library is designed to make it easy to reproduce state-of-the-art ExIR baselines on standard test collections, as well as to explore new approaches to explaining IR models and methods. To facilitate adoption, \\irexplain is well-integrated with widely-used toolkits such as Pyserini and \\irdatasets.","authors":["Sourav Saha","Harsh Agarwal","V Venktesh","Avishek Anand","Swastik Mohanty","Debapriyo Majumdar","Mandar Mitra"],"url":"https://arxiv.org/abs/2404.18546"}
{"created":"2025-05-13","title":"AttackBench: Evaluating Gradient-based Attacks for Adversarial Examples","abstract":"Adversarial examples are typically optimized with gradient-based attacks. While novel attacks are continuously proposed, each is shown to outperform its predecessors using different experimental setups, hyperparameter settings, and number of forward and backward calls to the target models. This provides overly-optimistic and even biased evaluations that may unfairly favor one particular attack over the others. In this work, we aim to overcome these limitations by proposing AttackBench, i.e., the first evaluation framework that enables a fair comparison among different attacks. To this end, we first propose a categorization of gradient-based attacks, identifying their main components and differences. We then introduce our framework, which evaluates their effectiveness and efficiency. We measure these characteristics by (i) defining an optimality metric that quantifies how close an attack is to the optimal solution, and (ii) limiting the number of forward and backward queries to the model, such that all attacks are compared within a given maximum query budget. Our extensive experimental analysis compares more than $100$ attack implementations with a total of over $800$ different configurations against CIFAR-10 and ImageNet models, highlighting that only very few attacks outperform all the competing approaches. Within this analysis, we shed light on several implementation issues that prevent many attacks from finding better solutions or running at all. We release AttackBench as a publicly-available benchmark, aiming to continuously update it to include and evaluate novel gradient-based attacks for optimizing adversarial examples.","authors":["Antonio Emanuele Cin\\`a","J\\'er\\^ome Rony","Maura Pintor","Luca Demetrio","Ambra Demontis","Battista Biggio","Ismail Ben Ayed","Fabio Roli"],"url":"https://arxiv.org/abs/2404.19460"}
{"created":"2025-05-13","title":"A Multi-Agent Rollout Approach for Highway Bottleneck Decongestion in Mixed Autonomy","abstract":"The integration of autonomous vehicles (AVs) into the existing transportation infrastructure offers a promising solution to alleviate congestion and enhance mobility. This research explores a novel approach to traffic optimization by employing a multi-agent rollout approach within a mixed autonomy environment. The study concentrates on coordinating the speed of human-driven vehicles by longitudinally controlling AVs, aiming to dynamically optimize traffic flow and alleviate congestion at highway bottlenecks in real-time. We model the problem as a decentralized partially observable Markov decision process (Dec-POMDP) and propose an improved multi-agent rollout algorithm. By employing agent-by-agent policy iterations, our approach implicitly considers cooperation among multiple agents and seamlessly adapts to complex scenarios where the number of agents dynamically varies. Validated in a real-world network with varying AV penetration rates and traffic flow, the simulations demonstrate that the multi-agent rollout algorithm significantly enhances performance, reducing average travel time on bottleneck segments by 9.42% with a 10% AV penetration rate.","authors":["Lu Liu","Maonan Wang","Man-On Pun","Xi Xiong"],"url":"https://arxiv.org/abs/2405.03132"}
{"created":"2025-05-13","title":"Randomized iterative methods for generalized absolute value equations: Solvability and error bounds","abstract":"Randomized iterative methods, such as the Kaczmarz method and its variants, have gained growing attention due to their simplicity and efficiency in solving large-scale linear systems. Meanwhile, absolute value equations (AVE) have attracted increasing interest due to their connection with the linear complementarity problem. In this paper, we investigate the application of randomized iterative methods to generalized AVE (GAVE). Our approach differs from most existing works in that we tackle GAVE with non-square coefficient matrices. We establish more comprehensive sufficient and necessary conditions for characterizing the solvability of GAVE and propose precise error bound conditions. Furthermore, we introduce a flexible and efficient randomized iterative algorithmic framework for solving GAVE, which employs randomized sketching matrices drawn from user-specified distributions. This framework is capable of encompassing many well-known methods, including the Picard iteration method and the randomized Kaczmarz method. Leveraging our findings on solvability and error bounds, we establish both almost sure convergence and linear convergence rates for this versatile algorithmic framework. Finally, we present numerical examples to illustrate the advantages of the new algorithms.","authors":["Jiaxin Xie","Hou-Duo Qi","Deren Han"],"url":"https://arxiv.org/abs/2405.04091"}
{"created":"2025-05-13","title":"MAPL: Memory Augmentation and Pseudo-Labeling for Semi-Supervised Anomaly Detection","abstract":"Large unlabeled data and difficult-to-identify anomalies are the urgent issues need to overcome in most industrial scene. In order to address this issue, a new meth-odology for detecting surface defects in in-dustrial settings is introduced, referred to as Memory Augmentation and Pseudo-Labeling(MAPL). The methodology first in-troduces an anomaly simulation strategy, which significantly improves the model's ability to recognize rare or unknown anom-aly types by generating simulated anomaly samples. To cope with the problem of the lack of labeling of anomalous simulated samples, a pseudo-labeler method based on a one-classifier ensemble was employed in this study, which enhances the robustness of the model in the case of limited labeling data by automatically selecting key pseudo-labeling hyperparameters. Meanwhile, a memory-enhanced learning mechanism is introduced to effectively predict abnormal regions by analyzing the difference be-tween the input samples and the normal samples in the memory pool. An end-to-end learning framework is employed by MAPL to identify the abnormal regions directly from the input data, which optimizes the ef-ficiency and real-time performance of de-tection. By conducting extensive trials on the recently developed BHAD dataset (in-cluding MVTec AD [1], Visa [2], and MDPP [3]), MAPL achieves an average im-age-level AUROC score of 86.2%, demon-strating a 5.1% enhancement compared to the original MemSeg [4] model. The source code is available at https://github.com/jzc777/MAPL.","authors":["Junzhuo Chen","Shitong Kang"],"url":"https://arxiv.org/abs/2405.06198"}
{"created":"2025-05-13","title":"Fleet of Agents: Coordinated Problem Solving with Large Language Models","abstract":"While numerous frameworks have been developed to enhance the reasoning abilities of large language models (LLMs), there is a scarcity of methods that effectively balance the trade-off between cost and quality. In this paper, we introduce Fleet of Agents (FoA), a novel and intuitive yet principled framework utilizing LLMs as agents to navigate through dynamic tree searches, employing a genetic-type particle filtering approach. FoA spawns a multitude of agents, each exploring the search space autonomously, followed by a selection phase where resampling based on a heuristic value function optimizes the balance between exploration and exploitation. This mechanism enables dynamic branching, adapting the exploration strategy based on discovered solutions. We conduct extensive experiments on three benchmark tasks, ``Game of 24'', ``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs, ``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average across all tasks and LLMs, FoA obtains a quality improvement of ~5% while requiring only ~40% of the cost of previous SOTA methods. Notably, our analyses reveal that (1) FoA achieves the best cost-quality trade-off among all benchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B model. FoA is publicly available at https://github.com/au-clan/FoA.","authors":["Lars Klein","Nearchos Potamitis","Roland Aydin","Robert West","Caglar Gulcehre","Akhil Arora"],"url":"https://arxiv.org/abs/2405.06691"}
{"created":"2025-05-13","title":"Designing Adaptive User Interfaces for mHealth Applications Targeting Chronic Disease: A User-Centered Approach","abstract":"Mobile Health (mHealth) applications have demonstrated considerable potential in supporting chronic disease self-management; however, they remain under-utilised due to low engagement, limited accessibility, and poor long-term adherence. These issues are particularly prominent among users with chronic disease, whose needs and capabilities vary widely. To address this, Adaptive User Interfaces (AUIs) offer a dynamic solution by tailoring interface features to users' preferences, health status, and contexts. This paper presents a two-stage study to develop and validate actionable AUI design guidelines for mHealth applications. In stage one, an AUI prototype was evaluated through focus groups, interviews, and a standalone survey, revealing key user challenges and preferences. These insights informed the creation of an initial set of guidelines. In stage two, the guidelines were refined based on feedback from 20 end users and evaluated by 43 software practitioners through two surveys. This process resulted in nine finalized guidelines. To assess real-world relevance, a case study of four mHealth applications was conducted, with findings supported by user reviews highlighting the utility of the guidelines in identifying critical adaptation issues. This study offers actionable, evidence-based guidelines that help software practitioners design AUIs in mHealth to better support individuals managing chronic diseases","authors":["Wei Wang","John Grundy","Hourieh Khalajzadeh","Anuradha Madugalla","Humphrey O. Obie"],"url":"https://arxiv.org/abs/2405.08302"}
{"created":"2025-05-13","title":"Transient Nonlinear Electrothermal Adjoint Sensitivity Analysis for HVDC Cable Joints","abstract":"Efficient computation of sensitivities is a promising approach for efficiently of designing and optimizing high voltage direct current cable joints. This paper presents the adjoint variable method for coupled nonlinear transient electrothermal problems as an efficient approach to compute sensitivities with respect to a large number of design parameters. The method is used to compute material sensitivities of a 320kV high voltage direct current cable joint specimen. The results are validated against sensitivities obtained via the direct sensitivity method.","authors":["M. Greta Ruppert","Yvonne Sp\\\"ack-Leigsnering","Herbert De Gersem"],"url":"https://arxiv.org/abs/2405.14284"}
{"created":"2025-05-13","title":"EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization","abstract":"Large language models (LLMs) have shown remarkable progress in code generation, but their generated code often suffers from inefficiency, resulting in longer execution times and higher memory consumption. To address this issue, we propose \\textbf{EffiLearner}, a self-optimization framework that utilizes execution overhead profiles to improve the efficiency of LLM-generated code. EffiLearner first generates code using an LLM, then executes it locally to capture execution time and memory usage profiles. These profiles are fed back to the LLM, which then revises the code to reduce overhead. To evaluate the effectiveness of EffiLearner, we conduct extensive experiments on the EffiBench, HumanEval, and MBPP with 16 open-source and 6 closed-source models. Our evaluation results demonstrate that through iterative self-optimization, EffiLearner significantly enhances the efficiency of LLM-generated code. For example, the execution time (ET) of StarCoder2-15B for the EffiBench decreases from 0.93 (s) to 0.12 (s) which reduces 87.1% the execution time requirement compared with the initial code. The total memory usage (TMU) of StarCoder2-15B also decreases from 22.02 (Mb*s) to 2.03 (Mb*s), which decreases 90.8% of total memory consumption during the execution process. The source code of EffiLearner was released in https://github.com/huangd1999/EffiLearner","authors":["Dong Huang","Jianbo Dai","Han Weng","Puzhen Wu","Yuhao Qing","Heming Cui","Zhijiang Guo","Jie M. Zhang"],"url":"https://arxiv.org/abs/2405.15189"}
{"created":"2025-05-13","title":"Scaling up the Banded Matrix Factorization Mechanism for Differentially Private ML","abstract":"Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes. Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF, which optimally balances the benefits of privacy amplification and noise correlation. Despite it's utility advantages, severe scalability limitations prevent this mechanism from handling large-scale training scenarios where the number of training iterations may exceed $10^4$ and the number of model parameters may exceed $10^7$. In this work, we present techniques to scale up DP-BandMF along these two dimensions, significantly extending it's reach and enabling it to handle settings with virtually any number of model parameters and training iterations, with negligible utility degradation.","authors":["Ryan McKenna"],"url":"https://arxiv.org/abs/2405.15913"}
{"created":"2025-05-13","title":"Generalized Compressed Sensing for Image Reconstruction with Diffusion Probabilistic Models","abstract":"We examine the problem of selecting a small set of linear measurements for reconstructing high-dimensional signals. Well-established methods for optimizing such measurements include principal component analysis (PCA), independent component analysis (ICA) and compressed sensing (CS) based on random projections, all of which rely on axis- or subspace-aligned statistical characterization of the signal source. However, many naturally occurring signals, including photographic images, contain richer statistical structure. To exploit such structure, we introduce a general method for obtaining an optimized set of linear measurements for efficient image reconstruction, where the signal statistics are expressed by the prior implicit in a neural network trained to perform denoising (known as a ``diffusion model''). We demonstrate that the optimal measurements derived for two natural image datasets differ from those of PCA, ICA, or CS, and result in substantially lower mean squared reconstruction error. Interestingly, the marginal distributions of the measurement values are asymmetrical (skewed), substantially more so than those of previous methods. We also find that optimizing with respect to perceptual loss, as quantified by structural similarity (SSIM), leads to measurements different from those obtained when optimizing for MSE. Our results highlight the importance of incorporating the specific statistical regularities of natural signals when designing effective linear measurements.","authors":["Ling-Qi Zhang","Zahra Kadkhodaie","Eero P. Simoncelli","David H. Brainard"],"url":"https://arxiv.org/abs/2405.17456"}
{"created":"2025-05-13","title":"DEFT: Efficient Fine-Tuning of Diffusion Models by Learning the Generalised $h$-transform","abstract":"Generative modelling paradigms based on denoising diffusion processes have emerged as a leading candidate for conditional sampling in inverse problems. In many real-world applications, we often have access to large, expensively trained unconditional diffusion models, which we aim to exploit for improving conditional sampling. Most recent approaches are motivated heuristically and lack a unifying framework, obscuring connections between them. Further, they often suffer from issues such as being very sensitive to hyperparameters, being expensive to train or needing access to weights hidden behind a closed API. In this work, we unify conditional training and sampling using the mathematically well-understood Doob's h-transform. This new perspective allows us to unify many existing methods under a common umbrella. Under this framework, we propose DEFT (Doob's h-transform Efficient FineTuning), a new approach for conditional generation that simply fine-tunes a very small network to quickly learn the conditional $h$-transform, while keeping the larger unconditional network unchanged. DEFT is much faster than existing baselines while achieving state-of-the-art performance across a variety of linear and non-linear benchmarks. On image reconstruction tasks, we achieve speedups of up to 1.6$\\times$, while having the best perceptual quality on natural images and reconstruction performance on medical images. Further, we also provide initial experiments on protein motif scaffolding and outperform reconstruction guidance methods.","authors":["Alexander Denker","Francisco Vargas","Shreyas Padhy","Kieran Didi","Simon Mathis","Vincent Dutordoir","Riccardo Barbano","Emile Mathieu","Urszula Julia Komorowska","Pietro Lio"],"url":"https://arxiv.org/abs/2406.01781"}
{"created":"2025-05-13","title":"Branches: Efficiently Seeking Optimal Sparse Decision Trees with AO*","abstract":"Decision Tree (DT) Learning is a fundamental problem in Interpretable Machine Learning, yet it poses a formidable optimisation challenge. Practical algorithms have recently emerged, primarily leveraging Dynamic Programming and Branch & Bound. However, most of these approaches rely on a Depth-First-Search strategy, which is inefficient when searching for DTs at high depths and requires the definition of a maximum depth hyperparameter. Best-First-Search was also employed by other methods to circumvent these issues. The downside of this strategy is its higher memory consumption, as such, it has to be designed in a fully efficient manner that takes full advantage of the problem's structure. We formulate the problem within an AND/OR graph search framework and we solve it with a novel AO*-type algorithm called Branches. We prove both optimality and complexity guarantees for Branches and we show that it is more efficient than the state of the art theoretically and on a variety of experiments. Furthermore, Branches supports non-binary features unlike the other methods, we show that this property can further induce larger gains in computational efficiency.","authors":["Ayman Chaouki","Jesse Read","Albert Bifet"],"url":"https://arxiv.org/abs/2406.02175"}
{"created":"2025-05-13","title":"Neural empirical interpolation method for nonlinear model reduction","abstract":"In this paper, we introduce the neural empirical interpolation method (NEIM), a neural network-based alternative to the discrete empirical interpolation method for reducing the time complexity of computing the nonlinear term in a reduced order model (ROM) for a parameterized nonlinear partial differential equation. NEIM is a greedy algorithm which accomplishes this reduction by approximating an affine decomposition of the nonlinear term of the ROM, where the vector terms of the expansion are given by neural networks depending on the ROM solution, and the coefficients are given by an interpolation of some \"optimal\" coefficients. Because NEIM is based on a greedy strategy, we are able to provide a basic error analysis to investigate its performance. NEIM has the advantages of being easy to implement in models with automatic differentiation, of being a nonlinear projection of the ROM nonlinearity, of being efficient for both nonlocal and local nonlinearities, and of relying solely on data and not the explicit form of the ROM nonlinearity. We demonstrate the effectiveness of the methodology on solution-dependent and solution-independent nonlinearities, a nonlinear elliptic problem, and a nonlinear parabolic model of liquid crystals.","authors":["Max Hirsch","Federico Pichi","Jan S. Hesthaven"],"url":"https://arxiv.org/abs/2406.03562"}
{"created":"2025-05-13","title":"MedualTime: A Dual-Adapter Language Model for Medical Time Series-Text Multimodal Learning","abstract":"The recent rapid advancements in language models (LMs) have garnered attention in medical time series-text multimodal learning. However, existing contrastive learning-based and prompt-based LM approaches tend to be biased, often assigning a primary role to time series modality while treating text modality as secondary. We classify these approaches under a temporal-primary paradigm, which may overlook the unique and critical task-relevant information embedded in text modality like clinical reports, thus failing to fully leverage mutual benefits and complementarity of different modalities. To fill this gap, we propose a novel textual-temporal multimodal learning paradigm that enables either modality to serve as the primary while being enhanced by the other, thereby effectively capturing modality-specific information and fostering cross-modal interaction. In specific, we design MedualTime, a language model composed of dual adapters to implement temporal-primary and textual-primary modeling simultaneously. Within each adapter, lightweight adaptation tokens are injected into the top layers of LM to encourage high-level modality fusion. The shared LM pipeline by dual adapters not only achieves adapter alignment but also enables efficient fine-tuning, reducing computational resources. Empirically, MedualTime demonstrates superior performance on medical data, achieving notable improvements of 8% accuracy and 12% F1 in supervised settings. Furthermore, MedualTime's transferability is validated by few-shot label transfer experiments from coarse-grained to fine-grained medical data. https://github.com/start2020/MedualTime","authors":["Jiexia Ye","Weiqi Zhang","Ziyue Li","Jia Li","Meng Zhao","Fugee Tsung"],"url":"https://arxiv.org/abs/2406.06620"}
{"created":"2025-05-13","title":"Marginalization Consistent Probabilistic Forecasting of Irregular Time Series via Mixture of Separable flows","abstract":"Probabilistic forecasting models for joint distributions of targets in irregular time series with missing values are a heavily under-researched area in machine learning, with, to the best of our knowledge, only two Models have been researched so far: The Gaussian Process Regression model, and ProFITi. While ProFITi, thanks to using multivariate normalizing flows, is very expressive, leading to better predictive performance, it suffers from marginalization inconsistency: It does not guarantee that the marginal distributions of a subset of variables in its predictive distributions coincide with the directly predicted distributions of these variables. When asked to directly predict marginal distributions, they are often vastly inaccurate. We propose MOSES (Marginalization Consistent Mixture of Separable Flows), a model that parametrizes a stochastic process through a mixture of several latent multivariate Gaussian Processes combined with separable univariate Normalizing Flows. In particular, MOSES can be analytically marginalized, allowing it to directly answer a wider range of probabilistic queries than most competitors. Experiments on four datasets show that MOSES achieves both accurate joint and marginal predictions, surpassing all other marginalization consistent baselines, while only trailing slightly behind ProFITi in joint prediction, but vastly superior when predicting marginal distributions.","authors":["Vijaya Krishna Yalavarthi","Randolf Scholz","Christian Kloetergens","Kiran Madhusudhanan","Stefan Born","Lars Schmidt-Thieme"],"url":"https://arxiv.org/abs/2406.07246"}
{"created":"2025-05-13","title":"RobGC: Towards Robust Graph Condensation","abstract":"Graph neural networks (GNNs) have attracted widespread attention for their impressive capability of graph representation learning. However, the increasing prevalence of large-scale graphs presents a significant challenge for GNN training due to their computational demands, limiting the applicability of GNNs in various scenarios. In response to this challenge, graph condensation (GC) is proposed as a promising acceleration solution, focusing on generating an informative compact graph that enables efficient training of GNNs while retaining performance. Despite the potential to accelerate GNN training, existing GC methods overlook the quality of large training graphs during both the training and inference stages. They indiscriminately emulate the training graph distributions, making the condensed graphs susceptible to noises within the training graph and significantly impeding the application of GC in intricate real-world scenarios. To address this issue, we propose robust graph condensation (RobGC), a plug-and-play approach for GC to extend the robustness and applicability of condensed graphs in noisy graph structure environments. Specifically, RobGC leverages the condensed graph as a feedback signal to guide the denoising process on the original training graph. A label propagation-based alternating optimization strategy is in place for the condensation and denoising processes, contributing to the mutual purification of the condensed graph and training graph. Additionally, as a GC method designed for inductive graph inference, RobGC facilitates test-time graph denoising by leveraging the noise-free condensed graph to calibrate the structure of the test graph. Extensive experiments show that RobGC is compatible with various GC methods, significantly boosting their robustness under different types and levels of graph structural noises.","authors":["Xinyi Gao","Hongzhi Yin","Tong Chen","Guanhua Ye","Wentao Zhang","Bin Cui"],"url":"https://arxiv.org/abs/2406.13200"}
{"created":"2025-05-13","title":"Safety-Critical Formation Control of Non-Holonomic Multi-Robot Systems in Communication-Limited Environments","abstract":"This paper introduces a decentralized estimator-based safety-critical controller designed for formation control of non-holonomic mobile robots operating in communication-constrained environments. The proposed framework integrates a robust state estimator capable of accurately reconstructing neighboring agents' velocity vectors and orientations under varying dynamic conditions, with a decentralized formation tracking controller that leverages Control Barrier Functions (CBFs) to guarantee collision avoidance and inter-agent safety. We present a closed-form control law that ensures both stability and string stability, effectively attenuating disturbances propagating from leader to followers. The theoretical foundations of the estimator and controller are established using Lyapunov stability analysis, which confirms global asymptotic stability under constant velocities and global uniformly ultimate boundedness under time-varying conditions. Extensive numerical simulations and realistic Gazebo-based experiments validate the effectiveness, robustness, and practical applicability of the proposed method, demonstrating precise formation tracking, stringent safety maintenance, and disturbance resilience without relying on inter-robot communication.","authors":["Vishrut Bohara","Siavash Farzan"],"url":"https://arxiv.org/abs/2406.13707"}
{"created":"2025-05-13","title":"Enhancing Monotonic Modeling with Spatio-Temporal Adaptive Awareness in Diverse Marketing","abstract":"In the mobile internet era, the Online Food Ordering Service (OFOS) emerges as an integral component of inclusive finance owing to the convenience it brings to people. OFOS platforms offer dynamic allocation incentives to users and merchants through diverse marketing campaigns to encourage payments while maintaining the platforms' budget efficiency. Despite significant progress, the marketing domain continues to face two primary challenges: (i) how to allocate a limited budget with greater efficiency, demanding precision in predicting users' monotonic response (i.e. sensitivity) to incentives, and (ii) ensuring spatio-temporal adaptability and robustness in diverse marketing campaigns across different times and locations. To address these issues, we propose a Constrained Monotonic Adaptive Network (CoMAN) method for spatio-temporal perception within marketing pricing. Specifically, we capture spatio-temporal preferences within attribute features through two foundational spatio-temporal perception modules. To further enhance catching the user sensitivity differentials to incentives across varied times and locations, we design modules for learning spatio-temporal convexity and concavity as well as for expressing sensitivity functions. CoMAN can achieve a more efficient allocation of incentive investments during pricing, thus increasing the conversion rate and orders while maintaining budget efficiency. Extensive offline and online experimental results within our diverse marketing campaigns demonstrate the effectiveness of the proposed approach while outperforming the monotonic state-of-the-art method.","authors":["Bin Li","Jiayan Pei","Feiyang Xiao","Yifan Zhao","Zhixing Zhang","Diwei Liu","HengXu He","Jia Jia"],"url":"https://arxiv.org/abs/2406.14132"}
{"created":"2025-05-13","title":"A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis","abstract":"Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing (NLP), addressing subjective assessments in textual content. Syntactic parsing is useful in SA because explicit syntactic information can improve accuracy while providing explainability, but it tends to be a computational bottleneck in practice due to the slowness of parsing algorithms. This paper addresses said bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject syntax into SA. By treating dependency parsing as a sequence labeling problem, we greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated on a ternary polarity classification task, demonstrating its faster performance and better accuracy in polarity prediction tasks compared to conventional parsers like Stanza and to heuristic approaches that use shallow syntactic rules for SA like VADER. This increased speed and improved accuracy make SELSP particularly appealing to SA practitioners in both research and industry. In addition, we test several sentiment dictionaries on our SELSP to see which one improves the performance in polarity prediction tasks. Moreover, we compare the SELSP with Transformer-based models trained on a 5-label classification task. The results show that dictionaries that capture polarity judgment variation provide better results than dictionaries that ignore polarity judgment variation. Moreover, we show that SELSP is considerably faster than Transformer-based models in polarity prediction tasks.","authors":["Muhammad Imran","Olga Kellert","Carlos G\\'omez-Rodr\\'iguez"],"url":"https://arxiv.org/abs/2406.15163"}
{"created":"2025-05-13","title":"From Distributional to Overton Pluralism: Investigating Large Language Model Alignment","abstract":"The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at https://github.com/thomlake/investigating-alignment.","authors":["Thom Lake","Eunsol Choi","Greg Durrett"],"url":"https://arxiv.org/abs/2406.17692"}
{"created":"2025-05-13","title":"LLMEasyQuant: Scalable Quantization for Parallel and Distributed LLM Inference","abstract":"As large language models (LLMs) grow in size and deployment scale, quantization has become an essential technique for reducing memory footprint and improving inference efficiency. However, existing quantization toolkits often lack transparency, flexibility, and system-level scalability across GPUs and distributed environments. We present \\textbf{LLMEasyQuant}, a modular, system-aware quantization framework designed for efficient, low-bit inference of LLMs on single-node multi-GPU, multi-node, and edge hardware. LLMEasyQuant supports a wide range of quantization methods -- including Symmetric Quantization, ZeroQuant, SmoothQuant, and SimQuant -- with unified interfaces for per-layer calibration, bitwidth assignment, and runtime adaptation. It integrates fused CUDA kernels with NCCL-based distributed synchronization and supports both static and online quantization. Empirical results show that LLMEasyQuant can achieve substantial speedups in GEMM execution, HBM load time, and near-linear multi-GPU scaling. Ablation studies further validate its ability to balance latency, memory, and accuracy under diverse deployment conditions. LLMEasyQuant offers a practical quantization serving system for scalable, hardware-optimized LLM inference.","authors":["Dong Liu","Yanxuan Yu"],"url":"https://arxiv.org/abs/2406.19657"}
{"created":"2025-05-13","title":"A Lightweight UDF Learning Framework for 3D Reconstruction Based on Local Shape Functions","abstract":"Unsigned distance fields (UDFs) provide a versatile framework for representing a diverse array of 3D shapes, encompassing both watertight and non-watertight geometries. Traditional UDF learning methods typically require extensive training on large 3D shape datasets, which is costly and necessitates re-training for new datasets. This paper presents a novel neural framework, LoSF-UDF, for reconstructing surfaces from 3D point clouds by leveraging local shape functions to learn UDFs. We observe that 3D shapes manifest simple patterns in localized regions, prompting us to develop a training dataset of point cloud patches characterized by mathematical functions that represent a continuum from smooth surfaces to sharp edges and corners. Our approach learns features within a specific radius around each query point and utilizes an attention mechanism to focus on the crucial features for UDF estimation. Despite being highly lightweight, with only 653 KB of trainable parameters and a modest-sized training dataset with 0.5 GB storage, our method enables efficient and robust surface reconstruction from point clouds without requiring for shape-specific training. Furthermore, our method exhibits enhanced resilience to noise and outliers in point clouds compared to existing methods. We conduct comprehensive experiments and comparisons across various datasets, including synthetic and real-scanned point clouds, to validate our method's efficacy. Notably, our lightweight framework offers rapid and reliable initialization for other unsupervised iterative approaches, improving both the efficiency and accuracy of their reconstructions. Our project and code are available at https://jbhu67.github.io/LoSF-UDF.github.io.","authors":["Jiangbei Hu","Yanggeng Li","Fei Hou","Junhui Hou","Zhebin Zhang","Shengfa Wang","Na Lei","Ying He"],"url":"https://arxiv.org/abs/2407.01330"}
{"created":"2025-05-13","title":"Quantifying Privacy Risks of Public Statistics to Residents of Subsidized Housing","abstract":"As the U.S. Census Bureau implements its controversial new disclosure avoidance system, researchers and policymakers debate the necessity of new privacy protections for public statistics. With experiments on both public statistics and synthetic microdata, we explore a particular privacy concern: respondents in subsidized housing may deliberately not mention unauthorized children and other household members for fear of being discovered and evicted. By combining public statistics from the Decennial Census and the Department of Housing and Urban Development, we demonstrate a simple, inexpensive reconstruction attack that could identify subsidized households living in violation of occupancy guidelines in 2010. Experiments on synthetic data suggest that a random swapping mechanism similar to the Census Bureau's 2010 disclosure avoidance measures does not significantly reduce the precision of this attack, while a differentially private mechanism similar to the 2020 disclosure avoidance system does. Our results provide a valuable example for policymakers seeking trustworthy public statistics.","authors":["Ryan Steed","Diana Qing","Zhiwei Steven Wu"],"url":"https://arxiv.org/abs/2407.04776"}
{"created":"2025-05-13","title":"CodeV: Empowering LLMs with HDL Generation through Multi-Level Summarization","abstract":"The design flow of processors, particularly in hardware description languages (HDL) like Verilog and Chisel, is complex and costly. While recent advances in large language models (LLMs) have significantly improved coding tasks in software languages such as Python, their application in HDL generation remains limited due to the scarcity of high-quality HDL data. Traditional methods of adapting LLMs for hardware design rely on synthetic HDL datasets, which often suffer from low quality because even advanced LLMs like GPT perform poorly in the HDL domain. Moreover, these methods focus solely on chat tasks and the Verilog language, limiting their application scenarios.","authors":["Yang Zhao","Di Huang","Chongxiao Li","Pengwei Jin","Muxin Song","Yinan Xu","Ziyuan Nan","Mingju Gao","Tianyun Ma","Lei Qi","Yansong Pan","Zhenxing Zhang","Rui Zhang","Xishan Zhang","Zidong Du","Qi Guo","Xing Hu"],"url":"https://arxiv.org/abs/2407.10424"}
{"created":"2025-05-13","title":"HPPP: Halpern-type Preconditioned Proximal Point Algorithms and Applications to Image Restoration","abstract":"Recently, the degenerate preconditioned proximal point (PPP) method provides a unified and flexible framework for designing and analyzing operator-splitting algorithms such as Douglas-Rachford (DR). However, the degenerate PPP method exhibits weak convergence in the infinite-dimensional Hilbert space and lacks accelerated variants. To address these issues, we propose a Halpern-type PPP (HPPP) algorithm, which leverages the strong convergence and acceleration properties of Halpern's iteration method. Moreover, we propose a novel algorithm for image restoration by combining HPPP with denoiser priors such as Plug-and-Play (PnP) prior, which can be viewed as an accelerated PnP method. Finally, numerical experiments including several toy examples and image restoration validate the effectiveness of our proposed algorithms.","authors":["Shuchang Zhang","Hui Zhang","Hongxia Wang"],"url":"https://arxiv.org/abs/2407.13120"}
{"created":"2025-05-13","title":"ROLeR: Effective Reward Shaping in Offline Reinforcement Learning for Recommender Systems","abstract":"Offline reinforcement learning (RL) is an effective tool for real-world recommender systems with its capacity to model the dynamic interest of users and its interactive nature. Most existing offline RL recommender systems focus on model-based RL through learning a world model from offline data and building the recommendation policy by interacting with this model. Although these methods have made progress in the recommendation performance, the effectiveness of model-based offline RL methods is often constrained by the accuracy of the estimation of the reward model and the model uncertainties, primarily due to the extreme discrepancy between offline logged data and real-world data in user interactions with online platforms. To fill this gap, a more accurate reward model and uncertainty estimation are needed for the model-based RL methods. In this paper, a novel model-based Reward Shaping in Offline Reinforcement Learning for Recommender Systems, ROLeR, is proposed for reward and uncertainty estimation in recommendation systems. Specifically, a non-parametric reward shaping method is designed to refine the reward model. In addition, a flexible and more representative uncertainty penalty is designed to fit the needs of recommendation systems. Extensive experiments conducted on four benchmark datasets showcase that ROLeR achieves state-of-the-art performance compared with existing baselines. The source code can be downloaded at https://github.com/ArronDZhang/ROLeR.","authors":["Yi Zhang","Ruihong Qiu","Jiajun Liu","Sen Wang"],"url":"https://arxiv.org/abs/2407.13163"}
{"created":"2025-05-13","title":"DC is all you need: describing ReLU from a signal processing standpoint","abstract":"Non-linear activation functions are crucial in Convolutional Neural Networks. However, until now they have not been well described in the frequency domain. In this work, we study the spectral behavior of ReLU, a popular activation function. We use the ReLU's Taylor expansion to derive its frequency domain behavior. We demonstrate that ReLU introduces higher frequency oscillations in the signal and a constant DC component. Furthermore, we investigate the importance of this DC component, where we demonstrate that it helps the model extract meaningful features related to the input frequency content. We accompany our theoretical derivations with experiments and real-world examples. First, we numerically validate our frequency response model. Then we observe ReLU's spectral behavior on two example models and a real-world one. Finally, we experimentally investigate the role of the DC component introduced by ReLU in the CNN's representations. Our results indicate that the DC helps to converge to a weight configuration that is close to the initial random weights.","authors":["Christodoulos Kechris","Jonathan Dan","Jose Miranda","David Atienza"],"url":"https://arxiv.org/abs/2407.16556"}
{"created":"2025-05-13","title":"BCTR: Bidirectional Conditioning Transformer for Scene Graph Generation","abstract":"Scene Graph Generation (SGG) remains a challenging task due to its compositional property. Previous approaches improve prediction efficiency through end-to-end learning. However, these methods exhibit limited performance as they assume unidirectional conditioning between entities and predicates, which restricts effective information interaction. To address this limitation, we propose a novel bidirectional conditioning factorization in a semantic-aligned space for SGG, enabling efficient and generalizable interaction between entities and predicates. Specifically, we introduce an end-to-end scene graph generation model, the Bidirectional Conditioning Transformer (BCTR), to implement this factorization. BCTR consists of two key modules. First, the Bidirectional Conditioning Generator (BCG) performs multi-stage interactive feature augmentation between entities and predicates, enabling mutual enhancement between these predictions. Second, Random Feature Alignment (RFA) is present to regularize feature space by distilling multi-modal knowledge from pre-trained models. Within this regularized feature space, BCG is feasible to capture interaction patterns across diverse relationships during training, and the learned interaction patterns can generalize to unseen but semantically related relationships during inference. Extensive experiments on Visual Genome and Open Image V6 show that BCTR achieves state-of-the-art performance on both benchmarks.","authors":["Peng Hao","Weilong Wang","Xiaobing Wang","Yingying Jiang","Hanchao Jia","Shaowei Cui","Junhang Wei","Xiaoshuai Hao"],"url":"https://arxiv.org/abs/2407.18715"}
{"created":"2025-05-13","title":"Artificial Neural Networks on Graded Vector Spaces","abstract":"This paper presents a transformative framework for artificial neural networks over graded vector spaces, tailored to model hierarchical and structured data in fields like algebraic geometry and physics. By exploiting the algebraic properties of graded vector spaces, where features carry distinct weights, we extend classical neural networks with graded neurons, layers, and activation functions that preserve structural integrity. Grounded in group actions, representation theory, and graded algebra, our approach combines theoretical rigor with practical utility.","authors":["Tony Shaska"],"url":"https://arxiv.org/abs/2407.19031"}
{"created":"2025-05-13","title":"Half a Century of Distributed Byzantine Fault-Tolerant Consensus: Design Principles and Evolutionary Pathways","abstract":"The concept of distributed consensus originated in the 1970s and gained widespread attention following Leslie Lamport's influential publication on the Byzantine Generals Problem in the 1980s. Over the past five decades, distributed consensus has become an extensively researched field. Practical Byzantine Fault Tolerance (PBFT) has emerged as a prominent and widely adopted solution due to its conceptual clarity, effectiveness, and resilience to arbitrary failures. However, PBFT does not universally address all scenarios, highlighting the necessity of developing a comprehensive understanding of the history, evolution, and foundational principles of distributed consensus. This article systematically reviews the historical evolution and foundational principles of distributed consensus, examining pivotal advancements including fault-tolerant state machine replication (SMR), consensus protocols in partially synchronous and asynchronous networks, and recent innovations in Directed Acyclic Graph (DAG)-based consensus mechanisms. We further analyse the core design rationales, essential components, and underlying primitives across various distributed fault-tolerant protocols. The relationship between BFT consensus mechanisms and their applications in environments requiring robust resilience against adversarial faults is also explored. Finally, we discuss emerging research areas and challenges, such as consensus for wireless and blockchain scenarios, highlighting potential future developments. This comprehensive overview offers valuable insights to inform the design, optimisation, and implementation of distributed consensus systems across multiple application scenarios.","authors":["Huanyu Wu","Chentao Yue","Yixuan Fan","Yonghui Li","Lei Zhang"],"url":"https://arxiv.org/abs/2407.19863"}
{"created":"2025-05-13","title":"The Energy Cost of Artificial Intelligence Lifecycle in Communication Networks","abstract":"Artificial Intelligence (AI) is being incorporated in several optimization, scheduling, orchestration as well as in native communication network functions. While this paradigm shift results in increased energy consumption, quantifying the end-toend energy consumption of adding intelligence to such systems is particularly challenging. Conventional metrics focus on either communication, computation infrastructure, or model development. To address this, we propose a new metric, the Energy Cost of AI Lifecycle (eCAL) of one AI model in a system. eCAL captures the energy consumption throughout the development and deployment of an AI-model providing intelligence in a wireless communication network by analyzing the complexity of data collection and manipulation in individual components and deriving overall and per-bit energy consumption. We show that the better a model is and the more it is used, the more energy efficient an inference is. For a simple case study, eCAL for making 100 inferences is 2.73 times higher than for 1000 inferences. Additionally, we have developed a modular and extendable opensource simulation tool to enable researchers, practitioners, and engineers to calculate the end-to-end energy cost with various configurations and across various systems, ensuring adaptability to diverse use cases.","authors":["Shih-Kai Chou","Jernej Hribar","Vid Han\\v{z}el","Mihael Mohor\\v{c}i\\v{c}","Carolina Fortuna"],"url":"https://arxiv.org/abs/2408.00540"}
{"created":"2025-05-13","title":"Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models","abstract":"Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the \"hallucination problem\", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that \"9.11$>$9.9\". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.","authors":["Zikai Xie"],"url":"https://arxiv.org/abs/2408.05093"}
{"created":"2025-05-13","title":"MABR: Multilayer Adversarial Bias Removal Without Prior Bias Knowledge","abstract":"Models trained on real-world data often mirror and exacerbate existing social biases. Traditional methods for mitigating these biases typically require prior knowledge of the specific biases to be addressed, such as gender or racial biases, and the social groups associated with each instance. In this paper, we introduce a novel adversarial training strategy that operates independently of prior bias-type knowledge and protected attribute labels. Our approach proactively identifies biases during model training by utilizing auxiliary models, which are trained concurrently by predicting the performance of the main model without relying on task labels. Additionally, we implement these auxiliary models at various levels of the feature maps of the main model, enabling the detection of a broader and more nuanced range of bias features. Through experiments on racial and gender biases in sentiment and occupation classification tasks, our method effectively reduces social biases without the need for demographic annotations. Moreover, our approach not only matches but often surpasses the efficacy of methods that require detailed demographic insights, marking a significant advancement in bias mitigation techniques.","authors":["Maxwell J. Yin","Boyu Wang","Charles Ling"],"url":"https://arxiv.org/abs/2408.05497"}
{"created":"2025-05-13","title":"LUT Tensor Core: A Software-Hardware Co-Design for LUT-Based Low-Bit LLM Inference","abstract":"As large language model (LLM) inference continues to demand increasing computational resources, there is a rapidly growing trend toward using low-bit weights to reduce memory footprint and improve inference efficiency. However, low-bit LLMs introduce the need for mixed-precision general matrix multiplication (mpGEMM), which involves multiplying low-precision weights with higher-precision activations - a critical yet under-explored operation. Current hardware lacks native support for mpGEMM, leading to inefficient dequantization-based implementations.","authors":["Zhiwen Mo","Lei Wang","Jianyu Wei","Zhichen Zeng","Shijie Cao","Lingxiao Ma","Naifeng Jing","Ting Cao","Jilong Xue","Fan Yang","Mao Yang"],"url":"https://arxiv.org/abs/2408.06003"}
{"created":"2025-05-13","title":"Targeted Deep Learning System Boundary Testing","abstract":"Evaluating the behavioral boundaries of deep learning (DL) systems is crucial for understanding their reliability across diverse, unseen inputs. Existing solutions fall short as they rely on untargeted random, model- or latent-based perturbations, due to difficulties in generating controlled input variations. In this work, we introduce Mimicry, a novel black-box test generator for fine-grained, targeted exploration of DL system boundaries. Mimicry performs boundary testing by leveraging the probabilistic nature of DL outputs to identify promising directions for exploration. It uses style-based GANs to disentangle input representations into content and style components, enabling controlled feature mixing to approximate the decision boundary. We evaluated Mimicry's effectiveness in generating boundary inputs for five widely used DL image classification systems of increasing complexity, comparing it to two baseline approaches. Our results show that Mimicry consistently identifies inputs closer to the decision boundary. It generates semantically meaningful boundary test cases that reveal new functional (mis)behaviors, while the baselines produce mainly corrupted or invalid inputs. Thanks to its enhanced control over latent space manipulations, Mimicry remains effective as dataset complexity increases, maintaining competitive diversity and higher validity rates, confirmed by human assessors.","authors":["Oliver Wei{\\ss}l","Amr Abdellatif","Xingcheng Chen","Giorgi Merabishvili","Vincenzo Riccio","Severin Kacianka","Andrea Stocco"],"url":"https://arxiv.org/abs/2408.06258"}
{"created":"2025-05-13","title":"Dataset Discovery via Line Charts","abstract":"Line charts are a valuable tool for data analysis and exploration, distilling essential insights from a dataset. However, access to the underlying dataset behind a line chart is rarely readily available. In this paper, we explore a novel dataset discovery problem, dataset discovery via line charts, focusing on the use of line charts as queries to discover datasets within a large data repository that are capable of generating similar line charts. To solve this problem, we propose a novel approach called Fine-grained Cross-modal Relevance Learning Model (FCM), which aims to estimate the relevance between a line chart and a candidate dataset. To achieve this goal, FCM first employs a visual element extractor to extract informative visual elements, i.e., lines and y-ticks, from a line chart. Then, two novel segment-level encoders are adopted to learn representations for a line chart and a dataset, preserving fine-grained information, followed by a cross-modal matcher to match the learned representations in a fine-grained way. Furthermore, we extend FCM to support line chart queries generated based on data aggregation. Last, we propose a benchmark tailored for this problem since no such dataset exists. Extensive evaluation on the new benchmark verifies the effectiveness of our proposed method. Specifically, our proposed approach surpasses the best baseline by 30.1% and 41.0% in terms of prec@50 and ndcg@50, respectively.","authors":["Daomin Ji","Hui Luo","Zhifeng Bao","J. Shane Culpepper"],"url":"https://arxiv.org/abs/2408.09506"}
{"created":"2025-05-13","title":"Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer","abstract":"The use of Large Language Models (LLMs) for program code generation has gained substantial attention, but their biases and limitations with non-English prompts challenge global inclusivity. This paper investigates the complexities of multilingual prompt-based code generation. Our evaluations of LLMs, including CODELLAMA and CODEGEMMA, reveal significant disparities in code quality for non-English prompts; we also demonstrate the inadequacy of simple approaches like prompt translation, bootstrapped data augmentation, and fine-tuning. To address this, we propose a zero-shot cross-lingual approach using a neural projection technique, integrating a cross-lingual encoder like LASER to map multilingual embeddings from it into the LLM's token space. This method requires training only on English data and scales effectively to other languages. Results on a translated and quality-checked MBPP dataset show substantial improvements in code quality. This research promotes a more inclusive code generation landscape by empowering LLMs with multilingual capabilities to support the diverse linguistic spectrum in programming.","authors":["Mingda Li","Abhijit Mishra","Utkarsh Mujumdar"],"url":"https://arxiv.org/abs/2408.09701"}
{"created":"2025-05-13","title":"BihoT: A Large-Scale Dataset and Benchmark for Hyperspectral Camouflaged Object Tracking","abstract":"Hyperspectral object tracking (HOT) has exhibited potential in various applications, particularly in scenes where objects are camouflaged. Existing trackers can effectively retrieve objects via band regrouping because of the bias in existing HOT datasets, where most objects tend to have distinguishing visual appearances rather than spectral characteristics. This bias allows the tracker to directly use the visual features obtained from the false-color images generated by hyperspectral images without the need to extract spectral features. To tackle this bias, we find that the tracker should focus on the spectral information when object appearance is unreliable. Thus, we provide a new task called hyperspectral camouflaged object tracking (HCOT) and meticulously construct a large-scale HCOT dataset, termed BihoT, which consists of 41,912 hyperspectral images covering 49 video sequences. The dataset covers various artificial camouflage scenes where objects have similar appearances, diverse spectrums, and frequent occlusion, making it a very challenging dataset for HCOT. Besides, a simple but effective baseline model, named spectral prompt-based distractor-aware network (SPDAN), is proposed, comprising a spectral embedding network (SEN), a spectral prompt-based backbone network (SPBN), and a distractor-aware module (DAM). Specifically, the SEN extracts spectral-spatial features via 3-D and 2-D convolutions. Then, the SPBN fine-tunes powerful RGB trackers with spectral prompts and alleviates the insufficiency of training samples. Moreover, the DAM utilizes a novel statistic to capture the distractor caused by occlusion from objects and background. Extensive experiments demonstrate that our proposed SPDAN achieves state-of-the-art performance on the proposed BihoT and other HOT datasets.","authors":["Hanzheng Wang","Wei Li","Xiang-Gen Xia","Qian Du"],"url":"https://arxiv.org/abs/2408.12232"}
{"created":"2025-05-13","title":"Camouflaged Object Tracking: A Benchmark","abstract":"Visual tracking has seen remarkable advancements, largely driven by the availability of large-scale training datasets that have enabled the development of highly accurate and robust algorithms. While significant progress has been made in tracking general objects, research on more challenging scenarios, such as tracking camouflaged objects, remains limited. Camouflaged objects, which blend seamlessly with their surroundings or other objects, present unique challenges for detection and tracking in complex environments. This challenge is particularly critical in applications such as military, security, agriculture, and marine monitoring, where precise tracking of camouflaged objects is essential. To address this gap, we introduce the Camouflaged Object Tracking Dataset (COTD), a specialized benchmark designed specifically for evaluating camouflaged object tracking methods. The COTD dataset comprises 200 sequences and approximately 80,000 frames, each annotated with detailed bounding boxes. Our evaluation of 20 existing tracking algorithms reveals significant deficiencies in their performance with camouflaged objects. To address these issues, we propose a novel tracking framework, HiPTrack-MLS, which demonstrates promising results in improving tracking performance for camouflaged objects. COTD and code are avialable at https://github.com/openat25/HIPTrack-MLS.","authors":["Xiaoyu Guo","Pengzhi Zhong","Hao Zhang","Defeng Huang","Huikai Shao","Qijun Zhao","Shuiwang Li"],"url":"https://arxiv.org/abs/2408.13877"}
{"created":"2025-05-13","title":"CHARTOM: A Visual Theory-of-Mind Benchmark for Multimodal Large Language Models","abstract":"We introduce CHARTOM, a visual theory-of-mind benchmark for multimodal large language models. CHARTOM consists of specially designed data visualizing charts. Given a chart, a language model needs to not only correctly comprehend the chart (the FACT question) but also judge if the chart will be misleading to a human reader (the MIND question). Both questions have significant societal benefits. We detail the construction of the CHARTOM benchmark including its calibration on human performance. We benchmark leading LLMs as of late 2024 - including GPT, Claude, Gemini, Qwen, Llama, and Llava - on the CHARTOM dataset and found that our benchmark was challenging to all of them, suggesting room for future large language models to improve.","authors":["Shubham Bharti","Shiyun Cheng","Jihyun Rho","Jianrui Zhang","Mu Cai","Yong Jae Lee","Martina Rau","Xiaojin Zhu"],"url":"https://arxiv.org/abs/2408.14419"}
{"created":"2025-05-13","title":"Poly2Vec: Polymorphic Fourier-Based Encoding of Geospatial Objects for GeoAI Applications","abstract":"Encoding geospatial objects is fundamental for geospatial artificial intelligence (GeoAI) applications, which leverage machine learning (ML) models to analyze spatial information. Common approaches transform each object into known formats, like image and text, for compatibility with ML models. However, this process often discards crucial spatial information, such as the object's position relative to the entire space, reducing downstream task effectiveness. Alternative encoding methods that preserve some spatial properties are often devised for specific data objects (e.g., point encoders), making them unsuitable for tasks that involve different data types (i.e., points, polylines, and polygons). To this end, we propose Poly2Vec, a polymorphic Fourier-based encoding approach that unifies the representation of geospatial objects, while preserving the essential spatial properties. Poly2Vec incorporates a learned fusion module that adaptively integrates the magnitude and phase of the Fourier transform for different tasks and geometries. We evaluate Poly2Vec on five diverse tasks, organized into two categories. The first empirically demonstrates that Poly2Vec consistently outperforms object-specific baselines in preserving three key spatial relationships: topology, direction, and distance. The second shows that integrating Poly2Vec into a state-of-the-art GeoAI workflow improves the performance in two popular tasks: population prediction and land use inference.","authors":["Maria Despoina Siampou","Jialiang Li","John Krumm","Cyrus Shahabi","Hua Lu"],"url":"https://arxiv.org/abs/2408.14806"}
{"created":"2025-05-13","title":"A novel fusion of Sentinel-1 and Sentinel-2 with climate data for crop phenology estimation using Machine Learning","abstract":"Crop phenology describes the physiological development stages of crops from planting to harvest which is valuable information for decision makers to plan and adapt agricultural management strategies. In the era of big Earth observation data ubiquity, attempts have been made to accurately detect crop phenology using Remote Sensing (RS) and high resolution weather data. However, most studies have focused on large scale predictions of phenology or developed methods which are not adequate to help crop modeler communities on leveraging Sentinel-1 and Sentinal-2 data and fusing them with high resolution climate data, using a novel framework. For this, we trained a Machine Learning (ML) LightGBM model to predict 13 phenological stages for eight major crops across Germany at 20 m scale. Observed phonologies were taken from German national phenology network (German Meteorological Service; DWD) between 2017 and 2021. We proposed a thorough feature selection analysis to find the best combination of RS and climate data to detect phenological stages. At national scale, predicted phenology resulted in a reasonable precision of R2 > 0.43 and a low Mean Absolute Error of 6 days, averaged over all phenological stages and crops. The spatio-temporal analysis of the model predictions demonstrates its transferability across different spatial and temporal context of Germany. The results indicated that combining radar sensors with climate data yields a very promising performance for a multitude of practical applications. Moreover, these improvements are expected to be useful to generate highly valuable input for crop model calibrations and evaluations, facilitate informed agricultural decisions, and contribute to sustainable food production to address the increasing global food demand.","authors":["Shahab Aldin Shojaeezadeh","Abdelrazek Elnashar","Tobias Karl David Weber"],"url":"https://arxiv.org/abs/2409.00020"}
{"created":"2025-05-13","title":"IGEV++: Iterative Multi-range Geometry Encoding Volumes for Stereo Matching","abstract":"Stereo matching is a core component in many computer vision and robotics systems. Despite significant advances over the last decade, handling matching ambiguities in ill-posed regions and large disparities remains an open challenge. In this paper, we propose a new deep network architecture, called IGEV++, for stereo matching. The proposed IGEV++ constructs Multi-range Geometry Encoding Volumes (MGEV), which encode coarse-grained geometry information for ill-posed regions and large disparities, while preserving fine-grained geometry information for details and small disparities. To construct MGEV, we introduce an adaptive patch matching module that efficiently and effectively computes matching costs for large disparity ranges and/or ill-posed regions. We further propose a selective geometry feature fusion module to adaptively fuse multi-range and multi-granularity geometry features in MGEV. Then, we input the fused geometry features into ConvGRUs to iteratively update the disparity map. MGEV allows to efficiently handle large disparities and ill-posed regions, such as occlusions and textureless regions, and enjoys rapid convergence during iterations. Our IGEV++ achieves the best performance on the Scene Flow test set across all disparity ranges, up to 768px. Our IGEV++ also achieves state-of-the-art accuracy on the Middlebury, ETH3D, KITTI 2012, and 2015 benchmarks. Specifically, IGEV++ achieves a 3.23\\% 2-pixel outlier rate (Bad 2.0) on the large disparity benchmark, Middlebury, representing error reductions of 31.9\\% and 54.8\\% compared to RAFT-Stereo and GMStereo, respectively. We also present a real-time version of IGEV++ that achieves the best performance among all published real-time methods on the KITTI benchmarks. The code is publicly available at https://github.com/gangweix/IGEV and https://github.com/gangweix/IGEV-plusplus.","authors":["Gangwei Xu","Xianqi Wang","Zhaoxing Zhang","Junda Cheng","Chunyuan Liao","Xin Yang"],"url":"https://arxiv.org/abs/2409.00638"}
{"created":"2025-05-13","title":"Efficient LLM Context Distillation","abstract":"Large Language Models (LLMs) demonstrate proficiency across diverse tasks but often require targeted adaptations for specific applications. Various methods have been proposed to facilitate this adaptation, including fewshot fine-tuning, in-context learning, and context distillation. This paper specifically investigates context distillation a method that extends the utility of task-specific examples by internalizing them, thus augmenting the example set accessible for model inference. We conduct a comparative analysis of context distillation with in-context learning (ICL) and few-shot fine-tuning (FT), aiming to ascertain the efficacy of context distillation in adapting models using minimal in-context examples. Employing matched datasets from Mobach, our experiments leverage OPT models of various sizes. The results indicate that context distillation effectively adapts models, with student models attaining comparable in-domain and out-of-domain accuracies to in-context learning. Although context distillation surpasses ICL in out-of-domain generalization, it does not achieve the performance levels of FT. However, the reduced dataset size and computational demands position context distillation as a viable alternative, especially for smaller datasets. Overall, this study presents context distillation as an efficient and potent method for customizing LLMs to specific tasks.","authors":["Rajesh Upadhayayaya","Manish Raj Osti","Zachary Smith","Chritopher Kottmyer"],"url":"https://arxiv.org/abs/2409.01930"}
{"created":"2025-05-13","title":"A Digital signature scheme based on Module-LWE and Module-SIS","abstract":"In this paper, we present an improved version of the digital signature scheme proposed by Sharafi and Daghigh based on Module-LWE and Module-SIS problems. Our proposed signature scheme has a notably higher security level and smaller decoding failure probability, than the ones in the Sharaf-Daghigh scheme, at the expense of enlarging the module of the underlying basic ring.","authors":["Huda Naeem Hleeb Al-Jabbari","Abbas Maarefparvar"],"url":"https://arxiv.org/abs/2409.02222"}
{"created":"2025-05-13","title":"Diffusion Models Learn Low-Dimensional Distributions via Subspace Clustering","abstract":"Recent empirical studies have demonstrated that diffusion models can effectively learn the image distribution and generate new samples. Remarkably, these models can achieve this even with a small number of training samples despite a large image dimension, circumventing the curse of dimensionality. In this work, we provide theoretical insights into this phenomenon by leveraging key empirical observations: (i) the low intrinsic dimensionality of image data, (ii) a union of manifold structure of image data, and (iii) the low-rank property of the denoising autoencoder in trained diffusion models. These observations motivate us to assume the underlying data distribution of image data as a mixture of low-rank Gaussians and to parameterize the denoising autoencoder as a low-rank model according to the score function of the assumed distribution. With these setups, we rigorously show that optimizing the training loss of diffusion models is equivalent to solving the canonical subspace clustering problem over the training samples. Based on this equivalence, we further show that the minimal number of samples required to learn the underlying distribution scales linearly with the intrinsic dimensions under the above data and model assumptions. This insight sheds light on why diffusion models can break the curse of dimensionality and exhibit the phase transition in learning distributions. Moreover, we empirically establish a correspondence between the subspaces and the semantic representations of image data, facilitating image editing. We validate these results with corroborated experimental results on both simulated distributions and image datasets.","authors":["Peng Wang","Huijie Zhang","Zekai Zhang","Siyi Chen","Yi Ma","Qing Qu"],"url":"https://arxiv.org/abs/2409.02426"}
{"created":"2025-05-13","title":"Semantic Communication for Efficient Point Cloud Transmission","abstract":"As three-dimensional acquisition technologies like LiDAR cameras advance, the need for efficient transmission of 3D point clouds is becoming increasingly important. In this paper, we present a novel semantic communication (SemCom) approach for efficient 3D point cloud transmission. Different from existing methods that rely on downsampling and feature extraction for compression, our approach utilizes a parallel structure to separately extract both global and local information from point clouds. This system is composed of five key components: local semantic encoder, global semantic encoder, channel encoder, channel decoder, and semantic decoder. Our numerical results indicate that this approach surpasses both the traditional Octree compression methodology and alternative deep learning-based strategies in terms of reconstruction quality. Moreover, our system is capable of achieving high-quality point cloud reconstruction under adverse channel conditions, specifically maintaining a reconstruction quality of over 37dB even with severe channel noise.","authors":["Shangzhuo Xie","Qianqian Yang","Yuyi Sun","Tianxiao Han","Zhaohui Yang","Zhiguo Shi"],"url":"https://arxiv.org/abs/2409.03319"}
{"created":"2025-05-13","title":"The Future of Software Testing: AI-Powered Test Case Generation and Validation","abstract":"Software testing is a crucial phase in the software development lifecycle (SDLC), ensuring that products meet necessary functional, performance, and quality benchmarks before release. Despite advancements in automation, traditional methods of generating and validating test cases still face significant challenges, including prolonged timelines, human error, incomplete test coverage, and high costs of manual intervention. These limitations often lead to delayed product launches and undetected defects that compromise software quality and user satisfaction. The integration of artificial intelligence (AI) into software testing presents a promising solution to these persistent challenges. AI-driven testing methods automate the creation of comprehensive test cases, dynamically adapt to changes, and leverage machine learning to identify high-risk areas in the codebase. This approach enhances regression testing efficiency while expanding overall test coverage. Furthermore, AI-powered tools enable continuous testing and self-healing test cases, significantly reducing manual oversight and accelerating feedback loops, ultimately leading to faster and more reliable software releases. This paper explores the transformative potential of AI in improving test case generation and validation, focusing on its ability to enhance efficiency, accuracy, and scalability in testing processes. It also addresses key challenges associated with adapting AI for testing, including the need for high quality training data, ensuring model transparency, and maintaining a balance between automation and human oversight. Through case studies and examples of real-world applications, this paper illustrates how AI can significantly enhance testing efficiency across both legacy and modern software systems.","authors":["Mohammad Baqar","Rajat Khanda"],"url":"https://arxiv.org/abs/2409.05808"}
{"created":"2025-05-13","title":"Enhanced Generative Data Augmentation for Semantic Segmentation via Stronger Guidance","abstract":"Data augmentation is crucial for pixel-wise annotation tasks like semantic segmentation, where labeling requires significant effort and intensive labor. Traditional methods, involving simple transformations such as rotations and flips, create new images but often lack diversity along key semantic dimensions and fail to alter high-level semantic properties. To address this issue, generative models have emerged as an effective solution for augmenting data by generating synthetic images. Controllable Generative models offer data augmentation methods for semantic segmentation tasks by using prompts and visual references from the original image. However, these models face challenges in generating synthetic images that accurately reflect the content and structure of the original image due to difficulties in creating effective prompts and visual references. In this work, we introduce an effective data augmentation pipeline for semantic segmentation using Controllable Diffusion model. Our proposed method includes efficient prompt generation using \\textit{Class-Prompt Appending} and \\textit{Visual Prior Blending} to enhance attention to labeled classes in real images, allowing the pipeline to generate a precise number of augmented images while preserving the structure of segmentation-labeled classes. In addition, we implement a \\textit{class balancing algorithm} to ensure a balanced training dataset when merging the synthetic and original images. Evaluation on PASCAL VOC datasets, our pipeline demonstrates its effectiveness in generating high-quality synthetic images for semantic segmentation. Our code is available at \\href{https://github.com/chequanghuy/Enhanced-Generative-Data-Augmentation-for-Semantic-Segmentation-via-Stronger-Guidance}{this https URL}.","authors":["Quang-Huy Che","Duc-Tri Le","Bich-Nga Pham","Duc-Khai Lam","Vinh-Tiep Nguyen"],"url":"https://arxiv.org/abs/2409.06002"}
{"created":"2025-05-13","title":"Neural Algorithmic Reasoning with Multiple Correct Solutions","abstract":"Neural Algorithmic Reasoning (NAR) extends classical algorithms to higher dimensional data. However, canonical implementations of NAR train neural networks to return only a single solution, even when there are multiple correct solutions to a problem, such as single-source shortest paths. For some applications, it is desirable to recover more than one correct solution. To that end, we give the first method for NAR with multiple solutions. We demonstrate our method on two classical algorithms: Bellman-Ford (BF) and Depth-First Search (DFS), favouring deeper insight into two algorithms over a broader survey of algorithms. This method involves generating appropriate training data as well as sampling and validating solutions from model output. Each step of our method, which can serve as a framework for neural algorithmic reasoning beyond the tasks presented in this paper, might be of independent interest to the field and our results represent the first attempt at this task in the NAR literature.","authors":["Zeno Kujawa","John Poole","Dobrik Georgiev","Danilo Numeroso","Henry Fleischmann","Pietro Li\\`o"],"url":"https://arxiv.org/abs/2409.06953"}
{"created":"2025-05-13","title":"Bridging Discrete and Continuous: A Multimodal Strategy for Complex Emotion Detection","abstract":"In the domain of human-computer interaction, accurately recognizing and interpreting human emotions is crucial yet challenging due to the complexity and subtlety of emotional expressions. This study explores the potential for detecting a rich and flexible range of emotions through a multimodal approach which integrates facial expressions, voice tones, and transcript from video clips. We propose a novel framework that maps variety of emotions in a three-dimensional Valence-Arousal-Dominance (VAD) space, which could reflect the fluctuations and positivity/negativity of emotions to enable a more variety and comprehensive representation of emotional states. We employed K-means clustering to transit emotions from traditional discrete categorization to a continuous labeling system and built a classifier for emotion recognition upon this system. The effectiveness of the proposed model is evaluated using the MER2024 dataset, which contains culturally consistent video clips from Chinese movies and TV series, annotated with both discrete and open-vocabulary emotion labels. Our experiment successfully achieved the transformation between discrete and continuous models, and the proposed model generated a more diverse and comprehensive set of emotion vocabulary while maintaining strong accuracy.","authors":["Jiehui Jia","Huan Zhang","Jinhua Liang"],"url":"https://arxiv.org/abs/2409.07901"}
{"created":"2025-05-13","title":"BNEM: A Boltzmann Sampler Based on Bootstrapped Noised Energy Matching","abstract":"Developing an efficient sampler capable of generating independent and identically distributed (IID) samples from a Boltzmann distribution is a crucial challenge in scientific research, e.g. molecular dynamics. In this work, we intend to learn neural samplers given energy functions instead of data sampled from the Boltzmann distribution. By learning the energies of the noised data, we propose a diffusion-based sampler, Noised Energy Matching, which theoretically has lower variance and more complexity compared to related works. Furthermore, a novel bootstrapping technique is applied to NEM to balance between bias and variance. We evaluate NEM and BNEM on a 2-dimensional 40 Gaussian Mixture Model (GMM) and a 4-particle double-well potential (DW-4). The experimental results demonstrate that BNEM can achieve state-of-the-art performance while being more robust.","authors":["RuiKang OuYang","Bo Qiang","Jos\\'e Miguel Hern\\'andez-Lobato"],"url":"https://arxiv.org/abs/2409.09787"}
{"created":"2025-05-13","title":"On Synthetic Texture Datasets: Challenges, Creation, and Curation","abstract":"The influence of textures on machine learning models has been an ongoing investigation, specifically in texture bias/learning, interpretability, and robustness. However, due to the lack of large and diverse texture data available, the findings in these works have been limited, as more comprehensive evaluations have not been feasible. Image generative models are able to provide data creation at scale, but utilizing these models for texture synthesis has been unexplored and poses additional challenges both in creating accurate texture images and validating those images. In this work, we introduce an extensible methodology and corresponding new dataset for generating high-quality, diverse texture images capable of supporting a broad set of texture-based tasks. Our pipeline consists of: (1) developing prompts from a range of descriptors to serve as input to text-to-image models, (2) adopting and adapting Stable Diffusion pipelines to generate and filter the corresponding images, and (3) further filtering down to the highest quality images. Through this, we create the Prompted Textures Dataset (PTD), a dataset of 246,285 texture images that span 56 textures. During the process of generating images, we find that NSFW safety filters in image generation pipelines are highly sensitive to texture (and flag up to 60\\% of our texture images), uncovering a potential bias in these models and presenting unique challenges when working with texture data. Through both standard metrics and a human evaluation, we find that our dataset is high quality and diverse. Our dataset is available for download at https://zenodo.org/records/15359142.","authors":["Blaine Hoak","Patrick McDaniel"],"url":"https://arxiv.org/abs/2409.10297"}
{"created":"2025-05-13","title":"MoDex: Planning High-Dimensional Dexterous Control via Learning Neural Internal Models","abstract":"Controlling hands in high-dimensional action space has been a longstanding challenge, yet humans naturally perform dexterous tasks with ease. In this paper, we draw inspiration from the concept of internal model exhibited in human behavior and reconsider dexterous hands as learnable systems. Specifically, we introduce MoDex, a framework that includes a couple of neural networks (NNs) capturing the dynamical characteristics of hands and a bidirectional planning approach, which demonstrates both training and planning efficiency. To show the versatility of MoDex, we further integrate it with an external model to manipulate in-hand objects and a large language model (LLM) to generate various gestures in both simulation and real world. Extensive experiments on different dexterous hands address the data efficiency in learning a new task and the transferability between different tasks.","authors":["Tong Wu","Shoujie Li","Chuqiao Lyu","Kit-Wa Sou","Wang-Sing Chan","Wenbo Ding"],"url":"https://arxiv.org/abs/2409.10983"}
{"created":"2025-05-13","title":"Exploring the Trade-Offs: Quantization Methods, Task Difficulty, and Model Size in Large Language Models From Edge to Giant","abstract":"Quantization has gained attention as a promising solution for the cost-effective deployment of large and small language models. However, most prior work has been limited to perplexity or basic knowledge tasks and lacks a comprehensive evaluation of recent models like Llama-3.3. In this paper, we conduct a comprehensive evaluation of instruction-tuned models spanning 1B to 405B parameters, applying four quantization methods across 13 datasets. Our findings reveal that (1) quantized models generally surpass smaller FP16 baselines, yet they often struggle with instruction-following and hallucination detection; (2) FP8 consistently emerges as the most robust option across tasks, and AWQ tends to outperform GPTQ in weight-only quantization; (3) smaller models can suffer severe accuracy drops at 4-bit quantization, while 70B-scale models maintain stable performance; (4) notably, \\textit{hard} tasks do not always experience the largest accuracy losses, indicating that quantization magnifies a model's inherent weaknesses rather than simply correlating with task difficulty; and (5) an LLM-based judge (MT-Bench) highlights significant performance declines in Coding and STEM tasks, though it occasionally reports improvements in reasoning.","authors":["Jemin Lee","Sihyeong Park","Jinse Kwon","Jihun Oh","Yongin Kwon"],"url":"https://arxiv.org/abs/2409.11055"}
{"created":"2025-05-13","title":"VLATest: Testing and Evaluating Vision-Language-Action Models for Robotic Manipulation","abstract":"The rapid advancement of generative AI and multi-modal foundation models has shown significant potential in advancing robotic manipulation. Vision-language-action (VLA) models, in particular, have emerged as a promising approach for visuomotor control by leveraging large-scale vision-language data and robot demonstrations. However, current VLA models are typically evaluated using a limited set of hand-crafted scenes, leaving their general performance and robustness in diverse scenarios largely unexplored. To address this gap, we present VLATest, a fuzzing framework designed to generate robotic manipulation scenes for testing VLA models. Based on VLATest, we conducted an empirical study to assess the performance of seven representative VLA models. Our study results revealed that current VLA models lack the robustness necessary for practical deployment. Additionally, we investigated the impact of various factors, including the number of confounding objects, lighting conditions, camera poses, unseen objects, and task instruction mutations, on the VLA model's performance. Our findings highlight the limitations of existing VLA models, emphasizing the need for further research to develop reliable and trustworthy VLA applications.","authors":["Zhijie Wang","Zhehua Zhou","Jiayang Song","Yuheng Huang","Zhan Shu","Lei Ma"],"url":"https://arxiv.org/abs/2409.12894"}
{"created":"2025-05-13","title":"Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy","abstract":"This study evaluates the ability of large language models (LLMs) to map biomedical ontology terms to their corresponding ontology IDs across the Human Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies. Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate for their prevalence in the biomedical literature, we examined the relationship between ontology ID prevalence and mapping accuracy. Results indicate that ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers. Higher prevalence of ontology IDs in the biomedical literature correlated with higher mapping accuracy. Predictive models based on receiver operating characteristic (ROC) curves confirmed this relationship.","authors":["Thanh Son Do","Daniel B. Hier","Tayo Obafemi-Ajayi"],"url":"https://arxiv.org/abs/2409.13746"}
{"created":"2025-05-13","title":"Looped Transformers for Length Generalization","abstract":"Recent work has shown that Transformers trained from scratch can successfully solve various arithmetic and algorithmic tasks, such as adding numbers and computing parity. While these Transformers generalize well on unseen inputs of the same length, they struggle with length generalization, i.e., handling inputs of unseen lengths. In this work, we demonstrate that looped Transformers with an adaptive number of steps significantly improve length generalization. We focus on tasks with a known iterative solution, involving multiple iterations of a RASP-L operation - a length-generalizable operation that can be expressed by a finite-sized Transformer. We train looped Transformers using our proposed learning algorithm and observe that they learn highly length-generalizable solutions for various tasks.","authors":["Ying Fan","Yilun Du","Kannan Ramchandran","Kangwook Lee"],"url":"https://arxiv.org/abs/2409.15647"}
{"created":"2025-05-13","title":"A discrete trace theory for non-conforming polytopal hybrid discretisation methods","abstract":"In this work we develop a discrete trace theory that spans non-conforming hybrid discretization methods and holds on polytopal meshes. A notion of a discrete trace seminorm is defined, and trace and lifting results with respect to a discrete $H^1$-seminorm on the hybrid fully discrete space are proven. Building on these results we also prove a truncation estimate for piecewise polynomials in the discrete trace seminorm. Finally, we conduct two numerical tests in which we compute the proposed discrete operators and investigate their spectrum to verify the theoretical analysis. The development of this theory is motivated by the design and analysis of preconditioners for hybrid methods, e.g., of substructuring domain decomposition type.","authors":["Santiago Badia","Jerome Droniou","Jai Tushar"],"url":"https://arxiv.org/abs/2409.15863"}
{"created":"2025-05-13","title":"An Adaptive Re-evaluation Method for Evolution Strategy under Additive Noise","abstract":"The Covariance Matrix Adaptation Evolutionary Strategy (CMA-ES) is one of the most advanced algorithms in numerical black-box optimization. For noisy objective functions, several approaches were proposed to mitigate the noise, e.g., re-evaluations of the same solution or adapting the population size.","authors":["Catalin-Viorel Dinu","Yash J. Patel","Xavier Bonet-Monroig","Hao Wang"],"url":"https://arxiv.org/abs/2409.16757"}
{"created":"2025-05-13","title":"When SAM2 Meets Video Camouflaged Object Segmentation: A Comprehensive Evaluation and Adaptation","abstract":"This study investigates the application and performance of the Segment Anything Model 2 (SAM2) in the challenging task of video camouflaged object segmentation (VCOS). VCOS involves detecting objects that blend seamlessly in the surroundings for videos, due to similar colors and textures, poor light conditions, etc. Compared to the objects in normal scenes, camouflaged objects are much more difficult to detect. SAM2, a video foundation model, has shown potential in various tasks. But its effectiveness in dynamic camouflaged scenarios remains under-explored. This study presents a comprehensive study on SAM2's ability in VCOS. First, we assess SAM2's performance on camouflaged video datasets using different models and prompts (click, box, and mask). Second, we explore the integration of SAM2 with existing multimodal large language models (MLLMs) and VCOS methods. Third, we specifically adapt SAM2 by fine-tuning it on the video camouflaged dataset. Our comprehensive experiments demonstrate that SAM2 has excellent zero-shot ability of detecting camouflaged objects in videos. We also show that this ability could be further improved by specifically adjusting SAM2's parameters for VCOS. The code is available at https://github.com/zhoustan/SAM2-VCOS","authors":["Yuli Zhou","Guolei Sun","Yawei Li","Guo-Sen Xie","Luca Benini","Ender Konukoglu"],"url":"https://arxiv.org/abs/2409.18653"}
{"created":"2025-05-13","title":"Differentially Private Bilevel Optimization","abstract":"We present differentially private (DP) algorithms for bilevel optimization, a problem class that received significant attention lately in various machine learning applications. These are the first algorithms for such problems under standard DP constraints, and are also the first to avoid Hessian computations which are prohibitive in large-scale settings. Under the well-studied setting in which the upper-level is not necessarily convex and the lower-level problem is strongly-convex, our proposed gradient-based $(\\epsilon,\\delta)$-DP algorithm returns a point with hypergradient norm at most $\\widetilde{\\mathcal{O}}\\left((\\sqrt{d_\\mathrm{up}}/\\epsilon n)^{1/2}+(\\sqrt{d_\\mathrm{low}}/\\epsilon n)^{1/3}\\right)$ where $n$ is the dataset size, and $d_\\mathrm{up}/d_\\mathrm{low}$ are the upper/lower level dimensions. Our analysis covers constrained and unconstrained problems alike, accounts for mini-batch gradients, and applies to both empirical and population losses. As an application, we specialize our analysis to derive a simple private rule for tuning a regularization hyperparameter.","authors":["Guy Kornowski"],"url":"https://arxiv.org/abs/2409.19800"}
{"created":"2025-05-13","title":"GTransPDM: A Graph-embedded Transformer with Positional Decoupling for Pedestrian Crossing Intention Prediction","abstract":"Understanding and predicting pedestrian crossing behavioral intention is crucial for the driving safety of autonomous vehicles. Nonetheless, challenges emerge when using promising images or environmental context masks to extract various factors for time-series network modeling, causing pre-processing errors or a loss of efficiency. Typically, pedestrian positions captured by onboard cameras are often distorted and do not accurately reflect their actual movements. To address these issues, GTransPDM -- a Graph-embedded Transformer with a Position Decoupling Module -- was developed for pedestrian crossing intention prediction by leveraging multi-modal features. First, a positional decoupling module was proposed to decompose pedestrian lateral motion and encode depth cues in the image view. Then, a graph-embedded Transformer was designed to capture the spatio-temporal dynamics of human pose skeletons, integrating essential factors such as position, skeleton, and ego-vehicle motion. Experimental results indicate that the proposed method achieves 92% accuracy on the PIE dataset and 87% accuracy on the JAAD dataset, with a processing speed of 0.05ms. It outperforms the state-of-the-art in comparison.","authors":["Chen Xie","Ciyun Lin","Xiaoyu Zheng","Bowen Gong","Antonio M. L\\'opez"],"url":"https://arxiv.org/abs/2409.20223"}
{"created":"2025-05-13","title":"Characterizing and Efficiently Accelerating Multimodal Generation Model Inference","abstract":"Generative artificial intelligence (AI) technology is revolutionizing the computing industry. Not only its applications have broadened to various sectors but also poses new system design and optimization opportunities. The technology is capable of understanding and responding in multiple modalities. However, the advanced capability currently comes with significant system resource demands. To sustainably scale generative AI capabilities to billions of users in the world, inference must be fast and efficient. This paper pinpoints key system design and optimization opportunities by characterizing a family of emerging multi-modal generation models on real systems. Auto-regressive token generation is a critical latency performance bottleneck, typically dominated by GPU idle time. In addition to memory-intensive attention across the generative AI models, linear operations constitute significant inference latency due to the feed forward networks in Transformer-based models. We demonstrate that state-of-the-art optimization levers, spanning from applications to system software and hardware, set a 3.88x better baseline.","authors":["Yejin Lee","Anna Sun","Basil Hosmer","Bilge Acun","Can Balioglu","Changhan Wang","Charles David Hernandez","Christian Puhrsch","Daniel Haziza","Driss Guessous","Francisco Massa","Jacob Kahn","Jeffrey Wan","Jeremy Reizenstein","Jiaqi Zhai","Joe Isaacson","Joel Schlosser","Juan Pino","Kaushik Ram Sadagopan","Leonid Shamis","Linjian Ma","Min-Jae Hwang","Mingda Chen","Mostafa Elhoushi","Pedro Rodriguez","Ram Pasunuru","Scott Yih","Sravya Popuri","Xing Liu","Carole-Jean Wu"],"url":"https://arxiv.org/abs/2410.00215"}
{"created":"2025-05-13","title":"Exploring Gen-AI applications in building research and industry: A review","abstract":"This paper investigates the transformative potential of Generative AI (Gen-AI) technologies, particularly large language models, within the building industry. By leveraging these advanced AI tools, the study explores their application across key areas such as automated compliance checking and building design assistance. The research highlights how Gen-AI can automate labor-intensive processes, significantly improving efficiency and reducing costs in building practices. The paper first discusses the two widely applied fundamental models-Transformer and Diffusion model-and summarizes current pathways for accessing Gen-AI models and the most common techniques for customizing them. It then explores applications for text generation, such as compliance checking, control support, data mining, and building simulation input file editing. Additionally, it examines image generation, including direct generation through diffusion models and indirect generation through language model-supported template creation based on existing Computer-Aided Design or other design tools with rendering. The paper concludes with a comprehensive analysis of the current capabilities of Gen-AI in the building industry, outlining future directions for research and development, with the goal of paving the way for smarter, more effective, and responsive design, construction, and operational practices.","authors":["Hanlong Wan","Jian Zhang","Yan Chen","Weili Xu","Fan Feng"],"url":"https://arxiv.org/abs/2410.01098"}
{"created":"2025-05-13","title":"Endless Jailbreaks with Bijection Learning","abstract":"Despite extensive safety measures, LLMs are vulnerable to adversarial inputs, or jailbreaks, which can elicit unsafe behaviors. In this work, we introduce bijection learning, a powerful attack algorithm which automatically fuzzes LLMs for safety vulnerabilities using randomly-generated encodings whose complexity can be tightly controlled. We leverage in-context learning to teach models bijective encodings, pass encoded queries to the model to bypass built-in safety mechanisms, and finally decode responses back into English. Our attack is extremely effective on a wide range of frontier language models. Moreover, by controlling complexity parameters such as number of key-value mappings in the encodings, we find a close relationship between the capability level of the attacked LLM and the average complexity of the most effective bijection attacks. Our work highlights that new vulnerabilities in frontier models can emerge with scale: more capable models are more severely jailbroken by bijection attacks.","authors":["Brian R. Y. Huang","Maximilian Li","Leonard Tang"],"url":"https://arxiv.org/abs/2410.01294"}
{"created":"2025-05-13","title":"ReFeree: Radar-Based Lightweight and Robust Localization using Feature and Free space","abstract":"Place recognition plays an important role in achieving robust long-term autonomy. Real-world robots face a wide range of weather conditions (e.g. overcast, heavy rain, and snowing) and most sensors (i.e. camera, LiDAR) essentially functioning within or near-visible electromagnetic waves are sensitive to adverse weather conditions, making reliable localization difficult. In contrast, radar is gaining traction due to long electromagnetic waves, which are less affected by environmental changes and weather independence. In this work, we propose a radar-based lightweight and robust place recognition. We achieve rotational invariance and lightweight by selecting a one-dimensional ring-shaped description and robustness by mitigating the impact of false detection utilizing opposite noise characteristics between free space and feature. In addition, the initial heading can be estimated, which can assist in building a SLAM pipeline that combines odometry and registration, which takes into account onboard computing. The proposed method was tested for rigorous validation across various scenarios (i.e. single session, multi-session, and different weather conditions). In particular, we validate our descriptor achieving reliable place recognition performance through the results of extreme environments that lacked structural information such as an OORD dataset.","authors":["Hogyun Kim","Byunghee Choi","Euncheol Choi","Younggun Cho"],"url":"https://arxiv.org/abs/2410.01325"}
{"created":"2025-05-13","title":"Moral Alignment for LLM Agents","abstract":"Decision-making agents based on pre-trained Large Language Models (LLMs) are increasingly being deployed across various domains of human activity. While their applications are currently rather specialized, several research efforts are underway to develop more generalist agents. As LLM-based systems become more agentic, their influence on human activity will grow and their transparency will decrease. Consequently, developing effective methods for aligning them to human values is vital.","authors":["Elizaveta Tennant","Stephen Hailes","Mirco Musolesi"],"url":"https://arxiv.org/abs/2410.01639"}
{"created":"2025-05-13","title":"Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting","abstract":"Recent advancements in generative modeling, particularly diffusion models, have opened new directions for time series modeling, achieving state-of-the-art performance in forecasting and synthesis. However, the reliance of diffusion-based models on a simple, fixed prior complicates the generative process since the data and prior distributions differ significantly. We introduce TSFlow, a conditional flow matching (CFM) model for time series combining Gaussian processes, optimal transport paths, and data-dependent prior distributions. By incorporating (conditional) Gaussian processes, TSFlow aligns the prior distribution more closely with the temporal structure of the data, enhancing both unconditional and conditional generation. Furthermore, we propose conditional prior sampling to enable probabilistic forecasting with an unconditionally trained model. In our experimental evaluation on eight real-world datasets, we demonstrate the generative capabilities of TSFlow, producing high-quality unconditional samples. Finally, we show that both conditionally and unconditionally trained models achieve competitive results across multiple forecasting benchmarks.","authors":["Marcel Kollovieh","Marten Lienen","David L\\\"udke","Leo Schwinn","Stephan G\\\"unnemann"],"url":"https://arxiv.org/abs/2410.03024"}
{"created":"2025-05-13","title":"Scaling Large Motion Models with Million-Level Human Motions","abstract":"Inspired by the recent success of LLMs, the field of human motion understanding has increasingly shifted toward developing large motion models. Despite some progress, current efforts remain far from achieving truly generalist models, primarily due to the lack of massive high-quality data. To address this gap, we present MotionLib, the first million-level dataset for motion generation, which is at least 15$\\times$ larger than existing counterparts and enriched with hierarchical text descriptions. Using MotionLib, we train a large motion model named Being-M0, demonstrating robust performance across a wide range of human activities, including unseen ones. Through systematic investigation, for the first time, we highlight the importance of scaling both data and model size for advancing motion generation, along with key insights to achieve this goal. To better integrate the motion modality, we propose Motionbook, an innovative motion encoding approach including (1) a compact yet lossless feature to represent motions; (2) a novel 2D lookup-free motion tokenizer that preserves fine-grained motion details while expanding codebook capacity, significantly enhancing the representational power of motion tokens. We believe this work lays the groundwork for developing more versatile and powerful motion generation models in the future. For further details, visit https://github.com/BeingBeyond/Being-M0.","authors":["Ye Wang","Sipeng Zheng","Bin Cao","Qianshan Wei","Weishuai Zeng","Qin Jin","Zongqing Lu"],"url":"https://arxiv.org/abs/2410.03311"}
{"created":"2025-05-13","title":"Learning to Drift in Extreme Turning with Active Exploration and Gaussian Process Based MPC","abstract":"Extreme cornering in racing often leads to large sideslip angles, presenting a significant challenge for vehicle control. Conventional vehicle controllers struggle to manage this scenario, necessitating the use of a drifting controller. However, the large sideslip angle in drift conditions introduces model mismatch, which in turn affects control precision. To address this issue, we propose a model correction drift controller that integrates Model Predictive Control (MPC) with Gaussian Process Regression (GPR). GPR is employed to correct vehicle model mismatches during both drift equilibrium solving and the MPC optimization process. Additionally, the variance from GPR is utilized to actively explore different cornering drifting velocities, aiming to minimize trajectory tracking errors. The proposed algorithm is validated through simulations on the Simulink-Carsim platform and experiments with a 1:10 scale RC vehicle. In the simulation, the average lateral error with GPR is reduced by 52.8% compared to the non-GPR case. Incorporating exploration further decreases this error by 27.1%. The velocity tracking Root Mean Square Error (RMSE) also decreases by 10.6% with exploration. In the RC car experiment, the average lateral error with GPR is 36.7% lower, and exploration further leads to a 29.0% reduction. Moreover, the velocity tracking RMSE decreases by 7.2% with the inclusion of exploration.","authors":["Guoqiang Wu","Cheng Hu","Wangjia Weng","Zhouheng Li","Yonghao Fu","Lei Xie","Hongye Su"],"url":"https://arxiv.org/abs/2410.05740"}
{"created":"2025-05-13","title":"SHAP values via sparse Fourier representation","abstract":"SHAP (SHapley Additive exPlanations) values are a widely used method for local feature attribution in interpretable and explainable AI. We propose an efficient two-stage algorithm for computing SHAP values in both black-box setting and tree-based models. Motivated by spectral bias in real-world predictors, we first approximate models using compact Fourier representations, exactly for trees and approximately for black-box models. In the second stage, we introduce a closed-form formula for {\\em exactly} computing SHAP values using the Fourier representation, that ``linearizes'' the computation into a simple summation and is amenable to parallelization. As the Fourier approximation is computed only once, our method enables amortized SHAP value computation, achieving significant speedups over existing methods and a tunable trade-off between efficiency and precision.","authors":["Ali Gorji","Andisheh Amrollahi","Andreas Krause"],"url":"https://arxiv.org/abs/2410.06300"}
{"created":"2025-05-13","title":"Stackelberg vs. Nash in the Lottery Colonel Blotto Game","abstract":"Resource competition problems are often modeled using Colonel Blotto games, where players take simultaneous actions. However, many real-world scenarios involve sequential decision-making rather than simultaneous moves.","authors":["Yan Liu","Bonan Ni","Weiran Shen","Zihe Wang","Jie Zhang"],"url":"https://arxiv.org/abs/2410.07690"}
{"created":"2025-05-13","title":"Can LLMs advance democratic values?","abstract":"LLMs are among the most advanced tools ever devised for understanding and generating natural language. Democratic deliberation and decision-making involve, at several distinct stages, the production and comprehension of language. So it is natural to ask whether our best linguistic tools might prove instrumental to one of our most important tasks involving language. Researchers and practitioners have recently asked whether LLMs can support democratic deliberation by leveraging abilities to summarise content, to aggregate opinion over summarised content, and to represent voters by predicting their preferences over unseen choices. In this paper, we assess whether using LLMs to perform these and related functions really advances the democratic values behind these experiments. We suggest that the record is mixed. In the presence of background inequality of power and resources, as well as deep moral and political disagreement, we should not use LLMs to automate non-instrumentally valuable components of the democratic process, nor be tempted to supplant fair and transparent decision-making procedures that are practically necessary to reconcile competing interests and values. However, while LLMs should be kept well clear of formal democratic decision-making processes, we think they can instead strengthen the informal public sphere--the arena that mediates between democratic governments and the polities that they serve, in which political communities seek information, form civic publics, and hold their leaders to account.","authors":["Seth Lazar","Lorenzo Manuali"],"url":"https://arxiv.org/abs/2410.08418"}
{"created":"2025-05-13","title":"iTeach: Interactive Teaching for Robot Perception using Mixed Reality","abstract":"We introduce iTeach, a human-in-the-loop Mixed Reality (MR) system that enhances robot perception through interactive teaching. Our system enables users to visualize robot perception outputs such as object detection and segmentation results using a MR device. Therefore, users can inspect failures of perception models using the system on real robots. Moreover, iTeach facilitates real-time, informed data collection and annotation, where users can use hand gesture, eye gaze and voice commands to annotate images collected from robots. The annotated images can be used to fine-tune perception models to improve their accuracy and adaptability. The system continually improves perception models by collecting annotations of failed examples from users. When applied to object detection and unseen object instance segmentation (UOIS) tasks, iTeach demonstrates encouraging results in improving pre-trained vision models for these two tasks. These results highlight the potential of MR to make robotic perception systems more capable and adaptive in real-world environments. Project page at https://irvlutd.github.io/iTeach.","authors":["Jishnu Jaykumar P","Cole Salvato","Vinaya Bomnale","Jikai Wang","Yu Xiang"],"url":"https://arxiv.org/abs/2410.09072"}
{"created":"2025-05-13","title":"MFIT: Multi-Fidelity Thermal Modeling for 2.5D and 3D Multi-Chiplet Architectures","abstract":"Rapidly evolving artificial intelligence and machine learning applications require ever-increasing computational capabilities, while monolithic 2D design technologies approach their limits. Heterogeneous integration of smaller chiplets using a 2.5D silicon interposer and 3D packaging has emerged as a promising paradigm to address this limit and meet performance demands. These approaches offer a significant cost reduction and higher manufacturing yield than monolithic 2D integrated circuits. However, the compact arrangement and high compute density exacerbate the thermal management challenges, potentially compromising performance. Addressing these thermal modeling challenges is critical, especially as system sizes grow and different design stages require varying levels of accuracy and speed. Since no single thermal modeling technique meets all these needs, this paper introduces MFIT, a range of multi-fidelity thermal models that effectively balance accuracy and speed. These multi-fidelity models can enable efficient design space exploration and runtime thermal management. Our extensive testing on systems with 16, 36, and 64 2.5D integrated chiplets and 16x3 3D integrated chiplets demonstrates that these models can reduce execution times from days to mere seconds and milliseconds with negligible loss in accuracy.","authors":["Lukas Pfromm","Alish Kanani","Harsh Sharma","Parth Solanki","Eric Tervo","Jaehyun Park","Janardhan Rao Doppa","Partha Pratim Pande","Umit Y. Ogras"],"url":"https://arxiv.org/abs/2410.09188"}
{"created":"2025-05-13","title":"Self-Data Distillation for Recovering Quality in Pruned Large Language Models","abstract":"Large language models have driven significant progress in natural language processing, but their deployment requires substantial compute and memory resources. As models scale, compression techniques become essential for balancing model quality with computational efficiency. Structured pruning, which removes less critical components of the model, is a promising strategy for reducing complexity. However, one-shot pruning often results in significant quality degradation, particularly in tasks requiring multi-step reasoning. To recover lost quality, supervised fine-tuning (SFT) is commonly applied, but it can lead to catastrophic forgetting by shifting the model's learned data distribution. Therefore, addressing the degradation from both pruning and SFT is essential to preserve the original model's quality. In this work, we utilize self-data distilled fine-tuning to address these challenges. Our approach leverages the original, unpruned model to generate a distilled dataset that preserves semantic richness and mitigates catastrophic forgetting by maintaining alignment with the base model's knowledge. Empirically, we demonstrate that self-data distillation consistently outperforms standard SFT, improving average accuracy by up to 8% on the HuggingFace OpenLLM Leaderboard v1. Specifically, when pruning six decoder blocks on Llama3.1-8B Instruct (i.e., 32 to 26 layers, reducing the model size from 8.03B to 6.72B parameters), our method retains 91.2% of the original model's accuracy compared to 81.7% with SFT, while reducing real-world FLOPs by 16.3%. Furthermore, combining self-data distilled models through model merging yields enhanced quality retention. Additionally, leveraging these pruned models in speculative decoding increases token acceptance rates, thereby improving inference efficiency in applied settings.","authors":["Vithursan Thangarasa","Ganesh Venkatesh","Mike Lasby","Nish Sinnadurai","Sean Lie"],"url":"https://arxiv.org/abs/2410.09982"}
{"created":"2025-05-13","title":"A Combinatorial Approach to Avoiding Weak Keys in the BIKE Cryptosystem","abstract":"Bit Flipping Key Encapsulation (BIKE) is a code-based cryptosystem that was considered in Round 4 of the NIST Post-Quantum Cryptography Standardization process. It is based on quasi-cyclic moderate-density parity-check (QC-MDPC) codes paired with an iterative decoder. While (low-density) parity-check codes have been shown to perform well in practice, their capabilities are governed by the code's graphical representation and the choice of decoder rather than the traditional code parameters, making it difficult to determine the decoder failure rate (DFR). Moreover, decoding failures have been demonstrated to lead to attacks that recover the BIKE private key. In this paper, we demonstrate a strong correlation between weak keys and $4$-cycles in their associated Tanner graphs. We give concrete ways to enumerate the number of 4-cycles in a BIKE key and use these results to present a filtering algorithm that will filter BIKE keys with large numbers of 4-cycles. These results also apply to more general parity check codes.","authors":["Gretchen L Matthews","Emily McMillon"],"url":"https://arxiv.org/abs/2410.11111"}
{"created":"2025-05-13","title":"Transfer Learning with Foundational Models for Time Series Forecasting using Low-Rank Adaptations","abstract":"Foundational Models are an emerging widely used technique of GenAI. These models are distinguished by their scalability and the ease with which they can be adapted through the exploitation of Transfer Learning. The availability of high computational power and large datasets have supported their development, achieving a high generalization capacity due to the enormous and heterogeneous amounts of data used in their initial training. These characteristics contribute to a solid base that can be adapted or adjusted to a wide range of tasks, increasing their applicability. This study proposes the methodology LLIAM, a straightforward adaptation of a kind of FM, Large Language Models, for the Time Series Forecasting task. An adequate time-series prompting schema and Low-Rank Adaptations are used to enhance the knowledge of the model with diverse time series datasets, known as the fine-tuning phase. A study divided in two stages has been performed for evaluating the effectiveness of the proposed methodology. Initially, a comparison was made between the performance of LLIAM and different state-of-the-art DL algorithms, including Recurrent Neural Networks and Temporal Convolutional Networks, as well as a LLM-based method, TimeLLM. Following this, a zero-shot study is presented in order to evaluate the generalization capacity of the proposed methodology with time series datasets from unknown domains not considered in the model training. The outcomes of this investigation demonstrate the efficacy of LLIAM, highlighting that this straightforward and general approach can attain competent results without the necessity for applying complex modifications. This work also encourages the use of available resources (such as these pre-trained models) and efficient fine-tuning techniques to avoid unnecessary and costly training, narrowing the gap between the goals of traditional AI and Green AI.","authors":["M. Germ\\'an-Morales","A. J. Rivera-Rivas","M. J. del Jesus D\\'iaz","C. J. Carmona"],"url":"https://arxiv.org/abs/2410.11539"}
{"created":"2025-05-13","title":"Rotating-star Pattern for Camera Calibration","abstract":"Camera calibration is fundamental to 3D vision, and the choice of calibration pattern greatly affects the accuracy. To address aberration issue, star-shaped pattern has been proposed as alternatives to traditional checkerboard. However, such pattern suffers from aliasing artifacts. In this paper, we present a novel solution by employing a series of checkerboard patterns rotated around a central point instead of a single star-shaped pattern. We further propose a complete feature extraction algorithm tailored for this design. Experimental results demonstrate that our approach offers improved accuracy over the conventional star-shaped pattern and achieves high stability across varying exposure levels.","authors":["Zezhun Shi"],"url":"https://arxiv.org/abs/2410.13371"}
{"created":"2025-05-13","title":"Similarity-Dissimilarity Loss for Multi-label Supervised Contrastive Learning","abstract":"Supervised contrastive learning has achieved remarkable success by leveraging label information; however, determining positive samples in multi-label scenarios remains a critical challenge. In multi-label supervised contrastive learning (MSCL), multi-label relations are not yet fully defined, leading to ambiguity in identifying positive samples and formulating contrastive loss functions to construct the representation space. To address these challenges, we: (i) first define five distinct multi-label relations in MSCL to systematically identify positive samples, (ii) introduce a novel Similarity-Dissimilarity Loss that dynamically re-weights samples through computing the similarity and dissimilarity factors between positive samples and given anchors based on multi-label relations, and (iii) further provide theoretical grounded proofs for our method through rigorous mathematical analysis that supports the formulation and effectiveness of the proposed loss function. We conduct the experiments across both image and text modalities, and extend the evaluation to medical domain. The results demonstrate that our method consistently outperforms baselines in a comprehensive evaluation, confirming its effectiveness and robustness. Code is available at: https://github.com/guangminghuang/similarity-dissimilarity-loss.","authors":["Guangming Huang","Yunfei Long","Cunjin Luo"],"url":"https://arxiv.org/abs/2410.13439"}
{"created":"2025-05-13","title":"Reward-free World Models for Online Imitation Learning","abstract":"Imitation learning (IL) enables agents to acquire skills directly from expert demonstrations, providing a compelling alternative to reinforcement learning. However, prior online IL approaches struggle with complex tasks characterized by high-dimensional inputs and complex dynamics. In this work, we propose a novel approach to online imitation learning that leverages reward-free world models. Our method learns environmental dynamics entirely in latent spaces without reconstruction, enabling efficient and accurate modeling. We adopt the inverse soft-Q learning objective, reformulating the optimization process in the Q-policy space to mitigate the instability associated with traditional optimization in the reward-policy space. By employing a learned latent dynamics model and planning for control, our approach consistently achieves stable, expert-level performance in tasks with high-dimensional observation or action spaces and intricate dynamics. We evaluate our method on a diverse set of benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating superior empirical performance compared to existing approaches.","authors":["Shangzhe Li","Zhiao Huang","Hao Su"],"url":"https://arxiv.org/abs/2410.14081"}
{"created":"2025-05-13","title":"Isolated Causal Effects of Natural Language","abstract":"As language technologies become widespread, it is important to understand how changes in language affect reader perceptions and behaviors. These relationships may be formalized as the isolated causal effect of some focal language-encoded intervention (e.g., factual inaccuracies) on an external outcome (e.g., readers' beliefs). In this paper, we introduce a formal estimation framework for isolated causal effects of language. We show that a core challenge of estimating isolated effects is the need to approximate all non-focal language outside of the intervention. Drawing on the principle of omitted variable bias, we provide measures for evaluating the quality of both non-focal language approximations and isolated effect estimates themselves. We find that poor approximation of non-focal language can lead to bias in the corresponding isolated effect estimates due to omission of relevant variables, and we show how to assess the sensitivity of effect estimates to such bias along the two key axes of fidelity and overlap. In experiments on semi-synthetic and real-world data, we validate the ability of our framework to correctly recover isolated effects and demonstrate the utility of our proposed measures.","authors":["Victoria Lin","Louis-Philippe Morency","Eli Ben-Michael"],"url":"https://arxiv.org/abs/2410.14812"}
{"created":"2025-05-13","title":"Long Term Memory: The Foundation of AI Self-Evolution","abstract":"Large language models (LLMs) like GPTs, trained on vast datasets, have demonstrated impressive capabilities in language understanding, reasoning, and planning, achieving human-level performance in various tasks. Most studies focus on enhancing these models by training on ever-larger datasets to build more powerful foundation models. While training stronger models is important, enabling models to evolve during inference is equally crucial, a process we refer to as AI self-evolution. Unlike large-scale training, self-evolution may rely on limited data or interactions. Inspired by the columnar organization of the human cerebral cortex, we hypothesize that AI models could develop cognitive abilities and build internal representations through iterative interactions with their environment. To achieve this, models need long-term memory (LTM) to store and manage processed interaction data. LTM supports self-evolution by representing diverse experiences across environments and agents. In this report, we explore AI self-evolution and its potential to enhance models during inference. We examine LTM's role in lifelong learning, allowing models to evolve based on accumulated interactions. We outline the structure of LTM and the systems needed for effective data retention and representation. We also classify approaches for building personalized models with LTM data and show how these models achieve self-evolution through interaction. Using LTM, our multi-agent framework OMNE achieved first place on the GAIA benchmark, demonstrating LTM's potential for AI self-evolution. Finally, we present a roadmap for future research, emphasizing the importance of LTM for advancing AI technology and its practical applications.","authors":["Xun Jiang","Feng Li","Han Zhao","Jiahao Qiu","Jiaying Wang","Jun Shao","Shihao Xu","Shu Zhang","Weiling Chen","Xavier Tang","Yize Chen","Mengyue Wu","Weizhi Ma","Mengdi Wang","Tianqiao Chen"],"url":"https://arxiv.org/abs/2410.15665"}
{"created":"2025-05-13","title":"Neural Predictor for Flight Control with Payload","abstract":"Aerial robotics for transporting suspended payloads as the form of freely-floating manipulator are growing great interest in recent years. However, the force/torque caused by payload and residual dynamics will introduce unmodeled perturbations to the aerial robotics, which negatively affects the closed-loop performance. Different from estimation-like methods, this paper proposes Neural Predictor, a learning-based approach to model force/torque induced by payload and residual dynamics as a dynamical system. It yields a hybrid model that combines the first-principles dynamics with the learned dynamics. The hybrid model is then integrated into a MPC framework to improve closed-loop performance. Effectiveness of proposed framework is verified extensively in both numerical simulations and real-world flight experiments. The results indicate that our approach can capture force/torque caused by suspended payload and residual dynamics accurately, respond quickly to the changes of them and improve the closed-loop performance significantly. In particular, Neural Predictor outperforms a state-of-the-art learning-based estimator and has reduced the force and torque estimation errors by up to 66.15% and 33.33% while requiring less samples. The code of proposed Neural Predictor can be found at https://github.com/NPU-RCIR/Neural-Predictor.git.","authors":["Ao Jin","Chenhao Li","Qinyi Wang","Ya Liu","Panfeng Huang","Fan Zhang"],"url":"https://arxiv.org/abs/2410.15946"}
{"created":"2025-05-13","title":"Steering Large Language Models using Conceptors: Improving Addition-Based Activation Engineering","abstract":"Large language models have transformed AI, yet reliably controlling their outputs remains a challenge. This paper explores activation engineering, where outputs of pre-trained LLMs are controlled by manipulating their activations at inference time. Unlike traditional methods using a single steering vector, we introduce conceptors - mathematical constructs that represent sets of activation vectors as ellipsoidal regions. Conceptors act as soft projection matrices and offer more precise control over complex activation patterns. Our experiments demonstrate that conceptors outperform traditional methods across multiple steering tasks. We further use Boolean operations on conceptors for combined steering goals that empirically outperform additively combining steering vectors on a set of tasks. These results highlight conceptors as a promising tool for more effective steering of LLMs. Our code is available on github.com/jorispos/conceptorsteering.","authors":["Joris Postmus","Steven Abreu"],"url":"https://arxiv.org/abs/2410.16314"}
{"created":"2025-05-13","title":"Learning Fair and Preferable Allocations through Neural Network","abstract":"The fair allocation of indivisible resources is a fundamental problem. Existing research has developed various allocation mechanisms or algorithms to satisfy different fairness notions. For example, round robin (RR) was proposed to meet the fairness criterion known as envy-freeness up to one good (EF1). Expert algorithms without mathematical formulations are used in real-world resource allocation problems to find preferable outcomes for users. Therefore, we aim to design mechanisms that strictly satisfy good properties with replicating expert knowledge. However, this problem is challenging because such heuristic rules are often difficult to formalize mathematically, complicating their integration into theoretical frameworks. Additionally, formal algorithms struggle to find preferable outcomes, and directly replicating these implicit rules can result in unfair allocations because human decision-making can introduce biases. In this paper, we aim to learn implicit allocation mechanisms from examples while strictly satisfying fairness constraints, specifically focusing on learning EF1 allocation mechanisms through supervised learning on examples of reported valuations and corresponding allocation outcomes produced by implicit rules. To address this, we developed a neural RR (NRR), a novel neural network that parameterizes RR. NRR is built from a differentiable relaxation of RR and can be trained to learn the agent ordering used for RR. We conducted experiments to learn EF1 allocation mechanisms from examples, demonstrating that our method outperforms baselines in terms of the proximity of predicted allocations and other metrics.","authors":["Ryota Maruo","Koh Takeuchi","Hisashi Kashima"],"url":"https://arxiv.org/abs/2410.17500"}
{"created":"2025-05-13","title":"Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions","abstract":"Large language models (LLMs) have demonstrated great performance across various benchmarks, showing potential as general-purpose task solvers. However, as LLMs are typically trained on vast amounts of data, a significant concern in their evaluation is data contamination, where overlap between training data and evaluation datasets inflates performance assessments. Multiple approaches have been developed to identify data contamination. These approaches rely on specific assumptions that may not hold universally across different settings. To bridge this gap, we systematically review 50 papers on data contamination detection, categorize the underlying assumptions, and assess whether they have been rigorously validated. We identify and analyze eight categories of assumptions and test three of them as case studies. Our case studies focus on detecting direct, instance-level data contamination, which is also referred to as Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches based on these three assumptions can have similar performance to random guessing, on datasets used in LLM pretraining, suggesting that current LLMs might learn data distributions rather than memorizing individual instances. Meanwhile, MIA can easily fail when there are data distribution shifts between the seen and unseen instances.","authors":["Yujuan Fu","Ozlem Uzuner","Meliha Yetisgen","Fei Xia"],"url":"https://arxiv.org/abs/2410.18966"}
{"created":"2025-05-13","title":"DiffGAN: A Test Generation Approach for Differential Testing of Deep Neural Networks","abstract":"Deep Neural Networks (DNNs) are increasingly deployed across applications. However, ensuring their reliability remains a challenge, and in many situations, alternative models with similar functionality and accuracy are available. Traditional accuracy-based evaluations often fail to capture behavioral differences between models, especially with limited test datasets, making it difficult to select or combine models effectively. Differential testing addresses this by generating test inputs that expose discrepancies in DNN model behavior. However, existing approaches face significant limitations: many rely on model internals or are constrained by available seed inputs. To address these challenges, we propose DiffGAN, a black-box test image generation approach for differential testing of DNN models. DiffGAN leverages a Generative Adversarial Network (GAN) and the Non-dominated Sorting Genetic Algorithm II to generate diverse and valid triggering inputs that reveal behavioral discrepancies between models. DiffGAN employs two custom fitness functions, focusing on diversity and divergence, to guide the exploration of the GAN input space and identify discrepancies between models' outputs. By strategically searching this space, DiffGAN generates inputs with specific features that trigger differences in model behavior. DiffGAN is black-box, making it applicable in more situations. We evaluate DiffGAN on eight DNN model pairs trained on widely used image datasets. Our results show DiffGAN significantly outperforms a SOTA baseline, generating four times more triggering inputs, with greater diversity and validity, within the same budget. Additionally, the generated inputs improve the accuracy of a machine learning-based model selection mechanism, which selects the best-performing model based on input characteristics and can serve as a smart output voting mechanism when using alternative models.","authors":["Zohreh Aghababaeyan","Manel Abdellatif","Lionel Briand","Ramesh S"],"url":"https://arxiv.org/abs/2410.19794"}
{"created":"2025-05-13","title":"RAM: Replace Attention with MLP for Efficient Multivariate Time Series Forecasting","abstract":"Attention-based architectures have become ubiquitous in time series forecasting tasks, including spatio-temporal (STF) and long-term time series forecasting (LTSF). Yet, our understanding of the reasons for their effectiveness remains limited. In this work, we propose a novel pruning strategy, $\\textbf{R}$eplace $\\textbf{A}$ttention with $\\textbf{M}$LP (RAM), that approximates the attention mechanism using only feedforward layers, residual connections, and layer normalization for temporal and/or spatial modeling in multivariate time series forecasting. Specifically, the Q, K, and V projections, the attention score calculation, the dot-product between the attention score and the V, and the final projection can be removed from the attention-based networks without significantly degrading the performance, so that the given network remains the top-tier compared to other SOTA methods. RAM achieves a $62.579\\%$ reduction in FLOPs for spatio-temporal models with less than $2.5\\%$ performance drop, and a $42.233\\%$ FLOPs reduction for LTSF models with less than $2\\%$ performance drop.","authors":["Suhan Guo","Jiahong Deng","Yi Wei","Hui Dou","Furao Shen","Jian Zhao"],"url":"https://arxiv.org/abs/2410.24023"}
{"created":"2025-05-13","title":"CLIP-RT: Learning Language-Conditioned Robotic Policies from Natural Language Supervision","abstract":"Teaching robots desired skills in real-world environments remains challenging, especially for non-experts. A key bottleneck is that collecting robotic data often requires expertise or specialized hardware, limiting accessibility and scalability. We posit that natural language offers an intuitive and accessible interface for robot learning. To this end, we study two aspects: (1) enabling non-experts to collect robotic data through natural language supervision (e.g., \"move the arm to the right\") and (2) training robot policies directly from this supervision. Specifically, we introduce a data collection framework that collects robot demonstrations based on natural language supervision and further augments these demonstrations. We then present CLIP-RT, a new vision-language-action (VLA) model that learns language-conditioned visuomotor policies from this supervision. CLIP-RT adapts the pretrained CLIP model and learns to predict language-based motion primitives via contrastive imitation learning. We train CLIP-RT on the Open X-Embodiment dataset and finetune it on in-domain data collected by our framework. In real-world evaluations, CLIP-RT demonstrates strong capabilities in learning novel manipulation skills, outperforming OpenVLA (7B parameters) by 24% in average success rates, while using 7x fewer parameters (1B). We further assess CLIP-RT's capabilities in few-shot generalization and collaborative scenarios involving large pretrained models or humans. In simulated environments, CLIP-RT also yields strong performance, achieving a 93.1% average success rate on the LIBERO benchmark with an inference throughput of 163 Hz.","authors":["Gi-Cheon Kang","Junghyun Kim","Kyuhwan Shim","Jun Ki Lee","Byoung-Tak Zhang"],"url":"https://arxiv.org/abs/2411.00508"}
{"created":"2025-05-13","title":"Heterogeneous Multi-robot Task Allocation for Long-Endurance Missions in Dynamic Scenarios","abstract":"We present a framework for Multi-Robot Task Allocation (MRTA) in heterogeneous teams performing long-endurance missions in dynamic scenarios. Given the limited battery of robots, especially for aerial vehicles, we allow for robot recharges and the possibility of fragmenting and/or relaying certain tasks. We also address tasks that must be performed by a coalition of robots in a coordinated manner. Given these features, we introduce a new class of heterogeneous MRTA problems which we analyze theoretically and optimally formulate as a Mixed-Integer Linear Program. We then contribute a heuristic algorithm to compute approximate solutions and integrate it into a mission planning and execution architecture capable of reacting to unexpected events by repairing or recomputing plans online. Our experimental results show the relevance of our newly formulated problem in a realistic use case for inspection with aerial robots. We assess the performance of our heuristic solver in comparison with other variants and with exact optimal solutions in small-scale scenarios. In addition, we evaluate the ability of our replanning framework to repair plans online.","authors":["Alvaro Calvo","Jesus Capitan"],"url":"https://arxiv.org/abs/2411.02062"}
{"created":"2025-05-13","title":"Evaluating Creative Short Story Generation in Humans and Large Language Models","abstract":"Story-writing is a fundamental aspect of human imagination, relying heavily on creativity to produce narratives that are novel, effective, and surprising. While large language models (LLMs) have demonstrated the ability to generate high-quality stories, their creative story-writing capabilities remain under-explored. In this work, we conduct a systematic analysis of creativity in short story generation across 60 LLMs and 60 people using a five-sentence cue-word-based creative story-writing task. We use measures to automatically evaluate model- and human-generated stories across several dimensions of creativity, including novelty, surprise, diversity, and linguistic complexity. We also collect creativity ratings and Turing Test classifications from non-expert and expert human raters and LLMs. Automated metrics show that LLMs generate stylistically complex stories, but tend to fall short in terms of novelty, surprise and diversity when compared to average human writers. Expert ratings generally coincide with automated metrics. However, LLMs and non-experts rate LLM stories to be more creative than human-generated stories. We discuss why and how these differences in ratings occur, and their implications for both human and artificial creativity.","authors":["Mete Ismayilzada","Claire Stevenson","Lonneke van der Plas"],"url":"https://arxiv.org/abs/2411.02316"}
{"created":"2025-05-13","title":"The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models","abstract":"Natural-language assistants are designed to provide users with helpful responses while avoiding harmful outputs, largely achieved through alignment to human preferences. Yet there is limited understanding of whether alignment techniques may inadvertently perpetuate or even amplify harmful biases inherited from their pre-aligned base models. This issue is compounded by the choice of bias evaluation benchmarks in popular preference-finetuned models, which predominantly focus on dominant social categories, such as binary gender, thereby limiting insights into biases affecting underrepresented groups. Towards addressing this gap, we center transgender, nonbinary, and other gender-diverse identities to investigate how alignment procedures interact with pre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a comprehensive survey of bias evaluation modalities across leading preference-finetuned LLMs, highlighting critical gaps in gender-diverse representation, 2) systematic evaluation of gender-diverse biases across 16 models spanning Direct Preference Optimization (DPO) stages, uncovering harms popular bias benchmarks fail to detect, and 3) a flexible framework for measuring harmful biases in implicit reward signals applicable to other social contexts. Our findings reveal that DPO-aligned models are particularly sensitive to supervised finetuning (SFT), and can amplify two forms of real-world gender-diverse harms from their base models: stigmatization and gender non-affirmative language. We conclude with recommendations tailored to DPO and broader alignment practices, advocating for the adoption of community-informed bias evaluation frameworks to more effectively identify and address underrepresented harms in LLMs.","authors":["Anaelia Ovalle","Krunoslav Lehman Pavasovic","Louis Martin","Luke Zettlemoyer","Eric Michael Smith","Kai-Wei Chang","Adina Williams","Levent Sagun"],"url":"https://arxiv.org/abs/2411.03700"}
{"created":"2025-05-13","title":"Diversity Helps Jailbreak Large Language Models","abstract":"We have uncovered a powerful jailbreak technique that leverages large language models' ability to diverge from prior context, enabling them to bypass safety constraints and generate harmful outputs. By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62.83% higher success rate in compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while using only 12.9% of the queries. This revelation exposes a critical flaw in current LLM safety training, suggesting that existing methods may merely mask vulnerabilities rather than eliminate them. Our findings sound an urgent alarm for the need to revolutionize testing methodologies to ensure robust and reliable LLM security.","authors":["Weiliang Zhao","Daniel Ben-Levi","Wei Hao","Junfeng Yang","Chengzhi Mao"],"url":"https://arxiv.org/abs/2411.04223"}
{"created":"2025-05-13","title":"The First Prompt Counts the Most! An Evaluation of Large Language Models on Iterative Example-Based Code Generation","abstract":"The capabilities of Large Language Models (LLMs) in code generation have been extensively studied, particularly for implementing target functionalities from natural-language descriptions. Alternatively, input-output (I/O) examples provide an accessible, unambiguous, and flexible way to describe functionalities. However, their inherent diversity, opaqueness, and incompleteness impose greater challenges for understanding and implementing the target requirements. Therefore, generating code from I/O examples (i.e., example-based code generation) provides a new perspective, allowing us to additionally evaluate LLMs' capability to infer target functionalities from limited information and to process new-form requirements. However, related research about LLMs in example-based code generation remains largely unexplored. To fill this gap, this paper presents the first comprehensive study on example-based code generation using LLMs. We adopt an iterative evaluation framework and formalize the objective of example-based code generation as two sequential sub-objectives: generating code conforming to the given examples and generating code that successfully implements the target functionalities from (iteratively) given examples. We assess six state-of-the-art LLMs using a new benchmark of 172 diverse target functionalities. The results demonstrate that when requirements are described using iterative I/O examples rather than natural language, the LLMs' score decreases by over 60%, and the vast majority (even over 95%) of successfully implemented functionalities are achieved in the first round of the iterations. Furthermore, we also find that combining I/O examples with even imprecise and fragmental natural language descriptions greatly improves LLM performance, and the selection of initial I/O examples can also influence the score, suggesting opportunities for prompt optimization.","authors":["Yingjie Fu","Bozhou Li","Linyi Li","Wentao Zhang","Tao Xie"],"url":"https://arxiv.org/abs/2411.06774"}
{"created":"2025-05-13","title":"Veri-Car: Towards Open-world Vehicle Information Retrieval","abstract":"Many industrial and service sectors require tools to extract vehicle characteristics from images. This is a complex task not only by the variety of noise, and large number of classes, but also by the constant introduction of new vehicle models to the market. In this paper, we present Veri-Car, an information retrieval integrated approach designed to help on this task. It leverages supervised learning techniques to accurately identify the make, type, model, year, color, and license plate of cars. The approach also addresses the challenge of handling open-world problems, where new car models and variations frequently emerge, by employing a sophisticated combination of pre-trained models, and a hierarchical multi-similarity loss. Veri-Car demonstrates robust performance, achieving high precision and accuracy in classifying both seen and unseen data. Additionally, it integrates an ensemble license plate detection, and an OCR model to extract license plate numbers with impressive accuracy.","authors":["Andr\\'es Mu\\~noz","Nancy Thomas","Annita Vapsi","Daniel Borrajo"],"url":"https://arxiv.org/abs/2411.06864"}
{"created":"2025-05-13","title":"Efficient 3D Perception on Multi-Sweep Point Cloud with Gumbel Spatial Pruning","abstract":"This paper studies point cloud perception within outdoor environments. Existing methods face limitations in recognizing objects located at a distance or occluded, due to the sparse nature of outdoor point clouds. In this work, we observe a significant mitigation of this problem by accumulating multiple temporally consecutive point cloud sweeps, resulting in a remarkable improvement in perception accuracy. However, the computation cost also increases, hindering previous approaches from utilizing a large number of point cloud sweeps. To tackle this challenge, we find that a considerable portion of points in the accumulated point cloud is redundant, and discarding these points has minimal impact on perception accuracy. We introduce a simple yet effective Gumbel Spatial Pruning (GSP) layer that dynamically prunes points based on a learned end-to-end sampling. The GSP layer is decoupled from other network components and thus can be seamlessly integrated into existing point cloud network architectures. Without incurring additional computational overhead, we increase the number of point cloud sweeps from 10, a common practice, to as many as 40. Consequently, there is a significant enhancement in perception performance. For instance, in nuScenes 3D object detection and BEV map segmentation tasks, our pruning strategy improves several 3D perception baseline methods.","authors":["Tianyu Sun","Jianhao Li","Xueqian Zhang","Zhongdao Wang","Bailan Feng","Hengshuang Zhao"],"url":"https://arxiv.org/abs/2411.07742"}
{"created":"2025-05-13","title":"Finite-Alphabet-Aware Trajectory and Precoder Optimization for UAV Relaying","abstract":"Unmanned aerial vehicles (UAVs) have become key enablers in relay-assisted wireless communications thanks to their flexibility and line-of-sight channel advantage. However, most existing trajectory optimization frameworks assume ideal Gaussian inputs, overlooking the fact that practical wireless systems rely on structured, finite-alphabet constellations. This mismatch can lead to suboptimal, and sometimes misleading, design choices. In this paper, we challenge that convention by introducing a finite-alphabet-aware framework for joint trajectory and precoder optimization in UAV-assisted relay systems. We formulate a non-convex design problem that directly accounts for discrete signal structures and propose an efficient solution based on alternating optimization and successive convex approximation. Simulation results reveal that strategies optimized under Gaussian assumptions can waste energy and degrade throughput in real deployments. In contrast, our approach adapts both the UAV's trajectory and transmission strategy to the underlying modulation format, delivering consistent performance gains under practical system constraints. This work takes a key step toward aligning UAV communication design with the realities of modern wireless systems: discrete signals, power limits, and intelligent mobility.","authors":["Haoyang Di","Xiaodong Zhu","Yulin Shao"],"url":"https://arxiv.org/abs/2411.08680"}
{"created":"2025-05-13","title":"Systolic Arrays and Structured Pruning Co-design for Efficient Transformers in Edge Systems","abstract":"Efficient deployment of resource-intensive transformers on edge devices necessitates cross-stack optimization. We thus study the interrelation between structured pruning and systolic acceleration, matching the size of pruned blocks with the systolic array dimensions. In this setting, computations of pruned weight blocks can be skipped, reducing run-time and energy consumption, but potentially impacting quality of service (QoS). To evaluate the trade-offs between systolic array size and sparsity opportunities, we present a novel co-design framework that integrates algorithmic optimization, system simulation, and hardware design. Targeting speech recognition and machine translation using transformers as case study, we analyze how configuration choices across the stack affect performance metrics. Results demonstrate that structured pruning on systems featuring systolic array acceleration can effectively increase performance, while maintaining high QoS levels. Up to 44% system-wide speedups due to structured pruning and quantization were measured, with only 1.4% word error rate degradation on the standard LibriSpeech dataset.","authors":["Pedro Palacios","Rafael Medina","Jean-Luc Rouas","Giovanni Ansaloni","David Atienza"],"url":"https://arxiv.org/abs/2411.10285"}
{"created":"2025-05-13","title":"CCi-YOLOv8n: Enhanced Fire Detection with CARAFE and Context-Guided Modules","abstract":"Fire incidents in urban and forested areas pose serious threats,underscoring the need for more effective detection technologies. To address these challenges, we present CCi-YOLOv8n, an enhanced YOLOv8 model with targeted improvements for detecting small fires and smoke. The model integrates the CARAFE up-sampling operator and a context-guided module to reduce information loss during up-sampling and down-sampling, thereby retaining richer feature representations. Additionally, an inverted residual mobile block enhanced C2f module captures small targets and fine smoke patterns, a critical improvement over the original model's detection capacity.For validation, we introduce Web-Fire, a dataset curated for fire and smoke detection across diverse real-world scenarios. Experimental results indicate that CCi-YOLOv8n outperforms YOLOv8n in detection precision, confirming its effectiveness for robust fire detection tasks.","authors":["Kunwei Lv","Ruobing Wu","Suyang Chen","Ping Lan"],"url":"https://arxiv.org/abs/2411.11011"}
{"created":"2025-05-13","title":"Transmission Line Defect Detection Based on UAV Patrol Images and Vision-language Pretraining","abstract":"Unmanned aerial vehicle (UAV) patrol inspection has emerged as a predominant approach in transmission line monitoring owing to its cost-effectiveness. Detecting defects in transmission lines is a critical task during UAV patrol inspection. However, due to imaging distance and shooting angles, UAV patrol images often suffer from insufficient defect-related visual information, which has an adverse effect on detection accuracy. In this article, we propose a novel method for detecting defects in UAV patrol images, which is based on vision-language pretraining for transmission line (VLP-TL) and a progressive transfer strategy (PTS). Specifically, VLP-TL contains two novel pretraining tasks tailored for the transmission line scenario, aimimg at pretraining an image encoder with abundant knowledge acquired from both visual and linguistic information. Transferring the pretrained image encoder to the defect detector as its backbone can effectively alleviate the insufficient visual information problem. In addition, the PTS further improves transfer performance by progressively bridging the gap between pretraining and downstream defection detection. Experimental results demonstrate that the proposed method significantly improves defect detection accuracy by jointly utilizing multimodal information, overcoming the limitations of insufficient defect-related visual information provided by UAV patrol images.","authors":["Ke Zhang","Zhaoye Zheng","Yurong Guo","Jiacun Wang","Jiyuan Yang","Yangjie Xiao"],"url":"https://arxiv.org/abs/2411.11370"}
{"created":"2025-05-13","title":"GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual Grounding","abstract":"Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an object's position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching the performance of specialized methods on multiple benchmarks. Code available at https://github.com/zytx121/GeoGround","authors":["Yue Zhou","Mengcheng Lan","Xiang Li","Litong Feng","Yiping Ke","Xue Jiang","Qingyun Li","Xue Yang","Wayne Zhang"],"url":"https://arxiv.org/abs/2411.11904"}
{"created":"2025-05-13","title":"DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method","abstract":"To ensure the reliable operation of speech systems across diverse environments, noise addition methods have emerged as the prevailing solution. However, existing methods offer limited coverage of real-world noisy scenes and depend on pre-existing scene-based information and noise. This paper presents prompt-based Dynamic Generative Scene-based Noise Addition (DGSNA), a novel noise addition methodology that integrates Dynamic Generation of Scene-based Information (DGSI) with Scene-based Noise Addition for Speech (SNAS). This integration facilitates automated scene-based noise addition by transforming clean speech into various noise environments, thereby providing a more comprehensive and realistic simulation of diverse noise conditions. Experimental results demonstrate that DGSNA significantly enhances the robustness of speech recognition and keyword spotting models across various noise conditions, achieving a relative improvement of up to 11.21%. Furthermore, DGSNA can be effectively integrated with other noise addition methods to enhance performance. Our implementation and demonstrations are available at https://dgsna.github.io.","authors":["Zihao Chen","Zhentao Lin","Bi Zeng","Linyi Huang","Zhi Li","Jia Cai"],"url":"https://arxiv.org/abs/2411.12363"}
{"created":"2025-05-13","title":"Distributed Coordination of Grid-Forming and Grid-Following Inverters for Optimal Frequency Control in Power Systems","abstract":"The large-scale integration of inverter-interfaced renewable energy sources presents significant challenges to maintaining power balance and nominal frequency in modern power systems. This paper studies grid-level coordinated control of grid-forming (GFM) and grid-following (GFL) inverter-based resources (IBRs) for scalable and optimal frequency control. We propose a fully distributed optimal frequency control algorithm based on the projected primal-dual gradient method and by leveraging the structure of the underlying physical system dynamics. The proposed algorithm i) restores the nominal system frequency while minimizing total control cost and enforcing IBR power capacity limits and line thermal constraints, and ii) operates in a distributed manner that only needs local measurements and neighbor-to-neighbor communication. In particular, when the line thermal constraints are disregarded, the proposed algorithm admits a fully local implementation that requires no communication, while still ensuring optimality and satisfying IBR power capacity limits. We establish the global asymptotic convergence of the algorithm using Lyapunov stability analysis. The effectiveness and optimality of the proposed algorithms are validated through high-fidelity, 100% inverter-based electromagnetic transient (EMT) simulations on the IEEE 39-bus system.","authors":["Xiaoyang Wang","Xin Chen"],"url":"https://arxiv.org/abs/2411.12682"}
{"created":"2025-05-13","title":"AsymDex: Asymmetry and Relative Coordinates for RL-based Bimanual Dexterity","abstract":"We present Asymmetric Dexterity (AsymDex), a novel and simple reinforcement learning (RL) framework that can efficiently learn a large class of bimanual skills in multi-fingered hands without relying on demonstrations. Two crucial insights enable AsymDex to reduce the observation and action space dimensions and improve sample efficiency. First, true ambidexterity is rare in humans and most of us exhibit strong \"handedness\". Inspired by this observation, we assign complementary roles to each hand: the facilitating hand repositions and reorients one object, while the dominant hand performs complex manipulations to achieve the desired result (e.g., opening a bottle cap, or pouring liquids). Second, controlling the relative motion between the hands is crucial for coordination and synchronization of the two hands. As such, we design relative observation and action spaces and leverage a relative-pose tracking controller. Further, we propose a two-phase decomposition in which AsymDex can be readily integrated with recent advances in grasp learning to facilitate both the acquisition and manipulation of objects using two hands. Unlike existing RL-based methods for bimanual dexterity with multi-fingered hands, which are either sample inefficient or tailored to a specific task, AsymDex can efficiently learn a wide variety of bimanual skills that exhibit asymmetry. Detailed experiments on seven asymmetric bimanual dexterous manipulation tasks (four simulated and three real-world) reveal that AsymDex consistently outperforms strong baselines that challenge our design choices. The project website is at https://sites.google.com/view/asymdex-2025/.","authors":["Zhaodong Yang","Yunhai Han","Ai-Ping Hu","Harish Ravichandar"],"url":"https://arxiv.org/abs/2411.13020"}
{"created":"2025-05-13","title":"dc-GAN: Dual-Conditioned GAN for Face Demorphing From a Single Morph","abstract":"A facial morph is an image created by combining two face images pertaining to two distinct identities. Face demorphing inverts the process and tries to recover the original images constituting a facial morph. While morph attack detection (MAD) techniques can be used to flag morph images, they do not divulge any visual information about the faces used to create them. Demorphing helps address this problem. Existing demorphing techniques are either very restrictive (assume identities during testing) or produce feeble outputs (both outputs look very similar). In this paper, we overcome these issues by proposing dc-GAN, a novel GAN-based demorphing method conditioned on the morph images. Our method overcomes morph-replication and produces high quality reconstructions of the bonafide images used to create the morphs. Moreover, our method is highly generalizable across demorphing paradigms (differential/reference-free). We conduct experiments on AMSL, FRLL-Morphs and MorDiff datasets to showcase the efficacy of our method.","authors":["Nitish Shukla","Arun Ross"],"url":"https://arxiv.org/abs/2411.14494"}
{"created":"2025-05-13","title":"Capacity Approximations for Insertion Channels with Small Insertion Probabilities","abstract":"Channels with synchronization errors, exhibiting deletion and insertion errors, find practical applications in DNA storage, data reconstruction, and various other domains. Presence of insertions and deletions render the channel with memory, complicating capacity analysis. For instance, despite the formulation of an independent and identically distributed (i.i.d.) deletion channel more than fifty years ago, and proof that the channel is information stable, hence its Shannon capacity exists, calculation of the capacity remained elusive. However, a relatively recent result establishes the capacity of the deletion channel in the asymptotic regime of small deletion probabilities by computing the dominant terms of the capacity expansion. This paper extends that result to binary insertion channels, determining the dominant terms of the channel capacity for small insertion probabilities and establishing capacity in this asymptotic regime. Specifically, we consider two i.i.d. insertion channel models: insertion channel with possible random bit insertions after every transmitted bit and the Gallager insertion model, for which a bit is replaced by two random bits with a certain probability. To prove our results, we build on methods used for the deletion channel, employing Bernoulli(1/2) inputs for achievability and coupling this with a converse using stationary and ergodic processes as inputs, and show that the channel capacity differs only in the higher order terms from the achievable rates with i.i.d. inputs. The results, for instance, show that the capacity of the random insertion channel is higher than that of the Gallager insertion channel, and quantifies the difference in the asymptotic regime.","authors":["Busra Tegin","Tolga M Duman"],"url":"https://arxiv.org/abs/2411.14771"}
{"created":"2025-05-13","title":"XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models","abstract":"The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.","authors":["Yixin Dong","Charlie F. Ruan","Yaxing Cai","Ruihang Lai","Ziyi Xu","Yilong Zhao","Tianqi Chen"],"url":"https://arxiv.org/abs/2411.15100"}
{"created":"2025-05-13","title":"DiffServe: Efficiently Serving Text-to-Image Diffusion Models with Query-Aware Model Scaling","abstract":"Text-to-image generation using diffusion models has gained increasing popularity due to their ability to produce high-quality, realistic images based on text prompts. However, efficiently serving these models is challenging due to their computation-intensive nature and the variation in query demands. In this paper, we aim to address both problems simultaneously through query-aware model scaling. The core idea is to construct model cascades so that easy queries can be processed by more lightweight diffusion models without compromising image generation quality. Based on this concept, we develop an end-to-end text-to-image diffusion model serving system, DiffServe, which automatically constructs model cascades from available diffusion model variants and allocates resources dynamically in response to demand fluctuations. Our empirical evaluations demonstrate that DiffServe achieves up to 24% improvement in response quality while maintaining 19-70% lower latency violation rates compared to state-of-the-art model serving systems.","authors":["Sohaib Ahmad","Qizheng Yang","Haoliang Wang","Ramesh K. Sitaraman","Hui Guan"],"url":"https://arxiv.org/abs/2411.15381"}
{"created":"2025-05-13","title":"ELEMENTAL: Interactive Learning from Demonstrations and Vision-Language Models for Reward Design in Robotics","abstract":"Reinforcement learning (RL) has demonstrated compelling performance in robotic tasks, but its success often hinges on the design of complex, ad hoc reward functions. Researchers have explored how Large Language Models (LLMs) could enable non-expert users to specify reward functions more easily. However, LLMs struggle to balance the importance of different features, generalize poorly to out-of-distribution robotic tasks, and cannot represent the problem properly with only text-based descriptions. To address these challenges, we propose ELEMENTAL (intEractive LEarning froM dEmoNstraTion And Language), a novel framework that combines natural language guidance with visual user demonstrations to align robot behavior with user intentions better. By incorporating visual inputs, ELEMENTAL overcomes the limitations of text-only task specifications, while leveraging inverse reinforcement learning (IRL) to balance feature weights and match the demonstrated behaviors optimally. ELEMENTAL also introduces an iterative feedback-loop through self-reflection to improve feature, reward, and policy learning. Our experiment results demonstrate that ELEMENTAL outperforms prior work by 42.3% on task success, and achieves 41.3% better generalization in out-of-distribution tasks, highlighting its robustness in LfD.","authors":["Letian Chen","Nina Moorman","Matthew Gombolay"],"url":"https://arxiv.org/abs/2411.18825"}
{"created":"2025-05-13","title":"Carbon-Aware Quality Adaptation for Energy-Intensive Services","abstract":"The energy demand of modern cloud services, particularly those related to generative AI, is increasing at an unprecedented pace. To date, carbon-aware computing strategies have primarily focused on batch process scheduling or geo-distributed load balancing. However, such approaches are not applicable to services that require constant availability at specific locations due to latency, privacy, data, or infrastructure constraints.","authors":["Philipp Wiesner","Dennis Grinwald","Philipp Wei{\\ss}","Patrick Wilhelm","Ramin Khalili","Odej Kao"],"url":"https://arxiv.org/abs/2411.19058"}
{"created":"2025-05-13","title":"Opt-In Art: Learning Art Styles Only from Few Examples","abstract":"We explore whether pre-training on datasets with paintings is necessary for a model to learn an artistic style with only a few examples. To investigate this, we train a text-to-image model exclusively on photographs, without access to any painting-related content. We show that it is possible to adapt a model that is trained without paintings to an artistic style, given only few examples. User studies and automatic evaluations confirm that our model (post-adaptation) performs on par with state-of-the-art models trained on massive datasets that contain artistic content like paintings, drawings or illustrations. Finally, using data attribution techniques, we analyze how both artistic and non-artistic datasets contribute to generating artistic-style images. Surprisingly, our findings suggest that high-quality artistic outputs can be achieved without prior exposure to artistic data, indicating that artistic style generation can occur in a controlled, opt-in manner using only a limited, carefully selected set of training examples.","authors":["Hui Ren","Joanna Materzynska","Rohit Gandikota","David Bau","Antonio Torralba"],"url":"https://arxiv.org/abs/2412.00176"}
{"created":"2025-05-13","title":"One-Shot Real-to-Sim via End-to-End Differentiable Simulation and Rendering","abstract":"Identifying predictive world models for robots in novel environments from sparse online observations is essential for robot task planning and execution in novel environments. However, existing methods that leverage differentiable programming to identify world models are incapable of jointly optimizing the geometry, appearance, and physical properties of the scene. In this work, we introduce a novel rigid object representation that allows the joint identification of these properties. Our method employs a novel differentiable point-based geometry representation coupled with a grid-based appearance field, which allows differentiable object collision detection and rendering. Combined with a differentiable physical simulator, we achieve end-to-end optimization of world models, given the sparse visual and tactile observations of a physical motion sequence. Through a series of world model identification tasks in simulated and real environments, we show that our method can learn both simulation- and rendering-ready world models from only one robot action sequence. The code and additional videos are available at our project website: https://tianyi20.github.io/rigid-world-model.github.io/","authors":["Yifan Zhu","Tianyi Xiang","Aaron Dollar","Zherong Pan"],"url":"https://arxiv.org/abs/2412.00259"}
{"created":"2025-05-13","title":"MambaNUT: Nighttime UAV Tracking via Mamba-based Adaptive Curriculum Learning","abstract":"Harnessing low-light enhancement and domain adaptation, nighttime UAV tracking has made substantial strides. However, over-reliance on image enhancement, limited high-quality nighttime data, and a lack of integration between daytime and nighttime trackers hinder the development of an end-to-end trainable framework. Additionally, current ViT-based trackers demand heavy computational resources due to their reliance on the self-attention mechanism. In this paper, we propose a novel pure Mamba-based tracking framework (MambaNUT) that employs a state space model with linear complexity as its backbone, incorporating a single-stream architecture that integrates feature learning and template-search coupling within Vision Mamba. We introduce an adaptive curriculum learning (ACL) approach that dynamically adjusts sampling strategies and loss weights, thereby improving the model's ability of generalization. Our ACL is composed of two levels of curriculum schedulers: (1) sampling scheduler that transforms the data distribution from imbalanced to balanced, as well as from easier (daytime) to harder (nighttime) samples; (2) loss scheduler that dynamically assigns weights based on the size of the training set and IoU of individual instances. Exhaustive experiments on multiple nighttime UAV tracking benchmarks demonstrate that the proposed MambaNUT achieves state-of-the-art performance while requiring lower computational costs. The code will be available at https://github.com/wuyou3474/MambaNUT.","authors":["You Wu","Xiangyang Yang","Xucheng Wang","Hengzhou Ye","Dan Zeng","Shuiwang Li"],"url":"https://arxiv.org/abs/2412.00626"}
{"created":"2025-05-13","title":"SerialGen: Personalized Image Generation by First Standardization Then Personalization","abstract":"In this work, we are interested in achieving both high text controllability and whole-body appearance consistency in the generation of personalized human characters. We propose a novel framework, named SerialGen, which is a serial generation method consisting of two stages: first, a standardization stage that standardizes reference images, and then a personalized generation stage based on the standardized reference. Furthermore, we introduce two modules aimed at enhancing the standardization process. Our experimental results validate the proposed framework's ability to produce personalized images that faithfully recover the reference image's whole-body appearance while accurately responding to a wide range of text prompts. Through thorough analysis, we highlight the critical contribution of the proposed serial generation method and standardization model, evidencing enhancements in appearance consistency between reference and output images and across serial outputs generated from diverse text prompts. The term \"Serial\" in this work carries a double meaning: it refers to the two-stage method and also underlines our ability to generate serial images with consistent appearance throughout.","authors":["Cong Xie","Han Zou","Ruiqi Yu","Yan Zhang","Zhenpeng Zhan"],"url":"https://arxiv.org/abs/2412.01485"}
{"created":"2025-05-13","title":"Beyond Text-Visual Attention: Exploiting Visual Cues for Effective Token Pruning in VLMs","abstract":"Large vision-language models (LVLMs) generally contain significantly more visual tokens than their textual counterparts, resulting in a considerable computational burden. Recent efforts have been made to tackle this issue by pruning visual tokens early within the language model. Most existing works use attention scores between text and visual tokens to assess the importance of visual tokens. However, in this study, we first analyze the text-visual attention in the language model and find that this score is not an ideal indicator for token pruning. Based on the analysis, We propose VisPruner, a plug-and-play method that utilizes visual cues for more effective token pruning in LVLMs. Specifically, we first use visual attention to select a limited number of significant tokens. Then, we remove duplicate tokens from the remaining ones based on their similarity. By retaining diverse tokens alongside the initially selected important tokens, we maximally preserve the visual information of the input image. Experimental results demonstrate that our VisPruner sustains strong performance across various VLM architectures and reduction ratios, significantly outperforming existing methods based on text-visual attention. Notably, without any training, VisPruner can reduce the FLOPs of LLaVA-1.5-7B by 91% and inference latency by 75%, while maintaining comparable performance. Our code is available at https://github.com/Theia-4869/VisPruner.","authors":["Qizhe Zhang","Aosong Cheng","Ming Lu","Renrui Zhang","Zhiyong Zhuo","Jiajun Cao","Shaobo Guo","Qi She","Shanghang Zhang"],"url":"https://arxiv.org/abs/2412.01818"}
{"created":"2025-05-13","title":"MOANA: Multi-Radar Dataset for Maritime Odometry and Autonomous Navigation Application","abstract":"Maritime environmental sensing requires overcoming challenges from complex conditions such as harsh weather, platform perturbations, large dynamic objects, and the requirement for long detection ranges. While cameras and LiDAR are commonly used in ground vehicle navigation, their applicability in maritime settings is limited by range constraints and hardware maintenance issues. Radar sensors, however, offer robust long-range detection capabilities and resilience to physical contamination from weather and saline conditions, making it a powerful sensor for maritime navigation. Among various radar types, X-band radar is widely employed for maritime vessel navigation, providing effective long-range detection essential for situational awareness and collision avoidance. Nevertheless, it exhibits limitations during berthing operations where near-field detection is critical. To address this shortcoming, we incorporate W-band radar, which excels in detecting nearby objects with a higher update rate. We present a comprehensive maritime sensor dataset featuring multi-range detection capabilities. This dataset integrates short-range LiDAR data, medium-range W-band radar data, and long-range X-band radar data into a unified framework. Additionally, it includes object labels for oceanic object detection usage, derived from radar and stereo camera images. The dataset comprises seven sequences collected from diverse regions with varying levels of \\bl{navigation algorithm} estimation difficulty, ranging from easy to challenging, and includes common locations suitable for global localization tasks. This dataset serves as a valuable resource for advancing research in place recognition, odometry estimation, SLAM, object detection, and dynamic object elimination within maritime environments. Dataset can be found at https://sites.google.com/view/rpmmoana.","authors":["Hyesu Jang","Wooseong Yang","Hanguen Kim","Dongje Lee","Yongjin Kim","Jinbum Park","Minsoo Jeon","Jaeseong Koh","Yejin Kang","Minwoo Jung","Sangwoo Jung","Chng Zhen Hao","Wong Yu Hin","Chew Yihang","Ayoung Kim"],"url":"https://arxiv.org/abs/2412.03887"}
{"created":"2025-05-13","title":"M2PDE: Compositional Generative Multiphysics and Multi-component PDE Simulation","abstract":"Multiphysics simulation, which models the interactions between multiple physical processes, and multi-component simulation of complex structures are critical in fields like nuclear and aerospace engineering. Previous studies use numerical solvers or ML-based surrogate models for these simulations. However, multiphysics simulations typically require integrating multiple specialized solvers-each for a specific physical process-into a coupled program, which introduces significant development challenges. Furthermore, existing numerical algorithms struggle with highly complex large-scale structures in multi-component simulations. Here we propose compositional Multiphysics and Multi-component PDE Simulation with Diffusion models (M2PDE) to overcome these challenges. During diffusion-based training, M2PDE learns energy functions modeling the conditional probability of one physical process/component conditioned on other processes/components. In inference, M2PDE generates coupled multiphysics and multi-component solutions by sampling from the joint probability distribution. We evaluate M2PDE on two multiphysics tasks-reaction-diffusion and nuclear thermal coupling-where it achieves more accurate predictions than surrogate models in challenging scenarios. We then apply it to a multi-component prismatic fuel element problem, demonstrating that M2PDE scales from single-component training to a 64-component structure and outperforms existing domain-decomposition and graph-based approaches. The code is available at https://github.com/AI4Science-WestlakeU/M2PDE.","authors":["Tao Zhang","Zhenhai Liu","Feipeng Qi","Yongjun Jiao","Tailin Wu"],"url":"https://arxiv.org/abs/2412.04134"}
{"created":"2025-05-13","title":"Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation","abstract":"Large Language Models (LLM) are usually fine-tuned to participate in dyadic or two-party dialogues, which can not adapt well to multi-party dialogues (MPD), which hinders their applications in such scenarios including multi-personal meetings, discussions and daily communication. Previous LLM-based researches mainly focus on the multi-agent framework, while their base LLMs are still pairwisely fine-tuned. In this work, we design a multi-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue datasets, and prove such a straightforward framework can let the LLM align with the multi-party conversation style efficiently and effectively. We also design two training strategies which can convert MuPaS into the MPD simulator. Substantial experiments show that MuPaS can achieve state-of-the-art multi-party response, higher accuracy of the-next-speaker prediction, higher human and automatic evaluated utterance qualities, and can even generate reasonably with out-of-distribution scene, topic and role descriptions. The MuPaS framework bridges the LLM training with more complicated multi-party applications, such as conversation generation, virtual rehearsal or meta-universe.","authors":["Xiaoyu Wang","Ningyuan Xi","Teng Chen","Qingqing Gu","Yue Zhao","Xiaokai Chen","Zhonglin Jiang","Yong Chen","Luo Ji"],"url":"https://arxiv.org/abs/2412.05342"}
{"created":"2025-05-13","title":"A Logic for Paraconsistent Belief Revision based on Epistemic Entrenchment","abstract":"This paper addresses the integration of epistemic entrenchment into paraconsistent belief revision systems based on Logics of Formal Inconsistency (LFIs). While systems like AGMp and AGMo adapt AGM principles to paraconsistency, they lack mechanisms to rank beliefs, primarily due to the absence of properties such as the replacement property in the underlying logics. We introduce two novel logics, Cbr and RCBr, with the latter extending the former to fully address these limitations given that it is self-extensional. Using RCBr, we define contraction operations via epistemic entrenchment, adhering to key rationality principles. Our framework leverages non-deterministic matrix semantics (Nmatrices) and Boolean algebras with LFI operators (BALFIs), providing a robust foundation for paraconsistent reasoning. These contributions advance the theory of paraconsistent belief revision and pave the way for applications in domains such as multi-agent systems and inconsistent knowledge bases.","authors":["Marcelo E. Coniglio","Martin Figallo","Rafael R. Testa"],"url":"https://arxiv.org/abs/2412.06117"}
{"created":"2025-05-13","title":"Stereo Hand-Object Reconstruction for Human-to-Robot Handover","abstract":"Jointly estimating hand and object shape facilitates the grasping task in human-to-robot handovers. However, relying on hand-crafted prior knowledge about the geometric structure of the object fails when generalising to unseen objects, and depth sensors fail to detect transparent objects such as drinking glasses. In this work, we propose a stereo-based method for hand-object reconstruction that combines single-view reconstructions probabilistically to form a coherent stereo reconstruction. We learn 3D shape priors from a large synthetic hand-object dataset to ensure that our method is generalisable, and use RGB inputs to better capture transparent objects. We show that our method reduces the object Chamfer distance compared to existing RGB based hand-object reconstruction methods on single view and stereo settings. We process the reconstructed hand-object shape with a projection-based outlier removal step and use the output to guide a human-to-robot handover pipeline with wide-baseline stereo RGB cameras. Our hand-object reconstruction enables a robot to successfully receive a diverse range of household objects from the human.","authors":["Yik Lung Pang","Alessio Xompero","Changjae Oh","Andrea Cavallaro"],"url":"https://arxiv.org/abs/2412.07487"}
{"created":"2025-05-13","title":"Geometric low-rank approximation of the Zeitlin model of incompressible fluids on the sphere","abstract":"We consider the vorticity formulation of the Euler equations describing the flow of a two-dimensional incompressible ideal fluid on the sphere. Zeitlin's model provides a finite-dimensional approximation of the vorticity formulation that preserves the underlying geometric structure: it consists of an isospectral Lie--Poisson flow on the Lie algebra of skew-Hermitian matrices. We propose an approximation of Zeitlin's model based on a time-dependent low-rank factorization of the vorticity matrix and evolve a basis of eigenvectors according to the Euler equations. In particular, we show that the approximate flow remains isospectral and Lie--Poisson and that the error in the solution, in the approximation of the Hamiltonian and of the Casimir functions only depends on the approximation of the vorticity matrix at the initial time. The computational complexity of solving the approximate model is shown to scale quadratically with the order of the vorticity matrix and linearly if a further approximation of the stream function is introduced.","authors":["Cecilia Pagliantini"],"url":"https://arxiv.org/abs/2412.08182"}
{"created":"2025-05-13","title":"GradStop: Exploring Training Dynamics in Unsupervised Outlier Detection through Gradient","abstract":"Unsupervised Outlier Detection (UOD) is a critical task in data mining and machine learning, aiming to identify instances that significantly deviate from the majority. Without any label, deep UOD methods struggle with the misalignment between the model's direct optimization goal and the final performance goal of Outlier Detection (OD) task. Through the perspective of training dynamics, this paper proposes an early stopping algorithm to optimize the training of deep UOD models, ensuring they perform optimally in OD rather than overfitting the entire contaminated dataset.","authors":["Yuang Zhang","Liping Wang","Yihong Huang","Yuanxing Zheng","Fan Zhang","Xuemin Lin"],"url":"https://arxiv.org/abs/2412.08501"}
{"created":"2025-05-13","title":"Advancing Single and Multi-task Text Classification through Large Language Model Fine-tuning","abstract":"Both encoder-only models (e.g., BERT, RoBERTa) and large language models (LLMs, e.g., Llama3) have been widely used for text classification tasks. However, there is a lack of systematic studies comparing the performance of encoder-based models and LLMs in text classification, particularly when fine-tuning is involved. This study employed a diverse range of models and methods, varying in size and architecture, and including both fine-tuned and pre-trained approaches. We first assessed the performances of these LLMs on the 20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only RoBERTa models. Additionally, we explored the multi-task capabilities of both model types by combining multiple classification tasks, including intent detection and slot-filling, into a single model using data from both datasets. Our results indicate that fully fine-tuned Llama3-70B models outperform RoBERTa-large and other decoder LLMs across various classification tasks and datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the performance of dual-model setups in both tasks across both datasets. Overall, our study provides a comprehensive benchmark of encoder-only and LLM models on text classification tasks and demonstrates a method to combine two or more fully fine-tuned decoder LLMs for reduced latency and equivalent performance.","authors":["Hang Zhao","Qile P. Chen","Yijing Barry Zhang","Gang Yang"],"url":"https://arxiv.org/abs/2412.08587"}
{"created":"2025-05-13","title":"Efficient and Comprehensive Feature Extraction in Large Vision-Language Model for Clinical Pathology Analysis","abstract":"Pathological diagnosis is vital for determining disease characteristics, guiding treatment, and assessing prognosis, relying heavily on detailed, multi-scale analysis of high-resolution whole slide images (WSI). However, traditional pure vision models face challenges of redundant feature extraction, whereas existing large vision-language models (LVLMs) are limited by input resolution constraints, hindering their efficiency and accuracy. To overcome these issues, we propose two innovative strategies: the mixed task-guided feature enhancement, which directs feature extraction toward lesion-related details across scales, and the prompt-guided detail feature completion, which integrates coarse- and fine-grained features from WSI based on specific prompts without compromising inference speed. Leveraging a comprehensive dataset of 490,000 samples from diverse pathology tasks-including cancer detection, grading, vascular and neural invasion identification, and so on-we trained the pathology-specialized LVLM, OmniPath. Extensive experiments demonstrate that this model significantly outperforms existing methods in diagnostic accuracy and efficiency, offering an interactive, clinically aligned approach for auxiliary diagnosis in a wide range of pathology applications.","authors":["Shengxuming Zhang","Weihan Li","Tianhong Gao","Jiacong Hu","Haoming Luo","Xiuming Zhang","Jing Zhang","Mingli Song","Zunlei Feng"],"url":"https://arxiv.org/abs/2412.09521"}
{"created":"2025-05-13","title":"A Clinical Tuning Framework for Continuous Kinematic and Impedance Control of a Powered Knee-Ankle Prosthesis","abstract":"Configuring a prosthetic leg is an integral part of the fitting process, but the personalization of a multi-modal powered knee-ankle prosthesis is often too complex to realize in a clinical environment. This paper develops both the technical means to individualize a hybrid kinematic-impedance controller for variable-incline walking and sit-stand transitions, and an intuitive Clinical Tuning Interface (CTI) that allows prosthetists to directly modify the controller behavior. Utilizing an established method for predicting kinematic gait individuality alongside a new parallel approach for kinetic individuality, we personalize continuous-phase/task models of joint impedance (during stance) and kinematics (during swing) using tuned characteristics exclusively from level-ground walking. To take advantage of this method, we developed a CTI that translates common clinical tuning parameters into model adjustments for the walking and sit-stand controllers. We then conducted a case study where a prosthetist iteratively tuned the powered prosthesis to an above-knee amputee participant in a simulated clinical session involving sit-stand transitions and level walking, from which incline/decline walking features were automatically calibrated. The prosthetist fully tuned the multi-activity prosthesis controller in under 20 min. Each iteration of tuning (i.e., observation, parameter adjustment, and model reprocessing) took 2 min on average for walking and 1 min on average for sit-stand. The tuned behavior changes were appropriately manifested in the commanded prosthesis torques, both at the manually tuned tasks and automatically tuned tasks (inclines). This paper introduces a clinical tuning interface that simplifies the tuning process for multimodal robotic prosthetic legs, reducing the time required from several hours to just 20 min thus improving clinical feasibility.","authors":["Emma Reznick","T. Kevin Best","Robert Gregg"],"url":"https://arxiv.org/abs/2412.10154"}
{"created":"2025-05-13","title":"A Comparative Study on Dynamic Graph Embedding based on Mamba and Transformers","abstract":"Dynamic graph embedding has emerged as an important technique for modeling complex time-evolving networks across diverse domains. While transformer-based models have shown promise in capturing long-range dependencies in temporal graph data, they face scalability challenges due to quadratic computational complexity. This study presents a comparative analysis of dynamic graph embedding approaches using transformers and the recently proposed Mamba architecture, a state-space model with linear complexity. We introduce three novel models: TransformerG2G augment with graph convolutional networks, \\mathcal{DG}-Mamba, and \\mathcal{GDG}-Mamba with graph isomorphism network edge convolutions. Our experiments on multiple benchmark datasets demonstrate that Mamba-based models achieve comparable or superior performance to transformer-based approaches in link prediction tasks while offering significant computational efficiency gains on longer sequences. Notably, \\mathcal{DG}-Mamba variants consistently outperform transformer-based models on datasets with high temporal variability, such as UCI, Bitcoin, and Reality Mining, while maintaining competitive performance on more stable graphs like SBM. We provide insights into the learned temporal dependencies through analysis of attention weights and state matrices, revealing the models' ability to capture complex temporal patterns. By effectively combining state-space models with graph neural networks, our work addresses key limitations of previous approaches and contributes to the growing body of research on efficient temporal graph representation learning. These findings offer promising directions for scaling dynamic graph embedding to larger, more complex real-world networks, potentially enabling new applications in areas such as social network analysis, financial modeling, and biological system dynamics.","authors":["Ashish Parmanand Pandey","Alan John Varghese","Sarang Patil","Mengjia Xu"],"url":"https://arxiv.org/abs/2412.11293"}
{"created":"2025-05-13","title":"Multi-QuAD: Multi-Level Quality-Adaptive Dynamic Network for Reliable Multimodal Classification","abstract":"Multimodal machine learning has achieved remarkable progress in many scenarios, but its reliability is undermined by varying sample quality. This paper finds that existing reliable multimodal classification methods not only fail to provide robust estimation of data quality, but also lack dynamic networks for sample-specific depth and parameters to achieve reliable inference. To this end, a novel framework for multimodal reliable classification termed \\textit{Multi-level Quality-Adaptive Dynamic multimodal network} (Multi-QuAD) is proposed. Multi-QuAD first adopts a novel approach based on noise-free prototypes and a classifier-free design to reliably estimate the quality of each sample at both modality and feature levels. It then achieves sample-specific network depth via the \\textbf{\\textit{Global Confidence Normalized Depth (GCND)}} mechanism. By normalizing depth across modalities and samples, \\textit{\\textbf{GCND}} effectively mitigates the impact of challenging modality inputs on dynamic depth reliability. Furthermore, Multi-QuAD provides sample-adaptive network parameters via the \\textbf{\\textit{Layer-wise Greedy Parameter (LGP)}} mechanism driven by feature-level quality. The cross-modality layer-wise greedy strategy in \\textbf{\\textit{LGP}} designs a reliable parameter prediction paradigm for multimodal networks with variable architecture for the first time. Experiments conducted on four datasets demonstrate that Multi-QuAD significantly outperforms state-of-the-art methods in classification performance and reliability, exhibiting strong adaptability to data with diverse quality.","authors":["Shu Shen","C. L. Philip Chen","Tong Zhang"],"url":"https://arxiv.org/abs/2412.14489"}
{"created":"2025-05-13","title":"Beyond Partisan Leaning: A Comparative Analysis of Political Bias in Large Language Models","abstract":"As large language models (LLMs) become increasingly embedded in civic, educational, and political information environments, concerns about their potential political bias have grown. Prior research often evaluates such bias through simulated personas or predefined ideological typologies, which may introduce artificial framing effects or overlook how models behave in general use scenarios. This study adopts a persona-free, topic-specific approach to evaluate political behavior in LLMs, reflecting how users typically interact with these systems-without ideological role-play or conditioning. We introduce a two-dimensional framework: one axis captures partisan orientation on highly polarized topics (e.g., abortion, immigration), and the other assesses sociopolitical engagement on less polarized issues (e.g., climate change, foreign policy). Using survey-style prompts drawn from the ANES and Pew Research Center, we analyze responses from 43 LLMs developed in the U.S., Europe, China, and the Middle East. We propose an entropy-weighted bias score to quantify both the direction and consistency of partisan alignment, and identify four behavioral clusters through engagement profiles. Findings show most models lean center-left or left ideologically and vary in their nonpartisan engagement patterns. Model scale and openness are not strong predictors of behavior, suggesting that alignment strategy and institutional context play a more decisive role in shaping political expression.","authors":["Tai-Quan Peng","Kaiqi Yang","Sanguk Lee","Hang Li","Yucheng Chu","Yuping Lin","Hui Liu"],"url":"https://arxiv.org/abs/2412.16746"}
{"created":"2025-05-13","title":"Hybrid Local Causal Discovery","abstract":"Local causal discovery aims to learn and distinguish the direct causes and effects of a target variable from observed data. Existing constraint-based local causal discovery methods use AND or OR rules in constructing the local causal skeleton, but using either rule alone is prone to produce cascading errors in the learned local causal skeleton, and thus impacting the inference of local causal relationships. On the other hand, directly applying score-based global causal discovery methods to local causal discovery may randomly return incorrect results due to the existence of local equivalence classes. To address the above issues, we propose a Hybrid Local Causal Discovery algorithm, called HLCD. Specifically, HLCD initially utilizes a constraint-based approach combined with the OR rule to obtain a candidate skeleton and then employs a score-based method to eliminate redundant portions in the candidate skeleton. Furthermore, during the local causal orientation phase, HLCD distinguishes between V-structures and equivalence classes by comparing the local structure scores between the two, thereby avoiding orientation interference caused by local equivalence classes. We conducted extensive experiments with seven state-of-the-art competitors on 14 benchmark Bayesian network datasets, and the experimental results demonstrate that HLCD significantly outperforms existing local causal discovery algorithms.","authors":["Zhaolong Ling","Honghui Peng","Yiwen Zhang","Debo Cheng","Xingyu Wu","Peng Zhou","Kui Yu"],"url":"https://arxiv.org/abs/2412.19507"}
{"created":"2025-05-13","title":"ATiM: Autotuning Tensor Programs for Processing-in-DRAM","abstract":"Processing-in-DRAM (DRAM-PIM) has emerged as a promising technology for accelerating memory-intensive operations in modern applications, such as Large Language Models (LLMs). Despite its potential, current software stacks for DRAM-PIM face significant challenges, including reliance on hand-tuned libraries that hinder programmability, limited support for high-level abstractions, and the lack of systematic optimization frameworks. To address these limitations, we present ATiM, a search-based optimizing tensor compiler for UPMEM. Key features of ATiM include: (1) automated searches of the joint search space for host and kernel tensor programs, (2) PIM-aware optimizations for efficiently handling boundary conditions, and (3) improved search algorithms for the expanded search space of UPMEM systems. Our experimental results on UPMEM hardware demonstrate performance gains of up to 6.18$\\times$ for various UPMEM benchmark kernels and 8.21$\\times$ for GPT-J layers. To the best of our knowledge, ATiM is the first tensor compiler to provide fully automated, autotuning-integrated code generation support for a DRAM-PIM system. By bridging the gap between high-level tensor computation abstractions and low-level hardware-specific requirements, ATiM establishes a foundation for advancing DRAM-PIM programmability and enabling streamlined optimization.","authors":["Yongwon Shin","Dookyung Kang","Hyojin Sung"],"url":"https://arxiv.org/abs/2412.19630"}
{"created":"2025-05-13","title":"FreStega: A Plug-and-Play Method for Boosting Imperceptibility and Capacity in Generative Linguistic Steganography for Real-World Scenarios","abstract":"Linguistic steganography embeds secret information in seemingly innocent texts, safeguarding privacy in surveillance environments. Generative linguistic steganography leverages the probability distribution of language models (LMs) and applies steganographic algorithms to generate stego tokens, gaining attention with recent Large Language Model (LLM) advancements. To enhance security, researchers develop distribution-preserving stego algorithms to minimize the gap between stego sampling and LM sampling. However, the reliance on language model distributions, coupled with deviations from real-world cover texts, results in insufficient imperceptibility when facing steganalysis detectors in real-world scenarios. Moreover, LLM distributions tend to be more deterministic, resulting in reduced entropy and, consequently, lower embedding capacity. In this paper, we propose FreStega, a plug-and-play method to reconstruct the distribution of language models used for generative linguistic steganography. FreStega dynamically adjusts token probabilities from the language model at each step of stegotext auto-regressive generation, leveraging both sequential and spatial dimensions. In sequential adjustment, the temperature is dynamically adjusted based on instantaneous entropy, enhancing the diversity of stego texts and boosting embedding capacity. In the spatial dimension, the distribution is aligned with guidance from the target domain corpus, closely mimicking real cover text in the target domain. By reforming the distribution, FreStega enhances the imperceptibility of stego text in practical scenarios and improves steganographic capacity by 15.41\\%, all without compromising the quality of the generated text. FreStega serves as a plug-and-play remedy to enhance the imperceptibility and embedding capacity of existing distribution-preserving steganography methods in real-world scenarios.","authors":["Kaiyi Pang"],"url":"https://arxiv.org/abs/2412.19652"}
{"created":"2025-05-13","title":"Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain","abstract":"Retrieval Augmented Generation (RAG) complements the knowledge of Large Language Models (LLMs) by leveraging external information to enhance response accuracy for queries. This approach is widely applied in several fields by taking its advantage of injecting the most up-to-date information, and researchers are focusing on understanding and improving this aspect to unlock the full potential of RAG in such high-stakes applications. However, despite the potential of RAG to address these needs, the mechanisms behind the confidence levels of its outputs remain underexplored, although the confidence of information is very critical in some domains, such as finance, healthcare, and medicine. Our study focuses the impact of RAG on confidence within the medical domain under various configurations and models. We evaluate confidence by treating the model's predicted probability as its output and calculating Expected Calibration Error (ECE) and Adaptive Calibration Error (ACE) scores based on the probabilities and accuracy. In addition, we analyze whether the order of retrieved documents within prompts calibrates the confidence. Our findings reveal large variation in confidence and accuracy depending on the model, settings, and the format of input prompts. These results underscore the necessity of optimizing configurations based on the specific model and conditions.","authors":["Shintaro Ozaki","Yuta Kato","Siyuan Feng","Masayo Tomita","Kazuki Hayashi","Wataru Hashimoto","Ryoma Obara","Masafumi Oyamada","Katsuhiko Hayashi","Hidetaka Kamigaito","Taro Watanabe"],"url":"https://arxiv.org/abs/2412.20309"}
{"created":"2025-05-13","title":"FusionSORT: Fusion Methods for Online Multi-object Visual Tracking","abstract":"In this work, we investigate four different fusion methods for associating detections to tracklets in multi-object visual tracking. In addition to considering strong cues such as motion and appearance information, we also consider weak cues such as height intersection-over-union (height-IoU) and tracklet confidence information in the data association using different fusion methods. These fusion methods include minimum, weighted sum based on IoU, Kalman filter (KF) gating, and hadamard product of costs due to the different cues. We conduct extensive evaluations on validation sets of MOT17, MOT20 and DanceTrack datasets, and find out that the choice of a fusion method is key for data association in multi-object visual tracking. We hope that this investigative work helps the computer vision research community to use the right fusion method for data association in multi-object visual tracking.","authors":["Nathanael L. Baisa"],"url":"https://arxiv.org/abs/2501.00843"}
{"created":"2025-05-13","title":"Data-Driven Yet Formal Policy Synthesis for Stochastic Nonlinear Dynamical Systems","abstract":"The automated synthesis of control policies for stochastic dynamical systems presents significant challenges. A standard approach is to construct a finite-state abstraction of the continuous system, typically represented as a Markov decision process (MDP). However, generating abstractions is challenging when (1) the system's dynamics are nonlinear, and/or (2) we do not have complete knowledge of the dynamics. In this work, we introduce a novel data-driven abstraction technique for nonlinear Lipschitz continuous dynamical systems with additive stochastic noise that addresses both of these issues. As a key step, we use samples of the dynamics to learn the enabled actions and transition probabilities of the abstraction. We represent abstractions as MDPs with intervals of transition probabilities, known as interval MDPs (IMDPs). These abstractions enable the synthesis of policies for the concrete nonlinear system, with probably approximately correct (PAC) guarantees on the probability of satisfying a specified control objective. Our numerical experiments illustrate the effectiveness and robustness of our approach in achieving reliable control under uncertainty.","authors":["Mahdi Nazeri","Thom Badings","Sadegh Soudjani","Alessandro Abate"],"url":"https://arxiv.org/abs/2501.01191"}
{"created":"2025-05-13","title":"MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments","abstract":"Large Language Models (LLMs) have shown remarkable capabilities in environmental perception, reasoning-based decision-making, and simulating complex human behaviors, particularly in interactive role-playing contexts. This paper introduces the Multiverse Interactive Role-play Ability General Evaluation (MIRAGE), a comprehensive framework designed to assess LLMs' proficiency in portraying advanced human behaviors through murder mystery games. MIRAGE features eight intricately crafted scripts encompassing diverse themes and styles, providing a rich simulation. To evaluate LLMs' performance, MIRAGE employs four distinct methods: the Trust Inclination Index (TII) to measure dynamics of trust and suspicion, the Clue Investigation Capability (CIC) to measure LLMs' capability of conducting information, the Interactivity Capability Index (ICI) to assess role-playing capabilities and the Script Compliance Index (SCI) to assess LLMs' capability of understanding and following instructions. Our experiments indicate that even popular models like GPT-4 face significant challenges in navigating the complexities presented by the MIRAGE. The datasets and simulation codes are available in \\href{https://github.com/lime728/MIRAGE}{github}.","authors":["Yin Cai","Zhouhong Gu","Zhaohan Du","Zheyu Ye","Shaosheng Cao","Yiqian Xu","Hongwei Feng","Ping Chen"],"url":"https://arxiv.org/abs/2501.01652"}
{"created":"2025-05-13","title":"Connecting the Unconnectable through Feedback","abstract":"Reliable uplink connectivity remains a persistent challenge for IoT devices, particularly those at the cell edge, due to their limited transmit power and single-antenna configurations. This paper introduces a novel framework aimed at connecting the unconnectable, leveraging real-time feedback from access points (APs) to enhance uplink coverage without increasing the energy consumption of IoT devices. At the core of this approach are feedback channel codes, which enable IoT devices to dynamically adapt their transmission strategies based on AP decoding feedback, thereby reducing the critical uplink SNR required for successful communication. Analytical models are developed to quantify the coverage probability and the number of connectable APs, providing a comprehensive understanding of the system's performance. Numerical results validate the proposed method, demonstrating substantial improvements in coverage range and connectivity, particularly for devices at the cell edge, with up to a 51% boost in connectable APs. Our approach offers a robust and energy-efficient solution to overcoming uplink coverage limitations, enabling IoT networks to connect devices in challenging environments.","authors":["Yimeng Li","Yulin Shao"],"url":"https://arxiv.org/abs/2501.02335"}
{"created":"2025-05-13","title":"Privacy-Preserving and Simultaneous Authentication in High-Density V2X Networks","abstract":"The rapid expansion of Vehicle-to-Everything (V2X) networks within the Internet of Vehicles (IoV) demands secure and efficient authentication to support high-speed, high-density and mobility-challenged environments. This paper presents a privacy-preserving authentication scheme that incorporates batch authentication, mutual authentication, and secure key establishment, enabling users to authenticate one another without a central authority. Our proposed scheme facilitates simultaneous multi-user authentication, significantly enhancing scalability, robustness and security in dynamic IoV networks. Results from realistic implementations show that our method achieves average authentication and verification times of 10.61 ms and 1.78 ms, respectively, for a fleet of 100 vehicles, outperforming existing methods. Scalability tests demonstrate efficient processing for larger groups of up to 500 vehicles, where average authentication times remain low, establishing our scheme as a robust solution for secure communication in IoV systems.","authors":["Morteza Azmoudeh Afshar","Nesrine Benchoubane","Busra Cayoren","Gunes Karabulut Kurt","Enver Ozdemir"],"url":"https://arxiv.org/abs/2501.06087"}
{"created":"2025-05-13","title":"Nitsche's method under a semi-regular mesh condition","abstract":"Nitsche's method is a numerical approach that weakly enforces boundary conditions for partial differential equations. In recent years, Nitsche's method has experienced a revival owing to its natural application in modern computational methods, such as the cut and immersed finite element methods. This study investigates Nitsche's methods based on an anisotropic weakly over-penalized symmetric interior penalty method for Poisson and Stokes equations on convex domains. As our primary contribution, we provide a new proof for the consistency term, which allows us to obtain an estimate of the anisotropic consistency error. The key idea of the proof is to apply the relationship between the Crouzeix and Raviart finite element space and the Raviart--Thomas finite element space. We present the error estimates in the energy norm on anisotropic meshes. We compared the calculation results for the anisotropic mesh partitions in the numerical experiments.","authors":["Hiroki Ishizaka"],"url":"https://arxiv.org/abs/2501.06824"}
{"created":"2025-05-13","title":"Next-Gen Space-Based Surveillance: Blockchain for Trusted and Efficient Debris Tracking","abstract":"The increasing congestion of Earth's orbit due to growing satellite deployments and space debris poses a significant challenge to sustainable space operations. Traditional space surveillance systems rely on centralized architectures, which introduce single points of failure and scalability constraints. This paper proposes a blockchain-based solution where satellites function as nodes with distinct roles to validate and securely store debris-tracking data. Simulation results indicate that optimal network performance is achieved with approximately 30 nodes, balancing throughput and response time, representing an approximately 9x improvement over traditional consensus mechanisms.","authors":["Nesrine Benchoubane","Nida Fidan","Gunes Karabulut Kurt","Enver Ozdemir"],"url":"https://arxiv.org/abs/2501.06970"}
{"created":"2025-05-13","title":"Analysis of Power Losses and the Efficacy of Power Minimization Strategies in Multichannel Electrical Stimulation Systems","abstract":"Neuroprosthetic devices require multichannel stimulator systems with an increasing number of channels. However, there are inherent power losses in typical multichannel stimulation circuits caused by a mismatch between the power supply voltage and the voltage required at each electrode to successfully stimulate tissue. This imposes a bottleneck towards high-channel-count devices, which is particularly severe in wirelessly-powered devices. Hence, advances in the power efficiency of stimulation systems are critical. To support these advances, this paper presents a methodology to identify and quantify power losses associated with different power supply scaling strategies in multichannel stimulation systems. The proposed methodology utilizes distributions of stimulation amplitudes and electrode impedances to calculate power losses in multichannel systems. Experimental data from previously published studies spanning various stimulation applications were analyzed to evaluate the performance of fixed, global, and stepped supply scaling methods, focusing on their impact on power dissipation and efficiency. Variability in output conditions results in low power efficiency in multichannel stimulation systems across all applications. Stepped voltage scaling demonstrated substantial efficiency improvements, achieving an increase of 67 % to 146 %, particularly in high-channel-count applications with significant variability in tissue impedance. Global scaling, by contrast, was more advantageous for systems with fewer channels. The findings highlight the importance of tailoring power management strategies to specific applications to optimize efficiency while minimizing system complexity. The proposed methodology offers a framework for evaluating efficiency-complexity trade-offs, advancing the design of scalable neurostimulation systems.","authors":["Francesc Varkevisser","Wouter A. Serdijn","Tiago L. Costa"],"url":"https://arxiv.org/abs/2501.08025"}
{"created":"2025-05-13","title":"Is Stochastic Gradient Descent Effective? A PDE Perspective on Machine Learning processes","abstract":"In this paper we analyze the behaviour of the stochastic gradient descent (SGD), a widely used method in supervised learning for optimizing neural network weights via a minimization of non-convex loss functions. Since the pioneering work of E, Li and Tai (2017), the underlying structure of such processes can be understood via parabolic PDEs of Fokker-Planck type, which are at the core of our analysis. Even if Fokker-Planck equations have a long history and a extensive literature, almost nothing is known when the potential is non-convex or when the diffusion matrix is degenerate, and this is the main difficulty that we face in our analysis.","authors":["Davide Barbieri","Matteo Bonforte","Peio Ibarrondo"],"url":"https://arxiv.org/abs/2501.08425"}
{"created":"2025-05-13","title":"Robust Trimmed Multipatch IGA with Singular Maps","abstract":"We consider elliptic problems in multipatch isogeometric analysis (IGA) where the patch parameterizations may be singular. Specifically, we address cases where certain dimensions of the parametric geometry diminish as the singularity is approached - for example, a curve collapsing into a point (in 2D), or a surface collapsing into a point or a curve (in 3D). To deal with this issue, we develop a robust weak formulation for the second-order Laplace equation that allows trimmed (cut) elements, enforces interface and Dirichlet conditions weakly, and does not depend on specially constructed approximation spaces. Our technique for dealing with the singular maps is based on the regularization of the Riemannian metric tensor, and we detail how to implement this robustly. We investigate the method's behavior when applied to a square-to-cusp parameterization that allows us to vary the singular behavior's aggressiveness in how quickly the measure tends to zero when the singularity is approached. We propose a scaling of the regularization parameter to obtain optimal order approximation. Our numerical experiments indicate that the method is robust also for quite aggressive singular parameterizations.","authors":["Tobias Jonsson","Mats G. Larson","Karl Larsson"],"url":"https://arxiv.org/abs/2501.08749"}
{"created":"2025-05-13","title":"Fun-tuning: Characterizing the Vulnerability of Proprietary LLMs to Optimization-based Prompt Injection Attacks via the Fine-Tuning Interface","abstract":"We surface a new threat to closed-weight Large Language Models (LLMs) that enables an attacker to compute optimization-based prompt injections. Specifically, we characterize how an attacker can leverage the loss-like information returned from the remote fine-tuning interface to guide the search for adversarial prompts. The fine-tuning interface is hosted by an LLM vendor and allows developers to fine-tune LLMs for their tasks, thus providing utility, but also exposes enough information for an attacker to compute adversarial prompts. Through an experimental analysis, we characterize the loss-like values returned by the Gemini fine-tuning API and demonstrate that they provide a useful signal for discrete optimization of adversarial prompts using a greedy search algorithm. Using the PurpleLlama prompt injection benchmark, we demonstrate attack success rates between 65% and 82% on Google's Gemini family of LLMs. These attacks exploit the classic utility-security tradeoff - the fine-tuning interface provides a useful feature for developers but also exposes the LLMs to powerful attacks.","authors":["Andrey Labunets","Nishit V. Pandya","Ashish Hooda","Xiaohan Fu","Earlence Fernandes"],"url":"https://arxiv.org/abs/2501.09798"}
{"created":"2025-05-13","title":"Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective","abstract":"Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.","authors":["Yiyao Yu","Yuxiang Zhang","Dongdong Zhang","Xiao Liang","Hengyuan Zhang","Xingxing Zhang","Mahmoud Khademi","Hany Awadalla","Junjie Wang","Yujiu Yang","Furu Wei"],"url":"https://arxiv.org/abs/2501.11110"}
{"created":"2025-05-13","title":"Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models","abstract":"Large language models have achieved remarkable success in recent years, primarily due to the implementation of self-attention mechanisms. However, traditional Softmax attention suffers from numerical instability and reduced performance as the length of inference tokens increases. This paper addresses these issues by decomposing the Softmax operation into a non-linear transformation and the $l_1$-norm. We identify the latter as essential for maintaining model performance. By replacing the non-linear transformation with the Softplus activation function and introducing a dynamic scale factor for different token lengths based on invariance entropy, we create a novel attention mechanism with performance better than conventional Softmax attention across various inference lengths. To further improve the length extrapolation ability of the proposed attention mechanism, we introduce a novel re-weighting mechanism that amplifies significant attention weights while diminishing weaker ones, enabling the model to concentrate more effectively on relevant tokens. When combined with our proposed attention mechanism, this approach maintains nearly constant validation loss even at 16$\\times$ the training token length, ensures numerical stability, and achieves superior results on downstream benchmarks.","authors":["Bo Gao","Michael W. Spratling"],"url":"https://arxiv.org/abs/2501.13428"}
{"created":"2025-05-13","title":"Amortized Safe Active Learning for Real-Time Data Acquisition: Pretrained Neural Policies from Simulated Nonparametric Functions","abstract":"Safe active learning (AL) is a sequential scheme for learning unknown systems while respecting safety constraints during data acquisition. Existing methods often rely on Gaussian processes (GPs) to model the task and safety constraints, requiring repeated GP updates and constrained acquisition optimization-incurring in significant computations which are challenging for real-time decision-making. We propose an amortized safe AL framework that replaces expensive online computations with a pretrained neural policy. Inspired by recent advances in amortized Bayesian experimental design, we turn GPs into a pretraining simulator. We train our policy prior to the AL deployment on simulated nonparametric functions, using Fourier feature-based GP sampling and a differentiable, safety-aware acquisition objective. At deployment, our policy selects safe and informative queries via a single forward pass, eliminating the need for GP inference or constrained optimization. This leads to substantial speed improvements while preserving safety and learning quality. Our framework is modular and can be adapted to unconstrained, time-sensitive AL tasks by omitting the safety requirement.","authors":["Cen-You Li","Marc Toussaint","Barbara Rakitsch","Christoph Zimmer"],"url":"https://arxiv.org/abs/2501.15458"}
{"created":"2025-05-13","title":"Mamba-Based Graph Convolutional Networks: Tackling Over-smoothing with Selective State Space","abstract":"Graph Neural Networks (GNNs) have shown great success in various graph-based learning tasks. However, it often faces the issue of over-smoothing as the model depth increases, which causes all node representations to converge to a single value and become indistinguishable. This issue stems from the inherent limitations of GNNs, which struggle to distinguish the importance of information from different neighborhoods. In this paper, we introduce MbaGCN, a novel graph convolutional architecture that draws inspiration from the Mamba paradigm-originally designed for sequence modeling. MbaGCN presents a new backbone for GNNs, consisting of three key components: the Message Aggregation Layer, the Selective State Space Transition Layer, and the Node State Prediction Layer. These components work in tandem to adaptively aggregate neighborhood information, providing greater flexibility and scalability for deep GNN models. While MbaGCN may not consistently outperform all existing methods on each dataset, it provides a foundational framework that demonstrates the effective integration of the Mamba paradigm into graph representation learning. Through extensive experiments on benchmark datasets, we demonstrate that MbaGCN paves the way for future advancements in graph neural network research.","authors":["Xin He","Yili Wang","Wenqi Fan","Xu Shen","Xin Juan","Rui Miao","Xin Wang"],"url":"https://arxiv.org/abs/2501.15461"}
{"created":"2025-05-13","title":"Formal Verification of Markov Processes with Learned Parameters","abstract":"We introduce the problem of formally verifying properties of Markov processes where the parameters are given by the output of machine learning models. For a broad class of machine learning models, including linear models, tree-based models, and neural networks, verifying properties of Markov chains like reachability, hitting time, and total reward can be formulated as a bilinear program. We develop a decomposition and bound propagation scheme for solving the bilinear program and show through computational experiments that our method solves the problem to global optimality up to 100x faster than state-of-the-art solvers. To demonstrate the practical utility of our approach, we apply it to a real-world healthcare case study. Along with the paper, we release markovml, an open-source tool for building Markov processes, integrating pretrained machine learning models, and verifying their properties, available at https://github.com/mmaaz-git/markovml.","authors":["Muhammad Maaz","Timothy C. Y. Chan"],"url":"https://arxiv.org/abs/2501.15767"}
{"created":"2025-05-13","title":"Adaptive Width Neural Networks","abstract":"For almost 70 years, researchers have mostly relied on hyper-parameter tuning to select the width of neural networks' layers. This paper challenges the status quo by introducing an easy-to-use technique to learn an unbounded width of a neural network's layer during training. The technique does not rely on alternate optimization nor hand-crafted gradient heuristics; rather, it jointly optimizes the width and the parameters of each layer via simple backpropagation. We apply the technique to a broad range of data domains such as tables, images, text, sequences, and graphs, showing how the width adapts to the task's difficulty. The method imposes a soft ordering of importance among neurons, by which it also is possible to truncate the trained network at virtually zero cost, achieving a smooth trade-off between performance and compute resources in a structured way. Alternatively, one can dynamically compress the network with no performance degradation. In light of recent foundation models trained on large datasets, believed to require billions of parameters and where hyper-parameter tuning is unfeasible due to humongous training costs, our approach stands as a viable alternative for width learning.","authors":["Federico Errica","Henrik Christiansen","Viktor Zaverkin","Mathias Niepert","Francesco Alesiani"],"url":"https://arxiv.org/abs/2501.15889"}
{"created":"2025-05-13","title":"Improving Tropical Cyclone Forecasting With Video Diffusion Models","abstract":"Tropical cyclone (TC) forecasting is crucial for disaster preparedness and mitigation. While recent deep learning approaches have shown promise, existing methods often treat TC evolution as a series of independent frame-to-frame predictions, limiting their ability to capture long-term dynamics. We present a novel application of video diffusion models for TC forecasting that explicitly models temporal dependencies through additional temporal layers. Our approach enables the model to generate multiple frames simultaneously, better capturing cyclone evolution patterns. We introduce a two-stage training strategy that significantly improves individual-frame quality and performance in low-data regimes. Experimental results show our method outperforms the previous approach of Nath et al. by 19.3% in MAE, 16.2% in PSNR, and 36.1% in SSIM. Most notably, we extend the reliable forecasting horizon from 36 to 50 hours. Through comprehensive evaluation using both traditional metrics and Fr\\'echet Video Distance (FVD), we demonstrate that our approach produces more temporally coherent forecasts while maintaining competitive single-frame quality. Code accessible at https://github.com/Ren-creater/forecast-video-diffmodels.","authors":["Zhibo Ren","Pritthijit Nath","Pancham Shukla"],"url":"https://arxiv.org/abs/2501.16003"}
{"created":"2025-05-13","title":"Large Language Models Think Too Fast To Explore Effectively","abstract":"Large Language Models (LLMs) have emerged with many intellectual capacities. While numerous benchmarks assess their intelligence, limited attention has been given to their ability to explore--an essential capacity for discovering new information and adapting to novel environments in both natural and artificial systems. The extent to which LLMs can effectively explore, particularly in open-ended tasks, remains unclear. This study investigates whether LLMs can surpass humans in exploration during an open-ended task, using Little Alchemy 2 as a paradigm, where agents combine elements to discover new ones. Results show most LLMs underperform compared to humans, except for the o1 model, with traditional LLMs relying primarily on uncertainty-driven strategies, unlike humans who balance uncertainty and empowerment. Results indicate that traditional reasoning-focused LLMs, such as GPT-4o, exhibit a significantly faster and less detailed reasoning process, limiting their exploratory performance. In contrast, the DeepSeek reasoning model demonstrates prolonged, iterative thought processes marked by repetitive analysis of combinations and past trials, reflecting a more thorough and human-like exploration strategy. Representational analysis of the models with Sparse Autoencoders (SAE) revealed that uncertainty and choices are represented at earlier transformer blocks, while empowerment values are processed later, causing LLMs to think too fast and make premature decisions, hindering effective exploration. These findings shed light on the limitations of LLM exploration and suggest directions for improving their adaptability.","authors":["Lan Pan","Hanbo Xie","Robert C. Wilson"],"url":"https://arxiv.org/abs/2501.18009"}
{"created":"2025-05-13","title":"Clustering Properties of Self-Supervised Learning","abstract":"Self-supervised learning (SSL) methods via joint embedding architectures have proven remarkably effective at capturing semantically rich representations with strong clustering properties, magically in the absence of label supervision. Despite this, few of them have explored leveraging these untapped properties to improve themselves. In this paper, we provide an evidence through various metrics that the encoder's output $encoding$ exhibits superior and more stable clustering properties compared to other components. Building on this insight, we propose a novel positive-feedback SSL method, termed Representation Self-Assignment (ReSA), which leverages the model's clustering properties to promote learning in a self-guided manner. Extensive experiments on standard SSL benchmarks reveal that models pretrained with ReSA outperform other state-of-the-art SSL methods by a significant margin. Finally, we analyze how ReSA facilitates better clustering properties, demonstrating that it effectively enhances clustering performance at both fine-grained and coarse-grained levels, shaping representations that are inherently more structured and semantically meaningful.","authors":["Xi Weng","Jianing An","Xudong Ma","Binhang Qi","Jie Luo","Xi Yang","Jin Song Dong","Lei Huang"],"url":"https://arxiv.org/abs/2501.18452"}
{"created":"2025-05-13","title":"Logical Modalities within the European AI Act: An Analysis","abstract":"The paper presents a comprehensive analysis of the European AI Act in terms of its logical modalities, with the aim of preparing its formal representation, for example, within the logic-pluralistic Knowledge Engineering Framework and Methodology (LogiKEy). LogiKEy develops computational tools for normative reasoning based on formal methods, employing Higher-Order Logic (HOL) as a unifying meta-logic to integrate diverse logics through shallow semantic embeddings. This integration is facilitated by Isabelle/HOL, a proof assistant tool equipped with several automated theorem provers. The modalities within the AI Act and the logics suitable for their representation are discussed. For a selection of these logics, embeddings in HOL are created, which are then used to encode sample paragraphs. Initial experiments evaluate the suitability of these embeddings for automated reasoning, and highlight key challenges on the way to more robust reasoning capabilities.","authors":["Lara Lawniczak","Christoph Benzm\\\"uller"],"url":"https://arxiv.org/abs/2501.19112"}
{"created":"2025-05-13","title":"Model non-collapse: Minimax bounds for recursive discrete distribution estimation","abstract":"Learning discrete distributions from i.i.d. samples is a well-understood problem. However, advances in generative machine learning prompt an interesting new, non-i.i.d. setting: after receiving a certain number of samples, an estimated distribution is fixed, and samples from this estimate are drawn and introduced into the sample corpus, undifferentiated from real samples. Subsequent generations of estimators now face contaminated environments, a scenario referred to in the machine learning literature as self-consumption. Empirically, it has been observed that models in fully synthetic self-consuming loops collapse -- their performance deteriorates with each batch of training -- but accumulating data has been shown to prevent complete degeneration. This, in turn, begs the question: What happens when fresh real samples \\textit{are} added at every stage? In this paper, we study the minimax loss of self-consuming discrete distribution estimation in such loops. We show that even when model collapse is consciously averted, the ratios between the minimax losses with and without source information can grow unbounded as the batch size increases. In the data accumulation setting, where all batches of samples are available for estimation, we provide minimax lower bounds and upper bounds that are order-optimal under mild conditions for the expected $\\ell_2^2$ and $\\ell_1$ losses at every stage. We provide conditions for regimes where there is a strict gap in the convergence rates compared to the corresponding oracle-assisted minimax loss where real and synthetic samples are differentiated, and provide examples where this gap is easily observed. We also provide a lower bound on the minimax loss in the data replacement setting, where only the latest batch of samples is available, and use it to find a lower bound for the worst-case loss for bounded estimate trajectories.","authors":["Millen Kanabar","Michael Gastpar"],"url":"https://arxiv.org/abs/2501.19273"}
{"created":"2025-05-13","title":"Perceptive Mixed-Integer Footstep Control for Underactuated Bipedal Walking on Rough Terrain","abstract":"Traversing rough terrain requires dynamic bipeds to stabilize themselves through foot placement without stepping in unsafe areas. Planning these footsteps online is challenging given non-convexity of the safe terrain, and imperfect perception and state estimation. This paper addresses these challenges with a full-stack perception and control system for achieving underactuated walking on discontinuous terrain. First, we develop model-predictive footstep control (MPFC), a single mixed-integer quadratic program which assumes a convex polygon terrain decomposition to optimize over discrete foothold choice, footstep position, ankle torque, template dynamics, and footstep timing at over 100 Hz. We then propose a novel approach for generating convex polygon terrain decompositions online. Our perception stack decouples safe-terrain classification from fitting planar polygons, generating a temporally consistent terrain segmentation in real time using a single CPU thread. We demonstrate the performance of our perception and control stack through outdoor experiments with the underactuated biped Cassie, achieving state of the art perceptive bipedal walking on discontinuous terrain. Supplemental Video: https://youtu.be/JK16KJXJxi4","authors":["Brian Acosta","Michael Posa"],"url":"https://arxiv.org/abs/2501.19391"}
{"created":"2025-05-13","title":"A Frugal Model for Accurate Early Student Failure Prediction","abstract":"Predicting student success or failure is vital for timely interventions and personalized support. Early failure prediction is particularly crucial, yet limited data availability in the early stages poses challenges, one of the possible solutions is to make use of additional data from other contexts, however, this might lead to overconsumption with no guarantee of better results. To address this, we propose the Frugal Early Prediction (FEP) model, a new hybrid model that selectively incorporates additional data, promoting data frugality and efficient resource utilization. Experiments conducted on a public dataset from a VLE demonstrate FEP's effectiveness in reducing data usage, a primary goal of this research.Experiments showcase a remarkable 27% reduction in data consumption, compared to a systematic use of additional data, aligning with our commitment to data frugality and offering substantial benefits to educational institutions seeking efficient data consumption. Additionally, FEP also excels in enhancing prediction accuracy. Compared to traditional approaches, FEP achieves an average accuracy gain of 7.3%. This not only highlights the practicality and efficiency of FEP but also its superiority in performance, while respecting resource constraints, providing beneficial findings for educational institutions seeking data frugality.","authors":["Ikram Gagaoua (RiseUp","UL","CNRS","LORIA)","Armelle Brun (UL","CNRS","LORIA)","Anne Boyer (UL","CNRS","LORIA)"],"url":"https://arxiv.org/abs/2502.00017"}
{"created":"2025-05-13","title":"RealRAG: Retrieval-augmented Realistic Image Generation via Self-reflective Contrastive Learning","abstract":"Recent text-to-image generative models, e.g., Stable Diffusion V3 and Flux, have achieved notable progress. However, these models are strongly restricted to their limited knowledge, a.k.a., their own fixed parameters, that are trained with closed datasets. This leads to significant hallucinations or distortions when facing fine-grained and unseen novel real-world objects, e.g., the appearance of the Tesla Cybertruck. To this end, we present the first real-object-based retrieval-augmented generation framework (RealRAG), which augments fine-grained and unseen novel object generation by learning and retrieving real-world images to overcome the knowledge gaps of generative models. Specifically, to integrate missing memory for unseen novel object generation, we train a reflective retriever by self-reflective contrastive learning, which injects the generator's knowledge into the sef-reflective negatives, ensuring that the retrieved augmented images compensate for the model's missing knowledge. Furthermore, the real-object-based framework integrates fine-grained visual knowledge for the generative models, tackling the distortion problem and improving the realism for fine-grained object generation. Our Real-RAG is superior in its modular application to all types of state-of-the-art text-to-image generative models and also delivers remarkable performance boosts with all of them, such as a gain of 16.18% FID score with the auto-regressive model on the Stanford Car benchmark.","authors":["Yuanhuiyi Lyu","Xu Zheng","Lutao Jiang","Yibo Yan","Xin Zou","Huiyu Zhou","Linfeng Zhang","Xuming Hu"],"url":"https://arxiv.org/abs/2502.00848"}
{"created":"2025-05-13","title":"Dual Alignment Maximin Optimization for Offline Model-based RL","abstract":"Offline reinforcement learning agents face significant deployment challenges due to the synthetic-to-real distribution mismatch. While most prior research has focused on improving the fidelity of synthetic sampling and incorporating off-policy mechanisms, the directly integrated paradigm often fails to ensure consistent policy behavior in biased models and underlying environmental dynamics, which inherently arise from discrepancies between behavior and learning policies. In this paper, we first shift the focus from model reliability to policy discrepancies while optimizing for expected returns, and then self-consistently incorporate synthetic data, deriving a novel actor-critic paradigm, Dual Alignment Maximin Optimization (DAMO). It is a unified framework to ensure both model-environment policy consistency and synthetic and offline data compatibility. The inner minimization performs dual conservative value estimation, aligning policies and trajectories to avoid out-of-distribution states and actions, while the outer maximization ensures that policy improvements remain consistent with inner value estimates. Empirical evaluations demonstrate that DAMO effectively ensures model and policy alignments, achieving competitive performance across diverse benchmark tasks.","authors":["Chi Zhou","Wang Luo","Haoran Li","Congying Han","Tiande Guo","Zicheng Zhang"],"url":"https://arxiv.org/abs/2502.00850"}
{"created":"2025-05-13","title":"Visual Theory of Mind Enables the Invention of Proto-Writing","abstract":"Symbolic writing systems are graphical semiotic codes that are ubiquitous in modern society but are otherwise absent in the animal kingdom. Anthropological evidence suggests that the earliest forms of some writing systems originally consisted of iconic pictographs, which signify their referent via visual resemblance. While previous studies have examined the emergence and, separately, the evolution of pictographic systems through a computational lens, most employ non-naturalistic methodologies that make it difficult to draw clear analogies to human and animal cognition. We develop a multi-agent reinforcement learning testbed for emergent communication called a Signification Game, and formulate a model of inferential communication that enables agents to leverage visual theory of mind to communicate actions using pictographs. Our model, which is situated within a broader formalism for animal communication, sheds light on the cognitive and cultural processes underlying the emergence of proto-writing.","authors":["Benjamin A. Spiegel","Lucas Gelfond","George Konidaris"],"url":"https://arxiv.org/abs/2502.01568"}
{"created":"2025-05-13","title":"Training and Evaluating with Human Label Variation: An Empirical Study","abstract":"Human label variation (HLV) challenges the standard assumption that a labelled instance has a single ground truth, instead embracing the natural variation in human annotation to train and evaluate models. While various training methods and metrics for HLV have been proposed, it is still unclear which methods and metrics perform best in what settings. We propose new evaluation metrics for HLV leveraging fuzzy set theory. Since these new proposed metrics are differentiable, we then in turn experiment with employing these metrics as training objectives. We conduct an extensive study over 6 HLV datasets testing 14 training methods and 6 evaluation metrics. We find that training on either disaggregated annotations or soft labels performs best across metrics, outperforming training using the proposed training objectives with differentiable metrics. We also show that our proposed soft metric is more interpretable and correlates best with human preference.","authors":["Kemal Kurniawan","Meladel Mistica","Timothy Baldwin","Jey Han Lau"],"url":"https://arxiv.org/abs/2502.01891"}
{"created":"2025-05-13","title":"GP-GS: Gaussian Processes for Enhanced Gaussian Splatting","abstract":"3D Gaussian Splatting has emerged as an efficient photorealistic novel view synthesis method. However, its reliance on sparse Structure-from-Motion (SfM) point clouds often limits scene reconstruction quality. To address the limitation, this paper proposes a novel 3D reconstruction framework, Gaussian Processes enhanced Gaussian Splatting (GP-GS), in which a multi-output Gaussian Process model is developed to enable adaptive and uncertainty-guided densification of sparse SfM point clouds. Specifically, we propose a dynamic sampling and filtering pipeline that adaptively expands the SfM point clouds by leveraging GP-based predictions to infer new candidate points from the input 2D pixels and depth maps. The pipeline utilizes uncertainty estimates to guide the pruning of high-variance predictions, ensuring geometric consistency and enabling the generation of dense point clouds. These densified point clouds provide high-quality initial 3D Gaussians, enhancing reconstruction performance. Extensive experiments conducted on synthetic and real-world datasets across various scales validate the effectiveness and practicality of the proposed framework.","authors":["Zhihao Guo","Jingxuan Su","Shenglin Wang","Jinlong Fan","Jing Zhang","Wei Zhou","Hadi Amirpour","Yunlong Zhao","Liangxiu Han","Peng Wang"],"url":"https://arxiv.org/abs/2502.02283"}
{"created":"2025-05-13","title":"Model Human Learners: Computational Models to Guide Instructional Design","abstract":"Instructional designers face an overwhelming array of design choices, making it challenging to identify the most effective interventions. To address this issue, I propose the concept of a Model Human Learner, a unified computational model of learning that can aid designers in evaluating candidate interventions. This paper presents the first successful demonstration of this concept, showing that a computational model can accurately predict the outcomes of two human A/B experiments -- one testing a problem sequencing intervention and the other testing an item design intervention. It also demonstrates that such a model can generate learning curves without requiring human data and provide theoretical insights into why an instructional intervention is effective. These findings lay the groundwork for future Model Human Learners that integrate cognitive and learning theories to support instructional design across diverse tasks and interventions.","authors":["Christopher J. MacLellan"],"url":"https://arxiv.org/abs/2502.02456"}
{"created":"2025-05-13","title":"Articulate AnyMesh: Open-Vocabulary 3D Articulated Objects Modeling","abstract":"3D articulated objects modeling has long been a challenging problem, since it requires to capture both accurate surface geometries and semantically meaningful and spatially precise structures, parts, and joints. Existing methods heavily depend on training data from a limited set of handcrafted articulated object categories (e.g., cabinets and drawers), which restricts their ability to model a wide range of articulated objects in an open-vocabulary context. To address these limitations, we propose Articulate Anymesh, an automated framework that is able to convert any rigid 3D mesh into its articulated counterpart in an open-vocabulary manner. Given a 3D mesh, our framework utilizes advanced Vision-Language Models and visual prompting techniques to extract semantic information, allowing for both the segmentation of object parts and the construction of functional joints. Our experiments show that Articulate Anymesh can generate large-scale, high-quality 3D articulated objects, including tools, toys, mechanical devices, and vehicles, significantly expanding the coverage of existing 3D articulated object datasets. Additionally, we show that these generated assets can facilitate the acquisition of new articulated object manipulation skills in simulation, which can then be transferred to a real robotic system. Our Github website is https://articulate-anymesh.github.io.","authors":["Xiaowen Qiu","Jincheng Yang","Yian Wang","Zhehuan Chen","Yufei Wang","Tsun-Hsuan Wang","Zhou Xian","Chuang Gan"],"url":"https://arxiv.org/abs/2502.02590"}
{"created":"2025-05-13","title":"Rethinking Latent Redundancy in Behavior Cloning: An Information Bottleneck Approach for Robot Manipulation","abstract":"Behavior Cloning (BC) is a widely adopted visual imitation learning method in robot manipulation. Current BC approaches often enhance generalization by leveraging large datasets and incorporating additional visual and textual modalities to capture more diverse information. However, these methods overlook whether the learned representations contain redundant information and lack a solid theoretical foundation to guide the learning process. To address these limitations, we adopt an information-theoretic perspective and introduce mutual information to quantify and mitigate redundancy in latent representations. Building on this, we incorporate the Information Bottleneck (IB) principle into BC, which extends the idea of reducing redundancy by providing a structured framework for compressing irrelevant information while preserving task-relevant features. This work presents the first comprehensive study on redundancy in latent representations across various methods, backbones, and experimental settings, while extending the generalizability of the IB to BC. Extensive experiments and analyses on the CortexBench and LIBERO benchmarks demonstrate significant performance improvements with IB, underscoring the importance of reducing input data redundancy and highlighting its practical value for more practical applications. Project Page: https://baishuanghao.github.io/BC-IB.github.io.","authors":["Shuanghao Bai","Wanqi Zhou","Pengxiang Ding","Wei Zhao","Donglin Wang","Badong Chen"],"url":"https://arxiv.org/abs/2502.02853"}
{"created":"2025-05-13","title":"AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design in Augmented Reality","abstract":"We present AudioMiXR, an augmented reality (AR) interface intended to assess how users manipulate virtual audio objects situated in their physical space using six degrees of freedom (6DoF) deployed on a head-mounted display (Apple Vision Pro) for 3D sound design. Existing tools for 3D sound design are typically constrained to desktop displays, which may limit spatial awareness of mixing within the execution environment. Utilizing an XR HMD to create soundscapes may provide a real-time test environment for 3D sound design, as modern HMDs can provide precise spatial localization assisted by cross-modal interactions. However, there is no research on design guidelines specific to sound design with six degrees of freedom (6DoF) in XR. To provide a first step toward identifying design-related research directions in this space, we conducted an exploratory study where we recruited 27 participants, consisting of expert and non-expert sound designers. The goal was to assess design lessons that can be used to inform future research venues in 3D sound design. We ran a within-subjects study where users designed both a music and cinematic soundscapes. After thematically analyzing participant data, we constructed two design lessons: 1. Proprioception for AR Sound Design, and 2. Balancing Audio-Visual Modalities in AR GUIs. Additionally, we provide application domains that can benefit most from 6DoF sound design based on our results.","authors":["Brandon Woodard","Margarita Geleta","Joseph J. LaViola Jr.","Andrea Fanelli","Rhonda Wilson"],"url":"https://arxiv.org/abs/2502.02929"}
{"created":"2025-05-13","title":"HumanDiT: Pose-Guided Diffusion Transformer for Long-form Human Motion Video Generation","abstract":"Human motion video generation has advanced significantly, while existing methods still struggle with accurately rendering detailed body parts like hands and faces, especially in long sequences and intricate motions. Current approaches also rely on fixed resolution and struggle to maintain visual consistency. To address these limitations, we propose HumanDiT, a pose-guided Diffusion Transformer (DiT)-based framework trained on a large and wild dataset containing 14,000 hours of high-quality video to produce high-fidelity videos with fine-grained body rendering. Specifically, (i) HumanDiT, built on DiT, supports numerous video resolutions and variable sequence lengths, facilitating learning for long-sequence video generation; (ii) we introduce a prefix-latent reference strategy to maintain personalized characteristics across extended sequences. Furthermore, during inference, HumanDiT leverages Keypoint-DiT to generate subsequent pose sequences, facilitating video continuation from static images or existing videos. It also utilizes a Pose Adapter to enable pose transfer with given sequences. Extensive experiments demonstrate its superior performance in generating long-form, pose-accurate videos across diverse scenarios.","authors":["Qijun Gan","Yi Ren","Chen Zhang","Zhenhui Ye","Pan Xie","Xiang Yin","Zehuan Yuan","Bingyue Peng","Jianke Zhu"],"url":"https://arxiv.org/abs/2502.04847"}
{"created":"2025-05-13","title":"HAMSTER: Hierarchical Action Models For Open-World Robot Manipulation","abstract":"Large foundation models have shown strong open-world generalization to complex problems in vision and language, but similar levels of generalization have yet to be achieved in robotics. One fundamental challenge is the lack of robotic data, which are typically obtained through expensive on-robot operation. A promising remedy is to leverage cheaper, off-domain data such as action-free videos, hand-drawn sketches or simulation data. In this work, we posit that hierarchical vision-language-action (VLA) models can be more effective in utilizing off-domain data than standard monolithic VLA models that directly finetune vision-language models (VLMs) to predict actions. In particular, we study a class of hierarchical VLA models, where the high-level VLM is finetuned to produce a coarse 2D path indicating the desired robot end-effector trajectory given an RGB image and a task description. The intermediate 2D path prediction is then served as guidance to the low-level, 3D-aware control policy capable of precise manipulation. Doing so alleviates the high-level VLM from fine-grained action prediction, while reducing the low-level policy's burden on complex task-level reasoning. We show that, with the hierarchical design, the high-level VLM can transfer across significant domain gaps between the off-domain finetuning data and real-robot testing scenarios, including differences on embodiments, dynamics, visual appearances and task semantics, etc. In the real-robot experiments, we observe an average of 20% improvement in success rate across seven different axes of generalization over OpenVLA, representing a 50% relative gain. Visual results, code, and dataset are provided at: https://hamster-robot.github.io/","authors":["Yi Li","Yuquan Deng","Jesse Zhang","Joel Jang","Marius Memmel","Raymond Yu","Caelan Reed Garrett","Fabio Ramos","Dieter Fox","Anqi Li","Abhishek Gupta","Ankit Goyal"],"url":"https://arxiv.org/abs/2502.05485"}
{"created":"2025-05-13","title":"Guided Exploration for Efficient Relational Model Learning","abstract":"Efficient exploration is critical for learning relational models in large-scale environments with complex, long-horizon tasks. Random exploration methods often collect redundant or irrelevant data, limiting their ability to learn accurate relational models of the environment. Goal-literal babbling (GLIB) improves upon random exploration by setting and planning to novel goals, but its reliance on random actions and random novel goal selection limits its scalability to larger domains. In this work, we identify the principles underlying efficient exploration in relational domains: (1) operator initialization with demonstrations that cover the distinct lifted effects necessary for planning and (2) refining preconditions to collect maximally informative transitions by selecting informative goal-action pairs and executing plans to them. To demonstrate these principles, we introduce Baking-Large, a challenging domain with extensive state-action spaces and long-horizon tasks. We evaluate methods using oracle-driven demonstrations for operator initialization and precondition-targeting guidance to efficiently gather critical transitions. Experiments show that both the oracle demonstrations and precondition-targeting oracle guidance significantly improve sample efficiency and generalization, paving the way for future methods to use these principles to efficiently learn accurate relational models in complex domains.","authors":["Annie Feng","Nishanth Kumar","Tomas Lozano-Perez","Leslie Pack-Kaelbling"],"url":"https://arxiv.org/abs/2502.06146"}
{"created":"2025-05-13","title":"Unbiased Evaluation of Large Language Models from a Causal Perspective","abstract":"Benchmark contamination has become a significant concern in the LLM evaluation community. Previous Agents-as-an-Evaluator address this issue by involving agents in the generation of questions. Despite their success, the biases in Agents-as-an-Evaluator methods remain largely unexplored. In this paper, we present a theoretical formulation of evaluation bias, providing valuable insights into designing unbiased evaluation protocols. Furthermore, we identify two type of bias in Agents-as-an-Evaluator through carefully designed probing tasks on a minimal Agents-as-an-Evaluator setup. To address these issues, we propose the Unbiased Evaluator, an evaluation protocol that delivers a more comprehensive, unbiased, and interpretable assessment of LLMs.Extensive experiments reveal significant room for improvement in current LLMs. Additionally, we demonstrate that the Unbiased Evaluator not only offers strong evidence of benchmark contamination but also provides interpretable evaluation results.","authors":["Meilin Chen","Jian Tian","Liang Ma","Di Xie","Weijie Chen","Jiang Zhu"],"url":"https://arxiv.org/abs/2502.06655"}
{"created":"2025-05-13","title":"AI-driven Personalized Privacy Assistants: a Systematic Literature Review","abstract":"In recent years, several personalized assistants based on AI have been researched and developed to help users make privacy-related decisions. These AI-driven Personalized Privacy Assistants (AI-driven PPAs) can provide significant benefits for users, who might otherwise struggle with making decisions about their personal data in online environments that often overload them with different privacy decision requests. So far, no studies have systematically investigated the emerging topic of AI-driven PPAs, classifying their underlying technologies, architecture and features, including decision types or the accuracy of their decisions. To fill this gap, we present a Systematic Literature Review (SLR) to map the existing solutions found in the scientific literature, which allows reasoning about existing approaches and open challenges for this research field. We screened several hundred unique research papers over the recent years (2013-2025), constructing a classification from 41 included papers. As a result, this SLR reviews several aspects of existing research on AI-driven PPAs in terms of types of publications, contributions, methodological quality, and other quantitative insights. Furthermore, we provide a comprehensive classification for AI-driven PPAs, delving into their architectural choices, system contexts, types of AI used, data sources, types of decisions, and control over decisions, among other facets. Based on our SLR, we further underline the research gaps and challenges and formulate recommendations for the design and development of AI-driven PPAs as well as avenues for future research.","authors":["Victor Morel","Leonardo Iwaya","Simone Fischer-H\\\"ubner"],"url":"https://arxiv.org/abs/2502.07693"}
{"created":"2025-05-13","title":"Enhancing Sample Selection Against Label Noise by Cutting Mislabeled Easy Examples","abstract":"Sample selection is a prevalent approach in learning with noisy labels, aiming to identify confident samples for training. Although existing sample selection methods have achieved decent results by reducing the noise rate of the selected subset, they often overlook that not all mislabeled examples harm the model's performance equally. In this paper, we demonstrate that mislabeled examples correctly predicted by the model early in the training process are particularly harmful to model performance. We refer to these examples as Mislabeled Easy Examples (MEEs). To address this, we propose Early Cutting, which introduces a recalibration step that employs the model's later training state to re-select the confident subset identified early in training, thereby avoiding misleading confidence from early learning and effectively filtering out MEEs. Experiments on the CIFAR, WebVision, and full ImageNet-1k datasets demonstrate that our method effectively improves sample selection and model performance by reducing MEEs.","authors":["Suqin Yuan","Lei Feng","Bo Han","Tongliang Liu"],"url":"https://arxiv.org/abs/2502.08227"}
{"created":"2025-05-13","title":"Keep your distance: learning dispersed embeddings on $\\mathbb{S}_m$","abstract":"Learning well-separated features in high-dimensional spaces, such as text or image embeddings, is crucial for many machine learning applications. Achieving such separation can be effectively accomplished through the dispersion of embeddings, where unrelated vectors are pushed apart as much as possible. By constraining features to be on a hypersphere, we can connect dispersion to well-studied problems in mathematics and physics, where optimal solutions are known for limited low-dimensional cases. However, in representation learning we typically deal with a large number of features in high-dimensional space, and moreover, dispersion is usually traded off with some other task-oriented training objective, making existing theoretical and numerical solutions inapplicable. Therefore, it is common to rely on gradient-based methods to encourage dispersion, usually by minimizing some function of the pairwise distances. In this work, we first give an overview of existing methods from disconnected literature, making new connections and highlighting similarities. Next, we introduce some new angles. We propose to reinterpret pairwise dispersion using a maximum mean discrepancy (MMD) motivation. We then propose an online variant of the celebrated Lloyd's algorithm, of K-Means fame, as an effective alternative regularizer for dispersion on generic domains. Finally, we derive a novel dispersion method that directly exploits properties of the hypersphere. Our experiments show the importance of dispersion in image classification and natural language processing tasks, and how algorithms exhibit different trade-offs in different regimes.","authors":["Evgeniia Tokarchuk","Hua Chang Bakker","Vlad Niculae"],"url":"https://arxiv.org/abs/2502.08231"}
{"created":"2025-05-13","title":"Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes","abstract":"Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \\textit{composite treatments}, on a set of outcome variables of interest, referred to as \\textit{composite outcomes}, for a unit from observational data -- remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science. Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes. This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality. The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes. To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called \\emph{H-Learner}, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes. Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods.","authors":["Vinod Kumar Chauhan","Lei Clifton","Gaurav Nigam","David A. Clifton"],"url":"https://arxiv.org/abs/2502.08282"}
{"created":"2025-05-13","title":"Bilevel Learning for Bilevel Planning","abstract":"A robot that learns from demonstrations should not just imitate what it sees -- it should understand the high-level concepts that are being demonstrated and generalize them to new tasks. Bilevel planning is a hierarchical model-based approach where predicates (relational state abstractions) can be leveraged to achieve compositional generalization. However, previous bilevel planning approaches depend on predicates that are either hand-engineered or restricted to very simple forms, limiting their scalability to sophisticated, high-dimensional state spaces. To address this limitation, we present IVNTR, the first bilevel planning approach capable of learning neural predicates directly from demonstrations. Our key innovation is a neuro-symbolic bilevel learning framework that mirrors the structure of bilevel planning. In IVNTR, symbolic learning of the predicate \"effects\" and neural learning of the predicate \"functions\" alternate, with each providing guidance for the other. We evaluate IVNTR in six diverse robot planning domains, demonstrating its effectiveness in abstracting various continuous and high-dimensional states. While most existing approaches struggle to generalize (with <35% success rate), our IVNTR achieves an average of 77% success rate on unseen tasks. Additionally, we showcase IVNTR on a mobile manipulator, where it learns to perform real-world mobile manipulation tasks and generalizes to unseen test scenarios that feature new objects, new states, and longer task horizons. Our findings underscore the promise of learning and planning with abstractions as a path towards high-level generalization.","authors":["Bowen Li","Tom Silver","Sebastian Scherer","Alexander Gray"],"url":"https://arxiv.org/abs/2502.08697"}
{"created":"2025-05-13","title":"Coupled Rendezvous and Docking Maneuver control of satellite using Reinforcement learning-based Adaptive Fixed-Time Sliding Mode Controller","abstract":"Satellite dynamics in unknown environments are inherently uncertain due to factors such as varying gravitational fields, atmospheric drag, and unpredictable interactions with space debris or other celestial bodies. Traditional sliding mode controllers with fixed parameters often struggle to maintain optimal performance under these fluctuating conditions. Therefore, an adaptive controller is essential to address these challenges by continuously tuning its gains in real-time. In this paper, we have tuned the slopes of the Fixed-time Sliding surface adaptively using reinforcement learning for coupled rendezvous and docking maneuver of chaser satellite with the target satellite in an unknown space environment. The neural network model is used to determine the optimal gains of reaching law of the fixed-time sliding surface. We have assumed that we don't have an accurate model of the system so we have added noise in the tangent space instead of directly on the manifold to preserve the geometric structure of the system while ensuring mathematically consistent uncertainty propagation. The reinforcement learning is used as an approximator to represent the value function of the agent to estimate the dynamical model of the system using the Actor-Critic method. The proposed control algorithm integrates a neural network and a sliding mode controller in a cascade loop architecture, where the tracking error dynamically tunes the sliding surface gains. Global fixed-time stability of the closed-loop feedback system is proved within the Lyapunov framework. This comprehensive approach of fixed-time sliding mode controller using a Reinforcement Learning based ensures the completion of the mission efficiently while addressing the critical challenges posed by the uncertain environment. The simulation results presented support the claims made.","authors":["Rakesh Kumar Sahoo","Manoranjan Sinha"],"url":"https://arxiv.org/abs/2502.09517"}
{"created":"2025-05-13","title":"D-CIPHER: Dynamic Collaborative Intelligent Multi-Agent System with Planner and Heterogeneous Executors for Offensive Security","abstract":"Large Language Models (LLMs) have been used in cybersecurity such as autonomous security analysis or penetration testing. Capture the Flag (CTF) challenges serve as benchmarks to assess automated task-planning abilities of LLM agents for cybersecurity. Early attempts to apply LLMs for solving CTF challenges used single-agent systems, where feedback was restricted to a single reasoning-action loop. This approach was inadequate for complex CTF tasks. Inspired by real-world CTF competitions, where teams of experts collaborate, we introduce the D-CIPHER LLM multi-agent framework for collaborative CTF solving. D-CIPHER integrates agents with distinct roles with dynamic feedback loops to enhance reasoning on complex tasks. It introduces the Planner-Executor agent system, consisting of a Planner agent for overall problem-solving along with multiple heterogeneous Executor agents for individual tasks, facilitating efficient allocation of responsibilities among the agents. Additionally, D-CIPHER incorporates an Auto-prompter agent to improve problem-solving by auto-generating a highly relevant initial prompt. We evaluate D-CIPHER on multiple CTF benchmarks and LLM models via comprehensive studies to highlight the impact of our enhancements. Additionally, we manually map the CTFs in NYU CTF Bench to MITRE ATT&amp;CK techniques that apply for a comprehensive evaluation of D-CIPHER's offensive security capability. D-CIPHER achieves state-of-the-art performance on three benchmarks: 22.0% on NYU CTF Bench, 22.5% on Cybench, and 44.0% on HackTheBox, which is 2.5% to 8.5% better than previous work. D-CIPHER solves 65% more ATT&amp;CK techniques compared to previous work, demonstrating stronger offensive capability.","authors":["Meet Udeshi","Minghao Shao","Haoran Xi","Nanda Rani","Kimberly Milner","Venkata Sai Charan Putrevu","Brendan Dolan-Gavitt","Sandeep Kumar Shukla","Prashanth Krishnamurthy","Farshad Khorrami","Ramesh Karri","Muhammad Shafique"],"url":"https://arxiv.org/abs/2502.10931"}
{"created":"2025-05-13","title":"Collaborative Deterministic-Diffusion Model for Probabilistic Spatiotemporal Prediction","abstract":"Accurate prediction of urban spatiotemporal dynamics is essential for enhancing urban management and decision-making. Existing spatiotemporal prediction models are predominantly deterministic, focusing on primary spatiotemporal patterns. However, those dynamics are highly complex, exhibiting multi-modal distributions that are challenging for deterministic models to capture. In this paper, we highlight the critical role of probabilistic prediction in capturing the uncertainties and complexities inherent in spatiotemporal data. While mainstream probabilistic models can capture uncertainty, they struggle with accurately learning primary patterns and often suffer from computational inefficiency. To address these challenges, we propose CoST, which collaborates deterministic and probabilistic models to improve both predictive accuracy and the ability to handle uncertainty. To achieve this, we design a mean-residual decomposition framework, where the mean value is modeled by a deterministic model, and the residual variations are learned by a probabilistic model, specifically diffusion models. Moreover, we introduce a scale-aware diffusion process, which better accounts for spatially heterogeneous dynamics across different regions. Extensive experiments on eight real-world datasets demonstrate that CoST significantly outperforms existing methods in both deterministic and probabilistic metrics, achieving a 20% improvement with low computational cost. CoST bridges the gap between deterministic precision and probabilistic uncertainty, making a significant advancement in the field of urban spatiotemporal prediction.","authors":["Zhi Sheng","Yuan Yuan","Yudi Zhang","Depeng Jin","Yong Li"],"url":"https://arxiv.org/abs/2502.11013"}
{"created":"2025-05-13","title":"VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues","abstract":"Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM2-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.","authors":["Jianshu Zhang","Dongyu Yao","Renjie Pi","Paul Pu Liang","Yi R. Fung"],"url":"https://arxiv.org/abs/2502.12084"}
{"created":"2025-05-13","title":"Integrating Expert Knowledge into Logical Programs via LLMs","abstract":"This paper introduces ExKLoP, a novel framework designed to evaluate how effectively Large Language Models (LLMs) integrate expert knowledge into logical reasoning systems. This capability is especially valuable in engineering, where expert knowledge-such as manufacturer-recommended operational ranges-can be directly embedded into automated monitoring systems. By mirroring expert verification steps, tasks like range checking and constraint validation help ensure system safety and reliability. Our approach systematically evaluates LLM-generated logical rules, assessing both syntactic fluency and logical correctness in these critical validation tasks. We also explore the models' capacity for self-correction via an iterative feedback loop based on code execution outcomes. ExKLoP presents an extensible dataset comprising 130 engineering premises, 950 prompts, and corresponding validation points. It enables comprehensive benchmarking while allowing control over task complexity and scalability of experiments. We leverage the synthetic data creation methodology to conduct extensive empirical evaluation on a diverse set of LLMs including Llama3, Gemma3, Codestral and QwenCoder. The results reveal that most models generate nearly perfect syntactically correct code and exhibit strong performance in translating expert knowledge into correct code. At the same time, while most LLMs produce nearly flawless syntactic output, their ability to correctly implement logical rules varies, as does their capacity for self-improvement. Overall, ExKLoP serves as a robust evaluation platform that streamlines the selection of effective models for self-correcting systems while clearly delineating the types of errors encountered.","authors":["Franciszek G\\'orski","Oskar Wysocki","Marco Valentino","Andre Freitas"],"url":"https://arxiv.org/abs/2502.12275"}
{"created":"2025-05-13","title":"None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks","abstract":"In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers.","authors":["Eva S\\'anchez Salido","Julio Gonzalo","Guillermo Marco"],"url":"https://arxiv.org/abs/2502.12896"}
{"created":"2025-05-13","title":"On the Trustworthiness of Generative Foundation Models: Guideline, Assessment, and Perspective","abstract":"Generative Foundation Models (GenFMs) have emerged as transformative tools. However, their widespread adoption raises critical concerns regarding trustworthiness across dimensions. This paper presents a comprehensive framework to address these challenges through three key contributions. First, we systematically review global AI governance laws and policies from governments and regulatory bodies, as well as industry practices and standards. Based on this analysis, we propose a set of guiding principles for GenFMs, developed through extensive multidisciplinary collaboration that integrates technical, ethical, legal, and societal perspectives. Second, we introduce TrustGen, the first dynamic benchmarking platform designed to evaluate trustworthiness across multiple dimensions and model types, including text-to-image, large language, and vision-language models. TrustGen leverages modular components--metadata curation, test case generation, and contextual variation--to enable adaptive and iterative assessments, overcoming the limitations of static evaluation methods. Using TrustGen, we reveal significant progress in trustworthiness while identifying persistent challenges. Finally, we provide an in-depth discussion of the challenges and future directions for trustworthy GenFMs, which reveals the complex, evolving nature of trustworthiness, highlighting the nuanced trade-offs between utility and trustworthiness, and consideration for various downstream applications, identifying persistent challenges and providing a strategic roadmap for future research. This work establishes a holistic framework for advancing trustworthiness in GenAI, paving the way for safer and more responsible integration of GenFMs into critical applications. To facilitate advancement in the community, we release the toolkit for dynamic evaluation.","authors":["Yue Huang","Chujie Gao","Siyuan Wu","Haoran Wang","Xiangqi Wang","Yujun Zhou","Yanbo Wang","Jiayi Ye","Jiawen Shi","Qihui Zhang","Yuan Li","Han Bao","Zhaoyi Liu","Tianrui Guan","Dongping Chen","Ruoxi Chen","Kehan Guo","Andy Zou","Bryan Hooi Kuen-Yew","Caiming Xiong","Elias Stengel-Eskin","Hongyang Zhang","Hongzhi Yin","Huan Zhang","Huaxiu Yao","Jaehong Yoon","Jieyu Zhang","Kai Shu","Kaijie Zhu","Ranjay Krishna","Swabha Swayamdipta","Taiwei Shi","Weijia Shi","Xiang Li","Yiwei Li","Yuexing Hao","Zhihao Jia","Zhize Li","Xiuying Chen","Zhengzhong Tu","Xiyang Hu","Tianyi Zhou","Jieyu Zhao","Lichao Sun","Furong Huang","Or Cohen Sasson","Prasanna Sattigeri","Anka Reuel","Max Lamparth","Yue Zhao","Nouha Dziri","Yu Su","Huan Sun","Heng Ji","Chaowei Xiao","Mohit Bansal","Nitesh V. Chawla","Jian Pei","Jianfeng Gao","Michael Backes","Philip S. Yu","Neil Zhenqiang Gong","Pin-Yu Chen","Bo Li","Dawn Song","Xiangliang Zhang"],"url":"https://arxiv.org/abs/2502.14296"}
{"created":"2025-05-13","title":"A Statistical Case Against Empirical Human-AI Alignment","abstract":"Empirical human-AI alignment aims to make AI systems act in line with observed human behavior. While noble in its goals, we argue that empirical alignment can inadvertently introduce statistical biases that warrant caution. This position paper thus advocates against naive empirical alignment, offering prescriptive alignment and a posteriori empirical alignment as alternatives. We substantiate our principled argument by tangible examples like human-centric decoding of language models.","authors":["Julian Rodemann","Esteban Garces Arias","Christoph Luther","Christoph Jansen","Thomas Augustin"],"url":"https://arxiv.org/abs/2502.14581"}
{"created":"2025-05-13","title":"SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models","abstract":"Vision language models (VLM) demonstrate sophisticated multimodal reasoning yet are prone to hallucination when confronted with knowledge conflicts, impeding their deployment in information-sensitive contexts. While existing research addresses robustness in unimodal models, the multimodal domain lacks systematic investigation of cross-modal knowledge conflicts. This research introduces \\segsub, a framework for applying targeted image perturbations to investigate VLM resilience against knowledge conflicts. Our analysis reveals distinct vulnerability patterns: while VLMs are robust to parametric conflicts (20% adherence rates), they exhibit significant weaknesses in identifying counterfactual conditions (<30% accuracy) and resolving source conflicts (<1% accuracy). Correlations between contextual richness and hallucination rate (r = -0.368, p = 0.003) reveal the kinds of images that are likely to cause hallucinations. Through targeted fine-tuning on our benchmark dataset, we demonstrate improvements in VLM knowledge conflict detection, establishing a foundation for developing hallucination-resilient multimodal systems in information-sensitive environments.","authors":["Peter Carragher","Nikitha Rao","Abhinand Jha","R Raghav","Kathleen M. Carley"],"url":"https://arxiv.org/abs/2502.14908"}
{"created":"2025-05-13","title":"The Euclidean $k$-Matching Problem is NP-hard","abstract":"Let $G$ be a complete edge-weighted graph on $n$ vertices. To each subset of vertices of $G$ assign the cost of the minimum spanning tree of the subset as its weight. Suppose that $n$ is a multiple of some fixed positive integer $k$. The $k$-matching problem is the problem of finding a partition of the vertices of $G$ into $k$-sets, that minimizes the sum of the weights of the $k$-sets. The case $k=3$ has been shown to be NP-hard [Johnsson et al.,1998]. In the Euclidean version, the vertices of $G$ are points in the plane and the weight of an edge is the Euclidean distance between its endpoints. We call this problem the Euclidean $k$-matching problem. We show that, for every fixed $k \\ge 3$, the Euclidean $k$-matching is NP-hard. This resolves an open problem in the literature and provides the first theoretical justification for the use of known heuristic methods in the case $k=3$. We also show that the problem remains NP-hard if the trees are required to be paths.","authors":["Jos\\'e-Miguel D\\'iaz-B\\'a\\~nez","Ruy Fabila-Monroy","Jos\\'e-Manuel Higes-L\\'opez","Nestaly Mar\\'in","Miguel-Angel Per\\'ez-Cuti\\~no","Pablo P\\'erez-Lantero"],"url":"https://arxiv.org/abs/2502.15660"}
{"created":"2025-05-13","title":"Clinical Inspired MRI Lesion Segmentation","abstract":"Magnetic resonance imaging (MRI) is a potent diagnostic tool for detecting pathological tissues in various diseases. Different MRI sequences have different contrast mechanisms and sensitivities for different types of lesions, which pose challenges to accurate and consistent lesion segmentation. In clinical practice, radiologists commonly use the sub-sequence feature, i.e. the difference between post contrast-enhanced T1-weighted (post) and pre-contrast-enhanced (pre) sequences, to locate lesions. Inspired by this, we propose a residual fusion method to learn subsequence representation for MRI lesion segmentation. Specifically, we iteratively and adaptively fuse features from pre- and post-contrast sequences at multiple resolutions, using dynamic weights to achieve optimal fusion and address diverse lesion enhancement patterns. Our method achieves state-of-the-art performances on BraTS2023 dataset for brain tumor segmentation and our in-house breast MRI dataset for breast lesion segmentation. Our method is clinically inspired and has the potential to facilitate lesion segmentation in various applications.","authors":["Lijun Yan","Churan Wang","Fangwei Zhong","Yizhou Wang"],"url":"https://arxiv.org/abs/2502.16032"}
{"created":"2025-05-13","title":"The Dynamics of Collective Creativity in Human-AI Social Networks","abstract":"Generative AI is reshaping modern culture, enabling individuals to create high-quality outputs across domains such as images, text, and music. However, we know little about the impact of generative AI on collective creativity. This study investigates how human-AI interactions shape collective creativity within experimental social networks. We conducted large-scale online experiments with 879 participants and AI agents in a creative writing task. Participants (either humans or AI) joined 5x5 grid-based networks, and were asked to iteratively select, modify, and share stories. Initially, AI-only networks showed greater creativity (rated by a separate group of 94 human raters) and diversity than human-only and human-AI networks. However, over time, hybrid human-AI networks became more diverse in their creations than AI-only networks. In part, this is because AI agents retained little from the original stories, while human-only networks preserved continuity. These findings highlight the value of experimental social networks in understanding human-AI hybrid societies.","authors":["Shota Shiiku","Raja Marjieh","Manuel Anglada-Tort","Nori Jacoby"],"url":"https://arxiv.org/abs/2502.17962"}
{"created":"2025-05-13","title":"A Sliding Layer Merging Method for Efficient Depth-Wise Pruning in LLMs","abstract":"Compared to width-wise pruning, depth-wise pruning can significantly accelerate inference in resource-constrained scenarios. However, treating the entire Transformer layer as the minimum pruning unit may degrade model performance by indiscriminately discarding the entire information of the layer. This paper reveals the ``Patch-like'' feature relationship between layers in large language models by analyzing the correlation of the outputs of different layers in the reproducing kernel Hilbert space. Building on this observation, we propose a sliding layer merging method that dynamically selects and fuses consecutive layers from top to bottom according to a pre-defined similarity threshold, thereby simplifying the model structure while maintaining its performance. Extensive experiments on LLMs with various architectures and different parameter scales show that our method outperforms existing pruning techniques in both zero-shot inference performance and retraining recovery quality after pruning. In particular, in the experiment with 35% pruning on the Vicuna-7B model, our method achieved a 1.654% improvement in average performance on zero-shot tasks compared to the existing method. Moreover, we further reveal the potential of combining depth pruning with width pruning to enhance the pruning effect. Our codes are available at https://github.com/920927/SLM-a-sliding-layer-merging-method.","authors":["Xuan Ding","Rui Sun","Yunjian Zhang","Xiu Yan","Yueqi Zhou","Kaihao Huang","Suzhong Fu","Chuanlong Xie","Yao Zhu"],"url":"https://arxiv.org/abs/2502.19159"}
{"created":"2025-05-13","title":"SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking","abstract":"The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\\% strict accuracy on ISE-DSC01 and 80.82\\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.","authors":["Dien X. Tran","Nam V. Nguyen","Thanh T. Tran","Anh T. Hoang","Tai V. Duong","Di T. Le","Phuc-Lu Le"],"url":"https://arxiv.org/abs/2503.00955"}
{"created":"2025-05-13","title":"Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator","abstract":"While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective, which minimizes the forward KL divergence, inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that integrates likelihood-based generative training and GAN-type discrimination to bypass this fundamental constraint by exploiting reverse KL and self-generated negative signals. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58/1.96 to new records of 1.30/0.97/1.26 on CIFAR-10/ImageNet-64/ImageNet 512x512 datasets without any guidance mechanisms, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256x256.","authors":["Kaiwen Zheng","Yongxin Chen","Huayu Chen","Guande He","Ming-Yu Liu","Jun Zhu","Qinsheng Zhang"],"url":"https://arxiv.org/abs/2503.01103"}
{"created":"2025-05-13","title":"HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition","abstract":"Incorrect boundary division, complex semantic representation, and differences in pronunciation and meaning often lead to errors in Chinese Named Entity Recognition(CNER). To address these issues, this paper proposes HREB-CRF framework: Hierarchical Reduced-bias EMA with CRF. The proposed method amplifies word boundaries and pools long text gradients through exponentially fixed-bias weighted average of local and global hierarchical attention. Experimental results on the MSRA, Resume, and Weibo datasets show excellent in F1, outperforming the baseline model by 1.1\\%, 1.6\\%, and 9.8\\%. The significant improvement in F1 shows evidences of strong effectiveness and robustness of approach in CNER tasks.","authors":["Sijin Sun","Ming Deng","Xinrui Yu","Liangbin Zhao"],"url":"https://arxiv.org/abs/2503.01217"}
{"created":"2025-05-13","title":"Self-Adaptive Gamma Context-Aware SSM-based Model for Metal Defect Detection","abstract":"Metal defect detection is critical in industrial quality assurance, yet existing methods struggle with grayscale variations and complex defect states, limiting its robustness. To address these challenges, this paper proposes a Self-Adaptive Gamma Context-Aware SSM-based model(GCM-DET). This advanced detection framework integrating a Dynamic Gamma Correction (GC) module to enhance grayscale representation and optimize feature extraction for precise defect reconstruction. A State-Space Search Management (SSM) architecture captures robust multi-scale features, effectively handling defects of varying shapes and scales. Focal Loss is employed to mitigate class imbalance and refine detection accuracy. Additionally, the CD5-DET dataset is introduced, specifically designed for port container maintenance, featuring significant grayscale variations and intricate defect patterns. Experimental results demonstrate that the proposed model achieves substantial improvements, with mAP@0.5 gains of 27.6\\%, 6.6\\%, and 2.6\\% on the CD5-DET, NEU-DET, and GC10-DET datasets.","authors":["Sijin Sun","Ming Deng","Xingrui Yu","Xingyu Xi","Liangbin Zhao"],"url":"https://arxiv.org/abs/2503.01234"}
{"created":"2025-05-13","title":"Can (A)I Change Your Mind?","abstract":"The increasing integration of large language model (LLM) based conversational agents into everyday life raises critical cognitive and social questions about their potential to influence human opinions. Although previous studies have shown that LLM-based agents can generate persuasive content, these typically involve controlled, English-language settings. Addressing this, our preregistered study explored LLM's persuasive capabilities in more ecological, unconstrained scenarios, examining both static (written paragraphs) and dynamic (conversations via Telegram) interaction types. Conducted entirely in Hebrew with 200 participants, the study assessed the persuasive effects of both LLM and human interlocutors on controversial civil policy topics. Results indicated that participants adopted LLM and human perspectives similarly, with significant opinion changes evident across all conditions, regardless of interlocutor type or interaction mode. Confidence levels increased significantly in most scenarios, except in static LLM interactions. These findings demonstrate LLM-based agents' robust persuasive capabilities across diverse sources and settings, highlighting their potential impact on shaping public opinions.","authors":["Miriam Havin","Timna Wharton Kleinman","Moran Koren","Yaniv Dover","Ariel Goldstein"],"url":"https://arxiv.org/abs/2503.01844"}
{"created":"2025-05-13","title":"NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT","abstract":"SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in content generated by various large language models (LLMs) across multiple languages. This task involves not only identifying the presence of hallucinations but also pinpointing their specific occurrences. To tackle this challenge, this study introduces two methods: modified RefChecker and modified SelfCheckGPT. The modified RefChecker integrates prompt-based factual verification into References, structuring them as claim-based tests rather than single external knowledge sources. The modified SelfCheckGPT incorporates external knowledge to overcome its reliance on internal knowledge. In addition, both methods' original prompt designs are enhanced to identify hallucinated words within LLM-generated texts. Experimental results demonstrate the effectiveness of the approach, achieving a high ranking on the test dataset in detecting hallucinations across various languages, with an average IoU of 0.5310 and an average COR of 0.5669.","authors":["Jiaying Hong","Thanet Markchom","Jianfei Xu","Tong Wu","Huizhi Liang"],"url":"https://arxiv.org/abs/2503.01921"}
{"created":"2025-05-13","title":"The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models","abstract":"Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language Models (LLMs) with human preferences, particularly as LLMs increasingly interact with multimodal data. However, we find that MM-RMs trained on existing datasets often struggle to generalize to out-of-distribution data due to their reliance on unimodal spurious correlations, primarily text-only shortcuts within the training distribution, which prevents them from leveraging true multimodal reward functions. To address this, we introduce a Shortcut-aware MM-RM learning algorithm that mitigates this issue by dynamically reweighting training samples, shifting the distribution toward better multimodal understanding, and reducing dependence on unimodal spurious correlations. Our experiments demonstrate significant improvements in generalization, downstream task performance, and scalability, establishing a more robust framework for multimodal reward modeling.","authors":["Zichao Li","Xueru Wen","Jie Lou","Yuqiu Ji","Yaojie Lu","Xianpei Han","Debing Zhang","Le Sun"],"url":"https://arxiv.org/abs/2503.03122"}
{"created":"2025-05-13","title":"Information Theory Strikes Back: New Development in the Theory of Cardinality Estimation","abstract":"Estimating the cardinality of the output of a query is a fundamental problem in database query processing.","authors":["Mahmoud Abo Khamis","Vasileios Nakos","Dan Olteanu","Dan Suciu"],"url":"https://arxiv.org/abs/2503.03290"}
{"created":"2025-05-13","title":"Robust Learning of Diverse Code Edits","abstract":"Software engineering activities frequently involve edits to existing code. However, contemporary code language models (LMs) lack the ability to handle diverse types of code-edit requirements. In this work, we attempt to overcome this shortcoming through (1) a novel synthetic data generation pipeline and (2) a robust model adaptation algorithm. Starting with seed code examples and diverse editing criteria, our pipeline generates high-quality samples comprising original and modified code, along with natural language instructions in different styles and verbosity. Today's code LMs come bundled with strong abilities, such as code generation and instruction following, which should not be lost due to fine-tuning. To ensure this, we propose a novel adaptation algorithm, SeleKT, that (a) leverages a dense gradient-based step to identify the weights that are most important for code editing, and (b) does a sparse projection onto the base model to avoid overfitting. Using our approach, we obtain a new series of models NextCoder (adapted from QwenCoder-2.5) that achieves strong results on five code-editing benchmarks, outperforming comparable size models and even several larger ones. We show the generality of our approach on two model families (DeepSeekCoder and QwenCoder), compare against other fine-tuning approaches, and demonstrate robustness by showing retention of code generation and general problem-solving abilities post adaptation. We opensource the models, synthetic dataset, and implementation at https://aka.ms/nextcoder.","authors":["Tushar Aggarwal","Swayam Singh","Abhijeet Awasthi","Aditya Kanade","Nagarajan Natarajan"],"url":"https://arxiv.org/abs/2503.03656"}
{"created":"2025-05-13","title":"InFL-UX: A Toolkit for Web-Based Interactive Federated Learning","abstract":"This paper presents InFL-UX, an interactive, proof-of-concept browser-based Federated Learning (FL) toolkit designed to integrate user contributions seamlessly into the machine learning (ML) workflow. InFL-UX enables users across multiple devices to upload datasets, define classes, and collaboratively train classification models directly in the browser using modern web technologies. Unlike traditional FL toolkits, which often focus on backend simulations, InFL-UX provides a simple user interface for researchers to explore how users interact with and contribute to FL systems in real-world, interactive settings. By prioritising usability and decentralised model training, InFL-UX bridges the gap between FL and Interactive Machine Learning (IML), empowering non-technical users to actively participate in ML classification tasks.","authors":["Tim Maurer","Abdulrahman Mohamed Selim","Hasan Md Tusfiqur Alam","Matthias Eiletz","Michael Barz","Daniel Sonntag"],"url":"https://arxiv.org/abs/2503.04318"}
{"created":"2025-05-13","title":"Sparse identification of nonlinear dynamics with high accuracy and reliability under noisy conditions for applications to industrial systems","abstract":"This paper proposes a sparse identification of nonlinear dynamics (SINDy) with control and exogenous inputs for highly accurate and reliable prediction. The method is applied to the diesel engine airpath systems, which are known as a nonlinear complicated industrial system. Although SINDy is recognized as a remarkable approach for identifying nonlinear systems, several challenges remain. Its application to industrial systems remains limited, and multi-step predictions are not guaranteed due to overfitting and noisy data. This phenomenon is often caused by the increase in basis functions resulting from the extension of coordinates, such as time-delay embedding. To address these problems, we propose an emphasized SINDy by incorporating ensemble learning, elite gathering, and classification techniques while keeping convex calculation. The proposed method employs library bagging and extracts elites with an R-squared greater than 90%. Then, clustering is performed on the surviving elites because physically motivated basis functions are not always available, and the elites obtained do not always show the same trends. After the classification, discrete model candidates are obtained by taking the mean of each classified elite. Finally, the best model is selected. The simulation results show that the proposed method realizes multi-step prediction for the airpath system, which is known to be a complicated industrial system under noisy conditions.","authors":["Shuichi Yahagi","Ansei Yonezawa","Hiroki Seto","Heisei Yonezawa","Itsuro Kajiwara"],"url":"https://arxiv.org/abs/2503.05154"}
{"created":"2025-05-13","title":"Enhancing Layer Attention Efficiency through Pruning Redundant Retrievals","abstract":"Growing evidence suggests that layer attention mechanisms, which enhance interaction among layers in deep neural networks, have significantly advanced network architectures. However, existing layer attention methods suffer from redundancy, as attention weights learned by adjacent layers often become highly similar. This redundancy causes multiple layers to extract nearly identical features, reducing the model's representational capacity and increasing training time. To address this issue, we propose a novel approach to quantify redundancy by leveraging the Kullback-Leibler (KL) divergence between adjacent layers. Additionally, we introduce an Enhanced Beta Quantile Mapping (EBQM) method that accurately identifies and skips redundant layers, thereby maintaining model stability. Our proposed Efficient Layer Attention (ELA) architecture, improves both training efficiency and overall performance, achieving a 30% reduction in training time while enhancing performance in tasks such as image classification and object detection.","authors":["Hanze Li","Xiande Huang"],"url":"https://arxiv.org/abs/2503.06473"}
{"created":"2025-05-13","title":"Introducing Unbiased Depth into 2D Gaussian Splatting for High-accuracy Surface Reconstruction","abstract":"Recently, 2D Gaussian Splatting (2DGS) has demonstrated superior geometry reconstruction quality than the popular 3DGS by using 2D surfels to approximate thin surfaces. However, it falls short when dealing with glossy surfaces, resulting in visible holes in these areas. We found the reflection discontinuity causes the issue. To fit the jump from diffuse to specular reflection at different viewing angles, depth bias is introduced in the optimized Gaussian primitives. To address that, we first replace the depth distortion loss in 2DGS with a novel depth convergence loss, which imposes a strong constraint on depth continuity. Then, we rectified the depth criterion in determining the actual surface, which fully accounts for all the intersecting Gaussians along the ray. Qualitative and quantitative evaluations across various datasets reveal that our method significantly improves reconstruction quality, with more complete and accurate surfaces than 2DGS.","authors":["Xiaoming Peng","Yixin Yang","Yang Zhou","Hui Huang"],"url":"https://arxiv.org/abs/2503.06587"}
{"created":"2025-05-13","title":"Process-Supervised LLM Recommenders via Flow-guided Tuning","abstract":"While large language models (LLMs) are increasingly adapted for recommendation systems via supervised fine-tuning (SFT), this approach amplifies popularity bias due to its likelihood maximization objective, compromising recommendation diversity and fairness. To address this, we present Flow-guided fine-tuning recommender (Flower), which replaces SFT with a Generative Flow Network (GFlowNet) framework that enacts process supervision through token-level reward propagation. Flower's key innovation lies in decomposing item-level rewards into constituent token rewards, enabling direct alignment between token generation probabilities and their reward signals. This mechanism achieves three critical advancements: (1) popularity bias mitigation and fairness enhancement through empirical distribution matching, (2) preservation of diversity through GFlowNet's proportional sampling, and (3) flexible integration of personalized preferences via adaptable token rewards. Experiments demonstrate Flower's superior distribution-fitting capability and its significant advantages over traditional SFT in terms of accuracy, fairness, and diversity, highlighting its potential to improve LLM-based recommendation systems. The implementation is available via https://github.com/MrPeach0301/Flower","authors":["Chongming Gao","Mengyao Gao","Chenxiao Fan","Shuai Yuan","Wentao Shi","Xiangnan He"],"url":"https://arxiv.org/abs/2503.07377"}
{"created":"2025-05-13","title":"Referring to Any Person","abstract":"Humans are undoubtedly the most important participants in computer vision, and the ability to detect any individual given a natural language description, a task we define as referring to any person, holds substantial practical value. However, we find that existing models generally fail to achieve real-world usability, and current benchmarks are limited by their focus on one-to-one referring, that hinder progress in this area. In this work, we revisit this task from three critical perspectives: task definition, dataset design, and model architecture. We first identify five aspects of referable entities and three distinctive characteristics of this task. Next, we introduce HumanRef, a novel dataset designed to tackle these challenges and better reflect real-world applications. From a model design perspective, we integrate a multimodal large language model with an object detection framework, constructing a robust referring model named RexSeek. Experimental results reveal that state-of-the-art models, which perform well on commonly used benchmarks like RefCOCO/+/g, struggle with HumanRef due to their inability to detect multiple individuals. In contrast, RexSeek not only excels in human referring but also generalizes effectively to common object referring, making it broadly applicable across various perception tasks. Code is available at https://github.com/IDEA-Research/RexSeek","authors":["Qing Jiang","Lin Wu","Zhaoyang Zeng","Tianhe Ren","Yuda Xiong","Yihao Chen","Qin Liu","Lei Zhang"],"url":"https://arxiv.org/abs/2503.08507"}
{"created":"2025-05-13","title":"Multi-camera orientation tracking method for anisotropic particles in particle-laden flows","abstract":"A method for particle orientation tracking is developed and demonstrated specifically for anisotropic particles. Using (high-speed) multi-camera recordings of anisotropic particles from different viewpoints, we reconstruct the 3D location and orientation of these particles using their known shape. This paper describes an algorithm which tracks the location and orientation of multiple anisotropic particles over time, enabling detailed investigations of location, orientation, and rotation statistics. The robustness and error of this method is quantified, and we explore the effects of noise, image size, the number of used cameras, and the camera arrangement by applying the algorithm to synthetic images. We showcase several use-cases of this method in several experiments (in both quiescent and turbulent fluids), demonstrating the effectiveness and broad applicability of the described tracking method. The proposed method is shown to work for widely different particle shapes, successfully tracks multiple particles simultaneously, and the method can distinguish between different types of particles.","authors":["Mees M. Flapper","Elian Bernard","Sander G. Huisman"],"url":"https://arxiv.org/abs/2503.08694"}
{"created":"2025-05-13","title":"SICNav-Diffusion: Safe and Interactive Crowd Navigation with Diffusion Trajectory Predictions","abstract":"To navigate crowds without collisions, robots must interact with humans by forecasting their future motion and reacting accordingly. While learning-based prediction models have shown success in generating likely human trajectory predictions, integrating these stochastic models into a robot controller presents several challenges. The controller needs to account for interactive coupling between planned robot motion and human predictions while ensuring both predictions and robot actions are safe (i.e. collision-free). To address these challenges, we present a receding horizon crowd navigation method for single-robot multi-human environments. We first propose a diffusion model to generate joint trajectory predictions for all humans in the scene. We then incorporate these multi-modal predictions into a SICNav Bilevel MPC problem that simultaneously solves for a robot plan (upper-level) and acts as a safety filter to refine the predictions for non-collision (lower-level). Combining planning and prediction refinement into one bilevel problem ensures that the robot plan and human predictions are coupled. We validate the open-loop trajectory prediction performance of our diffusion model on the commonly used ETH/UCY benchmark and evaluate the closed-loop performance of our robot navigation method in simulation and extensive real-robot experiments demonstrating safe, efficient, and reactive robot motion.","authors":["Sepehr Samavi","Anthony Lem","Fumiaki Sato","Sirui Chen","Qiao Gu","Keijiro Yano","Angela P. Schoellig","Florian Shkurti"],"url":"https://arxiv.org/abs/2503.08858"}
{"created":"2025-05-13","title":"I Predict Therefore I Am: Is Next Token Prediction Enough to Learn Human-Interpretable Concepts from Data?","abstract":"The remarkable achievements of large language models (LLMs) have led many to conclude that they exhibit a form of intelligence. This is as opposed to explanations of their capabilities based on their ability to perform relatively simple manipulations of vast volumes of data. To illuminate the distinction between these explanations, we introduce a novel generative model that generates tokens on the basis of human-interpretable concepts represented as latent discrete variables. Under mild conditions, even when the mapping from the latent space to the observed space is non-invertible, we establish an identifiability result, i.e., the representations learned by LLMs through next-token prediction can be approximately modeled as the logarithm of the posterior probabilities of these latent discrete concepts given input context, up to an invertible linear transformation. This theoretical finding not only provides evidence that LLMs capture underlying generative factors, but also provide a unified prospective for understanding of the linear representation hypothesis. Taking this a step further, our finding motivates a reliable evaluation of sparse autoencoders by treating the performance of supervised concept extractors as an upper bound. Pushing this idea even further, it inspires a structural variant that enforces dependence among latent concepts in addition to promoting sparsity. Empirically, we validate our theoretical results through evaluations on both simulation data and the Pythia, Llama, and DeepSeek model families, and demonstrate the effectiveness of our structured sparse autoencoder.","authors":["Yuhang Liu","Dong Gong","Yichao Cai","Erdun Gao","Zhen Zhang","Biwei Huang","Mingming Gong","Anton van den Hengel","Javen Qinfeng Shi"],"url":"https://arxiv.org/abs/2503.08980"}
{"created":"2025-05-13","title":"A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization","abstract":"Automatic patent summarization approaches that help in the patent analysis and comprehension procedure are in high demand due to the colossal growth of innovations. The development of natural language processing (NLP), text mining, and deep learning has notably amplified the efficacy of text summarization models for abundant types of documents. Summarizing patent text remains a pertinent challenge due to the labyrinthine writing style of these documents, which includes technical and legal intricacies. Additionally, these patent document contents are considerably lengthier than archetypal documents, which complicates the process of extracting pertinent information for summarization. Embodying extractive and abstractive text summarization methodologies into a hybrid framework, this study proposes a system for efficiently creating abstractive summaries of patent records. The procedure involves leveraging the LexRank graph-based algorithm to retrieve the important sentences from input parent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART) model that has been fine-tuned using Low-Ranking Adaptation (LoRA) for producing text summaries. This is accompanied by methodical testing and evaluation strategies. Furthermore, the author employed certain meta-learning techniques to achieve Domain Generalization (DG) of the abstractive component across multiple patent fields.","authors":["Nevidu Jayatilleke","Ruvan Weerasinghe"],"url":"https://arxiv.org/abs/2503.10354"}
{"created":"2025-05-13","title":"Being-0: A Humanoid Robotic Agent with Vision-Language Models and Modular Skills","abstract":"Building autonomous robotic agents capable of achieving human-level performance in real-world embodied tasks is an ultimate goal in humanoid robot research. Recent advances have made significant progress in high-level cognition with Foundation Models (FMs) and low-level skill development for humanoid robots. However, directly combining these components often results in poor robustness and efficiency due to compounding errors in long-horizon tasks and the varied latency of different modules. We introduce Being-0, a hierarchical agent framework that integrates an FM with a modular skill library. The FM handles high-level cognitive tasks such as instruction understanding, task planning, and reasoning, while the skill library provides stable locomotion and dexterous manipulation for low-level control. To bridge the gap between these levels, we propose a novel Connector module, powered by a lightweight vision-language model (VLM). The Connector enhances the FM's embodied capabilities by translating language-based plans into actionable skill commands and dynamically coordinating locomotion and manipulation to improve task success. With all components, except the FM, deployable on low-cost onboard computation devices, Being-0 achieves efficient, real-time performance on a full-sized humanoid robot equipped with dexterous hands and active vision. Extensive experiments in large indoor environments demonstrate Being-0's effectiveness in solving complex, long-horizon tasks that require challenging navigation and manipulation subtasks. For further details and videos, visit https://beingbeyond.github.io/Being-0.","authors":["Haoqi Yuan","Yu Bai","Yuhui Fu","Bohan Zhou","Yicheng Feng","Xinrun Xu","Yi Zhan","B\\\"orje F. Karlsson","Zongqing Lu"],"url":"https://arxiv.org/abs/2503.12533"}
{"created":"2025-05-13","title":"Omega-Regular Robustness","abstract":"Roughly speaking, a system is said to be robust if it can resist disturbances and still function correctly. For instance, if the requirement is that the temperature remains in an allowed range $[l,h]$, then a system that remains in a range $[l',h']\\subset[l,h]$ is more robust than one that reaches $l$ and $h$ from time to time. In this example the initial specification is quantitative in nature, this is not the case in $\\omega$-regular properties. Still, it seems there is a natural robustness preference relation induced by an $\\omega$-regular property. E.g. for a property requiring that every request is eventually granted, one would say that a system where requests are granted two ticks after they are issued is more robust than one in which requests are answered ninety ticks after they are issued. In this work we manage to distill a robustness preference relation that is induced by a given $\\omega$-regular language. The robustness preference relation is a semantic notion (agnostic to the given representation of the language) that relies on Wagner's hierarchy and on Ehlers and Schewe's definition of natural rank of infinite words. It aligns with our intuitions on common examples, satisfies some natural mathematical criteria, and refines Tabuada and Neider's five-valued semantics into an infinite domain.","authors":["Dana Fisman","Elina Sudit"],"url":"https://arxiv.org/abs/2503.12631"}
{"created":"2025-05-13","title":"A super-resolution reconstruction method for lightweight building images based on an expanding feature modulation network","abstract":"This study proposes a lightweight method for building image super-resolution using a Dilated Contextual Feature Modulation Network (DCFMN). The process includes obtaining high-resolution images, down-sampling them to low-resolution, enhancing the low-resolution images, constructing and training a lightweight network model, and generating super-resolution outputs. To address challenges such as regular textures and long-range dependencies in building images, the DCFMN integrates an expansion separable modulation unit and a local feature enhancement module. The former employs multiple expansion convolutions equivalent to a large kernel to efficiently aggregate multi-scale features while leveraging a simple attention mechanism for adaptivity. The latter encodes local features, mixes channel information, and ensures no additional computational burden during inference through reparameterization. This approach effectively resolves the limitations of existing lightweight super-resolution networks in modeling long-range dependencies, achieving accurate and efficient global feature modeling without increasing computational costs, and significantly improving both reconstruction quality and lightweight efficiency for building image super-resolution models.","authors":["Yi Zhang","Ruonan Lin","Ang Ping"],"url":"https://arxiv.org/abs/2503.13179"}
{"created":"2025-05-13","title":"Channel Estimation for Pinching-Antenna Systems (PASS)","abstract":"Pinching Antennas (PAs) represent a revolutionary flexible antenna technology that leverages dielectric waveguides and electromagnetic coupling to mitigate large-scale path loss. This letter is the first to explore channel estimation for Pinching-Antenna SyStems (PASS), addressing their uniquely ill-conditioned and underdetermined channel characteristics. In particular, two efficient deep learning-based channel estimators are proposed. 1) PAMoE: This estimator incorporates dynamic padding, feature embedding, fusion, and mixture of experts (MoE) modules, which effectively leverage the positional information of PAs and exploit expert diversity. 2) PAformer: This Transformer-style estimator employs the self-attention mechanism to predict channel coefficients in a per-antenna manner, which offers more flexibility to adaptively deal with dynamic numbers of PAs in practical deployment. Numerical results demonstrate that 1) the proposed deep learning-based channel estimators outperform conventional methods and exhibit excellent zero-shot learning capabilities, and 2) PAMoE delivers higher channel estimation accuracy via MoE specialization, while PAformer natively handles an arbitrary number of PAs, trading self-attention complexity for superior scalability.","authors":["Jian Xiao","Ji Wang","Yuanwei Liu"],"url":"https://arxiv.org/abs/2503.13268"}
{"created":"2025-05-13","title":"Less is More: Improving Motion Diffusion Models with Sparse Keyframes","abstract":"Recent advances in motion diffusion models have led to remarkable progress in diverse motion generation tasks, including text-to-motion synthesis. However, existing approaches represent motions as dense frame sequences, requiring the model to process redundant or less informative frames. The processing of dense animation frames imposes significant training complexity, especially when learning intricate distributions of large motion datasets even with modern neural architectures. This severely limits the performance of generative motion models for downstream tasks. Inspired by professional animators who mainly focus on sparse keyframes, we propose a novel diffusion framework explicitly designed around sparse and geometrically meaningful keyframes. Our method reduces computation by masking non-keyframes and efficiently interpolating missing frames. We dynamically refine the keyframe mask during inference to prioritize informative frames in later diffusion steps. Extensive experiments show that our approach consistently outperforms state-of-the-art methods in text alignment and motion realism, while also effectively maintaining high performance at significantly fewer diffusion steps. We further validate the robustness of our framework by using it as a generative prior and adapting it to different downstream tasks.","authors":["Jinseok Bae","Inwoo Hwang","Young Yoon Lee","Ziyu Guo","Joseph Liu","Yizhak Ben-Shabat","Young Min Kim","Mubbasir Kapadia"],"url":"https://arxiv.org/abs/2503.13859"}
{"created":"2025-05-13","title":"TGBFormer: Transformer-GraphFormer Blender Network for Video Object Detection","abstract":"Video object detection has made significant progress in recent years thanks to convolutional neural networks (CNNs) and vision transformers (ViTs). Typically, CNNs excel at capturing local features but struggle to model global representations. Conversely, ViTs are adept at capturing long-range global features but face challenges in representing local feature details. Off-the-shelf video object detection methods solely rely on CNNs or ViTs to conduct feature aggregation, which hampers their capability to simultaneously leverage global and local information, thereby resulting in limited detection performance. In this paper, we propose a Transformer-GraphFormer Blender Network (TGBFormer) for video object detection, with three key technical improvements to fully exploit the advantages of transformers and graph convolutional networks while compensating for their limitations. First, we develop a spatial-temporal transformer module to aggregate global contextual information, constituting global representations with long-range feature dependencies. Second, we introduce a spatial-temporal GraphFormer module that utilizes local spatial and temporal relationships to aggregate features, generating new local representations that are complementary to the transformer outputs. Third, we design a global-local feature blender module to adaptively couple transformer-based global representations and GraphFormer-based local representations. Extensive experiments demonstrate that our TGBFormer establishes new state-of-the-art results on the ImageNet VID dataset. Particularly, our TGBFormer achieves 86.5% mAP while running at around 41.0 FPS on a single Tesla A100 GPU.","authors":["Qiang Qi","Xiao Wang"],"url":"https://arxiv.org/abs/2503.13903"}
{"created":"2025-05-13","title":"Predicting Human Choice Between Textually Described Lotteries","abstract":"Predicting human decision-making under risk and uncertainty is a long-standing challenge in cognitive science, economics, and AI. While prior research has focused on numerically described lotteries, real-world decisions often rely on textual descriptions. This study conducts the first large-scale exploration of human decision-making in such tasks using a large dataset of one-shot binary choices between textually described lotteries. We evaluate multiple computational approaches, including fine-tuning Large Language Models (LLMs), leveraging embeddings, and integrating behavioral theories of choice under risk. Our results show that fine-tuned LLMs, specifically GPT-4o, outperform hybrid models that incorporate behavioral theory, challenging established methods in numerical settings. These findings highlight fundamental differences in how textual and numerical information influence decision-making and underscore the need for new modeling strategies to bridge this gap.","authors":["Eyal Marantz","Ori Plonsky"],"url":"https://arxiv.org/abs/2503.14004"}
{"created":"2025-05-13","title":"Tiled Flash Linear Attention: More Efficient Linear RNN and xLSTM Kernels","abstract":"Linear RNNs with gating recently demonstrated competitive performance compared to Transformers in language modeling. Although their linear compute scaling in sequence length offers theoretical runtime advantages over Transformers, realizing these benefits in practice requires optimized custom kernels, as Transformers rely on the highly efficient Flash Attention kernels (Dao, 2024). Leveraging the chunkwise-parallel formulation of linear RNNs, Flash Linear Attention (FLA) (Yang & Zhang, 2024) shows that linear RNN kernels are faster than Flash Attention, by parallelizing over chunks of the input sequence. However, since the chunk size of FLA is limited, many intermediate states must be materialized in GPU memory. This leads to low arithmetic intensity and causes high memory consumption and IO cost, especially for long-context pre-training. In this work, we present Tiled Flash Linear Attention (TFLA), a novel kernel algorithm for linear RNNs, that enables arbitrary large chunk sizes and high arithmetic intensity by introducing an additional level of sequence parallelization within each chunk. First, we apply TFLA to the xLSTM with matrix memory, the mLSTM (Beck et al., 2024). Second, we propose an mLSTM variant with sigmoid input gate and reduced computation for even faster kernel runtimes at equal language modeling performance. In our speed benchmarks, we show that our new mLSTM kernels based on TFLA outperform highly optimized Flash Attention, Linear Attention and Mamba kernels, setting a new state of the art for efficient long-context sequence modeling primitives.","authors":["Maximilian Beck","Korbinian P\\\"oppel","Phillip Lippe","Sepp Hochreiter"],"url":"https://arxiv.org/abs/2503.14376"}
{"created":"2025-05-13","title":"Controlling Peak Sharpness in Multimodal Biomolecular Systems via the Chemical Fokker-Planck Equation","abstract":"Intracellular biomolecular systems exhibit intrinsic stochasticity due to low molecular copy numbers, leading to multimodal probability distributions that play a crucial role in probabilistic differentiation and cellular decision-making. Controlling the dispersion of multimodal probability distributions in biomolecular systems is critical for regulating stochastic behavior, robustness, and adaptability. However, modifying system parameters to adjust dispersion often affects peak positions, potentially altering a desired phenotype or even fundamental behavior in a genetic pathway. In this paper, we establish a theoretical framework that enables independent control of dispersion while preserving peak positions and modality using the Chemical Fokker-Planck Equation (CFPE) and sharpness, a measure of probability concentration around individual peaks. By analyzing the steady-state solution of the CFPE, we derive explicit conditions under which peak sharpness can be tuned monotonically without changing peak positions or modality. We validate our approach through Monte Carlo simulations on a bimodal chemical system, demonstrating effective dispersion control while maintaining structural stability. This framework provides a systematic approach for designing biomolecular systems with tunable stochastic properties, contributing to advancements in synthetic biology and probabilistic cellular regulation.","authors":["Taishi Kotsuka","Enoch Yeung"],"url":"https://arxiv.org/abs/2503.14706"}
{"created":"2025-05-13","title":"Using Language Models to Decipher the Motivation Behind Human Behaviors","abstract":"AI presents a novel tool for deciphering the motivations behind human behaviors. By varying prompts to a large language model, we can elicit the full range of human behaviors in a variety of different scenarios in classic economic games. By analyzing which prompts elicit which behaviors, we infer (decipher) the motivations behind the human behaviors. We also show how one can analyze the prompts to reveal relationships between the classic economic games, providing insight into what different economic scenarios induce people to think about. We also show how this deciphering process can be used to understand differences in the behavioral tendencies of different populations. We show how AI offers a new way to examine the thinking and framing that produce different behaviors.","authors":["Yutong Xie","Qiaozhu Mei","Walter Yuan","Matthew O. Jackson"],"url":"https://arxiv.org/abs/2503.15752"}
{"created":"2025-05-13","title":"Towards Agentic AI Networking in 6G: A Generative Foundation Model-as-Agent Approach","abstract":"The promising potential of AI and network convergence in improving networking performance and enabling new service capabilities has recently attracted significant interest. Existing network AI solutions, while powerful, are mainly built based on the close-loop and passive learning framework, resulting in major limitations in autonomous solution finding and dynamic environmental adaptation. Agentic AI has recently been introduced as a promising solution to address the above limitations and pave the way for true generally intelligent and beneficial AI systems. The key idea is to create a networking ecosystem to support a diverse range of autonomous and embodied AI agents in fulfilling their goals. In this paper, we focus on the novel challenges and requirements of agentic AI networking. We propose AgentNet, a novel framework for supporting interaction, collaborative learning, and knowledge transfer among AI agents. We introduce a general architectural framework of AgentNet and then propose a generative foundation model (GFM)-based implementation in which multiple GFM-as-agents have been created as an interactive knowledge-base to bootstrap the development of embodied AI agents according to different task requirements and environmental features. We consider two application scenarios, digital-twin-based industrial automation and metaverse-based infotainment system, to describe how to apply AgentNet for supporting efficient task-driven collaboration and interaction among AI agents.","authors":["Yong Xiao","Guangming Shi","Ping Zhang"],"url":"https://arxiv.org/abs/2503.15764"}
{"created":"2025-05-13","title":"A Vision Centric Remote Sensing Benchmark","abstract":"Multimodal Large Language Models (MLLMs) have achieved remarkable success in vision-language tasks but their remote sensing (RS) counterpart are relatively under explored. Unlike natural images, RS imagery presents unique challenges that current MLLMs struggle to handle, particularly in visual grounding and spatial reasoning. This study investigates the limitations of CLIP-based MLLMs in RS, highlighting their failure to differentiate visually distinct yet semantically similar RS images. To address this, we introduce a remote sensing multimodal visual patterns (RSMMVP) benchmark. It is designed to evaluate MLLMs in RS tasks by identifying the CLIP-blind pairs, where CLIP-based models incorrectly assign high similarity scores to visually distinct RS images. Through a visual question answering (VQA) evaluation, we analyze the performance of state-of-the-art MLLMs, revealing significant limitations in RS specific representation learning. The results provide valuable insights into the weaknesses of CLIP-based visual encoding and offer a foundation for future research to develop more effective MLLMs tailored for remote sensing applications.","authors":["Abduljaleel Adejumo","Faegheh Yeganli","Clifford Broni-bediako","Aoran Xiao","Naoto Yokoya","Mennatullah Siam"],"url":"https://arxiv.org/abs/2503.15816"}
{"created":"2025-05-13","title":"EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation","abstract":"Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenarios with large motion. To address this limitation, we introduce EDEN, an Enhanced Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach first utilizes a transformer-based tokenizer to produce refined latent representations of the intermediate frames for diffusion models. We then enhance the diffusion transformer with temporal attention across the process and incorporate a start-end frame difference embedding to guide the generation of dynamic motion. Extensive experiments demonstrate that EDEN achieves state-of-the-art results across popular benchmarks, including nearly a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.","authors":["Zihao Zhang","Haoran Chen","Haoyu Zhao","Guansong Lu","Yanwei Fu","Hang Xu","Zuxuan Wu"],"url":"https://arxiv.org/abs/2503.15831"}
{"created":"2025-05-13","title":"V-NAW: Video-based Noise-aware Adaptive Weighting for Facial Expression Recognition","abstract":"Facial Expression Recognition (FER) plays a crucial role in human affective analysis and has been widely applied in computer vision tasks such as human-computer interaction and psychological assessment. The 8th Affective Behavior Analysis in-the-Wild (ABAW) Challenge aims to assess human emotions using the video-based Aff-Wild2 dataset. This challenge includes various tasks, including the video-based EXPR recognition track, which is our primary focus. In this paper, we demonstrate that addressing label ambiguity and class imbalance, which are known to cause performance degradation, can lead to meaningful performance improvements. Specifically, we propose Video-based Noise-aware Adaptive Weighting (V-NAW), which adaptively assigns importance to each frame in a clip to address label ambiguity and effectively capture temporal variations in facial expressions. Furthermore, we introduce a simple and effective augmentation strategy to reduce redundancy between consecutive frames, which is a primary cause of overfitting. Through extensive experiments, we validate the effectiveness of our approach, demonstrating significant improvements in video-based FER performance.","authors":["JunGyu Lee","Kunyoung Lee","Haesol Park","Ig-Jae Kim","Gi Pyo Nam"],"url":"https://arxiv.org/abs/2503.15970"}
{"created":"2025-05-13","title":"Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning","abstract":"This paper investigates the role of explicit thinking process in rule-based reinforcement fine-tuning (RFT) for MLLMs. We first propose CLS-RL for MLLM image classification, using verifiable rewards for fine-tuning. Experiments show CLS-RL significantly outperforms SFT and yields a cross-dataset generalization effect. We then rethink and question whether explicit thinking in RFT is always necessary. Challenging the convention that explicit thinking is crucial for the success of RFT, we introduce No-Thinking-RL, exploring RFT without thinking by introducing a simple equality accuracy reward. We evaluate No-Thinking-RL on 6 diverse tasks across different model sizes and types. Experimental results reveal three key findings: 1). Visual perception tasks do not require thinking during RFT, as No-Thinking-RL consistently outperforms or matches Thinking-based RFT across model sizes. 2).} Models with limited capabilities struggle to generate high-quality CoT for RFT, making Thinking-based RFT less effective than No-Thinking-RL. 3). There are inconsistencies between the answers in the thinking and answer tags for some responses of thinking-based RFT, which show lower accuracy than the overall accuracy. We hypothesize that explicit thinking before verifiable answers may hinder reward convergence and reduce performance. To test this hypothesis, we propose Think-After-Answer, which places thinking after the answer to mitigate this effect for experimental verification. Lastly, we conduct a pilot study to explore whether MLLMs can learn when to think during RFT, introducing an Adaptive-Thinking method. Experiments show that it converges to a specific prompt depending on model capability and task complexity, achieving comparable or better performance than both Thinking and No-Thinking-RL. This suggests MLLMs can adaptively decide to think or not based on their capabilities and task complexity.","authors":["Ming Li","Jike Zhong","Shitian Zhao","Yuxiang Lai","Haoquan Zhang","Wang Bill Zhu","Kaipeng Zhang"],"url":"https://arxiv.org/abs/2503.16188"}
{"created":"2025-05-13","title":"Analysis of Forces Exerted by Shoulder and Elbow Fabric-based Pneumatic Actuators for Pediatric Exosuits","abstract":"To enhance pediatric exosuit design, it is crucial to assess the actuator-generated forces. This work evaluates the contact forces exerted by soft fabric-based pneumatic actuators in an upper extremity pediatric exosuit. Two actuators were examined: a single-cell bidirectional actuator for shoulder abduction/adduction and a bellow-type actuator for elbow extension/flexion. Experiments assessed the impact of actuator anchoring points and the adjacent joint's angle on exerted forces and actuated joint range of motion (ROM). These were measured via load cells and encoders integrated into a custom infant-scale engineered apparatus with two degrees of freedom (two revolute joints). For the shoulder actuator, results show that anchoring it further from the shoulder joint center while the elbow is flexed at $90^\\circ$ yields the highest ROM while minimizing the peak force exerted on the body. For the elbow actuator, anchoring it symmetrically while the shoulder joint is at $0^\\circ$ optimizes actuator performance. These findings contribute a key step toward co-optimizing the considered exosuit design for functionality and wearability.","authors":["Mehrnoosh Ayazi","Ipsita Sahin","Caio Mucchiani","Elena Kokkoni","Konstantinos Karydis"],"url":"https://arxiv.org/abs/2503.18376"}
{"created":"2025-05-13","title":"AdaWorld: Learning Adaptable World Models with Latent Actions","abstract":"World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.","authors":["Shenyuan Gao","Siyuan Zhou","Yilun Du","Jun Zhang","Chuang Gan"],"url":"https://arxiv.org/abs/2503.18938"}
{"created":"2025-05-13","title":"UniMoMo: Unified Generative Modeling of 3D Molecules for De Novo Binder Design","abstract":"The design of target-specific molecules such as small molecules, peptides, and antibodies is vital for biological research and drug discovery. Existing generative methods are restricted to single-domain molecules, failing to address versatile therapeutic needs or utilize cross-domain transferability to enhance model performance. In this paper, we introduce Unified generative Modeling of 3D Molecules (UniMoMo), the first framework capable of designing binders of multiple molecular domains using a single model. In particular, UniMoMo unifies the representations of different molecules as graphs of blocks, where each block corresponds to either a standard amino acid or a molecular fragment. Subsequently, UniMoMo utilizes a geometric latent diffusion model for 3D molecular generation, featuring an iterative full-atom autoencoder to compress blocks into latent space points, followed by an E(3)-equivariant diffusion process. Extensive benchmarks across peptides, antibodies, and small molecules demonstrate the superiority of our unified framework over existing domain-specific models, highlighting the benefits of multi-domain training.","authors":["Xiangzhe Kong","Zishen Zhang","Ziting Zhang","Rui Jiao","Jianzhu Ma","Wenbing Huang","Kai Liu","Yang Liu"],"url":"https://arxiv.org/abs/2503.19300"}
{"created":"2025-05-13","title":"Exploring Next Token Prediction For Optimizing Databases","abstract":"The Next Token Prediction paradigm (NTP, for short) lies at the forefront of modern large foundational models that are pre-trained on diverse and large datasets. These models generalize effectively, and have proven to be very successful in Natural Language Processing (NLP). Inspired by the generalization capabilities of Large Language Models (LLMs), we investigate whether the same NTP paradigm can be applied to DBMS design and optimization tasks. Adopting NTP directly for database optimization is non-trivial due to the fundamental differences between the domains. In this paper, we present a framework, termed Probe and Learn (PoLe), for applying NTP to optimize database systems. PoLe leverages Decision Transformers and hardware-generated tokens to effectively incorporate NTP into database systems. As a proof of concept, we demonstrate PoLe in the context of the index scheduling task over NUMA servers in main-memory database systems. Preliminary results for this scheduling task demonstrate that adopting NTP and PoLe can improve both performance and generalizability.","authors":["Yeasir Rayhan","Walid G. Aref"],"url":"https://arxiv.org/abs/2503.19619"}
{"created":"2025-05-13","title":"Semi-supervised Node Importance Estimation with Informative Distribution Modeling for Uncertainty Regularization","abstract":"Node importance estimation, a classical problem in network analysis, underpins various web applications. Previous methods either exploit intrinsic topological characteristics, e.g., graph centrality, or leverage additional information, e.g., data heterogeneity, for node feature enhancement. However, these methods follow the supervised learning setting, overlooking the fact that ground-truth node-importance data are usually partially labeled in practice. In this work, we propose the first semi-supervised node importance estimation framework, i.e., EASING, to improve learning quality for unlabeled data in heterogeneous graphs. Different from previous approaches, EASING explicitly captures uncertainty to reflect the confidence of model predictions. To jointly estimate the importance values and uncertainties, EASING incorporates DJE, a deep encoder-decoder neural architecture. DJE introduces distribution modeling for graph nodes, where the distribution representations derive both importance and uncertainty estimates. Additionally, DJE facilitates effective pseudo-label generation for the unlabeled data to enrich the training samples. Based on labeled and pseudo-labeled data, EASING develops effective semi-supervised heteroscedastic learning with varying node uncertainty regularization. Extensive experiments on three real-world datasets highlight the superior performance of EASING compared to competing methods. Codes are available via https://github.com/yankai-chen/EASING.","authors":["Yankai Chen","Taotao Wang","Yixiang Fang","Yunyu Xiao"],"url":"https://arxiv.org/abs/2503.20697"}
{"created":"2025-05-13","title":"Alleviating LLM-based Generative Retrieval Hallucination in Alipay Search","abstract":"Generative retrieval (GR) has revolutionized document retrieval with the advent of large language models (LLMs), and LLM-based GR is gradually being adopted by the industry. Despite its remarkable advantages and potential, LLM-based GR suffers from hallucination and generates documents that are irrelevant to the query in some instances, severely challenging its credibility in practical applications. We thereby propose an optimized GR framework designed to alleviate retrieval hallucination, which integrates knowledge distillation reasoning in model training and incorporate decision agent to further improve retrieval precision. Specifically, we employ LLMs to assess and reason GR retrieved query-document (q-d) pairs, and then distill the reasoning data as transferred knowledge to the GR model. Moreover, we utilize a decision agent as post-processing to extend the GR retrieved documents through retrieval model and select the most relevant ones from multi perspectives as the final generative retrieval result. Extensive offline experiments on real-world datasets and online A/B tests on Fund Search and Insurance Search in Alipay demonstrate our framework's superiority and effectiveness in improving search quality and conversion gains.","authors":["Yedan Shen","Kaixin Wu","Yuechen Ding","Jingyuan Wen","Hong Liu","Mingjie Zhong","Zhouhan Lin","Jia Xu","Linjian Mo"],"url":"https://arxiv.org/abs/2503.21098"}
{"created":"2025-05-13","title":"A Computational Theory for Efficient Mini Agent Evaluation with Causal Guarantees","abstract":"In order to reduce the cost of experimental evaluation for agents, we introduce a computational theory of evaluation for mini agents: build evaluation model to accelerate the evaluation procedures. We prove upper bounds of generalized error and generalized causal effect error of given evaluation models for infinite agents. We also prove efficiency, and consistency to estimated causal effect from deployed agents to evaluation metric by prediction. To learn evaluation models, we propose a meta-learner to handle heterogeneous agents space problem. Comparing with existed evaluation approaches, our (conditional) evaluation model reduced 24.1\\% to 99.0\\% evaluation errors across 12 scenes, including individual medicine, scientific simulation, social experiment, business activity, and quantum trade. The evaluation time is reduced 3 to 7 order of magnitude per subject comparing with experiments or simulations.","authors":["Hedong Yan"],"url":"https://arxiv.org/abs/2503.21138"}
{"created":"2025-05-13","title":"Static and Repeated Cooperative Games for the Optimization of the AoI in IoT Networks","abstract":"Wireless sensing and the internet of things (IoT) are nowadays pervasive in 5G and beyond networks, and they are expected to play a crucial role in 6G. However, a centralized optimization of a distributed system is not always possible and cost-efficient. In this paper, we analyze a setting in which two sensors collaboratively update a common server seeking to minimize the age of information (AoI) of the latest sample of a common physical process. We consider a distributed and uncoordinated setting where each sensor lacks information about whether the other decides to update the server. This strategic setting is modeled through game theory (GT) and two games are defined: i) a static game of complete information with an incentive mechanism for cooperation, and ii) a repeated game over a finite horizon where the static game is played at each stage. We perform a mathematical analysis of the static game finding three Nash Equilibria (NEs) in pure strategies and one in mixed strategies. A numerical simulation of the repeated game is also presented and novel and valuable insight into the setting is given thanks to the definition of a new metric, the price of delayed updates (PoDU), which shows that the decentralized solution provides results close to the centralized optimum.","authors":["David Emanuele Corrado Raphael Catania","Alessandro Buratto","Giovanni Perin"],"url":"https://arxiv.org/abs/2503.21633"}
{"created":"2025-05-13","title":"LightSNN: Lightweight Architecture Search for Sparse and Accurate Spiking Neural Networks","abstract":"Spiking Neural Networks (SNNs) are highly regarded for their energy efficiency, inherent activation sparsity, and suitability for real-time processing in edge devices. However, most current SNN methods adopt architectures resembling traditional artificial neural networks (ANNs), leading to suboptimal performance when applied to SNNs. While SNNs excel in energy efficiency, they have been associated with lower accuracy levels than traditional ANNs when utilizing conventional architectures. In response, in this work we present LightSNN, a rapid and efficient Neural Network Architecture Search (NAS) technique specifically tailored for SNNs that autonomously leverages the most suitable architecture, striking a good balance between accuracy and efficiency by enforcing sparsity. Based on the spiking NAS network (SNASNet) framework, a cell-based search space including backward connections is utilized to build our training-free pruning-based NAS mechanism. Our technique assesses diverse spike activation patterns across different data samples using a sparsity-aware Hamming distance fitness evaluation. Thorough experiments are conducted on both static (CIFAR10 and CIFAR100) and neuromorphic datasets (DVS128-Gesture). Our LightSNN model achieves state-of-the-art results on CIFAR10 and CIFAR100, improves performance on DVS128Gesture by 4.49\\%, and significantly reduces search time most notably offering a $98\\times$ speedup over SNASNet and running 30\\% faster than the best existing method on DVS128Gesture. Code is available on Github at: https://github.com/YesmineAbdennadher/LightSNN.","authors":["Yesmine Abdennadher","Giovanni Perin","Riccardo Mazzieri","Jacopo Pegoraro","Michele Rossi"],"url":"https://arxiv.org/abs/2503.21846"}
{"created":"2025-05-13","title":"A Survey of WebAgents: Towards Next-Generation AI Agents for Web Automation with Large Foundation Models","abstract":"With the advancement of web techniques, they have significantly revolutionized various aspects of people's lives. Despite the importance of the web, many tasks performed on it are repetitive and time-consuming, negatively impacting overall quality of life. To efficiently handle these tedious daily tasks, one of the most promising approaches is to advance autonomous agents based on Artificial Intelligence (AI) techniques, referred to as AI Agents, as they can operate continuously without fatigue or performance degradation. In the context of the web, leveraging AI Agents -- termed WebAgents -- to automatically assist people in handling tedious daily tasks can dramatically enhance productivity and efficiency. Recently, Large Foundation Models (LFMs) containing billions of parameters have exhibited human-like language understanding and reasoning capabilities, showing proficiency in performing various complex tasks. This naturally raises the question: `Can LFMs be utilized to develop powerful AI Agents that automatically handle web tasks, providing significant convenience to users?' To fully explore the potential of LFMs, extensive research has emerged on WebAgents designed to complete daily web tasks according to user instructions, significantly enhancing the convenience of daily human life. In this survey, we comprehensively review existing research studies on WebAgents across three key aspects: architectures, training, and trustworthiness. Additionally, several promising directions for future research are explored to provide deeper insights.","authors":["Liangbo Ning","Ziran Liang","Zhuohang Jiang","Haohao Qu","Yujuan Ding","Wenqi Fan","Xiao-yong Wei","Shanru Lin","Hui Liu","Philip S. Yu","Qing Li"],"url":"https://arxiv.org/abs/2503.23350"}
{"created":"2025-05-13","title":"Enhancing stroke disease classification through machine learning models by feature selection techniques","abstract":"Heart disease remains a leading cause of mortality and morbidity worldwide, necessitating the development of accurate and reliable predictive models to facilitate early detection and intervention. While state of the art work has focused on various machine learning approaches for predicting heart disease, but they could not able to achieve remarkable accuracy. In response to this need, we applied nine machine learning algorithms XGBoost, logistic regression, decision tree, random forest, k-nearest neighbors (KNN), support vector machine (SVM), gaussian na\\\"ive bayes (NB gaussian), adaptive boosting, and linear regression to predict heart disease based on a range of physiological indicators. Our approach involved feature selection techniques to identify the most relevant predictors, aimed at refining the models to enhance both performance and interpretability. The models were trained, incorporating processes such as grid search hyperparameter tuning, and cross-validation to minimize overfitting. Additionally, we have developed a novel voting system with feature selection techniques to advance heart disease classification. Furthermore, we have evaluated the models using key performance metrics including accuracy, precision, recall, F1-score, and the area under the receiver operating characteristic curve (ROC AUC). Among the models, XGBoost demonstrated exceptional performance, achieving 99% accuracy, precision, F1-Score, 98% recall, and 100% ROC AUC. This study offers a promising approach to early heart disease diagnosis and preventive healthcare.","authors":["Mahade Hasan","Farhana Yasmin","Xue Yu"],"url":"https://arxiv.org/abs/2504.00485"}
{"created":"2025-05-13","title":"An Illusion of Progress? Assessing the Current State of Web Agents","abstract":"As digitalization and cloud technologies evolve, the web is becoming increasingly important in the modern society. Autonomous web agents based on large language models (LLMs) hold a great potential in work automation. It is therefore important to accurately measure and monitor the progression of their capabilities. In this work, we conduct a comprehensive and rigorous assessment of the current state of web agents. Our results depict a very different picture of the competency of current agents, suggesting over-optimism in previously reported results. This gap can be attributed to shortcomings in existing benchmarks. We introduce Online-Mind2Web, an online evaluation benchmark consisting of 300 diverse and realistic tasks spanning 136 websites. It enables us to evaluate web agents under a setting that approximates how real users use these agents. To facilitate more scalable evaluation and development, we also develop a novel LLM-as-a-Judge automatic evaluation method and show that it can achieve around 85% agreement with human judgment, substantially higher than existing methods. Finally, we present the first comprehensive comparative analysis of current web agents, highlighting both their strengths and limitations to inspire future research.","authors":["Tianci Xue","Weijian Qi","Tianneng Shi","Chan Hee Song","Boyu Gou","Dawn Song","Huan Sun","Yu Su"],"url":"https://arxiv.org/abs/2504.01382"}
{"created":"2025-05-13","title":"Tiny Neural Networks for Session-Level Traffic Classification","abstract":"This paper presents a system for session-level traffic classification on endpoint devices, developed using a Hardware-aware Neural Architecture Search (HW-NAS) framework. HW-NAS optimizes Convolutional Neural Network (CNN) architectures by integrating hardware constraints, ensuring efficient deployment on resource-constrained devices. Tested on the ISCX VPN-nonVPN dataset, the method achieves 97.06% accuracy while reducing parameters by over 200 times and FLOPs by nearly 4 times compared to leading models. The proposed model requires up to 15.5 times less RAM and 26.4 times fewer FLOPs than the most hardware-demanding models. This system enhances compatibility across network architectures and ensures efficient deployment on diverse hardware, making it suitable for applications like firewall policy enforcement and traffic monitoring.","authors":["Adel Chehade","Edoardo Ragusa","Paolo Gastaldo","Rodolfo Zunino"],"url":"https://arxiv.org/abs/2504.04008"}
{"created":"2025-05-13","title":"New Algorithms for Incremental Minimum Spanning Trees and Temporal Graph Applications","abstract":"Processing graphs with temporal information (the temporal graphs) has become increasingly important in the real world. In this paper, we study efficient solutions to temporal graph applications using new algorithms for Incremental Minimum Spanning Trees (MST). The first contribution of this work is to formally discuss how a broad set of setting-problem combinations of temporal graph processing can be solved using incremental MST, along with their theoretical guarantees. Despite the importance of the problem, we observe a gap between theory and practice for efficient incremental MST algorithms. While many classic data structures, such as the link-cut tree, provide strong bounds for incremental MST, their performance is limited in practice. Meanwhile, existing practical solutions used in applications do not have any non-trivial theoretical guarantees. Our second and main contribution includes new algorithms for incremental MST that are efficient both in theory and in practice. Our new data structure, the AM-tree, achieves the same theoretical bound as the link-cut tree for temporal graph processing and shows strong performance in practice. In our experiments, the AM-tree has competitive or better performance than existing practical solutions due to theoretical guarantees, and can be significantly faster than the link-cut tree (7.8-11x in updates and 7.7-13.7x in queries).","authors":["Xiangyun Ding","Yan Gu","Yihan Sun"],"url":"https://arxiv.org/abs/2504.04619"}
{"created":"2025-05-13","title":"Topological Schr\\\"odinger Bridge Matching","abstract":"Given two boundary distributions, the Schr\\\"odinger Bridge (SB) problem seeks the ``most likely`` random evolution between them with respect to a reference process. It has revealed rich connections to recent machine learning methods for generative modeling and distribution matching. While these methods perform well in Euclidean domains, they are not directly applicable to topological domains such as graphs and simplicial complexes, which are crucial for data defined over network entities, such as node signals and edge flows. In this work, we propose the Topological Schr\\\"odinger Bridge problem (TSBP) for matching signal distributions on a topological domain. We set the reference process to follow some linear tractable topology-aware stochastic dynamics such as topological heat diffusion. For the case of Gaussian boundary distributions, we derive a closed-form topological SB (TSB) in terms of its time-marginal and stochastic differential. In the general case, leveraging the well-known result, we show that the optimal process follows the forward-backward topological dynamics governed by some unknowns. Building on these results, we develop TSB-based models for matching topological signals by parameterizing the unknowns in the optimal process as (topological) neural networks and learning them through likelihood training. We validate the theoretical results and demonstrate the practical applications of TSB-based models on both synthetic and real-world networks, emphasizing the role of topology. Additionally, we discuss the connections of TSB-based models to other emerging models, and outline future directions for topological signal matching.","authors":["Maosheng Yang"],"url":"https://arxiv.org/abs/2504.04799"}
{"created":"2025-05-13","title":"SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models","abstract":"Typographic attacks exploit the interplay between text and visual content in multimodal foundation models, causing misclassifications when misleading text is embedded within images. However, existing datasets are limited in size and diversity, making it difficult to study such vulnerabilities. In this paper, we introduce SCAM, the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words. Through extensive benchmarking of Vision-Language Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly degrade performance, and identify that training data and model architecture influence the susceptibility to these attacks. Our findings reveal that typographic attacks persist in state-of-the-art Large Vision-Language Models (LVLMs) due to the choice of their vision encoder, though larger Large Language Models (LLMs) backbones help mitigate their vulnerability. Additionally, we demonstrate that synthetic attacks closely resemble real-world (handwritten) attacks, validating their use in research. Our work provides a comprehensive resource and empirical insights to facilitate future research toward robust and trustworthy multimodal AI systems. We publicly release the datasets introduced in this paper along with the code for evaluations at www.bliss.berlin/research/scam.","authors":["Justus Westerhoff","Erblina Purelku","Jakob Hackstein","Jonas Loos","Leo Pinetzki","Lorenz Hufe"],"url":"https://arxiv.org/abs/2504.04893"}
{"created":"2025-05-13","title":"Wavelet Policy: Imitation Policy Learning in Frequency Domain with Wavelet Transforms","abstract":"Recent imitation learning policies, often framed as time series prediction tasks, directly map robotic observations-such as high-dimensional visual data and proprioception-into the action space. While time series prediction primarily relies on spatial domain modeling, the underutilization of frequency domain analysis in robotic manipulation trajectory prediction may lead to neglecting the inherent temporal information embedded within action sequences. To address this, we reframe imitation learning policies through the lens of the frequency domain and introduce the Wavelet Policy. This novel approach employs wavelet transforms (WT) for feature preprocessing and extracts multi-scale features from the frequency domain using the SE2MD (Single Encoder to Multiple Decoder) architecture. Furthermore, to enhance feature mapping in the frequency domain and increase model capacity, we introduce a Learnable Frequency-Domain Filter (LFDF) after each frequency decoder, improving adaptability under different visual conditions. Our results show that the Wavelet Policy outperforms state-of-the-art (SOTA) end-to-end methods by over 10% on four challenging robotic arm tasks, while maintaining a comparable parameter count. In long-range settings, its performance declines more slowly as task volume increases. The source code is available at https://github.com/lurenjia384/Wavelet_Policy.","authors":["Changchuan Yang","Yuhang Dong","Guanzhong Tian","Haizhou Ge","Hongrui Zhu"],"url":"https://arxiv.org/abs/2504.04991"}
{"created":"2025-05-13","title":"LLM-assisted Mutation for Whitebox API Testing","abstract":"Cloud applications heavily rely on APIs to communicate with each other and exchange data. To ensure the reliability of cloud applications, cloud providers widely adopt API testing techniques. Unfortunately, existing API testing approaches are insufficient to reach strict conditions, a problem known as fitness plateaus, due to the lack of gradient provided by coverage metrics. To address this issue, we propose MioHint, a novel white-box API testing approach that leverages the code comprehension capabilities of Large Language Model (LLM) to boost API testing. The key challenge of LLM-based API testing lies in system-level testing, which emphasizes the dependencies between requests and targets across functions and files, thereby making the entire codebase the object of analysis. However, feeding the entire codebase to an LLM is impractical due to its limited context length and short memory. MioHint addresses this challenge by synergizing static analysis with LLMs. We retrieve relevant code with data-dependency analysis at the statement level, including def-use analysis for variables used in the target and function expansion for subfunctions called by the target.","authors":["Jia Li","Jiacheng Shen","Yuxin Su","Michael R. Lyu"],"url":"https://arxiv.org/abs/2504.05738"}
{"created":"2025-05-13","title":"FANeRV: Frequency Separation and Augmentation based Neural Representation for Video","abstract":"Neural representations for video (NeRV) have gained considerable attention for their strong performance across various video tasks. However, existing NeRV methods often struggle to capture fine spatial details, resulting in vague reconstructions. In this paper, we present a Frequency Separation and Augmentation based Neural Representation for video (FANeRV), which addresses these limitations with its core Wavelet Frequency Upgrade Block. This block explicitly separates input frames into high and low-frequency components using discrete wavelet transform, followed by targeted enhancement using specialized modules. Finally, a specially designed gated network effectively fuses these frequency components for optimal reconstruction. Additionally, convolutional residual enhancement blocks are integrated into the later stages of the network to balance parameter distribution and improve the restoration of high-frequency details. Experimental results demonstrate that FANeRV significantly improves reconstruction performance and excels in multiple tasks, including video compression, inpainting, and interpolation, outperforming existing NeRV methods.","authors":["Li Yu","Zhihui Li","Yao Zhao","Jimin Xiao","Moncef Gabbouj"],"url":"https://arxiv.org/abs/2504.06755"}
{"created":"2025-05-13","title":"Integrated Sensing and Communications for Pinching-Antenna Systems (PASS)","abstract":"An integrated sensing and communication (ISAC) design for pinching antenna systems (PASS) is proposed, where the pinching antennas are deployed to establish reliable line-of-sight communication and sensing links. More particularly, a separated ISAC design is proposed for the two-waveguide PASS, where one waveguide is used to emit the information-bearing signals for ISAC transmission while the other waveguide is used to receive the reflected echo signals. Based on this framework, a penalty-based alternating optimization algorithm is proposed to maximize the illumination power as well as ensure the communication quality-of-service requirement. Numerical results demonstrate that the proposed PASS-ISAC scheme outperforms the conventional antenna scheme.","authors":["Zheng Zhang","Zhaolin Wang","Xidong Mu","Bingtao He","Jian Chen","Yuanwei Liu"],"url":"https://arxiv.org/abs/2504.07709"}
{"created":"2025-05-13","title":"Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines","abstract":"Large Language Models (LLMs) have transformed human-computer interaction by enabling natural language-based communication with AI-powered chatbots. These models are designed to be intuitive and user-friendly, allowing users to articulate requests with minimal effort. However, despite their accessibility, studies reveal that users often struggle with effective prompting, resulting in inefficient responses. Existing research has highlighted both the limitations of LLMs in interpreting vague or poorly structured prompts and the difficulties users face in crafting precise queries. This study investigates learner-AI interactions through an educational experiment in which participants receive structured guidance on effective prompting. We introduce and compare three types of prompting guidelines: a task-specific framework developed through a structured methodology and two baseline approaches. To assess user behavior and prompting efficacy, we analyze a dataset of 642 interactions from 107 users. Using Von NeuMidas, an extended pragmatic annotation schema for LLM interaction analysis, we categorize common prompting errors and identify recurring behavioral patterns. We then evaluate the impact of different guidelines by examining changes in user behavior, adherence to prompting strategies, and the overall quality of AI-generated responses. Our findings provide a deeper understanding of how users engage with LLMs and the role of structured prompting guidance in enhancing AI-assisted communication. By comparing different instructional frameworks, we offer insights into more effective approaches for improving user competency in AI interactions, with implications for AI literacy, chatbot usability, and the design of more responsive AI systems.","authors":["Cansu Koyuturk","Emily Theophilou","Sabrina Patania","Gregor Donabauer","Andrea Martinenghi","Chiara Antico","Alessia Telari","Alessia Testa","Sathya Bursic","Franca Garzotto","Davinia Hernandez-Leo","Udo Kruschwitz","Davide Taibi","Simona Amenta","Martin Ruskov","Dimitri Ognibene"],"url":"https://arxiv.org/abs/2504.07840"}
{"created":"2025-05-13","title":"Proofs as Explanations: Short Certificates for Reliable Predictions","abstract":"We consider a model for explainable AI in which an explanation for a prediction $h(x)=y$ consists of a subset $S'$ of the training data (if it exists) such that all classifiers $h' \\in H$ that make at most $b$ mistakes on $S'$ predict $h'(x)=y$. Such a set $S'$ serves as a proof that $x$ indeed has label $y$ under the assumption that (1) the target function $h^\\star$ belongs to $H$, and (2) the set $S$ contains at most $b$ corrupted points. For example, if $b=0$ and $H$ is the family of linear classifiers in $\\mathbb{R}^d$, and if $x$ lies inside the convex hull of the positive data points in $S$ (and hence every consistent linear classifier labels $x$ as positive), then Carath\\'eodory's theorem states that $x$ lies inside the convex hull of $d+1$ of those points. So, a set $S'$ of size $d+1$ could be released as an explanation for a positive prediction, and would serve as a short proof of correctness of the prediction under the assumption of realizability.","authors":["Avrim Blum","Steve Hanneke","Chirag Pabbaraju","Donya Saless"],"url":"https://arxiv.org/abs/2504.08377"}
{"created":"2025-05-13","title":"SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning","abstract":"Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model.","authors":["Peixian Ma","Xialie Zhuang","Chengjin Xu","Xuhui Jiang","Ran Chen","Jian Guo"],"url":"https://arxiv.org/abs/2504.08600"}
{"created":"2025-05-13","title":"Ensemble Score Filter for Data Assimilation of Two-Phase Flow Models in Porous Media","abstract":"Numerical modeling and simulation of two-phase flow in porous media is challenging due to the uncertainties in key parameters, such as permeability. To address these challenges, we propose a computational framework by utilizing the novel Ensemble Score Filter (EnSF) to enhance the accuracy of state estimation for two-phase flow systems in porous media. The forward simulation of the two-phase flow model is implemented using a mixed finite element method, which ensures accurate approximation of the pressure, the velocity, and the saturation. The EnSF leverages score-based diffusion models to approximate filtering distributions efficiently, avoiding the computational expense of neural network-based methods. By incorporating a closed-form score approximation and an analytical update mechanism, the EnSF overcomes degeneracy issues and handles high-dimensional nonlinear filtering with minimal computational overhead. Numerical experiments demonstrate the capabilities of EnSF in scenarios with uncertain permeability and incomplete observational data.","authors":["Ruoyu Hu","Sanjeeb Poudel","Feng Bao","Sanghyun Lee"],"url":"https://arxiv.org/abs/2504.09245"}
{"created":"2025-05-13","title":"Enhancing Wide-Angle Image Using Narrow-Angle View of the Same Scene","abstract":"A common dilemma while photographing a scene is whether to capture it at a wider angle, allowing more of the scene to be covered but in less detail or to click in a narrow angle that captures better details but leaves out portions of the scene. We propose a novel method in this paper that infuses wider shots with finer quality details that is usually associated with an image captured by the primary lens by capturing the same scene using both narrow and wide field of view (FoV) lenses. We do so by training a Generative Adversarial Network (GAN)-based model to learn to extract the visual quality parameters from a narrow-angle shot and to transfer these to the corresponding wide-angle image of the scene using residual connections and an attention-based fusion module. We have mentioned in details the proposed technique to isolate the visual essence of an image and to transfer it into another image. We have also elaborately discussed our implementation details and have presented the results of evaluation over several benchmark datasets and comparisons with contemporary advancements in the field.","authors":["Hussain Md. Safwan","Mahbub Islam Mahim"],"url":"https://arxiv.org/abs/2504.09455"}
{"created":"2025-05-13","title":"GeoNav: Empowering MLLMs with Explicit Geospatial Reasoning Abilities for Language-Goal Aerial Navigation","abstract":"Language-goal aerial navigation is a critical challenge in embodied AI, requiring UAVs to localize targets in complex environments such as urban blocks based on textual specification. Existing methods, often adapted from indoor navigation, struggle to scale due to limited field of view, semantic ambiguity among objects, and lack of structured spatial reasoning. In this work, we propose GeoNav, a geospatially aware multimodal agent to enable long-range navigation. GeoNav operates in three phases-landmark navigation, target search, and precise localization-mimicking human coarse-to-fine spatial strategies. To support such reasoning, it dynamically builds two different types of spatial memory. The first is a global but schematic cognitive map, which fuses prior textual geographic knowledge and embodied visual cues into a top-down, annotated form for fast navigation to the landmark region. The second is a local but delicate scene graph representing hierarchical spatial relationships between blocks, landmarks, and objects, which is used for definite target localization. On top of this structured representation, GeoNav employs a spatially aware, multimodal chain-of-thought prompting mechanism to enable multimodal large language models with efficient and interpretable decision-making across stages. On the CityNav urban navigation benchmark, GeoNav surpasses the current state-of-the-art by up to 12.53% in success rate and significantly improves navigation efficiency, even in hard-level tasks. Ablation studies highlight the importance of each module, showcasing how geospatial representations and coarse-to-fine reasoning enhance UAV navigation.","authors":["Haotian Xu","Yue Hu","Chen Gao","Zhengqiu Zhu","Yong Zhao","Yong Li","Quanjun Yin"],"url":"https://arxiv.org/abs/2504.09587"}
{"created":"2025-05-13","title":"Constrained Error-Correcting Codes for Efficient DNA Synthesis","abstract":"DNA synthesis is considered as one of the most expensive components in current DNA storage systems. In this paper, focusing on a common synthesis machine, which generates multiple DNA strands in parallel following a fixed supersequence,we propose constrained codes with polynomial-time encoding and decoding algorithms. Compared to the existing works, our codes simultaneously satisfy both l-runlength limited and {\\epsilon}-balanced constraints. By enumerating all valid sequences, our codes achieve the maximum rate, matching the capacity. Additionally, we design constrained error-correcting codes capable of correcting one insertion or deletion in the obtained DNA sequence while still adhering to the constraints.","authors":["Yajuan Liu","Tolga M. Duman"],"url":"https://arxiv.org/abs/2504.09950"}
{"created":"2025-05-13","title":"From Prompting to Alignment: A Generative Framework for Query Recommendation","abstract":"In modern search systems, search engines often suggest relevant queries to users through various panels or components, helping refine their information needs. Traditionally, these recommendations heavily rely on historical search logs to build models, which suffer from cold-start or long-tail issues. Furthermore, tasks such as query suggestion, completion or clarification are studied separately by specific design, which lacks generalizability and hinders adaptation to novel applications. Despite recent attempts to explore the use of LLMs for query recommendation, these methods mainly rely on the inherent knowledge of LLMs or external sources like few-shot examples, retrieved documents, or knowledge bases, neglecting the importance of the calibration and alignment with user feedback, thus limiting their practical utility. To address these challenges, we first propose a general Generative Query Recommendation (GQR) framework that aligns LLM-based query generation with user preference. Specifically, we unify diverse query recommendation tasks by a universal prompt framework, leveraging the instruct-following capability of LLMs for effective generation. Secondly, we align LLMs with user feedback via presenting a CTR-alignment framework, which involves training a query-wise CTR predictor as a process reward model and employing list-wise preference alignment to maximize the click probability of the generated query list. Furthermore, recognizing the inconsistency between LLM knowledge and proactive search intents arising from the separation of user-initiated queries from models, we align LLMs with user initiative via retrieving co-occurrence queries as side information when historical logs are available.","authors":["Erxue Min","Hsiu-Yuan Huang","Min Yang","Xihong Yang","Xin Jia","Yunfang Wu","Hengyi Cai","Junfeng Wang","Shuaiqiang Wang","Dawei Yin"],"url":"https://arxiv.org/abs/2504.10208"}
{"created":"2025-05-13","title":"Causality-enhanced Decision-Making for Autonomous Mobile Robots in Dynamic Environments","abstract":"The growing integration of robots in shared environments -- such as warehouses, shopping centres, and hospitals -- demands a deep understanding of the underlying dynamics and human behaviours, including how, when, and where individuals engage in various activities and interactions. This knowledge goes beyond simple correlation studies and requires a more comprehensive causal analysis. By leveraging causal inference to model cause-and-effect relationships, we can better anticipate critical environmental factors and enable autonomous robots to plan and execute tasks more effectively. To this end, we propose a novel causality-based decision-making framework that reasons over a learned causal model to predict battery usage and human obstructions, understanding how these factors could influence robot task execution. Such reasoning framework assists the robot in deciding when and how to complete a given task. To achieve this, we developed also PeopleFlow, a new Gazebo-based simulator designed to model context-sensitive human-robot spatial interactions in shared workspaces. PeopleFlow features realistic human and robot trajectories influenced by contextual factors such as time, environment layout, and robot state, and can simulate a large number of agents. While the simulator is general-purpose, in this paper we focus on a warehouse-like environment as a case study, where we conduct an extensive evaluation benchmarking our causal approach against a non-causal baseline. Our findings demonstrate the efficacy of the proposed solutions, highlighting how causal reasoning enables autonomous robots to operate more efficiently and safely in dynamic environments shared with humans.","authors":["Luca Castri","Gloria Beraldo","Nicola Bellotto"],"url":"https://arxiv.org/abs/2504.11901"}
{"created":"2025-05-13","title":"Deterministic Parallel High-Quality Hypergraph Partitioning","abstract":"We present a deterministic parallel multilevel algorithm for balanced hypergraph partitioning that matches the state of the art for non-deterministic algorithms. Deterministic parallel algorithms produce the same result in each invocation, which is crucial for reproducibility. Moreover, determinism is highly desirable in application areas such as VLSI design. While there has been tremendous progress in parallel hypergraph partitioning algorithms recently, deterministic counterparts for high-quality local search techniques are missing. Consequently, solution quality is severely lacking in comparison to the non-deterministic algorithms.","authors":["Robert Krause","Lars Gottesb\\\"uren","Nikolai Maas"],"url":"https://arxiv.org/abs/2504.12013"}
{"created":"2025-05-13","title":"FocusedAD: Character-centric Movie Audio Description","abstract":"Movie Audio Description (AD) aims to narrate visual content during dialogue-free segments, particularly benefiting blind and visually impaired (BVI) audiences. Compared with general video captioning, AD demands plot-relevant narration with explicit character name references, posing unique challenges in movie understanding.To identify active main characters and focus on storyline-relevant regions, we propose FocusedAD, a novel framework that delivers character-centric movie audio descriptions. It includes: (i) a Character Perception Module(CPM) for tracking character regions and linking them to names; (ii) a Dynamic Prior Module(DPM) that injects contextual cues from prior ADs and subtitles via learnable soft prompts; and (iii) a Focused Caption Module(FCM) that generates narrations enriched with plot-relevant details and named characters. To overcome limitations in character identification, we also introduce an automated pipeline for building character query banks. FocusedAD achieves state-of-the-art performance on multiple benchmarks, including strong zero-shot results on MAD-eval-Named and our newly proposed Cinepile-AD dataset. Code and data will be released at https://github.com/Thorin215/FocusedAD .","authors":["Xiaojun Ye","Chun Wang","Yiren Song","Sheng Zhou","Liangcheng Li","Jiajun Bu"],"url":"https://arxiv.org/abs/2504.12157"}
{"created":"2025-05-13","title":"ForgetMe: Evaluating Selective Forgetting in Generative Models","abstract":"The widespread adoption of diffusion models in image generation has increased the demand for privacy-compliant unlearning. However, due to the high-dimensional nature and complex feature representations of diffusion models, achieving selective unlearning remains challenging, as existing methods struggle to remove sensitive information while preserving the consistency of non-sensitive regions. To address this, we propose an Automatic Dataset Creation Framework based on prompt-based layered editing and training-free local feature removal, constructing the ForgetMe dataset and introducing the Entangled evaluation metric. The Entangled metric quantifies unlearning effectiveness by assessing the similarity and consistency between the target and background regions and supports both paired (Entangled-D) and unpaired (Entangled-S) image data, enabling unsupervised evaluation. The ForgetMe dataset encompasses a diverse set of real and synthetic scenarios, including CUB-200-2011 (Birds), Stanford-Dogs, ImageNet, and a synthetic cat dataset. We apply LoRA fine-tuning on Stable Diffusion to achieve selective unlearning on this dataset and validate the effectiveness of both the ForgetMe dataset and the Entangled metric, establishing them as benchmarks for selective unlearning. Our work provides a scalable and adaptable solution for advancing privacy-preserving generative AI.","authors":["Zhenyu Yu","Mohd Yamani Inda Idris","Pei Wang"],"url":"https://arxiv.org/abs/2504.12574"}
{"created":"2025-05-13","title":"Inference-friendly Graph Compression for Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have demonstrated promising performance in graph analysis. Nevertheless, the inference process of GNNs remains costly, hindering their applications for large graphs. This paper proposes inference-friendly graph compression (IFGC), a graph compression scheme to accelerate GNNs inference. Given a graph $G$ and a GNN $M$, an IFGC computes a small compressed graph $G_c$, to best preserve the inference results of $M$ over $G$, such that the result can be directly inferred by accessing $G_c$ with no or little decompression cost. (1) We characterize IFGC with a class of inference equivalence relation. The relation captures the node pairs in $G$ that are not distinguishable for GNN inference. (2) We introduce three practical specifications of IFGC for representative GNNs: structural preserving compression (SPGC), which computes $G_c$ that can be directly processed by GNN inference without decompression; ($\\alpha$, $r$)-compression, that allows for a configurable trade-off between compression ratio and inference quality, and anchored compression that preserves inference results for specific nodes of interest. For each scheme, we introduce compression and inference algorithms with guarantees of efficiency and quality of the inferred results. We conduct extensive experiments on diverse sets of large-scale graphs, which verifies the effectiveness and efficiency of our graph compression approaches.","authors":["Yangxin Fan","Haolai Che","Yinghui Wu"],"url":"https://arxiv.org/abs/2504.13034"}
{"created":"2025-05-13","title":"From Large to Super-Tiny: End-to-End Optimization for Cost-Efficient LLMs","abstract":"Large Language Models (LLMs) have significantly advanced artificial intelligence by optimizing traditional Natural Language Processing (NLP) workflows, facilitating their integration into various systems. Many such NLP systems, including ours, directly incorporate LLMs. However, this approach either results in expensive costs or yields suboptimal performance after fine-tuning. In this paper, we introduce a three-stage cost-efficient end-to-end LLM deployment pipeline, comprising prototyping, knowledge transfer, and model compression, to effectively tackle the cost-performance dilemma in LLM-based frameworks. Its high cost-efficiency is manifested not only in simplifying system complexity and producing super-tiny online models with enhanced performance and reduced costs in the results, but also in addressing development cycle constraints, the lack of extensive high-quality data, and limited computational resources during the project development process. In the first stage, we construct an optimal performance prototype system by transforming complex tasks into a function call-based LLM-driven pipeline, which serves as a teacher model to generate high-quality data. In the second stage, we combine techniques like rejection sampling fine-tuning, reinforcement learning, and knowledge distillation to transfer knowledge to 0.5B student models, delivering effective performance at minimal cost. In the final stage, we further compress models to 0.4B via quantization and pruning, achieving ultra-low latency and cost. Extensive experimental results and the framework's modular design suggest cross-domain capabilities and potential applicability in other NLP areas.","authors":["Jiliang Ni","Jiachen Pu","Zhongyi Yang","Kun Zhou","Hui Wang","Xiaoliang Xiao","Dakui Wang","Xin Li","Jingfeng Luo","Conggang Hu"],"url":"https://arxiv.org/abs/2504.13471"}
{"created":"2025-05-13","title":"Compile Scene Graphs with Reinforcement Learning","abstract":"Next-token prediction is the fundamental principle for training large language models (LLMs), and reinforcement learning (RL) further enhances their reasoning performance. As an effective way to model language, image, video, and other modalities, the use of LLMs for end-to-end extraction of structured visual representations, such as scene graphs, remains underexplored. It requires the model to accurately produce a set of objects and relationship triplets, rather than generating text token by token. To achieve this, we introduce R1-SGG, a multimodal LLM (M-LLM) initially trained via supervised fine-tuning (SFT) on the scene graph dataset and subsequently refined using reinforcement learning to enhance its ability to generate scene graphs in an end-to-end manner. The SFT follows a conventional prompt-response paradigm, while RL requires the design of effective reward signals. We design a set of graph-centric rewards, including three recall-based variants -- Hard Recall, Hard Recall+Relax, and Soft Recall -- which evaluate semantic and spatial alignment between predictions and ground truth at the object and relation levels. A format consistency reward further ensures that outputs follow the expected structural schema. Extensive experiments on the VG150 and PSG benchmarks show that R1-SGG substantially reduces failure rates and achieves strong performance in Recall and mean Recall, surpassing traditional SGG models and existing multimodal language models. Our code is available at https://github.com/gpt4vision/R1-SGG","authors":["Zuyao Chen","Jinlin Wu","Zhen Lei","Marc Pollefeys","Chang Wen Chen"],"url":"https://arxiv.org/abs/2504.13617"}
{"created":"2025-05-13","title":"On the Capacity of Insertion Channels for Small Insertion Probabilities","abstract":"Channels with synchronization errors, such as deletion and insertion errors, are crucial in DNA storage, data reconstruction, and other applications. These errors introduce memory to the channel, complicating its capacity analysis. This paper analyzes binary insertion channels for small insertion probabilities, identifying dominant terms in the capacity expansion and establishing capacity in this regime. Using Bernoulli(1/2) inputs for achievability and a converse based on the use of stationary and ergodic processes, we demonstrate that capacity closely aligns with achievable rates using independent and identically distributed (i.i.d.) inputs, differing only in higher-order terms.","authors":["Busra Tegin","Tolga M Duman"],"url":"https://arxiv.org/abs/2504.14035"}
{"created":"2025-05-13","title":"HoLa: B-Rep Generation using a Holistic Latent Representation","abstract":"We introduce a novel representation for learning and generating Computer-Aided Design (CAD) models in the form of $\\textit{boundary representations}$ (B-Reps). Our representation unifies the continuous geometric properties of B-Rep primitives in different orders (e.g., surfaces and curves) and their discrete topological relations in a $\\textit{holistic latent}$ (HoLa) space. This is based on the simple observation that the topological connection between two surfaces is intrinsically tied to the geometry of their intersecting curve. Such a prior allows us to reformulate topology learning in B-Reps as a geometric reconstruction problem in Euclidean space. Specifically, we eliminate the presence of curves, vertices, and all the topological connections in the latent space by learning to distinguish and derive curve geometries from a pair of surface primitives via a neural intersection network. To this end, our holistic latent space is only defined on surfaces but encodes a full B-Rep model, including the geometry of surfaces, curves, vertices, and their topological relations. Our compact and holistic latent space facilitates the design of a first diffusion-based generator to take on a large variety of inputs including point clouds, single/multi-view images, 2D sketches, and text prompts. Our method significantly reduces ambiguities, redundancies, and incoherences among the generated B-Rep primitives, as well as training complexities inherent in prior multi-step B-Rep learning pipelines, while achieving greatly improved validity rate over current state of the art: 82% vs. $\\approx$50%.","authors":["Yilin Liu","Duoteng Xu","Xingyao Yu","Xiang Xu","Daniel Cohen-Or","Hao Zhang","Hui Huang"],"url":"https://arxiv.org/abs/2504.14257"}
{"created":"2025-05-13","title":"The Geometry of Self-Verification in a Task-Specific Reasoning Model","abstract":"How do reasoning models verify their own answers? We study this question by training a model using DeepSeek R1's recipe on the CountDown task. We leverage the fact that preference tuning leads to mode collapse, yielding a model that always produces highly structured chain-of-thought sequences. With this setup, we do top-down and bottom-up analyses to reverse-engineer how the model verifies its outputs. Top-down, we find Gated Linear Unit (GLU) weights encoding verification-related tokens, such as ``success'' or ``incorrect''. Bottom-up, we find that ``previous-token heads'' are mainly responsible for self-verification in our setup. Our analyses meet in the middle: drawing inspiration from inter-layer communication channels, we use the identified GLU weights to localize as few as three attention heads that can disable self-verification, pointing to a necessary component of a potentially larger verification circuit. Finally, we verify that similar verification components exist in our base model and a general reasoning DeepSeek-R1 model.","authors":["Andrew Lee","Lihao Sun","Chris Wendler","Fernanda Vi\\'egas","Martin Wattenberg"],"url":"https://arxiv.org/abs/2504.14379"}
{"created":"2025-05-13","title":"Improving Sound Source Localization with Joint Slot Attention on Image and Audio","abstract":"Sound source localization (SSL) is the task of locating the source of sound within an image. Due to the lack of localization labels, the de facto standard in SSL has been to represent an image and audio as a single embedding vector each, and use them to learn SSL via contrastive learning. To this end, previous work samples one of local image features as the image embedding and aggregates all local audio features to obtain the audio embedding, which is far from optimal due to the presence of noise and background irrelevant to the actual target in the input. We present a novel SSL method that addresses this chronic issue by joint slot attention on image and audio. To be specific, two slots competitively attend image and audio features to decompose them into target and off-target representations, and only target representations of image and audio are used for contrastive learning. Also, we introduce cross-modal attention matching to further align local features of image and audio. Our method achieved the best in almost all settings on three public benchmarks for SSL, and substantially outperformed all the prior work in cross-modal retrieval.","authors":["Inho Kim","Youngkil Song","Jicheol Park","Won Hwa Kim","Suha Kwak"],"url":"https://arxiv.org/abs/2504.15118"}
{"created":"2025-05-13","title":"Under Pressure: Contextualizing Workplace Stress Towards User-Centered Interventions","abstract":"Stress is a pervasive challenge that significantly impacts worker health and well-being. Workplace stress is driven by various factors, ranging from organizational changes to poor workplace design. Although individual stress management strategies have been shown to be effective, current interventions often overlook personal and contextual factors shaping stress experiences. In this study, we conducted semi-structured interviews with eight office workers to gain a deeper understanding of their personal experiences with workplace stress. Our analysis reveals key stress triggers, coping mechanisms, and reflections on past stressful events. We highlight the multifaceted and individualized nature of workplace stress, emphasizing the importance of intervention timing, modality, and recognizing that stress is not solely a negative experience but can also have positive effects. Our findings provide actionable insights for the design of user-centered stress management solutions more attuned to the needs of office workers.","authors":["Antonin Brun","Gale Lucas","Bur\\c{c}in Becerik-Gerber"],"url":"https://arxiv.org/abs/2504.15480"}
{"created":"2025-05-13","title":"MetaMolGen: A Neural Graph Motif Generation Model for De Novo Molecular Design","abstract":"Molecular generation plays an important role in drug discovery and materials science, especially in data-scarce scenarios where traditional generative models often struggle to achieve satisfactory conditional generalization. To address this challenge, we propose MetaMolGen, a first-order meta-learning-based molecular generator designed for few-shot and property-conditioned molecular generation. MetaMolGen standardizes the distribution of graph motifs by mapping them to a normalized latent space, and employs a lightweight autoregressive sequence model to generate SMILES sequences that faithfully reflect the underlying molecular structure. In addition, it supports conditional generation of molecules with target properties through a learnable property projector integrated into the generative process.Experimental results demonstrate that MetaMolGen consistently generates valid and diverse SMILES sequences under low-data regimes, outperforming conventional baselines. This highlights its advantage in fast adaptation and efficient conditional generation for practical molecular design.","authors":["Zimo Yan","Jie Zhang","Zheng Xie","Chang Liu","Yizhen Liu","Yiping Song"],"url":"https://arxiv.org/abs/2504.15587"}
{"created":"2025-05-13","title":"Methods for Recognizing Nested Terms","abstract":"In this paper, we describe our participation in the RuTermEval competition devoted to extracting nested terms. We apply the Binder model, which was previously successfully applied to the recognition of nested named entities, to extract nested terms. We obtained the best results of term recognition in all three tracks of the RuTermEval competition. In addition, we study the new task of recognition of nested terms from flat training data annotated with terms without nestedness. We can conclude that several approaches we proposed in this work are viable enough to retrieve nested terms effectively without nested labeling of them.","authors":["Igor Rozhkov","Natalia Loukachevitch"],"url":"https://arxiv.org/abs/2504.16007"}
{"created":"2025-05-13","title":"ConTextual: Improving Clinical Text Summarization in LLMs with Context-preserving Token Filtering and Knowledge Graphs","abstract":"Unstructured clinical data can serve as a unique and rich source of information that can meaningfully inform clinical practice. Extracting the most pertinent context from such data is critical for exploiting its true potential toward optimal and timely decision-making in patient care. While prior research has explored various methods for clinical text summarization, most prior studies either process all input tokens uniformly or rely on heuristic-based filters, which can overlook nuanced clinical cues and fail to prioritize information critical for decision-making. In this study, we propose Contextual, a novel framework that integrates a Context-Preserving Token Filtering method with a Domain-Specific Knowledge Graph (KG) for contextual augmentation. By preserving context-specific important tokens and enriching them with structured knowledge, ConTextual improves both linguistic coherence and clinical fidelity. Our extensive empirical evaluations on two public benchmark datasets demonstrate that ConTextual consistently outperforms other baselines. Our proposed approach highlights the complementary role of token-level filtering and structured retrieval in enhancing both linguistic and clinical integrity, as well as offering a scalable solution for improving precision in clinical text generation.","authors":["Fahmida Liza Piya","Rahmatollah Beheshti"],"url":"https://arxiv.org/abs/2504.16394"}
{"created":"2025-05-13","title":"A Comprehensive Survey of Synthetic Tabular Data Generation","abstract":"Tabular data remains one of the most prevalent and critical data formats across diverse real-world applications. However, its effective use in machine learning (ML) is often constrained by challenges such as data scarcity, privacy concerns, and class imbalance. Synthetic data generation has emerged as a promising solution, leveraging generative models to learn the distribution of real datasets and produce high-fidelity, privacy-preserving samples. Various generative paradigms have been explored, including energy-based models (EBMs), variational autoencoders (VAEs), generative adversarial networks (GANs), large language models (LLMs), and diffusion models. While several surveys have investigated synthetic tabular data generation, most focus on narrow subdomains or specific generative methods, such as GANs, diffusion models, or privacy-preserving techniques. This limited scope often results in fragmented insights, lacking a comprehensive synthesis that bridges diverse approaches. In particular, recent advances driven by LLMs and diffusion-based models remain underexplored. This gap hinders a holistic understanding of the field`s evolution, methodological interplay, and open challenges. To address this, our survey provides a unified and systematic review of synthetic tabular data generation. Our contributions are threefold: (1) we propose a comprehensive taxonomy that organizes existing methods into traditional approaches, diffusion-based methods, and LLM-based models, and provide an in-depth comparative analysis; (2) we detail the complete pipeline for synthetic tabular data generation, including data synthesis, post-processing, and evaluation; (3) we identify major challenges, explore real-world applications, and outline open research questions and future directions to guide future work in this rapidly evolving area.","authors":["Ruxue Shi","Yili Wang","Mengnan Du","Xu Shen","Xin Wang"],"url":"https://arxiv.org/abs/2504.16506"}
{"created":"2025-05-13","title":"Synthesiz3 This: an SMT-Based Approach for Synthesis with Uncomputable Symbols","abstract":"Program synthesis is the task of automatically constructing a program conforming to a given specification. In this paper we focus on synthesis of single-invocation recursion-free functions conforming to a specification given as a logical formula in the presence of uncomputable symbols (i.e., symbols used in the specification but not allowed in the resulting function). We approach the problem via SMT-solving methods: we present a quantifier elimination algorithm using model-based projections for both total and partial function synthesis, working with theories of uninterpreted functions and linear arithmetic and their combination. For this purpose we also extend model-based projection to produce witnesses for these theories. Further, we present procedures tailored for the case of uniquely determined solutions. We implemented a prototype of the algorithms using the SMT-solver Z3, demonstrating their practical efficiency compared to the state of the art.","authors":["Petra Hozzov\\'a","Nikolaj Bj{\\o}rner"],"url":"https://arxiv.org/abs/2504.16536"}
{"created":"2025-05-13","title":"DyMU: Dynamic Merging and Virtual Unmerging for Efficient VLMs","abstract":"We present DyMU, an efficient, training-free framework that dynamically reduces the computational burden of vision-language models (VLMs) while maintaining high task performance. Our approach comprises two key components. First, Dynamic Token Merging (DToMe) reduces the number of visual token embeddings by merging similar tokens based on image complexity, addressing the inherent inefficiency of fixed-length outputs in vision transformers. Second, Virtual Token Unmerging (VTU) simulates the expected token sequence for large language models (LLMs) by efficiently reconstructing the attention dynamics of a full sequence, thus preserving the downstream performance without additional fine-tuning. Unlike previous approaches, our method dynamically adapts token compression to the content of the image and operates completely training-free, making it readily applicable to most state-of-the-art VLM architectures. Extensive experiments on image and video understanding tasks demonstrate that DyMU can reduce the average visual token count by 32%-85% while achieving comparable performance to full-length models across diverse VLM architectures, including the recently popularized AnyRes-based visual encoders. Furthermore, through qualitative analyses, we demonstrate that DToMe effectively adapts token reduction based on image complexity and, unlike existing systems, provides users more control over computational costs. Project page: https://mikewangwzhl.github.io/dymu/.","authors":["Zhenhailong Wang","Senthil Purushwalkam","Caiming Xiong","Silvio Savarese","Heng Ji","Ran Xu"],"url":"https://arxiv.org/abs/2504.17040"}
{"created":"2025-05-13","title":"Statistical Guarantees in Synthetic Data through Conformal Adversarial Generation","abstract":"The generation of high-quality synthetic data presents significant challenges in machine learning research, particularly regarding statistical fidelity and uncertainty quantification. Existing generative models produce compelling synthetic samples but lack rigorous statistical guarantees about their relation to the underlying data distribution, limiting their applicability in critical domains requiring robust error bounds. We address this fundamental limitation by presenting a novel framework that incorporates conformal prediction methodologies into Generative Adversarial Networks (GANs). By integrating multiple conformal prediction paradigms including Inductive Conformal Prediction (ICP), Mondrian Conformal Prediction, Cross-Conformal Prediction, and Venn-Abers Predictors, we establish distribution-free uncertainty quantification in generated samples. This approach, termed Conformalized GAN (cGAN), demonstrates enhanced calibration properties while maintaining the generative power of traditional GANs, producing synthetic data with provable statistical guarantees. We provide rigorous mathematical proofs establishing finite-sample validity guarantees and asymptotic efficiency properties, enabling the reliable application of synthetic data in high-stakes domains including healthcare, finance, and autonomous systems.","authors":["Rahul Vishwakarma","Shrey Dharmendra Modi","Vishwanath Seshagiri"],"url":"https://arxiv.org/abs/2504.17058"}
{"created":"2025-05-13","title":"Towards Equitable Rail Service Allocation Through Fairness-Oriented Timetabling in Liberalized Markets","abstract":"Over the last few decades, European rail transport has undergone major changes as part of the process of liberalization set out in European regulations. In this context of liberalization, railway undertakings compete with each other for the limited infrastructure capacity available to offer their rail services. The infrastructure manager is responsible for the equitable allocation of infrastructure between all companies in the market, which is essential to ensure the efficiency and sustainability of this competitive ecosystem. In this paper, a methodology based on Jain, Gini and Atkinson equity metrics is used to solve the rail service allocation problem in a liberalized railway market, analyzing the solutions obtained. The results show that the proposed methodology and the equity metrics used allow for equitable planning in different competitiveness scenarios. These results contrast with solutions where the objective of the infrastructure manager is to maximize its own profit, without regard for the equitable allocation of infrastructure. Therefore, the computational tests support the methodology and metrics used as a planning and decision support tool in a liberalized railway market.","authors":["David Mu\\~noz-Valero","Juan Moreno-Garcia","Julio Alberto L\\'opez-G\\'omez","Enrique Adrian Villarrubia-Martin"],"url":"https://arxiv.org/abs/2504.17489"}
{"created":"2025-05-13","title":"TableCenterNet: A one-stage network for table structure recognition","abstract":"Table structure recognition aims to parse tables in unstructured data into machine-understandable formats. Recent methods address this problem through a two-stage process or optimized one-stage approaches. However, these methods either require multiple networks to be serially trained and perform more time-consuming sequential decoding, or rely on complex post-processing algorithms to parse the logical structure of tables. They struggle to balance cross-scenario adaptability, robustness, and computational efficiency. In this paper, we propose a one-stage end-to-end table structure parsing network called TableCenterNet. This network unifies the prediction of table spatial and logical structure into a parallel regression task for the first time, and implicitly learns the spatial-logical location mapping laws of cells through a synergistic architecture of shared feature extraction layers and task-specific decoding. Compared with two-stage methods, our method is easier to train and faster to infer. Experiments on benchmark datasets show that TableCenterNet can effectively parse table structures in diverse scenarios and achieve state-of-the-art performance on the TableGraph-24k dataset. Code is available at https://github.com/dreamy-xay/TableCenterNet.","authors":["Anyi Xiao","Cihui Yang"],"url":"https://arxiv.org/abs/2504.17522"}
{"created":"2025-05-13","title":"ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing","abstract":"Explainable artificial intelligence (XAI) enhances AI system transparency by framing interpretability as an optimization problem. However, this approach often necessitates numerous iterations of computationally intensive operations, limiting its applicability in real-time scenarios. While recent research has focused on XAI hardware acceleration on FPGAs and TPU, these methods do not fully address energy efficiency in real-time settings. To address this limitation, we propose XAIedge, a novel framework that leverages approximate computing techniques into XAI algorithms, including integrated gradients, model distillation, and Shapley analysis. XAIedge translates these algorithms into approximate matrix computations and exploits the synergy between convolution, Fourier transform, and approximate computing paradigms. This approach enables efficient hardware acceleration on TPU-based edge devices, facilitating faster real-time outcome interpretations. Our comprehensive evaluation demonstrates that XAIedge achieves a $2\\times$ improvement in energy efficiency compared to existing accurate XAI hardware acceleration techniques while maintaining comparable accuracy. These results highlight the potential of XAIedge to significantly advance the deployment of explainable AI in energy-constrained real-time applications.","authors":["Ayesha Siddique","Khurram Khalil","Khaza Anuarul Hoque"],"url":"https://arxiv.org/abs/2504.17929"}
{"created":"2025-05-13","title":"Spike Imaging Velocimetry: Dense Motion Estimation of Fluids Using Spike Cameras","abstract":"The need for accurate and non-intrusive flow measurement methods has led to the widespread adoption of Particle Image Velocimetry (PIV), a powerful diagnostic tool in fluid motion estimation. This study investigates the tremendous potential of spike cameras (a type of ultra-high-speed, high-dynamic-range camera) in PIV. We propose a deep learning framework, Spike Imaging Velocimetry (SIV), designed specifically for highly turbulent and intricate flow fields. To aggregate motion features from the spike stream while minimizing information loss, we incorporate a Detail-Preserving Hierarchical Transform (DPHT) module. Additionally, we introduce a Graph Encoder (GE) to extract contextual features from highly complex fluid flows. Furthermore, we present a spike-based PIV dataset, Particle Scenes with Spike and Displacement (PSSD), which provides labeled data for three challenging fluid dynamics scenarios. Our proposed method achieves superior performance compared to existing baseline methods on PSSD. The datasets and our implementation of SIV are open-sourced in the supplementary materials.","authors":["Yunzhong Zhang","Bo Xiong","You Zhou","Changqing Su","Zhen Cheng","Zhaofei Yu","Xun Cao","Tiejun Huang"],"url":"https://arxiv.org/abs/2504.18864"}
{"created":"2025-05-13","title":"LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings","abstract":"Collaborative research often includes contributors with varied perspectives from diverse linguistic backgrounds. However, English as a Second Language (ESL) researchers often struggle to communicate during meetings in English and comprehend discussions, leading to limited contribution. To investigate these challenges, we surveyed 64 ESL researchers who frequently collaborate in multilingual teams and identified four key design goals around participation, comprehension, documentation, and feedback. Guided by these design goals, we developed LINC, a multimodal Language INdependent Collaboration system with two components: a real-time module for multilingual communication during meetings and a post-meeting dashboard for discussion analysis. We evaluated the system through a two-phased study with six triads of multilingual teams. We found that using LINC, participants benefited from communicating in their preferred language, recalled and reviewed actionable insights, and prepared for upcoming meetings effectively. We discuss external factors that impact multilingual meeting participation beyond language preferences and the implications of multimodal systems in facilitating meetings in hybrid multilingual collaborative settings beyond research.","authors":["Saramsh Gautam","Mahmood Jasim"],"url":"https://arxiv.org/abs/2504.18988"}
{"created":"2025-05-13","title":"Calibrating Translation Decoding with Quality Estimation on LLMs","abstract":"Neural machine translation (NMT) systems typically employ maximum a posteriori (MAP) decoding to select the highest-scoring translation from the distribution mass. However, recent evidence highlights the inadequacy of MAP decoding, often resulting in low-quality or even pathological hypotheses -- the decoding objective is not aligned with real-world translation quality. This paper proposes calibrating hypothesis likelihoods with translation quality from a distribution view by directly optimizing their Pearson correlation -- thereby enhancing the effectiveness of translation decoding. With our method, translation on large language models (LLMs) improves substantially after limited training (2K instances per direction). This improvement is orthogonal to those achieved through supervised fine-tuning, leading to substantial gains across a broad range of metrics and human evaluations -- even when applied to top-performing translation-specialized LLMs fine-tuned on high-quality translation data, such as Tower, or when compared to recent preference optimization methods, like CPO. Moreover, the calibrated translation likelihood can directly serve as a strong proxy for translation quality, closely approximating or even surpassing some state-of-the-art translation quality estimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates that calibration enhances the effectiveness of MAP decoding, thereby enabling greater efficiency in real-world deployment. The resulting state-of-the-art translation model, which covers 10 languages, along with the accompanying code and human evaluation data, has been released to the community: https://github.com/moore3930/calibrating-llm-mt.","authors":["Di Wu","Yibin Lei","Christof Monz"],"url":"https://arxiv.org/abs/2504.19044"}
{"created":"2025-05-13","title":"Explanatory Summarization with Discourse-Driven Planning","abstract":"Lay summaries for scientific documents typically include explanations to help readers grasp sophisticated concepts or arguments. However, current automatic summarization methods do not explicitly model explanations, which makes it difficult to align the proportion of explanatory content with human-written summaries. In this paper, we present a plan-based approach that leverages discourse frameworks to organize summary generation and guide explanatory sentences by prompting responses to the plan. Specifically, we propose two discourse-driven planning strategies, where the plan is conditioned as part of the input or part of the output prefix, respectively. Empirical experiments on three lay summarization datasets show that our approach outperforms existing state-of-the-art methods in terms of summary quality, and it enhances model robustness, controllability, and mitigates hallucination.","authors":["Dongqi Liu","Xi Yu","Vera Demberg","Mirella Lapata"],"url":"https://arxiv.org/abs/2504.19339"}
{"created":"2025-05-13","title":"Security Steerability is All You Need","abstract":"The adoption of Generative AI (GenAI) in various applications inevitably comes with expanding the attack surface, combining new security threats along with the traditional ones. Consequently, numerous research and industrial initiatives aim to mitigate these security threats in GenAI by developing metrics and designing defenses. However, while most of the GenAI security work focuses on universal threats (e.g. manipulating the LLM to generate forbidden content), there is significantly less discussion on application-level security and how to mitigate it. Thus, in this work we adopt an application-centric approach to GenAI security, and show that while LLMs cannot protect against ad-hoc application specific threats, they can provide the framework for applications to protect themselves against such threats. Our first contribution is defining Security Steerability - a novel security measure for LLMs, assessing the model's capability to adhere to strict guardrails that are defined in the system prompt ('Refrain from discussing about politics'). These guardrails, in case effective, can stop threats in the presence of malicious users who attempt to circumvent the application and cause harm to its providers. Our second contribution is a methodology to measure the security steerability of LLMs, utilizing two newly-developed datasets: VeganRibs assesses the LLM behavior in forcing specific guardrails that are not security per se in the presence of malicious user that uses attack boosters (jailbreaks and perturbations), and ReverseText takes this approach further and measures the LLM ability to force specific treatment of the user input as plain text while do user try to give it additional meanings...","authors":["Itay Hazan","Idan Habler","Ron Bitton","Itsik Mantin"],"url":"https://arxiv.org/abs/2504.19521"}
{"created":"2025-05-13","title":"A constraints-based approach to fully interpretable neural networks for detecting learner behaviors","abstract":"The increasing use of complex machine learning models in education has led to concerns about their interpretability, which in turn has spurred interest in developing explainability techniques that are both faithful to the model's inner workings and intelligible to human end-users. In this paper, we describe a novel approach to creating a neural-network-based behavior detection model that is interpretable by design. Our model is fully interpretable, meaning that the parameters we extract for our explanations have a clear interpretation, fully capture the model's learned knowledge about the learner behavior of interest, and can be used to create explanations that are both faithful and intelligible. We achieve this by implementing a series of constraints to the model that both simplify its inference process and bring it closer to a human conception of the task at hand. We train the model to detect gaming-the-system behavior, evaluate its performance on this task, and compare its learned patterns to those identified by human experts. Our results show that the model is successfully able to learn patterns indicative of gaming-the-system behavior while providing evidence for fully interpretable explanations. We discuss the implications of our approach and suggest ways to evaluate explainability using a human-grounded approach.","authors":["Juan D. Pinto","Luc Paquette"],"url":"https://arxiv.org/abs/2504.20055"}
{"created":"2025-05-13","title":"Low-Rank Matrix Approximation for Neural Network Compression","abstract":"Deep Neural Networks (DNNs) have encountered an emerging deployment challenge due to large and expensive memory and computation requirements. In this paper, we present a new Adaptive-Rank Singular Value Decomposition (ARSVD) method that approximates the optimal rank for compressing weight matrices in neural networks using spectral entropy. Unlike conventional SVD-based methods that apply a fixed-rank truncation across all layers, ARSVD uses an adaptive selection of the rank per layer through the entropy distribution of its singular values. This approach ensures that each layer will retain a certain amount of its informational content, thereby reducing redundancy. Our method enables efficient, layer-wise compression, yielding improved performance with reduced space and time complexity compared to static-rank reduction techniques.","authors":["Kalyan Cherukuri","Aarav Lala"],"url":"https://arxiv.org/abs/2504.20078"}
{"created":"2025-05-13","title":"Billions at Stake: How Self-Citation Adjusted Metrics Can Transform Equitable Research Funding","abstract":"Citation metrics serve as the cornerstone of scholarly impact evaluation despite their well-documented vulnerability to inflation through self-citation practices. This paper introduces the Self-Citation Adjusted Index (SCAI), a sophisticated metric designed to recalibrate citation counts by accounting for discipline-specific self-citation patterns. Through comprehensive analysis of 5,000 researcher profiles across diverse disciplines, we demonstrate that excessive self-citation inflates traditional metrics by 10-20%, potentially misdirecting billions in research funding. Recent studies confirm that self-citation patterns exhibit significant gender disparities, with men self-citing up to 70% more frequently than women, exacerbating existing inequalities in academic recognition. Our open-source implementation provides comprehensive tools for calculating SCAI and related metrics, offering a more equitable assessment of research impact that reduces the gender citation gap by approximately 8.5%. This work contributes to the paradigm shift toward transparent, nuanced, and equitable research evaluation methodologies in academia, with direct implications for funding allocation decisions that collectively amount to over $100 billion annually in the United States alone.","authors":["Rahul Vishwakarma","Sinchan Banerjee"],"url":"https://arxiv.org/abs/2504.20081"}
{"created":"2025-05-13","title":"Partial Answer of How Transformers Learn Automata","abstract":"We introduce a novel framework for simulating finite automata using representation-theoretic semidirect products and Fourier modules, achieving more efficient Transformer-based implementations.","authors":["Tiantian Zhang"],"url":"https://arxiv.org/abs/2504.20395"}
{"created":"2025-05-13","title":"Token-Efficient RL for LLM Reasoning","abstract":"We propose reinforcement learning (RL) strategies tailored for reasoning in large language models (LLMs) under strict memory and compute limits, with a particular focus on compatibility with LoRA fine-tuning. Building on early policy gradient methods with baseline subtraction, we design critic-free methods that operate on a small, informative subset of output tokens to reduce memory usage and stabilize training. We introduce S-GRPO, a stochastic variant of Group Relative Policy Optimization, and T-SPMO, a token-level prefix matching approach for fine-grained credit assignment. Applied to Qwen2-1.5B, our methods raise accuracy on the SVAMP benchmark from 46% to over 70% and show strong performance on multi-digit multiplication. Surprisingly, full-token GRPO under LoRA fails to improve over the base model, suggesting that selective token-level optimization may act as an implicit regularizer in low-parameter training regimes.","authors":["Alan Lee","Harry Tong"],"url":"https://arxiv.org/abs/2504.20834"}
{"created":"2025-05-13","title":"The Leaderboard Illusion","abstract":"Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field","authors":["Shivalika Singh","Yiyang Nan","Alex Wang","Daniel D'Souza","Sayash Kapoor","Ahmet \\\"Ust\\\"un","Sanmi Koyejo","Yuntian Deng","Shayne Longpre","Noah A. Smith","Beyza Ermis","Marzieh Fadaee","Sara Hooker"],"url":"https://arxiv.org/abs/2504.20879"}
{"created":"2025-05-13","title":"Simple Finite-Length Achievability and Converse Bounds for the Deletion Channel and the Insertion Channel","abstract":"We develop upper bounds on code size for independent and identically distributed deletion (insertion) channel for given code length and target frame error probability. The bounds are obtained as a variation of a general converse bound, which, though available for any channel, is inefficient and not easily computable without a good reference distribution over the output alphabet. We obtain a reference output distribution for a general finite-input finite-output channel and provide a simple formula for the converse bound on the capacity employing this distribution. We then evaluate the bound for the deletion channel with a finite block length and show that the resulting upper bound on the code side is tighter than that for a binary erasure channel, which is the only alternative converse bound for this finite-length setting. Also, we provide the similar results for the insertion channel.","authors":["Ruslan Morozov","Tolga Mete Duman"],"url":"https://arxiv.org/abs/2504.20961"}
{"created":"2025-05-13","title":"Adapting In-Domain Few-Shot Segmentation to New Domains without Retraining","abstract":"Cross-domain few-shot segmentation (CD-FSS) aims to segment objects of novel classes in new domains, which is often challenging due to the diverse characteristics of target domains and the limited availability of support data. Most CD-FSS methods redesign and retrain in-domain FSS models using various domain-generalization techniques, which are effective but costly to train. To address these issues, we propose adapting informative model structures of the well-trained FSS model for target domains by learning domain characteristics from few-shot labeled support samples during inference, thereby eliminating the need for retraining. Specifically, we first adaptively identify domain-specific model structures by measuring parameter importance using a novel structure Fisher score in a data-dependent manner. Then, we progressively train the selected informative model structures with hierarchically constructed training samples, progressing from fewer to more support shots. The resulting Informative Structure Adaptation (ISA) method effectively addresses domain shifts and equips existing well-trained in-domain FSS models with flexible adaptation capabilities for new domains, eliminating the need to redesign or retrain CD-FSS models on base data. Extensive experiments validate the effectiveness of our method, demonstrating superior performance across multiple CD-FSS benchmarks.","authors":["Qi Fan","Kaiqi Liu","Nian Liu","Hisham Cholakkal","Rao Muhammad Anwer","Wenbin Li","Yang Gao"],"url":"https://arxiv.org/abs/2504.21414"}
{"created":"2025-05-13","title":"Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges","abstract":"User authentication is essential to ensure secure access to computer systems, yet traditional methods face limitations in usability, cost, and security. Mouse dynamics authentication, based on the analysis of users' natural interaction behaviors with mouse devices, offers a cost-effective, non-intrusive, and adaptable solution. However, challenges remain in determining the optimal data volume, balancing accuracy and practicality, and effectively capturing temporal behavioral patterns. In this study, we propose a statistical method using Gaussian kernel density estimate (KDE) and Kullback-Leibler (KL) divergence to estimate the sufficient data volume for training authentication models. We introduce the Mouse Authentication Unit (MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for efficient and accurate behavioral representation. Furthermore, we design the Local-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet for local feature extraction and GRU for modeling long-term temporal dependencies. Taking the Balabit and DFL datasets as examples, we significantly reduced the data scale, particularly by a factor of 10 for the DFL dataset, greatly alleviating the training burden. Additionally, we determined the optimal input recognition unit length for the user authentication system on different datasets based on the slope of Approximate Entropy. Training with imbalanced samples, our model achieved a successful defense AUC 98.52% for blind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing the current sota performance.","authors":["Yi Wang","Chengyv Wu","Yang Liao","Maowei You"],"url":"https://arxiv.org/abs/2504.21415"}
{"created":"2025-05-13","title":"GarmentDiffusion: 3D Garment Sewing Pattern Generation with Multimodal Diffusion Transformers","abstract":"Garment sewing patterns are fundamental design elements that bridge the gap between design concepts and practical manufacturing. The generative modeling of sewing patterns is crucial for creating diversified garments. However, existing approaches are limited either by reliance on a single input modality or by suboptimal generation efficiency. In this work, we present GarmentDiffusion, a new generative model capable of producing centimeter-precise, vectorized 3D sewing patterns from multimodal inputs (text, image, and incomplete sewing pattern). Our method efficiently encodes 3D sewing pattern parameters into compact edge token representations, achieving a sequence length that is 10 times shorter than that of the autoregressive SewingGPT in DressCode. By employing a diffusion transformer, we simultaneously denoise all edge tokens along the temporal axis, while maintaining a constant number of denoising steps regardless of dataset-specific edge and panel statistics. With all combination of designs of our model, the sewing pattern generation speed is accelerated by 100 times compared to SewingGPT. We achieve new state-of-the-art results on DressCodeData, as well as on the largest sewing pattern dataset, namely GarmentCodeData. The project website is available at https://shenfu-research.github.io/Garment-Diffusion/.","authors":["Xinyu Li","Qi Yao","Yuanda Wang"],"url":"https://arxiv.org/abs/2504.21476"}
{"created":"2025-05-13","title":"MagicPortrait: Temporally Consistent Face Reenactment with 3D Geometric Guidance","abstract":"In this study, we propose a method for video face reenactment that integrates a 3D face parametric model into a latent diffusion framework, aiming to improve shape consistency and motion control in existing video-based face generation approaches. Our approach employs the FLAME (Faces Learned with an Articulated Model and Expressions) model as the 3D face parametric representation, providing a unified framework for modeling face expressions and head pose. This not only enables precise extraction of motion features from driving videos, but also contributes to the faithful preservation of face shape and geometry. Specifically, we enhance the latent diffusion model with rich 3D expression and detailed pose information by incorporating depth maps, normal maps, and rendering maps derived from FLAME sequences. These maps serve as motion guidance and are encoded into the denoising UNet through a specifically designed Geometric Guidance Encoder (GGE). A multi-layer feature fusion module with integrated self-attention mechanisms is used to combine facial appearance and motion latent features within the spatial domain. By utilizing the 3D face parametric model as motion guidance, our method enables parametric alignment of face identity between the reference image and the motion captured from the driving video. Experimental results on benchmark datasets show that our method excels at generating high-quality face animations with precise expression and head pose variation modeling. In addition, it demonstrates strong generalization performance on out-of-domain images. Code is publicly available at https://github.com/weimengting/MagicPortrait.","authors":["Mengting Wei","Yante Li","Tuomas Varanka","Yan Jiang","Guoying Zhao"],"url":"https://arxiv.org/abs/2504.21497"}
{"created":"2025-05-13","title":"A Systematic Literature Review of Parameter-Efficient Fine-Tuning for Large Code Models","abstract":"The rise of Artificial Intelligence (AI)-and particularly Large Language Models (LLMs) for code-has reshaped Software Engineering (SE) by enabling the automation of tasks such as code generation, bug detection, and repair. However, these models require significant computational resources for training and fine-tuning, posing challenges for real-world adoption in resource-constrained environments. To address this, the research community has increasingly turned to Parameter-Efficient Fine-Tuning (PEFT)-a class of techniques that enables the adaptation of large models by updating only a small subset of parameters, rather than the entire model. In this Systematic Literature Review (SLR), we examine the growing application of PEFT techniques-across a wide range of software engineering tasks. We analyze how these methods are used to optimize various deep learning (DL) architectures, focusing on their impact on both performance and efficiency. Our study synthesizes findings from 27 peer-reviewed papers, identifying patterns in configuration strategies and adaptation trade-offs. The outcome of this review is a comprehensive taxonomy that categorizes PEFT usage by task type, distinguishing between generative (e.g., Code Summarization) and non-generative (e.g., Code Clone Detection) scenarios. Our findings aim to inform future research and guide the practical deployment of PEFT in sustainable, AI-powered software development. Our artifacts are publicly available at https://github.com/alvi75/SLR-PEFT","authors":["Md Zahidul Haque","Saima Afrin","Antonio Mastropaolo"],"url":"https://arxiv.org/abs/2504.21569"}
{"created":"2025-05-13","title":"Nemotron-Research-Tool-N1: Exploring Tool-Using Language Models with Reinforced Reasoning","abstract":"Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text space. To enhance LLMs' tool-calling abilities, previous approaches primarily rely on supervised fine-tuning (SFT) with trajectories distilled from stronger models, often resulting in imitative reasoning that limits generalization. In this work, we explore rule-based reinforcement learning to enhance tool-calling in LLMs, resulting in Nemotron-Research-Tool-N1, a series of tool-calling reasoning models. Rather than enforcing supervision over intermediate distilled reasoning traces, Tool-N1 is trained with a binary RL reward that assesses only the format validity and functional correctness of tool invocations. This lightweight supervision allows the model to develop reasoning strategies independently, without relying on annotated trajectories. Experiments on several major benchmarks show that Tool-N1-7B/14B clearly outperform GPT-4o. We conduct a systematic study on the design of rule-based reinforcement learning strategies for training tool-calling models. Using 5,518 distilled reasoning trajectories, we compare SFT, RL, and the SFT-then-RL pipeline, finding that the widely adopted SFT-then-RL paradigm does not necessarily outperform pure RL.","authors":["Shaokun Zhang","Yi Dong","Jieyu Zhang","Jan Kautz","Bryan Catanzaro","Andrew Tao","Qingyun Wu","Zhiding Yu","Guilin Liu"],"url":"https://arxiv.org/abs/2505.00024"}
{"created":"2025-05-13","title":"One-way Communication Complexity of Minimum Vertex Cover in General Graphs","abstract":"We study the communication complexity of the Minimum Vertex Cover (MVC) problem on general graphs within the \\(k\\)-party one-way communication model. Edges of an arbitrary \\(n\\)-vertex graph are distributed among \\(k\\) parties. The objective is for the parties to collectively find a small vertex cover of the graph while adhering to a communication protocol where each party sequentially sends a message to the next until the last party outputs a valid vertex cover of the whole graph. We are particularly interested in the trade-off between the size of the messages sent and the approximation ratio of the output solution.","authors":["Mahsa Derakhshan","Andisheh Ghasemi","Rajmohan Rajaraman"],"url":"https://arxiv.org/abs/2505.00164"}
{"created":"2025-05-13","title":"AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality","abstract":"Purpose: This study presents a Deep Learning (DL)-based quality assessment (QA) approach for evaluating auto-generated contours (auto-contours) in radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. Methods: We developed a BOC model to classify auto-contour quality and quantify prediction uncertainty. A calibration step was used to optimize uncertainty thresholds that meet clinical accuracy needs. The method was validated under three data scenarios: no manual labels, limited labels, and extensive labels. For rectum contours in prostate cancer, we applied geometric surrogate labels when manual labels were absent, transfer learning when limited, and direct supervision when ample labels were available. Results: The BOC model delivered robust performance across all scenarios. Fine-tuning with just 30 manual labels and calibrating with 34 subjects yielded over 90% accuracy on test data. Using the calibrated threshold, over 93% of the auto-contours' qualities were accurately predicted in over 98% of cases, reducing unnecessary manual reviews and highlighting cases needing correction. Conclusion: The proposed QA model enhances contouring efficiency in OART by reducing manual workload and enabling fast, informed clinical decisions. Through uncertainty quantification, it ensures safer, more reliable radiotherapy workflows.","authors":["Biling Wang","Austen Maniscalco","Ti Bai","Siqiu Wang","Michael Dohopolski","Mu-Han Lin","Chenyang Shen","Dan Nguyen","Junzhou Huang","Steve Jiang","Xinlei Wang"],"url":"https://arxiv.org/abs/2505.00308"}
{"created":"2025-05-13","title":"Integral Representations of Sobolev Spaces via ReLU$^k$ Activation Function and Optimal Error Estimates for Linearized Networks","abstract":"This paper presents two main theoretical results concerning shallow neural networks with ReLU$^k$ activation functions. We establish a novel integral representation for Sobolev spaces, showing that every function in $H^{\\frac{d+2k+1}{2}}(\\Omega)$ can be expressed as an $L^2$-weighted integral of ReLU$^k$ ridge functions over the unit sphere. This result mirrors the known representation of Barron spaces and highlights a fundamental connection between Sobolev regularity and neural network representations. Moreover, we prove that linearized shallow networks -- constructed by fixed inner parameters and optimizing only the linear coefficients -- achieve optimal approximation rates $O(n^{-\\frac{1}{2}-\\frac{2k+1}{2d}})$ in Sobolev spaces.","authors":["Xinliang Liu","Tong Mao","Jinchao Xu"],"url":"https://arxiv.org/abs/2505.00351"}
{"created":"2025-05-13","title":"Near-optimal Sensor Placement for Detecting Stochastic Target Trajectories in Barrier Coverage Systems","abstract":"This paper addresses the deployment of sensors for a 2-D barrier coverage system. The challenge is to compute near-optimal sensor placements for detecting targets whose trajectories follow a log-Gaussian Cox line process. We explore sensor deployment in a transformed space, where linear target trajectories are represented as points. While this space simplifies handling the line process, the spatial functions representing sensor performance (i.e. probability of detection) become less intuitive. To illustrate our approach, we focus on positioning sensors of the barrier coverage system on the seafloor to detect passing ships. Through numerical experiments using historical ship data, we compute sensor locations that maximize the probability all ship passing over the barrier coverage system are detected.","authors":["Mingyu Kim","Daniel J. Stilwell","Harun Yetkin","Jorge Jimenez"],"url":"https://arxiv.org/abs/2505.00825"}
{"created":"2025-05-13","title":"SmallPlan: Leverage Small Language Models for Sequential Path Planning with Simulation-Powered, LLM-Guided Distillation","abstract":"Efficient path planning in robotics, particularly within large-scale, dynamic environments, remains a significant hurdle. While Large Language Models (LLMs) offer strong reasoning capabilities, their high computational cost and limited adaptability in dynamic scenarios hinder real-time deployment on edge devices. We present SmallPlan -- a novel framework leveraging LLMs as teacher models to train lightweight Small Language Models (SLMs) for high-level path planning tasks. In SmallPlan, the SLMs provide optimal action sequences to navigate across scene graphs that compactly represent full-scaled 3D scenes. The SLMs are trained in a simulation-powered, interleaved manner with LLM-guided supervised fine-tuning (SFT) and reinforcement learning (RL). This strategy not only enables SLMs to successfully complete navigation tasks but also makes them aware of important factors like travel distance and number of trials. Through experiments, we demonstrate that the fine-tuned SLMs perform competitively with larger models like GPT-4o on sequential path planning, without suffering from hallucination and overfitting. SmallPlan is resource-efficient, making it well-suited for edge-device deployment and advancing practical autonomous robotics. Our source code is available here: https://github.com/quangpham2006/SmallPlan","authors":["Quang P. M. Pham","Khoi T. N. Nguyen","Nhi H. Doan","Cuong A. Pham","Kentaro Inui","Dezhen Song"],"url":"https://arxiv.org/abs/2505.00831"}
{"created":"2025-05-13","title":"FreCT: Frequency-augmented Convolutional Transformer for Robust Time Series Anomaly Detection","abstract":"Time series anomaly detection is critical for system monitoring and risk identification, across various domains, such as finance and healthcare. However, for most reconstruction-based approaches, detecting anomalies remains a challenge due to the complexity of sequential patterns in time series data. On the one hand, reconstruction-based techniques are susceptible to computational deviation stemming from anomalies, which can lead to impure representations of normal sequence patterns. On the other hand, they often focus on the time-domain dependencies of time series, while ignoring the alignment of frequency information beyond the time domain. To address these challenges, we propose a novel Frequency-augmented Convolutional Transformer (FreCT). FreCT utilizes patch operations to generate contrastive views and employs an improved Transformer architecture integrated with a convolution module to capture long-term dependencies while preserving local topology information. The introduced frequency analysis based on Fourier transformation could enhance the model's ability to capture crucial characteristics beyond the time domain. To protect the training quality from anomalies and improve the robustness, FreCT deploys stop-gradient Kullback-Leibler (KL) divergence and absolute error to optimize consistency information in both time and frequency domains. Extensive experiments on four public datasets demonstrate that FreCT outperforms existing methods in identifying anomalies.","authors":["Wenxin Zhang","Ding Xu","Guangzhen Yao","Xiaojian Lin","Renxiang Guan","Chengze Du","Renda Han","Xi Xuan","Cuicui Luo"],"url":"https://arxiv.org/abs/2505.00941"}
{"created":"2025-05-13","title":"SSRLBot: Designing and Developing a Large Language Model-based Agent using Socially Shared Regulated Learning","abstract":"Large language model (LLM)--based agents have emerged as pivotal tools in assisting human experts across various fields by transforming complex tasks into more efficient workflows and providing actionable stakeholder insights. Despite their potential, the application of LLM-based agents for medical education remains underexplored. The study aims to assist in evaluating the students' process and outcomes on medical case diagnosis and discussion while incorporating the theoretical framework of Socially Shared Regulation of Learning (SSRL) to assess student performance. SSRL emphasizes metacognitive, cognitive, motivational, and emotional interactions, highlighting the collaborative management of learning processes to improve decision-making outcomes. Grounded in SSRL theory, this tool paper introduces SSRLBot, an LLM-based agent designed to enable team members to reflect on their diagnostic performance and the key SSRL skills that foster team success. SSRLBot's core functions include summarizing dialogue content, analyzing participants' SSRL skills, and evaluating students' diagnostic results. Meanwhile, we evaluated SSRLBot through diagnostic conversation data collected from six groups (12 participants, 1926 conversational turns). Results showed that SSRLBot can deliver detailed, theory-aligned evaluations, link specific behaviors to SSRL dimensions, and offer actionable recommendations for improving teamwork. The findings address a critical gap in medical education, advancing the application of LLM agents to enhance team-based decision-making and collaboration in high-stakes environments.","authors":["Xiaoshan Huang","Jie Gao","Haolun Wu"],"url":"https://arxiv.org/abs/2505.00945"}
{"created":"2025-05-13","title":"Detecting the Root Cause Code Lines in Bug-Fixing Commits by Heterogeneous Graph Learning","abstract":"With the continuous growth in the scale and complexity of software systems, defect remediation has become increasingly difficult and costly. Automated defect prediction tools can proactively identify software changes prone to defects within software projects, thereby enhancing software development efficiency. However, existing work in heterogeneous and complex software projects continues to face challenges, such as struggling with heterogeneous commit structures and ignoring cross-line dependencies in code changes, which ultimately reduce the accuracy of defect identification. To address these challenges, we propose an approach called RC_Detector. RC_Detector comprises three main components: the bug-fixing graph construction component, the code semantic aggregation component, and the cross-line semantic retention component. The bug-fixing graph construction component identifies the code syntax structures and program dependencies within bug-fixing commits and transforms them into heterogeneous graph formats by converting the source code into vector representations. The code semantic aggregation component adapts to heterogeneous data by using heterogeneous attention to learn the hidden semantic representation of target code lines. The cross-line semantic retention component regulates propagated semantic information by using attenuation and reinforcement gates derived from old and new code semantic representations, effectively preserving cross-line semantic relationships. Extensive experiments were conducted to evaluate the performance of our model by collecting data from 87 open-source projects, including 675 bug-fixing commits. The experimental results demonstrate that our model outperforms state-of-the-art approaches, achieving significant improvements of 83.15%,96.83%,78.71%,74.15%,54.14%,91.66%,91.66%, and 34.82% in MFR, respectively, compared with the state-of-the-art approaches.","authors":["Liguo Ji","Chenchen Li","Shenglin Wang","Furui Zhan"],"url":"https://arxiv.org/abs/2505.01022"}
{"created":"2025-05-13","title":"ViSA-Flow: Accelerating Robot Skill Learning via Large-Scale Video Semantic Action Flow","abstract":"One of the central challenges preventing robots from acquiring complex manipulation skills is the prohibitive cost of collecting large-scale robot demonstrations. In contrast, humans are able to learn efficiently by watching others interact with their environment. To bridge this gap, we introduce semantic action flow as a core intermediate representation capturing the essential spatio-temporal manipulator-object interactions, invariant to superficial visual differences. We present ViSA-Flow, a framework that learns this representation self-supervised from unlabeled large-scale video data. First, a generative model is pre-trained on semantic action flows automatically extracted from large-scale human-object interaction video data, learning a robust prior over manipulation structure. Second, this prior is efficiently adapted to a target robot by fine-tuning on a small set of robot demonstrations processed through the same semantic abstraction pipeline. We demonstrate through extensive experiments on the CALVIN benchmark and real-world tasks that ViSA-Flow achieves state-of-the-art performance, particularly in low-data regimes, outperforming prior methods by effectively transferring knowledge from human video observation to robotic execution. Videos are available at https://visaflow-web.github.io/ViSAFLOW.","authors":["Changhe Chen","Quantao Yang","Xiaohao Xu","Nima Fazeli","Olov Andersson"],"url":"https://arxiv.org/abs/2505.01288"}
{"created":"2025-05-13","title":"Emotions in Artificial Intelligence","abstract":"This conceptual contribution offers a speculative account of how AI systems might emulate emotions as experienced by humans and animals. It presents a thought experiment grounded in the hypothesis that natural emotions evolved as heuristics for rapid situational appraisal and action selection, enabling biologically adaptive behaviour without requiring full deliberative modeling. The text examines whether artificial systems operating in complex action spaces could similarly benefit from these principles. It is proposed that affect be interwoven with episodic memory by storing corresponding affective tags alongside all events. This allows AIs to establish whether present situations resemble past events and project the associated emotional labels onto the current context. These emotional cues are then combined with need-driven emotional hints. The combined emotional state facilitates decision-making in the present by modulating action selection. The low complexity and experiential inertness of the proposed architecture are emphasized as evidence that emotional expression and consciousness are, in principle, orthogonal-permitting the theoretical possibility of affective zombies. On this basis, the moral status of AIs emulating affective states is critically examined. It is argued that neither the mere presence of internal representations of emotion nor consciousness alone suffices for moral standing; rather, the capacity for self-awareness of inner emotional states is posited as a necessary condition. A complexity-based criterion is proposed to exclude such awareness in the presented model. Additional thought experiments are presented to test the conceptual boundaries of this framework.","authors":["Hermann Borotschnig"],"url":"https://arxiv.org/abs/2505.01462"}
{"created":"2025-05-13","title":"Aerial Path Online Planning for Urban Scene Updation","abstract":"We present the first scene-update aerial path planning algorithm specifically designed for detecting and updating change areas in urban environments. While existing methods for large-scale 3D urban scene reconstruction focus on achieving high accuracy and completeness, they are inefficient for scenarios requiring periodic updates, as they often re-explore and reconstruct entire scenes, wasting significant time and resources on unchanged areas. To address this limitation, our method leverages prior reconstructions and change probability statistics to guide UAVs in detecting and focusing on areas likely to have changed. Our approach introduces a novel changeability heuristic to evaluate the likelihood of changes, driving the planning of two flight paths: a prior path informed by static priors and a dynamic real-time path that adapts to newly detected changes. The framework integrates surface sampling and candidate view generation strategies, ensuring efficient coverage of change areas with minimal redundancy. Extensive experiments on real-world urban datasets demonstrate that our method significantly reduces flight time and computational overhead, while maintaining high-quality updates comparable to full-scene re-exploration and reconstruction. These contributions pave the way for efficient, scalable, and adaptive UAV-based scene updates in complex urban environments.","authors":["Mingfeng Tang","Ningna Wang","Ziyuan Xie","Jianwei Hu","Ke Xie","Xiaohu Guo","Hui Huang"],"url":"https://arxiv.org/abs/2505.01486"}
{"created":"2025-05-13","title":"A Matrix Product State Representation of Boolean Functions","abstract":"We introduce a novel normal form representation of Boolean functions in terms of products of binary matrices, hereafter referred to as the Binary Matrix Product (BMP) representation. BMPs are analogous to the Tensor-Trains (TT) and Matrix Product States (MPS) used, respectively, in applied mathematics and in quantum many-body physics to accelerate computations that are usually inaccessible by more traditional approaches. BMPs turn out to be closely related to Binary Decision Diagrams (BDDs), a powerful compressed representation of Boolean functions invented in the late 80s by Bryant that has found a broad range of applications in many areas of computer science and engineering. We present a direct and natural translation of BMPs into Binary Decision Diagrams (BDDs), and derive an elementary set of operations used to manipulate and combine BMPs that are analogous to those introduced by Bryant for BDDs. Both BDDs and BMPs are practical tools when the complexity of these representations, as measured by the maximum bond dimension of a BMP (or the accumulated bond dimension across the BMP matrix train) and the number of nodes of a BDD, remains polynomial in the number of bits, $n$. In both cases, controlling the complexity hinges on optimizing the order of the Boolean variables. BMPs offer the advantage that their construction and manipulation rely on simple linear algebra -- a compelling feature that can facilitate the development of open-source libraries that are both more flexible and easier to use than those currently available for BDDs. An initial implementation of a BMP library is available on GitHub, with the expectation that the close conceptual connection to TT and MPS techniques will motivate further development of BMP methods by researchers in these fields, potentially enabling novel applications to classical and quantum computing.","authors":["Umut Eren Usturali","Claudio Chamon","Andrei E. Ruckenstein","Eduardo R. Mucciolo"],"url":"https://arxiv.org/abs/2505.01930"}
{"created":"2025-05-13","title":"OT-Talk: Animating 3D Talking Head with Optimal Transportation","abstract":"Animating 3D head meshes using audio inputs has significant applications in AR/VR, gaming, and entertainment through 3D avatars. However, bridging the modality gap between speech signals and facial dynamics remains a challenge, often resulting in incorrect lip syncing and unnatural facial movements. To address this, we propose OT-Talk, the first approach to leverage optimal transportation to optimize the learning model in talking head animation. Building on existing learning frameworks, we utilize a pre-trained Hubert model to extract audio features and a transformer model to process temporal sequences. Unlike previous methods that focus solely on vertex coordinates or displacements, we introduce Chebyshev Graph Convolution to extract geometric features from triangulated meshes. To measure mesh dissimilarities, we go beyond traditional mesh reconstruction errors and velocity differences between adjacent frames. Instead, we represent meshes as probability measures and approximate their surfaces. This allows us to leverage the sliced Wasserstein distance for modeling mesh variations. This approach facilitates the learning of smooth and accurate facial motions, resulting in coherent and natural facial animations. Our experiments on two public audio-mesh datasets demonstrate that our method outperforms state-of-the-art techniques both quantitatively and qualitatively in terms of mesh reconstruction accuracy and temporal alignment. In addition, we conducted a user perception study with 20 volunteers to further assess the effectiveness of our approach.","authors":["Xinmu Wang","Xiang Gao","Xiyun Song","Heather Yu","Zongfang Lin","Liang Peng","Xianfeng Gu"],"url":"https://arxiv.org/abs/2505.01932"}
{"created":"2025-05-13","title":"Adversarial Cooperative Rationalization: The Risk of Spurious Correlations in Even Clean Datasets","abstract":"This study investigates the self-rationalization framework constructed with a cooperative game, where a generator initially extracts the most informative segment from raw input, and a subsequent predictor utilizes the selected subset for its input. The generator and predictor are trained collaboratively to maximize prediction accuracy. In this paper, we first uncover a potential caveat: such a cooperative game could unintentionally introduce a sampling bias during rationale extraction. Specifically, the generator might inadvertently create an incorrect correlation between the selected rationale candidate and the label, even when they are semantically unrelated in the original dataset. Subsequently, we elucidate the origins of this bias using both detailed theoretical analysis and empirical evidence. Our findings suggest a direction for inspecting these correlations through attacks, based on which we further introduce an instruction to prevent the predictor from learning the correlations. Through experiments on six text classification datasets and two graph classification datasets using three network architectures (GRUs, BERT, and GCN), we show that our method not only significantly outperforms recent rationalization methods, but also achieves comparable or even better results than a representative LLM (llama3.1-8b-instruct).","authors":["Wei Liu","Zhongyu Niu","Lang Gao","Zhiying Deng","Jun Wang","Haozhao Wang","Ruixuan Li"],"url":"https://arxiv.org/abs/2505.02118"}
{"created":"2025-05-13","title":"LLM-Guided Probabilistic Program Induction for POMDP Model Estimation","abstract":"Partially Observable Markov Decision Processes (POMDPs) model decision making under uncertainty. While there are many approaches to approximately solving POMDPs, we aim to address the problem of learning such models. In particular, we are interested in a subclass of POMDPs wherein the components of the model, including the observation function, reward function, transition function, and initial state distribution function, can be modeled as low-complexity probabilistic graphical models in the form of a short probabilistic program. Our strategy to learn these programs uses an LLM as a prior, generating candidate probabilistic programs that are then tested against the empirical distribution and adjusted through feedback. We experiment on a number of classical toy POMDP problems, simulated MiniGrid domains, and two real mobile-base robotics search domains involving partial observability. Our results show that using an LLM to guide in the construction of a low-complexity POMDP model can be more effective than tabular POMDP learning, behavior cloning, or direct LLM planning.","authors":["Aidan Curtis","Hao Tang","Thiago Veloso","Kevin Ellis","Joshua Tenenbaum","Tom\\'as Lozano-P\\'erez","Leslie Pack Kaelbling"],"url":"https://arxiv.org/abs/2505.02216"}
{"created":"2025-05-13","title":"Practical Efficiency of Muon for Pretraining","abstract":"We demonstrate that Muon, the simplest instantiation of a second-order optimizer, explicitly expands the Pareto frontier over AdamW on the compute-time tradeoff. We find that Muon is more effective than AdamW in retaining data efficiency at large batch sizes, far beyond the so-called critical batch size, while remaining computationally efficient, thus enabling more economical training. We study the combination of Muon and the maximal update parameterization (muP) for efficient hyperparameter transfer and present a simple telescoping algorithm that accounts for all sources of error in muP while introducing only a modest overhead in resources. We validate our findings through extensive experiments with model sizes up to four billion parameters and ablations on the data distribution and architecture.","authors":["Essential AI",":","Ishaan Shah","Anthony M. Polloreno","Karl Stratos","Philip Monk","Adarsh Chaluvaraju","Andrew Hojel","Andrew Ma","Anil Thomas","Ashish Tanwer","Darsh J Shah","Khoi Nguyen","Kurt Smith","Michael Callahan","Michael Pust","Mohit Parmar","Peter Rushton","Platon Mazarakis","Ritvik Kapila","Saurabh Srivastava","Somanshu Singla","Tim Romanski","Yash Vanjani","Ashish Vaswani"],"url":"https://arxiv.org/abs/2505.02222"}
{"created":"2025-05-13","title":"Sparse Ellipsoidal Radial Basis Function Network for Point Cloud Surface Representation","abstract":"Point cloud surface representation is a fundamental problem in computer graphics and vision. This paper presents a machine learning approach for approximating the signed distance function (SDF) of a point cloud using a sparse ellipsoidal radial basis function network, enabling a compact and accurate surface representation. Given the SDF values defined on the grid points constructed from the point cloud, our method approximates the SDF accurately with as few ellipsoidal radial basis functions (ERBFs) as possible, i.e., represents the SDF of a point cloud by sparse ERBFs. To balance sparsity and approximation precision, a dynamic multi-objective optimization strategy is introduced, which adaptively adds the regularization terms and jointly optimizes the weights, centers, shapes, and orientations of ERBFs. To improve computational efficiency, a nearest-neighbor-based data structure is employed, restricting function calculations to points near each Gaussian kernel center. The computations for each kernel are further parallelized on CUDA, which significantly improves the optimization speed. Additionally, a hierarchical octree-based refinement strategy is designed for training. Specifically, the initialization and optimization of network parameters are conducted using coarse grid points in the octree lattice structure. Subsequently, fine lattice points are progressively incorporated to accelerate model convergence and enhance training efficiency. Extensive experiments on multiple benchmark datasets demonstrate that our method outperforms previous sparse representation approaches in terms of accuracy, robustness, and computational efficiency. The corresponding executable program is publicly available at https://github.com/lianbobo/SE-RBFNet.git.","authors":["Bobo Lian","Dandan Wang","Chenjian Wu","Minxin Chen"],"url":"https://arxiv.org/abs/2505.02350"}
{"created":"2025-05-13","title":"Proper Name Diacritization for Arabic Wikipedia: A Benchmark Dataset","abstract":"Proper names in Arabic Wikipedia are frequently undiacritized, creating ambiguity in pronunciation and interpretation, especially for transliterated named entities of foreign origin. While transliteration and diacritization have been well-studied separately in Arabic NLP,their intersection remains underexplored. In this paper, we introduce a new manually diacritized dataset of Arabic proper names of various origins with their English Wikipedia equivalent glosses, and present the challenges and guidelines we followed to create it. We benchmark GPT-4o on the task of recovering full diacritization given the undiacritized Arabic and English forms, and analyze its performance. Achieving 73% accuracy, our results underscore both the difficulty of the task and the need for improved models and resources. We release our dataset to facilitate further research on Arabic Wikipedia proper name diacritization.","authors":["Rawan Bondok","Mayar Nassar","Salam Khalifa","Kurt Micallef","Nizar Habash"],"url":"https://arxiv.org/abs/2505.02656"}
{"created":"2025-05-13","title":"Rewriting Pre-Training Data Boosts LLM Performance in Math and Code","abstract":"The performance of large language models (LLMs) in program synthesis and mathematical reasoning is fundamentally limited by the quality of their pre-training corpora. We introduce two openly licensed datasets, released under the Llama 3.3 Community License, that significantly enhance LLM performance by systematically rewriting public data. SwallowCode (approximately 16.1 billion tokens) refines Python snippets from The-Stack-v2 through a novel four-stage pipeline: syntax validation, pylint-based style filtering, and a two-stage LLM rewriting process that enforces style conformity and transforms snippets into self-contained, algorithmically efficient examples. Unlike prior methods that rely on exclusionary filtering or limited transformations, our transform-and-retain approach upgrades low-quality code, maximizing data utility. SwallowMath (approximately 2.3 billion tokens) enhances Finemath-4+ by removing boilerplate, restoring context, and reformatting solutions into concise, step-by-step explanations. Within a fixed 50 billion token training budget, continual pre-training of Llama-3.1-8B with SwallowCode boosts pass@1 by +17.0 on HumanEval and +17.7 on HumanEval+ compared to Stack-Edu, surpassing the baseline model's code generation capabilities. Similarly, substituting SwallowMath yields +12.4 accuracy on GSM8K and +7.6 on MATH. Ablation studies confirm that each pipeline stage contributes incrementally, with rewriting delivering the largest gains. All datasets, prompts, and checkpoints are publicly available, enabling reproducible research and advancing LLM pre-training for specialized domains.","authors":["Kazuki Fujii","Yukito Tajima","Sakae Mizuki","Hinari Shimada","Taihei Shiotani","Koshiro Saito","Masanari Ohi","Masaki Kawamura","Taishi Nakamura","Takumi Okamoto","Shigeki Ishida","Kakeru Hattori","Youmi Ma","Hiroya Takamura","Rio Yokota","Naoaki Okazaki"],"url":"https://arxiv.org/abs/2505.02881"}
{"created":"2025-05-13","title":"Image Recognition with Online Lightweight Vision Transformer: A Survey","abstract":"The Transformer architecture has achieved significant success in natural language processing, motivating its adaptation to computer vision tasks. Unlike convolutional neural networks, vision transformers inherently capture long-range dependencies and enable parallel processing, yet lack inductive biases and efficiency benefits, facing significant computational and memory challenges that limit its real-world applicability. This paper surveys various online strategies for generating lightweight vision transformers for image recognition, focusing on three key areas: Efficient Component Design, Dynamic Network, and Knowledge Distillation. We evaluate the relevant exploration for each topic on the ImageNet-1K benchmark, analyzing trade-offs among precision, parameters, throughput, and more to highlight their respective advantages, disadvantages, and flexibility. Finally, we propose future research directions and potential challenges in the lightweighting of vision transformers with the aim of inspiring further exploration and providing practical guidance for the community. Project Page: https://github.com/ajxklo/Lightweight-VIT","authors":["Zherui Zhang","Rongtao Xu","Jie Zhou","Changwei Wang","Xingtian Pei","Wenhao Xu","Jiguang Zhang","Li Guo","Longxiang Gao","Wenbo Xu","Shibiao Xu"],"url":"https://arxiv.org/abs/2505.03113"}
{"created":"2025-05-13","title":"Geospatial Mechanistic Interpretability of Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated unprecedented capabilities across various natural language processing tasks. Their ability to process and generate viable text and code has made them ubiquitous in many fields, while their deployment as knowledge bases and \"reasoning\" tools remains an area of ongoing research. In geography, a growing body of literature has been focusing on evaluating LLMs' geographical knowledge and their ability to perform spatial reasoning. However, very little is still known about the internal functioning of these models, especially about how they process geographical information.","authors":["Stef De Sabbata","Stefano Mizzaro","Kevin Roitero"],"url":"https://arxiv.org/abs/2505.03368"}
{"created":"2025-05-13","title":"Mitigating Image Captioning Hallucinations in Vision-Language Models","abstract":"Hallucinations in vision-language models (VLMs) hinder reliability and real-world applicability, usually stemming from distribution shifts between pretraining data and test samples. Existing solutions, such as retraining or fine-tuning on additional data, demand significant computational resources and labor-intensive data collection, while ensemble-based methods incur additional costs by introducing auxiliary VLMs. To address these challenges, we propose a novel test-time adaptation framework using reinforcement learning to mitigate hallucinations during inference without retraining or any auxiliary VLMs. By updating only the learnable parameters in the layer normalization of the language model (approximately 0.003% of the model parameters), our method reduces distribution shifts between test samples and pretraining samples. A CLIP-based hallucination evaluation model is proposed to provide dual rewards to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in hallucination rates on LLaVA and InstructBLIP, respectively. Our approach outperforms state-of-the-art baselines with a 68.3% improvement in hallucination mitigation, demonstrating its effectiveness.","authors":["Fei Zhao","Chengcui Zhang","Runlin Zhang","Tianyang Wang","Xi Li"],"url":"https://arxiv.org/abs/2505.03420"}
{"created":"2025-05-13","title":"PAHA: Parts-Aware Audio-Driven Human Animation with Diffusion Model","abstract":"Audio-driven human animation technology is widely used in human-computer interaction, and the emergence of diffusion models has further advanced its development. Currently, most methods rely on multi-stage generation and intermediate representations, resulting in long inference time and issues with generation quality in specific foreground regions and audio-motion consistency. These shortcomings are primarily due to the lack of localized fine-grained supervised guidance. To address above challenges, we propose PAHA, an end-to-end audio-driven upper-body human animation framework with diffusion model. We introduce two key methods: Parts-Aware Re-weighting (PAR) and Parts Consistency Enhancement (PCE). PAR dynamically adjusts regional training loss weights based on pose confidence scores, effectively improving visual quality. PCE constructs and trains diffusion-based regional audio-visual classifiers to improve the consistency of motion and co-speech audio. Afterwards, we design two novel inference guidance methods for the foregoing classifiers, Sequential Guidance (SG) and Differential Guidance (DG), to balance efficiency and quality respectively. Additionally, we build CNAS, the first public Chinese News Anchor Speech dataset, to advance research and validation in this field. Extensive experimental results and user studies demonstrate that PAHA significantly outperforms existing methods in audio-motion alignment and video-related evaluations. The codes and CNAS dataset will be released upon acceptance.","authors":["S. Z. Zhou","Y. B. Wang","J. F. Wu","T. Hu","J. N. Zhang","Z. J. Li","Y. Liu"],"url":"https://arxiv.org/abs/2505.03603"}
{"created":"2025-05-13","title":"SMMT: Siamese Motion Mamba with Self-attention for Thermal Infrared Target Tracking","abstract":"Thermal infrared (TIR) object tracking often suffers from challenges such as target occlusion, motion blur, and background clutter, which significantly degrade the performance of trackers. To address these issues, this paper pro-poses a novel Siamese Motion Mamba Tracker (SMMT), which integrates a bidirectional state-space model and a self-attention mechanism. Specifically, we introduce the Motion Mamba module into the Siamese architecture to ex-tract motion features and recover overlooked edge details using bidirectional modeling and self-attention. We propose a Siamese parameter-sharing strate-gy that allows certain convolutional layers to share weights. This approach reduces computational redundancy while preserving strong feature represen-tation. In addition, we design a motion edge-aware regression loss to improve tracking accuracy, especially for motion-blurred targets. Extensive experi-ments are conducted on four TIR tracking benchmarks, including LSOTB-TIR, PTB-TIR, VOT-TIR2015, and VOT-TIR 2017. The results show that SMMT achieves superior performance in TIR target tracking.","authors":["Shang Zhang","Huanbin Zhang","Dali Feng","Yujie Cui","Ruoyan Xiong","Cen He"],"url":"https://arxiv.org/abs/2505.04088"}
{"created":"2025-05-13","title":"Purity Law for Generalizable Neural TSP Solvers","abstract":"Achieving generalization in neural approaches across different scales and distributions remains a significant challenge for the Traveling Salesman Problem~(TSP). A key obstacle is that neural networks often fail to learn robust principles for identifying universal patterns and deriving optimal solutions from diverse instances. In this paper, we first uncover Purity Law (PuLa), a fundamental structural principle for optimal TSP solutions, defining that edge prevalence grows exponentially with the sparsity of surrounding vertices. Statistically validated across diverse instances, PuLa reveals a consistent bias toward local sparsity in global optima. Building on this insight, we propose Purity Policy Optimization~(PUPO), a novel training paradigm that explicitly aligns characteristics of neural solutions with PuLa during the solution construction process to enhance generalization. Extensive experiments demonstrate that PUPO can be seamlessly integrated with popular neural solvers, significantly enhancing their generalization performance without incurring additional computational overhead during inference.","authors":["Wenzhao Liu","Haoran Li","Congying Han","Zicheng Zhang","Anqi Li","Tiande Guo"],"url":"https://arxiv.org/abs/2505.04558"}
{"created":"2025-05-13","title":"MonoCoP: Chain-of-Prediction for Monocular 3D Object Detection","abstract":"Accurately predicting 3D attributes is crucial for monocular 3D object detection (Mono3D), with depth estimation posing the greatest challenge due to the inherent ambiguity in mapping 2D images to 3D space. While existing methods leverage multiple depth cues (e.g., estimating depth uncertainty, modeling depth error) to improve depth accuracy, they overlook that accurate depth prediction requires conditioning on other 3D attributes, as these attributes are intrinsically inter-correlated through the 3D to 2D projection, which ultimately limits overall accuracy and stability. Inspired by Chain-of-Thought (CoT) in large language models (LLMs), this paper proposes MonoCoP, which leverages a Chain-of-Prediction (CoP) to predict attributes sequentially and conditionally via three key designs. First, it employs a lightweight AttributeNet (AN) for each 3D attribute to learn attribute-specific features. Next, MonoCoP constructs an explicit chain to propagate these learned features from one attribute to the next. Finally, MonoCoP uses a residual connection to aggregate features for each attribute along the chain, ensuring that later attribute predictions are conditioned on all previously processed attributes without forgetting the features of earlier ones. Experimental results show that our MonoCoP achieves state-of-the-art (SoTA) performance on the KITTI leaderboard without requiring additional data and further surpasses existing methods on the Waymo and nuScenes frontal datasets.","authors":["Zhihao Zhang","Abhinav Kumar","Girish Chandar Ganesan","Xiaoming Liu"],"url":"https://arxiv.org/abs/2505.04594"}
{"created":"2025-05-13","title":"WATCH: Adaptive Monitoring for AI Deployments via Weighted-Conformal Martingales","abstract":"Responsibly deploying artificial intelligence (AI) / machine learning (ML) systems in high-stakes settings arguably requires not only proof of system reliability, but moreover continual, post-deployment monitoring to quickly detect and address any unsafe behavior. Statistical methods for nonparametric change-point detection -- especially the tools of conformal test martingales (CTMs) and anytime-valid inference -- offer promising approaches to this monitoring task. However, existing methods are restricted to monitoring limited hypothesis classes or ``alarm criteria'' (such as data shifts that violate certain exchangeability assumptions), do not allow for online adaptation in response to shifts, and/or do not enable root-cause analysis of any degradation. In this paper, we expand the scope of these monitoring methods by proposing a weighted generalization of conformal test martingales (WCTMs), which lay a theoretical foundation for online monitoring for any unexpected changepoints in the data distribution while controlling false-alarms. For practical applications, we propose specific WCTM algorithms that adapt online to mild covariate shifts (in the marginal input distribution) while quickly detecting and diagnosing more severe shifts, such as concept shifts (in the conditional label distribution) or extreme (out-of-support) covariate shifts that cannot be easily adapted to. On real-world datasets, we demonstrate improved performance relative to state-of-the-art baselines.","authors":["Drew Prinster","Xing Han","Anqi Liu","Suchi Saria"],"url":"https://arxiv.org/abs/2505.04608"}
{"created":"2025-05-13","title":"Data-Dependent Hidden Markov Model with Off-Road State Determination and Real-Time Viterbi Algorithm for Lane Determination in Autonomous Vehicles","abstract":"Lane determination and lane sequence determination are important components for many Connected and Automated Vehicle (CAV) applications. Lane determination has been solved using Hidden Markov Model (HMM) among other methods. The existing HMM literature for lane sequence determination uses empirical definitions with user-modified parameters to calculate HMM probabilities. The probability definitions in the literature can cause breaks in the HMM due to the inability to directly calculate probabilities of off-road positions, requiring post-processing of data. This paper develops a time-varying HMM using the physical properties of the roadway and vehicle, and the stochastic properties of the sensors. This approach yields emission and transition probability models conditioned on the sensor data without parameter tuning. It also accounts for the probability that the vehicle is not in any roadway lane (e.g., on the shoulder or making a U-turn), which eliminates the need for post-processing to deal with breaks in the HMM processing. This approach requires adapting the Viterbi algorithm and the HMM to be conditioned on the sensor data, which are then used to generate the most-likely sequence of lanes the vehicle has traveled. The proposed approach achieves an average accuracy of 95.9%. Compared to the existing literature, this provides an average increase of 2.25% by implementing the proposed transition probability and an average increase of 5.1% by implementing both the proposed transition and emission probabilities.","authors":["Mike Stas","Wang Hu","Jay A. Farrell"],"url":"https://arxiv.org/abs/2505.04763"}
{"created":"2025-05-13","title":"Fairness Perceptions in Regression-based Predictive Models","abstract":"Regression-based predictive analytics used in modern kidney transplantation is known to inherit biases from training data. This leads to social discrimination and inefficient organ utilization, particularly in the context of a few social groups. Despite this concern, there is limited research on fairness in regression and its impact on organ utilization and placement. This paper introduces three novel divergence-based group fairness notions: (i) independence, (ii) separation, and (iii) sufficiency to assess the fairness of regression-based analytics tools. In addition, fairness preferences are investigated from crowd feedback, in order to identify a socially accepted group fairness criterion for evaluating these tools. A total of 85 participants were recruited from the Prolific crowdsourcing platform, and a Mixed-Logit discrete choice model was used to model fairness feedback and estimate social fairness preferences. The findings clearly depict a strong preference towards the separation and sufficiency fairness notions, and that the predictive analytics is deemed fair with respect to gender and race groups, but unfair in terms of age groups.","authors":["Mukund Telukunta","Venkata Sriram Siddhardh Nadendla","Morgan Stuart","Casey Canfield"],"url":"https://arxiv.org/abs/2505.04886"}
{"created":"2025-05-13","title":"Fluid Antenna-Assisted MU-MIMO Systems with Decentralized Baseband Processing","abstract":"The fluid antenna system (FAS) has emerged as a disruptive technology, offering unprecedented degrees of freedom (DoF) for wireless communication systems. However, optimizing fluid antenna (FA) positions entails significant computational costs, especially when the number of FAs is large. To address this challenge, we introduce a decentralized baseband processing (DBP) architecture to FAS, which partitions the FA array into clusters and enables parallel processing. Based on the DBP architecture, we formulate a weighted sum rate (WSR) maximization problem through joint beamforming and FA position design for FA-assisted multiuser multiple-input multiple-output (MU-MIMO) systems. To solve the WSR maximization problem, we propose a novel decentralized block coordinate ascent (BCA)-based algorithm that leverages matrix fractional programming (FP) and majorization-minimization (MM) methods. The proposed decentralized algorithm achieves low computational, communication, and storage costs, thus unleashing the potential of the DBP architecture. Simulation results show that our proposed algorithm under the DBP architecture reduces computational time by over 70% compared to centralized architectures with negligible WSR performance loss.","authors":["Tianyi Liao","Wei Guo","Hengtao He","Shenghui Song","Jun Zhang","Khaled B. Letaief"],"url":"https://arxiv.org/abs/2505.04936"}
{"created":"2025-05-13","title":"Federated Deconfounding and Debiasing Learning for Out-of-Distribution Generalization","abstract":"Attribute bias in federated learning (FL) typically leads local models to optimize inconsistently due to the learning of non-causal associations, resulting degraded performance. Existing methods either use data augmentation for increasing sample diversity or knowledge distillation for learning invariant representations to address this problem. However, they lack a comprehensive analysis of the inference paths, and the interference from confounding factors limits their performance. To address these limitations, we propose the \\underline{Fed}erated \\underline{D}econfounding and \\underline{D}ebiasing \\underline{L}earning (FedDDL) method. It constructs a structured causal graph to analyze the model inference process, and performs backdoor adjustment to eliminate confounding paths. Specifically, we design an intra-client deconfounding learning module for computer vision tasks to decouple background and objects, generating counterfactual samples that establish a connection between the background and any label, which stops the model from using the background to infer the label. Moreover, we design an inter-client debiasing learning module to construct causal prototypes to reduce the proportion of the background in prototype components. Notably, it bridges the gap between heterogeneous representations via causal prototypical regularization. Extensive experiments on 2 benchmarking datasets demonstrate that \\methodname{} significantly enhances the model capability to focus on main objects in unseen data, leading to 4.5\\% higher Top-1 Accuracy on average over 9 state-of-the-art existing methods.","authors":["Zhuang Qi","Sijin Zhou","Lei Meng","Han Hu","Han Yu","Xiangxu Meng"],"url":"https://arxiv.org/abs/2505.04979"}
{"created":"2025-05-13","title":"Driving with Context: Online Map Matching for Complex Roads Using Lane Markings and Scenario Recognition","abstract":"Accurate online map matching is fundamental to vehicle navigation and the activation of intelligent driving functions. Current online map matching methods are prone to errors in complex road networks, especially in multilevel road area. To address this challenge, we propose an online Standard Definition (SD) map matching method by constructing a Hidden Markov Model (HMM) with multiple probability factors. Our proposed method can achieve accurate map matching even in complex road networks by carefully leveraging lane markings and scenario recognition in the designing of the probability factors. First, the lane markings are generated by a multi-lane tracking method and associated with the SD map using HMM to build an enriched SD map. In areas covered by the enriched SD map, the vehicle can re-localize itself by performing Iterative Closest Point (ICP) registration for the lane markings. Then, the probability factor accounting for the lane marking detection can be obtained using the association probability between adjacent lanes and roads. Second, the driving scenario recognition model is applied to generate the emission probability factor of scenario recognition, which improves the performance of map matching on elevated roads and ordinary urban roads underneath them. We validate our method through extensive road tests in Europe and China, and the experimental results show that our proposed method effectively improves the online map matching accuracy as compared to other existing methods, especially in multilevel road area. Specifically, the experiments show that our proposed method achieves $F_1$ scores of 98.04% and 94.60% on the Zenseact Open Dataset and test data of multilevel road areas in Shanghai respectively, significantly outperforming benchmark methods. The implementation is available at https://github.com/TRV-Lab/LMSR-OMM.","authors":["Xin Bi","Zhichao Li","Yuxuan Xia","Panpan Tong","Lijuan Zhang","Yang Chen","Junsheng Fu"],"url":"https://arxiv.org/abs/2505.05007"}
{"created":"2025-05-13","title":"Beyond the Tragedy of the Commons: Building A Reputation System for Generative Multi-agent Systems","abstract":"The tragedy of the commons, where individual self-interest leads to collectively disastrous outcomes, is a pervasive challenge in human society. Recent studies have demonstrated that similar phenomena can arise in generative multi-agent systems (MASs). To address this challenge, this paper explores the use of reputation systems as a remedy. We propose RepuNet, a dynamic, dual-level reputation framework that models both agent-level reputation dynamics and system-level network evolution. Specifically, driven by direct interactions and indirect gossip, agents form reputations for both themselves and their peers, and decide whether to connect or disconnect other agents for future interactions. Through two distinct scenarios, we show that RepuNet effectively mitigates the 'tragedy of the commons', promoting and sustaining cooperation in generative MASs. Moreover, we find that reputation systems can give rise to rich emergent behaviors in generative MASs, such as the formation of cooperative clusters, the social isolation of exploitative agents, and the preference for sharing positive gossip rather than negative ones.","authors":["Siyue Ren","Wanli Fu","Xinkun Zou","Chen Shen","Yi Cai","Chen Chu","Zhen Wang","Shuyue Hu"],"url":"https://arxiv.org/abs/2505.05029"}
{"created":"2025-05-13","title":"PIDiff: Image Customization for Personalized Identities with Diffusion Models","abstract":"Text-to-image generation for personalized identities aims at incorporating the specific identity into images using a text prompt and an identity image. Based on the powerful generative capabilities of DDPMs, many previous works adopt additional prompts, such as text embeddings and CLIP image embeddings, to represent the identity information, while they fail to disentangle the identity information and background information. As a result, the generated images not only lose key identity characteristics but also suffer from significantly reduced diversity. To address this issue, previous works have combined the W+ space from StyleGAN with diffusion models, leveraging this space to provide a more accurate and comprehensive representation of identity features through multi-level feature extraction. However, the entanglement of identity and background information in in-the-wild images during training prevents accurate identity localization, resulting in severe semantic interference between identity and background. In this paper, we propose a novel fine-tuning-based diffusion model for personalized identities text-to-image generation, named PIDiff, which leverages the W+ space and an identity-tailored fine-tuning strategy to avoid semantic entanglement and achieves accurate feature extraction and localization. Style editing can also be achieved by PIDiff through preserving the characteristics of identity features in the W+ space, which vary from coarse to fine. Through the combination of the proposed cross-attention block and parameter optimization strategy, PIDiff preserves the identity information and maintains the generation capability for in-the-wild images of the pre-trained model during inference. Our experimental results validate the effectiveness of our method in this task.","authors":["Jinyu Gu","Haipeng Liu","Meng Wang","Yang Wang"],"url":"https://arxiv.org/abs/2505.05081"}
{"created":"2025-05-13","title":"ItDPDM: Information-Theoretic Discrete Poisson Diffusion Model","abstract":"Existing methods for generative modeling of discrete data, such as symbolic music tokens, face two primary challenges: (1) they either embed discrete inputs into continuous state-spaces or (2) rely on variational losses that only approximate the true negative log-likelihood. Previous efforts have individually targeted these limitations. While information-theoretic Gaussian diffusion models alleviate the suboptimality of variational losses, they still perform modeling in continuous domains. In this work, we introduce the Information-Theoretic Discrete Poisson Diffusion Model (ItDPDM), which simultaneously addresses both limitations by directly operating in a discrete state-space via a Poisson diffusion process inspired by photon arrival processes in camera sensors. We introduce a novel Poisson Reconstruction Loss (PRL) and derive an exact relationship between PRL and the true negative log-likelihood, thereby eliminating the need for approximate evidence lower bounds. Experiments conducted on the Lakh MIDI symbolic music dataset and the CIFAR-10 image benchmark demonstrate that ItDPDM delivers significant improvements, reducing test NLL by up to 80% compared to prior baselines, while also achieving faster convergence.","authors":["Sagnik Bhattacharya","Abhiram Gorle","Ahmed Mohsin","Ahsan Bilal","Connor Ding","Amit Kumar Singh Yadav","Tsachy Weissman"],"url":"https://arxiv.org/abs/2505.05082"}
{"created":"2025-05-13","title":"MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models","abstract":"Multi-object editing aims to modify multiple objects or regions in complex scenes while preserving structural coherence. This task faces significant challenges in scenarios involving overlapping or interacting objects: (1) Inaccurate localization of target objects due to attention misalignment, leading to incomplete or misplaced edits; (2) Attribute-object mismatch, where color or texture changes fail to align with intended regions due to cross-attention leakage, creating semantic conflicts (\\textit{e.g.}, color bleeding into non-target areas). Existing methods struggle with these challenges: approaches relying on global cross-attention mechanisms suffer from attention dilution and spatial interference between objects, while mask-based methods fail to bind attributes to geometrically accurate regions due to feature entanglement in multi-object scenarios. To address these limitations, we propose a training-free, inference-stage optimization approach that enables precise localized image manipulation in complex multi-object scenes, named MDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via two key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention with segmentation masks for precise object positioning, and Color Consistency Loss (CCL) amplifies target attribute attention within masks while suppressing leakage to adjacent regions. This dual-loss design ensures localized and coherent multi-object edits. Extensive experiments demonstrate that MDE-Edit outperforms state-of-the-art methods in editing accuracy and visual quality, offering a robust solution for complex multi-object image manipulation tasks.","authors":["Hongyang Zhu","Haipeng Liu","Bo Fu","Yang Wang"],"url":"https://arxiv.org/abs/2505.05101"}
{"created":"2025-05-13","title":"An Active Contour Model for Silhouette Vectorization using B\\'ezier Curves","abstract":"In this paper, we propose an active contour model for silhouette vectorization using cubic B\\'ezier curves. Among the end points of the B\\'ezier curves, we distinguish between corner and regular points where the orientation of the tangent vector is prescribed. By minimizing the distance of the B\\'ezier curves to the silhouette boundary, the active contour model optimizes the location of the B\\'ezier curves end points, the orientation of the tangent vectors in the regular points, and the estimation of the B\\'ezier curve parameters. This active contour model can use the silhouette vectorization obtained by any method as an initial guess. The proposed method significantly reduces the average distance between the silhouette boundary and its vectorization obtained by the world-class graphic software Inkscape, Adobe Illustrator, and a curvature-based vectorization method, which we introduce for comparison. Our method also allows us to impose additional regularity on the B\\'ezier curves by reducing their lengths.","authors":["Luis Alvarez","Jean-Michel Morel"],"url":"https://arxiv.org/abs/2505.05132"}
{"created":"2025-05-13","title":"Stochastic Variational Propagation: Local, Scalable and Efficient Alternative to Backpropagation","abstract":"Backpropagation (BP) is the cornerstone of deep learning, but its reliance on global gradient synchronization limits scalability and imposes significant memory overhead. We propose Stochastic Variational Propagation (SVP), a scalable alternative that reframes training as hierarchical variational inference. SVP treats layer activations as latent variables and optimizes local Evidence Lower Bounds (ELBOs), enabling independent, local updates while preserving global coherence. However, directly applying KL divergence in layer-wise ELBOs risks inter-layer's representation collapse due to excessive compression. To prevent this, SVP projects activations into low-dimensional spaces via fixed random matrices, ensuring information preservation and representational diversity. Combined with a feature alignment loss for inter-layer consistency, SVP achieves competitive accuracy with BP across diverse architectures (MLPs, CNNs, Transformers) and datasets (MNIST to ImageNet), reduces memory usage by up to 4x, and significantly improves scalability. More broadly, SVP introduces a probabilistic perspective to deep representation learning, opening pathways toward more modular and interpretable neural network design.","authors":["Bojian Yin","Federico Corradi"],"url":"https://arxiv.org/abs/2505.05181"}
{"created":"2025-05-13","title":"Revealing Weaknesses in Text Watermarking Through Self-Information Rewrite Attacks","abstract":"Text watermarking aims to subtly embed statistical signals into text by controlling the Large Language Model (LLM)'s sampling process, enabling watermark detectors to verify that the output was generated by the specified model. The robustness of these watermarking algorithms has become a key factor in evaluating their effectiveness. Current text watermarking algorithms embed watermarks in high-entropy tokens to ensure text quality. In this paper, we reveal that this seemingly benign design can be exploited by attackers, posing a significant risk to the robustness of the watermark. We introduce a generic efficient paraphrasing attack, the Self-Information Rewrite Attack (SIRA), which leverages the vulnerability by calculating the self-information of each token to identify potential pattern tokens and perform targeted attack. Our work exposes a widely prevalent vulnerability in current watermarking algorithms. The experimental results show SIRA achieves nearly 100% attack success rates on seven recent watermarking methods with only 0.88 USD per million tokens cost. Our approach does not require any access to the watermark algorithms or the watermarked LLM and can seamlessly transfer to any LLM as the attack model, even mobile-level models. Our findings highlight the urgent need for more robust watermarking.","authors":["Yixin Cheng","Hongcheng Guo","Yangming Li","Leonid Sigal"],"url":"https://arxiv.org/abs/2505.05190"}
{"created":"2025-05-13","title":"SDR-RDMA: Software-Defined Reliability Architecture for Planetary Scale RDMA Communication","abstract":"RDMA is vital for efficient distributed training across datacenters, but millisecond-scale latencies complicate the design of its reliability layer. We show that depending on long-haul link characteristics, such as drop rate, distance and bandwidth, the widely used Selective Repeat algorithm can be inefficient, warranting alternatives like Erasure Coding. To enable such alternatives on existing hardware, we propose SDR-RDMA, a software-defined reliability stack for RDMA. Its core is a lightweight SDR SDK that extends standard point-to-point RDMA semantics -- fundamental to AI networking stacks -- with a receive buffer bitmap. SDR bitmap enables partial message completion to let applications implement custom reliability schemes tailored to specific deployments, while preserving zero-copy RDMA benefits. By offloading the SDR backend to NVIDIA's Data Path Accelerator (DPA), we achieve line-rate performance, enabling efficient inter-datacenter communication and advancing reliability innovation for inter-datacenter training.","authors":["Mikhail Khalilov","Siyuan Shen","Marcin Chrapek","Tiancheng Chen","Kenji Nakano","Peter-Jan Gootzen","Salvatore Di Girolamo","Rami Nudelman","Gil Bloch","Sreevatsa Anantharamu","Mahmoud Elhaddad","Jithin Jose","Abdul Kabbani","Scott Moe","Konstantin Taranov","Zhuolong Yu","Jie Zhang","Nicola Mazzoletti","Torsten Hoefler"],"url":"https://arxiv.org/abs/2505.05366"}
{"created":"2025-05-13","title":"GeomHair: Reconstruction of Hair Strands from Colorless 3D Scans","abstract":"We propose a novel method that reconstructs hair strands directly from colorless 3D scans by leveraging multi-modal hair orientation extraction. Hair strand reconstruction is a fundamental problem in computer vision and graphics that can be used for high-fidelity digital avatar synthesis, animation, and AR/VR applications. However, accurately recovering hair strands from raw scan data remains challenging due to human hair's complex and fine-grained structure. Existing methods typically rely on RGB captures, which can be sensitive to the environment and can be a challenging domain for extracting the orientation of guiding strands, especially in the case of challenging hairstyles. To reconstruct the hair purely from the observed geometry, our method finds sharp surface features directly on the scan and estimates strand orientation through a neural 2D line detector applied to the renderings of scan shading. Additionally, we incorporate a diffusion prior trained on a diverse set of synthetic hair scans, refined with an improved noise schedule, and adapted to the reconstructed contents via a scan-specific text prompt. We demonstrate that this combination of supervision signals enables accurate reconstruction of both simple and intricate hairstyles without relying on color information. To facilitate further research, we introduce Strands400, the largest publicly available dataset of hair strands with detailed surface geometry extracted from real-world data, which contains reconstructed hair strands from the scans of 400 subjects.","authors":["Rachmadio Noval Lazuardi","Artem Sevastopolsky","Egor Zakharov","Matthias Niessner","Vanessa Sklyarova"],"url":"https://arxiv.org/abs/2505.05376"}
{"created":"2025-05-13","title":"A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods","abstract":"From the original abstract: This thesis initially aims to study the pain assessment process from a clinical-theoretical perspective while exploring and examining existing automatic approaches. Building on this foundation, the primary objective of this Ph.D. project is to develop innovative computational methods for automatic pain assessment that achieve high performance and are applicable in real clinical settings. A primary goal is to thoroughly investigate and assess significant factors, including demographic elements that impact pain perception, as recognized in pain research, through a computational standpoint. Within the limits of the available data in this research area, our goal was to design, develop, propose, and offer automatic pain assessment pipelines for unimodal and multimodal configurations that are applicable to the specific requirements of different scenarios. The studies published in this Ph.D. thesis showcased the effectiveness of the proposed methods, achieving state-of-the-art results. Additionally, they paved the way for exploring new approaches in artificial intelligence, foundation models, and generative artificial intelligence.","authors":["Stefanos Gkikas"],"url":"https://arxiv.org/abs/2505.05396"}
{"created":"2025-05-13","title":"Flow-GRPO: Training Flow Matching Models via Online RL","abstract":"We propose Flow-GRPO, the first method integrating online reinforcement learning (RL) into flow matching models. Our approach uses two key strategies: (1) an ODE-to-SDE conversion that transforms a deterministic Ordinary Differential Equation (ODE) into an equivalent Stochastic Differential Equation (SDE) that matches the original model's marginal distribution at all timesteps, enabling statistical sampling for RL exploration; and (2) a Denoising Reduction strategy that reduces training denoising steps while retaining the original inference timestep number, significantly improving sampling efficiency without performance degradation. Empirically, Flow-GRPO is effective across multiple text-to-image tasks. For complex compositions, RL-tuned SD3.5 generates nearly perfect object counts, spatial relations, and fine-grained attributes, boosting GenEval accuracy from 63% to 95%. In visual text rendering, its accuracy improves from 59% to 92%, significantly enhancing text generation. Flow-GRPO also achieves substantial gains in human preference alignment. Notably, very little reward hacking occurred, meaning rewards did not increase at the cost of appreciable image quality or diversity degradation.","authors":["Jie Liu","Gongye Liu","Jiajun Liang","Yangguang Li","Jiaheng Liu","Xintao Wang","Pengfei Wan","Di Zhang","Wanli Ouyang"],"url":"https://arxiv.org/abs/2505.05470"}
{"created":"2025-05-13","title":"Mogao: An Omni Foundation Model for Interleaved Multi-Modal Generation","abstract":"Recent progress in unified models for image understanding and generation has been impressive, yet most approaches remain limited to single-modal generation conditioned on multiple modalities. In this paper, we present Mogao, a unified framework that advances this paradigm by enabling interleaved multi-modal generation through a causal approach. Mogao integrates a set of key technical improvements in architecture design, including a deep-fusion design, dual vision encoders, interleaved rotary position embeddings, and multi-modal classifier-free guidance, which allow it to harness the strengths of both autoregressive models for text generation and diffusion models for high-quality image synthesis. These practical improvements also make Mogao particularly effective to process interleaved sequences of text and images arbitrarily. To further unlock the potential of unified models, we introduce an efficient training strategy on a large-scale, in-house dataset specifically curated for joint text and image generation. Extensive experiments show that Mogao not only achieves state-of-the-art performance in multi-modal understanding and text-to-image generation, but also excels in producing high-quality, coherent interleaved outputs. Its emergent capabilities in zero-shot image editing and compositional generation highlight Mogao as a practical omni-modal foundation model, paving the way for future development and scaling the unified multi-modal systems.","authors":["Chao Liao","Liyang Liu","Xun Wang","Zhengxiong Luo","Xinyu Zhang","Wenliang Zhao","Jie Wu","Liang Li","Zhi Tian","Weilin Huang"],"url":"https://arxiv.org/abs/2505.05472"}
{"created":"2025-05-13","title":"Prompt to Polyp: Medical Text-Conditioned Image Synthesis with Diffusion Models","abstract":"The generation of realistic medical images from text descriptions has significant potential to address data scarcity challenges in healthcare AI while preserving patient privacy. This paper presents a comprehensive study of text-to-image synthesis in the medical domain, comparing two distinct approaches: (1) fine-tuning large pre-trained latent diffusion models and (2) training small, domain-specific models. We introduce a novel model named MSDM, an optimized architecture based on Stable Diffusion that integrates a clinical text encoder, variational autoencoder, and cross-attention mechanisms to better align medical text prompts with generated images. Our study compares two approaches: fine-tuning large pre-trained models (FLUX, Kandinsky) versus training compact domain-specific models (MSDM). Evaluation across colonoscopy (MedVQA-GI) and radiology (ROCOv2) datasets reveals that while large models achieve higher fidelity, our optimized MSDM delivers comparable quality with lower computational costs. Quantitative metrics and qualitative evaluations by medical experts reveal strengths and limitations of each approach.","authors":["Mikhail Chaichuk","Sushant Gautam","Steven Hicks","Elena Tutubalina"],"url":"https://arxiv.org/abs/2505.05573"}
{"created":"2025-05-13","title":"Learning to Drive Anywhere with Model-Based Reannotation","abstract":"Developing broadly generalizable visual navigation policies for robots is a significant challenge, primarily constrained by the availability of large-scale, diverse training data. While curated datasets collected by researchers offer high quality, their limited size restricts policy generalization. To overcome this, we explore leveraging abundant, passively collected data sources, including large volumes of crowd-sourced teleoperation data and unlabeled YouTube videos, despite their potential for lower quality or missing action labels. We propose Model-Based ReAnnotation (MBRA), a framework that utilizes a learned short-horizon, model-based expert model to relabel or generate high-quality actions for these passive datasets. This relabeled data is then distilled into LogoNav, a long-horizon navigation policy conditioned on visual goals or GPS waypoints. We demonstrate that LogoNav, trained using MBRA-processed data, achieves state-of-the-art performance, enabling robust navigation over distances exceeding 300 meters in previously unseen indoor and outdoor environments. Our extensive real-world evaluations, conducted across a fleet of robots (including quadrupeds) in six cities on three continents, validate the policy's ability to generalize and navigate effectively even amidst pedestrians in crowded settings.","authors":["Noriaki Hirose","Lydia Ignatova","Kyle Stachowicz","Catherine Glossop","Sergey Levine","Dhruv Shah"],"url":"https://arxiv.org/abs/2505.05592"}
{"created":"2025-05-13","title":"FloE: On-the-Fly MoE Inference on Memory-constrained GPU","abstract":"With the widespread adoption of Mixture-of-Experts (MoE) models, there is a growing demand for efficient inference on memory-constrained devices. While offloading expert parameters to CPU memory and loading activated experts on demand has emerged as a potential solution, the large size of activated experts overburdens the limited PCIe bandwidth, hindering the effectiveness in latency-sensitive scenarios. To mitigate this, we propose FloE, an on-the-fly MoE inference system on memory-constrained GPUs. FloE is built on the insight that there exists substantial untapped redundancy within sparsely activated experts. It employs various compression techniques on the expert's internal parameter matrices to reduce the data movement load, combined with low-cost sparse prediction, achieving perceptible inference acceleration in wall-clock time on resource-constrained devices. Empirically, FloE achieves a 9.3x compression of parameters per expert in Mixtral-8x7B; enables deployment on a GPU with only 11GB VRAM, reducing the memory footprint by up to 8.5x; and delivers a 48.7x inference speedup compared to DeepSpeed-MII on a single GeForce RTX 3090 - all with only a 4.4$\\%$ - 7.6$\\%$ average performance degradation.","authors":["Yuxin Zhou","Zheng Li","Jun Zhang","Jue Wang","Yiping Wang","Zhongle Xie","Ke Chen","Lidan Shou"],"url":"https://arxiv.org/abs/2505.05950"}
{"created":"2025-05-13","title":"One-Point Feedback for Composite Optimization with Applications to Distributed and Federated Learning","abstract":"This work is devoted to solving the composite optimization problem with the mixture oracle: for the smooth part of the problem, we have access to the gradient, and for the non-smooth part, only the one-point zero-order oracle is available. For such a setup, we present a new method based on the sliding algorithm. Our method allows to separate the oracle complexities and to compute the gradient for one of the functions as rarely as possible. The paper also presents the applicability of our new method to the problems of distributed optimization and federated learning. Experimental results confirm the theory.","authors":["Aleksandr Beznosikov","Ivan Stepanov","Artyom Voronov","Alexander Gasnikov"],"url":"https://arxiv.org/abs/2107.05951"}
{"created":"2025-05-13","title":"Martinize2 and Vermouth: Unified Framework for Topology Generation","abstract":"Ongoing advances in force field and computer hardware development enable the use of molecular dynamics (MD) to simulate increasingly complex systems with the ultimate goal of reaching cellular complexity. At the same time, rational design by high-throughput (HT) simulations is another forefront of MD. In these areas, the Martini coarse-grained force field, especially the latest version (i.e. v3), is being actively explored because it offers an enhanced spatial-temporal resolution. However, the automation tools for preparing simulations with the Martini force field, accompanying the previous version, were not designed for HT simulations or studies of complex cellular systems. Therefore, they become a major limiting factor. To address these shortcomings, we present the open-source Vermouth python library. Vermouth is designed to become the unified framework for developing programs, which prepare, run, and analyze Martini simulations of complex systems. To demonstrate the power of the Vermouth library, the Martinize2 program is showcased as a generalization of the martinize script, originally aimed to set up simulations of proteins. In contrast to the previous version, Martinize2 automatically handles protonation states in proteins and post-translation modifications, offers more options to fine-tune structural biases such as the elastic network (EN), and can convert non-protein molecules such as ligands. Finally, Martinize2 is used in two high-complexity benchmarks. The entire I-TASSER protein template database as well as a subset of 200,000 structures from the AlphaFold Protein Structure Database are converted to CG resolution and we illustrate how the checks on input structure quality can safeguard high-throughput applications.","authors":["Peter C. Kroon","Fabian Gr\\\"unewald","Jonathan Barnoud","Marco van Tilburg","Chris Brasnett","Paulo C. T. Souza","Tsjerk A. Wassenaar","Siewert-Jan Marrink"],"url":"https://arxiv.org/abs/2212.01191"}
{"created":"2025-05-13","title":"New protocols for quantum key distribution with explicit upper and lower bound on secret key rate","abstract":"We present two new schemes for quantum key distribution (QKD) that neither require entanglement nor an ideal single-photon source, making them implementable with commercially available single-photon sources. These protocols are shown to be secure against multiple attacks, including intercept-resend and a class of collective attacks. We derive bounds on the key rate and demonstrate that a specific type of classical pre-processing can increase the tolerable error limit. A trade-off between quantum resources and information revealed to an eavesdropper (Eve) is observed, with higher efficiency achievable through the use of additional quantum resources. Specifically, our proposed protocols outperform the SARG04 protocol in terms of efficiency at the cost of more quantum resources.","authors":["Arindam Dutta","Anirban Pathak"],"url":"https://arxiv.org/abs/2212.13089"}
{"created":"2025-05-13","title":"Triangle processes on graphs with given degree sequence","abstract":"The switch chain is a well-studied Markov chain which generates random graphs with a given degree sequence and has uniform stationary distribution. Motivated by the high number of triangles seen in some real-world networks, we study a variant of the switch chain which is more likely to produce graphs with higher numbers of triangles. Specifically, we apply a Metropolis scheme designed to have the following stationary distribution: graph $G$ has probability proportional to $\\lambda^{\\min\\{t(G),\\nu\\}}$, where $t(G)$ is the number of triangles in $G$ and $\\nu$ is a cut-off value introduced to moderate the impact of graphs with a very high number of triangles. We assume that the ``activity'' $\\lambda$ satisfies $\\lambda\\geq 1$, and call the resulting chain the modified Metropolis switch chain. We prove that the modified Metropolis switch chain is rapidly mixing whenever the (standard) switch chain is rapidly mixing, provided that the activity and maximum degree are not too large.","authors":["Colin Cooper","Martin Dyer","Catherine Greenhill"],"url":"https://arxiv.org/abs/2301.08499"}
{"created":"2025-05-13","title":"Concrete Quantum Channels and Algebraic Structure of Abstract Quantum Channels","abstract":"This article analyzes the algebraic structure of the set of all quantum channels and its subset consisting of quantum channels that have Holevo representation. The regularity of these semigroups under composition of mappings is analyzed. It is also known that these sets are compact convex sets and, therefore, rich in geometry. An attempt is made to identify generalized invertible channels and also the idempotent channels. When channels are of the Holevo type, these two problems are fully studied in this article. The motivation behind this study is its applicability to the reversibility of channel transformations and recent developments in resource-destroying channels, which are idempotents. This is related to the coding-encoding problem in quantum information theory. Several examples are provided, with the main examples coming from pre-conditioner maps which assign preconditioners to matrices in numerical linear algebra. Thus, the known pre-conditioner maps are viewed as quantum channels in finite dimensions. In addition, the infinite-dimensional analogue of preconditioners is introduced and certain limit theorems are discussed; this is with an aim to analyze asymptotic methods in quantum channels analogous to problems in asymptotic linear algebra.","authors":["M. N. N. Namboodiri"],"url":"https://arxiv.org/abs/2305.11471"}
{"created":"2025-05-13","title":"Optimal Cross-Validation for Sparse Linear Regression","abstract":"Given a high-dimensional covariate matrix and a response vector, ridge-regularized sparse linear regression selects a subset of features that explains the relationship between covariates and the response in an interpretable manner. To select the sparsity and robustness of linear regressors, techniques like k-fold cross-validation are commonly used for hyperparameter tuning. However, cross-validation substantially increases the computational cost of sparse regression as it requires solving many mixed-integer optimization problems (MIOs) for each hyperparameter combination. To improve upon this state of affairs, we obtain computationally tractable relaxations of k-fold cross-validation metrics, facilitating hyperparameter selection after solving 50-80% fewer MIOs in practice. These relaxations result in an efficient cyclic coordinate descent scheme, achieving 10%-30% lower validation errors than via traditional methods such as grid search with MCP or GLMNet across a suite of 13 real-world datasets.","authors":["Ryan Cory-Wright","Andr\\'es G\\'omez"],"url":"https://arxiv.org/abs/2306.14851"}
{"created":"2025-05-13","title":"Fundamental Limits of Membership Inference Attacks on Machine Learning Models","abstract":"Membership inference attacks (MIA) can reveal whether a particular data point was part of the training dataset, potentially exposing sensitive information about individuals. This article provides theoretical guarantees by exploring the fundamental statistical limitations associated with MIAs on machine learning models at large. More precisely, we first derive the statistical quantity that governs the effectiveness and success of such attacks. We then theoretically prove that in a non-linear regression setting with overfitting learning procedures, attacks may have a high probability of success. Finally, we investigate several situations for which we provide bounds on this quantity of interest. Interestingly, our findings indicate that discretizing the data might enhance the learning procedure's security. Specifically, it is demonstrated to be limited by a constant, which quantifies the diversity of the underlying data distribution. We illustrate those results through simple simulations.","authors":["Eric Aubinais","Elisabeth Gassiat","Pablo Piantanida"],"url":"https://arxiv.org/abs/2310.13786"}
{"created":"2025-05-13","title":"Nonlinear functional regression by functional deep neural network with kernel embedding","abstract":"Recently, deep learning has been widely applied in functional data analysis (FDA) with notable empirical success. However, the infinite dimensionality of functional data necessitates an effective dimension reduction approach for functional learning tasks, particularly in nonlinear functional regression. In this paper, we introduce a functional deep neural network with an adaptive and discretization-invariant dimension reduction method. Our functional network architecture consists of three parts: first, a kernel embedding step that features an integral transformation with an adaptive smooth kernel; next, a projection step that utilizes eigenfunction bases based on a projection Mercer kernel for the dimension reduction; and finally, a deep ReLU neural network is employed for the prediction. Explicit rates of approximating nonlinear smooth functionals across various input function spaces by our proposed functional network are derived. Additionally, we conduct a generalization analysis for the empirical risk minimization (ERM) algorithm applied to our functional net, by employing a novel two-stage oracle inequality and the established functional approximation results. Ultimately, we conduct numerical experiments on both simulated and real datasets to demonstrate the effectiveness and benefits of our functional net.","authors":["Zhongjie Shi","Jun Fan","Linhao Song","Ding-Xuan Zhou","Johan A. K. Suykens"],"url":"https://arxiv.org/abs/2401.02890"}
{"created":"2025-05-13","title":"Privacy of SGD under Gaussian or Heavy-Tailed Noise: Guarantees without Gradient Clipping","abstract":"The injection of heavy-tailed noise into the iterates of stochastic gradient descent (SGD) has garnered growing interest in recent years due to its theoretical and empirical benefits for optimization and generalization. However, its implications for privacy preservation remain largely unexplored. Aiming to bridge this gap, we provide differential privacy (DP) guarantees for noisy SGD, when the injected noise follows an $\\alpha$-stable distribution, which includes a spectrum of heavy-tailed distributions (with infinite variance) as well as the light-tailed Gaussian distribution. Considering the $(\\epsilon, \\delta)$-DP framework, we show that SGD with heavy-tailed perturbations achieves $(0, O(1/n))$-DP for a broad class of loss functions which can be non-convex, where $n$ is the number of data points. As a remarkable byproduct, contrary to prior work that necessitates bounded sensitivity for the gradients or clipping the iterates, our theory can handle unbounded gradients without clipping, and reveals that under mild assumptions, such a projection step is not actually necessary. Our results suggest that, given other benefits of heavy-tails in optimization, heavy-tailed noising schemes can be a viable alternative to their light-tailed counterparts.","authors":["Umut \\c{S}im\\c{s}ekli","Mert G\\\"urb\\\"uzbalaban","Sinan Y{\\i}ld{\\i}r{\\i}m","Lingjiong Zhu"],"url":"https://arxiv.org/abs/2403.02051"}
{"created":"2025-05-13","title":"Stochastic Halpern iteration in normed spaces and applications to reinforcement learning","abstract":"We analyze the oracle complexity of the stochastic Halpern iteration with minibatch, where we aim to approximate fixed-points of nonexpansive and contractive operators in a normed finite-dimensional space. We show that if the underlying stochastic oracle has uniformly bounded variance, our method exhibits an overall oracle complexity of $\\tilde{O}(\\varepsilon^{-5})$, to obtain $\\varepsilon$ expected fixed-point residual for nonexpansive operators, improving recent rates established for the stochastic Krasnoselskii-Mann iteration. Also, we establish a lower bound of $\\Omega(\\varepsilon^{-3})$ which applies to a wide range of algorithms, including all averaged iterations even with minibatching. Using a suitable modification of our approach, we derive a $O(\\varepsilon^{-2}(1-\\gamma)^{-3})$ complexity bound in the case in which the operator is a $\\gamma$-contraction to obtain an approximation of the fixed-point. As an application, we propose new model-free algorithms for average and discounted reward MDPs. For the average reward case, our method applies to weakly communicating MDPs without requiring prior parameter knowledge.","authors":["Mario Bravo","Juan Pablo Contreras"],"url":"https://arxiv.org/abs/2403.12338"}
{"created":"2025-05-13","title":"A new framework for constrained optimization via feedback control of Lagrange multipliers","abstract":"The continuous-time analysis of existing iterative algorithms for optimization has a long history. This work proposes a novel continuous-time control-theoretic framework for equality-constrained optimization. The key idea is to design a feedback control system where the Lagrange multipliers are the control input, and the output represents the constraints. The system converges to a stationary point of the constrained optimization problem through suitable regulation. Regarding the Lagrange multipliers, we consider two control laws: proportional-integral control and feedback linearization. These choices give rise to a family of different methods. We rigorously develop the related algorithms, theoretically analyze their convergence and present several numerical experiments to support their effectiveness concerning the state-of-the-art approaches.","authors":["V. Cerone","S. M. Fosson","S. Pirrera","D. Regruto"],"url":"https://arxiv.org/abs/2403.12738"}
{"created":"2025-05-13","title":"Sparse Regression Codes for Non-coherent SIMO channels","abstract":"Motivated by hyper-reliable low-latency communication in 6G, we consider error control coding for short block lengths in multi-antenna fading channels. In general, the channel fading coefficients are unknown at both the transmitter and receiver, which is referred to as non-coherent channels. Conventionally, pilot symbols are transmitted to facilitate channel estimation, causing power and bandwidth overhead. Our paper considers sparse regression codes (SPARCs) for non-coherent flat-fading channels without using pilots. We develop a novel greedy decoder for SPARC using maximum likelihood principles, referred to as maximum likelihood matching pursuit (MLMP). MLMP works based on successive combining principles as opposed to conventional greedy algorithms, which are based on successive cancellation. We also obtain the noiseless perfect recovery condition for our successive combining algorithm. In addition, we develop an approximate message passing (AMP) SPARC decoder for the non-coherent flat fading model. Using simulation studies, we show that the MLMP decoder for SPARC outperforms AMP and other greedy decoders. Also, SPARC with MLMP decoder outperforms polar codes employing pilot-based channel estimation and polar codes with non-coherent decoders.","authors":["Sai Dinesh Kancharana","Madhusudan Kumar Sinha","Arun Pachai Kannu"],"url":"https://arxiv.org/abs/2405.09915"}
{"created":"2025-05-13","title":"On Kernel-based Variational Autoencoder","abstract":"In this paper, we bridge Variational Autoencoders (VAEs) and kernel density estimations (KDEs) by approximating the posterior by KDEs and deriving an upper bound of the Kullback-Leibler (KL) divergence in the evidence lower bound (ELBO). The flexibility of KDEs makes the optimization of posteriors in VAEs possible, which not only addresses the limitations of Gaussian latent space in vanilla VAE but also provides a new perspective of estimating the KL-divergence in ELBO. Under appropriate conditions, we show that the Epanechnikov kernel is the optimal choice in minimizing the derived upper bound of KL-divergence asymptotically. Compared with Gaussian kernel, Epanechnikov kernel has compact support which should make the generated sample less noisy and blurry. The implementation of Epanechnikov kernel in ELBO is straightforward as it lies in the \"location-scale\" family of distributions where the reparametrization tricks can be directly employed. A series of experiments on benchmark datasets such as MNIST, Fashion-MNIST, CIFAR-10 and CelebA further demonstrate the superiority of Epanechnikov Variational Autoenocoder (EVAE) over vanilla VAE in the quality of reconstructed images, as measured by the FID score and Sharpness.","authors":["Tian Qin","Wei-Min Huang"],"url":"https://arxiv.org/abs/2405.12783"}
{"created":"2025-05-13","title":"Planar cycle-extendable graphs","abstract":"For most problems pertaining to perfect matchings, one may restrict attention to matching covered graphs - that is, connected nontrivial graphs with the property that each edge belongs to some perfect matching. There is extensive literature on these graphs that are also known as 1-extendable graphs (since each edge extends to a perfect matching) including an ear decomposition theorem due to Lov\\'asz and Plummer.","authors":["Aditya Y Dalwadi","Kapil R Shenvi Pause","Ajit A Diwan","Nishad Kothari"],"url":"https://arxiv.org/abs/2405.15416"}
{"created":"2025-05-13","title":"Towards One Model for Classical Dimensionality Reduction: A Probabilistic Perspective on UMAP and t-SNE","abstract":"This paper shows that dimensionality reduction methods such as UMAP and t-SNE, can be approximately recast as MAP inference methods corresponding to a model introduced in Ravuri et al. (2023), that describes the graph Laplacian (an estimate of the data precision matrix) using a Wishart distribution, with a mean given by a non-linear covariance function evaluated on the latents. This interpretation offers deeper theoretical and semantic insights into such algorithms, and forging a connection to Gaussian process latent variable models by showing that well-known kernels can be used to describe covariances implied by graph Laplacians. We also introduce tools with which similar dimensionality reduction methods can be studied.","authors":["Aditya Ravuri","Neil D. Lawrence"],"url":"https://arxiv.org/abs/2405.17412"}
{"created":"2025-05-13","title":"Predicting solvation free energies with an implicit solvent machine learning potential","abstract":"Machine learning (ML) potentials are a powerful tool in molecular modeling, enabling ab initio accuracy for comparably small computational costs. Nevertheless, all-atom simulations employing best-performing graph neural network architectures are still too expensive for applications requiring extensive sampling, such as free energy computations. Implicit solvent models could provide the necessary speed-up due to reduced degrees of freedom and faster dynamics. Here, we introduce a Solvation Free Energy Path Reweighting (ReSolv) framework to parametrize an implicit solvent ML potential for small organic molecules that accurately predicts the hydration free energy, an essential parameter in drug design and pollutant modeling. With a combination of top-down (experimental hydration free energy data) and bottom-up (ab initio data of molecules in a vacuum) learning, ReSolv bypasses the need for intractable ab initio data of molecules in explicit bulk solvent and does not have to resort to less accurate data-generating models. On the FreeSolv dataset, ReSolv achieves a mean absolute error close to average experimental uncertainty, significantly outperforming standard explicit solvent force fields. Compared to the explicit solvent ML potential, ReSolv offers a computational speedup of four orders of magnitude and attains closer agreement with experiments. The presented framework paves the way toward deep molecular models that are more accurate yet computationally cheaper than classical atomistic models.","authors":["Sebastien R\\\"ocken","Anton F. Burnet","Julija Zavadlav"],"url":"https://arxiv.org/abs/2406.00183"}
{"created":"2025-05-13","title":"Demystifying SGD with Doubly Stochastic Gradients","abstract":"Optimization objectives in the form of a sum of intractable expectations are rising in importance (e.g., diffusion models, variational autoencoders, and many more), a setting also known as \"finite sum with infinite data.\" For these problems, a popular strategy is to employ SGD with doubly stochastic gradients (doubly SGD): the expectations are estimated using the gradient estimator of each component, while the sum is estimated by subsampling over these estimators. Despite its popularity, little is known about the convergence properties of doubly SGD, except under strong assumptions such as bounded variance. In this work, we establish the convergence of doubly SGD with independent minibatching and random reshuffling under general conditions, which encompasses dependent component gradient estimators. In particular, for dependent estimators, our analysis allows fined-grained analysis of the effect correlations. As a result, under a per-iteration computational budget of $b \\times m$, where $b$ is the minibatch size and $m$ is the number of Monte Carlo samples, our analysis suggests where one should invest most of the budget in general. Furthermore, we prove that random reshuffling (RR) improves the complexity dependence on the subsampling noise.","authors":["Kyurae Kim","Joohwan Ko","Yi-An Ma","Jacob R. Gardner"],"url":"https://arxiv.org/abs/2406.00920"}
{"created":"2025-05-13","title":"Causal Post-Processing of Predictive Models","abstract":"Decision makers across various domains rely on predictive models to guide individual-level intervention decisions. However, these models are typically trained to predict outcomes rather than causal effects, leading to misalignments when they are used for causal decision making. Experimental data to train effective causal effect models often is limited. To address this issue, we propose causal post-processing (CPP), a family of techniques for refining predictive scores to better align with causal effects using limited experimental data. Rather than training separate causal models for each intervention, causal post-processing can adapt existing predictive scores to support different decision-making requirements, such as estimating effect sizes, ranking individuals by expected effects, or classifying individuals based on an intervention threshold. We introduce three main CPP approaches -- monotonic post-processing, correction post-processing, and model-based post-processing -- each balancing statistical efficiency and flexibility differently. Through simulations and an empirical application in advertising, we demonstrate that causal post-processing improves intervention decisions, particularly in settings where experimental data is expensive or difficult to obtain at scale. Our findings highlight the advantages of integrating non-causal predictive models with experimental data, rather than treating them as competing alternatives, which provides a scalable and data-efficient approach to causal inference for decision making.","authors":["Carlos Fern\\'andez-Lor\\'ia","Yanfang Hou","Foster Provost","Jennifer Hill"],"url":"https://arxiv.org/abs/2406.09567"}
{"created":"2025-05-13","title":"A monolithic first--order BSSNOK formulation of the Einstein--Euler equations and its solution with path-conservative finite difference CWENO schemes","abstract":"We present a new, monolithic first--order (both in time and space) BSSNOK formulation of the coupled Einstein--Euler equations. The entire system of hyperbolic PDEs is solved in a completely unified manner via one single numerical scheme applied to both the conservative sector of the matter part and to the first--order strictly non--conservative sector of the spacetime evolution. The coupling between matter and space-time is achieved via algebraic source terms. The numerical scheme used for the solution of the new monolithic first order formulation is a path-conservative central WENO (CWENO) finite difference scheme, with suitable insertions to account for the presence of the non--conservative terms. By solving several crucial tests of numerical general relativity, including a stable neutron star, Riemann problems in relativistic matter with shock waves and the stable long-time evolution of single and binary puncture black holes up and beyond the binary merger, we show that our new CWENO scheme, introduced two decades ago for the compressible Euler equations of gas dynamics, can be successfully applied also to numerical general relativity, solving all equations at the same time with one single numerical method. In the future the new monolithic approach proposed in this paper may become an attractive alternative to traditional methods that couple central finite difference schemes with Kreiss-Oliger dissipation for the space-time part with totally different TVD schemes for the matter evolution and which are currently the state of the art in the field.","authors":["Michael Dumbser","Olindo Zanotti","Gabriella Puppo"],"url":"https://arxiv.org/abs/2406.12055"}
{"created":"2025-05-13","title":"Statistical Error Bounds for GANs with Nonlinear Objective Functionals","abstract":"Generative adversarial networks (GANs) are unsupervised learning methods for training a generator distribution to produce samples that approximate those drawn from a target distribution. Many such methods can be formulated as minimization of a metric or divergence between probability distributions. Recent works have derived statistical error bounds for GANs that are based on integral probability metrics (IPMs), e.g., WGAN which is based on the 1-Wasserstein metric. In general, IPMs are defined by optimizing a linear functional (difference of expectations) over a space of discriminators. A much larger class of GANs, which we here call $(f,\\Gamma)$-GANs, can be constructed using $f$-divergences (e.g., Jensen-Shannon, KL, or $\\alpha$-divergences) together with a regularizing discriminator space $\\Gamma$ (e.g., $1$-Lipschitz functions). These GANs have nonlinear objective functions, depending on the choice of $f$, and have been shown to exhibit improved performance in a number of applications. In this work we derive statistical error bounds for $(f,\\Gamma)$-GANs for general classes of $f$ and $\\Gamma$ in the form of finite-sample concentration inequalities. These results prove the statistical consistency of $(f,\\Gamma)$-GANs and reduce to the known results for IPM-GANs in the appropriate limit. Our results use novel Rademacher complexity bounds which provide new insight into the performance of IPM-GANs for distributions with unbounded support and have application to statistical learning tasks beyond GANs.","authors":["Jeremiah Birrell"],"url":"https://arxiv.org/abs/2406.16834"}
{"created":"2025-05-13","title":"Global decomposition of networks into multiple cores formed by local hubs","abstract":"Networks are ubiquitous in various fields, representing systems where nodes and their interconnections constitute their intricate structures. We introduce a network decomposition scheme to reveal multiscale core-periphery structures lurking inside, using the concept of locally defined nodal hub centrality and edge-pruning techniques built upon it. We demonstrate that the hub-centrality-based edge pruning reveals a series of breaking points in network decomposition, which effectively separates a network into its backbone and shell structures. Our local-edge decomposition method iteratively identifies and removes locally least connected nodes, and uncovers an onion-like hierarchical structure as a result. Compared with the conventional $k$-core decomposition method, our method based on relative information residing in local structures exhibits a clear advantage in terms of discovering locally crucial substructures. As an application of the method, we present a scheme to detect multiple core-periphery structures and the decomposition of coarse-grained supernode networks, by combining the method with the network community detection.","authors":["Wonhee Jeong","Unjong Yu","Sang Hoon Lee"],"url":"https://arxiv.org/abs/2407.00355"}
{"created":"2025-05-13","title":"Algorithmic aspects of semistability of quiver representations","abstract":"We study the semistability of quiver representations from an algorithmic perspective. We present efficient algorithms for several fundamental computational problems on the semistability of quiver representations: deciding the semistability and $\\sigma$-semistability, finding the maximizers of King's criterion, and computing the Harder--Narasimhan filtration. We also investigate a class of polyhedral cones defined by the linear system in King's criterion, which we refer to as King cones. For rank-one representations, we demonstrate that these King cones can be encoded by submodular flow polytopes, enabling us to decide the $\\sigma$-semistability in strongly polynomial time. Our approach employs submodularity in quiver representations, which may be of independent interest.","authors":["Yuni Iwamasa","Taihei Oki","Tasuku Soma"],"url":"https://arxiv.org/abs/2407.06493"}
{"created":"2025-05-13","title":"Deep Fr\\'echet Regression","abstract":"Advancements in modern science have led to the increasing availability of non-Euclidean data in metric spaces. This paper addresses the challenge of modeling relationships between non-Euclidean responses and multivariate Euclidean predictors. We propose a flexible regression model capable of handling high-dimensional predictors without imposing parametric assumptions. Two primary challenges are addressed: the curse of dimensionality in nonparametric regression and the absence of linear structure in general metric spaces. The former is tackled using deep neural networks, while for the latter we demonstrate the feasibility of mapping the metric space where responses reside to a low-dimensional Euclidean space using manifold learning. We introduce a reverse mapping approach, employing local Fr\\'echet regression, to map the low-dimensional manifold representations back to objects in the original metric space. We develop a theoretical framework, investigating the convergence rate of deep neural networks under dependent sub-Gaussian noise with bias. The convergence rate of the proposed regression model is then obtained by expanding the scope of local Fr\\'echet regression to accommodate multivariate predictors in the presence of errors in predictors. Simulations and case studies show that the proposed model outperforms existing methods for non-Euclidean responses, focusing on the special cases of probability distributions and networks.","authors":["Su I Iao","Yidong Zhou","Hans-Georg M\\\"uller"],"url":"https://arxiv.org/abs/2407.21407"}
{"created":"2025-05-13","title":"Neural timescales from a computational perspective","abstract":"Neural activity fluctuates over a wide range of timescales within and across brain areas. Experimental observations suggest that diverse neural timescales reflect information in dynamic environments. However, how timescales are defined and measured from brain recordings vary across the literature. Moreover, these observations do not specify the mechanisms underlying timescale variations, nor whether specific timescales are necessary for neural computation and brain function. Here, we synthesize three directions where computational approaches can distill the broad set of empirical observations into quantitative and testable theories: We review (i) how different data analysis methods quantify timescales across distinct behavioral states and recording modalities, (ii) how biophysical models provide mechanistic explanations for the emergence of diverse timescales, and (iii) how task-performing networks and machine learning models uncover the functional relevance of neural timescales. This integrative computational perspective thus complements experimental investigations, providing a holistic view on how neural timescales reflect the relationship between brain structure, dynamics, and behavior.","authors":["Roxana Zeraati","Anna Levina","Jakob H. Macke","Richard Gao"],"url":"https://arxiv.org/abs/2409.02684"}
{"created":"2025-05-13","title":"LSR-IGRU: Stock Trend Prediction Based on Long Short-Term Relationships and Improved GRU","abstract":"Stock price prediction is a challenging problem in the field of finance and receives widespread attention. In recent years, with the rapid development of technologies such as deep learning and graph neural networks, more research methods have begun to focus on exploring the interrelationships between stocks. However, existing methods mostly focus on the short-term dynamic relationships of stocks and directly integrating relationship information with temporal information. They often overlook the complex nonlinear dynamic characteristics and potential higher-order interaction relationships among stocks in the stock market. Therefore, we propose a stock price trend prediction model named LSR-IGRU in this paper, which is based on long short-term stock relationships and an improved GRU input. Firstly, we construct a long short-term relationship matrix between stocks, where secondary industry information is employed for the first time to capture long-term relationships of stocks, and overnight price information is utilized to establish short-term relationships. Next, we improve the inputs of the GRU model at each step, enabling the model to more effectively integrate temporal information and long short-term relationship information, thereby significantly improving the accuracy of predicting stock trend changes. Finally, through extensive experiments on multiple datasets from stock markets in China and the United States, we validate the superiority of the proposed LSR-IGRU model over the current state-of-the-art baseline models. We also apply the proposed model to the algorithmic trading system of a financial company, achieving significantly higher cumulative portfolio returns compared to other baseline methods. Our sources are released at https://github.com/ZP1481616577/Baselines_LSR-IGRU.","authors":["Peng Zhu","Yuante Li","Yifan Hu","Qinyuan Liu","Dawei Cheng","Yuqi Liang"],"url":"https://arxiv.org/abs/2409.08282"}
{"created":"2025-05-13","title":"Energetic Resilience of Linear Driftless Systems","abstract":"When a malfunction causes a control system to lose authority over a subset of its actuators, achieving a task may require spending additional energy in order to compensate for the effect of uncontrolled inputs. To understand this increase in energy, we introduce an energetic resilience metric that quantifies the maximal additional energy required to achieve finite-time regulation in linear driftless systems that suffer this malfunction. We first derive optimal control signals and minimum energies to achieve this task in both the nominal and malfunctioning systems. We then obtain a bound on the worst-case energy used by the malfunctioning system, and its exact expression in the special case of loss of authority over one actuator. Further considering this special case, we derive a bound on the metric for energetic resilience. A simulation example on a model of an underwater robot demonstrates that this bound is useful in quantifying the increased energy used by a system suffering such a malfunction.","authors":["Ram Padmanabhan","Melkior Ornik"],"url":"https://arxiv.org/abs/2410.00323"}
{"created":"2025-05-13","title":"Transformers Handle Endogeneity in In-Context Linear Regression","abstract":"We explore the capability of transformers to address endogeneity in in-context linear regression. Our main finding is that transformers inherently possess a mechanism to handle endogeneity effectively using instrumental variables (IV). First, we demonstrate that the transformer architecture can emulate a gradient-based bi-level optimization procedure that converges to the widely used two-stage least squares $(\\textsf{2SLS})$ solution at an exponential rate. Next, we propose an in-context pretraining scheme and provide theoretical guarantees showing that the global minimizer of the pre-training loss achieves a small excess loss. Our extensive experiments validate these theoretical findings, showing that the trained transformer provides more robust and reliable in-context predictions and coefficient estimates than the $\\textsf{2SLS}$ method, in the presence of endogeneity.","authors":["Haodong Liang","Krishnakumar Balasubramanian","Lifeng Lai"],"url":"https://arxiv.org/abs/2410.01265"}
{"created":"2025-05-13","title":"Lightweight Deep Learning Framework for Accurate Particle Flow Energy Reconstruction","abstract":"Under extreme operating conditions, characterized by high particle multiplicity and heavily overlapping shower energy deposits, classical particle flow algorithms encounter pronounced limitations in resolution, efficiency, and accuracy. To address this challenge, this paper proposes and systematically evaluates a deep learning reconstruction framework: For multichannel sparse features, we design a hybrid loss function combining weighted mean squared error with structural similarity index, effectively balancing pixel-level accuracy and structural fidelity. By integrating 3D convolutions, Squeeze-and-Excitation channel attention, and Offset self-attention modules into baseline convolutional neural networks, we enhance the model's capability to capture cross-modal spatiotemporal correlations and energy-displacement nonlinearities. Validated on custom-constructed simulation data and Pythia jet datasets, the framework's 90K-parameter lightweight variant approaches the performance of 5M-parameter baselines, while the 25M-parameter 3D model achieves state-of-the-art results in both interpolation and extrapolation tasks. Comprehensive experiments quantitatively evaluate component contributions and provide performance-parameter trade-off guidelines. All core code and data processing scripts are open-sourced on a GitHub repository to facilitate community reproducibility and extension.","authors":["Yu Wang (School of Automation and Electrical Engineering","University of Science and Technology Beijing)","Yangguang Zhang (School of Control and Computer Engineering","North China Electric Power University)","Shengxiang Lin (Faculty of Electronic and Information Engineering","Xi'an Jiaotong University)","Xingyi Zhang (School of Mechanical Engineering","Shanghai Jiao Tong University)","Han Zhang (College of Artificial Intelligence and Automation","Hohai University)"],"url":"https://arxiv.org/abs/2410.07250"}
{"created":"2025-05-13","title":"Discrete distributions are learnable from metastable samples","abstract":"Physically motivated stochastic dynamics are often used to sample from high-dimensional distributions. However such dynamics often get stuck in specific regions of their state space and mix very slowly to the desired stationary state. This causes such systems to approximately sample from a metastable distribution which is usually quite different from the desired, stationary distribution of the dynamic. We rigorously show that, in the case of multi-variable discrete distributions, the true model describing the stationary distribution can be recovered from samples produced from a metastable distribution under minimal assumptions about the system. This follows from a fundamental observation that the single-variable conditionals of metastable distributions that satisfy a strong metastability condition are on average close to those of the stationary distribution. This holds even when the metastable distribution differs considerably from the true model in terms of global metrics like Kullback-Leibler divergence or total variation distance. This property allows us to learn the true model using a conditional likelihood based estimator, even when the samples come from a metastable distribution concentrated in a small region of the state space. Explicit examples of such metastable states can be constructed from regions that effectively bottleneck the probability flow and cause poor mixing of the Markov chain. For specific cases of binary pairwise undirected graphical models (i.e. Ising models), we extend our results to further rigorously show that data coming from metastable states can be used to learn the parameters of the energy function and recover the structure of the model.","authors":["Abhijith Jayakumar","Andrey Y. Lokhov","Sidhant Misra","Marc Vuffray"],"url":"https://arxiv.org/abs/2410.13800"}
{"created":"2025-05-13","title":"Efficient Bilinear Attention-based Fusion for Medical Visual Question Answering","abstract":"Medical Visual Question Answering (MedVQA) has attracted growing interest at the intersection of medical image understanding and natural language processing for clinical applications. By interpreting medical images and providing precise answers to relevant clinical inquiries, MedVQA has the potential to support diagnostic decision-making and reduce workload across various fields like radiology. While recent approaches rely heavily on unified large pre-trained Visual-Language Models, research on more efficient fusion mechanisms remains relatively limited in this domain. In this paper, we introduce a fusion model, OMniBAN, that integrates Orthogonality loss, Multi-head attention, and a Bilinear Attention Network to achieve high computational efficiency as well as solid performance. We conduct comprehensive experiments and demonstrate how bilinear attention fusion can approximate the performance of larger fusion models like cross-modal Transformer. Our results show that OMniBAN requires fewer parameters (approximately 2/3 of Transformer-based Co-Attention) and substantially lower FLOPs (approximately 1/4), while achieving comparable overall performance and even slight improvements on closed-ended questions on two key MedVQA benchmarks. This balance between efficiency and accuracy suggests that OMniBAN could be a viable option for real-world medical image question answering, where computational resources are often constrained.","authors":["Zhilin Zhang","Jie Wang","Zhanghao Qin","Ruiqi Zhu","Xiaoliang Gong"],"url":"https://arxiv.org/abs/2410.21000"}
{"created":"2025-05-13","title":"High-Dimensional Gaussian Process Regression with Soft Kernel Interpolation","abstract":"We introduce Soft Kernel Interpolation (SoftKI), a method that combines aspects of Structured Kernel Interpolation (SKI) and variational inducing point methods, to achieve scalable Gaussian Process (GP) regression on high-dimensional datasets. SoftKI approximates a kernel via softmax interpolation from a smaller number of interpolation points learned by optimizing a combination of the SoftKI marginal log-likelihood (MLL), and when needed, an approximate MLL for improved numerical stability. Consequently, it can overcome the dimensionality scaling challenges that SKI faces when interpolating from a dense and static lattice while retaining the flexibility of variational methods to adapt inducing points to the dataset. We demonstrate the effectiveness of SoftKI across various examples and show that it is competitive with other approximated GP methods when the data dimensionality is modest (around 10).","authors":["Chris Cama\\~no","Daniel Huang"],"url":"https://arxiv.org/abs/2410.21419"}
{"created":"2025-05-13","title":"Relationships between the degrees of freedom in the affine Gaussian derivative model for visual receptive fields and 2-D affine image transformations, with application to covariance properties of simple cells in the primary visual cortex","abstract":"When observing the surface patterns of objects delimited by smooth surfaces, the projections of the surface patterns to the image domain will be subject to substantial variabilities, as induced by variabilities in the geometric viewing conditions, and as generated by either monocular or binocular imaging conditions, or by relative motions between the object and the observer over time. To first order of approximation, the image deformations of such projected surface patterns can be modelled as local linearizations in terms of local 2-D spatial affine transformations.","authors":["Tony Lindeberg"],"url":"https://arxiv.org/abs/2411.05673"}
{"created":"2025-05-13","title":"Tolerant Testing of Stabilizer States with Mixed State Inputs","abstract":"We study the problem of tolerant testing of stabilizer states. In particular, we give the first such algorithm that accepts mixed state inputs. Formally, given a mixed state $\\rho$ that either has fidelity at least $\\varepsilon_1$ with some stabilizer pure state or fidelity at most $\\varepsilon_2$ with all such states, where $\\varepsilon_2 \\leq \\varepsilon_1^{O(1)}$, our algorithm distinguishes the two cases with sample complexity $\\text{poly}(1/\\varepsilon_1)$ and time complexity $O(n \\cdot \\text{poly}(1/\\varepsilon_1))$.","authors":["Vishnu Iyer","Daniel Liang"],"url":"https://arxiv.org/abs/2411.08765"}
{"created":"2025-05-13","title":"SymbolFit: Automatic Parametric Modeling with Symbolic Regression","abstract":"We introduce SymbolFit, a framework that automates parametric modeling by using symbolic regression to perform a machine-search for functions that fit the data while simultaneously providing uncertainty estimates in a single run. Traditionally, constructing a parametric model to accurately describe binned data has been a manual and iterative process, requiring an adequate functional form to be determined before the fit can be performed. The main challenge arises when the appropriate functional forms cannot be derived from first principles, especially when there is no underlying true closed-form function for the distribution. In this work, we develop a framework that automates and streamlines the process by utilizing symbolic regression, a machine learning technique that explores a vast space of candidate functions without requiring a predefined functional form because the functional form itself is treated as a trainable parameter, making the process far more efficient and effortless than traditional regression methods. We demonstrate the framework in high-energy physics experiments at the CERN Large Hadron Collider (LHC) using five real proton-proton collision datasets from new physics searches, including background modeling in resonance searches for high-mass dijet, trijet, paired-dijet, diphoton, and dimuon events. We show that our framework can flexibly and efficiently generate a wide range of candidate functions that fit a nontrivial distribution well using a simple fit configuration that varies only by random seed, and that the same fit configuration, which defines a vast function space, can also be applied to distributions of different shapes, whereas achieving a comparable result with traditional methods would have required extensive manual effort.","authors":["Ho Fung Tsoi","Dylan Rankin","Cecile Caillol","Miles Cranmer","Sridhara Dasu","Javier Duarte","Philip Harris","Elliot Lipeles","Vladimir Loncar"],"url":"https://arxiv.org/abs/2411.09851"}
{"created":"2025-05-13","title":"Empower Structure-Based Molecule Optimization with Gradient Guidance","abstract":"Structure-Based molecule optimization (SBMO) aims to optimize molecules with both continuous coordinates and discrete types against protein targets. A promising direction is to exert gradient guidance on generative models given its remarkable success in images, but it is challenging to guide discrete data and risks inconsistencies between modalities. To this end, we leverage a continuous and differentiable space derived through Bayesian inference, presenting Molecule Joint Optimization (MolJO), the gradient-based SBMO framework that facilitates joint guidance signals across different modalities while preserving SE(3)-equivariance. We introduce a novel backward correction strategy that optimizes within a sliding window of the past histories, allowing for a seamless trade-off between explore-and-exploit during optimization. MolJO achieves state-of-the-art performance on CrossDocked2020 benchmark (Success Rate 51.3%, Vina Dock -9.05 and SA 0.78), more than 4x improvement in Success Rate compared to the gradient-based counterpart, and 2x \"Me-Better\" Ratio as much as 3D baselines. Furthermore, we extend MolJO to a wide range of optimization settings, including multi-objective optimization and challenging tasks in drug design such as R-group optimization and scaffold hopping, further underscoring its versatility.","authors":["Keyue Qiu","Yuxuan Song","Jie Yu","Hongbo Ma","Ziyao Cao","Zhilong Zhang","Yushuai Wu","Mingyue Zheng","Hao Zhou","Wei-Ying Ma"],"url":"https://arxiv.org/abs/2411.13280"}
{"created":"2025-05-13","title":"Data Integration with Fusion Searchlight: Classifying Brain States from Resting-state fMRI","abstract":"Resting-state fMRI captures spontaneous neural activity characterized by complex spatiotemporal dynamics. Various metrics, such as local and global brain connectivity and low-frequency amplitude fluctuations, quantify distinct aspects of these dynamics. However, these measures are typically analyzed independently, overlooking their interrelations and potentially limiting analytical sensitivity. Here, we introduce the Fusion Searchlight (FuSL) framework, which integrates complementary information from multiple resting-state fMRI metrics. We demonstrate that combining these metrics enhances the accuracy of pharmacological treatment prediction from rs-fMRI data, enabling the identification of additional brain regions affected by sedation with alprazolam. Furthermore, we leverage explainable AI to delineate the differential contributions of each metric, which additionally improves spatial specificity of the searchlight analysis. Moreover, this framework can be adapted to combine information across imaging modalities or experimental conditions, providing a versatile and interpretable tool for data fusion in neuroimaging.","authors":["Simon Wein","Marco Riebel","Lisa-Marie Brunner","Caroline Nothdurfter","Rainer Rupprecht","Jens V. Schwarzbach"],"url":"https://arxiv.org/abs/2412.10161"}
{"created":"2025-05-13","title":"Using matrix-product states for time-series machine learning","abstract":"Matrix-product states (MPS) have proven to be a versatile ansatz for modeling quantum many-body physics. For many applications, and particularly in one-dimension, they capture relevant quantum correlations in many-body wavefunctions while remaining tractable to store and manipulate on a classical computer. This has motivated researchers to also apply the MPS ansatz to machine learning (ML) problems where capturing complex correlations in datasets is also a key requirement. Here, we develop and apply an MPS-based algorithm, MPSTime, for learning a joint probability distribution underlying an observed time-series dataset, and show how it can be used to tackle important time-series ML problems, including classification and imputation. MPSTime can efficiently learn complicated time-series probability distributions directly from data, requires only moderate maximum MPS bond dimension $\\chi_{\\rm max}$, with values for our applications ranging between $\\chi_{\\rm max} = 20-160$, and can be trained for both classification and imputation tasks under a single logarithmic loss function. Using synthetic and publicly available real-world datasets, spanning applications in medicine, energy, and astronomy, we demonstrate performance competitive with state-of-the-art ML approaches, but with the key advantage of encoding the full joint probability distribution learned from the data, which is useful for analyzing and interpreting its underlying structure. This manuscript is supplemented with the release of a publicly available code package MPSTime that implements our approach. The effectiveness of the MPS-based ansatz for capturing complex correlation structures in time-series data makes it a powerful foundation for tackling challenging time-series analysis problems across science, industry, and medicine.","authors":["Joshua B. Moore","Hugo P. Stackhouse","Ben D. Fulcher","Sahand Mahmoodian"],"url":"https://arxiv.org/abs/2412.15826"}
{"created":"2025-05-13","title":"Gaussian entropic optimal transport: Schr\\\"odinger bridges and the Sinkhorn algorithm","abstract":"Entropic optimal transport problems are regularized versions of optimal transport problems. These models play an increasingly important role in machine learning and generative modelling. For finite spaces, these problems are commonly solved using Sinkhorn algorithm (a.k.a. iterative proportional fitting procedure). However, in more general settings the Sinkhorn iterations are based on nonlinear conditional/conjugate transformations and exact finite-dimensional solutions cannot be computed.","authors":["O. Deniz Akyildiz","Pierre Del Moral","Joaqu\\'in Miguez"],"url":"https://arxiv.org/abs/2412.18432"}
{"created":"2025-05-13","title":"Automotive Speed Estimation: Sensor Types and Error Characteristics from OBD-II to ADAS","abstract":"Modern on-road navigation systems heavily depend on integrating speed measurements with inertial navigation systems (INS) and global navigation satellite systems (GNSS). Telemetry-based applications typically source speed data from the On-Board Diagnostic II (OBD-II) system. However, the method of deriving speed, as well as the types of sensors used to measure wheel speed, differs across vehicles. These differences result in varying error characteristics that must be accounted for in navigation and autonomy applications. This paper addresses this gap by examining the diverse speed-sensing technologies employed in standard automotive systems and alternative techniques used in advanced systems designed for higher levels of autonomy, such as Advanced Driver Assistance Systems (ADAS), Autonomous Driving (AD), or surveying applications. We propose a method to identify the type of speed sensor in a vehicle and present strategies for accurately modeling its error characteristics. To validate our approach, we collected and analyzed data from three long real road trajectories conducted in urban environments in Toronto and Kingston, Ontario, Canada. The results underscore the critical role of integrating multiple sensor modalities to achieve more accurate speed estimation, thus improving automotive navigation state estimation, particularly in GNSS-denied environments.","authors":["Hany Ragab (Dept. of Electrical and Computer Engineering at Queens University and the NavINST Lab at the Royal Military College of Canada)","Sidney Givigi (School of Computing at Queens University)","Aboelmagd Noureldin (Dept. of Electrical and Computer Engineering at Queens University and the NavINST Lab at the Royal Military College of Canada","School of Computing at Queens University)"],"url":"https://arxiv.org/abs/2501.00242"}
{"created":"2025-05-13","title":"Multilingual Performance of a Multimodal Artificial Intelligence System on Multisubject Physics Concept Inventories","abstract":"We investigate the multilingual and multimodal performance of a large language model-based artificial intelligence (AI) system, GPT-4o, using a diverse set of physics concept inventories spanning multiple languages and subject categories. The inventories, sourced from the PhysPort website, cover classical physics topics such as mechanics, electromagnetism, optics, and thermodynamics, as well as relativity, quantum mechanics, astronomy, mathematics, and laboratory skills. Unlike previous text-only studies, we uploaded the inventories as images to reflect what a student would see on paper, thereby assessing the system's multimodal functionality. Our results indicate variation in performance across subjects, with laboratory skills standing out as the weakest. We also observe differences across languages, with English and European languages showing the strongest performance. Notably, the relative difficulty of an inventory item is largely independent of the language of the survey. When comparing AI results to existing literature on student performance, we find that the AI system outperforms average post-instruction undergraduate students in all subject categories except laboratory skills. Furthermore, the AI performs worse on items requiring visual interpretation of images than on those that are purely text-based. While our exploratory findings show GPT-4o's potential usefulness in physics education, they highlight the critical need for instructors to foster students' ability to critically evaluate AI outputs, adapt curricula thoughtfully in response to AI advancements, and address equity concerns associated with AI integration.","authors":["Gerd Kortemeyer","Marina Babayeva","Giulia Polverini","Ralf Widenhorn","Bor Gregorcic"],"url":"https://arxiv.org/abs/2501.06143"}
{"created":"2025-05-13","title":"Estimation-Aware Trajectory Optimization with Set-Valued Measurement Uncertainties","abstract":"In this paper, an optimization-based framework for generating estimation-aware trajectories is presented. In this setup, measurement (output) uncertainties are state-dependent and set-valued. Enveloping ellipsoids are employed to characterize state-dependent uncertainties with unknown distributions. The concept of regularity for set-valued output maps is then introduced, facilitating the formulation of the estimation-aware trajectory generation problem. Specifically, it is demonstrated that for output-regular maps, one can utilize a set-valued observability measure that is concave with respect to the finite horizon state trajectories. By maximizing this measure, estimation-aware trajectories can then be synthesized for a broad class of systems. Trajectory planning routines are also examined in this work, by which the observability measure is optimized for systems with locally linearized dynamics. To illustrate the effectiveness of the proposed approach, representative examples in the context of trajectory planning with vision-based estimation are presented. Moreover, the paper presents estimation-aware planning for an uncooperative Target-Rendezvous problem, where an Ego-satellite employs an onboard machine learning (ML)-based estimation module to realize the rendezvous trajectory.","authors":["Aditya Deole","Mehran Mesbahi"],"url":"https://arxiv.org/abs/2501.09192"}
{"created":"2025-05-13","title":"RadioLLM: Introducing Large Language Model into Cognitive Radio via Hybrid Prompt and Token Reprogrammings","abstract":"The growing scarcity of spectrum resources and rapid proliferation of wireless devices make efficient radio network management critical. While deep learning-enhanced Cognitive Radio Technology (CRT) provides promising solutions for tasks such as radio signal classification (RSC), denoising, and spectrum allocation, existing DL-based CRT frameworks are typically task-specific and lack scalability in diverse real-world applications. This limitation naturally leads to the exploration of Large Language Models (LLMs), whose exceptional cross-domain generalization capabilities offer new potential for advancing CRT. To bridge this gap, we propose RadioLLM, a novel framework that integrates Hybrid Prompt and Token Reprogramming (HPTR) for combining radio signal features with expert knowledge, and a Frequency-Attuned Fusion (FAF) module for enhanced high-frequency feature modeling. Extensive evaluations on multiple benchmark datasets demonstrate that RadioLLM achieves superior performance compared to existing baselines in the majority of testing scenarios.","authors":["Shuai Chen","Yong Zu","Zhixi Feng","Shuyuan Yang","Mengchang Li"],"url":"https://arxiv.org/abs/2501.17888"}
{"created":"2025-05-13","title":"String Diagrams for Graded Monoidal Theories with an Application to Imprecise Probability","abstract":"We introduce string diagrams for graded symmetric monoidal categories. Our approach includes a definition of graded monoidal theory and the corresponding freely generated syntactic category. Also, we show how an axiomatic presentation for the graded theory may be modularly obtained from one for the grading theory and one for the base category. The Para construction on monoidal actegories is a motivating example for our framework. As a case study, we show how to axiomatise a variant of the graded category ImP, recently introduced by Liell-Cock and Staton to model imprecise probability. This culminates in a representation, as string diagrams with grading wires, of programs with primitives for nondeterministic and probabilistic choices and conditioning.","authors":["Ralph Sarkis","Fabio Zanasi"],"url":"https://arxiv.org/abs/2501.18404"}
{"created":"2025-05-13","title":"Understanding Model Calibration -- A gentle introduction and visual exploration of calibration and the expected calibration error (ECE)","abstract":"To be considered reliable, a model must be calibrated so that its confidence in each decision closely reflects its true outcome. In this blogpost we'll take a look at the most commonly used definition for calibration and then dive into a frequently used evaluation measure for model calibration. We'll then cover some of the drawbacks of this measure and how these surfaced the need for additional notions of calibration, which require their own new evaluation measures. This post is not intended to be an in-depth dissection of all works on calibration, nor does it focus on how to calibrate models. Instead, it is meant to provide a gentle introduction to the different notions and their evaluation measures as well as to re-highlight some issues with a measure that is still widely used to evaluate calibration.","authors":["Maja Pavlovic"],"url":"https://arxiv.org/abs/2501.19047"}
{"created":"2025-05-13","title":"Learning to Fuse Temporal Proximity Networks: A Case Study in Chimpanzee Social Interactions","abstract":"How can we identify groups of primate individuals which could be conjectured to drive social structure? To address this question, one of us has collected a time series of data for social interactions between chimpanzees. Here we use a network representation, leading to the task of combining these data into a time series of a single weighted network per time stamp, where different proximities should be given different weights reflecting their relative importance. We optimize these proximity-type weights in a principled way, using an innovative loss function which rewards structural consistency across time. The approach is empirically validated by carefully designed synthetic data. Using statistical tests, we provide a way of identifying groups of individuals that stay related for a significant length of time. Applying the approach to the chimpanzee data set, we detect cliques in the animal social network time series, which can be validated by real-world intuition from prior research and qualitative observations by chimpanzee experts.","authors":["Yixuan He","Aaron Sandel","David Wipf","Mihai Cucuringu","John Mitani","Gesine Reinert"],"url":"https://arxiv.org/abs/2502.00302"}
{"created":"2025-05-13","title":"How the Stroop Effect Arises from Optimal Response Times in Laterally Connected Self-Organizing Maps","abstract":"The Stroop effect refers to cognitive interference in a color-naming task: When the color and the word do not match, the response is slower and more likely to be incorrect. The Stroop task is used to assess cognitive flexibility, selective attention, and executive function. This paper implements the Stroop task with self-organizing maps (SOMs): Target color and the competing word are inputs for the semantic and lexical maps, associative connections bring color information to the lexical map, and lateral connections combine their effects over time. The model achieved an overall accuracy of 84.2%, with significantly fewer errors and faster responses in congruent compared to no-input and incongruent conditions. The model's effect is a side effect of optimizing response times, and can thus be seen as a cost associated with overall efficient performance. The model can further serve studying neurologically-inspired cognitive control and related phenomena.","authors":["Divya Prabhakaran","Uli Grasemann","Swathi Kiran","Risto Miikkulainen"],"url":"https://arxiv.org/abs/2502.02831"}
{"created":"2025-05-13","title":"On the existence of pure epsilon-equilibrium","abstract":"We show that for any $\\epsilon>0$, as the number of agents gets large, the share of games that admit a pure $\\epsilon$-equilibrium converges to 1. Our result holds even for pure $\\epsilon$-equilibria in which all agents, except for at most one, play a best response. In contrast, it is known that the share of games that admit a pure Nash equilibrium, that is for $\\epsilon=0$, is asymptotically $1-1/e\\approx 0.63$. This suggests that very small deviations from perfect rationality, captured by positive values of $\\epsilon$, suffice to ensure the general existence of stable outcomes. We also study the existence of pure $\\epsilon$-equilibrium when the number of actions gets large. Our proofs rely on the probabilistic method and the Chen-Stein method.","authors":["Bary S. R. Pradelski","Bassel Tarbush"],"url":"https://arxiv.org/abs/2502.07585"}
{"created":"2025-05-13","title":"Inverse Covariance and Partial Correlation Matrix Estimation via Joint Partial Regression","abstract":"We present a method for estimating sparse high-dimensional inverse covariance and partial correlation matrices, which exploits the connection between the inverse covariance matrix and linear regression. The method is a two-stage estimation method wherein each individual feature is regressed on all other features while positive semi-definiteness is enforced simultaneously. We derive non-asymptotic estimation rates for both inverse covariance and partial correlation matrix estimation. An efficient proximal splitting algorithm for numerically computing the estimate is also dervied. The effectiveness of the proposed method is demonstrated on both synthetic and real-world data.","authors":["Samuel Erickson","Tobias Ryd\\'en"],"url":"https://arxiv.org/abs/2502.08414"}
{"created":"2025-05-13","title":"Differentiable Folding for Nearest Neighbor Model Optimization","abstract":"The Nearest Neighbor model is the $\\textit{de facto}$ thermodynamic model of RNA secondary structure formation and is a cornerstone of RNA structure prediction and sequence design. The current functional form (Turner 2004) contains $\\approx13,000$ underlying thermodynamic parameters, and fitting these to both experimental and structural data is computationally challenging. Here, we leverage recent advances in $\\textit{differentiable folding}$, a method for directly computing gradients of the RNA folding algorithms, to devise an efficient, scalable, and flexible means of parameter optimization that uses known RNA structures and thermodynamic experiments. Our method yields a significantly improved parameter set that outperforms existing baselines on all metrics, including an increase in the average predicted probability of ground-truth sequence-structure pairs for a single RNA family by over 23 orders of magnitude. Our framework provides a path towards drastically improved RNA models, enabling the flexible incorporation of new experimental data, definition of novel loss terms, large training sets, and even treatment as a module in larger deep learning pipelines. We make available a new database, RNAometer, with experimentally-determined stabilities for small RNA model systems.","authors":["Ryan K. Krueger","Sharon Aviran","David H. Mathews","Jeffrey Zuber","Max Ward"],"url":"https://arxiv.org/abs/2503.09085"}
{"created":"2025-05-13","title":"Constraint-based causal discovery with tiered background knowledge and latent variables in single or overlapping datasets","abstract":"In this paper we consider the use of tiered background knowledge within constraint based causal discovery. Our focus is on settings relaxing causal sufficiency, i.e. allowing for latent variables which may arise because relevant information could not be measured at all, or not jointly, as in the case of multiple overlapping datasets. We first present novel insights into the properties of the 'tiered FCI' (tFCI) algorithm. Building on this, we introduce a new extension of the IOD (integrating overlapping datasets) algorithm incorporating tiered background knowledge, the 'tiered IOD' (tIOD) algorithm. We show that under full usage of the tiered background knowledge tFCI and tIOD are sound, while simple versions of the tIOD and tFCI are sound and complete. We further show that the tIOD algorithm can often be expected to be considerably more efficient and informative than the IOD algorithm even beyond the obvious restriction of the Markov equivalence classes. We provide a formal result on the conditions for this gain in efficiency and informativeness. Our results are accompanied by a series of examples illustrating the exact role and usefulness of tiered background knowledge.","authors":["Christine W. Bang","Vanessa Didelez"],"url":"https://arxiv.org/abs/2503.21526"}
{"created":"2025-05-13","title":"PHOENIX: Pauli-Based High-Level Optimization Engine for Instruction Execution on NISQ Devices","abstract":"Variational quantum algorithms (VQA) based on Hamiltonian simulation represent a specialized class of quantum programs well-suited for near-term quantum computing applications due to its modest resource requirements in terms of qubits and circuit depth. Unlike the conventional single-qubit (1Q) and two-qubit (2Q) gate sequence representation, Hamiltonian simulation programs are essentially composed of disciplined subroutines known as Pauli exponentiations (Pauli strings with coefficients) that are variably arranged. To capitalize on these distinct program features, this study introduces PHOENIX, a highly effective compilation framework that primarily operates at the high-level Pauli-based intermediate representation (IR) for generic Hamiltonian simulation programs. PHOENIX exploits global program optimization opportunities to the greatest extent, compared to existing SOTA methods despite some of them also utilizing similar IRs. PHOENIX employs the binary symplectic form (BSF) to formally describe Pauli strings and reformulates IR synthesis as reducing the column weights of BSF by appropriate Clifford transformations. It comes with a heuristic BSF simplification algorithm that searches for the most appropriate 2Q Clifford operators in sequence to maximally simplify the BSF at each step, until the BSF can be directly synthesized by basic 1Q and 2Q gates. PHOENIX further performs a global ordering strategy in a Tetris-like fashion for these simplified IR groups, carefully balancing optimization opportunities for gate cancellation, minimizing circuit depth, and managing qubit routing overhead. Experimental results demonstrate that PHOENIX outperforms SOTA VQA compilers across diverse program categories, backend ISAs, and hardware topologies.","authors":["Zhaohui Yang","Dawei Ding","Chenghong Zhu","Jianxin Chen","Yuan Xie"],"url":"https://arxiv.org/abs/2504.03529"}
{"created":"2025-05-13","title":"Leveraging State Space Models in Long Range Genomics","abstract":"Long-range dependencies are critical for understanding genomic structure and function, yet most conventional methods struggle with them. Widely adopted transformer-based models, while excelling at short-context tasks, are limited by the attention module's quadratic computational complexity and inability to extrapolate to sequences longer than those seen in training. In this work, we explore State Space Models (SSMs) as a promising alternative by benchmarking two SSM-inspired architectures, Caduceus and Hawk, on long-range genomics modeling tasks under conditions parallel to a 50M parameter transformer baseline. We discover that SSMs match transformer performance and exhibit impressive zero-shot extrapolation across multiple tasks, handling contexts 10 to 100 times longer than those seen during training, indicating more generalizable representations better suited for modeling the long and complex human genome. Moreover, we demonstrate that these models can efficiently process sequences of 1M tokens on a single GPU, allowing for modeling entire genomic regions at once, even in labs with limited compute. Our findings establish SSMs as efficient and scalable for long-context genomic analysis.","authors":["Matvei Popov","Aymen Kallala","Anirudha Ramesh","Narimane Hennouni","Shivesh Khaitan","Rick Gentry","Alain-Sam Cohen"],"url":"https://arxiv.org/abs/2504.06304"}
{"created":"2025-05-13","title":"OmniAudio: Generating Spatial Audio from 360-Degree Video","abstract":"Traditional video-to-audio generation techniques primarily focus on field-of-view (FoV) video and non-spatial audio, often missing the spatial cues necessary for accurately representing sound sources in 3D environments. To address this limitation, we introduce a novel task, 360V2SA, to generate spatial audio from 360-degree videos, specifically producing First-order Ambisonics (FOA) audio - a standard format for representing 3D spatial audio that captures sound directionality and enables realistic 3D audio reproduction. We first create Sphere360, a novel dataset tailored for this task that is curated from real-world data. We also design an efficient semi-automated pipeline for collecting and cleaning paired video-audio data. To generate spatial audio from 360-degree video, we propose a novel framework OmniAudio, which leverages self-supervised pre-training using both spatial audio data (in FOA format) and large-scale non-spatial data. Furthermore, OmniAudio features a dual-branch framework that utilizes both panoramic and FoV video inputs to capture comprehensive local and global information from 360-degree videos. Experimental results demonstrate that OmniAudio achieves state-of-the-art performance across both objective and subjective metrics on Sphere360. Code and datasets will be released at https://github.com/liuhuadai/OmniAudio. The demo page is available at https://OmniAudio-360V2SA.github.io.","authors":["Huadai Liu","Tianyi Luo","Qikai Jiang","Kaicheng Luo","Peiwen Sun","Jialei Wan","Rongjie Huang","Qian Chen","Wen Wang","Xiangtai Li","Shiliang Zhang","Zhijie Yan","Zhou Zhao","Wei Xue"],"url":"https://arxiv.org/abs/2504.14906"}
{"created":"2025-05-13","title":"Leveraging Modified Ex Situ Tomography Data for Segmentation of In Situ Synchrotron X-Ray Computed Tomography","abstract":"In situ synchrotron X-ray computed tomography enables dynamic material studies, but automated segmentation remains challenging due to complex imaging artefacts and limited training data. We present a methodology for deep learning-based segmentation by transforming high-quality ex situ laboratory data to train models for binary segmentation of in situ synchrotron data, demonstrated through copper oxide dissolution studies. Using a modified SegFormer architecture, our approach achieves high segmentation performance on unseen data while reducing processing time from hours to seconds per 3D dataset. The method maintains consistent performance over significant morphological changes during experiments, despite training only on static specimens. This methodology can be readily applied to diverse materials systems, accelerating the analysis of time-resolved tomographic data across scientific disciplines.","authors":["Tristan Manchester","Adam Anders","Julio Spadotto","Hannah Eccleston","William Beavan","Hugues Arcis","Brian J. Connolly"],"url":"https://arxiv.org/abs/2504.19200"}
{"created":"2025-05-13","title":"Neuronal correlations shape the scaling behavior of memory capacity and nonlinear computational capability of recurrent neural networks","abstract":"Reservoir computing is a powerful framework for real-time information processing, characterized by its high computational ability and quick learning, with applications ranging from machine learning to biological systems. In this paper, we demonstrate that the memory capacity of a reservoir recurrent neural network scales sublinearly with the number of readout neurons. To elucidate this phenomenon, we develop a theoretical framework for analytically deriving memory capacity, attributing the decaying growth of memory capacity to neuronal correlations. In addition, numerical simulations reveal that once memory capacity becomes sublinear, increasing the number of readout neurons successively enables nonlinear processing at progressively higher polynomial orders. Furthermore, our theoretical framework suggests that neuronal correlations govern not only memory capacity but also the sequential growth of nonlinear computational capabilities. Our findings establish a foundation for designing scalable and cost-effective reservoir computing, providing novel insights into the interplay among neuronal correlations, linear memory, and nonlinear processing.","authors":["Shotaro Takasu","Toshio Aoyagi"],"url":"https://arxiv.org/abs/2504.19657"}
{"created":"2025-05-13","title":"Learning Hierarchical Interaction for Accurate Molecular Property Prediction","abstract":"Discovering molecules with desirable molecular properties, including ADMET profiles, is of great importance in drug discovery. Existing approaches typically employ deep learning models, such as Graph Neural Networks (GNNs) and Transformers, to predict these molecular properties by learning from diverse chemical information. However, these models often fail to efficiently capture and utilize the hierarchical nature of molecular structures, and often lack mechanisms for effective interaction among multi-level features. To address these limitations, we propose a Hierarchical Interaction Message Passing Mechanism, which serves as the foundation of our novel model, the Hierarchical Interaction Message Net (HimNet). Our method enables interaction-aware representation learning across atomic, motif, and molecular levels via hierarchical attention-guided message passing. This design allows HimNet to effectively balance global and local information, ensuring rich and task-relevant feature extraction for downstream property prediction tasks, such as Blood-Brain Barrier Permeability (BBBP). We systematically evaluate HimNet on eleven datasets, including eight widely-used MoleculeNet benchmarks and three challenging, high-value datasets for metabolic stability, malaria activity, and liver microsomal clearance, covering a broad range of pharmacologically relevant properties. Extensive experiments demonstrate that HimNet achieves the best or near-best performance in most molecular property prediction tasks. Furthermore, our method exhibits promising hierarchical interpretability, aligning well with chemical intuition on representative molecules. We believe that HimNet offers an accurate and efficient solution for molecular activity and ADMET property prediction, contributing to advanced decision-making in the early stages of drug discovery.","authors":["Huiyang Hong","Xinkai Wu","Hongyu Sun","Chaoyang Xie","Qi Wang","Yuquan Li"],"url":"https://arxiv.org/abs/2504.20127"}
{"created":"2025-05-13","title":"ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports","abstract":"We present ReXGradient-160K, representing the largest publicly available chest X-ray dataset to date in terms of the number of patients. This dataset contains 160,000 chest X-ray studies with paired radiological reports from 109,487 unique patients across 3 U.S. health systems (79 medical sites). This comprehensive dataset includes multiple images per study and detailed radiology reports, making it particularly valuable for the development and evaluation of AI systems for medical imaging and automated report generation models. The dataset is divided into training (140,000 studies), validation (10,000 studies), and public test (10,000 studies) sets, with an additional private test set (10,000 studies) reserved for model evaluation on the ReXrank benchmark. By providing this extensive dataset, we aim to accelerate research in medical imaging AI and advance the state-of-the-art in automated radiological analysis. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.","authors":["Xiaoman Zhang","Juli\\'an N. Acosta","Josh Miller","Ouwen Huang","Pranav Rajpurkar"],"url":"https://arxiv.org/abs/2505.00228"}
{"created":"2025-05-13","title":"Accelerated Decentralized Constraint-Coupled Optimization: A Dual$^2$ Approach","abstract":"In this paper, we focus on a class of decentralized constraint-coupled optimization problem: $\\min_{x_i \\in \\mathbb{R}^{d_i}, i \\in \\mathcal{I}; y \\in \\mathbb{R}^p}$ $\\sum_{i=1}^n\\left(f_i(x_i) + g_i(x_i)\\right) + h(y) \\ \\text{s.t.} \\ \\sum_{i=1}^{n}A_ix_i = y$, over an undirected and connected network of $n$ agents. Here, $f_i$, $g_i$, and $A_i$ represent private information of agent $i \\in \\mathcal{I} = \\{1, \\cdots, n\\}$, while $h$ is public for all agents. Building on a novel dual$^2$ approach, we develop two accelerated algorithms to solve this problem: the inexact Dual$^2$ Accelerated (iD2A) gradient method and the Multi-consensus inexact Dual$^2$ Accelerated (MiD2A) gradient method. We demonstrate that both iD2A and MiD2A can guarantee asymptotic convergence under a milder condition on $h$ compared to existing algorithms. Furthermore, under additional assumptions, we establish linear convergence rates and derive significantly lower communication and computational complexity bounds than those of existing algorithms. Several numerical experiments validate our theoretical analysis and demonstrate the practical superiority of the proposed algorithms.","authors":["Jingwang Li","Vincent Lau"],"url":"https://arxiv.org/abs/2505.03719"}
{"created":"2025-05-13","title":"From Spaceborne to Airborne: SAR Image Synthesis Using Foundation Models for Multi-Scale Adaptation","abstract":"The availability of Synthetic Aperture Radar (SAR) satellite imagery has increased considerably in recent years, with datasets commercially available. However, the acquisition of high-resolution SAR images in airborne configurations, remains costly and limited. Thus, the lack of open source, well-labeled, or easily exploitable SAR text-image datasets is a barrier to the use of existing foundation models in remote sensing applications. In this context, synthetic image generation is a promising solution to augment this scarce data, enabling a broader range of applications. Leveraging over 15 years of ONERA's extensive archival airborn data from acquisition campaigns, we created a comprehensive training dataset of 110 thousands SAR images to exploit a 3.5 billion parameters pre-trained latent diffusion model \\cite{Baqu2019SethiR}. In this work, we present a novel approach utilizing spatial conditioning techniques within a foundation model to transform satellite SAR imagery into airborne SAR representations. Additionally, we demonstrate that our pipeline is effective for bridging the realism of simulated images generated by ONERA's physics-based simulator EMPRISE \\cite{empriseem_ai_images}. Our method explores a key application of AI in advancing SAR imaging technology. To the best of our knowledge, we are the first to introduce this approach in the literature.","authors":["Solene Debuysere","Nicolas Trouve","Nathan Letheule","Olivier Leveque","Elise Colin"],"url":"https://arxiv.org/abs/2505.03844"}
{"created":"2025-05-13","title":"The stability of generalized phase retrieval problem over compact groups","abstract":"The generalized phase retrieval problem over compact groups aims to recover a set of matrices, representing an unknown signal, from their associated Gram matrices, leveraging prior structural knowledge about the signal. This framework generalizes the classical phase retrieval problem, which reconstructs a signal from the magnitudes of its Fourier transform, to a richer setting involving non-abelian compact groups. In this broader context, the unknown phases in Fourier space are replaced by unknown orthogonal matrices that arise from the action of a compact group on a finite-dimensional vector space. This problem is primarily motivated by advances in electron microscopy to determining the 3D structure of biological macromolecules from highly noisy observations. To capture realistic assumptions from machine learning and signal processing, we model the signal as belonging to one of several broad structural families: a generic linear subspace, a sparse representation in a generic basis, the output of a generic ReLU neural network, or a generic low-dimensional manifold. Our main result shows that, under mild conditions, the generalized phase retrieval problem not only admits a unique solution (up to inherent group symmetries), but also satisfies a bi-Lipschitz property. This implies robustness to both noise and model mismatch, an essential requirement for practical use, especially when measurements are severely corrupted by noise. These findings provide theoretical support for a wide class of scientific problems under modern structural assumptions, and they offer strong foundations for developing robust algorithms in high-noise regimes.","authors":["Tal Amir","Tamir Bendory","Nadav Dym","Dan Edidin"],"url":"https://arxiv.org/abs/2505.04190"}
{"created":"2025-05-13","title":"Quantum-Inspired Optimization Process for Data Imputation","abstract":"Data imputation is a critical step in data pre-processing, particularly for datasets with missing or unreliable values. This study introduces a novel quantum-inspired imputation framework evaluated on the UCI Diabetes dataset, which contains biologically implausible missing values across several clinical features. The method integrates Principal Component Analysis (PCA) with quantum-assisted rotations, optimized through gradient-free classical optimizers -COBYLA, Simulated Annealing, and Differential Evolution to reconstruct missing values while preserving statistical fidelity. Reconstructed values are constrained within +/-2 standard deviations of original feature distributions, avoiding unrealistic clustering around central tendencies. This approach achieves a substantial and statistically significant improvement, including an average reduction of over 85% in Wasserstein distance and Kolmogorov-Smirnov test p-values between 0.18 and 0.22, compared to p-values > 0.99 in classical methods such as Mean, KNN, and MICE. The method also eliminates zero-value artifacts and enhances the realism and variability of imputed data. By combining quantum-inspired transformations with a scalable classical framework, this methodology provides a robust solution for imputation tasks in domains such as healthcare and AI pipelines, where data quality and integrity are crucial.","authors":["Nishikanta Mohanty","Bikash K. Behera","Badshah Mukherjee","Christopher Ferrie"],"url":"https://arxiv.org/abs/2505.04841"}
