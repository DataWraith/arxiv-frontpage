{"created":"2025-04-11","title":"Assessment of FAIR (Findability, Accessibility, Interoperability, and Reusability) data implementation frameworks: a parametric approach","abstract":"Open science movement has established reproducibility, transparency, and validation of research outputs as essential norms for conducting scientific research. It advocates for open access to research outputs, especially research data, to enable verification of published findings and its optimum reuse. The FAIR (Findable, Accessible, Interoperable, and Reusable) data principles support the philosophy of open science and have emerged as a foundational framework for making digital assets machine-actionable and enhancing their reusability and value in various domains, particularly in scientific research and data management. In response to the growing demand for making data FAIR, various FAIR implementation frameworks have been developed by various organizations to educate and make the scientific community more aware of FAIR and its principles and to make the adoption and implementation of FAIR easier. This paper provides a comprehensive review of the openly available FAIR implementation frameworks based on a parametric evaluation of these frameworks. The current work identifies 13 frameworks and compares them against their coverage of the four foundational principles of FAIR, including an assessment of these frameworks against 36 parameters related to technical specifications, basic features, and FAIR implementation features and FAIR coverage. The study identifies that most of the frameworks only offer a step-by-step guide to FAIR implementation and seem to be adopting the technology-first approach, mostly guiding the deployment of various tools for FAIR implementation. Many frameworks are missing the critical aspects of explaining what, why, and how for the four foundational principles of FAIR, giving less consideration to the social aspects of FAIR. The study concludes that more such frameworks should be developed, considering the people-first approach rather than the technology-first.","authors":["Ranjeet Kumar Singh","Akanksha Nagpal","Arun Jadhav","Devika P. Madalli"],"url":"https://arxiv.org/abs/2504.06268"}
{"created":"2025-04-11","title":"EXCLAIM: An Explainable Cross-Modal Agentic System for Misinformation Detection with Hierarchical Retrieval","abstract":"Misinformation continues to pose a significant challenge in today's information ecosystem, profoundly shaping public perception and behavior. Among its various manifestations, Out-of-Context (OOC) misinformation is particularly obscure, as it distorts meaning by pairing authentic images with misleading textual narratives. Existing methods for detecting OOC misinformation predominantly rely on coarse-grained similarity metrics between image-text pairs, which often fail to capture subtle inconsistencies or provide meaningful explainability. While multi-modal large language models (MLLMs) demonstrate remarkable capabilities in visual reasoning and explanation generation, they have not yet demonstrated the capacity to address complex, fine-grained, and cross-modal distinctions necessary for robust OOC detection. To overcome these limitations, we introduce EXCLAIM, a retrieval-based framework designed to leverage external knowledge through multi-granularity index of multi-modal events and entities. Our approach integrates multi-granularity contextual analysis with a multi-agent reasoning architecture to systematically evaluate the consistency and integrity of multi-modal news content. Comprehensive experiments validate the effectiveness and resilience of EXCLAIM, demonstrating its ability to detect OOC misinformation with 4.3% higher accuracy compared to state-of-the-art approaches, while offering explainable and actionable insights.","authors":["Yin Wu","Zhengxuan Zhang","Fuling Wang","Yuyu Luo","Hui Xiong","Nan Tang"],"url":"https://arxiv.org/abs/2504.06269"}
{"created":"2025-04-11","title":"Addressing Cold-start Problem in Click-Through Rate Prediction via Supervised Diffusion Modeling","abstract":"Predicting Click-Through Rates is a crucial function within recommendation and advertising platforms, as the output of CTR prediction determines the order of items shown to users. The Embedding \\& MLP paradigm has become a standard approach for industrial recommendation systems and has been widely deployed. However, this paradigm suffers from cold-start problems, where there is either no or only limited user action data available, leading to poorly learned ID embeddings. The cold-start problem hampers the performance of new items. To address this problem, we designed a novel diffusion model to generate a warmed-up embedding for new items. Specifically, we define a novel diffusion process between the ID embedding space and the side information space. In addition, we can derive a sub-sequence from the diffusion steps to expedite training, given that our diffusion model is non-Markovian. Our diffusion model is supervised by both the variational inference and binary cross-entropy objectives, enabling it to generate warmed-up embeddings for items in both the cold-start and warm-up phases. Additionally, we have conducted extensive experiments on three recommendation datasets. The results confirmed the effectiveness of our approach.","authors":["Wenqiao Zhu","Lulu Wang","Jun Wu"],"url":"https://arxiv.org/abs/2504.06270"}
{"created":"2025-04-11","title":"ER-RAG: Enhance RAG with ER-Based Unified Modeling of Heterogeneous Data Sources","abstract":"Large language models (LLMs) excel in question-answering (QA) tasks, and retrieval-augmented generation (RAG) enhances their precision by incorporating external evidence from diverse sources like web pages, databases, and knowledge graphs. However, current RAG methods rely on agent-specific strategies for individual data sources, posing challenges low-resource or black-box environments and complicates operations when evidence is fragmented across sources. To address these limitations, we propose ER-RAG, a framework that unifies evidence integration across heterogeneous data sources using the Entity-Relationship (ER) model. ER-RAG standardizes entity retrieval and relationship querying through ER-based APIs with GET and JOIN operations. It employs a two-stage generation process: first, a preference optimization module selects optimal sources; second, another module constructs API chains based on source schemas. This unified approach allows efficient fine-tuning and seamless integration across diverse data sources. ER-RAG demonstrated its effectiveness by winning all three tracks of the 2024 KDDCup CRAG Challenge, achieving performance on par with commercial RAG pipelines using an 8B LLM backbone. It outperformed hybrid competitors by 3.1% in LLM score and accelerated retrieval by 5.5X.","authors":["Yikuan Xia","Jiazun Chen","Yirui Zhan","Suifeng Zhao","Weipeng Jiang","Chaorui Zhang","Wei Han","Bo Bai","Jun Gao"],"url":"https://arxiv.org/abs/2504.06271"}
{"created":"2025-04-11","title":"RAVEN: An Agentic Framework for Multimodal Entity Discovery from Large-Scale Video Collections","abstract":"We present RAVEN an adaptive AI agent framework designed for multimodal entity discovery and retrieval in large-scale video collections. Synthesizing information across visual, audio, and textual modalities, RAVEN autonomously processes video data to produce structured, actionable representations for downstream tasks. Key contributions include (1) a category understanding step to infer video themes and general-purpose entities, (2) a schema generation mechanism that dynamically defines domain-specific entities and attributes, and (3) a rich entity extraction process that leverages semantic retrieval and schema-guided prompting. RAVEN is designed to be model-agnostic, allowing the integration of different vision-language models (VLMs) and large language models (LLMs) based on application-specific requirements. This flexibility supports diverse applications in personalized search, content discovery, and scalable information retrieval, enabling practical applications across vast datasets.","authors":["Kevin Dela Rosa"],"url":"https://arxiv.org/abs/2504.06272"}
{"created":"2025-04-11","title":"A Diverse and Effective Retrieval-Based Debt Collection System with Expert Knowledge","abstract":"Designing effective debt collection systems is crucial for improving operational efficiency and reducing costs in the financial industry. However, the challenges of maintaining script diversity, contextual relevance, and coherence make this task particularly difficult. This paper presents a debt collection system based on real debtor-collector data from a major commercial bank. We construct a script library from real-world debt collection conversations, and propose a two-stage retrieval based response system for contextual relevance. Experimental results show that our system improves script diversity, enhances response relevance, and achieves practical deployment efficiency through knowledge distillation. This work offers a scalable and automated solution, providing valuable insights for advancing debt collection practices in real-world applications.","authors":["Jiaming Luo","Weiyi Luo","Guoqing Sun","Mengchen Zhu","Haifeng Tang","Kunyao Lan","Mengyue Wu","Kenny Q. Zhu"],"url":"https://arxiv.org/abs/2504.06273"}
{"created":"2025-04-11","title":"Joint Group Profiling and Recommendation via Deep Neural Network-based Multi-Task Learning","abstract":"Group recommender systems aim to generate recommendations that align with the collective preferences of a group, introducing challenges that differ significantly from those in individual recommendation scenarios. This paper presents Joint Group Profiling and Recommendation via Deep Neural Network-based Multi-Task Learning, a framework that unifies group profiling and recommendation tasks within a single model. By jointly learning these tasks, the model develops a deeper understanding of group dynamics, leading to improved recommendation accuracy. The shared representations between the two tasks facilitate the discovery of latent features essential to both, resulting in richer and more informative group embeddings. To further enhance performance, an attention mechanism is integrated to dynamically evaluate the relevance of different group features and item attributes, ensuring the model prioritizes the most impactful information. Experiments and evaluations on real-world datasets demonstrate that our multi-task learning approach consistently outperforms baseline models in terms of accuracy, validating its effectiveness and robustness.","authors":["Ngoc Luyen Le","Marie-H\\'el\\`ene Abel"],"url":"https://arxiv.org/abs/2504.06274"}
{"created":"2025-04-11","title":"A Cascaded Architecture for Extractive Summarization of Multimedia Content via Audio-to-Text Alignment","abstract":"This study presents a cascaded architecture for extractive summarization of multimedia content via audio-to-text alignment. The proposed framework addresses the challenge of extracting key insights from multimedia sources like YouTube videos. It integrates audio-to-text conversion using Microsoft Azure Speech with advanced extractive summarization models, including Whisper, Pegasus, and Facebook BART XSum. The system employs tools such as Pytube, Pydub, and SpeechRecognition for content retrieval, audio extraction, and transcription. Linguistic analysis is enhanced through named entity recognition and semantic role labeling. Evaluation using ROUGE and F1 scores demonstrates that the cascaded architecture outperforms conventional summarization methods, despite challenges like transcription errors. Future improvements may include model fine-tuning and real-time processing. This study contributes to multimedia summarization by improving information retrieval, accessibility, and user experience.","authors":["Tanzir Hossain","Ar-Rafi Islam","Md. Sabbir Hossain","Annajiat Alim Rasel"],"url":"https://arxiv.org/abs/2504.06275"}
{"created":"2025-04-11","title":"Can we repurpose multiple-choice question-answering models to rerank retrieved documents?","abstract":"Yes, repurposing multiple-choice question-answering (MCQA) models for document reranking is both feasible and valuable. This preliminary work is founded on mathematical parallels between MCQA decision-making and cross-encoder semantic relevance assessments, leading to the development of R*, a proof-of-concept model that harmonizes these approaches. Designed to assess document relevance with depth and precision, R* showcases how MCQA's principles can improve reranking in information retrieval (IR) and retrieval-augmented generation (RAG) systems -- ultimately enhancing search and dialogue in AI-powered systems. Through experimental validation, R* proves to improve retrieval accuracy and contribute to the field's advancement by demonstrating a practical prototype of MCQA for reranking by keeping it lightweight.","authors":["Jasper Kyle Catapang"],"url":"https://arxiv.org/abs/2504.06276"}
{"created":"2025-04-11","title":"Dynamic Evaluation Framework for Personalized and Trustworthy Agents: A Multi-Session Approach to Preference Adaptability","abstract":"Recent advancements in generative AI have significantly increased interest in personalized agents. With increased personalization, there is also a greater need for being able to trust decision-making and action taking capabilities of these agents. However, the evaluation methods for these agents remain outdated and inadequate, often failing to capture the dynamic and evolving nature of user interactions. In this conceptual article, we argue for a paradigm shift in evaluating personalized and adaptive agents. We propose a comprehensive novel framework that models user personas with unique attributes and preferences. In this framework, agents interact with these simulated users through structured interviews to gather their preferences and offer customized recommendations. These recommendations are then assessed dynamically using simulations driven by Large Language Models (LLMs), enabling an adaptive and iterative evaluation process. Our flexible framework is designed to support a variety of agents and applications, ensuring a comprehensive and versatile evaluation of recommendation strategies that focus on proactive, personalized, and trustworthy aspects.","authors":["Chirag Shah","Hideo Joho","Kirandeep Kaur","Preetam Prabhu Srikar Dammu"],"url":"https://arxiv.org/abs/2504.06277"}
{"created":"2025-04-11","title":"Different Paths, Same Destination: Designing New Physics-Inspired Dynamical Systems with Engineered Stability to Minimize the Ising Hamiltonian","abstract":"Oscillator Ising machines (OIMs) represent an exemplar case of using physics-inspired non-linear dynamical systems to solve computationally challenging combinatorial optimization problems (COPs). The computational performance of such systems is highly sensitive to the underlying dynamical properties, the topology of the input graph, and their relative compatibility. In this work, we explore the concept of designing different dynamical systems that minimize the same objective function but exhibit drastically different dynamical properties. Our goal is to leverage this diversification in dynamics to reduce the sensitivity of the computational performance to the underlying graph, and subsequently, enhance the overall effectiveness of such physics-based computational methods. To this end, we introduce a novel dynamical system, the Dynamical Ising Machine (DIM), which, like the OIM, minimizes the Ising Hamiltonian but offers significantly different dynamical properties. We analyze the characteristic properties of the DIM and compare them with those of the OIM. We also show that the relative performance of each model is dependent on the input graph. Our work illustrates that using multiple dynamical systems with varying properties to solve the same COP enables an effective method that is less sensitive to the input graph, while producing robust solutions.","authors":["E. M. H. E. B. Ekanayake","N. Shukla"],"url":"https://arxiv.org/abs/2504.06280"}
{"created":"2025-04-11","title":"Reducing Formal Context Extraction: A Newly Proposed Framework from Big Corpora","abstract":"Automating the extraction of concept hierarchies from free text is advantageous because manual generation is frequently labor- and resource-intensive. Free result, the whole procedure for concept hierarchy learning from free text entails several phases, including sentence-level text processing, sentence splitting, and tokenization. Lemmatization is after formal context analysis (FCA) to derive the pairings. Nevertheless, there could be a few uninteresting and incorrect pairings in the formal context. It may take a while to generate formal context; thus, size reduction formal context is necessary to weed out irrelevant and incorrect pairings to extract the concept lattice and hierarchies more quickly. This study aims to propose a framework for reducing formal context in extracting concept hierarchies from free text to reduce the ambiguity of the formal context. We achieve this by reducing the size of the formal context using a hybrid of a WordNet-based method and a frequency-based technique. Using 385 samples from the Wikipedia corpus and the suggested framework, tests are carried out to examine the reduced size of formal context, leading to concept lattice and concept hierarchy. With the help of concept lattice-invariants, the generated formal context lattice is compared to the normal one. In contrast to basic ones, the homomorphic between the resultant lattices retains up to 98% of the quality of the generating concept hierarchies, and the reduced concept lattice receives the structural connection of the standard one. Additionally, the new framework is compared to five baseline techniques to calculate the running time on random datasets with various densities. The findings demonstrate that, in various fill ratios, hybrid approaches of the proposed method outperform other indicated competing strategies in concept lattice performance.","authors":["Bryar A. Hassan","Shko M. Qader","Alla A. Hassan","Joan Lu","Aram M. Ahmed","Jafar Majidpour","Tarik A. Rashid"],"url":"https://arxiv.org/abs/2504.06285"}
{"created":"2025-04-11","title":"Temporal-contextual Event Learning for Pedestrian Crossing Intent Prediction","abstract":"Ensuring the safety of vulnerable road users through accurate prediction of pedestrian crossing intention (PCI) plays a crucial role in the context of autonomous and assisted driving. Analyzing the set of observation video frames in ego-view has been widely used in most PCI prediction methods to forecast the cross intent. However, they struggle to capture the critical events related to pedestrian behaviour along the temporal dimension due to the high redundancy of the video frames, which results in the sub-optimal performance of PCI prediction. Our research addresses the challenge by introducing a novel approach called \\underline{T}emporal-\\underline{c}ontextual Event \\underline{L}earning (TCL). The TCL is composed of the Temporal Merging Module (TMM), which aims to manage the redundancy by clustering the observed video frames into multiple key temporal events. Then, the Contextual Attention Block (CAB) is employed to adaptively aggregate multiple event features along with visual and non-visual data. By synthesizing the temporal feature extraction and contextual attention on the key information across the critical events, TCL can learn expressive representation for the PCI prediction. Extensive experiments are carried out on three widely adopted datasets, including PIE, JAAD-beh, and JAAD-all. The results show that TCL substantially surpasses the state-of-the-art methods. Our code can be accessed at https://github.com/dadaguailhb/TCL.","authors":["Hongbin Liang","Hezhe Qiao","Wei Huang","Qizhou Wang","Mingsheng Shang","Lin Chen"],"url":"https://arxiv.org/abs/2504.06292"}
{"created":"2025-04-11","title":"Resurrecting Socrates in the Age of AI: A Study Protocol for Evaluating a Socratic Tutor to Support Research Question Development in Higher Education","abstract":"Formulating research questions is a foundational yet challenging academic skill, one that generative AI systems often oversimplify by offering instant answers at the expense of student reflection. This protocol lays out a study grounded in constructivist learning theory to evaluate a novel AI-based Socratic Tutor, designed to foster cognitive engagement and scaffold research question development in higher education. Anchored in dialogic pedagogy, the tutor engages students through iterative, reflective questioning, aiming to promote System 2 thinking and counteract overreliance on AI-generated outputs. In a quasi-experimental design, approximately 80 German pre-service biology teacher students will be randomly assigned to one of two groups: an AI Socratic Tutor condition and an uninstructed chatbot control. Across multiple cycles, students are expected to formulate research questions based on background texts, with quality assessed through double-blind expert review. The study also examines transfer of skills to novel phenomena and captures student perceptions through mixed-methods analysis, including surveys, interviews and reflective journals. This study aims to advance the understanding of how generative AI can be pedagogically aligned to support, not replace, human cognition and offers design principles for human-AI collaboration in education.","authors":["Ben Degen"],"url":"https://arxiv.org/abs/2504.06294"}
{"created":"2025-04-11","title":"Bottom-Up Generation of Verilog Designs for Testing EDA Tools","abstract":"Testing Electronic Design Automation (EDA) tools rely on benchmarks -- designs written in Hardware Description Languages (HDLs) such as Verilog, SystemVerilog, or VHDL. Although collections of benchmarks for these languages exist, they are typically limited in size. This scarcity has recently drawn more attention due to the increasing need for training large language models in this domain. To deal with such limitation, this paper presents a methodology and a corresponding tool for generating realistic Verilog designs. The tool, ChiGen, was originally developed to test the Jasper\\textregistered\\ Formal Verification Platform, a product by Cadence Design Systems. Now, released as open-source software, ChiGen has been able to identify zero-day bugs in a range of tools, including Verible, Verilator, and Yosys. This paper outlines the principles behind ChiGen's design, focusing on three aspects of it: (i) generation guided by probabilistic grammars, (ii) type inference via the Hindley-Milner algorithm, and (iii) code injection enabled by data-flow analysis. Once deployed on standard hardware, ChiGen outperforms existing Verilog Fuzzers such as Verismith, TransFuzz, and VlogHammer regarding structural diversity, code coverage, and bug-finding ability.","authors":["Jo\\~ao Victor Amorim Vieira","Luiza de Melo Gomes","Rafael Sumitani","Raissa Maciel","Augusto Mafra","Mirlaine Crepalde","Fernando Magno Quint\\~ao Pereira"],"url":"https://arxiv.org/abs/2504.06295"}
{"created":"2025-04-11","title":"Assessing Computer Science Student Attitudes Towards AI Ethics and Policy","abstract":"As artificial intelligence (AI) grows in popularity and importance-both as a domain within broader computing research and in society at large-increasing focus will need to be paid to the ethical governance of this emerging technology. The attitudes and competencies with respect to AI ethics and policy among post-secondary students studying computer science (CS) are of particular interest, as many of these students will go on to play key roles in the development and deployment of future AI innovations. Despite this population of computer scientists being at the forefront of learning about and using AI tools, their attitudes towards AI remain understudied in the literature. In an effort to begin to close this gap, in fall 2024 we fielded a survey ($n=117$) to undergraduate and graduate students enrolled in CS courses at a large public university in the United States to assess their attitudes towards the nascent fields of AI ethics and policy. Additionally, we conducted one-on-one follow-up interviews with 13 students to elicit more in-depth responses on topics such as the use of AI tools in the classroom, ethical impacts of AI, and government regulation of AI. In this paper, we describe the findings of both the survey and interviews, drawing parallels and contrasts to broader public opinion polling in the United States. We conclude by evaluating the implications of CS student attitudes on the future of AI education and governance.","authors":["James Weichert","Dayoung Kim","Qin Zhu","Junghwan Kim","Hoda Eldardiry"],"url":"https://arxiv.org/abs/2504.06296"}
{"created":"2025-04-11","title":"Ternarization of Vision Language Models for use on edge devices","abstract":"We propose a process to compress a pre-trained Vision Language Model into a ternary version of itself instead of training a ternary model from scratch. A new initialization scheme from pre-trained weights based on the k-means algorithm is proposed to reduce the ternarization time. We implement different custom operators for executing the ternary model on the TensorFlow Lite Engine. We compare the original model with its ternary and binary versions in terms of memory consumption, inference speed and perplexity. We find that the ternary model using our custom ternary matrix multiplication operator provides a good compromise in term of memory usage and perplexity, while having the fastest token generation speed.","authors":["Ben Crulis","Cyril De Runz","Barthelemy Serres","Gilles Venturini"],"url":"https://arxiv.org/abs/2504.06298"}
{"created":"2025-04-11","title":"On the Effectiveness and Generalization of Race Representations for Debiasing High-Stakes Decisions","abstract":"Understanding and mitigating biases is critical for the adoption of large language models (LLMs) in high-stakes decision-making. We introduce Admissions and Hiring, decision tasks with hypothetical applicant profiles where a person's race can be inferred from their name, as simplified test beds for racial bias. We show that Gemma 2B Instruct and LLaMA 3.2 3B Instruct exhibit strong biases. Gemma grants admission to 26% more White than Black applicants, and LLaMA hires 60% more Asian than White applicants. We demonstrate that these biases are resistant to prompt engineering: multiple prompting strategies all fail to promote fairness. In contrast, using distributed alignment search, we can identify \"race subspaces\" within model activations and intervene on them to debias model decisions. Averaging the representation across all races within the subspaces reduces Gemma's bias by 37-57%. Finally, we examine the generalizability of Gemma's race subspaces, and find limited evidence for generalization, where changing the prompt format can affect the race representation. Our work suggests mechanistic approaches may provide a promising venue for improving the fairness of LLMs, but a universal race representation remains elusive.","authors":["Dang Nguyen","Chenhao Tan"],"url":"https://arxiv.org/abs/2504.06303"}
{"created":"2025-04-11","title":"Well2Flow: Reconstruction of reservoir states from sparse wells using score-based generative models","abstract":"This study investigates the use of score-based generative models for reservoir simulation, with a focus on reconstructing spatially varying permeability and saturation fields in saline aquifers, inferred from sparse observations at two well locations. By modeling the joint distribution of permeability and saturation derived from high-fidelity reservoir simulations, the proposed neural network is trained to learn the complex spatiotemporal dynamics governing multiphase fluid flow in porous media. During inference, the framework effectively reconstructs both permeability and saturation fields by conditioning on sparse vertical profiles extracted from well log data. This approach introduces a novel methodology for incorporating physical constraints and well log guidance into generative models, significantly enhancing the accuracy and physical plausibility of the reconstructed subsurface states. Furthermore, the framework demonstrates strong generalization capabilities across varying geological scenarios, highlighting its potential for practical deployment in data-scarce reservoir management tasks.","authors":["Shiqin Zeng","Haoyun Li","Abhinav Prakash Gahlot","Felix J. Herrmann"],"url":"https://arxiv.org/abs/2504.06305"}
{"created":"2025-04-11","title":"Optimizing Large Language Models: Metrics, Energy Efficiency, and Case Study Insights","abstract":"The rapid adoption of large language models (LLMs) has led to significant energy consumption and carbon emissions, posing a critical challenge to the sustainability of generative AI technologies. This paper explores the integration of energy-efficient optimization techniques in the deployment of LLMs to address these environmental concerns. We present a case study and framework that demonstrate how strategic quantization and local inference techniques can substantially lower the carbon footprints of LLMs without compromising their operational effectiveness. Experimental results reveal that these methods can reduce energy consumption and carbon emissions by up to 45\\% post quantization, making them particularly suitable for resource-constrained environments. The findings provide actionable insights for achieving sustainability in AI while maintaining high levels of accuracy and responsiveness.","authors":["Tahniat Khan","Soroor Motie","Sedef Akinli Kocak","Shaina Raza"],"url":"https://arxiv.org/abs/2504.06307"}
{"created":"2025-04-11","title":"Rethinking RoPE: A Mathematical Blueprint for N-dimensional Positional Encoding","abstract":"Rotary Position Embedding (RoPE) is widely adopted in Transformers due to its ability to encode relative positions with high efficiency and extrapolation capability. However, existing RoPE variants lack a unified theoretical foundation, especially in higher dimensions. In this paper, we propose a systematic mathematical framework for RoPE grounded in Lie group and Lie algebra theory. We identify two core properties of RoPE, named relativity and reversibility, and derive general constraints and constructions for valid RoPE in 1D, 2D, and N-dimensional (ND). We prove that RoPE must lie in the basis of a maximal abelian subalgebra (MASA) of the special orthogonal Lie algebra, and show that standard RoPE corresponds to the maximal toral subalgebra. Furthermore, we propose to model inter-dimensional interactions by learning an orthogonal basis transformation. Our framework unifies and explains existing RoPE designs, while enabling principled extensions to new modalities and tasks.","authors":["Haiping Liu","Hongpeng Zhou"],"url":"https://arxiv.org/abs/2504.06308"}
{"created":"2025-04-11","title":"Conformal Slit Mapping Based Spiral Tool Trajectory Planning for Ball-end Milling on Complex Freeform Surfaces","abstract":"This study presents a spiral-based complete coverage strategy for ball-end milling on freeform surfaces, utilizing conformal slit mapping to generate milling trajectories that are more compact, smoother, and evenly distributed when machining 2D cavities with islands. This approach, an upgrade from traditional methods, extends the original algorithm to effectively address 3D perforated surface milling. Unlike conventional algorithms, the method embeds a continuous spiral trajectory within perforated surfaces without requiring cellular decomposition or additional boundaries. The proposed method addresses three primary challenges, including modifying conformal slit mapping for mesh surfaces, maintaining uniform scallop height between adjacent spiral trajectories, and optimizing the mapped origin point to ensure uniform scallop height distribution. To overcome these challenges, surface flattening techniques are incorporated into the original approach to accommodate mesh surfaces effectively. Tool path spacing is then optimized using a binary search strategy to regulate scallop height. A functional energy metric associated with scallop height uniformity is introduced for rapid evaluation of points mapped to the origin, with the minimum functional energy determined through perturbation techniques. The optimal placement of this point is identified using a modified gradient descent approach applied to the energy function. Validation on intricate surfaces, including low-quality and high-genus meshes, verifies the robustness of the algorithm. Surface milling experiments comparing this method with conventional techniques indicate a 15.63% improvement in scallop height uniformity while reducing machining time, average spindle impact, and spindle impact variance by up to 7.36%, 27.79%, and 55.98%, respectively.","authors":["Changqing Shen","BingZhou Xu","Xiaojian Zhang","Sijie Yan","Han Ding"],"url":"https://arxiv.org/abs/2504.06310"}
{"created":"2025-04-11","title":"BIFROST: Reinventing WiFi Signals Based on Dispersion Effect for Accurate Indoor Localization","abstract":"WiFi-based device localization is a key enabling technology for smart applications, which has attracted numerous research studies in the past decade. Most of the existing approaches rely on Line-of-Sight (LoS) signals to work, while a critical problem is often neglected: In the real-world indoor environments, WiFi signals are everywhere, but very few of them are usable for accurate localization. As a result, the localization accuracy in practice is far from being satisfactory. This paper presents BIFROST, a novel hardware-software co-design for accurate indoor localization. The core idea of BIFROST is to reinvent WiFi signals, so as to provide sufficient LoS signals for localization. This is realized by exploiting the dispersion effect of signals emitted by the leaky wave antenna (LWA). We present a low-cost plug-in design of LWA that can generate orthogonal polarized signals: On one hand, LWA disperses signals of different frequencies to different angles, thus providing Angle-of-Arrival (AoA) information for the localized target. On the other hand, the target further leverages the antenna polarization mismatch to distinguish AoAs from different LWAs. In the software layer, fine-grained information in Channel State Information (CSI) is exploited to cope with multipath and noise. We implement BIFROST and evaluate its performance under various settings. The results show that the median localization error of BIFROST is 0.81m, which is 52.35% less than that of SpotFi, a state-of-the-art approach. SpotFi, when combined with BIFROST to work in the realistic settings, can reduce the localization error by 33.54%.","authors":["Yimiao Sun","Yuan He","Jiacheng Zhang","Xin Na","Yande Chen","Weiguo Wang","Xiuzhen Guo"],"url":"https://arxiv.org/abs/2504.06311"}
{"created":"2025-04-11","title":"DMol: A Schedule-Driven Diffusion Model for Highly Efficient and Versatile Molecule Generation","abstract":"We introduce a new graph diffusion model for small molecule generation, \\emph{DMol}, which outperforms the state-of-the-art DiGress model in terms of validity by roughly $1.5\\%$ across all benchmarking datasets while reducing the number of diffusion steps by at least $10$-fold, and the running time to roughly one half. The performance improvements are a result of a careful change in the objective function and a ``graph noise\" scheduling approach which, at each diffusion step, allows one to only change a subset of nodes of varying size in the molecule graph. Another relevant property of the method is that it can be easily combined with junction-tree-like graph representations that arise by compressing a collection of relevant ring structures into supernodes. Unlike classical junction-tree techniques that involve VAEs and require complicated reconstruction steps, compressed DMol directly performs graph diffusion on a graph that compresses only a carefully selected set of frequent carbon rings into supernodes, which results in straightforward sample generation. This compressed DMol method offers additional validity improvements over generic DMol of roughly $2\\%$, increases the novelty of the method, and further improves the running time due to reductions in the graph size.","authors":["Peizhi Niu","Yu-Hsiang Wang","Vishal Rana","Chetan Rupakheti","Abhishek Pandey","Olgica Milenkovic"],"url":"https://arxiv.org/abs/2504.06312"}
{"created":"2025-04-11","title":"Analyzing How Text-to-Image Models Represent Nationalities in Everyday Tasks","abstract":"The primary objective of this paper is to investigate how a popular Text-to-Image (T2I) model represents people from 208 different nationalities when prompted to generate images of individuals performing typical everyday tasks. Two scenarios were developed, and images were generated based on input prompts that specified nationalities. The results show that in one scenario, the majority of images, and in the other, a substantial portion, depict individuals wearing traditional attire. This suggests that the model emphasizes such characteristics even when they are impractical for the given task. A statistically significant relationship was observed between this representation pattern and the regions associated with the specified countries. This indicates that the issue disproportionately affects certain areas, particularly the Middle East & North Africa and Sub-Saharan Africa. A notable association with income groups was also found. CLIP was used to measure alignment scores between generated images and various prompts and captions. The findings indicate statistically significant higher scores for images featuring individuals in traditional attire in one scenario. The study also examined revised prompts (additional contextual information automatically added to the original input prompts) to assess their potential influence on how individuals are represented in the generated images, finding that the word \"traditional\" was commonly added to revised prompts. These findings provide valuable insights into how T2I models represent individuals from various countries and highlight potential areas for improvement in future models.","authors":["Abdulkareem Alsudais"],"url":"https://arxiv.org/abs/2504.06313"}
{"created":"2025-04-11","title":"Beyond authorship: Analyzing contributions in PLOS ONE and the challenges of appropriate attribution","abstract":"Purpose This study aims to evaluate the accuracy of authorship attributions in scientific publications, focusing on the fairness and precision of individual contributions within academic works. Design/methodology/approach The study analyzes 81,823 publications from the journal PLOS ONE , covering the period from January 2018 to June 2023. It examines the authorship attributions within these publications to try and determine the prevalence of inappropriate authorship. It also investigates the demographic and professional profiles of affected authors, exploring trends and potential factors contributing to inaccuracies in authorship. Findings Surprisingly, 9.14% of articles feature at least one author with inappropriate authorship, affecting over 14,000 individuals (2.56% of the sample). Inappropriate authorship is more concentrated in Asia, Africa, and specific European countries like Italy. Established researchers with significant publication records and those affiliated with companies or nonprofits show higher instances of potential monetary authorship. Research limitations Our findings are based on contributions as declared by the authors, which implies a degree of trust in their transparency. However, this reliance on self-reporting may introduce biases or inaccuracies into the dataset. Further research could employ additional verification methods to enhance the reliability of the findings. Practical implications These findings have significant implications for journal publishers, highlighting the necessity for robust control mechanisms to ensure the integrity of authorship attributions. Moreover, researchers must exercise discernment in determining when to acknowledge a contributor and when to include them in the author list. Addressing these issues is crucial for maintaining the credibility and fairness of academic publications. Originality/value This study contributes to an understanding of critical issues within academic authorship, shedding light on the prevalence and impact of inappropriate authorship attributions. By calling for a nuanced approach to ensure accurate credit is given where it is due, the study underscores the importance of upholding ethical standards in scholarly publishing.","authors":["Abdelghani Maddi (GEMASS)","Jaime Teixeira Da Silva (MIDAP)"],"url":"https://arxiv.org/abs/2504.06314"}
{"created":"2025-04-11","title":"The Schwurbelarchiv: a German Language Telegram dataset for the Study of Conspiracy Theories","abstract":"Sociality borne by language, as is the predominant digital trace on text-based social media platforms, harbours the raw material for exploring multiple social phenomena. Distinctively, the messaging service Telegram provides functionalities that allow for socially interactive as well as one-to-many communication. Our Telegram dataset contains over 6,000 groups and channels, 40 million text messages, and over 3 million transcribed audio files, originating from a data-hoarding initiative named the ``Schwurbelarchiv'' (from German schwurbeln: speaking nonsense). This dataset publication details the structure, scope, and methodological specifics of the Schwurbelarchiv, emphasising its relevance for further research on the German-language conspiracy theory discourse. We validate its predominantly German origin by linguistic and temporal markers and situate it within the context of similar datasets. We describe process and extent of the transcription of multimedia files. Thanks to this effort the dataset uniquely supports multimodal analysis of online social dynamics and content dissemination. Researchers can employ this resource to explore societal dynamics in misinformation, political extremism, opinion adaptation, and social network structures on Telegram. The Schwurbelarchiv thus offers unprecedented opportunities for investigations into digital communication and its societal implications.","authors":["Mathias Angermaier","Joao Pinheiro-Neto","Elisabeth Hoeldrich","Jana Lasser"],"url":"https://arxiv.org/abs/2504.06318"}
{"created":"2025-04-11","title":"Accelerating LLM Inference Throughput via Asynchronous KV Cache Prefetching","abstract":"Large Language Models (LLMs) exhibit pronounced memory-bound characteristics during inference due to High Bandwidth Memory (HBM) bandwidth constraints. In this paper, we propose an L2 Cache-oriented asynchronous KV Cache prefetching method to break through the memory bandwidth bottleneck in LLM inference through computation-load overlap. By strategically scheduling idle memory bandwidth during active computation windows, our method proactively prefetches required KV Cache into GPU L2 cache, enabling high-speed L2 cache hits for subsequent accesses and effectively hiding HBM access latency within computational cycles. Extensive experiments on NVIDIA H20 GPUs demonstrate that the proposed method achieves 2.15x improvement in attention kernel efficiency and up to 1.97x end-to-end throughput enhancement, surpassing state-of-the-art baseline FlashAttention-3. Notably, our solution maintains orthogonality to existing optimization techniques and can be integrated with current inference frameworks, providing a scalable latency-hiding solution for next-generation LLM inference engines.","authors":["Yanhao Dong","Yubo Miao","Weinan Li","Xiao Zheng","Chao Wang","Feng Lyu"],"url":"https://arxiv.org/abs/2504.06319"}
{"created":"2025-04-11","title":"Hybrid Temporal Differential Consistency Autoencoder for Efficient and Sustainable Anomaly Detection in Cyber-Physical Systems","abstract":"Cyberattacks on critical infrastructure, particularly water distribution systems, have increased due to rapid digitalization and the integration of IoT devices and industrial control systems (ICS). These cyber-physical systems (CPS) introduce new vulnerabilities, requiring robust and automated intrusion detection systems (IDS) to mitigate potential threats. This study addresses key challenges in anomaly detection by leveraging time correlations in sensor data, integrating physical principles into machine learning models, and optimizing computational efficiency for edge applications. We build upon the concept of temporal differential consistency (TDC) loss to capture the dynamics of the system, ensuring meaningful relationships between dynamic states. Expanding on this foundation, we propose a hybrid autoencoder-based approach, referred to as hybrid TDC-AE, which extends TDC by incorporating both deterministic nodes and conventional statistical nodes. This hybrid structure enables the model to account for non-deterministic processes. Our approach achieves state-of-the-art classification performance while improving time to detect anomalies by 3%, outperforming the BATADAL challenge leader without requiring domain-specific knowledge, making it broadly applicable. Additionally, it maintains the computational efficiency of conventional autoencoders while reducing the number of fully connected layers, resulting in a more sustainable and efficient solution. The method demonstrates how leveraging physics-inspired consistency principles enhances anomaly detection and strengthens the resilience of cyber-physical systems.","authors":["Michael Somma"],"url":"https://arxiv.org/abs/2504.06320"}
{"created":"2025-04-11","title":"Assessing employment and labour issues implicated by using AI","abstract":"This chapter critiques the dominant reductionist approach in AI and work studies, which isolates tasks and skills as replaceable components. Instead, it advocates for a systemic perspective that emphasizes the interdependence of tasks, roles, and workplace contexts. Two complementary approaches are proposed: an ethnographic, context-rich method that highlights how AI reconfigures work environments and expertise; and a relational task-based analysis that bridges micro-level work descriptions with macro-level labor trends. The authors argue that effective AI impact assessments must go beyond predicting automation rates to include ethical, well-being, and expertise-related questions. Drawing on empirical case studies, they demonstrate how AI reshapes human-technology relations, professional roles, and tacit knowledge practices. The chapter concludes by calling for a human-centric, holistic framework that guides organizational and policy decisions, balancing technological possibilities with social desirability and sustainability of work.","authors":["Thijs Willems","Darion Jin Hotan","Jiawen Cheryl Tang","Norakmal Hakim bin Norhashim","King Wang Poon","Zi An Galvyn Goh","Radha Vinod"],"url":"https://arxiv.org/abs/2504.06322"}
{"created":"2025-04-11","title":"Mosaic: Composite Projection Pruning for Resource-efficient LLMs","abstract":"Extensive compute and memory requirements limit the deployment of large language models (LLMs) on any hardware. Compression methods, such as pruning, can reduce model size, which in turn reduces resource requirements. State-of-the-art pruning is based on coarse-grained methods. They are time-consuming and inherently remove critical model parameters, adversely impacting the quality of the pruned model. This paper introduces projection pruning, a novel fine-grained method for pruning LLMs. In addition, LLM projection pruning is enhanced by a new approach we refer to as composite projection pruning - the synergistic combination of unstructured pruning that retains accuracy and structured pruning that reduces model size. We develop Mosaic, a novel system to create and deploy pruned LLMs using composite projection pruning. Mosaic is evaluated using a range of performance and quality metrics on multiple hardware platforms, LLMs, and datasets. Mosaic is 7.19x faster in producing models than existing approaches. Mosaic models achieve up to 84.2% lower perplexity and 31.4% higher accuracy than models obtained from coarse-grained pruning. Up to 67% faster inference and 68% lower GPU memory use is noted for Mosaic models.","authors":["Bailey J. Eccles","Leon Wong","Blesson Varghese"],"url":"https://arxiv.org/abs/2504.06323"}
{"created":"2025-04-11","title":"From Stability to Inconsistency: A Study of Moral Preferences in LLMs","abstract":"As large language models (LLMs) increasingly integrate into our daily lives, it becomes crucial to understand their implicit biases and moral tendencies. To address this, we introduce a Moral Foundations LLM dataset (MFD-LLM) grounded in Moral Foundations Theory, which conceptualizes human morality through six core foundations. We propose a novel evaluation method that captures the full spectrum of LLMs' revealed moral preferences by answering a range of real-world moral dilemmas. Our findings reveal that state-of-the-art models have remarkably homogeneous value preferences, yet demonstrate a lack of consistency.","authors":["Monika Jotautaite","Mary Phuong","Chatrik Singh Mangat","Maria Angelica Martinez"],"url":"https://arxiv.org/abs/2504.06324"}
{"created":"2025-04-11","title":"MM-STFlowNet: A Transportation Hub-Oriented Multi-Mode Passenger Flow Prediction Method via Spatial-Temporal Dynamic Graph Modeling","abstract":"Accurate and refined passenger flow prediction is essential for optimizing the collaborative management of multiple collection and distribution modes in large-scale transportation hubs. Traditional methods often focus only on the overall passenger volume, neglecting the interdependence between different modes within the hub. To address this limitation, we propose MM-STFlowNet, a comprehensive multi-mode prediction framework grounded in dynamic spatial-temporal graph modeling. Initially, an integrated temporal feature processing strategy is implemented using signal decomposition and convolution techniques to address data spikes and high volatility. Subsequently, we introduce the Spatial-Temporal Dynamic Graph Convolutional Recurrent Network (STDGCRN) to capture detailed spatial-temporal dependencies across multiple traffic modes, enhanced by an adaptive channel attention mechanism. Finally, the self-attention mechanism is applied to incorporate various external factors, further enhancing prediction accuracy. Experiments on a real-world dataset from Guangzhounan Railway Station in China demonstrate that MM-STFlowNet achieves state-of-the-art performance, particularly during peak periods, providing valuable insight for transportation hub management.","authors":["Ronghui Zhang","Wenbin Xing","Mengran Li","Zihan Wang","Junzhou Chen","Xiaolei Ma","Zhiyuan Liu","Zhengbing He"],"url":"https://arxiv.org/abs/2504.06325"}
{"created":"2025-04-11","title":"Physics-informed KAN PointNet: Deep learning for simultaneous solutions to inverse problems in incompressible flow on numerous irregular geometries","abstract":"Kolmogorov-Arnold Networks (KANs) have gained attention as a promising alternative to traditional Multilayer Perceptrons (MLPs) for deep learning applications in computational physics, especially within the framework of physics-informed neural networks (PINNs). Physics-informed Kolmogorov-Arnold Networks (PIKANs) and their variants have been introduced and evaluated to solve inverse problems. However, similar to PINNs, current versions of PIKANs are limited to obtaining solutions for a single computational domain per training run; consequently, a new geometry requires retraining the model from scratch. Physics-informed PointNet (PIPN) was introduced to address this limitation for PINNs. In this work, we introduce physics-informed Kolmogorov-Arnold PointNet (PI-KAN-PointNet) to extend this capability to PIKANs. PI-KAN-PointNet enables the simultaneous solution of an inverse problem over multiple irregular geometries within a single training run, reducing computational costs. We construct KANs using Jacobi polynomials and investigate their performance by considering Jacobi polynomials of different degrees and types in terms of both computational cost and prediction accuracy. As a benchmark test case, we consider natural convection in a square enclosure with a cylinder, where the cylinder's shape varies across a dataset of 135 geometries. We compare the performance of PI-KAN-PointNet with that of PIPN (i.e., physics-informed PointNet with MLPs) and observe that, with approximately an equal number of trainable parameters and similar computational cost, PI-KAN-PointNet provides more accurate predictions. Finally, we explore the combination of KAN and MLP in constructing a physics-informed PointNet. Our findings indicate that a physics-informed PointNet model employing MLP layers as the encoder and KAN layers as the decoder represents the optimal configuration among all models investigated.","authors":["Ali Kashefi","Tapan Mukerji"],"url":"https://arxiv.org/abs/2504.06327"}
{"created":"2025-04-11","title":"Analyzing the Impact of Low-Rank Adaptation for Cross-Domain Few-Shot Object Detection in Aerial Images","abstract":"This paper investigates the application of Low-Rank Adaptation (LoRA) to small models for cross-domain few-shot object detection in aerial images. Originally designed for large-scale models, LoRA helps mitigate overfitting, making it a promising approach for resource-constrained settings. We integrate LoRA into DiffusionDet, and evaluate its performance on the DOTA and DIOR datasets. Our results show that LoRA applied after an initial fine-tuning slightly improves performance in low-shot settings (e.g., 1-shot and 5-shot), while full fine-tuning remains more effective in higher-shot configurations. These findings highlight LoRA's potential for efficient adaptation in aerial object detection, encouraging further research into parameter-efficient fine-tuning strategies for few-shot learning. Our code is available here: https://github.com/HichTala/LoRA-DiffusionDet.","authors":["Hicham Talaoubrid","Anissa Mokraoui","Ismail Ben Ayed","Axel Prouvost","Sonimith Hang","Monit Korn","R\\'emi Harvey"],"url":"https://arxiv.org/abs/2504.06330"}
{"created":"2025-04-11","title":"An Information-Geometric Approach to Artificial Curiosity","abstract":"Learning in environments with sparse rewards remains a fundamental challenge in reinforcement learning. Artificial curiosity addresses this limitation through intrinsic rewards to guide exploration, however, the precise formulation of these rewards has remained elusive. Ideally, such rewards should depend on the agent's information about the environment, remaining agnostic to the representation of the information -- an invariance central to information geometry. Leveraging information geometry, we show that invariance under congruent Markov morphisms and the agent-environment interaction, uniquely constrains intrinsic rewards to concave functions of the reciprocal occupancy. Additional geometrically motivated restrictions effectively limits the candidates to those determined by a real parameter that governs the occupancy space geometry. Remarkably, special values of this parameter are found to correspond to count-based and maximum entropy exploration, revealing a geometric exploration-exploitation trade-off. This framework provides important constraints to the engineering of intrinsic reward while integrating foundational exploration methods into a single, cohesive model.","authors":["Alexander Nedergaard","Pablo A. Morales"],"url":"https://arxiv.org/abs/2504.06355"}
{"created":"2025-04-11","title":"Query Understanding in LLM-based Conversational Information Seeking","abstract":"Query understanding in Conversational Information Seeking (CIS) involves accurately interpreting user intent through context-aware interactions. This includes resolving ambiguities, refining queries, and adapting to evolving information needs. Large Language Models (LLMs) enhance this process by interpreting nuanced language and adapting dynamically, improving the relevance and precision of search results in real-time. In this tutorial, we explore advanced techniques to enhance query understanding in LLM-based CIS systems. We delve into LLM-driven methods for developing robust evaluation metrics to assess query understanding quality in multi-turn interactions, strategies for building more interactive systems, and applications like proactive query management and query reformulation. We also discuss key challenges in integrating LLMs for query understanding in conversational search systems and outline future research directions. Our goal is to deepen the audience's understanding of LLM-based conversational query understanding and inspire discussions to drive ongoing advancements in this field.","authors":["Yifei Yuan","Zahra Abbasiantaeb","Yang Deng","Mohammad Aliannejadi"],"url":"https://arxiv.org/abs/2504.06356"}
{"created":"2025-04-11","title":"From Broadcast to Minimap: Achieving State-of-the-Art SoccerNet Game State Reconstruction","abstract":"Game State Reconstruction (GSR), a critical task in Sports Video Understanding, involves precise tracking and localization of all individuals on the football field-players, goalkeepers, referees, and others - in real-world coordinates. This capability enables coaches and analysts to derive actionable insights into player movements, team formations, and game dynamics, ultimately optimizing training strategies and enhancing competitive advantage. Achieving accurate GSR using a single-camera setup is highly challenging due to frequent camera movements, occlusions, and dynamic scene content. In this work, we present a robust end-to-end pipeline for tracking players across an entire match using a single-camera setup. Our solution integrates a fine-tuned YOLOv5m for object detection, a SegFormer-based camera parameter estimator, and a DeepSORT-based tracking framework enhanced with re-identification, orientation prediction, and jersey number recognition. By ensuring both spatial accuracy and temporal consistency, our method delivers state-of-the-art game state reconstruction, securing first place in the SoccerNet Game State Reconstruction Challenge 2024 and significantly outperforming competing methods.","authors":["Vladimir Golovkin","Nikolay Nemtsev","Vasyl Shandyba","Oleg Udin","Nikita Kasatkin","Pavel Kononov","Anton Afanasiev","Sergey Ulasen","Andrei Boiarov"],"url":"https://arxiv.org/abs/2504.06357"}
{"created":"2025-04-11","title":"Towards Calibration Enhanced Network by Inverse Adversarial Attack","abstract":"Test automation has become increasingly important as the complexity of both design and content in Human Machine Interface (HMI) software continues to grow. Current standard practice uses Optical Character Recognition (OCR) techniques to automatically extract textual information from HMI screens for validation. At present, one of the key challenges faced during the automation of HMI screen validation is the noise handling for the OCR models. In this paper, we propose to utilize adversarial training techniques to enhance OCR models in HMI testing scenarios. More specifically, we design a new adversarial attack objective for OCR models to discover the decision boundaries in the context of HMI testing. We then adopt adversarial training to optimize the decision boundaries towards a more robust and accurate OCR model. In addition, we also built an HMI screen dataset based on real-world requirements and applied multiple types of perturbation onto the clean HMI dataset to provide a more complete coverage for the potential scenarios. We conduct experiments to demonstrate how using adversarial training techniques yields more robust OCR models against various kinds of noises, while still maintaining high OCR model accuracy. Further experiments even demonstrate that the adversarial training models exhibit a certain degree of robustness against perturbations from other patterns.","authors":["Yupeng Cheng","Zi Pong Lim","Sarthak Ketanbhai Modi","Yon Shin Teo","Yushi Cao","Shang-Wei Lin"],"url":"https://arxiv.org/abs/2504.06358"}
{"created":"2025-04-11","title":"Restoring Feasibility in Power Grid Optimization: A Counterfactual ML Approach","abstract":"Electric power grids are essential components of modern life, delivering reliable power to end-users while adhering to a multitude of engineering constraints and requirements. In grid operations, the Optimal Power Flow problem plays a key role in determining cost-effective generator dispatch that satisfies load demands and operational limits. However, due to stressed operating conditions, volatile demand profiles, and increased generation from intermittent energy sources, this optimization problem may become infeasible, posing risks such as voltage instability and line overloads. This study proposes a learning framework that combines machine learning with counterfactual explanations to automatically diagnose and restore feasibility in the OPF problem. Our method provides transparent and actionable insights by methodically identifying infeasible conditions and suggesting minimal demand response actions. We evaluate the proposed approach on IEEE 30-bus and 300-bus systems, demonstrating its capability to recover feasibility with high success rates and generating diverse corrective options, appropriate for real-time decision-making. These preliminary findings illustrate the potential of combining classical optimization with explainable AI techniques to enhance grid reliability and resilience.","authors":["Mostafa Mohammadian","Anna Van Boven","Kyri Baker"],"url":"https://arxiv.org/abs/2504.06369"}
{"created":"2025-04-11","title":"Automated Fabrication of Magnetic Soft Microrobots","abstract":"The advent of 3D printing has revolutionized many industries and has had similar improvements for soft robots. However, many challenges persist for these functional devices. Magnetic soft robots require the addition of magnetic particles that must be correctly oriented. There is a significant gap in the automated fabrication of 3D geometric structures with 3D magnetization direction. A fully automated 3D printer was designed to improve accuracy, speed, and reproducibility. This design was able to achieve a circular spot size (voxels) of 1.6mm in diameter. An updated optical system can improve the resolution to a square spot size of 50$\\mu$m by 50$\\mu$m. The new system achieves higher resolution designs as shown through magneto-mechanical simulations. Various microrobots including 'worm', 'gripper' and 'zipper' designs are evaluated with the new spot size.","authors":["Kaitlyn Clancy","Siwen Xie","Griffin Smith","Onaizah Onaizah"],"url":"https://arxiv.org/abs/2504.06370"}
{"created":"2025-04-11","title":"Efficient Simulation of Singularly Perturbed Systems Using a Stabilized Multirate Explicit Scheme","abstract":"Singularly perturbed systems (SPSs) are prevalent in engineering applications, where numerically solving their initial value problems (IVPs) is challenging due to stiffness arising from multiple time scales. Classical explicit methods require impractically small time steps for stability, while implicit methods developed for SPSs are computationally intensive and less efficient for strongly nonlinear systems. This paper introduces a Stabilized Multirate Explicit Scheme (SMES) that stabilizes classical explicit methods without the need for small time steps or implicit formulations. By employing a multirate approach with variable time steps, SMES allows the fast dynamics to rapidly converge to their equilibrium manifold while slow dynamics evolve with larger steps. Analysis shows that SMES achieves numerical stability with significantly reduced computational effort and controlled error. Its effectiveness is illustrated with a numerical example.","authors":["Yibo Shi","Cristian R. Rojas"],"url":"https://arxiv.org/abs/2504.06371"}
{"created":"2025-04-11","title":"A Metropolis-Adjusted Langevin Algorithm for Sampling Jeffreys Prior","abstract":"Inference and estimation are fundamental aspects of statistics, system identification and machine learning. For most inference problems, prior knowledge is available on the system to be modeled, and Bayesian analysis is a natural framework to impose such prior information in the form of a prior distribution. However, in many situations, coming out with a fully specified prior distribution is not easy, as prior knowledge might be too vague, so practitioners prefer to use a prior distribution that is as `ignorant' or `uninformative' as possible, in the sense of not imposing subjective beliefs, while still supporting reliable statistical analysis. Jeffreys prior is an appealing uninformative prior because it offers two important benefits: (i) it is invariant under any re-parameterization of the model, (ii) it encodes the intrinsic geometric structure of the parameter space through the Fisher information matrix, which in turn enhances the diversity of parameter samples. Despite these benefits, drawing samples from Jeffreys prior is a challenging task. In this paper, we propose a general sampling scheme using the Metropolis-Adjusted Langevin Algorithm that enables sampling of parameter values from Jeffreys prior, and provide numerical illustrations of our approach through several examples.","authors":["Yibo Shi","Braghadeesh Lakshminarayanan","Cristian R. Rojas"],"url":"https://arxiv.org/abs/2504.06372"}
{"created":"2025-04-11","title":"Review, Definition and Challenges of Electrical Energy Hubs","abstract":"To transition towards a carbon-neutral power system, considerable amounts of renewable energy generation capacity are being installed in the North Sea area. Consequently, projects aggregating many gigawatts of power generation capacity and transmitting renewable energy to the main load centers are being developed. Given the electrical challenges arising from having bulk power capacity in a compact geographical area with several connections to the main grid, and a lack of a robust definition identifying the type of system under study, this paper proposes a general technical definition of such projects introducing the term Electrical Energy Hub (EEH). The concept, purpose, and functionalities of EEHs are introduced in the text, emphasizing the importance of a clear technical definition for future planning procedures, grid codes, regulations, and support schemes for EEHs and multiterminal HVDC (MTDC) grids in general. Furthermore, the unique electrical challenges associated with integrating EEHs into the power system are discussed. Three research areas of concern are identified, namely control, planning, and protection. Through this analysis, insights are provided into the effective implementation of multi-GW scale EEH projects and their integration into the power grid through multiple interconnections. Finally, a list of ongoing and planned grid development projects is evaluated to assess whether they fall within the EEH category","authors":["Giacomo Bastianel","Jan Kircheis","Merijn Van Deyck","Dongyeong Lee","Geraint Chaffey","Marta Vanin","Hakan Ergun","Jef Beerten","Dirk Van Hertem"],"url":"https://arxiv.org/abs/2504.06373"}
{"created":"2025-04-11","title":"Comparing Self-Disclosure Themes and Semantics to a Human, a Robot, and a Disembodied Agent","abstract":"As social robots and other artificial agents become more conversationally capable, it is important to understand whether the content and meaning of self-disclosure towards these agents changes depending on the agent's embodiment. In this study, we analysed conversational data from three controlled experiments in which participants self-disclosed to a human, a humanoid social robot, and a disembodied conversational agent. Using sentence embeddings and clustering, we identified themes in participants' disclosures, which were then labelled and explained by a large language model. We subsequently assessed whether these themes and the underlying semantic structure of the disclosures varied by agent embodiment. Our findings reveal strong consistency: thematic distributions did not significantly differ across embodiments, and semantic similarity analyses showed that disclosures were expressed in highly comparable ways. These results suggest that while embodiment may influence human behaviour in human-robot and human-agent interactions, people tend to maintain a consistent thematic focus and semantic structure in their disclosures, whether speaking to humans or artificial interlocutors.","authors":["Sophie Chiang","Guy Laban","Emily S. Cross","Hatice Gunes"],"url":"https://arxiv.org/abs/2504.06374"}
{"created":"2025-04-11","title":"Quantum Combine and Conquer and Its Applications to Sublinear Quantum Convex Hull and Maxima Set Construction","abstract":"We introduce a quantum algorithm design paradigm called combine and conquer, which is a quantum version of the \"marriage-before-conquest\" technique of Kirkpatrick and Seidel. In a quantum combine-and-conquer algorithm, one performs the essential computation of the combine step of a quantum divide-and-conquer algorithm prior to the conquer step while avoiding recursion. This model is better suited for the quantum setting, due to its non-recursive nature. We show the utility of this approach by providing quantum algorithms for 2D maxima set and convex hull problems for sorted point sets running in $\\tilde{O}(\\sqrt{nh})$ time, w.h.p., where $h$ is the size of the output.","authors":["Shion Fukuzawa","Michael T. Goodrich","Sandy Irani"],"url":"https://arxiv.org/abs/2504.06376"}
{"created":"2025-04-11","title":"On Mixed-Precision Iterative Methods and Analysis for Nearly Completely Decomposable Markov Processes","abstract":"In this paper we consider the problem of computing the stationary distribution of nearly completely decomposable Markov processes, a well-established area in the classical theory of Markov processes with broad applications in the design, modeling, analysis and optimization of computer systems. We design general classes of algorithmic solution approaches that exploit forms of mixed-precision computation to significantly reduce computation times and that exploit forms of iterative approximate methods to mitigate the impact of inaccurate computations, further reduce computation times, and ensure convergence. Then we derive a mathematical analysis of our general algorithmic approaches that establishes theoretical results on approximation errors, convergence behaviors, and other algorithmic properties. Numerical experiments demonstrate that our general algorithmic approaches provide significant improvements in computation times over the most efficient existing numerical methods.","authors":["Vasileios Kalantzis","Mark S. Squillante","Chai Wah Wu"],"url":"https://arxiv.org/abs/2504.06378"}
{"created":"2025-04-11","title":"User-Centered Insights into Assistive Navigation Technologies for Individuals with Visual Impairment","abstract":"Navigational challenges significantly impact the independence and mobility of Individuals with Visual Impairment (IVI). While numerous assistive technologies exist, their adoption remains limited due to usability challenges, financial constraints, and a lack of alignment with user needs. This study employs a mixed-methods approach, combining structured surveys and virtual workshops with 19 IVI to investigate their experiences, needs, and preferences regarding assistive technologies for navigation and daily living. The survey results provide insights into participants technological competence, preferences for assistive devices, and willingness to adopt new solutions. In parallel, workshop discussions offer qualitative perspectives on key navigation challenges, including difficulties in detecting overhead obstacles, navigating environments with complex layout, and the limitations of existing technologies. Findings highlight the need for assistive devices that integrate both navigational guidance and high-level spatial awareness, allowing users to build mental maps of their surroundings. Additionally, multimodal feedback, combining audio, haptic, and tactile cues, emerges as a crucial feature to accommodate diverse user preferences and environmental conditions. The study also underscores financial and training barriers that limit access to advanced assistive technologies. Based on these insights, we recommend the development of customizable, user-friendly, and most importantly affordable navigation aids that align with the daily needs of IVI. The findings from this study provide guidance for technology developers, researchers, and policymakers working toward more inclusive and effective assistive solutions.","authors":["Iman Soltani","Johnaton Schofield","Mehran Madani","Daniel Kish","Parisa Emami-Naeini"],"url":"https://arxiv.org/abs/2504.06379"}
{"created":"2025-04-11","title":"Fast Globally Optimal and Geometrically Consistent 3D Shape Matching","abstract":"Geometric consistency, i.e. the preservation of neighbourhoods, is a natural and strong prior in 3D shape matching. Geometrically consistent matchings are crucial for many downstream applications, such as texture transfer or statistical shape modelling. Yet, in practice, geometric consistency is often overlooked, or only achieved under severely limiting assumptions (e.g. a good initialisation). In this work, we propose a novel formalism for computing globally optimal and geometrically consistent matchings between 3D shapes which is scalable in practice. Our key idea is to represent the surface of the source shape as a collection of cyclic paths, which are then consistently matched to the target shape. Mathematically, we construct a hyper product graph (between source and target shape), and then cast 3D shape matching as a minimum-cost circulation flow problem in this hyper graph, which yields global geometrically consistent matchings between both shapes. We empirically show that our formalism is efficiently solvable and that it leads to high-quality results.","authors":["Paul Roetzer","Florian Bernard"],"url":"https://arxiv.org/abs/2504.06385"}
{"created":"2025-04-11","title":"SPoRt -- Safe Policy Ratio: Certified Training and Deployment of Task Policies in Model-Free RL","abstract":"To apply reinforcement learning to safety-critical applications, we ought to provide safety guarantees during both policy training and deployment. In this work we present novel theoretical results that provide a bound on the probability of violating a safety property for a new task-specific policy in a model-free, episodic setup: the bound, based on a `maximum policy ratio' that is computed with respect to a `safe' base policy, can also be more generally applied to temporally-extended properties (beyond safety) and to robust control problems. We thus present SPoRt, which also provides a data-driven approach for obtaining such a bound for the base policy, based on scenario theory, and which includes Projected PPO, a new projection-based approach for training the task-specific policy while maintaining a user-specified bound on property violation. Hence, SPoRt enables the user to trade off safety guarantees in exchange for task-specific performance. Accordingly, we present experimental results demonstrating this trade-off, as well as a comparison of the theoretical bound to posterior bounds based on empirical violation rates.","authors":["Jacques Cloete","Nikolaus Vertovec","Alessandro Abate"],"url":"https://arxiv.org/abs/2504.06386"}
{"created":"2025-04-11","title":"Towards Evidence-Based Tech Hiring Pipelines","abstract":"Software engineers are responsible for developing, maintaining, and innovating software. To hire software engineers, organizations employ a tech hiring pipeline. This process typically consists of a series of steps to evaluate the extent to which applicants meet job requirements and can effectively contribute to a development team -- such as resume screenings and technical interviews. However, research highlights substantial flaws with current tech hiring practices -- such as bias from stress-inducing assessments. As the landscape of software engineering (SE) is dramatically changing, assessing the technical proficiency and abilities of software engineers is an increasingly crucial task to meet technological needs and demands. In this paper, we outline challenges in current hiring practices and present future directions to promote fair and evidence-based evaluations in tech hiring pipelines. Our vision aims to enhance outcomes for candidates and assessments for employers to enhance the workforce in the tech industry.","authors":["Chris Brown","Swanand Vaishampayan"],"url":"https://arxiv.org/abs/2504.06387"}
{"created":"2025-04-11","title":"SemiDAViL: Semi-supervised Domain Adaptation with Vision-Language Guidance for Semantic Segmentation","abstract":"Domain Adaptation (DA) and Semi-supervised Learning (SSL) converge in Semi-supervised Domain Adaptation (SSDA), where the objective is to transfer knowledge from a source domain to a target domain using a combination of limited labeled target samples and abundant unlabeled target data. Although intuitive, a simple amalgamation of DA and SSL is suboptimal in semantic segmentation due to two major reasons: (1) previous methods, while able to learn good segmentation boundaries, are prone to confuse classes with similar visual appearance due to limited supervision; and (2) skewed and imbalanced training data distribution preferring source representation learning whereas impeding from exploring limited information about tailed classes. Language guidance can serve as a pivotal semantic bridge, facilitating robust class discrimination and mitigating visual ambiguities by leveraging the rich semantic relationships encoded in pre-trained language models to enhance feature representations across domains. Therefore, we propose the first language-guided SSDA setting for semantic segmentation in this work. Specifically, we harness the semantic generalization capabilities inherent in vision-language models (VLMs) to establish a synergistic framework within the SSDA paradigm. To address the inherent class-imbalance challenges in long-tailed distributions, we introduce class-balanced segmentation loss formulations that effectively regularize the learning process. Through extensive experimentation across diverse domain adaptation scenarios, our approach demonstrates substantial performance improvements over contemporary state-of-the-art (SoTA) methodologies. Code is available: \\href{https://github.com/hritam-98/SemiDAViL}{GitHub}.","authors":["Hritam Basak","Zhaozheng Yin"],"url":"https://arxiv.org/abs/2504.06389"}
{"created":"2025-04-11","title":"Towards practicable Machine Learning development using AI Engineering Blueprints","abstract":"The implementation of artificial intelligence (AI) in business applications holds considerable promise for significant improvements. The development of AI systems is becoming increasingly complex, thereby underscoring the growing importance of AI engineering and MLOps techniques. Small and medium-sized enterprises (SMEs) face considerable challenges when implementing AI in their products or processes. These enterprises often lack the necessary resources and expertise to develop, deploy, and operate AI systems that are tailored to address their specific problems.","authors":["Nicolas Weeger","Annika Stiehl","J\\'oakim vom Kistowski","Stefan Gei{\\ss}els\\\"oder","Christian Uhl"],"url":"https://arxiv.org/abs/2504.06391"}
{"created":"2025-04-11","title":"The Zero Body Problem: Probing LLM Use of Sensory Language","abstract":"Sensory language expresses embodied experiences ranging from taste and sound to excitement and stomachache. This language is of interest to scholars from a wide range of domains including robotics, narratology, linguistics, and cognitive science. In this work, we explore whether language models, which are not embodied, can approximate human use of embodied language. We extend an existing corpus of parallel human and model responses to short story prompts with an additional 18,000 stories generated by 18 popular models. We find that all models generate stories that differ significantly from human usage of sensory language, but the direction of these differences varies considerably between model families. Namely, Gemini models use significantly more sensory language than humans along most axes whereas most models from the remaining five families use significantly less. Linear probes run on five models suggest that they are capable of identifying sensory language. However, we find preliminary evidence suggesting that instruction tuning may discourage usage of sensory language. Finally, to support further work, we release our expanded story dataset.","authors":["Rebecca M. M. Hicke","Sil Hamilton","David Mimno"],"url":"https://arxiv.org/abs/2504.06393"}
{"created":"2025-04-11","title":"A Scalable Automatic Model Generation Tool for Cyber-Physical Network Topologies and Data Flows for Large-Scale Synthetic Power Grid Models","abstract":"Power grids and their cyber infrastructure are classified as Critical Energy Infrastructure/Information (CEII) and are not publicly accessible. While realistic synthetic test cases for power systems have been developed in recent years, they often lack corresponding cyber network models. This work extends synthetic grid models by incorporating cyber-physical representations. To address the growing need for realistic and scalable models that integrate both cyber and physical layers in electric power systems, this paper presents the Scalable Automatic Model Generation Tool (SAM-GT). This tool enables the creation of large-scale cyber-physical topologies for power system models. The resulting cyber-physical network models include power system switches, routers, and firewalls while accounting for data flows and industrial communication protocols. Case studies demonstrate the tool's application to synthetic grid models of 500, 2,000, and 10,000 buses, considering three distinct network topologies. Results from these case studies include network metrics on critical nodes, hops, and generation times, showcasing effectiveness, adaptability, and scalability of SAM-GT.","authors":["Samantha Israel","Sanjana Kunkolienkar","Ana Goulart","Kate Davis","Thomas Overbye"],"url":"https://arxiv.org/abs/2504.06396"}
{"created":"2025-04-11","title":"PromptHMR: Promptable Human Mesh Recovery","abstract":"Human pose and shape (HPS) estimation presents challenges in diverse scenarios such as crowded scenes, person-person interactions, and single-view reconstruction. Existing approaches lack mechanisms to incorporate auxiliary \"side information\" that could enhance reconstruction accuracy in such challenging scenarios. Furthermore, the most accurate methods rely on cropped person detections and cannot exploit scene context while methods that process the whole image often fail to detect people and are less accurate than methods that use crops. While recent language-based methods explore HPS reasoning through large language or vision-language models, their metric accuracy is well below the state of the art. In contrast, we present PromptHMR, a transformer-based promptable method that reformulates HPS estimation through spatial and semantic prompts. Our method processes full images to maintain scene context and accepts multiple input modalities: spatial prompts like bounding boxes and masks, and semantic prompts like language descriptions or interaction labels. PromptHMR demonstrates robust performance across challenging scenarios: estimating people from bounding boxes as small as faces in crowded scenes, improving body shape estimation through language descriptions, modeling person-person interactions, and producing temporally coherent motions in videos. Experiments on benchmarks show that PromptHMR achieves state-of-the-art performance while offering flexible prompt-based control over the HPS estimation process.","authors":["Yufu Wang","Yu Sun","Priyanka Patel","Kostas Daniilidis","Michael J. Black","Muhammed Kocabas"],"url":"https://arxiv.org/abs/2504.06397"}
{"created":"2025-04-11","title":"Sharpness-Aware Parameter Selection for Machine Unlearning","abstract":"It often happens that some sensitive personal information, such as credit card numbers or passwords, are mistakenly incorporated in the training of machine learning models and need to be removed afterwards. The removal of such information from a trained model is a complex task that needs to partially reverse the training process. There have been various machine unlearning techniques proposed in the literature to address this problem. Most of the proposed methods revolve around removing individual data samples from a trained model. Another less explored direction is when features/labels of a group of data samples need to be reverted. While the existing methods for these tasks do the unlearning task by updating the whole set of model parameters or only the last layer of the model, we show that there are a subset of model parameters that have the largest contribution in the unlearning target features. More precisely, the model parameters with the largest corresponding diagonal value in the Hessian matrix (computed at the learned model parameter) have the most contribution in the unlearning task. By selecting these parameters and updating them during the unlearning stage, we can have the most progress in unlearning. We provide theoretical justifications for the proposed strategy by connecting it to sharpness-aware minimization and robust unlearning. We empirically show the effectiveness of the proposed strategy in improving the efficacy of unlearning with a low computational cost.","authors":["Saber Malekmohammadi","Hong kyu Lee","Li Xiong"],"url":"https://arxiv.org/abs/2504.06398"}
{"created":"2025-04-11","title":"Low Rank Learning for Offline Query Optimization","abstract":"Recent deployments of learned query optimizers use expensive neural networks and ad-hoc search policies. To address these issues, we introduce \\textsc{LimeQO}, a framework for offline query optimization leveraging low-rank learning to efficiently explore alternative query plans with minimal resource usage. By modeling the workload as a partially observed, low-rank matrix, we predict unobserved query plan latencies using purely linear methods, significantly reducing computational overhead compared to neural networks. We formalize offline exploration as an active learning problem, and present simple heuristics that reduces a 3-hour workload to 1.5 hours after just 1.5 hours of exploration. Additionally, we propose a transductive Tree Convolutional Neural Network (TCNN) that, despite higher computational costs, achieves the same workload reduction with only 0.5 hours of exploration. Unlike previous approaches that place expensive neural networks directly in the query processing ``hot'' path, our approach offers a low-overhead solution and a no-regressions guarantee, all without making assumptions about the underlying DBMS. The code is available in \\href{https://github.com/zixy17/LimeQO}{https://github.com/zixy17/LimeQO}.","authors":["Zixuan Yi","Yao Tian","Zachary G. Ives","Ryan Marcus"],"url":"https://arxiv.org/abs/2504.06399"}
{"created":"2025-04-11","title":"Leveraging Non-Steady-State Frequency-Domain Data in Willems' Fundamental Lemma","abstract":"Willems' fundamental lemma enables data-driven analysis and control by characterizing an unknown system's behavior directly in terms of measured data. In this work, we extend a recent frequency-domain variant of this result--previously limited to steady-state data--to incorporate non-steady-state data including transient phenomena. This approach eliminates the need to wait for transients to decay during data collection, significantly reducing the experiment duration. Unlike existing frequency-domain system identification methods, our approach integrates transient data without preprocessing, making it well-suited for direct data-driven analysis and control. We demonstrate its effectiveness by isolating transients in the collected data and performing FRF evaluation at arbitrary frequencies in a numerical case study including noise.","authors":["T. J. Meijer","M. Wind","V. S. Dolk","W. P. M. H. Heemels"],"url":"https://arxiv.org/abs/2504.06403"}
{"created":"2025-04-11","title":"Physical spline for denoising object trajectory data by combining splines, ML feature regression and model knowledge","abstract":"This article presents a method for estimating the dynamic driving states (position, velocity, acceleration and heading) from noisy measurement data. The proposed approach is effective with both complete and partial observations, producing refined trajectory signals with kinematic consistency, ensuring that velocity is the integral of acceleration and position is the integral of velocity. Additionally, the method accounts for the constraint that vehicles can only move in the direction of their orientation. The method is implemented as a configurable python library that also enables trajectory estimation solely based on position data. Regularization is applied to prevent extreme state variations. A key application is enhancing recorded trajectory data for use as reference inputs in machine learning models. At the end, the article presents the results of the method along with a comparison to ground truth data.","authors":["Jonas Torzewski"],"url":"https://arxiv.org/abs/2504.06404"}
{"created":"2025-04-11","title":"Scalable Routing in a City-Scale Wi-Fi Network for Disaster Recovery","abstract":"In this paper, we present a new city-scale decentralized mesh network system suited for disaster recovery and emergencies. When wide-area connectivity is unavailable or significantly degraded, our system, MapMesh, enables static access points and mobile devices equipped with Wi-Fi in a city to route packets via each other for intra-city connectivity and to/from any nodes that might have Internet access, e.g., via satellite. The chief contribution of our work is a new routing protocol that scales to millions of nodes, a significant improvement over prior work on wireless mesh and mobile ad hoc networks. Our approach uses detailed information about buildings from widely available maps--data that was unavailable at scale over a decade ago, but is widely available now--to compute paths in a scalable way.","authors":["Ziqian Liu","Om Chabra","James Lynch","Chenning Li","Manya Ghobadi","Hari Balakrishnan"],"url":"https://arxiv.org/abs/2504.06406"}
{"created":"2025-04-11","title":"Understanding Machine Unlearning Through the Lens of Mode Connectivity","abstract":"Machine Unlearning aims to remove undesired information from trained models without requiring full retraining from scratch. Despite recent advancements, their underlying loss landscapes and optimization dynamics received less attention. In this paper, we investigate and analyze machine unlearning through the lens of mode connectivity - the phenomenon where independently trained models can be connected by smooth low-loss paths in the parameter space. We define and study mode connectivity in unlearning across a range of overlooked conditions, including connections between different unlearning methods, models trained with and without curriculum learning, and models optimized with first-order and secondorder techniques. Our findings show distinct patterns of fluctuation of different evaluation metrics along the curve, as well as the mechanistic (dis)similarity between unlearning methods. To the best of our knowledge, this is the first study on mode connectivity in the context of machine unlearning.","authors":["Jiali Cheng","Hadi Amiri"],"url":"https://arxiv.org/abs/2504.06407"}
{"created":"2025-04-11","title":"Parallel GPU-Enabled Algorithms for SpGEMM on Arbitrary Semirings with Hybrid Communication","abstract":"Sparse General Matrix Multiply (SpGEMM) is key for various High-Performance Computing (HPC) applications such as genomics and graph analytics. Using the semiring abstraction, many algorithms can be formulated as SpGEMM, allowing redefinition of addition, multiplication, and numeric types. Today large input matrices require distributed memory parallelism to avoid disk I/O, and modern HPC machines with GPUs can greatly accelerate linear algebra computation. In this paper, we implement a GPU-based distributed-memory SpGEMM routine on top of the CombBLAS library. Our implementation achieves a speedup of over 2x compared to the CPU-only CombBLAS implementation and up to 3x compared to PETSc for large input matrices. Furthermore, we note that communication between processes can be optimized by either direct host-to-host or device-to-device communication, depending on the message size. To exploit this, we introduce a hybrid communication scheme that dynamically switches data paths depending on the message size, thus improving runtimes in communication-bound scenarios.","authors":["Thomas McFarland","Julian Bellavita","Giulia Guidi"],"url":"https://arxiv.org/abs/2504.06408"}
{"created":"2025-04-11","title":"PEEL the Layers and Find Yourself: Revisiting Inference-time Data Leakage for Residual Neural Networks","abstract":"This paper explores inference-time data leakage risks of deep neural networks (NNs), where a curious and honest model service provider is interested in retrieving users' private data inputs solely based on the model inference results. Particularly, we revisit residual NNs due to their popularity in computer vision and our hypothesis that residual blocks are a primary cause of data leakage owing to the use of skip connections. By formulating inference-time data leakage as a constrained optimization problem, we propose a novel backward feature inversion method, \\textbf{PEEL}, which can effectively recover block-wise input features from the intermediate output of residual NNs. The surprising results in high-quality input data recovery can be explained by the intuition that the output from these residual blocks can be considered as a noisy version of the input and thus the output retains sufficient information for input recovery. We demonstrate the effectiveness of our layer-by-layer feature inversion method on facial image datasets and pre-trained classifiers. Our results show that PEEL outperforms the state-of-the-art recovery methods by an order of magnitude when evaluated by mean squared error (MSE). The code is available at \\href{https://github.com/Huzaifa-Arif/PEEL}{https://github.com/Huzaifa-Arif/PEEL}","authors":["Huzaifa Arif","Keerthiram Murugesan","Payel Das","Alex Gittens","Pin-Yu Chen"],"url":"https://arxiv.org/abs/2504.06410"}
{"created":"2025-04-11","title":"Unifying Autoregressive and Diffusion-Based Sequence Generation","abstract":"We present significant extensions to diffusion-based sequence generation models, blurring the line with autoregressive language models. We introduce hyperschedules, which assign distinct noise schedules to individual token positions, generalizing both autoregressive models (e.g., GPT) and conventional diffusion models (e.g., SEDD, MDLM) as special cases. Second, we propose two hybrid token-wise noising processes that interpolate between absorbing and uniform processes, enabling the model to fix past mistakes, and we introduce a novel inference algorithm that leverages this new feature in a simplified context inspired from MDLM. To support efficient training and inference, we design attention masks compatible with KV-caching. Our methods achieve state-of-the-art perplexity and generate diverse, high-quality sequences across standard benchmarks, suggesting a promising path for autoregressive diffusion-based sequence generation.","authors":["Nima Fathi","Torsten Scholak","Pierre-Andr\\'e No\\\"el"],"url":"https://arxiv.org/abs/2504.06416"}
{"created":"2025-04-11","title":"TRIDENT: Tri-modal Real-time Intrusion Detection Engine for New Targets","abstract":"The increasing availability of drones and their potential for malicious activities pose significant privacy and security risks, necessitating fast and reliable detection in real-world environments. However, existing drone detection systems often struggle in real-world settings due to environmental noise and sensor limitations. This paper introduces TRIDENT, a tri-modal drone detection framework that integrates synchronized audio, visual, and RF data to enhance robustness and reduce dependence on individual sensors. TRIDENT introduces two fusion strategies - Late Fusion and GMU Fusion - to improve multi-modal integration while maintaining efficiency. The framework incorporates domain-specific feature extraction techniques alongside a specialized data augmentation pipeline that simulates real-world sensor degradation to improve generalization capabilities. A diverse multi-sensor dataset is collected in urban and non-urban environments under varying lighting conditions, ensuring comprehensive evaluation. Experimental results show that TRIDENT achieves 98.8 percent accuracy in real-world recordings and 83.26 percent in a more complex setting (augmented data), outperforming unimodal and dual-modal baselines. Moreover, TRIDENT operates in real-time, detecting drones in just 6.09 ms while consuming only 75.27 mJ per detection, making it highly efficient for resource-constrained devices. The dataset and code have been released to ensure reproducibility (https://github.com/TRIDENT-2025/TRIDENT).","authors":["Ildi Alla","Selma Yahia","Valeria Loscri"],"url":"https://arxiv.org/abs/2504.06417"}
{"created":"2025-04-11","title":"Releasing Differentially Private Event Logs Using Generative Models","abstract":"In recent years, the industry has been witnessing an extended usage of process mining and automated event data analysis. Consequently, there is a rising significance in addressing privacy apprehensions related to the inclusion of sensitive and private information within event data utilized by process mining algorithms. State-of-the-art research mainly focuses on providing quantifiable privacy guarantees, e.g., via differential privacy, for trace variants that are used by the main process mining techniques, e.g., process discovery. However, privacy preservation techniques designed for the release of trace variants are still insufficient to meet all the demands of industry-scale utilization. Moreover, ensuring privacy guarantees in situations characterized by a high occurrence of infrequent trace variants remains a challenging endeavor. In this paper, we introduce two novel approaches for releasing differentially private trace variants based on trained generative models. With TraVaG, we leverage \\textit{Generative Adversarial Networks} (GANs) to sample from a privatized implicit variant distribution. Our second method employs \\textit{Denoising Diffusion Probabilistic Models} that reconstruct artificial trace variants from noise via trained Markov chains. Both methods offer industry-scale benefits and elevate the degree of privacy assurances, particularly in scenarios featuring a substantial prevalence of infrequent variants. Also, they overcome the shortcomings of conventional privacy preservation techniques, such as bounding the length of variants and introducing fake variants. Experimental results on real-life event data demonstrate that our approaches surpass state-of-the-art techniques in terms of privacy guarantees and utility preservation.","authors":["Frederik Wangelik","Majid Rafiei","Mahsa Pourbafrani","Wil M. P. van der Aalst"],"url":"https://arxiv.org/abs/2504.06418"}
{"created":"2025-04-11","title":"SPIRe: Boosting LLM Inference Throughput with Speculative Decoding","abstract":"Speculative decoding (SD) has been shown to reduce the latency of autoregressive decoding (AD) by 2-3x for small batch sizes. However, increasing throughput and therefore reducing the cost per token requires decoding with large batch sizes. Recent work shows that SD can accelerate decoding with large batch sizes too if the context is sufficiently long and the draft model's KV cache is sparse. We introduce SPIRe, a draft model that combines static sparse attention, pruned initialization, and feedback memory to increase the modeled throughput of speculative decoding by over 100% compared to speculation with a much smaller draft model and by over 35% compared to the strong baseline of sparse self-speculation. Our approach is particularly effective when context lengths vary significantly across requests.","authors":["Sanjit Neelam","Daniel Heinlein","Vaclav Cvicek","Akshay Mishra","Reiner Pope"],"url":"https://arxiv.org/abs/2504.06419"}
{"created":"2025-04-11","title":"We've Got You Covered: Type-Guided Repair of Incomplete Input Generators","abstract":"Property-based testing is a popular technique for automatically testing semantic properties of a program, specified as a pair of pre- and post-conditions. The efficacy of this approach depends on being able to quickly generate inputs that meet the precondition, in order to maximize the set of program behaviors that are probed. For semantically rich preconditions, purely random generation is unlikely to produce many valid inputs; when this occurs, users are forced to manually write their own specialized input generators. One common problem with handwritten generators is that they may be \\emph{incomplete}, i.e.,\\ they are unable to generate some values meeting the target precondition. This paper presents a novel program repair technique that patches an incomplete generator so that its range includes every valid input. Our approach uses a novel enumerative synthesis algorithm that leverages the recently developed notion of \\emph{coverage types} to characterize the set of missing test values as well as the coverage provided by candidate repairs. We have implemented a repair tool for OCaml input generators, called Cobb, and have used it to repair a suite of benchmarks drawn from the property-based testing literature.","authors":["Patrick LaFontaine","Zhe Zhou","Ashish Mishra","Suresh Jagannathan","Benjamin Delaware"],"url":"https://arxiv.org/abs/2504.06421"}
{"created":"2025-04-11","title":"Neural Network Enhanced Polyconvexification of Isotropic Energy Densities in Computational Mechanics","abstract":"We present a neural network approach for fast evaluation of parameter-dependent polyconvex envelopes, which are crucial in computational mechanics. Our method uses a neural network architecture that inherently encodes polyconvexity in the main variable by combining a feature extraction layer that computes the minors function on the signed singular value characterisation of isotropic energy densities with a partially input convex neural network (PICNN). Polyconvex underestimation is weakly enforced by penalisation during training, as are the symmetries of the function. As a guiding example, we focus on a well-known isotropic damage problem, reformulated in terms of signed singular values, and apply a splitting approach to reduce the dimensionality of the parameter space, thereby making training more tractable. Numerical experiments show that the networks achieve sufficient accuracy for engineering applications while providing high compression and significant speed-up over traditional polyconvexification schemes. Most importantly, the network adapts to varying physical or material parameters, enabling real-time polyconvexification in large-scale computational mechanics scenarios.","authors":["Lo\\\"ic Balazi","Timo Neumeier","Malte A. Peter","Daniel Peterseim"],"url":"https://arxiv.org/abs/2504.06425"}
{"created":"2025-04-11","title":"S'MoRE: Structural Mixture of Residual Experts for LLM Fine-tuning","abstract":"Fine-tuning pre-trained large language models (LLMs) presents a dual challenge of balancing parameter efficiency and model capacity. Existing methods like low-rank adaptations (LoRA) are efficient but lack flexibility, while Mixture-of-Experts (MoE) architectures enhance model capacity at the cost of more & under-utilized parameters. To address these limitations, we propose Structural Mixture of Residual Experts (S'MoRE), a novel framework that seamlessly integrates the efficiency of LoRA with the flexibility of MoE. Specifically, S'MoRE employs hierarchical low-rank decomposition of expert weights, yielding residuals of varying orders interconnected in a multi-layer structure. By routing input tokens through sub-trees of residuals, S'MoRE emulates the capacity of many experts by instantiating and assembling just a few low-rank matrices. We craft the inter-layer propagation of S'MoRE's residuals as a special type of Graph Neural Network (GNN), and prove that under similar parameter budget, S'MoRE improves \"structural flexibility\" of traditional MoE (or Mixture-of-LoRA) by exponential order. Comprehensive theoretical analysis and empirical results demonstrate that S'MoRE achieves superior fine-tuning performance, offering a transformative approach for efficient LLM adaptation.","authors":["Hanqing Zeng","Yinglong Xia","Zhuokai Zhao","Gilbert Jiang","Qiang Zhang","Jiayi Liu","Lizhu Zhang","Xiangjun Fan","Benyu Zhang"],"url":"https://arxiv.org/abs/2504.06426"}
{"created":"2025-04-11","title":"Extended Version: Multi-Robot Motion Planning with Cooperative Localization","abstract":"We consider the uncertain multi-robot motion planning (MRMP) problem with cooperative localization (CL-MRMP), under both motion and measurement noise, where each robot can act as a sensor for its nearby teammates. We formalize CL-MRMP as a chance-constrained motion planning problem, and propose a safety-guaranteed algorithm that explicitly accounts for robot-robot correlations. Our approach extends a sampling-based planner to solve CL-MRMP while preserving probabilistic completeness. To improve efficiency, we introduce novel biasing techniques. We evaluate our method across diverse benchmarks, demonstrating its effectiveness in generating motion plans, with significant performance gains from biasing strategies.","authors":["Anne Theurkauf","Nisar Ahmed","Morteza Lahijanian"],"url":"https://arxiv.org/abs/2504.06429"}
{"created":"2025-04-11","title":"Automatically Generating Single-Responsibility Unit Tests","abstract":"Automatic test generation aims to save developers time and effort by producing test suites with reasonably high coverage and fault detection. However, the focus of search-based generation tools in maximizing coverage leaves other properties, such as test quality, coincidental. The evidence shows that developers remain skeptical of using generated tests as they face understandability challenges. Generated tests do not follow a defined structure while evolving, which can result in tests that contain method calls to improve coverage but lack a clear relation to the generated assertions. In my doctoral research, I aim to investigate the effects of providing a pre-process structure to the generated tests, based on the single-responsibility principle to favor the identification of the focal method under test. To achieve this, we propose to implement different test representations for evolution and evaluate their impact on coverage, fault detection, and understandability. We hypothesize that improving the structure of generated tests will report positive effects on the tests' understandability without significantly affecting the effectiveness. We aim to conduct a quantitative analysis of this proposed approach as well as a developer evaluation of the understandability of these tests.","authors":["Geraldine Galindo-Gutierrez"],"url":"https://arxiv.org/abs/2504.06431"}
{"created":"2025-04-11","title":"D-Feat Occlusions: Diffusion Features for Robustness to Partial Visual Occlusions in Object Recognition","abstract":"Applications of diffusion models for visual tasks have been quite noteworthy. This paper targets making classification models more robust to occlusions for the task of object recognition by proposing a pipeline that utilizes a frozen diffusion model. Diffusion features have demonstrated success in image generation and image completion while understanding image context. Occlusion can be posed as an image completion problem by deeming the pixels of the occluder to be `missing.' We hypothesize that such features can help hallucinate object visual features behind occluding objects, and hence we propose using them to enable models to become more occlusion robust. We design experiments to include input-based augmentations as well as feature-based augmentations. Input-based augmentations involve finetuning on images where the occluder pixels are inpainted, and feature-based augmentations involve augmenting classification features with intermediate diffusion features. We demonstrate that our proposed use of diffusion-based features results in models that are more robust to partial object occlusions for both Transformers and ConvNets on ImageNet with simulated occlusions. We also propose a dataset that encompasses real-world occlusions and demonstrate that our method is more robust to partial object occlusions.","authors":["Rupayan Mallick","Sibo Dong","Nataniel Ruiz","Sarah Adel Bargal"],"url":"https://arxiv.org/abs/2504.06432"}
{"created":"2025-04-11","title":"Faster Algorithms for Reverse Shortest Path in Unit-Disk Graphs and Related Geometric Optimization Problems: Improving the Shrink-and-Bifurcate Technique","abstract":"In a series of papers, Avraham, Filtser, Kaplan, Katz, and Sharir (SoCG'14), Kaplan, Katz, Saban, and Sharir (ESA'23), and Katz, Saban, and Sharir (ESA'24) studied a class of geometric optimization problems -- including reverse shortest path in unweighted and weighted unit-disk graphs, discrete Fr\\'{e}chet distance with one-sided shortcuts, and reverse shortest path in visibility graphs on 1.5-dimensional terrains -- for which standard parametric search does not work well due to a lack of efficient parallel algorithms for the corresponding decision problems. The best currently known algorithms for all the above problems run in $O^*(n^{6/5})=O^*(n^{1.2})$ time (ignoring subpolynomial factors), and they were obtained using a technique called \\emph{shrink-and-bifurcate}. We improve the running time to $\\tilde{O}(n^{8/7}) \\approx O(n^{1.143})$ for these problems. Furthermore, specifically for reverse shortest path in unweighted unit-disk graphs, we improve the running time further to $\\tilde{O}(n^{9/8})=\\tilde{O}(n^{1.125})$.","authors":["Timothy M. Chan","Zhengcheng Huang"],"url":"https://arxiv.org/abs/2504.06434"}
{"created":"2025-04-11","title":"Human Trust in AI Search: A Large-Scale Experiment","abstract":"Large Language Models (LLMs) increasingly power generative search engines which, in turn, drive human information seeking and decision making at scale. The extent to which humans trust generative artificial intelligence (GenAI) can therefore influence what we buy, how we vote and our health. Unfortunately, no work establishes the causal effect of generative search designs on human trust. Here we execute ~12,000 search queries across seven countries, generating ~80,000 real-time GenAI and traditional search results, to understand the extent of current global exposure to GenAI search. We then use a preregistered, randomized experiment on a large study sample representative of the U.S. population to show that while participants trust GenAI search less than traditional search on average, reference links and citations significantly increase trust in GenAI, even when those links and citations are incorrect or hallucinated. Uncertainty highlighting, which reveals GenAI's confidence in its own conclusions, makes us less willing to trust and share generative information whether that confidence is high or low. Positive social feedback increases trust in GenAI while negative feedback reduces trust. These results imply that GenAI designs can increase trust in inaccurate and hallucinated information and reduce trust when GenAI's certainty is made explicit. Trust in GenAI varies by topic and with users' demographics, education, industry employment and GenAI experience, revealing which sub-populations are most vulnerable to GenAI misrepresentations. Trust, in turn, predicts behavior, as those who trust GenAI more click more and spend less time evaluating GenAI search results. These findings suggest directions for GenAI design to safely and productively address the AI \"trust gap.\"","authors":["Haiwen Li","Sinan Aral"],"url":"https://arxiv.org/abs/2504.06435"}
{"created":"2025-04-11","title":"Language-Dependent Political Bias in AI: A Study of ChatGPT and Gemini","abstract":"As leading examples of large language models, ChatGPT and Gemini claim to provide accurate and unbiased information, emphasizing their commitment to political neutrality and avoidance of personal bias. This research investigates the political tendency of large language models and the existence of differentiation according to the query language. For this purpose, ChatGPT and Gemini were subjected to a political axis test using 14 different languages. The findings of the study suggest that these large language models do exhibit political tendencies, with both models demonstrating liberal and leftist biases. A comparative analysis revealed that Gemini exhibited a more pronounced liberal and left-wing tendency compared to ChatGPT. The study also found that these political biases varied depending on the language used for inquiry. The study delves into the factors that constitute political tendencies and linguistic differentiation, exploring differences in the sources and scope of educational data, structural and grammatical features of languages, cultural and political contexts, and the model's response to linguistic features. From this standpoint, and an ethical perspective, it is proposed that artificial intelligence tools should refrain from asserting a lack of political tendencies and neutrality, instead striving for political neutrality and executing user queries by incorporating these tendencies.","authors":["Dogus Yuksel","Mehmet Cem Catalbas","Bora Oc"],"url":"https://arxiv.org/abs/2504.06436"}
{"created":"2025-04-11","title":"DBaS-Log-MPPI: Efficient and Safe Trajectory Optimization via Barrier States","abstract":"Optimizing trajectory costs for nonlinear control systems remains a significant challenge. Model Predictive Control (MPC), particularly sampling-based approaches such as the Model Predictive Path Integral (MPPI) method, has recently demonstrated considerable success by leveraging parallel computing to efficiently evaluate numerous trajectories. However, MPPI often struggles to balance safe navigation in constrained environments with effective exploration in open spaces, leading to infeasibility in cluttered conditions. To address these limitations, we propose DBaS-Log-MPPI, a novel algorithm that integrates Discrete Barrier States (DBaS) to ensure safety while enabling adaptive exploration with enhanced feasibility. Our method is efficiently validated through three simulation missions and one real-world experiment, involving a 2D quadrotor and a ground vehicle navigating through cluttered obstacles. We demonstrate that our algorithm surpasses both Vanilla MPPI and Log-MPPI, achieving higher success rates, lower tracking errors, and a conservative average speed.","authors":["Fanxin Wang","Haolong Jiang","Chuyuan Tao","Wenbin Wan","Yikun Cheng"],"url":"https://arxiv.org/abs/2504.06437"}
{"created":"2025-04-11","title":"Don't Let It Hallucinate: Premise Verification via Retrieval-Augmented Logical Reasoning","abstract":"Large language models (LLMs) have shown substantial capacity for generating fluent, contextually appropriate responses. However, they can produce hallucinated outputs, especially when a user query includes one or more false premises-claims that contradict established facts. Such premises can mislead LLMs into offering fabricated or misleading details. Existing approaches include pretraining, fine-tuning, and inference-time techniques that often rely on access to logits or address hallucinations after they occur. These methods tend to be computationally expensive, require extensive training data, or lack proactive mechanisms to prevent hallucination before generation, limiting their efficiency in real-time applications. We propose a retrieval-based framework that identifies and addresses false premises before generation. Our method first transforms a user's query into a logical representation, then applies retrieval-augmented generation (RAG) to assess the validity of each premise using factual sources. Finally, we incorporate the verification results into the LLM's prompt to maintain factual consistency in the final output. Experiments show that this approach effectively reduces hallucinations, improves factual accuracy, and does not require access to model logits or large-scale fine-tuning.","authors":["Yuehan Qin","Shawn Li","Yi Nian","Xinyan Velocity Yu","Yue Zhao","Xuezhe Ma"],"url":"https://arxiv.org/abs/2504.06438"}
{"created":"2025-04-11","title":"Graph Neural Network-Based Distributed Optimal Control for Linear Networked Systems: An Online Distributed Training Approach","abstract":"In this paper, we consider the distributed optimal control problem for linear networked systems. In particular, we are interested in learning distributed optimal controllers using graph recurrent neural networks (GRNNs). Most of the existing approaches result in centralized optimal controllers with offline training processes. However, as the increasing demand of network resilience, the optimal controllers are further expected to be distributed, and are desirable to be trained in an online distributed fashion, which are also the main contributions of our work. To solve this problem, we first propose a GRNN-based distributed optimal control method, and we cast the problem as a self-supervised learning problem. Then, the distributed online training is achieved via distributed gradient computation, and inspired by the (consensus-based) distributed optimization idea, a distributed online training optimizer is designed. Furthermore, the local closed-loop stability of the linear networked system under our proposed GRNN-based controller is provided by assuming that the nonlinear activation function of the GRNN-based controller is both local sector-bounded and slope-restricted. The effectiveness of our proposed method is illustrated by numerical simulations using a specifically developed simulator.","authors":["Zihao Song","Panos J. Antsaklis","Hai Lin"],"url":"https://arxiv.org/abs/2504.06439"}
{"created":"2025-04-11","title":"Classifying Subjective Time Perception in a Multi-robot Control Scenario Using Eye-tracking Information","abstract":"As automation and mobile robotics reshape work environments, rising expectations for productivity increase cognitive demands on human operators, leading to potential stress and cognitive overload. Accurately assessing an operator's mental state is critical for maintaining performance and well-being. We use subjective time perception, which can be altered by stress and cognitive load, as a sensitive, low-latency indicator of well-being and cognitive strain. Distortions in time perception can affect decision-making, reaction times, and overall task effectiveness, making it a valuable metric for adaptive human-swarm interaction systems.","authors":["Till Aust","Julian Kaduk","Heiko Hamann"],"url":"https://arxiv.org/abs/2504.06442"}
{"created":"2025-04-11","title":"cuTeSpMM: Accelerating Sparse-Dense Matrix Multiplication using GPU Tensor Cores","abstract":"Many recent GPUs feature matrix multiplication engines (aka Tensor Core Units or TCUs) that perform small fixed-size matrix-matrix products at very high throughput. They have been used very effectively to speed up dense matrix-matrix multiplication libraries like Nvidia's cuBLAS, enabling significantly higher performance over use of the traditional scalar GPU cores. There also been recent interest in using these dense TCUs for the important sparse-dense matrix-matrix multiplication (SpMM) kernel via explicit zero-filling.","authors":["Lizhi Xiang","Omid Asudeh","Gerald Sabin","Aravind Sukumaran-Rajam","P. Sadayappan"],"url":"https://arxiv.org/abs/2504.06443"}
{"created":"2025-04-11","title":"Can you Finetune your Binoculars? Embedding Text Watermarks into the Weights of Large Language Models","abstract":"The indistinguishability of AI-generated content from human text raises challenges in transparency and accountability. While several methods exist to watermark models behind APIs, embedding watermark strategies directly into model weights that are later reflected in the outputs of the model is challenging. In this study we propose a strategy to finetune a pair of low-rank adapters of a model, one serving as the text-generating model, and the other as the detector, so that a subtle watermark is embedded into the text generated by the first model and simultaneously optimized for detectability by the second. In this way, the watermarking strategy is fully learned end-to-end. This process imposes an optimization challenge, as balancing watermark robustness, naturalness, and task performance requires trade-offs. We discuss strategies on how to optimize this min-max objective and present results showing the effect of this modification to instruction finetuning.","authors":["Fay Elhassan","Niccol\\`o Ajroldi","Antonio Orvieto","Jonas Geiping"],"url":"https://arxiv.org/abs/2504.06446"}
{"created":"2025-04-11","title":"Federated Neural Architecture Search with Model-Agnostic Meta Learning","abstract":"Federated Learning (FL) often struggles with data heterogeneity due to the naturally uneven distribution of user data across devices. Federated Neural Architecture Search (NAS) enables collaborative search for optimal model architectures tailored to heterogeneous data to achieve higher accuracy. However, this process is time-consuming due to extensive search space and retraining. To overcome this, we introduce FedMetaNAS, a framework that integrates meta-learning with NAS within the FL context to expedite the architecture search by pruning the search space and eliminating the retraining stage. Our approach first utilizes the Gumbel-Softmax reparameterization to facilitate relaxation of the mixed operations in the search space. We then refine the local search process by incorporating Model-Agnostic Meta-Learning, where a task-specific learner adapts both weights and architecture parameters (alphas) for individual tasks, while a meta learner adjusts the overall model weights and alphas based on the gradient information from task learners. Following the meta-update, we propose soft pruning using the same trick on search space to gradually sparsify the architecture, ensuring that the performance of the chosen architecture remains robust after pruning which allows for immediate use of the model without retraining. Experimental evaluations demonstrate that FedMetaNAS significantly accelerates the search process by more than 50\\% with higher accuracy compared to FedNAS.","authors":["Xinyuan Huang","Jiechao Gao"],"url":"https://arxiv.org/abs/2504.06457"}
{"created":"2025-04-11","title":"Solving Power System Problems using Adiabatic Quantum Computing","abstract":"This letter proposes a novel combinatorial optimization framework that reformulates existing power system problems into a format executable on quantum annealers. The proposed framework accommodates both normal and complex numbers and enables efficient handling of large-scale problems, thus ensuring broad applicability across power system problems. As a proof of concept, we demonstrate its applicability in two classical problems: (i) power system parameter identification, where we estimate the admittance matrix given voltage and current measurements, and (ii) power flow analysis, where we reformulate the nonlinear equations governing active and reactive power balance. The results show that the proposed framework effectively and efficiently solves both linear and nonlinear power system problems, and thus offers significant advantages in scenarios where traditional solvers face challenges, such as ill-conditioned systems and fault conditions.","authors":["Zeynab Kaseb","Matthias Moller","Peter Palensky","Pedro P. Vergara"],"url":"https://arxiv.org/abs/2504.06458"}
{"created":"2025-04-11","title":"Can LLMs Simulate Personas with Reversed Performance? A Benchmark for Counterfactual Instruction Following","abstract":"Large Language Models (LLMs) are now increasingly widely used to simulate personas in virtual environments, leveraging their instruction-following capability. However, we discovered that even state-of-the-art LLMs cannot simulate personas with reversed performance (e.g., student personas with low proficiency in educational settings), which impairs the simulation diversity and limits the practical applications of the simulated environments. In this work, using mathematical reasoning as a representative scenario, we propose the first benchmark dataset for evaluating LLMs on simulating personas with reversed performance, a capability that we dub \"counterfactual instruction following\". We evaluate both open-weight and closed-source LLMs on this task and find that LLMs, including the OpenAI o1 reasoning model, all struggle to follow counterfactual instructions for simulating reversedly performing personas. Intersectionally simulating both the performance level and the race population of a persona worsens the effect even further. These results highlight the challenges of counterfactual instruction following and the need for further research.","authors":["Sai Adith Senthil Kumar","Hao Yan","Saipavan Perepa","Murong Yue","Ziyu Yao"],"url":"https://arxiv.org/abs/2504.06460"}
{"created":"2025-04-11","title":"Towards Intelligent VR Training: A Physiological Adaptation Framework for Cognitive Load and Stress Detection","abstract":"Adaptive Virtual Reality (VR) systems have the potential to enhance training and learning experiences by dynamically responding to users' cognitive states. This research investigates how eye tracking and heart rate variability (HRV) can be used to detect cognitive load and stress in VR environments, enabling real-time adaptation. The study follows a three-phase approach: (1) conducting a user study with the Stroop task to label cognitive load data and train machine learning models to detect high cognitive load, (2) fine-tuning these models with new users and integrating them into an adaptive VR system that dynamically adjusts training difficulty based on physiological signals, and (3) developing a privacy-aware approach to detect high cognitive load and compare this with the adaptive VR in Phase two. This research contributes to affective computing and adaptive VR using physiological sensing, with applications in education, training, and healthcare. Future work will explore scalability, real-time inference optimization, and ethical considerations in physiological adaptive VR.","authors":["Mahsa Nasri"],"url":"https://arxiv.org/abs/2504.06461"}
{"created":"2025-04-11","title":"Implementation of a Zed 2i Stereo Camera for High-Frequency Shoreline Change and Coastal Elevation Monitoring","abstract":"The increasing population, thus financial interests, in coastal areas have increased the need to monitor coastal elevation and shoreline change. Though several resources exist to obtain this information, they often lack the required temporal resolution for short-term monitoring (e.g., every hour). To address this issue, this study implements a low-cost ZED 2i stereo camera system and close-range photogrammetry to collect images for generating 3D point clouds, digital surface models (DSMs) of beach elevation, and georectified imagery at a localized scale and high temporal resolution. The main contributions of this study are (i) intrinsic camera calibration, (ii) georectification and registration of acquired imagery and point cloud, (iii) generation of the DSM of the beach elevation, and (iv) a comparison of derived products against those from uncrewed aircraft system structure-from-motion photogrammetry. Preliminary results show that despite its limitations, the ZED 2i can provide the desired mapping products at localized and high temporal scales. The system achieved a mean reprojection error of 0.20 px, a point cloud registration of 27 cm, a vertical error of 37.56 cm relative to ground truth, and georectification root mean square errors of 2.67 cm and 2.81 cm for x and y.","authors":["Jos\\'e A. Pilartes-Congo","Matthew Kastl","Michael J. Starek","Marina Vicens-Miquel","Philippe Tissot"],"url":"https://arxiv.org/abs/2504.06464"}
{"created":"2025-04-11","title":"Analyzing Examinee Comments using DistilBERT and Machine Learning to Ensure Quality Control in Exam Content","abstract":"This study explores using Natural Language Processing (NLP) to analyze candidate comments for identifying problematic test items. We developed and validated machine learning models that automatically identify relevant negative feedback, evaluated approaches of incorporating psychometric features enhances model performance, and compared NLP-flagged items with traditionally flagged items. Results demonstrate that candidate feedback provides valuable complementary information to statistical methods, potentially improving test validity while reducing manual review burden. This research offers testing organizations an efficient mechanism to incorporate direct candidate experience into quality assurance processes.","authors":["Ye (Cheryl)","Ma"],"url":"https://arxiv.org/abs/2504.06465"}
{"created":"2025-04-11","title":"ZETA: a library for Zonotope-based EsTimation and fAult diagnosis of discrete-time systems","abstract":"This paper introduces ZETA, a new MATLAB library for Zonotope-based EsTimation and fAult diagnosis of discrete-time systems. It features user-friendly implementations of set representations based on zonotopes, namely zonotopes, constrained zonotopes, and line zonotopes, in addition to a basic implementation of interval arithmetic. This library has capabilities starting from the basic set operations with these sets, including propagations through nonlinear functions using various approximation methods. The features of ZETA allow for reachability analysis and state estimation of discrete-time linear, nonlinear, and descriptor systems, in addition to active fault diagnosis of linear systems. Efficient order reduction methods are also implemented for the respective set representations. Some examples are presented in order to illustrate the functionalities of the new library.","authors":["Brenner S. Rego","Joseph K. Scott","Davide M. Raimondo","Marco H. Terra","Guilherme V. Raffo"],"url":"https://arxiv.org/abs/2504.06467"}
{"created":"2025-04-11","title":"Agent-Arena: A General Framework for Evaluating Control Algorithms","abstract":"Robotic research is inherently challenging, requiring expertise in diverse environments and control algorithms. Adapting algorithms to new environments often poses significant difficulties, compounded by the need for extensive hyper-parameter tuning in data-driven methods. To address these challenges, we present Agent-Arena, a Python framework designed to streamline the integration, replication, development, and testing of decision-making policies across a wide range of benchmark environments. Unlike existing frameworks, Agent-Arena is uniquely generalised to support all types of control algorithms and is adaptable to both simulation and real-robot scenarios. Please see our GitHub repository https://github.com/halid1020/agent-arena-v0.","authors":["Halid Abdulrahim Kadi","Kasim Terzi\\'c"],"url":"https://arxiv.org/abs/2504.06468"}
{"created":"2025-04-11","title":"Membrane: Accelerating Database Analytics with Bank-Level DRAM-PIM Filtering","abstract":"In-memory database query processing frequently involves substantial data transfers between the CPU and memory, leading to inefficiencies due to Von Neumann bottleneck. Processing-in-Memory (PIM) architectures offer a viable solution to alleviate this bottleneck. In our study, we employ a commonly used software approach that streamlines JOIN operations into simpler selection or filtering tasks using pre-join denormalization which makes query processing workload more amenable to PIM acceleration. This research explores DRAM design landscape to evaluate how effectively these filtering tasks can be efficiently executed across DRAM hierarchy and their effect on overall application speedup. We also find that operations such as aggregates are more suitably executed on the CPU rather than PIM. Thus, we propose a cooperative query processing framework that capitalizes on both CPU and PIM strengths, where (i) the DRAM-based PIM block, with its massive parallelism, supports scan operations while (ii) CPU, with its flexible architecture, supports the rest of query execution. This allows us to utilize both PIM and CPU where appropriate and prevent dramatic changes to the overall system architecture.","authors":["Akhil Shekar","Kevin Gaffney","Martin Prammer","Khyati Kiyawat","Lingxi Wu","Helena Caminal","Zhenxing Fan","Yimin Gao","Ashish Venkat","Jos\\'e F. Mart\\'inez","Jignesh Patel","Kevin Skadron"],"url":"https://arxiv.org/abs/2504.06473"}
{"created":"2025-04-11","title":"FETTA: Flexible and Efficient Hardware Accelerator for Tensorized Neural Network Training","abstract":"The increasing demand for on-device training of deep neural networks (DNNs) aims to leverage personal data for high-performance applications while addressing privacy concerns and reducing communication latency. However, resource-constrained platforms face significant challenges due to the intensive computational and memory demands of DNN training. Tensor decomposition emerges as a promising approach to compress model size without sacrificing accuracy. Nevertheless, training tensorized neural networks (TNNs) incurs non-trivial overhead and severe performance degradation on conventional accelerators due to complex tensor shaping requirements. To address these challenges, we propose FETTA, an algorithm and hardware co-optimization framework for efficient TNN training. On the algorithm side, we develop a contraction sequence search engine (CSSE) to identify the optimal contraction sequence with the minimal computational overhead. On the hardware side, FETTA features a flexible and efficient architecture equipped with a reconfigurable contraction engine (CE) array to support diverse dataflows. Furthermore, butterfly-based distribution and reduction networks are implemented to perform flexible tensor shaping operations during computation. Evaluation results demonstrate that FETTA achieves reductions of 20.5x/100.9x, 567.5x/45.03x, and 11609.7x/4544.8x in terms of processing latency, energy, and energy-delay product (EDP) over GPU and TPU, respectively. Moreover, working on the tensorized training, FETTA outperforms prior accelerators with a speedup of 3.87~14.63x, and an energy efficiency improvement of 1.41~2.73x on average.","authors":["Jinming Lu","Jiayi Tian","Hai Li","Ian Young","Zheng Zhang"],"url":"https://arxiv.org/abs/2504.06474"}
{"created":"2025-04-11","title":"Accelerating Hybrid XOR$-$CNF SAT Problems Natively with In-Memory Computing","abstract":"The Boolean satisfiability (SAT) problem is a computationally challenging decision problem central to many industrial applications. For SAT problems in cryptanalysis, circuit design, and telecommunication, solutions can often be found more efficiently by representing them with a combination of exclusive OR (XOR) and conjunctive normal form (CNF) clauses. We propose a hardware accelerator architecture that natively embeds and solves such hybrid CNF$-$XOR problems using in-memory computing hardware. To achieve this, we introduce an algorithm and demonstrate, both experimentally and through simulations, how it can be efficiently implemented with memristor crossbar arrays. Compared to the conventional approaches that translate CNF$-$XOR problems to pure CNF problems, our simulations show that the accelerator improves computation speed, energy efficiency, and chip area utilization by $\\sim$10$\\times$ for a set of hard cryptographic benchmarking problems. Moreover, the accelerator achieves a $\\sim$10$\\times$ speedup and a $\\sim$1000$\\times$ gain in energy efficiency over state-of-the-art SAT solvers running on CPUs.","authors":["Haesol Im","Fabian B\\\"ohm","Giacomo Pedretti","Noriyuki Kushida","Moslem Noori","Elisabetta Valiante","Xiangyi Zhang","Chan-Woo Yang","Tinish Bhattacharya","Xia Sheng","Jim Ignowski","Arne Heittmann","John Paul Strachan","Masoud Mohseni","Ray Beausoleil","Thomas Van Vaerenbergh","Ignacio Rozada"],"url":"https://arxiv.org/abs/2504.06476"}
{"created":"2025-04-11","title":"Spectrum Sharing by Space-Time Waveform Shaping","abstract":"In this paper, we consider the task of introducing a new wireless data link over a given occupied frequency band using a multi-antenna transmitter and receiver. We design formally a dynamic multiple-input multiple-output (MIMO) wireless link that can coexist in the fixed congested frequency band by (a) optimally avoiding sensed interference in the joint space-time domain, and (b) protecting existing links by minimizing its own transmitted power in the band. In particular, the transmit beam weight vector and time domain pulse code sequence are jointly optimized to minimize the transmit energy per bit per antenna, while maintaining a pre-defined signal-to-interference-plus-noise ratio (SINR) at the output of the joint space-time maximum SINR receiver filter. Extensive numerical studies are carried out to demonstrate the derived algorithmic solution in light and heavily congested band scenarios with non-cooperative co-channel links. We show that the proposed autonomously reconfigurable 4x4 MIMO link outperforms a non-adaptive transceiver and other forms of waveform shaping in terms of the pre-detection SINR performance and the capability to protect ongoing non-cooperative links by not occupying the band with redundant transmissions.","authors":["Hatef Nouri","George Sklivanitis","Dimitris A. Pados","Elizabeth Serena Bentley"],"url":"https://arxiv.org/abs/2504.06478"}
{"created":"2025-04-11","title":"Holistic Fusion: Task- and Setup-Agnostic Robot Localization and State Estimation with Factor Graphs","abstract":"Seamless operation of mobile robots in challenging environments requires low-latency local motion estimation (e.g., dynamic maneuvers) and accurate global localization (e.g., wayfinding). While most existing sensor-fusion approaches are designed for specific scenarios, this work introduces a flexible open-source solution for task- and setup-agnostic multimodal sensor fusion that is distinguished by its generality and usability. Holistic Fusion formulates sensor fusion as a combined estimation problem of i) the local and global robot state and ii) a (theoretically unlimited) number of dynamic context variables, including automatic alignment of reference frames; this formulation fits countless real-world applications without any conceptual modifications. The proposed factor-graph solution enables the direct fusion of an arbitrary number of absolute, local, and landmark measurements expressed with respect to different reference frames by explicitly including them as states in the optimization and modeling their evolution as random walks. Moreover, local smoothness and consistency receive particular attention to prevent jumps in the robot state belief. HF enables low-latency and smooth online state estimation on typical robot hardware while simultaneously providing low-drift global localization at the IMU measurement rate. The efficacy of this released framework is demonstrated in five real-world scenarios on three robotic platforms, each with distinct task requirements.","authors":["Julian Nubert","Turcan Tuna","Jonas Frey","Cesar Cadena","Katherine J. Kuchenbecker","Shehryar Khattak","Marco Hutter"],"url":"https://arxiv.org/abs/2504.06479"}
{"created":"2025-04-11","title":"Cooperative Dilemmas in Rational Debate","abstract":"As an epistemic activity, rational debate and discussion requires cooperation, yet involves a tension between collective and individual interests. While all participants benefit from collective outcomes like reaching consensus on true beliefs, individuals face personal costs when changing their minds. This creates an incentive for each debater to let others bear the cognitive burden of exploring alternative perspectives. We present a model to examine the strategic dynamics between debaters motivated by two competing goals: discovering truth and minimizing belief revisions. Our model demonstrates that this tension creates social dilemmas where strategies that are optimal for individuals systematically undermine the collective pursuit of truth. Paradoxically, our analysis reveals that increasing debaters' motivation to seek truth can sometimes produce equilibria with worse outcomes for collective truth discovery. These findings illuminate why rational debate can fail to achieve optimal epistemic outcomes, even when participants genuinely value truth.","authors":["Toby Handfield","Juli\\'an Garcia","Christian Hilbe","Shang Long Yeo"],"url":"https://arxiv.org/abs/2504.06485"}
{"created":"2025-04-11","title":"Mind the Gap: Evaluating Vision Systems in Small Data Applications","abstract":"The practical application of AI tools for specific computer vision tasks relies on the \"small-data regime\" of hundreds to thousands of labeled samples. This small-data regime is vital for applications requiring expensive expert annotations, such as ecological monitoring, medical diagnostics or industrial quality control. We find, however, that computer vision research has ignored the small data regime as evaluations increasingly focus on zero- and few-shot learning. We use the Natural World Tasks (NeWT) benchmark to compare multi-modal large language models (MLLMs) and vision-only methods across varying training set sizes. MLLMs exhibit early performance plateaus, while vision-only methods improve throughout the small-data regime, with performance gaps widening beyond 10 training examples. We provide the first comprehensive comparison between these approaches in small-data contexts and advocate for explicit small-data evaluations in AI research to better bridge theoretical advances with practical deployments.","authors":["Samuel Stevens","S M Rayeed","Jenna Kline"],"url":"https://arxiv.org/abs/2504.06486"}
{"created":"2025-04-11","title":"Exploiting Meta-Learning-based Poisoning Attacks for Graph Link Prediction","abstract":"Link prediction in graph data utilizes various algorithms and machine learning/deep learning models to predict potential relationships between graph nodes. This technique has found widespread use in numerous real-world applications, including recommendation systems, community networks, and biological structures. However, recent research has highlighted the vulnerability of link prediction models to adversarial attacks, such as poisoning and evasion attacks. Addressing the vulnerability of these models is crucial to ensure stable and robust performance in link prediction applications. While many works have focused on enhancing the robustness of the Graph Convolution Network (GCN) model, the Variational Graph Auto-Encoder (VGAE), a sophisticated model for link prediction, has not been thoroughly investigated in the context of graph adversarial attacks. To bridge this gap, this article proposes an unweighted graph poisoning attack approach using meta-learning techniques to undermine VGAE's link prediction performance. We conducted comprehensive experiments on diverse datasets to evaluate the proposed method and its parameters, comparing it with existing approaches in similar settings. Our results demonstrate that our approach significantly diminishes link prediction performance and outperforms other state-of-the-art methods.","authors":["Mingchen Li","Di Zhuang","Keyu Chen","Dumindu Samaraweera","Morris Chang"],"url":"https://arxiv.org/abs/2504.06492"}
{"created":"2025-04-11","title":"Towards Holistic Prompt Craft","abstract":"We present an account of an ongoing practice-based Design Research programme that explores the interaction affordances of real-time AI image generators. Based on our experiences from three installations, we reflect on the design of PromptJ, a user interface built around the concept of a prompt mixer. Our first contribution is a series of strong concepts based on our reflections of designing and deploying PromptJ. We cohere and abstract our strong concepts into the notion of Holistic Prompt Craft, which describes the importance of considering all relevant parameters concurrently. Finally, we present PromptTank, a prototype design which exemplifies the principles of Holistic Prompt Craft. Our contributions are articulated as strong concepts or intermediate knowledge that are intended to inform and inspire practitioners and researchers who are designing with image generation models or developing novel interaction paradigms for generative AI systems more generally.","authors":["Joseph Lindley","Roger Whitham"],"url":"https://arxiv.org/abs/2504.06496"}
{"created":"2025-04-11","title":"Data-driven Fuzzy Control for Time-Optimal Aggressive Trajectory Following","abstract":"Optimal trajectories that minimize a user-defined cost function in dynamic systems require the solution of a two-point boundary value problem. The optimization process yields an optimal control sequence that depends on the initial conditions and system parameters. However, the optimal sequence may result in undesirable behavior if the system's initial conditions and parameters are erroneous. This work presents a data-driven fuzzy controller synthesis framework that is guided by a time-optimal trajectory for multicopter tracking problems. In particular, we consider an aggressive maneuver consisting of a mid-air flip and generate a time-optimal trajectory by numerically solving the two-point boundary value problem. A fuzzy controller consisting of a stabilizing controller near hover conditions and an autoregressive moving average (ARMA) controller, trained to mimic the time-optimal aggressive trajectory, is constructed using the Takagi-Sugeno fuzzy framework.","authors":["August Phelps","Juan Augusto Paredes Salazar","Ankit Goel"],"url":"https://arxiv.org/abs/2504.06500"}
{"created":"2025-04-11","title":"Embedding Graphs as Euclidean kNN-Graphs","abstract":"Let G = (V, E) be a directed graph on n vertices where each vertex has out-degree k. We say that G is kNN-realizable in d-dimensional Euclidean space if there exists a point set P = {p1, p2, ..., pn} in R^d along with a one-to-one mapping phi: V -> P such that for any u, v in V, u is an out-neighbor of v in G if and only if phi(u) is one of the k nearest neighbors of phi(v); we call the map phi a \"kNN realization\" of G in R^d. The kNN realization problem, which aims to compute such a mapping in R^d, is known to be NP-hard already for d = 2 and k = 1 (Eades and Whitesides, Theoretical Computer Science, 1996), and to the best of our knowledge, has not been studied in dimension d = 1. The main results of this paper are the following: (1) For any fixed dimension d >= 2, we can efficiently compute an embedding realizing at least a (1 - epsilon) fraction of G's edges, or conclude that G is not kNN-realizable in R^d. (2) For d = 1, we can decide in O(kn) time whether G is kNN-realizable and, if so, compute a realization in O(n^{2.5} * polylog(n)) time.","authors":["T. Schibler","S. Suri","J. Xue"],"url":"https://arxiv.org/abs/2504.06503"}
{"created":"2025-04-11","title":"STaR: Seamless Spatial-Temporal Aware Motion Retargeting with Penetration and Consistency Constraints","abstract":"Motion retargeting seeks to faithfully replicate the spatio-temporal motion characteristics of a source character onto a target character with a different body shape. Apart from motion semantics preservation, ensuring geometric plausibility and maintaining temporal consistency are also crucial for effective motion retargeting. However, many existing methods prioritize either geometric plausibility or temporal consistency. Neglecting geometric plausibility results in interpenetration while neglecting temporal consistency leads to motion jitter. In this paper, we propose a novel sequence-to-sequence model for seamless Spatial-Temporal aware motion Retargeting (STaR), with penetration and consistency constraints. STaR consists of two modules: (1) a spatial module that incorporates dense shape representation and a novel limb penetration constraint to ensure geometric plausibility while preserving motion semantics, and (2) a temporal module that utilizes a temporal transformer and a novel temporal consistency constraint to predict the entire motion sequence at once while enforcing multi-level trajectory smoothness. The seamless combination of the two modules helps us achieve a good balance between the semantic, geometric, and temporal targets. Extensive experiments on the Mixamo and ScanRet datasets demonstrate that our method produces plausible and coherent motions while significantly reducing interpenetration rates compared with other approaches.","authors":["Xiaohang Yang","Qing Wang","Jiahao Yang","Gregory Slabaugh","Shanxin Yuan"],"url":"https://arxiv.org/abs/2504.06504"}
{"created":"2025-04-11","title":"GTS-LUM: Reshaping User Behavior Modeling with LLMs in Telecommunications Industry","abstract":"As telecommunication service providers shifting their focus to analyzing user behavior for package design and marketing interventions, a critical challenge lies in developing a unified, end-to-end framework capable of modeling long-term and periodic user behavior sequences with diverse time granularities, multi-modal data inputs, and heterogeneous labels. This paper introduces GTS-LUM, a novel user behavior model that redefines modeling paradigms in telecommunication settings. GTS-LUM adopts a (multi-modal) encoder-adapter-LLM decoder architecture, enhanced with several telecom-specific innovations. Specifically, the model incorporates an advanced timestamp processing method to handle varying time granularities. It also supports multi-modal data inputs -- including structured tables and behavior co-occurrence graphs -- and aligns these with semantic information extracted by a tokenizer using a Q-former structure. Additionally, GTS-LUM integrates a front-placed target-aware mechanism to highlight historical behaviors most relevant to the target. Extensive experiments on industrial dataset validate the effectiveness of this end-to-end framework and also demonstrate that GTS-LUM outperforms LLM4Rec approaches which are popular in recommendation systems, offering an effective and generalizing solution for user behavior modeling in telecommunications.","authors":["Liu Shi","Tianwu Zhou","Wei Xu","Li Liu","Zhexin Cui","Shaoyi Liang","Haoxing Niu","Yichong Tian","Jianwei Guo"],"url":"https://arxiv.org/abs/2504.06511"}
{"created":"2025-04-11","title":"ICPS: Real-Time Resource Configuration for Cloud Serverless Functions Considering Affinity","abstract":"Serverless computing, with its operational simplicity and on-demand scalability, has become a preferred paradigm for deploying workflow applications. However, resource allocation for workflows, particularly those with branching structures, is complicated by cold starts and network delays between dependent functions, significantly degrading execution efficiency and response times. In this paper, we propose the Invocation Concurrency Prediction-Based Scaling (ICPS) algorithm to address these challenges. ICPS employs Long Short-Term Memory (LSTM) networks to predict function concurrency, dynamically pre-warming function instances, and an affinity-based deployment strategy to co-locate dependent functions on the same worker node, minimizing network latency. The experimental results demonstrate that ICPS consistently outperforms existing approaches in diverse scenarios. The results confirm ICPS as a robust and scalable solution for optimizing serverless workflow execution.","authors":["Long Chen","Xinshuai Hua","Jinquan Zhang","Wenshuai Li","Xiaoping Li","Shijie Guo"],"url":"https://arxiv.org/abs/2504.06512"}
{"created":"2025-04-11","title":"Safe Navigation in Uncertain Crowded Environments Using Risk Adaptive CVaR Barrier Functions","abstract":"Robot navigation in dynamic, crowded environments poses a significant challenge due to the inherent uncertainties in the obstacle model. In this work, we propose a risk-adaptive approach based on the Conditional Value-at-Risk Barrier Function (CVaR-BF), where the risk level is automatically adjusted to accept the minimum necessary risk, achieving a good performance in terms of safety and optimization feasibility under uncertainty. Additionally, we introduce a dynamic zone-based barrier function which characterizes the collision likelihood by evaluating the relative state between the robot and the obstacle. By integrating risk adaptation with this new function, our approach adaptively expands the safety margin, enabling the robot to proactively avoid obstacles in highly dynamic environments. Comparisons and ablation studies demonstrate that our method outperforms existing social navigation approaches, and validate the effectiveness of our proposed framework.","authors":["Xinyi Wang","Taekyung Kim","Bardh Hoxha","Georgios Fainekos","Dimitra Panagou"],"url":"https://arxiv.org/abs/2504.06513"}
{"created":"2025-04-11","title":"Missing Premise exacerbates Overthinking: Are Reasoning Models losing Critical Thinking Skill?","abstract":"We find that the response length of reasoning LLMs, whether trained by reinforcement learning or supervised learning, drastically increases for ill-posed questions with missing premises (MiP), ending up with redundant and ineffective thinking. This newly introduced scenario exacerbates the general overthinking issue to a large extent, which we name as the MiP-Overthinking. Such failures are against the ``test-time scaling law'' but have been widely observed on multiple datasets we curated with MiP, indicating the harm of cheap overthinking and a lack of critical thinking. Surprisingly, LLMs not specifically trained for reasoning exhibit much better performance on the MiP scenario, producing much shorter responses that quickly identify ill-posed queries. This implies a critical flaw of the current training recipe for reasoning LLMs, which does not encourage efficient thinking adequately, leading to the abuse of thinking patterns. To further investigate the reasons behind such failures, we conduct fine-grained analyses of the reasoning length, overthinking patterns, and location of critical thinking on different types of LLMs. Moreover, our extended ablation study reveals that the overthinking is contagious through the distillation of reasoning models' responses. These results improve the understanding of overthinking and shed novel insights into mitigating the problem.","authors":["Chenrui Fan","Ming Li","Lichao Sun","Tianyi Zhou"],"url":"https://arxiv.org/abs/2504.06514"}
{"created":"2025-04-11","title":"Can dialogues with AI systems help humans better discern visual misinformation?","abstract":"The widespread emergence of manipulated news media content poses significant challenges to online information integrity. This study investigates whether dialogues with AI about AI-generated images and associated news statements can increase human discernment abilities and foster short-term learning in detecting misinformation. We conducted a study with 80 participants who engaged in structured dialogues with an AI system about news headline-image pairs, generating 1,310 human-AI dialogue exchanges. Results show that AI interaction significantly boosts participants' accuracy in identifying real versus fake news content from approximately 60\\% to 90\\% (p$<$0.001). However, these improvements do not persist when participants are presented with new, unseen image-statement pairs without AI assistance, with accuracy returning to baseline levels (~60\\%, p=0.88). These findings suggest that while AI systems can effectively change immediate beliefs about specific content through persuasive dialogue, they may not produce lasting improvements that transfer to novel examples, highlighting the need for developing more effective interventions that promote durable learning outcomes.","authors":["Anku Rani","Valdemar Danry","Andy Lippman","Pattie Maes"],"url":"https://arxiv.org/abs/2504.06517"}
{"created":"2025-04-11","title":"DUKAE: DUal-level Knowledge Accumulation and Ensemble for Pre-Trained Model-Based Continual Learning","abstract":"Pre-trained model-based continual learning (PTMCL) has garnered growing attention, as it enables more rapid acquisition of new knowledge by leveraging the extensive foundational understanding inherent in pre-trained model (PTM). Most existing PTMCL methods use Parameter-Efficient Fine-Tuning (PEFT) to learn new knowledge while consolidating existing memory. However, they often face some challenges. A major challenge lies in the misalignment of classification heads, as the classification head of each task is trained within a distinct feature space, leading to inconsistent decision boundaries across tasks and, consequently, increased forgetting. Another critical limitation stems from the restricted feature-level knowledge accumulation, with feature learning typically restricted to the initial task only, which constrains the model's representation capabilities. To address these issues, we propose a method named DUal-level Knowledge Accumulation and Ensemble (DUKAE) that leverages both feature-level and decision-level knowledge accumulation by aligning classification heads into a unified feature space through Gaussian distribution sampling and introducing an adaptive expertise ensemble to fuse knowledge across feature subspaces.Extensive experiments on CIFAR-100, ImageNet-R, CUB-200, and Cars-196 datasets demonstrate the superior performance of our approach.","authors":["Songze Li","Tonghua Su","Xu-Yao Zhang","Qixing Xu","Zhongjie Wang"],"url":"https://arxiv.org/abs/2504.06521"}
{"created":"2025-04-11","title":"Controller Distillation Reduces Fragile Brain-Body Co-Adaptation and Enables Migrations in MAP-Elites","abstract":"Brain-body co-optimization suffers from fragile co-adaptation where brains become over-specialized for particular bodies, hindering their ability to transfer well to others. Evolutionary algorithms tend to discard such low-performing solutions, eliminating promising morphologies. Previous work considered applying MAP-Elites, where niche descriptors are based on morphological features, to promote better search over morphology space. In this work, we show that this approach still suffers from fragile co-adaptation: where a core mechanism of MAP-Elites, creating stepping stones through solutions that migrate from one niche to another, is disrupted. We suggest that this disruption occurs because the body mutations that move an offspring to a new morphological niche break the robots' fragile brain-body co-adaptation and thus significantly decrease the performance of those potential solutions -- reducing their likelihood of outcompeting an existing elite in that new niche. We utilize a technique, we call Pollination, that periodically replaces the controllers of certain solutions with a distilled controller with better generalization across morphologies to reduce fragile brain-body co-adaptation and thus promote MAP-Elites migrations. Pollination increases the success of body mutations and the number of migrations, resulting in better quality-diversity metrics. We believe we develop important insights that could apply to other domains where MAP-Elites is used.","authors":["Alican Mertan","Nick Cheney"],"url":"https://arxiv.org/abs/2504.06523"}
{"created":"2025-04-11","title":"The Power of the Pareto Front: Balancing Uncertain Rewards for Adaptive Experimentation in scanning probe microscopy","abstract":"Automated experimentation has the potential to revolutionize scientific discovery, but its effectiveness depends on well-defined optimization targets, which are often uncertain or probabilistic in real-world settings. In this work, we demonstrate the application of Multi-Objective Bayesian Optimization (MOBO) to balance multiple, competing rewards in autonomous experimentation. Using scanning probe microscopy (SPM) imaging, one of the most widely used and foundational SPM modes, we show that MOBO can optimize imaging parameters to enhance measurement quality, reproducibility, and efficiency. A key advantage of this approach is the ability to compute and analyze the Pareto front, which not only guides optimization but also provides physical insights into the trade-offs between different objectives. Additionally, MOBO offers a natural framework for human-in-the-loop decision-making, enabling researchers to fine-tune experimental trade-offs based on domain expertise. By standardizing high-quality, reproducible measurements and integrating human input into AI-driven optimization, this work highlights MOBO as a powerful tool for advancing autonomous scientific discovery.","authors":["Yu Liu","Sergei V. Kalinin"],"url":"https://arxiv.org/abs/2504.06525"}
{"created":"2025-04-11","title":"TSP-OCS: A Time-Series Prediction for Optimal Camera Selection in Multi-Viewpoint Surgical Video Analysis","abstract":"Recording the open surgery process is essential for educational and medical evaluation purposes; however, traditional single-camera methods often face challenges such as occlusions caused by the surgeon's head and body, as well as limitations due to fixed camera angles, which reduce comprehensibility of the video content. This study addresses these limitations by employing a multi-viewpoint camera recording system, capturing the surgical procedure from six different angles to mitigate occlusions. We propose a fully supervised learning-based time series prediction method to choose the best shot sequences from multiple simultaneously recorded video streams, ensuring optimal viewpoints at each moment. Our time series prediction model forecasts future camera selections by extracting and fusing visual and semantic features from surgical videos using pre-trained models. These features are processed by a temporal prediction network with TimeBlocks to capture sequential dependencies. A linear embedding layer reduces dimensionality, and a Softmax classifier selects the optimal camera view based on the highest probability. In our experiments, we created five groups of open thyroidectomy videos, each with simultaneous recordings from six different angles. The results demonstrate that our method achieves competitive accuracy compared to traditional supervised methods, even when predicting over longer time horizons. Furthermore, our approach outperforms state-of-the-art time series prediction techniques on our dataset. This manuscript makes a unique contribution by presenting an innovative framework that advances surgical video analysis techniques, with significant implications for improving surgical education and patient safety.","authors":["Xinyu Liu","Xiaoguang Lin","Xiang Liu","Yong Yang","Hongqian Wang","Qilong Sun"],"url":"https://arxiv.org/abs/2504.06527"}
{"created":"2025-04-11","title":"CDER: Collaborative Evidence Retrieval for Document-level Relation Extraction","abstract":"Document-level Relation Extraction (DocRE) involves identifying relations between entities across multiple sentences in a document. Evidence sentences, crucial for precise entity pair relationships identification, enhance focus on essential text segments, improving DocRE performance. However, existing evidence retrieval systems often overlook the collaborative nature among semantically similar entity pairs in the same document, hindering the effectiveness of the evidence retrieval task. To address this, we propose a novel evidence retrieval framework, namely CDER. CDER employs an attentional graph-based architecture to capture collaborative patterns and incorporates a dynamic sub-structure for additional robustness in evidence retrieval. Experimental results on the benchmark DocRE dataset show that CDER not only excels in the evidence retrieval task but also enhances overall performance of existing DocRE system.","authors":["Khai Phan Tran","Xue Li"],"url":"https://arxiv.org/abs/2504.06529"}
{"created":"2025-04-11","title":"Beyond Moore's Law: Harnessing the Redshift of Generative AI with Effective Hardware-Software Co-Design","abstract":"For decades, Moore's Law has served as a steadfast pillar in computer architecture and system design, promoting a clear abstraction between hardware and software. This traditional Moore's computing paradigm has deepened the rift between the two, enabling software developers to achieve near-exponential performance gains often without needing to delve deeply into hardware-specific optimizations. Yet today, Moore's Law -- with its once relentless performance gains now diminished to incremental improvements -- faces inevitable physical barriers. This stagnation necessitates a reevaluation of the conventional system design philosophy. The traditional decoupled system design philosophy, which maintains strict abstractions between hardware and software, is increasingly obsolete. The once-clear boundary between software and hardware is rapidly dissolving, replaced by co-design. It is imperative for the computing community to intensify its commitment to hardware-software co-design, elevating system abstractions to first-class citizens and reimagining design principles to satisfy the insatiable appetite of modern computing. Hardware-software co-design is not a recent innovation. To illustrate its historical evolution, I classify its development into five relatively distinct ``epochs''. This post also highlights the growing influence of the architecture community in interdisciplinary teams -- particularly alongside ML researchers -- and explores why current co-design paradigms are struggling in today's computing landscape. Additionally, I will examine the concept of the ``hardware lottery'' and explore directions to mitigate its constraining influence on the next era of computing innovation.","authors":["Amir Yazdanbakhsh"],"url":"https://arxiv.org/abs/2504.06531"}
{"created":"2025-04-11","title":"WaveHiTS: Wavelet-Enhanced Hierarchical Time Series Modeling for Wind Direction Nowcasting in Eastern Inner Mongolia","abstract":"Wind direction forecasting plays a crucial role in optimizing wind energy production, but faces significant challenges due to the circular nature of directional data, error accumulation in multi-step forecasting, and complex meteorological interactions. This paper presents a novel model, WaveHiTS, which integrates wavelet transform with Neural Hierarchical Interpolation for Time Series to address these challenges. Our approach decomposes wind direction into U-V components, applies wavelet transform to capture multi-scale frequency patterns, and utilizes a hierarchical structure to model temporal dependencies at multiple scales, effectively mitigating error propagation. Experiments conducted on real-world meteorological data from Inner Mongolia, China demonstrate that WaveHiTS significantly outperforms deep learning models (RNN, LSTM, GRU), transformer-based approaches (TFT, Informer, iTransformer), and hybrid models (EMD-LSTM). The proposed model achieves RMSE values of approximately 19.2{\\deg}-19.4{\\deg} compared to 56{\\deg}-64{\\deg} for deep learning recurrent models, maintaining consistent accuracy across all forecasting steps up to 60 minutes ahead. Moreover, WaveHiTS demonstrates superior robustness with vector correlation coefficients (VCC) of 0.985-0.987 and hit rates of 88.5%-90.1%, substantially outperforming baseline models. Ablation studies confirm that each component-wavelet transform, hierarchical structure, and U-V decomposition-contributes meaningfully to overall performance. These improvements in wind direction nowcasting have significant implications for enhancing wind turbine yaw control efficiency and grid integration of wind energy.","authors":["Hailong Shu","Weiwei Song","Yue Wang","Jiping Zhang"],"url":"https://arxiv.org/abs/2504.06532"}
{"created":"2025-04-11","title":"Flexible Graph Similarity Computation With A Proactive Optimization Strategy","abstract":"Graph Edit Distance (GED) is an important similarity measure in graph retrieval, which quantifies the minimum cost of transforming one graph into another through edit operations, and offers flexibility by allowing customizable operation costs. Recent learning-based approaches approximate GEDs with the distances between representations in vector spaces. However, these methods often struggle with varying operation costs due to neglecting the impact of these costs on determining optimal graph mappings. Furthermore, they rely on isolated node distances as guidance, necessitating inefficient reactive refinements of mappings. To address these issues, we propose Graph Edit Network (GEN), a novel learning-based approach for flexible GED computation. By identifying the limitations of existing methods in capturing flexibility of GED, we introduce a principled yet simple solution that incorporates the operation costs before establishing mappings. To improve matching efficiency, we propose a strategy that proactively optimizes guidance from a graph perspective. This strategy initializes guidance as each node's alignment difficulty and captures the interdependencies between matches within and across graphs through a difficulty propagation mechanism, enabling more informed decisions. As a result, GEN selects optimal matches in a single step, minimizing the need for costly refinements. Results on real-world and synthetic datasets demonstrate the effectiveness, time efficiency, and adaptability of GEN, achieving up to 37.8\\% error reduction and 72.7\\% inference time reduction compared with state-of-the-art models, while performing robustly under varying cost settings and graph sizes.","authors":["Zhouyang Liu","Ning Liu","Yixin Chen","Jiezhong He","Dongsheng Li"],"url":"https://arxiv.org/abs/2504.06533"}
{"created":"2025-04-11","title":"Single-Source Shortest Path Problem in Weighted Disk Graphs","abstract":"In this paper, we present efficient algorithms for the single-source shortest path problem in weighted disk graphs. A disk graph is the intersection graph of a family of disks in the plane. Here, the weight of an edge is defined as the Euclidean distance between the centers of the disks corresponding to the endpoints of the edge. Given a family of $n$ disks in the plane whose radii lie in $[1,\\Psi]$ and a source disk, we can compute a shortest path tree from a source vertex in the weighted disk graph in $O(n\\log^2 n \\log \\Psi)$ time. Moreover, in the case that the radii of disks are arbitrarily large, we can compute a shortest path tree from a source vertex in the weighted disk graph in $O(n\\log^4 n)$ time. This improves the best-known algorithm running in $O(n\\log^6 n)$ time presented in ESA'23.","authors":["Shinwoo An","Eunjin Oh","Jie Xue"],"url":"https://arxiv.org/abs/2504.06534"}
{"created":"2025-04-11","title":"Lugha-Llama: Adapting Large Language Models for African Languages","abstract":"Large language models (LLMs) have achieved impressive results in a wide range of natural language applications. However, they often struggle to recognize low-resource languages, in particular African languages, which are not well represented in large training corpora. In this paper, we consider how to adapt LLMs to low-resource African languages. We find that combining curated data from African languages with high-quality English educational texts results in a training mix that substantially improves the model's performance on these languages. On the challenging IrokoBench dataset, our models consistently achieve the best performance amongst similarly sized baselines, particularly on knowledge-intensive multiple-choice questions (AfriMMLU). Additionally, on the cross-lingual question answering benchmark AfriQA, our models outperform the base model by over 10%. To better understand the role of English data during training, we translate a subset of 200M tokens into Swahili language and perform an analysis which reveals that the content of these data is primarily responsible for the strong performance. We release our models and data to encourage future research on African languages.","authors":["Happy Buzaaba","Alexander Wettig","David Ifeoluwa Adelani","Christiane Fellbaum"],"url":"https://arxiv.org/abs/2504.06536"}
{"created":"2025-04-11","title":"OPAL: Encoding Causal Understanding of Physical Systems for Robot Learning","abstract":"We present OPAL (Operant Physical Agent with Language), a novel vision-language-action architecture that introduces topological constraints to flow matching for robotic control. To do so, we further introduce topological attention. Our approach models action sequences as topologically-structured representations with non-trivial constraints. Experimental results across 10 complex manipulation tasks demonstrate OPAL's superior performance compared to previous approaches, including Octo, OpenVLA, and ${\\pi}$0.","authors":["Daniel Tcheurekdjian","Joshua Klasmeier","Tom Cooney","Christopher McCann","Tyler Fenstermaker"],"url":"https://arxiv.org/abs/2504.06538"}
{"created":"2025-04-11","title":"Data-Driven Reachability with Scenario Optimization and the Holdout Method","abstract":"Reachability analysis is an important method in providing safety guarantees for systems with unknown or uncertain dynamics. Due to the computational intractability of exact reachability analysis for general nonlinear, high-dimensional systems, recent work has focused on the use of probabilistic methods for computing approximate reachable sets. In this work, we advocate for the use of a general purpose, practical, and sharp method for data-driven reachability: the holdout method. Despite the simplicity of the holdout method, we show -- on several numerical examples including scenario-based reach tubes -- that the resulting probabilistic bounds are substantially sharper and require fewer samples than existing methods for data-driven reachability. Furthermore, we complement our work with a discussion on the necessity of probabilistic reachability bounds. We argue that any method that attempts to de-randomize the bounds, by converting the guarantees to hold deterministically, requires (a) an exponential in state-dimension amount of samples to achieve non-vacuous guarantees, and (b) extra assumptions on the dynamics.","authors":["Elizabeth Dietrich","Rosalyn Devonport","Stephen Tu","Murat Arcak"],"url":"https://arxiv.org/abs/2504.06541"}
{"created":"2025-04-11","title":"Polygon: Symbolic Reasoning for SQL using Conflict-Driven Under-Approximation Search","abstract":"We present a novel symbolic reasoning engine for SQL which can efficiently generate an input $I$ for $n$ queries $P_1, \\cdots, P_n$, such that their outputs on $I$ satisfy a given property (expressed in SMT). This is useful in different contexts, such as disproving equivalence of two SQL queries and disambiguating a set of queries. Our first idea is to reason about an under-approximation of each $P_i$ -- that is, a subset of $P_i$'s input-output behaviors. While it makes our approach both semantics-aware and lightweight, this idea alone is incomplete (as a fixed under-approximation might miss some behaviors of interest). Therefore, our second idea is to perform search over an expressive family of under-approximations (which collectively cover all program behaviors of interest), thereby making our approach complete. We have implemented these ideas in a tool, Polygon, and evaluated it on over 30,000 benchmarks across two tasks (namely, SQL equivalence refutation and query disambiguation). Our evaluation results show that Polygon significantly outperforms all prior techniques.","authors":["Pinhan Zhao","Yuepeng Wang","Xinyu Wang"],"url":"https://arxiv.org/abs/2504.06542"}
{"created":"2025-04-11","title":"DiffusionCom: Structure-Aware Multimodal Diffusion Model for Multimodal Knowledge Graph Completion","abstract":"Most current MKGC approaches are predominantly based on discriminative models that maximize conditional likelihood. These approaches struggle to efficiently capture the complex connections in real-world knowledge graphs, thereby limiting their overall performance. To address this issue, we propose a structure-aware multimodal Diffusion model for multimodal knowledge graph Completion (DiffusionCom). DiffusionCom innovatively approaches the problem from the perspective of generative models, modeling the association between the $(head, relation)$ pair and candidate tail entities as their joint probability distribution $p((head, relation), (tail))$, and framing the MKGC task as a process of gradually generating the joint probability distribution from noise. Furthermore, to fully leverage the structural information in MKGs, we propose Structure-MKGformer, an adaptive and structure-aware multimodal knowledge representation learning method, as the encoder for DiffusionCom. Structure-MKGformer captures rich structural information through a multimodal graph attention network (MGAT) and adaptively fuses it with entity representations, thereby enhancing the structural awareness of these representations. This design effectively addresses the limitations of existing MKGC methods, particularly those based on multimodal pre-trained models, in utilizing structural information. DiffusionCom is trained using both generative and discriminative losses for the generator, while the feature extractor is optimized exclusively with discriminative loss. This dual approach allows DiffusionCom to harness the strengths of both generative and discriminative models. Extensive experiments on the FB15k-237-IMG and WN18-IMG datasets demonstrate that DiffusionCom outperforms state-of-the-art models.","authors":["Wei Huang","Meiyu Liang","Peining Li","Xu Hou","Yawen Li","Junping Du","Zhe Xue","Zeli Guan"],"url":"https://arxiv.org/abs/2504.06543"}
{"created":"2025-04-11","title":"LCGC: Learning from Consistency Gradient Conflicting for Class-Imbalanced Semi-Supervised Debiasing","abstract":"Classifiers often learn to be biased corresponding to the class-imbalanced dataset, especially under the semi-supervised learning (SSL) set. While previous work tries to appropriately re-balance the classifiers by subtracting a class-irrelevant image's logit, but lacks a firm theoretical basis. We theoretically analyze why exploiting a baseline image can refine pseudo-labels and prove that the black image is the best choice. We also indicated that as the training process deepens, the pseudo-labels before and after refinement become closer. Based on this observation, we propose a debiasing scheme dubbed LCGC, which Learning from Consistency Gradient Conflicting, by encouraging biased class predictions during training. We intentionally update the pseudo-labels whose gradient conflicts with the debiased logits, representing the optimization direction offered by the over-imbalanced classifier predictions. Then, we debiased the predictions by subtracting the baseline image logits during testing. Extensive experiments demonstrate that LCGC can significantly improve the prediction accuracy of existing CISSL models on public benchmarks.","authors":["Weiwei Xing","Yue Cheng","Hongzhu Yi","Xiaohui Gao","Xiang Wei","Xiaoyu Guo","Yuming Zhang","Xinyu Pang"],"url":"https://arxiv.org/abs/2504.06544"}
{"created":"2025-04-11","title":"Several new infinite families of NMDS codes with arbitrary dimensions supporting $t$-designs","abstract":"Near maximum distance separable (NMDS) codes, where both the code and its dual are almost maximum distance separable, play pivotal roles in combinatorial design theory and cryptographic applications. Despite progress in fixed dimensions (e.g., dimension 4 codes by Ding and Tang \\cite{Ding2020}), constructing NMDS codes with arbitrary dimensions supporting $t$-designs ($t\\geq 2$) has remained open. In this paper, we construct two infinite families of NMDS codes over $\\mathbb{F}_q$ for any prime power $q$ with flexible dimensions and determine their weight distributions. Further, two additional families with arbitrary dimensions over $\\mathbb{F}_{2^m}$ supporting $2$-designs and $3$-designs, and their weight distributions are obtained. Our results fully generalize prior fixed-dimension works~\\cite{DingY2024,Heng2023,Heng20231,Xu2022}, and affirmatively settle the Heng-Wang conjecture \\cite{Heng2023} on the existence of NMDS codes with flexible parameters supporting $2$-designs.","authors":["Yaozong Zhang","Dabin Zheng","Xiaoqiang Wang","Wei Lu"],"url":"https://arxiv.org/abs/2504.06546"}
{"created":"2025-04-11","title":"Societal Impacts Research Requires Benchmarks for Creative Composition Tasks","abstract":"Foundation models that are capable of automating cognitive tasks represent a pivotal technological shift, yet their societal implications remain unclear. These systems promise exciting advances, yet they also risk flooding our information ecosystem with formulaic, homogeneous, and potentially misleading synthetic content. Developing benchmarks grounded in real use cases where these risks are most significant is therefore critical. Through a thematic analysis using 2 million language model user prompts, we identify creative composition tasks as a prevalent usage category where users seek help with personal tasks that require everyday creativity. Our fine-grained analysis identifies mismatches between current benchmarks and usage patterns among these tasks. Crucially, we argue that the same use cases that currently lack thorough evaluations can lead to negative downstream impacts. This position paper argues that benchmarks focused on creative composition tasks is a necessary step towards understanding the societal harms of AI-generated content. We call for greater transparency in usage patterns to inform the development of new benchmarks that can effectively measure both the progress and the impacts of models with creative capabilities.","authors":["Judy Hanwen Shen","Carlos Guestrin"],"url":"https://arxiv.org/abs/2504.06549"}
{"created":"2025-04-11","title":"Bridging Queries and Tables through Entities in Table Retrieval","abstract":"Table retrieval is essential for accessing information stored in structured tabular formats; however, it remains less explored than text retrieval. The content of the table primarily consists of phrases and words, which include a large number of entities, such as time, locations, persons, and organizations. Entities are well-studied in the context of text retrieval, but there is a noticeable lack of research on their applications in table retrieval. In this work, we explore how to leverage entities in tables to improve retrieval performance. First, we investigate the important role of entities in table retrieval from a statistical perspective and propose an entity-enhanced training framework. Subsequently, we use the type of entities to highlight entities instead of introducing an external knowledge base. Moreover, we design an interaction paradigm based on entity representations. Our proposed framework is plug-and-play and flexible, making it easy to integrate into existing table retriever training processes. Empirical results on two table retrieval benchmarks, NQ-TABLES and OTT-QA, show that our proposed framework is both simple and effective in enhancing existing retrievers. We also conduct extensive analyses to confirm the efficacy of different components. Overall, our work provides a promising direction for elevating table retrieval, enlightening future research in this area.","authors":["Da Li","Keping Bi","Jiafeng Guo","Xueqi Cheng"],"url":"https://arxiv.org/abs/2504.06551"}
{"created":"2025-04-11","title":"Understanding Users' Security and Privacy Concerns and Attitudes Towards Conversational AI Platforms","abstract":"The widespread adoption of conversational AI platforms has introduced new security and privacy risks. While these risks and their mitigation strategies have been extensively researched from a technical perspective, users' perceptions of these platforms' security and privacy remain largely unexplored. In this paper, we conduct a large-scale analysis of over 2.5M user posts from the r/ChatGPT Reddit community to understand users' security and privacy concerns and attitudes toward conversational AI platforms. Our qualitative analysis reveals that users are concerned about each stage of the data lifecycle (i.e., collection, usage, and retention). They seek mitigations for security vulnerabilities, compliance with privacy regulations, and greater transparency and control in data handling. We also find that users exhibit varied behaviors and preferences when interacting with these platforms. Some users proactively safeguard their data and adjust privacy settings, while others prioritize convenience over privacy risks, dismissing privacy concerns in favor of benefits, or feel resigned to inevitable data sharing. Through qualitative content and regression analysis, we discover that users' concerns evolve over time with the evolving AI landscape and are influenced by technological developments and major events. Based on our findings, we provide recommendations for users, platforms, enterprises, and policymakers to enhance transparency, improve data controls, and increase user trust and adoption.","authors":["Mutahar Ali","Arjun Arunasalam","Habiba Farrukh"],"url":"https://arxiv.org/abs/2504.06552"}
{"created":"2025-04-11","title":"ASHiTA: Automatic Scene-grounded HIerarchical Task Analysis","abstract":"While recent work in scene reconstruction and understanding has made strides in grounding natural language to physical 3D environments, it is still challenging to ground abstract, high-level instructions to a 3D scene. High-level instructions might not explicitly invoke semantic elements in the scene, and even the process of breaking a high-level task into a set of more concrete subtasks, a process called hierarchical task analysis, is environment-dependent. In this work, we propose ASHiTA, the first framework that generates a task hierarchy grounded to a 3D scene graph by breaking down high-level tasks into grounded subtasks. ASHiTA alternates LLM-assisted hierarchical task analysis, to generate the task breakdown, with task-driven 3D scene graph construction to generate a suitable representation of the environment. Our experiments show that ASHiTA performs significantly better than LLM baselines in breaking down high-level tasks into environment-dependent subtasks and is additionally able to achieve grounding performance comparable to state-of-the-art methods.","authors":["Yun Chang","Leonor Fermoselle","Duy Ta","Bernadette Bucher","Luca Carlone","Jiuguang Wang"],"url":"https://arxiv.org/abs/2504.06553"}
{"created":"2025-04-11","title":"TabKAN: Advancing Tabular Data Analysis using Kolmograv-Arnold Network","abstract":"Tabular data analysis presents unique challenges due to its heterogeneous feature types, missing values, and complex interactions. While traditional machine learning methods, such as gradient boosting, often outperform deep learning approaches, recent advancements in neural architectures offer promising alternatives. This paper introduces TabKAN, a novel framework that advances tabular data modeling using Kolmogorov-Arnold Networks (KANs). Unlike conventional deep learning models, KANs leverage learnable activation functions on edges, enhancing both interpretability and training efficiency. Our contributions include: (1) the introduction of modular KAN-based architectures tailored for tabular data analysis, (2) the development of a transfer learning framework for KAN models, enabling effective knowledge transfer between domains, (3) the development of model-specific interpretability for tabular data learning, reducing reliance on post hoc and model-agnostic analysis, and (4) comprehensive evaluation of vanilla supervised learning across binary and multi-class classification tasks. Through extensive benchmarking on diverse public datasets, TabKAN demonstrates superior performance in supervised learning while significantly outperforming classical and Transformer-based models in transfer learning scenarios. Our findings highlight the advantage of KAN-based architectures in efficiently transferring knowledge across domains, bridging the gap between traditional machine learning and deep learning for structured data.","authors":["Ali Eslamian","Alireza Afzal Aghaei","Qiang Cheng"],"url":"https://arxiv.org/abs/2504.06559"}
{"created":"2025-04-11","title":"NeedleInATable: Exploring Long-Context Capability of Large Language Models towards Long-Structured Tables","abstract":"Processing structured tabular data, particularly lengthy tables, constitutes a fundamental yet challenging task for large language models (LLMs). However, existing long-context benchmarks primarily focus on unstructured text, neglecting the challenges of long and complex structured tables. To address this gap, we introduce NeedleInATable (NIAT), a novel task that treats each table cell as a \"needle\" and requires the model to extract the target cell under different queries. Evaluation results of mainstream LLMs on this benchmark show they lack robust long-table comprehension, often relying on superficial correlations or shortcuts for complex table understanding tasks, revealing significant limitations in processing intricate tabular data. To this end, we propose a data synthesis method to enhance models' long-table comprehension capabilities. Experimental results show that our synthesized training data significantly enhances LLMs' performance on the NIAT task, outperforming both long-context LLMs and long-table agent methods. This work advances the evaluation of LLMs' genuine long-structured table comprehension capabilities and paves the way for progress in long-context and table understanding applications.","authors":["Lanrui Wang","Mingyu Zheng","Hongyin Tang","Zheng Lin","Yanan Cao","Jingang Wang","Xunliang Cai","Weiping Wang"],"url":"https://arxiv.org/abs/2504.06560"}
{"created":"2025-04-11","title":"A Streamable Neural Audio Codec with Residual Scalar-Vector Quantization for Real-Time Communication","abstract":"This paper proposes StreamCodec, a streamable neural audio codec designed for real-time communication. StreamCodec adopts a fully causal, symmetric encoder-decoder structure and operates in the modified discrete cosine transform (MDCT) domain, aiming for low-latency inference and real-time efficient generation. To improve codebook utilization efficiency and compensate for the audio quality loss caused by structural causality, StreamCodec introduces a novel residual scalar-vector quantizer (RSVQ). The RSVQ sequentially connects scalar quantizers and improved vector quantizers in a residual manner, constructing coarse audio contours and refining acoustic details, respectively. Experimental results confirm that the proposed StreamCodec achieves decoded audio quality comparable to advanced non-streamable neural audio codecs. Specifically, on the 16 kHz LibriTTS dataset, StreamCodec attains a ViSQOL score of 4.30 at 1.5 kbps. It has a fixed latency of only 20 ms and achieves a generation speed nearly 20 times real-time on a CPU, with a lightweight model size of just 7M parameters, making it highly suitable for real-time communication applications.","authors":["Xiao-Hang Jiang","Yang Ai","Rui-Chen Zheng","Zhen-Hua Ling"],"url":"https://arxiv.org/abs/2504.06561"}
{"created":"2025-04-11","title":"FuseRL: Dense Preference Optimization for Heterogeneous Model Fusion","abstract":"Heterogeneous model fusion enhances the performance of LLMs by integrating the knowledge and capabilities of multiple structurally diverse models. However, existing approaches often rely solely on selecting the best output for each prompt from source models, which underutilizes their full potential due to limited source knowledge and results in sparse optimization signals. To address this limitation, we propose FuseRL, a novel two-stage framework comprising FuseSFT and FusePO to maximize the utilization of source LLMs. FuseSFT establishes a robust initialization by integrating the strengths of heterogeneous source models through weighted supervised fine-tuning (SFT) on diverse outputs for each prompt. FusePO optimizes weighted preferences based on the outputs of multiple source models to enable superior alignment performance. Extensive experiments demonstrate the effectiveness of our framework across various preference alignment methods, including RLOO, DPO, and SimPO. Using Llama-3.1-8B-Instruct as the target model, our approach achieves state-of-the-art performance among 8B LLMs on the AlpacaEval-2 and Arena-Hard benchmarks. Further analysis suggests that FuseSFT regularizes the training process to reduce overfitting, while FusePO introduces dense and diverse signals for preference optimization.","authors":["Longguang Zhong","Fanqi Wan","Ziyi Yang","Guosheng Liang","Tianyuan Shi","Xiaojun Quan"],"url":"https://arxiv.org/abs/2504.06562"}
{"created":"2025-04-11","title":"Do Reasoning Models Show Better Verbalized Calibration?","abstract":"Large reasoning models (LRMs) have recently shown impressive capabilities in complex reasoning by leveraging increased test-time computation and exhibiting behaviors akin to human-like deliberation. Despite these advances, it remains an open question whether LRMs are better calibrated - particularly in their verbalized confidence - compared to instruction-tuned counterparts. In this paper, we investigate the calibration properties of LRMs trained via supervised fine-tuning distillation on long reasoning traces (henceforth SFT reasoning models) and outcome-based reinforcement learning for reasoning (henceforth RL reasoning models) across diverse domains. Our findings reveal that LRMs significantly outperform instruction-tuned models on complex reasoning tasks in both accuracy and confidence calibration. In contrast, we find surprising trends in the domain of factuality in particular. On factuality tasks, while Deepseek-R1 shows strong calibration behavior, smaller QwQ-32B shows no improvement over instruct models; moreover, SFT reasoning models display worse calibration (greater overconfidence) compared to instruct models. Our results provide evidence for a potentially critical role of reasoning-oriented RL training in improving LLMs' capacity for generating trustworthy, self-aware outputs.","authors":["Qingcheng Zeng","Weihao Xuan","Leyang Cui","Rob Voigt"],"url":"https://arxiv.org/abs/2504.06564"}
{"created":"2025-04-11","title":"Domain Generalization via Discrete Codebook Learning","abstract":"Domain generalization (DG) strives to address distribution shifts across diverse environments to enhance model's generalizability. Current DG approaches are confined to acquiring robust representations with continuous features, specifically training at the pixel level. However, this DG paradigm may struggle to mitigate distribution gaps in dealing with a large space of continuous features, rendering it susceptible to pixel details that exhibit spurious correlations or noise. In this paper, we first theoretically demonstrate that the domain gaps in continuous representation learning can be reduced by the discretization process. Based on this inspiring finding, we introduce a novel learning paradigm for DG, termed Discrete Domain Generalization (DDG). DDG proposes to use a codebook to quantize the feature map into discrete codewords, aligning semantic-equivalent information in a shared discrete representation space that prioritizes semantic-level information over pixel-level intricacies. By learning at the semantic level, DDG diminishes the number of latent features, optimizing the utilization of the representation space and alleviating the risks associated with the wide-ranging space of continuous features. Extensive experiments across widely employed benchmarks in DG demonstrate DDG's superior performance compared to state-of-the-art approaches, underscoring its potential to reduce the distribution gaps and enhance the model's generalizability.","authors":["Shaocong Long","Qianyu Zhou","Xikun Jiang","Chenhao Ying","Lizhuang Ma","Yuan Luo"],"url":"https://arxiv.org/abs/2504.06572"}
{"created":"2025-04-11","title":"Defending LLM Watermarking Against Spoofing Attacks with Contrastive Representation Learning","abstract":"Watermarking has emerged as a promising technique for detecting texts generated by LLMs. Current research has primarily focused on three design criteria: high quality of the watermarked text, high detectability, and robustness against removal attack. However, the security against spoofing attacks remains relatively understudied. For example, a piggyback attack can maliciously alter the meaning of watermarked text-transforming it into hate speech-while preserving the original watermark, thereby damaging the reputation of the LLM provider. We identify two core challenges that make defending against spoofing difficult: (1) the need for watermarks to be both sensitive to semantic-distorting changes and insensitive to semantic-preserving edits, and (2) the contradiction between the need to detect global semantic shifts and the local, auto-regressive nature of most watermarking schemes. To address these challenges, we propose a semantic-aware watermarking algorithm that post-hoc embeds watermarks into a given target text while preserving its original meaning. Our method introduces a semantic mapping model, which guides the generation of a green-red token list, contrastively trained to be sensitive to semantic-distorting changes and insensitive to semantic-preserving changes. Experiments on two standard benchmarks demonstrate strong robustness against removal attacks and security against spoofing attacks, including sentiment reversal and toxic content insertion, while maintaining high watermark detectability. Our approach offers a significant step toward more secure and semantically aware watermarking for LLMs. Our code is available at https://github.com/UCSB-NLP-Chang/contrastive-watermark.","authors":["Li An","Yujian Liu","Yepeng Liu","Yang Zhang","Yuheng Bu","Shiyu Chang"],"url":"https://arxiv.org/abs/2504.06575"}
{"created":"2025-04-11","title":"Bypassing Safety Guardrails in LLMs Using Humor","abstract":"In this paper, we show it is possible to bypass the safety guardrails of large language models (LLMs) through a humorous prompt including the unsafe request. In particular, our method does not edit the unsafe request and follows a fixed template -- it is simple to implement and does not need additional LLMs to craft prompts. Extensive experiments show the effectiveness of our method across different LLMs. We also show that both removing and adding more humor to our method can reduce its effectiveness -- excessive humor possibly distracts the LLM from fulfilling its unsafe request. Thus, we argue that LLM jailbreaking occurs when there is a proper balance between focus on the unsafe request and presence of humor.","authors":["Pedro Cisneros-Velarde"],"url":"https://arxiv.org/abs/2504.06577"}
{"created":"2025-04-11","title":"Attributes-aware Visual Emotion Representation Learning","abstract":"Visual emotion analysis or recognition has gained considerable attention due to the growing interest in understanding how images can convey rich semantics and evoke emotions in human perception. However, visual emotion analysis poses distinctive challenges compared to traditional vision tasks, especially due to the intricate relationship between general visual features and the different affective states they evoke, known as the affective gap. Researchers have used deep representation learning methods to address this challenge of extracting generalized features from entire images. However, most existing methods overlook the importance of specific emotional attributes such as brightness, colorfulness, scene understanding, and facial expressions. Through this paper, we introduce A4Net, a deep representation network to bridge the affective gap by leveraging four key attributes: brightness (Attribute 1), colorfulness (Attribute 2), scene context (Attribute 3), and facial expressions (Attribute 4). By fusing and jointly training all aspects of attribute recognition and visual emotion analysis, A4Net aims to provide a better insight into emotional content in images. Experimental results show the effectiveness of A4Net, showcasing competitive performance compared to state-of-the-art methods across diverse visual emotion datasets. Furthermore, visualizations of activation maps generated by A4Net offer insights into its ability to generalize across different visual emotion datasets.","authors":["Rahul Singh Maharjan","Marta Romeo","Angelo Cangelosi"],"url":"https://arxiv.org/abs/2504.06578"}
{"created":"2025-04-11","title":"Exploring Ordinal Bias in Action Recognition for Instructional Videos","abstract":"Action recognition models have achieved promising results in understanding instructional videos. However, they often rely on dominant, dataset-specific action sequences rather than true video comprehension, a problem that we define as ordinal bias. To address this issue, we propose two effective video manipulation methods: Action Masking, which masks frames of frequently co-occurring actions, and Sequence Shuffling, which randomizes the order of action segments. Through comprehensive experiments, we demonstrate that current models exhibit significant performance drops when confronted with nonstandard action sequences, underscoring their vulnerability to ordinal bias. Our findings emphasize the importance of rethinking evaluation strategies and developing models capable of generalizing beyond fixed action patterns in diverse instructional videos.","authors":["Joochan Kim","Minjoon Jung","Byoung-Tak Zhang"],"url":"https://arxiv.org/abs/2504.06580"}
{"created":"2025-04-11","title":"Right Prediction, Wrong Reasoning: Uncovering LLM Misalignment in RA Disease Diagnosis","abstract":"Large language models (LLMs) offer a promising pre-screening tool, improving early disease detection and providing enhanced healthcare access for underprivileged communities. The early diagnosis of various diseases continues to be a significant challenge in healthcare, primarily due to the nonspecific nature of early symptoms, the shortage of expert medical practitioners, and the need for prolonged clinical evaluations, all of which can delay treatment and adversely affect patient outcomes. With impressive accuracy in prediction across a range of diseases, LLMs have the potential to revolutionize clinical pre-screening and decision-making for various medical conditions. In this work, we study the diagnostic capability of LLMs for Rheumatoid Arthritis (RA) with real world patients data. Patient data was collected alongside diagnoses from medical experts, and the performance of LLMs was evaluated in comparison to expert diagnoses for RA disease prediction. We notice an interesting pattern in disease diagnosis and find an unexpected \\textit{misalignment between prediction and explanation}. We conduct a series of multi-round analyses using different LLM agents. The best-performing model accurately predicts rheumatoid arthritis (RA) diseases approximately 95\\% of the time. However, when medical experts evaluated the reasoning generated by the model, they found that nearly 68\\% of the reasoning was incorrect. This study highlights a clear misalignment between LLMs high prediction accuracy and its flawed reasoning, raising important questions about relying on LLM explanations in clinical settings. \\textbf{LLMs provide incorrect reasoning to arrive at the correct answer for RA disease diagnosis.}","authors":["Umakanta Maharana","Sarthak Verma","Avarna Agarwal","Prakashini Mruthyunjaya","Dwarikanath Mahapatra","Sakir Ahmed","Murari Mandal"],"url":"https://arxiv.org/abs/2504.06581"}
{"created":"2025-04-11","title":"Aplicando diferencias finitas para resolver ecuaciones y sistemas de ecuaciones diferenciales parciales sobre dominios planos irregulares simplemente conexos y no conexos","abstract":"Using exhaustion method and finite differences a new method to solve system of partial differential equations and is presented. This method allows design algorithm to solve linear and nonlinear systems in irregular domains. Applying this method to solve linear and nonlinear problems with prescribed conditions Dirichlet over two-dimensional irregular domains are analyzed.","authors":["Miriam Sosa-D\\'iaz","Faustino Sanchez-Garduno"],"url":"https://arxiv.org/abs/2504.06583"}
{"created":"2025-04-11","title":"CAFE-AD: Cross-Scenario Adaptive Feature Enhancement for Trajectory Planning in Autonomous Driving","abstract":"Imitation learning based planning tasks on the nuPlan dataset have gained great interest due to their potential to generate human-like driving behaviors. However, open-loop training on the nuPlan dataset tends to cause causal confusion during closed-loop testing, and the dataset also presents a long-tail distribution of scenarios. These issues introduce challenges for imitation learning. To tackle these problems, we introduce CAFE-AD, a Cross-Scenario Adaptive Feature Enhancement for Trajectory Planning in Autonomous Driving method, designed to enhance feature representation across various scenario types. We develop an adaptive feature pruning module that ranks feature importance to capture the most relevant information while reducing the interference of noisy information during training. Moreover, we propose a cross-scenario feature interpolation module that enhances scenario information to introduce diversity, enabling the network to alleviate over-fitting in dominant scenarios. We evaluate our method CAFE-AD on the challenging public nuPlan Test14-Hard closed-loop simulation benchmark. The results demonstrate that CAFE-AD outperforms state-of-the-art methods including rule-based and hybrid planners, and exhibits the potential in mitigating the impact of long-tail distribution within the dataset. Additionally, we further validate its effectiveness in real-world environments. The code and models will be made available at https://github.com/AlniyatRui/CAFE-AD.","authors":["Junrui Zhang","Chenjie Wang","Jie Peng","Haoyu Li","Jianmin Ji","Yu Zhang","Yanyong Zhang"],"url":"https://arxiv.org/abs/2504.06584"}
{"created":"2025-04-11","title":"Sim-to-Real of Humanoid Locomotion Policies via Joint Torque Space Perturbation Injection","abstract":"This paper proposes a novel alternative to existing sim-to-real methods for training control policies with simulated experiences. Prior sim-to-real methods for legged robots mostly rely on the domain randomization approach, where a fixed finite set of simulation parameters is randomized during training. Instead, our method adds state-dependent perturbations to the input joint torque used for forward simulation during the training phase. These state-dependent perturbations are designed to simulate a broader range of reality gaps than those captured by randomizing a fixed set of simulation parameters. Experimental results show that our method enables humanoid locomotion policies that achieve greater robustness against complex reality gaps unseen in the training domain.","authors":["Woohyun Cha","Junhyeok Cha","Jaeyong Shin","Donghyeon Kim","Jaeheung Park"],"url":"https://arxiv.org/abs/2504.06585"}
{"created":"2025-04-11","title":"Diversity-aware Dual-promotion Poisoning Attack on Sequential Recommendation","abstract":"Sequential recommender systems (SRSs) excel in capturing users' dynamic interests, thus playing a key role in various industrial applications. The popularity of SRSs has also driven emerging research on their security aspects, where data poisoning attack for targeted item promotion is a typical example. Existing attack mechanisms primarily focus on increasing the ranks of target items in the recommendation list by injecting carefully crafted interactions (i.e., poisoning sequences), which comes at the cost of demoting users' real preferences. Consequently, noticeable recommendation accuracy drops are observed, restricting the stealthiness of the attack. Additionally, the generated poisoning sequences are prone to substantial repetition of target items, which is a result of the unitary objective of boosting their overall exposure and lack of effective diversity regularizations. Such homogeneity not only compromises the authenticity of these sequences, but also limits the attack effectiveness, as it ignores the opportunity to establish sequential dependencies between the target and many more items in the SRS. To address the issues outlined, we propose a Diversity-aware Dual-promotion Sequential Poisoning attack method named DDSP for SRSs. Specifically, by theoretically revealing the conflict between recommendation and existing attack objectives, we design a revamped attack objective that promotes the target item while maintaining the relevance of preferred items in a user's ranking list. We further develop a diversity-aware, auto-regressive poisoning sequence generator, where a re-ranking method is in place to sequentially pick the optimal items by integrating diversity constraints.","authors":["Yuchuan Zhao","Tong Chen","Junliang Yu","Kai Zheng","Lizhen Cui","Hongzhi Yin"],"url":"https://arxiv.org/abs/2504.06586"}
{"created":"2025-04-11","title":"SigChord: Sniffing Wide Non-sparse Multiband Signals for Terrestrial and Non-terrestrial Wireless Networks","abstract":"While unencrypted information inspection in physical layer (e.g., open headers) can provide deep insights for optimizing wireless networks, the state-of-the-art (SOTA) methods heavily depend on full sampling rate (a.k.a Nyquist rate), and high-cost radios, due to terrestrial and non-terrestrial networks densely occupying multiple bands across large bandwidth (e.g., from 4G/5G at 0.4-7 GHz to LEO satellite at 4-40 GHz). To this end, we present SigChord, an efficient physical layer inspection system built on low-cost and sub-Nyquist sampling radios. We first design a deep and rule-based interleaving algorithm based on Transformer network to perform spectrum sensing and signal recovery under sub-Nyquist sampling rate, and second, cascade protocol identifier and decoder based on Transformer neural networks to help physical layer packets analysis. We implement SigChord using software-defined radio platforms, and extensively evaluate it on over-the-air terrestrial and non-terrestrial wireless signals. The experiments demonstrate that SigChord delivers over 99% accuracy in detecting and decoding, while still decreasing 34% sampling rate, compared with the SOTA approaches.","authors":["Jinbo Peng","Junwen Duan","Zheng Lin","Haoxuan Yuan","Yue Gao","Zhe Chen"],"url":"https://arxiv.org/abs/2504.06587"}
{"created":"2025-04-11","title":"A Digital Twin of an Electrical Distribution Grid: SoCal 28-Bus Dataset","abstract":"We provide an open-access dataset of phasor & waveform measurement units (PMUs/WMUs) of a real-world electrical distribution network. The network consists of diverse sets of generation resources (including solar panels, fuel cells, natural gas generators, and utility interconnections), loads (including large-scale electric vehicle charging, data centers, central cooling, offices), topology changes (such as line outages and load transfers), as well as a mixture of single- and three-phase networks. We describe a densely deployed PMU sensor network in a distribution grid, in which all buses with non-zero power injections are measured. This approach enables a range of applications such as state estimation, system identification, power flow optimization, and feedback control, several of which are discussed in this paper. Additionally, we provide a synchronized waveform dataset which allows the analysis of harmonics, transient events, dynamic grid impedance, and stability. Data collection started in 2023 while new data is generated continuously and made available online. A characterization of measurement error is provided. Finally, we provide circuit topology and parameters as a part of the dataset. Together, the circuit and timeseries data offer an opportunity for researchers to develop and test algorithms on a real-world system.","authors":["Yiheng Xie","Lucien Werner","Kaibo Chen","Thuy-Linh Le","Christine Ortega","Steven Low"],"url":"https://arxiv.org/abs/2504.06588"}
{"created":"2025-04-11","title":"NAPER: Fault Protection for Real-Time Resource-Constrained Deep Neural Networks","abstract":"Fault tolerance in Deep Neural Networks (DNNs) deployed on resource-constrained systems presents unique challenges for high-accuracy applications with strict timing requirements. Memory bit-flips can severely degrade DNN accuracy, while traditional protection approaches like Triple Modular Redundancy (TMR) often sacrifice accuracy to maintain reliability, creating a three-way dilemma between reliability, accuracy, and timeliness. We introduce NAPER, a novel protection approach that addresses this challenge through ensemble learning. Unlike conventional redundancy methods, NAPER employs heterogeneous model redundancy, where diverse models collectively achieve higher accuracy than any individual model. This is complemented by an efficient fault detection mechanism and a real-time scheduler that prioritizes meeting deadlines by intelligently scheduling recovery operations without interrupting inference. Our evaluations demonstrate NAPER's superiority: 40% faster inference in both normal and fault conditions, maintained accuracy 4.2% higher than TMR-based strategies, and guaranteed uninterrupted operation even during fault recovery. NAPER effectively balances the competing demands of accuracy, reliability, and timeliness in real-time DNN applications","authors":["Rian Adam Rajagede","Muhammad Husni Santriaji","Muhammad Arya Fikriansyah","Hilal Hudan Nuha","Yanjie Fu","Yan Solihin"],"url":"https://arxiv.org/abs/2504.06591"}
{"created":"2025-04-11","title":"On Coalgebraic Product Constructions for Markov Chains and Automata","abstract":"Verifying traces of systems is a central topic in formal verification. We study model checking of Markov chains (MCs) against temporal properties represented as (finite) automata. For instance, given an MC and a deterministic finite automaton (DFA), a simple but practically useful model checking problem asks for the probability of traces on the MC that are accepted by the DFA. A standard approach to solving this problem constructs a product MC of the given MC and DFA, reducing the task to a simple reachability probability problem on the resulting product MC.","authors":["Mayuko Kori","Kazuki Watanabe"],"url":"https://arxiv.org/abs/2504.06592"}
{"created":"2025-04-11","title":"A Multi-Modal Interaction Framework for Efficient Human-Robot Collaborative Shelf Picking","abstract":"The growing presence of service robots in human-centric environments, such as warehouses, demands seamless and intuitive human-robot collaboration. In this paper, we propose a collaborative shelf-picking framework that combines multimodal interaction, physics-based reasoning, and task division for enhanced human-robot teamwork.","authors":["Abhinav Pathak","Kalaichelvi Venkatesan","Tarek Taha","Rajkumar Muthusamy"],"url":"https://arxiv.org/abs/2504.06593"}
{"created":"2025-04-11","title":"Overcoming Dynamic Environments: A Hybrid Approach to Motion Planning for Manipulators","abstract":"Robotic manipulators operating in dynamic and uncertain environments require efficient motion planning to navigate obstacles while maintaining smooth trajectories. Velocity Potential Field (VPF) planners offer real-time adaptability but struggle with complex constraints and local minima, leading to suboptimal performance in cluttered spaces. Traditional approaches rely on pre-planned trajectories, but frequent recomputation is computationally expensive. This study proposes a hybrid motion planning approach, integrating an improved VPF with a Sampling-Based Motion Planner (SBMP). The SBMP ensures optimal path generation, while VPF provides real-time adaptability to dynamic obstacles. This combination enhances motion planning efficiency, stability, and computational feasibility, addressing key challenges in uncertain environments such as warehousing and surgical robotics.","authors":["Ho Minh Quang Ngo","Dac Dang Khoa Nguyen","Dinh Tung Le","Gavin Paul"],"url":"https://arxiv.org/abs/2504.06596"}
{"created":"2025-04-11","title":"Stochastic Ray Tracing of 3D Transparent Gaussians","abstract":"3D Gaussian splatting has recently been widely adopted as a 3D representation for novel-view synthesis, relighting, and text-to-3D generation tasks, offering realistic and detailed results through a collection of explicit 3D Gaussians carrying opacities and view-dependent colors. However, efficient rendering of many transparent primitives remains a significant challenge. Existing approaches either rasterize the 3D Gaussians with approximate sorting per view or rely on high-end RTX GPUs to exhaustively process all ray-Gaussian intersections (bounding Gaussians by meshes). This paper proposes a stochastic ray tracing method to render 3D clouds of transparent primitives. Instead of processing all ray-Gaussian intersections in sequential order, each ray traverses the acceleration structure only once, randomly accepting and shading a single intersection (or N intersections, using a simple extension). This approach minimizes shading time and avoids sorting the Gaussians along the ray while minimizing the register usage and maximizing parallelism even on low-end GPUs. The cost of rays through the Gaussian asset is comparable to that of standard mesh-intersection rays. While our method introduces noise, the shading is unbiased, and the variance is slight, as stochastic acceptance is importance-sampled based on accumulated opacity. The alignment with the Monte Carlo philosophy simplifies implementation and easily integrates our method into a conventional path-tracing framework.","authors":["Xin Sun","Iliyan Georgiev","Yun Fei","Milo\\v{s} Ha\\v{s}an"],"url":"https://arxiv.org/abs/2504.06598"}
{"created":"2025-04-11","title":"Automated Business Process Analysis: An LLM-Based Approach to Value Assessment","abstract":"Business processes are fundamental to organizational operations, yet their optimization remains challenging due to the timeconsuming nature of manual process analysis. Our paper harnesses Large Language Models (LLMs) to automate value-added analysis, a qualitative process analysis technique that aims to identify steps in the process that do not deliver value. To date, this technique is predominantly manual, time-consuming, and subjective. Our method offers a more principled approach which operates in two phases: first, decomposing high-level activities into detailed steps to enable granular analysis, and second, performing a value-added analysis to classify each step according to Lean principles. This approach enables systematic identification of waste while maintaining the semantic understanding necessary for qualitative analysis. We develop our approach using 50 business process models, for which we collect and publish manual ground-truth labels. Our evaluation, comparing zero-shot baselines with more structured prompts reveals (a) a consistent benefit of structured prompting and (b) promising performance for both tasks. We discuss the potential for LLMs to augment human expertise in qualitative process analysis while reducing the time and subjectivity inherent in manual approaches.","authors":["William De Michele","Abel Armas Cervantes","Lea Frermann"],"url":"https://arxiv.org/abs/2504.06600"}
{"created":"2025-04-11","title":"Asymptotic Variance in the Central Limit Theorem for Multilevel Markovian Stochastic Approximation","abstract":"In this note we consider the finite-dimensional parameter estimation problem associated to inverse problems. In such scenarios, one seeks to maximize the marginal likelihood associated to a Bayesian model. This latter model is connected to the solution of partial or ordinary differential equation. As such, there are two primary difficulties in maximizing the marginal likelihood (i) that the solution of differential equation is not always analytically tractable and (ii) neither is the marginal likelihood. Typically (i) is dealt with using a numerical solution of the differential equation, leading to a numerical bias and (ii) has been well studied in the literature using, for instance, Markovian stochastic approximation. It is well-known that to reduce the computational effort to obtain the maximal value of the parameter, one can use a hierarchy of solutions of the differential equation and combine with stochastic gradient methods. Several approaches do exactly this. In this paper we consider the asymptotic variance in the central limit theorem, associated to known estimates and find bounds on the asymptotic variance in terms of the precision of the solution of the differential equation. The significance of these bounds are the that they provide missing theoretical guidelines on how to set simulation parameters; that is, these appear to be the first mathematical results which help to run the methods efficiently in practice.","authors":["Ajay Jasra","Abylay Zhumekenov"],"url":"https://arxiv.org/abs/2504.06603"}
{"created":"2025-04-11","title":"Benchmarking Multimodal CoT Reward Model Stepwise by Visual Program","abstract":"Recent advancements in reward signal usage for Large Language Models (LLMs) are remarkable. However, significant challenges exist when transitioning reward signal to the multimodal domain, including labor-intensive annotations, over-reliance on one-step rewards, and inadequate evaluation. To address these issues, we propose SVIP, a novel approach to train a step-level multi-dimensional Chain-of-Thought~(CoT) reward model automatically. It generates code for solving visual tasks and transforms the analysis of code blocks into the evaluation of CoT step as training samples. Then, we train SVIP-Reward model using a multi-head attention mechanism called TriAtt-CoT. The advantages of SVIP-Reward are evident throughout the entire process of MLLM. We also introduce a benchmark for CoT reward model training and testing. Experimental results demonstrate that SVIP-Reward improves MLLM performance across training and inference-time scaling, yielding better results on benchmarks while reducing hallucinations and enhancing reasoning ability.","authors":["Minghe Gao","Xuqi Liu","Zhongqi Yue","Yang Wu","Shuang Chen","Juncheng Li","Siliang Tang","Fei Wu","Tat-Seng Chua","Yueting Zhuang"],"url":"https://arxiv.org/abs/2504.06606"}
{"created":"2025-04-11","title":"Visually Similar Pair Alignment for Robust Cross-Domain Object Detection","abstract":"Domain gaps between training data (source) and real-world environments (target) often degrade the performance of object detection models. Most existing methods aim to bridge this gap by aligning features across source and target domains but often fail to account for visual differences, such as color or orientation, in alignment pairs. This limitation leads to less effective domain adaptation, as the model struggles to manage both domain-specific shifts (e.g., fog) and visual variations simultaneously. In this work, we demonstrate for the first time, using a custom-built dataset, that aligning visually similar pairs significantly improves domain adaptation. Based on this insight, we propose a novel memory-based system to enhance domain alignment. This system stores precomputed features of foreground objects and background areas from the source domain, which are periodically updated during training. By retrieving visually similar source features for alignment with target foreground and background features, the model effectively addresses domain-specific differences while reducing the impact of visual variations. Extensive experiments across diverse domain shift scenarios validate our method's effectiveness, achieving 53.1 mAP on Foggy Cityscapes and 62.3 on Sim10k, surpassing prior state-of-the-art methods by 1.2 and 4.1 mAP, respectively.","authors":["Onkar Krishna","Hiroki Ohashi"],"url":"https://arxiv.org/abs/2504.06607"}
{"created":"2025-04-11","title":"A Cross-Domain Few-Shot Learning Method Based on Domain Knowledge Mapping","abstract":"In task-based few-shot learning paradigms, it is commonly assumed that different tasks are independently and identically distributed (i.i.d.). However, in real-world scenarios, the distribution encountered in few-shot learning can significantly differ from the distribution of existing data. Thus, how to effectively leverage existing data knowledge to enable models to quickly adapt to class variations under non-i.i.d. assumptions has emerged as a key research challenge. To address this challenge, this paper proposes a new cross-domain few-shot learning approach based on domain knowledge mapping, applied consistently throughout the pre-training, training, and testing phases. In the pre-training phase, our method integrates self-supervised and supervised losses by maximizing mutual information, thereby mitigating mode collapse. During the training phase, the domain knowledge mapping layer collaborates with a domain classifier to learn both domain mapping capabilities and the ability to assess domain adaptation difficulty. Finally, this approach is applied during the testing phase, rapidly adapting to domain variations through meta-training tasks on support sets, consequently enhancing the model's capability to transfer domain knowledge effectively. Experimental validation conducted across six datasets from diverse domains demonstrates the effectiveness of the proposed method.","authors":["Jiajun Chen","Hongpeng Yin","Yifu Yang"],"url":"https://arxiv.org/abs/2504.06608"}
{"created":"2025-04-11","title":"InteractRank: Personalized Web-Scale Search Pre-Ranking with Cross Interaction Features","abstract":"Modern search systems use a multi-stage architecture to deliver personalized results efficiently. Key stages include retrieval, pre-ranking, full ranking, and blending, which refine billions of items to top selections. The pre-ranking stage, vital for scoring and filtering hundreds of thousands of items down to a few thousand, typically relies on two tower models due to their computational efficiency, despite often lacking in capturing complex interactions. While query-item cross interaction features are paramount for full ranking, integrating them into pre-ranking models presents efficiency-related challenges. In this paper, we introduce InteractRank, a novel two tower pre-ranking model with robust cross interaction features used at Pinterest. By incorporating historical user engagement-based query-item interactions in the scoring function along with the two tower dot product, InteractRank significantly boosts pre-ranking performance with minimal latency and computation costs. In real-world A/B experiments at Pinterest, InteractRank improves the online engagement metric by 6.5% over a BM25 baseline and by 3.7% over a vanilla two tower baseline. We also highlight other components of InteractRank, like real-time user-sequence modeling, and analyze their contributions through offline ablation studies. The code for InteractRank is available at https://github.com/pinterest/atg-research/tree/main/InteractRank.","authors":["Sujay Khandagale","Bhawna Juneja","Prabhat Agarwal","Aditya Subramanian","Jaewon Yang","Yuting Wang"],"url":"https://arxiv.org/abs/2504.06609"}
{"created":"2025-04-11","title":"Disentangle and Regularize: Sign Language Production with Articulator-Based Disentanglement and Channel-Aware Regularization","abstract":"In this work, we propose a simple gloss-free, transformer-based sign language production (SLP) framework that directly maps spoken-language text to sign pose sequences. We first train a pose autoencoder that encodes sign poses into a compact latent space using an articulator-based disentanglement strategy, where features corresponding to the face, right hand, left hand, and body are modeled separately to promote structured and interpretable representation learning. Next, a non-autoregressive transformer decoder is trained to predict these latent representations from sentence-level text embeddings. To guide this process, we apply channel-aware regularization by aligning predicted latent distributions with priors extracted from the ground-truth encodings using a KL-divergence loss. The contribution of each channel to the loss is weighted according to its associated articulator region, enabling the model to account for the relative importance of different articulators during training. Our approach does not rely on gloss supervision or pretrained models, and achieves state-of-the-art results on the PHOENIX14T dataset using only a modest training set.","authors":["Sumeyye Meryem Tasyurek","Tugce Kiziltepe","Hacer Yalim Keles"],"url":"https://arxiv.org/abs/2504.06610"}
{"created":"2025-04-11","title":"Wanting to be Understood","abstract":"This paper explores an intrinsic motivation for mutual awareness, hypothesizing that humans possess a fundamental drive to understand and to be understood even in the absence of extrinsic rewards. Through simulations of the perceptual crossing paradigm, we explore the effect of various internal reward functions in reinforcement learning agents. The drive to understand is implemented as an active inference type artificial curiosity reward, whereas the drive to be understood is implemented through intrinsic rewards for imitation, influence/impressionability, and sub-reaction time anticipation of the other. Results indicate that while artificial curiosity alone does not lead to a preference for social interaction, rewards emphasizing reciprocal understanding successfully drive agents to prioritize interaction. We demonstrate that this intrinsic motivation can facilitate cooperation in tasks where only one agent receives extrinsic reward for the behaviour of the other.","authors":["Chrisantha Fernando","Dylan Banarse","Simon Osindero"],"url":"https://arxiv.org/abs/2504.06611"}
{"created":"2025-04-11","title":"AgentFM: Role-Aware Failure Management for Distributed Databases with LLM-Driven Multi-Agents","abstract":"Distributed databases are critical infrastructures for today's large-scale software systems, making effective failure management essential to ensure software availability. However, existing approaches often overlook the role distinctions within distributed databases and rely on small-scale models with limited generalization capabilities. In this paper, we conduct a preliminary empirical study to emphasize the unique significance of different roles. Building on this insight, we propose AgentFM, a role-aware failure management framework for distributed databases powered by LLM-driven multi-agents. AgentFM addresses failure management by considering system roles, data roles, and task roles, with a meta-agent orchestrating these components. Preliminary evaluations using Apache IoTDB demonstrate the effectiveness of AgentFM and open new directions for further research.","authors":["Lingzhe Zhang","Yunpeng Zhai","Tong Jia","Xiaosong Huang","Chiming Duan","Ying Li"],"url":"https://arxiv.org/abs/2504.06614"}
{"created":"2025-04-11","title":"Reliability Assessment of Low-Cost PM Sensors under High Humidity and High PM Level Outdoor Conditions","abstract":"Low-cost particulate matter (PM) sensors have become increasingly popular due to their compact size, low power consumption, and cost-effective installation and maintenance. While several studies have explored the effects of meteorological conditions and pollution exposure on low-cost sensor (LCS) performance, few have addressed the combined impact of high PM concentration and high humidity levels. In contrast to most evaluation studies, which generally report $\\text{PM}_{2.5}$ levels below $150~\\mu\\text{g/m}^3$, our study observed hourly average $\\text{PM}_{2.5}$ concentrations ranging from $6-611~\\mu\\text{g/m}^3$ (mean value of $137~\\mu\\text{g/m}^3$), with relative humidity between $25-95\\%$ (mean value of $72\\%$), and temperature varying from $6-29^\\circ$C (mean value of $16^\\circ$C). We evaluate three LCS models (SPS30, PMS7003, HPMA115C0-004) in outdoor conditions during the winter season in New Delhi, India, deployed alongside a reference-grade beta attenuation monitor (BAM). The results indicate a strong correlation between LCS and BAM measurements (${R^2} > 90\\%$). The RMSE increases with increasing PM concentration and humidity levels but the narrow $95\\%$ confidence interval range of LCS as a function of the reference BAM suggests the importance of LCS in air pollution monitoring. Among the evaluated LCS models, SPS30 showed the highest overall accuracy. Overall, the study demonstrates that LCS can effectively monitor air quality in regions with high PM and high humidity levels, provided appropriate correction models are applied.","authors":["Gulshan Kumar","Prasannaa Kumar D","Jay Dhariwal","Seshan Srirangarajan"],"url":"https://arxiv.org/abs/2504.06615"}
{"created":"2025-04-11","title":"Human-like compositional learning of visually-grounded concepts using synthetic environments","abstract":"The compositional structure of language enables humans to decompose complex phrases and map them to novel visual concepts, showcasing flexible intelligence. While several algorithms exhibit compositionality, they fail to elucidate how humans learn to compose concept classes and ground visual cues through trial and error. To investigate this multi-modal learning challenge, we designed a 3D synthetic environment in which an agent learns, via reinforcement, to navigate to a target specified by a natural language instruction. These instructions comprise nouns, attributes, and critically, determiners, prepositions, or both. The vast array of word combinations heightens the compositional complexity of the visual grounding task, as navigating to a blue cube above red spheres is not rewarded when the instruction specifies navigating to \"some blue cubes below the red sphere\". We first demonstrate that reinforcement learning agents can ground determiner concepts to visual targets but struggle with more complex prepositional concepts. Second, we show that curriculum learning, a strategy humans employ, enhances concept learning efficiency, reducing the required training episodes by 15% in determiner environments and enabling agents to easily learn prepositional concepts. Finally, we establish that agents trained on determiner or prepositional concepts can decompose held-out test instructions and rapidly adapt their navigation policies to unseen visual object combinations. Leveraging synthetic environments, our findings demonstrate that multi-modal reinforcement learning agents can achieve compositional understanding of complex concept classes and highlight the efficacy of human-like learning strategies in improving artificial systems' learning efficiency.","authors":["Zijun Lin","M Ganesh Kumar","Cheston Tan"],"url":"https://arxiv.org/abs/2504.06618"}
{"created":"2025-04-11","title":"InstantSticker: Realistic Decal Blending via Disentangled Object Reconstruction","abstract":"We present InstantSticker, a disentangled reconstruction pipeline based on Image-Based Lighting (IBL), which focuses on highly realistic decal blending, simulates stickers attached to the reconstructed surface, and allows for instant editing and real-time rendering. To achieve stereoscopic impression of the decal, we introduce shadow factor into IBL, which can be adaptively optimized during training. This allows the shadow brightness of surfaces to be accurately decomposed rather than baked into the diffuse color, ensuring that the edited texture exhibits authentic shading. To address the issues of warping and blurriness in previous methods, we apply As-Rigid-As-Possible (ARAP) parameterization to pre-unfold a specified area of the mesh and use the local UV mapping combined with a neural texture map to enhance the ability to express high-frequency details in that area. For instant editing, we utilize the Disney BRDF model, explicitly defining material colors with 3-channel diffuse albedo. This enables instant replacement of albedo RGB values during the editing process, avoiding the prolonged optimization required in previous approaches. In our experiment, we introduce the Ratio Variance Warping (RVW) metric to evaluate the local geometric warping of the decal area. Extensive experimental results demonstrate that our method surpasses previous decal blending methods in terms of editing quality, editing speed and rendering speed, achieving the state-of-the-art.","authors":["Yi Zhang","Xiaoyang Huang","Yishun Dou","Yue Shi","Rui Shi","Ye Chen","Bingbing Ni","Wenjun Zhang"],"url":"https://arxiv.org/abs/2504.06620"}
{"created":"2025-04-11","title":"Computation of shape Taylor expansions","abstract":"Shape derivative is an important analytical tool for studying scattering problems involving perturbations in scatterers. Many applications, including inverse scattering, optimal design, and uncertainty quantification, are based on shape derivatives. However, computing high order shape derivatives is challenging due to the complexity of shape calculus. This work introduces a comprehensive method for computing shape Taylor expansions in two dimensions using recurrence formulas. The approach is developed under sound-soft, sound-hard, impedance, and transmission boundary conditions. Additionally, we apply the shape Taylor expansion to uncertainty quantification in wave scattering, enabling high order moment estimation for the scattered field under random boundary perturbations. Numerical examples are provided to illustrate the effectiveness of the shape Taylor expansion in achieving high order approximations.","authors":["Gang Bao","Jun Lai","Haoran Ma"],"url":"https://arxiv.org/abs/2504.06621"}
{"created":"2025-04-11","title":"FACT: Multinomial Misalignment Classification for Point Cloud Registration","abstract":"We present FACT, a method for predicting alignment quality (i.e., registration error) of registered lidar point cloud pairs. This is useful e.g. for quality assurance of large, automatically registered 3D models. FACT extracts local features from a registered pair and processes them with a point transformer-based network to predict a misalignment class. We generalize prior work that study binary alignment classification of registration errors, by recasting it as multinomial misalignment classification. To achieve this, we introduce a custom regression-by-classification loss function that combines the cross-entropy and Wasserstein losses, and demonstrate that it outperforms both direct regression and prior binary classification. FACT successfully classifies point-cloud pairs registered with both the classical ICP and GeoTransformer, while other choices, such as standard point-cloud-quality metrics and registration residuals are shown to be poor choices for predicting misalignment. On a synthetically perturbed point-cloud task introduced by the CorAl method, we show that FACT achieves substantially better performance than CorAl. Finally, we demonstrate how FACT can assist experts in correcting misaligned point-cloud maps. Our code is available at https://github.com/LudvigDillen/FACT_for_PCMC.","authors":["Ludvig Dill\\'en","Per-Erik Forss\\'en","Johan Edstedt"],"url":"https://arxiv.org/abs/2504.06627"}
{"created":"2025-04-11","title":"Rethinking LayerNorm in Image Restoration Transformers","abstract":"This work investigates abnormal feature behaviors observed in image restoration (IR) Transformers. Specifically, we identify two critical issues: feature entropy becoming excessively small and feature magnitudes diverging up to a million-fold scale. We pinpoint the root cause to the per-token normalization aspect of conventional LayerNorm, which disrupts essential spatial correlations and internal feature statistics. To address this, we propose a simple normalization strategy tailored for IR Transformers. Our approach applies normalization across the entire spatio-channel dimension, effectively preserving spatial correlations. Additionally, we introduce an input-adaptive rescaling method that aligns feature statistics to the unique statistical requirements of each input. Experimental results verify that this combined strategy effectively resolves feature divergence, significantly enhancing both the stability and performance of IR Transformers across various IR tasks.","authors":["MinKyu Lee","Sangeek Hyun","Woojin Jun","Hyunjun Kim","Jiwoo Chung","Jae-Pil Heo"],"url":"https://arxiv.org/abs/2504.06629"}
{"created":"2025-04-11","title":"The Method for Storing Patterns in Neural Networks-Memorization and Recall of QR code Patterns-","abstract":"In this paper, we propose a mechanism for storing complex patterns within a neural network and subsequently recalling them. This model is based on our work published in 2018(Inazawa, 2018), which we have refined and extended in this work. With the recent advancements in deep learning and large language model (LLM)-based AI technologies (generative AI), it can be considered that methodologies for the learning are becoming increasingly well-established. In the future, we expect to see further research on memory using models based on Transformers (Vaswani, et. al., 2017, Rae, et. al., 2020), but in this paper we propose a simpler and more powerful model of memory and recall in neural networks. The advantage of storing patterns in a neural network lies in its ability to recall the original pattern even when an incomplete version is presented. The patterns we have produced for use in this study have been QR code (DENSO WAVE, 1994), which has become widely used as an information transmission tool in recent years.","authors":["Hiroshi Inazawa"],"url":"https://arxiv.org/abs/2504.06631"}
{"created":"2025-04-11","title":"PosterMaker: Towards High-Quality Product Poster Generation with Accurate Text Rendering","abstract":"Product posters, which integrate subject, scene, and text, are crucial promotional tools for attracting customers. Creating such posters using modern image generation methods is valuable, while the main challenge lies in accurately rendering text, especially for complex writing systems like Chinese, which contains over 10,000 individual characters. In this work, we identify the key to precise text rendering as constructing a character-discriminative visual feature as a control signal. Based on this insight, we propose a robust character-wise representation as control and we develop TextRenderNet, which achieves a high text rendering accuracy of over 90%. Another challenge in poster generation is maintaining the fidelity of user-specific products. We address this by introducing SceneGenNet, an inpainting-based model, and propose subject fidelity feedback learning to further enhance fidelity. Based on TextRenderNet and SceneGenNet, we present PosterMaker, an end-to-end generation framework. To optimize PosterMaker efficiently, we implement a two-stage training strategy that decouples text rendering and background generation learning. Experimental results show that PosterMaker outperforms existing baselines by a remarkable margin, which demonstrates its effectiveness.","authors":["Yifan Gao","Zihang Lin","Chuanbin Liu","Min Zhou","Tiezheng Ge","Bo Zheng","Hongtao Xie"],"url":"https://arxiv.org/abs/2504.06632"}
{"created":"2025-04-11","title":"A Serendipitous Recommendation System Considering User Curiosity","abstract":"To address the problem of narrow recommendation ranges caused by an emphasis on prediction accuracy, serendipitous recommendations, which consider both usefulness and unexpectedness, have attracted attention. However, realizing serendipitous recommendations is challenging due to the varying proportions of usefulness and unexpectedness preferred by different users, which is influenced by their differing desires for knowledge. In this paper, we propose a method to estimate the proportion of usefulness and unexpectedness that each user desires based on their curiosity, and make recommendations that match this preference. The proposed method estimates a user's curiosity by considering both their long-term and short-term interests. Offline experiments were conducted using the MovieLens-1M dataset to evaluate the effectiveness of the proposed method. The experimental results demonstrate that our method achieves the same level of performance as state-of-the-art method while successfully providing serendipitous recommendations.","authors":["Zhelin Xu","Atsushi Matsumura"],"url":"https://arxiv.org/abs/2504.06633"}
{"created":"2025-04-11","title":"Crafting Query-Aware Selective Attention for Single Image Super-Resolution","abstract":"Single Image Super-Resolution (SISR) reconstructs high-resolution images from low-resolution inputs, enhancing image details. While Vision Transformer (ViT)-based models improve SISR by capturing long-range dependencies, they suffer from quadratic computational costs or employ selective attention mechanisms that do not explicitly focus on query-relevant regions. Despite these advancements, prior work has overlooked how selective attention mechanisms should be effectively designed for SISR. We propose SSCAN, which dynamically selects the most relevant key-value windows based on query similarity, ensuring focused feature extraction while maintaining efficiency. In contrast to prior approaches that apply attention globally or heuristically, our method introduces a query-aware window selection strategy that better aligns attention computation with important image regions. By incorporating fixed-sized windows, SSCAN reduces memory usage and enforces linear token-to-token complexity, making it scalable for large images. Our experiments demonstrate that SSCAN outperforms existing attention-based SISR methods, achieving up to 0.14 dB PSNR improvement on urban datasets, guaranteeing both computational efficiency and reconstruction quality in SISR.","authors":["Junyoung Kim","Youngrok Kim","Siyeol Jung","Donghyun Min"],"url":"https://arxiv.org/abs/2504.06634"}
{"created":"2025-04-11","title":"BBQRec: Behavior-Bind Quantization for Multi-Modal Sequential Recommendation","abstract":"Multi-modal sequential recommendation systems leverage auxiliary signals (e.g., text, images) to alleviate data sparsity in user-item interactions. While recent methods exploit large language models to encode modalities into discrete semantic IDs for autoregressive prediction, we identify two critical limitations: (1) Existing approaches adopt fragmented quantization, where modalities are independently mapped to semantic spaces misaligned with behavioral objectives, and (2) Over-reliance on semantic IDs disrupts inter-modal semantic coherence, thereby weakening the expressive power of multi-modal representations for modeling diverse user preferences.","authors":["Kaiyuan Li","Rui Xiang","Yong Bai","Yongxiang Tang","Yanhua Cheng","Xialong Liu","Peng Jiang","Kun Gai"],"url":"https://arxiv.org/abs/2504.06636"}
{"created":"2025-04-11","title":"SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex Multimodal Reasoning in Academic Areas","abstract":"Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate impressive problem-solving skills in many tasks and domains. However, their ability to reason with complex images in academic domains has not been systematically investigated. To bridge this gap, we present SCI-Reason, a dataset for complex multimodel reasoning in academic areas. SCI-Reason aims to test and improve the reasoning ability of large multimodal models using real complex images in academic domains. The dataset contains 12,066 images and 12,626 question-answer pairs extracted from PubMed, divided into training, validation and test splits. Each question-answer pair also contains an accurate and efficient inference chain as a guide to improving the inference properties of the dataset. With SCI-Reason, we performed a comprehensive evaluation of 8 well-known models. The best performing model, Claude-3.7-Sonnet, only achieved an accuracy of 55.19%. Error analysis shows that more than half of the model failures are due to breakdowns in multi-step inference chains rather than errors in primary visual feature extraction. This finding underscores the inherent limitations in reasoning capabilities exhibited by current multimodal models when processing complex image analysis tasks within authentic academic contexts. Experiments on open-source models show that SCI-Reason not only enhances reasoning ability but also demonstrates cross-domain generalization in VQA tasks. We also explore future applications of model inference capabilities in this domain, highlighting its potential for future research.","authors":["Chenghao Ma","Haihong E.","Junpeng Ding","Jun Zhang","Ziyan Ma","Huang Qing","Bofei Gao","Liang Chen","Meina Song"],"url":"https://arxiv.org/abs/2504.06637"}
{"created":"2025-04-11","title":"HGMamba: Enhancing 3D Human Pose Estimation with a HyperGCN-Mamba Network","abstract":"3D human pose lifting is a promising research area that leverages estimated and ground-truth 2D human pose data for training. While existing approaches primarily aim to enhance the performance of estimated 2D poses, they often struggle when applied to ground-truth 2D pose data. We observe that achieving accurate 3D pose reconstruction from ground-truth 2D poses requires precise modeling of local pose structures, alongside the ability to extract robust global spatio-temporal features. To address these challenges, we propose a novel Hyper-GCN and Shuffle Mamba (HGMamba) block, which processes input data through two parallel streams: Hyper-GCN and Shuffle-Mamba. The Hyper-GCN stream models the human body structure as hypergraphs with varying levels of granularity to effectively capture local joint dependencies. Meanwhile, the Shuffle Mamba stream leverages a state space model to perform spatio-temporal scanning across all joints, enabling the establishment of global dependencies. By adaptively fusing these two representations, HGMamba achieves strong global feature modeling while excelling at local structure modeling. We stack multiple HGMamba blocks to create three variants of our model, allowing users to select the most suitable configuration based on the desired speed-accuracy trade-off. Extensive evaluations on the Human3.6M and MPI-INF-3DHP benchmark datasets demonstrate the effectiveness of our approach. HGMamba-B achieves state-of-the-art results, with P1 errors of 38.65 mm and 14.33 mm on the respective datasets. Code and models are available: https://github.com/HuCui2022/HGMamba","authors":["Hu Cui","Tessai Hayama"],"url":"https://arxiv.org/abs/2504.06638"}
{"created":"2025-04-11","title":"Design and use of devices to assist movement of the upper limb: review of the literature","abstract":"This article explores assistive devices for upper limb movement in people with disabilities through a systematic review based on the PRISMA methodology. The studied devices encompass technologies ranging from orthoses to advanced robotics, aiming to compensate for or supplement motor impairments. The results highlight the diversity of applications (rehabilitation, daily living activities), targeted body segments (distal, proximal, or global), as well as control mechanisms and interfaces used. However, despite the variety of promising prototypes, few devices are commercially available, limiting their real impact on end users. Existing technologies, while effective in improving functional autonomy and quality of life, still face challenges in terms of ergonomics, cost, and portability. In conclusion, this article emphasizes the importance of a user-centered approach and proposes avenues for the development of innovative, modular, and accessible assistive devices.","authors":["Charlotte Le Goff (CAMIN)","Pauline Coignard (CAMIN)","Christine Azevedo-Coste (CAMIN)","Franck Geffard (DIN)","Charles Fattal (CAMIN)"],"url":"https://arxiv.org/abs/2504.06640"}
{"created":"2025-04-11","title":"AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection","abstract":"Unsupervised multivariate time series anomaly detection (UMTSAD) plays a critical role in various domains, including finance, networks, and sensor systems. In recent years, due to the outstanding performance of deep learning in general sequential tasks, many models have been specialized for deep UMTSAD tasks and have achieved impressive results, particularly those based on the Transformer and self-attention mechanisms. However, the sequence anomaly association assumptions underlying these models are often limited to specific predefined patterns and scenarios, such as concentrated or peak anomaly patterns. These limitations hinder their ability to generalize to diverse anomaly situations, especially where the lack of labels poses significant challenges. To address these issues, we propose AMAD, which integrates \\textbf{A}uto\\textbf{M}asked Attention for UMTS\\textbf{AD} scenarios. AMAD introduces a novel structure based on the AutoMask mechanism and an attention mixup module, forming a simple yet generalized anomaly association representation framework. This framework is further enhanced by a Max-Min training strategy and a Local-Global contrastive learning approach. By combining multi-scale feature extraction with automatic relative association modeling, AMAD provides a robust and adaptable solution to UMTSAD challenges. Extensive experimental results demonstrate that the proposed model achieving competitive performance results compared to SOTA benchmarks across a variety of datasets.","authors":["Tiange Huang","Yongjun Li"],"url":"https://arxiv.org/abs/2504.06643"}
{"created":"2025-04-11","title":"Beyond the Winding Path of Learning: Exploring Affective, Cognitive, and Action-Oriented Prompts for Communication Skills","abstract":"Since high dropout rates in online learning platforms were reported, various factors affecting learner retention have been identified, with learners' perceptions of their experiences playing a crucial role in shaping their persistence. For instance, Kittur et al. highlight how success expectations are shaped by perceived system fit and course difficulty.","authors":["Naoko Hayashida"],"url":"https://arxiv.org/abs/2504.06646"}
{"created":"2025-04-11","title":"Uni-PrevPredMap: Extending PrevPredMap to a Unified Framework of Prior-Informed Modeling for Online Vectorized HD Map Construction","abstract":"Safety constitutes a foundational imperative for autonomous driving systems, necessitating the maximal incorporation of accessible external prior information. This study establishes that temporal perception buffers and cost-efficient maps inherently form complementary prior sources for online vectorized high-definition (HD) map construction. We present Uni-PrevPredMap, a unified prior-informed framework that systematically integrates two synergistic information sources: previous predictions and simulated outdated HD maps. The framework introduces two core innovations: a tile-indexed 3D vectorized global map processor enabling efficient refreshment, storage, and retrieval of 3D vectorized priors; a tri-mode operational optimization paradigm ensuring consistency across non-prior, temporal-prior, and temporal-map-fusion-prior scenarios while mitigating reliance on idealized map fidelity assumptions. Uni-PrevPredMap achieves state-of-the-art performance in map-absent scenarios across established online vectorized HD map construction benchmarks. When provided with simulated outdated HD maps, the framework exhibits robust capabilities in error-resilient prior fusion, empirically confirming the synergistic complementarity between previous predictions and simulated outdated HD maps. Code will be available at https://github.com/pnnnnnnn/Uni-PrevPredMap.","authors":["Nan Peng","Xun Zhou","Mingming Wang","Guisong Chen","Wenqi Xu"],"url":"https://arxiv.org/abs/2504.06647"}
{"created":"2025-04-11","title":"GRAIN: Multi-Granular and Implicit Information Aggregation Graph Neural Network for Heterophilous Graphs","abstract":"Graph neural networks (GNNs) have shown significant success in learning graph representations. However, recent studies reveal that GNNs often fail to outperform simple MLPs on heterophilous graph tasks, where connected nodes may differ in features or labels, challenging the homophily assumption. Existing methods addressing this issue often overlook the importance of information granularity and rarely consider implicit relationships between distant nodes. To overcome these limitations, we propose the Granular and Implicit Graph Network (GRAIN), a novel GNN model specifically designed for heterophilous graphs. GRAIN enhances node embeddings by aggregating multi-view information at various granularity levels and incorporating implicit data from distant, non-neighboring nodes. This approach effectively integrates local and global information, resulting in smoother, more accurate node representations. We also introduce an adaptive graph information aggregator that efficiently combines multi-granularity and implicit data, significantly improving node representation quality, as shown by experiments on 13 datasets covering varying homophily and heterophily. GRAIN consistently outperforms 12 state-of-the-art models, excelling on both homophilous and heterophilous graphs.","authors":["Songwei Zhao","Yuan Jiang","Zijing Zhang","Yang Yu","Hechang Chen"],"url":"https://arxiv.org/abs/2504.06649"}
{"created":"2025-04-11","title":"ThoughtProbe: Classifier-Guided Thought Space Exploration Leveraging LLM Intrinsic Reasoning","abstract":"Pre-trained large language models (LLMs) have been demonstrated to possess intrinsic reasoning capabilities that can emerge naturally when expanding the response space. However, the neural representation mechanisms underlying these intrinsic capabilities and approaches for their optimal utilization remain inadequately understood. In this work, we make the key discovery that a simple linear classifier can effectively detect intrinsic reasoning capabilities in LLMs' activation space, particularly within specific representation types and network layers. Based on this finding, we propose a classifier-guided search framework that strategically explore a tree-structured response space. In each node expansion, the classifier serves as a scoring and ranking mechanism that efficiently allocates computational resources by identifying and prioritizing more thoughtful reasoning directions for continuation. After completing the tree expansion, we collect answers from all branches to form a candidate answer pool. We propose a branch-aggregation selection method that marginalizes over all supporting branches by aggregating their thoughtfulness scores, thereby identifying the optimal answer from the pool. Experimental results show that our framework's comprehensive exploration not only covers valid reasoning chains but also effectively identifies them, achieving significant improvements across multiple arithmetic reasoning benchmarks.","authors":["Zijian Wang","Chang Xu"],"url":"https://arxiv.org/abs/2504.06650"}
{"created":"2025-04-11","title":"Collision avoidance from monocular vision trained with novel view synthesis","abstract":"Collision avoidance can be checked in explicit environment models such as elevation maps or occupancy grids, yet integrating such models with a locomotion policy requires accurate state estimation. In this work, we consider the question of collision avoidance from an implicit environment model. We use monocular RGB images as inputs and train a collisionavoidance policy from photorealistic images generated by 2D Gaussian splatting. We evaluate the resulting pipeline in realworld experiments under velocity commands that bring the robot on an intercept course with obstacles. Our results suggest that RGB images can be enough to make collision-avoidance decisions, both in the room where training data was collected and in out-of-distribution environments.","authors":["Valentin Tordjman--Levavasseur (WILLOW)","St\\'ephane Caron (WILLOW)"],"url":"https://arxiv.org/abs/2504.06651"}
{"created":"2025-04-11","title":"A Neuro-inspired Interpretation of Unlearning in Large Language Models through Sample-level Unlearning Difficulty","abstract":"Driven by privacy protection laws and regulations, unlearning in Large Language Models (LLMs) is gaining increasing attention. However, current research often neglects the interpretability of the unlearning process, particularly concerning sample-level unlearning difficulty. Existing studies typically assume a uniform unlearning difficulty across samples. This simplification risks attributing the performance of unlearning algorithms to sample selection rather than the algorithm's design, potentially steering the development of LLM unlearning in the wrong direction. Thus, we investigate the relationship between LLM unlearning and sample characteristics, with a focus on unlearning difficulty. Drawing inspiration from neuroscience, we propose a Memory Removal Difficulty ($\\mathrm{MRD}$) metric to quantify sample-level unlearning difficulty. Using $\\mathrm{MRD}$, we analyze the characteristics of hard-to-unlearn versus easy-to-unlearn samples. Furthermore, we propose an $\\mathrm{MRD}$-based weighted sampling method to optimize existing unlearning algorithms, which prioritizes easily forgettable samples, thereby improving unlearning efficiency and effectiveness. We validate the proposed metric and method using public benchmarks and datasets, with results confirming its effectiveness.","authors":["Xiaohua Feng","Yuyuan Li","Chengye Wang","Junlin Liu","Li Zhang","Chaochao Chen"],"url":"https://arxiv.org/abs/2504.06658"}
{"created":"2025-04-11","title":"Bridging the Gap Between Preference Alignment and Machine Unlearning","abstract":"Despite advances in Preference Alignment (PA) for Large Language Models (LLMs), mainstream methods like Reinforcement Learning with Human Feedback (RLHF) face notable challenges. These approaches require high-quality datasets of positive preference examples, which are costly to obtain and computationally intensive due to training instability, limiting their use in low-resource scenarios. LLM unlearning technique presents a promising alternative, by directly removing the influence of negative examples. However, current research has primarily focused on empirical validation, lacking systematic quantitative analysis. To bridge this gap, we propose a framework to explore the relationship between PA and LLM unlearning. Specifically, we introduce a bi-level optimization-based method to quantify the impact of unlearning specific negative examples on PA performance. Our analysis reveals that not all negative examples contribute equally to alignment improvement when unlearned, and the effect varies significantly across examples. Building on this insight, we pose a crucial question: how can we optimally select and weight negative examples for unlearning to maximize PA performance? To answer this, we propose a framework called Unlearning to Align (U2A), which leverages bi-level optimization to efficiently select and unlearn examples for optimal PA performance. We validate the proposed method through extensive experiments, with results confirming its effectiveness.","authors":["Xiaohua Feng","Yuyuan Li","Huwei Ji","Jiaming Zhang","Li Zhang","Tianyu Du","Chaochao Chen"],"url":"https://arxiv.org/abs/2504.06659"}
{"created":"2025-04-11","title":"Robust and Noise-resilient Long-Term Prediction of Spatiotemporal Data Using Variational Mode Graph Neural Networks with 3D Attention","abstract":"This paper focuses on improving the robustness of spatiotemporal long-term prediction using a variational mode graph convolutional network (VMGCN) by introducing 3D channel attention. The deep learning network for this task relies on historical data inputs, yet real-time data can be corrupted by sensor noise, altering its distribution. We model this noise as independent and identically distributed (i.i.d.) Gaussian noise and incorporate it into the LargeST traffic volume dataset, resulting in data with both inherent and additive noise components. Our approach involves decomposing the corrupted signal into modes using variational mode decomposition, followed by feeding the data into a learning pipeline for prediction. We integrate a 3D attention mechanism encompassing spatial, temporal, and channel attention. The spatial and temporal attention modules learn their respective correlations, while the channel attention mechanism is used to suppress noise and highlight the significant modes in the spatiotemporal signals. Additionally, a learnable soft thresholding method is implemented to exclude unimportant modes from the feature vector, and a feature reduction method based on the signal-to-noise ratio (SNR) is applied. We compare the performance of our approach against baseline models, demonstrating that our method achieves superior long-term prediction accuracy, robustness to noise, and improved performance with mode truncation compared to the baseline models. The code of the paper is available at https://github.com/OsamaAhmad369/VMGCN.","authors":["Osama Ahmad","Zubair Khalid"],"url":"https://arxiv.org/abs/2504.06660"}
{"created":"2025-04-11","title":"Domain-Conditioned Scene Graphs for State-Grounded Task Planning","abstract":"Recent robotic task planning frameworks have integrated large multimodal models (LMMs) such as GPT-4V. To address grounding issues of such models, it has been suggested to split the pipeline into perceptional state grounding and subsequent state-based planning. As we show in this work, the state grounding ability of LMM-based approaches is still limited by weaknesses in granular, structured, domain-specific scene understanding. To address this shortcoming, we develop a more structured state grounding framework that features a domain-conditioned scene graph as its scene representation. We show that such representation is actionable in nature as it is directly mappable to a symbolic state in classical planning languages such as PDDL. We provide an instantiation of our state grounding framework where the domain-conditioned scene graph generation is implemented with a lightweight vision-language approach that classifies domain-specific predicates on top of domain-relevant object detections. Evaluated across three domains, our approach achieves significantly higher state estimation accuracy and task planning success rates compared to the previous LMM-based approaches.","authors":["Jonas Herzog","Jiangpin Liu","Yue Wang"],"url":"https://arxiv.org/abs/2504.06661"}
{"created":"2025-04-11","title":"RAMBO: RL-augmented Model-based Optimal Control for Whole-body Loco-manipulation","abstract":"Loco-manipulation -- coordinated locomotion and physical interaction with objects -- remains a major challenge for legged robots due to the need for both accurate force interaction and robustness to unmodeled dynamics. While model-based controllers provide interpretable dynamics-level planning and optimization, they are limited by model inaccuracies and computational cost. In contrast, learning-based methods offer robustness while struggling with precise modulation of interaction forces. We introduce RAMBO -- RL-Augmented Model-Based Optimal Control -- a hybrid framework that integrates model-based reaction force optimization using a simplified dynamics model and a feedback policy trained with reinforcement learning. The model-based module generates feedforward torques by solving a quadratic program, while the policy provides feedback residuals to enhance robustness in control execution. We validate our framework on a quadruped robot across a diverse set of real-world loco-manipulation tasks -- such as pushing a shopping cart, balancing a plate, and holding soft objects -- in both quadrupedal and bipedal walking. Our experiments demonstrate that RAMBO enables precise manipulation while achieving robust and dynamic locomotion, surpassing the performance of policies trained with end-to-end scheme. In addition, our method enables flexible trade-off between end-effector tracking accuracy with compliance.","authors":["Jin Cheng","Dongho Kang","Gabriele Fadini","Guanya Shi","Stelian Coros"],"url":"https://arxiv.org/abs/2504.06662"}
{"created":"2025-04-11","title":"SEE: Continual Fine-tuning with Sequential Ensemble of Experts","abstract":"Continual fine-tuning of large language models (LLMs) suffers from catastrophic forgetting. Rehearsal-based methods mitigate this problem by retaining a small set of old data. Nevertheless, they still suffer inevitable performance loss. Although training separate experts for each task can help prevent forgetting, effectively assembling them remains a challenge. Some approaches use routers to assign tasks to experts, but in continual learning, they often require retraining for optimal performance. To address these challenges, we introduce the Sequential Ensemble of Experts (SEE) framework. SEE removes the need for an additional router, allowing each expert to independently decide whether a query should be handled. The framework employs distributed routing, and during continual fine-tuning, SEE only requires the training of new experts for incoming tasks rather than retraining the entire system. Experiments reveal that the SEE outperforms prior approaches, including multi-task learning, in continual fine-tuning. It also demonstrates remarkable generalization ability, as the expert can effectively identify out-of-distribution queries, which can then be directed to a more generalized model for resolution. This work highlights the promising potential of integrating routing and response mechanisms within each expert, paving the way for the future of distributed model ensembling.","authors":["Zhilin Wang","Yafu Li","Xiaoye Qu","Yu Cheng"],"url":"https://arxiv.org/abs/2504.06664"}
{"created":"2025-04-11","title":"Patch Matters: Training-free Fine-grained Image Caption Enhancement via Local Perception","abstract":"High-quality image captions play a crucial role in improving the performance of cross-modal applications such as text-to-image generation, text-to-video generation, and text-image retrieval. To generate long-form, high-quality captions, many recent studies have employed multimodal large language models (MLLMs). However, current MLLMs often produce captions that lack fine-grained details or suffer from hallucinations, a challenge that persists in both open-source and closed-source models. Inspired by Feature-Integration theory, which suggests that attention must focus on specific regions to integrate visual information effectively, we propose a \\textbf{divide-then-aggregate} strategy. Our method first divides the image into semantic and spatial patches to extract fine-grained details, enhancing the model's local perception of the image. These local details are then hierarchically aggregated to generate a comprehensive global description. To address hallucinations and inconsistencies in the generated captions, we apply a semantic-level filtering process during hierarchical aggregation. This training-free pipeline can be applied to both open-source models (LLaVA-1.5, LLaVA-1.6, Mini-Gemini) and closed-source models (Claude-3.5-Sonnet, GPT-4o, GLM-4V-Plus). Extensive experiments demonstrate that our method generates more detailed, reliable captions, advancing multimodal description generation without requiring model retraining. The source code are available at https://github.com/GeWu-Lab/Patch-Matters","authors":["Ruotian Peng","Haiying He","Yake Wei","Yandong Wen","Di Hu"],"url":"https://arxiv.org/abs/2504.06666"}
{"created":"2025-04-11","title":"Toward Holistic Evaluation of Recommender Systems Powered by Generative Models","abstract":"Recommender systems powered by generative models (Gen-RecSys) extend beyond classical item ranking by producing open-ended content, which simultaneously unlocks richer user experiences and introduces new risks. On one hand, these systems can enhance personalization and appeal through dynamic explanations and multi-turn dialogues. On the other hand, they might venture into unknown territory-hallucinating nonexistent items, amplifying bias, or leaking private information. Traditional accuracy metrics cannot fully capture these challenges, as they fail to measure factual correctness, content safety, or alignment with user intent.","authors":["Yashar Deldjoo","Nikhil Mehta","Maheswaran Sathiamoorthy","Shuai Zhang","Pablo Castells","Julian McAuley"],"url":"https://arxiv.org/abs/2504.06667"}
{"created":"2025-04-11","title":"NLP Security and Ethics, in the Wild","abstract":"As NLP models are used by a growing number of end-users, an area of increasing importance is NLP Security (NLPSec): assessing the vulnerability of models to malicious attacks and developing comprehensive countermeasures against them. While work at the intersection of NLP and cybersecurity has the potential to create safer NLP for all, accidental oversights can result in tangible harm (e.g., breaches of privacy or proliferation of malicious models). In this emerging field, however, the research ethics of NLP have not yet faced many of the long-standing conundrums pertinent to cybersecurity, until now. We thus examine contemporary works across NLPSec, and explore their engagement with cybersecurity's ethical norms. We identify trends across the literature, ultimately finding alarming gaps on topics like harm minimization and responsible disclosure. To alleviate these concerns, we provide concrete recommendations to help NLP researchers navigate this space more ethically, bridging the gap between traditional cybersecurity and NLP ethics, which we frame as ``white hat NLP''. The goal of this work is to help cultivate an intentional culture of ethical research for those working in NLP Security.","authors":["Heather Lent","Erick Galinkin","Yiyi Chen","Jens Myrup Pedersen","Leon Derczynski","Johannes Bjerva"],"url":"https://arxiv.org/abs/2504.06669"}
{"created":"2025-04-11","title":"Dynamic Residual Safe Reinforcement Learning for Multi-Agent Safety-Critical Scenarios Decision-Making","abstract":"In multi-agent safety-critical scenarios, traditional autonomous driving frameworks face significant challenges in balancing safety constraints and task performance. These frameworks struggle to quantify dynamic interaction risks in real-time and depend heavily on manual rules, resulting in low computational efficiency and conservative strategies. To address these limitations, we propose a Dynamic Residual Safe Reinforcement Learning (DRS-RL) framework grounded in a safety-enhanced networked Markov decision process. It's the first time that the weak-to-strong theory is introduced into multi-agent decision-making, enabling lightweight dynamic calibration of safety boundaries via a weak-to-strong safety correction paradigm. Based on the multi-agent dynamic conflict zone model, our framework accurately captures spatiotemporal coupling risks among heterogeneous traffic participants and surpasses the static constraints of conventional geometric rules. Moreover, a risk-aware prioritized experience replay mechanism mitigates data distribution bias by mapping risk to sampling probability. Experimental results reveal that the proposed method significantly outperforms traditional RL algorithms in safety, efficiency, and comfort. Specifically, it reduces the collision rate by up to 92.17%, while the safety model accounts for merely 27% of the main model's parameters.","authors":["Kaifeng Wang","Yinsong Chen","Qi Liu","Xueyuan Li","Xin Gao"],"url":"https://arxiv.org/abs/2504.06670"}
{"created":"2025-04-11","title":"RAGME: Retrieval Augmented Video Generation for Enhanced Motion Realism","abstract":"Video generation is experiencing rapid growth, driven by advances in diffusion models and the development of better and larger datasets. However, producing high-quality videos remains challenging due to the high-dimensional data and the complexity of the task. Recent efforts have primarily focused on enhancing visual quality and addressing temporal inconsistencies, such as flickering. Despite progress in these areas, the generated videos often fall short in terms of motion complexity and physical plausibility, with many outputs either appearing static or exhibiting unrealistic motion. In this work, we propose a framework to improve the realism of motion in generated videos, exploring a complementary direction to much of the existing literature. Specifically, we advocate for the incorporation of a retrieval mechanism during the generation phase. The retrieved videos act as grounding signals, providing the model with demonstrations of how the objects move. Our pipeline is designed to apply to any text-to-video diffusion model, conditioning a pretrained model on the retrieved samples with minimal fine-tuning. We demonstrate the superiority of our approach through established metrics, recently proposed benchmarks, and qualitative results, and we highlight additional applications of the framework.","authors":["Elia Peruzzo","Dejia Xu","Xingqian Xu","Humphrey Shi","Nicu Sebe"],"url":"https://arxiv.org/abs/2504.06672"}
{"created":"2025-04-11","title":"Probability Density Geodesics in Image Diffusion Latent Space","abstract":"Diffusion models indirectly estimate the probability density over a data space, which can be used to study its structure. In this work, we show that geodesics can be computed in diffusion latent space, where the norm induced by the spatially-varying inner product is inversely proportional to the probability density. In this formulation, a path that traverses a high density (that is, probable) region of image latent space is shorter than the equivalent path through a low density region. We present algorithms for solving the associated initial and boundary value problems and show how to compute the probability density along the path and the geodesic distance between two points. Using these techniques, we analyze how closely video clips approximate geodesics in a pre-trained image diffusion space. Finally, we demonstrate how these techniques can be applied to training-free image sequence interpolation and extrapolation, given a pre-trained image diffusion model.","authors":["Qingtao Yu","Jaskirat Singh","Zhaoyuan Yang","Peter Henry Tu","Jing Zhang","Hongdong Li","Richard Hartley","Dylan Campbell"],"url":"https://arxiv.org/abs/2504.06675"}
{"created":"2025-04-11","title":"Setup-Invariant Augmented Reality for Teaching by Demonstration with Surgical Robots","abstract":"Augmented reality (AR) is an effective tool in robotic surgery education as it combines exploratory learning with three-dimensional guidance. However, existing AR systems require expert supervision and do not account for differences in the mentor and mentee robot configurations. To enable novices to train outside the operating room while receiving expert-informed guidance, we present dV-STEAR: an open-source system that plays back task-aligned expert demonstrations without assuming identical setup joint positions between expert and novice. Pose estimation was rigorously quantified, showing a registration error of 3.86 (SD=2.01)mm. In a user study (N=24), dV-STEAR significantly improved novice performance on tasks from the Fundamentals of Laparoscopic Surgery. In a single-handed ring-over-wire task, dV-STEAR increased completion speed (p=0.03) and reduced collision time (p=0.01) compared to dry-lab training alone. During a pick-and-place task, it improved success rates (p=0.004). Across both tasks, participants using dV-STEAR exhibited significantly more balanced hand use and reported lower frustration levels. This work presents a novel educational tool implemented on the da Vinci Research Kit, demonstrates its effectiveness in teaching novices, and builds the foundation for further AR integration into robot-assisted surgery.","authors":["Alexandre Banks","Richard Cook","Septimiu E. Salcudean"],"url":"https://arxiv.org/abs/2504.06677"}
{"created":"2025-04-11","title":"Deep Learning for Cardiovascular Risk Assessment: Proxy Features from Carotid Sonography as Predictors of Arterial Damage","abstract":"In this study, hypertension is utilized as an indicator of individual vascular damage. This damage can be identified through machine learning techniques, providing an early risk marker for potential major cardiovascular events and offering valuable insights into the overall arterial condition of individual patients. To this end, the VideoMAE deep learning model, originally developed for video classification, was adapted by finetuning for application in the domain of ultrasound imaging. The model was trained and tested using a dataset comprising over 31,000 carotid sonography videos sourced from the Gutenberg Health Study (15,010 participants), one of the largest prospective population health studies. This adaptation facilitates the classification of individuals as hypertensive or non-hypertensive (75.7% validation accuracy), functioning as a proxy for detecting visual arterial damage. We demonstrate that our machine learning model effectively captures visual features that provide valuable insights into an individual's overall cardiovascular health.","authors":["Christoph Balada","Aida Romano-Martinez","Vincent ten Cate","Katharina Geschke","Jonas Tesarz","Paul Cla{\\ss}en","Alexander K. Schuster","Dativa Tibyampansha","Karl-Patrik Kresoja","Philipp S. Wild","Sheraz Ahmed","Andreas Dengel"],"url":"https://arxiv.org/abs/2504.06680"}
{"created":"2025-04-11","title":"Bridging Research and Standardization: Innovations and Methodology for 6G Standard Contributions","abstract":"The transition towards 6G presents unique challenges and opportunities in mobile networks design and standardization. Addressing these challenges requires a robust methodology for analyzing and selecting innovations that can be effectively translated into 3rd Generation Partnership Project (3GPP) contributions. This paper presents a systematic approach to bridging research and standardization, ensuring that cutting-edge advancements extend beyond academia and translate into concrete standardization efforts. The proposed methodology has been applied within the Italian RESTART framework to two ongoing research areas: Morphable Programmable Networks (MPNs) and Network Digital Twins (NDTs), both key enablers of next-generation networks. MPNs enhance dynamic adaptability and resource management, while NDTs enable real-time simulation, predictive analytics, and intelligent decision-making. Their integration into 3GPP Release 20 will be instrumental in shaping a flexible and future-proof mobile ecosystem. These innovations exemplify how research-driven solutions can align with 6G standardization objectives. By applying the proposed methodology, we aim to establish a systematic pathway for transitioning research into impactful 3GPP contributions, ultimately driving the evolution of next-generation networks.","authors":["Francesca Conserva","Fabio Busacca","Corrado Puligheddu","Simone Bizzarri","Maurizio Fodrini","Giampaolo Cuozzo","Riccardo Marini"],"url":"https://arxiv.org/abs/2504.06682"}
{"created":"2025-04-11","title":"Hyperparameter Optimisation with Practical Interpretability and Explanation Methods in Probabilistic Curriculum Learning","abstract":"Hyperparameter optimisation (HPO) is crucial for achieving strong performance in reinforcement learning (RL), as RL algorithms are inherently sensitive to hyperparameter settings. Probabilistic Curriculum Learning (PCL) is a curriculum learning strategy designed to improve RL performance by structuring the agent's learning process, yet effective hyperparameter tuning remains challenging and computationally demanding. In this paper, we provide an empirical analysis of hyperparameter interactions and their effects on the performance of a PCL algorithm within standard RL tasks, including point-maze navigation and DC motor control. Using the AlgOS framework integrated with Optuna's Tree-Structured Parzen Estimator (TPE), we present strategies to refine hyperparameter search spaces, enhancing optimisation efficiency. Additionally, we introduce a novel SHAP-based interpretability approach tailored specifically for analysing hyperparameter impacts, offering clear insights into how individual hyperparameters and their interactions influence RL performance. Our work contributes practical guidelines and interpretability tools that significantly improve the effectiveness and computational feasibility of hyperparameter optimisation in reinforcement learning.","authors":["Llewyn Salt","Marcus Gallagher"],"url":"https://arxiv.org/abs/2504.06683"}
{"created":"2025-04-11","title":"SDHN: Skewness-Driven Hypergraph Networks for Enhanced Localized Multi-Robot Coordination","abstract":"Multi-Agent Reinforcement Learning is widely used for multi-robot coordination, where simple graphs typically model pairwise interactions. However, such representations fail to capture higher-order collaborations, limiting effectiveness in complex tasks. While hypergraph-based approaches enhance cooperation, existing methods often generate arbitrary hypergraph structures and lack adaptability to environmental uncertainties. To address these challenges, we propose the Skewness-Driven Hypergraph Network (SDHN), which employs stochastic Bernoulli hyperedges to explicitly model higher-order multi-robot interactions. By introducing a skewness loss, SDHN promotes an efficient structure with Small-Hyperedge Dominant Hypergraph, allowing robots to prioritize localized synchronization while still adhering to the overall information, similar to human coordination. Extensive experiments on Moving Agents in Formation and Robotic Warehouse tasks validate SDHN's effectiveness, demonstrating superior performance over state-of-the-art baselines.","authors":["Delin Zhao","Yanbo Shan","Chang Liu","Shenghang Lin","Yingxin Shou","Bin Xu"],"url":"https://arxiv.org/abs/2504.06684"}
{"created":"2025-04-11","title":"Efficient Timestamping for Sampling-based Race Detection","abstract":"Dynamic race detection based on the happens before (HB) partial order has now become the de facto approach to quickly identify data races in multi-threaded software. Most practical implementations for detecting these races use timestamps to infer causality between events and detect races based on these timestamps. Such an algorithm updates timestamps (stored in vector clocks) at every event in the execution, and is known to induce excessive overhead. Random sampling has emerged as a promising algorithmic paradigm to offset this overhead. It offers the promise of making sound race detection scalable. In this work we consider the task of designing an efficient sampling based race detector with low overhead for timestamping when the number of sampled events is much smaller than the total events in an execution. To solve this problem, we propose (1) a new notion of freshness timestamp, (2) a new data structure to store timestamps, and (3) an algorithm that uses a combination of them to reduce the cost of timestamping in sampling based race detection. Further, we prove that our algorithm is close to optimal -- the number of vector clock traversals is bounded by the number of sampled events and number of threads, and further, on any given dynamic execution, the cost of timestamping due to our algorithm is close to the amount of work any timestamping-based algorithm must perform on that execution, that is it is instance optimal. Our evaluation on real world benchmarks demonstrates the effectiveness of our proposed algorithm over prior timestamping algorithms that are agnostic to sampling.","authors":["Minjian Zhang","Daniel Wee Soong Lim","Mosaad Al Thokair","Umang Mathur","Mahesh Viswanathan"],"url":"https://arxiv.org/abs/2504.06688"}
{"created":"2025-04-11","title":"\"Sorry for bugging you so much.\" Exploring Developers' Behavior Towards Privacy-Compliant Implementation","abstract":"While protecting user data is essential, software developers often fail to fulfill privacy requirements. However, the reasons why they struggle with privacy-compliant implementation remain unclear. Is it due to a lack of knowledge, or is it because of insufficient support? To provide foundational insights in this field, we conducted a qualitative 5-hour programming study with 30 professional software developers implementing 3 privacy-sensitive programming tasks that were designed with GDPR compliance in mind. To explore if and how developers implement privacy requirements, participants were divided into 3 groups: control, privacy prompted, and privacy expert-supported. After task completion, we conducted follow-up interviews. Alarmingly, almost all participants submitted non-GDPR-compliant solutions (79/90). In particular, none of the 3 tasks were solved privacy-compliant by all 30 participants, with the non-prompted group having the lowest number of 3 out of 30 privacy-compliant solution attempts. Privacy prompting and expert support only slightly improved participants' submissions, with 6/30 and 8/30 privacy-compliant attempts, respectively. In fact, all participants reported severe issues addressing common privacy requirements such as purpose limitation, user consent, or data minimization. Counterintuitively, although most developers exhibited minimal confidence in their solutions, they rarely sought online assistance or contacted the privacy expert, with only 4 out of 10 expert-supported participants explicitly asking for compliance confirmation. Instead, participants often relied on existing implementations and focused on implementing functionality and security first.","authors":["Stefan Albert Horstmann","Sandy Hong","David Klein","Raphael Serafini","Martin Degeling","Martin Johns","Veelasha Moonsamy","Alena Naiakshina"],"url":"https://arxiv.org/abs/2504.06697"}
{"created":"2025-04-11","title":"Benchmarking Convolutional Neural Network and Graph Neural Network based Surrogate Models on a Real-World Car External Aerodynamics Dataset","abstract":"Aerodynamic optimization is crucial for developing eco-friendly, aerodynamic, and stylish cars, which requires close collaboration between aerodynamicists and stylists, a collaboration impaired by the time-consuming nature of aerodynamic simulations. Surrogate models offer a viable solution to reduce this overhead, but they are untested in real-world aerodynamic datasets. We present a comparative evaluation of two surrogate modeling approaches for predicting drag on a real-world dataset: a Convolutional Neural Network (CNN) model that uses a signed distance field as input and a commercial tool based on Graph Neural Networks (GNN) that directly processes a surface mesh. In contrast to previous studies based on datasets created from parameterized geometries, our dataset comprises 343 geometries derived from 32 baseline vehicle geometries across five distinct car projects, reflecting the diverse, free-form modifications encountered in the typical vehicle development process. Our results show that the CNN-based method achieves a mean absolute error of 2.3 drag counts, while the GNN-based method achieves 3.8. Both methods achieve approximately 77% accuracy in predicting the direction of drag change relative to the baseline geometry. While both methods effectively capture the broader trends between baseline groups (set of samples derived from a single baseline geometry), they struggle to varying extents in capturing the finer intra-baseline group variations. In summary, our findings suggest that aerodynamicists can effectively use both methods to predict drag in under two minutes, which is at least 600 times faster than performing a simulation. However, there remains room for improvement in capturing the finer details of the geometry.","authors":["Sam Jacob Jacob","Markus Mrosek","Carsten Othmer","Harald K\\\"ostler"],"url":"https://arxiv.org/abs/2504.06699"}
{"created":"2025-04-11","title":"Handling LP-Rounding for Hierarchical Clustering and Fitting Distances by Ultrametrics","abstract":"We consider the classic correlation clustering problem in the hierarchical setting. Given a complete graph $G=(V,E)$ and $\\ell$ layers of input information, where the input of each layer consists of a nonnegative weight and a labeling of the edges with either + or -, this problem seeks to compute for each layer a partition of $V$ such that the partition for any non-top layer subdivides the partition in the upper-layer and the weighted number of disagreements over the layers is minimized.","authors":["Hyung-Chan An","Mong-Jen Kao","Changyeol Lee","Mu-Ting Lee"],"url":"https://arxiv.org/abs/2504.06700"}
{"created":"2025-04-11","title":"CAT: Circular-Convolutional Attention for Sub-Quadratic Transformers","abstract":"Transformers have driven remarkable breakthroughs in natural language processing and computer vision, yet their standard attention mechanism still imposes O(N^2) complexity, hindering scalability to longer sequences. We introduce Circular-convolutional ATtention (CAT), a Fourier-based approach that efficiently applies circular convolutions to reduce complexity without sacrificing representational power. CAT achieves O(NlogN) computations, requires fewer learnable parameters by streamlining fully-connected layers, and introduces no heavier operations, resulting in consistent accuracy improvements and about a 10% speedup in naive PyTorch implementations on large-scale benchmarks such as ImageNet-1k and WikiText-103. Grounded in an engineering-isomorphism framework, CAT's design not only offers practical efficiency and ease of implementation but also provides insights to guide the development of next-generation, high-performance Transformer architectures. Finally, our ablation studies highlight the key conditions underlying CAT's success, shedding light on broader principles for scalable attention mechanisms.","authors":["Yoshihiro Yamada"],"url":"https://arxiv.org/abs/2504.06704"}
{"created":"2025-04-11","title":"Learning-Inspired Fuzzy Logic Algorithms for Enhanced Control of Oscillatory Systems","abstract":"The transportation of sensitive equipment often suffers from vibrations caused by terrain, weather, and motion speed, leading to inefficiencies and potential damage. To address this challenge, this paper explores an intelligent control framework leveraging fuzzy logic, a foundational AI technique, to suppress oscillations in suspension systems. Inspired by learning based methodologies, the proposed approach utilizes fuzzy inference and Gaussian membership functions to emulate adaptive, human like decision making. By minimizing the need for explicit mathematical models, the method demonstrates robustness in both linear and nonlinear systems. Experimental validation highlights the controllers ability to adapt to varying suspension lengths, reducing oscillation amplitudes and improving stability under dynamic conditions. This research bridges the gap between traditional control systems and learning inspired techniques, offering a scalable, data efficient solution for modern transportation challenges","authors":["Vuong Anh Trung","Thanh Son Pham","Truc Thanh Tran","Tran le Thang Dong","Tran Thuan Hoang"],"url":"https://arxiv.org/abs/2504.06706"}
{"created":"2025-04-11","title":"Clustering and novel class recognition: evaluating bioacoustic deep learning feature extractors","abstract":"In computational bioacoustics, deep learning models are composed of feature extractors and classifiers. The feature extractors generate vector representations of the input sound segments, called embeddings, which can be input to a classifier. While benchmarking of classification scores provides insights into specific performance statistics, it is limited to species that are included in the models' training data. Furthermore, it makes it impossible to compare models trained on very different taxonomic groups. This paper aims to address this gap by analyzing the embeddings generated by the feature extractors of 15 bioacoustic models spanning a wide range of setups (model architectures, training data, training paradigms). We evaluated and compared different ways in which models structure embedding spaces through clustering and kNN classification, which allows us to focus our comparison on feature extractors independent of their classifiers. We believe that this approach lets us evaluate the adaptability and generalization potential of models going beyond the classes they were trained on.","authors":["Vincent S. Kather","Burooj Ghani","Dan Stowell"],"url":"https://arxiv.org/abs/2504.06710"}
{"created":"2025-04-11","title":"Large-Scale (Semi-)Automated Security Assessment of Consumer IoT Devices -- A Roadmap","abstract":"The Internet of Things (IoT) has rapidly expanded across various sectors, with consumer IoT devices - such as smart thermostats and security cameras - experiencing growth. Although these devices improve efficiency and promise additional comfort, they also introduce new security challenges. Common and easy-to-explore vulnerabilities make IoT devices prime targets for malicious actors. Upcoming mandatory security certifications offer a promising way to mitigate these risks by enforcing best practices and providing transparency. Regulatory bodies are developing IoT security frameworks, but a universal standard for large-scale systematic security assessment is lacking. Existing manual testing approaches are expensive, limiting their efficacy in the diverse and rapidly evolving IoT domain. This paper reviews current IoT security challenges and assessment efforts, identifies gaps, and proposes a roadmap for scalable, automated security assessment, leveraging a model-based testing approach and machine learning techniques to strengthen consumer IoT security.","authors":["Pascal Sch\\\"ottle","Matthias Janetschek","Florian Merkle","Martin Nocker","Christoph Egger"],"url":"https://arxiv.org/abs/2504.06712"}
{"created":"2025-04-11","title":"Unifying Search and Recommendation: A Generative Paradigm Inspired by Information Theory","abstract":"Recommender systems and search engines serve as foundational elements of online platforms, with the former delivering information proactively and the latter enabling users to seek information actively. Unifying both tasks in a shared model is promising since it can enhance user modeling and item understanding. Previous approaches mainly follow a discriminative paradigm, utilizing shared encoders to process input features and task-specific heads to perform each task. However, this paradigm encounters two key challenges: gradient conflict and manual design complexity. From the information theory perspective, these challenges potentially both stem from the same issue -- low mutual information between the input features and task-specific outputs during the optimization process.","authors":["Jujia Zhao","Wenjie Wang","Chen Xu","Xiuying Wang","Zhaochun Ren","Suzan Verberne"],"url":"https://arxiv.org/abs/2504.06714"}
{"created":"2025-04-11","title":"GSta: Efficient Training Scheme with Siestaed Gaussians for Monocular 3D Scene Reconstruction","abstract":"Gaussian Splatting (GS) is a popular approach for 3D reconstruction, mostly due to its ability to converge reasonably fast, faithfully represent the scene and render (novel) views in a fast fashion. However, it suffers from large storage and memory requirements, and its training speed still lags behind the hash-grid based radiance field approaches (e.g. Instant-NGP), which makes it especially difficult to deploy them in robotics scenarios, where 3D reconstruction is crucial for accurate operation. In this paper, we propose GSta that dynamically identifies Gaussians that have converged well during training, based on their positional and color gradient norms. By forcing such Gaussians into a siesta and stopping their updates (freezing) during training, we improve training speed with competitive accuracy compared to state of the art. We also propose an early stopping mechanism based on the PSNR values computed on a subset of training images. Combined with other improvements, such as integrating a learning rate scheduler, GSta achieves an improved Pareto front in convergence speed, memory and storage requirements, while preserving quality. We also show that GSta can improve other methods and complement orthogonal approaches in efficiency improvement; once combined with Trick-GS, GSta achieves up to 5x faster training, 16x smaller disk size compared to vanilla GS, while having comparable accuracy and consuming only half the peak memory. More visualisations are available at https://anilarmagan.github.io/SRUK-GSta.","authors":["Anil Armagan","Albert Sa\\`a-Garriga","Bruno Manganelli","Kyuwon Kim","M. Kerim Yucel"],"url":"https://arxiv.org/abs/2504.06716"}
{"created":"2025-04-11","title":"Ice-Breakers, Turn-Takers and Fun-Makers: Exploring Robots for Groups with Teenagers","abstract":"Successful, enjoyable group interactions are important in public and personal contexts, especially for teenagers whose peer groups are important for self-identity and self-esteem. Social robots seemingly have the potential to positively shape group interactions, but it seems difficult to effect such impact by designing robot behaviors solely based on related (human interaction) literature. In this article, we take a user-centered approach to explore how teenagers envisage a social robot \"group assistant\". We engaged 16 teenagers in focus groups, interviews, and robot testing to capture their views and reflections about robots for groups. Over the course of a two-week summer school, participants co-designed the action space for such a robot and experienced working with/wizarding it for 10+ hours. This experience further altered and deepened their insights into using robots as group assistants. We report results regarding teenagers' views on the applicability and use of a robot group assistant, how these expectations evolved throughout the study, and their repeat interactions with the robot. Our results indicate that each group moves on a spectrum of need for the robot, reflected in use of the robot more (or less) for ice-breaking, turn-taking, and fun-making as the situation demanded.","authors":["Sarah Gillet","Katie Winkle","Giulia Belgiovine","Iolanda Leite"],"url":"https://arxiv.org/abs/2504.06718"}
{"created":"2025-04-11","title":"Masked Scene Modeling: Narrowing the Gap Between Supervised and Self-Supervised Learning in 3D Scene Understanding","abstract":"Self-supervised learning has transformed 2D computer vision by enabling models trained on large, unannotated datasets to provide versatile off-the-shelf features that perform similarly to models trained with labels. However, in 3D scene understanding, self-supervised methods are typically only used as a weight initialization step for task-specific fine-tuning, limiting their utility for general-purpose feature extraction. This paper addresses this shortcoming by proposing a robust evaluation protocol specifically designed to assess the quality of self-supervised features for 3D scene understanding. Our protocol uses multi-resolution feature sampling of hierarchical models to create rich point-level representations that capture the semantic capabilities of the model and, hence, are suitable for evaluation with linear probing and nearest-neighbor methods. Furthermore, we introduce the first self-supervised model that performs similarly to supervised models when only off-the-shelf features are used in a linear probing setup. In particular, our model is trained natively in 3D with a novel self-supervised approach based on a Masked Scene Modeling objective, which reconstructs deep features of masked patches in a bottom-up manner and is specifically tailored to hierarchical 3D models. Our experiments not only demonstrate that our method achieves competitive performance to supervised models, but also surpasses existing self-supervised approaches by a large margin. The model and training code can be found at our Github repository (https://github.com/phermosilla/msm).","authors":["Pedro Hermosilla","Christian Stippel","Leon Sick"],"url":"https://arxiv.org/abs/2504.06719"}
{"created":"2025-04-11","title":"Learning global control of underactuated systems with Model-Based Reinforcement Learning","abstract":"This short paper describes our proposed solution for the third edition of the \"AI Olympics with RealAIGym\" competition, held at ICRA 2025. We employed Monte-Carlo Probabilistic Inference for Learning Control (MC-PILCO), an MBRL algorithm recognized for its exceptional data efficiency across various low-dimensional robotic tasks, including cart-pole, ball \\& plate, and Furuta pendulum systems. MC-PILCO optimizes a system dynamics model using interaction data, enabling policy refinement through simulation rather than direct system data optimization. This approach has proven highly effective in physical systems, offering greater data efficiency than Model-Free (MF) alternatives. Notably, MC-PILCO has previously won the first two editions of this competition, demonstrating its robustness in both simulated and real-world environments. Besides briefly reviewing the algorithm, we discuss the most critical aspects of the MC-PILCO implementation in the tasks at hand: learning a global policy for the pendubot and acrobot systems.","authors":["Niccol\\`o Turcato","Marco Cal\\`i","Alberto Dalla Libera","Giulio Giacomuzzo","Ruggero Carli","Diego Romeres"],"url":"https://arxiv.org/abs/2504.06721"}
{"created":"2025-04-11","title":"Plastic tensor networks for interpretable generative modeling","abstract":"A structural optimization scheme for a single-layer nonnegative adaptive tensor tree (NATT) that models a target probability distribution is proposed. The NATT scheme, by construction, has the advantage that it is interpretable as a probabilistic graphical model. We consider the NATT scheme and a recently proposed Born machine adaptive tensor tree (BMATT) optimization scheme and demonstrate their effectiveness on a variety of generative modeling tasks where the objective is to infer the hidden structure of a provided dataset. Our results show that in terms of minimizing the negative log-likelihood, the single-layer scheme has model performance comparable to the Born machine scheme, though not better. The tasks include deducing the structure of binary bitwise operations, learning the internal structure of random Bayesian networks given only visible sites, and a real-world example related to hierarchical clustering where a cladogram is constructed from mitochondrial DNA sequences. In doing so, we also show the importance of the choice of network topology and the versatility of a least-mutual information criterion in selecting a candidate structure for a tensor tree, as well as discuss aspects of these tensor tree generative models including their information content and interpretability.","authors":["Katsuya O. Akamatsu","Kenji Harada","Tsuyoshi Okubo","Naoki Kawashima"],"url":"https://arxiv.org/abs/2504.06722"}
{"created":"2025-04-11","title":"Optimal Duration of Reserve Capacity Ancillary Services for Distributed Energy Resources","abstract":"The increasing integration of distributed energy resources (DERs) into power systems presents opportunities and challenges for ancillary services (AS) provision. Technical requirements of existing AS (i.e., duration, reliability, ramp rate, and lead time) have been designed for traditional generating units, making their provision by DER aggregates particularly challenging. This paper proposes a method to design the duration of reserve capacity AS products considering the operational constraints of DERs and the temporal dynamics of system imbalances. The optimal product duration is determined by maximizing product availability and aligning the supply profile with the system's balancing needs. We apply the methodology to a realistic Swiss low-voltage network with a diverse DER portfolio. The results reveal that (i) shorter product durations maximize average availability and (ii) long product durations improve the alignment with system balancing needs. This paper offers valuable insights for system operators to design AS products tailored for DER participation.","authors":["Lorenzo Zapparoli","Blazhe Gjorgiev","Giovanni Sansavini"],"url":"https://arxiv.org/abs/2504.06729"}
{"created":"2025-04-11","title":"PETNet -- Coincident Particle Event Detection using Spiking Neural Networks","abstract":"Spiking neural networks (SNN) hold the promise of being a more biologically plausible, low-energy alternative to conventional artificial neural networks. Their time-variant nature makes them particularly suitable for processing time-resolved, sparse binary data. In this paper, we investigate the potential of leveraging SNNs for the detection of photon coincidences in positron emission tomography (PET) data. PET is a medical imaging technique based on injecting a patient with a radioactive tracer and detecting the emitted photons. One central post-processing task for inferring an image of the tracer distribution is the filtering of invalid hits occurring due to e.g. absorption or scattering processes. Our approach, coined PETNet, interprets the detector hits as a binary-valued spike train and learns to identify photon coincidence pairs in a supervised manner. We introduce a dedicated multi-objective loss function and demonstrate the effects of explicitly modeling the detector geometry on simulation data for two use-cases. Our results show that PETNet can outperform the state-of-the-art classical algorithm with a maximal coincidence detection $F_1$ of 95.2%. At the same time, PETNet is able to predict photon coincidences up to 36 times faster than the classical approach, highlighting the great potential of SNNs in particle physics applications.","authors":["Jan Debus","Charlotte Debus","G\\\"unther Dissertori","Markus G\\\"otz"],"url":"https://arxiv.org/abs/2504.06730"}
{"created":"2025-04-11","title":"FJ-MM: The Friedkin-Johnsen Opinion Dynamics Model with Memory and Higher-Order Neighbors","abstract":"The Friedkin-Johnsen (FJ) model has been extensively explored and validated, spanning applications in social science, systems and control, game theory, and algorithmic research. In this paper, we introduce an advanced generalization of the FJ model, termed FJ-MM which incorporates both memory effects and multi-hop (higher-order neighbor) influence. This formulation allows agents to naturally incorporate both current and previous opinions at each iteration stage. Our numerical results demonstrate that incorporating memory and multi-hop influence significantly reshapes the opinion landscape; for example, the final opinion profile can exhibit reduced polarization. We analyze the stability and equilibrium properties of the FJ-MM model, showing that these properties can be reduced to those of a comparison model--namely, the standard FJ model with a modified influence matrix. This reduction enables us to leverage established stability results from FJ dynamics. Additionally, we examine the convergence rate of the FJ-MM model and demonstrate that, as can be expected, the time lags introduced by memory and higher-order neighbor influences result in slower convergence.","authors":["Roberta Raineri","Lorenzo Zino","Anton Proskurnikov"],"url":"https://arxiv.org/abs/2504.06731"}
{"created":"2025-04-11","title":"Locally Repairable Convertible Codes: Improved Lower Bound and General Construction","abstract":"In this paper, we consider the convertible code with locally repairable property. We present an improved lower bound on access cost associated with $(r,\\delta)$. Then, we provide a general construction of convertible codes with optimal access cost which shows that those codes can be with super-linear length or maximum repairable property. Additionally, employing the known locally repairable codes with super-linear length or maximum repairable property, we provide explicit constructions of convertible codes with super-linear length or maximum repairable property.","authors":["Songping Ge","Han Cai","Xiaohu Tang"],"url":"https://arxiv.org/abs/2504.06734"}
{"created":"2025-04-11","title":"Interactive Expressive Motion Generation Using Dynamic Movement Primitives","abstract":"Our goal is to enable social robots to interact autonomously with humans in a realistic, engaging, and expressive manner. The 12 Principles of Animation [1] are a well-established framework animators use to create movements that make characters appear convincing, dynamic, and emotionally expressive. This paper proposes a novel approach that leverages Dynamic Movement Primitives (DMPs) to implement key animation principles, providing a learnable, explainable, modulable, online adaptable and composable model for automatic expressive motion generation. DMPs, originally developed for general imitation learning in robotics and grounded in a spring-damper system design, offer mathematical properties that make them particularly suitable for this task. Specifically, they enable modulation of the intensities of individual principles and facilitate the decomposition of complex, expressive motion sequences into learnable and parametrizable primitives. We present the mathematical formulation of the parameterized animation principles and demonstrate the effectiveness of our framework through experiments and application on three robotic platforms with different kinematic configurations, in simulation, on actual robots and in a user study. Our results show that the approach allows for creating diverse and nuanced expressions using a single base model.","authors":["Till Hielscher","Andreas Bulling","Kai O. Arras"],"url":"https://arxiv.org/abs/2504.06735"}
{"created":"2025-04-11","title":"EDIT: Enhancing Vision Transformers by Mitigating Attention Sink through an Encoder-Decoder Architecture","abstract":"In this paper, we propose EDIT (Encoder-Decoder Image Transformer), a novel architecture designed to mitigate the attention sink phenomenon observed in Vision Transformer models. Attention sink occurs when an excessive amount of attention is allocated to the [CLS] token, distorting the model's ability to effectively process image patches. To address this, we introduce a layer-aligned encoder-decoder architecture, where the encoder utilizes self-attention to process image patches, while the decoder uses cross-attention to focus on the [CLS] token. Unlike traditional encoder-decoder framework, where the decoder depends solely on high-level encoder representations, EDIT allows the decoder to extract information starting from low-level features, progressively refining the representation layer by layer. EDIT is naturally interpretable demonstrated through sequential attention maps, illustrating the refined, layer-by-layer focus on key image features. Experiments on ImageNet-1k and ImageNet-21k, along with transfer learning tasks, show that EDIT achieves consistent performance improvements over DeiT3 models. These results highlight the effectiveness of EDIT's design in addressing attention sink and improving visual feature extraction.","authors":["Wenfeng Feng","Guoying Sun"],"url":"https://arxiv.org/abs/2504.06738"}
{"created":"2025-04-11","title":"MultiADS: Defect-aware Supervision for Multi-type Anomaly Detection and Segmentation in Zero-Shot Learning","abstract":"Precise optical inspection in industrial applications is crucial for minimizing scrap rates and reducing the associated costs. Besides merely detecting if a product is anomalous or not, it is crucial to know the distinct type of defect, such as a bent, cut, or scratch. The ability to recognize the \"exact\" defect type enables automated treatments of the anomalies in modern production lines. Current methods are limited to solely detecting whether a product is defective or not without providing any insights on the defect type, nevertheless detecting and identifying multiple defects. We propose MultiADS, a zero-shot learning approach, able to perform Multi-type Anomaly Detection and Segmentation. The architecture of MultiADS comprises CLIP and extra linear layers to align the visual- and textual representation in a joint feature space. To the best of our knowledge, our proposal, is the first approach to perform a multi-type anomaly segmentation task in zero-shot learning. Contrary to the other baselines, our approach i) generates specific anomaly masks for each distinct defect type, ii) learns to distinguish defect types, and iii) simultaneously identifies multiple defect types present in an anomalous product. Additionally, our approach outperforms zero/few-shot learning SoTA methods on image-level and pixel-level anomaly detection and segmentation tasks on five commonly used datasets: MVTec-AD, Visa, MPDD, MAD and Real-IAD.","authors":["Ylli Sadikaj","Hongkuan Zhou","Lavdim Halilaj","Stefan Schmid","Steffen Staab","Claudia Plant"],"url":"https://arxiv.org/abs/2504.06740"}
{"created":"2025-04-11","title":"Large Scale Supervised Pretraining For Traumatic Brain Injury Segmentation","abstract":"The segmentation of lesions in Moderate to Severe Traumatic Brain Injury (msTBI) presents a significant challenge in neuroimaging due to the diverse characteristics of these lesions, which vary in size, shape, and distribution across brain regions and tissue types. This heterogeneity complicates traditional image processing techniques, resulting in critical errors in tasks such as image registration and brain parcellation. To address these challenges, the AIMS-TBI Segmentation Challenge 2024 aims to advance innovative segmentation algorithms specifically designed for T1-weighted MRI data, the most widely utilized imaging modality in clinical practice. Our proposed solution leverages a large-scale multi-dataset supervised pretraining approach inspired by the MultiTalent method. We train a Resenc L network on a comprehensive collection of datasets covering various anatomical and pathological structures, which equips the model with a robust understanding of brain anatomy and pathology. Following this, the model is fine-tuned on msTBI-specific data to optimize its performance for the unique characteristics of T1-weighted MRI scans and outperforms the baseline without pretraining up to 2 Dice points.","authors":["Constantin Ulrich","Tassilo Wald","Fabian Isensee","Klaus H. Maier-Hein"],"url":"https://arxiv.org/abs/2504.06741"}
{"created":"2025-04-11","title":"nnLandmark: A Self-Configuring Method for 3D Medical Landmark Detection","abstract":"Landmark detection plays a crucial role in medical imaging tasks that rely on precise spatial localization, including specific applications in diagnosis, treatment planning, image registration, and surgical navigation. However, manual annotation is labor-intensive and requires expert knowledge. While deep learning shows promise in automating this task, progress is hindered by limited public datasets, inconsistent benchmarks, and non-standardized baselines, restricting reproducibility, fair comparisons, and model generalizability. This work introduces nnLandmark, a self-configuring deep learning framework for 3D medical landmark detection, adapting nnU-Net to perform heatmap-based regression. By leveraging nnU-Net's automated configuration, nnLandmark eliminates the need for manual parameter tuning, offering out-of-the-box usability. It achieves state-of-the-art accuracy across two public datasets, with a mean radial error (MRE) of 1.5 mm on the Mandibular Molar Landmark (MML) dental CT dataset and 1.2 mm for anatomical fiducials on a brain MRI dataset (AFIDs), where nnLandmark aligns with the inter-rater variability of 1.5 mm. With its strong generalization, reproducibility, and ease of deployment, nnLandmark establishes a reliable baseline for 3D landmark detection, supporting research in anatomical localization and clinical workflows that depend on precise landmark identification. The code will be available soon.","authors":["Alexandra Ertl","Shuhan Xiao","Stefan Denner","Robin Peretzke","David Zimmerer","Peter Neher","Fabian Isensee","Klaus Maier-Hein"],"url":"https://arxiv.org/abs/2504.06742"}
{"created":"2025-04-11","title":"More Efficient Stealth Address Protocol","abstract":"The integration of privacy-preserving transactions into public blockchains such as Ethereum remains a major challenge. The Stealth Address Protocol (SAP) provides recipient anonymity by generating unlinkable stealth addresses. Existing SAPs, such as the Dual-Key Stealth Address Protocol and the Curvy Protocol, have shown significant improvements in efficiency, but remain vulnerable to quantum attacks. Post-quantum SAPs based on lattice-based cryptography, such as the Module-LWE SAP, on the other hand, offer quantum resistance while achieving better performance.","authors":["Marija Mikic","Mihajlo Srbakoski","Strahinja Praska"],"url":"https://arxiv.org/abs/2504.06744"}
{"created":"2025-04-11","title":"Adaptive Human-Robot Collaborative Missions using Hybrid Task Planning","abstract":"Producing robust task plans in human-robot collaborative missions is a critical activity in order to increase the likelihood of these missions completing successfully. Despite the broad research body in the area, which considers different classes of constraints and uncertainties, its applicability is confined to relatively simple problems that can be comfortably addressed by the underpinning mathematically-based or heuristic-driven solver engines. In this paper, we introduce a hybrid approach that effectively solves the task planning problem by decomposing it into two intertwined parts, starting with the identification of a feasible plan and followed by its uncertainty augmentation and verification yielding a set of Pareto optimal plans. To enhance its robustness, adaptation tactics are devised for the evolving system requirements and agents' capabilities. We demonstrate our approach through an industrial case study involving workers and robots undertaking activities within a vineyard, showcasing the benefits of our hybrid approach both in the generation of feasible solutions and scalability compared to native planners.","authors":["Gricel V\\'azquez","Alexandros Evangelidis","Sepeedeh Shahbeigi","Simos Gerasimou"],"url":"https://arxiv.org/abs/2504.06746"}
{"created":"2025-04-11","title":"Efficient Deployment of Spiking Neural Networks on SpiNNaker2 for DVS Gesture Recognition Using Neuromorphic Intermediate Representation","abstract":"Spiking Neural Networks (SNNs) are highly energy-efficient during inference, making them particularly suitable for deployment on neuromorphic hardware. Their ability to process event-driven inputs, such as data from dynamic vision sensors (DVS), further enhances their applicability to edge computing tasks. However, the resource constraints of edge hardware necessitate techniques like weight quantization, which reduce the memory footprint of SNNs while preserving accuracy. Despite its importance, existing quantization methods typically focus on synaptic weights quantization without taking account of other critical parameters, such as scaling neuron firing thresholds.","authors":["Sirine Arfa","Bernhard Vogginger","Chen Liu","Johannes Partzsch","Mark Schone","Christian Mayr"],"url":"https://arxiv.org/abs/2504.06748"}
{"created":"2025-04-11","title":"Visualisation of a multidimensional point cloud as a 3D swarm of avatars","abstract":"The article presents an innovative approach to the visualisation of multidimensional data, using icons inspired by Chernoff faces. The approach merges classical projection techniques with the assignment of particular data dimensions to mimic features, capitalizing on the natural ability of the human brain to interpret facial expressions. The technique is implemented as a plugin to the dpVision open-source image handling platform. The plugin allows the data to be interactively explored in the form of a swarm of \"totems\" whose position in hyperspace as well as facial features represent various aspects of the data. Sample visualisations, based on synthetic test data as well as the vinhoverde 15-dimensional database on Portuguese wines, confirm the usefulness of our approach to the analysis of complex data structures.","authors":["Leszek Luchowski","Dariusz Pojda"],"url":"https://arxiv.org/abs/2504.06751"}
{"created":"2025-04-11","title":"Compass Control: Multi Object Orientation Control for Text-to-Image Generation","abstract":"Existing approaches for controlling text-to-image diffusion models, while powerful, do not allow for explicit 3D object-centric control, such as precise control of object orientation. In this work, we address the problem of multi-object orientation control in text-to-image diffusion models. This enables the generation of diverse multi-object scenes with precise orientation control for each object. The key idea is to condition the diffusion model with a set of orientation-aware \\textbf{compass} tokens, one for each object, along with text tokens. A light-weight encoder network predicts these compass tokens taking object orientation as the input. The model is trained on a synthetic dataset of procedurally generated scenes, each containing one or two 3D assets on a plain background. However, direct training this framework results in poor orientation control as well as leads to entanglement among objects. To mitigate this, we intervene in the generation process and constrain the cross-attention maps of each compass token to its corresponding object regions. The trained model is able to achieve precise orientation control for a) complex objects not seen during training and b) multi-object scenes with more than two objects, indicating strong generalization capabilities. Further, when combined with personalization methods, our method precisely controls the orientation of the new object in diverse contexts. Our method achieves state-of-the-art orientation control and text alignment, quantified with extensive evaluations and a user study.","authors":["Rishubh Parihar","Vaibhav Agrawal","Sachidanand VS","R. Venkatesh Babu"],"url":"https://arxiv.org/abs/2504.06752"}
{"created":"2025-04-11","title":"Detect All-Type Deepfake Audio: Wavelet Prompt Tuning for Enhanced Auditory Perception","abstract":"The rapid advancement of audio generation technologies has escalated the risks of malicious deepfake audio across speech, sound, singing voice, and music, threatening multimedia security and trust. While existing countermeasures (CMs) perform well in single-type audio deepfake detection (ADD), their performance declines in cross-type scenarios. This paper is dedicated to studying the alltype ADD task. We are the first to comprehensively establish an all-type ADD benchmark to evaluate current CMs, incorporating cross-type deepfake detection across speech, sound, singing voice, and music. Then, we introduce the prompt tuning self-supervised learning (PT-SSL) training paradigm, which optimizes SSL frontend by learning specialized prompt tokens for ADD, requiring 458x fewer trainable parameters than fine-tuning (FT). Considering the auditory perception of different audio types,we propose the wavelet prompt tuning (WPT)-SSL method to capture type-invariant auditory deepfake information from the frequency domain without requiring additional training parameters, thereby enhancing performance over FT in the all-type ADD task. To achieve an universally CM, we utilize all types of deepfake audio for co-training. Experimental results demonstrate that WPT-XLSR-AASIST achieved the best performance, with an average EER of 3.58% across all evaluation sets. The code is available online.","authors":["Yuankun Xie","Ruibo Fu","Zhiyong Wang","Xiaopeng Wang","Songjun Cao","Long Ma","Haonan Cheng","Long Ye"],"url":"https://arxiv.org/abs/2504.06753"}
{"created":"2025-04-11","title":"FANeRV: Frequency Separation and Augmentation based Neural Representation for Video","abstract":"Neural representations for video (NeRV) have gained considerable attention for their strong performance across various video tasks. However, existing NeRV methods often struggle to capture fine spatial details, resulting in vague reconstructions. In this paper, we present a Frequency Separation and Augmentation based Neural Representation for video (FANeRV), which addresses these limitations with its core Wavelet Frequency Upgrade Block.This block explicitly separates input frames into high and low-frequency components using discrete wavelet transform, followed by targeted enhancement using specialized modules. Finally, a specially designed gated network effectively fuses these frequency components for optimal reconstruction. Additionally, convolutional residual enhancement blocks are integrated into the later stages of the network to balance parameter distribution and improve the restoration of high-frequency details. Experimental results demonstrate that FANeRV significantly improves reconstruction performance and excels in multiple tasks, including video compression, inpainting, and interpolation, outperforming existing NeRV methods.","authors":["Li Yu","Zhihui Li","Jimin Xiao","Moncef Gabbouj"],"url":"https://arxiv.org/abs/2504.06755"}
{"created":"2025-04-11","title":"Matching and Edge Cover in Temporal Graphs","abstract":"Temporal graphs are a special class of graphs for which a temporal component is added to edges, that is, each edge possesses a set of times at which it is available and can be traversed. Many classical problems on graphs can be translated to temporal graphs, and the results may differ. In this paper, we define the Temporal Edge Cover and Temporal Matching problems and show that they are NP-complete even when fixing the lifetime or when the underlying graph is a tree. We then describe two FPT algorithms, with parameters lifetime and treewidth, that solve the two problems. We also find lower bounds for the approximation of the two problems and give two approximation algorithms which match these bounds. Finally, we discuss the differences between the problems in the temporal and the static framework.","authors":["Lapo Cioni","Riccardo Dondi","Andrea Marino","Jason Schoeters","Ana Silva"],"url":"https://arxiv.org/abs/2504.06762"}
{"created":"2025-04-11","title":"Convergence of a continuous Galerkin method for the Biot-Allard poroelasticity system","abstract":"We study a space-time finite element method for a system of poromechanics with memory effects that are modeled by a convolution integral. In the literature, the system is referred to as the Biot-Allard model. We recast the model as a first-order system in time, where the memory effects are transformed into an auxiliary differential equation. This allows for a computationally efficient numerical scheme. The system is discretized by continuous Galerkin methods in time and equal-order finite element methods in space. An optimal order error estimate is proved for the norm of the first-order energy of the unknowns of the system. The estimate is confirmed by numerical experiments.","authors":["Jakob S. Stokke","Markus Bause","Florin A. Radu"],"url":"https://arxiv.org/abs/2504.06763"}
{"created":"2025-04-11","title":"FamilyTool: A Multi-hop Personalized Tool Use Benchmark","abstract":"The integration of tool learning with Large Language Models (LLMs) has expanded their capabilities in handling complex tasks by leveraging external tools. However, existing benchmarks for tool learning inadequately address critical real-world personalized scenarios, particularly those requiring multi-hop reasoning and inductive knowledge adaptation in dynamic environments. To bridge this gap, we introduce FamilyTool, a novel benchmark grounded in a family-based knowledge graph (KG) that simulates personalized, multi-hop tool use scenarios. FamilyTool challenges LLMs with queries spanning 1 to 3 relational hops (e.g., inferring familial connections and preferences) and incorporates an inductive KG setting where models must adapt to unseen user preferences and relationships without re-training, a common limitation in prior approaches that compromises generalization. We further propose KGETool: a simple KG-augmented evaluation pipeline to systematically assess LLMs' tool use ability in these settings. Experiments reveal significant performance gaps in state-of-the-art LLMs, with accuracy dropping sharply as hop complexity increases and inductive scenarios exposing severe generalization deficits. These findings underscore the limitations of current LLMs in handling personalized, evolving real-world contexts and highlight the urgent need for advancements in tool-learning frameworks. FamilyTool serves as a critical resource for evaluating and advancing LLM agents' reasoning, adaptability, and scalability in complex, dynamic environments. Code and dataset are available at Github.","authors":["Yuxin Wang","Yiran Guo","Yining Zheng","Zhangyue Yin","Shuo Chen","Jie Yang","Jiajun Chen","Xuanjing Huang","Xipeng Qiu"],"url":"https://arxiv.org/abs/2504.06766"}
{"created":"2025-04-11","title":"FedMerge: Federated Personalization via Model Merging","abstract":"One global model in federated learning (FL) might not be sufficient to serve many clients with non-IID tasks and distributions. While there has been advances in FL to train multiple global models for better personalization, they only provide limited choices to clients so local finetuning is still indispensable. In this paper, we propose a novel ``FedMerge'' approach that can create a personalized model per client by simply merging multiple global models with automatically optimized and customized weights. In FedMerge, a few global models can serve many non-IID clients, even without further local finetuning. We formulate this problem as a joint optimization of global models and the merging weights for each client. Unlike existing FL approaches where the server broadcasts one or multiple global models to all clients, the server only needs to send a customized, merged model to each client. Moreover, instead of periodically interrupting the local training and re-initializing it to a global model, the merged model aligns better with each client's task and data distribution, smoothening the local-global gap between consecutive rounds caused by client drift. We evaluate FedMerge on three different non-IID settings applied to different domains with diverse tasks and data types, in which FedMerge consistently outperforms existing FL approaches, including clustering-based and mixture-of-experts (MoE) based methods.","authors":["Shutong Chen","Tianyi Zhou","Guodong Long","Jing Jiang","Chengqi Zhang"],"url":"https://arxiv.org/abs/2504.06768"}
{"created":"2025-04-11","title":"AI, Help Me Think$\\unicode{x2014}$but for Myself: Assisting People in Complex Decision-Making by Providing Different Kinds of Cognitive Support","abstract":"How can we design AI tools that effectively support human decision-making by complementing and enhancing users' reasoning processes? Common recommendation-centric approaches face challenges such as inappropriate reliance or a lack of integration with users' decision-making processes. Here, we explore an alternative interaction model in which the AI outputs build upon users' own decision-making rationales. We compare this approach, which we call ExtendAI, with a recommendation-based AI. Participants in our mixed-methods user study interacted with both AIs as part of an investment decision-making task. We found that the AIs had different impacts, with ExtendAI integrating better into the decision-making process and people's own thinking and leading to slightly better outcomes. RecommendAI was able to provide more novel insights while requiring less cognitive effort. We discuss the implications of these and other findings along with three tensions of AI-assisted decision-making which our study revealed.","authors":["Leon Reicherts","Zelun Tony Zhang","Elisabeth von Oswald","Yuanting Liu","Yvonne Rogers","Mariam Hassib"],"url":"https://arxiv.org/abs/2504.06771"}
{"created":"2025-04-11","title":"Towards Efficient Roadside LiDAR Deployment: A Fast Surrogate Metric Based on Entropy-Guided Visibility","abstract":"The deployment of roadside LiDAR sensors plays a crucial role in the development of Cooperative Intelligent Transport Systems (C-ITS). However, the high cost of LiDAR sensors necessitates efficient placement strategies to maximize detection performance. Traditional roadside LiDAR deployment methods rely on expert insight, making them time-consuming. Automating this process, however, demands extensive computation, as it requires not only visibility evaluation but also assessing detection performance across different LiDAR placements. To address this challenge, we propose a fast surrogate metric, the Entropy-Guided Visibility Score (EGVS), based on information gain to evaluate object detection performance in roadside LiDAR configurations. EGVS leverages Traffic Probabilistic Occupancy Grids (TPOG) to prioritize critical areas and employs entropy-based calculations to quantify the information captured by LiDAR beams. This eliminates the need for direct detection performance evaluation, which typically requires extensive labeling and computational resources. By integrating EGVS into the optimization process, we significantly accelerate the search for optimal LiDAR configurations. Experimental results using the AWSIM simulator demonstrate that EGVS strongly correlates with Average Precision (AP) scores and effectively predicts object detection performance. This approach offers a computationally efficient solution for roadside LiDAR deployment, facilitating scalable smart infrastructure development.","authors":["Yuze Jiang","Ehsan Javanmardi","Manabu Tsukada","Hiroshi Esaki"],"url":"https://arxiv.org/abs/2504.06772"}
{"created":"2025-04-11","title":"End2end-ALARA: Approaching the ALARA Law in CT Imaging with End-to-end Learning","abstract":"Computed tomography (CT) examination poses radiation injury to patient. A consensus performing CT imaging is to make the radiation dose as low as reasonably achievable, i.e. the ALARA law. In this paper, we propose an end-to-end learning framework, named End2end-ALARA, that jointly optimizes dose modulation and image reconstruction to meet the goal of ALARA in CT imaging. End2end-ALARA works by building a dose modulation module and an image reconstruction module, connecting these modules with a differentiable simulation function, and optimizing the them with a constrained hinge loss function. The objective is to minimize radiation dose subject to a prescribed image quality (IQ) index. The results show that End2end-ALARA is able to preset personalized dose levels to gain a stable IQ level across patients, which may facilitate image-based diagnosis and downstream model training. Moreover, compared to fixed-dose and conventional dose modulation strategies, End2end-ALARA consumes lower dose to reach the same IQ level. Our study sheds light on a way of realizing the ALARA law in CT imaging.","authors":["Xi Tao","Liyan Lin"],"url":"https://arxiv.org/abs/2504.06777"}
{"created":"2025-04-11","title":"Controllable Automatic Foley Artist","abstract":"Foley is a key element in video production, refers to the process of adding an audio signal to a silent video while ensuring semantic and temporal alignment. In recent years, the rise of personalized content creation and advancements in automatic video-to-audio models have increased the demand for greater user control in the process. One possible approach is to incorporate text to guide audio generation. While supported by existing methods, challenges remain in ensuring compatibility between modalities, particularly when the text introduces additional information or contradicts the sounds naturally inferred from the visuals. In this work, we introduce CAFA (Controllable Automatic Foley Artist) a video-and-text-to-audio model that generates semantically and temporally aligned audio for a given video, guided by text input. CAFA is built upon a text-to-audio model and integrates video information through a modality adapter mechanism. By incorporating text, users can refine semantic details and introduce creative variations, guiding the audio synthesis beyond the expected video contextual cues. Experiments show that besides its superior quality in terms of semantic alignment and audio-visual synchronization the proposed method enable high textual controllability as demonstrated in subjective and objective evaluations.","authors":["Roi Benita","Michael Finkelson","Tavi Halperin","Gleb Sterkin","Yossi Adi"],"url":"https://arxiv.org/abs/2504.06778"}
{"created":"2025-04-11","title":"CHIME: A Compressive Framework for Holistic Interest Modeling","abstract":"Modeling holistic user interests is important for improving recommendation systems but is challenged by high computational cost and difficulty in handling diverse information with full behavior context. Existing search-based methods might lose critical signals during behavior selection. To overcome these limitations, we propose CHIME: A Compressive Framework for Holistic Interest Modeling. It uses adapted large language models to encode complete user behaviors with heterogeneous inputs. We introduce multi-granular contrastive learning objectives to capture both persistent and transient interest patterns and apply residual vector quantization to generate compact embeddings. CHIME demonstrates superior ranking performance across diverse datasets, establishing a robust solution for scalable holistic interest modeling in recommendation systems.","authors":["Yong Bai","Rui Xiang","Kaiyuan Li","Yongxiang Tang","Yanhua Cheng","Xialong Liu","Peng Jiang","Kun Gai"],"url":"https://arxiv.org/abs/2504.06780"}
{"created":"2025-04-11","title":"Domain Generalization through Attenuation of Domain-Specific Information","abstract":"In this paper, we propose a new evaluation metric called Domain Independence (DI) and Attenuation of Domain-Specific Information (ADSI) which is specifically designed for domain-generalized semantic segmentation in automotive images. DI measures the presence of domain-specific information: a lower DI value indicates strong domain dependence, while a higher DI value suggests greater domain independence. This makes it roughly where domain-specific information exists and up to which frequency range it is present. As a result, it becomes possible to effectively suppress only the regions in the image that contain domain-specific information, enabling feature extraction independent of the domain. ADSI uses a Butterworth filter to remove the low-frequency components of images that contain inherent domain-specific information such as sensor characteristics and lighting conditions. However, since low-frequency components also contain important information such as color, we should not remove them completely. Thus, a scalar value (ranging from 0 to 1) is multiplied by the low-frequency components to retain essential information. This helps the model learn more domain-independent features. In experiments, GTA5 (synthetic dataset) was used as training images, and a real-world dataset was used for evaluation, and the proposed method outperformed conventional approaches. Similarly, in experiments that the Cityscapes (real-world dataset) was used for training and various environment datasets such as rain and nighttime were used for evaluation, the proposed method demonstrated its robustness under nighttime conditions.","authors":["Reiji Saito","Kazuhiro Hotta"],"url":"https://arxiv.org/abs/2504.06781"}
{"created":"2025-04-11","title":"Zero-Shot Image-Based Large Language Model Approach to Road Pavement Monitoring","abstract":"Effective and rapid evaluation of pavement surface condition is critical for prioritizing maintenance, ensuring transportation safety, and minimizing vehicle wear and tear. While conventional manual inspections suffer from subjectivity, existing machine learning-based methods are constrained by their reliance on large and high-quality labeled datasets, which require significant resources and limit adaptability across varied road conditions. The revolutionary advancements in Large Language Models (LLMs) present significant potential for overcoming these challenges. In this study, we propose an innovative automated zero-shot learning approach that leverages the image recognition and natural language understanding capabilities of LLMs to assess road conditions effectively. Multiple LLM-based assessment models were developed, employing prompt engineering strategies aligned with the Pavement Surface Condition Index (PSCI) standards. These models' accuracy and reliability were evaluated against official PSCI results, with an optimized model ultimately selected. Extensive tests benchmarked the optimized model against evaluations from various levels experts using Google Street View road images. The results reveal that the LLM-based approach can effectively assess road conditions, with the optimized model -employing comprehensive and structured prompt engineering strategies -outperforming simpler configurations by achieving high accuracy and consistency, even surpassing expert evaluations. Moreover, successfully applying the optimized model to Google Street View images demonstrates its potential for future city-scale deployments. These findings highlight the transformative potential of LLMs in automating road damage evaluations and underscore the pivotal role of detailed prompt engineering in achieving reliable assessments.","authors":["Shuoshuo Xu","Kai Zhao","James Loney","Zili Li","Andrea Visentin"],"url":"https://arxiv.org/abs/2504.06785"}
{"created":"2025-04-11","title":"When is the partial map classifier a Sierpi\\'nski cone?","abstract":"We study the relationship between partial map classifiers, Sierpi\\'nski cones, and axioms for synthetic higher categories and domains within univalent foundations. In particular, we show that synthetic $\\infty$-categories are closed under partial map classifiers assuming Phoa's principle, and we isolate a new reflective subuniverse of types within which the Sierpi\\'nski cone (a lax colimit) can be computed as a partial map classifier by strengthening the Segal condition.","authors":["Leoni Pugh","Jonathan Sterling"],"url":"https://arxiv.org/abs/2504.06789"}
{"created":"2025-04-11","title":"Analog Computing with Microwave Networks","abstract":"Analog computing has been recently revived due to its potential for energy-efficient and highly parallel computations. In this paper, we investigate analog computers that linearly process microwave signals, named microwave linear analog computers (MiLACs), and their applications in signal processing for communications. We model a MiLAC as a multiport microwave network with tunable impedance components, which enables the execution of mathematical operations by reconfiguring the microwave network and applying input signals at its ports. We demonstrate that a MiLAC can efficiently compute the linear minimum mean square error (LMMSE) estimator, widely used in multiple-input multiple-output (MIMO) communications beamforming and detection, with remarkably low computational complexity, unachievable through digital computing. Specifically, the LMMSE estimator can be computed with complexity growing with the square of its input size, rather than the cube, with revolutionary applications to gigantic MIMO beamforming and detection.","authors":["Matteo Nerini","Bruno Clerckx"],"url":"https://arxiv.org/abs/2504.06790"}
{"created":"2025-04-11","title":"Beware of \"Explanations\" of AI","abstract":"Understanding the decisions made and actions taken by increasingly complex AI system remains a key challenge. This has led to an expanding field of research in explainable artificial intelligence (XAI), highlighting the potential of explanations to enhance trust, support adoption, and meet regulatory standards. However, the question of what constitutes a \"good\" explanation is dependent on the goals, stakeholders, and context. At a high level, psychological insights such as the concept of mental model alignment can offer guidance, but success in practice is challenging due to social and technical factors. As a result of this ill-defined nature of the problem, explanations can be of poor quality (e.g. unfaithful, irrelevant, or incoherent), potentially leading to substantial risks. Instead of fostering trust and safety, poorly designed explanations can actually cause harm, including wrong decisions, privacy violations, manipulation, and even reduced AI adoption. Therefore, we caution stakeholders to beware of explanations of AI: while they can be vital, they are not automatically a remedy for transparency or responsible AI adoption, and their misuse or limitations can exacerbate harm. Attention to these caveats can help guide future research to improve the quality and impact of AI explanations.","authors":["David Martens","Galit Shmueli","Theodoros Evgeniou","Kevin Bauer","Christian Janiesch","Stefan Feuerriegel","Sebastian Gabel","Sofie Goethals","Travis Greene","Nadja Klein","Mathias Kraus","Niklas K\\\"uhl","Claudia Perlich","Wouter Verbeke","Alona Zharova","Patrick Zschech","Foster Provost"],"url":"https://arxiv.org/abs/2504.06791"}
{"created":"2025-04-11","title":"Domain-Specific Pruning of Large Mixture-of-Experts Models with Few-shot Demonstrations","abstract":"Mixture-of-Experts (MoE) models achieve a favorable trade-off between performance and inference efficiency by activating only a subset of experts. However, the memory overhead of storing all experts remains a major limitation, especially in large-scale MoE models such as DeepSeek-R1 (671B). In this study, we investigate domain specialization and expert redundancy in large-scale MoE models and uncover a consistent behavior we term few-shot expert localization, with only a few demonstrations, the model consistently activates a sparse and stable subset of experts. Building on this observation, we propose a simple yet effective pruning framework, EASY-EP, that leverages a few domain-specific demonstrations to identify and retain only the most relevant experts. EASY-EP comprises two key components: output-aware expert importance assessment and expert-level token contribution estimation. The former evaluates the importance of each expert for the current token by considering the gating scores and magnitudes of the outputs of activated experts, while the latter assesses the contribution of tokens based on representation similarities after and before routed experts. Experiments show that our method can achieve comparable performances and $2.99\\times$ throughput under the same memory budget with full DeepSeek-R1 with only half the experts. Our code is available at https://github.com/RUCAIBox/EASYEP.","authors":["Zican Dong","Han Peng","Peiyu Liu","Wayne Xin Zhao","Dong Wu","Feng Xiao","Zhifeng Wang"],"url":"https://arxiv.org/abs/2504.06792"}
{"created":"2025-04-11","title":"Variable Metric Splitting Methods for Neuromorphic Circuits Simulation","abstract":"This paper proposes a variable metric splitting algorithm to solve the electrical behavior of neuromorphic circuits made of capacitors, memristive elements, and batteries. The gradient property of the memristive elements is exploited to split the current to voltage operator as the sum of the derivative operator, a Riemannian gradient operator, and a nonlinear residual operator that is linearized at each step of the algorithm. The diagonal structure of the three operators makes the variable metric forward-backward splitting algorithm scalable and amenable to the simulation of large-scale neuromorphic circuits.","authors":["Amir Shahhosseini","Thomas Burger","Rodolphe Sepulchre"],"url":"https://arxiv.org/abs/2504.06793"}
{"created":"2025-04-11","title":"Learning in Spiking Neural Networks with a Calcium-based Hebbian Rule for Spike-timing-dependent Plasticity","abstract":"Understanding how biological neural networks are shaped via local plasticity mechanisms can lead to energy-efficient and self-adaptive information processing systems, which promises to mitigate some of the current roadblocks in edge computing systems. While biology makes use of spikes to seamless use both spike timing and mean firing rate to modulate synaptic strength, most models focus on one of the two. In this work, we present a Hebbian local learning rule that models synaptic modification as a function of calcium traces tracking neuronal activity. We show how the rule reproduces results from spike time and spike rate protocols from neuroscientific studies. Moreover, we use the model to train spiking neural networks on MNIST digit recognition to show and explain what sort of mechanisms are needed to learn real-world patterns. We show how our model is sensitive to correlated spiking activity and how this enables it to modulate the learning rate of the network without altering the mean firing rate of the neurons nor the hyparameters of the learning rule. To the best of our knowledge, this is the first work that showcases how spike timing and rate can be complementary in their role of shaping the connectivity of spiking neural networks.","authors":["Willian Soares Gir\\~ao","Nicoletta Risi","Elisabetta Chicca"],"url":"https://arxiv.org/abs/2504.06796"}
{"created":"2025-04-11","title":"A Meaningful Perturbation Metric for Evaluating Explainability Methods","abstract":"Deep neural networks (DNNs) have demonstrated remarkable success, yet their wide adoption is often hindered by their opaque decision-making. To address this, attribution methods have been proposed to assign relevance values to each part of the input. However, different methods often produce entirely different relevance maps, necessitating the development of standardized metrics to evaluate them. Typically, such evaluation is performed through perturbation, wherein high- or low-relevance regions of the input image are manipulated to examine the change in prediction. In this work, we introduce a novel approach, which harnesses image generation models to perform targeted perturbation. Specifically, we focus on inpainting only the high-relevance pixels of an input image to modify the model's predictions while preserving image fidelity. This is in contrast to existing approaches, which often produce out-of-distribution modifications, leading to unreliable results. Through extensive experiments, we demonstrate the effectiveness of our approach in generating meaningful rankings across a wide range of models and attribution methods. Crucially, we establish that the ranking produced by our metric exhibits significantly higher correlation with human preferences compared to existing approaches, underscoring its potential for enhancing interpretability in DNNs.","authors":["Danielle Cohen","Hila Chefer","Lior Wolf"],"url":"https://arxiv.org/abs/2504.06800"}
{"created":"2025-04-11","title":"MonoPlace3D: Learning 3D-Aware Object Placement for 3D Monocular Detection","abstract":"Current monocular 3D detectors are held back by the limited diversity and scale of real-world datasets. While data augmentation certainly helps, it's particularly difficult to generate realistic scene-aware augmented data for outdoor settings. Most current approaches to synthetic data generation focus on realistic object appearance through improved rendering techniques. However, we show that where and how objects are positioned is just as crucial for training effective 3D monocular detectors. The key obstacle lies in automatically determining realistic object placement parameters - including position, dimensions, and directional alignment when introducing synthetic objects into actual scenes. To address this, we introduce MonoPlace3D, a novel system that considers the 3D scene content to create realistic augmentations. Specifically, given a background scene, MonoPlace3D learns a distribution over plausible 3D bounding boxes. Subsequently, we render realistic objects and place them according to the locations sampled from the learned distribution. Our comprehensive evaluation on two standard datasets KITTI and NuScenes, demonstrates that MonoPlace3D significantly improves the accuracy of multiple existing monocular 3D detectors while being highly data efficient.","authors":["Rishubh Parihar","Srinjay Sarkar","Sarthak Vora","Jogendra Kundu","R. Venkatesh Babu"],"url":"https://arxiv.org/abs/2504.06801"}
{"created":"2025-04-11","title":"DyDiT++: Dynamic Diffusion Transformers for Efficient Visual Generation","abstract":"Diffusion Transformer (DiT), an emerging diffusion model for visual generation, has demonstrated superior performance but suffers from substantial computational costs. Our investigations reveal that these costs primarily stem from the \\emph{static} inference paradigm, which inevitably introduces redundant computation in certain \\emph{diffusion timesteps} and \\emph{spatial regions}. To overcome this inefficiency, we propose \\textbf{Dy}namic \\textbf{Di}ffusion \\textbf{T}ransformer (DyDiT), an architecture that \\emph{dynamically} adjusts its computation along both \\emph{timestep} and \\emph{spatial} dimensions. Specifically, we introduce a \\emph{Timestep-wise Dynamic Width} (TDW) approach that adapts model width conditioned on the generation timesteps. In addition, we design a \\emph{Spatial-wise Dynamic Token} (SDT) strategy to avoid redundant computation at unnecessary spatial locations. TDW and SDT can be seamlessly integrated into DiT and significantly accelerates the generation process. Building on these designs, we further enhance DyDiT in three key aspects. First, DyDiT is integrated seamlessly with flow matching-based generation, enhancing its versatility. Furthermore, we enhance DyDiT to tackle more complex visual generation tasks, including video generation and text-to-image generation, thereby broadening its real-world applications. Finally, to address the high cost of full fine-tuning and democratize technology access, we investigate the feasibility of training DyDiT in a parameter-efficient manner and introduce timestep-based dynamic LoRA (TD-LoRA). Extensive experiments on diverse visual generation models, including DiT, SiT, Latte, and FLUX, demonstrate the effectiveness of DyDiT.","authors":["Wangbo Zhao","Yizeng Han","Jiasheng Tang","Kai Wang","Hao Luo","Yibing Song","Gao Huang","Fan Wang","Yang You"],"url":"https://arxiv.org/abs/2504.06803"}
{"created":"2025-04-11","title":"Robust Classification with Noisy Labels Based on Posterior Maximization","abstract":"Designing objective functions robust to label noise is crucial for real-world classification algorithms. In this paper, we investigate the robustness to label noise of an $f$-divergence-based class of objective functions recently proposed for supervised classification, herein referred to as $f$-PML. We show that, in the presence of label noise, any of the $f$-PML objective functions can be corrected to obtain a neural network that is equal to the one learned with the clean dataset. Additionally, we propose an alternative and novel correction approach that, during the test phase, refines the posterior estimated by the neural network trained in the presence of label noise. Then, we demonstrate that, even if the considered $f$-PML objective functions are not symmetric, they are robust to symmetric label noise for any choice of $f$-divergence, without the need for any correction approach. This allows us to prove that the cross-entropy, which belongs to the $f$-PML class, is robust to symmetric label noise. Finally, we show that such a class of objective functions can be used together with refined training strategies, achieving competitive performance against state-of-the-art techniques of classification with label noise.","authors":["Nicola Novello","Andrea M. Tonello"],"url":"https://arxiv.org/abs/2504.06805"}
{"created":"2025-04-11","title":"How do Copilot Suggestions Impact Developers' Frustration and Productivity?","abstract":"Context. AI-based development tools, such as GitHub Copilot, are transforming the software development process by offering real-time code suggestions. These tools promise to improve the productivity by reducing cognitive load and speeding up task completion. Previous exploratory studies, however, show that developers sometimes perceive the automatic suggestions as intrusive. As a result, they feel like their productivity decreased. Theory. We propose two theories on the impact of automatic suggestions on frustration and productivity. First, we hypothesize that experienced developers are frustrated from automatic suggestions (mostly from irrelevant ones), and this also negatively impacts their productivity. Second, we conjecture that novice developers benefit from automatic suggestions, which reduce the frustration caused from being stuck on a technical problem and thus increase their productivity. Objective. We plan to conduct a quasi-experimental study to test our theories. The empirical evidence we will collect will allow us to either corroborate or reject our theories. Method. We will involve at least 32 developers, both experts and novices. We will ask each of them to complete two software development tasks, one with automatic suggestions enabled and one with them disabled, allowing for within-subject comparisons. We will measure independent and dependent variables by monitoring developers' actions through an IDE plugin and screen recording. Besides, we will collect physiological data through a wearable device. We will use statistical hypothesis tests to study the effects of the treatments (i.e., automatic suggestions enabled/disabled) on the outcomes (frustration and productivity).","authors":["Emanuela Guglielmi","Venera Arnoudova","Gabriele Bavota","Rocco Oliveto","Simone Scalabrino"],"url":"https://arxiv.org/abs/2504.06808"}
{"created":"2025-04-11","title":"Hybrid CNN with Chebyshev Polynomial Expansion for Medical Image Analysis","abstract":"Lung cancer remains one of the leading causes of cancer-related mortality worldwide, with early and accurate diagnosis playing a pivotal role in improving patient outcomes. Automated detection of pulmonary nodules in computed tomography (CT) scans is a challenging task due to variability in nodule size, shape, texture, and location. Traditional Convolutional Neural Networks (CNNs) have shown considerable promise in medical image analysis; however, their limited ability to capture fine-grained spatial-spectral variations restricts their performance in complex diagnostic scenarios. In this study, we propose a novel hybrid deep learning architecture that incorporates Chebyshev polynomial expansions into CNN layers to enhance expressive power and improve the representation of underlying anatomical structures. The proposed Chebyshev-CNN leverages the orthogonality and recursive properties of Chebyshev polynomials to extract high-frequency features and approximate complex nonlinear functions with greater fidelity. The model is trained and evaluated on benchmark lung cancer imaging datasets, including LUNA16 and LIDC-IDRI, achieving superior performance in classifying pulmonary nodules as benign or malignant. Quantitative results demonstrate significant improvements in accuracy, sensitivity, and specificity compared to traditional CNN-based approaches. This integration of polynomial-based spectral approximation within deep learning provides a robust framework for enhancing automated medical diagnostics and holds potential for broader applications in clinical decision support systems.","authors":["Abhinav Roy","Bhavesh Gyanchandani","Aditya Oza"],"url":"https://arxiv.org/abs/2504.06811"}
{"created":"2025-04-11","title":"Introducing the Arm-membench Throughput Benchmark","abstract":"Application performance of modern day processors is often limited by the memory subsystem rather than actual compute capabilities. Therefore, data throughput specifications play a key role in modeling application performance and determining possible bottlenecks. However, while peak instruction throughputs and bandwidths for local caches are often documented, the achievable throughput can also depend on the relation between memory access and compute instructions. In this paper, we present an Arm version of the well established x86-membench throughput benchmark, which we have adapted to support all current SIMD extensions of the Armv8 instruction set architecture. We describe aspects of the Armv8 ISA that need to be considered in the portable design of this benchmark. We use the benchmark to analyze the memory subsystem at a fine spatial granularity and to unveil microarchitectural details of three processors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the resulting performance information, we show that instruction fetch and decoder widths become a potential bottleneck for cache-bandwidth-sensitive workloads due to the load-store concept of the Arm ISA.","authors":["Cyrill Burth","Markus Velten","Robert Sch\\\"one"],"url":"https://arxiv.org/abs/2504.06813"}
{"created":"2025-04-11","title":"SVG-IR: Spatially-Varying Gaussian Splatting for Inverse Rendering","abstract":"Reconstructing 3D assets from images, known as inverse rendering (IR), remains a challenging task due to its ill-posed nature. 3D Gaussian Splatting (3DGS) has demonstrated impressive capabilities for novel view synthesis (NVS) tasks. Methods apply it to relighting by separating radiance into BRDF parameters and lighting, yet produce inferior relighting quality with artifacts and unnatural indirect illumination due to the limited capability of each Gaussian, which has constant material parameters and normal, alongside the absence of physical constraints for indirect lighting. In this paper, we present a novel framework called Spatially-vayring Gaussian Inverse Rendering (SVG-IR), aimed at enhancing both NVS and relighting quality. To this end, we propose a new representation-Spatially-varying Gaussian (SVG)-that allows per-Gaussian spatially varying parameters. This enhanced representation is complemented by a SVG splatting scheme akin to vertex/fragment shading in traditional graphics pipelines. Furthermore, we integrate a physically-based indirect lighting model, enabling more realistic relighting. The proposed SVG-IR framework significantly improves rendering quality, outperforming state-of-the-art NeRF-based methods by 2.5 dB in peak signal-to-noise ratio (PSNR) and surpassing existing Gaussian-based techniques by 3.5 dB in relighting tasks, all while maintaining a real-time rendering speed.","authors":["Hanxiao Sun","YuPeng Gao","Jin Xie","Jian Yang","Beibei Wang"],"url":"https://arxiv.org/abs/2504.06815"}
{"created":"2025-04-11","title":"A Graph Diffusion Algorithm for Lexical Similarity Evaluation","abstract":"In this paper, we present an algorithm for evaluating lexical similarity between a given language and several reference language clusters. As an input, we have a list of concepts and the corresponding translations in all considered languages. Moreover, each reference language is assigned to one of $c$ language clusters. For each of the concepts, the algorithm computes the distance between each pair of translations. Based on these distances, it constructs a weighted directed graph, where every vertex represents a language. After, it solves a graph diffusion equation with a Dirichlet boundary condition, where the unknown is a map from the vertex set to $\\mathbb{R}^c$. The resulting coordinates are values from the interval $[0,1]$ and they can be interpreted as probabilities of belonging to each of the clusters or as a lexical similarity distribution with respect to the reference clusters. The distances between translations are calculated using phonetic transcriptions and a modification of the Damerau-Levenshtein distance. The algorithm can be useful in analyzing relationships between languages spoken in multilingual territories with a lot of mutual influences. We demonstrate this by presenting a case study regarding various European languages.","authors":["Karol Mikula","Mariana Sarkociov\\'a Reme\\v{s}\\'ikov\\'a"],"url":"https://arxiv.org/abs/2504.06816"}
{"created":"2025-04-11","title":"Deep Neural Koopman Operator-based Economic Model Predictive Control of Shipboard Carbon Capture System","abstract":"Shipboard carbon capture is a promising solution to help reduce carbon emissions in international shipping. In this work, we propose a data-driven dynamic modeling and economic predictive control approach within the Koopman framework. This integrated modeling and control approach is used to achieve safe and energy-efficient process operation of shipboard post-combustion carbon capture plants. Specifically, we propose a deep neural Koopman operator modeling approach, based on which a Koopman model with time-varying model parameters is established. This Koopman model predicts the overall economic operational cost and key system outputs, based on accessible partial state measurements. By leveraging this learned model, a constrained economic predictive control scheme is developed. Despite time-varying parameters involved in the formulated model, the formulated optimization problem associated with the economic predictive control design is convex, and it can be solved efficiently during online control implementations. Extensive tests are conducted on a high-fidelity simulation environment for shipboard post-combustion carbon capture processes. Four ship operational conditions are taken into account. The results show that the proposed method significantly improves the overall economic operational performance and carbon capture rate. Additionally, the proposed method guarantees safe operation by ensuring that hard constraints on the system outputs are satisfied.","authors":["Minghao Han","Xunyuan Yin"],"url":"https://arxiv.org/abs/2504.06818"}
{"created":"2025-04-11","title":"Developing Modular Grasping and Manipulation Pipeline Infrastructure to Streamline Performance Benchmarking","abstract":"The robot manipulation ecosystem currently faces issues with integrating open-source components and reproducing results. This limits the ability of the community to benchmark and compare the performance of different solutions to one another in an effective manner, instead relying on largely holistic evaluations. As part of the COMPARE Ecosystem project, we are developing modular grasping and manipulation pipeline infrastructure in order to streamline performance benchmarking. The infrastructure will be used towards the establishment of standards and guidelines for modularity and improved open-source development and benchmarking. This paper provides a high-level overview of the architecture of the pipeline infrastructure, experiments conducted to exercise it during development, and future work to expand its modularity.","authors":["Brian Flynn","Kostas Bekris","Berk Calli","Aaron Dollar","Adam Norton","Yu Sun","Holly Yanco"],"url":"https://arxiv.org/abs/2504.06819"}
{"created":"2025-04-11","title":"Regret Bounds for Robust Online Decision Making","abstract":"We propose a framework which generalizes \"decision making with structured observations\" by allowing robust (i.e. multivalued) models. In this framework, each model associates each decision with a convex set of probability distributions over outcomes. Nature can choose distributions out of this set in an arbitrary (adversarial) manner, that can be nonoblivious and depend on past history. The resulting framework offers much greater generality than classical bandits and reinforcement learning, since the realizability assumption becomes much weaker and more realistic. We then derive a theory of regret bounds for this framework. Although our lower and upper bounds are not tight, they are sufficient to fully characterize power-law learnability. We demonstrate this theory in two special cases: robust linear bandits and tabular robust online reinforcement learning. In both cases, we derive regret bounds that improve state-of-the-art (except that we do not address computational efficiency).","authors":["Alexander Appel","Vanessa Kosoy"],"url":"https://arxiv.org/abs/2504.06820"}
{"created":"2025-04-11","title":"Inducing Programmatic Skills for Agentic Tasks","abstract":"To succeed in common digital tasks such as web navigation, agents must carry out a variety of specialized tasks such as searching for products or planning a travel route. To tackle these tasks, agents can bootstrap themselves by learning task-specific skills online through interaction with the web environment. In this work, we demonstrate that programs are an effective representation for skills. We propose agent skill induction (ASI), which allows agents to adapt themselves by inducing, verifying, and utilizing program-based skills on the fly. We start with an evaluation on the WebArena agent benchmark and show that ASI outperforms the static baseline agent and its text-skill counterpart by 23.5% and 11.3% in success rate, mainly thanks to the programmatic verification guarantee during the induction phase. ASI also improves efficiency by reducing 10.7-15.3% of the steps over baselines, by composing primitive actions (e.g., click) into higher-level skills (e.g., search product). We then highlight the efficacy of ASI in remaining efficient and accurate under scaled-up web activities. Finally, we examine the generalizability of induced skills when transferring between websites, and find that ASI can effectively reuse common skills, while also updating incompatible skills to versatile website changes.","authors":["Zora Zhiruo Wang","Apurva Gandhi","Graham Neubig","Daniel Fried"],"url":"https://arxiv.org/abs/2504.06821"}
{"created":"2025-04-11","title":"Open Problems and a Hypothetical Path Forward in LLM Knowledge Paradigms","abstract":"Knowledge is fundamental to the overall capabilities of Large Language Models (LLMs). The knowledge paradigm of a model, which dictates how it encodes and utilizes knowledge, significantly affects its performance. Despite the continuous development of LLMs under existing knowledge paradigms, issues within these frameworks continue to constrain model potential.","authors":["Xiaotian Ye","Mengqi Zhang","Shu Wu"],"url":"https://arxiv.org/abs/2504.06823"}
{"created":"2025-04-11","title":"IAAO: Interactive Affordance Learning for Articulated Objects in 3D Environments","abstract":"This work presents IAAO, a novel framework that builds an explicit 3D model for intelligent agents to gain understanding of articulated objects in their environment through interaction. Unlike prior methods that rely on task-specific networks and assumptions about movable parts, our IAAO leverages large foundation models to estimate interactive affordances and part articulations in three stages. We first build hierarchical features and label fields for each object state using 3D Gaussian Splatting (3DGS) by distilling mask features and view-consistent labels from multi-view images. We then perform object- and part-level queries on the 3D Gaussian primitives to identify static and articulated elements, estimating global transformations and local articulation parameters along with affordances. Finally, scenes from different states are merged and refined based on the estimated transformations, enabling robust affordance-based interaction and manipulation of objects. Experimental results demonstrate the effectiveness of our method.","authors":["Can Zhang","Gim Hee Lee"],"url":"https://arxiv.org/abs/2504.06827"}
{"created":"2025-04-11","title":"Adaptive Locally Linear Embedding","abstract":"Manifold learning techniques, such as Locally linear embedding (LLE), are designed to preserve the local neighborhood structures of high-dimensional data during dimensionality reduction. Traditional LLE employs Euclidean distance to define neighborhoods, which can struggle to capture the intrinsic geometric relationships within complex data. A novel approach, Adaptive locally linear embedding(ALLE), is introduced to address this limitation by incorporating a dynamic, data-driven metric that enhances topological preservation. This method redefines the concept of proximity by focusing on topological neighborhood inclusion rather than fixed distances. By adapting the metric based on the local structure of the data, it achieves superior neighborhood preservation, particularly for datasets with complex geometries and high-dimensional structures. Experimental results demonstrate that ALLE significantly improves the alignment between neighborhoods in the input and feature spaces, resulting in more accurate and topologically faithful embeddings. This approach advances manifold learning by tailoring distance metrics to the underlying data, providing a robust solution for capturing intricate relationships in high-dimensional datasets.","authors":["Ali Goli","Mahdieh Alizadeh","Hadi Sadoghi Yazdi"],"url":"https://arxiv.org/abs/2504.06829"}
{"created":"2025-04-11","title":"On a Characterization of Spartan Graphs","abstract":"The eternal vertex cover game is played between an attacker and a defender on an undirected graph $G$. The defender identifies $k$ vertices to position guards on to begin with. The attacker, on their turn, attacks an edge $e$, and the defender must move a guard along $e$ to defend the attack. The defender may move other guards as well, under the constraint that every guard moves at most once and to a neighboring vertex. The smallest number of guards required to defend attacks forever is called the eternal vertex cover number of $G$, denoted $evc(G)$.","authors":["Neeldhara Misra","Saraswati Girish Nanoti"],"url":"https://arxiv.org/abs/2504.06832"}
{"created":"2025-04-11","title":"Symbolic Parallel Composition for Multi-language Protocol Verification","abstract":"The implementation of security protocols often combines different languages. This practice, however, poses a challenge to traditional verification techniques, which typically assume a single-language environment and, therefore, are insufficient to handle challenges presented by the interplay of different languages. To address this issue, we establish principles for combining multiple programming languages operating on different atomic types using a symbolic execution semantics. This facilitates the (parallel) composition of labeled transition systems, improving the analysis of complex systems by streamlining communication between diverse programming languages. By treating the Dolev-Yao (DY) model as a symbolic abstraction, our approach eliminates the need for translation between different base types, such as bitstrings and DY terms. Our technique provides a foundation for securing interactions in multi-language environments, enhancing program verification and system analysis in complex, interconnected systems.","authors":["Faezeh Nasrabadi","Robert K\\\"unnemann","Hamed Nemati"],"url":"https://arxiv.org/abs/2504.06833"}
{"created":"2025-04-11","title":"LVC: A Lightweight Compression Framework for Enhancing VLMs in Long Video Understanding","abstract":"Long video understanding is a complex task that requires both spatial detail and temporal awareness. While Vision-Language Models (VLMs) obtain frame-level understanding capabilities through multi-frame input, they suffer from information loss due to the sparse sampling strategy. In contrast, Video Large Language Models (Video-LLMs) capture temporal relationships within visual features but are limited by the scarcity of high-quality video-text datasets. To transfer long video understanding capabilities to VLMs with minimal data and computational cost, we propose Lightweight Video Compression (LVC), a novel method featuring the Query-Attention Video Compression mechanism, which effectively tackles the sparse sampling problem in VLMs. By training only the alignment layer with 10k short video-text pairs, LVC significantly enhances the temporal reasoning abilities of VLMs. Extensive experiments show that LVC provides consistent performance improvements across various models, including the InternVL2 series and Phi-3.5-Vision. Notably, the InternVL2-40B-LVC achieves scores of 68.2 and 65.9 on the long video understanding benchmarks MLVU and Video-MME, respectively, with relative improvements of 14.6% and 7.7%. The enhanced models and code will be publicly available soon.","authors":["Ziyi Wang","Haoran Wu","Yiming Rong","Deyang Jiang","Yixin Zhang","Yunlong Zhao","Shuang Xu","Bo XU"],"url":"https://arxiv.org/abs/2504.06835"}
{"created":"2025-04-11","title":"Determining Fetal Orientations From Blind Sweep Ultrasound Video","abstract":"Cognitive demands of fetal ultrasound examinations pose unique challenges among clinicians. With the goal of providing an assistive tool, we developed an automated pipeline for predicting fetal orientation from ultrasound videos acquired following a simple blind sweep protocol. Leveraging on a pre-trained head detection and segmentation model, this is achieved by first determining the fetal presentation (cephalic or breech) with a template matching approach, followed by the fetal lie (facing left or right) by analyzing the spatial distribution of segmented brain anatomies. Evaluation on a dataset of third-trimester ultrasound scans demonstrated the promising accuracy of our pipeline. This work distinguishes itself by introducing automated fetal lie prediction and by proposing an assistive paradigm that augments sonographer expertise rather than replacing it. Future research will focus on enhancing acquisition efficiency, and exploring real-time clinical integration to improve workflow and support for obstetric clinicians.","authors":["Jakub Maciej Wi\\'sniewski","Anders Nymark Christensen","Mary Le Ngo","Martin Gr{\\o}nneb{\\ae}k Tolsgaard","Chun Kit Wong"],"url":"https://arxiv.org/abs/2504.06836"}
{"created":"2025-04-11","title":"ZIP: An Efficient Zeroth-order Prompt Tuning for Black-box Vision-Language Models","abstract":"Recent studies have introduced various approaches for prompt-tuning black-box vision-language models, referred to as black-box prompt-tuning (BBPT). While BBPT has demonstrated considerable potential, it is often found that many existing methods require an excessive number of queries (i.e., function evaluations), which poses a significant challenge in real-world scenarios where the number of allowed queries is limited. To tackle this issue, we propose Zeroth-order Intrinsic-dimensional Prompt-tuning (ZIP), a novel approach that enables efficient and robust prompt optimization in a purely black-box setting. The key idea of ZIP is to reduce the problem dimensionality and the variance of zeroth-order gradient estimates, such that the training is done fast with far less queries. We achieve this by re-parameterizing prompts in low-rank representations and designing intrinsic-dimensional clipping of estimated gradients. We evaluate ZIP on 13+ vision-language tasks in standard benchmarks and show that it achieves an average improvement of approximately 6% in few-shot accuracy and 48% in query efficiency compared to the best-performing alternative BBPT methods, establishing a new state of the art. Our ablation analysis further shows that the proposed clipping mechanism is robust and nearly optimal, without the need to manually select the clipping threshold, matching the result of expensive hyperparameter search.","authors":["Seonghwan Park","Jaehyeon Jeong","Yongjun Kim","Jaeho Lee","Namhoon Lee"],"url":"https://arxiv.org/abs/2504.06838"}
{"created":"2025-04-11","title":"Classifying the Unknown: In-Context Learning for Open-Vocabulary Text and Symbol Recognition","abstract":"We introduce Rosetta, a multimodal model that leverages Multimodal In-Context Learning (MICL) to classify sequences of novel script patterns in documents by leveraging minimal examples, thus eliminating the need for explicit retraining. To enhance contextual learning, we designed a dataset generation process that ensures varying degrees of contextual informativeness, improving the model's adaptability in leveraging context across different scenarios. A key strength of our method is the use of a Context-Aware Tokenizer (CAT), which enables open-vocabulary classification. This allows the model to classify text and symbol patterns across an unlimited range of classes, extending its classification capabilities beyond the scope of its training alphabet of patterns. As a result, it unlocks applications such as the recognition of new alphabets and languages. Experiments on synthetic datasets demonstrate the potential of Rosetta to successfully classify Out-Of-Distribution visual patterns and diverse sets of alphabets and scripts, including but not limited to Chinese, Greek, Russian, French, Spanish, and Japanese.","authors":["Tom Simon","William Mocaer","Pierrick Tranouez","Clement Chatelain","Thierry Paquet"],"url":"https://arxiv.org/abs/2504.06841"}
{"created":"2025-04-11","title":"Optimality of Gradient-MUSIC for Spectral Estimation","abstract":"The goal of spectral estimation is to estimate the frequencies and amplitudes of a nonharmonic Fourier sum given noisy time samples. This paper introduces the Gradient-MUSIC algorithm, which is a novel nonconvex optimization reformulation of the classical MUSIC algorithm. Under the assumption that $m\\Delta\\geq 8\\pi$, where $\\pi/m$ is the Nyquist rate and $\\Delta$ is the minimum separation of the frequencies normalized to be in $[0,2\\pi)$, we provide a thorough geometric analysis of the objective functions generated by the algorithm. Gradient-MUSIC thresholds the objective function on a set that is as coarse as possible and locates a set of suitable initialization for gradient descent. Although the objective function is nonconvex, gradient descent converges exponentially fast to the desired local minima, which are the estimated frequencies of the signal. For deterministic $\\ell^p$ perturbations and any $p\\in [1,\\infty]$, Gradient-MUSIC estimates the frequencies and amplitudes at the minimax optimal rate in terms of the noise level and $m$. For example, if the noise has $\\ell^\\infty$ norm at most $\\epsilon$, then the frequencies and amplitudes are recovered up to error at most $C\\epsilon/m$ and $C\\epsilon$, respectively, which are optimal in $\\epsilon$ and $m$. Aside from logarithmic factors, Gradient-MUSIC is optimal for white noise and matches the rate achieved by nonlinear least squares for various families of nonstationary independent Gaussian noise. Our results show that classical MUSIC is equally optimal, but it requires an expensive search on a thin grid, whereas Gradient-MUSIC is always computationally more efficient, especially for small noise. As a consequence of this paper, for sufficiently well separated frequencies, both Gradient-MUSIC and classical MUSIC are the first provably optimal and computationally tractable algorithms for deterministic $\\ell^p$ perturbations.","authors":["Albert Fannjiang","Weilin Li","Wenjing Liao"],"url":"https://arxiv.org/abs/2504.06842"}
{"created":"2025-04-11","title":"Integrating Cognitive Processing Signals into Language Models: A Review of Advances, Applications and Future Directions","abstract":"Recently, the integration of cognitive neuroscience in Natural Language Processing (NLP) has gained significant attention. This article provides a critical and timely overview of recent advancements in leveraging cognitive signals, particularly Eye-tracking (ET) signals, to enhance Language Models (LMs) and Multimodal Large Language Models (MLLMs). By incorporating user-centric cognitive signals, these approaches address key challenges, including data scarcity and the environmental costs of training large-scale models. Cognitive signals enable efficient data augmentation, faster convergence, and improved human alignment. The review emphasises the potential of ET data in tasks like Visual Question Answering (VQA) and mitigating hallucinations in MLLMs, and concludes by discussing emerging challenges and research trends.","authors":["Angela Lopez-Cardona","Sebastian Idesis","Ioannis Arapakis"],"url":"https://arxiv.org/abs/2504.06843"}
{"created":"2025-04-11","title":"TXSQL: Lock Optimizations Towards High Contented Workloads (Extended Version)","abstract":"Two-phase locking (2PL) is a fundamental and widely used concurrency control protocol. It regulates concurrent access to database data by following a specific sequence of acquiring and releasing locks during transaction execution, thereby ensuring transaction isolation. However, in strict 2PL, transactions must wait for conflicting transactions to commit and release their locks, which reduces concurrency and system throughput. We have observed this issue is exacerbated in high-contented workloads at Tencent, where lock contention can severely degrade system performance. While existing optimizations demonstrate some effectiveness in high-contention scenarios, their performance remains insufficient, as they suffer from lock contention and waiting in hotspot access.","authors":["Donghui Wang","Yuxing Chen","Chengyao Jiang","Anqun Pan","Wei Jiang","Songli Wang","Hailin Lei","Chong Zhu","Lixiong Zheng","Wei Lu","Yunpeng Chai","Feng Zhang","Xiaoyong Du"],"url":"https://arxiv.org/abs/2504.06854"}
{"created":"2025-04-11","title":"CasTex: Cascaded Text-to-Texture Synthesis via Explicit Texture Maps and Physically-Based Shading","abstract":"This work investigates text-to-texture synthesis using diffusion models to generate physically-based texture maps. We aim to achieve realistic model appearances under varying lighting conditions. A prominent solution for the task is score distillation sampling. It allows recovering a complex texture using gradient guidance given a differentiable rasterization and shading pipeline. However, in practice, the aforementioned solution in conjunction with the widespread latent diffusion models produces severe visual artifacts and requires additional regularization such as implicit texture parameterization. As a more direct alternative, we propose an approach using cascaded diffusion models for texture synthesis (CasTex). In our setup, score distillation sampling yields high-quality textures out-of-the box. In particular, we were able to omit implicit texture parameterization in favor of an explicit parameterization to improve the procedure. In the experiments, we show that our approach significantly outperforms state-of-the-art optimization-based solutions on public texture synthesis benchmarks.","authors":["Mishan Aliev","Dmitry Baranchuk","Kirill Struminsky"],"url":"https://arxiv.org/abs/2504.06856"}
{"created":"2025-04-11","title":"Machine Learning (ML) based Reduced Order Modeling (ROM) for linear and non-linear solid and structural mechanics","abstract":"Multiple model reduction techniques have been proposed to tackle linear and non linear problems. Intrusive model order reduction techniques exhibit high accuracy levels, however, they are rarely used as a standalone industrial tool, because of the required high level knowledge involved in the construction and usage of these techniques. Moreover, the computation time benefit is compromised for highly nonlinear problems. On the other hand, non-intrusive methods often struggle with accuracy in nonlinear cases, typically requiring a large design of experiment and a large number of snapshots achieve a reliable performance. However, generating the stiffness matrix in a non-intrusive approach presents an optimal way to align accuracy with efficiency, allying the advantages of both intrusive and non-intrusive methods.This work introduces a lightly intrusive model order reduction technique that employs machine learning within a Proper Orthogonal Decomposition framework to achieve this alliance. By leveraging outputs from commercial full-order models, this method constructs a reduced-order model that operates effectively without requiring expert user intervention. The proposed technique has the possibility to approximate linear non affine as well as non linear terms. It is showcased for linear and nonlinear structural mechanics problems.","authors":["Mikhael Tannous (Arts et Metiers Institute of Technology","Paris","France)","Chady Ghnatios (University of North Florida","Jacklsonville","United States)","Eivind Fonn (SINTEF Digital","Trondheim","Norway)","Trond Kvamsdal (Norwegian University of Science and Technology","Trondheim","Norway","SINTEF Digital","Trondheim","Norway)","Francisco Chinesta (Arts et Metiers Institute of Technology","Paris","France)"],"url":"https://arxiv.org/abs/2504.06860"}
{"created":"2025-04-11","title":"EIDT-V: Exploiting Intersections in Diffusion Trajectories for Model-Agnostic, Zero-Shot, Training-Free Text-to-Video Generation","abstract":"Zero-shot, training-free, image-based text-to-video generation is an emerging area that aims to generate videos using existing image-based diffusion models. Current methods in this space require specific architectural changes to image generation models, which limit their adaptability and scalability. In contrast to such methods, we provide a model-agnostic approach. We use intersections in diffusion trajectories, working only with the latent values. We could not obtain localized frame-wise coherence and diversity using only the intersection of trajectories. Thus, we instead use a grid-based approach. An in-context trained LLM is used to generate coherent frame-wise prompts; another is used to identify differences between frames. Based on these, we obtain a CLIP-based attention mask that controls the timing of switching the prompts for each grid cell. Earlier switching results in higher variance, while later switching results in more coherence. Therefore, our approach can ensure appropriate control between coherence and variance for the frames. Our approach results in state-of-the-art performance while being more flexible when working with diverse image-generation models. The empirical analysis using quantitative metrics and user studies confirms our model's superior temporal consistency, visual fidelity and user satisfaction, thus providing a novel way to obtain training-free, image-based text-to-video generation.","authors":["Diljeet Jagpal","Xi Chen","Vinay P. Namboodiri"],"url":"https://arxiv.org/abs/2504.06861"}
{"created":"2025-04-11","title":"MovSAM: A Single-image Moving Object Segmentation Framework Based on Deep Thinking","abstract":"Moving object segmentation plays a vital role in understanding dynamic visual environments. While existing methods rely on multi-frame image sequences to identify moving objects, single-image MOS is critical for applications like motion intention prediction and handling camera frame drops. However, segmenting moving objects from a single image remains challenging for existing methods due to the absence of temporal cues. To address this gap, we propose MovSAM, the first framework for single-image moving object segmentation. MovSAM leverages a Multimodal Large Language Model (MLLM) enhanced with Chain-of-Thought (CoT) prompting to search the moving object and generate text prompts based on deep thinking for segmentation. These prompts are cross-fused with visual features from the Segment Anything Model (SAM) and a Vision-Language Model (VLM), enabling logic-driven moving object segmentation. The segmentation results then undergo a deep thinking refinement loop, allowing MovSAM to iteratively improve its understanding of the scene context and inter-object relationships with logical reasoning. This innovative approach enables MovSAM to segment moving objects in single images by considering scene understanding. We implement MovSAM in the real world to validate its practical application and effectiveness for autonomous driving scenarios where the multi-frame methods fail. Furthermore, despite the inherent advantage of multi-frame methods in utilizing temporal information, MovSAM achieves state-of-the-art performance across public MOS benchmarks, reaching 92.5\\% on J\\&amp;F. Our implementation will be available at https://github.com/IRMVLab/MovSAM.","authors":["Chang Nie","Yiqing Xu","Guangming Wang","Zhe Liu","Yanzi Miao","Hesheng Wang"],"url":"https://arxiv.org/abs/2504.06863"}
{"created":"2025-04-11","title":"GraspClutter6D: A Large-scale Real-world Dataset for Robust Perception and Grasping in Cluttered Scenes","abstract":"Robust grasping in cluttered environments remains an open challenge in robotics. While benchmark datasets have significantly advanced deep learning methods, they mainly focus on simplistic scenes with light occlusion and insufficient diversity, limiting their applicability to practical scenarios. We present GraspClutter6D, a large-scale real-world grasping dataset featuring: (1) 1,000 highly cluttered scenes with dense arrangements (14.1 objects/scene, 62.6\\% occlusion), (2) comprehensive coverage across 200 objects in 75 environment configurations (bins, shelves, and tables) captured using four RGB-D cameras from multiple viewpoints, and (3) rich annotations including 736K 6D object poses and 9.3B feasible robotic grasps for 52K RGB-D images. We benchmark state-of-the-art segmentation, object pose estimation, and grasping detection methods to provide key insights into challenges in cluttered environments. Additionally, we validate the dataset's effectiveness as a training resource, demonstrating that grasping networks trained on GraspClutter6D significantly outperform those trained on existing datasets in both simulation and real-world experiments. The dataset, toolkit, and annotation tools are publicly available on our project website: https://sites.google.com/view/graspclutter6d.","authors":["Seunghyeok Back","Joosoon Lee","Kangmin Kim","Heeseon Rho","Geonhyup Lee","Raeyoung Kang","Sangbeom Lee","Sangjun Noh","Youngjin Lee","Taeyeop Lee","Kyoobin Lee"],"url":"https://arxiv.org/abs/2504.06866"}
{"created":"2025-04-11","title":"Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games","abstract":"Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: PersonalityAdapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.","authors":["Seungwon Lim","Seungbeen Lee","Dongjun Min","Youngjae Yu"],"url":"https://arxiv.org/abs/2504.06868"}
{"created":"2025-04-11","title":"Grouping Strategies on Two-Phase Methods for Bi-objective Combinatorial Optimization","abstract":"Two-phase methods are commonly used to solve bi-objective combinatorial optimization problems. In the first phase, all extreme supported nondominated points are generated through a dichotomic search. This phase also allows the identification of search zones that may contain other nondominated points. The second phase focuses on exploring these search zones to locate the remaining points, which typically accounts for most of the computational cost. Ranking algorithms are frequently employed to explore each zone individually, but this approach leads to redundancies, causing multiple visits to the same solutions. To mitigate these redundancies, we propose several strategies that group adjacent zones, allowing a single run of the ranking algorithm for the entire group. Additionally, we explore an implicit grouping approach based on a new concept of coverage. Our experiments on the Bi-Objective Spanning Tree Problem demonstrate the beneficial impact of these grouping strategies when combined with coverage.","authors":["Felipe O. Mota","Lu\\'is Paquete","Daniel Vanderpooten"],"url":"https://arxiv.org/abs/2504.06869"}
{"created":"2025-04-11","title":"A Dataset of Software Bill of Materials for Evaluating SBOM Consumption Tools","abstract":"A Software Bill of Materials (SBOM) is becoming an essential tool for effective software dependency management. An SBOM is a list of components used in software, including details such as component names, versions, and licenses. Using SBOMs, developers can quickly identify software components and assess whether their software depends on vulnerable libraries. Numerous tools support software dependency management through SBOMs, which can be broadly categorized into two types: tools that generate SBOMs and tools that utilize SBOMs. A substantial collection of accurate SBOMs is required to evaluate tools that utilize SBOMs. However, there is no publicly available dataset specifically designed for this purpose, and research on SBOM consumption tools remains limited. In this paper, we present a dataset of SBOMs to address this gap. The dataset we constructed comprises 46 SBOMs generated from real-world Java projects, with plans to expand it to include a broader range of projects across various programming languages. Accurate and well-structured SBOMs enable researchers to evaluate the functionality of SBOM consumption tools and identify potential issues. We collected 3,271 Java projects from GitHub and generated SBOMs for 798 of them using Maven with an open-source SBOM generation tool. These SBOMs were refined through both automatic and manual corrections to ensure accuracy, currently resulting in 46 SBOMs that comply with the SPDX Lite profile, which defines minimal requirements tailored to practical workflows in industries. This process also revealed issues with the SBOM generation tools themselves. The dataset is publicly available on Zenodo (DOI: 10.5281/zenodo.14233415).","authors":["Rio Kishimoto","Tetsuya Kanda","Yuki Manabe","Katsuro Inoue","Shi Qiu","Yoshiki Higo"],"url":"https://arxiv.org/abs/2504.06880"}
{"created":"2025-04-11","title":"Compound and Parallel Modes of Tropical Convolutional Neural Networks","abstract":"Convolutional neural networks have become increasingly deep and complex, leading to higher computational costs. While tropical convolutional neural networks (TCNNs) reduce multiplications, they underperform compared to standard CNNs. To address this, we propose two new variants - compound TCNN (cTCNN) and parallel TCNN (pTCNN)-that use combinations of tropical min-plus and max-plus kernels to replace traditional convolution kernels. This reduces multiplications and balances efficiency with performance. Experiments on various datasets show that cTCNN and pTCNN match or exceed the performance of other CNN methods. Combining these with conventional CNNs in deeper architectures also improves performance. We are further exploring simplified TCNN architectures that reduce parameters and multiplications with minimal accuracy loss, aiming for efficient and effective models.","authors":["Mingbo Li","Liying Liu","Ye Luo"],"url":"https://arxiv.org/abs/2504.06881"}
{"created":"2025-04-11","title":"Audio-visual Event Localization on Portrait Mode Short Videos","abstract":"Audio-visual event localization (AVEL) plays a critical role in multimodal scene understanding. While existing datasets for AVEL predominantly comprise landscape-oriented long videos with clean and simple audio context, short videos have become the primary format of online video content due to the the proliferation of smartphones. Short videos are characterized by portrait-oriented framing and layered audio compositions (e.g., overlapping sound effects, voiceovers, and music), which brings unique challenges unaddressed by conventional methods. To this end, we introduce AVE-PM, the first AVEL dataset specifically designed for portrait mode short videos, comprising 25,335 clips that span 86 fine-grained categories with frame-level annotations. Beyond dataset creation, our empirical analysis shows that state-of-the-art AVEL methods suffer an average 18.66% performance drop during cross-mode evaluation. Further analysis reveals two key challenges of different video formats: 1) spatial bias from portrait-oriented framing introduces distinct domain priors, and 2) noisy audio composition compromise the reliability of audio modality. To address these issues, we investigate optimal preprocessing recipes and the impact of background music for AVEL on portrait mode videos. Experiments show that these methods can still benefit from tailored preprocessing and specialized model design, thus achieving improved performance. This work provides both a foundational benchmark and actionable insights for advancing AVEL research in the era of mobile-centric video content. Dataset and code will be released.","authors":["Wuyang Liu","Yi Chai","Yongpeng Yan","Yanzhen Ren"],"url":"https://arxiv.org/abs/2504.06884"}
{"created":"2025-04-11","title":"Mixed-Precision in High-Order Methods: the Impact of Floating-Point Precision on the ADER-DG Algorithm","abstract":"We present a mixed-precision implementation of the high-order discontinuous Galerkin method with ADER time stepping (ADER-DG) for solving hyperbolic systems of partial differential equations (PDEs) in the hyperbolic PDE engine ExaHyPE. The implementation provides a simple API extension for specifying the numerical precision for individual kernels, and thus allows for testing the effect of low and mixed precision on the accuracy of the solution. To showcase this, we study the impact of precision on the overall convergence order and actual accuracy of the method as achieved for four common hyperbolic PDE systems and five relevant scenarios that feature an analytic solution. For all scenarios, we also assess how sensitive each kernel of the ADER-DG algorithm is to using double, single or even half precision. This addresses the question where thoughtful adoption of mixed precision can mitigate hurtful effects of low precision on the overall simulation.","authors":["Marc Marot-Lassauzaie","Michael Bader"],"url":"https://arxiv.org/abs/2504.06889"}
{"created":"2025-04-11","title":"AI-Driven Consensus: Modeling Multi-Agent Networks with Long-Range Interactions through path-Laplacian Matrices","abstract":"Extended connectivity in graphs can be analyzed through k-path Laplacian matrices, which permit the capture of long-range interactions in various real-world networked systems such as social, transportation, and multi-agent networks. In this work, we present several alternative methods based on machine learning methods (LSTM, xLSTM, Transformer, XGBoost, and ConvLSTM) to predict the final consensus value based on directed networks (Erd\\\"os-Renyi, Watts-Strogatz, and Barab\\'asi-Albert) and on the initial state. We highlight how different k-hop interactions affect the performance of the tested methods. This framework opens new avenues for analyzing multi-scale diffusion processes in large-scale, complex networks.","authors":["Yusef Ahsini","Bel\\'en Reverte","J. Alberto Conejero"],"url":"https://arxiv.org/abs/2504.06894"}
{"created":"2025-04-11","title":"ColorizeDiffusion v2: Enhancing Reference-based Sketch Colorization Through Separating Utilities","abstract":"Reference-based sketch colorization methods have garnered significant attention due to their potential applications in the animation production industry. However, most existing methods are trained with image triplets of sketch, reference, and ground truth that are semantically and spatially well-aligned, while real-world references and sketches often exhibit substantial misalignment. This mismatch in data distribution between training and inference leads to overfitting, consequently resulting in spatial artifacts and significant degradation in overall colorization quality, limiting potential applications of current methods for general purposes. To address this limitation, we conduct an in-depth analysis of the \\textbf{carrier}, defined as the latent representation facilitating information transfer from reference to sketch. Based on this analysis, we propose a novel workflow that dynamically adapts the carrier to optimize distinct aspects of colorization. Specifically, for spatially misaligned artifacts, we introduce a split cross-attention mechanism with spatial masks, enabling region-specific reference injection within the diffusion process. To mitigate semantic neglect of sketches, we employ dedicated background and style encoders to transfer detailed reference information in the latent feature space, achieving enhanced spatial control and richer detail synthesis. Furthermore, we propose character-mask merging and background bleaching as preprocessing steps to improve foreground-background integration and background generation. Extensive qualitative and quantitative evaluations, including a user study, demonstrate the superior performance of our proposed method compared to existing approaches. An ablation study further validates the efficacy of each proposed component.","authors":["Dingkun Yan","Xinrui Wang","Yusuke Iwasawa","Yutaka Matsuo","Suguru Saito","Jiaxian Guo"],"url":"https://arxiv.org/abs/2504.06895"}
{"created":"2025-04-11","title":"MedSegFactory: Text-Guided Generation of Medical Image-Mask Pairs","abstract":"This paper presents MedSegFactory, a versatile medical synthesis framework that generates high-quality paired medical images and segmentation masks across modalities and tasks. It aims to serve as an unlimited data repository, supplying image-mask pairs to enhance existing segmentation tools. The core of MedSegFactory is a dual-stream diffusion model, where one stream synthesizes medical images and the other generates corresponding segmentation masks. To ensure precise alignment between image-mask pairs, we introduce Joint Cross-Attention (JCA), enabling a collaborative denoising paradigm by dynamic cross-conditioning between streams. This bidirectional interaction allows both representations to guide each other's generation, enhancing consistency between generated pairs. MedSegFactory unlocks on-demand generation of paired medical images and segmentation masks through user-defined prompts that specify the target labels, imaging modalities, anatomical regions, and pathological conditions, facilitating scalable and high-quality data generation. This new paradigm of medical image synthesis enables seamless integration into diverse medical imaging workflows, enhancing both efficiency and accuracy. Extensive experiments show that MedSegFactory generates data of superior quality and usability, achieving competitive or state-of-the-art performance in 2D and 3D segmentation tasks while addressing data scarcity and regulatory constraints.","authors":["Jiawei Mao","Yuhan Wang","Yucheng Tang","Daguang Xu","Kang Wang","Yang Yang","Zongwei Zhou","Yuyin Zhou"],"url":"https://arxiv.org/abs/2504.06897"}
{"created":"2025-04-11","title":"UKBOB: One Billion MRI Labeled Masks for Generalizable 3D Medical Image Segmentation","abstract":"In medical imaging, the primary challenge is collecting large-scale labeled data due to privacy concerns, logistics, and high labeling costs. In this work, we present the UK Biobank Organs and Bones (UKBOB), the largest labeled dataset of body organs, comprising 51,761 MRI 3D samples (equivalent to 17.9 million 2D images) and more than 1.37 billion 2D segmentation masks of 72 organs, all based on the UK Biobank MRI dataset. We utilize automatic labeling, introduce an automated label cleaning pipeline with organ-specific filters, and manually annotate a subset of 300 MRIs with 11 abdominal classes to validate the quality (referred to as UKBOB-manual). This approach allows for scaling up the dataset collection while maintaining confidence in the labels. We further confirm the validity of the labels by demonstrating zero-shot generalization of trained models on the filtered UKBOB to other small labeled datasets from similar domains (e.g., abdominal MRI). To further mitigate the effect of noisy labels, we propose a novel method called Entropy Test-time Adaptation (ETTA) to refine the segmentation output. We use UKBOB to train a foundation model, Swin-BOB, for 3D medical image segmentation based on the Swin-UNetr architecture, achieving state-of-the-art results in several benchmarks in 3D medical imaging, including the BRATS brain MRI tumor challenge (with a 0.4% improvement) and the BTCV abdominal CT scan benchmark (with a 1.3% improvement). The pre-trained models and the code are available at https://emmanuelleb985.github.io/ukbob , and the filtered labels will be made available with the UK Biobank.","authors":["Emmanuelle Bourigault","Amir Jamaludin","Abdullah Hamdi"],"url":"https://arxiv.org/abs/2504.06908"}
{"created":"2025-04-11","title":"Identifying Aspects in Peer Reviews","abstract":"Peer review is central to academic publishing, but the growing volume of submissions is straining the process. This motivates the development of computational approaches to support peer review. While each review is tailored to a specific paper, reviewers often make assessments according to certain aspects such as Novelty, which reflect the values of the research community. This alignment creates opportunities for standardizing the reviewing process, improving quality control, and enabling computational support. While prior work has demonstrated the potential of aspect analysis for peer review assistance, the notion of aspect remains poorly formalized. Existing approaches often derive aspect sets from review forms and guidelines of major NLP venues, yet data-driven methods for aspect identification are largely underexplored. To address this gap, our work takes a bottom-up approach: we propose an operational definition of aspect and develop a data-driven schema for deriving fine-grained aspects from a corpus of peer reviews. We introduce a dataset of peer reviews augmented with aspects and show how it can be used for community-level review analysis. We further show how the choice of aspects can impact downstream applications, such as LLM-generated review detection. Our results lay a foundation for a principled and data-driven investigation of review aspects, and pave the path for new applications of NLP to support peer review.","authors":["Sheng Lu","Ilia Kuznetsov","Iryna Gurevych"],"url":"https://arxiv.org/abs/2504.06910"}
{"created":"2025-04-11","title":"Controlling a Social Network of Individuals with Coevolving Actions and Opinions","abstract":"In this paper, we consider a population of individuals who have actions and opinions, which coevolve, mutually influencing one another on a complex network structure. In particular, we formulate a control problem for this social network, in which we assume that we can inject into the network a committed minority -- a set of stubborn nodes -- with the objective of steering the population, initially at a consensus, to a different consensus state. Our study focuses on two main objectives: i) determining the conditions under which the committed minority succeeds in its goal, and ii) identifying the optimal placement for such a committed minority. After deriving general monotone convergence result for the controlled dynamics, we leverage these results to build a computationally-efficient algorithm to solve the first problem and an effective heuristics for the second problem, which we prove to be NP-complete. The proposed methodology is illustrated though academic examples, and demonstrated on a real-world case study.","authors":["Roberta Raineri","Giacomo Como","Fabio Fagnani","Mengbin Ye","Lorenzo Zino"],"url":"https://arxiv.org/abs/2504.06913"}
{"created":"2025-04-11","title":"An Analysis of Temporal Dropout in Earth Observation Time Series for Regression Tasks","abstract":"Missing instances in time series data impose a significant challenge to deep learning models, particularly in regression tasks. In the Earth Observation field, satellite failure or cloud occlusion frequently results in missing time-steps, introducing uncertainties in the predicted output and causing a decline in predictive performance. While many studies address missing time-steps through data augmentation to improve model robustness, the uncertainty arising at the input level is commonly overlooked. To address this gap, we introduce Monte Carlo Temporal Dropout (MC-TD), a method that explicitly accounts for input-level uncertainty by randomly dropping time-steps during inference using a predefined dropout ratio, thereby simulating the effect of missing data. To bypass the need for costly searches for the optimal dropout ratio, we extend this approach with Monte Carlo Concrete Temporal Dropout (MC-ConcTD), a method that learns the optimal dropout distribution directly. Both MC-TD and MC-ConcTD are applied during inference, leveraging Monte Carlo sampling for uncertainty quantification. Experiments on three EO time-series datasets demonstrate that MC-ConcTD improves predictive performance and uncertainty calibration compared to existing approaches. Additionally, we highlight the advantages of adaptive dropout tuning over manual selection, making uncertainty quantification more robust and accessible for EO applications.","authors":["Miro Miranda","Francisco Mena","Andreas Dengel"],"url":"https://arxiv.org/abs/2504.06915"}
{"created":"2025-04-11","title":"Data Augmentation for Fake Reviews Detection in Multiple Languages and Multiple Domains","abstract":"With the growth of the Internet, buying habits have changed, and customers have become more dependent on the online opinions of other customers to guide their purchases. Identifying fake reviews thus became an important area for Natural Language Processing (NLP) research. However, developing high-performance NLP models depends on the availability of large amounts of training data, which are often not available for low-resource languages or domains. In this research, we used large language models to generate datasets to train fake review detectors. Our approach was used to generate fake reviews in different domains (book reviews, restaurant reviews, and hotel reviews) and different languages (English and Chinese). Our results demonstrate that our data augmentation techniques result in improved performance at fake review detection for all domains and languages. The accuracy of our fake review detection model can be improved by 0.3 percentage points on DeRev TEST, 10.9 percentage points on Amazon TEST, 8.3 percentage points on Yelp TEST and 7.2 percentage points on DianPing TEST using the augmented datasets.","authors":["Ming Liu","Massimo Poesio"],"url":"https://arxiv.org/abs/2504.06917"}
{"created":"2025-04-11","title":"S-EO: A Large-Scale Dataset for Geometry-Aware Shadow Detection in Remote Sensing Applications","abstract":"We introduce the S-EO dataset: a large-scale, high-resolution dataset, designed to advance geometry-aware shadow detection. Collected from diverse public-domain sources, including challenge datasets and government providers such as USGS, our dataset comprises 702 georeferenced tiles across the USA, each covering 500x500 m. Each tile includes multi-date, multi-angle WorldView-3 pansharpened RGB images, panchromatic images, and a ground-truth DSM of the area obtained from LiDAR scans. For each image, we provide a shadow mask derived from geometry and sun position, a vegetation mask based on the NDVI index, and a bundle-adjusted RPC model. With approximately 20,000 images, the S-EO dataset establishes a new public resource for shadow detection in remote sensing imagery and its applications to 3D reconstruction. To demonstrate the dataset's impact, we train and evaluate a shadow detector, showcasing its ability to generalize, even to aerial images. Finally, we extend EO-NeRF - a state-of-the-art NeRF approach for satellite imagery - to leverage our shadow predictions for improved 3D reconstructions.","authors":["Masquil El\\'ias","Mar\\'i Roger","Ehret Thibaud","Meinhardt-Llopis Enric","Mus\\'e Pablo","Facciolo Gabriele"],"url":"https://arxiv.org/abs/2504.06920"}
{"created":"2025-04-11","title":"The Importance of Being Discrete: Measuring the Impact of Discretization in End-to-End Differentially Private Synthetic Data","abstract":"Differentially Private (DP) generative marginal models are often used in the wild to release synthetic tabular datasets in lieu of sensitive data while providing formal privacy guarantees. These models approximate low-dimensional marginals or query workloads; crucially, they require the training data to be pre-discretized, i.e., continuous values need to first be partitioned into bins. However, as the range of values (or their domain) is often inferred directly from the training data, with the number of bins and bin edges typically defined arbitrarily, this approach can ultimately break end-to-end DP guarantees and may not always yield optimal utility.","authors":["Georgi Ganev","Meenatchi Sundaram Muthu Selva Annamalai","Sofiane Mahiou","Emiliano De Cristofaro"],"url":"https://arxiv.org/abs/2504.06923"}
{"created":"2025-04-11","title":"Are Vision-Language Models Ready for Dietary Assessment? Exploring the Next Frontier in AI-Powered Food Image Recognition","abstract":"Automatic dietary assessment based on food images remains a challenge, requiring precise food detection, segmentation, and classification. Vision-Language Models (VLMs) offer new possibilities by integrating visual and textual reasoning. In this study, we evaluate six state-of-the-art VLMs (ChatGPT, Gemini, Claude, Moondream, DeepSeek, and LLaVA), analyzing their capabilities in food recognition at different levels. For the experimental framework, we introduce the FoodNExTDB, a unique food image database that contains 9,263 expert-labeled images across 10 categories (e.g., \"protein source\"), 62 subcategories (e.g., \"poultry\"), and 9 cooking styles (e.g., \"grilled\"). In total, FoodNExTDB includes 50k nutritional labels generated by seven experts who manually annotated all images in the database. Also, we propose a novel evaluation metric, Expert-Weighted Recall (EWR), that accounts for the inter-annotator variability. Results show that closed-source models outperform open-source ones, achieving over 90% EWR in recognizing food products in images containing a single product. Despite their potential, current VLMs face challenges in fine-grained food recognition, particularly in distinguishing subtle differences in cooking styles and visually similar food items, which limits their reliability for automatic dietary assessment. The FoodNExTDB database is publicly available at https://github.com/AI4Food/FoodNExtDB.","authors":["Sergio Romero-Tapiador","Ruben Tolosana","Blanca Lacruz-Pleguezuelos","Laura Judith Marcos Zambrano","Guadalupe X. Baz\\'an","Isabel Espinosa-Salinas","Julian Fierrez","Javier Ortega-Garcia","Enrique Carrillo de Santa Pau","Aythami Morales"],"url":"https://arxiv.org/abs/2504.06925"}
{"created":"2025-04-11","title":"RO-FIGS: Efficient and Expressive Tree-Based Ensembles for Tabular Data","abstract":"Tree-based models are often robust to uninformative features and can accurately capture non-smooth, complex decision boundaries. Consequently, they often outperform neural network-based models on tabular datasets at a significantly lower computational cost. Nevertheless, the capability of traditional tree-based ensembles to express complex relationships efficiently is limited by using a single feature to make splits. To improve the efficiency and expressiveness of tree-based methods, we propose Random Oblique Fast Interpretable Greedy-Tree Sums (RO-FIGS). RO-FIGS builds on Fast Interpretable Greedy-Tree Sums, and extends it by learning trees with oblique or multivariate splits, where each split consists of a linear combination learnt from random subsets of features. This helps uncover interactions between features and improves performance. The proposed method is suitable for tabular datasets with both numerical and categorical features. We evaluate RO-FIGS on 22 real-world tabular datasets, demonstrating superior performance and much smaller models over other tree- and neural network-based methods. Additionally, we analyse their splits to reveal valuable insights into feature interactions, enriching the information learnt from SHAP summary plots, and thereby demonstrating the enhanced interpretability of RO-FIGS models. The proposed method is well-suited for applications, where balance between accuracy and interpretability is essential.","authors":["Ur\\v{s}ka Matja\\v{s}ec","Nikola Simidjievski","Mateja Jamnik"],"url":"https://arxiv.org/abs/2504.06927"}
{"created":"2025-04-11","title":"Beyond Tools: Generative AI as Epistemic Infrastructure in Education","abstract":"As generative AI rapidly integrates into educational infrastructures worldwide, it transforms how knowledge gets created, validated, and shared, yet current discourse inadequately addresses its implications as epistemic infrastructure mediating teaching and learning. This paper investigates how AI systems function as epistemic infrastructures in education and their impact on human epistemic agency. Adopting a situated cognition perspective and following a value-sensitive design approach, the study conducts a technical investigation of two representative AI systems in educational settings, analyzing their impact on teacher practice across three dimensions: affordances for skilled epistemic actions, support for epistemic sensitivity, and implications for long-term habit formation. The analysis reveals that current AI systems inadequately support teachers' skilled epistemic actions, insufficiently foster epistemic sensitivity, and potentially cultivate problematic habits that prioritize efficiency over epistemic agency. To address these challenges, the paper recommends recognizing the infrastructural transformation occurring in education, developing AI environments that stimulate skilled actions while upholding epistemic norms, and involving educators in AI design processes -- recommendations aimed at fostering AI integration that aligns with core educational values and maintains human epistemic agency.","authors":["Bodong Chen"],"url":"https://arxiv.org/abs/2504.06928"}
{"created":"2025-04-11","title":"ASRL:A robust loss function with potential for development","abstract":"In this article, we proposed a partition:wise robust loss function based on the previous robust loss function. The characteristics of this loss function are that it achieves high robustness and a wide range of applicability through partition-wise design and adaptive parameter adjustment. Finally, the advantages and development potential of this loss function were verified by applying this loss function to the regression question and using five different datasets (with different dimensions, different sample numbers, and different fields) to compare with the other loss functions. The results of multiple experiments have proven the advantages of our loss function .","authors":["Chenyu Hui","Anran Zhang","Xintong Li"],"url":"https://arxiv.org/abs/2504.06935"}
{"created":"2025-04-11","title":"Finite Field Multiple Access III: from 2-ary to p-ary","abstract":"This paper extends finite-field multiple-access (FFMA) techniques from binary to general $p$-ary source transmission. We introduce element-assemblage (EA) codes over GF($p^m$), generalizing element-pair (EP) codes, and define two specific types for ternary transmission: orthogonal EA codes and double codeword EA (D-CWEA) codes. A unique sum-pattern mapping (USPM) constraint is proposed for the design of uniquely-decodable CWEA (UD-CWEA) codes, including additive inverse D-CWEA (AI-D-CWEA) and basis decomposition D-CWEA (BD-D-CWEA) codes. Moreover, we extend EP-coding to EA-coding, focusing on non-orthogonal CWEA (NO-CWEA) codes and their USPM constraint in the complex field. Additionally, $p$-ary CWEA codes are constructed using a basis decomposition method, leveraging ternary decomposition for faster convergence and simplified encoder/decoder design. We present a comprehensive performance analysis of the proposed FFMA system from two complementary perspectives: channel capacity and error performance. We demonstrate that equal power allocation (EPA) achieves the theoretical channel capacity bound, while independently developing a rate-driven capacity alignment (CA) theorem based on the capacity-to-rate ratio (CRR) metric for error performance analysis. We then explore the multiuser finite blocklength (FBL) characteristics of FFMA systems. Finally, a comparative analysis of $p$-ary transmission systems against classical binary systems is conducted, revealing that low-order $p$-ary systems (e.g., $p=3$) outperform binary systems at small loading factors, while higher-order systems (e.g., $p=257$) excel at larger loading factors. These findings highlight the potential of $p$-ary systems, although practical implementations may benefit from decomposing $p$-ary systems into ternary systems to manage complexity.","authors":["Qi-yue Yu"],"url":"https://arxiv.org/abs/2504.06937"}
{"created":"2025-04-11","title":"On the Compressibility of Integral Operators in Anisotropic Wavelet Coordinates","abstract":"The present article is concerned with the s*-compressibility of classical boundary integral operators in anisotropic wavelet coordinates. Having the s*-compressibility at hand, one can design adaptive wavelet algorithms which are asymptotically optimal, meaning that any target accuracy can be achieved at a computational expense that stays proportional to the number of degrees of freedom (within the setting determined by an underlying wavelet basis) that would ideally be necessary for realising that target accuracy if full knowledge about the unknown solution were given. As we consider here anisotropic wavelet coordinates, we can achieve higher convergence rates compared to the standard, isotropic setting. Especially, edge singularities of anisotropic nature can be resolved.","authors":["Helmut Harbrecht","Remo von Rickenbach"],"url":"https://arxiv.org/abs/2504.06938"}
{"created":"2025-04-11","title":"FeedbackEval: A Benchmark for Evaluating Large Language Models in Feedback-Driven Code Repair Tasks","abstract":"Code repair is a fundamental task in software development, facilitating efficient bug resolution and software maintenance. Although large language models (LLMs) have demonstrated considerable potential in automated code repair, their ability to comprehend and effectively leverage diverse types of feedback remains insufficiently understood. To bridge this gap, we introduce FeedbackEval, a systematic benchmark for evaluating LLMs' feedback comprehension and performance in code repair tasks. We conduct a comprehensive empirical study on five state-of-the-art LLMs, including GPT-4o, Claude-3.5, Gemini-1.5, GLM-4, and Qwen2.5, to evaluate their behavior under both single-iteration and iterative code repair settings. Our results show that structured feedback, particularly in the form of test feedback, leads to the highest repair success rates, while unstructured feedback proves significantly less effective. Iterative feedback further enhances repair performance, though the marginal benefit diminishes after two or three rounds. Moreover, prompt structure is shown to be critical: incorporating docstrings, contextual information, and explicit guidelines substantially improves outcomes, whereas persona-based, chain-of-thought, and few-shot prompting strategies offer limited benefits in single-iteration scenarios. This work introduces a robust benchmark and delivers practical insights to advance the understanding and development of feedback-driven code repair using LLMs.","authors":["Dekun Dai","MingWei Liu","Anji Li","Jialun Cao","Yanlin Wang","Chong Wang","Xin Peng","Zibin Zheng"],"url":"https://arxiv.org/abs/2504.06939"}
{"created":"2025-04-11","title":"Review of Case-Based Reasoning for LLM Agents: Theoretical Foundations, Architectural Components, and Cognitive Integration","abstract":"Agents powered by Large Language Models (LLMs) have recently demonstrated impressive capabilities in various tasks. Still, they face limitations in tasks requiring specific, structured knowledge, flexibility, or accountable decision-making. While agents are capable of perceiving their environments, forming inferences, planning, and executing actions towards goals, they often face issues such as hallucinations and lack of contextual memory across interactions. This paper explores how Case-Based Reasoning (CBR), a strategy that solves new problems by referencing past experiences, can be integrated into LLM agent frameworks. This integration allows LLMs to leverage explicit knowledge, enhancing their effectiveness. We systematically review the theoretical foundations of these enhanced agents, identify critical framework components, and formulate a mathematical model for the CBR processes of case retrieval, adaptation, and learning. We also evaluate CBR-enhanced agents against other methods like Chain-of-Thought reasoning and standard Retrieval-Augmented Generation, analyzing their relative strengths. Moreover, we explore how leveraging CBR's cognitive dimensions (including self-reflection, introspection, and curiosity) via goal-driven autonomy mechanisms can further enhance the LLM agent capabilities. Contributing to the ongoing research on neuro-symbolic hybrid systems, this work posits CBR as a viable technique for enhancing the reasoning skills and cognitive aspects of autonomous LLM agents.","authors":["Kostas Hatalis","Despina Christou","Vyshnavi Kondapalli"],"url":"https://arxiv.org/abs/2504.06943"}
{"created":"2025-04-11","title":"RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts","abstract":"In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts.","authors":["Natalia Loukachevitch","Natalia Tkachenko","Anna Lapanitsyna","Mikhail Tikhomirov","Nicolay Rusnachenko"],"url":"https://arxiv.org/abs/2504.06947"}
{"created":"2025-04-11","title":"Adaptive Computation Pruning for the Forgetting Transformer","abstract":"The recently proposed Forgetting Transformer (FoX) incorporates a forget gate into softmax attention and has shown consistently better or on-par performance compared to the standard RoPE-based Transformer. Notably, many attention heads in FoX tend to forget quickly, causing their output at each timestep to rely primarily on the local context. Based on this observation, we propose Adaptive Computation Pruning (ACP) for FoX, a method that dynamically prunes computations involving input-output dependencies that are strongly decayed by the forget gate. This is achieved using a dynamically set pruning threshold that ensures that the pruned attention weights remain negligible. We apply ACP to language model pretraining with FoX and show it consistently reduces the number of FLOPs in softmax attention by around 70% across different model sizes and context lengths, resulting in a roughly 10% to 35% improvement in training throughput. Furthermore, longer context lengths yield greater computational savings. All these speed improvements are achieved without any performance degradation. We also perform several analyses to provide deeper insights into our method, such as examining the pruning patterns and analyzing the distribution of FLOP savings across different attention heads. Our code is available at https://github.com/zhixuan-lin/arctic-fox.","authors":["Zhixuan Lin","Johan Obando-Ceron","Xu Owen He","Aaron Courville"],"url":"https://arxiv.org/abs/2504.06949"}
{"created":"2025-04-11","title":"PathSegDiff: Pathology Segmentation using Diffusion model representations","abstract":"Image segmentation is crucial in many computational pathology pipelines, including accurate disease diagnosis, subtyping, outcome, and survivability prediction. The common approach for training a segmentation model relies on a pre-trained feature extractor and a dataset of paired image and mask annotations. These are used to train a lightweight prediction model that translates features into per-pixel classes. The choice of the feature extractor is central to the performance of the final segmentation model, and recent literature has focused on finding tasks to pre-train the feature extractor. In this paper, we propose PathSegDiff, a novel approach for histopathology image segmentation that leverages Latent Diffusion Models (LDMs) as pre-trained featured extractors. Our method utilizes a pathology-specific LDM, guided by a self-supervised encoder, to extract rich semantic information from H\\&amp;E stained histopathology images. We employ a simple, fully convolutional network to process the features extracted from the LDM and generate segmentation masks. Our experiments demonstrate significant improvements over traditional methods on the BCSS and GlaS datasets, highlighting the effectiveness of domain-specific diffusion pre-training in capturing intricate tissue structures and enhancing segmentation accuracy in histopathology images.","authors":["Sachin Kumar Danisetty","Alexandros Graikos","Srikar Yellapragada","Dimitris Samaras"],"url":"https://arxiv.org/abs/2504.06950"}
{"created":"2025-04-11","title":"A Comparison of Deep Learning Methods for Cell Detection in Digital Cytology","abstract":"Accurate and efficient cell detection is crucial in many biomedical image analysis tasks. We evaluate the performance of several Deep Learning (DL) methods for cell detection in Papanicolaou-stained cytological Whole Slide Images (WSIs), focusing on accuracy of predictions and computational efficiency. We examine recentoff-the-shelf algorithms as well as custom-designed detectors, applying them to two datasets: the CNSeg Dataset and the Oral Cancer (OC) Dataset. Our comparison includes well-established segmentation methods such as StarDist, Cellpose, and the Segment Anything Model 2 (SAM2), alongside centroid-based Fully Convolutional Regression Network (FCRN) approaches. We introduce a suitable evaluation metric to assess the accuracy of predictions based on the distance from ground truth positions. We also explore the impact of dataset size and data augmentation techniques on model performance. Results show that centroid-based methods, particularly the Improved Fully Convolutional Regression Network (IFCRN) method, outperform segmentation-based methods in terms of both detection accuracy and computational efficiency. This study highlights the potential of centroid-based detectors as a preferred option for cell detection in resource-limited environments, offering faster processing times and lower GPU memory usage without compromising accuracy.","authors":["Marco Acerbis","Nata\\v{s}a Sladoje","Joakim Lindblad"],"url":"https://arxiv.org/abs/2504.06957"}
{"created":"2025-04-11","title":"VideoChat-R1: Enhancing Spatio-Temporal Perception via Reinforcement Fine-Tuning","abstract":"Recent advancements in reinforcement learning have significantly advanced the reasoning capabilities of multimodal large language models (MLLMs). While approaches such as Group Relative Policy Optimization (GRPO) and rule-based reward mechanisms demonstrate promise in text and image domains, their application to video understanding remains limited. This paper presents a systematic exploration of Reinforcement Fine-Tuning (RFT) with GRPO for video MLLMs, aiming to enhance spatio-temporal perception while maintaining general capabilities. Our experiments reveal that RFT is highly data-efficient for task-specific improvements. Through multi-task RFT on spatio-temporal perception objectives with limited samples, we develop VideoChat-R1, a powerful video MLLM that achieves state-of-the-art performance on spatio-temporal perception tasks without sacrificing chat ability, while exhibiting emerging spatio-temporal reasoning abilities. Compared to Qwen2.5-VL-7B, VideoChat-R1 boosts performance several-fold in tasks like temporal grounding (+31.8) and object tracking (+31.2). Additionally, it significantly improves on general QA benchmarks such as VideoMME (+0.9), MVBench (+1.0), and Perception Test (+0.9). Our findings underscore the potential of RFT for specialized task enhancement of Video MLLMs. We hope our work offers valuable insights for future RL research in video MLLMs.","authors":["Xinhao Li","Ziang Yan","Desen Meng","Lu Dong","Xiangyu Zeng","Yinan He","Yali Wang","Yu Qiao","Yi Wang","Limin Wang"],"url":"https://arxiv.org/abs/2504.06958"}
{"created":"2025-04-11","title":"Higher-Order Color Voronoi Diagrams and the Colorful Clarkson-Shor Framework","abstract":"Given a set $S$ of $n$ colored sites, each $s\\in S$ associated with a distance-to-site function $\\delta_s \\colon \\mathbb{R}^2 \\to \\mathbb{R}$, we consider two distance-to-color functions for each color: one takes the minimum of $\\delta_s$ for sites $s\\in S$ in that color and the other takes the maximum. These two sets of distance functions induce two families of higher-order Voronoi diagrams for colors in the plane, namely, the minimal and maximal order-$k$ color Voronoi diagrams, which include various well-studied Voronoi diagrams as special cases. In this paper, we derive an exact upper bound $4k(n-k)-2n$ on the total number of vertices in both the minimal and maximal order-$k$ color diagrams for a wide class of distance functions $\\delta_s$ that satisfy certain conditions, including the case of point sites $S$ under convex distance functions and the $L_p$ metric for any $1\\leq p \\leq\\infty$. For the $L_1$ (or, $L_\\infty$) metric, and other convex polygonal metrics, we show that the order-$k$ minimal diagram of point sites has $O(\\min\\{k(n-k), (n-k)^2\\})$ complexity, while its maximal counterpart has $O(\\min\\{k(n-k), k^2\\})$ complexity. To obtain these combinatorial results, we extend the Clarkson--Shor framework to colored objects, and demonstrate its application to several fundamental geometric structures, including higher-order color Voronoi diagrams, colored $j$-facets, and levels in the arrangements of piecewise linear/algebraic curves/surfaces. We also present an iterative approach to compute higher-order color Voronoi diagrams.","authors":["Sang Won Bae","Nicolau Oliver","Evanthia Papadopoulou"],"url":"https://arxiv.org/abs/2504.06960"}
{"created":"2025-04-11","title":"Two by Two: Learning Multi-Task Pairwise Objects Assembly for Generalizable Robot Manipulation","abstract":"3D assembly tasks, such as furniture assembly and component fitting, play a crucial role in daily life and represent essential capabilities for future home robots. Existing benchmarks and datasets predominantly focus on assembling geometric fragments or factory parts, which fall short in addressing the complexities of everyday object interactions and assemblies. To bridge this gap, we present 2BY2, a large-scale annotated dataset for daily pairwise objects assembly, covering 18 fine-grained tasks that reflect real-life scenarios, such as plugging into sockets, arranging flowers in vases, and inserting bread into toasters. 2BY2 dataset includes 1,034 instances and 517 pairwise objects with pose and symmetry annotations, requiring approaches that align geometric shapes while accounting for functional and spatial relationships between objects. Leveraging the 2BY2 dataset, we propose a two-step SE(3) pose estimation method with equivariant features for assembly constraints. Compared to previous shape assembly methods, our approach achieves state-of-the-art performance across all 18 tasks in the 2BY2 dataset. Additionally, robot experiments further validate the reliability and generalization ability of our method for complex 3D assembly tasks.","authors":["Yu Qi","Yuanchen Ju","Tianming Wei","Chi Chu","Lawson L. S. Wong","Huazhe Xu"],"url":"https://arxiv.org/abs/2504.06961"}
{"created":"2025-04-11","title":"Efficient Self-Supervised Learning for Earth Observation via Dynamic Dataset Curation","abstract":"Self-supervised learning (SSL) has enabled the development of vision foundation models for Earth Observation (EO), demonstrating strong transferability across diverse remote sensing tasks. While prior work has focused on network architectures and training strategies, the role of dataset curation, especially in balancing and diversifying pre-training datasets, remains underexplored. In EO, this challenge is amplified by the redundancy and heavy-tailed distributions common in satellite imagery, which can lead to biased representations and inefficient training.","authors":["Thomas Kerdreux","Alexandre Tuel","Quentin Febvre","Alexis Mouche","Bertrand Chapron"],"url":"https://arxiv.org/abs/2504.06962"}
{"created":"2025-04-11","title":"A Deep Single Image Rectification Approach for Pan-Tilt-Zoom Cameras","abstract":"Pan-Tilt-Zoom (PTZ) cameras with wide-angle lenses are widely used in surveillance but often require image rectification due to their inherent nonlinear distortions. Current deep learning approaches typically struggle to maintain fine-grained geometric details, resulting in inaccurate rectification. This paper presents a Forward Distortion and Backward Warping Network (FDBW-Net), a novel framework for wide-angle image rectification. It begins by using a forward distortion model to synthesize barrel-distorted images, reducing pixel redundancy and preventing blur. The network employs a pyramid context encoder with attention mechanisms to generate backward warping flows containing geometric details. Then, a multi-scale decoder is used to restore distorted features and output rectified images. FDBW-Net's performance is validated on diverse datasets: public benchmarks, AirSim-rendered PTZ camera imagery, and real-scene PTZ camera datasets. It demonstrates that FDBW-Net achieves SOTA performance in distortion rectification, boosting the adaptability of PTZ cameras for practical visual applications.","authors":["Teng Xiao","Qi Hu","Qingsong Yan","Wei Liu","Zhiwei Ye","Fei Deng"],"url":"https://arxiv.org/abs/2504.06965"}
{"created":"2025-04-11","title":"Towards LLMs Robustness to Changes in Prompt Format Styles","abstract":"Large language models (LLMs) have gained popularity in recent years for their utility in various applications. However, they are sensitive to non-semantic changes in prompt formats, where small changes in the prompt format can lead to significant performance fluctuations. In the literature, this problem is commonly referred to as prompt brittleness. Previous research on prompt engineering has focused mainly on developing techniques for identifying the optimal prompt for specific tasks. Some studies have also explored the issue of prompt brittleness and proposed methods to quantify performance variations; however, no simple solution has been found to address this challenge. We propose Mixture of Formats (MOF), a simple and efficient technique for addressing prompt brittleness in LLMs by diversifying the styles used in the prompt few-shot examples. MOF was inspired by computer vision techniques that utilize diverse style datasets to prevent models from associating specific styles with the target variable. Empirical results show that our proposed technique reduces style-induced prompt brittleness in various LLMs while also enhancing overall performance across prompt variations and different datasets.","authors":["Lilian Ngweta","Kiran Kate","Jason Tsay","Yara Rizk"],"url":"https://arxiv.org/abs/2504.06969"}
{"created":"2025-04-11","title":"AWDIT: An Optimal Weak Database Isolation Tester","abstract":"In order to achieve low latency, high throughput, and partition tolerance, modern databases forgo strong transaction isolation for weak isolation guarantees. However, several production databases have been found to suffer from isolation bugs, breaking their data-consistency contract. Black-box testing is a prominent technique for detecting isolation bugs, by checking whether histories of database transactions adhere to a prescribed isolation level.","authors":["Lasse M{\\o}ldrup","Andreas Pavlogiannis"],"url":"https://arxiv.org/abs/2504.06975"}
{"created":"2025-04-11","title":"A Year of the DSA Transparency Database: What it (Does Not) Reveal About Platform Moderation During the 2024 European Parliament Election","abstract":"Social media platforms face heightened risks during major political events; yet, how platforms adapt their moderation practices in response remains unclear. The Digital Services Act Transparency Database offers an unprecedented opportunity to systematically study content moderation at scale, enabling researchers and policymakers to assess platforms' compliance and effectiveness. Herein, we analyze 1.58 billion self-reported moderation actions taken by eight large social media platforms during an extended period of eight months surrounding the 2024 European Parliament elections. Our findings reveal a lack of adaptation in moderation strategies, as platforms did not exhibit significant changes in their enforcement behaviors surrounding the elections. This raises concerns about whether platforms adapted their moderation practices at all, or if structural limitations of the database concealed possible adjustments. Moreover, we found that noted transparency and accountability issues persist nearly a year after initial concerns were raised. These results highlight the limitations of current self-regulatory approaches and underscore the need for stronger enforcement and data access mechanisms to ensure that online platforms uphold their responsibility in safeguarding democratic processes.","authors":["Gautam Kishore Shahi","Benedetta Tessa","Amaury Trujillo","Stefano Cresci"],"url":"https://arxiv.org/abs/2504.06976"}
{"created":"2025-04-11","title":"Wheat3DGS: In-field 3D Reconstruction, Instance Segmentation and Phenotyping of Wheat Heads with Gaussian Splatting","abstract":"Automated extraction of plant morphological traits is crucial for supporting crop breeding and agricultural management through high-throughput field phenotyping (HTFP). Solutions based on multi-view RGB images are attractive due to their scalability and affordability, enabling volumetric measurements that 2D approaches cannot directly capture. While advanced methods like Neural Radiance Fields (NeRFs) have shown promise, their application has been limited to counting or extracting traits from only a few plants or organs. Furthermore, accurately measuring complex structures like individual wheat heads-essential for studying crop yields-remains particularly challenging due to occlusions and the dense arrangement of crop canopies in field conditions. The recent development of 3D Gaussian Splatting (3DGS) offers a promising alternative for HTFP due to its high-quality reconstructions and explicit point-based representation. In this paper, we present Wheat3DGS, a novel approach that leverages 3DGS and the Segment Anything Model (SAM) for precise 3D instance segmentation and morphological measurement of hundreds of wheat heads automatically, representing the first application of 3DGS to HTFP. We validate the accuracy of wheat head extraction against high-resolution laser scan data, obtaining per-instance mean absolute percentage errors of 15.1%, 18.3%, and 40.2% for length, width, and volume. We provide additional comparisons to NeRF-based approaches and traditional Muti-View Stereo (MVS), demonstrating superior results. Our approach enables rapid, non-destructive measurements of key yield-related traits at scale, with significant implications for accelerating crop breeding and improving our understanding of wheat development.","authors":["Daiwei Zhang","Joaquin Gajardo","Tomislav Medic","Isinsu Katircioglu","Mike Boss","Norbert Kirchgessner","Achim Walter","Lukas Roth"],"url":"https://arxiv.org/abs/2504.06978"}
{"created":"2025-04-11","title":"Coreset Strikes Back: Improved Parameterized Approximation Schemes for (Constrained) k-Median/Means","abstract":"Algorithmic scatter dimension is a notion of metric spaces introduced recently by Abbasi et al. (FOCS 2023), which unifies many well-known metric spaces, including continuous Euclidean space, bounded doubling space, planar and bounded treewidth metrics. Recently, Bourneuf and Pilipczuk (SODA 2025) showed that metrics induced by graphs from any fixed proper minor closed graph class have bounded scatter dimension. Abbasi et al. presented a unified approach to obtain EPASes (i.e., $(1+\\epsilon)$-approximations running in time FPT in $k$ and $\\epsilon$) for $k$-Clustering in metrics of bounded scatter dimension. However, a seemingly inherent limitation of their approach was that it could only handle clustering objectives where each point was assigned to the closest chosen center. They explicitly asked, if there exist EPASes for constrained $k$-Clustering in metrics of bounded scatter dimension.","authors":["Sujoy Bhore","Ameet Gadekar","Tanmay Inamdar"],"url":"https://arxiv.org/abs/2504.06980"}
{"created":"2025-04-11","title":"LCL Resonance Analysis and Damping in Single-Loop Grid-Forming Wind Turbines","abstract":"A dynamic phenomenon known as LCL resonance is often neglected when stability analysis is carried out for grid-forming (GFM) control schemes by wind turbine systems, due to its high frequency. This paper shows that this simplification is not always valid for single-loop (SL) control schemes. A detailed small-signal analysis reveals that reactive power (RAP) control significantly influences the resonant modes, which may be dominant in determining overall system stability, even if the resonant frequency is high. The underlying mechanism via which the LCL resonance may dominate the overall system stability is systematically analyzed. Furthermore, various RAP control strategies are compared to assess their different effects on resonant modes. An active damping (AD) strategy favorable for SL-GFM control is then designed. We also provide a comparison between SL-GFM and well-studied grid-following control schemes, highlighting quite different resonance features between them. Finally, case studies associated with a 14-bus, 5-machine IEEE test system are presented. These show that instability originates from the LCL resonance rather than low-frequency interactions among multiple machines, validating the theoretical analysis and the proposed AD strategy.","authors":["Meng Chen","Yufei Xi","Frede Blaabjerg","Lin Cheng","Ioannis Lestas"],"url":"https://arxiv.org/abs/2504.06981"}
{"created":"2025-04-11","title":"SIGMAN:Scaling 3D Human Gaussian Generation with Millions of Assets","abstract":"3D human digitization has long been a highly pursued yet challenging task. Existing methods aim to generate high-quality 3D digital humans from single or multiple views, but remain primarily constrained by current paradigms and the scarcity of 3D human assets. Specifically, recent approaches fall into several paradigms: optimization-based and feed-forward (both single-view regression and multi-view generation with reconstruction). However, they are limited by slow speed, low quality, cascade reasoning, and ambiguity in mapping low-dimensional planes to high-dimensional space due to occlusion and invisibility, respectively. Furthermore, existing 3D human assets remain small-scale, insufficient for large-scale training. To address these challenges, we propose a latent space generation paradigm for 3D human digitization, which involves compressing multi-view images into Gaussians via a UV-structured VAE, along with DiT-based conditional generation, we transform the ill-posed low-to-high-dimensional mapping problem into a learnable distribution shift, which also supports end-to-end inference. In addition, we employ the multi-view optimization approach combined with synthetic data to construct the HGS-1M dataset, which contains $1$ million 3D Gaussian assets to support the large-scale training. Experimental results demonstrate that our paradigm, powered by large-scale training, produces high-quality 3D human Gaussians with intricate textures, facial details, and loose clothing deformation.","authors":["Yuhang Yang","Fengqi Liu","Yixing Lu","Qin Zhao","Pingyu Wu","Wei Zhai","Ran Yi","Yang Cao","Lizhuang Ma","Zheng-Jun Zha","Junting Dong"],"url":"https://arxiv.org/abs/2504.06982"}
{"created":"2025-04-11","title":"Free Random Projection for In-Context Reinforcement Learning","abstract":"Hierarchical inductive biases are hypothesized to promote generalizable policies in reinforcement learning, as demonstrated by explicit hyperbolic latent representations and architectures. Therefore, a more flexible approach is to have these biases emerge naturally from the algorithm. We introduce Free Random Projection, an input mapping grounded in free probability theory that constructs random orthogonal matrices where hierarchical structure arises inherently. The free random projection integrates seamlessly into existing in-context reinforcement learning frameworks by encoding hierarchical organization within the input space without requiring explicit architectural modifications. Empirical results on multi-environment benchmarks show that free random projection consistently outperforms the standard random projection, leading to improvements in generalization. Furthermore, analyses within linearly solvable Markov decision processes and investigations of the spectrum of kernel random matrices reveal the theoretical underpinnings of free random projection's enhanced performance, highlighting its capacity for effective adaptation in hierarchically structured state spaces.","authors":["Tomohiro Hayase","Beno\\^it Collins","Nakamasa Inoue"],"url":"https://arxiv.org/abs/2504.06983"}
{"created":"2025-04-11","title":"Solving \"pseudo-injective\" polynomial equations over finite dynamical systems","abstract":"We consider the semiring of abstract finite dynamical systems up to isomorphism, with the operations of alternative and synchronous execution. We continue searching for efficient algorithms for solving polynomial equations of the form $P(X) = B$, with a constant side B, with the goal of decomposing complex behaviors into simpler systems. Taking inspiration from the characterization of injective polynomials P over dynamical systems, which is based on a condition on the lengths of limit cycles of their coefficients, we introduce a more general notion of pseudo-injectivity by relaxing this constraint. We prove that the associated equations can be solved efficiently, even in certain cases where the input is encoded in an exponentially more compact way.","authors":["Antonio E. Porreca","Marius Rolland"],"url":"https://arxiv.org/abs/2504.06986"}
{"created":"2025-04-11","title":"Enhancing Metabolic Syndrome Prediction with Hybrid Data Balancing and Counterfactuals","abstract":"Metabolic Syndrome (MetS) is a cluster of interrelated risk factors that significantly increases the risk of cardiovascular diseases and type 2 diabetes. Despite its global prevalence, accurate prediction of MetS remains challenging due to issues such as class imbalance, data scarcity, and methodological inconsistencies in existing studies. In this paper, we address these challenges by systematically evaluating and optimizing machine learning (ML) models for MetS prediction, leveraging advanced data balancing techniques and counterfactual analysis. Multiple ML models, including XGBoost, Random Forest, TabNet, etc., were trained and compared under various data balancing techniques such as random oversampling (ROS), SMOTE, ADASYN, and CTGAN. Additionally, we introduce MetaBoost, a novel hybrid framework that integrates SMOTE, ADASYN, and CTGAN, optimizing synthetic data generation through weighted averaging and iterative weight tuning to enhance the model's performance (achieving a 1.14% accuracy improvement over individual balancing techniques). A comprehensive counterfactual analysis is conducted to quantify feature-level changes required to shift individuals from high-risk to low-risk categories. The results indicate that blood glucose (50.3%) and triglycerides (46.7%) were the most frequently modified features, highlighting their clinical significance in MetS risk reduction. Additionally, probabilistic analysis shows elevated blood glucose (85.5% likelihood) and triglycerides (74.9% posterior probability) as the strongest predictors. This study not only advances the methodological rigor of MetS prediction but also provides actionable insights for clinicians and researchers, highlighting the potential of ML in mitigating the public health burden of metabolic syndrome.","authors":["Sanyam Paresh Shah","Abdullah Mamun","Shovito Barua Soumma","Hassan Ghasemzadeh"],"url":"https://arxiv.org/abs/2504.06987"}
{"created":"2025-04-11","title":"Dissimilar Batch Decompositions of Random Datasets","abstract":"For better learning, large datasets are often split into small batches and fed sequentially to the predictive model. In this paper, we study such batch decompositions from a probabilistic perspective. We assume that data points (possibly corrupted) are drawn independently from a given space and define a concept of similarity between two data points. We then consider decompositions that restrict the amount of similarity within each batch and obtain high probability bounds for the minimum size. We demonstrate an inherent tradeoff between relaxing the similarity constraint and the overall size and also use martingale methods to obtain bounds for the maximum size of data subsets with a given similarity.","authors":["Ghurumuruhan Ganesan"],"url":"https://arxiv.org/abs/2504.06991"}
{"created":"2025-04-11","title":"RayFronts: Open-Set Semantic Ray Frontiers for Online Scene Understanding and Exploration","abstract":"Open-set semantic mapping is crucial for open-world robots. Current mapping approaches either are limited by the depth range or only map beyond-range entities in constrained settings, where overall they fail to combine within-range and beyond-range observations. Furthermore, these methods make a trade-off between fine-grained semantics and efficiency. We introduce RayFronts, a unified representation that enables both dense and beyond-range efficient semantic mapping. RayFronts encodes task-agnostic open-set semantics to both in-range voxels and beyond-range rays encoded at map boundaries, empowering the robot to reduce search volumes significantly and make informed decisions both within & beyond sensory range, while running at 8.84 Hz on an Orin AGX. Benchmarking the within-range semantics shows that RayFronts's fine-grained image encoding provides 1.34x zero-shot 3D semantic segmentation performance while improving throughput by 16.5x. Traditionally, online mapping performance is entangled with other system components, complicating evaluation. We propose a planner-agnostic evaluation framework that captures the utility for online beyond-range search and exploration, and show RayFronts reduces search volume 2.2x more efficiently than the closest online baselines.","authors":["Omar Alama","Avigyan Bhattacharya","Haoyang He","Seungchan Kim","Yuheng Qiu","Wenshan Wang","Cherie Ho","Nikhil Keetha","Sebastian Scherer"],"url":"https://arxiv.org/abs/2504.06994"}
{"created":"2025-04-11","title":"Neural Signal Compression using RAMAN tinyML Accelerator for BCI Applications","abstract":"High-quality, multi-channel neural recording is indispensable for neuroscience research and clinical applications. Large-scale brain recordings often produce vast amounts of data that must be wirelessly transmitted for subsequent offline analysis and decoding, especially in brain-computer interfaces (BCIs) utilizing high-density intracortical recordings with hundreds or thousands of electrodes. However, transmitting raw neural data presents significant challenges due to limited communication bandwidth and resultant excessive heating. To address this challenge, we propose a neural signal compression scheme utilizing Convolutional Autoencoders (CAEs), which achieves a compression ratio of up to 150 for compressing local field potentials (LFPs). The CAE encoder section is implemented on RAMAN, an energy-efficient tinyML accelerator designed for edge computing, and subsequently deployed on an Efinix Ti60 FPGA with 37.3k LUTs and 8.6k register utilization. RAMAN leverages sparsity in activation and weights through zero skipping, gating, and weight compression techniques. Additionally, we employ hardware-software co-optimization by pruning CAE encoder model parameters using a hardware-aware balanced stochastic pruning strategy, resolving workload imbalance issues and eliminating indexing overhead to reduce parameter storage requirements by up to 32.4%. Using the proposed compact depthwise separable convolutional autoencoder (DS-CAE) model, the compressed neural data from RAMAN is reconstructed offline with superior signal-to-noise and distortion ratios (SNDR) of 22.6 dB and 27.4 dB, along with R2 scores of 0.81 and 0.94, respectively, evaluated on two monkey neural recordings.","authors":["Adithya Krishna","Sohan Debnath","Andr\\'e van Schaik","Mahesh Mehendale","Chetan Singh Thakur"],"url":"https://arxiv.org/abs/2504.06996"}
{"created":"2025-04-11","title":"A Krylov projection algorithm for large symmetric matrices with dense spectra","abstract":"We consider the approximation of $B^T (A+sI)^{-1} B$ for large s.p.d. $A\\in\\mathbb{R}^{n\\times n}$ with dense spectrum and $B\\in\\mathbb{R}^{n\\times p}$, $p\\ll n$. We target the computations of Multiple-Input Multiple-Output (MIMO) transfer functions for large-scale discretizations of problems with continuous spectral measures, such as linear time-invariant (LTI) PDEs on unbounded domains. Traditional Krylov methods, such as the Lanczos or CG algorithm, are known to be optimal for the computation of $(A+sI)^{-1}B$ with real positive $s$, resulting in an adaptation to the distinctively discrete and nonuniform spectra. However, the adaptation is damped for matrices with dense spectra. It was demonstrated in [Zimmerling, Druskin, Simoncini, Journal of Scientific Computing 103(1), 5 (2025)] that averaging Gau{\\ss} and Gau\\ss -Radau quadratures computed using the block-Lanczos method significantly reduces approximation errors for such problems. Here, we introduce an adaptive Kre\\u{i}n-Nudelman extension to the (block) Lanczos recursions, allowing further acceleration at negligible $o(n)$ cost. Similar to the Gau\\ss -Radau quadrature, a low-rank modification is applied to the (block) Lanczos matrix. However, unlike the Gau\\ss -Radau quadrature, this modification depends on $\\sqrt{s}$ and can be considered in the framework of the Hermite-Pad\\'e approximants, which are known to be efficient for problems with branch-cuts, that can be good approximations to dense spectral intervals. Numerical results for large-scale discretizations of heat-diffusion and quasi-magnetostatic Maxwell's operators in unbounded domains confirm the efficiency of the proposed approach.","authors":["Vladimir Druskin J\\\"orn Zimmerling"],"url":"https://arxiv.org/abs/2504.06998"}
{"created":"2025-04-11","title":"Leveraging GCN-based Action Recognition for Teleoperation in Daily Activity Assistance","abstract":"Caregiving of older adults is an urgent global challenge, with many older adults preferring to age in place rather than enter residential care. However, providing adequate home-based assistance remains difficult, particularly in geographically vast regions. Teleoperated robots offer a promising solution, but conventional motion-mapping teleoperation imposes unnatural movement constraints on operators, leading to muscle fatigue and reduced usability. This paper presents a novel teleoperation framework that leverages action recognition to enable intuitive remote robot control. Using our simplified Spatio-Temporal Graph Convolutional Network (S-ST-GCN), the system recognizes human actions and executes corresponding preset robot trajectories, eliminating the need for direct motion synchronization. A finite-state machine (FSM) is integrated to enhance reliability by filtering out misclassified actions. Our experiments demonstrate that the proposed framework enables effortless operator movement while ensuring accurate robot execution. This proof-of-concept study highlights the potential of teleoperation with action recognition for enabling caregivers to remotely assist older adults during activities of daily living (ADLs). Future work will focus on improving the S-ST-GCN's recognition accuracy and generalization, integrating advanced motion planning techniques to further enhance robotic autonomy in older adult care, and conducting a user study to evaluate the system's telepresence and ease of control.","authors":["Thomas M. Kwok","Jiaan Li","Yue Hu"],"url":"https://arxiv.org/abs/2504.07001"}
{"created":"2025-04-11","title":"DeCoMa: Detecting and Purifying Code Dataset Watermarks through Dual Channel Code Abstraction","abstract":"Watermarking is a technique to help identify the source of data points, which can be used to help prevent the misuse of protected datasets. Existing methods on code watermarking, leveraging the idea from the backdoor research, embed stealthy triggers as watermarks.Despite their high resilience against dilution attacks and backdoor detections, the robustness has not been fully evaluated. To fill this gap, we propose DeCoMa, a dual-channel approach to Detect and purify Code dataset waterMarks.To overcome the high barrier created by the stealthy and hidden nature of code watermarks, DeCoMa leverages dual-channel constraints on code to generalize and map code samples into standardized templates. Subsequently, DeCoMa extracts hidden watermarks by identifying outlier associations between paired elements within the standardized templates. Finally, DeCoMa purifies the watermarked dataset by removing all samples containing the detected watermark, enabling the silent appropriation of protected code. We conduct extensive experiments to evaluate the effectiveness and efficiency of DeCoMa, covering 14 types of code watermarks and 3 representative intelligent code tasks (a total of 14 scenarios). Experimental results demonstrate that DeCoMa achieves a stable recall of 100% in 14 code watermark detection scenarios, significantly outperforming the baselines. Additionally, DeCoMa effectively attacks code watermarks with embedding rates as low as 0.1%, while maintaining comparable model performance after training on the purified dataset. Furthermore, as DeCoMa requires no model training for detection, it achieves substantially higher efficiency than all baselines, with a speedup ranging from 31.5 to 130.9X. The results call for more advanced watermarking techniques for code models, while DeCoMa can serve as a baseline for future evaluation.","authors":["Yuan Xiao","Yuchen Chen","Shiqing Ma","Haocheng Huang","Chunrong Fang","Yanwei Chen","Weisong Sun","Yunfeng Zhu","Xiaofang Zhang","Zhenyu Chen"],"url":"https://arxiv.org/abs/2504.07002"}
{"created":"2025-04-11","title":"Task-Based Tensor Computations on Modern GPUs","abstract":"Domain-specific, fixed-function units are becoming increasingly common in modern processors. As the computational demands of applications evolve, the capabilities and programming interfaces of these fixed-function units continue to change. NVIDIA's Hopper GPU architecture contains multiple fixed-function units per compute unit, including an asynchronous data movement unit (TMA) and an asynchronous matrix multiplication unit (Tensor Core). Efficiently utilizing these units requires a fundamentally different programming style than previous architectures; programmers must now develop warp-specialized kernels that orchestrate producer-consumer pipelines between the asynchronous units. To manage the complexity of programming these new architectures, we introduce Cypress, a task-based programming model with sequential semantics. Cypress programs are a set of designated functions called \\emph{tasks} that operate on \\emph{tensors} and are free of communication and synchronization. Cypress programs are bound to the target machine through a \\emph{mapping} specification that describes where tasks should run and in which memories tensors should be materialized. We present a compiler architecture that lowers Cypress programs into CUDA programs that perform competitively with expert-written codes. Cypress achieves 0.88x-1.06x the performance of cuBLAS on GEMM, and between 0.80x-0.98x the performance of the currently best-known Flash Attention implementation while eliminating all aspects of explicit data movement and asynchronous computation from application code.","authors":["Rohan Yadav","Michael Garland","Alex Aiken","Michael Bauer"],"url":"https://arxiv.org/abs/2504.07004"}
{"created":"2025-04-11","title":"Latent Diffusion U-Net Representations Contain Positional Embeddings and Anomalies","abstract":"Diffusion models have demonstrated remarkable capabilities in synthesizing realistic images, spurring interest in using their representations for various downstream tasks. To better understand the robustness of these representations, we analyze popular Stable Diffusion models using representational similarity and norms. Our findings reveal three phenomena: (1) the presence of a learned positional embedding in intermediate representations, (2) high-similarity corner artifacts, and (3) anomalous high-norm artifacts. These findings underscore the need to further investigate the properties of diffusion model representations before considering them for downstream tasks that require robust features. Project page: https://jonasloos.github.io/sd-representation-anomalies","authors":["Jonas Loos","Lorenz Linhardt"],"url":"https://arxiv.org/abs/2504.07008"}
{"created":"2025-04-11","title":"FAME: Introducing Fuzzy Additive Models for Explainable AI","abstract":"In this study, we introduce the Fuzzy Additive Model (FAM) and FAM with Explainability (FAME) as a solution for Explainable Artificial Intelligence (XAI). The family consists of three layers: (1) a Projection Layer that compresses the input space, (2) a Fuzzy Layer built upon Single Input-Single Output Fuzzy Logic Systems (SFLS), where SFLS functions as subnetworks within an additive index model, and (3) an Aggregation Layer. This architecture integrates the interpretability of SFLS, which uses human-understandable if-then rules, with the explainability of input-output relationships, leveraging the additive model structure. Furthermore, using SFLS inherently addresses issues such as the curse of dimensionality and rule explosion. To further improve interpretability, we propose a method for sculpting antecedent space within FAM, transforming it into FAME. We show that FAME captures the input-output relationships with fewer active rules, thus improving clarity. To learn the FAM family, we present a deep learning framework. Through the presented comparative results, we demonstrate the promising potential of FAME in reducing model complexity while retaining interpretability, positioning it as a valuable tool for XAI.","authors":["Omer Bahadir Gokmen","Yusuf Guven","Tufan Kumbasar"],"url":"https://arxiv.org/abs/2504.07011"}
{"created":"2025-04-11","title":"Thin Coalgebraic Behaviours Are Inductive","abstract":"Coalgebras for analytic functors uniformly model graph-like systems where the successors of a state may admit certain symmetries. Examples of successor structure include ordered tuples, cyclic lists and multisets. Motivated by goals in automata-based verification and results on thin trees, we introduce thin coalgebras as those coalgebras with only countably many infinite paths from each state. Our main result is an inductive characterisation of thinness via an initial algebra. To this end, we develop a syntax for thin behaviours and capture with a single equation when two terms represent the same thin behaviour. Finally, for the special case of polynomial functors, we retrieve from our syntax the notion of Cantor-Bendixson rank of a thin tree.","authors":["Anton Chernev","Corina C\\^irstea","Helle Hvid Hansen","Clemens Kupke"],"url":"https://arxiv.org/abs/2504.07013"}
{"created":"2025-04-11","title":"LLM-IFT: LLM-Powered Information Flow Tracking for Secure Hardware","abstract":"As modern hardware designs grow in complexity and size, ensuring security across the confidentiality, integrity, and availability (CIA) triad becomes increasingly challenging. Information flow tracking (IFT) is a widely-used approach to tracing data propagation, identifying unauthorized activities that may compromise confidentiality or/and integrity in hardware. However, traditional IFT methods struggle with scalability and adaptability, particularly in high-density and interconnected architectures, leading to tracing bottlenecks that limit applicability in large-scale hardware. To address these limitations and show the potential of transformer-based models in integrated circuit (IC) design, this paper introduces LLM-IFT that integrates large language models (LLM) for the realization of the IFT process in hardware. LLM-IFT exploits LLM-driven structured reasoning to perform hierarchical dependency analysis, systematically breaking down even the most complex designs. Through a multi-step LLM invocation, the framework analyzes both intra-module and inter-module dependencies, enabling comprehensive IFT assessment. By focusing on a set of Trust-Hub vulnerability test cases at both the IP level and the SoC level, our experiments demonstrate a 100\\% success rate in accurate IFT analysis for confidentiality and integrity checks in hardware.","authors":["Nowfel Mashnoor","Mohammad Akyash","Hadi Kamali","Kimia Azar"],"url":"https://arxiv.org/abs/2504.07015"}
{"created":"2025-04-11","title":"Adapting GT2-FLS for Uncertainty Quantification: A Blueprint Calibration Strategy","abstract":"Uncertainty Quantification (UQ) is crucial for deploying reliable Deep Learning (DL) models in high-stakes applications. Recently, General Type-2 Fuzzy Logic Systems (GT2-FLSs) have been proven to be effective for UQ, offering Prediction Intervals (PIs) to capture uncertainty. However, existing methods often struggle with computational efficiency and adaptability, as generating PIs for new coverage levels $(\\phi_d)$ typically requires retraining the model. Moreover, methods that directly estimate the entire conditional distribution for UQ are computationally expensive, limiting their scalability in real-world scenarios. This study addresses these challenges by proposing a blueprint calibration strategy for GT2-FLSs, enabling efficient adaptation to any desired $\\phi_d$ without retraining. By exploring the relationship between $\\alpha$-plane type reduced sets and uncertainty coverage, we develop two calibration methods: a lookup table-based approach and a derivative-free optimization algorithm. These methods allow GT2-FLSs to produce accurate and reliable PIs while significantly reducing computational overhead. Experimental results on high-dimensional datasets demonstrate that the calibrated GT2-FLS achieves superior performance in UQ, highlighting its potential for scalable and practical applications.","authors":["Yusuf Guven","Tufan Kumbasar"],"url":"https://arxiv.org/abs/2504.07017"}
{"created":"2025-04-11","title":"ShadowBinding: Realizing Effective Microarchitectures for In-Core Secure Speculation Schemes","abstract":"Secure speculation schemes have shown great promise in the war against speculative side-channel attacks, and will be a key building block for developing secure, high-performance architectures moving forward. As the field matures, the need for rigorous microarchitectures, and corresponding performance and cost analysis, become critical for evaluating secure schemes and for enabling their future adoption.","authors":["Amund Bergland Kvalsvik","Magnus Sj\\\"alander"],"url":"https://arxiv.org/abs/2504.07018"}
{"created":"2025-04-11","title":"Evaluating Retrieval Augmented Generative Models for Document Queries in Transportation Safety","abstract":"Applications of generative Large Language Models LLMs are rapidly expanding across various domains, promising significant improvements in workflow efficiency and information retrieval. However, their implementation in specialized, high-stakes domains such as hazardous materials transportation is challenging due to accuracy and reliability concerns. This study evaluates the performance of three fine-tuned generative models, ChatGPT, Google's Vertex AI, and ORNL Retrieval Augmented Generation augmented LLaMA 2 and LLaMA in retrieving regulatory information essential for hazardous material transportation compliance in the United States. Utilizing approximately 40 publicly available federal and state regulatory documents, we developed 100 realistic queries relevant to route planning and permitting requirements. Responses were qualitatively rated based on accuracy, detail, and relevance, complemented by quantitative assessments of semantic similarity between model outputs. Results demonstrated that the RAG-augmented LLaMA models significantly outperformed Vertex AI and ChatGPT, providing more detailed and generally accurate information, despite occasional inconsistencies. This research introduces the first known application of RAG in transportation safety, emphasizing the need for domain-specific fine-tuning and rigorous evaluation methodologies to ensure reliability and minimize the risk of inaccuracies in high-stakes environments.","authors":["Chad Melton","Alex Sorokine","Steve Peterson"],"url":"https://arxiv.org/abs/2504.07022"}
{"created":"2025-04-11","title":"Data Augmentation and Hyperparameter Tuning for Low-Resource MFA","abstract":"A continued issue for those working with computational tools and endangered and under-resourced languages is the lower accuracy of results for languages with smaller amounts of data. We attempt to ameliorate this issue by using data augmentation methods to increase corpus size, comparing augmentation to hyperparameter tuning for multilingual forced alignment. Unlike text augmentation methods, audio augmentation does not lead to substantially increased performance. Hyperparameter tuning, on the other hand, results in substantial improvement without (for this amount of data) infeasible additional training time. For languages with small to medium amounts of training data, this is a workable alternative to adapting models from high-resource languages.","authors":["Alessio Tosolini","Claire Bowern"],"url":"https://arxiv.org/abs/2504.07024"}
{"created":"2025-04-11","title":"Glossy Object Reconstruction with Cost-effective Polarized Acquisition","abstract":"The challenge of image-based 3D reconstruction for glossy objects lies in separating diffuse and specular components on glossy surfaces from captured images, a task complicated by the ambiguity in discerning lighting conditions and material properties using RGB data alone. While state-of-the-art methods rely on tailored and/or high-end equipment for data acquisition, which can be cumbersome and time-consuming, this work introduces a scalable polarization-aided approach that employs cost-effective acquisition tools. By attaching a linear polarizer to readily available RGB cameras, multi-view polarization images can be captured without the need for advance calibration or precise measurements of the polarizer angle, substantially reducing system construction costs. The proposed approach represents polarimetric BRDF, Stokes vectors, and polarization states of object surfaces as neural implicit fields. These fields, combined with the polarizer angle, are retrieved by optimizing the rendering loss of input polarized images. By leveraging fundamental physical principles for the implicit representation of polarization rendering, our method demonstrates superiority over existing techniques through experiments in public datasets and real captured images on both reconstruction and novel view synthesis.","authors":["Bojian Wu","Yifan Peng","Ruizhen Hu","Xiaowei Zhou"],"url":"https://arxiv.org/abs/2504.07025"}
{"created":"2025-04-11","title":"Using ML filters to help automated vulnerability repairs: when it helps and when it doesn't","abstract":"[Context:] The acceptance of candidate patches in automated program repair has been typically based on testing oracles. Testing requires typically a costly process of building the application while ML models can be used to quickly classify patches, thus allowing more candidate patches to be generated in a positive feedback loop. [Problem:] If the model predictions are unreliable (as in vulnerability detection) they can hardly replace the more reliable oracles based on testing. [New Idea:] We propose to use an ML model as a preliminary filter of candidate patches which is put in front of a traditional filter based on testing. [Preliminary Results:] We identify some theoretical bounds on the precision and recall of the ML algorithm that makes such operation meaningful in practice. With these bounds and the results published in the literature, we calculate how fast some of state-of-the art vulnerability detectors must be to be more effective over a traditional AVR pipeline such as APR4Vuln based just on testing.","authors":["Maria Camporese","Fabio Massacci"],"url":"https://arxiv.org/abs/2504.07027"}
{"created":"2025-04-11","title":"UAV Position Estimation using a LiDAR-based 3D Object Detection Method","abstract":"This paper explores the use of applying a deep learning approach for 3D object detection to compute the relative position of an Unmanned Aerial Vehicle (UAV) from an Unmanned Ground Vehicle (UGV) equipped with a LiDAR sensor in a GPS-denied environment. This was achieved by evaluating the LiDAR sensor's data through a 3D detection algorithm (PointPillars). The PointPillars algorithm incorporates a column voxel point-cloud representation and a 2D Convolutional Neural Network (CNN) to generate distinctive point-cloud features representing the object to be identified, in this case, the UAV. The current localization method utilizes point-cloud segmentation, Euclidean clustering, and predefined heuristics to obtain the relative position of the UAV. Results from the two methods were then compared to a reference truth solution.","authors":["Uthman Olawoye","Jason N. Gross"],"url":"https://arxiv.org/abs/2504.07028"}
{"created":"2025-04-11","title":"Distilling Textual Priors from LLM to Efficient Image Fusion","abstract":"Multi-modality image fusion aims to synthesize a single, comprehensive image from multiple source inputs. Traditional approaches, such as CNNs and GANs, offer efficiency but struggle to handle low-quality or complex inputs. Recent advances in text-guided methods leverage large model priors to overcome these limitations, but at the cost of significant computational overhead, both in memory and inference time. To address this challenge, we propose a novel framework for distilling large model priors, eliminating the need for text guidance during inference while dramatically reducing model size. Our framework utilizes a teacher-student architecture, where the teacher network incorporates large model priors and transfers this knowledge to a smaller student network via a tailored distillation process. Additionally, we introduce spatial-channel cross-fusion module to enhance the model's ability to leverage textual priors across both spatial and channel dimensions. Our method achieves a favorable trade-off between computational efficiency and fusion quality. The distilled network, requiring only 10\\% of the parameters and inference time of the teacher network, retains 90\\% of its performance and outperforms existing SOTA methods. Extensive experiments demonstrate the effectiveness of our approach. The implementation will be made publicly available as an open-source resource.","authors":["Ran Zhang","Xuanhua He","Ke Cao","Liu Liu","Li Zhang","Man Zhou","Jie Zhang"],"url":"https://arxiv.org/abs/2504.07029"}
{"created":"2025-04-11","title":"Identifying Key Challenges of Hardness-Based Resampling","abstract":"Performance gap across classes remains a persistent challenge in machine learning, often attributed to variations in class hardness. One way to quantify class hardness is through sample complexity - the minimum number of samples required to effectively learn a given class. Sample complexity theory suggests that class hardness is driven by differences in the amount of data required for generalization. That is, harder classes need substantially more samples to achieve generalization. Therefore, hardness-based resampling is a promising approach to mitigate these performance disparities. While resampling has been studied extensively in data-imbalanced settings, its impact on balanced datasets remains unexplored.","authors":["Pawel Pukowski","Venet Osmani"],"url":"https://arxiv.org/abs/2504.07031"}
{"created":"2025-04-11","title":"A Uniform Framework for Handling Position Constraints in String Solving (Technical Report)","abstract":"We introduce a novel decision procedure for solving the class of position string constraints, which includes string disequalities, not-prefixof, not-suffixof, str$.$at, and not-str$.$at. These constraints are generated frequently in almost any application of string constraint solving. Our procedure avoids expensive encoding of the constraints to word equations and, instead, reduces the problem to checking conflicts on positions satisfying an integer constraint obtained from the Parikh image of a polynomial-sized finite automaton with a special structure. By the reduction to counting, solving position constraints becomes NP-complete and for some cases even falls into PTime. This is much cheaper than the previously used techniques, which either used reductions generating word equations and length constraints (for which modern string solvers use exponential-space algorithms) or incomplete techniques. Our method is relevant especially for automata-based string solvers, which have recently achieved the best results in terms of practical efficiency, generality, and completeness guarantees. This work allows them to excel also on position constraints, which used to be their weakness. Besides the efficiency gains, we show that our framework may be extended to solve a large fragment of not-contains (in NExpTime), for which decidability has been long open, and gives a hope to solve the general problem. Our implementation of the technique within the Z3-Noodler solver significantly improves its performance on position constraints.","authors":["Yu-Fang Chen","Vojt\\v{e}ch Havlena","Michal He\\v{c}ko","Luk\\'a\\v{s} Hol\\'ik","Ond\\v{r}ej Leng\\'al"],"url":"https://arxiv.org/abs/2504.07033"}
{"created":"2025-04-11","title":"Efficient Storage Integrity in Adversarial Settings","abstract":"Storage integrity is essential to systems and applications that use untrusted storage (e.g., public clouds, end-user devices). However, known methods for achieving storage integrity either suffer from high (and often prohibitive) overheads or provide weak integrity guarantees. In this work, we demonstrate a hybrid approach to storage integrity that simultaneously reduces overhead while providing strong integrity guarantees. Our system, partially asynchronous integrity checking (PAC), allows disk write commitments to be deferred while still providing guarantees around read integrity. PAC delivers a 5.5X throughput and latency improvement over the state of the art, and 85% of the throughput achieved by non-integrity-assuring approaches. In this way, we show that untrusted storage can be used for integrity-critical workloads without meaningfully sacrificing performance.","authors":["Quinn Burke","Ryan Sheatsley","Yohan Beugin","Eric Pauley","Owen Hines","Michael Swift","Patrick McDaniel"],"url":"https://arxiv.org/abs/2504.07041"}
{"created":"2025-04-11","title":"Towards a Higher Roofline for Matrix-Vector Multiplication in Matrix-Free HOSFEM","abstract":"The high-order/spectral finite element method (HOSFEM) is a widely used numerical method for solving PDEs, with its performance primarily relying on axhelm, a matrix-free kernel for element-local matrix-vector multiplications. In axhelm, geometric factors account for over half of memory access but minimally contribute to computational workload. This imbalance significantly constrains the performance roofline, indicating that further optimization of tensor contraction, the core computation in axhelm, yields only minimal improvements. To overcome this bottleneck, we propose a low-cost on-the-fly recalculation of geometric factors for trilinear elements, thereby unlocking substantial potential for optimizing tensor contraction. The proposed approach is implemented in Nekbone, a standard HOSFEM benchmark. With optimizations such as merging scalar factors, partial recalculation, Tensor Core acceleration, and constant memory utilization, performance reaches 85%-100% of the higher roofline. The optimized kernels achieve speedups of 1.74x-4.10x on NVIDIA A100 and 1.99x-3.77x on DCU K100. This leads to a 1.12x-1.40x speedup for Nekbone.","authors":["Zijian Cao","Qiao Sun","Tiangong Zhang","Huiyuan Li"],"url":"https://arxiv.org/abs/2504.07042"}
{"created":"2025-04-11","title":"BIA Transmission in Rate Splitting-based Optical Wireless Networks","abstract":"Optical wireless communication (OWC) has recently received massive interest as a new technology that can support the enormous data traffic increasing on daily basis. In particular, laser-based OWC networks can provide terabits per second (Tbps) aggregate data rates. However, the emerging OWC networks require a high number of optical access points (APs), each AP corresponding to an optical cell, to provide uniform coverage for multiple users. Therefore, inter-cell interference (ICI) and multi-user interference (MUI) are crucial issues that must be managed efficiently to provide high spectral efficiency. In radio frequency (RF) networks, rate splitting (RS) is proposed as a transmission scheme to serve multiple users simultaneously following a certain strategy. It was shown that RS provides high data rates compared to orthogonal and non-orthogonal interference management schemes. Considering the high density of OWC networks, the application of RS within each optical cell might not be practical due to severe ICI. In this paper, a new strategy is derived referred to as blind interference alignment-rate splitting (BIA-RS) to fully coordinate the transmission among the optical APs, while determining the precoding matrices of multiple groups of users formed beforehand. Therefore, RS can be implemented within each group to manage MUI. The proposed BIA-RS scheme requires two layers of power allocation to achieve high performance. Given that, a max-min fractional optimization problem is formulated to optimally distribute the power budget among the groups and the messages intended to the users of each group. Finally, a power allocation algorithm is designed with multiple Lagrangian multipliers to provide practical and sub-optimal solutions. The results show the high performance of the proposed scheme compared to other counterpart schemes.","authors":["Ahmad Adnan Qidan","Khulood Alazwary","Taisir El-Gorashi","Majid Safari","Harald Haas","Richard V. Penty","Ian H. White","Jaafar M. H. Elmirghani"],"url":"https://arxiv.org/abs/2504.07043"}
{"created":"2025-04-11","title":"Buffer Centering for bittide Synchronization via Frame Rotation","abstract":"Maintaining consistent time in distributed systems is a fundamental challenge. The bittide system addresses this by providing logical synchronization through a decentralized control mechanism that observes local buffer occupancies and controls the frequency of an oscillator at each node. A critical aspect of bittide's stability and performance is ensuring that these elastic buffers operate around a desired equilibrium point, preventing data loss due to overflow or underflow. This paper introduces a novel method for centering buffer occupancies in a bittide network using a technique we term frame rotation. We propose a control strategy utilizing a directed spanning tree of the network graph. By adjusting the frequencies of nodes in a specific order dictated by this tree, and employing a pulsed feedback controller that targets the buffer occupancy of edges within the spanning tree, we prove that all elastic buffers in the network can be driven to their desired equilibrium. This ordered adjustment approach ensures that prior centering efforts are not disrupted, providing a robust mechanism for managing buffer occupancy in bittide synchronized systems.","authors":["Sanjay Lall","Tammo Spalink"],"url":"https://arxiv.org/abs/2504.07044"}
{"created":"2025-04-11","title":"A Unified Agentic Framework for Evaluating Conditional Image Generation","abstract":"Conditional image generation has gained significant attention for its ability to personalize content. However, the field faces challenges in developing task-agnostic, reliable, and explainable evaluation metrics. This paper introduces CIGEval, a unified agentic framework for comprehensive evaluation of conditional image generation tasks. CIGEval utilizes large multimodal models (LMMs) as its core, integrating a multi-functional toolbox and establishing a fine-grained evaluation framework. Additionally, we synthesize evaluation trajectories for fine-tuning, empowering smaller LMMs to autonomously select appropriate tools and conduct nuanced analyses based on tool outputs. Experiments across seven prominent conditional image generation tasks demonstrate that CIGEval (GPT-4o version) achieves a high correlation of 0.4625 with human assessments, closely matching the inter-annotator correlation of 0.47. Moreover, when implemented with 7B open-source LMMs using only 2.3K training trajectories, CIGEval surpasses the previous GPT-4o-based state-of-the-art method. Case studies on GPT-4o image generation highlight CIGEval's capability in identifying subtle issues related to subject consistency and adherence to control guidance, indicating its great potential for automating evaluation of image generation tasks with human-level reliability.","authors":["Jifang Wang","Xue Yang","Longyue Wang","Zhenran Xu","Yiyu Wang","Yaowei Wang","Weihua Luo","Kaifu Zhang","Baotian Hu","Min Zhang"],"url":"https://arxiv.org/abs/2504.07046"}
{"created":"2025-04-11","title":"Context Switching for Secure Multi-programming of Near-Term Quantum Computers","abstract":"Multi-programming quantum computers improve device utilization and throughput. However, crosstalk from concurrent two-qubit CNOT gates poses security risks, compromising the fidelity and output of co-running victim programs. We design Zero Knowledge Tampering Attacks (ZKTAs), using which attackers can exploit crosstalk without knowledge of the hardware error profile. ZKTAs can alter victim program outputs in 40% of cases on commercial systems.","authors":["Avinash Kumar","Meng Wang","Chenxu Liu","Ang Li","Prashant J. Nair","Poulami Das"],"url":"https://arxiv.org/abs/2504.07048"}
{"created":"2025-04-11","title":"To Backtrack or Not to Backtrack: When Sequential Search Limits Model Reasoning","abstract":"Recent advancements in large language models have significantly improved their reasoning abilities, particularly through techniques involving search and backtracking. Backtracking naturally scales test-time compute by enabling sequential, linearized exploration via long chain-of-thought (CoT) generation. However, this is not the only strategy for scaling test-time compute: parallel sampling with best-of-n selection provides an alternative that generates diverse solutions simultaneously. Despite the growing adoption of sequential search, its advantages over parallel sampling--especially under a fixed compute budget remain poorly understood. In this paper, we systematically compare these two approaches on two challenging reasoning tasks: CountDown and Sudoku. Surprisingly, we find that sequential search underperforms parallel sampling on CountDown but outperforms it on Sudoku, suggesting that backtracking is not universally beneficial. We identify two factors that can cause backtracking to degrade performance: (1) training on fixed search traces can lock models into suboptimal strategies, and (2) explicit CoT supervision can discourage \"implicit\" (non-verbalized) reasoning. Extending our analysis to reinforcement learning (RL), we show that models with backtracking capabilities benefit significantly from RL fine-tuning, while models without backtracking see limited, mixed gains. Together, these findings challenge the assumption that backtracking universally enhances LLM reasoning, instead revealing a complex interaction between task structure, training data, model scale, and learning paradigm.","authors":["Tian Qin","David Alvarez-Melis","Samy Jelassi","Eran Malach"],"url":"https://arxiv.org/abs/2504.07052"}
{"created":"2025-04-11","title":"TASTE: Text-Aligned Speech Tokenization and Embedding for Spoken Language Modeling","abstract":"Large Language Models (LLMs) excel in text-based natural language processing tasks but remain constrained by their reliance on textual inputs and outputs. To enable more natural human-LLM interaction, recent progress have focused on deriving a spoken language model (SLM) that can not only listen but also generate speech. To achieve this, a promising direction is to conduct speech-text joint modeling. However, recent SLM still lag behind text LLM due to the modality mismatch. One significant mismatch can be the sequence lengths between speech and text tokens. To address this, we introduce Text-Aligned Speech Tokenization and Embedding (TASTE), a method that directly addresses the modality gap by aligning speech token with the corresponding text transcription during the tokenization stage. We propose a method that can achieve this through the special aggregation mechanism and with speech reconstruction as the training objective. We conduct extensive experiments and show that TASTE can preserve essential paralinguistic information while dramatically reducing the token sequence length. Furthermore, by leveraging TASTE, we can adapt text-based LLMs into effective SLMs with parameter-efficient fine-tuning techniques such as Low-Rank Adaptation (LoRA). Experimental results on benchmark tasks, including SALMON and StoryCloze, demonstrate that TASTE-based SLMs perform similarly to previous full-finetuning methods. To our knowledge, TASTE is the first end-to-end approach that utilizes a reconstruction objective to automatically learn a text-aligned speech tokenization and embedding suitable for spoken language modeling. Our demo, code, and models are publicly available at https://github.com/mtkresearch/TASTE-SpokenLM.","authors":["Liang-Hsuan Tseng","Yi-Chang Chen","Kuan-Yi Lee","Da-Shan Shiu","Hung-yi Lee"],"url":"https://arxiv.org/abs/2504.07053"}
{"created":"2025-04-11","title":"$\\Pi$-NeSy: A Possibilistic Neuro-Symbolic Approach","abstract":"In this article, we introduce a neuro-symbolic approach that combines a low-level perception task performed by a neural network with a high-level reasoning task performed by a possibilistic rule-based system. The goal is to be able to derive for each input instance the degree of possibility that it belongs to a target (meta-)concept. This (meta-)concept is connected to intermediate concepts by a possibilistic rule-based system. The probability of each intermediate concept for the input instance is inferred using a neural network. The connection between the low-level perception task and the high-level reasoning task lies in the transformation of neural network outputs modeled by probability distributions (through softmax activation) into possibility distributions. The use of intermediate concepts is valuable for the explanation purpose: using the rule-based system, the classification of an input instance as an element of the (meta-)concept can be justified by the fact that intermediate concepts have been recognized.","authors":["Isma\\\"il Baaj","Pierre Marquis"],"url":"https://arxiv.org/abs/2504.07055"}
{"created":"2025-04-11","title":"Physics informed neural network for forward and inverse modeling of low grade brain tumors","abstract":"A low grade tumor is a slow growing tumor with a lower likelihood of spreading compared to high grade tumors. Mathematical modeling using partial differential equations (PDEs) plays a crucial role in describing tumor behavior, growth and progression. This study employs the Burgess and extended Fisher Kolmogorov equations to model low-grade brain tumors. We utilize Physics Informed Neural Networks (PINNs) based algorithm to develop an automated numerical solver for these models and explore their application in solving forward and inverse problems in brain tumor modeling. The study aims to demonstrate that the PINN based algorithms serve as advanced methodologies for modeling brain tumor dynamics by integrating deep learning with physics-informed principles. Additionally, we establish generalized error bounds in terms of training and quadrature errors. The convergence and stability of the neural network are derived for both models. Numerical tests confirm the accuracy and efficiency of the algorithms in both linear and nonlinear cases. Additionally, a statistical analysis of the numerical results is presented.","authors":["K. Murari","P. Roul","S. Sundar"],"url":"https://arxiv.org/abs/2504.07058"}
{"created":"2025-04-11","title":"Generalized Semantic Contrastive Learning via Embedding Side Information for Few-Shot Object Detection","abstract":"The objective of few-shot object detection (FSOD) is to detect novel objects with few training samples. The core challenge of this task is how to construct a generalized feature space for novel categories with limited data on the basis of the base category space, which could adapt the learned detection model to unknown scenarios. However, limited by insufficient samples for novel categories, two issues still exist: (1) the features of the novel category are easily implicitly represented by the features of the base category, leading to inseparable classifier boundaries, (2) novel categories with fewer data are not enough to fully represent the distribution, where the model fine-tuning is prone to overfitting. To address these issues, we introduce the side information to alleviate the negative influences derived from the feature space and sample viewpoints and formulate a novel generalized feature representation learning method for FSOD. Specifically, we first utilize embedding side information to construct a knowledge matrix to quantify the semantic relationship between the base and novel categories. Then, to strengthen the discrimination between semantically similar categories, we further develop contextual semantic supervised contrastive learning which embeds side information. Furthermore, to prevent overfitting problems caused by sparse samples, a side-information guided region-aware masked module is introduced to augment the diversity of samples, which finds and abandons biased information that discriminates between similar categories via counterfactual explanation, and refines the discriminative representation space further. Extensive experiments using ResNet and ViT backbones on PASCAL VOC, MS COCO, LVIS V1, FSOD-1K, and FSVOD-500 benchmarks demonstrate that our model outperforms the previous state-of-the-art methods, significantly improving the ability of FSOD in most shots/splits.","authors":["Ruoyu Chen","Hua Zhang","Jingzhi Li","Li Liu","Zhen Huang","Xiaochun Cao"],"url":"https://arxiv.org/abs/2504.07060"}
{"created":"2025-04-11","title":"Teaching pathology foundation models to accurately predict gene expression with parameter efficient knowledge transfer","abstract":"Gene expression profiling provides critical insights into cellular heterogeneity, biological processes and disease mechanisms. There has been an increasing interest in computational approaches that can predict gene expression directly from digitalized histopathology images. While image foundation models have shown promise in a variety of pathology downstream analysis, their performances on gene-expression prediction are still limited. Explicitly incorporating information from the transcriptomic models can help image models to address domain shift, yet the fine-tuning and alignment of foundation models can be expensive. In the work, we propose Parameter Efficient Knowledge trAnsfer (PEKA), a novel framework that leverages Block-Affine Adaptation and integrates knowledge distillation and structure alignment losses for cross-modal knowledge transfer. We evaluated PEKA for gene expression prediction using multiple spatial transcriptomics datasets (comprising 206,123 image tiles with matched gene expression profiles) that encompassed various types of tissue. PEKA achieved at least 5\\% performance improvement over baseline foundation models while also outperforming alternative parameter-efficient fine-tuning strategies. We will release the code, datasets and aligned models after peer-review to facilitate broader adoption and further development for parameter efficient model alignment.","authors":["Shi Pan","Jianan Chen","Maria Secrier"],"url":"https://arxiv.org/abs/2504.07061"}
{"created":"2025-04-11","title":"HalluciNot: Hallucination Detection Through Context and Common Knowledge Verification","abstract":"This paper introduces a comprehensive system for detecting hallucinations in large language model (LLM) outputs in enterprise settings. We present a novel taxonomy of LLM responses specific to hallucination in enterprise applications, categorizing them into context-based, common knowledge, enterprise-specific, and innocuous statements. Our hallucination detection model HDM-2 validates LLM responses with respect to both context and generally known facts (common knowledge). It provides both hallucination scores and word-level annotations, enabling precise identification of problematic content. To evaluate it on context-based and common-knowledge hallucinations, we introduce a new dataset HDMBench. Experimental results demonstrate that HDM-2 out-performs existing approaches across RagTruth, TruthfulQA, and HDMBench datasets. This work addresses the specific challenges of enterprise deployment, including computational efficiency, domain specialization, and fine-grained error identification. Our evaluation dataset, model weights, and inference code are publicly available.","authors":["Bibek Paudel","Alexander Lyzhov","Preetam Joshi","Puneet Anand"],"url":"https://arxiv.org/abs/2504.07069"}
{"created":"2025-04-11","title":"A Survey on Personalized and Pluralistic Preference Alignment in Large Language Models","abstract":"Personalized preference alignment for large language models (LLMs), the process of tailoring LLMs to individual users' preferences, is an emerging research direction spanning the area of NLP and personalization. In this survey, we present an analysis of works on personalized alignment and modeling for LLMs. We introduce a taxonomy of preference alignment techniques, including training time, inference time, and additionally, user-modeling based methods. We provide analysis and discussion on the strengths and limitations of each group of techniques and then cover evaluation, benchmarks, as well as open problems in the field.","authors":["Zhouhang Xie","Junda Wu","Yiran Shen","Yu Xia","Xintong Li","Aaron Chang","Ryan Rossi","Sachin Kumar","Bodhisattwa Prasad Majumder","Jingbo Shang","Prithviraj Ammanabrolu","Julian McAuley"],"url":"https://arxiv.org/abs/2504.07070"}
{"created":"2025-04-11","title":"Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation","abstract":"The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam benchmark to date for the multilingual evaluation of vision-language models. Kaleidoscope is a large-scale, in-language multimodal benchmark designed to evaluate VLMs across diverse languages and visual inputs. Kaleidoscope covers 18 languages and 14 different subjects, amounting to a total of 20,911 multiple-choice questions. Built through an open science collaboration with a diverse group of researchers worldwide, Kaleidoscope ensures linguistic and cultural authenticity. We evaluate top-performing multilingual vision-language models and find that they perform poorly on low-resource languages and in complex multimodal scenarios. Our results highlight the need for progress on culturally inclusive multimodal evaluation frameworks.","authors":["Israfel Salazar","Manuel Fern\\'andez Burda","Shayekh Bin Islam","Arshia Soltani Moakhar","Shivalika Singh","Fabian Farestam","Angelika Romanou","Danylo Boiko","Dipika Khullar","Mike Zhang","Dominik Krzemi\\'nski","Jekaterina Novikova","Lu\\'isa Shimabucoro","Joseph Marvin Imperial","Rishabh Maheshwary","Sharad Duwal","Alfonso Amayuelas","Swati Rajwal","Jebish Purbey","Ahmed Ruby","Nicholas Popovi\\v{c}","Marek Suppa","Azmine Toushik Wasi","Ram Mohan Rao Kadiyala","Olga Tsymboi","Maksim Kostritsya","Bardia Soltani Moakhar","Gabriel da Costa Merlin","Ot\\'avio Ferracioli Coletti","Maral Jabbari Shiviari","MohammadAmin farahani fard","Silvia Fernandez","Mar\\'ia Grandury","Dmitry Abulkhanov","Drishti Sharma","Andre Guarnier De Mitri","Leticia Bossatto Marchezi","Johan Obando-Ceron","Nazar Kohut","Beyza Ermis","Desmond Elliott","Enzo Ferrante","Sara Hooker","Marzieh Fadaee"],"url":"https://arxiv.org/abs/2504.07072"}
{"created":"2025-04-11","title":"Detecting AI-generated Artwork","abstract":"The high efficiency and quality of artwork generated by Artificial Intelligence (AI) has created new concerns and challenges for human artists. In particular, recent improvements in generative AI have made it difficult for people to distinguish between human-generated and AI-generated art. In this research, we consider the potential utility of various types of Machine Learning (ML) and Deep Learning (DL) models in distinguishing AI-generated artwork from human-generated artwork. We focus on three challenging artistic styles, namely, baroque, cubism, and expressionism. The learning models we test are Logistic Regression (LR), Support Vector Machine (SVM), Multilayer Perceptron (MLP), and Convolutional Neural Network (CNN). Our best experimental results yield a multiclass accuracy of 0.8208 over six classes, and an impressive accuracy of 0.9758 for the binary classification problem of distinguishing AI-generated from human-generated art.","authors":["Meien Li","Mark Stamp"],"url":"https://arxiv.org/abs/2504.07078"}
{"created":"2025-04-11","title":"SkillWeaver: Web Agents can Self-Improve by Discovering and Honing Skills","abstract":"To survive and thrive in complex environments, humans have evolved sophisticated self-improvement mechanisms through environment exploration, hierarchical abstraction of experiences into reuseable skills, and collaborative construction of an ever-growing skill repertoire. Despite recent advancements, autonomous web agents still lack crucial self-improvement capabilities, struggling with procedural knowledge abstraction, refining skills, and skill composition. In this work, we introduce SkillWeaver, a skill-centric framework enabling agents to self-improve by autonomously synthesizing reusable skills as APIs. Given a new website, the agent autonomously discovers skills, executes them for practice, and distills practice experiences into robust APIs. Iterative exploration continually expands a library of lightweight, plug-and-play APIs, significantly enhancing the agent's capabilities. Experiments on WebArena and real-world websites demonstrate the efficacy of SkillWeaver, achieving relative success rate improvements of 31.8% and 39.8%, respectively. Additionally, APIs synthesized by strong agents substantially enhance weaker agents through transferable skills, yielding improvements of up to 54.3% on WebArena. These results demonstrate the effectiveness of honing diverse website interactions into APIs, which can be seamlessly shared among various web agents.","authors":["Boyuan Zheng","Michael Y. Fatemi","Xiaolong Jin","Zora Zhiruo Wang","Apurva Gandhi","Yueqi Song","Yu Gu","Jayanth Srinivasa","Gaowen Liu","Graham Neubig","Yu Su"],"url":"https://arxiv.org/abs/2504.07079"}
{"created":"2025-04-11","title":"DeduCE: Deductive Consistency as a Framework to Evaluate LLM Reasoning","abstract":"Despite great performance on Olympiad-level reasoning problems, frontier large language models can still struggle on high school math when presented with novel problems outside standard benchmarks. Going beyond final accuracy, we propose a deductive consistency metric to analyze chain-of-thought output from language models (LMs).Formally, deductive reasoning involves two subtasks: understanding a set of input premises and inferring the conclusions that follow from them. The proposed metric studies LMs' performance on these subtasks, with the goal of explaining LMs' reasoning errors on novel problems: how well do LMs understand input premises with increasing context lengths, and how well can they infer conclusions over multiple reasoning hops? Since existing benchmarks may be memorized, we develop a pipeline to evaluate LMs' deductive consistency on novel, perturbed versions of benchmark problems. On novel grade school math problems (GSM-8k), we find that LMs are fairly robust to increasing number of input premises, but suffer significant accuracy decay as the number of reasoning hops is increased. Interestingly, these errors are masked in the original benchmark as all models achieve near 100% accuracy. As we increase the number of solution steps using a synthetic dataset, prediction over multiple hops still remains the major source of error compared to understanding input premises. Other factors, such as shifts in language style or natural propagation of early errors do not explain the trends. Our analysis provides a new view to characterize LM reasoning -- as computations over a window of input premises and reasoning hops -- that can provide unified evaluation across problem domains.","authors":["Atharva Pandey","Kshitij Dubey","Rahul Sharma","Amit Sharma"],"url":"https://arxiv.org/abs/2504.07080"}
{"created":"2025-04-11","title":"Self-Steering Language Models","abstract":"While test-time reasoning enables language models to tackle complex tasks, searching or planning in natural language can be slow, costly, and error-prone. But even when LMs struggle to emulate the precise reasoning steps needed to solve a problem, they often excel at describing its abstract structure--both how to verify solutions and how to search for them. This paper introduces DisCIPL, a method for \"self-steering\" LMs where a Planner model generates a task-specific inference program that is executed by a population of Follower models. Our approach equips LMs with the ability to write recursive search procedures that guide LM inference, enabling new forms of verifiable and efficient reasoning. When instantiated with a small Follower (e.g., Llama-3.2-1B), DisCIPL matches (and sometimes outperforms) much larger models, including GPT-4o and o1, on challenging constrained generation tasks. In decoupling planning from execution, our work opens up a design space of highly-parallelized Monte Carlo inference strategies that outperform standard best-of-N sampling, require no finetuning, and can be implemented automatically by existing LMs.","authors":["Gabriel Grand","Joshua B. Tenenbaum","Vikash K. Mansinghka","Alexander K. Lew","Jacob Andreas"],"url":"https://arxiv.org/abs/2504.07081"}
{"created":"2025-04-11","title":"GenDoP: Auto-regressive Camera Trajectory Generation as a Director of Photography","abstract":"Camera trajectory design plays a crucial role in video production, serving as a fundamental tool for conveying directorial intent and enhancing visual storytelling. In cinematography, Directors of Photography meticulously craft camera movements to achieve expressive and intentional framing. However, existing methods for camera trajectory generation remain limited: Traditional approaches rely on geometric optimization or handcrafted procedural systems, while recent learning-based methods often inherit structural biases or lack textual alignment, constraining creative synthesis. In this work, we introduce an auto-regressive model inspired by the expertise of Directors of Photography to generate artistic and expressive camera trajectories. We first introduce DataDoP, a large-scale multi-modal dataset containing 29K real-world shots with free-moving camera trajectories, depth maps, and detailed captions in specific movements, interaction with the scene, and directorial intent. Thanks to the comprehensive and diverse database, we further train an auto-regressive, decoder-only Transformer for high-quality, context-aware camera movement generation based on text guidance and RGBD inputs, named GenDoP. Extensive experiments demonstrate that compared to existing methods, GenDoP offers better controllability, finer-grained trajectory adjustments, and higher motion stability. We believe our approach establishes a new standard for learning-based cinematography, paving the way for future advancements in camera control and filmmaking. Our project website: https://kszpxxzmc.github.io/GenDoP/.","authors":["Mengchen Zhang","Tong Wu","Jing Tan","Ziwei Liu","Gordon Wetzstein","Dahua Lin"],"url":"https://arxiv.org/abs/2504.07083"}
{"created":"2025-04-11","title":"Identifying Unknown Stochastic Dynamics via Finite expression methods","abstract":"Modeling stochastic differential equations (SDEs) is crucial for understanding complex dynamical systems in various scientific fields. Recent methods often employ neural network-based models, which typically represent SDEs through a combination of deterministic and stochastic terms. However, these models usually lack interpretability and have difficulty generalizing beyond their training domain. This paper introduces the Finite Expression Method (FEX), a symbolic learning approach designed to derive interpretable mathematical representations of the deterministic component of SDEs. For the stochastic component, we integrate FEX with advanced generative modeling techniques to provide a comprehensive representation of SDEs. The numerical experiments on linear, nonlinear, and multidimensional SDEs demonstrate that FEX generalizes well beyond the training domain and delivers more accurate long-term predictions compared to neural network-based methods. The symbolic expressions identified by FEX not only improve prediction accuracy but also offer valuable scientific insights into the underlying dynamics of the systems, paving the way for new scientific discoveries.","authors":["Senwei Liang","Chunmei Wang","Xingjian Xu"],"url":"https://arxiv.org/abs/2504.07085"}
{"created":"2025-04-11","title":"A Sober Look at Progress in Language Model Reasoning: Pitfalls and Paths to Reproducibility","abstract":"Reasoning has emerged as the next major frontier for language models (LMs), with rapid advances from both academic and industrial labs. However, this progress often outpaces methodological rigor, with many evaluations relying on benchmarking practices that lack transparency, robustness, or statistical grounding. In this work, we conduct a comprehensive empirical study and find that current mathematical reasoning benchmarks are highly sensitive to subtle implementation choices - including decoding parameters, random seeds, prompt formatting, and even hardware and software-framework configurations. Performance gains reported in recent studies frequently hinge on unclear comparisons or unreported sources of variance. To address these issues, we propose a standardized evaluation framework with clearly defined best practices and reporting standards. Using this framework, we reassess recent methods and find that reinforcement learning (RL) approaches yield only modest improvements - far below prior claims - and are prone to overfitting, especially on small-scale benchmarks like AIME24. In contrast, supervised finetuning (SFT) methods show consistently stronger generalization. To foster reproducibility, we release all code, prompts, and model outputs, for reasoning benchmarks, establishing more rigorous foundations for future work.","authors":["Andreas Hochlehnert","Hardik Bhatnagar","Vishaal Udandarao","Samuel Albanie","Ameya Prabhu","Matthias Bethge"],"url":"https://arxiv.org/abs/2504.07086"}
{"created":"2025-04-11","title":"KG-LLM-Bench: A Scalable Benchmark for Evaluating LLM Reasoning on Textualized Knowledge Graphs","abstract":"Knowledge graphs have emerged as a popular method for injecting up-to-date, factual knowledge into large language models (LLMs). This is typically achieved by converting the knowledge graph into text that the LLM can process in context. While multiple methods of encoding knowledge graphs have been proposed, the impact of this textualization process on LLM performance remains under-explored. We introduce KG-LLM-Bench, a comprehensive and extensible benchmark spanning five knowledge graph understanding tasks, and evaluate how different encoding strategies affect performance across various base models. Our extensive experiments with seven language models and five textualization strategies provide insights for optimizing LLM performance on KG reasoning tasks.","authors":["Elan Markowitz","Krupa Galiya","Greg Ver Steeg","Aram Galstyan"],"url":"https://arxiv.org/abs/2504.07087"}
{"created":"2025-04-11","title":"OmniCaptioner: One Captioner to Rule Them All","abstract":"We propose OmniCaptioner, a versatile visual captioning framework for generating fine-grained textual descriptions across a wide variety of visual domains. Unlike prior methods limited to specific image types (e.g., natural images or geometric visuals), our framework provides a unified solution for captioning natural images, visual text (e.g., posters, UIs, textbooks), and structured visuals (e.g., documents, tables, charts). By converting low-level pixel information into semantically rich textual representations, our framework bridges the gap between visual and textual modalities. Our results highlight three key advantages: (i) Enhanced Visual Reasoning with LLMs, where long-context captions of visual modalities empower LLMs, particularly the DeepSeek-R1 series, to reason effectively in multimodal scenarios; (ii) Improved Image Generation, where detailed captions improve tasks like text-to-image generation and image transformation; and (iii) Efficient Supervised Fine-Tuning (SFT), which enables faster convergence with less data. We believe the versatility and adaptability of OmniCaptioner can offer a new perspective for bridging the gap between language and visual modalities.","authors":["Yiting Lu","Jiakang Yuan","Zhen Li","Shitian Zhao","Qi Qin","Xinyue Li","Le Zhuo","Licheng Wen","Dongyang Liu","Yuewen Cao","Xiangchao Yan","Xin Li","Botian Shi","Tao Chen","Zhibo Chen","Lei Bai","Bo Zhang","Peng Gao"],"url":"https://arxiv.org/abs/2504.07089"}
{"created":"2025-04-11","title":"AssistanceZero: Scalably Solving Assistance Games","abstract":"Assistance games are a promising alternative to reinforcement learning from human feedback (RLHF) for training AI assistants. Assistance games resolve key drawbacks of RLHF, such as incentives for deceptive behavior, by explicitly modeling the interaction between assistant and user as a two-player game where the assistant cannot observe their shared goal. Despite their potential, assistance games have only been explored in simple settings. Scaling them to more complex environments is difficult because it requires both solving intractable decision-making problems under uncertainty and accurately modeling human users' behavior. We present the first scalable approach to solving assistance games and apply it to a new, challenging Minecraft-based assistance game with over $10^{400}$ possible goals. Our approach, AssistanceZero, extends AlphaZero with a neural network that predicts human actions and rewards, enabling it to plan under uncertainty. We show that AssistanceZero outperforms model-free RL algorithms and imitation learning in the Minecraft-based assistance game. In a human study, our AssistanceZero-trained assistant significantly reduces the number of actions participants take to complete building tasks in Minecraft. Our results suggest that assistance games are a tractable framework for training effective AI assistants in complex environments. Our code and models are available at https://github.com/cassidylaidlaw/minecraft-building-assistance-game.","authors":["Cassidy Laidlaw","Eli Bronstein","Timothy Guo","Dylan Feng","Lukas Berglund","Justin Svegliato","Stuart Russell","Anca Dragan"],"url":"https://arxiv.org/abs/2504.07091"}
{"created":"2025-04-11","title":"Are We Done with Object-Centric Learning?","abstract":"Object-centric learning (OCL) seeks to learn representations that only encode an object, isolated from other objects or background cues in a scene. This approach underpins various aims, including out-of-distribution (OOD) generalization, sample-efficient composition, and modeling of structured environments. Most research has focused on developing unsupervised mechanisms that separate objects into discrete slots in the representation space, evaluated using unsupervised object discovery. However, with recent sample-efficient segmentation models, we can separate objects in the pixel space and encode them independently. This achieves remarkable zero-shot performance on OOD object discovery benchmarks, is scalable to foundation models, and can handle a variable number of slots out-of-the-box. Hence, the goal of OCL methods to obtain object-centric representations has been largely achieved. Despite this progress, a key question remains: How does the ability to separate objects within a scene contribute to broader OCL objectives, such as OOD generalization? We address this by investigating the OOD generalization challenge caused by spurious background cues through the lens of OCL. We propose a novel, training-free probe called $\\textbf{Object-Centric Classification with Applied Masks (OCCAM)}$, demonstrating that segmentation-based encoding of individual objects significantly outperforms slot-based OCL methods. However, challenges in real-world applications remain. We provide the toolbox for the OCL community to use scalable object-centric representations, and focus on practical applications and fundamental questions, such as understanding object perception in human cognition. Our code is available $\\href{https://github.com/AlexanderRubinstein/OCCAM}{here}$.","authors":["Alexander Rubinstein","Ameya Prabhu","Matthias Bethge","Seong Joon Oh"],"url":"https://arxiv.org/abs/2504.07092"}
{"created":"2025-04-11","title":"FlashDepth: Real-time Streaming Video Depth Estimation at 2K Resolution","abstract":"A versatile video depth estimation model should (1) be accurate and consistent across frames, (2) produce high-resolution depth maps, and (3) support real-time streaming. We propose FlashDepth, a method that satisfies all three requirements, performing depth estimation on a 2044x1148 streaming video at 24 FPS. We show that, with careful modifications to pretrained single-image depth models, these capabilities are enabled with relatively little data and training. We evaluate our approach across multiple unseen datasets against state-of-the-art depth models, and find that ours outperforms them in terms of boundary sharpness and speed by a significant margin, while maintaining competitive accuracy. We hope our model will enable various applications that require high-resolution depth, such as video editing, and online decision-making, such as robotics.","authors":["Gene Chou","Wenqi Xian","Guandao Yang","Mohamed Abdelfattah","Bharath Hariharan","Noah Snavely","Ning Yu","Paul Debevec"],"url":"https://arxiv.org/abs/2504.07093"}
{"created":"2025-04-11","title":"Neural Motion Simulator: Pushing the Limit of World Models in Reinforcement Learning","abstract":"An embodied system must not only model the patterns of the external world but also understand its own motion dynamics. A motion dynamic model is essential for efficient skill acquisition and effective planning. In this work, we introduce the neural motion simulator (MoSim), a world model that predicts the future physical state of an embodied system based on current observations and actions. MoSim achieves state-of-the-art performance in physical state prediction and provides competitive performance across a range of downstream tasks. This works shows that when a world model is accurate enough and performs precise long-horizon predictions, it can facilitate efficient skill acquisition in imagined worlds and even enable zero-shot reinforcement learning. Furthermore, MoSim can transform any model-free reinforcement learning (RL) algorithm into a model-based approach, effectively decoupling physical environment modeling from RL algorithm development. This separation allows for independent advancements in RL algorithms and world modeling, significantly improving sample efficiency and enhancing generalization capabilities. Our findings highlight that world models for motion dynamics is a promising direction for developing more versatile and capable embodied systems.","authors":["Chenjie Hao","Weyl Lu","Yifan Xu","Yubei Chen"],"url":"https://arxiv.org/abs/2504.07095"}
{"created":"2025-04-11","title":"OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens","abstract":"We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.","authors":["Jiacheng Liu","Taylor Blanton","Yanai Elazar","Sewon Min","YenSung Chen","Arnavi Chheda-Kothary","Huy Tran","Byron Bischoff","Eric Marsh","Michael Schmitz","Cassidy Trier","Aaron Sarnat","Jenna James","Jon Borchardt","Bailey Kuehl","Evie Cheng","Karen Farley","Sruthi Sreeram","Taira Anderson","David Albright","Carissa Schoenick","Luca Soldaini","Dirk Groeneveld","Rock Yuren Pang","Pang Wei Koh","Noah A. Smith","Sophie Lebrecht","Yejin Choi","Hannaneh Hajishirzi","Ali Farhadi","Jesse Dodge"],"url":"https://arxiv.org/abs/2504.07096"}
{"created":"2025-04-11","title":"Sculpting Subspaces: Constrained Full Fine-Tuning in LLMs for Continual Learning","abstract":"Continual learning in large language models (LLMs) is prone to catastrophic forgetting, where adapting to new tasks significantly degrades performance on previously learned ones. Existing methods typically rely on low-rank, parameter-efficient updates that limit the model's expressivity and introduce additional parameters per task, leading to scalability issues. To address these limitations, we propose a novel continual full fine-tuning approach leveraging adaptive singular value decomposition (SVD). Our method dynamically identifies task-specific low-rank parameter subspaces and constrains updates to be orthogonal to critical directions associated with prior tasks, thus effectively minimizing interference without additional parameter overhead or storing previous task gradients. We evaluate our approach extensively on standard continual learning benchmarks using both encoder-decoder (T5-Large) and decoder-only (LLaMA-2 7B) models, spanning diverse tasks including classification, generation, and reasoning. Empirically, our method achieves state-of-the-art results, up to 7% higher average accuracy than recent baselines like O-LoRA, and notably maintains the model's general linguistic capabilities, instruction-following accuracy, and safety throughout the continual learning process by reducing forgetting to near-negligible levels. Our adaptive SVD framework effectively balances model plasticity and knowledge retention, providing a practical, theoretically grounded, and computationally scalable solution for continual learning scenarios in large language models.","authors":["Nikhil Shivakumar Nayak","Krishnateja Killamsetty","Ligong Han","Abhishek Bhandwaldar","Prateek Chanda","Kai Xu","Hao Wang","Aldo Pareja","Oleg Silkin","Mustafa Eyceoz","Akash Srivastava"],"url":"https://arxiv.org/abs/2504.07097"}
{"created":"2025-04-11","title":"MultiDelete for Multimodal Machine Unlearning","abstract":"Machine Unlearning removes specific knowledge about training data samples from an already trained model. It has significant practical benefits, such as purging private, inaccurate, or outdated information from trained models without the need for complete re-training. Unlearning within a multimodal setting presents unique challenges due to the complex dependencies between different data modalities and the expensive cost of training on large multimodal datasets and architectures. This paper presents the first machine unlearning approach for multimodal data and models, titled MultiDelete, which is designed to decouple associations between unimodal data points during unlearning without losing the overall representation strength of the trained model. MultiDelete advocates for three key properties for effective multimodal unlearning: (a): modality decoupling, which effectively decouples the association between individual unimodal data points marked for deletion, rendering them as unrelated data points, (b): multimodal knowledge retention, which retains the multimodal representation post-unlearning, and (c): unimodal knowledge retention, which retains the unimodal representation postunlearning. MultiDelete is efficient to train and is not constrained by using a strongly convex loss -- a common restriction among existing baselines. Experiments on two architectures and four datasets, including image-text and graph-text datasets, show that MultiDelete gains an average improvement of 17.6 points over best performing baseline in unlearning multimodal samples, can maintain the multimodal and unimodal knowledge of the original model post unlearning, and can provide better protection to unlearned data against adversarial attacks.","authors":["Jiali Cheng","Hadi Amiri"],"url":"https://arxiv.org/abs/2311.12047"}
{"created":"2025-04-11","title":"CMAT: A Multi-Agent Collaboration Tuning Framework for Enhancing Small Language Models","abstract":"Open large language models (LLMs) have significantly advanced the field of natural language processing, showcasing impressive performance across various tasks.Despite the significant advancements in LLMs, their effective operation still relies heavily on human input to accurately guide the dialogue flow, with agent tuning being a crucial optimization technique that involves human adjustments to the model for better response to such guidance.Addressing this dependency, our work introduces the TinyAgent model, trained on a meticulously curated high-quality dataset. We also present the Collaborative Multi-Agent Tuning (CMAT) framework, an innovative system designed to augment language agent capabilities through adaptive weight updates based on environmental feedback. This framework fosters collaborative learning and real-time adaptation among multiple intelligent agents, enhancing their context-awareness and long-term memory. In this research, we propose a new communication agent framework that integrates multi-agent systems with environmental feedback mechanisms, offering a scalable method to explore cooperative behaviors. Notably, our TinyAgent-7B model exhibits performance on par with GPT-3.5, despite having fewer parameters, signifying a substantial improvement in the efficiency and effectiveness of LLMs.","authors":["Xuechen Liang","Meiling Tao","Yinghui Xia","Tianyu Shi","Jun Wang","JingSong Yang"],"url":"https://arxiv.org/abs/2404.01663"}
{"created":"2025-04-11","title":"Multi-objective Optimization in CPU Design Space Exploration: Attention is All You Need","abstract":"Design space exploration (DSE) enables architects to systematically evaluate various design options, guiding decisions on the most suitable configurations to meet specific objectives such as optimizing performance, power, and area. However, the growing complexity of modern CPUs has dramatically increased the number of micro-architectural parameters and expanded the overall design space, making DSE more challenging and time-consuming. Existing DSE frameworks struggle in large-scale design spaces due to inaccurate models and limited insights into parameter impact, hindering efficient identification of optimal micro-architectures within tight timeframes.","authors":["Runzhen Xue","Hao Wu","Mingyu Yan","Ziheng Xiao","Xiaochun Ye","Dongrui Fan"],"url":"https://arxiv.org/abs/2410.18368"}
{"created":"2025-04-11","title":"Application of Quantum Approximate Optimization Algorithm in Solving the Total Domination Problem","abstract":"Recent advancements in quantum computing have led to significant research into applying quantum algorithms to combinatorial optimization problems. Among these challenges, the Total Domination Problem (TDP) is particularly noteworthy, representing a classic and critical example in the field. Since the last century, research efforts have focused on establishing its NP-completeness and developing algorithms for its resolution, which have been fundamental to combinatorial mathematics. Despite this rich history, the application of quantum algorithms to the TDP remains largely unexplored. In this study, we present a pioneering application of the Quantum Approximate Optimization Algorithm (QAOA) to tackle the TDP, evaluating its efficacy across a diverse array of parameters. Our experimental findings indicate that QAOA is effective in addressing the TDP; under most parameter combinations, it successfully computes the correct total dominating set (TDS). However, the algorithm's performance in identifying the optimal TDS is contingent upon the specific parameter choices, revealing a significant bias in the distribution of effective parameter points. This research contributes valuable insights into the potential of quantum algorithms for addressing the TDP and lays the groundwork for future investigations in this area.","authors":["Haoqian Pan","Shiyue Wang","Changhong Lu"],"url":"https://arxiv.org/abs/2411.00364"}
{"created":"2025-04-11","title":"The Future of IPTV: Security, AI Integration, 5G, and Next-Gen Streaming","abstract":"The evolution of Internet Protocol Television (IPTV) has transformed the landscape of digital broadcasting by leveraging high-speed internet connectivity to deliver high-quality multimedia content. IPTV provides a dynamic and interactive television experience through managed networks, ensuring superior Quality of Service (QoS) compared to open-network Internet TV. This study explores the technical infrastructure of IPTV, including its network architecture, data compression techniques, and the role of protocols such as IGMP and RTSP. It also examines security challenges, including encryption, digital rights management (DRM), and authentication mechanisms that safeguard IPTV services from unauthorized access and piracy. Moreover, the paper analyzes the distinctions between IPTV and open-network Internet TV, highlighting their respective advantages and limitations in terms of service control, bandwidth optimization, and content security. The integration of artificial intelligence (AI) and machine learning (ML) in IPTV enhances personalized content recommendations and predictive analytics, leading to improved user engagement and efficient network management. Additionally, emerging technologies such as 5G and cloud-based IPTV services are explored for their potential to further revolutionize the industry. While IPTV presents a robust alternative to traditional broadcasting, challenges such as bandwidth constraints, cybersecurity threats, and regulatory compliance remain significant. The study concludes that IPTV's future success will depend on advancements in network infrastructure, AI-driven optimizations, and strategic regulatory adaptations. As IPTV continues to evolve, hybrid models integrating IPTV and open-network streaming services are expected to enhance content accessibility, security, and overall user experience.","authors":["Georgios Giannakopoulos","Peter Adegbenro","Maria Antonnette Perez"],"url":"https://arxiv.org/abs/2503.13450"}
{"created":"2025-04-11","title":"Data-driven Power Loss Identification through Physics-Based Thermal Model Backpropagation","abstract":"Digital twins for power electronics require accurate power losses whose direct measurements are often impractical or impossible in real-world applications. This paper presents a novel hybrid framework that combines physics-based thermal modeling with data-driven techniques to identify and correct power losses accurately using only temperature measurements. Our approach leverages a cascaded architecture where a neural network learns to correct the outputs of a nominal power loss model by backpropagating through a reduced-order thermal model. We explore two neural architectures, a bootstrapped feedforward network, and a recurrent neural network, demonstrating that the bootstrapped feedforward approach achieves superior performance while maintaining computational efficiency for real-time applications. Between the interconnection, we included normalization strategies and physics-guided training loss functions to preserve stability and ensure physical consistency. Experimental results show that our hybrid model reduces both temperature estimation errors (from 7.2+-6.8{\\deg}C to 0.3+-0.3{\\deg}C) and power loss prediction errors (from 5.4+-6.6W to 0.2+-0.3W) compared to traditional physics-based approaches, even in the presence of thermal model uncertainties. This methodology allows us to accurately estimate power losses without direct measurements, making it particularly helpful for real-time industrial applications where sensor placement is hindered by cost and physical limitations.","authors":["Mattia Scarpa","Francesco Pase","Ruggero Carli","Mattia Bruschetta","Franscesco Toso"],"url":"https://arxiv.org/abs/2504.00133"}
{"created":"2025-04-11","title":"Leveraging LLMs for User Stories in AI Systems: UStAI Dataset","abstract":"AI systems are gaining widespread adoption across various sectors and domains. Creating high-quality AI system requirements is crucial for aligning the AI system with business goals and consumer values and for social responsibility. However, with the uncertain nature of AI systems and the heavy reliance on sensitive data, more research is needed to address the elicitation and analysis of AI systems requirements. With the proprietary nature of many AI systems, there is a lack of open-source requirements artifacts and technical requirements documents for AI systems, limiting broader research and investigation. With Large Language Models (LLMs) emerging as a promising alternative to human-generated text, this paper investigates the potential use of LLMs to generate user stories for AI systems based on abstracts from scholarly papers. We conducted an empirical evaluation using three LLMs and generated $1260$ user stories from $42$ abstracts from $26$ domains. We assess their quality using the Quality User Story (QUS) framework. Moreover, we identify relevant non-functional requirements (NFRs) and ethical principles. Our analysis demonstrates that the investigated LLMs can generate user stories inspired by the needs of various stakeholders, offering a promising approach for generating user stories for research purposes and for aiding in the early requirements elicitation phase of AI systems. We have compiled and curated a collection of stories generated by various LLMs into a dataset (UStAI), which is now publicly available for use.","authors":["Asma Yamani","Malak Baslyman","Moataz Ahmed"],"url":"https://arxiv.org/abs/2504.00513"}
{"created":"2025-04-11","title":"Data-driven Optimization and Transfer Learning for Cellular Network Antenna Configurations","abstract":"We propose a data-driven approach for large-scale cellular network optimization, using a production cellular network in London as a case study and employing Sionna ray tracing for site-specific channel propagation modeling. We optimize base station antenna tilts and half-power beamwidths, resulting in more than double the 10\\%-worst user rates compared to a 3GPP baseline. In scenarios involving aerial users, we identify configurations that increase their median rates fivefold without compromising ground user performance. We further demonstrate the efficacy of model generalization through transfer learning, leveraging available data from a scenario source to predict the optimal solution for a scenario target within a similar number of iterations, without requiring a new initial dataset, and with a negligible performance loss.","authors":["Mohamed Benzaghta","Giovanni Geraci","David L\\'opez-P\\'erez","Alvaro Valcarce"],"url":"https://arxiv.org/abs/2504.00825"}
{"created":"2025-04-11","title":"Data-Driven Design of 3GPP Handover Parameters with Bayesian Optimization and Transfer Learning","abstract":"Mobility management in dense cellular networks is challenging due to varying user speeds and deployment conditions. Traditional 3GPP handover (HO) schemes, relying on fixed A3-offset and time-to-trigger (TTT) parameters, struggle to balance radio link failures (RLFs) and ping-pongs. We propose a data-driven HO optimization framework based on high-dimensional Bayesian optimization (HD-BO) and enhanced with transfer learning to reduce training time and improve generalization across different user speeds. Evaluations on a real-world deployment show that HD-BO outperforms 3GPP set-1 and set-5 benchmarks, while transfer learning enables rapid adaptation without loss in performance. This highlights the potential of data-driven, site-specific mobility management in large-scale networks.","authors":["Mohamed Benzaghta","Sahar Ammar","David L\\'opez-P\\'erez","Basem Shihada","Giovanni Geraci"],"url":"https://arxiv.org/abs/2504.02633"}
{"created":"2025-04-11","title":"Cellular Network Design for UAV Corridors via Data-driven High-dimensional Bayesian Optimization","abstract":"We address the challenge of designing cellular networks for uncrewed aerial vehicles (UAVs) corridors through a novel data-driven approach. We assess multiple state-of-the-art high-dimensional Bayesian optimization (HD-BO) techniques to jointly optimize the cell antenna tilts and half-power beamwidth (HPBW). We find that some of these approaches achieve over 20dB gains in median SINR along UAV corridors, with negligible degradation to ground user performance. Furthermore, we explore the HD-BO's capabilities in terms of model generalization via transfer learning, where data from a previously observed scenario source is leveraged to predict the optimal solution for a new scenario target. We provide examples of scenarios where such transfer learning is successful and others where it fails. Moreover, we demonstrate that HD-BO enables multi-objective optimization, identifying optimal design trade-offs between data rates on the ground versus UAV coverage reliability. We observe that aiming to provide UAV coverage across the entire sky can lower the rates for ground users compared to setups specifically optimized for UAV corridors. Finally, we validate our approach through a case study in a real-world cellular network, where HD-BO identifies optimal and non-obvious antenna configurations that result in more than double the rates along 3D UAV corridors with negligible ground performance loss.","authors":["Mohamed Benzaghta","Giovanni Geraci","David L\\'opez-P\\'erez","Alvaro Valcarce"],"url":"https://arxiv.org/abs/2504.05176"}
{"created":"2025-04-11","title":"StealthRank: LLM Ranking Manipulation via Stealthy Prompt Optimization","abstract":"The integration of large language models (LLMs) into information retrieval systems introduces new attack surfaces, particularly for adversarial ranking manipulations. We present StealthRank, a novel adversarial ranking attack that manipulates LLM-driven product recommendation systems while maintaining textual fluency and stealth. Unlike existing methods that often introduce detectable anomalies, StealthRank employs an energy-based optimization framework combined with Langevin dynamics to generate StealthRank Prompts (SRPs)-adversarial text sequences embedded within product descriptions that subtly yet effectively influence LLM ranking mechanisms. We evaluate StealthRank across multiple LLMs, demonstrating its ability to covertly boost the ranking of target products while avoiding explicit manipulation traces that can be easily detected. Our results show that StealthRank consistently outperforms state-of-the-art adversarial ranking baselines in both effectiveness and stealth, highlighting critical vulnerabilities in LLM-driven recommendation systems.","authors":["Yiming Tang","Yi Fan","Chenxiao Yu","Tiankai Yang","Yue Zhao","Xiyang Hu"],"url":"https://arxiv.org/abs/2504.05804"}
{"created":"2025-04-11","title":"Information-Theoretic Reward Decomposition for Generalizable RLHF","abstract":"A generalizable reward model is crucial in Reinforcement Learning from Human Feedback (RLHF) as it enables correctly evaluating unseen prompt-response pairs. However, existing reward models lack this ability, as they are typically trained by increasing the reward gap between chosen and rejected responses, while overlooking the prompts that the responses are conditioned on. Consequently, when the trained reward model is evaluated on prompt-response pairs that lie outside the data distribution, neglecting the effect of prompts may result in poor generalization of the reward model. To address this issue, we decompose the reward value into two independent components: prompt-free reward and prompt-related reward. Prompt-free reward represents the evaluation that is determined only by responses, while the prompt-related reward reflects the reward that derives from both the prompt and the response. We extract these two components from an information-theoretic perspective, which requires no extra models. Subsequently, we propose a new reward learning algorithm by prioritizing data samples based on their prompt-free reward values. Through toy examples, we demonstrate that the extracted prompt-free and prompt-related rewards effectively characterize two parts of the reward model. Further, standard evaluations show that our method improves both the alignment performance and the generalization capability of the reward model.","authors":["Liyuan Mao","Haoran Xu","Amy Zhang","Weinan Zhang","Chenjia Bai"],"url":"https://arxiv.org/abs/2504.06020"}
{"created":"2025-04-11","title":"ProHap Explorer: Visualizing Haplotypes in Proteogenomic Datasets","abstract":"In mass spectrometry-based proteomics, experts usually project data onto a single set of reference sequences, overlooking the influence of common haplotypes (combinations of genetic variants inherited together from a parent). We recently introduced ProHap, a tool for generating customized protein haplotype databases. Here, we present ProHap Explorer, a visualization interface designed to investigate the influence of common haplotypes on the human proteome. It enables users to explore haplotypes, their effects on protein sequences, and the identification of non-canonical peptides in public mass spectrometry datasets. The design builds on well-established representations in biological sequence analysis, ensuring familiarity for domain experts while integrating novel interactive elements tailored to proteogenomic data exploration. User interviews with proteomics experts confirmed the tool's utility, highlighting its ability to reveal whether haplotypes affect proteins of interest. By facilitating the intuitive exploration of proteogenomic variation, ProHap Explorer supports research in personalized medicine and the development of targeted therapies.","authors":["Jakub Va\\v{s}\\'i\\v{c}ek","Dafni Skiadopoulou","Ksenia G. Kuznetsova","Lukas K\\\"all","Marc Vaudel","Stefan Bruckner"],"url":"https://arxiv.org/abs/2504.06282"}
{"created":"2025-04-11","title":"Generative AI Enhanced Financial Risk Management Information Retrieval","abstract":"Risk management in finance involves recognizing, evaluating, and addressing financial risks to maintain stability and ensure regulatory compliance. Extracting relevant insights from extensive regulatory documents is a complex challenge requiring advanced retrieval and language models. This paper introduces RiskData, a dataset specifically curated for finetuning embedding models in risk management, and RiskEmbed, a finetuned embedding model designed to improve retrieval accuracy in financial question-answering systems. The dataset is derived from 94 regulatory guidelines published by the Office of the Superintendent of Financial Institutions (OSFI) from 1991 to 2024. We finetune a state-of-the-art sentence BERT embedding model to enhance domain-specific retrieval performance typically for Retrieval-Augmented Generation (RAG) systems. Experimental results demonstrate that RiskEmbed significantly outperforms general-purpose and financial embedding models, achieving substantial improvements in ranking metrics. By open-sourcing both the dataset and the model, we provide a valuable resource for financial institutions and researchers aiming to develop more accurate and efficient risk management AI solutions.","authors":["Amin Haeri","Jonathan Vitrano","Mahdi Ghelichi"],"url":"https://arxiv.org/abs/2504.06293"}
{"created":"2025-04-11","title":"Going beyond explainability in multi-modal stroke outcome prediction models","abstract":"Aim: This study aims to enhance interpretability and explainability of multi-modal prediction models integrating imaging and tabular patient data.","authors":["Jonas Br\\\"andli","Maurice Schneeberger","Lisa Herzog","Loran Avci","Nordin Dari","Martin H\\\"aansel","Hakim Baazaoui","Pascal B\\\"uhler","Susanne Wegener","Beate Sick"],"url":"https://arxiv.org/abs/2504.06299"}
{"created":"2025-04-11","title":"Subjective Visual Quality Assessment for High-Fidelity Learning-Based Image Compression","abstract":"Learning-based image compression methods have recently emerged as promising alternatives to traditional codecs, offering improved rate-distortion performance and perceptual quality. JPEG AI represents the latest standardized framework in this domain, leveraging deep neural networks for high-fidelity image reconstruction. In this study, we present a comprehensive subjective visual quality assessment of JPEG AI-compressed images using the JPEG AIC-3 methodology, which quantifies perceptual differences in terms of Just Noticeable Difference (JND) units. We generated a dataset of 50 compressed images with fine-grained distortion levels from five diverse sources. A large-scale crowdsourced experiment collected 96,200 triplet responses from 459 participants. We reconstructed JND-based quality scales using a unified model based on boosted and plain triplet comparisons. Additionally, we evaluated the alignment of multiple objective image quality metrics with human perception in the high-fidelity range. The CVVDP metric achieved the overall highest performance; however, most metrics including CVVDP were overly optimistic in predicting the quality of JPEG AI-compressed images. These findings emphasize the necessity for rigorous subjective evaluations in the development and benchmarking of modern image codecs, particularly in the high-fidelity range. Another technical contribution is the introduction of the well-known Meng-Rosenthal-Rubin statistical test to the field of Quality of Experience research. This test can reliably assess the significance of difference in performance of quality metrics in terms of correlation between metrics and ground truth. The complete dataset, including all subjective scores, is publicly available at https://github.com/jpeg-aic/dataset-JPEG-AI-SDR25.","authors":["Mohsen Jenadeleh","Jon Sneyers","Panqi Jia","Shima Mohammadi","Joao Ascenso","Dietmar Saupe"],"url":"https://arxiv.org/abs/2504.06301"}
{"created":"2025-04-11","title":"Leveraging State Space Models in Long Range Genomics","abstract":"Long-range dependencies are critical for understanding genomic structure and function, yet most conventional methods struggle with them. Widely adopted transformer-based models, while excelling at short-context tasks, are limited by the attention module's quadratic computational complexity and inability to extrapolate to sequences longer than those seen in training. In this work, we explore State Space Models (SSMs) as a promising alternative by benchmarking two SSM-inspired architectures, Caduceus and Hawk, on long-range genomics modeling tasks under conditions parallel to a 50M parameter transformer baseline. We discover that SSMs match transformer performance and exhibit impressive zero-shot extrapolation across multiple tasks, handling contexts 10 to 100 times longer than those seen during training, indicating more generalizable representations better suited for modeling the long and complex human genome. Moreover, we demonstrate that these models can efficiently process sequences of 1M tokens on a single GPU, allowing for modeling entire genomic regions at once, even in labs with limited compute. Our findings establish SSMs as efficient and scalable for long-context genomic analysis.","authors":["Matvei Popov","Aymen Kallala","Anirudha Ramesh","Narimane Hennouni","Shivesh Khaitan","Rick Gentry","Alain-Sam Cohen"],"url":"https://arxiv.org/abs/2504.06304"}
{"created":"2025-04-11","title":"Predicting Survivability of Cancer Patients with Metastatic Patterns Using Explainable AI","abstract":"Cancer remains a leading global health challenge and a major cause of mortality. This study leverages machine learning (ML) to predict the survivability of cancer patients with metastatic patterns using the comprehensive MSK-MET dataset, which includes genomic and clinical data from 25,775 patients across 27 cancer types. We evaluated five ML models-XGBoost, Na\\\"ive Bayes, Decision Tree, Logistic Regression, and Random Fores using hyperparameter tuning and grid search. XGBoost emerged as the best performer with an area under the curve (AUC) of 0.82. To enhance model interpretability, SHapley Additive exPlanations (SHAP) were applied, revealing key predictors such as metastatic site count, tumor mutation burden, fraction of genome altered, and organ-specific metastases. Further survival analysis using Kaplan-Meier curves, Cox Proportional Hazards models, and XGBoost Survival Analysis identified significant predictors of patient outcomes, offering actionable insights for clinicians. These findings could aid in personalized prognosis and treatment planning, ultimately improving patient care.","authors":["Polycarp Nalela","Deepthi Rao","Praveen Rao"],"url":"https://arxiv.org/abs/2504.06306"}
{"created":"2025-04-11","title":"DeepGDel: Deep Learning-based Gene Deletion Prediction Framework for Growth-Coupled Production in Genome-Scale Metabolic Models","abstract":"In genome-scale constraint-based metabolic models, gene deletion strategies are crucial for achieving growth-coupled production, where cell growth and target metabolite production are simultaneously achieved. While computational methods for calculating gene deletions have been widely explored and contribute to developing gene deletion strategy databases, current approaches are limited in leveraging new data-driven paradigms, such as machine learning, for more efficient strain design. Therefore, it is necessary to propose a fundamental framework for this objective. In this study, we first formulate the problem of gene deletion strategy prediction and then propose a framework for predicting gene deletion strategies for growth-coupled production in genome-scale metabolic models. The proposed framework leverages deep learning algorithms to learn and integrate sequential gene and metabolite data representation, enabling the automatic gene deletion strategy prediction. Computational experiment results demonstrate the feasibility of the proposed framework, showing substantial improvements over the baseline method. Specifically, the proposed framework achieves a 17.64%, 27.15%, and 18.07% increase in overall accuracy across three metabolic models of different scales under study, while maintaining balanced precision and recall in predicting gene deletion statuses. The source code and examples for the framework are publicly available at https://github.com/MetNetComp/DeepGDel.","authors":["Ziwei Yang","Takeyuki Tamura"],"url":"https://arxiv.org/abs/2504.06316"}
{"created":"2025-04-11","title":"A Geometric-Aware Perspective and Beyond: Hybrid Quantum-Classical Machine Learning Methods","abstract":"Geometric Machine Learning (GML) has shown that respecting non-Euclidean geometry in data spaces can significantly improve performance over naive Euclidean assumptions. In parallel, Quantum Machine Learning (QML) has emerged as a promising paradigm that leverages superposition, entanglement, and interference within quantum state manifolds for learning tasks. This paper offers a unifying perspective by casting QML as a specialized yet more expressive branch of GML. We argue that quantum states, whether pure or mixed, reside on curved manifolds (e.g., projective Hilbert spaces or density-operator manifolds), mirroring how covariance matrices inhabit the manifold of symmetric positive definite (SPD) matrices or how image sets occupy Grassmann manifolds. However, QML also benefits from purely quantum properties, such as entanglement-induced curvature, that can yield richer kernel structures and more nuanced data embeddings.","authors":["Azadeh Alavia","Hossein Akhoundib","Fatemeh Kouchmeshkib","Mojtaba Mahmoodianc","Sanduni Jayasinghec","Yongli Rena","Abdolrahman Alavi"],"url":"https://arxiv.org/abs/2504.06328"}
{"created":"2025-04-11","title":"Multihead self-attention in cortico-thalamic circuits","abstract":"Both biological cortico-thalamic networks and artificial transformer networks use canonical computations to perform a wide range of cognitive tasks. In this work, we propose that the structure of cortico-thalamic circuits is well suited to realize a computation analogous to multihead self-attention, the main algorithmic innovation of transformers. We start with the concept of a cortical unit module or microcolumn, and propose that superficial and deep pyramidal cells carry distinct computational roles. Specifically, superficial pyramidal cells encode an attention mask applied onto deep pyramidal cells to compute attention-modulated values. We show how to wire such microcolumns into a circuit equivalent to a single head of self-attention. We then suggest the parallel between one head of attention and a cortical area. On this basis, we show how to wire cortico-thalamic circuits to perform multihead self-attention. Along these constructions, we refer back to existing experimental data, and find noticeable correspondence. Finally, as a first step towards a mechanistic theory of synaptic learning in this framework, we derive formal gradients of a tokenwise mean squared error loss for a multihead linear self-attention block.","authors":["Arno Granier","Walter Senn"],"url":"https://arxiv.org/abs/2504.06354"}
{"created":"2025-04-11","title":"Deep spatio-temporal point processes: Advances and new directions","abstract":"Spatio-temporal point processes (STPPs) model discrete events distributed in time and space, with important applications in areas such as criminology, seismology, epidemiology, and social networks. Traditional models often rely on parametric kernels, limiting their ability to capture heterogeneous, nonstationary dynamics. Recent innovations integrate deep neural architectures -- either by modeling the conditional intensity function directly or by learning flexible, data-driven influence kernels, substantially broadening their expressive power. This article reviews the development of the deep influence kernel approach, which enjoys statistical explainability, since the influence kernel remains in the model to capture the spatiotemporal propagation of event influence and its impact on future events, while also possessing strong expressive power, thereby benefiting from both worlds. We explain the main components in developing deep kernel point processes, leveraging tools such as functional basis decomposition and graph neural networks to encode complex spatial or network structures, as well as estimation using both likelihood-based and likelihood-free methods, and address computational scalability for large-scale data. We also discuss the theoretical foundation of kernel identifiability. Simulated and real-data examples highlight applications to crime analysis, earthquake aftershock prediction, and sepsis prediction modeling, and we conclude by discussing promising directions for the field.","authors":["Xiuyuan Cheng","Zheng Dong","Yao Xie"],"url":"https://arxiv.org/abs/2504.06364"}
{"created":"2025-04-11","title":"Evaluating Mutation Techniques in Genetic Algorithm-Based Quantum Circuit Synthesis","abstract":"Quantum computing leverages the unique properties of qubits and quantum parallelism to solve problems intractable for classical systems, offering unparalleled computational potential. However, the optimization of quantum circuits remains critical, especially for noisy intermediate-scale quantum (NISQ) devices with limited qubits and high error rates. Genetic algorithms (GAs) provide a promising approach for efficient quantum circuit synthesis by automating optimization tasks. This work examines the impact of various mutation strategies within a GA framework for quantum circuit synthesis. By analyzing how different mutations transform circuits, it identifies strategies that enhance efficiency and performance. Experiments utilized a fitness function emphasizing fidelity, while accounting for circuit depth and T operations, to optimize circuits with four to six qubits. Comprehensive hyperparameter testing revealed that combining delete and swap strategies outperformed other approaches, demonstrating their effectiveness in developing robust GA-based quantum circuit optimizers.","authors":["Michael K\\\"olle","Tom Bintener","Maximilian Zorn","Gerhard Stenzel","Leo S\\\"unkel","Thomas Gabor","Claudia Linnhoff-Popien"],"url":"https://arxiv.org/abs/2504.06413"}
{"created":"2025-04-11","title":"Retuve: Automated Multi-Modality Analysis of Hip Dysplasia with Open Source AI","abstract":"Developmental dysplasia of the hip (DDH) poses significant diagnostic challenges, hindering timely intervention. Current screening methodologies lack standardization, and AI-driven studies suffer from reproducibility issues due to limited data and code availability. To address these limitations, we introduce Retuve, an open-source framework for multi-modality DDH analysis, encompassing both ultrasound (US) and X-ray imaging. Retuve provides a complete and reproducible workflow, offering open datasets comprising expert-annotated US and X-ray images, pre-trained models with training code and weights, and a user-friendly Python Application Programming Interface (API). The framework integrates segmentation and landmark detection models, enabling automated measurement of key diagnostic parameters such as the alpha angle and acetabular index. By adhering to open-source principles, Retuve promotes transparency, collaboration, and accessibility in DDH research. This initiative has the potential to democratize DDH screening, facilitate early diagnosis, and ultimately improve patient outcomes by enabling widespread screening and early intervention. The GitHub repository/code can be found here: https://github.com/radoss-org/retuve","authors":["Adam McArthur","Stephanie Wichuk","Stephen Burnside","Andrew Kirby","Alexander Scammon","Damian Sol","Abhilash Hareendranathan","Jacob L. Jaremko"],"url":"https://arxiv.org/abs/2504.06422"}
{"created":"2025-04-11","title":"Tight bounds on depth-2 QAC-circuits computing parity","abstract":"We show that the parity of more than three non-target input bits cannot be computed by QAC-circuits of depth-2, not even uncleanly, regardless of the number of ancilla qubits. This result is incomparable with other recent lower bounds on constant-depth QAC-circuits by Rosenthal [ICTS~2021,arXiv:2008.07470] and uses different techniques which may be of independent interest:","authors":["Stephen Fenner","Daniel Grier","Daniel Pad\\'e","Thomas Thierauf"],"url":"https://arxiv.org/abs/2504.06433"}
{"created":"2025-04-11","title":"AstroClearNet: Deep image prior for multi-frame astronomical image restoration","abstract":"Recovering high-fidelity images of the night sky from blurred observations is a fundamental problem in astronomy, where traditional methods typically fall short. In ground-based astronomy, combining multiple exposures to enhance signal-to-noise ratios is further complicated by variations in the point-spread function caused by atmospheric turbulence. In this work, we present a self-supervised multi-frame method, based on deep image priors, for denoising, deblurring, and coadding ground-based exposures. Central to our approach is a carefully designed convolutional neural network that integrates information across multiple observations and enforces physically motivated constraints. We demonstrate the method's potential by processing Hyper Suprime-Cam exposures, yielding promising preliminary results with sharper restored images.","authors":["Yashil Sukurdeep","Fausto Navarro","Tam\\'as Budav\\'ari"],"url":"https://arxiv.org/abs/2504.06463"}
{"created":"2025-04-11","title":"AI-Assisted Transport of Radioactive Ion Beams","abstract":"Beams of radioactive heavy ions allow researchers to study rare and unstable atomic nuclei, shedding light into the internal structure of exotic nuclei and on how chemical elements are formed in stars. However, the extraction and transport of radioactive beams rely on time-consuming expert-driven tuning methods, where hundreds of parameters are manually optimized. Here, we introduce a system that uses Artificial Intelligence (AI) to assist in the radioactive beam transport process. We apply our methodology to real-life scenarios showing advantages when compared with standard tuning methods. Our method can be extended to other radioactive beam facilities around the world to improve operational efficiency and enhance scientific output.","authors":["Sergio Lopez-Caceres","Daniel Santiago-Gonzalez"],"url":"https://arxiv.org/abs/2504.06469"}
{"created":"2025-04-11","title":"Deep Fair Learning: A Unified Framework for Fine-tuning Representations with Sufficient Networks","abstract":"Ensuring fairness in machine learning is a critical and challenging task, as biased data representations often lead to unfair predictions. To address this, we propose Deep Fair Learning, a framework that integrates nonlinear sufficient dimension reduction with deep learning to construct fair and informative representations. By introducing a novel penalty term during fine-tuning, our method enforces conditional independence between sensitive attributes and learned representations, addressing bias at its source while preserving predictive performance. Unlike prior methods, it supports diverse sensitive attributes, including continuous, discrete, binary, or multi-group types. Experiments on various types of data structure show that our approach achieves a superior balance between fairness and utility, significantly outperforming state-of-the-art baselines.","authors":["Enze Shi","Linglong Kong","Bei Jiang"],"url":"https://arxiv.org/abs/2504.06470"}
{"created":"2025-04-11","title":"Successive randomized compression: A randomized algorithm for the compressed MPO-MPS product","abstract":"Tensor networks like matrix product states (MPSs) and matrix product operators (MPOs) are powerful tools for representing exponentially large states and operators, with applications in quantum many-body physics, machine learning, numerical analysis, and other areas. In these applications, computing a compressed representation of the MPO--MPS product is a fundamental computational primitive. For this operation, this paper introduces a new single-pass, randomized algorithm, called successive randomized compression (SRC), that improves on existing approaches in speed or in accuracy. The performance of the new algorithm is evaluated on synthetic problems and unitary time evolution problems for quantum spin systems.","authors":["Chris Cama\\~no","Ethan N. Epperly","Joel A. Tropp"],"url":"https://arxiv.org/abs/2504.06475"}
{"created":"2025-04-11","title":"Sparsified-Learning for Heavy-Tailed Locally Stationary Processes","abstract":"Sparsified Learning is ubiquitous in many machine learning tasks. It aims to regularize the objective function by adding a penalization term that considers the constraints made on the learned parameters. This paper considers the problem of learning heavy-tailed LSP. We develop a flexible and robust sparse learning framework capable of handling heavy-tailed data with locally stationary behavior and propose concentration inequalities. We further provide non-asymptotic oracle inequalities for different types of sparsity, including $\\ell_1$-norm and total variation penalization for the least square loss.","authors":["Yingjie Wang","Mokhtar Z. Alaya","Salim Bouzebda","Xinsheng Liu"],"url":"https://arxiv.org/abs/2504.06477"}
{"created":"2025-04-11","title":"Continuous-Variable Quantum Encoding Techniques: A Comparative Study of Embedding Techniques and Their Impact on Machine Learning Performance","abstract":"This study explores the intersection of continuous-variable quantum computing (CVQC) and classical machine learning, focusing on CVQC data encoding techniques, including Displacement encoding and squeezing encoding, alongside Instantaneous Quantum Polynomial (IQP) encoding from discrete quantum computing. We perform an extensive empirical analysis to assess the impact of these encoding methods on classical machine learning models, such as Logistic Regression, Support Vector Machines, K-Nearest Neighbors, and ensemble methods like Random Forest and LightGBM. Our findings indicate that CVQC-based encoding methods significantly enhance feature expressivity, resulting in improved classification accuracy and F1 scores, especially in high-dimensional and complex datasets. However, these improvements come with varying computational costs, which depend on the complexity of the encoding and the architecture of the machine learning models. Additionally, we examine the trade-off between quantum expressibility and classical learnability, offering valuable insights into the practical feasibility of incorporating these quantum encodings into real-world applications. This study contributes to the growing body of research on quantum-classical hybrid learning, emphasizing the role of CVQC in advancing quantum data representation and its integration into classical machine learning workflows.","authors":["Minati Rath","Hema Date"],"url":"https://arxiv.org/abs/2504.06497"}
{"created":"2025-04-11","title":"Market, power, gift, and concession economies: Comparison using four-mode primitive network models","abstract":"Reducing wealth inequality is a global challenge, and the problems of capitalism stem from the enclosure of the commons and the breakdown of the community. According to previous studies by Polanyi, Karatani, and Graeber, economic modes can be divided into capitalist market economy (enclosure and exchange), power economy (de-enclosure and redistribution), gift economy (obligation to return and reciprocity), and concession economy (de-obligation to return). The concession economy reflects Graeber's baseline communism (from each according to their abilities, to each according to their needs) and Deguchi's We-turn philosophy (the \"I\" as an individual has a \"fundamental incapability\" and the subject of physical action, responsibility, and freedom is \"We\" as a multi-agent system, including the \"I\"). In this study, we constructed novel network models for these four modes and compared their properties (cluster coefficient, graph density, reciprocity, assortativity, centrality, and Gini coefficient). From the calculation results, it became clear that the market economy leads to inequality; the power economy mitigates inequality but cannot eliminate it; the gift and concession economies lead to a healthy and equal economy; and the concession economy, free from the ties of obligation to return, is possible without guaranteeing reciprocity. We intend to promote the transformation from a capitalist economy to a concession economy through activities that disseminate baseline communism and the We-turn philosophy that promotes concession, that is, developing a cooperative platform to support concession through information technology and empirical research through fieldwork.","authors":["Takeshi Kato","Junichi Miyakoshi","Misa Owa","Ryuji Mine"],"url":"https://arxiv.org/abs/2504.06557"}
{"created":"2025-04-11","title":"Diffusion Factor Models: Generating High-Dimensional Returns with Factor Structure","abstract":"Financial scenario simulation is essential for risk management and portfolio optimization, yet it remains challenging especially in high-dimensional and small data settings common in finance. We propose a diffusion factor model that integrates latent factor structure into generative diffusion processes, bridging econometrics with modern generative AI to address the challenges of the curse of dimensionality and data scarcity in financial simulation. By exploiting the low-dimensional factor structure inherent in asset returns, we decompose the score function--a key component in diffusion models--using time-varying orthogonal projections, and this decomposition is incorporated into the design of neural network architectures. We derive rigorous statistical guarantees, establishing nonasymptotic error bounds for both score estimation at O(d^{5/2} n^{-2/(k+5)}) and generated distribution at O(d^{5/4} n^{-1/2(k+5)}), primarily driven by the intrinsic factor dimension k rather than the number of assets d, surpassing the dimension-dependent limits in the classical nonparametric statistics literature and making the framework viable for markets with thousands of assets. Numerical studies confirm superior performance in latent subspace recovery under small data regimes. Empirical analysis demonstrates the economic significance of our framework in constructing mean-variance optimal portfolios and factor portfolios. This work presents the first theoretical integration of factor structure with diffusion models, offering a principled approach for high-dimensional financial simulation with limited data.","authors":["Minshuo Chen","Renyuan Xu","Yumin Xu","Ruixun Zhang"],"url":"https://arxiv.org/abs/2504.06566"}
{"created":"2025-04-11","title":"Recasting Arrow's Impossibility Theorem as G\\\"odelian Incomputability","abstract":"Incomputability results in formal logic and the Theory of Computation (i.e., incompleteness and undecidability) have deep implications for the foundations of mathematics and computer science. Likewise, Social Choice Theory, a branch of Welfare Economics, contains several impossibility results that place limits on the potential fairness, rationality and consistency of social decision-making processes. A formal relationship between G\\\"odel's Incompleteness Theorems in formal logic, and Arrow's Impossibility Theorem in Social Choice Theory has long been conjectured. In this paper, we address this gap by bringing these two theories closer by introducing a general mathematical object called a Self-Reference System. Impossibility in Social Choice Theory is demonstrated to correspond to the impossibility of a Self-Reference System to interpret its own internal consistency. We also provide a proof of G\\\"odel's First Incompleteness Theorem in the same terms. Together, this recasts Arrow's Impossibility Theorem as incomputability in the G\\\"odelian sense. The incomputability results in both fields are shown to arise out of self-referential paradoxes. This is exemplified by a new proof of Arrow's Impossibility Theorem centred around Condorcet Paradoxes.","authors":["Ori Livson","Mikhail Prokopenko"],"url":"https://arxiv.org/abs/2504.06589"}
{"created":"2025-04-11","title":"Image registration of 2D optical thin sections in a 3D porous medium: Application to a Berea sandstone digital rock image","abstract":"This study proposes a systematic image registration approach to align 2D optical thin-section images within a 3D digital rock volume. Using template image matching with differential evolution optimization, we identify the most similar 2D plane in 3D. The method is validated on a synthetic porous medium, achieving exact registration, and applied to Berea sandstone, where it achieves a structural similarity index (SSIM) of 0.990. With the registered images, we explore upscaling properties based on paired multimodal images, focusing on pore characteristics and effective elastic moduli. The thin-section image reveals 50 % more porosity and submicron pores than the registered CT plane. In addition, bulk and shear moduli from thin sections are 25 % and 30 % lower, respectively, than those derived from CT images. Beyond numerical comparisons, thin sections provide additional geological insights, including cementation, mineral phases, and weathering effects, which are not clear in CT images. This study demonstrates the potential of multimodal image registration to improve computed rock properties in digital rock physics by integrating complementary imaging modalities.","authors":["Jaehong Chung","Wei Cai","Tapan Mukerji"],"url":"https://arxiv.org/abs/2504.06604"}
{"created":"2025-04-11","title":"Quantum neural networks facilitating quantum state classification","abstract":"The classification of quantum states into distinct classes poses a significant challenge. In this study, we address this problem using quantum neural networks in combination with a problem-inspired circuit and customised as well as predefined ans\\\"{a}tz. To facilitate the resource-efficient quantum state classification, we construct the dataset of quantum states using the proposed problem-inspired circuit. The problem-inspired circuit incorporates two-qubit parameterised unitary gates of varying entangling power, which is further integrated with the ans\\\"{a}tz, developing an entire quantum neural network. To demonstrate the capability of the selected ans\\\"{a}tz, we visualise the mitigated barren plateaus. The designed quantum neural network demonstrates the efficiency in binary and multi-class classification tasks. This work establishes a foundation for the classification of multi-qubit quantum states and offers the potential for generalisation to multi-qubit pure quantum states.","authors":["Diksha Sharma","Vivek Balasaheb Sabale","Thirumalai M.","Atul Kumar"],"url":"https://arxiv.org/abs/2504.06622"}
{"created":"2025-04-11","title":"Robust Capacity Expansion Modelling for Renewable Energy Systems under Weather and Demand Uncertainty","abstract":"Future greenhouse gas neutral energy systems will be dominated by variable renewable energy technologies. However, renewable electricity generation from wind and solar technologies, as well as electricity demand, varies with the weather. This work addresses the problem of determining optimal capacities for renewable technologies in energy systems that ensure sufficient electricity supply when dealing with multi-year time-series data. An iterative algorithm is proposed that starts by optimising an arbitrary starting time-series, followed by adding additional constraints and reoptimising the modified optimisation problem until sufficient energy supply is provided for all time--series, i.e. the solution is robust to weather and demand variations. This is evaluated in a computational study on a German energy system model.The results show that the iterative algorithm finds robust solutions for an increase of 2-2.5% in total annual cost for a simplified model in gurobipy and 2.9% for a model built in the model framework ETHOS.FINE. Testing the feasibility for non robust solutions showed that supply gaps occurred in at least some of the remaining years. Based on the results of this work, ensuring feasibility within an energy system model for multiple time-series boils down to two factors: ensuring sufficient back-up capacity to overcome periods of high demand combined with low electricity generation from wind and photovoltaic, and enforcing sufficient total annual electricity generation. Our proposed open source iterative algorithm is able to ensure this. For general modelling, it is recommended to check for systematic effects of different years' time--series on energy system models especially for wind, but also for photovoltaics, include dark lull and cold period effects on generation and demand in time--series, and assess the feasibility of energy system models using different time-series.","authors":["Sebastian Kebrich","Felix Engelhardt","David Franzmann","Christina B\\\"using","Jochen Lin{\\ss}en","Heidi Heinrichs"],"url":"https://arxiv.org/abs/2504.06750"}
{"created":"2025-04-11","title":"DIMA: DIffusing Motion Artifacts for unsupervised correction in brain MRI images","abstract":"Motion artifacts remain a significant challenge in Magnetic Resonance Imaging (MRI), compromising diagnostic quality and potentially leading to misdiagnosis or repeated scans. Existing deep learning approaches for motion artifact correction typically require paired motion-free and motion-affected images for training, which are rarely available in clinical settings. To overcome this requirement, we present DIMA (DIffusing Motion Artifacts), a novel framework that leverages diffusion models to enable unsupervised motion artifact correction in brain MRI. Our two-phase approach first trains a diffusion model on unpaired motion-affected images to learn the distribution of motion artifacts. This model then generates realistic motion artifacts on clean images, creating paired datasets suitable for supervised training of correction networks. Unlike existing methods, DIMA operates without requiring k-space manipulation or detailed knowledge of MRI sequence parameters, making it adaptable across different scanning protocols and hardware. Comprehensive evaluations across multiple datasets and anatomical planes demonstrate that our method achieves comparable performance to state-of-the-art supervised approaches while offering superior generalizability to real clinical data. DIMA represents a significant advancement in making motion artifact correction more accessible for routine clinical use, potentially reducing the need for repeat scans and improving diagnostic accuracy.","authors":["Paolo Angella","Luca Balbi","Fabrizio Ferrando","Paolo Traverso","Rosario Varriale","Vito Paolo Pastore","Matteo Santacesaria"],"url":"https://arxiv.org/abs/2504.06767"}
{"created":"2025-04-11","title":"Hybrid machine learning models based on physical patterns to accelerate CFD simulations: a short guide on autoregressive models","abstract":"Accurate modeling of the complex dynamics of fluid flows is a fundamental challenge in computational physics and engineering. This study presents an innovative integration of High-Order Singular Value Decomposition (HOSVD) with Long Short-Term Memory (LSTM) architectures to address the complexities of reduced-order modeling (ROM) in fluid dynamics. HOSVD improves the dimensionality reduction process by preserving multidimensional structures, surpassing the limitations of Singular Value Decomposition (SVD). The methodology is tested across numerical and experimental data sets, including two- and three-dimensional (2D and 3D) cylinder wake flows, spanning both laminar and turbulent regimes. The emphasis is also on exploring how the depth and complexity of LSTM architectures contribute to improving predictive performance. Simpler architectures with a single dense layer effectively capture the periodic dynamics, demonstrating the network's ability to model non-linearities and chaotic dynamics. The addition of extra layers provides higher accuracy at minimal computational cost. These additional layers enable the network to expand its representational capacity, improving the prediction accuracy and reliability. The results demonstrate that HOSVD outperforms SVD in all tested scenarios, as evidenced by using different error metrics. Efficient mode truncation by HOSVD-based models enables the capture of complex temporal patterns, offering reliable predictions even in challenging, noise-influenced data sets. The findings underscore the adaptability and robustness of HOSVD-LSTM architectures, offering a scalable framework for modeling fluid dynamics.","authors":["Arindam Sengupta","Rodrigo Abad\\'ia-Heredia","Ashton Hetherington","Jos\\'e Miguel P\\'erez","Soledad Le Clainche"],"url":"https://arxiv.org/abs/2504.06774"}
{"created":"2025-04-11","title":"Probabilistic Grading and Classification System for End-of-Life Building Components Toward Circular Economy Loop","abstract":"The longevity and viability of construction components in a circular economy demand a robust, data-informed framework for reuse decision-making. This paper introduces a multi-level grading and classification system that combines Bayesian probabilistic modeling with scenario-based performance thresholds to assess the reusability of end-of-life modular components. By grading components across a five-tier scale, the system supports strategic decisions for reuse, up-use, or down-use, ensuring alignment with engineering standards and sustainability objectives. The model's development is grounded in empirical data from precast concrete wall panels, and its explainability is enhanced through decision tree logic and Sankey visualizations that trace the influence of contextual scenarios on classification outcomes. MGCS addresses the environmental, economic, and operational challenges of EoL management--reducing material waste, optimizing value recovery, and improving workflow efficiency. Through dynamic feature weighting and transparent reasoning, the system offers a practical yet rigorous pathway to embed circular thinking into construction industry practices.","authors":["Yiping Meng","Sergio Cavalaro","Mohamed Osmani"],"url":"https://arxiv.org/abs/2504.06782"}
{"created":"2025-04-11","title":"Mass Balance Approximation of Unfolding Improves Potential-Like Methods for Protein Stability Predictions","abstract":"The prediction of protein stability changes following single-point mutations plays a pivotal role in computational biology, particularly in areas like drug discovery, enzyme reengineering, and genetic disease analysis. Although deep-learning strategies have pushed the field forward, their use in standard workflows remains limited due to resource demands. Conversely, potential-like methods are fast, intuitive, and efficient. Yet, these typically estimate Gibbs free energy shifts without considering the free-energy variations in the unfolded protein state, an omission that may breach mass balance and diminish accuracy. This study shows that incorporating a mass-balance correction (MBC) to account for the unfolded state significantly enhances these methods. While many machine learning models partially model this balance, our analysis suggests that a refined representation of the unfolded state may improve the predictive performance.","authors":["Ivan Rossi","Guido Barducci","Tiziana Sanavia","Paola Turina","Emidio Capriotti","Piero Fariselli"],"url":"https://arxiv.org/abs/2504.06806"}
{"created":"2025-04-11","title":"Green building blocks reveal the complex anatomy of climate change mitigation technologies","abstract":"Climate-change mitigating innovation is considered essential for the world's transition toward a sustainable global economy. To guide this transition, integrated assessment models map sectoral emissions reduction targets into long-term trajectories towards carbon neutrality at the macro-level, while detailed engineering studies at the micro-level develop concrete carbon-mitigation technologies tailored to individual industries. However, we lack a meso-level understanding of how solutions connect across technological domains. Building on the notion that innovating often entails combining existing technologies in new ways, we identify Green Building Blocks (GBBs): modules of technologies that can be added to nongreen technologies to mitigate their climate-change impact. Using natural language processing and dimensionality reduction techniques, we show how GBBs can be extracted from large-scale patent data. Next, we describe the anatomy of the green transition as a network that connects nongreen technologies to GBBs. This network has a nontrivial structure: whereas some nongreen technologies can connect to various GBBs, opening up a variety of ways to mitigate their impact on the global climate, other nongreen technologies only connect to a single GBB. Similarly, some GBBs are general purpose technologies that can reduce green house gases in a vast range of applications, whereas others are tailored to specific use cases. Furthermore, GBBs prove predictive of the green technologies that firms develop, allowing us to map the green capabilities of firms not in terms of the specific green technological solutions they invent, but in terms of their capacity to develop broader classes of solutions with the GBBs they possess.","authors":["Yang Li","Frank Neffke"],"url":"https://arxiv.org/abs/2504.06834"}
{"created":"2025-04-11","title":"Discrete-to-continuum limit for nonlinear reaction-diffusion systems via EDP convergence for gradient systems","abstract":"We investigate the convergence of spatial discretizations for reaction-diffusion systems with mass-action law satisfying a detailed balance condition. Considering systems on the d-dimensional torus, we construct appropriate space-discrete processes and show convergence not only on the level of solutions, but also on the level of the gradient systems governing the evolutions. As an important step, we prove chain rule inequalities for the reaction-diffusion systems as well as their discretizations, featuring a non-convex dissipation functional. The convergence is then obtained with variational methods by building on the recently introduced notion of gradient systems in continuity equation format.","authors":["Georg Heinze","Alexander Mielke","Artur Stephan"],"url":"https://arxiv.org/abs/2504.06837"}
{"created":"2025-04-11","title":"CRYSIM: Prediction of Symmetric Structures of Large Crystals with GPU-based Ising Machines","abstract":"Solving black-box optimization problems with Ising machines is increasingly common in materials science. However, their application to crystal structure prediction (CSP) is still ineffective due to symmetry agnostic encoding of atomic coordinates. We introduce CRYSIM, an algorithm that encodes the space group, the Wyckoff positions combination, and coordinates of independent atomic sites as separate variables. This encoding reduces the search space substantially by exploiting the symmetry in space groups. When CRYSIM is interfaced to Fixstars Amplify, a GPU-based Ising machine, its prediction performance was competitive with CALYPSO and Bayesian optimization for crystals containing more than 150 atoms in a unit cell. Although it is not realistic to interface CRYSIM to current small-scale quantum devices, it has the potential to become the standard CSP algorithm in the coming quantum age.","authors":["Chen Liang","Diptesh Das","Jiang Guo","Ryo Tamura","Zetian Mao","Koji Tsuda"],"url":"https://arxiv.org/abs/2504.06878"}
{"created":"2025-04-11","title":"Leveraging Anatomical Priors for Automated Pancreas Segmentation on Abdominal CT","abstract":"An accurate segmentation of the pancreas on CT is crucial to identify pancreatic pathologies and extract imaging-based biomarkers. However, prior research on pancreas segmentation has primarily focused on modifying the segmentation model architecture or utilizing pre- and post-processing techniques. In this article, we investigate the utility of anatomical priors to enhance the segmentation performance of the pancreas. Two 3D full-resolution nnU-Net models were trained, one with 8 refined labels from the public PANORAMA dataset, and another that combined them with labels derived from the public TotalSegmentator (TS) tool. The addition of anatomical priors resulted in a 6\\% increase in Dice score ($p < .001$) and a 36.5 mm decrease in Hausdorff distance for pancreas segmentation ($p < .001$). Moreover, the pancreas was always detected when anatomy priors were used, whereas there were 8 instances of failed detections without their use. The use of anatomy priors shows promise for pancreas segmentation and subsequent derivation of imaging biomarkers.","authors":["Anisa V. Prasad","Tejas Sudharshan Mathai","Pritam Mukherjee","Jianfei Liu","Ronald M. Summers"],"url":"https://arxiv.org/abs/2504.06921"}
{"created":"2025-04-11","title":"Longitudinal Assessment of Lung Lesion Burden in CT","abstract":"In the U.S., lung cancer is the second major cause of death. Early detection of suspicious lung nodules is crucial for patient treatment planning, management, and improving outcomes. Many approaches for lung nodule segmentation and volumetric analysis have been proposed, but few have looked at longitudinal changes in total lung tumor burden. In this work, we trained two 3D models (nnUNet) with and without anatomical priors to automatically segment lung lesions and quantified total lesion burden for each patient. The 3D model without priors significantly outperformed ($p < .001$) the model trained with anatomy priors. For detecting clinically significant lesions $>$ 1cm, a precision of 71.3\\%, sensitivity of 68.4\\%, and F1-score of 69.8\\% was achieved. For segmentation, a Dice score of 77.1 $\\pm$ 20.3 and Hausdorff distance error of 11.7 $\\pm$ 24.1 mm was obtained. The median lesion burden was 6.4 cc (IQR: 2.1, 18.1) and the median volume difference between manual and automated measurements was 0.02 cc (IQR: -2.8, 1.2). Agreements were also evaluated with linear regression and Bland-Altman plots. The proposed approach can produce a personalized evaluation of the total tumor burden for a patient and facilitate interval change tracking over time.","authors":["Tejas Sudharshan Mathai","Benjamin Hou","Ronald M. Summers"],"url":"https://arxiv.org/abs/2504.06924"}
{"created":"2025-04-11","title":"Maximizing Battery Storage Profits via High-Frequency Intraday Trading","abstract":"Maximizing revenue for grid-scale battery energy storage systems in continuous intraday electricity markets requires strategies that are able to seize trading opportunities as soon as new information arrives. This paper introduces and evaluates an automated high-frequency trading strategy for battery energy storage systems trading on the intraday market for power while explicitly considering the dynamics of the limit order book, market rules, and technical parameters. The standard rolling intrinsic strategy is adapted for continuous intraday electricity markets and solved using a dynamic programming approximation that is two to three orders of magnitude faster than an exact mixed-integer linear programming solution. A detailed backtest over a full year of German order book data demonstrates that the proposed dynamic programming formulation does not reduce trading profits and enables the policy to react to every relevant order book update, enabling realistic rapid backtesting. Our results show the significant revenue potential of high-frequency trading: our policy earns 58% more than when re-optimizing only once every hour and 14% more than when re-optimizing once per minute, highlighting that profits critically depend on trading speed. Furthermore, we leverage the speed of our algorithm to train a parametric extension of the rolling intrinsic, increasing yearly revenue by 8.4% out of sample.","authors":["David Schaurecker","David Wozabal","Nils L\\\"ohndorf","Thorsten Staake"],"url":"https://arxiv.org/abs/2504.06932"}
{"created":"2025-04-11","title":"More-efficient Quantum Multivariate Mean Value Estimator from Generalized Grover Gate","abstract":"In this work, we present an efficient algorithm for multivariate mean value estimation. Our algorithm outperforms previous work by polylog factors and nearly saturates the known lower bound. We find an algorithm that uses $O\\left(n \\log \\frac{d}{\\delta}\\right)$ to achieve precision $\\frac{\\sqrt{\\text{tr } \\Sigma}}{n}$ in $\\lVert \\rVert_\\infty$ norm and hence $\\frac{\\sqrt{d \\text{ tr } \\Sigma}}{n}$ in $\\lVert \\rVert_2$ norm, where $d$ is the dimension and $\\Sigma$ is the covariance matrix. We also presented another algorithm that uses smaller memory but costs an extra $d^\\frac{1}{4}$ in complexity. The idea originates from the previous observations that the Grover gate, when generalized to allow for arbitrary phases instead of $\\pm 1$, becomes a good mean value estimator in some mathematical notion. The only remaining $\\log \\frac{d}{\\delta}$ as opposed to $\\log \\frac{1}{\\delta}$ is due to the phase estimation primitive we employed, which so far is the only major known method to tackle the problem. Our results demonstrates that our methodology with generalized Grover gate can be used locate the optimal algorithm, without polylog overhead, for different taks relating to mean value estimation.","authors":["Letian Tang"],"url":"https://arxiv.org/abs/2504.06940"}
{"created":"2025-04-11","title":"GLT hidden structures in mean-field quantum spin systems","abstract":"This work explores structured matrix sequences arising in mean-field quantum spin systems. We express these sequences within the framework of generalized locally Toeplitz (GLT) $*$-algebras, leveraging the fact that each GLT matrix sequence has a unique GLT symbol. This symbol characterizes both the asymptotic singular value distribution and, for Hermitian or quasi-Hermitian sequences, the asymptotic spectral distribution. Specifically, we analyze two cases of real symmetric matrix sequences stemming from mean-field quantum spin systems and determine their associated distributions using GLT theory. Our study concludes with visualizations and numerical tests that validate the theoretical findings, followed by a discussion of open problems and future directions.","authors":["Christiaan J. F. van de Ven","Muhammad Faisal Khan","S. Serra-Capizzano"],"url":"https://arxiv.org/abs/2504.06951"}
{"created":"2025-04-11","title":"Parametric Reachable Sets Via Controlled Dynamical Embeddings","abstract":"In this work, we propose a new framework for reachable set computation through continuous evolution of a set of parameters and offsets which define a parametope, through the intersection of constraints. This results in a dynamical approach towards nonlinear reachability analysis: a single trajectory of an embedding system provides a parametope reachable set for the original system, and uncertainties are accounted for through continuous parameter evolution. This is dual to most existing computational strategies, which define sets through some combination of generator vectors, and usually discretize the system dynamics. We show how, under some regularity assumptions of the dynamics and the set considered, any desired parameter evolution can be accommodated as long as the offset dynamics are set accordingly, providing a virtual \"control input\" for reachable set computation. In a special case of the theory, we demonstrate how closing the loop for the parameter dynamics using the adjoint of the linearization results in a desirable first-order cancellation of the original system dynamics. Using interval arithmetic in JAX, we demonstrate the efficiency and utility of reachable parametope computation through two numerical examples.","authors":["Akash Harapanahalli","Samuel Coogan"],"url":"https://arxiv.org/abs/2504.06955"}
{"created":"2025-04-11","title":"RNN-Transducer-based Losses for Speech Recognition on Noisy Targets","abstract":"Training speech recognition systems on noisy transcripts is a significant challenge in industrial pipelines, where datasets are enormous and ensuring accurate transcription for every instance is difficult. In this work, we introduce novel loss functions to mitigate the impact of transcription errors in RNN-Transducer models. Our Star-Transducer loss addresses deletion errors by incorporating \"skip frame\" transitions in the loss lattice, restoring over 90% of the system's performance compared to models trained with accurate transcripts. The Bypass-Transducer loss uses \"skip token\" transitions to tackle insertion errors, recovering more than 60% of the quality. Finally, the Target-Robust Transducer loss merges these approaches, offering robust performance against arbitrary errors. Experimental results demonstrate that the Target-Robust Transducer loss significantly improves RNN-T performance on noisy data by restoring over 70% of the quality compared to well-transcribed data.","authors":["Vladimir Bataev"],"url":"https://arxiv.org/abs/2504.06963"}
{"created":"2025-04-11","title":"Optimal promotions of new products on networks","abstract":"We present a novel methodology for analyzing the optimal promotion in the Bass model for the spreading of new products on networks. For general networks with $M$ nodes, the optimal promotion is the solution of $2^M-1$ nonlinearly-coupled boundary-value problems. On structured networks, however, the number of equations can be reduced to a manageable size which is amendable to simulations and analysis. This enables us to gain insight into the effect of the network structure on optimal promotions. We find that the optimal advertising strategy decreases with time, whereas the optimal boosting of peer effects increases from zero and then decreases. In low-degree networks, it is optimal to prioritize advertising over boosting peer effects, but this relation is flipped in high-degree networks. When the planning horizon is finite, the optimal promotion continues until the last minute, as opposed to an infinite planning horizon where the optimal promotion decays to zero. Finally, promotions with short planning horizons can yield an order of magnitude higher increase of profits, compared to those with long planning horizons.","authors":["Gadi Fibich","Amit Golan"],"url":"https://arxiv.org/abs/2504.06967"}
{"created":"2025-04-11","title":"Artificial Intelligence for Pediatric Height Prediction Using Large-Scale Longitudinal Body Composition Data","abstract":"This study developed an accurate artificial intelligence model for predicting future height in children and adolescents using anthropometric and body composition data from the GP Cohort Study (588,546 measurements from 96,485 children aged 7-18). The model incorporated anthropometric measures, body composition, standard deviation scores, and growth velocity parameters, with performance evaluated using RMSE, MAE, and MAPE. Results showed high accuracy with males achieving average RMSE, MAE, and MAPE of 2.51 cm, 1.74 cm, and 1.14%, and females showing 2.28 cm, 1.68 cm, and 1.13%, respectively. Explainable AI approaches identified height SDS, height velocity, and soft lean mass velocity as crucial predictors. The model generated personalized growth curves by estimating individual-specific height trajectories, offering a robust tool for clinical decision support, early identification of growth disorders, and optimization of growth outcomes.","authors":["Dohyun Chun","Hae Woon Jung","Jongho Kang","Woo Young Jang","Jihun Kim"],"url":"https://arxiv.org/abs/2504.06979"}
{"created":"2025-04-11","title":"Quasipolynomial bounds for the corners theorem","abstract":"Let $G$ be a finite abelian group and $A$ be a subset of $G \\times G$ which is corner-free, meaning that there are no $x, y \\in G$ and $d \\in G \\setminus \\{0\\}$ such that $(x, y)$, $(x+d, y)$, $(x, y+d) \\in A$. We prove that \\[|A| \\le |G|^2 \\cdot \\exp(-(\\log |G|)^{\\Omega(1)}).\\] As a consequence, we obtain polynomial (in the input length) lower bounds on the non-deterministic communication complexity of Exactly-N in the 3-player Number-on-Forehead model. We also obtain the first \"reasonable'' lower bounds on the coloring version of the $3$-dimensional corners problem and equivalently the deterministic communication complexity of Exactly-N in the 4-player Number-on-Forehead model.","authors":["Michael Jaber","Yang P. Liu","Shachar Lovett","Anthony Ostuni","Mehtaab Sawhney"],"url":"https://arxiv.org/abs/2504.07006"}
{"created":"2025-04-11","title":"Assumption-free fidelity bounds for hardware noise characterization","abstract":"In the Quantum Supremacy regime, quantum computers may overcome classical machines on several tasks if we can estimate, mitigate, or correct unavoidable hardware noise. Estimating the error requires classical simulations, which become unfeasible in the Quantum Supremacy regime. We leverage Machine Learning data-driven approaches and Conformal Prediction, a Machine Learning uncertainty quantification tool known for its mild assumptions and finite-sample validity, to find theoretically valid upper bounds of the fidelity between noiseless and noisy outputs of quantum devices. Under reasonable extrapolation assumptions, the proposed scheme applies to any Quantum Computing hardware, does not require modeling the device's noise sources, and can be used when classical simulations are unavailable, e.g. in the Quantum Supremacy regime.","authors":["Nicolo Colombo"],"url":"https://arxiv.org/abs/2504.07010"}
{"created":"2025-04-11","title":"Computably discrete represented spaces","abstract":"In computable topology, a represented space is called computably discrete if its equality predicate is semidecidable. While any such space is classically isomorphic to an initial segment of the natural numbers, the computable-isomorphism types of computably discrete represented spaces exhibit a rich structure. We show that the widely studied class of computably enumerable equivalence relations (ceers) corresponds precisely to the computably Quasi-Polish computably discrete spaces. We employ computably discrete spaces to exhibit several separating examples in computable topology. We construct a computably discrete computably Quasi-Polish space admitting no decidable properties, a computably discrete and computably Hausdorff precomputably Quasi-Polish space admitting no computable injection into the natural numbers, a two-point space which is computably Hausdorff but not computably discrete, and a two-point space which is computably discrete but not computably Hausdorff. We further expand an example due to Weihrauch that separates computably regular spaces from computably normal spaces.","authors":["Eike Neumann","Arno Pauly","C\\'ecilia Pradic","Manlio Valenti"],"url":"https://arxiv.org/abs/2504.07020"}
{"created":"2025-04-11","title":"Enhancing Downstream Analysis in Genome Sequencing: Species Classification While Basecalling","abstract":"The ability to quickly and accurately identify microbial species in a sample, known as metagenomic profiling, is critical across various fields, from healthcare to environmental science. This paper introduces a novel method to profile signals coming from sequencing devices in parallel with determining their nucleotide sequences, a process known as basecalling, via a multi-objective deep neural network for simultaneous basecalling and multi-class genome classification. We introduce a new loss strategy where losses for basecalling and classification are back-propagated separately, with model weights combined for the shared layers, and a pre-configured ranking strategy allowing top-K species accuracy, giving users flexibility to choose between higher accuracy or higher speed at identifying the species. We achieve state-of-the-art basecalling accuracies, while classification accuracies meet and exceed the results of state-of-the-art binary classifiers, attaining an average of 92.5%/98.9% accuracy at identifying the top-1/3 species among a total of 17 genomes in the Wick bacterial dataset. The work presented here has implications for future studies in metagenomic profiling by accelerating the bottleneck step of matching the DNA sequence to the correct genome.","authors":["Riselda Kodra","Hadjer Benmeziane","Irem Boybat","William Andrew Simon"],"url":"https://arxiv.org/abs/2504.07065"}
{"created":"2025-04-11","title":"Quantum Reverse Shannon Theorem Revisited","abstract":"Reverse Shannon theorems concern the use of noiseless channels to simulate noisy ones. This is dual to the usual noisy channel coding problem, where a noisy (classical or quantum) channel is used to simulate a noiseless one. The Quantum Reverse Shannon Theorem is extensively studied by Bennett and co-authors in [IEEE Trans. Inf. Theory, 2014]. They present two distinct theorems, each tailored to classical and quantum channel simulations respectively, explaining the fact that these theorems remain incomparable due to the fundamentally different nature of correlations they address. The authors leave as an open question the challenge of formulating a unified theorem that could encompass the principles of both and unify them. We unify these two theorems into a single, comprehensive theorem, extending it to the most general case by considering correlations with a general mixed-state reference system. Furthermore, we unify feedback and non-feedback theorems by simulating a general side information system at the encoder side.","authors":["Zahra Baghali Khanian","Debbie Leung"],"url":"https://arxiv.org/abs/2504.07068"}
{"created":"2025-04-11","title":"Between the deterministic and non-deterministic query complexity","abstract":"We consider problems that can be solved by asking certain queries. The deterministic query complexity $D(P,n)$ of a problem $P$ is the smallest number of queries needed to ask in order to find the solution with an input of size $n$ (in the worst case), while the non-deterministic query complexity $D_0(P,n)$ is the smallest number of queries needed to ask, in case we know the solution, to prove that it is indeed the solution (in the worst case). Equivalently, $D(P,n)$ is the largest number of queries needed to find the solution in case an Adversary is answering the queries, while $D_0(P)$ is the largest number of queries needed to find the solution in case an Adversary chooses the input. We define a series of quantities between these two values, $D_k(P,n)$ is the largest number of queries needed to find the solution in case an Adversary chooses the input, and answers the queries, but he can change the input at most $k$ times. We give bounds on $D_k(P,n)$ for various problems $P$.","authors":["D\\'aniel Gerbner"],"url":"https://arxiv.org/abs/1907.09531"}
{"created":"2025-04-11","title":"PyIT2FLS: A New Python Toolkit for Interval Type 2 Fuzzy Logic Systems","abstract":"Fuzzy logic is an accepted and well-developed approach for constructing verbal models. Fuzzy based methods are getting more popular, while the engineers deal with more daily life tasks. This paper presents a new Python toolkit for Interval Type 2 Fuzzy Logic Systems (IT2FLS). Developing software tools is an important issue for facilitating the practical use of theoretical results. There are limited tools for implementing IT2FLSs in Python. The developed PyIT2FLS is providing a set of tools for fast and easy modeling of fuzzy systems. This paper includes a brief description of how developed toolkit can be used. Also, three examples are given showing the usage of the developed toolkit for simulating IT2FLSs. First, a simple rule-based system is developed and it's codes are presented in the paper. The second example is the prediction of the Mackey-Glass chaotic time series using IT2FLS. In this example, the Particle Swarm Optimization (PSO) algorithm is used for determining system parameters while minimizing the mean square error. In the last example, an IT2FPID is designed and used for controlling a linear time-delay system. The code for the examples are available on toolkit's GitHub page: https://github.com/Haghrah/PyIT2FLS. The simulations and their results confirm the ability of the developed toolkit to be used in a wide range of the applications.","authors":["Amir Arslan Haghrah","Sehraneh Ghaemi"],"url":"https://arxiv.org/abs/1909.10051"}
{"created":"2025-04-11","title":"Fast Convex Optimization for Two-Layer ReLU Networks: Equivalent Model Classes and Cone Decompositions","abstract":"We develop fast algorithms and robust software for convex optimization of two-layer neural networks with ReLU activation functions. Our work leverages a convex reformulation of the standard weight-decay penalized training problem as a set of group-$\\ell_1$-regularized data-local models, where locality is enforced by polyhedral cone constraints. In the special case of zero-regularization, we show that this problem is exactly equivalent to unconstrained optimization of a convex \"gated ReLU\" network with non-singular gates. For problems with non-zero regularization, we show that convex gated ReLU models obtain data-dependent approximation bounds for the ReLU training problem. To optimize the convex reformulations, we develop an accelerated proximal gradient method and a practical augmented Lagrangian solver. We show that these approaches are faster than standard training heuristics for the non-convex problem, such as SGD, and outperform commercial interior-point solvers. Experimentally, we verify our theoretical results, explore the group-$\\ell_1$ regularization path, and scale convex optimization for neural networks to image classification on MNIST and CIFAR-10.","authors":["Aaron Mishkin","Arda Sahiner","Mert Pilanci"],"url":"https://arxiv.org/abs/2202.01331"}
{"created":"2025-04-11","title":"Physical configurations of a cell doublet with line tension, a theoretical study","abstract":"As a first approximation, early embryos may be modeled as foams whose shape depends on the surface tensions of each cell. However it has been remarked that exist line tensions at polarized exterior cellular interfaces (apical). In order to understand the changes it may imply on the usual foam model, a simple case study is considered: a double cell with line tension. Phase diagrams, bifurcations, possible new configurations are studied.","authors":["Fabrice Delbary"],"url":"https://arxiv.org/abs/2209.03098"}
{"created":"2025-04-11","title":"Mitigating Adversarial Effects of False Data Injection Attacks in Power Grid","abstract":"Deep Neural Networks have proven to be highly accurate at a variety of tasks in recent years. The benefits of Deep Neural Networks have also been embraced in power grids to detect False Data Injection Attacks (FDIA) while conducting critical tasks like state estimation. However, the vulnerabilities of DNNs along with the distinct infrastructure of the cyber-physical-system (CPS) can favor the attackers to bypass the detection mechanism. Moreover, the divergent nature of CPS engenders limitations to the conventional defense mechanisms for False Data Injection Attacks. In this paper, we propose a DNN framework with an additional layer that utilizes randomization to mitigate the adversarial effect by padding the inputs. The primary advantage of our method is when deployed to a DNN model it has a trivial impact on the model's performance even with larger padding sizes. We demonstrate the favorable outcome of the framework through simulation using the IEEE 14-bus, 30-bus, 118-bus, and 300-bus systems. Furthermore to justify the framework we select attack techniques that generate subtle adversarial examples that can bypass the detection mechanism effortlessly.","authors":["Farhin Farhad Riya","Shahinul Hoque","Yingyuan Yang","Jiangnan Li","Jinyuan Stella Sun","Hairong Qi"],"url":"https://arxiv.org/abs/2301.12487"}
{"created":"2025-04-11","title":"PointNorm-Net: Self-Supervised Normal Prediction of 3D Point Clouds via Multi-Modal Distribution Estimation","abstract":"Although supervised deep normal estimators have recently shown impressive results on synthetic benchmarks, their performance deteriorates significantly in real-world scenarios due to the domain gap between synthetic and real data. Building high-quality real training data to boost those supervised methods is not trivial because point-wise annotation of normals for varying-scale real-world 3D scenes is a tedious and expensive task. This paper introduces PointNorm-Net, the first self-supervised deep learning framework to tackle this challenge. The key novelty of PointNorm-Net is a three-stage multi-modal normal distribution estimation paradigm that can be integrated into either deep or traditional optimization-based normal estimation frameworks. Extensive experiments show that our method achieves superior generalization and outperforms state-of-the-art conventional and deep learning approaches across three real-world datasets that exhibit distinct characteristics compared to the synthetic training data.","authors":["Jie Zhang","Minghui Nie","Changqing Zou","Jian Liu","Ligang Liu","Junjie Cao"],"url":"https://arxiv.org/abs/2304.04884"}
{"created":"2025-04-11","title":"LostPaw: Finding Lost Pets using a Contrastive Learning-based Transformer with Visual Input","abstract":"Losing pets can be highly distressing for pet owners, and finding a lost pet is often challenging and time-consuming. An artificial intelligence-based application can significantly improve the speed and accuracy of finding lost pets. To facilitate such an application, this study introduces a contrastive neural network model capable of accurately distinguishing between images of pets. The model was trained on a large dataset of dog images and evaluated through 3-fold cross-validation. Following 350 epochs of training, the model achieved a test accuracy of 90%. Furthermore, overfitting was avoided, as the test accuracy closely matched the training accuracy. Our findings suggest that contrastive neural network models hold promise as a tool for locating lost pets. This paper presents the foundational framework for a potential web application designed to assist users in locating their missing pets. The application will allow users to upload images of their lost pets and provide notifications when matching images are identified within its image database. This functionality aims to enhance the efficiency and accuracy with which pet owners can search for and reunite with their beloved animals.","authors":["Andrei Voinea","Robin Kock","Maruf A. Dhali"],"url":"https://arxiv.org/abs/2304.14765"}
{"created":"2025-04-11","title":"Oil Spill Segmentation using Deep Encoder-Decoder models","abstract":"Crude oil is an integral component of the world economy and transportation sectors. With the growing demand for crude oil due to its widespread applications, accidental oil spills are unfortunate yet unavoidable. Even though oil spills are difficult to clean up, the first and foremost challenge is to detect them. In this research, the authors test the feasibility of deep encoder-decoder models that can be trained effectively to detect oil spills remotely. The work examines and compares the results from several segmentation models on high dimensional satellite Synthetic Aperture Radar (SAR) image data to pave the way for further in-depth research. Multiple combinations of models are used to run the experiments. The best-performing model is the one with the ResNet-50 encoder and DeepLabV3+ decoder. It achieves a mean Intersection over Union (IoU) of 64.868% and an improved class IoU of 61.549% for the ``oil spill\" class when compared with the previous benchmark model, which achieved a mean IoU of 65.05% and a class IoU of 53.38% for the ``oil spill\" class.","authors":["Abhishek Ramanathapura Satyanarayana","Maruf A. Dhali"],"url":"https://arxiv.org/abs/2305.01386"}
{"created":"2025-04-11","title":"Similarity of Neural Network Models: A Survey of Functional and Representational Measures","abstract":"Measuring similarity of neural networks to understand and improve their behavior has become an issue of great importance and research interest. In this survey, we provide a comprehensive overview of two complementary perspectives of measuring neural network similarity: (i) representational similarity, which considers how activations of intermediate layers differ, and (ii) functional similarity, which considers how models differ in their outputs. In addition to providing detailed descriptions of existing measures, we summarize and discuss results on the properties of and relationships between these measures, and point to open research problems. We hope our work lays a foundation for more systematic research on the properties and applicability of similarity measures for neural network models.","authors":["Max Klabunde","Tobias Schumacher","Markus Strohmaier","Florian Lemmerich"],"url":"https://arxiv.org/abs/2305.06329"}
{"created":"2025-04-11","title":"SIGMA: An Efficient Heterophilous Graph Neural Network with Fast Global Aggregation","abstract":"Graph neural networks (GNNs) realize great success in graph learning but suffer from performance loss when meeting heterophily, i.e. neighboring nodes are dissimilar, due to their local and uniform aggregation. Existing attempts of heterophilous GNNs incorporate long-range or global aggregations to distinguish nodes in the graph. However, these aggregations usually require iteratively maintaining and updating full-graph information, which limits their efficiency when applying to large-scale graphs. In this paper, we propose SIGMA, an efficient global heterophilous GNN aggregation integrating the structural similarity measurement SimRank. Our theoretical analysis illustrates that SIGMA inherently captures distant global similarity even under heterophily, that conventional approaches can only achieve after iterative aggregations. Furthermore, it enjoys efficient one-time computation with a complexity only linear to the node set size $\\mathcal{O}(n)$. Comprehensive evaluation demonstrates that SIGMA achieves state-of-the-art performance with superior aggregation and overall efficiency. Notably, it obtains $5\\times$ acceleration on the large-scale heterophily dataset pokec with over 30 million edges compared to the best baseline aggregation.","authors":["Haoyu Liu","Ningyi Liao","Siqiang Luo"],"url":"https://arxiv.org/abs/2305.09958"}
{"created":"2025-04-11","title":"GBG++: A Fast and Stable Granular Ball Generation Method for Classification","abstract":"Granular ball computing (GBC), as an efficient, robust, and scalable learning method, has become a popular research topic of granular computing. GBC includes two stages: granular ball generation (GBG) and multi-granularity learning based on the granular ball (GB). However, the stability and efficiency of existing GBG methods need to be further improved due to their strong dependence on $k$-means or $k$-division. In addition, GB-based classifiers only unilaterally consider the GB's geometric characteristics to construct classification rules, but the GB's quality is ignored. Therefore, in this paper, based on the attention mechanism, a fast and stable GBG (GBG++) method is proposed first. Specifically, the proposed GBG++ method only needs to calculate the distances from the data-driven center to the undivided samples when splitting each GB instead of randomly selecting the center and calculating the distances between it and all samples. Moreover, an outlier detection method is introduced to identify local outliers. Consequently, the GBG++ method can significantly improve effectiveness, robustness, and efficiency while being absolutely stable. Second, considering the influence of the sample size within the GB on the GB's quality, based on the GBG++ method, an improved GB-based $k$-nearest neighbors algorithm (GB$k$NN++) is presented, which can reduce misclassification at the class boundary. Finally, the experimental results indicate that the proposed method outperforms several existing GB-based classifiers and classical machine learning classifiers on $24$ public benchmark datasets. The implementation code of experiments is available at https://github.com/CherylTse/GBG-plusplus.","authors":["Qin Xie","Qinghua Zhang","Shuyin Xia","Fan Zhao","Chengying Wu","Guoyin Wang","Weiping Ding"],"url":"https://arxiv.org/abs/2305.18450"}
{"created":"2025-04-11","title":"Navigating Explanatory Multiverse Through Counterfactual Path Geometry","abstract":"Counterfactual explanations are the de facto standard when tasked with interpreting decisions of (opaque) predictive models. Their generation is often subject to technical and domain-specific constraints that aim to maximise their real-life utility. In addition to considering desiderata pertaining to the counterfactual instance itself, guaranteeing existence of a viable path connecting it with the factual data point has recently gained relevance. While current explainability approaches ensure that the steps of such a journey as well as its destination adhere to selected constraints, they neglect the multiplicity of these counterfactual paths. To address this shortcoming we introduce the novel concept of explanatory multiverse that encompasses all the possible counterfactual journeys. We define it using vector spaces, showing how to navigate, reason about and compare the geometry of counterfactual trajectories found within it. To this end, we overview their spatial properties -- such as affinity, branching, divergence and possible future convergence -- and propose an all-in-one metric, called opportunity potential, to quantify them. Notably, the explanatory process offered by our method grants explainees more agency by allowing them to select counterfactuals not only based on their absolute differences but also according to the properties of their connecting paths. To demonstrate real-life flexibility, benefit and efficacy of explanatory multiverse we propose its graph-based implementation, which we use for qualitative and quantitative evaluation on six tabular and image data sets.","authors":["Kacper Sokol","Edward Small","Yueqing Xuan"],"url":"https://arxiv.org/abs/2306.02786"}
{"created":"2025-04-11","title":"A Global Analysis of the Primal-Dual Method for Pliable Families","abstract":"We study a core algorithmic problem in network design called ${F}$-augmentation that involves increasing the connectivity of a given family of cuts ${F}$. Over 30 years ago, Williamson et al. (STOC `93) provided a 2-approximation primal-dual algorithm when ${F}$ is a so-called uncrossable family but extending their results to families that are non-uncrossable has remained a challenging question.","authors":["Ishan Bansal"],"url":"https://arxiv.org/abs/2308.15714"}
{"created":"2025-04-11","title":"Abstract Fractional Cauchy Problem: Existence of Propagators and Inhomogeneous Solution Representation","abstract":"We consider a Cauchy problem for the inhomogeneous differential equation given in terms of an unbounded linear operator $A$ and the Caputo fractional derivative of order $\\alpha \\in (0, 2)$ in time. The previously known representation of the mild solution to such a problem does not have a conventional variation-of-constants like form, with the propagator derived from the associated homogeneous problem. Instead, it relies on the existence of two propagators with different analytical properties. This fact limits theoretical and especially numerical applicability of the existing solution representation. Here, we propose an alternative representation of the mild solution to the given problem, that consolidates the solution formulas for sub-parabolic, parabolic and sub-hyperbolic equations with a positive sectorial operator $A$ and non-zero initial data. The new representation is solely based on the propagator of the homogeneous problem and, therefore, can be considered as a more natural fractional extension of the solution to the classical parabolic Cauchy problem. By exploiting a trade-off between the regularity assumptions on the initial data in terms of the fractional powers of $A$ and the regularity assumptions on the right-hand side in time, we show that the proposed solution formula is strongly convergent for $t \\geq 0$ under considerably weaker assumptions compared to the standard results from the literature. Crucially, the achieved relaxation of space regularity assumptions ensures that the new solution representation is practically feasible for any $\\alpha \\in (0, 2)$ and is amenable to the numerical evaluation using uniformly accurate quadrature-based algorithms.","authors":["Dmytro Sytnyk","Barbara Wohlmuth"],"url":"https://arxiv.org/abs/2308.16081"}
{"created":"2025-04-11","title":"A primal-dual perspective for distributed TD-learning","abstract":"The goal of this paper is to investigate distributed temporal difference (TD) learning for a networked multi-agent Markov decision process. The proposed approach is based on distributed optimization algorithms, which can be interpreted as primal-dual Ordinary differential equation (ODE) dynamics subject to null-space constraints. Based on the exponential convergence behavior of the primal-dual ODE dynamics subject to null-space constraints, we examine the behavior of the final iterate in various distributed TD-learning scenarios, considering both constant and diminishing step-sizes and incorporating both i.i.d. and Markovian observation models. Unlike existing methods, the proposed algorithm does not require the assumption that the underlying communication network structure is characterized by a doubly stochastic matrix.","authors":["Han-Dong Lim","Donghwan Lee"],"url":"https://arxiv.org/abs/2310.00638"}
{"created":"2025-04-11","title":"Dataset Condensation for Recommendation","abstract":"Training recommendation models on large datasets requires significant time and resources. It is desired to construct concise yet informative datasets for efficient training. Recent advances in dataset condensation show promise in addressing this problem by synthesizing small datasets. However, applying existing methods of dataset condensation to recommendation has limitations: (1) they fail to generate discrete user-item interactions, and (2) they could not preserve users' potential preferences. To address the limitations, we propose a lightweight condensation framework tailored for recommendation (DConRec), focusing on condensing user-item historical interaction sets. Specifically, we model the discrete user-item interactions via a probabilistic approach and design a pre-augmentation module to incorporate the potential preferences of users into the condensed datasets. While the substantial size of datasets leads to costly optimization, we propose a lightweight policy gradient estimation to accelerate the data synthesis. Experimental results on multiple real-world datasets have demonstrated the effectiveness and efficiency of our framework. Besides, we provide a theoretical analysis of the provable convergence of DConRec. Our implementation is available at: https://github.com/JiahaoWuGit/DConRec.","authors":["Jiahao Wu","Wenqi Fan","Jingfan Chen","Shengcai Liu","Qijiong Liu","Rui He","Qing Li","Ke Tang"],"url":"https://arxiv.org/abs/2310.01038"}
{"created":"2025-04-11","title":"Software Reconfiguration in Robotics","abstract":"Robots often need to be reconfigurable$-$to customize, calibrate, or optimize robots operating in varying environments with different hardware). A particular challenge in robotics is the automated and dynamic reconfiguration to load and unload software components, as well as parameterizing them. Over the last decades, a large variety of software reconfiguration techniques has been presented in the literature, many specifically for robotics systems. Also many robotics frameworks support reconfiguration. Unfortunately, there is a lack of empirical data on the actual use of reconfiguration techniques in real robotics projects and on their realization in robotics frameworks. To advance reconfiguration techniques and support their adoption, we need to improve our empirical understanding of them in practice. We present a study of automated reconfiguration at runtime in the robotics domain. We determine the state-of-the art by reviewing 78 relevant publications on reconfiguration. We determine the state-of-practice by analyzing how four major robotics frameworks support reconfiguration, and how reconfiguration is realized in 48 robotics (sub-)systems. We contribute a detailed analysis of the design space of reconfiguration techniques. We identify trends and research gaps. Our results show a significant discrepancy between the state-of-the-art and the state-of-practice. While the scientific community focuses on complex structural reconfiguration, only parameter reconfiguration is widely used in practice. Our results support practitioners to realize reconfiguration in robotics systems, as well as they support researchers and tool builders to create more effective reconfiguration techniques that are adopted in practice.","authors":["Sven Peldszus","Davide Brugali","Daniel Str\\\"uber","Patrizio Pelliccione","Thorsten Berger"],"url":"https://arxiv.org/abs/2310.01039"}
{"created":"2025-04-11","title":"Estimation of embedding vectors in high dimensions","abstract":"Embeddings are a basic initial feature extraction step in many machine learning models, particularly in natural language processing. An embedding attempts to map data tokens to a low-dimensional space where similar tokens are mapped to vectors that are close to one another by some metric in the embedding space. A basic question is how well can such embedding be learned? To study this problem, we consider a simple probability model for discrete data where there is some \"true\" but unknown embedding where the correlation of random variables is related to the similarity of the embeddings. Under this model, it is shown that the embeddings can be learned by a variant of low-rank approximate message passing (AMP) method. The AMP approach enables precise predictions of the accuracy of the estimation in certain high-dimensional limits. In particular, the methodology provides insight on the relations of key parameters such as the number of samples per value, the frequency of the terms, and the strength of the embedding correlation on the probability distribution. Our theoretical findings are validated by simulations on both synthetic data and real text data.","authors":["Golara Ahmadi Azar","Melika Emami","Alyson Fletcher","Sundeep Rangan"],"url":"https://arxiv.org/abs/2312.07802"}
{"created":"2025-04-11","title":"GastCoCo: Graph Storage and Coroutine-Based Prefetch Co-Design for Dynamic Graph Processing","abstract":"An efficient data structure is fundamental to meeting the growing demands in dynamic graph processing. However, the dual requirements for graph computation efficiency (with contiguous structures) and graph update efficiency (with linked list-like structures) present a conflict in the design principles of graph structures. After experimental studies of existing state-of-the-art dynamic graph structures, we observe that the overhead of cache misses accounts for a major portion of the graph computation time. This paper presents GastCoCo, a system with graph storage and coroutine-based prefetch co-design. By employing software prefetching via stackless coroutines and introducing a prefetch-friendly data structure CBList, GastCoCo significantly alleviates the performance degradation caused by cache misses. Our results show that GastCoCo outperforms state-of-the-art graph storage systems by 1.3x - 180x in graph updates and 1.4x - 41.1x in graph computation.","authors":["Hongfu Li","Qian Tao","Song Yu","Shufeng Gong","Yanfeng Zhang","Feng Yao","Wenyuan Yu","Ge Yu","Jingren Zhou"],"url":"https://arxiv.org/abs/2312.14396"}
{"created":"2025-04-11","title":"Measuring the Discrepancy between 3D Geometric Models using Directional Distance Fields","abstract":"Qualifying the discrepancy between 3D geometric models, which could be represented with either point clouds or triangle meshes, is a pivotal issue with board applications. Existing methods mainly focus on directly establishing the correspondence between two models and then aggregating point-wise distance between corresponding points, resulting in them being either inefficient or ineffective. In this paper, we propose DirDist, an efficient, effective, robust, and differentiable distance metric for 3D geometry data. Specifically, we construct DirDist based on the proposed implicit representation of 3D models, namely directional distance field (DDF), which defines the directional distances of 3D points to a model to capture its local surface geometry. We then transfer the discrepancy between two 3D geometric models as the discrepancy between their DDFs defined on an identical domain, naturally establishing model correspondence. To demonstrate the advantage of our DirDist, we explore various distance metric-driven 3D geometric modeling tasks, including template surface fitting, rigid registration, non-rigid registration, scene flow estimation and human pose optimization. Extensive experiments show that our DirDist achieves significantly higher accuracy under all tasks. As a generic distance metric, DirDist has the potential to advance the field of 3D geometric modeling. The source code is available at https://github.com/rsy6318/DirDist.","authors":["Siyu Ren","Junhui Hou","Xiaodong Chen","Hongkai Xiong","Wenping Wang"],"url":"https://arxiv.org/abs/2401.09736"}
{"created":"2025-04-11","title":"CroissantLLM: A Truly Bilingual French-English Language Model","abstract":"We introduce CroissantLLM, a 1.3B language model pretrained on a set of 3T English and French tokens, to bring to the research and industrial community a high-performance, fully open-sourced bilingual model that runs swiftly on consumer-grade local hardware. To that end, we pioneer the approach of training an intrinsically bilingual model with a 1:1 English-to-French pretraining data ratio, a custom tokenizer, and bilingual finetuning datasets. We release the training dataset, notably containing a French split with manually curated, high-quality, and varied data sources. To assess performance outside of English, we craft a novel benchmark, FrenchBench, consisting of an array of classification and generation tasks, covering various orthogonal aspects of model performance in the French Language. Additionally, rooted in transparency and to foster further Large Language Model research, we release codebases, and dozens of checkpoints across various model sizes, training data distributions, and training steps, as well as fine-tuned Chat models, and strong translation models. We evaluate our model through the FMTI framework, and validate 81 % of the transparency criteria, far beyond the scores of even most open initiatives. This work enriches the NLP landscape, breaking away from previous English-centric work in order to strengthen our understanding of multilinguality in language models.","authors":["Manuel Faysse","Patrick Fernandes","Nuno M. Guerreiro","Ant\\'onio Loison","Duarte M. Alves","Caio Corro","Nicolas Boizard","Jo\\~ao Alves","Ricardo Rei","Pedro H. Martins","Antoni Bigata Casademunt","Fran\\c{c}ois Yvon","Andr\\'e F. T. Martins","Gautier Viaud","C\\'eline Hudelot","Pierre Colombo"],"url":"https://arxiv.org/abs/2402.00786"}
{"created":"2025-04-11","title":"TESSERACT: Eliminating Experimental Bias in Malware Classification across Space and Time (Extended Version)","abstract":"Machine learning (ML) plays a pivotal role in detecting malicious software. Despite the high F1-scores reported in numerous studies reaching upwards of 0.99, the issue is not completely solved. Malware detectors often experience performance decay due to constantly evolving operating systems and attack methods, which can render previously learned knowledge insufficient for accurate decision-making on new inputs. This paper argues that commonly reported results are inflated due to two pervasive sources of experimental bias in the detection task: spatial bias caused by data distributions that are not representative of a real-world deployment; and temporal bias caused by incorrect time splits of data, leading to unrealistic configurations. To address these biases, we introduce a set of constraints for fair experiment design, and propose a new metric, AUT, for classifier robustness in real-world settings. We additionally propose an algorithm designed to tune training data to enhance classifier performance. Finally, we present TESSERACT, an open-source framework for realistic classifier comparison. Our evaluation encompasses both traditional ML and deep learning methods, examining published works on an extensive Android dataset with 259,230 samples over a five-year span. Additionally, we conduct case studies in the Windows PE and PDF domains. Our findings identify the existence of biases in previous studies and reveal that significant performance enhancements are possible through appropriate, periodic tuning. We explore how mitigation strategies may support in achieving a more stable and better performance over time by employing multiple strategies to delay performance decay.","authors":["Zeliang Kan","Shae McFadden","Daniel Arp","Feargus Pendlebury","Roberto Jordaney","Johannes Kinder","Fabio Pierazzi","Lorenzo Cavallaro"],"url":"https://arxiv.org/abs/2402.01359"}
{"created":"2025-04-11","title":"Pruner: A Draft-then-Verify Exploration Mechanism to Accelerate Tensor Program Tuning","abstract":"Tensor program tuning is essential for the efficient deployment of deep neural networks. Search-based approaches have demonstrated scalability and effectiveness in automatically finding high-performance programs for specific hardware. However, the search process is often inefficient, taking hours or even days to discover optimal programs due to the exploration mechanisms guided by an accurate but slow-learned cost model. Meanwhile, the learned cost model trained on one platform cannot seamlessly adapt online to another, which we call cross-platform online unawareness.","authors":["Liang Qiao","Jun Shi","Xiaoyu Hao","Xi Fang","Sen Zhang","Minfan Zhao","Ziqi Zhu","Junshi Chen","Hong An","Xulong Tang","Bing Li","Honghui Yuan","Xinyang Wang"],"url":"https://arxiv.org/abs/2402.02361"}
{"created":"2025-04-11","title":"FuseMoE: Mixture-of-Experts Transformers for Fleximodal Fusion","abstract":"As machine learning models in critical fields increasingly grapple with multimodal data, they face the dual challenges of handling a wide array of modalities, often incomplete due to missing elements, and the temporal irregularity and sparsity of collected samples. Successfully leveraging this complex data, while overcoming the scarcity of high-quality training samples, is key to improving these models' predictive performance. We introduce ``FuseMoE'', a mixture-of-experts framework incorporated with an innovative gating function. Designed to integrate a diverse number of modalities, FuseMoE is effective in managing scenarios with missing modalities and irregularly sampled data trajectories. Theoretically, our unique gating function contributes to enhanced convergence rates, leading to better performance in multiple downstream tasks. The practical utility of FuseMoE in the real world is validated by a diverse set of challenging prediction tasks.","authors":["Xing Han","Huy Nguyen","Carl Harris","Nhat Ho","Suchi Saria"],"url":"https://arxiv.org/abs/2402.03226"}
{"created":"2025-04-11","title":"Topic-aware Most Influential Community Search in Social Networks","abstract":"Influential community search (ICS) finds a set of densely connected and high-impact vertices from a social network. Although great effort has been devoted to ICS problems, most existing methods do not consider how relevant the influential community found is to specific topics. A few attempts at topic-aware ICS problems cannot capture the stochastic nature of community formation and influence propagation in social networks. To address these issues, we introduce a novel problem of topic-aware most influential community search (TAMICS) to discover a set of vertices such that for a given topic vector q, they induce a $(k, l, \\eta)$-core in an uncertain directed interaction graph and have the highest influence scores under the independent cascade (IC) model. We propose an online algorithm to provide an approximate result for any TAMICS query with bounded errors. Furthermore, we design two index structures and an index-based heuristic algorithm for efficient TAMICS query processing. Finally, we experimentally evaluate the efficacy and efficiency of our proposed approaches on various real-world datasets. The results show that (1) the communities of TAMICS have higher relevance and social influence w.r.t.~the query topics as well as structural cohesiveness than those of several state-of-the-art topic-aware and influential CS methods and (2) the index-based algorithm achieves speed-ups of up to three orders of magnitude over the online algorithm with an affordable overhead for index construction.","authors":["Long Teng","Yanhao Wang","Zhe Lin","Fei Yu"],"url":"https://arxiv.org/abs/2402.07601"}
{"created":"2025-04-11","title":"Induced Model Matching: Restricted Models Help Train Full-Featured Models","abstract":"We consider scenarios where a very accurate (often small) predictive model using restricted features is available when training a full-featured (often larger) model. This restricted model may be thought of as side-information'', and can come either from an auxiliary dataset or from the same dataset by forcing the restriction. How can the restricted model be useful to the full model? To answer this, we introduce a methodology called Induced Model Matching (IMM). IMM aligns the context-restricted, or induced, version of the large model with the restricted model. We relate IMM to approaches such as noising, which is implicit in addressing the problem, and reverse knowledge distillation from weak teachers, which is explicit but does not exploit restriction being the nature of the weakness. We show that these prior methods can be thought of as approximations to IMM and can be problematic in terms of consistency. Experimentally, we first motivate IMM using logistic regression as a toy example. We then explore it in language modeling, the application that initially inspired it, and demonstrate it on both LSTM and transformer full models, using bigrams as restricted models. We lastly give a simple RL example, which shows that POMDP policies can help learn better MDP policies. The IMM principle is thus generally applicable in common scenarios where restricted data is cheaper to collect or restricted models are easier to learn.","authors":["Usama Muneeb","Mesrob I. Ohannessian"],"url":"https://arxiv.org/abs/2402.12513"}
{"created":"2025-04-11","title":"Bayesian Off-Policy Evaluation and Learning for Large Action Spaces","abstract":"In interactive systems, actions are often correlated, presenting an opportunity for more sample-efficient off-policy evaluation (OPE) and learning (OPL) in large action spaces. We introduce a unified Bayesian framework to capture these correlations through structured and informative priors. In this framework, we propose sDM, a generic Bayesian approach for OPE and OPL, grounded in both algorithmic and theoretical foundations. Notably, sDM leverages action correlations without compromising computational efficiency. Moreover, inspired by online Bayesian bandits, we introduce Bayesian metrics that assess the average performance of algorithms across multiple problem instances, deviating from the conventional worst-case assessments. We analyze sDM in OPE and OPL, highlighting the benefits of leveraging action correlations. Empirical evidence showcases the strong performance of sDM.","authors":["Imad Aouali","Victor-Emmanuel Brunel","David Rohde","Anna Korba"],"url":"https://arxiv.org/abs/2402.14664"}
{"created":"2025-04-11","title":"A Crosstalk-Aware Timing Prediction Method in Routing","abstract":"With shrinking interconnect spacing in advanced technology nodes, existing timing predictions become less precise due to the challenging quantification of crosstalk-induced delay. During the routing, the crosstalk effect is typically modeled by predicting coupling capacitance with congestion information. However, the timing estimation tends to be overly pessimistic, as the crosstalk-induced delay depends not only on the coupling capacitance but also on the signal arrival time. This work presents a crosstalk-aware timing estimation method using a two-step machine learning approach. Interconnects that are physically adjacent and overlap in signal timing windows are filtered first. Crosstalk delay is predicted by integrating physical topology and timing features without relying on post-routing results and the parasitic extraction. Experimental results show a match rate of over 99% for identifying crosstalk nets compared to the commercial tool on the OpenCores benchmarks, with prediction results being more accurate than those of other state-of-the-art methods.","authors":["Leilei Jin","Jiajie Xu","Wenjie Fu","Hao Yan","Longxing Shi"],"url":"https://arxiv.org/abs/2403.04145"}
{"created":"2025-04-11","title":"Understanding the Effect of Opinion Polarization in Short Video Browsing","abstract":"This paper explores the impact of Opinion Polarization (OP) in the increasingly prevalent context of short video browsing, a dominant medium in the contemporary digital landscape with significant influence on public opinion and social dynamics. We investigate the effects of OP on user perceptions and behaviors in short video consumption, and find that traditional user feedback signals, such as like and browsing duration, are not suitable for detecting and measuring OP. Recognizing this problem, our study employs Electroencephalogram (EEG) signals as a novel, noninvasive approach to assess the neural processing of perception and cognition related to OP. Our user study reveals that OP notably affects users' sentiments, resulting in measurable changes in brain signals. Furthermore, we demonstrate the potential of using EEG signals to predict users' exposure to polarized short video content. By exploring the relationships between OP, brain signals, and user behavior, our research offers a novel perspective in understanding the dynamics of short video browsing and proposes an innovative method for quantifying the impact of OP in this context.","authors":["Bangde Du","Ziyi Ye","Monika Jankowska","Zhijing Wu","Qingyao Ai","Yiqun Liu"],"url":"https://arxiv.org/abs/2403.04184"}
{"created":"2025-04-11","title":"Optimizing LLM Queries in Relational Data Analytics Workloads","abstract":"Batch data analytics is a growing application for Large Language Models (LLMs). LLMs enable users to perform a wide range of natural language tasks, such as classification, entity extraction, and translation, over large datasets. However, LLM inference is highly costly and slow: for example, an NVIDIA L4 GPU running Llama3-8B can only process 6 KB of text per second, taking about a day to handle 15 GB of data; processing a similar amount of data costs around $10K on OpenAI's GPT-4o. In this paper, we propose novel techniques that can significantly reduce the cost of LLM calls for relational data analytics workloads. Our key contribution is developing efficient algorithms for reordering the rows and the fields within each row of an input table to maximize key-value (KV) cache reuse when performing LLM serving. As such, our approach can be easily applied to existing analytics systems and serving platforms. Our evaluation shows that our solution can yield up to 3.4x improvement in job completion time on a benchmark of diverse LLM-based queries using Llama 3 models. Our solution also achieves a 32% cost savings under OpenAI and Anthropic pricing models.","authors":["Shu Liu","Asim Biswal","Amog Kamsetty","Audrey Cheng","Luis Gaspar Schroeder","Liana Patel","Shiyi Cao","Xiangxi Mo","Ion Stoica","Joseph E. Gonzalez","Matei Zaharia"],"url":"https://arxiv.org/abs/2403.05821"}
{"created":"2025-04-11","title":"A Survey of Source Code Representations for Machine Learning-Based Cybersecurity Tasks","abstract":"Machine learning techniques for cybersecurity-related software engineering tasks are becoming increasingly popular. The representation of source code is a key portion of the technique that can impact the way the model is able to learn the features of the source code. With an increasing number of these techniques being developed, it is valuable to see the current state of the field to better understand what exists and what is not there yet. This article presents a study of these existing machine learning based approaches and demonstrates what type of representations were used for different cybersecurity tasks and programming languages. Additionally, we study what types of models are used with different representations. We have found that graph-based representations are the most popular category of representation, and tokenizers and Abstract Syntax Trees (ASTs) are the two most popular representations overall (e.g., AST and tokenizers are the representations with the highest count of papers, whereas graph-based representations is the category with the highest count of papers). We also found that the most popular cybersecurity task is vulnerability detection, and the language that is covered by the most techniques is C. Finally, we found that sequence-based models are the most popular category of models, and Support Vector Machines are the most popular model overall.","authors":["Beatrice Casey","Joanna C. S. Santos","George Perry"],"url":"https://arxiv.org/abs/2403.10646"}
{"created":"2025-04-11","title":"Floralens: a Deep Learning Model for the Portuguese Native Flora","abstract":"Machine-learning techniques, especially deep convolutional neural networks, are pivotal for image-based identification of biological species in many Citizen Science platforms. In this paper, we describe the construction of a dataset for the Portuguese native flora based on publicly available research-grade datasets, and the derivation of a high-accuracy model from it using off-the-shelf deep convolutional neural networks. We anchored the dataset in high-quality data provided by Sociedade Portuguesa de Bot\\^anica and added further sampled data from research-grade datasets available from GBIF. We find that with a careful dataset design, off-the-shelf machine-learning cloud services such as Google's AutoML Vision produce accurate models, with results comparable to those of Pl@ntNet, a state-of-the-art citizen science platform. The best model we derived, dubbed Floralens, has been integrated into the public website of Project Biolens, where we gather models for other taxa as well. The dataset used to train the model is also publicly available on Zenodo.","authors":["Ant\\'onio Filgueiras","Eduardo R. B. Marques","Lu\\'is M. B. Lopes","Miguel Marques","Hugo Silva"],"url":"https://arxiv.org/abs/2403.12072"}
{"created":"2025-04-11","title":"Genetic Programming for Explainable Manifold Learning","abstract":"Manifold learning techniques play a pivotal role in machine learning by revealing lower-dimensional embeddings within high-dimensional data, thus enhancing both the efficiency and interpretability of data analysis by transforming the data into a lower-dimensional representation. However, a notable challenge with current manifold learning methods is their lack of explicit functional mappings, crucial for explainability in many real-world applications. Genetic programming, known for its interpretable functional tree-based models, has emerged as a promising approach to address this challenge. Previous research leveraged multi-objective GP to balance manifold quality against embedding dimensionality, producing functional mappings across a range of embedding sizes. Yet, these mapping trees often became complex, hindering explainability. In response, in this paper, we introduce Genetic Programming for Explainable Manifold Learning (GP-EMaL), a novel approach that directly penalises tree complexity. Our new method is able to maintain high manifold quality while significantly enhancing explainability and also allows customisation of complexity measures, such as symmetry balancing, scaling, and node complexity, catering to diverse application needs. Our experimental analysis demonstrates that GP-EMaL is able to match the performance of the existing approach in most cases, while using simpler, smaller, and more interpretable tree structures. This advancement marks a significant step towards achieving interpretable manifold learning.","authors":["Ben Cravens","Andrew Lensen","Paula Maddigan","Bing Xue"],"url":"https://arxiv.org/abs/2403.14139"}
{"created":"2025-04-11","title":"Dynamic Relative Representations for Goal-Oriented Semantic Communications","abstract":"In future 6G wireless networks, semantic and effectiveness aspects of communications will play a fundamental role, incorporating meaning and relevance into transmissions. However, obstacles arise when devices employ diverse languages, logic, or internal representations, leading to semantic mismatches that might jeopardize understanding. In latent space communication, this challenge manifests as misalignment within high-dimensional representations where deep neural networks encode data. This paper presents a novel framework for goal-oriented semantic communication, leveraging relative representations to mitigate semantic mismatches via latent space alignment. We propose a dynamic optimization strategy that adapts relative representations, communication parameters, and computation resources for energy-efficient, low-latency, goal-oriented semantic communications. Numerical results demonstrate our methodology's effectiveness in mitigating mismatches among devices, while optimizing energy consumption, delay, and effectiveness.","authors":["Simone Fiorellino","Claudio Battiloro","Emilio Calvanese Strinati","Paolo Di Lorenzo"],"url":"https://arxiv.org/abs/2403.16986"}
{"created":"2025-04-11","title":"Unsolvable Problem Detection: Robust Understanding Evaluation for Large Multimodal Models","abstract":"This paper introduces a novel task to evaluate the robust understanding capability of Large Multimodal Models (LMMs), termed $\\textbf{Unsolvable Problem Detection (UPD)}$. Multiple-choice question answering (MCQA) is widely used to assess the understanding capability of LMMs, but it does not guarantee that LMMs truly comprehend the answer. UPD assesses the LMM's ability to withhold answers when encountering unsolvable problems of MCQA, verifying whether the model truly understands the answer. UPD encompasses three problems: Absent Answer Detection (AAD), Incompatible Answer Set Detection (IASD), and Incompatible Visual Question Detection (IVQD), covering unsolvable cases like answer-lacking or incompatible choices and image-question mismatches. For the evaluation, we introduce the MM-UPD Bench, a benchmark for assessing performance across various ability dimensions. Our experiments reveal that even most LMMs, which demonstrate adequate performance on existing benchmarks, struggle significantly with MM-UPD, underscoring a novel aspect of trustworthiness that current benchmarks have overlooked. A detailed analysis shows that LMMs have different bottlenecks and chain-of-thought and self-reflection improved performance for LMMs with the bottleneck in their LLM capability. We hope our insights will enhance the broader understanding and development of more reliable LMMs.","authors":["Atsuyuki Miyai","Jingkang Yang","Jingyang Zhang","Yifei Ming","Qing Yu","Go Irie","Yixuan Li","Hai Li","Ziwei Liu","Kiyoharu Aizawa"],"url":"https://arxiv.org/abs/2403.20331"}
{"created":"2025-04-11","title":"A Survey on Error-Bounded Lossy Compression for Scientific Datasets","abstract":"Error-bounded lossy compression has been effective in significantly reducing the data storage/transfer burden while preserving the reconstructed data fidelity very well. Many error-bounded lossy compressors have been developed for a wide range of parallel and distributed use cases for years. They are designed with distinct compression models and principles, such that each of them features particular pros and cons. In this paper we provide a comprehensive survey of emerging error-bounded lossy compression techniques. The key contribution is fourfold. (1) We summarize a novel taxonomy of lossy compression into 6 classic models. (2) We provide a comprehensive survey of 10 commonly used compression components/modules. (3) We summarized pros and cons of 46 state-of-the-art lossy compressors and present how state-of-the-art compressors are designed based on different compression techniques. (4) We discuss how customized compressors are designed for specific scientific applications and use-cases. We believe this survey is useful to multiple communities including scientific applications, high-performance computing, lossy compression, and big data.","authors":["Sheng Di","Jinyang Liu","Kai Zhao","Xin Liang","Robert Underwood","Zhaorui Zhang","Milan Shah","Yafan Huang","Jiajun Huang","Xiaodong Yu","Congrong Ren","Hanqi Guo","Grant Wilkins","Dingwen Tao","Jiannan Tian","Sian Jin","Zizhe Jian","Daoce Wang","MD Hasanur Rahman","Boyuan Zhang","Shihui Song","Jon C. Calhoun","Guanpeng Li","Kazutomo Yoshii","Khalid Ayed Alharthi","Franck Cappello"],"url":"https://arxiv.org/abs/2404.02840"}
{"created":"2025-04-11","title":"PiSSA: Principal Singular Values and Singular Vectors Adaptation of Large Language Models","abstract":"To parameter-efficiently fine-tune (PEFT) large language models (LLMs), the low-rank adaptation (LoRA) method approximates the model changes $\\Delta W \\in \\mathbb{R}^{m \\times n}$ through the product of two matrices $A \\in \\mathbb{R}^{m \\times r}$ and $B \\in \\mathbb{R}^{r \\times n}$, where $r \\ll \\min(m, n)$, $A$ is initialized with Gaussian noise, and $B$ with zeros. LoRA freezes the original model $W$ and updates the \"Noise & Zero\" adapter, which may lead to slow convergence. To overcome this limitation, we introduce Principal Singular values and Singular vectors Adaptation (PiSSA). PiSSA shares the same architecture as LoRA, but initializes the adaptor matrices $A$ and $B$ with the principal components of the original matrix $W$, and put the remaining components into a residual matrix $W^{res} \\in \\mathbb{R}^{m \\times n}$ which is frozen during fine-tuning. Compared to LoRA, PiSSA updates the principal components while freezing the \"residual\" parts, allowing faster convergence and enhanced performance. Comparative experiments of PiSSA and LoRA across 12 different models, ranging from 184M to 70B, encompassing 5 NLG and 8 NLU tasks, reveal that PiSSA consistently outperforms LoRA under identical experimental setups. On the GSM8K benchmark, Mistral-7B fine-tuned with PiSSA achieves an accuracy of 72.86%, surpassing LoRA's 67.7% by 5.16%. Due to the same architecture, PiSSA is also compatible with quantization to further reduce the memory requirement of fine-tuning. Compared to QLoRA, QPiSSA exhibits smaller quantization errors in the initial stages. Fine-tuning LLaMA-3-70B on GSM8K, QPiSSA attains an accuracy of 86.05%, exceeding the performances of QLoRA at 81.73%. Leveraging a fast SVD technique, PiSSA can be initialized in only a few seconds, presenting a negligible cost for transitioning from LoRA to PiSSA. Code is available at https://github.com/GraphPKU/PiSSA.","authors":["Fanxu Meng","Zhaohui Wang","Muhan Zhang"],"url":"https://arxiv.org/abs/2404.02948"}
{"created":"2025-04-11","title":"Efficient Ranking Function-Based Termination Analysis with Bi-Directional Feedback","abstract":"Synthesizing ranking functions is a common technique for proving the termination of loops. A ranking function must be bounded and decrease by a specified amount with each iteration for all reachable program states. However, the set of reachable program states is often unknown, and loop invariants are typically used to overapproximate it. So, proving the termination of a loop requires searching for both a ranking function and a loop invariant. Existing ranking function-based termination analysis techniques can be broadly categorized as (i) those that synthesize the ranking function and invariants independently, (ii) those that combine invariant synthesis with ranking function synthesis into a single query, and (iii) those that offer limited feedback from ranking function synthesis to guide invariant synthesis. These approaches either suffer from having too large a search space or inefficiently exploring the smaller, individual search spaces. In this work, we present a novel termination analysis framework Syndicate, which exploits bi-directional feedback to guide the searches for both ranking functions and invariants, increasing the number of programs that can be proven to terminate and reduces the average time needed to prove termination compared to baselines. Syndicate is general and allows different instantiations of templates, subprocedures, and parameters, offering users the flexibility to optimize the ranking function synthesis. Depending on the templates used, Syndicate is relatively complete and efficient, outperforming existing techniques that achieve at most one of these guarantees. Notably, despite a simpler approach compared to the baselines, Syndicate's performance is either comparable to or better than existing tools in terms of the number of benchmarks proved and average runtime.","authors":["Yasmin Sarita","Avaljot Singh","Shaurya Gomber","Gagandeep Singh","Mahesh Vishwanathan"],"url":"https://arxiv.org/abs/2404.05951"}
{"created":"2025-04-11","title":"Towards Collaborative Autonomous Driving: Simulation Platform and End-to-End System","abstract":"Vehicle-to-everything-aided autonomous driving (V2X-AD) has a huge potential to provide a safer driving solution. Despite extensive researches in transportation and communication to support V2X-AD, the actual utilization of these infrastructures and communication resources in enhancing driving performances remains largely unexplored. This highlights the necessity of collaborative autonomous driving: a machine learning approach that optimizes the information sharing strategy to improve the driving performance of each vehicle. This effort necessitates two key foundations: a platform capable of generating data to facilitate the training and testing of V2X-AD, and a comprehensive system that integrates full driving-related functionalities with mechanisms for information sharing. From the platform perspective, we present V2Xverse, a comprehensive simulation platform for collaborative autonomous driving. This platform provides a complete pipeline for collaborative driving. From the system perspective, we introduce CoDriving, a novel end-to-end collaborative driving system that properly integrates V2X communication over the entire autonomous pipeline, promoting driving with shared perceptual information. The core idea is a novel driving-oriented communication strategy. Leveraging this strategy, CoDriving improves driving performance while optimizing communication efficiency. We make comprehensive benchmarks with V2Xverse, analyzing both modular performance and closed-loop driving performance. Experimental results show that CoDriving: i) significantly improves the driving score by 62.49% and drastically reduces the pedestrian collision rate by 53.50% compared to the SOTA end-to-end driving method, and ii) achieves sustaining driving performance superiority over dynamic constraint communication conditions.","authors":["Genjia Liu","Yue Hu","Chenxin Xu","Weibo Mao","Junhao Ge","Zhengxiang Huang","Yifan Lu","Yinda Xu","Junkai Xia","Yafei Wang","Siheng Chen"],"url":"https://arxiv.org/abs/2404.09496"}
{"created":"2025-04-11","title":"Investigating Adversarial Trigger Transfer in Large Language Models","abstract":"Recent work has developed optimization procedures to find token sequences, called adversarial triggers, which can elicit unsafe responses from aligned language models. These triggers are believed to be highly transferable, i.e., a trigger optimized on one model can jailbreak other models. In this paper, we concretely show that such adversarial triggers are not consistently transferable. We extensively investigate trigger transfer amongst 13 open models and observe poor and inconsistent transfer. Our experiments further reveal a significant difference in robustness to adversarial triggers between models Aligned by Preference Optimization (APO) and models Aligned by Fine-Tuning (AFT). We find that APO models are extremely hard to jailbreak even when the trigger is optimized directly on the model. On the other hand, while AFT models may appear safe on the surface, exhibiting refusals to a range of unsafe instructions, we show that they are highly susceptible to adversarial triggers. Lastly, we observe that most triggers optimized on AFT models also generalize to new unsafe instructions from five diverse domains, further emphasizing their vulnerability. Overall, our work highlights the need for more comprehensive safety evaluations for aligned language models.","authors":["Nicholas Meade","Arkil Patel","Siva Reddy"],"url":"https://arxiv.org/abs/2404.16020"}
{"created":"2025-04-11","title":"LeanGaussian: Breaking Pixel or Point Cloud Correspondence in Modeling 3D Gaussians","abstract":"Recently, Gaussian splatting has demonstrated significant success in novel view synthesis. Current methods often regress Gaussians with pixel or point cloud correspondence, linking each Gaussian with a pixel or a 3D point. This leads to the redundancy of Gaussians being used to overfit the correspondence rather than the objects represented by the 3D Gaussians themselves, consequently wasting resources and lacking accurate geometries or textures. In this paper, we introduce LeanGaussian, a novel approach that treats each query in deformable Transformer as one 3D Gaussian ellipsoid, breaking the pixel or point cloud correspondence constraints. We leverage deformable decoder to iteratively refine the Gaussians layer-by-layer with the image features as keys and values. Notably, the center of each 3D Gaussian is defined as 3D reference points, which are then projected onto the image for deformable attention in 2D space. On both the ShapeNet SRN dataset (category level) and the Google Scanned Objects dataset (open-category level, trained with the Objaverse dataset), our approach, outperforms prior methods by approximately 6.1%, achieving a PSNR of 25.44 and 22.36, respectively. Additionally, our method achieves a 3D reconstruction speed of 7.2 FPS and rendering speed 500 FPS. Codes are available at https://github.com/jwubz123/LeanGaussian.","authors":["Jiamin Wu","Kenkun Liu","Han Gao","Xiaoke Jiang","Yao Yuan","Lei Zhang"],"url":"https://arxiv.org/abs/2404.16323"}
{"created":"2025-04-11","title":"Understanding the Cluster LP for Correlation Clustering","abstract":"In the classic Correlation Clustering problem introduced by Bansal, Blum, and Chawla~(FOCS 2002), the input is a complete graph where edges are labeled either $+$ or $-$, and the goal is to find a partition of the vertices that minimizes the sum of the +edges across parts plus the sum of the -edges within parts. In recent years, Chawla, Makarychev, Schramm and Yaroslavtsev~(STOC 2015) gave a 2.06-approximation by providing a near-optimal rounding of the standard LP, and Cohen-Addad, Lee, Li, and Newman~(FOCS 2022, 2023) finally bypassed the integrality gap of 2 for this LP giving a $1.73$-approximation for the problem.","authors":["Nairen Cao","Vincent Cohen-Addad","Euiwoong Lee","Shi Li","Alantha Newman","Lukas Vogl"],"url":"https://arxiv.org/abs/2404.17509"}
{"created":"2025-04-11","title":"Automating Customer Needs Analysis: A Comparative Study of Large Language Models in the Travel Industry","abstract":"In the rapidly evolving landscape of Natural Language Processing (NLP), Large Language Models (LLMs) have emerged as powerful tools for many tasks, such as extracting valuable insights from vast amounts of textual data. In this study, we conduct a comparative analysis of LLMs for the extraction of travel customer needs from TripAdvisor and Reddit posts. Leveraging a diverse range of models, including both open-source and proprietary ones such as GPT-4 and Gemini, we aim to elucidate their strengths and weaknesses in this specialized domain. Through an evaluation process involving metrics such as BERTScore, ROUGE, and BLEU, we assess the performance of each model in accurately identifying and summarizing customer needs. Our findings highlight the efficacy of opensource LLMs, particularly Mistral 7B, in achieving comparable performance to larger closed models while offering affordability and customization benefits. Additionally, we underscore the importance of considering factors such as model size, resource requirements, and performance metrics when selecting the most suitable LLM for customer needs analysis tasks. Overall, this study contributes valuable insights for businesses seeking to leverage advanced NLP techniques to enhance customer experience and drive operational efficiency in the travel industry.","authors":["Simone Barandoni","Filippo Chiarello","Lorenzo Cascone","Emiliano Marrale","Salvatore Puccio"],"url":"https://arxiv.org/abs/2404.17975"}
{"created":"2025-04-11","title":"On the Loewner framework, the Kolmogorov superposition theorem, and the curse of dimensionality","abstract":"The Loewner framework is an interpolatory approach for the approximation of linear and nonlinear systems. The purpose here is to extend this framework to linear parametric systems with an arbitrary number n of parameters. To achieve this, a new generalized multivariate rational function realization is proposed. Then, we introduce the n-dimensional multivariate Loewner matrices and show that they can be computed by solving a set of coupled Sylvester equations. The null space of these Loewner matrices allows the construction of the multivariate barycentric rational function. The principal result of this work is to show how the null space of the n-dimensional Loewner matrix can be computed using a sequence of 1-dimensional Loewner matrices, leading to a drastic reduction of the computational burden. Equally importantly, this burden is alleviated by avoiding the explicit construction of large-scale n-dimensional Loewner matrices of size $N \\times N$. Instead, the proposed methodology achieves decoupling of variables, leading to (i) a complexity reduction from $O(N^3)$ to below $O(N^{1.5})$ when $n > 5$ and (ii) to memory storage bounded by the largest variable dimension rather than their product, thus taming the curse of dimensionality and making the solution scalable to very large data sets. This decoupling of the variables leads to a result similar to the Kolmogorov superposition theorem for rational functions. Thus, making use of barycentric representations, every multivariate rational function can be computed using the composition and superposition of single-variable functions. Finally, we suggest two algorithms (one direct and one iterative) to construct, directly from data, multivariate (or parametric) realizations ensuring (approximate) interpolation. Numerical examples highlight the effectiveness and scalability of the method.","authors":["Athanasios C. Antoulas","Ion Victor Gosea","Charles Poussot-Vassal"],"url":"https://arxiv.org/abs/2405.00495"}
{"created":"2025-04-11","title":"SynFlowNet: Design of Diverse and Novel Molecules with Synthesis Constraints","abstract":"Generative models see increasing use in computer-aided drug design. However, while performing well at capturing distributions of molecular motifs, they often produce synthetically inaccessible molecules. To address this, we introduce SynFlowNet, a GFlowNet model whose action space uses chemical reactions and purchasable reactants to sequentially build new molecules. By incorporating forward synthesis as an explicit constraint of the generative mechanism, we aim at bridging the gap between in silico molecular generation and real world synthesis capabilities. We evaluate our approach using synthetic accessibility scores and an independent retrosynthesis tool to assess the synthesizability of our compounds, and motivate the choice of GFlowNets through considerable improvement in sample diversity compared to baselines. Additionally, we identify challenges with reaction encodings that can complicate traversal of the MDP in the backward direction. To address this, we introduce various strategies for learning the GFlowNet backward policy and thus demonstrate how additional constraints can be integrated into the GFlowNet MDP framework. This approach enables our model to successfully identify synthesis pathways for previously unseen molecules.","authors":["Miruna Cretu","Charles Harris","Ilia Igashov","Arne Schneuing","Marwin Segler","Bruno Correia","Julien Roy","Emmanuel Bengio","Pietro Li\\`o"],"url":"https://arxiv.org/abs/2405.01155"}
{"created":"2025-04-11","title":"Tabular and Deep Reinforcement Learning for Gittins Index","abstract":"In the realm of multi-arm bandit problems, the Gittins index policy is known to be optimal in maximizing the expected total discounted reward obtained from pulling the Markovian arms. In most realistic scenarios however, the Markovian state transition probabilities are unknown and therefore the Gittins indices cannot be computed. One can then resort to reinforcement learning (RL) algorithms that explore the state space to learn these indices while exploiting to maximize the reward collected. In this work, we propose tabular (QGI) and Deep RL (DGN) algorithms for learning the Gittins index that are based on the retirement formulation for the multi-arm bandit problem. When compared with existing RL algorithms that learn the Gittins index, our algorithms have a lower run time, require less storage space (small Q-table size in QGI and smaller replay buffer in DGN), and illustrate better empirical convergence to the Gittins index. This makes our algorithm well suited for problems with large state spaces and is a viable alternative to existing methods. As a key application, we demonstrate the use of our algorithms in minimizing the mean flowtime in a job scheduling problem when jobs are available in batches and have an unknown service time distribution.","authors":["Harshit Dhankhar","Kshitij Mishra","Tejas Bodas"],"url":"https://arxiv.org/abs/2405.01157"}
{"created":"2025-04-11","title":"Numerical Fuzz: A Type System for Rounding Error Analysis","abstract":"Algorithms operating on real numbers are implemented as floating-point computations in practice, but floating-point operations introduce roundoff errors that can degrade the accuracy of the result. We propose $\\Lambda_{num}$, a functional programming language with a type system that can express quantitative bounds on roundoff error. Our type system combines a sensitivity analysis, enforced through a linear typing discipline, with a novel graded monad to track the accumulation of roundoff errors. We prove that our type system is sound by relating the denotational semantics of our language to the exact and floating-point operational semantics. To demonstrate our system, we instantiate $\\Lambda_{num}$ with error metrics proposed in the numerical analysis literature and we show how to incorporate rounding operations that faithfully model aspects of the IEEE 754 floating-point standard. To show that $\\Lambda_{num}$ can be a useful tool for automated error analysis, we develop a prototype implementation for $\\Lambda_{num}$ that infers error bounds that are competitive with existing tools, while often running significantly faster. Finally, we consider semantic extensions of our graded monad to bound error under more complex rounding behaviors, such as non-deterministic and randomized rounding.","authors":["Ariel E. Kellison","Justin Hsu"],"url":"https://arxiv.org/abs/2405.04612"}
{"created":"2025-04-11","title":"Untangling Lariats: Subgradient Following of Variationally Penalized Objectives","abstract":"We describe an apparatus for subgradient-following of the optimum of convex problems with variational penalties. In this setting, we receive a sequence $y_i,\\ldots,y_n$ and seek a smooth sequence $x_1,\\ldots,x_n$. The smooth sequence needs to attain the minimum Bregman divergence to an input sequence with additive variational penalties in the general form of $\\sum_i{}g_i(x_{i+1}-x_i)$. We derive known algorithms such as the fused lasso and isotonic regression as special cases of our approach. Our approach also facilitates new variational penalties such as non-smooth barrier functions.","authors":["Kai-Chia Mo","Shai Shalev-Shwartz","Nis{\\ae}l Sh\\'artov"],"url":"https://arxiv.org/abs/2405.04710"}
{"created":"2025-04-11","title":"Recursive PAC-Bayes: A Frequentist Approach to Sequential Prior Updates with No Information Loss","abstract":"PAC-Bayesian analysis is a frequentist framework for incorporating prior knowledge into learning. It was inspired by Bayesian learning, which allows sequential data processing and naturally turns posteriors from one processing step into priors for the next. However, despite two and a half decades of research, the ability to update priors sequentially without losing confidence information along the way remained elusive for PAC-Bayes. While PAC-Bayes allows construction of data-informed priors, the final confidence intervals depend only on the number of points that were not used for the construction of the prior, whereas confidence information in the prior, which is related to the number of points used to construct the prior, is lost. This limits the possibility and benefit of sequential prior updates, because the final bounds depend only on the size of the final batch.","authors":["Yi-Shan Wu","Yijie Zhang","Badr-Eddine Ch\\'erief-Abdellatif","Yevgeny Seldin"],"url":"https://arxiv.org/abs/2405.14681"}
{"created":"2025-04-11","title":"The Writing is on the Wall: Analyzing the Boom of Inscriptions and its Impact on EVM-compatible Blockchains","abstract":"This paper examines inscription-related transactions on Ethereum and major EVM-compatible rollups, assessing their impact on scalability during transaction surges. Our results show that, on certain days, inscriptions accounted for nearly 90% of transactions on Arbitrum and ZKsync Era, while 53% on Ethereum, with 99% of these inscriptions involving meme coin minting. Furthermore, we show that ZKsync and Arbitrum saw lower median gas fees during these surges. ZKsync Era, a ZK-rollup, showed a greater fee reduction than the optimistic rollups studied -- Arbitrum, Base, and Optimism.","authors":["Johnnatan Messias","Krzysztof Gogol","Maria In\\^es Silva","Benjamin Livshits"],"url":"https://arxiv.org/abs/2405.15288"}
{"created":"2025-04-11","title":"Boost Your Human Image Generation Model via Direct Preference Optimization","abstract":"Human image generation is a key focus in image synthesis due to its broad applications, but even slight inaccuracies in anatomy, pose, or details can compromise realism. To address these challenges, we explore Direct Preference Optimization (DPO), which trains models to generate preferred (winning) images while diverging from non-preferred (losing) ones. However, conventional DPO methods use generated images as winning images, limiting realism. To overcome this limitation, we propose an enhanced DPO approach that incorporates high-quality real images as winning images, encouraging outputs to resemble real images rather than generated ones. However, implementing this concept is not a trivial task. Therefore, our approach, HG-DPO (Human image Generation through DPO), employs a novel curriculum learning framework that gradually improves the output of the model toward greater realism, making training more feasible. Furthermore, HG-DPO effectively adapts to personalized text-to-image tasks, generating high-quality and identity-specific images, which highlights the practical value of our approach.","authors":["Sanghyeon Na","Yonggyu Kim","Hyunjoon Lee"],"url":"https://arxiv.org/abs/2405.20216"}
{"created":"2025-04-11","title":"Locally Stationary Distributions: A Framework for Analyzing Slow-Mixing Markov Chains","abstract":"Many natural Markov chains fail to mix to their stationary distribution in polynomially many steps. Often, this slow mixing is inevitable since it is computationally intractable to sample from their stationary measure.","authors":["Kuikui Liu","Sidhanth Mohanty","Prasad Raghavendra","Amit Rajaraman","David X. Wu"],"url":"https://arxiv.org/abs/2405.20849"}
{"created":"2025-04-11","title":"A Concise Mathematical Description of Active Inference in Discrete Time","abstract":"In this paper we present a concise mathematical description of active inference in discrete time. The main part of the paper serves as a basic introduction to the topic, including a detailed example of the action selection mechanism. The appendix discusses the more subtle mathematical details, targeting readers who have already studied the active inference literature but struggle to make sense of the mathematical details and derivations. Throughout, we emphasize precise and standard mathematical notation, ensuring consistency with existing texts and linking all equations to widely used references on active inference. Additionally, we provide Python code that implements the action selection and learning mechanisms described in this paper and is compatible with pymdp environments.","authors":["Jesse van Oostrum","Carlotta Langer","Nihat Ay"],"url":"https://arxiv.org/abs/2406.07726"}
{"created":"2025-04-11","title":"Balancing Rigor and Utility: Mitigating Cognitive Biases in Large Language Models for Multiple-Choice Questions","abstract":"This paper examines the role of cognitive biases in the decision-making processes of large language models (LLMs), challenging the conventional goal of eliminating all biases. When properly balanced, we show that certain cognitive biases can enhance decision-making efficiency through rational deviations and heuristic shortcuts. By introducing heuristic moderation and an abstention option, which allows LLMs to withhold responses when uncertain, we reduce error rates, improve decision accuracy, and optimize decision rates. Using the Balance Rigor and Utility (BRU) dataset, developed through expert collaboration, our findings demonstrate that targeted inspection of cognitive biases aligns LLM decisions more closely with human reasoning, enhancing reliability and suggesting strategies for future improvements. This approach offers a novel way to leverage cognitive biases to improve the practical utility of LLMs across various applications.","authors":["Hanyang Zhong","Liman Wang","Wenting Cao","Zeyuan Sun"],"url":"https://arxiv.org/abs/2406.10999"}
{"created":"2025-04-11","title":"Generative AI Voting: Fair Collective Choice is Resilient to LLM Biases and Inconsistencies","abstract":"Scaling up deliberative and voting participation is a longstanding endeavor -- a cornerstone for direct democracy and legitimate collective choice. Recent breakthroughs in generative artificial intelligence (AI) and large language models (LLMs) unravel new capabilities for AI personal assistants to overcome cognitive bandwidth limitations of humans, providing decision support or even direct representation of human voters at large scale. However, the quality of this representation and what underlying biases manifest when delegating collective decision-making to LLMs is an alarming and timely challenge to tackle. By rigorously emulating with high realism more than >50K LLM voting personas in 306 real-world voting elections, we disentangle the nature of different biases in LLMS (GPT 3, GPT 3.5, and Llama2). Complex preferential ballot formats exhibit significant inconsistencies compared to simpler majoritarian elections that show higher consistency. Strikingly though, by demonstrating for the first time in real-world a proportional representation of voters in direct democracy, we are also able to show that fair ballot aggregation methods, such as equal shares, prove to be a win-win: fairer voting outcomes for humans with fairer AI representation, especially for voters who are likely to abstain. This novel underlying relationship proves paramount for democratic resilience in progressives scenarios with low voters turnout and voter fatigue supported by AI representatives: abstained voters are mitigated by recovering highly representative voting outcomes that are fairer. These interdisciplinary insights provide remarkable foundations for science, policymakers, and citizens to develop safeguards and resilience for AI risks in democratic innovations.","authors":["Srijoni Majumdar","Edith Elkind","Evangelos Pournaras"],"url":"https://arxiv.org/abs/2406.11871"}
{"created":"2025-04-11","title":"Demystifying Language Model Forgetting with Low-rank Example Associations","abstract":"Large Language models (LLMs) suffer from forgetting of upstream data when fine-tuned. Despite efforts on mitigating forgetting, few have investigated whether, and how forgotten upstream examples are dependent on newly learned tasks. Insights on such dependencies enable efficient and targeted mitigation of forgetting. In this paper, we empirically analyze forgetting that occurs in $N$ upstream examples of language modeling or instruction-tuning after fine-tuning LLMs on one of $M$ new tasks, visualized in $M\\times N$ matrices. We show that the matrices are often well-approximated with low-rank matrices, indicating the dominance of simple associations between the learned tasks and forgotten upstream examples. Leveraging the analysis, we predict forgetting of upstream examples when fine-tuning on unseen tasks with matrix completion over the empirical associations. This enables fast identification of most forgotten examples without expensive inference on the entire upstream data. The approach, despite simplicity, outperforms prior approaches that learn semantic relationships of learned tasks and upstream examples with LMs for predicting forgetting. We demonstrate the practical utility of our analysis by showing statistically significantly reduced forgetting as we upweight predicted examples for replay at fine-tuning. Project page: https://inklab.usc.edu/lm-forgetting-prediction/","authors":["Xisen Jin","Xiang Ren"],"url":"https://arxiv.org/abs/2406.14026"}
{"created":"2025-04-11","title":"HoTPP Benchmark: Are We Good at the Long Horizon Events Forecasting?","abstract":"Forecasting multiple future events within a given time horizon is essential for applications in finance, retail, social networks, and healthcare. Marked Temporal Point Processes (MTPP) provide a principled framework to model both the timing and labels of events. However, most existing research focuses on predicting only the next event, leaving long-horizon forecasting largely underexplored. To address this gap, we introduce HoTPP, the first benchmark specifically designed to rigorously evaluate long-horizon predictions. We identify shortcomings in widely used evaluation metrics, propose a theoretically grounded T-mAP metric, present strong statistical baselines, and offer efficient implementations of popular models. Our empirical results demonstrate that modern MTPP approaches often underperform simple statistical baselines. Furthermore, we analyze the diversity of predicted sequences and find that most methods exhibit mode collapse. Finally, we analyze the impact of autoregression and intensity-based losses on prediction quality, and outline promising directions for future research. The HoTPP source code, hyperparameters, and full evaluation results are available on GitHub.","authors":["Ivan Karpukhin","Foma Shipilov","Andrey Savchenko"],"url":"https://arxiv.org/abs/2406.14341"}
{"created":"2025-04-11","title":"Prompting or Fine-tuning? Exploring Large Language Models for Causal Graph Validation","abstract":"This study explores the capability of Large Language Models (LLMs) to evaluate causality in causal graphs generated by conventional statistical causal discovery methods-a task traditionally reliant on manual assessment by human subject matter experts. To bridge this gap in causality assessment, LLMs are employed to evaluate the causal relationships by determining whether a causal connection between variable pairs can be inferred from textual context. Our study compares two approaches: (1) prompting-based method for zero-shot and few-shot causal inference and, (2) fine-tuning language models for the causal relation prediction task. While prompt-based LLMs have demonstrated versatility across various NLP tasks, our experiments on biomedical and general-domain datasets show that fine-tuned models consistently outperform them, achieving up to a 20.5-point improvement in F1 score-even when using smaller-parameter language models. These findings provide valuable insights into the strengths and limitations of both approaches for causal graph evaluation.","authors":["Yuni Susanti","Nina Holsmoelle"],"url":"https://arxiv.org/abs/2406.16899"}
{"created":"2025-04-11","title":"PolygonGNN: Representation Learning for Polygonal Geometries with Heterogeneous Visibility Graph","abstract":"Polygon representation learning is essential for diverse applications, encompassing tasks such as shape coding, building pattern classification, and geographic question answering. While recent years have seen considerable advancements in this field, much of the focus has been on single polygons, overlooking the intricate inner- and inter-polygonal relationships inherent in multipolygons. To address this gap, our study introduces a comprehensive framework specifically designed for learning representations of polygonal geometries, particularly multipolygons. Central to our approach is the incorporation of a heterogeneous visibility graph, which seamlessly integrates both inner- and inter-polygonal relationships. To enhance computational efficiency and minimize graph redundancy, we implement a heterogeneous spanning tree sampling method. Additionally, we devise a rotation-translation invariant geometric representation, ensuring broader applicability across diverse scenarios. Finally, we introduce Multipolygon-GNN, a novel model tailored to leverage the spatial and semantic heterogeneity inherent in the visibility graph. Experiments on five real-world and synthetic datasets demonstrate its ability to capture informative representations for polygonal geometries. Code and data are available at \\href{https://github.com/dyu62/PolyGNN}{$github.com/dyu62/PolyGNN$}.","authors":["Dazhou Yu","Yuntong Hu","Yun Li","Liang Zhao"],"url":"https://arxiv.org/abs/2407.00742"}
{"created":"2025-04-11","title":"LLM-A*: Large Language Model Enhanced Incremental Heuristic Search on Path Planning","abstract":"Path planning is a fundamental scientific problem in robotics and autonomous navigation, requiring the derivation of efficient routes from starting to destination points while avoiding obstacles. Traditional algorithms like A* and its variants are capable of ensuring path validity but suffer from significant computational and memory inefficiencies as the state space grows. Conversely, large language models (LLMs) excel in broader environmental analysis through contextual understanding, providing global insights into environments. However, they fall short in detailed spatial and temporal reasoning, often leading to invalid or inefficient routes. In this work, we propose LLM-A*, an new LLM based route planning method that synergistically combines the precise pathfinding capabilities of A* with the global reasoning capability of LLMs. This hybrid approach aims to enhance pathfinding efficiency in terms of time and space complexity while maintaining the integrity of path validity, especially in large-scale scenarios. By integrating the strengths of both methodologies, LLM-A* addresses the computational and memory limitations of conventional algorithms without compromising on the validity required for effective pathfinding.","authors":["Silin Meng","Yiwei Wang","Cheng-Fu Yang","Nanyun Peng","Kai-Wei Chang"],"url":"https://arxiv.org/abs/2407.02511"}
{"created":"2025-04-11","title":"MedPix 2.0: A Comprehensive Multimodal Biomedical Data set for Advanced AI Applications with Retrieval Augmented Generation and Knowledge Graphs","abstract":"The increasing interest in developing Artificial Intelligence applications in the medical domain, suffers from the lack of high-quality data set, mainly due to privacy-related issues. In addition, the recent increase in Vision Language Models (VLM) leads to the need for multimodal medical data sets, where clinical reports and findings are attached to the corresponding medical scans. This paper illustrates the entire workflow for building the MedPix 2.0 data set. Starting with the well-known multimodal data set MedPix, mainly used by physicians, nurses, and healthcare students for Continuing Medical Education purposes, a semi-automatic pipeline was developed to extract visual and textual data followed by a manual curing procedure in which noisy samples were removed, thus creating a MongoDB database. Along with the data set, we developed a Graphical User Interface aimed at navigating efficiently the MongoDB instance and obtaining the raw data that can be easily used for training and/or fine-tuning VLMs. To enforce this point, in this work, we first recall DR-Minerva, a Retrieve Augmented Generation-based VLM model trained upon MedPix 2.0. DR-Minerva predicts the body part and the modality used to scan its input image. We also propose the extension of DR-Minerva with a Knowledge Graph that uses Llama 3.1 Instruct 8B, and leverages MedPix 2.0. The resulting architecture can be queried in a end-to-end manner, as a medical decision support system. MedPix 2.0 is available on GitHub https://github.com/CHILab1/MedPix-2.0","authors":["Irene Siragusa","Salvatore Contino","Massimo La Ciura","Rosario Alicata","Roberto Pirrone"],"url":"https://arxiv.org/abs/2407.02994"}
{"created":"2025-04-11","title":"Towards Federated RLHF with Aggregated Client Preference for LLMs","abstract":"Reinforcement learning with human feedback (RLHF) fine-tunes a pretrained large language model (LLM) using user preference data, enabling it to generate content aligned with human preferences. However, due to privacy concerns, users may be reluctant to share sensitive preference data. To address this, we propose utilizing Federated Learning (FL) techniques, allowing large-scale preference collection from diverse real-world users without requiring them to transmit data to a central server. Our federated RLHF methods (i.e., FedBis and FedBiscuit) encode each client's preferences into binary selectors and aggregate them to capture common preferences. In particular, FedBiscuit overcomes key challenges, such as preference heterogeneity and reward hacking, through innovative solutions like grouping clients with similar preferences to reduce heterogeneity and using multiple binary selectors to enhance LLM output quality. To evaluate the performance of the proposed methods, we establish the first federated RLHF benchmark with a heterogeneous human preference dataset. Experimental results show that by integrating the LLM with aggregated client preferences, FedBis and FedBiscuit significantly enhance the professionalism and readability of the generated content.","authors":["Feijie Wu","Xiaoze Liu","Haoyu Wang","Xingchen Wang","Lu Su","Jing Gao"],"url":"https://arxiv.org/abs/2407.03038"}
{"created":"2025-04-11","title":"Advancing Remote Medical Palpation through Cognition and Emotion","abstract":"This paper explores the cognitive and emotional processes involved in medical palpation to develop a more effective remote palpation system. Conventional remote palpation systems primarily rely on force feedback to convey a patient's tactile condition to doctors. However, an analysis of the palpation process suggests that its primary goal is not merely to assess the detailed tactile properties of the affected area but to integrate tactile sensations with other assessments, past experiences, memories, and patient reactions -- both physical and emotional -- to form a comprehensive understanding of the medical condition.","authors":["Matti Itkonen","Shotaro Okajima","Sayako Ueda","Alvaro Costa-Garcia","Yang Ningjia","Tadatoshi Kurogi","Takeshi Fujiwara","Shigeru Kurimoto","Shintaro Oyama","Masaomi Saeki","Michiro Yamamoto","Hidemasa Yoneda","Hitoshi Hirata","Shingo Shimoda"],"url":"https://arxiv.org/abs/2407.05595"}
{"created":"2025-04-11","title":"A Survey on Mixture of Experts in Large Language Models","abstract":"Large language models (LLMs) have garnered unprecedented advancements across diverse fields, ranging from natural language processing to computer vision and beyond. The prowess of LLMs is underpinned by their substantial model size, extensive and diverse datasets, and the vast computational power harnessed during training, all of which contribute to the emergent abilities of LLMs (e.g., in-context learning) that are not present in small models. Within this context, the mixture of experts (MoE) has emerged as an effective method for substantially scaling up model capacity with minimal computation overhead, gaining significant attention from academia and industry. Despite its growing prevalence, there lacks a systematic and comprehensive review of the literature on MoE. This survey seeks to bridge that gap, serving as an essential resource for researchers delving into the intricacies of MoE. We first briefly introduce the structure of the MoE layer, followed by proposing a new taxonomy of MoE. Next, we overview the core designs for various MoE models including both algorithmic and systemic aspects, alongside collections of available open-source implementations, hyperparameter configurations and empirical evaluations. Furthermore, we delineate the multifaceted applications of MoE in practice, and outline some potential directions for future research. To facilitate ongoing updates and the sharing of cutting-edge advances in MoE research, we have established a resource repository at https://github.com/withinmiaov/A-Survey-on-Mixture-of-Experts-in-LLMs.","authors":["Weilin Cai","Juyong Jiang","Fan Wang","Jing Tang","Sunghun Kim","Jiayi Huang"],"url":"https://arxiv.org/abs/2407.06204"}
{"created":"2025-04-11","title":"Sampling from mixture distributions based on regime-switching diffusions","abstract":"It is proposed to use stochastic differential equations with state-dependent switching rates (SDEwS) for sampling from finite mixture distributions. An Euler scheme with constant time step for SDEwS is considered. It is shown that the scheme converges with order one in weak sense and also in the ergodic limit. Numerical experiments illustrate the use of SDEwS for sampling from mixture distributions and confirm the theoretical results.","authors":["M. V. Tretyakov"],"url":"https://arxiv.org/abs/2407.13389"}
{"created":"2025-04-11","title":"Sublinear Regret for a Class of Continuous-Time Linear-Quadratic Reinforcement Learning Problems","abstract":"We study reinforcement learning (RL) for a class of continuous-time linear-quadratic (LQ) control problems for diffusions, where states are scalar-valued and running control rewards are absent but volatilities of the state processes depend on both state and control variables. We apply a model-free approach that relies neither on knowledge of model parameters nor on their estimations, and devise an RL algorithm to learn the optimal policy parameter directly. Our main contributions include the introduction of an exploration schedule and a regret analysis of the proposed algorithm. We provide the convergence rate of the policy parameter to the optimal one, and prove that the algorithm achieves a regret bound of $O(N^{\\frac{3}{4}})$ up to a logarithmic factor, where $N$ is the number of learning episodes. We conduct a simulation study to validate the theoretical results and demonstrate the effectiveness and reliability of the proposed algorithm. We also perform numerical comparisons between our method and those of the recent model-based stochastic LQ RL studies adapted to the state- and control-dependent volatility setting, demonstrating a better performance of the former in terms of regret bounds.","authors":["Yilie Huang","Yanwei Jia","Xun Yu Zhou"],"url":"https://arxiv.org/abs/2407.17226"}
{"created":"2025-04-11","title":"Carbon-Efficient Software Design and Development: A Systematic Literature Review","abstract":"The ICT sector, responsible for 2% of global carbon emissions, is under scrutiny calling for methodologies and tools to design and develop software in an environmentally sustainable-by-design manner. However, the software engineering solutions for designing and developing carbon-efficient software are currently scattered over multiple different pieces of literature, which makes it difficult to consult the body of knowledge on the topic. In this article, we precisely conduct a systematic literature review on state-of-the-art proposals for designing and developing carbon-efficient software. We identify and analyse 65 primary studies by classifying them through a taxonomy aimed at answering the 5W1H questions of carbon-efficient software design and development. We first provide a reasoned overview and discussion of the existing guidelines, reference models, measurement solutions and techniques for measuring, reducing, or minimising the carbon footprint of software. Ultimately, we identify open challenges and research gaps, offering insights for future work in this field.","authors":["Ornela Danushi","Stefano Forti","Jacopo Soldani"],"url":"https://arxiv.org/abs/2407.19901"}
{"created":"2025-04-11","title":"MoC-System: Efficient Fault Tolerance for Sparse Mixture-of-Experts Model Training","abstract":"As large language models continue to scale up, distributed training systems have expanded beyond 10k nodes, intensifying the importance of fault tolerance. Checkpoint has emerged as the predominant fault tolerance strategy, with extensive studies dedicated to optimizing its efficiency. However, the advent of the sparse Mixture-of-Experts (MoE) model presents new challenges due to the substantial increase in model size, despite comparable computational demands to dense models.","authors":["Weilin Cai","Le Qin","Jiayi Huang"],"url":"https://arxiv.org/abs/2408.04307"}
{"created":"2025-04-11","title":"SketchRef: a Multi-Task Evaluation Benchmark for Sketch Synthesis","abstract":"Sketching is a powerful artistic technique for capturing essential visual information about real-world objects and has increasingly attracted attention in image synthesis research. However, the field lacks a unified benchmark to evaluate the performance of various synthesis methods. To address this, we propose SketchRef, the first comprehensive multi-task evaluation benchmark for sketch synthesis. SketchRef fully leverages the shared characteristics between sketches and reference photos. It introduces two primary tasks: category prediction and structural consistency estimation, the latter being largely overlooked in previous studies. These tasks are further divided into five sub-tasks across four domains: animals, common things, human body, and faces. Recognizing the inherent trade-off between recognizability and simplicity in sketches, we are the first to quantify this balance by introducing a recognizability calculation method constrained by simplicity, mRS, ensuring fair and meaningful evaluations. To validate our approach, we collected 7,920 responses from art enthusiasts, confirming the effectiveness of our proposed evaluation metrics. Additionally, we evaluate the performance of existing sketch synthesis methods on our benchmark, highlighting their strengths and weaknesses. We hope this study establishes a standardized benchmark and offers valuable insights for advancing sketch synthesis algorithms.","authors":["Xingyue Lin","Xingjian Hu","Shuai Peng","Jianhua Zhu","Liangcai Gao"],"url":"https://arxiv.org/abs/2408.08623"}
{"created":"2025-04-11","title":"Beyond the Hype: A dispassionate look at vision-language models in medical scenario","abstract":"Recent advancements in Large Vision-Language Models (LVLMs) have demonstrated remarkable capabilities across diverse tasks, garnering significant attention in AI communities. However, their performance and reliability in specialized domains such as medicine remain insufficiently assessed. In particular, most assessments over-concentrate on evaluating VLMs based on simple Visual Question Answering (VQA) on multi-modality data, while ignoring the in-depth characteristics of LVLMs. In this study, we introduce RadVUQA, a novel Radiological Visual Understanding and Question Answering benchmark, to comprehensively evaluate existing LVLMs. RadVUQA mainly validates LVLMs across five dimensions: 1) Anatomical understanding, assessing the models' ability to visually identify biological structures; 2) Multimodal comprehension, which involves the capability of interpreting linguistic and visual instructions to produce desired outcomes; 3) Quantitative and spatial reasoning, evaluating the models' spatial awareness and proficiency in combining quantitative analysis with visual and linguistic information; 4) Physiological knowledge, measuring the models' capability to comprehend functions and mechanisms of organs and systems; and 5) Robustness, which assesses the models' capabilities against unharmonized and synthetic data. The results indicate that both generalized LVLMs and medical-specific LVLMs have critical deficiencies with weak multimodal comprehension and quantitative reasoning capabilities. Our findings reveal the large gap between existing LVLMs and clinicians, highlighting the urgent need for more robust and intelligent LVLMs. The code is available at https://github.com/Nandayang/RadVUQA","authors":["Yang Nan","Huichi Zhou","Xiaodan Xing","Guang Yang"],"url":"https://arxiv.org/abs/2408.08704"}
{"created":"2025-04-11","title":"Atlas Gaussians Diffusion for 3D Generation","abstract":"Using the latent diffusion model has proven effective in developing novel 3D generation techniques. To harness the latent diffusion model, a key challenge is designing a high-fidelity and efficient representation that links the latent space and the 3D space. In this paper, we introduce Atlas Gaussians, a novel representation for feed-forward native 3D generation. Atlas Gaussians represent a shape as the union of local patches, and each patch can decode 3D Gaussians. We parameterize a patch as a sequence of feature vectors and design a learnable function to decode 3D Gaussians from the feature vectors. In this process, we incorporate UV-based sampling, enabling the generation of a sufficiently large, and theoretically infinite, number of 3D Gaussian points. The large amount of 3D Gaussians enables the generation of high-quality details. Moreover, due to local awareness of the representation, the transformer-based decoding procedure operates on a patch level, ensuring efficiency. We train a variational autoencoder to learn the Atlas Gaussians representation, and then apply a latent diffusion model on its latent space for learning 3D Generation. Experiments show that our approach outperforms the prior arts of feed-forward native 3D generation. Project page: https://yanghtr.github.io/projects/atlas_gaussians.","authors":["Haitao Yang","Yuan Dong","Hanwen Jiang","Dejia Xu","Georgios Pavlakos","Qixing Huang"],"url":"https://arxiv.org/abs/2408.13055"}
{"created":"2025-04-11","title":"No Detail Left Behind: Revisiting Self-Retrieval for Fine-Grained Image Captioning","abstract":"Image captioning systems are unable to generate fine-grained captions as they are trained on data that is either noisy (alt-text) or generic (human annotations). This is further exacerbated by maximum likelihood training that encourages generation of frequently occurring phrases. Previous works have tried to address this limitation by fine-tuning captioners with a self-retrieval (SR) reward. However, we find that SR fine-tuning has a tendency to reduce caption faithfulness and even hallucinate. In this work, we circumvent this bottleneck by improving the MLE initialization of the captioning system and designing a curriculum for the SR fine-tuning process. To this extent, we present (1) Visual Caption Boosting, a novel framework to instill fine-grainedness in generic image captioning datasets while remaining anchored in human annotations; and (2) BagCurri, a carefully designed training curriculum that more optimally leverages the contrastive nature of the self-retrieval reward. Jointly, they enable the captioner to describe fine-grained aspects in the image while preserving faithfulness to ground-truth captions. Our approach outperforms previous work by +8.9% on SR against 99 random distractors (RD100) (Dessi et al., 2023); and +7.6% on ImageCoDe. Additionally, existing metrics to evaluate captioning systems fail to reward diversity or evaluate a model's fine-grained understanding ability. Our third contribution addresses this by proposing self-retrieval from the lens of evaluation. We introduce TrueMatch, a benchmark comprising bags of highly similar images that uses SR to assess the captioner's ability to capture subtle visual distinctions. We evaluate and compare several state-of-the-art open-source MLLMs on TrueMatch, and find that our SR approach outperforms them all by a significant margin (e.g. +4.8% - 7.1% over Cambrian) while having 1-2 orders of magnitude fewer parameters.","authors":["Manu Gaur","Darshan Singh","Makarand Tapaswi"],"url":"https://arxiv.org/abs/2409.03025"}
{"created":"2025-04-11","title":"A Deep Generative Learning Approach for Two-stage Adaptive Robust Optimization","abstract":"Two-stage adaptive robust optimization (ARO) is a powerful approach for planning under uncertainty, balancing first-stage decisions with recourse decisions made after uncertainty is realized. To account for uncertainty, modelers typically define a simple uncertainty set over which potential outcomes are considered. However, classical methods for defining these sets unintentionally capture a wide range of unrealistic outcomes, resulting in overly-conservative and costly planning in anticipation of unlikely contingencies. In this work, we introduce AGRO, a solution algorithm that performs adversarial generation for two-stage adaptive robust optimization using a variational autoencoder. AGRO generates high-dimensional contingencies that are simultaneously adversarial and realistic, improving the robustness of first-stage decisions at a lower planning cost than standard methods. To ensure generated contingencies lie in high-density regions of the uncertainty distribution, AGRO defines a tight uncertainty set as the image of \"latent\" uncertainty sets under the VAE decoding transformation. Projected gradient ascent is then used to maximize recourse costs over the latent uncertainty sets by leveraging differentiable optimization methods. We demonstrate the cost-efficiency of AGRO by applying it to both a synthetic production-distribution problem and a real-world power system expansion setting. We show that AGRO outperforms the standard column-and-constraint algorithm by up to 1.8% in production-distribution planning and up to 11.6% in power system expansion.","authors":["Aron Brenner","Rahman Khorramfar","Jennifer Sun","Saurabh Amin"],"url":"https://arxiv.org/abs/2409.03731"}
{"created":"2025-04-11","title":"MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation","abstract":"Processing long contexts presents a significant challenge for large language models (LLMs). While recent advancements allow LLMs to handle much longer contexts than before (e.g., 32K or 128K tokens), it is computationally expensive and can still be insufficient for many applications. Retrieval-Augmented Generation (RAG) is considered a promising strategy to address this problem. However, conventional RAG methods face inherent limitations because of two underlying requirements: 1) explicitly stated queries, and 2) well-structured knowledge. These conditions, however, do not hold in general long-context processing tasks.","authors":["Hongjin Qian","Zheng Liu","Peitian Zhang","Kelong Mao","Defu Lian","Zhicheng Dou","Tiejun Huang"],"url":"https://arxiv.org/abs/2409.05591"}
{"created":"2025-04-11","title":"PingPong: A Benchmark for Role-Playing Language Models with User Emulation and Multi-Model Evaluation","abstract":"We introduce a benchmark for evaluating the role-playing capabilities of language models. Our approach leverages different language models to simulate users in dynamic, multi-turn conversations and assess the resulting dialogues. Our methodology involves three main components: a player model that adopts a specific character role, an interrogator model that simulates user behavior in a specific situation, and a judge model ensemble that evaluates conversation quality with 3 metrics: character consistency, entertainment value, and language fluency. We evaluated more than 40 models in both English and Russian, with each model participating in 64 conversations with 8 characters and 8 situations. We conducted experiments comparing automated evaluations with human annotations to validate our approach, demonstrating strong correlations across multiple criteria. This work provides a foundation for a robust and dynamic evaluation of different model capabilities in interactive scenarios.","authors":["Ilya Gusev"],"url":"https://arxiv.org/abs/2409.06820"}
{"created":"2025-04-11","title":"Leveraging AI-Generated Emotional Self-Voice to Nudge People towards their Ideal Selves","abstract":"Emotions, shaped by past experiences, significantly influence decision-making and goal pursuit. Traditional cognitive-behavioral techniques for personal development rely on mental imagery to envision ideal selves, but may be less effective for individuals who struggle with visualization. This paper introduces Emotional Self-Voice (ESV), a novel system combining emotionally expressive language models and voice cloning technologies to render customized responses in the user's own voice. We investigate the potential of ESV to nudge individuals towards their ideal selves in a study with 60 participants. Across all three conditions (ESV, text-only, and mental imagination), we observed an increase in resilience, confidence, motivation, and goal commitment, and the ESV condition was perceived as uniquely engaging and personalized. We discuss the implications of designing generated self-voice systems as a personalized behavioral intervention for different scenarios.","authors":["Cathy Mengying Fang","Phoebe Chua","Samantha Chan","Joanne Leong","Andria Bao","Pattie Maes"],"url":"https://arxiv.org/abs/2409.11531"}
{"created":"2025-04-11","title":"GenCAD: Image-Conditioned Computer-Aided Design Generation with Transformer-Based Contrastive Representation and Diffusion Priors","abstract":"The creation of manufacturable and editable 3D shapes through Computer-Aided Design (CAD) remains a highly manual and time-consuming task, hampered by the complex topology of boundary representations of 3D solids and unintuitive design tools. While most work in the 3D shape generation literature focuses on representations like meshes, voxels, or point clouds, practical engineering applications demand the modifiability and manufacturability of CAD models and the ability for multi-modal conditional CAD model generation. This paper introduces GenCAD, a generative model that employs autoregressive transformers with a contrastive learning framework and latent diffusion models to transform image inputs into parametric CAD command sequences, resulting in editable 3D shape representations. Extensive evaluations demonstrate that GenCAD significantly outperforms existing state-of-the-art methods in terms of the unconditional and conditional generations of CAD models. Additionally, the contrastive learning framework of GenCAD facilitates the retrieval of CAD models using image queries from large CAD databases, which is a critical challenge within the CAD community. Our results provide a significant step forward in highlighting the potential of generative models to expedite the entire design-to-production pipeline and seamlessly integrate different design modalities.","authors":["Md Ferdous Alam","Faez Ahmed"],"url":"https://arxiv.org/abs/2409.16294"}
{"created":"2025-04-11","title":"A Simple but Strong Baseline for Sounding Video Generation: Effective Adaptation of Audio and Video Diffusion Models for Joint Generation","abstract":"In this work, we build a simple but strong baseline for sounding video generation. Given base diffusion models for audio and video, we integrate them with additional modules into a single model and train it to make the model jointly generate audio and video. To enhance alignment between audio-video pairs, we introduce two novel mechanisms in our model. The first one is timestep adjustment, which provides different timestep information to each base model. It is designed to align how samples are generated along with timesteps across modalities. The second one is a new design of the additional modules, termed Cross-Modal Conditioning as Positional Encoding (CMC-PE). In CMC-PE, cross-modal information is embedded as if it represents temporal position information, and the embeddings are fed into the model like positional encoding. Compared with the popular cross-attention mechanism, CMC-PE provides a better inductive bias for temporal alignment in the generated data. Experimental results validate the effectiveness of the two newly introduced mechanisms and also demonstrate that our method outperforms existing methods.","authors":["Masato Ishii","Akio Hayakawa","Takashi Shibuya","Yuki Mitsufuji"],"url":"https://arxiv.org/abs/2409.17550"}
{"created":"2025-04-11","title":"Learning Occlusion-aware Decision-making from Agent Interaction via Active Perception","abstract":"Occlusion-aware decision-making is essential in autonomous driving due to the high uncertainty of various occlusions. Recent occlusion-aware decision-making methods encounter issues such as high computational complexity, scenario scalability challenges, or reliance on limited expert data. Benefiting from automatically generating data by exploration randomization, we uncover that reinforcement learning (RL) may show promise in occlusion-aware decision-making. However, previous occlusion-aware RL faces challenges in expanding to various dynamic and static occlusion scenarios, low learning efficiency, and lack of predictive ability. To address these issues, we introduce Pad-AI, a self-reinforcing framework to learn occlusion-aware decision-making through active perception. Pad-AI utilizes vectorized representation to represent occluded environments efficiently and learns over the semantic motion primitives to focus on high-level active perception exploration. Furthermore, Pad-AI integrates prediction and RL within a unified framework to provide risk-aware learning and security guarantees. Our framework was tested in challenging scenarios under both dynamic and static occlusions and demonstrated efficient and general perception-aware exploration performance to other strong baselines in closed-loop evaluations.","authors":["Jie Jia","Yiming Shu","Zhongxue Gan","Wenchao Ding"],"url":"https://arxiv.org/abs/2409.17618"}
{"created":"2025-04-11","title":"Replacing Paths with Connection-Biased Attention for Knowledge Graph Completion","abstract":"Knowledge graph (KG) completion aims to identify additional facts that can be inferred from the existing facts in the KG. Recent developments in this field have explored this task in the inductive setting, where at test time one sees entities that were not present during training; the most performant models in the inductive setting have employed path encoding modules in addition to standard subgraph encoding modules. This work similarly focuses on KG completion in the inductive setting, without the explicit use of path encodings, which can be time-consuming and introduces several hyperparameters that require costly hyperparameter optimization. Our approach uses a Transformer-based subgraph encoding module only; we introduce connection-biased attention and entity role embeddings into the subgraph encoding module to eliminate the need for an expensive and time-consuming path encoding module. Evaluations on standard inductive KG completion benchmark datasets demonstrate that our \\textbf{C}onnection-\\textbf{B}iased \\textbf{Li}nk \\textbf{P}rediction (CBLiP) model has superior performance to models that do not use path information. Compared to models that utilize path information, CBLiP shows competitive or superior performance while being faster. Additionally, to show that the effectiveness of connection-biased attention and entity role embeddings also holds in the transductive setting, we compare CBLiP's performance on the relation prediction task in the transductive setting.","authors":["Sharmishtha Dutta","Alex Gittens","Mohammed J. Zaki","Charu C. Aggarwal"],"url":"https://arxiv.org/abs/2410.00876"}
{"created":"2025-04-11","title":"CodeMMLU: A Multi-Task Benchmark for Assessing Code Understanding & Reasoning Capabilities of CodeLLMs","abstract":"Recent advances in Code Large Language Models (CodeLLMs) have primarily focused on open-ended code generation, often overlooking the crucial aspect of code understanding and reasoning. To bridge this gap, we introduce CodeMMLU, a comprehensive multiple-choice benchmark designed to evaluate the depth of software and code comprehension in LLMs. CodeMMLU includes nearly 20,000 questions spanning diverse domains, including code analysis, defect detection, and software engineering principles across multiple programming languages. Unlike traditional benchmarks that emphasize code generation, CodeMMLU assesses a model's ability to reason about programs across a wide-range of tasks such as code repair, execution reasoning, and fill-in-the-blank challenges. Our extensive evaluation reveals that even state-of-the-art models struggle with CodeMMLU, highlighting significant gaps in comprehension beyond generation. By emphasizing the essential connection between code understanding and effective AI-assisted development, CodeMMLU provides a critical resource for advancing more reliable and capable coding assistants.","authors":["Dung Nguyen Manh","Thang Phan Chau","Nam Le Hai","Thong T. Doan","Nam V. Nguyen","Quang Pham","Nghi D. Q. Bui"],"url":"https://arxiv.org/abs/2410.01999"}
{"created":"2025-04-11","title":"LLM Safeguard is a Double-Edged Sword: Exploiting False Positives for Denial-of-Service Attacks","abstract":"Safety is a paramount concern for large language models (LLMs) in open deployment, motivating the development of safeguard methods that enforce ethical and responsible use through safety alignment or guardrail mechanisms. Jailbreak attacks that exploit the \\emph{false negatives} of safeguard methods have emerged as a prominent research focus in the field of LLM security. However, we found that the malicious attackers could also exploit false positives of safeguards, i.e., fooling the safeguard model to block safe content mistakenly, leading to a denial-of-service (DoS) affecting LLM users. To bridge the knowledge gap of this overlooked threat, we explore multiple attack methods that include inserting a short adversarial prompt into user prompt templates and corrupting the LLM on the server by poisoned fine-tuning. In both ways, the attack triggers safeguard rejections of user requests from the client. Our evaluation demonstrates the severity of this threat across multiple scenarios. For instance, in the scenario of white-box adversarial prompt injection, the attacker can use our optimization process to automatically generate seemingly safe adversarial prompts, approximately only 30 characters long, that universally block over 97% of user requests on Llama Guard 3. These findings reveal a new dimension in LLM safeguard evaluation -- adversarial robustness to false positives.","authors":["Qingzhao Zhang","Ziyang Xiong","Z. Morley Mao"],"url":"https://arxiv.org/abs/2410.02916"}
{"created":"2025-04-11","title":"AuroraCap: Efficient, Performant Video Detailed Captioning and a New Benchmark","abstract":"Video detailed captioning is a key task which aims to generate comprehensive and coherent textual descriptions of video content, benefiting both video understanding and generation. In this paper, we propose AuroraCap, a video captioner based on a large multimodal model. We follow the simplest architecture design without additional parameters for temporal modeling. To address the overhead caused by lengthy video sequences, we implement the token merging strategy, reducing the number of input visual tokens. Surprisingly, we found that this strategy results in little performance loss. AuroraCap shows superior performance on various video and image captioning benchmarks, for example, obtaining a CIDEr of 88.9 on Flickr30k, beating GPT-4V (55.3) and Gemini-1.5 Pro (82.2). However, existing video caption benchmarks only include simple descriptions, consisting of a few dozen words, which limits research in this field. Therefore, we develop VDC, a video detailed captioning benchmark with over one thousand carefully annotated structured captions. In addition, we propose a new LLM-assisted metric VDCscore for bettering evaluation, which adopts a divide-and-conquer strategy to transform long caption evaluation into multiple short question-answer pairs. With the help of human Elo ranking, our experiments show that this benchmark better correlates with human judgments of video detailed captioning quality.","authors":["Wenhao Chai","Enxin Song","Yilun Du","Chenlin Meng","Vashisht Madhavan","Omer Bar-Tal","Jenq-Neng Hwang","Saining Xie","Christopher D. Manning"],"url":"https://arxiv.org/abs/2410.03051"}
{"created":"2025-04-11","title":"Distribution Grids May Be a Barrier To Residential Electrification","abstract":"Replacing fossil-fueled appliances and vehicles with electric alternatives can reduce greenhouse gas emissions and air pollution in many settings. However, residential electrification can also raise electricity demand beyond the safe limits of electrical infrastructure. This can increase the risk of blackouts or require grid reinforcement that is often slow and expensive. Here, we estimate the physical and economic impacts on distribution grids of electrifying all housing and personal vehicles in each county of the lower 48 United States. We find that space heating is the main driver of grid impacts, with the coldest regions seeing demand peaks up to five times higher than today's peaks. Accommodating electrification of all housing and personal vehicles is estimated to require 600 GW of distribution grid reinforcement nationally, at a cost of \\$350 to \\$790 billion, or \\$2,800 to \\$6,400 per household (95% confidence intervals). However, demand-side management could eliminate three-quarters of grid reinforcement costs.","authors":["Priyadarshan","Constance Crozier","Kyri Baker","Kevin Kircher"],"url":"https://arxiv.org/abs/2410.04540"}
{"created":"2025-04-11","title":"ActiView: Evaluating Active Perception Ability for Multimodal Large Language Models","abstract":"Active perception, a crucial human capability, involves setting a goal based on the current understanding of the environment and performing actions to achieve that goal. Despite significant efforts in evaluating Multimodal Large Language Models (MLLMs), active perception has been largely overlooked. To address this gap, we propose a novel benchmark named ActiView to evaluate active perception in MLLMs. We focus on a specialized form of Visual Question Answering (VQA) that eases and quantifies the evaluation yet challenging for existing MLLMs. Meanwhile, intermediate reasoning behaviors of models are also discussed. Given an image, we restrict the perceptual field of a model, requiring it to actively zoom or shift its perceptual field based on reasoning to answer the question successfully. We conduct extensive evaluation over 30 models, including proprietary and open-source models, and observe that restricted perceptual fields play a significant role in enabling active perception. Results reveal a significant gap in the active perception capability of MLLMs, indicating that this area deserves more attention. We hope that ActiView could help develop methods for MLLMs to understand multimodal inputs in more natural and holistic ways.","authors":["Ziyue Wang","Chi Chen","Fuwen Luo","Yurui Dong","Yuanchi Zhang","Yuzhuang Xu","Xiaolong Wang","Peng Li","Yang Liu"],"url":"https://arxiv.org/abs/2410.04659"}
{"created":"2025-04-11","title":"A posteriori error estimates for Schr{\\\"o}dinger operators discretized with linear combinations of atomic orbitals","abstract":"We establish guaranteed and practically computable a posteriori error bounds for source problems and eigenvalue problems involving linear Schr{\\\"o}dinger operators with atom-centered potentials discretized with linear combinations of atomic orbitals. We show that the energy norm of the discretization error can be estimated by the dual energy norm of the residual, that further decomposes into atomic contributions, characterizing the error localized on atoms. Moreover, we show that the practical computation of the dual norms of atomic residuals involves diagonalizing radial Schr{\\\"o}dinger operators which can easily be precomputed in practice. We provide numerical illustrations of the performance of such a posteriori analysis on several test cases, showing that the error bounds accurately estimate the error, and that the localized error components allow for optimized adaptive basis sets.","authors":["Genevi\\`eve Dusson (LMB)","Mi-Song Dupuy (LJLL)","Ioanna-Maria Lygatsika (CEA/DAM)"],"url":"https://arxiv.org/abs/2410.04943"}
{"created":"2025-04-11","title":"Learning Equivariant Non-Local Electron Density Functionals","abstract":"The accuracy of density functional theory hinges on the approximation of non-local contributions to the exchange-correlation (XC) functional. To date, machine-learned and human-designed approximations suffer from insufficient accuracy, limited scalability, or dependence on costly reference data. To address these issues, we introduce Equivariant Graph Exchange Correlation (EG-XC), a novel non-local XC functional based on equivariant graph neural networks (GNNs). Where previous works relied on semi-local functionals or fixed-size descriptors of the density, we compress the electron density into an SO(3)-equivariant nuclei-centered point cloud for efficient non-local atomic-range interactions. By applying an equivariant GNN on this point cloud, we capture molecular-range interactions in a scalable and accurate manner. To train EG-XC, we differentiate through a self-consistent field solver requiring only energy targets. In our empirical evaluation, we find EG-XC to accurately reconstruct `gold-standard' CCSD(T) energies on MD17. On out-of-distribution conformations of 3BPA, EG-XC reduces the relative MAE by 35% to 50%. Remarkably, EG-XC excels in data efficiency and molecular size extrapolation on QM9, matching force fields trained on 5 times more and larger molecules. On identical training sets, EG-XC yields on average 51% lower MAEs.","authors":["Nicholas Gao","Eike Eberhard","Stephan G\\\"unnemann"],"url":"https://arxiv.org/abs/2410.07972"}
{"created":"2025-04-11","title":"Human and LLM Biases in Hate Speech Annotations: A Socio-Demographic Analysis of Annotators and Targets","abstract":"The rise of online platforms exacerbated the spread of hate speech, demanding scalable and effective detection. However, the accuracy of hate speech detection systems heavily relies on human-labeled data, which is inherently susceptible to biases. While previous work has examined the issue, the interplay between the characteristics of the annotator and those of the target of the hate are still unexplored. We fill this gap by leveraging an extensive dataset with rich socio-demographic information of both annotators and targets, uncovering how human biases manifest in relation to the target's attributes. Our analysis surfaces the presence of widespread biases, which we quantitatively describe and characterize based on their intensity and prevalence, revealing marked differences. Furthermore, we compare human biases with those exhibited by persona-based LLMs. Our findings indicate that while persona-based LLMs do exhibit biases, these differ significantly from those of human annotators. Overall, our work offers new and nuanced results on human biases in hate speech annotations, as well as fresh insights into the design of AI-driven hate speech detection systems.","authors":["Tommaso Giorgi","Lorenzo Cima","Tiziano Fagni","Marco Avvenuti","Stefano Cresci"],"url":"https://arxiv.org/abs/2410.07991"}
{"created":"2025-04-11","title":"Levels of Binary Equivalence for the Comparison of Binaries from Alternative Builds","abstract":"In response to challenges in software supply chain security, several organisations have created infrastructures to independently build commodity open source projects and release the resulting binaries. Build platform variability can strengthen security as it facilitates the detection of compromised build environments. Furthermore, by improving the security posture of the build platform and collecting provenance information during the build, the resulting artifacts can be used with greater trust. Such offerings are now available from Google, Oracle and RedHat. The availability of multiple binaries built from the same sources creates new challenges and opportunities, and raises questions such as: 'Does build A confirm the integrity of build B?' or 'Can build A reveal a compromised build B?'. To answer such questions requires a notion of equivalence between binaries. We demonstrate that the obvious approach based on bitwise equality has significant shortcomings in practice, and that there is value in opting for alternative notions. We conceptualise this by introducing levels of equivalence, inspired by clone detection types.","authors":["Jens Dietrich","Tim White","Behnaz Hassanshahi","Paddy Krishnan"],"url":"https://arxiv.org/abs/2410.08427"}
{"created":"2025-04-11","title":"AdvBDGen: Adversarially Fortified Prompt-Specific Fuzzy Backdoor Generator Against LLM Alignment","abstract":"With the growing adoption of reinforcement learning with human feedback (RLHF) for aligning large language models (LLMs), the risk of backdoor installation during alignment has increased, leading to unintended and harmful behaviors. Existing backdoor triggers are typically limited to fixed word patterns, making them detectable during data cleaning and easily removable post-poisoning. In this work, we explore the use of prompt-specific paraphrases as backdoor triggers, enhancing their stealth and resistance to removal during LLM alignment. We propose AdvBDGen, an adversarially fortified generative fine-tuning framework that automatically generates prompt-specific backdoors that are effective, stealthy, and transferable across models. AdvBDGen employs a generator-discriminator pair, fortified by an adversary, to ensure the installability and stealthiness of backdoors. It enables the crafting and successful installation of complex triggers using as little as 3% of the fine-tuning data. Once installed, these backdoors can jailbreak LLMs during inference, demonstrate improved stability against perturbations compared to traditional constant triggers, and are more challenging to remove. These findings underscore an urgent need for the research community to develop more robust defenses against adversarial backdoor threats in LLM alignment.","authors":["Pankayaraj Pathmanathan","Udari Madhushani Sehwag","Michael-Andrei Panaitescu-Liess","Furong Huang"],"url":"https://arxiv.org/abs/2410.11283"}
{"created":"2025-04-11","title":"A spatial hypergraph model to smoothly interpolate between pairwise graphs and hypergraphs to study higher-order structures","abstract":"We introduce a spatial graph and hypergraph model that smoothly interpolates between a graph with purely pairwise edges and a graph where all connections are in large hyperedges. The key component is a spatial clustering resolution parameter that varies between assigning all the vertices in a spatial region to individual clusters, resulting in the pairwise case, to assigning all the vertices in a spatial region to a single cluster, which results in the large hyperedge case. An important outcome of this model is that the spatial structure is invariant to the choice of hyperedges. Consequently, this model enables us to study clustering coefficients, graph diffusion, and epidemic spread and how their behavior changes as a function of the higher-order structure in the network with a fixed spatial substrate. We hope that our model will find future uses to distill or explain other behaviors in higher-order networks.","authors":["Omar Eldaghar","Yu Zhu","David F. Gleich"],"url":"https://arxiv.org/abs/2410.12688"}
{"created":"2025-04-11","title":"Holstein-Friesian Re-Identification using Multiple Cameras and Self-Supervision on a Working Farm","abstract":"We present MultiCamCows2024, a farm-scale image dataset filmed across multiple cameras for the biometric identification of individual Holstein-Friesian cattle exploiting their unique black and white coat-patterns. Captured by three ceiling-mounted visual sensors covering adjacent barn areas over seven days on a working dairy farm, the dataset comprises 101,329 images of 90 cows, plus underlying original CCTV footage. The dataset is provided with full computer vision recognition baselines, that is both a supervised and self-supervised learning framework for individual cow identification trained on cattle tracklets. We report a performance above 96% single image identification accuracy from the dataset and demonstrate that combining data from multiple cameras during learning enhances self-supervised identification. We show that our framework enables automatic cattle identification, barring only the simple human verification of tracklet integrity during data collection. Crucially, our study highlights that multi-camera, supervised and self-supervised components in tandem not only deliver highly accurate individual cow identification, but also achieve this efficiently with no labelling of cattle identities by humans. We argue that this improvement in efficacy has practical implications for livestock management, behaviour analysis, and agricultural monitoring. For reproducibility and practical ease of use, we publish all key software and code including re-identification components and the species detector with this paper, available at https://tinyurl.com/MultiCamCows2024.","authors":["Phoenix Yu","Tilo Burghardt","Andrew W Dowsey","Neill W Campbell"],"url":"https://arxiv.org/abs/2410.12695"}
{"created":"2025-04-11","title":"Inverter Output Impedance Estimation in Power Networks: A Variable Direction Forgetting Recursive-Least-Square Algorithm Based Approach","abstract":"As inverter-based loads and energy sources become increasingly prevalent, accurate estimation of line impedance between inverters and the grid is essential for optimizing performance and enhancing control strategies. This paper presents a non-invasive method for estimating output-line impedance using measurements local to the inverter. It provides a specific method for signal conditioning of signals measured at the inverter, which makes the measured data better suited to estimation algorithms. An algorithm based on the Variable Direction Forgetting Recursive Least Squares (VDF-RLS) method is introduced, which leverages these conditioned signals for precise impedance estimation. The signal conditioning process transforms measurements into the direct-quadrature (dq) coordinate frame, where the rotating frame frequency is determined to facilitate a simpler and more accurate estimation. This frequency is implemented using a secondary Phase-Locked Loop (PLL) to attenuate grid voltage measurement variations. By isolating the variation-sensitive q-axis and relying solely on the less sensitive d-axis, the method further minimizes the impact of variations. The VDF-RLS estimation method achieves rapid adaptation while ensuring stability in the absence of persistent excitation by selectively discarding outdated data during updates. Proposed conditioning and estimation methods are non-invasive; estimations are solely done using measured outputs, and no signal is injected into the power network. Simulation results demonstrate a significant improvement in impedance estimation stability, particularly in low-excitation conditions, where the VDF-RLS method achieves more than three time lower error compared to existing approaches such as constant forgetting RLS and the Kalman filter.","authors":["Jaesang Park","Alireza Askarian","Srinivasa Salapaka"],"url":"https://arxiv.org/abs/2410.14077"}
{"created":"2025-04-11","title":"ELOQ: Resources for Enhancing LLM Detection of Out-of-Scope Questions","abstract":"Large Language Models (LLMs) are widely used in Conversational AI systems to generate responses to user inquiries. However, many natural questions lack well-defined answers. While existing studies primarily focus on question types such as false premises, they often overlook out-of-scope questions, where the provided document is semantically highly similar to the query but does not contain the required answer. In this paper, we propose a guided hallucination-based method to efficiently generate a diverse set of out-of-scope questions from a given document corpus. We then evaluate multiple LLMs based on their effectiveness in confusion detection and appropriate response generation. Furthermore, we introduce an improved method for detecting such out-of-scope questions, enhancing the reliability of LLM-based question-answering systems.","authors":["Zhiyuan Peng","Jinming Nian","Alexandre Evfimievski","Yi Fang"],"url":"https://arxiv.org/abs/2410.14567"}
{"created":"2025-04-11","title":"Semantically Safe Robot Manipulation: From Semantic Scene Understanding to Motion Safeguards","abstract":"Ensuring safe interactions in human-centric environments requires robots to understand and adhere to constraints recognized by humans as \"common sense\" (e.g., \"moving a cup of water above a laptop is unsafe as the water may spill\" or \"rotating a cup of water is unsafe as it can lead to pouring its content\"). Recent advances in computer vision and machine learning have enabled robots to acquire a semantic understanding of and reason about their operating environments. While extensive literature on safe robot decision-making exists, semantic understanding is rarely integrated into these formulations. In this work, we propose a semantic safety filter framework to certify robot inputs with respect to semantically defined constraints (e.g., unsafe spatial relationships, behaviors, and poses) and geometrically defined constraints (e.g., environment-collision and self-collision constraints). In our proposed approach, given perception inputs, we build a semantic map of the 3D environment and leverage the contextual reasoning capabilities of large language models to infer semantically unsafe conditions. These semantically unsafe conditions are then mapped to safe actions through a control barrier certification formulation. We demonstrate the proposed semantic safety filter in teleoperated manipulation tasks and with learned diffusion policies applied in a real-world kitchen environment that further showcases its effectiveness in addressing practical semantic safety constraints. Together, these experiments highlight our approach's capability to integrate semantics into safety certification, enabling safe robot operation beyond traditional collision avoidance.","authors":["Lukas Brunke","Yanni Zhang","Ralf R\\\"omer","Jack Naimer","Nikola Staykov","Siqi Zhou","Angela P. Schoellig"],"url":"https://arxiv.org/abs/2410.15185"}
{"created":"2025-04-11","title":"Medical-GAT: Cancer Document Classification Leveraging Graph-Based Residual Network for Scenarios with Limited Data","abstract":"Accurate classification of cancer-related medical abstracts is crucial for healthcare management and research. However, obtaining large, labeled datasets in the medical domain is challenging due to privacy concerns and the complexity of clinical data. This scarcity of annotated data impedes the development of effective machine learning models for cancer document classification. To address this challenge, we present a curated dataset of 1,874 biomedical abstracts, categorized into thyroid cancer, colon cancer, lung cancer, and generic topics. Our research focuses on leveraging this dataset to improve classification performance, particularly in data-scarce scenarios. We introduce a Residual Graph Attention Network (R-GAT) with multiple graph attention layers that capture the semantic information and structural relationships within cancer-related documents. Our R-GAT model is compared with various techniques, including transformer-based models such as Bidirectional Encoder Representations from Transformers (BERT), RoBERTa, and domain-specific models like BioBERT and Bio+ClinicalBERT. We also evaluated deep learning models (CNNs, LSTMs) and traditional machine learning models (Logistic Regression, SVM). Additionally, we explore ensemble approaches that combine deep learning models to enhance classification. Various feature extraction methods are assessed, including Term Frequency-Inverse Document Frequency (TF-IDF) with unigrams and bigrams, Word2Vec, and tokenizers from BERT and RoBERTa. The R-GAT model outperforms other techniques, achieving precision, recall, and F1 scores of 0.99, 0.97, and 0.98 for thyroid cancer; 0.96, 0.94, and 0.95 for colon cancer; 0.96, 0.99, and 0.97 for lung cancer; and 0.95, 0.96, and 0.95 for generic topics.","authors":["Elias Hossain","Tasfia Nuzhat","Shamsul Masum","Shahram Rahimi","Noorbakhsh Amiri Golilarz"],"url":"https://arxiv.org/abs/2410.15198"}
{"created":"2025-04-11","title":"Approximate Feedback Nash Equilibria with Sparse Inter-Agent Dependencies","abstract":"Feedback Nash equilibrium strategies in multi-agent dynamic games require availability of all players' state information to compute control actions. However, in real-world scenarios, sensing and communication limitations between agents make full state feedback expensive or impractical, and such strategies can become fragile when state information from other agents is inaccurate. To this end, we propose a regularized dynamic programming approach for finding sparse feedback policies that selectively depend on the states of a subset of agents in dynamic games. The proposed approach solves convex adaptive group Lasso problems to compute sparse policies approximating Nash equilibrium solutions. We prove the regularized solutions' asymptotic convergence to a neighborhood of Nash equilibrium policies in linear-quadratic (LQ) games. Further, we extend the proposed approach to general non-LQ games via an iterative algorithm. Simulation results in multi-robot interaction scenarios show that the proposed approach effectively computes feedback policies with varying sparsity levels. When agents have noisy observations of other agents' states, simulation results indicate that the proposed regularized policies consistently achieve lower costs than standard Nash equilibrium policies by up to 77% for all interacting agents whose costs are coupled with other agents' states.","authors":["Xinjie Liu","Jingqi Li","Filippos Fotiadis","Mustafa O. Karabag","Jesse Milzman","David Fridovich-Keil","Ufuk Topcu"],"url":"https://arxiv.org/abs/2410.16441"}
{"created":"2025-04-11","title":"Irregular Tensor Low-Rank Representation for Hyperspectral Image Representation","abstract":"Spectral variations pose a common challenge in analyzing hyperspectral images (HSI). To address this, low-rank tensor representation has emerged as a robust strategy, leveraging inherent correlations within HSI data. However, the spatial distribution of ground objects in HSIs is inherently irregular, existing naturally in tensor format, with numerous class-specific regions manifesting as irregular tensors. Current low-rank representation techniques are designed for regular tensor structures and overlook this fundamental irregularity in real-world HSIs, leading to performance limitations. To tackle this issue, we propose a novel model for irregular tensor low-rank representation tailored to efficiently model irregular 3D cubes. By incorporating a non-convex nuclear norm to promote low-rankness and integrating a global negative low-rank term to enhance the discriminative ability, our proposed model is formulated as a constrained optimization problem and solved using an alternating augmented Lagrangian method. Experimental validation conducted on four public datasets demonstrates the superior performance of our method compared to existing state-of-the-art approaches. The code is publicly available at https://github.com/hb-studying/ITLRR.","authors":["Bo Han","Yuheng Jia","Hui Liu","Junhui Hou"],"url":"https://arxiv.org/abs/2410.18388"}
{"created":"2025-04-11","title":"Dynamic Content Caching with Waiting Costs via Restless Multi-Armed Bandits","abstract":"We consider a system with a local cache connected to a backend server and an end user population. A set of contents are stored at the the server where they continuously get updated. The local cache keeps copies, potentially stale, of a subset of the contents. The users make content requests to the local cache which either can serve the local version if available or can fetch a fresh version or can wait for additional requests before fetching and serving a fresh version. Serving a stale version of a content incurs an age-of-version(AoV) dependent ageing cost, fetching it from the server incurs a fetching cost, and making a request wait incurs a per unit time waiting cost. We focus on the optimal actions subject to the cache capacity constraint at each decision epoch, aiming at minimizing the long term average cost. We pose the problem as a Restless Multi-armed Bandit(RMAB) Problem and propose a Whittle index based policy which is known to be asymptotically optimal. We explicitly characterize the Whittle indices. We numerically evaluate the proposed policy and also compare it to a greedy policy. We show that it is close to the optimal policy and substantially outperforms the exising policies.","authors":["Ankita Koley","Chandramani Singh"],"url":"https://arxiv.org/abs/2410.18627"}
{"created":"2025-04-11","title":"Sort-free Gaussian Splatting via Weighted Sum Rendering","abstract":"Recently, 3D Gaussian Splatting (3DGS) has emerged as a significant advancement in 3D scene reconstruction, attracting considerable attention due to its ability to recover high-fidelity details while maintaining low complexity. Despite the promising results achieved by 3DGS, its rendering performance is constrained by its dependence on costly non-commutative alpha-blending operations. These operations mandate complex view dependent sorting operations that introduce computational overhead, especially on the resource-constrained platforms such as mobile phones. In this paper, we propose Weighted Sum Rendering, which approximates alpha blending with weighted sums, thereby removing the need for sorting. This simplifies implementation, delivers superior performance, and eliminates the \"popping\" artifacts caused by sorting. Experimental results show that optimizing a generalized Gaussian splatting formulation to the new differentiable rendering yields competitive image quality. The method was implemented and tested in a mobile device GPU, achieving on average $1.23\\times$ faster rendering.","authors":["Qiqi Hou","Randall Rauwendaal","Zifeng Li","Hoang Le","Farzad Farhadzadeh","Fatih Porikli","Alexei Bourd","Amir Said"],"url":"https://arxiv.org/abs/2410.18931"}
{"created":"2025-04-11","title":"Bio2Token: All-atom tokenization of any biomolecular structure with Mamba","abstract":"Efficient encoding and representation of large 3D molecular structures with high fidelity is critical for biomolecular design applications. Despite this, many representation learning approaches restrict themselves to modeling smaller systems or use coarse-grained approximations of the systems, for example modeling proteins at the resolution of amino acid residues rather than at the level of individual atoms. To address this, we develop quantized auto-encoders that learn atom-level tokenizations of complete proteins, RNA and small molecule structures with reconstruction accuracies well below 1 Angstrom. We demonstrate that a simple Mamba state space model architecture is efficient compared to an SE(3)-invariant IPA architecture, reaches competitive accuracies and can scale to systems with almost 100,000 atoms. The learned structure tokens of bio2token may serve as the input for all-atom generative models in the future.","authors":["Andrew Liu","Axel Elaldi","Nathan Russell","Olivia Viessmann"],"url":"https://arxiv.org/abs/2410.19110"}
{"created":"2025-04-11","title":"Model Equality Testing: Which Model Is This API Serving?","abstract":"Users often interact with large language models through black-box inference APIs, both for closed- and open-weight models (e.g., Llama models are popularly accessed via Amazon Bedrock and Azure AI Studio). In order to cut costs or add functionality, API providers may quantize, watermark, or finetune the underlying model, changing the output distribution -- possibly without notifying users. We formalize detecting such distortions as Model Equality Testing, a two-sample testing problem, where the user collects samples from the API and a reference distribution and conducts a statistical test to see if the two distributions are the same. We find that tests based on the Maximum Mean Discrepancy between distributions are powerful for this task: a test built on a simple string kernel achieves a median of 77.4% power against a range of distortions, using an average of just 10 samples per prompt. We then apply this test to commercial inference APIs from Summer 2024 for four Llama models, finding that 11 out of 31 endpoints serve different distributions than reference weights released by Meta.","authors":["Irena Gao","Percy Liang","Carlos Guestrin"],"url":"https://arxiv.org/abs/2410.20247"}
{"created":"2025-04-11","title":"Can Large Language Models Replace Data Scientists in Biomedical Research?","abstract":"Data science plays a critical role in biomedical research, but it requires professionals with expertise in coding and medical data analysis. Large language models (LLMs) have shown great potential in supporting medical tasks and performing well in general coding tests. However, existing evaluations fail to assess their capability in biomedical data science, particularly in handling diverse data types such as genomics and clinical datasets. To address this gap, we developed a benchmark of data science coding tasks derived from the analyses of 39 published studies. This benchmark comprises 293 coding tasks (128 in Python and 165 in R) performed on real-world TCGA-type genomics and clinical data. Our findings reveal that the vanilla prompting of LLMs yields suboptimal performances due to drawbacks in following input instructions, understanding target data, and adhering to standard analysis practices. Next, we benchmarked six cutting-edge LLMs and advanced adaptation methods, finding two methods to be particularly effective: chain-of-thought prompting, which provides a step-by-step plan for data analysis, which led to a 21% code accuracy improvement (56.6% versus 35.3%); and self-reflection, enabling LLMs to refine the buggy code iteratively, yielding an 11% code accuracy improvement (45.5% versus 34.3%). Building on these insights, we developed a platform that integrates LLMs into the data science workflow for medical professionals. In a user study with five medical professionals, we found that while LLMs cannot fully automate programming tasks, they significantly streamline the programming process. We found that 80% of their submitted code solutions were incorporated from LLM-generated code, with up to 96% reuse in some cases. Our analysis highlights the potential of LLMs to enhance data science efficiency in biomedical research when integrated into expert workflows.","authors":["Zifeng Wang","Benjamin Danek","Ziwei Yang","Zheng Chen","Jimeng Sun"],"url":"https://arxiv.org/abs/2410.21591"}
{"created":"2025-04-11","title":"PARDON: Privacy-Aware and Robust Federated Domain Generalization","abstract":"Federated Learning (FL) shows promise in preserving privacy and enabling collaborative learning. However, most current solutions focus on private data collected from a single domain. A significant challenge arises when client data comes from diverse domains (i.e., domain shift), leading to poor performance on unseen domains. Existing Federated Domain Generalization approaches address this problem but assume each client holds data for an entire domain, limiting their practicality in real-world scenarios with domain-based heterogeneity and client sampling. In addition, certain methods enable information sharing among clients, raising privacy concerns as this information could be used to reconstruct sensitive private data.","authors":["Dung Thuy Nguyen","Taylor T. Johnson","Kevin Leach"],"url":"https://arxiv.org/abs/2410.22622"}
{"created":"2025-04-11","title":"GWQ: Gradient-Aware Weight Quantization for Large Language Models","abstract":"Large language models (LLMs) show impressive performance in solving complex language tasks. However, its large number of parameters presents significant challenges for the deployment. So, compressing LLMs to low bits can enable to deploy on resource-constrained devices. To address this problem, we propose gradient-aware weight quantization (GWQ), the first quantization approach for low-bit weight quantization that leverages gradients to localize outliers, requiring only a minimal amount of calibration data for outlier detection. GWQ retains the top 1\\% outliers preferentially at FP16 precision, while the remaining non-outlier weights are stored in a low-bit. We widely evaluate GWQ on different task include language modeling, grounding detection, massive multitask language understanding and vision-language question and answering. Results show that models quantified by GWQ performs better than other quantization method. During quantization process, GWQ only need one calibration set to realize effective quant. Also, GWQ achieves 1.2x inference speedup in comparison to the original model and effectively reduces the inference memory.","authors":["Yihua Shao","Yan Gu","Siyu Chen","Haiyang Liu","Zijian Ling","Minxi Yan","Ziyang Yan","Chenyu Zhang","Michele Magno","Haotong Qin","Yan Wang","Jingcai Guo","Ling Shao","Hao Tang"],"url":"https://arxiv.org/abs/2411.00850"}
{"created":"2025-04-11","title":"Control Node Placement and Structural Controllability of Water Quality Dynamics in Drinking Networks","abstract":"Chlorine, the most widely used disinfectant, needs to be adequately distributed in water distribution networks (WDNs) to maintain consistent residual levels and ensure water safety. This is performed through control node injections at the treatment plant via booster stations scattered in WDNs. While previous studies have applied various optimization metrics for booster station placement, many have failed to consider the coverage of the station injections and the dynamic nature of WDNs. In particular, variations in hydraulics and demand significantly impact the reachability and efficacy of chlorine injections which then impact optimal placement of booster stations. This study introduces a novel formulation that combines control- and graph-theoretic approaches to solve the booster station placement problem. Unlike traditional methods, our approach emphasizes maximizing the system's ability to control disinfectant levels with minimal energy, taking into account the time-varying hydraulic profiles that lead to different optimal station placements. We propose a simple weighting technique to determine the placements by assessing the structural controllability of each configuration, based on the network's topology and independent of specific parameters like decay rates or pipe roughness. This method ensures effective chlorine coverage across the network. Our approach is validated on different networks, demonstrating its operational effectiveness, scalability, and practicality.","authors":["Salma M. Elsherif","Ahmad F. Taha"],"url":"https://arxiv.org/abs/2411.01361"}
{"created":"2025-04-11","title":"Concurrent Composition for Differentially Private Continual Mechanisms","abstract":"Many intended uses of differential privacy involve a $\\textit{continual mechanism}$ that is set up to run continuously over a long period of time, making more statistical releases as either queries come in or the dataset is updated. In this paper, we give the first general treatment of privacy against $\\textit{adaptive}$ adversaries for mechanisms that support dataset updates and a variety of queries, all arbitrarily interleaved. It also models a very general notion of neighboring, that includes both event-level and user-level privacy.","authors":["Monika Henzinger","Roodabeh Safavi","Salil Vadhan"],"url":"https://arxiv.org/abs/2411.03299"}
{"created":"2025-04-11","title":"FedSECA: Sign Election and Coordinate-wise Aggregation of Gradients for Byzantine Tolerant Federated Learning","abstract":"One of the most common defense strategies against Byzantine clients in federated learning (FL) is to employ a robust aggregator mechanism that makes the training more resilient. While many existing Byzantine robust aggregators provide theoretical convergence guarantees and are empirically effective against certain categories of attacks, we observe that certain high-strength attacks can subvert the robust aggregator and collapse the training. To overcome this limitation, we propose a method called FedSECA for robust Sign Election and Coordinate-wise Aggregation of gradients in FL that is less susceptible to malicious updates by an omniscient attacker. The proposed method has two main components. The Concordance Ratio Induced Sign Election(CRISE) module determines the consensus direction (elected sign) for each individual parameter gradient through a weighted voting strategy. The client weights are assigned based on a novel metric called concordance ratio, which quantifies the degree of sign agreement between the client gradient updates. Based on the elected sign, a Robust Coordinate-wise Aggregation(RoCA) strategy is employed, where variance-reduced sparse gradients are aggregated only if they are in alignment with the corresponding elected sign. We compare our proposed FedSECA method against 10 robust aggregators under 7 Byzantine attacks on 3 datasets and architectures. The results show that existing robust aggregators fail for at least some attacks, while FedSECA exhibits better robustness. Code - https://github.com/JosephGeoBenjamin/FedSECA-ByzantineTolerance","authors":["Joseph Geo Benjamin","Mothilal Asokan","Mohammad Yaqub","Karthik Nandakumar"],"url":"https://arxiv.org/abs/2411.03861"}
{"created":"2025-04-11","title":"An evolving surface finite element method for the Cahn-Hilliard equation with a logarithmic potential","abstract":"In this paper we study semi-discrete and fully discrete evolving surface finite element schemes for the Cahn-Hilliard equation with a logarithmic potential. Specifically we consider linear finite elements discretising space and backward Euler time discretisation. Our analysis relies on a specific geometric assumption on the evolution of the surface. Our main results are $L^2_{H^1}$ error bounds for both the semi-discrete and fully discrete schemes, and we provide some numerical results.","authors":["Charles M. Elliott","Thomas Sales"],"url":"https://arxiv.org/abs/2411.05650"}
{"created":"2025-04-11","title":"Foundation Model for Composite Microstructures: Reconstruction, Stiffness, and Nonlinear Behavior Prediction","abstract":"The rapid advancement of machine learning has unlocked numerous opportunities for materials science, particularly in accelerating the design and analysis of materials. However, a significant challenge lies in the scarcity and high cost of obtaining high-quality materials datasets. While foundation models pre-trained on large datasets have excelled in fields like natural language processing by leveraging latent features through transfer learning, their application in materials science remains limited. Here, we present a foundation model specifically designed for composite materials. Pre-trained on a dataset of short-fiber composites to learn robust latent features, the model accurately predicts homogenized stiffness during transfer learning, even with limited training data. Additionally, our model effectively predicts the material's nonlinear behavior by transferring these learned features to an Interaction-based Material Network, which is a constitutive surrogate model. These results demonstrate the potential of our foundation model to capture complex material behaviors. Our findings validate the feasibility and effectiveness of foundation models in composite materials. We anticipate extending this approach to more complex three-dimensional composite materials, polycrystalline materials, and beyond. Moreover, this framework enables high-accuracy predictions even when experimental data are scarce, paving the way for more efficient and cost-effective materials design and analysis.","authors":["Ting-Ju Wei","Chuin-Shan Chen"],"url":"https://arxiv.org/abs/2411.06565"}
{"created":"2025-04-11","title":"ODEStream: A Buffer-Free Online Learning Framework with ODE-based Adaptor for Streaming Time Series Forecasting","abstract":"Addressing the challenges of irregularity and concept drift in streaming time series is crucial for real-world predictive modelling. Previous studies in time series continual learning often propose models that require buffering long sequences, potentially restricting the responsiveness of the inference system. Moreover, these models are typically designed for regularly sampled data, an unrealistic assumption in real-world scenarios. This paper introduces ODEStream, a novel buffer-free continual learning framework that incorporates a temporal isolation layer to capture temporal dependencies within the data. Simultaneously, it leverages the capability of neural ordinary differential equations to process irregular sequences and generate a continuous data representation, enabling seamless adaptation to changing dynamics in a data streaming scenario. Our approach focuses on learning how the dynamics and distribution of historical data change over time, facilitating direct processing of streaming sequences. Evaluations on benchmark real-world datasets demonstrate that ODEStream outperforms the state-of-the-art online learning and streaming analysis baseline models, providing accurate predictions over extended periods while minimising performance degradation over time by learning how the sequence dynamics change. The implementation of ODEStream is available at: https://github.com/FtoonAbushaqra/ODEStream.git.","authors":["Futoon M. Abushaqra","Hao Xue","Yongli Ren","Flora D. Salim"],"url":"https://arxiv.org/abs/2411.07413"}
{"created":"2025-04-11","title":"CLaSP: Learning Concepts for Time-Series Signals from Natural Language Supervision","abstract":"This paper presents CLaSP, a novel model for retrieving time-series signals using natural language queries that describe signal characteristics. The ability to search time-series signals based on descriptive queries is essential in domains such as industrial diagnostics, where data scientists often need to find signals with specific characteristics. However, existing methods rely on sketch-based inputs, predefined synonym dictionaries, or domain-specific manual designs, limiting their scalability and adaptability. CLaSP addresses these challenges by employing contrastive learning to map time-series signals to natural language descriptions. Unlike prior approaches, it eliminates the need for predefined synonym dictionaries and leverages the rich contextual knowledge of large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair time-series signals with natural language descriptions, we demonstrate that CLaSP achieves high accuracy in retrieving a variety of time series patterns based on natural language queries.","authors":["Aoi Ito","Kota Dohi","Yohei Kawaguchi"],"url":"https://arxiv.org/abs/2411.08397"}
{"created":"2025-04-11","title":"LUDO: Low-Latency Understanding of Deformable Objects using Point Cloud Occupancy Functions","abstract":"Accurately determining the shape of objects and the location of their internal structures within deformable objects is crucial for medical tasks that require precise targeting, such as robotic biopsies. We introduce LUDO, a method for accurate low-latency understanding of deformable objects. LUDO reconstructs objects in their deformed state, including their internal structures, from a single-view point cloud observation in under 30 ms using occupancy networks. LUDO provides uncertainty estimates for its predictions. Additionally, it provides explainability by highlighting key features in its input observations. Both uncertainty and explainability are important for safety-critical applications such as surgical interventions. We demonstrate LUDO's abilities for autonomous targeting of internal regions of interest (ROIs) in deformable objects. %Additionally, LUDO provides uncertainty estimates and explainability for its predictions, both of which are important in safety-critical applications such as surgical interventions. We evaluate LUDO in real-world robotic experiments, achieving a success rate of 98.9% for puncturing various ROIs inside deformable objects. LUDO demonstrates the potential to interact with deformable objects without the need for deformable registration methods.","authors":["Pit Henrich","Franziska Mathis-Ullrich","Paul Maria Scheikl"],"url":"https://arxiv.org/abs/2411.08777"}
{"created":"2025-04-11","title":"UMGAD: Unsupervised Multiplex Graph Anomaly Detection","abstract":"Graph anomaly detection (GAD) is a critical task in graph machine learning, with the primary objective of identifying anomalous nodes that deviate significantly from the majority. This task is widely applied in various real-world scenarios, including fraud detection and social network analysis. However, existing GAD methods still face two major challenges: (1) They are often limited to detecting anomalies in single-type interaction graphs and struggle with multiple interaction types in multiplex heterogeneous graphs. (2) In unsupervised scenarios, selecting appropriate anomaly score thresholds remains a significant challenge for accurate anomaly detection. To address the above challenges, we propose a novel Unsupervised Multiplex Graph Anomaly Detection method, named UMGAD. We first learn multi-relational correlations among nodes in multiplex heterogeneous graphs and capture anomaly information during node attribute and structure reconstruction through graph-masked autoencoder (GMAE). Then, to further extract abnormal information, we generate attribute-level and subgraph-level augmented-view graphs, respectively, and perform attribute and structure reconstruction through GMAE. Finally, we learn to optimize node attributes and structural features through contrastive learning between original-view and augmented-view graphs to improve the model's ability to capture anomalies. Meanwhile, we propose a new anomaly score threshold selection strategy, which allows the model to be independent of ground truth information in real unsupervised scenarios. Extensive experiments on six datasets show that our UMGAD significantly outperforms state-of-the-art methods, achieving average improvements of 12.25% in AUC and 11.29% in Macro-F1 across all datasets.","authors":["Xiang Li","Jianpeng Qi","Zhongying Zhao","Guanjie Zheng","Lei Cao","Junyu Dong","Yanwei Yu"],"url":"https://arxiv.org/abs/2411.12556"}
{"created":"2025-04-11","title":"A Flexible Large Language Models Guardrail Development Methodology Applied to Off-Topic Prompt Detection","abstract":"Large Language Models (LLMs) are prone to off-topic misuse, where users may prompt these models to perform tasks beyond their intended scope. Current guardrails, which often rely on curated examples or custom classifiers, suffer from high false-positive rates, limited adaptability, and the impracticality of requiring real-world data that is not available in pre-production. In this paper, we introduce a flexible, data-free guardrail development methodology that addresses these challenges. By thoroughly defining the problem space qualitatively and passing this to an LLM to generate diverse prompts, we construct a synthetic dataset to benchmark and train off-topic guardrails that outperform heuristic approaches. Additionally, by framing the task as classifying whether the user prompt is relevant with respect to the system prompt, our guardrails effectively generalize to other misuse categories, including jailbreak and harmful prompts. Lastly, we further contribute to the field by open-sourcing both the synthetic dataset and the off-topic guardrail models, providing valuable resources for developing guardrails in pre-production environments and supporting future research and development in LLM safety.","authors":["Gabriel Chua","Shing Yee Chan","Shaun Khoo"],"url":"https://arxiv.org/abs/2411.12946"}
{"created":"2025-04-11","title":"Quantized symbolic time series approximation","abstract":"Time series are ubiquitous in numerous science and engineering domains, e.g., signal processing, bioinformatics, and astronomy. Previous work has verified the efficacy of symbolic time series representation in a variety of engineering applications due to its storage efficiency and numerosity reduction. The most recent symbolic aggregate approximation technique, ABBA, has been shown to preserve essential shape information of time series and improve downstream applications, e.g., neural network inference regarding prediction and anomaly detection in time series.","authors":["Erin Carson","Xinye Chen","Cheng Kang"],"url":"https://arxiv.org/abs/2411.15209"}
{"created":"2025-04-11","title":"Zero-Order Control Barrier Functions for Sampled-Data Systems with State and Input Dependent Safety Constraints","abstract":"We propose a novel zero-order control barrier function (ZOCBF) for sampled-data systems to ensure system safety. Our formulation generalizes conventional control barrier functions and straightforwardly handles safety constraints with high-relative degrees or those that explicitly depend on both system states and inputs. The proposed ZOCBF condition does not require any differentiation operation. Instead, it involves computing the difference of the ZOCBF values at two consecutive sampling instants. We propose three numerical approaches to enforce the ZOCBF condition, tailored to different problem settings and available computational resources. We demonstrate the effectiveness of our approach through a collision avoidance example and a rollover prevention example on uneven terrains.","authors":["Xiao Tan","Ersin Das","Aaron D. Ames","Joel W. Burdick"],"url":"https://arxiv.org/abs/2411.17079"}
{"created":"2025-04-11","title":"EzSQL: An SQL intermediate representation for improving SQL-to-text Generation","abstract":"The SQL-to-text generation task traditionally uses template base, Seq2Seq, tree-to-sequence, and graph-to-sequence models. Recent models take advantage of pre-trained generative language models for this task in the Seq2Seq framework. However, treating SQL as a sequence of inputs to the pre-trained models is not optimal. In this work, we put forward a new SQL intermediate representation called EzSQL to align SQL with the natural language text sequence. EzSQL simplifies the SQL queries and brings them closer to natural language text by modifying operators and keywords, which can usually be described in natural language. EzSQL also removes the need for set operators. Our proposed SQL-to-text generation model uses EzSQL as the input to a pre-trained generative language model for generating the text descriptions. We demonstrate that our model is an effective state-of-the-art method to generate text narrations from SQL queries on the WikiSQL and Spider datasets. We also show that by generating pretraining data using our SQL-to-text generation model, we can enhance the performance of Text-to-SQL parsers.","authors":["Meher Bhardwaj","Hrishikesh Ethari","Dennis Singh Moirangthem"],"url":"https://arxiv.org/abs/2411.18923"}
{"created":"2025-04-11","title":"FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling","abstract":"Achieving realistic animated human avatars requires accurate modeling of pose-dependent clothing deformations. Existing learning-based methods heavily rely on the Linear Blend Skinning (LBS) of minimally-clothed human models like SMPL to model deformation. However, they struggle to handle loose clothing, such as long dresses, where the canonicalization process becomes ill-defined when the clothing is far from the body, leading to disjointed and fragmented results. To overcome this limitation, we propose FreeCloth, a novel hybrid framework to model challenging clothed humans. Our core idea is to use dedicated strategies to model different regions, depending on whether they are close to or distant from the body. Specifically, we segment the human body into three categories: unclothed, deformed, and generated. We simply replicate unclothed regions that require no deformation. For deformed regions close to the body, we leverage LBS to handle the deformation. As for the generated regions, which correspond to loose clothing areas, we introduce a novel free-form, part-aware generator to model them, as they are less affected by movements. This free-form generation paradigm brings enhanced flexibility and expressiveness to our hybrid framework, enabling it to capture the intricate geometric details of challenging loose clothing, such as skirts and dresses. Experimental results on the benchmark dataset featuring loose clothing demonstrate that FreeCloth achieves state-of-the-art performance with superior visual fidelity and realism, particularly in the most challenging cases.","authors":["Hang Ye","Xiaoxuan Ma","Hai Ci","Wentao Zhu","Yizhou Wang"],"url":"https://arxiv.org/abs/2411.19942"}
{"created":"2025-04-11","title":"Dissipative iFIR filters for data-driven design","abstract":"We tackle the problem of providing closed-loop stability guarantees with a scalable data-driven design. We combine virtual reference feedback tuning with dissipativity constraints on the controller for closed-loop stability. The constraints are formulated as a set of linear inequalities in the frequency domain. This leads to a convex problem that is scalable with respect to the length of the data and the complexity of the controller. An extension of virtual reference feedback tuning to include disturbance dynamics is also discussed. The proposed data-driven control design is illustrated by a soft gripper impedance control example.","authors":["Zixing Wang","Yi Zhang","Fulvio Forni"],"url":"https://arxiv.org/abs/2412.00211"}
{"created":"2025-04-11","title":"EchoONE: Segmenting Multiple echocardiography Planes in One Model","abstract":"In clinical practice of echocardiography examinations, multiple planes containing the heart structures of different view are usually required in screening, diagnosis and treatment of cardiac disease. AI models for echocardiography have to be tailored for each specific plane due to the dramatic structure differences, thus resulting in repetition development and extra complexity. Effective solution for such a multi-plane segmentation (MPS) problem is highly demanded for medical images, yet has not been well investigated. In this paper, we propose a novel solution, EchoONE, for this problem with a SAM-based segmentation architecture, a prior-composable mask learning (PC-Mask) module for semantic-aware dense prompt generation, and a learnable CNN-branch with a simple yet effective local feature fusion and adaption (LFFA) module for SAM adapting. We extensively evaluated our method on multiple internal and external echocardiography datasets, and achieved consistently state-of-the-art performance for multi-source datasets with different heart planes. This is the first time that the MPS problem is solved in one model for echocardiography data. The code will be available at https://github.com/a2502503/EchoONE.","authors":["Jiongtong Hu","Wei Zhuo","Jun Cheng","Yingying Liu","Wufeng Xue","Dong Ni"],"url":"https://arxiv.org/abs/2412.02993"}
{"created":"2025-04-11","title":"GigaHands: A Massive Annotated Dataset of Bimanual Hand Activities","abstract":"Understanding bimanual human hand activities is a critical problem in AI and robotics. We cannot build large models of bimanual activities because existing datasets lack the scale, coverage of diverse hand activities, and detailed annotations. We introduce GigaHands, a massive annotated dataset capturing 34 hours of bimanual hand activities from 56 subjects and 417 objects, totaling 14k motion clips derived from 183 million frames paired with 84k text annotations. Our markerless capture setup and data acquisition protocol enable fully automatic 3D hand and object estimation while minimizing the effort required for text annotation. The scale and diversity of GigaHands enable broad applications, including text-driven action synthesis, hand motion captioning, and dynamic radiance field reconstruction. Our website are avaliable at https://ivl.cs.brown.edu/research/gigahands.html .","authors":["Rao Fu","Dingxi Zhang","Alex Jiang","Wanjia Fu","Austin Funk","Daniel Ritchie","Srinath Sridhar"],"url":"https://arxiv.org/abs/2412.04244"}
{"created":"2025-04-11","title":"International Scientific Report on the Safety of Advanced AI (Interim Report)","abstract":"This is the interim publication of the first International Scientific Report on the Safety of Advanced AI. The report synthesises the scientific understanding of general-purpose AI -- AI that can perform a wide variety of tasks -- with a focus on understanding and managing its risks. A diverse group of 75 AI experts contributed to this report, including an international Expert Advisory Panel nominated by 30 countries, the EU, and the UN. Led by the Chair, these independent experts collectively had full discretion over the report's content.","authors":["Yoshua Bengio","S\\\"oren Mindermann","Daniel Privitera","Tamay Besiroglu","Rishi Bommasani","Stephen Casper","Yejin Choi","Danielle Goldfarb","Hoda Heidari","Leila Khalatbari","Shayne Longpre","Vasilios Mavroudis","Mantas Mazeika","Kwan Yee Ng","Chinasa T. Okolo","Deborah Raji","Theodora Skeadas","Florian Tram\\`er","Bayo Adekanmbi","Paul Christiano","David Dalrymple","Thomas G. Dietterich","Edward Felten","Pascale Fung","Pierre-Olivier Gourinchas","Nick Jennings","Andreas Krause","Percy Liang","Teresa Ludermir","Vidushi Marda","Helen Margetts","John A. McDermid","Arvind Narayanan","Alondra Nelson","Alice Oh","Gopal Ramchurn","Stuart Russell","Marietje Schaake","Dawn Song","Alvaro Soto","Lee Tiedrich","Ga\\\"el Varoquaux","Andrew Yao","Ya-Qin Zhang"],"url":"https://arxiv.org/abs/2412.05282"}
{"created":"2025-04-11","title":"Design2GarmentCode: Turning Design Concepts to Tangible Garments Through Program Synthesis","abstract":"Sewing patterns, the essential blueprints for fabric cutting and tailoring, act as a crucial bridge between design concepts and producible garments. However, existing uni-modal sewing pattern generation models struggle to effectively encode complex design concepts with a multi-modal nature and correlate them with vectorized sewing patterns that possess precise geometric structures and intricate sewing relations. In this work, we propose a novel sewing pattern generation approach \\textbf{Design2GarmentCode} based on Large Multimodal Models (LMMs), to generate parametric pattern-making programs from multi-modal design concepts. LMM offers an intuitive interface for interpreting diverse design inputs, while pattern-making programs could serve as well-structured and semantically meaningful representations of sewing patterns, and act as a robust bridge connecting the cross-domain pattern-making knowledge embedded in LMMs with vectorized sewing patterns. Experimental results demonstrate that our method can flexibly handle various complex design expressions such as images, textual descriptions, designer sketches, or their combinations, and convert them into size-precise sewing patterns with correct stitches. Compared to previous methods, our approach significantly enhances training efficiency, generation quality, and authoring flexibility.","authors":["Feng Zhou","Ruiyang Liu","Chen Liu","Gaofeng He","Yong-Lu Li","Xiaogang Jin","Huamin Wang"],"url":"https://arxiv.org/abs/2412.08603"}
{"created":"2025-04-11","title":"DCSEG: Decoupled 3D Open-Set Segmentation using Gaussian Splatting","abstract":"Open-set 3D segmentation represents a major point of interest for multiple downstream robotics and augmented/virtual reality applications. We present a decoupled 3D segmentation pipeline to ensure modularity and adaptability to novel 3D representations as well as semantic segmentation foundation models. We first reconstruct a scene with 3D Gaussians and learn class-agnostic features through contrastive supervision from a 2D instance proposal network. These 3D features are then clustered to form coarse object- or part-level masks. Finally, we match each 3D cluster to class-aware masks predicted by a 2D open-vocabulary segmentation model, assigning semantic labels without retraining the 3D representation. Our decoupled design (1) provides a plug-and-play interface for swapping different 2D or 3D modules, (2) ensures multi-object instance segmentation at no extra cost, and (3) leverages rich 3D geometry for robust scene understanding. We evaluate on synthetic and real-world indoor datasets, demonstrating improved performance over comparable NeRF-based pipelines on mIoU and mAcc, particularly for challenging or long-tail classes. We also show how varying the 2D backbone affects the final segmentation, highlighting the modularity of our framework. These results confirm that decoupling 3D mask proposal and semantic classification can deliver flexible, efficient, and open-vocabulary 3D segmentation.","authors":["Luis Wiedmann","Luca Wiehe","David Rozenberszki"],"url":"https://arxiv.org/abs/2412.10972"}
{"created":"2025-04-11","title":"Future Sight and Tough Fights: Revolutionizing Sequential Recommendation with FENRec","abstract":"Sequential recommendation (SR) systems predict user preferences by analyzing time-ordered interaction sequences. A common challenge for SR is data sparsity, as users typically interact with only a limited number of items. While contrastive learning has been employed in previous approaches to address the challenges, these methods often adopt binary labels, missing finer patterns and overlooking detailed information in subsequent behaviors of users. Additionally, they rely on random sampling to select negatives in contrastive learning, which may not yield sufficiently hard negatives during later training stages. In this paper, we propose Future data utilization with Enduring Negatives for contrastive learning in sequential Recommendation (FENRec). Our approach aims to leverage future data with time-dependent soft labels and generate enduring hard negatives from existing data, thereby enhancing the effectiveness in tackling data sparsity. Experiment results demonstrate our state-of-the-art performance across four benchmark datasets, with an average improvement of 6.16\\% across all metrics.","authors":["Yu-Hsuan Huang","Ling Lo","Hongxia Xie","Hong-Han Shuai","Wen-Huang Cheng"],"url":"https://arxiv.org/abs/2412.11589"}
{"created":"2025-04-11","title":"DLF: Disentangled-Language-Focused Multimodal Sentiment Analysis","abstract":"Multimodal Sentiment Analysis (MSA) leverages heterogeneous modalities, such as language, vision, and audio, to enhance the understanding of human sentiment. While existing models often focus on extracting shared information across modalities or directly fusing heterogeneous modalities, such approaches can introduce redundancy and conflicts due to equal treatment of all modalities and the mutual transfer of information between modality pairs. To address these issues, we propose a Disentangled-Language-Focused (DLF) multimodal representation learning framework, which incorporates a feature disentanglement module to separate modality-shared and modality-specific information. To further reduce redundancy and enhance language-targeted features, four geometric measures are introduced to refine the disentanglement process. A Language-Focused Attractor (LFA) is further developed to strengthen language representation by leveraging complementary modality-specific information through a language-guided cross-attention mechanism. The framework also employs hierarchical predictions to improve overall accuracy. Extensive experiments on two popular MSA datasets, CMU-MOSI and CMU-MOSEI, demonstrate the significant performance gains achieved by the proposed DLF framework. Comprehensive ablation studies further validate the effectiveness of the feature disentanglement module, language-focused attractor, and hierarchical predictions. Our code is available at https://github.com/pwang322/DLF.","authors":["Pan Wang","Qiang Zhou","Yawen Wu","Tianlong Chen","Jingtong Hu"],"url":"https://arxiv.org/abs/2412.12225"}
{"created":"2025-04-11","title":"Task-Parameter Nexus: Task-Specific Parameter Learning for Model-Based Control","abstract":"This paper presents the Task-Parameter Nexus (TPN), a learning-based approach for online determination of the (near-)optimal control parameters of model-based controllers (MBCs) for tracking tasks. In TPN, a deep neural network is introduced to predict the control parameters for any given tracking task at runtime, especially when optimal parameters for new tasks are not immediately available. To train this network, we constructed a trajectory bank with various speeds and curvatures that represent different motion characteristics. Then, for each trajectory in the bank, we auto-tune the optimal control parameters offline and use them as the corresponding ground truth. With this dataset, the TPN is trained by supervised learning. We evaluated the TPN on the quadrotor platform. In simulation experiments, it is shown that the TPN can predict near-optimal control parameters for a spectrum of tracking tasks, demonstrating its robust generalization capabilities to unseen tasks.","authors":["Sheng Cheng","Ran Tao","Yuliang Gu","Shenlong Wang","Xiaofeng Wang","Naira Hovakimyan"],"url":"https://arxiv.org/abs/2412.12448"}
{"created":"2025-04-11","title":"Noise-based Local Learning using Stochastic Magnetic Tunnel Junctions","abstract":"Brain-inspired learning in physical hardware has enormous potential to learn fast at minimal energy expenditure. One of the characteristics of biological learning systems is their ability to learn in the presence of various noise sources. Inspired by this observation, we introduce a novel noise-based learning approach for physical systems implementing multi-layer neural networks. Simulation results show that our approach allows for effective learning whose performance approaches that of the conventional effective yet energy-costly backpropagation algorithm. Using a spintronics hardware implementation, we demonstrate experimentally that learning can be achieved in a small network composed of physical stochastic magnetic tunnel junctions. These results provide a path towards efficient learning in general physical systems which embraces rather than mitigates the noise inherent in physical devices.","authors":["Kees Koenders","Leo Schnitzpan","Fabian Kammerbauer","Sinan Shu","Gerhard Jakob","Mathis Kl\\\"aui","Johan Mentink","Nasir Ahmad","Marcel van Gerven"],"url":"https://arxiv.org/abs/2412.12783"}
{"created":"2025-04-11","title":"ClarityEthic: Explainable Moral Judgment Utilizing Contrastive Ethical Insights from Large Language Models","abstract":"With the rise and widespread use of Large Language Models (LLMs), ensuring their safety is crucial to prevent harm to humans and promote ethical behaviors. However, directly assessing value valence (i.e., support or oppose) by leveraging large-scale data training is untrustworthy and inexplainable. We assume that emulating humans to rely on social norms to make moral decisions can help LLMs understand and predict moral judgment. However, capturing human values remains a challenge, as multiple related norms might conflict in specific contexts. Consider norms that are upheld by the majority and promote the well-being of society are more likely to be accepted and widely adopted (e.g., \"don't cheat,\"). Therefore, it is essential for LLM to identify the appropriate norms for a given scenario before making moral decisions. To this end, we introduce a novel moral judgment approach called \\textit{ClarityEthic} that leverages LLMs' reasoning ability and contrastive learning to uncover relevant social norms for human actions from different perspectives and select the most reliable one to enhance judgment accuracy. Extensive experiments demonstrate that our method outperforms state-of-the-art approaches in moral judgment tasks. Moreover, human evaluations confirm that the generated social norms provide plausible explanations that support the judgments. This suggests that modeling human moral judgment with the emulating humans moral strategy is promising for improving the ethical behaviors of LLMs.","authors":["Yuxi Sun","Wei Gao","Jing Ma","Hongzhan Lin","Ziyang Luo","Wenxuan Zhang"],"url":"https://arxiv.org/abs/2412.12848"}
{"created":"2025-04-11","title":"Distribution Shifts at Scale: Out-of-distribution Detection in Earth Observation","abstract":"Training robust deep learning models is crucial in Earth Observation, where globally deployed models often face distribution shifts that degrade performance, especially in low-data regions. Out-of-distribution (OOD) detection addresses this by identifying inputs that deviate from in-distribution (ID) data. However, existing methods either assume access to OOD data or compromise primary task performance, limiting real-world use. We introduce TARDIS, a post-hoc OOD detection method designed for scalable geospatial deployment. Our core innovation lies in generating surrogate distribution labels by leveraging ID data within the feature space. TARDIS takes a pre-trained model, ID data, and data from an unknown distribution (WILD), separates WILD into surrogate ID and OOD labels based on internal activations, and trains a binary classifier to detect distribution shifts. We validate on EuroSAT and xBD across 17 setups covering covariate and semantic shifts, showing near-upper-bound surrogate labeling performance in 13 cases and matching the performance of top post-hoc activation- and scoring-based methods. Finally, deploying TARDIS on Fields of the World reveals actionable insights into pre-trained model behavior at scale. The code is available at \\href{https://github.com/microsoft/geospatial-ood-detection}{https://github.com/microsoft/geospatial-ood-detection}","authors":["Burak Ekim","Girmaw Abebe Tadesse","Caleb Robinson","Gilles Hacheme","Michael Schmitt","Rahul Dodhia","Juan M. Lavista Ferres"],"url":"https://arxiv.org/abs/2412.13394"}
{"created":"2025-04-11","title":"Large Language Model Can Be a Foundation for Hidden Rationale-Based Retrieval","abstract":"Despite the recent advancement in Retrieval-Augmented Generation (RAG) systems, most retrieval methodologies are often developed for factual retrieval, which assumes query and positive documents are semantically similar. In this paper, we instead propose and study a more challenging type of retrieval task, called hidden rationale retrieval, in which query and document are not similar but can be inferred by reasoning chains, logic relationships, or empirical experiences. To address such problems, an instruction-tuned Large language model (LLM) with a cross-encoder architecture could be a reasonable choice. To further strengthen pioneering LLM-based retrievers, we design a special instruction that transforms the retrieval task into a generative task by prompting LLM to answer a binary-choice question. The model can be fine-tuned with direct preference optimization (DPO). The framework is also optimized for computational efficiency with no performance degradation. We name this retrieval framework by RaHoRe and verify its zero-shot and fine-tuned performance superiority on Emotional Support Conversation (ESC), compared with previous retrieval works. Our study suggests the potential to employ LLM as a foundation for a wider scope of retrieval tasks. Our codes, models, and datasets are available on https://github.com/flyfree5/LaHoRe.","authors":["Luo Ji","Feixiang Guo","Teng Chen","Qingqing Gu","Xiaoyu Wang","Ningyuan Xi","Yihong Wang","Peng Yu","Yue Zhao","Hongyang Lei","Zhonglin Jiang","Yong Chen"],"url":"https://arxiv.org/abs/2412.16615"}
{"created":"2025-04-11","title":"EasyVis2: A Real Time Multi-view 3D Visualization System for Laparoscopic Surgery Training Enhanced by a Deep Neural Network YOLOv8-Pose","abstract":"EasyVis2 is a system designed to provide hands-free, real-time 3D visualization for laparoscopic surgery. It incorporates a surgical trocar equipped with an array of micro-cameras, which can be inserted into the body cavity to offer an enhanced field of view and a 3D perspective of the surgical procedure. A specialized deep neural network algorithm, YOLOv8-Pose, is utilized to estimate the position and orientation of surgical instruments in each individual camera view. These multi-view estimates enable the calculation of 3D poses of surgical tools, facilitating the rendering of a 3D surface model of the instruments, overlaid on the background scene, for real-time visualization. This study presents methods for adapting YOLOv8-Pose to the EasyVis2 system, including the development of a tailored training dataset. Experimental results demonstrate that, with an identical number of cameras, the new system improves 3D reconstruction accuracy and reduces computation time. Additionally, the adapted YOLOv8-Pose system shows high accuracy in 2D pose estimation.","authors":["Yung-Hong Sun","Gefei Shen","Jiangang Chen","Jayer Fernandes","Amber L. Shada","Charles P. Heise","Hongrui Jiang","Yu Hen Hu"],"url":"https://arxiv.org/abs/2412.16742"}
{"created":"2025-04-11","title":"A Novel Algorithm for Periodic Conformal Flattening of Genus-one and Multiply Connected Genus-zero Surfaces","abstract":"In this paper, we propose a novel parameterization method for genus-one and multiply connected genus-zero surfaces, called periodic conformal flattening. The conformal energy minimization technique is utilized to compute the desired conformal map, which is characterised as an easily solvable quadratic functional minimization problem, yielding a sparse linear system. The advantages of the proposed algorithms DPCF and SPCF are a) independence from the cut path selection, which introduces no additional conformal distortion near the cut seams; b) bijectivity guaranteeing for intrinsic Delaunay triangulations. The numerical experiments illustrate that DPCF and SPCF express high accuracy and a 4-5 times improvement in terms of efficiency compared with state-of-the-art algorithms.Based on the theoretical proof of the bijectivity guaranteeing, a simple strategy is applied for to guarantee the bijectivity of the resulting maps for non-Delaunay triangulations. The application on texture mapping illustrates the practicality of our developed algorithms.","authors":["Zhong-Heng Tan","Tiexiang Li","Wen-Wei Lin","Shing-Tung Yau"],"url":"https://arxiv.org/abs/2412.19052"}
{"created":"2025-04-11","title":"The Essence of Contextual Understanding in Theory of Mind: A Study on Question Answering with Story Characters","abstract":"Theory-of-Mind (ToM) is a fundamental psychological capability that allows humans to understand and interpret the mental states of others. Humans infer others' thoughts by integrating causal cues and indirect clues from broad contextual information, often derived from past interactions. In other words, human ToM heavily relies on the understanding about the backgrounds and life stories of others. Unfortunately, this aspect is largely overlooked in existing benchmarks for evaluating machines' ToM capabilities, due to their usage of short narratives without global context, especially personal background of characters. In this paper, we verify the importance of comprehensive contextual understanding about personal backgrounds in ToM and assess the performance of LLMs in such complex scenarios. To achieve this, we introduce CharToM benchmark, comprising 1,035 ToM questions based on characters from classic novels. Our human study reveals a significant disparity in performance: the same group of educated participants performs dramatically better when they have read the novels compared to when they have not. In parallel, our experiments on state-of-the-art LLMs, including the very recent o1 and DeepSeek-R1 models, show that LLMs still perform notably worse than humans, despite that they have seen these stories during pre-training. This highlights the limitations of current LLMs in capturing the nuanced contextual information required for ToM reasoning.","authors":["Chulun Zhou","Qiujing Wang","Mo Yu","Xiaoqian Yue","Rui Lu","Jiangnan Li","Yifan Zhou","Shunchi Zhang","Jie Zhou","Wai Lam"],"url":"https://arxiv.org/abs/2501.01705"}
{"created":"2025-04-11","title":"PRMBench: A Fine-grained and Challenging Benchmark for Process-Level Reward Models","abstract":"Process-level Reward Models (PRMs) are crucial for complex reasoning and decision-making tasks, where each intermediate step plays an important role in the reasoning process. Since language models are prone to various types of errors during the reasoning process, PRMs are required to possess nuanced capabilities for detecting various implicit error types in real-world scenarios. However, current benchmarks primarily focus on step correctness, failing to evaluate PRMs' performance systematically. To address this gap, we introduce PRMBench, a process-level benchmark specifically designed to assess the fine-grained error detection capabilities of PRMs. PRMBench comprises 6,216 carefully designed problems and 83,456 step-level labels, evaluating models across multiple dimensions, including simplicity, soundness, and sensitivity. In our experiments on 15 models, spanning both open-source PRMs and closed-source large language models prompted as critic models, we uncover significant weaknesses in current PRMs. These findings underscore the challenges inherent in process-level evaluation and highlight key directions for future research. We hope PRMBench can be a robust bench for advancing research on PRM evaluation and development.","authors":["Mingyang Song","Zhaochen Su","Xiaoye Qu","Jiawei Zhou","Yu Cheng"],"url":"https://arxiv.org/abs/2501.03124"}
{"created":"2025-04-11","title":"Automated Generation of Challenging Multiple-Choice Questions for Vision Language Model Evaluation","abstract":"The rapid development of vision language models (VLMs) demands rigorous and reliable evaluation. However, current visual question answering (VQA) benchmarks often depend on open-ended questions, making accurate evaluation difficult due to the variability in natural language responses. To address this, we introduce AutoConverter, an agentic framework that automatically converts these open-ended questions into multiple-choice format, enabling objective evaluation while reducing the costly multiple-choice question creation process. Our experiments demonstrate that AutoConverter can generate correct and challenging multiple-choice questions, with VLMs demonstrating consistently similar or lower accuracy on these questions compared to human-created ones. Using AutoConverter, we construct VMCBench, a benchmark created by transforming 20 existing VQA datasets into a unified multiple-choice format, totaling 9,018 questions. We comprehensively evaluate 33 state-of-the-art VLMs on VMCBench, setting a new standard for scalable, consistent, and reproducible VLM evaluation.","authors":["Yuhui Zhang","Yuchang Su","Yiming Liu","Xiaohan Wang","James Burgess","Elaine Sui","Chenyu Wang","Josiah Aklilu","Alejandro Lozano","Anjiang Wei","Ludwig Schmidt","Serena Yeung-Levy"],"url":"https://arxiv.org/abs/2501.03225"}
{"created":"2025-04-11","title":"Dolphin: Moving Towards Closed-loop Auto-research through Thinking, Practice, and Feedback","abstract":"The scientific research paradigm is undergoing a profound transformation owing to the development of Artificial Intelligence (AI). Recent works demonstrate that various AI-assisted research methods can largely improve research efficiency by improving data analysis, accelerating computation, and fostering novel idea generation. To further move towards the ultimate goal (i.e., automatic scientific research), in this paper, we introduce Dolphin, a closed-loop LLM-driven framework to enhance the automation level of scientific research. Dolphin first generates novel ideas based on feedback from previous experiments and relevant papers ranked by the topic and task attributes. Then, the generated ideas can be implemented using a code template refined and debugged with the designed exception-traceback-guided local code structure. Finally, Dolphin automatically analyzes the results of each idea and feeds the results back to the next round of idea generation. Experiments are conducted on the benchmark datasets of different topics and a subset of MLE-bench. Results show that Dolphin can continuously improve the performance of the input topic in a loop. We highlight that Dolphin can automatically propose methods that are comparable to the state-of-the-art in some tasks such as 3D point classification.","authors":["Jiakang Yuan","Xiangchao Yan","Shiyang Feng","Bo Zhang","Tao Chen","Botian Shi","Wanli Ouyang","Yu Qiao","Lei Bai","Bowen Zhou"],"url":"https://arxiv.org/abs/2501.03916"}
{"created":"2025-04-11","title":"Enhancing Virtual Try-On with Synthetic Pairs and Error-Aware Noise Scheduling","abstract":"Given an isolated garment image in a canonical product view and a separate image of a person, the virtual try-on task aims to generate a new image of the person wearing the target garment. Prior virtual try-on works face two major challenges in achieving this goal: a) the paired (human, garment) training data has limited availability; b) generating textures on the human that perfectly match that of the prompted garment is difficult, often resulting in distorted text and faded textures. Our work explores ways to tackle these issues through both synthetic data as well as model refinement. We introduce a garment extraction model that generates (human, synthetic garment) pairs from a single image of a clothed individual. The synthetic pairs can then be used to augment the training of virtual try-on. We also propose an Error-Aware Refinement-based Schr\\\"odinger Bridge (EARSB) that surgically targets localized generation errors for correcting the output of a base virtual try-on model. To identify likely errors, we propose a weakly-supervised error classifier that localizes regions for refinement, subsequently augmenting the Schr\\\"odinger Bridge's noise schedule with its confidence heatmap. Experiments on VITON-HD and DressCode-Upper demonstrate that our synthetic data augmentation enhances the performance of prior work, while EARSB improves the overall image quality. In user studies, our model is preferred by the users in an average of 59% of cases.","authors":["Nannan Li","Kevin J. Shih","Bryan A. Plummer"],"url":"https://arxiv.org/abs/2501.04666"}
{"created":"2025-04-11","title":"Identifying Information from Observations with Uncertainty and Novelty","abstract":"A machine learning tasks from observations must encounter and process uncertainty and novelty, especially when it is to maintain performance when observing new information and to choose the hypothesis that best fits the current observations. In this context, some key questions arise: what and how much information did the observations provide, how much information is required to identify the data-generating process, how many observations remain to get that information, and how does a predictor determine that it has observed novel information? This paper strengthens existing answers to these questions by formalizing the notion of identifiable information that arises from the language used to express the relationship between distinct states. Model identifiability and sample complexity are defined via computation of an indicator function over a set of hypotheses, bridging algorithmic and probabilistic information. Their properties and asymptotic statistics are described for data-generating processes ranging from deterministic processes to ergodic stationary stochastic processes. This connects the notion of identifying information in finite steps with asymptotic statistics and PAC-learning. The indicator function's computation naturally formalizes novel information and its identification from observations with respect to a hypothesis set. We also proved that computable PAC-Bayes learners' sample complexity distribution is determined by its moments in terms of the prior probability distribution over a fixed finite hypothesis set.","authors":["Derek S. Prijatelj (University of Notre Dame)","Timothy J. Ireland (Independent Researcher)","Walter J. Scheirer (University of Notre Dame)"],"url":"https://arxiv.org/abs/2501.09331"}
{"created":"2025-04-11","title":"Learning Latent Hardening (LLH): Enhancing Deep Learning with Domain Knowledge for Material Inverse Problems","abstract":"Advancements in deep learning and machine learning have improved the ability to model complex, nonlinear relationships, such as those encountered in complex material inverse problems. However, the effectiveness of these methods often depends on large datasets, which are not always available. In this study, the incorporation of domain-specific knowledge of the mechanical behavior of material microstructures is investigated to evaluate the impact on the predictive performance of the models in data-scarce scenarios. To overcome data limitations, a two-step framework, Learning Latent Hardening (LLH), is proposed. In the first step of LLH, a Deep Neural Network is employed to reconstruct full stress-strain curves from randomly selected portions of the stress-strain curves to capture the latent mechanical response of a material based on key microstructural features. In the second step of LLH, the results of the reconstructed stress-strain curves are leveraged to predict key microstructural features of porous materials. The performance of six deep learning and/or machine learning models trained with and without domain knowledge are compared: Convolutional Neural Networks, Deep Neural Networks, Extreme Gradient Boosting, K-Nearest Neighbors, Long Short-Term Memory, and Random Forest. The results from the models with domain-specific information consistently achieved higher $R^2$ values compared to models without prior knowledge. Models without domain knowledge missed critical patterns linking stress-strain behavior to microstructural changes, whereas domain-informed models better identified essential stress-strain features predictive of microstructure. These findings highlight the importance of integrating domain-specific knowledge with deep learning to achieve accurate outcomes in materials science.","authors":["Qinyi Tian","Winston Lindqwister","Manolis Veveakis","Laura E. Dalton"],"url":"https://arxiv.org/abs/2501.10481"}
{"created":"2025-04-11","title":"Prompt-Enabled Large AI Models for CSI Feedback","abstract":"Artificial intelligence (AI) has emerged as a promising tool for channel state information (CSI) feedback. While recent research primarily focuses on improving feedback accuracy on a specific dataset through novel architectures, the underlying mechanism of AI-based CSI feedback remains unclear. This study explores the mechanism through analyzing performance across diverse datasets, with findings suggesting that superior feedback performance stems from AI models' strong fitting capabilities and their ability to leverage environmental knowledge. Building on these findings, we propose a prompt enabled large AI model (LAM) for CSI feedback. The LAM employs powerful transformer blocks and is trained on extensive datasets from various scenarios. Meanwhile, the channel distribution (environmental knowledge) -- represented as the mean of channel magnitude in the angular-delay domain -- is incorporated as a prompt within the decoder to further enhance reconstruction quality. Simulation results confirm that the proposed prompt-enabled LAM significantly improves feedback accuracy and generalization performance while reducing data collection requirements in new scenarios.","authors":["Jiajia Guo","Yiming Cui","Chao-Kai Wen","Shi Jin"],"url":"https://arxiv.org/abs/2501.10629"}
{"created":"2025-04-11","title":"Unified CNNs and transformers underlying learning mechanism reveals multi-head attention modus vivendi","abstract":"Convolutional neural networks (CNNs) evaluate short-range correlations in input images which progress along the layers, whereas vision transformer (ViT) architectures evaluate long-range correlations, using repeated transformer encoders composed of fully connected layers. Both are designed to solve complex classification tasks but from different perspectives. This study demonstrates that CNNs and ViT architectures stem from a unified underlying learning mechanism, which quantitatively measures the single-nodal performance (SNP) of each node in feedforward (FF) and multi-head attention (MHA) sub-blocks. Each node identifies small clusters of possible output labels, with additional noise represented as labels outside these clusters. These features are progressively sharpened along the transformer encoders, enhancing the signal-to-noise ratio. This unified underlying learning mechanism leads to two main findings. First, it enables an efficient applied nodal diagonal connection (ANDC) pruning technique without affecting the accuracy. Second, based on the SNP, spontaneous symmetry breaking occurs among the MHA heads, such that each head focuses its attention on a subset of labels through cooperation among its SNPs. Consequently, each head becomes an expert in recognizing its designated labels, representing a quantitative MHA modus vivendi mechanism. This statistical mechanics inspired viewpoint enables to reveal macroscopic behavior of the entire network from the microscopic performance of each node. These results are based on a compact convolutional transformer architecture trained on the CIFAR-100 and Flowers-102 datasets and call for their extension to other architectures and applications, such as natural language processing.","authors":["Ella Koresh","Ronit D. Gross","Yuval Meir","Yarden Tzach","Tal Halevi","Ido Kanter"],"url":"https://arxiv.org/abs/2501.12900"}
{"created":"2025-04-11","title":"Towards Communication-Efficient Adversarial Federated Learning for Robust Edge Intelligence","abstract":"Federated learning (FL) has gained significant attention for enabling decentralized training on edge networks without exposing raw data. However, FL models remain susceptible to adversarial attacks and performance degradation in non-IID data settings, thus posing challenges to both robustness and accuracy. This paper aims to achieve communication-efficient adversarial federated learning (AFL) by leveraging a pre-trained model to enhance both robustness and accuracy under adversarial attacks and non-IID challenges in AFL. By leveraging the knowledge from a pre-trained model for both clean and adversarial images, we propose a pre-trained model-guided adversarial federated learning (PM-AFL) framework. This framework integrates vanilla and adversarial mixture knowledge distillation to effectively balance accuracy and robustness while promoting local models to learn from diverse data. Specifically, for clean accuracy, we adopt a dual distillation strategy where the class probabilities of randomly paired images, and their blended versions are aligned between the teacher model and the local models. For adversarial robustness, we employ a similar distillation approach but replace clean samples on the local side with adversarial examples. Moreover, by considering the bias between local and global models, we also incorporate a consistency regularization term to ensure that local adversarial predictions stay aligned with their corresponding global clean ones. These strategies collectively enable local models to absorb diverse knowledge from the teacher model while maintaining close alignment with the global model, thereby mitigating overfitting to local optima and enhancing the generalization of the global model. Experiments demonstrate that the PM-AFL-based framework not only significantly outperforms other methods but also maintains communication efficiency.","authors":["Yu Qiao","Apurba Adhikary","Huy Q. Le","Eui-Nam Huh","Zhu Han","Choong Seon Hong"],"url":"https://arxiv.org/abs/2501.15257"}
{"created":"2025-04-11","title":"Privacy Attacks on Image AutoRegressive Models","abstract":"Image autoregressive generation has emerged as a powerful new paradigm, with image autoregressive models (IARs) matching state-of-the-art diffusion models (DMs) in image quality (FID: 1.48 vs. 1.58) while allowing for higher generation speed. However, the privacy risks associated with IARs remain unexplored, raising concerns about their responsible deployment. To address this gap, we conduct a comprehensive privacy analysis of IARs, comparing their privacy risks to those of DMs as a reference point. Specifically, we develop a novel membership inference attack (MIA) that achieves a remarkably high success rate in detecting training images, with a True Positive Rate at False Positive Rate = 1% (TPR@FPR=1%) of 86.38%, compared to just 6.38% for DMs using comparable attacks. We leverage our novel MIA to perform dataset inference (DI) for IARs and show that it requires as few as 6 samples to detect dataset membership, compared to 200 samples for DI in DMs. This confirms a higher level of information leakage in IARs. Finally, we are able to extract hundreds of training data points from an IAR (e.g., 698 from VAR-d30). Our results suggest a fundamental privacy-utility trade-off: while IARs excel in image generation quality and speed, they are empirically significantly more vulnerable to privacy attacks compared to DMs that achieve similar performance. This trend suggests that incorporating techniques from DMs into IARs, such as modeling the per-token probability distribution using a diffusion procedure, could help mitigate IARs' vulnerability to privacy attacks. We make our code available at: https://github.com/sprintml/privacy_attacks_against_iars","authors":["Antoni Kowalczuk","Jan Dubi\\'nski","Franziska Boenisch","Adam Dziedzic"],"url":"https://arxiv.org/abs/2502.02514"}
{"created":"2025-04-11","title":"Intent Representation Learning with Large Language Model for Recommendation","abstract":"Intent-based recommender systems have garnered significant attention for uncovering latent fine-grained preferences. Intents, as underlying factors of interactions, are crucial for improving recommendation interpretability. Most methods define intents as learnable parameters updated alongside interactions. However, existing frameworks often overlook textual information (e.g., user reviews, item descriptions), which is crucial for alleviating the sparsity of interaction intents. Exploring these multimodal intents, especially the inherent differences in representation spaces, poses two key challenges: i) How to align multimodal intents and effectively mitigate noise issues; ii) How to extract and match latent key intents across modalities. To tackle these challenges, we propose a model-agnostic framework, Intent Representation Learning with Large Language Model (IRLLRec), which leverages large language models (LLMs) to construct multimodal intents and enhance recommendations. Specifically, IRLLRec employs a dual-tower architecture to learn multimodal intent representations. Next, we propose pairwise and translation alignment to eliminate inter-modal differences and enhance robustness against noisy input features. Finally, to better match textual and interaction-based intents, we employ momentum distillation to perform teacher-student learning on fused intent representations. Empirical evaluations on three datasets show that our IRLLRec framework outperforms baselines.Code available at https://github.com/wangyu0627/IRLLRec.","authors":["Yu Wang","Lei Sang","Yi Zhang","Yiwen Zhang"],"url":"https://arxiv.org/abs/2502.03307"}
{"created":"2025-04-11","title":"Interactive Visualization Recommendation with Hier-SUCB","abstract":"Visualization recommendation aims to enable rapid visual analysis of massive datasets. In real-world scenarios, it is essential to quickly gather and comprehend user preferences to cover users from diverse backgrounds, including varying skill levels and analytical tasks. Previous approaches to personalized visualization recommendations are non-interactive and rely on initial user data for new users. As a result, these models cannot effectively explore options or adapt to real-time feedback. To address this limitation, we propose an interactive personalized visualization recommendation (PVisRec) system that learns on user feedback from previous interactions. For more interactive and accurate recommendations, we propose Hier-SUCB, a contextual combinatorial semi-bandit in the PVisRec setting. Theoretically, we show an improved overall regret bound with the same rank of time but an improved rank of action space. We further demonstrate the effectiveness of Hier-SUCB through extensive experiments where it is comparable to offline methods and outperforms other bandit algorithms in the setting of visualization recommendation.","authors":["Songwen Hu","Ryan A. Rossi","Tong Yu","Junda Wu","Handong Zhao","Sungchul Kim","Shuai Li"],"url":"https://arxiv.org/abs/2502.03375"}
{"created":"2025-04-11","title":"Differential Adjusted Parity for Learning Fair Representations","abstract":"The development of fair and unbiased machine learning models remains an ongoing objective for researchers in the field of artificial intelligence. We introduce the Differential Adjusted Parity (DAP) loss to produce unbiased informative representations. It utilises a differentiable variant of the adjusted parity metric to create a unified objective function. By combining downstream task classification accuracy and its inconsistency across sensitive feature domains, it provides a single tool to increase performance and mitigate bias. A key element in this approach is the use of soft balanced accuracies. In contrast to previous non-adversarial approaches, DAP does not suffer a degeneracy where the metric is satisfied by performing equally poorly across all sensitive domains. It outperforms several adversarial models on downstream task accuracy and fairness in our analysis. Specifically, it improves the demographic parity, equalized odds and sensitive feature accuracy by as much as 22.5\\%, 44.1\\% and 40.1\\%, respectively, when compared to the best performing adversarial approaches on these metrics. Overall, the DAP loss and its associated metric can play a significant role in creating more fair machine learning models.","authors":["Bucher Sahyouni","Matthew Vowels","Liqun Chen","Simon Hadfield"],"url":"https://arxiv.org/abs/2502.09765"}
{"created":"2025-04-11","title":"SpeechCap: Leveraging Playful Impact Captions to Facilitate Interpersonal Communication in Social Virtual Reality","abstract":"Social Virtual Reality (VR) emerges as a promising platform bringing immersive, interactive, and engaging mechanisms for collaborative activities in virtual spaces. However, interpersonal communication in social VR is still limited with existing mediums and channels. To bridge the gap, we propose a novel method for mediating real-time conversation in social VR, which uses impact captions, a type of typographic visual effect widely used in videos, to convey both verbal and non-verbal information. We first investigated the design space of impact captions by content analysis and a co-design session with four experts. Next, we implemented SpeechCap as a proof-of-concept system, with which users can communicate with each other using speech-driven impact captions in VR. Through a user study (n=14), we evaluated the effectiveness of the visual and interaction design of impact captions, highlighting the interactivity and the integration of verbal and non-verbal information in communication mediums. Finally, we discussed topics of visual rhetoric, interactivity, and ambiguity as the main findings from the study, and further provided design implications for future work for facilitating interpersonal communication in social VR.","authors":["Yu Zhang","Yi Wen","Siying Hu","Zhicong Lu"],"url":"https://arxiv.org/abs/2502.10736"}
{"created":"2025-04-11","title":"LogiDynamics: Unraveling the Dynamics of Logical Inference in Large Language Model Reasoning","abstract":"Modern large language models (LLMs) employ various forms of logical inference, both implicitly and explicitly, when addressing reasoning tasks. Understanding how to optimally leverage these inference paradigms is critical for advancing LLMs' reasoning capabilities. This paper adopts an exploratory approach by introducing a controlled evaluation environment for analogical reasoning -- a fundamental cognitive task -- that is systematically parameterized across three dimensions: modality (textual, visual, symbolic), difficulty (easy, medium, hard), and task format (multiple-choice or free-text generation). We analyze the comparative dynamics of inductive, abductive, and deductive inference pipelines across these dimensions, and demonstrate that our findings generalize to broader in-context learning tasks. Additionally, we investigate advanced paradigms such as hypothesis selection, verification, and refinement, revealing their potential to scale up logical inference in LLM reasoning. This exploratory study provides a foundation for future research in enhancing LLM reasoning through systematic logical inference strategies. Resources are available at https://github.com/HKUST-KnowComp/LogiDynamics.","authors":["Tianshi Zheng","Jiayang Cheng","Chunyang Li","Haochen Shi","Zihao Wang","Jiaxin Bai","Yangqiu Song","Ginny Y. Wong","Simon See"],"url":"https://arxiv.org/abs/2502.11176"}
{"created":"2025-04-11","title":"Cohesive Subgraph Discovery in Hypergraphs: A Locality-Driven Indexing Framework","abstract":"Hypergraphs, increasingly utilised for modelling complex and diverse relationships in modern networks, gain much attention representing intricate higher-order interactions. Among various challenges, cohesive subgraph discovery is one of the fundamental problems and offers deep insights into these structures, yet the task of selecting appropriate parameters is an open question. To handle that question, we aim to design an efficient indexing structure to retrieve cohesive subgraphs in an online manner. The main idea is to enable the discovery of corresponding structures within a reasonable time without the need for exhaustive graph traversals. This work can facilitate efficient and informed decision-making in diverse applications based on a comprehensive understanding of the entire network landscape. Through extensive experiments on real-world networks, we demonstrate the superiority of our proposed indexing technique.","authors":["Song Kim","Dahee Kim","Taejoon Han","Junghoon Kim","Hyun Ji Jeong","Jungeun Kim"],"url":"https://arxiv.org/abs/2502.12523"}
{"created":"2025-04-11","title":"CW-BASS: Confidence-Weighted Boundary-Aware Learning for Semi-Supervised Semantic Segmentation","abstract":"Semi-supervised semantic segmentation (SSSS) aims to improve segmentation performance by utilizing large amounts of unlabeled data with limited labeled samples. Existing methods often suffer from coupling, where over-reliance on initial labeled data leads to suboptimal learning; confirmation bias, where incorrect predictions reinforce themselves repeatedly; and boundary blur caused by limited boundary-awareness and ambiguous edge cues. To address these issues, we propose CW-BASS, a novel framework for SSSS. In order to mitigate the impact of incorrect predictions, we assign confidence weights to pseudo-labels. Additionally, we leverage boundary-delineation techniques, which, despite being extensively explored in weakly-supervised semantic segmentation (WSSS), remain underutilized in SSSS. Specifically, our method: (1) reduces coupling via a confidence-weighted loss that adjusts pseudo-label influence based on their predicted confidence scores, (2) mitigates confirmation bias with a dynamic thresholding mechanism that learns to filter out pseudo-labels based on model performance, (3) tackles boundary blur using a boundary-aware module to refine segmentation near object edges, and (4) reduces label noise through a confidence decay strategy that progressively refines pseudo-labels during training. Extensive experiments on Pascal VOC 2012 and Cityscapes demonstrate that CW-BASS achieves state-of-the-art performance. Notably, CW-BASS achieves a 65.9% mIoU on Cityscapes under a challenging and underexplored 1/30 (3.3%) split (100 images), highlighting its effectiveness in limited-label settings. Our code is available at https://github.com/psychofict/CW-BASS.","authors":["Ebenezer Tarubinga","Jenifer Kalafatovich","Seong-Whan Lee"],"url":"https://arxiv.org/abs/2502.15152"}
{"created":"2025-04-11","title":"Monte Carlo Temperature: a robust sampling strategy for LLM's uncertainty quantification methods","abstract":"Uncertainty quantification (UQ) in Large Language Models (LLMs) is essential for their safe and reliable deployment, particularly in critical applications where incorrect outputs can have serious consequences. Current UQ methods typically rely on querying the model multiple times using non-zero temperature sampling to generate diverse outputs for uncertainty estimation. However, the impact of selecting a given temperature parameter is understudied, and our analysis reveals that temperature plays a fundamental role in the quality of uncertainty estimates. The conventional approach of identifying optimal temperature values requires expensive hyperparameter optimization (HPO) that must be repeated for each new model-dataset combination. We propose Monte Carlo Temperature (MCT), a robust sampling strategy that eliminates the need for temperature calibration. Our analysis reveals that: 1) MCT provides more robust uncertainty estimates across a wide range of temperatures, 2) MCT improves the performance of UQ methods by replacing fixed-temperature strategies that do not rely on HPO, and 3) MCT achieves statistical parity with oracle temperatures, which represent the ideal outcome of a well-tuned but computationally expensive HPO process. These findings demonstrate that effective UQ can be achieved without the computational burden of temperature parameter calibration.","authors":["Nicola Cecere","Andrea Bacciu","Ignacio Fern\\'andez Tob\\'ias","Amin Mantrach"],"url":"https://arxiv.org/abs/2502.18389"}
{"created":"2025-04-11","title":"A Lightweight and Extensible Cell Segmentation and Classification Model for Whole Slide Images","abstract":"Developing clinically useful cell-level analysis tools in digital pathology remains challenging due to limitations in dataset granularity, inconsistent annotations, high computational demands, and difficulties integrating new technologies into workflows. To address these issues, we propose a solution that enhances data quality, model performance, and usability by creating a lightweight, extensible cell segmentation and classification model. First, we update data labels through cross-relabeling to refine annotations of PanNuke and MoNuSAC, producing a unified dataset with seven distinct cell types. Second, we leverage the H-Optimus foundation model as a fixed encoder to improve feature representation for simultaneous segmentation and classification tasks. Third, to address foundation models' computational demands, we distill knowledge to reduce model size and complexity while maintaining comparable performance. Finally, we integrate the distilled model into QuPath, a widely used open-source digital pathology platform. Results demonstrate improved segmentation and classification performance using the H-Optimus-based model compared to a CNN-based model. Specifically, average $R^2$ improved from 0.575 to 0.871, and average $PQ$ score improved from 0.450 to 0.492, indicating better alignment with actual cell counts and enhanced segmentation quality. The distilled model maintains comparable performance while reducing parameter count by a factor of 48. By reducing computational complexity and integrating into workflows, this approach may significantly impact diagnostics, reduce pathologist workload, and improve outcomes. Although the method shows promise, extensive validation is necessary prior to clinical deployment.","authors":["Nikita Shvetsov","Thomas K. Kilvaer","Masoud Tafavvoghi","Anders Sildnes","Kajsa M{\\o}llersen","Lill-Tove Rasmussen Busund","Lars Ailo Bongo"],"url":"https://arxiv.org/abs/2502.19217"}
{"created":"2025-04-11","title":"Show and Tell: Visually Explainable Deep Neural Nets via Spatially-Aware Concept Bottleneck Models","abstract":"Modern deep neural networks have now reached human-level performance across a variety of tasks. However, unlike humans they lack the ability to explain their decisions by showing where and telling what concepts guided them. In this work, we present a unified framework for transforming any vision neural network into a spatially and conceptually interpretable model. We introduce a spatially-aware concept bottleneck layer that projects \"black-box\" features of pre-trained backbone models into interpretable concept maps, without requiring human labels. By training a classification layer over this bottleneck, we obtain a self-explaining model that articulates which concepts most influenced its prediction, along with heatmaps that ground them in the input image. Accordingly, we name this method \"Spatially-Aware and Label-Free Concept Bottleneck Model\" (SALF-CBM). Our results show that the proposed SALF-CBM: (1) Outperforms non-spatial CBM methods, as well as the original backbone, on a variety of classification tasks; (2) Produces high-quality spatial explanations, outperforming widely used heatmap-based methods on a zero-shot segmentation task; (3) Facilitates model exploration and debugging, enabling users to query specific image regions and refine the model's decisions by locally editing its concept maps.","authors":["Itay Benou","Tammy Riklin-Raviv"],"url":"https://arxiv.org/abs/2502.20134"}
{"created":"2025-04-11","title":"Lifted Frequency-Domain Identification of Closed-Loop Multirate Systems: Applied to Dual-Stage Actuator Hard Disk Drives","abstract":"Frequency-domain representations are crucial for the design and performance evaluation of controllers in multirate systems, specifically to address intersample performance. The aim of this paper is to develop an effective frequency-domain system identification technique for closed-loop multirate systems using solely slow-rate output measurements. By indirect identification of multivariable time-invariant representations through lifting, in combination with local modeling techniques, the multirate system is effectively identified. The developed method is capable of accurate identification of closed-loop multirate systems within a single identification experiment, using fast-rate excitation and inputs, and slow-rate outputs. Finally, the developed framework is validated using a benchmark problem consisting of a multivariable dual-stage actuator from a hard disk drive, demonstrating its applicability and accuracy.","authors":["Max van Haren","Masahiro Mae","Lennart Blanken","Tom Oomen"],"url":"https://arxiv.org/abs/2502.21065"}
{"created":"2025-04-11","title":"VideoPainter: Any-length Video Inpainting and Editing with Plug-and-Play Context Control","abstract":"Video inpainting, which aims to restore corrupted video content, has experienced substantial progress. Despite these advances, existing methods, whether propagating unmasked region pixels through optical flow and receptive field priors, or extending image-inpainting models temporally, face challenges in generating fully masked objects or balancing the competing objectives of background context preservation and foreground generation in one model, respectively. To address these limitations, we propose a novel dual-stream paradigm VideoPainter that incorporates an efficient context encoder (comprising only 6% of the backbone parameters) to process masked videos and inject backbone-aware background contextual cues to any pre-trained video DiT, producing semantically consistent content in a plug-and-play manner. This architectural separation significantly reduces the model's learning complexity while enabling nuanced integration of crucial background context. We also introduce a novel target region ID resampling technique that enables any-length video inpainting, greatly enhancing our practical applicability. Additionally, we establish a scalable dataset pipeline leveraging current vision understanding models, contributing VPData and VPBench to facilitate segmentation-based inpainting training and assessment, the largest video inpainting dataset and benchmark to date with over 390K diverse clips. Using inpainting as a pipeline basis, we also explore downstream applications including video editing and video editing pair data generation, demonstrating competitive performance and significant practical potential. Extensive experiments demonstrate VideoPainter's superior performance in both any-length video inpainting and editing, across eight key metrics, including video quality, mask region preservation, and textual coherence.","authors":["Yuxuan Bian","Zhaoyang Zhang","Xuan Ju","Mingdeng Cao","Liangbin Xie","Ying Shan","Qiang Xu"],"url":"https://arxiv.org/abs/2503.05639"}
{"created":"2025-04-11","title":"Multi-Fidelity Policy Gradient Algorithms","abstract":"Many reinforcement learning (RL) algorithms require large amounts of data, prohibiting their use in applications where frequent interactions with operational systems are infeasible, or high-fidelity simulations are expensive or unavailable. Meanwhile, low-fidelity simulators--such as reduced-order models, heuristic reward functions, or generative world models--can cheaply provide useful data for RL training, even if they are too coarse for direct sim-to-real transfer. We propose multi-fidelity policy gradients (MFPGs), an RL framework that mixes a small amount of data from the target environment with a large volume of low-fidelity simulation data to form unbiased, reduced-variance estimators (control variates) for on-policy policy gradients. We instantiate the framework by developing multi-fidelity variants of two policy gradient algorithms: REINFORCE and proximal policy optimization. Experimental results across a suite of simulated robotics benchmark problems demonstrate that when target-environment samples are limited, MFPG achieves up to 3.9x higher reward and improves training stability when compared to baselines that only use high-fidelity data. Moreover, even when the baselines are given more high-fidelity samples--up to 10x as many interactions with the target environment--MFPG continues to match or outperform them. Finally, we observe that MFPG is capable of training effective policies even when the low-fidelity environment is drastically different from the target environment. MFPG thus not only offers a novel paradigm for efficient sim-to-real transfer but also provides a principled approach to managing the trade-off between policy performance and data collection costs.","authors":["Xinjie Liu","Cyrus Neary","Kushagra Gupta","Christian Ellis","Ufuk Topcu","David Fridovich-Keil"],"url":"https://arxiv.org/abs/2503.05696"}
{"created":"2025-04-11","title":"A 2-6 GHz Ultra-Wideband CMOS Transceiver for Radar Applications","abstract":"This paper presents a low power, low cost transceiver architecture to implement radar-on-a-chip. The transceiver comprises of a full ultra-wideband (UWB) transmitter and a full UWB band receiver. A design methodology to maximize the tuning range of the voltage-controlled oscillator (VCO) is presented. At the transmitter side, a sub-harmonic mixer is used for signal up-conversion. The receiver low noise amplifier (LNA) has a 2 to 6 GHz input matching bandwidth with a power gain of 9 dB and a noise figure of 2.5 dB. The transceiver is implemented in Cadence EDA tools using 65nm CMOS technology. The system achieves a total dc power consumption of 50 mW. Good noise figure performance; good wide-band matching; gain; high level of integration; low power; low cost of the proposed UWB radar transceiver front-end make it a highly competitive SoC solution for low power UWB transceivers.","authors":["Alin Thomas Tharakan","Prince Philip","Gokulan T.","Sumit Kumar","Gaurab Banerjee"],"url":"https://arxiv.org/abs/2503.06057"}
{"created":"2025-04-11","title":"ChatGPT-4 in the Turing Test: A Critical Analysis","abstract":"This paper critically examines the recent publication \"ChatGPT-4 in the Turing Test\" by Restrepo Echavarr\\'ia (2025), challenging its central claims regarding the absence of minimally serious test implementations and the conclusion that ChatGPT-4 fails the Turing Test. The analysis reveals that the criticisms based on rigid criteria and limited experimental data are not fully justified. More importantly, the paper makes several constructive contributions that enrich our understanding of Turing Test implementations. It demonstrates that two distinct formats--the three-player and two-player tests--are both valid, each with unique methodological implications. The work distinguishes between absolute criteria (reflecting an optimal 50% identification rate in a three-player format) and relative criteria (which measure how closely a machine's performance approximates that of a human), offering a more nuanced evaluation framework. Furthermore, the paper clarifies the probabilistic underpinnings of both test types by modeling them as Bernoulli experiments--correlated in the three-player version and uncorrelated in the two-player version. This formalization allows for a rigorous separation between the theoretical criteria for passing the test, defined in probabilistic terms, and the experimental data that require robust statistical methods for proper interpretation. In doing so, the paper not only refutes key aspects of the criticized study but also lays a solid foundation for future research on objective measures of how closely an AI's behavior aligns with, or deviates from, that of a human being.","authors":["Marco Giunti"],"url":"https://arxiv.org/abs/2503.06551"}
{"created":"2025-04-11","title":"Using Large Language Models to Develop Requirements Elicitation Skills","abstract":"Requirements Elicitation (RE) is a crucial software engineering skill that involves interviewing a client and then devising a software design based on the interview results. Teaching this inherently experiential skill effectively has high cost, such as acquiring an industry partner to interview, or training course staff or other students to play the role of a client. As a result, a typical instructional approach is to provide students with transcripts of real or fictitious interviews to analyze, which exercises the skill of extracting technical requirements but fails to develop the equally important interview skill itself. As an alternative, we propose conditioning a large language model to play the role of the client during a chat-based interview. We perform a between-subjects study (n=120) in which students construct a high-level application design from either an interactive LLM-backed interview session or an existing interview transcript describing the same business processes. We evaluate our approach using both a qualitative survey and quantitative observations about participants' work. We find that both approaches provide sufficient information for participants to construct technically sound solutions and require comparable time on task, but the LLM-based approach is preferred by most participants. Importantly, we observe that LLM-backed interview is seen as both more realistic and more engaging, despite the LLM occasionally providing imprecise or contradictory information. These results, combined with the wide accessibility of LLMs, suggest a new way to practice critical RE skills in a scalable and realistic manner without the overhead of arranging live interviews.","authors":["Nelson Lojo (Univ. of California","Berkeley","CA","USA)","Rafael Gonz\\'alez (SCORE Lab","Univ. of Sevilla","Sevilla","Spain)","Rohan Philip (Oak Park High School","Oak Park","USA)","Jos\\'e Antonio Parejo (SCORE Lab","Univ. of Sevilla","Sevilla","Spain)","Amador Dur\\'an Toro (SCORE Lab","Univ. of Sevilla","Sevilla","Spain)","Armando Fox (Univ. of California","Berkeley","CA","USA)","Pablo Fern\\'andez (SCORE Lab","Univ. of Sevilla","Sevilla","Spain)"],"url":"https://arxiv.org/abs/2503.07800"}
{"created":"2025-04-11","title":"Preference-Based Alignment of Discrete Diffusion Models","abstract":"Diffusion models have achieved state-of-the-art performance across multiple domains, with recent advancements extending their applicability to discrete data. However, aligning discrete diffusion models with task-specific preferences remains challenging, particularly in scenarios where explicit reward functions are unavailable. In this work, we introduce Discrete Diffusion DPO (D2-DPO), the first adaptation of Direct Preference Optimization (DPO) to discrete diffusion models formulated as continuous-time Markov chains. Our approach derives a novel loss function that directly fine-tunes the generative process using preference data while preserving fidelity to a reference distribution. We validate D2-DPO on a structured binary sequence generation task, demonstrating that the method effectively aligns model outputs with preferences while maintaining structural validity. Our results highlight that D2-DPO enables controlled fine-tuning without requiring explicit reward models, making it a practical alternative to reinforcement learning-based approaches. Future research will explore extending D2-DPO to more complex generative tasks, including language modeling and protein sequence generation, as well as investigating alternative noise schedules, such as uniform noising, to enhance flexibility across different applications.","authors":["Umberto Borso","Davide Paglieri","Jude Wells","Tim Rockt\\\"aschel"],"url":"https://arxiv.org/abs/2503.08295"}
{"created":"2025-04-11","title":"Randomness, Not Representation: The Unreliability of Evaluating Cultural Alignment in LLMs","abstract":"Research on the 'cultural alignment' of Large Language Models (LLMs) has emerged in response to growing interest in understanding representation across diverse stakeholders. Current approaches to evaluating cultural alignment through survey-based assessments that borrow from social science methodologies often overlook systematic robustness checks. Here, we identify and test three assumptions behind current survey-based evaluation methods: (1) Stability: that cultural alignment is a property of LLMs rather than an artifact of evaluation design, (2) Extrapolability: that alignment with one culture on a narrow set of issues predicts alignment with that culture on others, and (3) Steerability: that LLMs can be reliably prompted to represent specific cultural perspectives. Through experiments examining both explicit and implicit preferences of leading LLMs, we find a high level of instability across presentation formats, incoherence between evaluated versus held-out cultural dimensions, and erratic behavior under prompt steering. We show that these inconsistencies can cause the results of an evaluation to be very sensitive to minor variations in methodology. Finally, we demonstrate in a case study on evaluation design that narrow experiments and a selective assessment of evidence can be used to paint an incomplete picture of LLMs' cultural alignment properties. Overall, these results highlight significant limitations of current survey-based approaches to evaluating the cultural alignment of LLMs and highlight a need for systematic robustness checks and red-teaming for evaluation results. Data and code are available at https://huggingface.co/datasets/akhan02/cultural-dimension-cover-letters and https://github.com/ariba-k/llm-cultural-alignment-evaluation, respectively.","authors":["Ariba Khan","Stephen Casper","Dylan Hadfield-Menell"],"url":"https://arxiv.org/abs/2503.08688"}
{"created":"2025-04-11","title":"Accurate Control under Voltage Drop for Rotor Drones","abstract":"This letter proposes an anti-disturbance control scheme for rotor drones to counteract voltage drop (VD) disturbance caused by voltage drop of the battery, which is a common case for long-time flight or aggressive maneuvers. Firstly, the refined dynamics of rotor drones considering VD disturbance are presented. Based on the dynamics, a voltage drop observer (VDO) is developed to accurately estimate the VD disturbance by decoupling the disturbance and state information of the drone, reducing the conservativeness of conventional disturbance observers. Subsequently, the control scheme integrates the VDO within the translational loop and a fixed-time sliding mode observer (SMO) within the rotational loop, enabling it to address force and torque disturbances caused by voltage drop of the battery. Sufficient real flight experiments are conducted to demonstrate the effectiveness of the proposed control scheme under VD disturbance.","authors":["Yuhang Liu","Jindou Jia","Zihan Yang","Kexin Guo"],"url":"https://arxiv.org/abs/2503.09017"}
{"created":"2025-04-11","title":"Discovering Influential Neuron Path in Vision Transformers","abstract":"Vision Transformer models exhibit immense power yet remain opaque to human understanding, posing challenges and risks for practical applications. While prior research has attempted to demystify these models through input attribution and neuron role analysis, there's been a notable gap in considering layer-level information and the holistic path of information flow across layers. In this paper, we investigate the significance of influential neuron paths within vision Transformers, which is a path of neurons from the model input to output that impacts the model inference most significantly. We first propose a joint influence measure to assess the contribution of a set of neurons to the model outcome. And we further provide a layer-progressive neuron locating approach that efficiently selects the most influential neuron at each layer trying to discover the crucial neuron path from input to output within the target model. Our experiments demonstrate the superiority of our method finding the most influential neuron path along which the information flows, over the existing baseline solutions. Additionally, the neuron paths have illustrated that vision Transformers exhibit some specific inner working mechanism for processing the visual information within the same image category. We further analyze the key effects of these neurons on the image classification task, showcasing that the found neuron paths have already preserved the model capability on downstream tasks, which may also shed some lights on real-world applications like model pruning. The project website including implementation code is available at https://foundation-model-research.github.io/NeuronPath/.","authors":["Yifan Wang","Yifei Liu","Yingdong Shi","Changming Li","Anqi Pang","Sibei Yang","Jingyi Yu","Kan Ren"],"url":"https://arxiv.org/abs/2503.09046"}
{"created":"2025-04-11","title":"Towards Reasoning Era: A Survey of Long Chain-of-Thought for Reasoning Large Language Models","abstract":"Recent advancements in reasoning with large language models (RLLMs), such as OpenAI-O1 and DeepSeek-R1, have demonstrated their impressive capabilities in complex domains like mathematics and coding. A central factor in their success lies in the application of long chain-of-thought (Long CoT) characteristics, which enhance reasoning abilities and enable the solution of intricate problems. However, despite these developments, a comprehensive survey on Long CoT is still lacking, limiting our understanding of its distinctions from traditional short chain-of-thought (Short CoT) and complicating ongoing debates on issues like \"overthinking\" and \"test-time scaling.\" This survey seeks to fill this gap by offering a unified perspective on Long CoT. (1) We first distinguish Long CoT from Short CoT and introduce a novel taxonomy to categorize current reasoning paradigms. (2) Next, we explore the key characteristics of Long CoT: deep reasoning, extensive exploration, and feasible reflection, which enable models to handle more complex tasks and produce more efficient, coherent outcomes compared to the shallower Short CoT. (3) We then investigate key phenomena such as the emergence of Long CoT with these characteristics, including overthinking, and test-time scaling, offering insights into how these processes manifest in practice. (4) Finally, we identify significant research gaps and highlight promising future directions, including the integration of multi-modal reasoning, efficiency improvements, and enhanced knowledge frameworks. By providing a structured overview, this survey aims to inspire future research and further the development of logical reasoning in artificial intelligence.","authors":["Qiguang Chen","Libo Qin","Jinhao Liu","Dengyun Peng","Jiannan Guan","Peng Wang","Mengkang Hu","Yuhang Zhou","Te Gao","Wanxiang Che"],"url":"https://arxiv.org/abs/2503.09567"}
{"created":"2025-04-11","title":"Unifying monitoring and modelling of water concentration levels in surface waters","abstract":"Accurate prediction of expected concentrations is essential for effective catchment management, requiring both extensive monitoring and advanced modeling techniques. However, due to limitations in the equation solving capacity, the integration of monitoring and modeling has been suffering suboptimal statistical approaches. This limitation results in models that can only partially leverage monitoring data, thus being an obstacle for realistic uncertainty assessments by overlooking critical correlations between both measurements and model parameters. This study presents a novel solution that integrates catchment monitoring and a unified hieratical statistical catchment modeling that employs a log-normal distribution for residuals within a left-censored likelihood function to address measurements below detection limits. This enables the estimation of concentrations within sub-catchments in conjunction with a source/fate sub-catchment model and monitoring data. This approach is possible due to a model builder R package denoted RTMB. The proposed approach introduces a statistical paradigm based on a hierarchical structure, capable of accommodating heterogeneous sampling across various sampling locations and the authors suggest that this also will encourage further refinement of other existing modeling platforms within the scientific community to improve synergy with monitoring programs. The application of the method is demonstrated through an analysis of nickel concentrations in Danish surface waters.","authors":["Peter B Sorensen","Anders Nielsen","Peter E Holm","Poul L Bjerg","Denitza Voutchkova","L{\\ae}rke Thorling","Dorte Rasmussen","Hans Estrup","Christian F Damgaard"],"url":"https://arxiv.org/abs/2503.10285"}
{"created":"2025-04-11","title":"Rotated Bitboards and Reinforcement Learning in Computer Chess and Beyond","abstract":"There exist several techniques for representing the chess board inside the computer. In the first part of this paper, the concepts of the bitboard-representation and the advantages of (rotated) bitboards in move generation are explained. In order to illustrate those ideas practice, the concrete implementation of the move-generator in FUSc# is discussed and we explain a technique how to verify the move-generator with the \"perft\"-command. We show that the move-generator of FUSc# works 100% correct.","authors":["Johannes Buchner"],"url":"https://arxiv.org/abs/2503.10822"}
{"created":"2025-04-11","title":"Non-Normalized Solutions of Generalized Nash Equilibrium in Autonomous Racing","abstract":"In dynamic games with shared constraints, Generalized Nash Equilibria (GNE) are often computed using the normalized solution concept, which assumes identical Lagrange multipliers for shared constraints across all players. While widely used, this approach excludes other potentially valuable GNE. This paper addresses the limitations of normalized solutions in racing scenarios through three key contributions. First, we highlight the shortcomings of normalized solutions with a simple racing example. Second, we propose a novel method based on the Mixed Complementarity Problem (MCP) formulation to compute non-normalized Generalized Nash Equilibria (GNE). Third, we demonstrate that our proposed method overcomes the limitations of normalized GNE solutions and enables richer multi-modal interactions in realistic racing scenarios.","authors":["Mark Pustilnik","Antonio Loquercio","Francesco Borrelli"],"url":"https://arxiv.org/abs/2503.12002"}
{"created":"2025-04-11","title":"SEAL: Semantic Aware Image Watermarking","abstract":"Generative models have rapidly evolved to generate realistic outputs. However, their synthetic outputs increasingly challenge the clear distinction between natural and AI-generated content, necessitating robust watermarking techniques. Watermarks are typically expected to preserve the integrity of the target image, withstand removal attempts, and prevent unauthorized replication onto unrelated images. To address this need, recent methods embed persistent watermarks into images produced by diffusion models using the initial noise. Yet, to do so, they either distort the distribution of generated images or rely on searching through a long dictionary of used keys for detection.","authors":["Kasra Arabi","R. Teal Witter","Chinmay Hegde","Niv Cohen"],"url":"https://arxiv.org/abs/2503.12172"}
{"created":"2025-04-11","title":"UniBERT: Adversarial Training for Language-Universal Representations","abstract":"This paper presents UniBERT, a compact multilingual language model that leverages an innovative training framework integrating three components: masked language modeling, adversarial training, and knowledge distillation. Pre-trained on a meticulously curated Wikipedia corpus spanning 107 languages, UniBERT is designed to reduce the computational demands of large-scale models while maintaining competitive performance across various natural language processing tasks. Comprehensive evaluations on four tasks -- named entity recognition, natural language inference, question answering, and semantic textual similarity -- demonstrate that our multilingual training strategy enhanced by an adversarial objective significantly improves cross-lingual generalization. Specifically, UniBERT models show an average relative improvement of 7.72% over traditional baselines, which achieved an average relative improvement of only 1.17%, with statistical analysis confirming the significance of these gains (p-value = 0.0181). This work highlights the benefits of combining adversarial training and knowledge distillation to build scalable and robust language models, thereby advancing the field of multilingual and cross-lingual natural language processing.","authors":["Andrei-Marius Avram","Marian Lupa\\c{s}cu","Dumitru-Clementin Cercel","Ionu\\c{t} Mironic\\u{a}","\\c{S}tefan Tr\\u{a}u\\c{s}an-Matu"],"url":"https://arxiv.org/abs/2503.12608"}
{"created":"2025-04-11","title":"Enhancing Job Salary Prediction with Disentangled Composition Effect Modeling: A Neural Prototyping Approach","abstract":"In the era of the knowledge economy, understanding how job skills influence salary is crucial for promoting recruitment with competitive salary systems and aligned salary expectations. Despite efforts on salary prediction based on job positions and talent demographics, there still lacks methods to effectively discern the set-structured skills' intricate composition effect on job salary. While recent advances in neural networks have significantly improved accurate set-based quantitative modeling, their lack of explainability hinders obtaining insights into the skills' composition effects. Indeed, model explanation for set data is challenging due to the combinatorial nature, rich semantics, and unique format. To this end, in this paper, we propose a novel intrinsically explainable set-based neural prototyping approach, namely \\textbf{LGDESetNet}, for explainable salary prediction that can reveal disentangled skill sets that impact salary from both local and global perspectives. Specifically, we propose a skill graph-enhanced disentangled discrete subset selection layer to identify multi-faceted influential input subsets with varied semantics. Furthermore, we propose a set-oriented prototype learning method to extract globally influential prototypical sets. The resulting output is transparently derived from the semantic interplay between these input subsets and global prototypes. Extensive experiments on four real-world datasets demonstrate that our method achieves superior performance than state-of-the-art baselines in salary prediction while providing explainable insights into salary-influencing patterns.","authors":["Yang Ji","Ying Sun","Hengshu Zhu"],"url":"https://arxiv.org/abs/2503.12978"}
{"created":"2025-04-11","title":"Verifying Equilibria in Finite-Horizon Probabilistic Concurrent Game Systems","abstract":"Finite-horizon probabilistic multiagent concurrent game systems, also known as finite multiplayer stochastic games, are a well-studied model in computer science due to their ability to represent a wide range of real-world scenarios involving strategic interactions among agents over a finite amount of iterations (given by the finite-horizon). The analysis of these games typically focuses on evaluating (verifying) and computing (synthesizing/realizing) which strategy profiles (functions that represent the behavior of each agent) qualify as equilibria. The two most prominent equilibrium concepts are the Nash equilibrium and the subgame perfect equilibrium, with the latter considered a conceptual refinement of the former. However, computing these equilibria from scratch is often computationally infeasible. Therefore, recent attention has shifted to the verification problem, where a given strategy profile must be evaluated to determine whether it satisfies equilibrium conditions. In this paper, we demonstrate that the verification problem for subgame perfect equilibria lies in PSPACE, while for Nash equilibria, it is EXPTIME-complete. This is a highly counterintuitive result since subgame perfect equilibria are often seen as a strict strengthening of Nash equilibria and are intuitively seen as more complicated.","authors":["Senthil Rajasekaran","Moshe Y. Vardi"],"url":"https://arxiv.org/abs/2503.14690"}
{"created":"2025-04-11","title":"Studying and Understanding the Effectiveness and Failures of Conversational LLM-Based Repair","abstract":"Automated program repair (APR) is designed to automate the process of bug-fixing. In recent years, thanks to the rapid development of large language models (LLMs), automated repair has achieved remarkable progress. Advanced APR techniques powered by conversational LLMs, most notably ChatGPT, have exhibited impressive repair abilities and gained increasing popularity due to the capabilities of the underlying LLMs in providing repair feedback and performing iterative patch improvement. Despite the superiority, conversational APR techniques still fail to repair a large number of bugs. For example, a state-of-the-art conversational technique ChatRepair does not correctly repair over half of the single-function bugs in the Defects4J dataset. To understand the effectiveness and failures of conversational LLM-based repair and provide possible directions for improvement, we studied the exemplary ChatRepair with a focus on comparing the effectiveness of its cloze-style and full function repair strategies, assessing its key iterative component for patch improvement, and analyzing the repair failures. Our study has led to a series of findings, which we believe provide key implications for future research.","authors":["Aolin Chen","Haojun Wu","Qi Xin","Steven P. Reiss","Jifeng Xuan"],"url":"https://arxiv.org/abs/2503.15050"}
{"created":"2025-04-11","title":"A Case Study of Scalable Content Annotation Using Multi-LLM Consensus and Human Review","abstract":"Content annotation at scale remains challenging, requiring substantial human expertise and effort. This paper presents a case study in code documentation analysis, where we explore the balance between automation efficiency and annotation accuracy. We present MCHR (Multi-LLM Consensus with Human Review), a novel semi-automated framework that enhances annotation scalability through the systematic integration of multiple LLMs and targeted human review. Our framework introduces a structured consensus-building mechanism among LLMs and an adaptive review protocol that strategically engages human expertise. Through our case study, we demonstrate that MCHR reduces annotation time by 32% to 100% compared to manual annotation while maintaining high accuracy (85.5% to 98%) across different difficulty levels, from basic binary classification to challenging open-set scenarios.","authors":["Mingyue Yuan","Jieshan Chen","Zhenchang Xing","Gelareh Mohammadi","Aaron Quigley"],"url":"https://arxiv.org/abs/2503.17620"}
{"created":"2025-04-11","title":"Literature Review: Cyber Security Monitoring in Maritime","abstract":"In recent years, many cyber incidents have occurred in the maritime sector, targeting the information technology (IT) and operational technology (OT) infrastructure. Although several literature review papers have been published in the maritime field, none of the previous studies has focused on cyber security monitoring, which aims at timely detection of cyber attacks with automated methods. The current article addresses this research gap and surveys the methods, algorithms, tools and architectures used for cyber security monitoring in the maritime sector. For the survey, a systematic literature review of cyber security monitoring studies is conducted. The first contribution of this article is the bibliometric analysis of related literature and the identification of the main research themes in previous works. For that purpose, our article presents a taxonomy for existing studies which highlights the main properties of maritime cyber security monitoring research. The second contribution of this article is an in-depth analysis of previous works and the identification of research gaps and limitations in existing literature. Based on our findings, we outline future research directions for cyber security monitoring in the maritime field.","authors":["Risto Vaarandi","Leonidas Tsiopoulos","Gabor Visky","Muaan Ur Rehman","Hayretdin Bahsi"],"url":"https://arxiv.org/abs/2503.18173"}
{"created":"2025-04-11","title":"MARS: Memory-Enhanced Agents with Reflective Self-improvement","abstract":"Large language models (LLMs) have made significant advances in the field of natural language processing, but they still face challenges such as continuous decision-making, lack of long-term memory, and limited context windows in dynamic environments. To address these issues, this paper proposes an innovative framework Memory-Enhanced Agents with Reflective Self-improvement. The MARS framework comprises three agents: the User, the Assistant, and the Checker. By integrating iterative feedback, reflective mechanisms, and a memory optimization mechanism based on the Ebbinghaus forgetting curve, it significantly enhances the agents capabilities in handling multi-tasking and long-span information.","authors":["Xuechen Liang","Meiling Tao","Yinghui Xia","Jianhui Wang","Kun Li","Yijin Wang","Jingsong Yang","Tianyu Shi","Yuantao Wang","Miao Zhang","Xueqian Wang"],"url":"https://arxiv.org/abs/2503.19271"}
{"created":"2025-04-11","title":"Countering threats to national security posed by AI systems through an incident regime","abstract":"Recent progress in AI capabilities has heightened concerns that AI systems could pose a threat to national security, for example, by making it easier for malicious actors to perform cyberattacks on critical national infrastructure, or through loss of control of autonomous AI systems. In parallel, federal legislators in the US have proposed nascent 'AI incident regimes' to identify and counter similar threats. In this paper, we consolidate these two trends and present a proposal for a legally mandated post-deployment AI incident regime that aims to counter potential national security threats from AI systems. We start the paper by introducing the concept of 'security-critical' to describe doctors that pose extreme risks to national security, before arguing that 'security-critical' describes civilian nuclear power, aviation, life science dual-use research of concern, and frontier AI development. We then present in detail our AI incident regime proposal, justifying each component of the proposal by demonstrating its similarity to US domestic incident regimes in other 'security-critical' sectors. Finally, we sketch a hypothetical scenario where our proposed AI incident regime deals with an AI cyber incident. Our proposed AI incident regime is split into three phases. The first phase revolves around a novel operationalization of what counts as an 'AI incident' and we suggest that AI providers must create a 'national security case' before deploying a frontier AI system. The second and third phases spell out that AI providers should notify a government agency about incidents, and that the government agency should be involved in amending AI providers' security and safety procedures, in order to counter future threats to national security.","authors":["Alejandro Ortega"],"url":"https://arxiv.org/abs/2503.19887"}
{"created":"2025-04-11","title":"Outlier dimensions favor frequent tokens in language models","abstract":"We study last-layer outlier dimensions, i.e. dimensions that display extreme activations for the majority of inputs. We show that outlier dimensions arise in many different modern language models, and trace their function back to the heuristic of constantly predicting frequent words. We further show how a model can block this heuristic when it is not contextually appropriate, by assigning a counterbalancing weight mass to the remaining dimensions, and we investigate which model parameters boost outlier dimensions and when they arise during training. We conclude that outlier dimensions are a specialized mechanism discovered by many distinct models to implement a useful token prediction heuristic.","authors":["Iuri Macocco","Nora Graichen","Gemma Boleda","Marco Baroni"],"url":"https://arxiv.org/abs/2503.21718"}
{"created":"2025-04-11","title":"Multispectral Demosaicing via Dual Cameras","abstract":"Multispectral (MS) images capture detailed scene information across a wide range of spectral bands, making them invaluable for applications requiring rich spectral data. Integrating MS imaging into multi camera devices, such as smartphones, has the potential to enhance both spectral applications and RGB image quality. A critical step in processing MS data is demosaicing, which reconstructs color information from the mosaic MS images captured by the camera. This paper proposes a method for MS image demosaicing specifically designed for dual-camera setups where both RGB and MS cameras capture the same scene. Our approach leverages co-captured RGB images, which typically have higher spatial fidelity, to guide the demosaicing of lower-fidelity MS images. We introduce the Dual-camera RGB-MS Dataset - a large collection of paired RGB and MS mosaiced images with ground-truth demosaiced outputs - that enables training and evaluation of our method. Experimental results demonstrate that our method achieves state-of-the-art accuracy compared to existing techniques.","authors":["SaiKiran Tedla","Junyong Lee","Beixuan Yang","Mahmoud Afifi","Michael S. Brown"],"url":"https://arxiv.org/abs/2503.22026"}
{"created":"2025-04-11","title":"Reflection on Code Contributor Demographics and Collaboration Patterns in the Rust Community","abstract":"Open-source software communities thrive on global collaboration and contributions from diverse participants. This study explores the Rust programming language ecosystem to understand its contributors' demographic composition and interaction patterns. Our objective is to investigate the phenomenon of participation inequality in key Rust projects and the presence of diversity among them. We studied GitHub pull request data from the year leading up to the release of the latest completed Rust community annual survey in 2023. Specifically, we extracted information from three leading repositories: Rust, Rust Analyzer, and Cargo, and used social network graphs to visualize the interactions and identify central contributors and sub-communities. Social network analysis has shown concerning disparities in gender and geographic representation among contributors who play pivotal roles in collaboration networks and the presence of varying diversity levels in the sub-communities formed. These results suggest that while the Rust community is globally active, the contributor base does not fully reflect the diversity of the wider user community. We conclude that there is a need for more inclusive practices to encourage broader participation and ensure that the contributor base aligns more closely with the diverse global community that utilizes Rust.","authors":["Rohit Dandamudi","Ifeoma Adaji","Gema Rodr\\'iguez-P\\'erez"],"url":"https://arxiv.org/abs/2503.22066"}
{"created":"2025-04-11","title":"Meta-LoRA: Meta-Learning LoRA Components for Domain-Aware ID Personalization","abstract":"Recent advancements in text-to-image generative models, particularly latent diffusion models (LDMs), have demonstrated remarkable capabilities in synthesizing high-quality images from textual prompts. However, achieving identity personalization-ensuring that a model consistently generates subject-specific outputs from limited reference images-remains a fundamental challenge. To address this, we introduce Meta-Low-Rank Adaptation (Meta-LoRA), a novel framework that leverages meta-learning to encode domain-specific priors into LoRA-based identity personalization. Our method introduces a structured three-layer LoRA architecture that separates identity-agnostic knowledge from identity-specific adaptation. In the first stage, the LoRA Meta-Down layers are meta-trained across multiple subjects, learning a shared manifold that captures general identity-related features. In the second stage, only the LoRA-Mid and LoRA-Up layers are optimized to specialize on a given subject, significantly reducing adaptation time while improving identity fidelity. To evaluate our approach, we introduce Meta-PHD, a new benchmark dataset for identity personalization, and compare Meta-LoRA against state-of-the-art methods. Our results demonstrate that Meta-LoRA achieves superior identity retention, computational efficiency, and adaptability across diverse identity conditions. Our code, model weights, and dataset are released on barisbatuhan.github.io/Meta-LoRA.","authors":["Bar{\\i}\\c{s} Batuhan Topal","Umut \\\"Ozyurt","Zafer Do\\u{g}an Budak","Ramazan Gokberk Cinbis"],"url":"https://arxiv.org/abs/2503.22352"}
{"created":"2025-04-11","title":"A Centralized Planning and Distributed Execution Method for Shape Filling with Homogeneous Mobile Robots","abstract":"Nature has inspired humans in different ways. The formation behavior of animals can perform tasks that exceed individual capability. For example, army ants could transverse gaps by forming bridges, and fishes could group up to protect themselves from predators. The pattern formation task is essential in a multiagent robotic system because it usually serves as the initial configuration of downstream tasks, such as collective manipulation and adaptation to various environments. The formation of complex shapes, especially hollow shapes, remains an open question. Traditional approaches either require global coordinates for each robot or are prone to failure when attempting to close the hole due to accumulated localization errors. Inspired by the ribbon idea introduced in the additive self-assembly algorithm by the Kilobot team, we develop a two-stage algorithm that does not require global coordinates information and effectively forms shapes with holes. In this paper, we investigate the partitioning of the shape using ribbons in a hexagonal lattice setting and propose the add-subtract algorithm based on the movement sequence induced by the ribbon structure. This advancement opens the door to tasks requiring complex pattern formations, such as the assembly of nanobots for medical applications involving intricate structures and the deployment of robots along the boundaries of areas of interest. We also provide simulation results on complex shapes, an analysis of the robustness as well as a proof of correctness of the proposed algorithm.","authors":["Shuqing Liu","Rong Su","Karl H. Johansson"],"url":"https://arxiv.org/abs/2503.22522"}
{"created":"2025-04-11","title":"Orchestrate Multimodal Data with Batch Post-Balancing to Accelerate Multimodal Large Language Model Training","abstract":"Multimodal large language models (MLLMs), such as GPT-4o, are garnering significant attention. During the exploration of MLLM training, we identified Modality Composition Incoherence, a phenomenon that the proportion of a certain modality varies dramatically across different examples. It exacerbates the challenges of addressing mini-batch imbalances, which lead to uneven GPU utilization between Data Parallel (DP) instances and severely degrades the efficiency and scalability of MLLM training, ultimately affecting training speed and hindering further research on MLLMs.","authors":["Yijie Zheng","Bangjun Xiao","Lei Shi","Xiaoyang Li","Faming Wu","Tianyu Li","Xuefeng Xiao","Yang Zhang","Yuxuan Wang","Shouda Liu"],"url":"https://arxiv.org/abs/2503.23830"}
{"created":"2025-04-11","title":"Synthetic News Generation for Fake News Classification","abstract":"This study explores the generation and evaluation of synthetic fake news through fact based manipulations using large language models (LLMs). We introduce a novel methodology that extracts key facts from real articles, modifies them, and regenerates content to simulate fake news while maintaining coherence. To assess the quality of the generated content, we propose a set of evaluation metrics coherence, dissimilarity, and correctness. The research also investigates the application of synthetic data in fake news classification, comparing traditional machine learning models with transformer based models such as BERT. Our experiments demonstrate that transformer models, especially BERT, effectively leverage synthetic data for fake news detection, showing improvements with smaller proportions of synthetic data. Additionally, we find that fact verification features, which focus on identifying factual inconsistencies, provide the most promising results in distinguishing synthetic fake news. The study highlights the potential of synthetic data to enhance fake news detection systems, offering valuable insights for future research and suggesting that targeted improvements in synthetic data generation can further strengthen detection models.","authors":["Abdul Sittar","Luka Golob","Mateja Smiljanic"],"url":"https://arxiv.org/abs/2503.24206"}
{"created":"2025-04-11","title":"ORAL: Prompting Your Large-Scale LoRAs via Conditional Recurrent Diffusion","abstract":"Parameter generation has emerged as a novel paradigm for neural network development, offering an alternative to traditional neural network training by synthesizing high-quality model weights directly. In the context of Low-Rank Adaptation (LoRA) for evolving ($\\textit{i.e.}$, constantly updated) large language models (LLMs), this approach promises efficient adaptation without costly retraining. However, existing methods face critical limitations in simultaneously achieving scalability and controllability. In this paper, we introduce $\\texttt{ORAL}$, a novel $\\textbf{conditional recurrent diffusion}$ framework that addresses these challenges. $\\texttt{ORAL}$ incorporates a novel conditioning mechanism that integrates model architecture and textual task specifications, enabling the generation of task-specific LoRA parameters that can seamlessly transfer across evolving foundation models. Our approach successfully scales to billions-of-parameter LLMs and maintains controllability. Through extensive experiments across seven language tasks, four vision tasks, and three multimodal tasks using five pre-trained LLMs, we demonstrate that $\\texttt{ORAL}$ generates high-quality LoRA parameters that achieve comparable or superior performance to vanilla trained counterparts.","authors":["Rana Muhammad Shahroz Khan","Dongwen Tang","Pingzhi Li","Kai Wang","Tianlong Chen"],"url":"https://arxiv.org/abs/2503.24354"}
{"created":"2025-04-11","title":"NeuRadar: Neural Radiance Fields for Automotive Radar Point Clouds","abstract":"Radar is an important sensor for autonomous driving (AD) systems due to its robustness to adverse weather and different lighting conditions. Novel view synthesis using neural radiance fields (NeRFs) has recently received considerable attention in AD due to its potential to enable efficient testing and validation but remains unexplored for radar point clouds. In this paper, we present NeuRadar, a NeRF-based model that jointly generates radar point clouds, camera images, and lidar point clouds. We explore set-based object detection methods such as DETR, and propose an encoder-based solution grounded in the NeRF geometry for improved generalizability. We propose both a deterministic and a probabilistic point cloud representation to accurately model the radar behavior, with the latter being able to capture radar's stochastic behavior. We achieve realistic reconstruction results for two automotive datasets, establishing a baseline for NeRF-based radar point cloud simulation models. In addition, we release radar data for ZOD's Sequences and Drives to enable further research in this field. To encourage further development of radar NeRFs, we release the source code for NeuRadar.","authors":["Mahan Rafidashti","Ji Lan","Maryam Fatemi","Junsheng Fu","Lars Hammarstrand","Lennart Svensson"],"url":"https://arxiv.org/abs/2504.00859"}
{"created":"2025-04-11","title":"ShieldGemma 2: Robust and Tractable Image Content Moderation","abstract":"We introduce ShieldGemma 2, a 4B parameter image content moderation model built on Gemma 3. This model provides robust safety risk predictions across the following key harm categories: Sexually Explicit, Violence \\& Gore, and Dangerous Content for synthetic images (e.g. output of any image generation model) and natural images (e.g. any image input to a Vision-Language Model). We evaluated on both internal and external benchmarks to demonstrate state-of-the-art performance compared to LlavaGuard \\citep{helff2024llavaguard}, GPT-4o mini \\citep{hurst2024gpt}, and the base Gemma 3 model \\citep{gemma_2025} based on our policies. Additionally, we present a novel adversarial data generation pipeline which enables a controlled, diverse, and robust image generation. ShieldGemma 2 provides an open image moderation tool to advance multimodal safety and responsible AI development.","authors":["Wenjun Zeng","Dana Kurniawan","Ryan Mullins","Yuchi Liu","Tamoghna Saha","Dirichi Ike-Njoku","Jindong Gu","Yiwen Song","Cai Xu","Jingjing Zhou","Aparna Joshi","Shravan Dheep","Mani Malek","Hamid Palangi","Joon Baek","Rick Pereira","Karthik Narasimhan"],"url":"https://arxiv.org/abs/2504.01081"}
{"created":"2025-04-11","title":"Mesh Mamba: A Unified State Space Model for Saliency Prediction in Non-Textured and Textured Meshes","abstract":"Mesh saliency enhances the adaptability of 3D vision by identifying and emphasizing regions that naturally attract visual attention. To investigate the interaction between geometric structure and texture in shaping visual attention, we establish a comprehensive mesh saliency dataset, which is the first to systematically capture the differences in saliency distribution under both textured and non-textured visual conditions. Furthermore, we introduce mesh Mamba, a unified saliency prediction model based on a state space model (SSM), designed to adapt across various mesh types. Mesh Mamba effectively analyzes the geometric structure of the mesh while seamlessly incorporating texture features into the topological framework, ensuring coherence throughout appearance-enhanced modeling. More importantly, by subgraph embedding and a bidirectional SSM, the model enables global context modeling for both local geometry and texture, preserving the topological structure and improving the understanding of visual details and structural complexity. Through extensive theoretical and empirical validation, our model not only improves performance across various mesh types but also demonstrates high scalability and versatility, particularly through cross validations of various visual features.","authors":["Kaiwei Zhang","Dandan Zhu","Xiongkuo Min","Guangtao Zhai"],"url":"https://arxiv.org/abs/2504.01466"}
{"created":"2025-04-11","title":"FIORD: A Fisheye Indoor-Outdoor Dataset with LIDAR Ground Truth for 3D Scene Reconstruction and Benchmarking","abstract":"The development of large-scale 3D scene reconstruction and novel view synthesis methods mostly rely on datasets comprising perspective images with narrow fields of view (FoV). While effective for small-scale scenes, these datasets require large image sets and extensive structure-from-motion (SfM) processing, limiting scalability. To address this, we introduce a fisheye image dataset tailored for scene reconstruction tasks. Using dual 200-degree fisheye lenses, our dataset provides full 360-degree coverage of 5 indoor and 5 outdoor scenes. Each scene has sparse SfM point clouds and precise LIDAR-derived dense point clouds that can be used as geometric ground-truth, enabling robust benchmarking under challenging conditions such as occlusions and reflections. While the baseline experiments focus on vanilla Gaussian Splatting and NeRF based Nerfacto methods, the dataset supports diverse approaches for scene reconstruction, novel view synthesis, and image-based rendering.","authors":["Ulas Gunes","Matias Turkulainen","Xuqian Ren","Arno Solin","Juho Kannala","Esa Rahtu"],"url":"https://arxiv.org/abs/2504.01732"}
{"created":"2025-04-11","title":"End-to-End Driving with Online Trajectory Evaluation via BEV World Model","abstract":"End-to-end autonomous driving has achieved remarkable progress by integrating perception, prediction, and planning into a fully differentiable framework. Yet, to fully realize its potential, an effective online trajectory evaluation is indispensable to ensure safety. By forecasting the future outcomes of a given trajectory, trajectory evaluation becomes much more effective. This goal can be achieved by employing a world model to capture environmental dynamics and predict future states. Therefore, we propose an end-to-end driving framework WoTE, which leverages a BEV World model to predict future BEV states for Trajectory Evaluation. The proposed BEV world model is latency-efficient compared to image-level world models and can be seamlessly supervised using off-the-shelf BEV-space traffic simulators. We validate our framework on both the NAVSIM benchmark and the closed-loop Bench2Drive benchmark based on the CARLA simulator, achieving state-of-the-art performance. Code is released at https://github.com/liyingyanUCAS/WoTE.","authors":["Yingyan Li","Yuqi Wang","Yang Liu","Jiawei He","Lue Fan","Zhaoxiang Zhang"],"url":"https://arxiv.org/abs/2504.01941"}
{"created":"2025-04-11","title":"F5R-TTS: Improving Flow-Matching based Text-to-Speech with Group Relative Policy Optimization","abstract":"We present F5R-TTS, a novel text-to-speech (TTS) system that integrates Gradient Reward Policy Optimization (GRPO) into a flow-matching based architecture. By reformulating the deterministic outputs of flow-matching TTS into probabilistic Gaussian distributions, our approach enables seamless integration of reinforcement learning algorithms. During pretraining, we train a probabilistically reformulated flow-matching based model which is derived from F5-TTS with an open-source dataset. In the subsequent reinforcement learning (RL) phase, we employ a GRPO-driven enhancement stage that leverages dual reward metrics: word error rate (WER) computed via automatic speech recognition and speaker similarity (SIM) assessed by verification models. Experimental results on zero-shot voice cloning demonstrate that F5R-TTS achieves significant improvements in both speech intelligibility (a 29.5% relative reduction in WER) and speaker similarity (a 4.6% relative increase in SIM score) compared to conventional flow-matching based TTS systems. Audio samples are available at https://frontierlabs.github.io/F5R.","authors":["Xiaohui Sun","Ruitong Xiao","Jianye Mo","Bowen Wu","Qun Yu","Baoxun Wang"],"url":"https://arxiv.org/abs/2504.02407"}
{"created":"2025-04-11","title":"Bridging the Theoretical Gap in Randomized Smoothing","abstract":"Randomized smoothing has become a leading approach for certifying adversarial robustness in machine learning models. However, a persistent gap remains between theoretical certified robustness and empirical robustness accuracy. This paper introduces a new framework that bridges this gap by leveraging Lipschitz continuity for certification and proposing a novel, less conservative method for computing confidence intervals in randomized smoothing. Our approach tightens the bounds of certified robustness, offering a more accurate reflection of model robustness in practice. Through rigorous experimentation we show that our method improves the robust accuracy, compressing the gap between empirical findings and previous theoretical results. We argue that investigating local Lipschitz constants and designing ad-hoc confidence intervals can further enhance the performance of randomized smoothing. These results pave the way for a deeper understanding of the relationship between Lipschitz continuity and certified robustness.","authors":["Blaise Delattre","Paul Caillon","Quentin Barth\\'elemy","Erwan Fagnou","Alexandre Allauzen"],"url":"https://arxiv.org/abs/2504.02412"}
{"created":"2025-04-11","title":"HGFormer: Topology-Aware Vision Transformer with HyperGraph Learning","abstract":"The computer vision community has witnessed an extensive exploration of vision transformers in the past two years. Drawing inspiration from traditional schemes, numerous works focus on introducing vision-specific inductive biases. However, the implicit modeling of permutation invariance and fully-connected interaction with individual tokens disrupts the regional context and spatial topology, further hindering higher-order modeling. This deviates from the principle of perceptual organization that emphasizes the local groups and overall topology of visual elements. Thus, we introduce the concept of hypergraph for perceptual exploration. Specifically, we propose a topology-aware vision transformer called HyperGraph Transformer (HGFormer). Firstly, we present a Center Sampling K-Nearest Neighbors (CS-KNN) algorithm for semantic guidance during hypergraph construction. Secondly, we present a topology-aware HyperGraph Attention (HGA) mechanism that integrates hypergraph topology as perceptual indications to guide the aggregation of global and unbiased information during hypergraph messaging. Using HGFormer as visual backbone, we develop an effective and unitive representation, achieving distinct and detailed scene depictions. Empirical experiments show that the proposed HGFormer achieves competitive performance compared to the recent SoTA counterparts on various visual benchmarks. Extensive ablation and visualization studies provide comprehensive explanations of our ideas and contributions.","authors":["Hao Wang","Shuo Zhang","Biao Leng"],"url":"https://arxiv.org/abs/2504.02440"}
{"created":"2025-04-11","title":"Randomized Pairwise Learning with Adaptive Sampling: A PAC-Bayes Analysis","abstract":"We study stochastic optimization with data-adaptive sampling schemes to train pairwise learning models. Pairwise learning is ubiquitous, and it covers several popular learning tasks such as ranking, metric learning and AUC maximization. A notable difference of pairwise learning from pointwise learning is the statistical dependencies among input pairs, for which existing analyses have not been able to handle in the general setting considered in this paper. To this end, we extend recent results that blend together two algorithm-dependent frameworks of analysis -- algorithmic stability and PAC-Bayes -- which allow us to deal with any data-adaptive sampling scheme in the optimizer. We instantiate this framework to analyze (1) pairwise stochastic gradient descent, which is a default workhorse in many machine learning problems, and (2) pairwise stochastic gradient descent ascent, which is a method used in adversarial training. All of these algorithms make use of a stochastic sampling from a discrete distribution (sample indices) before each update. Non-uniform sampling of these indices has been already suggested in the recent literature, to which our work provides generalization guarantees in both smooth and non-smooth convex problems.","authors":["Sijia Zhou","Yunwen Lei","Ata Kab\\'an"],"url":"https://arxiv.org/abs/2504.02957"}
{"created":"2025-04-11","title":"Sliced Wasserstein Discrepancy in Disentangling Representation and Adaptation Networks for Unsupervised Domain Adaptation","abstract":"This paper introduces DRANet-SWD as a novel complete pipeline for disentangling content and style representations of images for unsupervised domain adaptation (UDA). The approach builds upon DRANet by incorporating the sliced Wasserstein discrepancy (SWD) as a style loss instead of the traditional Gram matrix loss. The potential advantages of SWD over the Gram matrix loss for capturing style variations in domain adaptation are investigated. Experiments using digit classification datasets and driving scenario segmentation validate the method, demonstrating that DRANet-SWD enhances performance. Results indicate that SWD provides a more robust statistical comparison of feature distributions, leading to better style adaptation. These findings highlight the effectiveness of SWD in refining feature alignment and improving domain adaptation tasks across these benchmarks. Our code can be found here.","authors":["Joel Sol","Shadi Alijani","Homayoun Najjaran"],"url":"https://arxiv.org/abs/2504.03043"}
{"created":"2025-04-11","title":"Compressing 3D Gaussian Splatting by Noise-Substituted Vector Quantization","abstract":"3D Gaussian Splatting (3DGS) has demonstrated remarkable effectiveness in 3D reconstruction, achieving high-quality results with real-time radiance field rendering. However, a key challenge is the substantial storage cost: reconstructing a single scene typically requires millions of Gaussian splats, each represented by 59 floating-point parameters, resulting in approximately 1 GB of memory. To address this challenge, we propose a compression method by building separate attribute codebooks and storing only discrete code indices. Specifically, we employ noise-substituted vector quantization technique to jointly train the codebooks and model features, ensuring consistency between gradient descent optimization and parameter discretization. Our method reduces the memory consumption efficiently (around $45\\times$) while maintaining competitive reconstruction quality on standard 3D benchmark scenes. Experiments on different codebook sizes show the trade-off between compression ratio and image quality. Furthermore, the trained compressed model remains fully compatible with popular 3DGS viewers and enables faster rendering speed, making it well-suited for practical applications.","authors":["Haishan Wang","Mohammad Hassan Vali","Arno Solin"],"url":"https://arxiv.org/abs/2504.03059"}
{"created":"2025-04-11","title":"Joint Retrieval of Cloud properties using Attention-based Deep Learning Models","abstract":"Accurate cloud property retrieval is vital for understanding cloud behavior and its impact on climate, including applications in weather forecasting, climate modeling, and estimating Earth's radiation balance. The Independent Pixel Approximation (IPA), a widely used physics-based approach, simplifies radiative transfer calculations by assuming each pixel is independent of its neighbors. While computationally efficient, IPA has significant limitations, such as inaccuracies from 3D radiative effects, errors at cloud edges, and ineffectiveness for overlapping or heterogeneous cloud fields. Recent AI/ML-based deep learning models have improved retrieval accuracy by leveraging spatial relationships across pixels. However, these models are often memory-intensive, retrieve only a single cloud property, or struggle with joint property retrievals. To overcome these challenges, we introduce CloudUNet with Attention Module (CAM), a compact UNet-based model that employs attention mechanisms to reduce errors in thick, overlapping cloud regions and a specialized loss function for joint retrieval of Cloud Optical Thickness (COT) and Cloud Effective Radius (CER). Experiments on a Large Eddy Simulation (LES) dataset show that our CAM model outperforms state-of-the-art deep learning methods, reducing mean absolute errors (MAE) by 34% for COT and 42% for CER, and achieving 76% and 86% lower MAE for COT and CER retrievals compared to the IPA method.","authors":["Zahid Hassan Tushar","Adeleke Ademakinwa","Jianwu Wang","Zhibo Zhang","Sanjay Purushotham"],"url":"https://arxiv.org/abs/2504.03133"}
{"created":"2025-04-11","title":"JailDAM: Jailbreak Detection with Adaptive Memory for Vision-Language Model","abstract":"Multimodal large language models (MLLMs) excel in vision-language tasks but also pose significant risks of generating harmful content, particularly through jailbreak attacks. Jailbreak attacks refer to intentional manipulations that bypass safety mechanisms in models, leading to the generation of inappropriate or unsafe content. Detecting such attacks is critical to ensuring the responsible deployment of MLLMs. Existing jailbreak detection methods face three primary challenges: (1) Many rely on model hidden states or gradients, limiting their applicability to white-box models, where the internal workings of the model are accessible; (2) They involve high computational overhead from uncertainty-based analysis, which limits real-time detection, and (3) They require fully labeled harmful datasets, which are often scarce in real-world settings. To address these issues, we introduce a test-time adaptive framework called JAILDAM. Our method leverages a memory-based approach guided by policy-driven unsafe knowledge representations, eliminating the need for explicit exposure to harmful data. By dynamically updating unsafe knowledge during test-time, our framework improves generalization to unseen jailbreak strategies while maintaining efficiency. Experiments on multiple VLM jailbreak benchmarks demonstrate that JAILDAM delivers state-of-the-art performance in harmful content detection, improving both accuracy and speed.","authors":["Yi Nian","Shenzhe Zhu","Yuehan Qin","Li Li","Ziyi Wang","Chaowei Xiao","Yue Zhao"],"url":"https://arxiv.org/abs/2504.03770"}
{"created":"2025-04-11","title":"Scalable Robust Bayesian Co-Clustering with Compositional ELBOs","abstract":"Co-clustering exploits the duality of instances and features to simultaneously uncover meaningful groups in both dimensions, often outperforming traditional clustering in high-dimensional or sparse data settings. Although recent deep learning approaches successfully integrate feature learning and cluster assignment, they remain susceptible to noise and can suffer from posterior collapse within standard autoencoders. In this paper, we present the first fully variational Co-clustering framework that directly learns row and column clusters in the latent space, leveraging a doubly reparameterized ELBO to improve gradient signal-to-noise separation. Our unsupervised model integrates a Variational Deep Embedding with a Gaussian Mixture Model (GMM) prior for both instances and features, providing a built-in clustering mechanism that naturally aligns latent modes with row and column clusters. Furthermore, our regularized end-to-end noise learning Compositional ELBO architecture jointly reconstructs the data while regularizing against noise through the KL divergence, thus gracefully handling corrupted or missing inputs in a single training pipeline. To counteract posterior collapse, we introduce a scale modification that increases the encoder's latent means only in the reconstruction pathway, preserving richer latent representations without inflating the KL term. Finally, a mutual information-based cross-loss ensures coherent co-clustering of rows and columns. Empirical results on diverse real-world datasets from multiple modalities, numerical, textual, and image-based, demonstrate that our method not only preserves the advantages of prior Co-clustering approaches but also exceeds them in accuracy and robustness, particularly in high-dimensional or noisy settings.","authors":["Ashwin Vinod","Chandrajit Bajaj"],"url":"https://arxiv.org/abs/2504.04079"}
{"created":"2025-04-11","title":"Corrected with the Latest Version: Make Robust Asynchronous Federated Learning Possible","abstract":"As an emerging paradigm of federated learning, asynchronous federated learning offers significant speed advantages over traditional synchronous federated learning. Unlike synchronous federated learning, which requires waiting for all clients to complete updates before aggregation, asynchronous federated learning aggregates the models that have arrived in realtime, greatly improving training speed. However, this mechanism also introduces the issue of client model version inconsistency. When the differences between models of different versions during aggregation become too large, it may lead to conflicts, thereby reducing the models accuracy. To address this issue, this paper proposes an asynchronous federated learning version correction algorithm based on knowledge distillation, named FedADT. FedADT applies knowledge distillation before aggregating gradients, using the latest global model to correct outdated information, thus effectively reducing the negative impact of outdated gradients on the training process. Additionally, FedADT introduces an adaptive weighting function that adjusts the knowledge distillation weight according to different stages of training, helps mitigate the misleading effects caused by the poorer performance of the global model in the early stages of training. This method significantly improves the overall performance of asynchronous federated learning without adding excessive computational overhead. We conducted experimental comparisons with several classical algorithms, and the results demonstrate that FedADT achieves significant improvements over other asynchronous methods and outperforms all methods in terms of convergence speed.","authors":["Chaoyi Lu","Yiding Sun","Pengbo Li","Zhichuan Yang"],"url":"https://arxiv.org/abs/2504.04081"}
{"created":"2025-04-11","title":"Digital Gene: Learning about the Physical World through Analytic Concepts","abstract":"Reviewing the progress in artificial intelligence over the past decade, various significant advances (e.g. object detection, image generation, large language models) have enabled AI systems to produce more semantically meaningful outputs and achieve widespread adoption in internet scenarios. Nevertheless, AI systems still struggle when it comes to understanding and interacting with the physical world. This reveals an important issue: relying solely on semantic-level concepts learned from internet data (e.g. texts, images) to understand the physical world is far from sufficient -- machine intelligence currently lacks an effective way to learn about the physical world. This research introduces the idea of analytic concept -- representing the concepts related to the physical world through programs of mathematical procedures, providing machine intelligence a portal to perceive, reason about, and interact with the physical world. Except for detailing the design philosophy and providing guidelines for the application of analytic concepts, this research also introduce about the infrastructure that has been built around analytic concepts. I aim for my research to contribute to addressing these questions: What is a proper abstraction of general concepts in the physical world for machine intelligence? How to systematically integrate structured priors with neural networks to constrain AI systems to comply with physical laws?","authors":["Jianhua Sun","Cewu Lu"],"url":"https://arxiv.org/abs/2504.04170"}
{"created":"2025-04-11","title":"User-Centered AI for Data Exploration: Rethinking GenAI's Role in Visualization","abstract":"Recent advances in GenAI have enabled automation in data visualization, allowing users to generate visual representations using natural language. However, existing systems primarily focus on automation, overlooking users' varying expertise levels and analytical needs. In this position paper, we advocate for a shift toward adaptive GenAI-driven visualization tools that tailor interactions, reasoning, and visualizations to individual users. We first review existing automation-focused approaches and highlight their limitations. We then introduce methods for assessing user expertise, as well as key open challenges and research questions that must be addressed to allow for an adaptive approach. Finally, we present our vision for a user-centered system that leverages GenAI not only for automation but as an intelligent collaborator in visual data exploration. Our perspective contributes to the broader discussion on designing GenAI-based systems that enhance human cognition by dynamically adapting to the user, ultimately advancing toward systems that promote augmented cognition.","authors":["Kathrin Schnizer","Sven Mayer"],"url":"https://arxiv.org/abs/2504.04253"}
{"created":"2025-04-11","title":"Beyond the Hype: Embeddings vs. Prompting for Multiclass Classification Tasks","abstract":"Are traditional classification approaches irrelevant in this era of AI hype? We show that there are multiclass classification problems where predictive models holistically outperform LLM prompt-based frameworks. Given text and images from home-service project descriptions provided by Thumbtack customers, we build embeddings-based softmax models that predict the professional category (e.g., handyman, bathroom remodeling) associated with each problem description. We then compare against prompts that ask state-of-the-art LLM models to solve the same problem. We find that the embeddings approach outperforms the best LLM prompts in terms of accuracy, calibration, latency, and financial cost. In particular, the embeddings approach has 49.5% higher accuracy than the prompting approach, and its superiority is consistent across text-only, image-only, and text-image problem descriptions. Furthermore, it yields well-calibrated probabilities, which we later use as confidence signals to provide contextualized user experience during deployment. On the contrary, prompting scores are overly uninformative. Finally, the embeddings approach is 14 and 81 times faster than prompting in processing images and text respectively, while under realistic deployment assumptions, it can be up to 10 times cheaper. Based on these results, we deployed a variation of the embeddings approach, and through A/B testing we observed performance consistent with our offline analysis. Our study shows that for multiclass classification problems that can leverage proprietary datasets, an embeddings-based approach may yield unequivocally better results. Hence, scientists, practitioners, engineers, and business leaders can use our study to go beyond the hype and consider appropriate predictive models for their classification use cases.","authors":["Marios Kokkodis","Richard Demsyn-Jones","Vijay Raghavan"],"url":"https://arxiv.org/abs/2504.04277"}
{"created":"2025-04-11","title":"Learning Flatness-Preserving Residuals for Pure-Feedback Systems","abstract":"We study residual dynamics learning for differentially flat systems, where a nominal model is augmented with a learned correction term from data. A key challenge is that generic residual parameterizations may destroy flatness, limiting the applicability of flatness-based planning and control methods. To address this, we propose a framework for learning flatness-preserving residual dynamics in systems whose nominal model admits a pure-feedback form. We show that residuals with a lower-triangular structure preserve both the flatness of the system and the original flat outputs. Moreover, we provide a constructive procedure to recover the flatness diffeomorphism of the augmented system from that of the nominal model. We then introduce a learning algorithm that fits such residuals from trajectory data using smooth function approximators. Our approach is validated in simulation on a 2D quadrotor subject to unmodeled aerodynamic effects. We demonstrate that the resulting learned flat model enables tracking performance comparable to nonlinear model predictive control ($5\\times$ lower tracking error than the nominal flat model) while also achieving over a $20\\times$ speedup in computation.","authors":["Fengjun Yang","Jake Welde","Nikolai Matni"],"url":"https://arxiv.org/abs/2504.04324"}
{"created":"2025-04-11","title":"Saliency-driven Dynamic Token Pruning for Large Language Models","abstract":"Despite the recent success of large language models (LLMs), LLMs are particularly challenging in long-sequence inference scenarios due to the quadratic computational complexity of the attention mechanism. Inspired by the interpretability theory of feature attribution in neural network models, we observe that not all tokens have the same contribution. Based on this observation, we propose a novel token pruning framework, namely Saliency-driven Dynamic Token Pruning (SDTP), to gradually and dynamically prune redundant tokens based on the input context. Specifically, a lightweight saliency-driven prediction module is designed to estimate the importance score of each token with its hidden state, which is added to different layers of the LLM to hierarchically prune redundant tokens. Furthermore, a ranking-based optimization strategy is proposed to minimize the ranking divergence of the saliency score and the predicted importance score. Extensive experiments have shown that our framework is generalizable to various models and datasets. By hierarchically pruning 65\\% of the input tokens, our method greatly reduces 33\\% $\\sim$ 47\\% FLOPs and achieves speedup up to 1.75$\\times$ during inference, while maintaining comparable performance. We further demonstrate that SDTP can be combined with KV cache compression method for further compression.","authors":["Yao Tao","Yehui Tang","Yun Wang","Mingjian Zhu","Hailin Hu","Yunhe Wang"],"url":"https://arxiv.org/abs/2504.04514"}
{"created":"2025-04-11","title":"Sequential-NIAH: A Needle-In-A-Haystack Benchmark for Extracting Sequential Needles from Long Contexts","abstract":"Evaluating the ability of large language models (LLMs) to handle extended contexts is critical, particularly for retrieving information relevant to specific queries embedded within lengthy inputs. We introduce Sequential-NIAH, a benchmark specifically designed to evaluate the capability of LLMs to extract sequential information items (known as needles) from long contexts. The benchmark comprises three types of needle generation pipelines: synthetic, real, and open-domain QA. It includes contexts ranging from 8K to 128K tokens in length, with a dataset of 14,000 samples (2,000 reserved for testing). To facilitate evaluation on this benchmark, we trained a synthetic data-driven evaluation model capable of evaluating answer correctness based on chronological or logical order, achieving an accuracy of 99.49% on synthetic test data. We conducted experiments on six well-known LLMs, revealing that even the best-performing model achieved a maximum accuracy of only 63.15%. Further analysis highlights the growing challenges posed by increasing context lengths and the number of needles, underscoring substantial room for improvement. Additionally, noise robustness experiments validate the reliability of the benchmark, making Sequential-NIAH an important reference for advancing research on long text extraction capabilities of LLMs.","authors":["Yifei Yu","Qian-Wen Zhang","Lingfeng Qiao","Di Yin","Fang Li","Jie Wang","Zengxi Chen","Suncong Zheng","Xiaolong Liang","Xing Sun"],"url":"https://arxiv.org/abs/2504.04713"}
{"created":"2025-04-11","title":"TabRep: a Simple and Effective Continuous Representation for Training Tabular Diffusion Models","abstract":"Diffusion models have been the predominant generative model for tabular data generation. However, they face the conundrum of modeling under a separate versus a unified data representation. The former encounters the challenge of jointly modeling all multi-modal distributions of tabular data in one model. While the latter alleviates this by learning a single representation for all features, it currently leverages sparse suboptimal encoding heuristics and necessitates additional computation costs. In this work, we address the latter by presenting TabRep, a tabular diffusion architecture trained with a unified continuous representation. To motivate the design of our representation, we provide geometric insights into how the data manifold affects diffusion models. The key attributes of our representation are composed of its density, flexibility to provide ample separability for nominal features, and ability to preserve intrinsic relationships. Ultimately, TabRep provides a simple yet effective approach for training tabular diffusion models under a continuous data manifold. Our results showcase that TabRep achieves superior performance across a broad suite of evaluations. It is the first to synthesize tabular data that exceeds the downstream quality of the original datasets while preserving privacy and remaining computationally efficient.","authors":["Jacob Si","Zijing Ou","Mike Qu","Zhengrui Xiang","Yingzhen Li"],"url":"https://arxiv.org/abs/2504.04798"}
{"created":"2025-04-11","title":"Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning","abstract":"Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design.","authors":["Anja Surina","Amin Mansouri","Lars Quaedvlieg","Amal Seddas","Maryna Viazovska","Emmanuel Abbe","Caglar Gulcehre"],"url":"https://arxiv.org/abs/2504.05108"}
{"created":"2025-04-11","title":"EffOWT: Transfer Visual Language Models to Open-World Tracking Efficiently and Effectively","abstract":"Open-World Tracking (OWT) aims to track every object of any category, which requires the model to have strong generalization capabilities. Trackers can improve their generalization ability by leveraging Visual Language Models (VLMs). However, challenges arise with the fine-tuning strategies when VLMs are transferred to OWT: full fine-tuning results in excessive parameter and memory costs, while the zero-shot strategy leads to sub-optimal performance. To solve the problem, EffOWT is proposed for efficiently transferring VLMs to OWT. Specifically, we build a small and independent learnable side network outside the VLM backbone. By freezing the backbone and only executing backpropagation on the side network, the model's efficiency requirements can be met. In addition, EffOWT enhances the side network by proposing a hybrid structure of Transformer and CNN to improve the model's performance in the OWT field. Finally, we implement sparse interactions on the MLP, thus reducing parameter updates and memory costs significantly. Thanks to the proposed methods, EffOWT achieves an absolute gain of 5.5% on the tracking metric OWTA for unknown categories, while only updating 1.3% of the parameters compared to full fine-tuning, with a 36.4% memory saving. Other metrics also demonstrate obvious improvement.","authors":["Bingyang Wang","Kaer Huang","Bin Li","Yiqiang Yan","Lihe Zhang","Huchuan Lu","You He"],"url":"https://arxiv.org/abs/2504.05141"}
{"created":"2025-04-11","title":"Covariant Gradient Descent","abstract":"We present a manifestly covariant formulation of the gradient descent method, ensuring consistency across arbitrary coordinate systems and general curved trainable spaces. The optimization dynamics is defined using a covariant force vector and a covariant metric tensor, both computed from the first and second statistical moments of the gradients. These moments are estimated through time-averaging with an exponential weight function, which preserves linear computational complexity. We show that commonly used optimization methods such as RMSProp, Adam and AdaBelief correspond to special limits of the covariant gradient descent (CGD) and demonstrate how these methods can be further generalized and improved.","authors":["Dmitry Guskov","Vitaly Vanchurin"],"url":"https://arxiv.org/abs/2504.05279"}
{"created":"2025-04-11","title":"Pretraining Language Models for Diachronic Linguistic Change Discovery","abstract":"Large language models (LLMs) have shown potential as tools for scientific discovery. This has engendered growing interest in their use in humanistic disciplines, such as historical linguistics and literary studies. These fields often construct arguments on the basis of delineations like genre, or more inflexibly, time period. Although efforts have been made to restrict inference to specific domains via fine-tuning or model editing, we posit that the only true guarantee is domain-restricted pretraining -- typically, a data- and compute-expensive proposition.","authors":["Elisabeth Fittschen","Sabrina Li","Tom Lippincott","Leshem Choshen","Craig Messner"],"url":"https://arxiv.org/abs/2504.05523"}
{"created":"2025-04-11","title":"Caption Anything in Video: Fine-grained Object-centric Captioning via Spatiotemporal Multimodal Prompting","abstract":"We present CAT-V (Caption AnyThing in Video), a training-free framework for fine-grained object-centric video captioning that enables detailed descriptions of user-selected objects through time. CAT-V integrates three key components: a Segmenter based on SAMURAI for precise object segmentation across frames, a Temporal Analyzer powered by TRACE-Uni for accurate event boundary detection and temporal analysis, and a Captioner using InternVL-2.5 for generating detailed object-centric descriptions. Through spatiotemporal visual prompts and chain-of-thought reasoning, our framework generates detailed, temporally-aware descriptions of objects' attributes, actions, statuses, interactions, and environmental contexts without requiring additional training data. CAT-V supports flexible user interactions through various visual prompts (points, bounding boxes, and irregular regions) and maintains temporal sensitivity by tracking object states and interactions across different time segments. Our approach addresses limitations of existing video captioning methods, which either produce overly abstract descriptions or lack object-level precision, enabling fine-grained, object-specific descriptions while maintaining temporal coherence and spatial accuracy. The GitHub repository for this project is available at https://github.com/yunlong10/CAT-V","authors":["Yunlong Tang","Jing Bi","Chao Huang","Susan Liang","Daiki Shimada","Hang Hua","Yunzhong Xiao","Yizhi Song","Pinxin Liu","Mingqian Feng","Junjia Guo","Zhuo Liu","Luchuan Song","Ali Vosoughi","Jinxi He","Liu He","Zeliang Zhang","Jiebo Luo","Chenliang Xu"],"url":"https://arxiv.org/abs/2504.05541"}
{"created":"2025-04-11","title":"Reasoning Towards Fairness: Mitigating Bias in Language Models through Reasoning-Guided Fine-Tuning","abstract":"Recent advances in large-scale generative language models have shown that reasoning capabilities can significantly improve model performance across a variety of tasks. However, the impact of reasoning on a model's ability to mitigate stereotypical responses remains largely underexplored. In this work, we investigate the crucial relationship between a model's reasoning ability and fairness, and ask whether improved reasoning capabilities can mitigate harmful stereotypical responses, especially those arising due to shallow or flawed reasoning. We conduct a comprehensive evaluation of multiple open-source LLMs, and find that larger models with stronger reasoning abilities exhibit substantially lower stereotypical bias on existing fairness benchmarks. Building on this insight, we introduce ReGiFT -- Reasoning Guided Fine-Tuning, a novel approach that extracts structured reasoning traces from advanced reasoning models and infuses them into models that lack such capabilities. We use only general-purpose reasoning and do not require any fairness-specific supervision for bias mitigation. Notably, we see that models fine-tuned using ReGiFT not only improve fairness relative to their non-reasoning counterparts but also outperform advanced reasoning models on fairness benchmarks. We also analyze how variations in the correctness of the reasoning traces and their length influence model fairness and their overall performance. Our findings highlight that enhancing reasoning capabilities is an effective, fairness-agnostic strategy for mitigating stereotypical bias caused by reasoning flaws.","authors":["Sanchit Kabra","Akshita Jha","Chandan K. Reddy"],"url":"https://arxiv.org/abs/2504.05632"}
{"created":"2025-04-11","title":"STAGE: Stemmed Accompaniment Generation through Prefix-Based Conditioning","abstract":"Recent advances in generative models have made it possible to create high-quality, coherent music, with some systems delivering production-level output. Yet, most existing models focus solely on generating music from scratch, limiting their usefulness for musicians who want to integrate such models into a human, iterative composition workflow. In this paper we introduce STAGE, our STemmed Accompaniment GEneration model, fine-tuned from the state-of-the-art MusicGen to generate single-stem instrumental accompaniments conditioned on a given mixture. Inspired by instruction-tuning methods for language models, we extend the transformer's embedding matrix with a context token, enabling the model to attend to a musical context through prefix-based conditioning. Compared to the baselines, STAGE yields accompaniments that exhibit stronger coherence with the input mixture, higher audio quality, and closer alignment with textual prompts. Moreover, by conditioning on a metronome-like track, our framework naturally supports tempo-constrained generation, achieving state-of-the-art alignment with the target rhythmic structure--all without requiring any additional tempo-specific module. As a result, STAGE offers a practical, versatile tool for interactive music creation that can be readily adopted by musicians in real-world workflows.","authors":["Giorgio Strano","Chiara Ballanti","Donato Crisostomi","Michele Mancusi","Luca Cosmo","Emanuele Rodol\\`a"],"url":"https://arxiv.org/abs/2504.05690"}
{"created":"2025-04-11","title":"Architecture independent generalization bounds for overparametrized deep ReLU networks","abstract":"We prove that overparametrized neural networks are able to generalize with a test error that is independent of the level of overparametrization, and independent of the Vapnik-Chervonenkis (VC) dimension. We prove explicit bounds that only depend on the metric geometry of the test and training sets, on the regularity properties of the activation function, and on the operator norms of the weights and norms of biases. For overparametrized deep ReLU networks with a training sample size bounded by the input space dimension, we explicitly construct zero loss minimizers without use of gradient descent, and prove that the generalization error is independent of the network architecture.","authors":["Thomas Chen","Chun-Kai Kevin Chien","Patricia Mu\\~noz Ewald","Andrew G. Moore"],"url":"https://arxiv.org/abs/2504.05695"}
{"created":"2025-04-11","title":"DDT: Decoupled Diffusion Transformer","abstract":"Diffusion transformers have demonstrated remarkable generation quality, albeit requiring longer training iterations and numerous inference steps. In each denoising step, diffusion transformers encode the noisy inputs to extract the lower-frequency semantic component and then decode the higher frequency with identical modules. This scheme creates an inherent optimization dilemma: encoding low-frequency semantics necessitates reducing high-frequency components, creating tension between semantic encoding and high-frequency decoding. To resolve this challenge, we propose a new \\textbf{\\color{ddt}D}ecoupled \\textbf{\\color{ddt}D}iffusion \\textbf{\\color{ddt}T}ransformer~(\\textbf{\\color{ddt}DDT}), with a decoupled design of a dedicated condition encoder for semantic extraction alongside a specialized velocity decoder. Our experiments reveal that a more substantial encoder yields performance improvements as model size increases. For ImageNet $256\\times256$, Our DDT-XL/2 achieves a new state-of-the-art performance of {1.31 FID}~(nearly $4\\times$ faster training convergence compared to previous diffusion transformers). For ImageNet $512\\times512$, Our DDT-XL/2 achieves a new state-of-the-art FID of 1.28. Additionally, as a beneficial by-product, our decoupled architecture enhances inference speed by enabling the sharing self-condition between adjacent denoising steps. To minimize performance degradation, we propose a novel statistical dynamic programming approach to identify optimal sharing strategies.","authors":["Shuai Wang","Zhi Tian","Weilin Huang","Limin Wang"],"url":"https://arxiv.org/abs/2504.05741"}
{"created":"2025-04-11","title":"Radiative Backpropagation with Non-Static Geometry","abstract":"Radiative backpropagation-based methods efficiently compute reverse-mode derivatives in physically-based differentiable rendering by simulating the propagation of differential radiance. A key assumption is that differential radiance is transported like normal radiance. We observe that this holds only when scene geometry is static and demonstrate that current implementations of radiative backpropagation produce biased gradients when scene parameters change geometry. In this work, we derive the differential transport equation without assuming static geometry. An immediate consequence is that the parameterization matters when the sampling process is not differentiated: only surface integrals allow a local formulation of the derivatives, i.e., one in which moving surfaces do not affect the entire path geometry. While considerable effort has been devoted to handling discontinuities resulting from moving geometry, we show that a biased interior derivative compromises even the simplest inverse rendering tasks, regardless of discontinuities. An implementation based on our derivation leads to systematic convergence to the reference solution in the same setting and provides unbiased interior derivatives for path-space differentiable rendering.","authors":["Markus Worchel","Ugo Finnendahl","Marc Alexa"],"url":"https://arxiv.org/abs/2504.05750"}
{"created":"2025-04-11","title":"Unraveling Human-AI Teaming: A Review and Outlook","abstract":"Artificial Intelligence (AI) is advancing at an unprecedented pace, with clear potential to enhance decision-making and productivity. Yet, the collaborative decision-making process between humans and AI remains underdeveloped, often falling short of its transformative possibilities. This paper explores the evolution of AI agents from passive tools to active collaborators in human-AI teams, emphasizing their ability to learn, adapt, and operate autonomously in complex environments. This paradigm shifts challenges traditional team dynamics, requiring new interaction protocols, delegation strategies, and responsibility distribution frameworks. Drawing on Team Situation Awareness (SA) theory, we identify two critical gaps in current human-AI teaming research: the difficulty of aligning AI agents with human values and objectives, and the underutilization of AI's capabilities as genuine team members. Addressing these gaps, we propose a structured research outlook centered on four key aspects of human-AI teaming: formulation, coordination, maintenance, and training. Our framework highlights the importance of shared mental models, trust-building, conflict resolution, and skill adaptation for effective teaming. Furthermore, we discuss the unique challenges posed by varying team compositions, goals, and complexities. This paper provides a foundational agenda for future research and practical design of sustainable, high-performing human-AI teams.","authors":["Bowen Lou","Tian Lu","T. S. Raghu","Yingjie Zhang"],"url":"https://arxiv.org/abs/2504.05755"}
{"created":"2025-04-11","title":"RETROcode: Leveraging a Code Database for Improved Natural Language to Code Generation","abstract":"As text and code resources have expanded, large-scale pre-trained models have shown promising capabilities in code generation tasks, typically employing supervised fine-tuning with problem statement-program pairs. However, increasing model size and data volume for performance gains also raises computational demands and risks of overfitting. Addressing these challenges, we present RETROcode, a novel adaptation of the RETRO architecture \\cite{RETRO} for sequence-to-sequence models, utilizing a large code database as an auxiliary scaling method. This approach, diverging from simply enlarging model and dataset sizes, allows RETROcode to leverage a vast code database for prediction, enhancing the model's efficiency by integrating extensive memory. Our findings indicate that RETROcode not only outperforms similar-sized traditional architectures on test sets but also approaches the effectiveness of the much larger Codex model, despite being trained from scratch on a substantially smaller dataset.","authors":["Nathana\\\"el Beau","Beno\\^it Crabb\\'e"],"url":"https://arxiv.org/abs/2504.05759"}
{"created":"2025-04-11","title":"Robust Fusion Controller: Degradation-aware Image Fusion with Fine-grained Language Instructions","abstract":"Current image fusion methods struggle to adapt to real-world environments encompassing diverse degradations with spatially varying characteristics. To address this challenge, we propose a robust fusion controller (RFC) capable of achieving degradation-aware image fusion through fine-grained language instructions, ensuring its reliable application in adverse environments. Specifically, RFC first parses language instructions to innovatively derive the functional condition and the spatial condition, where the former specifies the degradation type to remove, while the latter defines its spatial coverage. Then, a composite control priori is generated through a multi-condition coupling network, achieving a seamless transition from abstract language instructions to latent control variables. Subsequently, we design a hybrid attention-based fusion network to aggregate multi-modal information, in which the obtained composite control priori is deeply embedded to linearly modulate the intermediate fused features. To ensure the alignment between language instructions and control outcomes, we introduce a novel language-feature alignment loss, which constrains the consistency between feature-level gains and the composite control priori. Extensive experiments on publicly available datasets demonstrate that our RFC is robust against various composite degradations, particularly in highly challenging flare scenarios.","authors":["Hao Zhang","Yanping Zha","Qingwei Zhuang","Zhenfeng Shao","Jiayi Ma"],"url":"https://arxiv.org/abs/2504.05795"}
{"created":"2025-04-11","title":"A Control-Oriented Simplified Single Particle Model with Grouped Parameter and Sensitivity Analysis for Lithium-Ion Batteries","abstract":"Lithium-ion batteries are widely used in transportation, energy storage, and consumer electronics, driving the need for reliable battery management systems (BMS) for state estimation and control. The Single Particle Model (SPM) balances computational efficiency and accuracy but faces challenges in parameter estimation due to numerous parameters. Current SPM models using parabolic approximation introduce intermediate variables and hard to do parameter grouping. This study presents a control-oriented SPM reformulation that employs parameter grouping and parabolic approximation to simplify model parameters while using average and surface lithium-ion concentrations as model output. By parameter grouping, the original 17 parameters were reduced to 9 grouped parameters. The reformulated model achieves a reduced-order ordinary differential equation form while maintaining mathematical accuracy equivalent to the pre-grouped discretized SPM. Through Sobol sensitivity analysis under various current profiles, the grouped parameters were reduced from 9 to 6 highly sensitive parameters. Results demonstrate that estimating these 6 parameters achieves comparable practical accuracy to estimating all 9 parameters, with faster convergence. This control-oriented SPM enhances BMS applications by facilitating state estimation and control while reducing parameter estimation requirements.","authors":["Feng Guo","Luis D. Couto"],"url":"https://arxiv.org/abs/2504.05924"}
{"created":"2025-04-11","title":"Security Vulnerabilities in Ethereum Smart Contracts: A Systematic Analysis","abstract":"Smart contracts are a secure and trustworthy application that plays a vital role in decentralized applications in various fields such as insurance,the internet, and gaming. However, in recent years, smart contract security breaches have occurred frequently, and due to their financial properties, they have caused huge economic losses, such as the most famous security incident \"The DAO\" which caused a loss of over $60 million in Ethereum. This has drawn a lot of attention from all sides. Writing a secure smart contract is now a critical issue. This paper focuses on Ether smart contracts and explains the main components of Ether, smart contract architecture and mechanism. The environment used in this paper is the Ethernet environment, using remix online compilation platform and Solidity language, according to the four security events of American Chain, The DAO, Parity and KotET, the principles of integer overflow attack, reentrant attack, access control attack and denial of service attack are studied and analyzed accordingly, and the scenarios of these vulnerabilities are reproduced, and the measures to prevent them are given. Finally, preventive measures are given. In addition, the principles of short address attack, early transaction attack and privileged function exposure attack are also introduced in detail, and security measures are proposed. As vulnerabilities continue to emerge, their classification will also evolve. The analysis and research of the current vulnerabilities are also to lay a solid foundation for avoiding more vulnerabilities.","authors":["Jixuan Wu","Lei Xie","Xiaoqi Li"],"url":"https://arxiv.org/abs/2504.05968"}
{"created":"2025-04-11","title":"CAI: An Open, Bug Bounty-Ready Cybersecurity AI","abstract":"By 2028 most cybersecurity actions will be autonomous, with humans teleoperating. We present the first classification of autonomy levels in cybersecurity and introduce Cybersecurity AI (CAI), an open-source framework that democratizes advanced security testing through specialized AI agents. Through rigorous empirical evaluation, we demonstrate that CAI consistently outperforms state-of-the-art results in CTF benchmarks, solving challenges across diverse categories with significantly greater efficiency -up to 3,600x faster than humans in specific tasks and averaging 11x faster overall. CAI achieved first place among AI teams and secured a top-20 position worldwide in the \"AI vs Human\" CTF live Challenge, earning a monetary reward of $750. Based on our results, we argue against LLM-vendor claims about limited security capabilities. Beyond cybersecurity competitions, CAI demonstrates real-world effectiveness, reaching top-30 in Spain and top-500 worldwide on Hack The Box within a week, while dramatically reducing security testing costs by an average of 156x. Our framework transcends theoretical benchmarks by enabling non-professionals to discover significant security bugs (CVSS 4.3-7.5) at rates comparable to experts during bug bounty exercises. By combining modular agent design with seamless tool integration and human oversight (HITL), CAI addresses critical market gaps, offering organizations of all sizes access to AI-powered bug bounty security testing previously available only to well-resourced firms -thereby challenging the oligopolistic ecosystem currently dominated by major bug bounty platforms.","authors":["V\\'ictor Mayoral-Vilches","Luis Javier Navarrete-Lozano","Mar\\'ia Sanz-G\\'omez","Lidia Salas Espejo","Marti\\~no Crespo-\\'Alvarez","Francisco Oca-Gonzalez","Francesco Balassone","Alfonso Glera-Pic\\'on","Unai Ayucar-Carbajo","Jon Ander Ruiz-Alcalde","Stefan Rass","Martin Pinzger","Endika Gil-Uriarte"],"url":"https://arxiv.org/abs/2504.06017"}
{"created":"2025-04-11","title":"Confidence Regularized Masked Language Modeling using Text Length","abstract":"Masked language modeling is a widely used method for learning language representations, where the model predicts a randomly masked word in each input. However, this approach typically considers only a single correct answer during training, ignoring the variety of plausible alternatives that humans might choose. This issue becomes more pronounced when the input text is short, as the possible word distribution tends to have higher entropy, potentially causing the model to become overconfident in its predictions. To mitigate this, we propose a novel confidence regularizer that adaptively adjusts the regularization strength based on the input length. Experiments on the GLUE and SQuAD benchmarks show that our method improves both accuracy and expected calibration error","authors":["Seunghyun Ji","Soowon Lee"],"url":"https://arxiv.org/abs/2504.06037"}
{"created":"2025-04-11","title":"Leanabell-Prover: Posttraining Scaling in Formal Reasoning","abstract":"Recent advances in automated theorem proving (ATP) through LLMs have highlighted the potential of formal reasoning with Lean 4 codes. However, ATP has not yet be revolutionized by the recent posttraining scaling as demonstrated by Open AI O1/O3 and Deepseek R1. In this work, we investigate the entire posttraining of ATP, aiming to align it with breakthroughs in reasoning models in natural languages. To begin, we continual train current ATP models with a hybrid dataset, which consists of numerous statement-proof pairs, and additional data aimed at incorporating cognitive behaviors that emulate human reasoning and hypothesis refinement. Next, we explore reinforcement learning with the use of outcome reward returned by Lean 4 compiler. Through our designed continual training and reinforcement learning processes, we have successfully improved existing formal provers, including both DeepSeek-Prover-v1.5 and Goedel-Prover, achieving state-of-the-art performance in the field of whole-proof generation. For example, we achieve a 59.8% pass rate (pass@32) on MiniF2F. This is an on-going project and we will progressively update our findings, release our data and training details.","authors":["Jingyuan Zhang","Qi Wang","Xingguang Ji","Yahui Liu","Yang Yue","Fuzheng Zhang","Di Zhang","Guorui Zhou","Kun Gai"],"url":"https://arxiv.org/abs/2504.06122"}
{"created":"2025-04-11","title":"Robo-taxi Fleet Coordination at Scale via Reinforcement Learning","abstract":"Fleets of robo-taxis offering on-demand transportation services, commonly known as Autonomous Mobility-on-Demand (AMoD) systems, hold significant promise for societal benefits, such as reducing pollution, energy consumption, and urban congestion. However, orchestrating these systems at scale remains a critical challenge, with existing coordination algorithms often failing to exploit the systems' full potential. This work introduces a novel decision-making framework that unites mathematical modeling with data-driven techniques. In particular, we present the AMoD coordination problem through the lens of reinforcement learning and propose a graph network-based framework that exploits the main strengths of graph representation learning, reinforcement learning, and classical operations research tools. Extensive evaluations across diverse simulation fidelities and scenarios demonstrate the flexibility of our approach, achieving superior system performance, computational efficiency, and generalizability compared to prior methods. Finally, motivated by the need to democratize research efforts in this area, we release publicly available benchmarks, datasets, and simulators for network-level coordination alongside an open-source codebase designed to provide accessible simulation platforms and establish a standardized validation process for comparing methodologies. Code available at: https://github.com/StanfordASL/RL4AMOD","authors":["Luigi Tresca","Carolin Schmidt","James Harrison","Filipe Rodrigues","Gioele Zardini","Daniele Gammelli","Marco Pavone"],"url":"https://arxiv.org/abs/2504.06125"}
{"created":"2025-04-11","title":"Navigating the Rabbit Hole: Emergent Biases in LLM-Generated Attack Narratives Targeting Mental Health Groups","abstract":"Large Language Models (LLMs) have been shown to demonstrate imbalanced biases against certain groups. However, the study of unprovoked targeted attacks by LLMs towards at-risk populations remains underexplored. Our paper presents three novel contributions: (1) the explicit evaluation of LLM-generated attacks on highly vulnerable mental health groups; (2) a network-based framework to study the propagation of relative biases; and (3) an assessment of the relative degree of stigmatization that emerges from these attacks. Our analysis of a recently released large-scale bias audit dataset reveals that mental health entities occupy central positions within attack narrative networks, as revealed by a significantly higher mean centrality of closeness (p-value = 4.06e-10) and dense clustering (Gini coefficient = 0.7). Drawing from sociological foundations of stigmatization theory, our stigmatization analysis indicates increased labeling components for mental health disorder-related targets relative to initial targets in generation chains. Taken together, these insights shed light on the structural predilections of large language models to heighten harmful discourse and highlight the need for suitable approaches for mitigation.","authors":["Rijul Magu","Arka Dutta","Sean Kim","Ashiqur R. KhudaBukhsh","Munmun De Choudhury"],"url":"https://arxiv.org/abs/2504.06160"}
{"created":"2025-04-11","title":"Earth-Adapter: Bridge the Geospatial Domain Gaps with Mixture of Frequency Adaptation","abstract":"Parameter-Efficient Fine-Tuning (PEFT) is a technique that allows us to adapt powerful Foundation Models (FMs) to diverse downstream tasks while preserving and unleashing their inherent capabilities. However, we have observed that existing PEFT methods, which are often designed with natural imagery in mind, struggle when applied to Remote Sensing (RS) scenarios. This is primarily due to their inability to handle artifact influences, a problem particularly severe in RS image features. To tackle this challenge, we introduce Earth-Adapter, the first PEFT method specifically designed for RS artifacts conquering. Earth-Adapter introduces a novel Mixture of Frequency Adaptation process that combines a Mixture of Adapter (MoA) with Discrete Fourier Transformation (DFT). By utilizing DFT, Earth-Adapter can decompose features into different frequency components, precisely separating artifacts from original features. The MoA then dynamically assigns weights to each adapter expert, allowing for the combination of features across various frequency domains. These simple-yet-effective approaches enable Earth-Adapter to more efficiently overcome the disturbances caused by artifacts than previous PEFT methods, significantly enhancing the FMs' performance on RS scenarios. Experiments on Domain Adaptation (DA), and Domain Generalization (DG) semantic segmentation benchmarks showcase the Earth-Adapter's effectiveness. Compared with baseline Rein, Earth-Adapter significantly improves 9.0% mIoU in DA and 3.1% mIoU in DG benchmarks. Our code will be released at https://github.com/VisionXLab/Earth-Adapter.","authors":["Xiaoxing Hu","Ziyang Gong","Yupei Wang","Yuru Jia","Gen Luo","Xue Yang"],"url":"https://arxiv.org/abs/2504.06220"}
{"created":"2025-04-11","title":"Hogwild! Inference: Parallel LLM Generation via Concurrent Attention","abstract":"Large Language Models (LLMs) have demonstrated the ability to tackle increasingly complex tasks through advanced reasoning, long-form content generation, and tool use. Solving these tasks often involves long inference-time computations. In human problem solving, a common strategy to expedite work is collaboration: by dividing the problem into sub-tasks, exploring different strategies concurrently, etc. Recent research has shown that LLMs can also operate in parallel by implementing explicit cooperation frameworks, such as voting mechanisms or the explicit creation of independent sub-tasks that can be executed in parallel. However, each of these frameworks may not be suitable for all types of tasks, which can hinder their applicability. In this work, we propose a different design approach: we run LLM \"workers\" in parallel , allowing them to synchronize via a concurrently-updated attention cache and prompt these workers to decide how best to collaborate. Our approach allows the instances to come up with their own collaboration strategy for the problem at hand, all the while \"seeing\" each other's partial progress in the concurrent cache. We implement this approach via Hogwild! Inference: a parallel LLM inference engine where multiple instances of the same LLM run in parallel with the same attention cache, with \"instant\" access to each other's generated tokens. Hogwild! inference takes advantage of Rotary Position Embeddings (RoPE) to avoid recomputation while improving parallel hardware utilization. We find that modern reasoning-capable LLMs can perform inference with shared Key-Value cache out of the box, without additional fine-tuning.","authors":["Gleb Rodionov","Roman Garipov","Alina Shutova","George Yakushev","Vage Egiazarian","Anton Sinitsin","Denis Kuznedelev","Dan Alistarh"],"url":"https://arxiv.org/abs/2504.06261"}
{"created":"2025-04-11","title":"The membership problem for constant-sized quantum correlations is undecidable","abstract":"When two spatially separated parties make measurements on an unknown entangled quantum state, what correlations can they achieve? How difficult is it to determine whether a given correlation is a quantum correlation? These questions are central to problems in quantum communication and computation. Previous work has shown that the general membership problem for quantum correlations is computationally undecidable. In the current work we show something stronger: there is a family of constant-sized correlations -- that is, correlations for which the number of measurements and number of measurement outcomes are fixed -- such that solving the quantum membership problem for this family is computationally impossible. Thus, the undecidability that arises in understanding Bell experiments is not dependent on varying the number of measurements in the experiment. This places strong constraints on the types of descriptions that can be given for quantum correlation sets. Our proof is based on a combination of techniques from quantum self-testing and from undecidability results of the third author for linear system nonlocal games.","authors":["Honghao Fu","Carl A. Miller","William Slofstra"],"url":"https://arxiv.org/abs/2101.11087"}
{"created":"2025-04-11","title":"Scalable mixed-domain Gaussian process modeling and model reduction for longitudinal data","abstract":"Gaussian process (GP) models that combine both categorical and continuous input variables have found use in analysis of longitudinal data and computer experiments. However, standard inference for these models has the typical cubic scaling, and common scalable approximation schemes for GPs cannot be applied since the covariance function is non-continuous. In this work, we derive a basis function approximation scheme for mixed-domain covariance functions, which scales linearly with respect to the number of observations and total number of basis functions. The proposed approach is naturally applicable to also Bayesian GP regression with discrete observation models. We demonstrate the scalability of the approach and compare model reduction techniques for additive GP models in a longitudinal data context. We confirm that we can approximate the exact GP model accurately in a fraction of the runtime compared to fitting the corresponding exact model. In addition, we demonstrate a scalable model reduction workflow for obtaining smaller and more interpretable models when dealing with a large number of candidate predictors.","authors":["Juho Timonen","Harri L\\\"ahdesm\\\"aki"],"url":"https://arxiv.org/abs/2111.02019"}
{"created":"2025-04-11","title":"Low-Rank Mirror-Prox for Nonsmooth and Low-Rank Matrix Optimization Problems","abstract":"Low-rank and nonsmooth matrix optimization problems capture many fundamental tasks in statistics and machine learning. While significant progress has been made in recent years in developing efficient methods for \\textit{smooth} low-rank optimization problems that avoid maintaining high-rank matrices and computing expensive high-rank SVDs, advances for nonsmooth problems have been slow paced. In this paper we consider standard convex relaxations for such problems. Mainly, we prove that under a \\textit{strict complementarity} condition and under the relatively mild assumption that the nonsmooth objective can be written as a maximum of smooth functions, approximated variants of two popular \\textit{mirror-prox} methods: the Euclidean \\textit{extragradient method} and mirror-prox with \\textit{matrix exponentiated gradient updates}, when initialized with a \"warm-start\", converge to an optimal solution with rate $O(1/t)$, while requiring only two \\textit{low-rank} SVDs per iteration. Moreover, for the extragradient method we also consider relaxed versions of strict complementarity which yield a trade-off between the rank of the SVDs required and the radius of the ball in which we need to initialize the method. We support our theoretical results with empirical experiments on several nonsmooth low-rank matrix recovery tasks, demonstrating both the plausibility of the strict complementarity assumption, and the efficient convergence of our proposed low-rank mirror-prox variants.","authors":["Dan Garber","Atara Kaplan"],"url":"https://arxiv.org/abs/2206.11523"}
{"created":"2025-04-11","title":"Off-the-grid learning of mixtures from a continuous dictionary","abstract":"We consider a general non-linear model where the signal is a finite mixture of an unknown, possibly increasing, number of features issued from a continuous dictionary parameterized by a real non-linear parameter. The signal is observed with Gaussian (possibly correlated) noise in either a continuous or a discrete setup. We propose an off-the-grid optimization method, that is, a method which does not use any discretization scheme on the parameter space, to estimate both the non-linear parameters of the features and the linear parameters of the mixture. We use recent results on the geometry of off-the-grid methods to give minimal separation on the true underlying non-linear parameters such that interpolating certificate functions can be constructed. Using also tail bounds for suprema of Gaussian processes we bound the prediction error with high probability. Assuming that the certificate functions can be constructed, our prediction error bound is up to $\\log$-factors similar to the rates attained by the Lasso predictor in the linear regression model. We also establish convergence rates that quantify with high probability the quality of estimation for both the linear and the non-linear parameters. We develop in full details our main results for two applications: the Gaussian spike deconvolution and the scaled exponential model.","authors":["Cristina Butucea (CREST","FAIRPLAY)","Jean-Fran\\c{c}ois Delmas (CERMICS)","Anne Dutfoy (EDF R\\&D)","Cl\\'ement Hardy (CERMICS","EDF R\\&D)"],"url":"https://arxiv.org/abs/2207.00171"}
{"created":"2025-04-11","title":"Network inference via approximate Bayesian computation. Illustration on a stochastic multi-population neural mass model","abstract":"In this article, we propose an adapted sequential Monte Carlo approximate Bayesian computation (SMC-ABC) algorithm for network inference in coupled stochastic differential equations (SDEs) used for multivariate time series modeling. Our approach is motivated by neuroscience, specifically the challenge of estimating brain connectivity before and during epileptic seizures. To this end, we make four key contributions. First, we introduce a 6N-dimensional SDE to model the activity of N coupled neuronal populations, extending the (single-population) stochastic Jansen and Rit neural mass model used to describe human electroencephalography (EEG) rhythms, particularly epileptic activity. Second, we construct a reliable and efficient numerical splitting scheme for the model simulation. Third, we apply the proposed adapted SMC-ABC algorithm to the neural mass model and validate it on different types of simulated data. Compared to standard SMC-ABC, our approach significantly reduces computational cost by requiring fewer model simulations to reach the desired posterior region, thanks to the inclusion of binary parameters describing the presence or absence of coupling directions. Finally, we apply our method to real multi-channel EEG data, uncovering potential similarities in patients' brain activities across different epileptic seizures, as well as differences between pre-seizure and seizure periods.","authors":["Susanne Ditlevsen","Massimiliano Tamborrino","Irene Tubikanec"],"url":"https://arxiv.org/abs/2306.15787"}
{"created":"2025-04-11","title":"Error estimate for regularized optimal transport problems via Bregman divergence","abstract":"Regularization by the Shannon entropy enables us to efficiently and approximately solve optimal transport problems on a finite set. This paper is concerned with regularized optimal transport problems via Bregman divergence. We introduce the required properties for Bregman divergences, provide a non-asymptotic error estimate for the regularized problem, and show that the error estimate becomes faster than exponentially.","authors":["Keiichi Morikuni","Koya Sakakibara","Asuka Takatsu"],"url":"https://arxiv.org/abs/2309.11666"}
{"created":"2025-04-11","title":"Computable and Faithful Lower Bound on Entanglement Cost","abstract":"Quantifying the minimum entanglement needed to prepare quantum states and implement quantum processes is a key challenge in quantum information theory. In this work, we develop computable and faithful lower bounds on the entanglement cost under quantum operations that completely preserve the positivity of partial transpose (PPT operations), by introducing logarithmic $k$-negativity, a generalization of logarithmic negativity. Our bounds are efficiently computable via semidefinite programming and provide non-trivial values for all states that are not PPT, establishing their faithfulness. Notably, we find and affirm the irreversibility of asymptotic entanglement manipulation under PPT operations for full-rank entangled states. Furthermore, we extend our methodology to derive lower bounds on the entanglement cost of both point-to-point and bipartite quantum channels. Our bound demonstrates improvements over previously known computable bounds for a wide range of states and channels. These findings push the boundaries of understanding the structure of entanglement and the fundamental limits of entanglement manipulation.","authors":["Xin Wang","Mingrui Jing","Chengkai Zhu"],"url":"https://arxiv.org/abs/2311.10649"}
{"created":"2025-04-11","title":"Voting power in the Council of the European Union: A comprehensive sensitivity analysis","abstract":"The Council of the European Union (EU) is one of the main decision-making bodies of the EU. A number of decisions require a qualified majority, the support of 55% of the member states (currently 15) that represent at least 65% of the total population. We investigate how the power distribution -- based on the Shapley--Shubik index -- and the proportion of winning coalitions change if these criteria are modified within reasonable bounds. The power of the two countries with about 4% of the total population each is found to be almost flat. The decisiveness index decreases if the population criterion is above 68% or the states criterion is at least 17. Some quota combinations contradict the principles of double majority. The proportion of winning coalitions can be increased from 13.2% to 20.8% (30.1%) such that the maximal relative change in the Shapley--Shubik indices remains below 3.5% (5.5%). Our results are indispensable in evaluating any proposal for reforming the qualified majority voting system.","authors":["D\\'ora Gr\\'eta Petr\\'oczy","L\\'aszl\\'o Csat\\'o"],"url":"https://arxiv.org/abs/2312.16878"}
{"created":"2025-04-11","title":"Assessing the risk of recurrence in early-stage breast cancer through H&E stained whole slide images","abstract":"Accurate prediction of the likelihood of recurrence is important in the selection of postoperative treatment for patients with early-stage breast cancer. In this study, we investigated whether deep learning algorithms can predict patients' risk of recurrence by analyzing the pathology images of their cancer histology.We analyzed 125 hematoxylin and eosin-stained whole slide images (WSIs) from 125 patients across two institutions (National Cancer Center and Korea University Medical Center Guro Hospital) to predict breast cancer recurrence risk using deep learning. Sensitivity reached 0.857, 0.746, and 0.529 for low, intermediate, and high-risk categories, respectively, with specificity of 0.816, 0.803, and 0.972, and a Pearson correlation of 0.61 with histological grade. Class activation maps highlighted features like tubule formation and mitotic rate, suggesting a cost-effective approach to risk stratification, pending broader validation. These findings suggest that deep learning models trained exclusively on hematoxylin and eosin stained whole slide images can approximate genomic assay results, offering a cost-effective and scalable tool for breast cancer recurrence risk assessment. However, further validation using larger and more balanced datasets is needed to confirm the clinical applicability of our approach.","authors":["Geongyu Lee","Joonho Lee","Tae-Yeong Kwak","Sun Woo Kim","Youngmee Kwon","Chungyeul Kim","Hyeyoon Chang"],"url":"https://arxiv.org/abs/2406.06650"}
{"created":"2025-04-11","title":"Performance Analysis and Low-Complexity Beamforming Design for Near-Field Physical Layer Security","abstract":"Extremely large-scale arrays (XL-arrays) have emerged as a key enabler in achieving the unprecedented performance requirements of future wireless networks, leading to a significant increase in the range of the near-field region. This transition necessitates the spherical wavefront model for characterizing the wireless propagation rather than the far-field planar counterpart, thereby introducing extra degrees of freedom (DoFs) to wireless system design. In this paper, we explore the beam focusing-based physical layer security (PLS) in the near field, where multiple legitimate users and one eavesdropper are situated in the near-field region of the XL-array base station (BS). First, we consider a special case with one legitimate user and one eavesdropper to shed useful insights into near-field PLS. In particular, it is shown that 1) Artificial noise (AN) is crucial to near-field security provisioning, transforming an insecure system to a secure one; 2) AN can yield numerous security gains, which considerably enhances PLS in the near field as compared to the case without AN taken into account. Next, for the general case with multiple legitimate users, we propose an efficient low-complexity approach to design the beamforming with AN to guarantee near-field secure transmission. Specifically, the low-complexity approach is conceived starting by introducing the concept of interference domain to capture the inter-user interference level, followed by a three-step identification framework for designing the beamforming. Finally, numerical results reveal that 1) the PLS enhancement in the near field is pronounced thanks to the additional spatial DoFs; 2) the proposed approach can achieve close performance to that of the computationally-extensive conventional approach yet with a significantly lower computational complexity.","authors":["Yunpu Zhang","Yuan Fang","Changsheng You","Ying-Jun Angela Zhang","Hing Cheung So"],"url":"https://arxiv.org/abs/2407.13491"}
{"created":"2025-04-11","title":"Amortized Bayesian Multilevel Models","abstract":"Multilevel models (MLMs) are a central building block of the Bayesian workflow. They enable joint, interpretable modeling of data across hierarchical levels and provide a fully probabilistic quantification of uncertainty. Despite their well-recognized advantages, MLMs pose significant computational challenges, often rendering their estimation and evaluation intractable within reasonable time constraints. Recent advances in simulation-based inference offer promising solutions for addressing complex probabilistic models using deep generative networks. However, the utility and reliability of deep learning methods for estimating Bayesian MLMs remains largely unexplored, especially when compared with gold-standard samplers. To this end, we explore a family of neural network architectures that leverage the probabilistic factorization of multilevel models to facilitate efficient neural network training and subsequent near-instant posterior inference on unseen datasets. We test our method on several real-world case studies and provide comprehensive comparisons to Stan's gold standard sampler, where possible. Finally, we provide an open-source implementation of our methods to stimulate further research in the nascent field of amortized Bayesian inference.","authors":["Daniel Habermann","Marvin Schmitt","Lars K\\\"uhmichel","Andreas Bulling","Stefan T. Radev","Paul-Christian B\\\"urkner"],"url":"https://arxiv.org/abs/2408.13230"}
{"created":"2025-04-11","title":"A Novel Massive Random Access in Cell-Free Massive MIMO Systems for High-Speed Mobility with OTFS Modulation","abstract":"In the research of next-generation wireless communication technologies, orthogonal time frequency space (OTFS) modulation is emerging as a promising technique for high-speed mobile environments due to its superior efficiency and robustness in doubly selective channels. Additionally, the cell-free architecture, which eliminates the issues associated with cell boundaries, offers broader coverage for radio access networks. By combining cell-free network architecture with OTFS modulation, the system may meet the demands of massive random access required by machine-type communication devices in high-speed scenarios. This paper explores a massive random access scheme based on OTFS modulation within a cell-free architecture. A transceiver model for uplink OTFS signals involving multiple access points (APs) is developed, where channel estimation with fractional channel parameters is approximated as a block sparse matrix recovery problem. Building on existing superimposed and embedded preamble schemes, a hybrid preamble scheme is proposed. This scheme leverages superimposed and embedded preambles to respectively achieve rough and accurate active user equipment (UEs) detection (AUD), as well as precise channel estimation, under the condition of supporting a large number of access UEs. Moreover, this study introduces a generalized approximate message passing and pattern coupling sparse Bayesian learning with Laplacian prior (GAMP-PCSBL-La) algorithm, which effectively captures block sparse features after discrete cosine transform (DCT), delivering precise estimation results with reduced computational complexity. Simulation results demonstrate that the proposed scheme is effective and provides superior performance compared to other existing schemes.","authors":["Yanfeng Hu","Dongming Wang","Xinjiang Xia","Jiamin Li","Pengcheng Zhu","Xiaohu You"],"url":"https://arxiv.org/abs/2409.01111"}
{"created":"2025-04-11","title":"Center-fixing of tropical cyclones using uncertainty-aware deep learning applied to high-temporal-resolution geostationary satellite imagery","abstract":"Determining the location of a tropical cyclone's (TC) surface circulation center -- \"center-fixing\" -- is a critical first step in the TC-forecasting process, affecting current and future estimates of track, intensity, and structure. Despite a recent increase in automated center-fixing methods, only one such method (ARCHER-2) is operational, and its best performance is achieved when using microwave or scatterometer data, which are not available at every forecast cycle. We develop a deep-learning algorithm called GeoCenter; besides a few scalars in the operational ATCF, it relies only on geostationary IR satellite imagery, which is available for all TC basins at high frequency (10 min) and low latency (< 10 min) during both day and night. GeoCenter ingests an animation (time series) of IR images, including 9 channels at lag times up to 4 hours. The animation is centered at a \"first guess\" location, offset from the true TC-center location by 48 km on average and sometimes > 100 km; GeoCenter is tasked with correcting this offset. On an independent testing dataset, GeoCenter achieves a mean/median/RMS (root mean square) error of 26.6/22.2/32.4 km for all systems, 24.7/20.8/30.0 km for tropical systems, and 14.6/12.5/17.3 km for category-2--5 hurricanes. These values are similar to ARCHER-2 errors with microwave or scatterometer data, and better than ARCHER-2 errors when only IR data are available. GeoCenter also performs skillful uncertainty quantification, producing a well calibrated ensemble of 150 TC-center locations. Furthermore, all predictors used by GeoCenter are available in real time, which would make GeoCenter easy to implement operationally every 10 min.","authors":["Ryan Lagerquist","Galina Chirokova","Robert DeMaria","Mark DeMaria","Imme Ebert-Uphoff"],"url":"https://arxiv.org/abs/2409.16507"}
{"created":"2025-04-11","title":"The geometry of covering codes in the sum-rank metric","abstract":"We introduce the concept of a sum-rank saturating system and outline its correspondence to a covering properties of a sum-rank metric code. We consider the problem of determining the shortest sum-rank-$\\rho$-saturating systems of a fixed dimension, which is equivalent to the covering problem in the sum-rank metric. We obtain upper and lower bounds on this quantity. We also give constructions of saturating systems arising from geometrical structures.","authors":["Matteo Bonini","Martino Borello","Eimear Byrne"],"url":"https://arxiv.org/abs/2410.12393"}
{"created":"2025-04-11","title":"Hyperparameter Optimization in Machine Learning","abstract":"Hyperparameters are configuration variables controlling the behavior of machine learning algorithms. They are ubiquitous in machine learning and artificial intelligence and the choice of their values determines the effectiveness of systems based on these technologies. Manual hyperparameter search is often unsatisfactory and becomes infeasible when the number of hyperparameters is large. Automating the search is an important step towards advancing, streamlining, and systematizing machine learning, freeing researchers and practitioners alike from the burden of finding a good set of hyperparameters by trial and error. In this survey, we present a unified treatment of hyperparameter optimization, providing the reader with examples, insights into the state-of-the-art, and numerous links to further reading. We cover the main families of techniques to automate hyperparameter search, often referred to as hyperparameter optimization or tuning, including random and quasi-random search, bandit-, model-, population-, and gradient-based approaches. We further discuss extensions, including online, constrained, and multi-objective formulations, touch upon connections with other fields such as meta-learning and neural architecture search, and conclude with open questions and future research directions.","authors":["Luca Franceschi","Michele Donini","Valerio Perrone","Aaron Klein","C\\'edric Archambeau","Matthias Seeger","Massimiliano Pontil","Paolo Frasconi"],"url":"https://arxiv.org/abs/2410.22854"}
{"created":"2025-04-11","title":"Symmetrizable systems","abstract":"Transforming an asymmetric system into a symmetric system makes it possible to exploit the simplifying properties of symmetry in control problems. We define and characterize the family of symmetrizable systems, which can be transformed into symmetric systems by a linear transformation of their inputs and outputs. In the special case of complete symmetry, the set of symmetrizable systems is convex and verifiable by a semidefinite program. We show that a Khatri-Rao rank needs to be satisfied for a system to be symmetrizable and conclude that linear systems are generically neither symmetric nor symmetrizable.","authors":["Hamed Taghavian","Jens Sj\\\"olund"],"url":"https://arxiv.org/abs/2411.06887"}
{"created":"2025-04-11","title":"Methods with Local Steps and Random Reshuffling for Generally Smooth Non-Convex Federated Optimization","abstract":"Non-convex Machine Learning problems typically do not adhere to the standard smoothness assumption. Based on empirical findings, Zhang et al. (2020b) proposed a more realistic generalized $(L_0, L_1)$-smoothness assumption, though it remains largely unexplored. Many existing algorithms designed for standard smooth problems need to be revised. However, in the context of Federated Learning, only a few works address this problem but rely on additional limiting assumptions. In this paper, we address this gap in the literature: we propose and analyze new methods with local steps, partial participation of clients, and Random Reshuffling without extra restrictive assumptions beyond generalized smoothness. The proposed methods are based on the proper interplay between clients' and server's stepsizes and gradient clipping. Furthermore, we perform the first analysis of these methods under the Polyak-{\\L} ojasiewicz condition. Our theory is consistent with the known results for standard smooth problems, and our experimental results support the theoretical insights.","authors":["Yury Demidovich","Petr Ostroukhov","Grigory Malinovsky","Samuel Horv\\'ath","Martin Tak\\'a\\v{c}","Peter Richt\\'arik","Eduard Gorbunov"],"url":"https://arxiv.org/abs/2412.02781"}
{"created":"2025-04-11","title":"Digital Twin Aided Channel Estimation: Zone-Specific Subspace Prediction and Calibration","abstract":"Effective channel estimation in sparse and high-dimensional environments is essential for next-generation wireless systems, particularly in large-scale MIMO deployments. This paper introduces a novel framework that leverages digital twins (DTs) as priors to enable efficient zone-specific subspace-based channel estimation (CE). Subspace-based CE significantly reduces feedback overhead by focusing on the dominant channel components, exploiting sparsity in the angular domain while preserving estimation accuracy. While DT channels may exhibit inaccuracies, their coarse-grained subspaces provide a powerful starting point, reducing the search space and accelerating convergence. The framework employs a two-step clustering process on the Grassmann manifold, combined with reinforcement learning (RL), to iteratively calibrate subspaces and align them with real-world counterparts. Simulations show that digital twins not only enable near-optimal performance but also enhance the accuracy of subspace calibration through RL, highlighting their potential as a step towards learnable digital twins.","authors":["Sadjad Alikhani","Ahmed Alkhateeb"],"url":"https://arxiv.org/abs/2501.02758"}
{"created":"2025-04-11","title":"Efficient Sparse Flow Decomposition Methods for RNA Multi-Assembly","abstract":"Decomposing a flow on a Directed Acyclic Graph (DAG) into a weighted sum of a small number of paths is an essential task in operations research and bioinformatics. This problem, referred to as Sparse Flow Decomposition (SFD), has gained significant interest, in particular for its application in RNA transcript multi-assembly, the identification of the multiple transcripts corresponding to a given gene and their relative abundance. Several recent approaches cast SFD variants as integer optimization problems, motivated by the NP-hardness of the formulations they consider. We propose an alternative formulation of SFD as a fitting problem on the conic hull of the flow polytope. By reformulating the problem on the flow polytope for compactness and solving it using specific variants of the Frank-Wolfe algorithm, we obtain a method converging rapidly to the minimizer of the chosen loss function while producing a parsimonious decomposition. Our approach subsumes previous formulations of SFD with exact and inexact flows and can model different priors on the error distributions. Computational experiments show that our method outperforms recent integer optimization approaches in runtime, but is also highly competitive in terms of reconstruction of the underlying transcripts, despite not explicitly minimizing the solution cardinality.","authors":["Mathieu Besan\\c{c}on"],"url":"https://arxiv.org/abs/2501.14662"}
{"created":"2025-04-11","title":"The Cloud and Flock Polynomials of q-Matroids","abstract":"We show that the Whitney function of a q-matroid can be determined from the cloud and flock polynomials associated to the cyclic flats. These polynomials capture information about the corank (resp., nullity) of certain spaces whose cyclic core (resp., closure) is the given cyclic flat. Going one step further, we prove that the Whitney function, and in fact the cloud-flock lattice, are determined by the configuration of the q-matroid, which is the abstract lattice of cyclic flats together with the corank-nullity data. Furthermore, we show that the configuration and cloud-flock lattice behave well under duality and direct sums, whereas the Whitney function does not contain enough information to behave well under taking direct sums. As an aside we show that every configuration of a matroid arises as a configuration of a q-matroid, whereas the converse is not true.","authors":["Heide Gluesing-Luerssen","Benjamin Jany"],"url":"https://arxiv.org/abs/2501.14984"}
{"created":"2025-04-11","title":"Optimizing Through Change: Bounds and Recommendations for Time-Varying Bayesian Optimization Algorithms","abstract":"Time-Varying Bayesian Optimization (TVBO) is the go-to framework for optimizing a time-varying, expensive, noisy black-box function. However, most of the solutions proposed so far either rely on unrealistic assumptions on the nature of the objective function or do not offer any theoretical guarantees. We propose the first analysis that asymptotically bounds the cumulative regret of TVBO algorithms under mild and realistic assumptions only. In particular, we provide an algorithm-independent lower regret bound and an upper regret bound that holds for a large class of TVBO algorithms. Based on this analysis, we formulate recommendations for TVBO algorithms and show how an algorithm (BOLT) that follows them performs better than the state-of-the-art of TVBO through experiments on synthetic and real-world problems.","authors":["Anthony Bardou","Patrick Thiran"],"url":"https://arxiv.org/abs/2501.18963"}
{"created":"2025-04-11","title":"Learning Generalizable Features for Tibial Plateau Fracture Segmentation Using Masked Autoencoder and Limited Annotations","abstract":"Accurate automated segmentation of tibial plateau fractures (TPF) from computed tomography (CT) requires large amounts of annotated data to train deep learning models, but obtaining such annotations presents unique challenges. The process demands expert knowledge to identify diverse fracture patterns, assess severity, and account for individual anatomical variations, making the annotation process highly time-consuming and expensive. Although semi-supervised learning methods can utilize unlabeled data, existing approaches often struggle with the complexity and variability of fracture morphologies, as well as limited generalizability across datasets. To tackle these issues, we propose an effective training strategy based on masked autoencoder (MAE) for the accurate TPF segmentation in CT. Our method leverages MAE pretraining to capture global skeletal structures and fine-grained fracture details from unlabeled data, followed by fine-tuning with a small set of labeled data. This strategy reduces the dependence on extensive annotations while enhancing the model's ability to learn generalizable and transferable features. The proposed method is evaluated on an in-house dataset containing 180 CT scans with TPF. Experimental results demonstrate that our method consistently outperforms semi-supervised methods, achieving an average Dice similarity coefficient (DSC) of 95.81%, average symmetric surface distance (ASSD) of 1.91mm, and Hausdorff distance (95HD) of 9.42mm with only 20 annotated cases. Moreover, our method exhibits strong transferability when applying to another public pelvic CT dataset with hip fractures, highlighting its potential for broader applications in fracture segmentation tasks.","authors":["Peiyan Yue","Die Cai","Chu Guo","Mengxing Liu","Jun Xia","Yi Wang"],"url":"https://arxiv.org/abs/2502.02862"}
{"created":"2025-04-11","title":"Distributional Autoencoders Know the Score","abstract":"This work presents novel and desirable properties of a recently introduced class of autoencoders - the Distributional Principal Autoencoder (DPA) - which combines distributionally correct reconstruction with principal components-like interpretability of the encodings. First, we show formally that the level sets of the encoder orient themselves exactly with regard to the score of the data distribution. This both explains the method's often remarkable performance in disentangling the factors of variation of the data, as well as opens up possibilities of recovering its distribution while having access to samples only. In settings where the score itself has physical meaning - such as when the data obeys the Boltzmann distribution - we demonstrate that the method can recover scientifically important quantities such as the minimum free energy path. Second, we prove that if the data lies on a manifold that can be approximated by the encoder, the optimal encoder's components beyond the dimension of the manifold will carry absolutely no additional information about the data distribution. This promises potentially new ways of determining the number of relevant dimensions of the data. The results thus demonstrate that the DPA elegantly combines two often disparate goals of unsupervised learning: the learning of the data distribution and the learning of the intrinsic data dimensionality.","authors":["Andrej Leban"],"url":"https://arxiv.org/abs/2502.11583"}
{"created":"2025-04-11","title":"Low-Rank Thinning","abstract":"The goal in thinning is to summarize a dataset using a small set of representative points. Remarkably, sub-Gaussian thinning algorithms like Kernel Halving and Compress can match the quality of uniform subsampling while substantially reducing the number of summary points. However, existing guarantees cover only a restricted range of distributions and kernel-based quality measures and suffer from pessimistic dimension dependence. To address these deficiencies, we introduce a new low-rank analysis of sub-Gaussian thinning that applies to any distribution and any kernel, guaranteeing high-quality compression whenever the kernel or data matrix is approximately low-rank. To demonstrate the broad applicability of the techniques, we design practical sub-Gaussian thinning approaches that improve upon the best known guarantees for approximating attention in transformers, accelerating stochastic gradient training through reordering, and distinguishing distributions in near-linear time.","authors":["Annabelle Michael Carrell","Albert Gong","Abhishek Shetty","Raaz Dwivedi","Lester Mackey"],"url":"https://arxiv.org/abs/2502.12063"}
{"created":"2025-04-11","title":"Differentially Private Joint Independence Test","abstract":"Identification of joint dependence among more than two random vectors plays an important role in many statistical applications, where the data may contain sensitive or confidential information. In this paper, we consider the the $d$-variable Hilbert-Schmidt independence criterion (dHSIC) in the context of differential privacy. Given the limiting distribution of the empirical estimate of dHSIC is complicated Gaussian chaos, constructing tests in the non-privacy regime is typically based on permutation and bootstrap. To detect joint dependence in privacy, we propose a dHSIC-based testing procedure by employing a differentially private permutation methodology. Our method enjoys privacy guarantee, valid level and pointwise consistency, while the bootstrap counterpart suffers inconsistent power. We further investigate the uniform power of the proposed test in dHSIC metric and $L_2$ metric, indicating that the proposed test attains the minimax optimal power across different privacy regimes. As a byproduct, our results also contain the pointwise and uniform power of the non-private permutation dHSIC, addressing an unsolved question remained in Pfister et al. (2018). Both numerical simulations and real data analysis on causal inference suggest our proposed test performs well empirically.","authors":["Xingwei Liu","Yuexin Chen","Wangli Xu"],"url":"https://arxiv.org/abs/2503.18721"}
{"created":"2025-04-11","title":"Accelerated Stein Variational Gradient Flow","abstract":"Stein variational gradient descent (SVGD) is a kernel-based particle method for sampling from a target distribution, e.g., in generative modeling and Bayesian inference. SVGD does not require estimating the gradient of the log-density, which is called score estimation. In practice, SVGD can be slow compared to score-estimation based sampling algorithms. To design fast and efficient high-dimensional sampling algorithms, we introduce ASVGD, an accelerated SVGD, based on an accelerated gradient flow in a metric space of probability densities following Nesterov's method. We then derive a momentum-based discrete-time sampling algorithm, which evolves a set of particles deterministically. To stabilize the particles' momentum update, we also study a Wasserstein metric regularization. For the generalized bilinear kernel and the Gaussian kernel, toy numerical examples with varied target distributions demonstrate the effectiveness of ASVGD compared to SVGD and other popular sampling methods.","authors":["Viktor Stein","Wuchen Li"],"url":"https://arxiv.org/abs/2503.23462"}
{"created":"2025-04-11","title":"Scalable Geometric Learning with Correlation-Based Functional Brain Networks","abstract":"The correlation matrix is a central representation of functional brain networks in neuroimaging. Traditional analyses often treat pairwise interactions independently in a Euclidean setting, overlooking the intrinsic geometry of correlation matrices. While earlier attempts have embraced the quotient geometry of the correlation manifold, they remain limited by computational inefficiency and numerical instability, particularly in high-dimensional contexts. This paper presents a novel geometric framework that employs diffeomorphic transformations to embed correlation matrices into a Euclidean space, preserving salient manifold properties and enabling large-scale analyses. The proposed method integrates with established learning algorithms - regression, dimensionality reduction, and clustering - and extends naturally to population-level inference of brain networks. Simulation studies demonstrate both improved computational speed and enhanced accuracy compared to conventional manifold-based approaches. Moreover, applications in real neuroimaging scenarios illustrate the framework's utility, enhancing behavior score prediction, subject fingerprinting in resting-state fMRI, and hypothesis testing in electroencephalogram data. An open-source MATLAB toolbox is provided to facilitate broader adoption and advance the application of correlation geometry in functional brain network research.","authors":["Kisung You","Yelim Lee","Hae-Jeong Park"],"url":"https://arxiv.org/abs/2503.23653"}
{"created":"2025-04-11","title":"PHOENIX: Pauli-Based High-Level Optimization Engine for Instruction Execution on NISQ Devices","abstract":"Variational quantum algorithms (VQA) based on Hamiltonian simulation represent a specialized class of quantum programs well-suited for near-term quantum computing applications due to its modest resource requirements in terms of qubits and circuit depth. Unlike the conventional single-qubit (1Q) and two-qubit (2Q) gate sequence representation, Hamiltonian simulation programs are essentially composed of disciplined subroutines known as Pauli exponentiations (Pauli strings with coefficients) that are variably arranged. To capitalize on these distinct program features, this study introduces PHOENIX, a highly effective compilation framework that primarily operates at the high-level Pauli-based intermediate representation (IR) for generic Hamiltonian simulation programs. PHOENIX exploits global program optimization opportunities to the greatest extent, compared to existing SOTA methods despite some of them also utilizing similar IRs. PHOENIX employs the binary symplectic form (BSF) to formally describe Pauli strings and reformulates IR synthesis as reducing the column weights of BSF by appropriate Clifford transformations. It comes with a heuristic BSF simplification algorithm that searches for the most appropriate 2Q Clifford operators in sequence to maximally simplify the BSF at each step, until the BSF can be directly synthesized by basic 1Q and 2Q gates. PHOENIX further performs a global ordering strategy in a Tetris-like fashion for these simplified IR groups, carefully balancing optimization opportunities for gate cancellation, minimizing circuit depth, and managing qubit routing overhead. Experimental results demonstrate that PHOENIX outperforms SOTA VQA compilers across diverse program categories, backend ISAs, and hardware topologies.","authors":["Zhaohui Yang","Dawei Ding","Chenghong Zhu","Jianxin Chen","Yuan Xie"],"url":"https://arxiv.org/abs/2504.03529"}
{"created":"2025-04-11","title":"Optimal Sensor Placement Using Combinations of Hybrid Measurements for Source Localization","abstract":"This paper focuses on static source localization employing different combinations of measurements, including time-difference-of-arrival (TDOA), received-signal-strength (RSS), angle-of-arrival (AOA), and time-of-arrival (TOA) measurements. Since sensor-source geometry significantly impacts localization accuracy, the strategies of optimal sensor placement are proposed systematically using combinations of hybrid measurements. Firstly, the relationship between sensor placement and source estimation accuracy is formulated by a derived Cram\\'er-Rao bound (CRB). Secondly, the A-optimality criterion, i.e., minimizing the trace of the CRB, is selected to calculate the smallest reachable estimation mean-squared-error (MSE) in a unified manner. Thirdly, the optimal sensor placement strategies are developed to achieve the optimal estimation bound. Specifically, the specific constraints of the optimal geometries deduced by specific measurement, i.e., TDOA, AOA, RSS, and TOA, are found and discussed theoretically. Finally, the new findings are verified by simulation studies.","authors":["Kang Tang","Sheng Xu","Yuqi Yang","He Kong","Yongsheng Ma"],"url":"https://arxiv.org/abs/2504.03769"}
{"created":"2025-04-11","title":"Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning","abstract":"Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset.","authors":["Kai Ye","Hongyi Zhou","Jin Zhu","Francesco Quinzan","Chengchung Shi"],"url":"https://arxiv.org/abs/2504.03784"}
{"created":"2025-04-11","title":"On the Nature of Fractal Numbers and the Classical Continuum Hypothesis (CH)","abstract":"We propose a reinterpretation of the continuum grounded in the stratified structure of definability rather than classical cardinality. In this framework, a real number is not an abstract point on the number line, but an object expressible at some level Fn of a formal hierarchy. We introduce the notion of \"fractal numbers\" -- entities defined not within a fixed set-theoretic universe, but through layered expressibility across constructive systems. This reconceptualizes irrationality as a relative property, depending on definability depth, and replaces the binary dichotomy between countable and uncountable sets with a gradated spectrum of definability classes. We show that the classical Continuum Hypothesis loses its force in this context: between aleph_0 and c lies not a single cardinal jump, but a stratified sequence of definitional stages, each forming a countable yet irreducible approximation to the continuum. We argue that the real line should not be seen as a completed totality but as an evolving architecture of formal expressibility. We conclude with a discussion of rational invariants, the relativity of irrationality, and the emergence of a fractal metric for definitional density.","authors":["Stanislav Semenov"],"url":"https://arxiv.org/abs/2504.04637"}
{"created":"2025-04-11","title":"asKAN: Active Subspace embedded Kolmogorov-Arnold Network","abstract":"The Kolmogorov-Arnold Network (KAN) has emerged as a promising neural network architecture for small-scale AI+Science applications. However, it suffers from inflexibility in modeling ridge functions, which is widely used in representing the relationships in physical systems. This study investigates this inflexibility through the lens of the Kolmogorov-Arnold theorem, which starts the representation of multivariate functions from constructing the univariate components rather than combining the independent variables. Our analysis reveals that incorporating linear combinations of independent variables can substantially simplify the network architecture in representing the ridge functions. Inspired by this finding, we propose active subspace embedded KAN (asKAN), a hierarchical framework that synergizes KAN's function representation with active subspace methodology. The architecture strategically embeds active subspace detection between KANs, where the active subspace method is used to identify the primary ridge directions and the independent variables are adaptively projected onto these critical dimensions. The proposed asKAN is implemented in an iterative way without increasing the number of neurons in the original KAN. The proposed method is validated through function fitting, solving the Poisson equation, and reconstructing sound field. Compared with KAN, asKAN significantly reduces the error using the same network architecture. The results suggest that asKAN enhances the capability of KAN in fitting and solving equations in the form of ridge functions.","authors":["Zhiteng Zhou","Zhaoyue Xu","Yi Liu","Shizhao Wang"],"url":"https://arxiv.org/abs/2504.04669"}
{"created":"2025-04-11","title":"Supports for Outerplanar and Bounded Treewidth Graphs","abstract":"We study the existence and construction of sparse supports for hypergraphs derived from subgraphs of a graph $G$. For a hypergraph $(X,\\mathcal{H})$, a support $Q$ is a graph on $X$ s.t. $Q[H]$, the graph induced on vertices in $H$ is connected for every $H\\in\\mathcal{H}$.","authors":["Rajiv Raman","Karamjeet Singh"],"url":"https://arxiv.org/abs/2504.05039"}
{"created":"2025-04-11","title":"Effective Method for Inverse Ising Problem under Missing Observations in Restricted Boltzmann Machines","abstract":"Restricted Boltzmann machines (RBMs) are energy-based models analogous to the Ising model and are widely applied in statistical machine learning. The standard inverse Ising problem with a complete dataset requires computing both data and model expectations and is computationally challenging because model expectations have a combinatorial explosion. Furthermore, in many applications, the available datasets are partially incomplete, making it difficult to compute even data expectations. In this study, we propose a approximation framework for these expectations in the practical inverse Ising problems that integrates mean-field approximation or persistent contrastive divergence to generate refined initial points and spatial Monte Carlo integration to enhance estimator accuracy. We demonstrate that the proposed method effectively and accurately tunes the model parameters in comparison to the conventional method.","authors":["Kaiji Sekimoto","Muneki Yasuda"],"url":"https://arxiv.org/abs/2504.05643"}
{"created":"2025-04-11","title":"Totally equimodular matrices: decomposition and triangulation","abstract":"Totally equimodular matrices generalize totally unimodular matrices and arise in the context of box-total dual integral polyhedra. This work further explores the parallels between these two classes and introduces foundational building blocks for constructing totally equimodular matrices. Consequently, we present a decomposition theorem for totally equimodular matrices of full row rank.","authors":["Patrick Chervet","Roland Grappe","Mathieu Vall\\'ee"],"url":"https://arxiv.org/abs/2504.05930"}
