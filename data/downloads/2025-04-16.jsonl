{"created":"2025-04-16","title":"Roamify: Designing and Evaluating an LLM Based Google Chrome Extension for Personalised Itinerary Planning","abstract":"In this paper, we present Roamify, an Artificial Intelligence powered travel assistant that aims to ease the process of travel planning. We have tested and used multiple Large Language Models like Llama and T5 to generate personalised itineraries per user preferences. Results from user surveys highlight the preference for AI powered mediums over existing methods to help in travel planning across all user age groups. These results firmly validate the potential need of such a travel assistant. We highlight the two primary design considerations for travel assistance: D1) incorporating a web-scraping method to gather up-to-date news articles about destinations from various blog sources, which significantly improves our itinerary suggestions, and D2) utilising user preferences to create customised travel experiences along with a recommendation system which changes the itinerary according to the user needs. Our findings suggest that Roamify has the potential to improve and simplify how users across multiple age groups plan their travel experiences.","authors":["Vikranth Udandarao","Noel Abraham Tiju","Muthuraj Vairamuthu","Harsh Mistry","Dhruv Kumar"],"url":"https://arxiv.org/abs/2504.10489"}
{"created":"2025-04-16","title":"GPT Meets Graphs and KAN Splines: Testing Novel Frameworks on Multitask Fine-Tuned GPT-2 with LoRA","abstract":"We explore the potential of integrating learnable and interpretable modules--specifically Kolmogorov-Arnold Networks (KAN) and graph-based representations--within a pre-trained GPT-2 model to enhance multi-task learning accuracy. Motivated by the recent surge in using KAN and graph attention (GAT) architectures in chain-of-thought (CoT) models and debates over their benefits compared to simpler architectures like MLPs, we begin by enhancing a standard self-attention transformer using Low-Rank Adaptation (LoRA), fine-tuning hyperparameters, and incorporating L2 regularization. This approach yields significant improvements. To further boost interpretability and richer representations, we develop two variants that attempt to improve the standard KAN and GAT: Graph LoRA and Hybrid-KAN LoRA (Learnable GPT). However, systematic evaluations reveal that neither variant outperforms the optimized LoRA-enhanced transformer, which achieves 55.249% accuracy on the SST test set, 99.18% on the CFIMDB dev set, and 89.9% paraphrase detection test accuracy. On sonnet generation, we get a CHRF score of 42.097. These findings highlight that efficient parameter adaptation via LoRA remains the most effective strategy for our tasks: sentiment analysis, paraphrase detection, and sonnet generation.","authors":["Gabriel Bo","Marc Bernardino","Justin Gu"],"url":"https://arxiv.org/abs/2504.10490"}
{"created":"2025-04-16","title":"ArxivBench: Can LLMs Assist Researchers in Conducting Research?","abstract":"Large language models (LLMs) have demonstrated remarkable effectiveness in completing various tasks such as reasoning, translation, and question answering. However the issue of factual incorrect content in LLM-generated responses remains a persistent challenge. In this study, we evaluate both proprietary and open-source LLMs on their ability to respond with relevant research papers and accurate links to articles hosted on the arXiv platform, based on high level prompts. To facilitate this evaluation, we introduce arXivBench, a benchmark specifically designed to assess LLM performance across eight major subject categories on arXiv and five subfields within computer science, one of the most popular categories among them. Our findings reveal a concerning accuracy of LLM-generated responses depending on the subject, with some subjects experiencing significantly lower accuracy than others. Notably, Claude-3.5-Sonnet exhibits a substantial advantage in generating both relevant and accurate responses. And interestingly, most LLMs achieve a much higher accuracy in the Artificial Intelligence sub-field than other sub-fields. This benchmark provides a standardized tool for evaluating the reliability of LLM-generated scientific responses, promoting more dependable use of LLMs in academic and research environments. Our code is open-sourced at https://github.com/arxivBenchLLM/arXivBench and our dataset is available on huggingface at https://huggingface.co/datasets/arXivBenchLLM/arXivBench.","authors":["Ning Li","Jingran Zhang","Justin Cui"],"url":"https://arxiv.org/abs/2504.10496"}
{"created":"2025-04-16","title":"Exploring Generative AI Techniques in Government: A Case Study","abstract":"The swift progress of Generative Artificial intelligence (GenAI), notably Large Language Models (LLMs), is reshaping the digital landscape. Recognizing this transformative potential, the National Research Council of Canada (NRC) launched a pilot initiative to explore the integration of GenAI techniques into its daily operation for performance excellence, where 22 projects were launched in May 2024. Within these projects, this paper presents the development of the intelligent agent Pubbie as a case study, targeting the automation of performance measurement, data management and insight reporting at the NRC. Cutting-edge techniques are explored, including LLM orchestration and semantic embedding via RoBERTa, while strategic fine-tuning and few-shot learning approaches are incorporated to infuse domain knowledge at an affordable cost. The user-friendly interface of Pubbie allows general government users to input queries in natural language and easily upload or download files with a simple button click, greatly reducing manual efforts and accessibility barriers.","authors":["Sunyi Liu","Mengzhe Geng","Rebecca Hart"],"url":"https://arxiv.org/abs/2504.10497"}
{"created":"2025-04-16","title":"CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation for Large Language Models","abstract":"The performance of large language models (LLMs) in Q&amp;A task increased substantially through Retrieval-Augmented Generation (RAG) which brings in external knowledge. However, the main difficulty lies in balancing the inherent self-knowledge of LLMs with external information retrieval (IR). The current threshold-based methods apply one-dimensional static mechanisms with single criterion. As a result, their IR decisions might be irrelevant to the LLMs' response under difficult queries. To alleviate this problem, we propose Cognitive Convection of Self-Knowledge (CCSK). Different from traditional methods that maintain single fixed IR activation criteria, CCSK implements a dynamic joint decision process via a Siamese Network module and a Response Quality Model. The Siamese Network calculates the cosine similarity between the current query and the historical queries. The Response Quality Model evaluates the responses of LLMs through LightGBM. The final decision of the CCSK is derived from the outputs of the two modules, as well as text features fused using a multi-head attention mechanism. Extensive experiments on real-world datasets show that CCSK significantly enhances the model's effectiveness in information retrieval.","authors":["Jianling Lu","Mingqi Lv"],"url":"https://arxiv.org/abs/2504.10498"}
{"created":"2025-04-16","title":"Graph-based Approaches and Functionalities in Retrieval-Augmented Generation: A Comprehensive Survey","abstract":"Large language models (LLMs) struggle with the factual error during inference due to the lack of sufficient training data and the most updated knowledge, leading to the hallucination problem. Retrieval-Augmented Generation (RAG) has gained attention as a promising solution to address the limitation of LLMs, by retrieving relevant information from external source to generate more accurate answers to the questions. Given the pervasive presence of structured knowledge in the external source, considerable strides in RAG have been made to employ the techniques related to graphs and achieve more complex reasoning based on the topological information between knowledge entities. However, there is currently neither unified review examining the diverse roles of graphs in RAG, nor a comprehensive resource to help researchers navigate and contribute to this evolving field. This survey offers a novel perspective on the functionality of graphs within RAG and their impact on enhancing performance across a wide range of graph-structured data. It provides a detailed breakdown of the roles that graphs play in RAG, covering database construction, algorithms, pipelines, and tasks. Finally, it identifies current challenges and outline future research directions, aiming to inspire further developments in this field. Our graph-centered analysis highlights the commonalities and differences in existing methods, setting the stage for future researchers in areas such as graph learning, database systems, and natural language processing.","authors":["Zulun Zhu","Tiancheng Huang","Kai Wang","Junda Ye","Xinghe Chen","Siqiang Luo"],"url":"https://arxiv.org/abs/2504.10499"}
{"created":"2025-04-16","title":"Leveraging Auto-Distillation and Generative Self-Supervised Learning in Residual Graph Transformers for Enhanced Recommender Systems","abstract":"This paper introduces a cutting-edge method for enhancing recommender systems through the integration of generative self-supervised learning (SSL) with a Residual Graph Transformer. Our approach emphasizes the importance of superior data enhancement through the use of pertinent pretext tasks, automated through rationale-aware SSL to distill clear ways of how users and items interact. The Residual Graph Transformer incorporates a topology-aware transformer for global context and employs residual connections to improve graph representation learning. Additionally, an auto-distillation process refines self-supervised signals to uncover consistent collaborative rationales. Experimental evaluations on multiple datasets demonstrate that our approach consistently outperforms baseline methods.","authors":["Eya Mhedhbi","Youssef Mourchid","Alice Othmani"],"url":"https://arxiv.org/abs/2504.10500"}
{"created":"2025-04-16","title":"Exposure to Content Written by Large Language Models Can Reduce Stigma Around Opioid Use Disorder in Online Communities","abstract":"Widespread stigma, both in the offline and online spaces, acts as a barrier to harm reduction efforts in the context of opioid use disorder (OUD). This stigma is prominently directed towards clinically approved medications for addiction treatment (MAT), people with the condition, and the condition itself. Given the potential of artificial intelligence based technologies in promoting health equity, and facilitating empathic conversations, this work examines whether large language models (LLMs) can help abate OUD-related stigma in online communities. To answer this, we conducted a series of pre-registered randomized controlled experiments, where participants read LLM-generated, human-written, or no responses to help seeking OUD-related content in online communities. The experiment was conducted under two setups, i.e., participants read the responses either once (N = 2,141), or repeatedly for 14 days (N = 107). We found that participants reported the least stigmatized attitudes toward MAT after consuming LLM-generated responses under both the setups. This study offers insights into strategies that can foster inclusive online discourse on OUD, e.g., based on our findings LLMs can be used as an education-based intervention to promote positive attitudes and increase people's propensity toward MAT.","authors":["Shravika Mittal","Darshi Shah","Shin Won Do","Mai ElSherief","Tanushree Mitra","Munmun De Choudhury"],"url":"https://arxiv.org/abs/2504.10501"}
{"created":"2025-04-16","title":"Human-Oriented Image Retrieval System (HORSE): A Neuro-Symbolic Approach to Optimizing Retrieval of Previewed Images","abstract":"Image retrieval remains a challenging task due to the complex interaction between human visual perception, memory, and computational processes. Current image search engines often struggle to efficiently retrieve images based on natural language descriptions, as they rely on time-consuming preprocessing, tagging, and machine learning pipelines. This paper introduces the Human-Oriented Retrieval Search Engine for Images (HORSE), a novel approach that leverages neuro-symbolic indexing to improve image retrieval by focusing on human-oriented indexing. By integrating cognitive science insights with advanced computational techniques, HORSE enhances the retrieval process, making it more aligned with how humans perceive, store, and recall visual information. The neuro-symbolic framework combines the strengths of neural networks and symbolic reasoning, mitigating their individual limitations. The proposed system optimizes image retrieval, offering a more intuitive and efficient solution for users. We discuss the design and implementation of HORSE, highlight its potential applications in fields such as design error detection and knowledge management, and suggest future directions for research to further refine the system's metrics and capabilities.","authors":["Abraham Itzhak Weinberg"],"url":"https://arxiv.org/abs/2504.10502"}
{"created":"2025-04-16","title":"LayerFlow: Layer-wise Exploration of LLM Embeddings using Uncertainty-aware Interlinked Projections","abstract":"Large language models (LLMs) represent words through contextual word embeddings encoding different language properties like semantics and syntax. Understanding these properties is crucial, especially for researchers investigating language model capabilities, employing embeddings for tasks related to text similarity, or evaluating the reasons behind token importance as measured through attribution methods. Applications for embedding exploration frequently involve dimensionality reduction techniques, which reduce high-dimensional vectors to two dimensions used as coordinates in a scatterplot. This data transformation step introduces uncertainty that can be propagated to the visual representation and influence users' interpretation of the data. To communicate such uncertainties, we present LayerFlow - a visual analytics workspace that displays embeddings in an interlinked projection design and communicates the transformation, representation, and interpretation uncertainty. In particular, to hint at potential data distortions and uncertainties, the workspace includes several visual components, such as convex hulls showing 2D and HD clusters, data point pairwise distances, cluster summaries, and projection quality metrics. We show the usability of the presented workspace through replication and expert case studies that highlight the need to communicate uncertainty through multiple visual components and different data perspectives.","authors":["Rita Sevastjanova","Robin Gerling","Thilo Spinner","Mennatallah El-Assady"],"url":"https://arxiv.org/abs/2504.10504"}
{"created":"2025-04-16","title":"WorldMove, a global open data for human mobility","abstract":"High-quality human mobility data is crucial for applications such as urban planning, transportation management, and public health, yet its collection is often hindered by privacy concerns and data scarcity-particularly in less-developed regions. To address this challenge, we introduce WorldMove, a large-scale synthetic mobility dataset covering over 1,600 cities across 179 countries and 6 continents. Our method leverages publicly available multi-source data, including gridded population distribution, point-of-interest (POI) maps, and commuting origin-destination (OD) flows-to generate realistic city-scale mobility trajectories using a diffusion-based generative model. The generation process involves defining city boundaries, collecting multi-source input features, and simulating individual-level movements that reflect plausible daily mobility behavior. Comprehensive validation demonstrates that the generated data closely aligns with real-world observations, both in terms of fine-grained individual mobility behavior and city-scale population flows. Alongside the pre-generated datasets, we release the trained model and a complete open-source pipeline, enabling researchers and practitioners to generate custom synthetic mobility data for any city worldwide. This work not only fills critical data gaps, but also lays a global foundation for scalable, privacy-preserving, and inclusive mobility research-empowering data-scarce regions and enabling universal access to human mobility insights.","authors":["Yuan Yuan","Yuheng Zhang","Jingtao Ding","Yong Li"],"url":"https://arxiv.org/abs/2504.10506"}
{"created":"2025-04-16","title":"PinRec: Outcome-Conditioned, Multi-Token Generative Retrieval for Industry-Scale Recommendation Systems","abstract":"Generative retrieval methods utilize generative sequential modeling techniques, such as transformers, to generate candidate items for recommender systems. These methods have demonstrated promising results in academic benchmarks, surpassing traditional retrieval models like two-tower architectures. However, current generative retrieval methods lack the scalability required for industrial recommender systems, and they are insufficiently flexible to satisfy the multiple metric requirements of modern systems.","authors":["Anirudhan Badrinath","Prabhat Agarwal","Laksh Bhasin","Jaewon Yang","Jiajing Xu","Charles Rosenberg"],"url":"https://arxiv.org/abs/2504.10507"}
{"created":"2025-04-16","title":"Poly-Vector Retrieval: Reference and Content Embeddings for Legal Documents","abstract":"Retrieval-Augmented Generation (RAG) has emerged as an effective paradigm for generating contextually accurate answers by integrating Large Language Models (LLMs) with retrieval mechanisms. However, in legal contexts, users frequently reference norms by their labels or nicknames (e.g., Article 5 of the Constitution or Consumer Defense Code (CDC)), rather than by their content, posing challenges for traditional RAG approaches that rely solely on semantic embeddings of text. Furthermore, legal texts themselves heavily rely on explicit cross-references (e.g., \"pursuant to Article 34\") that function as pointers. Both scenarios pose challenges for traditional RAG approaches that rely solely on semantic embeddings of text, often failing to retrieve the necessary referenced content. This paper introduces Poly-Vector Retrieval, a method assigning multiple distinct embeddings to each legal provision: one embedding captures the content (the full text), another captures the label (the identifier or proper name), and optionally additional embeddings capture alternative denominations. Inspired by Frege's distinction between Sense and Reference, this poly-vector retrieval approach treats labels, identifiers and reference markers as rigid designators and content embeddings as carriers of semantic substance. Experiments on the Brazilian Federal Constitution demonstrate that Poly-Vector Retrieval significantly improves retrieval accuracy for label-centric queries and potential to resolve internal and external cross-references, without compromising performance on purely semantic queries. The study discusses philosophical and practical implications of explicitly separating reference from content in vector embeddings and proposes future research directions for applying this approach to broader legal datasets and other domains characterized by explicit reference identifiers.","authors":["Jo\\~ao Alberto de Oliveira Lima"],"url":"https://arxiv.org/abs/2504.10508"}
{"created":"2025-04-16","title":"Beyond Reproducibility: Advancing Zero-shot LLM Reranking Efficiency with Setwise Insertion","abstract":"This study presents a comprehensive reproducibility and extension analysis of the Setwise prompting methodology for zero-shot ranking with Large Language Models (LLMs), as proposed by Zhuang et al. We evaluate its effectiveness and efficiency compared to traditional Pointwise, Pairwise, and Listwise approaches in document ranking tasks. Our reproduction confirms the findings of Zhuang et al., highlighting the trade-offs between computational efficiency and ranking effectiveness in Setwise methods. Building on these insights, we introduce Setwise Insertion, a novel approach that leverages the initial document ranking as prior knowledge, reducing unnecessary comparisons and uncertainty by focusing on candidates more likely to improve the ranking results. Experimental results across multiple LLM architectures (Flan-T5, Vicuna, and Llama) show that Setwise Insertion yields a 31% reduction in query time, a 23% reduction in model inferences, and a slight improvement in reranking effectiveness compared to the original Setwise method. These findings highlight the practical advantage of incorporating prior ranking knowledge into Setwise prompting for efficient and accurate zero-shot document reranking.","authors":["Jakub Podolak","Leon Peric","Mina Janicijevic","Roxana Petcu"],"url":"https://arxiv.org/abs/2504.10509"}
{"created":"2025-04-16","title":"TrustMap: Mapping Truthfulness Stance of Social Media Posts on Factual Claims for Geographical Analysis","abstract":"Factual claims and misinformation circulate widely on social media and affect how people form opinions and make decisions. This paper presents a truthfulness stance map (TrustMap), an application that identifies and maps public stances toward factual claims across U.S. regions. Each social media post is classified as positive, negative, or neutral/no stance, based on whether it believes a factual claim is true or false, expresses uncertainty about the truthfulness, or does not explicitly take a position on the claim's truthfulness. The tool uses a retrieval-augmented model with fine-tuned language models for automatic stance classification. The stance classification results and social media posts are grouped by location to show how stance patterns vary geographically. TrustMap allows users to explore these patterns by claim and region and connects stance detection with geographical analysis to better understand public engagement with factual claims.","authors":["Zhengyuan Zhu","Haiqi Zhang","Zeyu Zhang","Chengkai Li"],"url":"https://arxiv.org/abs/2504.10511"}
{"created":"2025-04-16","title":"JEPA4Rec: Learning Effective Language Representations for Sequential Recommendation via Joint Embedding Predictive Architecture","abstract":"Language representation learning has emerged as a promising approach for sequential recommendation, thanks to its ability to learn generalizable representations. However, despite its advantages, this approach still struggles with data sparsity and a limited understanding of common-sense user preferences. To address these limitations, we propose $\\textbf{JEPA4Rec}$, a framework that combines $\\textbf{J}$oint $\\textbf{E}$mbedding $\\textbf{P}$redictive $\\textbf{A}$rchitecture with language modeling of item textual descriptions. JEPA4Rec captures semantically rich and transferable representations, improving recommendation performance and reducing reliance on large-scale pre-training data. Specifically, JEPA4Rec represents items as text sentences by flattening descriptive information such as $\\textit{title, category}$, and other attributes. To encode these sentences, we employ a bidirectional Transformer encoder with modified embedding layers tailored for capturing item information in recommendation datasets. We apply masking to text sentences and use them to predict the representations of the unmasked sentences, helping the model learn generalizable item embeddings. To further improve recommendation performance and language understanding, we employ a two-stage training strategy incorporating self-supervised learning losses. Experiments on six real-world datasets demonstrate that JEPA4Rec consistently outperforms state-of-the-art methods, particularly in cross-domain, cross-platform, and low-resource scenarios.","authors":["Minh-Anh Nguyen","Dung D. Le"],"url":"https://arxiv.org/abs/2504.10512"}
{"created":"2025-04-16","title":"ColorBench: Can VLMs See and Understand the Colorful World? A Comprehensive Benchmark for Color Perception, Reasoning, and Robustness","abstract":"Color plays an important role in human perception and usually provides critical clues in visual reasoning. However, it is unclear whether and how vision-language models (VLMs) can perceive, understand, and leverage color as humans. This paper introduces ColorBench, an innovative benchmark meticulously crafted to assess the capabilities of VLMs in color understanding, including color perception, reasoning, and robustness. By curating a suite of diverse test scenarios, with grounding in real applications, ColorBench evaluates how these models perceive colors, infer meanings from color-based cues, and maintain consistent performance under varying color transformations. Through an extensive evaluation of 32 VLMs with varying language models and vision encoders, our paper reveals some undiscovered findings: (i) The scaling law (larger models are better) still holds on ColorBench, while the language model plays a more important role than the vision encoder. (ii) However, the performance gaps across models are relatively small, indicating that color understanding has been largely neglected by existing VLMs. (iii) CoT reasoning improves color understanding accuracies and robustness, though they are vision-centric tasks. (iv) Color clues are indeed leveraged by VLMs on ColorBench but they can also mislead models in some tasks. These findings highlight the critical limitations of current VLMs and underscore the need to enhance color comprehension. Our ColorBenchcan serve as a foundational tool for advancing the study of human-level color understanding of multimodal AI.","authors":["Yijun Liang","Ming Li","Chenrui Fan","Ziyue Li","Dang Nguyen","Kwesi Cobbina","Shweta Bhardwaj","Jiuhai Chen","Fuxiao Liu","Tianyi Zhou"],"url":"https://arxiv.org/abs/2504.10514"}
{"created":"2025-04-16","title":"Toward Super Agent System with Hybrid AI Routers","abstract":"AI Agents powered by Large Language Models are transforming the world through enormous applications. A super agent has the potential to fulfill diverse user needs, such as summarization, coding, and research, by accurately understanding user intent and leveraging the appropriate tools to solve tasks. However, to make such an agent viable for real-world deployment and accessible at scale, significant optimizations are required to ensure high efficiency and low cost. This paper presents a design of the Super Agent System. Upon receiving a user prompt, the system first detects the intent of the user, then routes the request to specialized task agents with the necessary tools or automatically generates agentic workflows. In practice, most applications directly serve as AI assistants on edge devices such as phones and robots. As different language models vary in capability and cloud-based models often entail high computational costs, latency, and privacy concerns, we then explore the hybrid mode where the router dynamically selects between local and cloud models based on task complexity. Finally, we introduce the blueprint of an on-device super agent enhanced with cloud. With advances in multi-modality models and edge hardware, we envision that most computations can be handled locally, with cloud collaboration only as needed. Such architecture paves the way for super agents to be seamlessly integrated into everyday life in the near future.","authors":["Yuhang Yao","Haixin Wang","Yibo Chen","Jiawen Wang","Min Chang Jordan Ren","Bosheng Ding","Salman Avestimehr","Chaoyang He"],"url":"https://arxiv.org/abs/2504.10519"}
{"created":"2025-04-16","title":"Integrating Emotion Distribution Networks and Textual Message Analysis for X User Emotional State Classification","abstract":"As the popularity and reach of social networks continue to surge, a vast reservoir of opinions and sentiments across various subjects inundates these platforms. Among these, X social network (formerly Twitter) stands as a juggernaut, boasting approximately 420 million active users. Extracting users' emotional and mental states from their expressed opinions on social media has become a common pursuit. While past methodologies predominantly focused on the textual content of messages to analyze user sentiment, the interactive nature of these platforms suggests a deeper complexity. This study employs hybrid methodologies, integrating textual analysis, profile examination, follower analysis, and emotion dissemination patterns. Initially, user interactions are leveraged to refine emotion classification within messages, encompassing exchanges where users respond to each other. Introducing the concept of a communication tree, a model is extracted to map these interactions. Subsequently, users' bios and interests from this tree are juxtaposed with message text to enrich analysis. Finally, influential figures are identified among users' followers in the communication tree, categorized into different topics to gauge interests. The study highlights that traditional sentiment analysis methodologies, focusing solely on textual content, are inadequate in discerning sentiment towards significant events, notably the presidential election. Comparative analysis with conventional methods reveals a substantial improvement in accuracy with the incorporation of emotion distribution patterns and user profiles. The proposed approach yields a 12% increase in accuracy with emotion distribution patterns and a 15% increase when considering user profiles, underscoring its efficacy in capturing nuanced sentiment dynamics.","authors":["Pardis Moradbeiki","Mohammad Ali Zare Chahooki"],"url":"https://arxiv.org/abs/2504.10521"}
{"created":"2025-04-16","title":"Explainable Artificial Intelligence techniques for interpretation of food datasets: a review","abstract":"Artificial Intelligence (AI) has become essential for analyzing complex data and solving highly-challenging tasks. It is being applied across numerous disciplines beyond computer science, including Food Engineering, where there is a growing demand for accurate and trustworthy predictions to meet stringent food quality standards. However, this requires increasingly complex AI models, raising reliability concerns. In response, eXplainable AI (XAI) has emerged to provide insights into AI decision-making, aiding model interpretation by developers and users. Nevertheless, XAI remains underutilized in Food Engineering, limiting model reliability. For instance, in food quality control, AI models using spectral imaging can detect contaminants or assess freshness levels, but their opaque decision-making process hinders adoption. XAI techniques such as SHAP (Shapley Additive Explanations) and Grad-CAM (Gradient-weighted Class Activation Mapping) can pinpoint which spectral wavelengths or image regions contribute most to a prediction, enhancing transparency and aiding quality control inspectors in verifying AI-generated assessments. This survey presents a taxonomy for classifying food quality research using XAI techniques, organized by data types and explanation methods, to guide researchers in choosing suitable approaches. We also highlight trends, challenges, and opportunities to encourage the adoption of XAI in Food Engineering.","authors":["Leonardo Arrighi","Ingrid Alves de Moraes","Marco Zullich","Michele Simonato","Douglas Fernandes Barbin","Sylvio Barbon Junior"],"url":"https://arxiv.org/abs/2504.10527"}
{"created":"2025-04-16","title":"HeteRAG: A Heterogeneous Retrieval-augmented Generation Framework with Decoupled Knowledge Representations","abstract":"Retrieval-augmented generation (RAG) methods can enhance the performance of LLMs by incorporating retrieved knowledge chunks into the generation process. In general, the retrieval and generation steps usually have different requirements for these knowledge chunks. The retrieval step benefits from comprehensive information to improve retrieval accuracy, whereas excessively long chunks may introduce redundant contextual information, thereby diminishing both the effectiveness and efficiency of the generation process. However, existing RAG methods typically employ identical representations of knowledge chunks for both retrieval and generation, resulting in suboptimal performance. In this paper, we propose a heterogeneous RAG framework (\\myname) that decouples the representations of knowledge chunks for retrieval and generation, thereby enhancing the LLMs in both effectiveness and efficiency. Specifically, we utilize short chunks to represent knowledge to adapt the generation step and utilize the corresponding chunk with its contextual information from multi-granular views to enhance retrieval accuracy. We further introduce an adaptive prompt tuning method for the retrieval model to adapt the heterogeneous retrieval augmented generation process. Extensive experiments demonstrate that \\myname achieves significant improvements compared to baselines.","authors":["Peiru Yang","Xintian Li","Zhiyang Hu","Jiapeng Wang","Jinhua Yin","Huili Wang","Lizhi He","Shuai Yang","Shangguang Wang","Yongfeng Huang","Tao Qi"],"url":"https://arxiv.org/abs/2504.10529"}
{"created":"2025-04-16","title":"Where Should I Deploy My Contracts? A Practical Experience Report","abstract":"Blockchain networks provide a reliable trust anchor to decentralized applications (DApps) backed by smart contracts. The Ethereum ecosystem now encompasses most blockchain networks that provide compatible support for smart contracts code. Recently, many Ethereum Layer 2 (L2) rollup solutions emerged, meant to scale the base Layer 1 (L1) network, consequently decreasing transaction fees and diversifying the usage scenarios. Furthermore, the number of blockchain providers that offer access to the network infrastructure for both L1 and L2 continuously increases. A developer is faced with a multitude of deployment options and must weigh between the gains in costs and the losses in trust that are still an issue with L2. A decisive factor in this trade-off can be the use case itself, depending on its security requirements. Still, the evaluation of costs and performance cannot be ignored and should rely on a set of measurable metrics, although choosing the right metrics can be complicated. In this practical experience report, we explore the relevance of several such metrics in choosing between different providers and rollups. For this purpose, we perform evaluations for two use cases of DApps: a voting DApp with high security demands, suited for L1 deployment, and a cost-sensitive supply chain DApp, where L2 can be an option. We analyze a set of basic metrics by comparing these between two highly used access providers, Alchemy and Infura, for the L1 deployment case, and between two of the most popular rollups, Arbitrum One and OP Mainnet (Optimism), for the L2 deployment scenario.","authors":["C\\u{a}t\\u{a}lina Laz\\u{a}r","Gabriela Secrieru","Emanuel Onica"],"url":"https://arxiv.org/abs/2504.10535"}
{"created":"2025-04-16","title":"Federated Learning with Layer Skipping: Efficient Training of Large Language Models for Healthcare NLP","abstract":"Federated learning (FL) enables collaborative model training across organizations without sharing raw data, addressing crucial privacy concerns in healthcare natural language processing (NLP). However, training large language models (LLMs) in federated settings faces significant challenges, including communication overhead and data heterogeneity. We propose Layer-Skipping Federated Learning, where only selected layers of a pre-trained LLM are fine-tuned across clients while others remain frozen. Applied to LLaMA 3.2-1B, our approach reduces communication costs by approximately 70% while maintaining performance within 2% of centralized training. We evaluate our method on clinical NER and classification tasks using i2b2 and MIMIC-III datasets. Our experiments demonstrate that Layer-Skipping FL outperforms competitive baselines, handles non-IID clinical data distributions effectively, and shows robustness when combined with differential privacy. This approach represents a practical solution for privacy-preserving collaborative learning in healthcare NLP.","authors":["Lihong Zhang","Yue Li"],"url":"https://arxiv.org/abs/2504.10536"}
{"created":"2025-04-16","title":"Distilling Transitional Pattern to Large Language Models for Multimodal Session-based Recommendation","abstract":"Session-based recommendation (SBR) predicts the next item based on anonymous sessions. Traditional SBR explores user intents based on ID collaborations or auxiliary content. To further alleviate data sparsity and cold-start issues, recent Multimodal SBR (MSBR) methods utilize simplistic pre-trained models for modality learning but have limitations in semantic richness. Considering semantic reasoning abilities of Large Language Models (LLM), we focus on the LLM-enhanced MSBR scenario in this paper, which leverages LLM cognition for comprehensive multimodal representation generation, to enhance downstream MSBR. Tackling this problem faces two challenges: i) how to obtain LLM cognition on both transitional patterns and inherent multimodal knowledge, ii) how to align both features into one unified LLM, minimize discrepancy while maximizing representation utility. To this end, we propose a multimodal LLM-enhanced framework TPAD, which extends a distillation paradigm to decouple and align transitional patterns for promoting MSBR. TPAD establishes parallel Knowledge-MLLM and Transfer-MLLM, where the former interprets item knowledge-reflected features and the latter extracts transition-aware features underneath sessions. A transitional pattern alignment module harnessing mutual information estimation theory unites two MLLMs, alleviating distribution discrepancy and distilling transitional patterns into modal representations. Extensive experiments on real-world datasets demonstrate the effectiveness of our framework.","authors":["Jiajie Su","Qiyong Zhong","Yunshan Ma","Weiming Liu","Chaochao Chen","Xiaolin Zheng","Jianwei Yin","Tat-Seng Chua"],"url":"https://arxiv.org/abs/2504.10538"}
{"created":"2025-04-16","title":"Multi-Modal Hypergraph Enhanced LLM Learning for Recommendation","abstract":"The burgeoning presence of Large Language Models (LLM) is propelling the development of personalized recommender systems. Most existing LLM-based methods fail to sufficiently explore the multi-view graph structure correlations inherent in recommendation scenarios. To this end, we propose a novel framework, Hypergraph Enhanced LLM Learning for multimodal Recommendation (HeLLM), designed to equip LLMs with the capability to capture intricate higher-order semantic correlations by fusing graph-level contextual signals with sequence-level behavioral patterns. In the recommender pre-training phase, we design a user hypergraph to uncover shared interest preferences among users and an item hypergraph to capture correlations within multimodal similarities among items. The hypergraph convolution and synergistic contrastive learning mechanism are introduced to enhance the distinguishability of learned representations. In the LLM fine-tuning phase, we inject the learned graph-structured embeddings directly into the LLM's architecture and integrate sequential features capturing each user's chronological behavior. This process enables hypergraphs to leverage graph-structured information as global context, enhancing the LLM's ability to perceive complex relational patterns and integrate multimodal information, while also modeling local temporal dynamics. Extensive experiments demonstrate the superiority of our proposed method over state-of-the-art baselines, confirming the advantages of fusing hypergraph-based context with sequential user behavior in LLMs for recommendation.","authors":["Xu Guo","Tong Zhang","Yuanzhi Wang","Chenxu Wang","Fuyun Wang","Xudong Wang","Xiaoya Zhang","Xin Liu","Zhen Cui"],"url":"https://arxiv.org/abs/2504.10541"}
{"created":"2025-04-16","title":"Integrating Textual Embeddings from Contrastive Learning with Generative Recommender for Enhanced Personalization","abstract":"Recent advances in recommender systems have highlighted the complementary strengths of generative modeling and pretrained language models. We propose a hybrid framework that augments the Hierarchical Sequential Transduction Unit (HSTU) generative recommender with BLaIR -- a contrastive text embedding model. This integration enriches item representations with semantic signals from textual metadata while preserving HSTU's powerful sequence modeling capabilities.","authors":["Yijun Liu"],"url":"https://arxiv.org/abs/2504.10545"}
{"created":"2025-04-16","title":"Automated Testing of COBOL to Java Transformation","abstract":"Recent advances in Large Language Model (LLM) based Generative AI techniques have made it feasible to translate enterprise-level code from legacy languages such as COBOL to modern languages such as Java or Python. While the results of LLM-based automatic transformation are encouraging, the resulting code cannot be trusted to correctly translate the original code, making manual validation of translated Java code from COBOL a necessary but time-consuming and labor-intensive process. In this paper, we share our experience of developing a testing framework for IBM Watsonx Code Assistant for Z (WCA4Z) [5], an industrial tool designed for COBOL to Java translation. The framework automates the process of testing the functional equivalence of the translated Java code against the original COBOL programs in an industry context. Our framework uses symbolic execution to generate unit tests for COBOL, mocking external calls and transforming them into JUnit tests to validate semantic equivalence with translated Java. The results not only help identify and repair any detected discrepancies but also provide feedback to improve the AI model.","authors":["Sandeep Hans","Atul Kumar","Toshikai Yasue","Kouichi Ono","Saravanan Krishnan","Devika Sondhi","Fumiko Satoh","Gerald Mitchell","Sachin Kumar","Diptikalyan Saha"],"url":"https://arxiv.org/abs/2504.10548"}
{"created":"2025-04-16","title":"MiMu: Mitigating Multiple Shortcut Learning Behavior of Transformers","abstract":"Empirical Risk Minimization (ERM) models often rely on spurious correlations between features and labels during the learning process, leading to shortcut learning behavior that undermines robustness generalization performance. Current research mainly targets identifying or mitigating a single shortcut; however, in real-world scenarios, cues within the data are diverse and unknown. In empirical studies, we reveal that the models rely to varying extents on different shortcuts. Compared to weak shortcuts, models depend more heavily on strong shortcuts, resulting in their poor generalization ability. To address these challenges, we propose MiMu, a novel method integrated with Transformer-based ERMs designed to Mitigate Multiple shortcut learning behavior, which incorporates self-calibration strategy and self-improvement strategy. In the source model, we preliminarily propose the self-calibration strategy to prevent the model from relying on shortcuts and make overconfident predictions. Then, we further design self-improvement strategy in target model to reduce the reliance on multiple shortcuts. The random mask strategy involves randomly masking partial attention positions to diversify the focus of target model other than concentrating on a fixed region. Meanwhile, the adaptive attention alignment module facilitates the alignment of attention weights to the calibrated source model, without the need for post-hoc attention maps or supervision. Finally, extensive experiments conducted on Natural Language Processing (NLP) and Computer Vision (CV) demonstrate the effectiveness of MiMu in improving robustness generalization abilities.","authors":["Lili Zhao","Qi Liu","Wei Chen","Liyi Chen","Ruijun Sun","Min Hou","Yang Wang","Shijin Wang"],"url":"https://arxiv.org/abs/2504.10551"}
{"created":"2025-04-16","title":"LEMUR Neural Network Dataset: Towards Seamless AutoML","abstract":"Neural networks are fundamental in artificial intelligence, driving progress in computer vision and natural language processing. High-quality datasets are crucial for their development, and there is growing interest in datasets composed of neural networks themselves to support benchmarking, automated machine learning (AutoML), and model analysis. We introduce LEMUR, an open source dataset of neural network models with well-structured code for diverse architectures across tasks such as object detection, image classification, segmentation, and natural language processing. LEMUR is primarily designed to enable fine-tuning of large language models (LLMs) for AutoML tasks, providing a rich source of structured model representations and associated performance data. Leveraging Python and PyTorch, LEMUR enables seamless extension to new datasets and models while maintaining consistency. It integrates an Optuna-powered framework for evaluation, hyperparameter optimization, statistical analysis, and graphical insights. LEMUR provides an extension that enables models to run efficiently on edge devices, facilitating deployment in resource-constrained environments. Providing tools for model evaluation, preprocessing, and database management, LEMUR supports researchers and practitioners in developing, testing, and analyzing neural networks. Additionally, it offers an API that delivers comprehensive information about neural network models and their complete performance statistics with a single request, which can be used in experiments with code-generating large language models. The LEMUR will be released as an open source project under the MIT license upon acceptance of the paper.","authors":["Arash Torabi Goodarzi","Roman Kochnev","Waleed Khalid","Furui Qin","Tolgay Atinc Uzun","Yashkumar Sanjaybhai Dhameliya","Yash Kanubhai Kathiriya","Zofia Antonina Bentyn","Dmitry Ignatov","Radu Timofte"],"url":"https://arxiv.org/abs/2504.10552"}
{"created":"2025-04-16","title":"Beyond the Generative Learning Trilemma: Generative Model Assessment in Data Scarcity Domains","abstract":"Data scarcity remains a critical bottleneck impeding technological advancements across various domains, including but not limited to medicine and precision agriculture. To address this challenge, we explore the potential of Deep Generative Models (DGMs) in producing synthetic data that satisfies the Generative Learning Trilemma: fidelity, diversity, and sampling efficiency. However, recognizing that these criteria alone are insufficient for practical applications, we extend the trilemma to include utility, robustness, and privacy, factors crucial for ensuring the applicability of DGMs in real-world scenarios. Evaluating these metrics becomes particularly challenging in data-scarce environments, as DGMs traditionally rely on large datasets to perform optimally. This limitation is especially pronounced in domains like medicine and precision agriculture, where ensuring acceptable model performance under data constraints is vital. To address these challenges, we assess the Generative Learning Trilemma in data-scarcity settings using state-of-the-art evaluation metrics, comparing three prominent DGMs: Variational Autoencoders (VAEs), Generative Adversarial Networks (GANs), and Diffusion Models (DMs). Furthermore, we propose a comprehensive framework to assess utility, robustness, and privacy in synthetic data generated by DGMs. Our findings demonstrate varying strengths among DGMs, with each model exhibiting unique advantages based on the application context. This study broadens the scope of the Generative Learning Trilemma, aligning it with real-world demands and providing actionable guidance for selecting DGMs tailored to specific applications.","authors":["Marco Salm\\`e","Lorenzo Tronchin","Rosa Sicilia","Paolo Soda","Valerio Guarrasi"],"url":"https://arxiv.org/abs/2504.10555"}
{"created":"2025-04-16","title":"VAE-based Feature Disentanglement for Data Augmentation and Compression in Generalized GNSS Interference Classification","abstract":"Distributed learning and Edge AI necessitate efficient data processing, low-latency communication, decentralized model training, and stringent data privacy to facilitate real-time intelligence on edge devices while reducing dependency on centralized infrastructure and ensuring high model performance. In the context of global navigation satellite system (GNSS) applications, the primary objective is to accurately monitor and classify interferences that degrade system performance in distributed environments, thereby enhancing situational awareness. To achieve this, machine learning (ML) models can be deployed on low-resource devices, ensuring minimal communication latency and preserving data privacy. The key challenge is to compress ML models while maintaining high classification accuracy. In this paper, we propose variational autoencoders (VAEs) for disentanglement to extract essential latent features that enable accurate classification of interferences. We demonstrate that the disentanglement approach can be leveraged for both data compression and data augmentation by interpolating the lower-dimensional latent representations of signal power. To validate our approach, we evaluate three VAE variants - vanilla, factorized, and conditional generative - on four distinct datasets, including two collected in controlled indoor environments and two real-world highway datasets. Additionally, we conduct extensive hyperparameter searches to optimize performance. Our proposed VAE achieves a data compression rate ranging from 512 to 8,192 and achieves an accuracy up to 99.92%.","authors":["Lucas Heublein","Simon Kocher","Tobias Feigl","Alexander R\\\"ugamer","Christopher Mutschler","Felix Ott"],"url":"https://arxiv.org/abs/2504.10556"}
{"created":"2025-04-16","title":"The Code Barrier: What LLMs Actually Understand?","abstract":"Understanding code represents a core ability needed for automating software development tasks. While foundation models like LLMs show impressive results across many software engineering challenges, the extent of their true semantic understanding beyond simple token recognition remains unclear. This research uses code obfuscation as a structured testing framework to evaluate LLMs' semantic understanding capabilities. We methodically apply controlled obfuscation changes to source code and measure comprehension through two complementary tasks: generating accurate descriptions of obfuscated code and performing deobfuscation, a skill with important implications for reverse engineering applications.","authors":["Serge Lionel Nikiema","Jordan Samhi","Abdoul Kader Kabor\\'e","Jacques Klein","Tegawend\\'e F. Bissyand\\'e"],"url":"https://arxiv.org/abs/2504.10557"}
{"created":"2025-04-16","title":"Enhancing Image Restoration through Learning Context-Rich and Detail-Accurate Features","abstract":"Image restoration involves recovering high-quality images from their corrupted versions, requiring a nuanced balance between spatial details and contextual information. While certain methods address this balance, they predominantly emphasize spatial aspects, neglecting frequency variation comprehension. In this paper, we present a multi-scale design that optimally balances these competing objectives, seamlessly integrating spatial and frequency domain knowledge to selectively recover the most informative information. Specifically, we develop a hybrid scale frequency selection block (HSFSBlock), which not only captures multi-scale information from the spatial domain, but also selects the most informative components for image restoration in the frequency domain. Furthermore, to mitigate the inherent noise introduced by skip connections employing only addition or concatenation, we introduce a skip connection attention mechanism (SCAM) to selectively determines the information that should propagate through skip connections. The resulting tightly interlinked architecture, named as LCDNet. Extensive experiments conducted across diverse image restoration tasks showcase that our model attains performance levels that are either superior or comparable to those of state-of-the-art algorithms.","authors":["Hu Gao","Depeng Dang"],"url":"https://arxiv.org/abs/2504.10558"}
{"created":"2025-04-16","title":"Efficient Process Reward Model Training via Active Learning","abstract":"Process Reward Models (PRMs) provide step-level supervision to large language models (LLMs), but scaling up training data annotation remains challenging for both humans and LLMs. To address this limitation, we propose an active learning approach, ActPRM, which proactively selects the most uncertain samples for training, substantially reducing labeling costs. During training, we use the PRM to estimate uncertainty after the forward pass, retaining only highly uncertain data. A capable yet costly reasoning model then labels this data. Then we compute the loss with respect to the labels and update the PRM's weights. We compare ActPRM vs. vanilla fine-tuning, on a pool-based active learning setting, demonstrating that ActPRM reduces 50% annotation, but achieving the comparable or even better performance. Beyond annotation efficiency, we further advance the actively trained PRM by filtering over 1M+ math reasoning trajectories with ActPRM, retaining 60% of the data. A subsequent training on this selected dataset yields a new state-of-the-art (SOTA) PRM on ProcessBench (75.0%) and PRMBench (65.5%) compared with same sized models.","authors":["Keyu Duan","Zichen Liu","Xin Mao","Tianyu Pang","Changyu Chen","Qiguang Chen","Michael Qizhe Shieh","Longxu Dou"],"url":"https://arxiv.org/abs/2504.10559"}
{"created":"2025-04-16","title":"Self-Controlled Dynamic Expansion Model for Continual Learning","abstract":"Continual Learning (CL) epitomizes an advanced training paradigm wherein prior data samples remain inaccessible during the acquisition of new tasks. Numerous investigations have delved into leveraging a pre-trained Vision Transformer (ViT) to enhance model efficacy in continual learning. Nonetheless, these approaches typically utilize a singular, static backbone, which inadequately adapts to novel tasks, particularly when engaging with diverse data domains, due to a substantial number of inactive parameters. This paper addresses this limitation by introducing an innovative Self-Controlled Dynamic Expansion Model (SCDEM), which orchestrates multiple distinct trainable pre-trained ViT backbones to furnish diverse and semantically enriched representations. Specifically, by employing the multi-backbone architecture as a shared module, the proposed SCDEM dynamically generates a new expert with minimal parameters to accommodate a new task. A novel Collaborative Optimization Mechanism (COM) is introduced to synergistically optimize multiple backbones by harnessing prediction signals from historical experts, thereby facilitating new task learning without erasing previously acquired knowledge. Additionally, a novel Feature Distribution Consistency (FDC) approach is proposed to align semantic similarity between previously and currently learned representations through an optimal transport distance-based mechanism, effectively mitigating negative knowledge transfer effects. Furthermore, to alleviate over-regularization challenges, this paper presents a novel Dynamic Layer-Wise Feature Attention Mechanism (DLWFAM) to autonomously determine the penalization intensity on each trainable representation layer. An extensive series of experiments have been conducted to evaluate the proposed methodology's efficacy, with empirical results corroborating that the approach attains state-of-the-art performance.","authors":["Runqing Wu","Fei Ye","Rongyao Hu","Guoxi Huang"],"url":"https://arxiv.org/abs/2504.10561"}
{"created":"2025-04-16","title":"Data Augmentation Through Random Style Replacement","abstract":"In this paper, we introduce a novel data augmentation technique that combines the advantages of style augmentation and random erasing by selectively replacing image subregions with style-transferred patches. Our approach first applies a random style transfer to training images, then randomly substitutes selected areas of these images with patches derived from the style-transferred versions. This method is able to seamlessly accommodate a wide range of existing style transfer algorithms and can be readily integrated into diverse data augmentation pipelines. By incorporating our strategy, the training process becomes more robust and less prone to overfitting. Comparative experiments demonstrate that, relative to previous style augmentation methods, our technique achieves superior performance and faster convergence.","authors":["Qikai Yang","Cheng Ji","Huaiying Luo","Panfeng Li","Zhicheng Ding"],"url":"https://arxiv.org/abs/2504.10563"}
{"created":"2025-04-16","title":"H3AE: High Compression, High Speed, and High Quality AutoEncoder for Video Diffusion Models","abstract":"Autoencoder (AE) is the key to the success of latent diffusion models for image and video generation, reducing the denoising resolution and improving efficiency. However, the power of AE has long been underexplored in terms of network design, compression ratio, and training strategy. In this work, we systematically examine the architecture design choices and optimize the computation distribution to obtain a series of efficient and high-compression video AEs that can decode in real time on mobile devices. We also unify the design of plain Autoencoder and image-conditioned I2V VAE, achieving multifunctionality in a single network. In addition, we find that the widely adopted discriminative losses, i.e., GAN, LPIPS, and DWT losses, provide no significant improvements when training AEs at scale. We propose a novel latent consistency loss that does not require complicated discriminator design or hyperparameter tuning, but provides stable improvements in reconstruction quality. Our AE achieves an ultra-high compression ratio and real-time decoding speed on mobile while outperforming prior art in terms of reconstruction metrics by a large margin. We finally validate our AE by training a DiT on its latent space and demonstrate fast, high-quality text-to-video generation capability.","authors":["Yushu Wu","Yanyu Li","Ivan Skorokhodov","Anil Kag","Willi Menapace","Sharath Girish","Aliaksandr Siarohin","Yanzhi Wang","Sergey Tulyakov"],"url":"https://arxiv.org/abs/2504.10567"}
{"created":"2025-04-16","title":"AgMMU: A Comprehensive Agricultural Multimodal Understanding and Reasoning Benchmark","abstract":"We curate a dataset AgMMU for evaluating and developing vision-language models (VLMs) to produce factually accurate answers for knowledge-intensive expert domains. Our AgMMU concentrates on one of the most socially beneficial domains, agriculture, which requires connecting detailed visual observation with precise knowledge to diagnose, e.g., pest identification, management instructions, etc. As a core uniqueness of our dataset, all facts, questions, and answers are extracted from 116,231 conversations between real-world users and authorized agricultural experts. After a three-step dataset curation pipeline with GPT-4o, LLaMA models, and human verification, AgMMU features an evaluation set of 5,460 multiple-choice questions (MCQs) and open-ended questions (OEQs). We also provide a development set that contains 205,399 pieces of agricultural knowledge information, including disease identification, symptoms descriptions, management instructions, insect and pest identification, and species identification. As a multimodal factual dataset, it reveals that existing VLMs face significant challenges with questions requiring both detailed perception and factual knowledge. Moreover, open-source VLMs still demonstrate a substantial performance gap compared to proprietary ones. To advance knowledge-intensive VLMs, we conduct fine-tuning experiments using our development set, which improves LLaVA-1.5 evaluation accuracy by up to 3.1%. We hope that AgMMU can serve both as an evaluation benchmark dedicated to agriculture and a development suite for incorporating knowledge-intensive expertise into general-purpose VLMs.","authors":["Aruna Gauba","Irene Pi","Yunze Man","Ziqi Pang","Vikram S. Adve","Yu-Xiong Wang"],"url":"https://arxiv.org/abs/2504.10568"}
{"created":"2025-04-16","title":"Demo: ViolentUTF as An Accessible Platform for Generative AI Red Teaming","abstract":"The rapid integration of Generative AI (GenAI) into various applications necessitates robust risk management strategies which includes Red Teaming (RT) - an evaluation method for simulating adversarial attacks. Unfortunately, RT for GenAI is often hindered by technical complexity, lack of user-friendly interfaces, and inadequate reporting features. This paper introduces Violent UTF - an accessible, modular, and scalable platform for GenAI red teaming. Through intuitive interfaces (Web GUI, CLI, API, MCP) powered by LLMs and for LLMs, Violent UTF aims to empower non-technical domain experts and students alongside technical experts, facilitate comprehensive security evaluation by unifying capabilities from RT frameworks like Microsoft PyRIT, Nvidia Garak and its own specialized evaluators. ViolentUTF is being used for evaluating the robustness of a flagship LLM-based product in a large US Government department. It also demonstrates effectiveness in evaluating LLMs' cross-domain reasoning capability between cybersecurity and behavioral psychology.","authors":["Tam n. Nguyen"],"url":"https://arxiv.org/abs/2504.10603"}
{"created":"2025-04-16","title":"Finding Pathways in Reaction Networks guided by Energy Barriers using Integer Linear Programming","abstract":"Analyzing synthesis pathways for target molecules in a chemical reaction network annotated with information on the kinetics of individual reactions is an area of active study. This work presents a computational methodology for searching for pathways in reaction networks which is based on integer linear programming and the modeling of reaction networks by directed hypergraphs. Often multiple pathways fit the given search criteria. To rank them, we develop an objective function based on physical arguments maximizing the probability of the pathway. We furthermore develop an automated pipeline to estimate the energy barriers of individual reactions in reaction networks. Combined, the methodology facilitates flexible and kinetically informed pathway investigations on large reaction networks by computational means, even for networks coming without kinetic annotation, such as those created via generative approaches for expanding molecular spaces.","authors":["Adittya Pal","Rolf Fagerberg","Jakob Lykke Andersen","Peter Dittrich","Daniel Merkle"],"url":"https://arxiv.org/abs/2504.10609"}
{"created":"2025-04-16","title":"Energy Matching: Unifying Flow Matching and Energy-Based Models for Generative Modeling","abstract":"Generative models often map noise to data by matching flows or scores, but these approaches become cumbersome for incorporating partial observations or additional priors. Inspired by recent advances in Wasserstein gradient flows, we propose Energy Matching, a framework that unifies flow-based approaches with the flexibility of energy-based models (EBMs). Far from the data manifold, samples move along curl-free, optimal transport paths from noise to data. As they approach the data manifold, an entropic energy term guides the system into a Boltzmann equilibrium distribution, explicitly capturing the underlying likelihood structure of the data. We parameterize this dynamic with a single time-independent scalar field, which serves as both a powerful generator and a flexible prior for effective regularization of inverse problems. Our method substantially outperforms existing EBMs on CIFAR-10 generation (FID 3.97 compared to 8.61), while retaining the simulation-free training of transport-based approaches away from the data manifold. Additionally, we exploit the flexibility of our method and introduce an interaction energy for diverse mode exploration. Our approach focuses on learning a static scalar potential energy -- without time conditioning, auxiliary generators, or additional networks -- marking a significant departure from recent EBM methods. We believe this simplified framework significantly advances EBM capabilities and paves the way for their broader adoption in generative modeling across diverse domains.","authors":["Michal Balcerak","Tamaz Amiranashvili","Suprosanna Shit","Antonio Terpin","Sebastian Kaltenbach","Petros Koumoutsakos","Bjoern Menze"],"url":"https://arxiv.org/abs/2504.10612"}
{"created":"2025-04-16","title":"Enhancing Document Retrieval for Curating N-ary Relations in Knowledge Bases","abstract":"Curation of biomedical knowledge bases (KBs) relies on extracting accurate multi-entity relational facts from the literature - a process that remains largely manual and expert-driven. An essential step in this workflow is retrieving documents that can support or complete partially observed n-ary relations. We present a neural retrieval model designed to assist KB curation by identifying documents that help fill in missing relation arguments and provide relevant contextual evidence.","authors":["Xing David Wang","Ulf Leser"],"url":"https://arxiv.org/abs/2504.10613"}
{"created":"2025-04-16","title":"Beyond Chains of Thought: Benchmarking Latent-Space Reasoning Abilities in Large Language Models","abstract":"Large language models (LLMs) can perform reasoning computations both internally within their latent space and externally by generating explicit token sequences like chains of thought. Significant progress in enhancing reasoning abilities has been made by scaling test-time compute. However, understanding and quantifying model-internal reasoning abilities - the inferential \"leaps\" models make between individual token predictions - remains crucial. This study introduces a benchmark (n = 4,000 items) designed to quantify model-internal reasoning in different domains. We achieve this by having LLMs indicate the correct solution to reasoning problems not through descriptive text, but by selecting a specific language of their initial response token that is different from English, the benchmark language. This not only requires models to reason beyond their context window, but also to overrise their default tendency to respond in the same language as the prompt, thereby posing an additional cognitive strain. We evaluate a set of 18 LLMs, showing significant performance variations, with GPT-4.5 achieving the highest accuracy (74.7%), outperforming models like Grok-2 (67.2%), and Llama 3.1 405B (65.6%). Control experiments and difficulty scaling analyses suggest that while LLMs engage in internal reasoning, we cannot rule out heuristic exploitations under certain conditions, marking an area for future investigation. Our experiments demonstrate that LLMs can \"think\" via latent-space computations, revealing model-internal inference strategies that need further understanding, especially regarding safety-related concerns such as covert planning, goal-seeking, or deception emerging without explicit token traces.","authors":["Thilo Hagendorff","Sarah Fabi"],"url":"https://arxiv.org/abs/2504.10615"}
{"created":"2025-04-16","title":"SPreV","abstract":"SPREV, short for hyperSphere Reduced to two-dimensional Regular Polygon for Visualisation, is a novel dimensionality reduction technique developed to address the challenges of reducing dimensions and visualizing labeled datasets that exhibit a unique combination of three characteristics: small class size, high dimensionality, and low sample size. SPREV is designed not only to uncover but also to visually represent hidden patterns within such datasets. Its distinctive integration of geometric principles, adapted for discrete computational environments, makes it an indispensable tool in the modern data science toolkit, enabling users to identify trends, extract insights, and navigate complex data efficiently and effectively.","authors":["Srivathsan Amruth"],"url":"https://arxiv.org/abs/2504.10620"}
{"created":"2025-04-16","title":"Improving Upon the generalized c-mu rule: a Whittle approach","abstract":"Scheduling a stream of jobs whose holding cost changes over time is a classic and practical problem. Specifically, each job is associated with a holding cost (penalty), where a job's instantaneous holding cost is some increasing function of its class and current age (the time it has spent in the system since its arrival). The goal is to schedule the jobs to minimize the time-average total holding cost across all jobs.","authors":["Zhouzi Li","Keerthana Gurushankar","Mor Harchol-Balter","Alan Scheller-Wolf"],"url":"https://arxiv.org/abs/2504.10622"}
{"created":"2025-04-16","title":"A Real-Time, Auto-Regression Method for In-Situ Feature Extraction in Hydrodynamics Simulations","abstract":"Hydrodynamics simulations are powerful tools for studying fluid behavior under physical forces, enabling extraction of features that reveal key flow characteristics. Traditional post-analysis methods offer high accuracy but incur significant computational and I/O costs. In contrast, in-situ methods reduce data movement by analyzing data during the simulation, yet often compromise either accuracy or performance. We propose a lightweight auto-regression algorithm for real-time in-situ feature extraction. It applies curve-fitting to temporal and spatial data, reducing data volume and minimizing simulation overhead. The model is trained incrementally using mini-batches, ensuring responsiveness and low computational cost. To facilitate adoption, we provide a flexible library with simple APIs for easy integration into existing workflows. We evaluate the method on simulations of material deformation and white dwarf (WD) mergers, extracting features such as shock propagation and delay-time distribution. Results show high accuracy (94.44%-99.60%) and low performance impact (0.11%-4.95%) demonstrating the method's effectiveness for accurate and efficient in-situ analysis.","authors":["Kewei Yan","Yonghong Yan"],"url":"https://arxiv.org/abs/2504.10632"}
{"created":"2025-04-16","title":"Skeleton-Based Intake Gesture Detection With Spatial-Temporal Graph Convolutional Networks","abstract":"Overweight and obesity have emerged as widespread societal challenges, frequently linked to unhealthy eating patterns. A promising approach to enhance dietary monitoring in everyday life involves automated detection of food intake gestures. This study introduces a skeleton based approach using a model that combines a dilated spatial-temporal graph convolutional network (ST-GCN) with a bidirectional long-short-term memory (BiLSTM) framework, as called ST-GCN-BiLSTM, to detect intake gestures. The skeleton-based method provides key benefits, including environmental robustness, reduced data dependency, and enhanced privacy preservation. Two datasets were employed for model validation. The OREBA dataset, which consists of laboratory-recorded videos, achieved segmental F1-scores of 86.18% and 74.84% for identifying eating and drinking gestures. Additionally, a self-collected dataset using smartphone recordings in more adaptable experimental conditions was evaluated with the model trained on OREBA, yielding F1-scores of 85.40% and 67.80% for detecting eating and drinking gestures. The results not only confirm the feasibility of utilizing skeleton data for intake gesture detection but also highlight the robustness of the proposed approach in cross-dataset validation.","authors":["Chunzhuo Wang","Zhewen Xue","T. Sunil Kumar","Guido Camps","Hans Hallez","Bart Vanrumste"],"url":"https://arxiv.org/abs/2504.10635"}
{"created":"2025-04-16","title":"Better Estimation of the KL Divergence Between Language Models","abstract":"Estimating the Kullback--Leibler (KL) divergence between language models has many applications, e.g., reinforcement learning from human feedback (RLHF), interpretability, and knowledge distillation. However, computing the exact KL divergence between two arbitrary language models is intractable. Thus, practitioners often resort to the use of sampling-based estimators. While it is easy to fashion a simple Monte Carlo (MC) estimator that provides an unbiased estimate of the KL divergence between language models, this estimator notoriously suffers from high variance, and can even result in a negative estimate of the KL divergence, a non-negative quantity. In this paper, we introduce a Rao--Blackwellized estimator that is also unbiased and provably has variance less than or equal to that of the standard Monte Carlo estimator. In an empirical study on sentiment-controlled fine-tuning, we show that our estimator provides more stable KL estimates and reduces variance substantially in practice. Additionally, we derive an analogous Rao--Blackwellized estimator of the gradient of the KL divergence, which leads to more stable training and produces models that more frequently appear on the Pareto frontier of reward vs. KL compared to the ones trained with the MC estimator of the gradient.","authors":["Afra Amini","Tim Vieira","Ryan Cotterell"],"url":"https://arxiv.org/abs/2504.10637"}
{"created":"2025-04-16","title":"Secure Estimation of Battery Voltage Under Sensor Attacks: A Self-Learning Koopman Approach","abstract":"Cloud-based battery management system (BMS) requires accurate terminal voltage measurement data to ensure optimal and safe charging of Lithium-ion batteries. Unfortunately, an adversary can corrupt the battery terminal voltage data as it passes from the local-BMS to the cloud-BMS through the communication network, with the objective of under- or over-charging the battery. To ensure accurate terminal voltage data under such malicious sensor attacks, this paper investigates a Koopman-based secure terminal voltage estimation scheme using a two-stage error-compensated self-learning feedback. During the first stage of error correction, the potential Koopman prediction error is estimated to compensate for the error accumulation due to the linear approximation of Koopman operator. The second stage of error compensation aims to recover the error amassing from the higher-order dynamics of the Lithium-ion batteries missed by the self-learning strategy. Specifically, we have proposed two different methods for this second stage error compensation. First, an interpretable empirical correction strategy has been obtained using the open circuit voltage to state-of-charge mapping for the battery. Second, a Gaussian process regression-based data-driven method has been explored. Finally, we demonstrate the efficacy of the proposed secure estimator using both empirical and data-driven corrections.","authors":["Sanchita Ghosh","Tanushree Roy"],"url":"https://arxiv.org/abs/2504.10639"}
{"created":"2025-04-16","title":"SilVar-Med: A Speech-Driven Visual Language Model for Explainable Abnormality Detection in Medical Imaging","abstract":"Medical Visual Language Models have shown great potential in various healthcare applications, including medical image captioning and diagnostic assistance. However, most existing models rely on text-based instructions, limiting their usability in real-world clinical environments especially in scenarios such as surgery, text-based interaction is often impractical for physicians. In addition, current medical image analysis models typically lack comprehensive reasoning behind their predictions, which reduces their reliability for clinical decision-making. Given that medical diagnosis errors can have life-changing consequences, there is a critical need for interpretable and rational medical assistance. To address these challenges, we introduce an end-to-end speech-driven medical VLM, SilVar-Med, a multimodal medical image assistant that integrates speech interaction with VLMs, pioneering the task of voice-based communication for medical image analysis. In addition, we focus on the interpretation of the reasoning behind each prediction of medical abnormalities with a proposed reasoning dataset. Through extensive experiments, we demonstrate a proof-of-concept study for reasoning-driven medical image interpretation with end-to-end speech interaction. We believe this work will advance the field of medical AI by fostering more transparent, interactive, and clinically viable diagnostic support systems. Our code and dataset are publicly available at SiVar-Med.","authors":["Tan-Hanh Pham","Chris Ngo","Trong-Duong Bui","Minh Luu Quang","Tan-Huong Pham","Truong-Son Hy"],"url":"https://arxiv.org/abs/2504.10642"}
{"created":"2025-04-16","title":"Weight-of-Thought Reasoning: Exploring Neural Network Weights for Enhanced LLM Reasoning","abstract":"Large language models (LLMs) have demonstrated remarkable reasoning capabilities when prompted with strategies such as Chain-of-Thought (CoT). However, these approaches focus on token-level output without considering internal weight dynamics. We introduce Weight-of-Thought (WoT) reasoning, a novel approach that examines neural network weights before inference to identify reasoning pathways. Unlike existing methods, WoT explores the weight space through graph-based message passing, multi-step reasoning processes, and attention mechanisms. Our implementation creates an interconnected graph of reasoning nodes. Experiments on diverse reasoning tasks (syllogistic, mathematical, algebraic, combinatorial, and geometric) demonstrate that WoT achieves superior performance compared to traditional methods, particularly for complex problems. This approach leads to both improved performance and greater interpretability of the reasoning process, offering a promising direction for enhancing LLM reasoning capabilities.","authors":["Saif Punjwani","Larry Heck"],"url":"https://arxiv.org/abs/2504.10646"}
{"created":"2025-04-16","title":"Improving In-Context Learning with Reasoning Distillation","abstract":"Language models rely on semantic priors to perform in-context learning, which leads to poor performance on tasks involving inductive reasoning. Instruction-tuning methods based on imitation learning can superficially enhance the in-context learning performance of language models, but they often fail to improve the model's understanding of the underlying rules that connect inputs and outputs in few-shot demonstrations. We propose ReDis, a reasoning distillation technique designed to improve the inductive reasoning capabilities of language models. Through a careful combination of data augmentation, filtering, supervised fine-tuning, and alignment, ReDis achieves significant performance improvements across a diverse range of tasks, including 1D-ARC, List Function, ACRE, and MiniSCAN. Experiments on three language model backbones show that ReDis outperforms equivalent few-shot prompting baselines across all tasks and even surpasses the teacher model, GPT-4o, in some cases. ReDis, based on the LLaMA-3 backbone, achieves relative improvements of 23.2%, 2.8%, and 66.6% over GPT-4o on 1D-ARC, ACRE, and MiniSCAN, respectively, within a similar hypothesis search space. The code, dataset, and model checkpoints will be made available at https://github.com/NafisSadeq/reasoning-distillation.git.","authors":["Nafis Sadeq","Xin Xu","Zhouhang Xie","Julian McAuley","Byungkyu Kang","Prarit Lamba","Xiang Gao"],"url":"https://arxiv.org/abs/2504.10647"}
{"created":"2025-04-16","title":"Ride-pool Assignment Algorithms: Modern Implementation and Swapping Heuristics","abstract":"On-demand ride-pooling has emerged as a popular urban transportation solution, addressing the efficiency limitations of traditional ride-hailing services by grouping multiple riding requests with spatiotemporal proximity into a single vehicle. Although numerous algorithms have been developed for the Ride-pool Assignment Problem (RAP) -- a core component of ride-pooling systems, there is a lack of open-source implementations, making it difficult to benchmark these algorithms on a common dataset and objective. In this paper, we present the implementation details of a ride-pool simulator that encompasses several key ride-pool assignment algorithms, along with associated components such as vehicle routing and rebalancing. We also open-source a highly optimized and modular C++ codebase, designed to facilitate the extension of new algorithms and features. Additionally, we introduce a family of swapping-based local-search heuristics to enhance existing ride-pool assignment algorithms, achieving a better balance between performance and computational efficiency. Extensive experiments on a large-scale, real-world dataset from Manhattan, NYC reveal that while all selected algorithms perform comparably, the newly proposed Multi-Round Linear Assignment with Cyclic Exchange (LA-MR-CE) algorithm achieves a state-of-the-art service rate with significantly reduced computational time. Furthermore, an in-depth analysis suggests that a performance barrier exists for all myopic ride-pool assignment algorithms due to the system's capacity bottleneck, and incorporating future information could be key to overcoming this limitation.","authors":["Matthew Zalesak","Hins Hu","Samitha Samaranayake"],"url":"https://arxiv.org/abs/2504.10649"}
{"created":"2025-04-16","title":"Will AI shape the way we speak? The emerging sociolinguistic influence of synthetic voices","abstract":"The growing prevalence of conversational voice interfaces, powered by developments in both speech and language technologies, raises important questions about their influence on human communication. While written communication can signal identity through lexical and stylistic choices, voice-based interactions inherently amplify socioindexical elements - such as accent, intonation, and speech style - which more prominently convey social identity and group affiliation. There is evidence that even passive media such as television is likely to influence the audience's linguistic patterns. Unlike passive media, conversational AI is interactive, creating a more immersive and reciprocal dynamic that holds a greater potential to impact how individuals speak in everyday interactions. Such heightened influence can be expected to arise from phenomena such as acoustic-prosodic entrainment and linguistic accommodation, which occur naturally during interaction and enable users to adapt their speech patterns in response to the system. While this phenomenon is still emerging, its potential societal impact could provide organisations, movements, and brands with a subtle yet powerful avenue for shaping and controlling public perception and social identity. We argue that the socioindexical influence of AI-generated speech warrants attention and should become a focus of interdisciplinary research, leveraging new and existing methodologies and technologies to better understand its implications.","authors":["\\'Eva Sz\\'ekely (Michaela)","J\\=ura Miniota (Michaela)","M\\'i\\v{s}a (Michaela)","Hejn\\'a"],"url":"https://arxiv.org/abs/2504.10650"}
{"created":"2025-04-16","title":"Un marco conceptual para la generaci\\'on de requerimientos de software de calidad","abstract":"Requirements expressed in natural language are an indispensable artifact in the software development process, as all stakeholders can understand them. However, their ambiguity poses a persistent challenge. To address this issue, organizations such as IEEE and INCOSE publish guidelines for writing requirements, offering rules that assist in this task. On the other hand, agile methodologies provide patterns and structures for expressing stakeholder needs in natural language, attempting to constrain the language to avoid ambiguity. Nevertheless, the knowledge gap among stakeholders regarding the requirements and the correct way to express them further complicates the specification task. In recent years, large language models (LLMs) have emerged to enhance natural language processing tasks. These are Deep learning-based architectures that emulate attention mechanisms like those of humans. This work aims to test the demonstrated power of LLMs in this domain. The objective is to use these models to improve the quality of software requirements written in natural language, assisting analysts in the requirements specification. The proposed framework, its architecture, key components, and their interactions are detailed. Furthermore, a conceptual test of the proposal is developed to assess its usefulness. Finally, the potential and limitations of the framework are discussed, along with future directions for its continued validation and refinement.","authors":["Mauro Jos\\'e Pacchiotti","Mariel Ale y Luciana Ballejos"],"url":"https://arxiv.org/abs/2504.10654"}
{"created":"2025-04-16","title":"Balanced TSP partitioning","abstract":"The traveling salesman problem (TSP) famously asks for a shortest tour that a salesperson can take to visit a given set of cities in any order. In this paper, we ask how much faster $k \\ge 2$ salespeople can visit the cities if they divide the task among themselves. We show that, in the two-dimensional Euclidean setting, two salespeople can always achieve a speedup of at least $\\frac12 + \\frac1\\pi \\approx 0.818$, for any given input, and there are inputs where they cannot do better. We also give (non-matching) upper and lower bounds for $k \\geq 3$.","authors":["Benjamin Aram Berendsohn","Hwi Kim","L\\'aszl\\'o Kozma"],"url":"https://arxiv.org/abs/2504.10657"}
{"created":"2025-04-16","title":"Transfer Learning Assisted XgBoost For Adaptable Cyberattack Detection In Battery Packs","abstract":"Optimal charging of electric vehicle (EVs) depends heavily on reliable sensor measurements from the battery pack to the cloud-controller of the smart charging station. However, an adversary could corrupt the voltage sensor data during transmission, potentially causing local to wide-scale disruptions. Therefore, it is essential to detect sensor cyberattacks in real-time to ensure secure EV charging, and the developed algorithms must be readily adaptable to variations, including pack configurations. To tackle these challenges, we propose adaptable fine-tuning of an XgBoost-based cell-level model using limited pack-level data to use for voltage prediction and residual generation. We used battery cell and pack data from high-fidelity charging experiments in PyBaMM and `liionpack' package to train and test the detection algorithm. The algorithm's performance has been evaluated for two large-format battery packs under sensor swapping and replay attacks. The simulation results also highlight the adaptability and efficacy of our proposed detection algorithm.","authors":["Sanchita Ghosh","Tanushree Roy"],"url":"https://arxiv.org/abs/2504.10658"}
{"created":"2025-04-16","title":"Relation-Rich Visual Document Generator for Visual Information Extraction","abstract":"Despite advances in Large Language Models (LLMs) and Multimodal LLMs (MLLMs) for visual document understanding (VDU), visual information extraction (VIE) from relation-rich documents remains challenging due to the layout diversity and limited training data. While existing synthetic document generators attempt to address data scarcity, they either rely on manually designed layouts and templates, or adopt rule-based approaches that limit layout diversity. Besides, current layout generation methods focus solely on topological patterns without considering textual content, making them impractical for generating documents with complex associations between the contents and layouts. In this paper, we propose a Relation-rIch visual Document GEnerator (RIDGE) that addresses these limitations through a two-stage approach: (1) Content Generation, which leverages LLMs to generate document content using a carefully designed Hierarchical Structure Text format which captures entity categories and relationships, and (2) Content-driven Layout Generation, which learns to create diverse, plausible document layouts solely from easily available Optical Character Recognition (OCR) results, requiring no human labeling or annotations efforts. Experimental results have demonstrated that our method significantly enhances the performance of document understanding models on various VIE benchmarks. The code and model will be available at https://github.com/AI-Application-and-Integration-Lab/RIDGE .","authors":["Zi-Han Jiang","Chien-Wei Lin","Wei-Hua Li","Hsuan-Tung Liu","Yi-Ren Yeh","Chu-Song Chen"],"url":"https://arxiv.org/abs/2504.10659"}
{"created":"2025-04-16","title":"LITERA: An LLM Based Approach to Latin-to-English Translation","abstract":"This paper introduces an LLM-based Latin-to-English translation platform designed to address the challenges of translating Latin texts. We named the model LITERA, which stands for Latin Interpretation and Translations into English for Research Assistance. Through a multi-layered translation process utilizing a fine-tuned version of GPT-4o-mini and GPT-4o, LITERA offers an unprecedented level of accuracy, showcased by greatly improved BLEU scores, particularly in classical Latin, along with improved BLEURT scores. The development of LITERA involved close collaboration with Duke University's Classical Studies Department, which was instrumental in creating a small, high-quality parallel Latin-English dataset. This paper details the architecture, fine-tuning methodology, and prompting strategies used in LITERA, emphasizing its ability to produce literal translations.","authors":["Paul Rosu"],"url":"https://arxiv.org/abs/2504.10660"}
{"created":"2025-04-16","title":"Emotion Alignment: Discovering the Gap Between Social Media and Real-World Sentiments in Persian Tweets and Images","abstract":"In contemporary society, widespread social media usage is evident in people's daily lives. Nevertheless, disparities in emotional expressions between the real world and online platforms can manifest. We comprehensively analyzed Persian community on X to explore this phenomenon. An innovative pipeline was designed to measure the similarity between emotions in the real world compared to social media. Accordingly, recent tweets and images of participants were gathered and analyzed using Transformers-based text and image sentiment analysis modules. Each participant's friends also provided insights into the their real-world emotions. A distance criterion was used to compare real-world feelings with virtual experiences. Our study encompassed N=105 participants, 393 friends who contributed their perspectives, over 8,300 collected tweets, and 2,000 media images. Results indicated a 28.67% similarity between images and real-world emotions, while tweets exhibited a 75.88% alignment with real-world feelings. Additionally, the statistical significance confirmed that the observed disparities in sentiment proportions.","authors":["Sina Elahimanesh","Mohammadali Mohammadkhani","Shohreh Kasaei"],"url":"https://arxiv.org/abs/2504.10662"}
{"created":"2025-04-16","title":"Characterizing Knowledge Manipulation in a Russian Wikipedia Fork","abstract":"Wikipedia is powered by MediaWiki, a free and open-source software that is also the infrastructure for many other wiki-based online encyclopedias. These include the recently launched website Ruwiki, which has copied and modified the original Russian Wikipedia content to conform to Russian law. To identify practices and narratives that could be associated with different forms of knowledge manipulation, this article presents an in-depth analysis of this Russian Wikipedia fork. We propose a methodology to characterize the main changes with respect to the original version. The foundation of this study is a comprehensive comparative analysis of more than 1.9M articles from Russian Wikipedia and its fork. Using meta-information and geographical, temporal, categorical, and textual features, we explore the changes made by Ruwiki editors. Furthermore, we present a classification of the main topics of knowledge manipulation in this fork, including a numerical estimation of their scope. This research not only sheds light on significant changes within Ruwiki, but also provides a methodology that could be applied to analyze other Wikipedia forks and similar collaborative projects.","authors":["Mykola Trokhymovych","Oleksandr Kosovan","Nathan Forrester","Pablo Arag\\'on","Diego Saez-Trumper","Ricardo Baeza-Yates"],"url":"https://arxiv.org/abs/2504.10663"}
{"created":"2025-04-16","title":"Perturbed State Space Feature Encoders for Optical Flow with Event Cameras","abstract":"With their motion-responsive nature, event-based cameras offer significant advantages over traditional cameras for optical flow estimation. While deep learning has improved upon traditional methods, current neural networks adopted for event-based optical flow still face temporal and spatial reasoning limitations. We propose Perturbed State Space Feature Encoders (P-SSE) for multi-frame optical flow with event cameras to address these challenges. P-SSE adaptively processes spatiotemporal features with a large receptive field akin to Transformer-based methods, while maintaining the linear computational complexity characteristic of SSMs. However, the key innovation that enables the state-of-the-art performance of our model lies in our perturbation technique applied to the state dynamics matrix governing the SSM system. This approach significantly improves the stability and performance of our model. We integrate P-SSE into a framework that leverages bi-directional flows and recurrent connections, expanding the temporal context of flow prediction. Evaluations on DSEC-Flow and MVSEC datasets showcase P-SSE's superiority, with 8.48% and 11.86% improvements in EPE performance, respectively.","authors":["Gokul Raju Govinda Raju","Nikola Zubi\\'c","Marco Cannici","Davide Scaramuzza"],"url":"https://arxiv.org/abs/2504.10669"}
{"created":"2025-04-16","title":"Token Sliding Reconfiguration on DAGs","abstract":"Given a graph $G$ and two independent sets of same size, the Independent Set Reconfiguration Problem under token sliding ask whether one can, in a step by step manner, transform the first independent set into the second one. In each step we must preserve the condition of independence. Further, referring to solution vertices as tokens, we are only permitted to slide a token along an edge. Until the recent work of Ito et al. [Ito et al. MFCS 2022] this problem was only considered on undirected graphs. In this work, we study reconfiguration under token sliding focusing on DAGs.","authors":["Jona Dirks","Alexandre Vigny"],"url":"https://arxiv.org/abs/2504.10671"}
{"created":"2025-04-16","title":"H-MoRe: Learning Human-centric Motion Representation for Action Analysis","abstract":"In this paper, we propose H-MoRe, a novel pipeline for learning precise human-centric motion representation. Our approach dynamically preserves relevant human motion while filtering out background movement. Notably, unlike previous methods relying on fully supervised learning from synthetic data, H-MoRe learns directly from real-world scenarios in a self-supervised manner, incorporating both human pose and body shape information. Inspired by kinematics, H-MoRe represents absolute and relative movements of each body point in a matrix format that captures nuanced motion details, termed world-local flows. H-MoRe offers refined insights into human motion, which can be integrated seamlessly into various action-related applications. Experimental results demonstrate that H-MoRe brings substantial improvements across various downstream tasks, including gait recognition(CL@R1: +16.01%), action recognition(Acc@1: +8.92%), and video generation(FVD: -67.07%). Additionally, H-MoRe exhibits high inference efficiency (34 fps), making it suitable for most real-time scenarios. Models and code will be released upon publication.","authors":["Zhanbo Huang","Xiaoming Liu","Yu Kong"],"url":"https://arxiv.org/abs/2504.10676"}
{"created":"2025-04-16","title":"Achieving Optimal Tissue Repair Through MARL with Reward Shaping and Curriculum Learning","abstract":"In this paper, we present a multi-agent reinforcement learning (MARL) framework for optimizing tissue repair processes using engineered biological agents. Our approach integrates: (1) stochastic reaction-diffusion systems modeling molecular signaling, (2) neural-like electrochemical communication with Hebbian plasticity, and (3) a biologically informed reward function combining chemical gradient tracking, neural synchronization, and robust penalties. A curriculum learning scheme guides the agent through progressively complex repair scenarios. In silico experiments demonstrate emergent repair strategies, including dynamic secretion control and spatial coordination.","authors":["Muhammad Al-Zafar Khan","Jamal Al-Karaki"],"url":"https://arxiv.org/abs/2504.10677"}
{"created":"2025-04-16","title":"Keyword Extraction, and Aspect Classification in Sinhala, English, and Code-Mixed Content","abstract":"Brand reputation in the banking sector is maintained through insightful analysis of customer opinion on code-mixed and multilingual content. Conventional NLP models misclassify or ignore code-mixed text, when mix with low resource languages such as Sinhala-English and fail to capture domain-specific knowledge. This study introduces a hybrid NLP method to improve keyword extraction, content filtering, and aspect-based classification of banking content. Keyword extraction in English is performed with a hybrid approach comprising a fine-tuned SpaCy NER model, FinBERT-based KeyBERT embeddings, YAKE, and EmbedRank, which results in a combined accuracy of 91.2%. Code-mixed and Sinhala keywords are extracted using a fine-tuned XLM-RoBERTa model integrated with a domain-specific Sinhala financial vocabulary, and it results in an accuracy of 87.4%. To ensure data quality, irrelevant comment filtering was performed using several models, with the BERT-base-uncased model achieving 85.2% for English and XLM-RoBERTa 88.1% for Sinhala, which was better than GPT-4o, SVM, and keyword-based filtering. Aspect classification followed the same pattern, with the BERT-base-uncased model achieving 87.4% for English and XLM-RoBERTa 85.9% for Sinhala, both exceeding GPT-4 and keyword-based approaches. These findings confirm that fine-tuned transformer models outperform traditional methods in multilingual financial text analysis. The present framework offers an accurate and scalable solution for brand reputation monitoring in code-mixed and low-resource banking environments.","authors":["F. A. Rizvi","T. Navojith","A. M. N. H. Adhikari","W. P. U. Senevirathna","Dharshana Kasthurirathna","Lakmini Abeywardhana"],"url":"https://arxiv.org/abs/2504.10679"}
{"created":"2025-04-16","title":"EMAFusion: A Self-Optimizing System for Seamless LLM Selection and Integration","abstract":"While recent advances in large language models (LLMs) have significantly enhanced performance across diverse natural language tasks, the high computational and financial costs associated with their deployment remain substantial barriers. Existing routing strategies partially alleviate this challenge by assigning queries to cheaper or specialized models, but they frequently rely on extensive labeled data or fragile task-specific heuristics. Conversely, fusion techniques aggregate multiple LLM outputs to boost accuracy and robustness, yet they often exacerbate cost and may reinforce shared biases.","authors":["Soham Shah","Kumar Shridhar","Surojit Chatterjee","Souvik Sen"],"url":"https://arxiv.org/abs/2504.10681"}
{"created":"2025-04-16","title":"NTIRE 2025 Challenge on Cross-Domain Few-Shot Object Detection: Methods and Results","abstract":"Cross-Domain Few-Shot Object Detection (CD-FSOD) poses significant challenges to existing object detection and few-shot detection models when applied across domains. In conjunction with NTIRE 2025, we organized the 1st CD-FSOD Challenge, aiming to advance the performance of current object detectors on entirely novel target domains with only limited labeled data. The challenge attracted 152 registered participants, received submissions from 42 teams, and concluded with 13 teams making valid final submissions. Participants approached the task from diverse perspectives, proposing novel models that achieved new state-of-the-art (SOTA) results under both open-source and closed-source settings. In this report, we present an overview of the 1st NTIRE 2025 CD-FSOD Challenge, highlighting the proposed solutions and summarizing the results submitted by the participants.","authors":["Yuqian Fu","Xingyu Qiu","Bin Ren","Yanwei Fu","Radu Timofte","Nicu Sebe","Ming-Hsuan Yang","Luc Van Gool","Kaijin Zhang","Qingpeng Nong","Xiugang Dong","Hong Gao","Xiangsheng Zhou","Jiancheng Pan","Yanxing Liu","Xiao He","Jiahao Li","Yuze Sun","Xiaomeng Huang","Zhenyu Zhang","Ran Ma","Yuhan Liu","Zijian Zhuang","Shuai Yi","Yixiong Zou","Lingyi Hong","Mingxi Chen","Runze Li","Xingdong Sheng","Wenqiang Zhang","Weisen Chen","Yongxin Yan","Xinguo Chen","Yuanjie Shao","Zhengrong Zuo","Nong Sang","Hao Wu","Haoran Sun","Shuming Hu","Yan Zhang","Zhiguang Shi","Yu Zhang","Chao Chen","Tao Wang","Da Feng","Linhai Zhuo","Ziming Lin","Yali Huang","Jie Me","Yiming Yang","Mi Guo","Mingyuan Jiu","Mingliang Xu","Maomao Xiong","Qunshu Zhang","Xinyu Cao","Yuqing Yang","Dianmo Sheng","Xuanpu Zhao","Zhiyu Li","Xuyang Ding","Wenqian Li"],"url":"https://arxiv.org/abs/2504.10685"}
{"created":"2025-04-16","title":"The Tenth NTIRE 2025 Efficient Super-Resolution Challenge Report","abstract":"This paper presents a comprehensive review of the NTIRE 2025 Challenge on Single-Image Efficient Super-Resolution (ESR). The challenge aimed to advance the development of deep models that optimize key computational metrics, i.e., runtime, parameters, and FLOPs, while achieving a PSNR of at least 26.90 dB on the $\\operatorname{DIV2K\\_LSDIR\\_valid}$ dataset and 26.99 dB on the $\\operatorname{DIV2K\\_LSDIR\\_test}$ dataset. A robust participation saw \\textbf{244} registered entrants, with \\textbf{43} teams submitting valid entries. This report meticulously analyzes these methods and results, emphasizing groundbreaking advancements in state-of-the-art single-image ESR techniques. The analysis highlights innovative approaches and establishes benchmarks for future research in the field.","authors":["Bin Ren","Hang Guo","Lei Sun","Zongwei Wu","Radu Timofte","Yawei Li","Yao Zhang","Xinning Chai","Zhengxue Cheng","Yingsheng Qin","Yucai Yang","Li Song","Hongyuan Yu","Pufan Xu","Cheng Wan","Zhijuan Huang","Peng Guo","Shuyuan Cui","Chenjun Li","Xuehai Hu","Pan Pan","Xin Zhang","Heng Zhang","Qing Luo","Linyan Jiang","Haibo Lei","Qifang Gao","Yaqing Li","Weihua Luo","Tsing Li","Qing Wang","Yi Liu","Yang Wang","Hongyu An","Liou Zhang","Shijie Zhao","Lianhong Song","Long Sun","Jinshan Pan","Jiangxin Dong","Jinhui Tang","Jing Wei","Mengyang Wang","Ruilong Guo","Qian Wang","Qingliang Liu","Yang Cheng","Davinci","Enxuan Gu","Pinxin Liu","Yongsheng Yu","Hang Hua","Yunlong Tang","Shihao Wang","Yukun Yang","Zhiyu Zhang","Yukun Yang","Jiyu Wu","Jiancheng Huang","Yifan Liu","Yi Huang","Shifeng Chen","Rui Chen","Yi Feng","Mingxi Li","Cailu Wan","Xiangji Wu","Zibin Liu","Jinyang Zhong","Kihwan Yoon","Ganzorig Gankhuyag","Shengyun Zhong","Mingyang Wu","Renjie Li","Yushen Zuo","Zhengzhong Tu","Zongang Gao","Guannan Chen","Yuan Tian","Wenhui Chen","Weijun Yuan","Zhan Li","Yihang Chen","Yifan Deng","Ruting Deng","Yilin Zhang","Huan Zheng","Yanyan Wei","Wenxuan Zhao","Suiyi Zhao","Fei Wang","Kun Li","Yinggan Tang","Mengjie Su","Jae-hyeon Lee","Dong-Hyeop Son","Ui-Jin Choi","Tiancheng Shao","Yuqing Zhang","Mengcheng Ma","Donggeun Ko","Youngsang Kwak","Jiun Lee","Jaehwa Kwak","Yuxuan Jiang","Qiang Zhu","Siyue Teng","Fan Zhang","Shuyuan Zhu","Bing Zeng","David Bull","Jing Hu","Hui Deng","Xuan Zhang","Lin Zhu","Qinrui Fan","Weijian Deng","Junnan Wu","Wenqin Deng","Yuquan Liu","Zhaohong Xu","Jameer Babu Pinjari","Kuldeep Purohit","Zeyu Xiao","Zhuoyuan Li","Surya Vashisth","Akshay Dudhane","Praful Hambarde","Sachin Chaudhary","Satya Naryan Tazi","Prashant Patil","Santosh Kumar Vipparthi","Subrahmanyam Murala","Wei-Chen Shen","I-Hsiang Chen","Yunzhe Xu","Chen Zhao","Zhizhou Chen","Akram Khatami-Rizi","Ahmad Mahmoudi-Aznaveh","Alejandro Merino","Bruno Longarela","Javier Abad","Marcos V. Conde","Simone Bianco","Luca Cogo","Gianmarco Corti"],"url":"https://arxiv.org/abs/2504.10686"}
{"created":"2025-04-16","title":"Introducing Large Language Models as the Next Challenging Internet Traffic Source","abstract":"This article explores the growing impact of large language models (LLMs) and Generative AI (GenAI) tools on Internet traffic, focusing on their role as a new and significant source of network load. As these AI tools continue to gain importance in applications ranging from virtual assistants to content generation, the volume of traffic they generate is expected to increase massively. These models use the Internet as the global infrastructure for delivering multimedia messages (text, voice, images, video, etc.) to users, by interconnecting users and devices with AI agents typically deployed in the cloud. We believe this represents a new paradigm that will lead to a considerable increase in network traffic, and network operators must be prepared to address the resulting demands. To support this claim, we provide a proof-of-concept and source code for measuring traffic in remote user-agent interactions, estimating the traffic generated per prompt for some of the most popular open-source LLMs in 2025. The average size of each prompt query and response is 7,593 bytes, with a standard deviation of 369 bytes. These numbers are comparable with email and web browsing traffic. However, we envision AI as the next ``killer application\" that will saturate networks with traffic, such as Peer-to-Peer traffic and Video-on-demand dominated in previous decades.","authors":["Nataliia Koneva","Alejandro Leonardo Garc\\'ia Navarro","Alfonso S\\'anchez-Maci\\'an","Jos\\'e Alberto Hern\\'andez","Moshe Zukerman","\\'Oscar Gonz\\'alez de Dios"],"url":"https://arxiv.org/abs/2504.10688"}
{"created":"2025-04-16","title":"Spectrum Sharing in STAR-RIS-assisted UAV with NOMA for Cognitive Radio Networks","abstract":"As an emerging technology, the simultaneous transmitting and reflecting reconfigurable intelligent surface (STAR-RIS) can improve the spectrum efficiency (SE) of primary users (PUs) and secondary users (SUs) in cognitive radio (CR) networks by mitigating the interference of the incident signals. The STAR-RIS-assisted unmanned aerial vehicle (UAV) can fully cover the dynamic environment through high mobility and fast deployment. According to the dynamic air-to-ground channels, the STAR-RIS-assisted UAV may face a challenge configuring their elements' coefficients (i.e., reflecting and transmitting the amplitude and phases). Hence, to meet the requirements of dynamic channel determination with the SE approach, this paper proposes the sum rate maximization of both PUs and SUs through non-orthogonal multiple access in CR network to jointly optimize the trajectory and transmission-reflection beamforming design of the STAR-RIS-assisted UAV, and power allocation. Since the non-convex joint optimization problem includes coupled optimization variables, we develop an alternative optimization algorithm. Simulation results study the impact of: 1) the significant parameters, 2) the performance of different intelligence surface modes and STAR-RIS operating protocols, 3) the joint trajectory and beamforming design with fixed and mobile users, and 4) STAR-RIS capabilities such as mitigating the interference, and how variations in the roles of elements dynamically.","authors":["Ali Nazari","Ali Olfat"],"url":"https://arxiv.org/abs/2504.10691"}
{"created":"2025-04-16","title":"PlantD: Performance, Latency ANalysis, and Testing for Data Pipelines -- An Open Source Measurement, Testing, and Simulation Framework","abstract":"As the volume of data available from sensor-enabled devices such as vehicles expands, it is increasingly hard for companies to make informed decisions about the cost of capturing, processing, and storing the data from every device. Business teams may forecast costs associated with deployments and use patterns of devices that they sell, yet lack ways of forecasting the cost and performance of the data pipelines needed to support their devices. Without such forecasting, a company's safest choice is to make worst-case capacity estimates, and pay for overprovisioned infrastructure. Existing data pipeline benchmarking tools can measure latency, cost, and throughput as needed for development, but cannot easily close the gap in communicating the implications with business teams to inform cost forecasting. In this paper, we introduce an open-source tool, PlantD, a harness for measuring data pipelines as they are being developed, and for interpreting that data in a business context. PlantD collects a complete suite of metrics and visualizations, when developing or evaluating data pipeline architectures, configurations, and business use cases. It acts as a metaphorical data pipeline wind tunnel, enabling experiments with synthetic data to characterize and compare the performance of pipelines. It then uses those results to allow modeling of expected annual cost and performance under projected real-world loads. We describe the architecture of PlantD, walk through an example of using it to measure and compare three variants of a pipeline for processing automotive telemetry, and demonstrate how business and engineering teams can simulate scenarios together and answer \"what-if\" questions about the pipeline's performance under different business assumptions, allowing them to intelligently predict performance and cost measures of their critical, high-data generation infrastructure.","authors":["Christopher Bogart","Rajeev Chhajer","Baljit Singh","Tony Fontana","Majd Sakr"],"url":"https://arxiv.org/abs/2504.10692"}
{"created":"2025-04-16","title":"Load Balancing with Network Latencies via Distributed Gradient Descent","abstract":"Motivated by the growing demand for serving large language model inference requests, we study distributed load balancing for global serving systems with network latencies. We consider a fluid model in which continuous flows of requests arrive at different frontends and need to be routed to distant backends for processing whose processing rates are workload dependent. Network latencies can lead to long travel times for requests and delayed feedback from backends. The objective is to minimize the average latency of requests, composed of the network latency and the serving latency at the backends.","authors":["Santiago R. Balseiro","Vahab S. Mirrokni","Bartek Wydrowski"],"url":"https://arxiv.org/abs/2504.10693"}
{"created":"2025-04-16","title":"The Jailbreak Tax: How Useful are Your Jailbreak Outputs?","abstract":"Jailbreak attacks bypass the guardrails of large language models to produce harmful outputs. In this paper, we ask whether the model outputs produced by existing jailbreaks are actually useful. For example, when jailbreaking a model to give instructions for building a bomb, does the jailbreak yield good instructions? Since the utility of most unsafe answers (e.g., bomb instructions) is hard to evaluate rigorously, we build new jailbreak evaluation sets with known ground truth answers, by aligning models to refuse questions related to benign and easy-to-evaluate topics (e.g., biology or math). Our evaluation of eight representative jailbreaks across five utility benchmarks reveals a consistent drop in model utility in jailbroken responses, which we term the jailbreak tax. For example, while all jailbreaks we tested bypass guardrails in models aligned to refuse to answer math, this comes at the expense of a drop of up to 92% in accuracy. Overall, our work proposes the jailbreak tax as a new important metric in AI safety, and introduces benchmarks to evaluate existing and future jailbreaks. We make the benchmark available at https://github.com/ethz-spylab/jailbreak-tax","authors":["Kristina Nikoli\\'c","Luze Sun","Jie Zhang","Florian Tram\\`er"],"url":"https://arxiv.org/abs/2504.10694"}
{"created":"2025-04-16","title":"Optimising Intrusion Detection Systems in Cloud-Edge Continuum with Knowledge Distillation for Privacy-Preserving and Efficient Communication","abstract":"The growth of the Internet of Things has amplified the need for secure data interactions in cloud-edge ecosystems, where sensitive information is constantly processed across various system layers. Intrusion detection systems are commonly used to protect such environments from malicious attacks. Recently, Federated Learning has emerged as an effective solution for implementing intrusion detection systems, owing to its decentralised architecture that avoids sharing raw data with a central server, thereby enhancing data privacy. Despite its benefits, Federated Learning faces criticism for high communication overhead from frequent model updates, especially in large-scale Cloud-Edge infrastructures. This paper explores Knowledge Distillation to reduce communication overhead in Cloud-Edge intrusion detection while preserving accuracy and data privacy. Experiments show significant improvements over state-of-the-art methods.","authors":["Soad Almabdy","Amjad Ullah"],"url":"https://arxiv.org/abs/2504.10698"}
{"created":"2025-04-16","title":"HyRRT-Connect: Bidirectional Motion Planning for Hybrid Dynamical Systems","abstract":"This paper proposes a bidirectional rapidly-exploring random trees (RRT) algorithm to solve the motion planning problem for hybrid systems. The proposed algorithm, called HyRRT-Connect, propagates in both forward and backward directions in hybrid time until an overlap between the forward and backward propagation results is detected. Then, HyRRT-Connect constructs a motion plan through the reversal and concatenation of functions defined on hybrid time domains, ensuring that the motion plan satisfies the given hybrid dynamics. To address the potential discontinuity along the flow caused by tolerating some distance between the forward and backward partial motion plans, we reconstruct the backward partial motion plan by a forward-in-hybrid-time simulation from the final state of the forward partial motion plan. effectively eliminating the discontinuity. The proposed algorithm is applied to an actuated bouncing ball system and a walking robot example to highlight its computational improvement.","authors":["Nan Wang","Ricardo G. Sanfelice"],"url":"https://arxiv.org/abs/2504.10699"}
{"created":"2025-04-16","title":"Optimizing Data Distribution and Kernel Performance for Efficient Training of Chemistry Foundation Models: A Case Study with MACE","abstract":"Chemistry Foundation Models (CFMs) that leverage Graph Neural Networks (GNNs) operating on 3D molecular graph structures are becoming indispensable tools for computational chemists and materials scientists. These models facilitate the understanding of matter and the discovery of new molecules and materials. In contrast to GNNs operating on a large homogeneous graphs, GNNs used by CFMs process a large number of geometric graphs of varying sizes, requiring different optimization strategies than those developed for large homogeneous GNNs. This paper presents optimizations for two critical phases of CFM training: data distribution and model training, targeting MACE - a state-of-the-art CFM. We address the challenge of load balancing in data distribution by formulating it as a multi-objective bin packing problem. We propose an iterative algorithm that provides a highly effective, fast, and practical solution, ensuring efficient data distribution. For the training phase, we identify symmetric tensor contraction as the key computational kernel in MACE and optimize this kernel to improve the overall performance. Our combined approach of balanced data distribution and kernel optimization significantly enhances the training process of MACE. Experimental results demonstrate a substantial speedup, reducing per-epoch execution time for training from 12 to 2 minutes on 740 GPUs with a 2.6M sample dataset.","authors":["Jesun Firoz","Franco Pellegrini","Mario Geiger","Darren Hsu","Jenna A. Bilbrey","Han-Yi Chou","Maximilian Stadler","Markus Hoehnerbach","Tingyu Wang","Dejun Lin","Emine Kucukbenli","Henry W. Sprueill","Ilyes Batatia","Sotiris S. Xantheas","MalSoon Lee","Chris Mundy","Gabor Csanyi","Justin S. Smith","Ponnuswamy Sadayappan","Sutanay Choudhury"],"url":"https://arxiv.org/abs/2504.10700"}
{"created":"2025-04-16","title":"Container-level Energy Observability in Kubernetes Clusters","abstract":"Kubernetes has been for a number of years the default cloud orchestrator solution across multiple application and research domains. As such, optimizing the energy efficiency of Kubernetes-deployed workloads is of primary interest towards controlling operational expenses by reducing energy consumption at data center level and allocated resources at application level. A lot of research in this direction aims on reducing the total energy usage of Kubernetes clusters without establishing an understanding of their workloads, i.e. the applications deployed on the cluster. This means that there are untapped potential improvements in energy efficiency that can be achieved through, for example, application refactoring or deployment optimization. For all these cases a prerequisite is establishing fine-grained observability down to the level of individual containers and their power draw over time. A state-of-the-art tool approved by the Cloud-Native Computing Foundation, Kepler, aims to provide this functionality, but has not been assessed for its accuracy and therefore fitness for purpose. In this work we start by developing an experimental procedure to this goal, and we conclude that the reported energy usage metrics provided by Kepler are not at a satisfactory level. As a reaction to this, we develop KubeWatt as an alternative to Kepler for specific use case scenarios, and demonstrate its higher accuracy through the same experimental procedure as we used for Kepler.","authors":["Bjorn Pijnacker","Brian Setz","Vasilios Andrikopoulos"],"url":"https://arxiv.org/abs/2504.10702"}
{"created":"2025-04-16","title":"The Trie Measure, Revisited","abstract":"In this paper, we study the following problem: given $n$ subsets $S_1, \\dots, S_n$ of an integer universe $U = \\{0,\\dots, u-1\\}$, having total cardinality $N = \\sum_{i=1}^n |S_i|$, find a prefix-free encoding $enc : U \\rightarrow \\{0,1\\}^+$ minimizing the so-called trie measure, i.e., the total number of edges in the $n$ binary tries $\\mathcal T_1, \\dots, \\mathcal T_n$, where $\\mathcal T_i$ is the trie packing the encoded integers $\\{enc(x):x\\in S_i\\}$. We first observe that this problem is equivalent to that of merging $u$ sets with the cheapest sequence of binary unions, a problem which in [Ghosh et al., ICDCS 2015] is shown to be NP-hard. Motivated by the hardness of the general problem, we focus on particular families of prefix-free encodings. We start by studying the fixed-length shifted encoding of [Gupta et al., Theoretical Computer Science 2007]. Given a parameter $0\\le a < u$, this encoding sends each $x \\in U$ to $(x + a) \\mod u$, interpreted as a bit-string of $\\log u$ bits. We develop the first efficient algorithms that find the value of $a$ minimizing the trie measure when this encoding is used. Our two algorithms run in $O(u + N\\log u)$ and $O(N\\log^2 u)$ time, respectively. We proceed by studying ordered encodings (a.k.a. monotone or alphabetic), and describe an algorithm finding the optimal such encoding in $O(N+u^3)$ time. Within the same running time, we show how to compute the best shifted ordered encoding, provably no worse than both the optimal shifted and optimal ordered encodings. We provide implementations of our algorithms and discuss how these encodings perform in practice.","authors":["Jarno N. Alanko","Ruben Becker","Davide Cenzato","Travis Gagie","Sung-Hwan Kim","Bojana Kodric","Nicola Prezza"],"url":"https://arxiv.org/abs/2504.10703"}
{"created":"2025-04-16","title":"PDSP-Bench: A Benchmarking System for Parallel and Distributed Stream Processing","abstract":"The paper introduces PDSP-Bench, a novel benchmarking system designed for a systematic understanding of performance of parallel stream processing in a distributed environment. Such an understanding is essential for determining how Stream Processing Systems (SPS) use operator parallelism and the available resources to process massive workloads of modern applications. Existing benchmarking systems focus on analyzing SPS using queries with sequential operator pipelines within a homogeneous centralized environment. Quite differently, PDSP-Bench emphasizes the aspects of parallel stream processing in a distributed heterogeneous environment and simultaneously allows the integration of machine learning models for SPS workloads. In our results, we benchmark a well-known SPS, Apache Flink, using parallel query structures derived from real-world applications and synthetic queries to show the capabilities of PDSP-Bench towards parallel stream processing. Moreover, we compare different learned cost models using generated SPS workloads on PDSP-Bench by showcasing their evaluations on model and training efficiency. We present key observations from our experiments using PDSP-Bench that highlight interesting trends given different query workloads, such as non-linearity and paradoxical effects of parallelism on the performance.","authors":["Pratyush Agnihotri","Boris Koldehofe","Roman Heinrich","Carsten Binnig","Manisha Luthra"],"url":"https://arxiv.org/abs/2504.10704"}
{"created":"2025-04-16","title":"GestureCoach: Rehearsing for Engaging Talks with LLM-Driven Gesture Recommendations","abstract":"This paper introduces GestureCoach, a system designed to help speakers deliver more engaging talks by guiding them to gesture effectively during rehearsal. GestureCoach combines an LLM-driven gesture recommendation model with a rehearsal interface that proactively cues speakers to gesture appropriately. Trained on experts' gesturing patterns from TED talks, the model consists of two modules: an emphasis proposal module, which predicts when to gesture by identifying gesture-worthy text segments in the presenter notes, and a gesture identification module, which determines what gesture to use by retrieving semantically appropriate gestures from a curated gesture database. Results of a model performance evaluation and user study (N=30) show that the emphasis proposal module outperforms off-the-shelf LLMs in identifying suitable gesture regions, and that participants rated the majority of these predicted regions and their corresponding gestures as highly appropriate. A subsequent user study (N=10) showed that rehearsing with GestureCoach encouraged speakers to gesture and significantly increased gesture diversity, resulting in more engaging talks. We conclude with design implications for future AI-driven rehearsal systems.","authors":["Ashwin Ram","Varsha Suresh","Artin Saberpour Abadian","Vera Demberg","J\\\"urgen Steimle"],"url":"https://arxiv.org/abs/2504.10706"}
{"created":"2025-04-16","title":"Legally-Informed Explainable AI","abstract":"Explanations for artificial intelligence (AI) systems are intended to support the people who are impacted by AI systems in high-stakes decision-making environments, such as doctors, patients, teachers, students, housing applicants, and many others. To protect people and support the responsible development of AI, explanations need to be actionable--helping people take pragmatic action in response to an AI system--and contestable--enabling people to push back against an AI system and its determinations. For many high-stakes domains, such as healthcare, education, and finance, the sociotechnical environment includes significant legal implications that impact how people use AI explanations. For example, physicians who use AI decision support systems may need information on how accepting or rejecting an AI determination will protect them from lawsuits or help them advocate for their patients. In this paper, we make the case for Legally-Informed Explainable AI, responding to the need to integrate and design for legal considerations when creating AI explanations. We describe three stakeholder groups with different informational and actionability needs, and provide practical recommendations to tackle design challenges around the design of explainable AI systems that incorporate legal considerations.","authors":["Gennie Mansi","Naveena Karusala","Mark Riedl"],"url":"https://arxiv.org/abs/2504.10708"}
{"created":"2025-04-16","title":"Vehicle Dynamics Control for Simultaneous Optimization of Tire Emissions and Performance in EVs","abstract":"In recent years, Electric Vehicles (EVs) have seen widespread public adoption. While EVs produce zero tailpipe emissions, they contribute to an increase in another type of vehicular emission: tire emissions. Battery-operated EVs are generally heavier than their combustion-engine counterparts and require greater acceleration forces, which their high-torque electric motors provide. This combination of increased weight and traction forces leads to higher tire emissions, which possess various adverse health and environmental effects. Here, we propose a control solution with promising results in mitigating tire wear in all-wheel-drive EVs. The idea is to utilize different tire profiles on each drive axis: a low-wear, low-traction axis and a high-wear, high-traction axis. Derived from detailed mathematical analyses, we propose a simple control scheme to counteract the performance difference from using the low-traction tires. The proposed control mechanism then distributes torque optimally between the two axes, maximizing usage from the low-wear axis and simultaneously maintaining stability and performance by leveraging high-traction tires. Through detailed numerical simulations, we demonstrate that the developed model significantly reduces tire emissions and maintains vehicle drivability and performance.","authors":["Chi-Bach Pham","Homayoun Hamedmoghadam Rafati","Robert Noel Shorten"],"url":"https://arxiv.org/abs/2504.10709"}
{"created":"2025-04-16","title":"On an efficient line smoother for the p-multigrid {\\gamma}-cycle","abstract":"As part of the development of a Poisson solver for the spectral element discretization used in the GeoFluid Object Workbench (GeoFLOW) code, we propose a solver for the linear system arising from a Gauss-Legendre-Lobatto global spectral method. We precondition using a p-multigrid {\\gamma}-cycle with highly-vectorizable smoothers, that we refer to as line smoothers. Our smoothers are restrictions of spectral and finite element discretizations to low-order one-dimensional problems along lines, that are solved by a reformulation of cyclic reduction as a direct multigrid method. We illustrate our method with numerical experiments showing the apparent boundedness of the iteration count for a fixed residual reduction over a range of moderately deformed domains, right hand sides and Dirichlet boundary conditions.","authors":["Jos\\'e Pablo Lucero Lorca","Duane Rosenberg","Isidora Jankov","Conor McCoid","Martin Jakob Gander"],"url":"https://arxiv.org/abs/2504.10710"}
{"created":"2025-04-16","title":"Can LLMs Classify CVEs? Investigating LLMs Capabilities in Computing CVSS Vectors","abstract":"Common Vulnerability and Exposure (CVE) records are fundamental to cybersecurity, offering unique identifiers for publicly known software and system vulnerabilities. Each CVE is typically assigned a Common Vulnerability Scoring System (CVSS) score to support risk prioritization and remediation. However, score inconsistencies often arise due to subjective interpretations of certain metrics. As the number of new CVEs continues to grow rapidly, automation is increasingly necessary to ensure timely and consistent scoring. While prior studies have explored automated methods, the application of Large Language Models (LLMs), despite their recent popularity, remains relatively underexplored.","authors":["Francesco Marchiori","Denis Donadel","Mauro Conti"],"url":"https://arxiv.org/abs/2504.10713"}
{"created":"2025-04-16","title":"Playing to Pay: Interplay of Monetization and Retention Strategies in Korean Mobile Gaming","abstract":"Mobile gaming's global growth has introduced evolving monetization strategies, such as in app purchases and ads, designed to boost revenue while maintaining player engagement. However, there is limited understanding of the scope and frequency of these strategies, particularly in mature markets like South Korea. To address this research gap, this study examines the monetization strategies used in the top 40 most popular Korean mobile games through direct gameplay observations and targeted video analyses. We identified the prevalence of specific strategies, including time gated progression, Conflict Driven Design, and social Dynamics, which are systematically categorized in our proposed framework for monetization. Our findings also highlight ethical concerns, including issues with transparency, probability disclosures, and the exploitation of competitive pressures areas that remain poorly regulated. To address these challenges, we emphasize the need for stricter consumer protections, cross regional research, and greater focus on protecting vulnerable populations to promote a more equitable and responsible gaming environment.","authors":["HwiJoon Lee","Kashif Imteyaz","Saiph Savage"],"url":"https://arxiv.org/abs/2504.10714"}
{"created":"2025-04-16","title":"SpinMeRound: Consistent Multi-View Identity Generation Using Diffusion Models","abstract":"Despite recent progress in diffusion models, generating realistic head portraits from novel viewpoints remains a significant challenge. Most current approaches are constrained to limited angular ranges, predominantly focusing on frontal or near-frontal views. Moreover, although the recent emerging large-scale diffusion models have been proven robust in handling 3D scenes, they underperform on facial data, given their complex structure and the uncanny valley pitfalls. In this paper, we propose SpinMeRound, a diffusion-based approach designed to generate consistent and accurate head portraits from novel viewpoints. By leveraging a number of input views alongside an identity embedding, our method effectively synthesizes diverse viewpoints of a subject whilst robustly maintaining its unique identity features. Through experimentation, we showcase our model's generation capabilities in 360 head synthesis, while beating current state-of-the-art multiview diffusion models.","authors":["Stathis Galanakis","Alexandros Lattas","Stylianos Moschoglou","Bernhard Kainz","Stefanos Zafeiriou"],"url":"https://arxiv.org/abs/2504.10716"}
{"created":"2025-04-16","title":"FuzzSense: Towards A Modular Fuzzing Framework for Autonomous Driving Software","abstract":"Fuzz testing to find semantic control vulnerabilities is an essential activity to evaluate the robustness of autonomous driving (AD) software. Whilst there is a preponderance of disparate fuzzing tools that target different parts of the test environment, such as the scenario, sensors, and vehicle dynamics, there is a lack of fuzzing strategies that ensemble these fuzzers to enable concurrent fuzzing, utilizing diverse techniques and targets. This research proposes FuzzSense, a modular, black-box, mutation-based fuzzing framework that is architected to ensemble diverse AD fuzzing tools. To validate the utility of FuzzSense, a LiDAR sensor fuzzer was developed as a plug-in, and the fuzzer was implemented in the new AD simulation platform AWSIM and Autoware.Universe AD software platform. The results demonstrated that FuzzSense was able to find vulnerabilities in the new Autoware.Universe software. We contribute to FuzzSense open-source with the aim of initiating a conversation in the community on the design of AD-specific fuzzers and the establishment of a community fuzzing framework to better target the diverse technology base of autonomous vehicles.","authors":["Andrew Roberts","Lorenz Teply","Mert D. Pese","Olaf Maennel","Mohammad Hamad","Sebastian Steinhorst"],"url":"https://arxiv.org/abs/2504.10717"}
{"created":"2025-04-16","title":"Leveraging Deep Operator Networks (DeepONet) for Acoustic Full Waveform Inversion (FWI)","abstract":"Full Waveform Inversion (FWI) is an important geophysical technique considered in subsurface property prediction. It solves the inverse problem of predicting high-resolution Earth interior models from seismic data. Traditional FWI methods are computationally demanding. Inverse problems in geophysics often face challenges of non-uniqueness due to limited data, as data are often collected only on the surface. In this study, we introduce a novel methodology that leverages Deep Operator Networks (DeepONet) to attempt to improve both the efficiency and accuracy of FWI. The proposed DeepONet methodology inverts seismic waveforms for the subsurface velocity field. This approach is able to capture some key features of the subsurface velocity field. We have shown that the architecture can be applied to noisy seismic data with an accuracy that is better than some other machine learning methods. We also test our proposed method with out-of-distribution prediction for different velocity models. The proposed DeepONet shows comparable and better accuracy in some velocity models than some other machine learning methods. To improve the FWI workflow, we propose using the DeepONet output as a starting model for conventional FWI and that it may improve FWI performance. While we have only shown that DeepONet facilitates faster convergence than starting with a homogeneous velocity field, it may have some benefits compared to other approaches to constructing starting models. This integration of DeepONet into FWI may accelerate the inversion process and may also enhance its robustness and reliability.","authors":["Kamaljyoti Nath","Khemraj Shukla","Victor C. Tsai","Umair bin Waheed","Christian Huber","Omer Alpak","Chuen-Song Chen","Ligang Lu","Amik St-Cyr"],"url":"https://arxiv.org/abs/2504.10720"}
{"created":"2025-04-16","title":"HELIOS: Adaptive Model And Early-Exit Selection for Efficient LLM Inference Serving","abstract":"Deploying large language models (LLMs) presents critical challenges due to the inherent trade-offs associated with key performance metrics, such as latency, accuracy, and throughput. Typically, gains in one metric is accompanied with degradation in others. Early-Exit LLMs (EE-LLMs) efficiently navigate this trade-off space by skipping some of the later model layers when it confidently finds an output token early, thus reducing latency without impacting accuracy. However, as the early exits taken depend on the task and are unknown apriori to request processing, EE-LLMs conservatively load the entire model, limiting resource savings and throughput. Also, current frameworks statically select a model for a user task, limiting our ability to adapt to changing nature of the input queries.","authors":["Avinash Kumar","Shashank Nag","Jason Clemons","Lizy John","Poulami Das"],"url":"https://arxiv.org/abs/2504.10724"}
{"created":"2025-04-16","title":"Beyond the Classroom: Bridging the Gap Between Academia and Industry with a Hands-on Learning Approach","abstract":"Modern software systems require various capabilities to meet architectural and operational demands, such as the ability to scale automatically and recover from sudden failures. Self-adaptive software systems have emerged as a critical focus in software design and operation due to their capacity to autonomously adapt to changing environments. However, educating students on this topic is scarce in academia, and a survey among practitioners identified that the lack of knowledgeable individuals has hindered its adoption in the industry. In this paper, we present our experience teaching a course on self-adaptive software systems that integrates theoretical knowledge and hands-on learning with industry-relevant technologies. To close the gap between academic education and industry practices, we incorporated guest lectures from experts and showcases featuring industry professionals as judges, improving technical and communication skills for our students. Feedback based on surveys from 21 students indicates significant improvements in their understanding of self-adaptive systems. The empirical analysis of the developed course demonstrates the effectiveness of the proposed course syllabus and teaching methodology. In addition, we provide a summary of the educational challenges of running this unique course, including balancing theory and practice, addressing the diverse backgrounds and motivations of students, and integrating the industry-relevant technologies. We believe these insights can provide valuable guidance for educating students in other emerging topics within software engineering.","authors":["Mingyang Xu","Ryan Zheng He Liu","Mark Stoodley","Ladan Tahvildari"],"url":"https://arxiv.org/abs/2504.10726"}
{"created":"2025-04-16","title":"Foundation Models for Remote Sensing: An Analysis of MLLMs for Object Localization","abstract":"Multimodal large language models (MLLMs) have altered the landscape of computer vision, obtaining impressive results across a wide range of tasks, especially in zero-shot settings. Unfortunately, their strong performance does not always transfer to out-of-distribution domains, such as earth observation (EO) imagery. Prior work has demonstrated that MLLMs excel at some EO tasks, such as image captioning and scene understanding, while failing at tasks that require more fine-grained spatial reasoning, such as object localization. However, MLLMs are advancing rapidly and insights quickly become out-dated. In this work, we analyze more recent MLLMs that have been explicitly trained to include fine-grained spatial reasoning capabilities, benchmarking them on EO object localization tasks. We demonstrate that these models are performant in certain settings, making them well suited for zero-shot scenarios. Additionally, we provide a detailed discussion focused on prompt selection, ground sample distance (GSD) optimization, and analyzing failure cases. We hope that this work will prove valuable as others evaluate whether an MLLM is well suited for a given EO localization task and how to optimize it.","authors":["Darryl Hannan","John Cooper","Dylan White","Timothy Doster","Henry Kvinge","Yijing Watkins"],"url":"https://arxiv.org/abs/2504.10727"}
{"created":"2025-04-16","title":"Iterative Recommendations based on Monte Carlo Sampling and Trust Estimation in Multi-Stage Vehicular Traffic Routing Games","abstract":"The shortest-time route recommendations offered by modern navigation systems fuel selfish routing in urban vehicular traffic networks and are therefore one of the main reasons for the growth of congestion. In contrast, intelligent transportation systems (ITS) prefer to steer driver-vehicle systems (DVS) toward system-optimal route recommendations, which are primarily designed to mitigate network congestion. However, due to the misalignment in motives, drivers exhibit a lack of trust in the ITS. This paper models the interaction between a DVS and an ITS as a novel, multi-stage routing game where the DVS exhibits dynamics in its trust towards the recommendations of ITS based on counterfactual and observed game outcomes. Specifically, DVS and ITS are respectively modeled as a travel-time minimizer and network congestion minimizer, each having nonidentical prior beliefs about the network state. A novel approximate algorithm to compute the Bayesian Nash equilibrium, called ROSTER(Recommendation Outcome Sampling with Trust Estimation and Re-evaluation), is proposed based on Monte Carlo sampling with trust belief updating to determine the best response route recommendations of the ITS at each stage of the game. Simulation results demonstrate that the trust prediction error in the proposed algorithm converges to zero with a growing number of multi-stage DVS-ITS interactions and is effectively able to both mitigate congestion and reduce driver travel times when compared to alternative route recommendation strategies.","authors":["Doris E. M. Brown","Venkata Sriram Siddhardh Nadendla","Sajal K. Das"],"url":"https://arxiv.org/abs/2504.10728"}
{"created":"2025-04-16","title":"PQ-CAN: A Framework for Simulating Post-Quantum Cryptography in Embedded Systems","abstract":"The rapid development of quantum computers threatens traditional cryptographic schemes, prompting the need for Post-Quantum Cryptography (PQC). Although the NIST standardization process has accelerated the development of such algorithms, their application in resource-constrained environments such as embedded systems remains a challenge. Automotive systems relying on the Controller Area Network (CAN) bus for communication are particularly vulnerable due to their limited computational capabilities, high traffic, and need for real-time response. These constraints raise concerns about the feasibility of implementing PQC in automotive environments, where legacy hardware and bit rate limitations must also be considered.","authors":["Mauro Conti","Francesco Marchiori","Sebastiano Matarazzo","Marco Rubin"],"url":"https://arxiv.org/abs/2504.10730"}
{"created":"2025-04-16","title":"Frozen Layers: Memory-efficient Many-fidelity Hyperparameter Optimization","abstract":"As model sizes grow, finding efficient and cost-effective hyperparameter optimization (HPO) methods becomes increasingly crucial for deep learning pipelines. While multi-fidelity HPO (MF-HPO) trades off computational resources required for DL training with lower fidelity estimations, existing fidelity sources often fail under lower compute and memory constraints. We propose a novel fidelity source: the number of layers that are trained or frozen during training. For deep networks, this approach offers significant compute and memory savings while preserving rank correlations between hyperparameters at low fidelities compared to full model training. We demonstrate this in our empirical evaluation across ResNets and Transformers and additionally analyze the utility of frozen layers as a fidelity in using GPU resources as a fidelity in HPO, and for a combined MF-HPO with other fidelity sources. This contribution opens new applications for MF-HPO with hardware resources as a fidelity and creates opportunities for improved algorithms navigating joint fidelity spaces.","authors":["Timur Carstensen","Neeratyoy Mallik","Frank Hutter","Martin Rapp"],"url":"https://arxiv.org/abs/2504.10735"}
{"created":"2025-04-16","title":"CleanMAP: Distilling Multimodal LLMs for Confidence-Driven Crowdsourced HD Map Updates","abstract":"The rapid growth of intelligent connected vehicles (ICVs) and integrated vehicle-road-cloud systems has increased the demand for accurate, real-time HD map updates. However, ensuring map reliability remains challenging due to inconsistencies in crowdsourced data, which suffer from motion blur, lighting variations, adverse weather, and lane marking degradation. This paper introduces CleanMAP, a Multimodal Large Language Model (MLLM)-based distillation framework designed to filter and refine crowdsourced data for high-confidence HD map updates. CleanMAP leverages an MLLM-driven lane visibility scoring model that systematically quantifies key visual parameters, assigning confidence scores (0-10) based on their impact on lane detection. A novel dynamic piecewise confidence-scoring function adapts scores based on lane visibility, ensuring strong alignment with human evaluations while effectively filtering unreliable data. To further optimize map accuracy, a confidence-driven local map fusion strategy ranks and selects the top-k highest-scoring local maps within an optimal confidence range (best score minus 10%), striking a balance between data quality and quantity. Experimental evaluations on a real-world autonomous vehicle dataset validate CleanMAP's effectiveness, demonstrating that fusing the top three local maps achieves the lowest mean map update error of 0.28m, outperforming the baseline (0.37m) and meeting stringent accuracy thresholds (<= 0.32m). Further validation with real-vehicle data confirms 84.88% alignment with human evaluators, reinforcing the model's robustness and reliability. This work establishes CleanMAP as a scalable and deployable solution for crowdsourced HD map updates, ensuring more precise and reliable autonomous navigation. The code will be available at https://Ankit-Zefan.github.io/CleanMap/","authors":["Ankit Kumar Shaw (Tsinghua University)","Kun Jiang (Tsinghua University)","Tuopu Wen (Tsinghua University)","Chandan Kumar Sah (Beihang University)","Yining Shi (Tsinghua University)","Mengmeng Yang (Tsinghua University)","Diange Yang (Tsinghua University)","Xiaoli Lian (Beihang University)"],"url":"https://arxiv.org/abs/2504.10738"}
{"created":"2025-04-16","title":"HippoMM: Hippocampal-inspired Multimodal Memory for Long Audiovisual Event Understanding","abstract":"Comprehending extended audiovisual experiences remains a fundamental challenge for computational systems. Current approaches struggle with temporal integration and cross-modal associations that humans accomplish effortlessly through hippocampal-cortical networks. We introduce HippoMM, a biologically-inspired architecture that transforms hippocampal mechanisms into computational advantages for multimodal understanding. HippoMM implements three key innovations: (i) hippocampus-inspired pattern separation and completion specifically designed for continuous audiovisual streams, (ii) short-to-long term memory consolidation that transforms perceptual details into semantic abstractions, and (iii) cross-modal associative retrieval pathways enabling modality-crossing queries. Unlike existing retrieval systems with static indexing schemes, HippoMM dynamically forms integrated episodic representations through adaptive temporal segmentation and dual-process memory encoding. Evaluations on our challenging HippoVlog benchmark demonstrate that HippoMM significantly outperforms state-of-the-art approaches (78.2% vs. 64.2% accuracy) while providing substantially faster response times (20.4s vs. 112.5s). Our results demonstrate that translating neuroscientific memory principles into computational architectures provides a promising foundation for next-generation multimodal understanding systems. The code and benchmark dataset are publicly available at https://github.com/linyueqian/HippoMM.","authors":["Yueqian Lin","Qinsi Wang","Hancheng Ye","Yuzhe Fu","Hai \"Helen\" Li","Yiran Chen"],"url":"https://arxiv.org/abs/2504.10739"}
{"created":"2025-04-16","title":"Robust Gittins for Stochastic Scheduling","abstract":"A common theme in stochastic optimization problems is that, theoretically, stochastic algorithms need to \"know\" relatively rich information about the underlying distributions. This is at odds with most applications, where distributions are rough predictions based on historical data. Thus, commonly, stochastic algorithms are making decisions using imperfect predicted distributions, while trying to optimize over some unknown true distributions. We consider the fundamental problem of scheduling stochastic jobs preemptively on a single machine to minimize expected mean completion time in the setting where the scheduler is only given imperfect predicted job size distributions. If the predicted distributions are perfect, then it is known that this problem can be solved optimally by the Gittins index policy. The goal of our work is to design a scheduling policy that is robust in the sense that it produces nearly optimal schedules even if there are modest discrepancies between the predicted distributions and the underlying real distributions. Our main contributions are:","authors":["Benjamin Moseley","Heather Newman","Kirk Pruhs","Rudy Zhou"],"url":"https://arxiv.org/abs/2504.10743"}
{"created":"2025-04-16","title":"Interactivity x Explainability: Toward Understanding How Interactivity Can Improve Computer Vision Explanations","abstract":"Explanations for computer vision models are important tools for interpreting how the underlying models work. However, they are often presented in static formats, which pose challenges for users, including information overload, a gap between semantic and pixel-level information, and limited opportunities for exploration. We investigate interactivity as a mechanism for tackling these issues in three common explanation types: heatmap-based, concept-based, and prototype-based explanations. We conducted a study (N=24), using a bird identification task, involving participants with diverse technical and domain expertise. We found that while interactivity enhances user control, facilitates rapid convergence to relevant information, and allows users to expand their understanding of the model and explanation, it also introduces new challenges. To address these, we provide design recommendations for interactive computer vision explanations, including carefully selected default views, independent input controls, and constrained output spaces.","authors":["Indu Panigrahi","Sunnie S. Y. Kim","Amna Liaqat","Rohan Jinturkar","Olga Russakovsky","Ruth Fong","Parastoo Abtahi"],"url":"https://arxiv.org/abs/2504.10745"}
{"created":"2025-04-16","title":"Hearing Anywhere in Any Environment","abstract":"In mixed reality applications, a realistic acoustic experience in spatial environments is as crucial as the visual experience for achieving true immersion. Despite recent advances in neural approaches for Room Impulse Response (RIR) estimation, most existing methods are limited to the single environment on which they are trained, lacking the ability to generalize to new rooms with different geometries and surface materials. We aim to develop a unified model capable of reconstructing the spatial acoustic experience of any environment with minimum additional measurements. To this end, we present xRIR, a framework for cross-room RIR prediction. The core of our generalizable approach lies in combining a geometric feature extractor, which captures spatial context from panorama depth images, with a RIR encoder that extracts detailed acoustic features from only a few reference RIR samples. To evaluate our method, we introduce ACOUSTICROOMS, a new dataset featuring high-fidelity simulation of over 300,000 RIRs from 260 rooms. Experiments show that our method strongly outperforms a series of baselines. Furthermore, we successfully perform sim-to-real transfer by evaluating our model on four real-world environments, demonstrating the generalizability of our approach and the realism of our dataset.","authors":["Xiulong Liu","Anurag Kumar","Paul Calamia","Sebastia V. Amengual","Calvin Murdock","Ishwarya Ananthabhotla","Philip Robinson","Eli Shlizerman","Vamsi Krishna Ithapu","Ruohan Gao"],"url":"https://arxiv.org/abs/2504.10746"}
{"created":"2025-04-16","title":"Encryption scheme based on Automorphism Group of Hermitian Function Field with Homomorphic Encryption","abstract":"This article proposes a comprehensive approach to implementing encryption schemes based on the automorphism group of the Hermitian function field. We utilize a three-parameter group with logarithmic representations outside the group's center. In this work, we enhance the Hermitian function field-based encryption scheme with homomorphic encryption capabilities, which constitutes a significant advantage of our implementation. Both the attack complexity and the encrypted message size are directly correlated with the order of the group.","authors":["Gennady Khalimov","Yevgen Kotukh"],"url":"https://arxiv.org/abs/2504.10747"}
{"created":"2025-04-16","title":"An Improved Fully Dynamic Algorithm for Counting 4-Cycles in General Graphs using Fast Matrix Multiplication","abstract":"We study subgraph counting over fully dynamic graphs, which undergo edge insertions and deletions. Counting subgraphs is a fundamental problem in graph theory with numerous applications across various fields, including database theory, social network analysis, and computational biology.","authors":["Sepehr Assadi","Vihan Shah"],"url":"https://arxiv.org/abs/2504.10748"}
{"created":"2025-04-16","title":"Real-time Seafloor Segmentation and Mapping","abstract":"Posidonia oceanica meadows are a species of seagrass highly dependent on rocks for their survival and conservation. In recent years, there has been a concerning global decline in this species, emphasizing the critical need for efficient monitoring and assessment tools. While deep learning-based semantic segmentation and visual automated monitoring systems have shown promise in a variety of applications, their performance in underwater environments remains challenging due to complex water conditions and limited datasets. This paper introduces a framework that combines machine learning and computer vision techniques to enable an autonomous underwater vehicle (AUV) to inspect the boundaries of Posidonia oceanica meadows autonomously. The framework incorporates an image segmentation module using an existing Mask R-CNN model and a strategy for Posidonia oceanica meadow boundary tracking. Furthermore, a new class dedicated to rocks is introduced to enhance the existing model, aiming to contribute to a comprehensive monitoring approach and provide a deeper understanding of the intricate interactions between the meadow and its surrounding environment. The image segmentation model is validated using real underwater images, while the overall inspection framework is evaluated in a realistic simulation environment, replicating actual monitoring scenarios with real underwater images. The results demonstrate that the proposed framework enables the AUV to autonomously accomplish the main tasks of underwater inspection and segmentation of rocks. Consequently, this work holds significant potential for the conservation and protection of marine environments, providing valuable insights into the status of Posidonia oceanica meadows and supporting targeted preservation efforts","authors":["Michele Grimaldi","Nouf Alkaabi","Francesco Ruscio","Sebastian Realpe Rua","Rafael Garcia","Nuno Gracias"],"url":"https://arxiv.org/abs/2504.10750"}
{"created":"2025-04-16","title":"Communication-aware Hierarchical Map Compression of Time-Varying Environments for Mobile Robots","abstract":"In this paper, we develop a systematic framework for the time-sequential compression of dynamic probabilistic occupancy grids. Our approach leverages ideas from signal compression theory to formulate an optimization problem that searches for a multi-resolution hierarchical encoder that balances the quality of the compressed map (distortion) with its description size, the latter of which relates to the bandwidth required to reliably transmit the map to other agents or to store map estimates in on-board memory. The resulting optimization problem allows for multi-resolution map compressions to be obtained that satisfy available communication or memory resources, and does not require knowledge of the occupancy map dynamics. We develop an algorithm to solve our problem, and demonstrate the utility of the proposed framework in simulation on both static (i.e., non-time varying) and dynamic (time-varying) occupancy maps.","authors":["Daniel T. Larsson","Dipankar Maity"],"url":"https://arxiv.org/abs/2504.10751"}
{"created":"2025-04-16","title":"Time-varying EEG spectral power predicts evoked and spontaneous fMRI motor brain activity","abstract":"Simultaneous EEG-fMRI recordings are increasingly used to investigate brain activity by leveraging the complementary high spatial and high temporal resolution of fMRI and EEG signals respectively. It remains unclear, however, to what degree these two imaging modalities capture shared information about neural activity. Here, we investigate whether it is possible to predict both task-evoked and spontaneous fMRI signals of motor brain networks from EEG time-varying spectral power using interpretable models trained for individual subjects with Sparse Group Lasso regularization. Critically, we test the trained models on data acquired from each subject on a different day and obtain statistical validation by comparison with appropriate null models as well as the conventional EEG sensorimotor rhythm. We find significant prediction results in most subjects, although less frequently for resting-state compared to task-based conditions. Furthermore, we interpret the model learned parameters to understand representations of EEG-fMRI coupling in terms of predictive EEG channels, frequencies, and haemodynamic delays. In conclusion, our work provides evidence of the ability to predict fMRI motor brain activity from EEG recordings alone across different days, in both task-evoked and spontaneous conditions, with statistical significance in individual subjects. These results present great potential for translation to EEG neurofeedback applications.","authors":["Neil Mehta","Ines Goncalves","Alberto Montagna","Mathis Fleury","Gustavo Caetano","Ines Esteves","Athanasios Vourvopoulos","Pulkit Grover","Patricia Figueiredo"],"url":"https://arxiv.org/abs/2504.10752"}
{"created":"2025-04-16","title":"Epistemic Uncertainty-aware Recommendation Systems via Bayesian Deep Ensemble Learning","abstract":"Recommending items to users has long been a fundamental task, and studies have tried to improve it ever since. Most well-known models commonly employ representation learning to map users and items into a unified embedding space for matching assessment. These approaches have primary limitations, especially when dealing with explicit feedback and sparse data contexts. Two primary limitations are their proneness to overfitting and failure to incorporate epistemic uncertainty in predictions. To address these problems, we propose a novel Bayesian Deep Ensemble Collaborative Filtering method named BDECF. To improve model generalization and quality, we utilize Bayesian Neural Networks, which incorporate uncertainty within their weight parameters. In addition, we introduce a new interpretable non-linear matching approach for the user and item embeddings, leveraging the advantages of the attention mechanism. Furthermore, we endorse the implementation of an ensemble-based supermodel to generate more robust and reliable predictions, resulting in a more complete model. Empirical evaluation through extensive experiments and ablation studies across a range of publicly accessible real-world datasets with differing sparsity characteristics confirms our proposed method's effectiveness and the importance of its components.","authors":["Radin Cheraghi","Amir Mohammad Mahfoozi","Sepehr Zolfaghari","Mohammadshayan Shabani","Maryam Ramezani","Hamid R. Rabiee"],"url":"https://arxiv.org/abs/2504.10753"}
{"created":"2025-04-16","title":"auto-fpt: Automating Free Probability Theory Calculations for Machine Learning Theory","abstract":"A large part of modern machine learning theory often involves computing the high-dimensional expected trace of a rational expression of large rectangular random matrices. To symbolically compute such quantities using free probability theory, we introduce auto-fpt, a lightweight Python and SymPy-based tool that can automatically produce a reduced system of fixed-point equations which can be solved for the quantities of interest, and effectively constitutes a theory. We overview the algorithmic ideas underlying auto-fpt and its applications to various interesting problems, such as the high-dimensional error of linearized feed-forward neural networks, recovering well-known results. We hope that auto-fpt streamlines the majority of calculations involved in high-dimensional analysis, while helping the machine learning community reproduce known and uncover new phenomena.","authors":["Arjun Subramonian","Elvis Dohmatob"],"url":"https://arxiv.org/abs/2504.10754"}
{"created":"2025-04-16","title":"ReasonDrive: Efficient Visual Question Answering for Autonomous Vehicles with Reasoning-Enhanced Small Vision-Language Models","abstract":"Vision-language models (VLMs) show promise for autonomous driving but often lack transparent reasoning capabilities that are critical for safety. We investigate whether explicitly modeling reasoning during fine-tuning enhances VLM performance on driving decision tasks. Using GPT-4o, we generate structured reasoning chains for driving scenarios from the DriveLM benchmark with category-specific prompting strategies. We compare reasoning-based fine-tuning, answer-only fine-tuning, and baseline instruction-tuned models across multiple small VLM families (Llama 3.2, Llava 1.5, and Qwen 2.5VL). Our results demonstrate that reasoning-based fine-tuning consistently outperforms alternatives, with Llama3.2-11B-reason achieving the highest performance. Models fine-tuned with reasoning show substantial improvements in accuracy and text generation quality, suggesting explicit reasoning enhances internal representations for driving decisions. These findings highlight the importance of transparent decision processes in safety-critical domains and offer a promising direction for developing more interpretable autonomous driving systems.","authors":["Amirhosein Chahe","Lifeng Zhou"],"url":"https://arxiv.org/abs/2504.10757"}
{"created":"2025-04-16","title":"Auto-Test: Learning Semantic-Domain Constraints for Unsupervised Error Detection in Tables","abstract":"Data cleaning is a long-standing challenge in data management. While powerful logic and statistical algorithms have been developed to detect and repair data errors in tables, existing algorithms predominantly rely on domain-experts to first manually specify data-quality constraints specific to a given table, before data cleaning algorithms can be applied.","authors":["Qixu Chen","Yeye He","Raymond Chi-Wing Wong","Weiwei Cui","Song Ge","Haidong Zhang","Dongmei Zhang","Surajit Chaudhuri"],"url":"https://arxiv.org/abs/2504.10762"}
{"created":"2025-04-16","title":"SeeTree -- A modular, open-source system for tree detection and orchard localization","abstract":"Accurate localization is an important functional requirement for precision orchard management. However, there are few off-the-shelf commercial solutions available to growers. In this paper, we present SeeTree, a modular, open source embedded system for tree trunk detection and orchard localization that is deployable on any vehicle. Building on our prior work on vision-based in-row localization using particle filters, SeeTree includes several new capabilities. First, it provides capacity for full orchard localization including out-of-row headland turning. Second, it includes the flexibility to integrate either visual, GNSS, or wheel odometry in the motion model. During field experiments in a commercial orchard, the system converged to the correct location 99% of the time over 800 trials, even when starting with large uncertainty in the initial particle locations. When turning out of row, the system correctly tracked 99% of the turns (860 trials representing 43 unique row changes). To help support adoption and future research and development, we make our dataset, design files, and source code freely available to the community.","authors":["Jostan Brown","Cindy Grimm","Joseph R. Davidson"],"url":"https://arxiv.org/abs/2504.10764"}
{"created":"2025-04-16","title":"Minimal Sensing for Orienting a Solar Panel","abstract":"A solar panel harvests the most energy when pointing in the direction that maximizes the total illumination (irradiance) falling on it. Given an arbitrary orientation of a panel and an arbitrary environmental illumination, we address the problem of finding the direction of maximum total irradiance. We develop a minimal sensing approach where measurements from just four photodetectors are used to iteratively vary the tilt of the panel to maximize the irradiance. Many environments produce irradiance functions with multiple local maxima. As a result, simply measuring the gradient of the irradiance function and applying gradient ascent will not work. We show that a larger, optimized tilt between the detectors and the panel is equivalent to blurring the irradiance function. This has the effect of eliminating local maxima and turning the irradiance function into a unimodal one, whose maximum can be found using gradient ascent. We show that there is a close relationship between our approach and scale space theory. We have collected a large dataset of high-dynamic range lighting environments in New York City, called \\textit{UrbanSky}. We used this dataset to conduct simulations to verify the robustness of our approach. Finally, we have built a portable solar panel with four compact detectors and an actuator to conduct experiments in various real-world settings: direct sunlight, cloudy sky, urban settings with occlusions and shadows, and complex indoor lighting. In all cases, we show significant improvements in harvested energy compared to standard approaches for controlling the orientation of a solar panel.","authors":["Jeremy Klotz","Shree K. Nayar"],"url":"https://arxiv.org/abs/2504.10765"}
{"created":"2025-04-16","title":"How Instruction and Reasoning Data shape Post-Training: Data Quality through the Lens of Layer-wise Gradients","abstract":"As the post-training of large language models (LLMs) advances from instruction-following to complex reasoning tasks, understanding how different data affect finetuning dynamics remains largely unexplored. In this paper, we present a spectral analysis of layer-wise gradients induced by low/high-quality instruction and reasoning data for LLM post-training. Our analysis reveals that widely-studied metrics for data evaluation, e.g., IFD, InsTag, Difficulty, and Reward, can be explained and unified by spectral properties computed from gradients' singular value decomposition (SVD). Specifically, higher-quality data are usually associated with lower nuclear norms and higher effective ranks. Notably, effective rank exhibits better robustness and resolution than nuclear norm in capturing subtle quality differences. For example, reasoning data achieves substantially higher effective ranks than instruction data, implying richer gradient structures on more complex tasks. Our experiments also highlight that models within the same family share similar gradient patterns regardless of their sizes, whereas different model families diverge significantly. Providing a unified view on the effects of data quality across instruction and reasoning data, this work illuminates the interplay between data quality and training stability, shedding novel insights into developing better data exploration strategies for post-training.","authors":["Ming Li","Yanhong Li","Ziyue Li","Tianyi Zhou"],"url":"https://arxiv.org/abs/2504.10766"}
{"created":"2025-04-16","title":"The Art of Audience Engagement: LLM-Based Thin-Slicing of Scientific Talks","abstract":"This paper examines the thin-slicing approach - the ability to make accurate judgments based on minimal information - in the context of scientific presentations. Drawing on research from nonverbal communication and personality psychology, we show that brief excerpts (thin slices) reliably predict overall presentation quality. Using a novel corpus of over one hundred real-life science talks, we employ Large Language Models (LLMs) to evaluate transcripts of full presentations and their thin slices. By correlating LLM-based evaluations of short excerpts with full-talk assessments, we determine how much information is needed for accurate predictions. Our results demonstrate that LLM-based evaluations align closely with human ratings, proving their validity, reliability, and efficiency. Critically, even very short excerpts (less than 10 percent of a talk) strongly predict overall evaluations. This suggests that the first moments of a presentation convey relevant information that is used in quality evaluations and can shape lasting impressions. The findings are robust across different LLMs and prompting strategies. This work extends thin-slicing research to public speaking and connects theories of impression formation to LLMs and current research on AI communication. We discuss implications for communication and social cognition research on message reception. Lastly, we suggest an LLM-based thin-slicing framework as a scalable feedback tool to enhance human communication.","authors":["Ralf Schm\\\"alzle","Sue Lim","Yuetong Du","Gary Bente"],"url":"https://arxiv.org/abs/2504.10768"}
{"created":"2025-04-16","title":"Collaborative Bayesian Optimization via Wasserstein Barycenters","abstract":"Motivated by the growing need for black-box optimization and data privacy, we introduce a collaborative Bayesian optimization (BO) framework that addresses both of these challenges. In this framework agents work collaboratively to optimize a function they only have oracle access to. In order to mitigate against communication and privacy constraints, agents are not allowed to share their data but can share their Gaussian process (GP) surrogate models. To enable collaboration under these constraints, we construct a central model to approximate the objective function by leveraging the concept of Wasserstein barycenters of GPs. This central model integrates the shared models without accessing the underlying data. A key aspect of our approach is a collaborative acquisition function that balances exploration and exploitation, allowing for the optimization of decision variables collaboratively in each iteration. We prove that our proposed algorithm is asymptotically consistent and that its implementation via Monte Carlo methods is numerically accurate. Through numerical experiments, we demonstrate that our approach outperforms other baseline collaborative frameworks and is competitive with centralized approaches that do not consider data privacy.","authors":["Donglin Zhan","Haoting Zhang","Rhonda Righter","Zeyu Zheng","James Anderson"],"url":"https://arxiv.org/abs/2504.10770"}
{"created":"2025-04-16","title":"Rainy: Unlocking Satellite Calibration for Deep Learning in Precipitation","abstract":"Precipitation plays a critical role in the Earth's hydrological cycle, directly affecting ecosystems, agriculture, and water resource management. Accurate precipitation estimation and prediction are crucial for understanding climate dynamics, disaster preparedness, and environmental monitoring. In recent years, artificial intelligence (AI) has gained increasing attention in quantitative remote sensing (QRS), enabling more advanced data analysis and improving precipitation estimation accuracy. Although traditional methods have been widely used for precipitation estimation, they face limitations due to the difficulty of data acquisition and the challenge of capturing complex feature relationships. Furthermore, the lack of standardized multi-source satellite datasets, and in most cases, the exclusive reliance on station data, significantly hinders the effective application of advanced AI models. To address these challenges, we propose the Rainy dataset, a multi-source spatio-temporal dataset that integrates pure satellite data with station data, and propose Taper Loss, designed to fill the gap in tasks where only in-situ data is available without area-wide support. The Rainy dataset supports five main tasks: (1) satellite calibration, (2) precipitation event prediction, (3) precipitation level prediction, (4) spatiotemporal prediction, and (5) precipitation downscaling. For each task, we selected benchmark models and evaluation metrics to provide valuable references for researchers. Using precipitation as an example, the Rainy dataset and Taper Loss demonstrate the seamless collaboration between QRS and computer vision, offering data support for AI for Science in the field of QRS and providing valuable insights for interdisciplinary collaboration and integration.","authors":["Zhenyu Yu","Hanqing Chen","Mohd Yamani Idna Idris","Pei Wang"],"url":"https://arxiv.org/abs/2504.10776"}
{"created":"2025-04-16","title":"AtlasD: Automatic Local Symmetry Discovery","abstract":"Existing symmetry discovery methods predominantly focus on global transformations across the entire system or space, but they fail to consider the symmetries in local neighborhoods. This may result in the reported symmetry group being a misrepresentation of the true symmetry. In this paper, we formalize the notion of local symmetry as atlas equivariance. Our proposed pipeline, automatic local symmetry discovery (AtlasD), recovers the local symmetries of a function by training local predictor networks and then learning a Lie group basis to which the predictors are equivariant. We demonstrate AtlasD is capable of discovering local symmetry groups with multiple connected components in top-quark tagging and partial differential equation experiments. The discovered local symmetry is shown to be a useful inductive bias that improves the performance of downstream tasks in climate segmentation and vision tasks.","authors":["Manu Bhat","Jonghyun Park","Jianke Yang","Nima Dehmamy","Robin Walters","Rose Yu"],"url":"https://arxiv.org/abs/2504.10777"}
{"created":"2025-04-16","title":"Deep Audio Watermarks are Shallow: Limitations of Post-Hoc Watermarking Techniques for Speech","abstract":"In the audio modality, state-of-the-art watermarking methods leverage deep neural networks to allow the embedding of human-imperceptible signatures in generated audio. The ideal is to embed signatures that can be detected with high accuracy when the watermarked audio is altered via compression, filtering, or other transformations. Existing audio watermarking techniques operate in a post-hoc manner, manipulating \"low-level\" features of audio recordings after generation (e.g. through the addition of a low-magnitude watermark signal). We show that this post-hoc formulation makes existing audio watermarks vulnerable to transformation-based removal attacks. Focusing on speech audio, we (1) unify and extend existing evaluations of the effect of audio transformations on watermark detectability, and (2) demonstrate that state-of-the-art post-hoc audio watermarks can be removed with no knowledge of the watermarking scheme and minimal degradation in audio quality.","authors":["Patrick O'Reilly","Zeyu Jin","Jiaqi Su","Bryan Pardo"],"url":"https://arxiv.org/abs/2504.10782"}
{"created":"2025-04-16","title":"Superfast Configuration-Space Convex Set Computation on GPUs for Online Motion Planning","abstract":"In this work, we leverage GPUs to construct probabilistically collision-free convex sets in robot configuration space on the fly. This extends the use of modern motion planning algorithms that leverage such representations to changing environments. These planners rapidly and reliably optimize high-quality trajectories, without the burden of challenging nonconvex collision-avoidance constraints. We present an algorithm that inflates collision-free piecewise linear paths into sequences of convex sets (SCS) that are probabilistically collision-free using massive parallelism. We then integrate this algorithm into a motion planning pipeline, which leverages dynamic roadmaps to rapidly find one or multiple collision-free paths, and inflates them. We then optimize the trajectory through the probabilistically collision-free sets, simultaneously using the candidate trajectory to detect and remove collisions from the sets. We demonstrate the efficacy of our approach on a simulation benchmark and a KUKA iiwa 7 robot manipulator with perception in the loop. On our benchmark, our approach runs 17.1 times faster and yields a 27.9% increase in reliability over the nonlinear trajectory optimization baseline, while still producing high-quality motion plans.","authors":["Peter Werner","Richard Cheng","Tom Stewart","Russ Tedrake","Daniela Rus"],"url":"https://arxiv.org/abs/2504.10783"}
{"created":"2025-04-16","title":"ATLASv2: LLM-Guided Adaptive Landmark Acquisition and Navigation on the Edge","abstract":"Autonomous systems deployed on edge devices face significant challenges, including resource constraints, real-time processing demands, and adapting to dynamic environments. This work introduces ATLASv2, a novel system that integrates a fine-tuned TinyLLM, real-time object detection, and efficient path planning to enable hierarchical, multi-task navigation and manipulation all on the edge device, Jetson Nano. ATLASv2 dynamically expands its navigable landmarks by detecting and localizing objects in the environment which are saved to its internal knowledge base to be used for future task execution. We evaluate ATLASv2 in real-world environments, including a handcrafted home and office setting constructed with diverse objects and landmarks. Results show that ATLASv2 effectively interprets natural language instructions, decomposes them into low-level actions, and executes tasks with high success rates. By leveraging generative AI in a fully on-board framework, ATLASv2 achieves optimized resource utilization with minimal prompting latency and power consumption, bridging the gap between simulated environments and real-world applications.","authors":["Mikolaj Walczak","Uttej Kallakuri","Tinoosh Mohsenin"],"url":"https://arxiv.org/abs/2504.10784"}
{"created":"2025-04-16","title":"Visual Language Models show widespread visual deficits on neuropsychological tests","abstract":"Visual Language Models (VLMs) show remarkable performance in visual reasoning tasks, successfully tackling college-level challenges that require high-level understanding of images. However, some recent reports of VLMs struggling to reason about elemental visual concepts like orientation, position, continuity, and occlusion suggest a potential gulf between human and VLM vision. Here we use the toolkit of neuropsychology to systematically assess the capabilities of three state-of-the-art VLMs across visual domains. Using 51 tests drawn from six clinical and experimental batteries, we characterise the visual abilities of leading VLMs relative to normative performance in healthy adults. While the models excel in straightforward object recognition tasks, we find widespread deficits in low- and mid-level visual abilities that would be considered clinically significant in humans. These selective deficits, profiled through validated test batteries, suggest that an artificial system can achieve complex object recognition without developing foundational visual concepts that in humans require no explicit training.","authors":["Gene Tangtartharakul","Katherine R. Storrs"],"url":"https://arxiv.org/abs/2504.10786"}
{"created":"2025-04-16","title":"GUM-SAGE: A Novel Dataset and Approach for Graded Entity Salience Prediction","abstract":"Determining and ranking the most salient entities in a text is critical for user-facing systems, especially as users increasingly rely on models to interpret long documents they only partially read. Graded entity salience addresses this need by assigning entities scores that reflect their relative importance in a text. Existing approaches fall into two main categories: subjective judgments of salience, which allow for gradient scoring but lack consistency, and summarization-based methods, which define salience as mention-worthiness in a summary, promoting explainability but limiting outputs to binary labels (entities are either summary-worthy or not). In this paper, we introduce a novel approach for graded entity salience that combines the strengths of both approaches. Using an English dataset spanning 12 spoken and written genres, we collect 5 summaries per document and calculate each entity's salience score based on its presence across these summaries. Our approach shows stronger correlation with scores based on human summaries and alignments, and outperforms existing techniques, including LLMs. We release our data and code at https://github.com/jl908069/gum_sum_salience to support further research on graded salient entity extraction.","authors":["Jessica Lin","Amir Zeldes"],"url":"https://arxiv.org/abs/2504.10792"}
{"created":"2025-04-16","title":"SonicSieve: Bringing Directional Speech Extraction to Smartphones Using Acoustic Microstructures","abstract":"Imagine placing your smartphone on a table in a noisy restaurant and clearly capturing the voices of friends seated around you, or recording a lecturer's voice with clarity in a reverberant auditorium. We introduce SonicSieve, the first intelligent directional speech extraction system for smartphones using a bio-inspired acoustic microstructure. Our passive design embeds directional cues onto incoming speech without any additional electronics. It attaches to the in-line mic of low-cost wired earphones which can be attached to smartphones. We present an end-to-end neural network that processes the raw audio mixtures in real-time on mobile devices. Our results show that SonicSieve achieves a signal quality improvement of 5.0 dB when focusing on a 30{\\deg} angular region. Additionally, the performance of our system based on only two microphones exceeds that of conventional 5-microphone arrays.","authors":["Kuang Yuan","Yifeng Wang","Xiyuxing Zhang","Chengyi Shen","Swarun Kumar","Justin Chan"],"url":"https://arxiv.org/abs/2504.10793"}
{"created":"2025-04-16","title":"3D Wavelet Convolutions with Extended Receptive Fields for Hyperspectral Image Classification","abstract":"Deep neural networks face numerous challenges in hyperspectral image classification, including high-dimensional data, sparse ground object distributions, and spectral redundancy, which often lead to classification overfitting and limited generalization capability. To better adapt to ground object distributions while expanding receptive fields without introducing excessive parameters and skipping redundant information, this paper proposes WCNet, an improved 3D-DenseNet model integrated with wavelet transforms. We introduce wavelet transforms to effectively extend convolutional receptive fields and guide CNNs to better respond to low frequencies through cascading, termed wavelet convolution. Each convolution focuses on different frequency bands of the input signal with gradually increasing effective ranges. This process enables greater emphasis on low-frequency components while adding only a small number of trainable parameters. This dynamic approach allows the model to flexibly focus on critical spatial structures when processing different regions, rather than relying on fixed receptive fields of single static kernels. The Wavelet Conv module enhances model representation capability by expanding receptive fields through 3D wavelet transforms without increasing network depth or width. Experimental results demonstrate superior performance on the IN, UP, and KSC datasets, outperforming mainstream hyperspectral image classification methods.","authors":["Guandong Li","Mengxia Ye"],"url":"https://arxiv.org/abs/2504.10795"}
{"created":"2025-04-16","title":"Name of Thrones: Evaluating How LLMs Rank Student Names, Race, and Gender in Status Hierarchies","abstract":"Across cultures, names tell a lot about their bearers as they carry deep personal and cultural significance. Names also serve as powerful signals of gender, race, and status in the social hierarchy - a pecking order in which individual positions shape others' expectations on their perceived competence and worth. With the widespread adoption of LLMs and as names are often an input for LLMs, it is crucial to evaluate whether LLMs may sort people into status positions based on first and last names and, if so, whether it is in an unfair, biased fashion. While prior work has primarily investigated biases in first names, little attention has been paid to last names and even less to the combined effects of first and last names. In this study, we conduct a large-scale analysis of name variations across 5 ethnicities to examine how AI exhibits name biases. Our study investigates three key characteristics of inequality and finds that LLMs reflect and reinforce status hierarchies based on names that signal gender and ethnicity as they encode differential expectations of competence, leadership, and economic potential. Contrary to the common assumption that AI tends to favor Whites, we show that East and, in some contexts, South Asian names receive higher rankings. We also disaggregate Asians, a population projected to be the largest immigrant group in the U.S. by 2055. Our results challenge the monolithic Asian model minority assumption, illustrating a more complex and stratified model of bias. Gender moderates biases, with girls facing unfair disadvantages in certain racial groups. Additionally, spanning cultural categories by adopting Western first names improves AI-perceived status for East and Southeast Asian students, particularly for girls. Our findings underscore the importance of intersectional and more nuanced understandings of race, gender, and mixed identities in the evaluation of LLMs.","authors":["Annabella Sakunkoo","Jonathan Sakunkoo"],"url":"https://arxiv.org/abs/2504.10797"}
{"created":"2025-04-16","title":"AdapCsiNet: Environment-Adaptive CSI Feedback via Scene Graph-Aided Deep Learning","abstract":"Accurate channel state information (CSI) is critical for realizing the full potential of multiple-antenna wireless communication systems. While deep learning (DL)-based CSI feedback methods have shown promise in reducing feedback overhead, their generalization capability across varying propagation environments remains limited due to their data-driven nature. Existing solutions based on online training improve adaptability but impose significant overhead in terms of data collection and computational resources. In this work, we propose AdapCsiNet, an environment-adaptive DL-based CSI feedback framework that eliminates the need for online training. By integrating environmental information -- represented as a scene graph -- into a hypernetwork-guided CSI reconstruction process, AdapCsiNet dynamically adapts to diverse channel conditions. A two-step training strategy is introduced to ensure baseline reconstruction performance and effective environment-aware adaptation. Simulation results demonstrate that AdapCsiNet achieves up to 46.4% improvement in CSI reconstruction accuracy and matches the performance of online learning methods without incurring additional runtime overhead.","authors":["Jiayi Liu","Jiajia Guo","Yiming Cui","Chao-Kai Wen","Shi Jin"],"url":"https://arxiv.org/abs/2504.10798"}
{"created":"2025-04-16","title":"Products of Recursive Programs for Hypersafety Verification","abstract":"We study the problem of automated hypersafety verification of infinite-state recursive programs. We propose an infinite class of product programs, specifically designed with recursion in mind, that reduce the hypersafety verification of a recursive program to standard safety verification. For this, we combine insights from language theory and concurrency theory to propose an algorithmic solution for constructing an infinite class of recursive product programs. One key insight is that, using the simple theory of visibly pushdown languages, one can maintain the recursive structure of syntactic program alignments which is vital to constructing a new product program that can be viewed as a classic recursive program -- that is, one that can be executed on a single stack. Another key insight is that techniques from concurrency theory can be generalized to help define product programs based on the view that the parallel composition of individual recursive programs includes all possible alignments from which a sound set of alignments that faithfully preserve the satisfaction of the hypersafety property can be selected. On the practical side, we formulate a family of parametric canonical product constructions that are intuitive to programmers and can be used as building blocks to specify recursive product programs for the purpose of relational and hypersafety verification, with the idea that the right product program can be verified automatically using existing techniques. We demonstrate the effectiveness of these techniques through an implementation and highly promising experimental results.","authors":["Ruotong Cheng","Azadeh Farzan"],"url":"https://arxiv.org/abs/2504.10800"}
{"created":"2025-04-16","title":"The Sword of Damocles in ViTs: Computational Redundancy Amplifies Adversarial Transferability","abstract":"Vision Transformers (ViTs) have demonstrated impressive performance across a range of applications, including many safety-critical tasks. However, their unique architectural properties raise new challenges and opportunities in adversarial robustness. In particular, we observe that adversarial examples crafted on ViTs exhibit higher transferability compared to those crafted on CNNs, suggesting that ViTs contain structural characteristics favorable for transferable attacks. In this work, we investigate the role of computational redundancy in ViTs and its impact on adversarial transferability. Unlike prior studies that aim to reduce computation for efficiency, we propose to exploit this redundancy to improve the quality and transferability of adversarial examples. Through a detailed analysis, we identify two forms of redundancy, including the data-level and model-level, that can be harnessed to amplify attack effectiveness. Building on this insight, we design a suite of techniques, including attention sparsity manipulation, attention head permutation, clean token regularization, ghost MoE diversification, and test-time adversarial training. Extensive experiments on the ImageNet-1k dataset validate the effectiveness of our approach, showing that our methods significantly outperform existing baselines in both transferability and generality across diverse model architectures.","authors":["Jiani Liu","Zhiyuan Wang","Zeliang Zhang","Chao Huang","Susan Liang","Yunlong Tang","Chenliang Xu"],"url":"https://arxiv.org/abs/2504.10804"}
{"created":"2025-04-16","title":"Power-scaled Bayesian Inference with Score-based Generative mModels","abstract":"We propose a score-based generative algorithm for sampling from power-scaled priors and likelihoods within the Bayesian inference framework. Our algorithm enables flexible control over prior-likelihood influence without requiring retraining for different power-scaling configurations. Specifically, we focus on synthesizing seismic velocity models conditioned on imaged seismic. Our method enables sensitivity analysis by sampling from intermediate power posteriors, allowing us to assess the relative influence of the prior and likelihood on samples of the posterior distribution. Through a comprehensive set of experiments, we evaluate the effects of varying the power parameter in different settings: applying it solely to the prior, to the likelihood of a Bayesian formulation, and to both simultaneously. The results show that increasing the power of the likelihood up to a certain threshold improves the fidelity of posterior samples to the conditioning data (e.g., seismic images), while decreasing the prior power promotes greater structural diversity among samples. Moreover, we find that moderate scaling of the likelihood leads to a reduced shot data residual, confirming its utility in posterior refinement.","authors":["Huseyin Tuna Erdinc","Yunlin Zeng","Abhinav Prakash Gahlot","Felix J. Herrmann"],"url":"https://arxiv.org/abs/2504.10807"}
{"created":"2025-04-16","title":"Tabular foundation model to detect empathy from visual cues","abstract":"Detecting empathy from video interactions is an emerging area of research. Video datasets, however, are often released as extracted features (i.e., tabular data) rather than raw footage due to privacy and ethical concerns. Prior research on such tabular datasets established tree-based classical machine learning approaches as the best-performing models. Motivated by the recent success of textual foundation models (i.e., large language models), we explore the use of tabular foundation models in empathy detection from tabular visual features. We experiment with two recent tabular foundation models $-$ TabPFN v2 and TabICL $-$ through in-context learning and fine-tuning setups. Our experiments on a public human-robot interaction benchmark demonstrate a significant boost in cross-subject empathy detection accuracy over several strong baselines (accuracy: $0.590 \\rightarrow 0.730$; AUC: $0.564 \\rightarrow 0.669$). In addition to performance improvement, we contribute novel insights and an evaluation setup to ensure generalisation on unseen subjects in this public benchmark. As the practice of releasing video features as tabular datasets is likely to persist due to privacy constraints, our findings will be widely applicable to future empathy detection video datasets as well.","authors":["Md Rakibul Hasan","Shafin Rahman","Md Zakir Hossain","Aneesh Krishna","Tom Gedeon"],"url":"https://arxiv.org/abs/2504.10808"}
{"created":"2025-04-16","title":"GaSLight: Gaussian Splats for Spatially-Varying Lighting in HDR","abstract":"We present GaSLight, a method that generates spatially-varying lighting from regular images. Our method proposes using HDR Gaussian Splats as light source representation, marking the first time regular images can serve as light sources in a 3D renderer. Our two-stage process first enhances the dynamic range of images plausibly and accurately by leveraging the priors embedded in diffusion models. Next, we employ Gaussian Splats to model 3D lighting, achieving spatially variant lighting. Our approach yields state-of-the-art results on HDR estimations and their applications in illuminating virtual objects and scenes. To facilitate the benchmarking of images as light sources, we introduce a novel dataset of calibrated and unsaturated HDR to evaluate images as light sources. We assess our method using a combination of this novel dataset and an existing dataset from the literature. The code to reproduce our method will be available upon acceptance.","authors":["Christophe Bolduc","Yannick Hold-Geoffroy","Zhixin Shu","Jean-Fran\\c{c}ois Lalonde"],"url":"https://arxiv.org/abs/2504.10809"}
{"created":"2025-04-16","title":"PatrolVision: Automated License Plate Recognition in the wild","abstract":"Adoption of AI driven techniques in public services remains low due to challenges related to accuracy and speed of information at population scale. Computer vision techniques for traffic monitoring have not gained much popularity despite their relative strength in areas such as autonomous driving. Despite large number of academic methods for Automatic License Plate Recognition (ALPR) systems, very few provide an end to end solution for patrolling in the city. This paper presents a novel prototype for a low power GPU based patrolling system to be deployed in an urban environment on surveillance vehicles for automated vehicle detection, recognition and tracking. In this work, we propose a complete ALPR system for Singapore license plates having both single and double line creating our own YOLO based network. We focus on unconstrained capture scenarios as would be the case in real world application, where the license plate (LP) might be considerably distorted due to oblique views. In this work, we first detect the license plate from the full image using RFB-Net and rectify multiple distorted license plates in a single image. After that, the detected license plate image is fed to our network for character recognition. We evaluate the performance of our proposed system on a newly built dataset covering more than 16,000 images. The system was able to correctly detect license plates with 86\\% precision and recognize characters of a license plate in 67\\% of the test set, and 89\\% accuracy with one incorrect character (partial match). We also test latency of our system and achieve 64FPS on Tesla P4 GPU","authors":["Anmol Singhal Navya Singhal"],"url":"https://arxiv.org/abs/2504.10810"}
{"created":"2025-04-16","title":"FlexiContracts: A Novel and Efficient Scheme for Upgrading Smart Contracts in Ethereum Blockchain","abstract":"Blockchain technology has revolutionized contractual processes, enhancing efficiency and trust through smart contracts. Ethereum, as a pioneer in this domain, offers a platform for decentralized applications but is challenged by the immutability of smart contracts, which makes upgrades cumbersome. Existing design patterns, while addressing upgradability, introduce complexity, increased development effort, and higher gas costs, thus limiting their effectiveness. In response, we introduce FlexiContracts, an innovative scheme that reimagines the evolution of smart contracts on Ethereum. By enabling secure, in-place upgrades without losing historical data, FlexiContracts surpasses existing approaches, introducing a previously unexplored path in smart contract evolution. Its streamlined design transcends the limitations of current design patterns by simplifying smart contract development, eliminating the need for extensive upfront planning, and significantly reducing the complexity of the design process. This advancement fosters an environment for continuous improvement and adaptation to new requirements, redefining the possibilities for dynamic, upgradable smart contracts.","authors":["Tahrim Hossain","Sakib Hassan","Faisal Haque Bappy","Muhammad Nur Yanhaona","Sarker Ahmed Rumee","Moinul Zaber","Tariqul Islam"],"url":"https://arxiv.org/abs/2504.10811"}
{"created":"2025-04-16","title":"E2E Parking Dataset: An Open Benchmark for End-to-End Autonomous Parking","abstract":"End-to-end learning has shown great potential in autonomous parking, yet the lack of publicly available datasets limits reproducibility and benchmarking. While prior work introduced a visual-based parking model and a pipeline for data generation, training, and close-loop test, the dataset itself was not released. To bridge this gap, we create and open-source a high-quality dataset for end-to-end autonomous parking. Using the original model, we achieve an overall success rate of 85.16% with lower average position and orientation errors (0.24 meters and 0.34 degrees).","authors":["Kejia Gao","Liguo Zhou","Mingjun Liu","Alois Knoll"],"url":"https://arxiv.org/abs/2504.10812"}
{"created":"2025-04-16","title":"Enhanced Data Race Prediction Through Modular Reasoning","abstract":"There are two orthogonal methodologies for efficient prediction of data races from concurrent program runs: commutativity and prefix reasoning. There are several instances of each methodology in the literature, with the goal of predicting data races using a streaming algorithm where the required memory does not grow proportional to the length of the observed run, but these instances were mostly created in an ad hoc manner, without much attention to their unifying underlying principles. In this paper, we identify and formalize these principles for each category with the ultimate goal of paving the way for combining them into a new algorithm which shares their efficiency characteristics but offers strictly more prediction power. In particular, we formalize three distinct classes of races predictable using commutativity reasoning, and compare them. We identify three different styles of prefix reasoning, and prove that they predict the same class of races, which provably contains all races predictable by any commutativity reasoning technique.","authors":["Zhendong Ang","Azadeh Farzan","Umang Mathur"],"url":"https://arxiv.org/abs/2504.10813"}
{"created":"2025-04-16","title":"CSPLADE: Learned Sparse Retrieval with Causal Language Models","abstract":"In recent years, dense retrieval has been the focus of information retrieval (IR) research. While effective, dense retrieval produces uninterpretable dense vectors, and suffers from the drawback of large index size. Learned sparse retrieval (LSR) has emerged as promising alternative, achieving competitive retrieval performance while also being able to leverage the classical inverted index data structure for efficient retrieval. However, limited works have explored scaling LSR beyond BERT scale. In this work, we identify two challenges in training large language models (LLM) for LSR: (1) training instability during the early stage of contrastive training; (2) suboptimal performance due to pre-trained LLM's unidirectional attention. To address these challenges, we propose two corresponding techniques: (1) a lightweight adaptation training phase to eliminate training instability; (2) two model variants to enable bidirectional information. With these techniques, we are able to train LSR models with 8B scale LLM, and achieve competitive retrieval performance with reduced index size. Furthermore, we are among the first to analyze the performance-efficiency tradeoff of LLM-based LSR model through the lens of model quantization. Our findings provide insights into adapting LLMs for efficient retrieval modeling.","authors":["Zhichao Xu","Aosong Feng","Yijun Tian","Haibo Ding","Lin Leee Cheong"],"url":"https://arxiv.org/abs/2504.10816"}
{"created":"2025-04-16","title":"FHBench: Towards Efficient and Personalized Federated Learning for Multimodal Healthcare","abstract":"Federated Learning (FL) has emerged as an effective solution for multi-institutional collaborations without sharing patient data, offering a range of methods tailored for diverse applications. However, real-world medical datasets are often multimodal, and computational resources are limited, posing significant challenges for existing FL approaches. Recognizing these limitations, we developed the Federated Healthcare Benchmark(FHBench), a benchmark specifically designed from datasets derived from real-world healthcare applications. FHBench encompasses critical diagnostic tasks across domains such as the nervous, cardiovascular, and respiratory systems and general pathology, providing comprehensive support for multimodal healthcare evaluations and filling a significant gap in existing benchmarks. Building on FHBench, we introduced Efficient Personalized Federated Learning with Adaptive LoRA(EPFL), a personalized FL framework that demonstrates superior efficiency and effectiveness across various healthcare modalities. Our results highlight the robustness of FHBench as a benchmarking tool and the potential of EPFL as an innovative approach to advancing healthcare-focused FL, addressing key limitations of existing methods.","authors":["Penghao Wang","Qian Chen","Teng Zhang","Yingwei Zhang","Wang Lu","Yiqiang Chen"],"url":"https://arxiv.org/abs/2504.10817"}
{"created":"2025-04-16","title":"Generalized Audio Deepfake Detection Using Frame-level Latent Information Entropy","abstract":"Generalizability, the capacity of a robust model to perform effectively on unseen data, is crucial for audio deepfake detection due to the rapid evolution of text-to-speech (TTS) and voice conversion (VC) technologies. A promising approach to differentiate between bonafide and spoof samples lies in identifying intrinsic disparities to enhance model generalizability. From an information-theoretic perspective, we hypothesize the information content is one of the intrinsic differences: bonafide sample represents a dense, information-rich sampling of the real world, whereas spoof sample is typically derived from lower-dimensional, less informative representations. To implement this, we introduce frame-level latent information entropy detector(f-InfoED), a framework that extracts distinctive information entropy from latent representations at the frame level to identify audio deepfakes. Furthermore, we present AdaLAM, which extends large pre-trained audio models with trainable adapters for enhanced feature extraction. To facilitate comprehensive evaluation, the audio deepfake forensics 2024 (ADFF 2024) dataset was built by the latest TTS and VC methods. Extensive experiments demonstrate that our proposed approach achieves state-of-the-art performance and exhibits remarkable generalization capabilities. Further analytical studies confirms the efficacy of AdaLAM in extracting discriminative audio features and f-InfoED in leveraging latent entropy information for more generalized deepfake detection.","authors":["Botao Zhao","Zuheng Kang","Yayun He","Xiaoyang Qu","Junqing Peng","Jing Xiao","Jianzong Wang"],"url":"https://arxiv.org/abs/2504.10819"}
{"created":"2025-04-16","title":"Progressive Rock Music Classification","abstract":"This study investigates the classification of progressive rock music, a genre characterized by complex compositions and diverse instrumentation, distinct from other musical styles. Addressing this Music Information Retrieval (MIR) task, we extracted comprehensive audio features, including spectrograms, Mel-Frequency Cepstral Coefficients (MFCCs), chromagrams, and beat positions from song snippets using the Librosa library. A winner-take-all voting strategy was employed to aggregate snippet-level predictions into final song classifications. We conducted a comparative analysis of various machine learning techniques. Ensemble methods, encompassing Bagging (Random Forest, ExtraTrees, Bagging Classifier) and Boosting (XGBoost, Gradient Boosting), were explored, utilizing Principal Component Analysis (PCA) for dimensionality reduction to manage computational constraints with high-dimensional feature sets. Additionally, deep learning approaches were investigated, including the development of custom 1D Convolutional Neural Network (1D CNN) architectures (named \"Zuck\" and \"Satya\") featuring specific layer configurations, normalization, and activation functions. Furthermore, we fine-tuned a state-of-the-art Audio Spectrogram Transformer (AST) model, leveraging its attention-based mechanisms for audio classification. Performance evaluation on validation and test sets revealed varying effectiveness across models, with ensemble methods like Extra Trees achieving test accuracies up to 76.38%. This research provides insights into the application and relative performance of diverse machine learning paradigms for the nuanced task of progressive rock genre classification.","authors":["Arpan Nagar","Joseph Bensabat","Jokent Gaza","Moinak Dey"],"url":"https://arxiv.org/abs/2504.10821"}
{"created":"2025-04-16","title":"IlluSign: Illustrating Sign Language Videos by Leveraging the Attention Mechanism","abstract":"Sign languages are dynamic visual languages that involve hand gestures, in combination with non manual elements such as facial expressions. While video recordings of sign language are commonly used for education and documentation, the dynamic nature of signs can make it challenging to study them in detail, especially for new learners and educators. This work aims to convert sign language video footage into static illustrations, which serve as an additional educational resource to complement video content. This process is usually done by an artist, and is therefore quite costly. We propose a method that illustrates sign language videos by leveraging generative models' ability to understand both the semantic and geometric aspects of images. Our approach focuses on transferring a sketch like illustration style to video footage of sign language, combining the start and end frames of a sign into a single illustration, and using arrows to highlight the hand's direction and motion. While many style transfer methods address domain adaptation at varying levels of abstraction, applying a sketch like style to sign languages, especially for hand gestures and facial expressions, poses a significant challenge. To tackle this, we intervene in the denoising process of a diffusion model, injecting style as keys and values into high resolution attention layers, and fusing geometric information from the image and edges as queries. For the final illustration, we use the attention mechanism to combine the attention weights from both the start and end illustrations, resulting in a soft combination. Our method offers a cost effective solution for generating sign language illustrations at inference time, addressing the lack of such resources in educational materials.","authors":["Janna Bruner","Amit Moryossef","Lior Wolf"],"url":"https://arxiv.org/abs/2504.10822"}
{"created":"2025-04-16","title":"CLASH: Evaluating Language Models on Judging High-Stakes Dilemmas from Multiple Perspectives","abstract":"Navigating high-stakes dilemmas involving conflicting values is challenging even for humans, let alone for AI. Yet prior work in evaluating the reasoning capabilities of large language models (LLMs) in such situations has been limited to everyday scenarios. To close this gap, this work first introduces CLASH (Character perspective-based LLM Assessments in Situations with High-stakes), a meticulously curated dataset consisting of 345 high-impact dilemmas along with 3,795 individual perspectives of diverse values. In particular, we design CLASH in a way to support the study of critical aspects of value-based decision-making processes which are missing from prior work, including understanding decision ambivalence and psychological discomfort as well as capturing the temporal shifts of values in characters' perspectives. By benchmarking 10 open and closed frontier models, we uncover several key findings. (1) Even the strongest models, such as GPT-4o and Claude-Sonnet, achieve less than 50% accuracy in identifying situations where the decision should be ambivalent, while they perform significantly better in clear-cut scenarios. (2) While LLMs reasonably predict psychological discomfort as marked by human, they inadequately comprehend perspectives involving value shifts, indicating a need for LLMs to reason over complex values. (3) Our experiments also reveal a significant correlation between LLMs' value preferences and their steerability towards a given value. (4) Finally, LLMs exhibit greater steerability when engaged in value reasoning from a third-party perspective, compared to a first-person setup, though certain value pairs benefit uniquely from the first-person framing.","authors":["Ayoung Lee","Ryan Sungmo Kwon","Peter Railton","Lu Wang"],"url":"https://arxiv.org/abs/2504.10823"}
{"created":"2025-04-16","title":"OmniVDiff: Omni Controllable Video Diffusion for Generation and Understanding","abstract":"In this paper, we propose a novel framework for controllable video diffusion, OmniVDiff, aiming to synthesize and comprehend multiple video visual content in a single diffusion model. To achieve this, OmniVDiff treats all video visual modalities in the color space to learn a joint distribution, while employing an adaptive control strategy that dynamically adjusts the role of each visual modality during the diffusion process, either as a generation modality or a conditioning modality. This allows flexible manipulation of each modality's role, enabling support for a wide range of tasks. Consequently, our model supports three key functionalities: (1) Text-conditioned video generation: multi-modal visual video sequences (i.e., rgb, depth, canny, segmentaion) are generated based on the text conditions in one diffusion process; (2) Video understanding: OmniVDiff can estimate the depth, canny map, and semantic segmentation across the input rgb frames while ensuring coherence with the rgb input; and (3) X-conditioned video generation: OmniVDiff generates videos conditioned on fine-grained attributes (e.g., depth maps or segmentation maps). By integrating these diverse tasks into a unified video diffusion framework, OmniVDiff enhances the flexibility and scalability for controllable video diffusion, making it an effective tool for a variety of downstream applications, such as video-to-video translation. Extensive experiments demonstrate the effectiveness of our approach, highlighting its potential for various video-related applications.","authors":["Dianbing Xi","Jiepeng Wang","Yuanzhi Liang","Xi Qiu","Yuchi Huo","Rui Wang","Chi Zhang","Xuelong Li"],"url":"https://arxiv.org/abs/2504.10825"}
{"created":"2025-04-16","title":"SteerMusic: Enhanced Musical Consistency for Zero-shot Text-Guided and Personalized Music Editing","abstract":"Music editing is an important step in music production, which has broad applications, including game development and film production. Most existing zero-shot text-guided methods rely on pretrained diffusion models by involving forward-backward diffusion processes for editing. However, these methods often struggle to maintain the music content consistency. Additionally, text instructions alone usually fail to accurately describe the desired music. In this paper, we propose two music editing methods that enhance the consistency between the original and edited music by leveraging score distillation. The first method, SteerMusic, is a coarse-grained zero-shot editing approach using delta denoising score. The second method, SteerMusic+, enables fine-grained personalized music editing by manipulating a concept token that represents a user-defined musical style. SteerMusic+ allows for the editing of music into any user-defined musical styles that cannot be achieved by the text instructions alone. Experimental results show that our methods outperform existing approaches in preserving both music content consistency and editing fidelity. User studies further validate that our methods achieve superior music editing quality. Audio examples are available on https://steermusic.pages.dev/.","authors":["Xinlei Niu","Kin Wai Cheuk","Jing Zhang","Naoki Murata","Chieh-Hsin Lai","Michele Mancusi","Woosung Choi","Giorgio Fabbro","Wei-Hsiang Liao","Charles Patrick Martin","Yuki Mitsufuji"],"url":"https://arxiv.org/abs/2504.10826"}
{"created":"2025-04-16","title":"Following Is All You Need: Robot Crowd Navigation Using People As Planners","abstract":"Navigating in crowded environments requires the robot to be equipped with high-level reasoning and planning techniques. Existing works focus on developing complex and heavyweight planners while ignoring the role of human intelligence. Since humans are highly capable agents who are also widely available in a crowd navigation setting, we propose an alternative scheme where the robot utilises people as planners to benefit from their effective planning decisions and social behaviours. Through a set of rule-based evaluations, we identify suitable human leaders who exhibit the potential to guide the robot towards its goal. Using a simple base planner, the robot follows the selected leader through shorthorizon subgoals that are designed to be straightforward to achieve. We demonstrate through both simulated and real-world experiments that our novel framework generates safe and efficient robot plans compared to existing planners, even without predictive or data-driven modules. Our method also brings human-like robot behaviours without explicitly defining traffic rules and social norms. Code will be available at https://github.com/centiLinda/PeopleAsPlanner.git.","authors":["Yuwen Liao","Xinhang Xu","Ruofei Bai","Yizhuo Yang","Muqing Cao","Shenghai Yuan","Lihua Xie"],"url":"https://arxiv.org/abs/2504.10828"}
{"created":"2025-04-16","title":"LayoutCoT: Unleashing the Deep Reasoning Potential of Large Language Models for Layout Generation","abstract":"Conditional layout generation aims to automatically generate visually appealing and semantically coherent layouts from user-defined constraints. While recent methods based on generative models have shown promising results, they typically require substantial amounts of training data or extensive fine-tuning, limiting their versatility and practical applicability. Alternatively, some training-free approaches leveraging in-context learning with Large Language Models (LLMs) have emerged, but they often suffer from limited reasoning capabilities and overly simplistic ranking mechanisms, which restrict their ability to generate consistently high-quality layouts. To this end, we propose LayoutCoT, a novel approach that leverages the reasoning capabilities of LLMs through a combination of Retrieval-Augmented Generation (RAG) and Chain-of-Thought (CoT) techniques. Specifically, LayoutCoT transforms layout representations into a standardized serialized format suitable for processing by LLMs. A Layout-aware RAG is used to facilitate effective retrieval and generate a coarse layout by LLMs. This preliminary layout, together with the selected exemplars, is then fed into a specially designed CoT reasoning module for iterative refinement, significantly enhancing both semantic coherence and visual quality. We conduct extensive experiments on five public datasets spanning three conditional layout generation tasks. Experimental results demonstrate that LayoutCoT achieves state-of-the-art performance without requiring training or fine-tuning. Notably, our CoT reasoning module enables standard LLMs, even those without explicit deep reasoning abilities, to outperform specialized deep-reasoning models such as deepseek-R1, highlighting the potential of our approach in unleashing the deep reasoning capabilities of LLMs for layout generation tasks.","authors":["Hengyu Shi","Junhao Su","Huansheng Ning","Xiaoming Wei","Jialin Gao"],"url":"https://arxiv.org/abs/2504.10829"}
{"created":"2025-04-16","title":"Radiation Footprint Control in Cell-Free Cooperative ISAC: Optimal Joint BS Activation and Beamforming Coordination","abstract":"Coordinated beamforming across distributed base stations (BSs) in cell-free architectures can efficiently support integrated sensing and communication (ISAC) users by improving resource sharing and reducing conflicts in the spatial domain. However, coordinating numerous BSs within the ISAC network poses risks of generating substantial interference for other networks sharing the spectrum, while also increasing operational costs from power consumption and signaling overhead. Therefore, in this paper, we propose an interference-suppressed and cost-optimized cell-free ISAC network by opportunistically and cooperatively orchestrating distributed radio resources to address competing sensing and communication (S\\&amp;C) demands. Specifically, we conceive a radiation footprint control mechanism that autonomously suppresses interference across the entire signal propagation space to safeguard other networks without exchanging signaling. Then, we propose joint BS activation and beamforming coordination to dynamically activate appropriate BSs and orchestrate their spatial beams for service provisioning. Building upon this framework, we formulate a cost-efficient utility maximization problem that considers individual S\\&amp;C demands and location-dependent radiation footprint constraints. Since this results in a non-convex optimization problem, we develop a monotonic optimization embedded branch-and-bound (MO-BRB) algorithm to find the optimal solution. Additionally, we apply a low-complexity iterative method to obtain near-optimal solutions. Finally, simulation results validate the effectiveness of the proposed algorithms.","authors":["Jie Chen","Xianbin Wang"],"url":"https://arxiv.org/abs/2504.10830"}
{"created":"2025-04-16","title":"Hallucination-Aware Generative Pretrained Transformer for Cooperative Aerial Mobility Control","abstract":"This paper proposes SafeGPT, a two-tiered framework that integrates generative pretrained transformers (GPTs) with reinforcement learning (RL) for efficient and reliable unmanned aerial vehicle (UAV) last-mile deliveries. In the proposed design, a Global GPT module assigns high-level tasks such as sector allocation, while an On-Device GPT manages real-time local route planning. An RL-based safety filter monitors each GPT decision and overrides unsafe actions that could lead to battery depletion or duplicate visits, effectively mitigating hallucinations. Furthermore, a dual replay buffer mechanism helps both the GPT modules and the RL agent refine their strategies over time. Simulation results demonstrate that SafeGPT achieves higher delivery success rates compared to a GPT-only baseline, while substantially reducing battery consumption and travel distance. These findings validate the efficacy of combining GPT-based semantic reasoning with formal safety guarantees, contributing a viable solution for robust and energy-efficient UAV logistics.","authors":["Hyojun Ahn","Seungcheol Oh","Gyu Seon Kim","Soyi Jung","Soohyun Park","Joongheon Kim"],"url":"https://arxiv.org/abs/2504.10831"}
{"created":"2025-04-16","title":"Unlimited Vector Processing for Wireless Baseband Based on RISC-V Extension","abstract":"Wireless baseband processing (WBP) serves as an ideal scenario for utilizing vector processing, which excels in managing data-parallel operations due to its parallel structure. However, conventional vector architectures face certain constraints such as limited vector register sizes, reliance on power-of-two vector length multipliers, and vector permutation capabilities tied to specific architectures. To address these challenges, we have introduced an instruction set extension (ISE) based on RISC-V known as unlimited vector processing (UVP). This extension enhances both the flexibility and efficiency of vector computations. UVP employs a novel programming model that supports non-power-of-two register groupings and hardware strip-mining, thus enabling smooth handling of vectors of varying lengths while reducing the software strip-mining burden. Vector instructions are categorized into symmetric and asymmetric classes, complemented by specialized load/store strategies to optimize execution. Moreover, we present a hardware implementation of UVP featuring sophisticated hazard detection mechanisms, optimized pipelines for symmetric tasks such as fixed-point multiplication and division, and a robust permutation engine for effective asymmetric operations. Comprehensive evaluations demonstrate that UVP significantly enhances performance, achieving up to 3.0$\\times$ and 2.1$\\times$ speedups in matrix multiplication and fast Fourier transform (FFT) tasks, respectively, when measured against lane-based vector architectures. Our synthesized RTL for a 16-lane configuration using SMIC 40nm technology spans 0.94 mm$^2$ and achieves an area efficiency of 21.2 GOPS/mm$^2$.","authors":["Limin Jiang","Yi Shi","Yihao Shen","Shan Cao","Zhiyuan Jiang","Sheng Zhou"],"url":"https://arxiv.org/abs/2504.10832"}
{"created":"2025-04-16","title":"Towards Spatially-Aware and Optimally Faithful Concept-Based Explanations","abstract":"Post-hoc, unsupervised concept-based explanation methods (U-CBEMs) are a promising tool for generating semantic explanations of the decision-making processes in deep neural networks, having applications in both model improvement and understanding. It is vital that the explanation is accurate, or faithful, to the model, yet we identify several limitations of prior faithfulness metrics that inhibit an accurate evaluation; most notably, prior metrics involve only the set of concepts present, ignoring how they may be spatially distributed. We address these limitations with Surrogate Faithfulness (SF), an evaluation method that introduces a spatially-aware surrogate and two novel faithfulness metrics. Using SF, we produce Optimally Faithful (OF) explanations, where concepts are found that maximize faithfulness. Our experiments show that (1) adding spatial-awareness to prior U-CBEMs increases faithfulness in all cases; (2) OF produces significantly more faithful explanations than prior U-CBEMs (30% or higher improvement in error); (3) OF's learned concepts generalize well to out-of-domain data and are more robust to adversarial examples, where prior U-CBEMs struggle.","authors":["Shubham Kumar","Dwip Dalal","Narendra Ahuja"],"url":"https://arxiv.org/abs/2504.10833"}
{"created":"2025-04-16","title":"LightFormer: A lightweight and efficient decoder for remote sensing image segmentation","abstract":"Deep learning techniques have achieved remarkable success in the semantic segmentation of remote sensing images and in land-use change detection. Nevertheless, their real-time deployment on edge platforms remains constrained by decoder complexity. Herein, we introduce LightFormer, a lightweight decoder for time-critical tasks that involve unstructured targets, such as disaster assessment, unmanned aerial vehicle search-and-rescue, and cultural heritage monitoring. LightFormer employs a feature-fusion and refinement module built on channel processing and a learnable gating mechanism to aggregate multi-scale, multi-range information efficiently, which drastically curtails model complexity. Furthermore, we propose a spatial information selection module (SISM) that integrates long-range attention with a detail preservation branch to capture spatial dependencies across multiple scales, thereby substantially improving the recognition of unstructured targets in complex scenes. On the ISPRS Vaihingen benchmark, LightFormer attains 99.9% of GLFFNet's mIoU (83.9% vs. 84.0%) while requiring only 14.7% of its FLOPs and 15.9% of its parameters, thus achieving an excellent accuracy-efficiency trade-off. Consistent results on LoveDA, ISPRS Potsdam, RescueNet, and FloodNet further demonstrate its robustness and superior perception of unstructured objects. These findings highlight LightFormer as a practical solution for remote sensing applications where both computational economy and high-precision segmentation are imperative.","authors":["Sihang Chen","Lijun Yun","Ze Liu","JianFeng Zhu","Jie Chen","Hui Wang","Yueping Nie"],"url":"https://arxiv.org/abs/2504.10834"}
{"created":"2025-04-16","title":"Rethinking Theory of Mind Benchmarks for LLMs: Towards A User-Centered Perspective","abstract":"The last couple of years have witnessed emerging research that appropriates Theory-of-Mind (ToM) tasks designed for humans to benchmark LLM's ToM capabilities as an indication of LLM's social intelligence. However, this approach has a number of limitations. Drawing on existing psychology and AI literature, we summarize the theoretical, methodological, and evaluation limitations by pointing out that certain issues are inherently present in the original ToM tasks used to evaluate human's ToM, which continues to persist and exacerbated when appropriated to benchmark LLM's ToM. Taking a human-computer interaction (HCI) perspective, these limitations prompt us to rethink the definition and criteria of ToM in ToM benchmarks in a more dynamic, interactional approach that accounts for user preferences, needs, and experiences with LLMs in such evaluations. We conclude by outlining potential opportunities and challenges towards this direction.","authors":["Qiaosi Wang","Xuhui Zhou","Maarten Sap","Jodi Forlizzi","Hong Shen"],"url":"https://arxiv.org/abs/2504.10839"}
{"created":"2025-04-16","title":"A comprehensive review of remote sensing in wetland classification and mapping","abstract":"Wetlands constitute critical ecosystems that support both biodiversity and human well-being; however, they have experienced a significant decline since the 20th century. Back in the 1970s, researchers began to employ remote sensing technologies for wetland classification and mapping to elucidate the extent and variations of wetlands. Although some review articles summarized the development of this field, there is a lack of a thorough and in-depth understanding of wetland classification and mapping: (1) the scientific importance of wetlands, (2) major data, methods used in wetland classification and mapping, (3) driving factors of wetland changes, (4) current research paradigm and limitations, (5) challenges and opportunities in wetland classification and mapping under the context of technological innovation and global environmental change. In this review, we aim to provide a comprehensive perspective and new insights into wetland classification and mapping for readers to answer these questions. First, we conduct a meta-analysis of over 1,200 papers, encompassing wetland types, methods, sensor types, and study sites, examining prevailing trends in wetland classification and mapping. Next, we review and synthesize the wetland features and existing data and methods in wetland classification and mapping. We also summarize typical wetland mapping products and explore the intrinsic driving factors of wetland changes across multiple spatial and temporal scales. Finally, we discuss current limitations and propose future directions in response to global environmental change and technological innovation. This review consolidates our understanding of wetland remote sensing and offers scientific recommendations that foster transformative progress in wetland science.","authors":["Shuai Yuan","Xiangan Liang","Tianwu Lin","Shuang Chen","Rui Liu","Jie Wang","Hongsheng Zhang","Peng Gong"],"url":"https://arxiv.org/abs/2504.10842"}
{"created":"2025-04-16","title":"Moving Beyond Next-Token Prediction: Transformers are Context-Sensitive Language Generators","abstract":"Large Language Models (LLMs), powered by Transformers, have demonstrated human-like intelligence capabilities, yet their underlying mechanisms remain poorly understood. This paper presents a novel framework for interpreting LLMs as probabilistic left context-sensitive languages (CSLs) generators. We hypothesize that Transformers can be effectively decomposed into three fundamental components: context windows, attention mechanisms, and autoregressive generation frameworks. This decomposition allows for the development of more flexible and interpretable computational models, moving beyond the traditional view of attention and autoregression as inseparable processes. We argue that next-token predictions can be understood as probabilistic, dynamic approximations of left CSL production rules, providing an intuitive explanation for how simple token predictions can yield human-like intelligence outputs. Given that all CSLs are left context-sensitive (Penttonen, 1974), we conclude that Transformers stochastically approximate CSLs, which are widely recognized as models of human-like intelligence. This interpretation bridges the gap between Formal Language Theory and the observed generative power of Transformers, laying a foundation for future advancements in generative AI theory and applications. Our novel perspective on Transformer architectures will foster a deeper understanding of LLMs and their future potentials.","authors":["Phill Kyu Rhee"],"url":"https://arxiv.org/abs/2504.10845"}
{"created":"2025-04-16","title":"Mosaic: Client-driven Account Allocation Framework in Sharded Blockchains","abstract":"Recent account allocation studies in sharded blockchains are typically miner-driven, requiring miners to perform global optimizations for all accounts to enhance system-wide performance. This forces each miner to maintain a complete copy of the entire ledger, resulting in significant storage, communication, and computation overhead.","authors":["Yuanzhe Zhang","Shirui Pan","Jiangshan Yu"],"url":"https://arxiv.org/abs/2504.10846"}
{"created":"2025-04-16","title":"Ichiyo: Fragile and Transient Interaction in Neighborhood","abstract":"As the Internet develops, social networking and other communication tools have transformed people's relationships into something fast, visible, and geographically huge. However, these communication tools have not expanded opportunities for acquainting oneself with neighbors outside one's social network; rather, they have comparatively diminished occasions for interacting with unfamiliar neighbors by prioritizing communication with existing friends. Therefore, we invented the medium Ichiyo to increase the opportunities to think of neighbors walking along the same street or in the same neighborhood and to expand the imagination of those who pass by and those who used to be there. Thus, users can engage in indirect interaction. We used commercially available laser cutters to engrave QR codes on leaves that are naturally found in our living space to prevent environmental invasion. The QR codes lead to a communal space on the web where users can freely leave messages. By engraving QR codes, information can be virtually expanded to be presented. To get the feedback of Ichiyo, we let a total of several thousand people experience a new way of communication as a part of the exhibition ''iii Exhibition 2022'', an art exhibition at the University of Tokyo. A total of more than 1,000 leaves engraved with QR codes were prepared and scattered at the exhibition site and along the road from the nearest station to the venue.","authors":["Hirofumi Shibata","Ayako Yogo","Naoto Nishida","Yu Shimada","Toma Ishii"],"url":"https://arxiv.org/abs/2504.10848"}
{"created":"2025-04-16","title":"Real-Time Word-Level Temporal Segmentation in Streaming Speech Recognition","abstract":"Rich-text captions are essential to help communication for Deaf and hard-of-hearing (DHH) people, second-language learners, and those with autism spectrum disorder (ASD). They also preserve nuances when converting speech to text, enhancing the realism of presentation scripts and conversation or speech logs. However, current real-time captioning systems lack the capability to alter text attributes (ex. capitalization, sizes, and fonts) at the word level, hindering the accurate conveyance of speaker intent that is expressed in the tones or intonations of the speech. For example, ''YOU should do this'' tends to be considered as indicating ''You'' as the focus of the sentence, whereas ''You should do THIS'' tends to be ''This'' as the focus. This paper proposes a solution that changes the text decorations at the word level in real time. As a prototype, we developed an application that adjusts word size based on the loudness of each spoken word. Feedback from users implies that this system helped to convey the speaker's intent, offering a more engaging and accessible captioning experience.","authors":["Naoto Nishida","Hirotaka Hiraki","Jun Rekimoto","Yoshio Ishiguro"],"url":"https://arxiv.org/abs/2504.10849"}
{"created":"2025-04-16","title":"How to Enhance Downstream Adversarial Robustness (almost) without Touching the Pre-Trained Foundation Model?","abstract":"With the rise of powerful foundation models, a pre-training-fine-tuning paradigm becomes increasingly popular these days: A foundation model is pre-trained using a huge amount of data from various sources, and then the downstream users only need to fine-tune and adapt it to specific downstream tasks. However, due to the high computation complexity of adversarial training, it is not feasible to fine-tune the foundation model to improve its robustness on the downstream task. Observing the above challenge, we want to improve the downstream robustness without updating/accessing the weights in the foundation model. Inspired from existing literature in robustness inheritance (Kim et al., 2020), through theoretical investigation, we identify a close relationship between robust contrastive learning with the adversarial robustness of supervised learning. To further validate and utilize this theoretical insight, we design a simple-yet-effective robust auto-encoder as a data pre-processing method before feeding the data into the foundation model. The proposed approach has zero access to the foundation model when training the robust auto-encoder. Extensive experiments demonstrate the effectiveness of the proposed method in improving the robustness of downstream tasks, verifying the connection between the feature robustness (implied by small adversarial contrastive loss) and the robustness of the downstream task.","authors":["Meiqi Liu","Zhuoqun Huang","Yue Xing"],"url":"https://arxiv.org/abs/2504.10850"}
{"created":"2025-04-16","title":"ICAFS: Inter-Client-Aware Feature Selection for Vertical Federated Learning","abstract":"Vertical federated learning (VFL) enables a paradigm for vertically partitioned data across clients to collaboratively train machine learning models. Feature selection (FS) plays a crucial role in Vertical Federated Learning (VFL) due to the unique nature that data are distributed across multiple clients. In VFL, different clients possess distinct subsets of features for overlapping data samples, making the process of identifying and selecting the most relevant features a complex yet essential task. Previous FS efforts have primarily revolved around intra-client feature selection, overlooking vital feature interaction across clients, leading to subpar model outcomes. We introduce ICAFS, a novel multi-stage ensemble approach for effective FS in VFL by considering inter-client interactions. By employing conditional feature synthesis alongside multiple learnable feature selectors, ICAFS facilitates ensemble FS over these selectors using synthetic embeddings. This method bypasses the limitations of private gradient sharing and allows for model training using real data with refined embeddings. Experiments on multiple real-world datasets demonstrate that ICAFS surpasses current state-of-the-art methods in prediction accuracy.","authors":["Ruochen Jin","Boning Tong","Shu Yang","Bojian Hou","Li Shen"],"url":"https://arxiv.org/abs/2504.10851"}
{"created":"2025-04-16","title":"Enhancing Features in Long-tailed Data Using Large Vision Mode","abstract":"Language-based foundation models, such as large language models (LLMs) or large vision-language models (LVLMs), have been widely studied in long-tailed recognition. However, the need for linguistic data is not applicable to all practical tasks. In this study, we aim to explore using large vision models (LVMs) or visual foundation models (VFMs) to enhance long-tailed data features without any language information. Specifically, we extract features from the LVM and fuse them with features in the baseline network's map and latent space to obtain the augmented features. Moreover, we design several prototype-based losses in the latent space to further exploit the potential of the augmented features. In the experimental section, we validate our approach on two benchmark datasets: ImageNet-LT and iNaturalist2018.","authors":["Pengxiao Han","Changkun Ye","Jinguang Tong","Cuicui Jiang","Jie Hong","Li Fang","Xuesong Li"],"url":"https://arxiv.org/abs/2504.10852"}
{"created":"2025-04-16","title":"PT-Mark: Invisible Watermarking for Text-to-image Diffusion Models via Semantic-aware Pivotal Tuning","abstract":"Watermarking for diffusion images has drawn considerable attention due to the widespread use of text-to-image diffusion models and the increasing need for their copyright protection. Recently, advanced watermarking techniques, such as Tree Ring, integrate watermarks by embedding traceable patterns (e.g., Rings) into the latent distribution during the diffusion process. Such methods disrupt the original semantics of the generated images due to the inevitable distribution shift caused by the watermarks, thereby limiting their practicality, particularly in digital art creation. In this work, we present Semantic-aware Pivotal Tuning Watermarks (PT-Mark), a novel invisible watermarking method that preserves both the semantics of diffusion images and the traceability of the watermark. PT-Mark preserves the original semantics of the watermarked image by gradually aligning the generation trajectory with the original (pivotal) trajectory while maintaining the traceable watermarks during whole diffusion denoising process. To achieve this, we first compute the salient regions of the watermark at each diffusion denoising step as a spatial prior to identify areas that can be aligned without disrupting the watermark pattern. Guided by the region, we then introduce an additional pivotal tuning branch that optimizes the text embedding to align the semantics while preserving the watermarks. Extensive evaluations demonstrate that PT-Mark can preserve the original semantics of the diffusion images while integrating robust watermarks. It achieves a 10% improvement in the performance of semantic preservation (i.e., SSIM, PSNR, and LPIPS) compared to state-of-the-art watermarking methods, while also showing comparable robustness against real-world perturbations and four times greater efficiency.","authors":["Yaopeng Wang","Huiyu Xu","Zhibo Wang","Jiacheng Du","Zhichao Li","Yiming Li","Qiu Wang","Kui Ren"],"url":"https://arxiv.org/abs/2504.10853"}
{"created":"2025-04-16","title":"LVLM_CSP: Accelerating Large Vision Language Models via Clustering, Scattering, and Pruning for Reasoning Segmentation","abstract":"Large Vision Language Models (LVLMs) have been widely adopted to guide vision foundation models in performing reasoning segmentation tasks, achieving impressive performance. However, the substantial computational overhead associated with LVLMs presents a new challenge. The primary source of this computational cost arises from processing hundreds of image tokens. Therefore, an effective strategy to mitigate such overhead is to reduce the number of image tokens, a process known as image token pruning. Previous studies on image token pruning for LVLMs have primarily focused on high level visual understanding tasks, such as visual question answering and image captioning. In contrast, guiding vision foundation models to generate accurate visual masks based on textual queries demands precise semantic and spatial reasoning capabilities. Consequently, pruning methods must carefully control individual image tokens throughout the LVLM reasoning process. Our empirical analysis reveals that existing methods struggle to adequately balance reductions in computational overhead with the necessity to maintain high segmentation accuracy. In this work, we propose LVLM_CSP, a novel training free visual token pruning method specifically designed for LVLM based reasoning segmentation tasks. LVLM_CSP consists of three stages: clustering, scattering, and pruning. Initially, the LVLM performs coarse-grained visual reasoning using a subset of selected image tokens. Next, fine grained reasoning is conducted, and finally, most visual tokens are pruned in the last stage. Extensive experiments demonstrate that LVLM_CSP achieves a 65% reduction in image token inference FLOPs with virtually no accuracy degradation, and a 70% reduction with only a minor 1% drop in accuracy on the 7B LVLM.","authors":["Hanning Chen","Yang Ni","Wenjun Huang","Hyunwoo Oh","Yezi Liu","Tamoghno Das","Mohsen Imani"],"url":"https://arxiv.org/abs/2504.10854"}
{"created":"2025-04-16","title":"Virtual Contraction Approach to Decentralized Adaptive Stabilization of Nonlinear Time-Delayed Networks","abstract":"In this paper, we utilize a diagonally dominant structure for the decentralized stabilization of unknown nonlinear time-delayed networks. Generalizing the idea of virtual contraction analysis to time-delayed systems, we demonstrate that nonlinear time-delayed networks can be stabilized by diagonal high-gains if the input matrices possess certain generalized (column/row) diagonally dominant properties. To achieve stabilization of unknown networks, we further propose a distributed adaptive tuning rule for each individual gain function, ensuring that all closed-loop trajectories converge to the origin. The effectiveness of the proposed decentralized adaptive control is verified in a case study on epidemic spreading control in SIS networks with transmission delays.","authors":["Yu Kawano","Zhiyong Sun"],"url":"https://arxiv.org/abs/2504.10855"}
{"created":"2025-04-16","title":"ZeroGrasp: Zero-Shot Shape Reconstruction Enabled Robotic Grasping","abstract":"Robotic grasping is a cornerstone capability of embodied systems. Many methods directly output grasps from partial information without modeling the geometry of the scene, leading to suboptimal motion and even collisions. To address these issues, we introduce ZeroGrasp, a novel framework that simultaneously performs 3D reconstruction and grasp pose prediction in near real-time. A key insight of our method is that occlusion reasoning and modeling the spatial relationships between objects is beneficial for both accurate reconstruction and grasping. We couple our method with a novel large-scale synthetic dataset, which comprises 1M photo-realistic images, high-resolution 3D reconstructions and 11.3B physically-valid grasp pose annotations for 12K objects from the Objaverse-LVIS dataset. We evaluate ZeroGrasp on the GraspNet-1B benchmark as well as through real-world robot experiments. ZeroGrasp achieves state-of-the-art performance and generalizes to novel real-world objects by leveraging synthetic data.","authors":["Shun Iwase","Zubair Irshad","Katherine Liu","Vitor Guizilini","Robert Lee","Takuya Ikeda","Ayako Amma","Koichi Nishiwaki","Kris Kitani","Rares Ambrus","Sergey Zakharov"],"url":"https://arxiv.org/abs/2504.10857"}
{"created":"2025-04-16","title":"A Sublinear Algorithm for Path Feasibility Among Rectangular Obstacles","abstract":"The problem of finding a path between two points while avoiding obstacles is critical in robotic path planning. We focus on the feasibility problem: determining whether such a path exists. We model the robot as a query-specific rectangular object capable of moving parallel to its sides. The obstacles are axis-aligned, rectangular, and may overlap. Most previous works only consider nondisjoint rectangular objects and point-sized or statically sized robots. Our approach introduces a novel technique leveraging generalized Gabriel graphs and constructs a data structure to facilitate online queries regarding path feasibility with varying robot sizes in sublinear time. To efficiently handle feasibility queries, we propose an online algorithm utilizing sweep line to construct a generalized Gabriel graph under the $L_\\infty$ norm, capturing key gap constraints between obstacles. We utilize a persistent disjoint-set union data structure to efficiently determine feasibility queries in $\\mathcal{O}(\\log n)$ time and $\\mathcal{O}(n)$ total space.","authors":["Alex Fan","Alicia Li","Arul Kolla","Jason Gonzalez"],"url":"https://arxiv.org/abs/2504.10859"}
{"created":"2025-04-16","title":"Ai2 Scholar QA: Organized Literature Synthesis with Attribution","abstract":"Retrieval-augmented generation is increasingly effective in answering scientific questions from literature, but many state-of-the-art systems are expensive and closed-source. We introduce Ai2 Scholar QA, a free online scientific question answering application. To facilitate research, we make our entire pipeline public: as a customizable open-source Python package and interactive web app, along with paper indexes accessible through public APIs and downloadable datasets. We describe our system in detail and present experiments analyzing its key design decisions. In an evaluation on a recent scientific QA benchmark, we find that Ai2 Scholar QA outperforms competing systems.","authors":["Amanpreet Singh","Joseph Chee Chang","Chloe Anastasiades","Dany Haddad","Aakanksha Naik","Amber Tanaka","Angele Zamarron","Cecile Nguyen","Jena D. Hwang","Jason Dunkleberger","Matt Latzke","Smita Rao","Jaron Lochner","Rob Evans","Rodney Kinney","Daniel S. Weld","Doug Downey","Sergey Feldman"],"url":"https://arxiv.org/abs/2504.10861"}
{"created":"2025-04-16","title":"Automata for the commutative closure of regular sets","abstract":"Consider $ A^* $, the free monoid generated by the finite alphabet $A$ with the concatenation operation. Two words have the same commutative image when one is a permutation of the symbols of the other. The commutative closure of a set $ L \\subseteq A^* $ is the set $ {C}(L) \\subseteq A^* $ of words whose commutative image coincides with that of some word in $ L $. We provide an algorithm that, given a regular set $ L $, produces a finite state automaton that accepts the commutative closure $ {C}(L) $, provided that this closure set is regular. The problem of deciding whether $ {C}(L) $ is regular was solved by Ginsburg and Spanier in 1966 using the decidability of Presburger sentences, and by Gohon in 1985 via formal power series. The problem of constructing an automaton that accepts $ {C}(L) $ has already been studied in the literature. We give a simpler algorithm using an algebraic approach.","authors":["Ver\\'onica Becher","Simon Lew Deveali","Ignacio Mollo Cunningham"],"url":"https://arxiv.org/abs/2504.10864"}
{"created":"2025-04-16","title":"Understanding the theoretical properties of projected Bellman equation, linear Q-learning, and approximate value iteration","abstract":"In this paper, we study the theoretical properties of the projected Bellman equation (PBE) and two algorithms to solve this equation: linear Q-learning and approximate value iteration (AVI). We consider two sufficient conditions for the existence of a solution to PBE : strictly negatively row dominating diagonal (SNRDD) assumption and a condition motivated by the convergence of AVI. The SNRDD assumption also ensures the convergence of linear Q-learning, and its relationship with the convergence of AVI is examined. Lastly, several interesting observations on the solution of PBE are provided when using $\\epsilon$-greedy policy.","authors":["Han-Dong Lim","Donghwan Lee"],"url":"https://arxiv.org/abs/2504.10865"}
{"created":"2025-04-16","title":"DAAF:Degradation-Aware Adaptive Fusion Framework for Robust Infrared and Visible Images Fusion","abstract":"Existing infrared and visible image fusion(IVIF) algorithms often prioritize high-quality images, neglecting image degradation such as low light and noise, which limits the practical potential. This paper propose Degradation-Aware Adaptive image Fusion (DAAF), which achieves unified modeling of adaptive degradation optimization and image fusion. Specifically, DAAF comprises an auxiliary Adaptive Degradation Optimization Network (ADON) and a Feature Interactive Local-Global Fusion (FILGF) Network. Firstly, ADON includes infrared and visible-light branches. Within the infrared branch, frequency-domain feature decomposition and extraction are employed to isolate Gaussian and stripe noise. In the visible-light branch, Retinex decomposition is applied to extract illumination and reflectance components, enabling complementary enhancement of detail and illumination distribution. Subsequently, FILGF performs interactive multi-scale local-global feature fusion. Local feature fusion consists of intra-inter model feature complement, while global feature fusion is achieved through a interactive cross-model attention. Extensive experiments have shown that DAAF outperforms current IVIF algorithms in normal and complex degradation scenarios.","authors":["Tianpei Zhang","Jufeng Zhao","Yiming Zhu","Guangmang Cui","Yuxin Jing","Yuhan Lyu"],"url":"https://arxiv.org/abs/2504.10871"}
{"created":"2025-04-16","title":"Can Vision-Language Models Understand and Interpret Dynamic Gestures from Pedestrians? Pilot Datasets and Exploration Towards Instructive Nonverbal Commands for Cooperative Autonomous Vehicles","abstract":"In autonomous driving, it is crucial to correctly interpret traffic gestures (TGs), such as those of an authority figure providing orders or instructions, or a pedestrian signaling the driver, to ensure a safe and pleasant traffic environment for all road users. This study investigates the capabilities of state-of-the-art vision-language models (VLMs) in zero-shot interpretation, focusing on their ability to caption and classify human gestures in traffic contexts. We create and publicly share two custom datasets with varying formal and informal TGs, such as 'Stop', 'Reverse', 'Hail', etc. The datasets are \"Acted TG (ATG)\" and \"Instructive TG In-The-Wild (ITGI)\". They are annotated with natural language, describing the pedestrian's body position and gesture. We evaluate models using three methods utilizing expert-generated captions as baseline and control: (1) caption similarity, (2) gesture classification, and (3) pose sequence reconstruction similarity. Results show that current VLMs struggle with gesture understanding: sentence similarity averages below 0.59, and classification F1 scores reach only 0.14-0.39, well below the expert baseline of 0.70. While pose reconstruction shows potential, it requires more data and refined metrics to be reliable. Our findings reveal that although some SOTA VLMs can interpret zero-shot human traffic gestures, none are accurate and robust enough to be trustworthy, emphasizing the need for further research in this domain.","authors":["Tonko E. W. Bossen","Andreas M{\\o}gelmose","Ross Greer"],"url":"https://arxiv.org/abs/2504.10873"}
{"created":"2025-04-16","title":"Weather-Aware Object Detection Transformer for Domain Adaptation","abstract":"RT-DETRs have shown strong performance across various computer vision tasks but are known to degrade under challenging weather conditions such as fog. In this work, we investigate three novel approaches to enhance RT-DETR robustness in foggy environments: (1) Domain Adaptation via Perceptual Loss, which distills domain-invariant features from a teacher network to a student using perceptual supervision; (2) Weather Adaptive Attention, which augments the attention mechanism with fog-sensitive scaling by introducing an auxiliary foggy image stream; and (3) Weather Fusion Encoder, which integrates a dual-stream encoder architecture that fuses clear and foggy image features via multi-head self and cross-attention. Despite the architectural innovations, none of the proposed methods consistently outperform the baseline RT-DETR. We analyze the limitations and potential causes, offering insights for future research in weather-aware object detection.","authors":["Soheil Gharatappeh","Salimeh Sekeh","Vikas Dhiman"],"url":"https://arxiv.org/abs/2504.10877"}
{"created":"2025-04-16","title":"Large Language Model-Informed Feature Discovery Improves Prediction and Interpretation of Credibility Perceptions of Visual Content","abstract":"In today's visually dominated social media landscape, predicting the perceived credibility of visual content and understanding what drives human judgment are crucial for countering misinformation. However, these tasks are challenging due to the diversity and richness of visual features. We introduce a Large Language Model (LLM)-informed feature discovery framework that leverages multimodal LLMs, such as GPT-4o, to evaluate content credibility and explain its reasoning. We extract and quantify interpretable features using targeted prompts and integrate them into machine learning models to improve credibility predictions. We tested this approach on 4,191 visual social media posts across eight topics in science, health, and politics, using credibility ratings from 5,355 crowdsourced workers. Our method outperformed zero-shot GPT-based predictions by 13 percent in R2, and revealed key features like information concreteness and image format. We discuss the implications for misinformation mitigation, visual credibility, and the role of LLMs in social science.","authors":["Yilang Peng","Sijia Qian","Yingdan Lu","Cuihua Shen"],"url":"https://arxiv.org/abs/2504.10878"}
{"created":"2025-04-16","title":"Safe-Construct: Redefining Construction Safety Violation Recognition as 3D Multi-View Engagement Task","abstract":"Recognizing safety violations in construction environments is critical yet remains underexplored in computer vision. Existing models predominantly rely on 2D object detection, which fails to capture the complexities of real-world violations due to: (i) an oversimplified task formulation treating violation recognition merely as object detection, (ii) inadequate validation under realistic conditions, (iii) absence of standardized baselines, and (iv) limited scalability from the unavailability of synthetic dataset generators for diverse construction scenarios. To address these challenges, we introduce Safe-Construct, the first framework that reformulates violation recognition as a 3D multi-view engagement task, leveraging scene-level worker-object context and 3D spatial understanding. We also propose the Synthetic Indoor Construction Site Generator (SICSG) to create diverse, scalable training data, overcoming data limitations. Safe-Construct achieves a 7.6% improvement over state-of-the-art methods across four violation types. We rigorously evaluate our approach in near-realistic settings, incorporating four violations, four workers, 14 objects, and challenging conditions like occlusions (worker-object, worker-worker) and variable illumination (back-lighting, overexposure, sunlight). By integrating 3D multi-view spatial understanding and synthetic data generation, Safe-Construct sets a new benchmark for scalable and robust safety monitoring in high-risk industries. Project Website: https://Safe-Construct.github.io/Safe-Construct","authors":["Aviral Chharia","Tianyu Ren","Tomotake Furuhata","Kenji Shimada"],"url":"https://arxiv.org/abs/2504.10880"}
{"created":"2025-04-16","title":"Bringing together invertible UNets with invertible attention modules for memory-efficient diffusion models","abstract":"Diffusion models have recently gained state of the art performance on many image generation tasks. However, most models require significant computational resources to achieve this. This becomes apparent in the application of medical image synthesis due to the 3D nature of medical datasets like CT-scans, MRIs, electron microscope, etc. In this paper we propose a novel architecture for a single GPU memory-efficient training for diffusion models for high dimensional medical datasets. The proposed model is built by using an invertible UNet architecture with invertible attention modules. This leads to the following two contributions: 1. denoising diffusion models and thus enabling memory usage to be independent of the dimensionality of the dataset, and 2. reducing the energy usage during training. While this new model can be applied to a multitude of image generation tasks, we showcase its memory-efficiency on the 3D BraTS2020 dataset leading to up to 15\\% decrease in peak memory consumption during training with comparable results to SOTA while maintaining the image quality.","authors":["Karan Jain","Mohammad Nayeem Teli"],"url":"https://arxiv.org/abs/2504.10883"}
{"created":"2025-04-16","title":"PuzzleBench: A Fully Dynamic Evaluation Framework for Large Multimodal Models on Puzzle Solving","abstract":"Large Multimodal Models (LMMs) have demonstrated impressive capabilities across a wide range of multimodal tasks, achieving ever-increasing performance on various evaluation benchmarks. However, existing benchmarks are typically static and often overlap with pre-training datasets, leading to fixed complexity constraints and substantial data contamination issues. Meanwhile, manually annotated datasets are labor-intensive, time-consuming, and subject to human bias and inconsistency, leading to reliability and reproducibility issues. To address these problems, we propose a fully dynamic multimodal evaluation framework, named Open-ended Visual Puzzle Generation (OVPG), which aims to generate fresh, diverse, and verifiable evaluation data automatically in puzzle-solving tasks. Specifically, the OVPG pipeline consists of a raw material sampling module, a visual content generation module, and a puzzle rule design module, which ensures that each evaluation instance is primitive, highly randomized, and uniquely solvable, enabling continual adaptation to the evolving capabilities of LMMs. Built upon OVPG, we construct PuzzleBench, a dynamic and scalable benchmark comprising 11,840 VQA samples. It features six carefully designed puzzle tasks targeting three core LMM competencies, visual recognition, logical reasoning, and context understanding. PuzzleBench differs from static benchmarks that quickly become outdated. It enables ongoing dataset refreshing through OVPG and a rich set of open-ended puzzle designs, allowing seamless adaptation to the evolving capabilities of LMMs.","authors":["Zeyu Zhang","Zijian Chen","Zicheng Zhang","Yuze Sun","Yuan Tian","Ziheng Jia","Chunyi Li","Xiaohong Liu","Xiongkuo Min","Guangtao Zhai"],"url":"https://arxiv.org/abs/2504.10885"}
{"created":"2025-04-16","title":"Exploring Persona-dependent LLM Alignment for the Moral Machine Experiment","abstract":"Deploying large language models (LLMs) with agency in real-world applications raises critical questions about how these models will behave. In particular, how will their decisions align with humans when faced with moral dilemmas? This study examines the alignment between LLM-driven decisions and human judgment in various contexts of the moral machine experiment, including personas reflecting different sociodemographics. We find that the moral decisions of LLMs vary substantially by persona, showing greater shifts in moral decisions for critical tasks than humans. Our data also indicate an interesting partisan sorting phenomenon, where political persona predominates the direction and degree of LLM decisions. We discuss the ethical implications and risks associated with deploying these models in applications that involve moral decisions.","authors":["Jiseon Kim","Jea Kwon","Luiz Felipe Vecchietti","Alice Oh","Meeyoung Cha"],"url":"https://arxiv.org/abs/2504.10886"}
{"created":"2025-04-16","title":"CDUPatch: Color-Driven Universal Adversarial Patch Attack for Dual-Modal Visible-Infrared Detectors","abstract":"Adversarial patches are widely used to evaluate the robustness of object detection systems in real-world scenarios. These patches were initially designed to deceive single-modal detectors (e.g., visible or infrared) and have recently been extended to target visible-infrared dual-modal detectors. However, existing dual-modal adversarial patch attacks have limited attack effectiveness across diverse physical scenarios. To address this, we propose CDUPatch, a universal cross-modal patch attack against visible-infrared object detectors across scales, views, and scenarios. Specifically, we observe that color variations lead to different levels of thermal absorption, resulting in temperature differences in infrared imaging. Leveraging this property, we propose an RGB-to-infrared adapter that maps RGB patches to infrared patches, enabling unified optimization of cross-modal patches. By learning an optimal color distribution on the adversarial patch, we can manipulate its thermal response and generate an adversarial infrared texture. Additionally, we introduce a multi-scale clipping strategy and construct a new visible-infrared dataset, MSDrone, which contains aerial vehicle images in varying scales and perspectives. These data augmentation strategies enhance the robustness of our patch in real-world conditions. Experiments on four benchmark datasets (e.g., DroneVehicle, LLVIP, VisDrone, MSDrone) show that our method outperforms existing patch attacks in the digital domain. Extensive physical tests further confirm strong transferability across scales, views, and scenarios.","authors":["Jiahuan Long","Wen Yao","Tingsong Jiang","Chao Ma"],"url":"https://arxiv.org/abs/2504.10888"}
{"created":"2025-04-16","title":"Fine-Grained Rib Fracture Diagnosis with Hyperbolic Embeddings: A Detailed Annotation Framework and Multi-Label Classification Model","abstract":"Accurate rib fracture identification and classification are essential for treatment planning. However, existing datasets often lack fine-grained annotations, particularly regarding rib fracture characterization, type, and precise anatomical location on individual ribs. To address this, we introduce a novel rib fracture annotation protocol tailored for fracture classification. Further, we enhance fracture classification by leveraging cross-modal embeddings that bridge radiological images and clinical descriptions. Our approach employs hyperbolic embeddings to capture the hierarchical nature of fracture, mapping visual features and textual descriptions into a shared non-Euclidean manifold. This framework enables more nuanced similarity computations between imaging characteristics and clinical descriptions, accounting for the inherent hierarchical relationships in fracture taxonomy. Experimental results demonstrate that our approach outperforms existing methods across multiple classification tasks, with average recall improvements of 6% on the AirRib dataset and 17.5% on the public RibFrac dataset.","authors":["Shripad Pate","Aiman Farooq","Suvrankar Dutta","Musadiq Aadil Sheikh","Atin Kumar","Deepak Mishra"],"url":"https://arxiv.org/abs/2504.10889"}
{"created":"2025-04-16","title":"ARise: Towards Knowledge-Augmented Reasoning via Risk-Adaptive Search","abstract":"Large language models (LLMs) have demonstrated impressive capabilities and are receiving increasing attention to enhance their reasoning through scaling test--time compute. However, their application in open--ended, knowledge--intensive, complex reasoning scenarios is still limited. Reasoning--oriented methods struggle to generalize to open--ended scenarios due to implicit assumptions of complete world knowledge. Meanwhile, knowledge--augmented reasoning (KAR) methods fail to address two core challenges: 1) error propagation, where errors in early steps cascade through the chain, and 2) verification bottleneck, where the explore--exploit tradeoff arises in multi--branch decision processes. To overcome these limitations, we introduce ARise, a novel framework that integrates risk assessment of intermediate reasoning states with dynamic retrieval--augmented generation (RAG) within a Monte Carlo tree search paradigm. This approach enables effective construction and optimization of reasoning plans across multiple maintained hypothesis branches. Experimental results show that ARise significantly outperforms the state--of--the--art KAR methods by up to 23.10%, and the latest RAG-equipped large reasoning models by up to 25.37%.","authors":["Yize Zhang","Tianshu Wang","Sirui Chen","Kun Wang","Xingyu Zeng","Hongyu Lin","Xianpei Han","Le Sun","Chaochao Lu"],"url":"https://arxiv.org/abs/2504.10893"}
{"created":"2025-04-16","title":"Xpose: Bi-directional Engineering for Hidden Query Extraction","abstract":"Query reverse engineering (QRE) aims to synthesize a SQL query to connect a given database and result instance. A recent variation of QRE is where an additional input, an opaque executable containing a ground-truth query, is provided, and the goal is to non-invasively extract this specific query through only input-output examples. This variant, called Hidden Query Extraction (HQE), has a spectrum of industrial use-cases including query recovery, database security, and vendor migration. The reverse engineering (RE) tools developed for HQE, which are based on database mutation and generation techniques, can only extract flat queries with key-based equi joins and conjunctive arithmetic filter predicates, making them limited wrt both query structure and query operators. In this paper, we present Xpose, a HQE solution that elevates the extraction scope to realistic complex queries, such as those found in the TPCH benchmark. A two-pronged approach is taken: (1) The existing RE scope is substantially extended to incorporate union connectors, algebraic filter predicates, and disjunctions for both values and predicates. (2) The predictive power of LLMs is leveraged to convert business descriptions of the opaque application into extraction guidance, representing ``forward engineering\" (FE). The FE module recognizes common constructs, such as nesting of sub-queries, outer joins, and scalar functions. In essence, FE establishes the broad query contours, while RE fleshes out the fine-grained details. We have evaluated Xpose on (a) E-TPCH, a query suite comprising the complete TPCH benchmark extended with queries featuring unions, diverse join types, and sub-queries; and (b) the real-world STACK benchmark. The experimental results demonstrate that its bi-directional engineering approach accurately extracts these complex queries, representing a significant step forward with regard to HQE coverage.","authors":["Ahana Pradhan","Jayant Haritsa"],"url":"https://arxiv.org/abs/2504.10898"}
{"created":"2025-04-16","title":"Bridging Distribution Gaps in Time Series Foundation Model Pretraining with Prototype-Guided Normalization","abstract":"Foundation models have achieved remarkable success across diverse machine-learning domains through large-scale pretraining on large, diverse datasets. However, pretraining on such datasets introduces significant challenges due to substantial mismatches in data distributions, a problem particularly pronounced with time series data. In this paper, we tackle this issue by proposing a domain-aware adaptive normalization strategy within the Transformer architecture. Specifically, we replace the traditional LayerNorm with a prototype-guided dynamic normalization mechanism (ProtoNorm), where learned prototypes encapsulate distinct data distributions, and sample-to-prototype affinity determines the appropriate normalization layer. This mechanism effectively captures the heterogeneity of time series characteristics, aligning pretrained representations with downstream tasks. Through comprehensive empirical evaluation, we demonstrate that our method significantly outperforms conventional pretraining techniques across both classification and forecasting tasks, while effectively mitigating the adverse effects of distribution shifts during pretraining. Incorporating ProtoNorm is as simple as replacing a single line of code. Extensive experiments on diverse real-world time series benchmarks validate the robustness and generalizability of our approach, advancing the development of more versatile time series foundation models.","authors":["Peiliang Gong","Emadeldeen Eldele","Min Wu","Zhenghua Chen","Xiaoli Li","Daoqiang Zhang"],"url":"https://arxiv.org/abs/2504.10900"}
{"created":"2025-04-16","title":"Design and Verification of a Synchronus First In First Out (FIFO)","abstract":"This project focuses on designing and verifying a synchronous FIFO First In First Out (FIFO) memory, a critical component in digital systems for temporary data storage and seamless data transfer. The FIFO operates under a single clock domain, ensuring synchronized read and write operations, making it suitable for systems requiring high-speed, reliable data buffering. This design includes FIFO's key features, such as read and write operations, full and empty flag generation, and pointer management for memory control. The FIFO was implemented using Verilog to define the Register Transfer Level (RTL) design, ensuring functionality and timing requirements were met. For verification, three approaches were employed: (1) UVM-based Verification: A Universal Verification Methodology (UVM) testbench was developed to test the FIFO design rigorously. The testbench includes components like interface, sequence item, driver, monitor, scoreboard, agent, and environment. Directed and random tests were performed to verify corner cases, such as simultaneous reads and writes, full and empty conditions, and overflow and underflow scenarios; (2) Traditional Verilog Testbench: A standalone Verilog testbench was also used to validate the functionality of the FIFO through directed test scenarios and waveform analysis; (3) FPGA implementation: Additionally, the design was implemented on an FPGA for real-time testing to verify its functionality and timing behavior in hardware. FPGA-based verification ensured the design performed as expected under practical conditions. The results confirmed the correct operation of the FIFO, including accurate data transfer, flag behavior, and timing synchronization. The project successfully demonstrated the robustness and reliability of the synchronous FIFO design, highlighting its importance in modern digital systems for efficient data handling and buffering.","authors":["Yatheeswar Penta","Riadul Islam"],"url":"https://arxiv.org/abs/2504.10901"}
{"created":"2025-04-16","title":"Leveraging Submodule Linearity Enhances Task Arithmetic Performance in LLMs","abstract":"Task arithmetic is a straightforward yet highly effective strategy for model merging, enabling the resultant model to exhibit multi-task capabilities. Recent research indicates that models demonstrating linearity enhance the performance of task arithmetic. In contrast to existing methods that rely on the global linearization of the model, we argue that this linearity already exists within the model's submodules. In particular, we present a statistical analysis and show that submodules (e.g., layers, self-attentions, and MLPs) exhibit significantly higher linearity than the overall model. Based on these findings, we propose an innovative model merging strategy that independently merges these submodules. Especially, we derive a closed-form solution for optimal merging weights grounded in the linear properties of these submodules. Experimental results demonstrate that our method consistently outperforms the standard task arithmetic approach and other established baselines across different model scales and various tasks. This result highlights the benefits of leveraging the linearity of submodules and provides a new perspective for exploring solutions for effective and practical multi-task model merging.","authors":["Rui Dai","Sile Hu","Xu Shen","Yonggang Zhang","Xinmei Tian","Jieping Ye"],"url":"https://arxiv.org/abs/2504.10902"}
{"created":"2025-04-16","title":"Efficient Reasoning Models: A Survey","abstract":"Reasoning models have demonstrated remarkable progress in solving complex and logic-intensive tasks by generating extended Chain-of-Thoughts (CoTs) prior to arriving at a final answer. Yet, the emergence of this \"slow-thinking\" paradigm, with numerous tokens generated in sequence, inevitably introduces substantial computational overhead. To this end, it highlights an urgent need for effective acceleration. This survey aims to provide a comprehensive overview of recent advances in efficient reasoning. It categorizes existing works into three key directions: (1) shorter - compressing lengthy CoTs into concise yet effective reasoning chains; (2) smaller - developing compact language models with strong reasoning capabilities through techniques such as knowledge distillation, other model compression techniques, and reinforcement learning; and (3) faster - designing efficient decoding strategies to accelerate inference. A curated collection of papers discussed in this survey is available in our GitHub repository.","authors":["Sicheng Feng","Gongfan Fang","Xinyin Ma","Xinchao Wang"],"url":"https://arxiv.org/abs/2504.10903"}
{"created":"2025-04-16","title":"A Pseudorandom Generator for Functions of Low-Degree Polynomial Threshold Functions","abstract":"Developing explicit pseudorandom generators (PRGs) for prominent categories of Boolean functions is a key focus in computational complexity theory. In this paper, we investigate the PRGs against the functions of degree-$d$ polynomial threshold functions (PTFs) over Gaussian space. Our main result is an explicit construction of PRG with seed length $\\mathrm{poly}(k,d,1/\\epsilon)\\cdot\\log n$ that can fool any function of $k$ degree-$d$ PTFs with probability at least $1-\\varepsilon$. More specifically, we show that the summation of $L$ independent $R$-moment-matching Gaussian vectors $\\epsilon$-fools functions of $k$ degree-$d$ PTFs, where $L=\\mathrm{poly}( k, d, \\frac{1}{\\epsilon})$ and $R = O({\\log \\frac{kd}{\\epsilon}})$. The PRG is then obtained by applying an appropriate discretization to Gaussian vectors with bounded independence.","authors":["Penghui Yao","Mingnan Zhao"],"url":"https://arxiv.org/abs/2504.10904"}
{"created":"2025-04-16","title":"InterAnimate: Taming Region-aware Diffusion Model for Realistic Human Interaction Animation","abstract":"Recent video generation research has focused heavily on isolated actions, leaving interactive motions-such as hand-face interactions-largely unexamined. These interactions are essential for emerging biometric authentication systems, which rely on interactive motion-based anti-spoofing approaches. From a security perspective, there is a growing need for large-scale, high-quality interactive videos to train and strengthen authentication models. In this work, we introduce a novel paradigm for animating realistic hand-face interactions. Our approach simultaneously learns spatio-temporal contact dynamics and biomechanically plausible deformation effects, enabling natural interactions where hand movements induce anatomically accurate facial deformations while maintaining collision-free contact. To facilitate this research, we present InterHF, a large-scale hand-face interaction dataset featuring 18 interaction patterns and 90,000 annotated videos. Additionally, we propose InterAnimate, a region-aware diffusion model designed specifically for interaction animation. InterAnimate leverages learnable spatial and temporal latents to effectively capture dynamic interaction priors and integrates a region-aware interaction mechanism that injects these priors into the denoising process. To the best of our knowledge, this work represents the first large-scale effort to systematically study human hand-face interactions. Qualitative and quantitative results show InterAnimate produces highly realistic animations, setting a new benchmark. Code and data will be made public to advance research.","authors":["Yukang Lin","Yan Hong","Zunnan Xu","Xindi Li","Chao Xu","Chuanbiao Song","Ronghui Li","Haoxing Chen","Jun Lan","Huijia Zhu","Weiqiang Wang","Jianfu Zhang","Xiu Li"],"url":"https://arxiv.org/abs/2504.10905"}
{"created":"2025-04-16","title":"Understanding LLMs' Cross-Lingual Context Retrieval: How Good It Is And Where It Comes From","abstract":"The ability of cross-lingual context retrieval is a fundamental aspect of cross-lingual alignment of large language models (LLMs), where the model extracts context information in one language based on requests in another language. Despite its importance in real-life applications, this ability has not been adequately investigated for state-of-the-art models. In this paper, we evaluate the cross-lingual context retrieval ability of over 40 LLMs across 12 languages to understand the source of this ability, using cross-lingual machine reading comprehension (xMRC) as a representative scenario. Our results show that several small, post-trained open LLMs show strong cross-lingual context retrieval ability, comparable to closed-source LLMs such as GPT-4o, and their estimated oracle performances greatly improve after post-training. Our interpretability analysis shows that the cross-lingual context retrieval process can be divided into two main phases: question encoding and answer retrieval, which are formed in pre-training and post-training, respectively. The phasing stability correlates with xMRC performance, and the xMRC bottleneck lies at the last model layers in the second phase, where the effect of post-training can be evidently observed. Our results also indicate that larger-scale pretraining cannot improve the xMRC performance. Instead, larger LLMs need further multilingual post-training to fully unlock their cross-lingual context retrieval potential. Our code and is available at https://github.com/NJUNLP/Cross-Lingual-Context-Retrieval","authors":["Changjiang Gao","Hankun Lin","Shujian Huang","Xin Huang","Xue Han","Junlan Feng","Chao Deng","Jiajun Chen"],"url":"https://arxiv.org/abs/2504.10906"}
{"created":"2025-04-16","title":"Full Cooperation in Repeated Multi-Player Games on Hypergraphs","abstract":"Nearly all living systems, especially humans, depend on collective cooperation for survival and prosperity. However, the mechanisms driving the evolution of cooperative behavior remain poorly understood, particularly in the context of simultaneous interactions involving multiple individuals, repeated encounters, and complex interaction structures. Here, we introduce a novel framework for studying repeated multi-player interactions in structured populations -- repeated multi-player games on hypergraphs -- where multiple individuals within each hyperedge engage in a repeated game, and each player can simultaneously participate in many games. We focus on public goods games, where individuals differ in their initial endowments, their allocation of endowments across games, and their productivity, which determines the impact of their contributions. Through Nash equilibrium analysis, we reveal the intricate interplay between full cooperation (all individuals contribute their entire endowments, maximizing collective benefits) and key factors such as initial endowments, productivity, contribution strategies, and interaction structure. Notably, while equal endowments are most effective in promoting full cooperation in homogeneous hypergraphs, they can hinder cooperation in heterogeneous hypergraphs, suggesting that equal endowments are not universally optimal. To address this, we propose two optimization strategies: one for policymakers to adjust endowment distributions and another for players to modify their contribution strategies. Both approaches successfully promote full cooperation across all studied hypergraphs. Our findings provide novel insights into the emergence of full cooperation, offering valuable guidance for both players and policymakers in fostering collective cooperation.","authors":["Juyi Li","Xiaoqun Wu","Qi Su"],"url":"https://arxiv.org/abs/2504.10910"}
{"created":"2025-04-16","title":"LOKA Protocol: A Decentralized Framework for Trustworthy and Ethical AI Agent Ecosystems","abstract":"The rise of autonomous AI agents, capable of perceiving, reasoning, and acting independently, signals a profound shift in how digital ecosystems operate, govern, and evolve. As these agents proliferate beyond centralized infrastructures, they expose foundational gaps in identity, accountability, and ethical alignment. Three critical questions emerge: Identity: Who or what is the agent? Accountability: Can its actions be verified, audited, and trusted? Ethical Consensus: Can autonomous systems reliably align with human values and prevent harmful emergent behaviors? We present the novel LOKA Protocol (Layered Orchestration for Knowledgeful Agents), a unified, systems-level architecture for building ethically governed, interoperable AI agent ecosystems. LOKA introduces a proposed Universal Agent Identity Layer (UAIL) for decentralized, verifiable identity; intent-centric communication protocols for semantic coordination across diverse agents; and a Decentralized Ethical Consensus Protocol (DECP) that enables agents to make context-aware decisions grounded in shared ethical baselines. Anchored in emerging standards such as Decentralized Identifiers (DIDs), Verifiable Credentials (VCs), and post-quantum cryptography, LOKA offers a scalable, future-resilient blueprint for multi-agent AI governance. By embedding identity, trust, and ethics into the protocol layer itself, LOKA establishes the foundation for a new era of responsible, transparent, and autonomous AI ecosystems operating across digital and physical domains.","authors":["Rajesh Ranjan","Shailja Gupta","Surya Narayan Singh"],"url":"https://arxiv.org/abs/2504.10915"}
{"created":"2025-04-16","title":"Towards A Universal Graph Structural Encoder","abstract":"Recent advancements in large-scale pre-training have shown the potential to learn generalizable representations for downstream tasks. In the graph domain, however, capturing and transferring structural information across different graph domains remains challenging, primarily due to the inherent differences in topological patterns across various contexts. Additionally, most existing models struggle to capture the complexity of rich graph structures, leading to inadequate exploration of the embedding space. To address these challenges, we propose GFSE, a universal graph structural encoder designed to capture transferable structural patterns across diverse domains such as molecular graphs, social networks, and citation networks. GFSE is the first cross-domain graph structural encoder pre-trained with multiple self-supervised learning objectives. Built on a Graph Transformer, GFSE incorporates attention mechanisms informed by graph inductive bias, enabling it to encode intricate multi-level and fine-grained topological features. The pre-trained GFSE produces generic and theoretically expressive positional and structural encoding for graphs, which can be seamlessly integrated with various downstream graph feature encoders, including graph neural networks for vectorized features and Large Language Models for text-attributed graphs. Comprehensive experiments on synthetic and real-world datasets demonstrate GFSE's capability to significantly enhance the model's performance while requiring substantially less task-specific fine-tuning. Notably, GFSE achieves state-of-the-art performance in 81.6% evaluated cases, spanning diverse graph models and datasets, highlighting its potential as a powerful and versatile encoder for graph-structured data.","authors":["Jialin Chen","Haolan Zuo","Haoyu Peter Wang","Siqi Miao","Pan Li","Rex Ying"],"url":"https://arxiv.org/abs/2504.10917"}
{"created":"2025-04-16","title":"Adaptive Human-Agent Teaming: A Review of Empirical Studies from the Process Dynamics Perspective","abstract":"The rapid advancement of AI, including Large Language Models, has propelled autonomous agents forward, accelerating the human-agent teaming (HAT) paradigm to leverage complementary strengths. However, HAT research remains fragmented, often focusing on isolated team development phases or specific challenges like trust calibration while overlooking the real-world need for adaptability. Addressing these gaps, a process dynamics perspective is adopted to systematically review HAT using the T$^4$ framework: Team Formation, Task and Role Development, Team Development, and Team Improvement. Each phase is examined in terms of its goals, actions, and evaluation metrics, emphasizing the co-evolution of task and team dynamics. Special focus is given to the second and third phases, highlighting key factors such as team roles, shared mental model, and backup behaviors. This holistic perspective identifies future research directions for advancing long-term adaptive HAT.","authors":["Mengyao Wang","Jiayun Wu","Shuai Ma","Nuo Li","Peng Zhang","Ning Gu","Tun Lu"],"url":"https://arxiv.org/abs/2504.10918"}
{"created":"2025-04-16","title":"Towards Efficient Partially Relevant Video Retrieval with Active Moment Discovering","abstract":"Partially relevant video retrieval (PRVR) is a practical yet challenging task in text-to-video retrieval, where videos are untrimmed and contain much background content. The pursuit here is of both effective and efficient solutions to capture the partial correspondence between text queries and untrimmed videos. Existing PRVR methods, which typically focus on modeling multi-scale clip representations, however, suffer from content independence and information redundancy, impairing retrieval performance. To overcome these limitations, we propose a simple yet effective approach with active moment discovering (AMDNet). We are committed to discovering video moments that are semantically consistent with their queries. By using learnable span anchors to capture distinct moments and applying masked multi-moment attention to emphasize salient moments while suppressing redundant backgrounds, we achieve more compact and informative video representations. To further enhance moment modeling, we introduce a moment diversity loss to encourage different moments of distinct regions and a moment relevance loss to promote semantically query-relevant moments, which cooperate with a partially relevant retrieval loss for end-to-end optimization. Extensive experiments on two large-scale video datasets (\\ie, TVR and ActivityNet Captions) demonstrate the superiority and efficiency of our AMDNet. In particular, AMDNet is about 15.5 times smaller (\\#parameters) while 6.0 points higher (SumR) than the up-to-date method GMMFormer on TVR.","authors":["Peipei Song","Long Zhang","Long Lan","Weidong Chen","Dan Guo","Xun Yang","Meng Wang"],"url":"https://arxiv.org/abs/2504.10920"}
{"created":"2025-04-16","title":"MSCRS: Multi-modal Semantic Graph Prompt Learning Framework for Conversational Recommender Systems","abstract":"Conversational Recommender Systems (CRSs) aim to provide personalized recommendations by interacting with users through conversations. Most existing studies of CRS focus on extracting user preferences from conversational contexts. However, due to the short and sparse nature of conversational contexts, it is difficult to fully capture user preferences by conversational contexts only. We argue that multi-modal semantic information can enrich user preference expressions from diverse dimensions (e.g., a user preference for a certain movie may stem from its magnificent visual effects and compelling storyline). In this paper, we propose a multi-modal semantic graph prompt learning framework for CRS, named MSCRS. First, we extract textual and image features of items mentioned in the conversational contexts. Second, we capture higher-order semantic associations within different semantic modalities (collaborative, textual, and image) by constructing modality-specific graph structures. Finally, we propose an innovative integration of multi-modal semantic graphs with prompt learning, harnessing the power of large language models to comprehensively explore high-dimensional semantic relationships. Experimental results demonstrate that our proposed method significantly improves accuracy in item recommendation, as well as generates more natural and contextually relevant content in response generation. We have released the code and the expanded multi-modal CRS datasets to facilitate further exploration in related research\\footnote{https://github.com/BIAOBIAO12138/MSCRS-main}.","authors":["Yibiao Wei","Jie Zou","Weikang Guo","Guoqing Wang","Xing Xu","Yang Yang"],"url":"https://arxiv.org/abs/2504.10921"}
{"created":"2025-04-16","title":"Fast-Powerformer: A Memory-Efficient Transformer for Accurate Mid-Term Wind Power Forecasting","abstract":"Wind power forecasting (WPF), as a significant research topic within renewable energy, plays a crucial role in enhancing the security, stability, and economic operation of power grids. However, due to the high stochasticity of meteorological factors (e.g., wind speed) and significant fluctuations in wind power output, mid-term wind power forecasting faces a dual challenge of maintaining high accuracy and computational efficiency. To address these issues, this paper proposes an efficient and lightweight mid-term wind power forecasting model, termed Fast-Powerformer. The proposed model is built upon the Reformer architecture, incorporating structural enhancements such as a lightweight Long Short-Term Memory (LSTM) embedding module, an input transposition mechanism, and a Frequency Enhanced Channel Attention Mechanism (FECAM). These improvements enable the model to strengthen temporal feature extraction, optimize dependency modeling across variables, significantly reduce computational complexity, and enhance sensitivity to periodic patterns and dominant frequency components. Experimental results conducted on multiple real-world wind farm datasets demonstrate that the proposed Fast-Powerformer achieves superior prediction accuracy and operational efficiency compared to mainstream forecasting approaches. Furthermore, the model exhibits fast inference speed and low memory consumption, highlighting its considerable practical value for real-world deployment scenarios.","authors":["Mingyi Zhu","Zhaoxin Li","Qiao Lin","Li Ding"],"url":"https://arxiv.org/abs/2504.10923"}
{"created":"2025-04-16","title":"Transfer Learning for Temporal Link Prediction","abstract":"Link prediction on graphs has applications spanning from recommender systems to drug discovery. Temporal link prediction (TLP) refers to predicting future links in a temporally evolving graph and adds additional complexity related to the dynamic nature of graphs. State-of-the-art TLP models incorporate memory modules alongside graph neural networks to learn both the temporal mechanisms of incoming nodes and the evolving graph topology. However, memory modules only store information about nodes seen at train time, and hence such models cannot be directly transferred to entirely new graphs at test time and deployment. In this work, we study a new transfer learning task for temporal link prediction, and develop transfer-effective methods for memory-laden models. Specifically, motivated by work showing the informativeness of structural signals for the TLP task, we augment a structural mapping module to the existing TLP model architectures, which learns a mapping from graph structural (topological) features to memory embeddings. Our work paves the way for a memory-free foundation model for TLP.","authors":["Ayan Chatterjee","Barbara Ikica","Babak Ravandi","John Palowitch"],"url":"https://arxiv.org/abs/2504.10925"}
{"created":"2025-04-16","title":"Cross-Frequency Implicit Neural Representation with Self-Evolving Parameters","abstract":"Implicit neural representation (INR) has emerged as a powerful paradigm for visual data representation. However, classical INR methods represent data in the original space mixed with different frequency components, and several feature encoding parameters (e.g., the frequency parameter $\\omega$ or the rank $R$) need manual configurations. In this work, we propose a self-evolving cross-frequency INR using the Haar wavelet transform (termed CF-INR), which decouples data into four frequency components and employs INRs in the wavelet space. CF-INR allows the characterization of different frequency components separately, thus enabling higher accuracy for data representation. To more precisely characterize cross-frequency components, we propose a cross-frequency tensor decomposition paradigm for CF-INR with self-evolving parameters, which automatically updates the rank parameter $R$ and the frequency parameter $\\omega$ for each frequency component through self-evolving optimization. This self-evolution paradigm eliminates the laborious manual tuning of these parameters, and learns a customized cross-frequency feature encoding configuration for each dataset. We evaluate CF-INR on a variety of visual data representation and recovery tasks, including image regression, inpainting, denoising, and cloud removal. Extensive experiments demonstrate that CF-INR outperforms state-of-the-art methods in each case.","authors":["Chang Yu","Yisi Luo","Kai Ye","Xile Zhao","Deyu Meng"],"url":"https://arxiv.org/abs/2504.10929"}
{"created":"2025-04-16","title":"Multi-scale DeepOnet (Mscale-DeepOnet) for Mitigating Spectral Bias in Learning High Frequency Operators of Oscillatory Functions","abstract":"In this paper, a multi-scale DeepOnet (Mscale-DeepOnet) is proposed to reduce the spectral bias of the DeepOnet in learning high-frequency mapping between highly oscillatory functions, with an application to the nonlinear mapping between the coefficient of the Helmholtz equation and its solution. The Mscale-DeepOnet introduces the multiscale neural network in the branch and trunk networks of the original DeepOnet, the resulting Mscale-DeepOnet is shown to be able to capture various high-frequency components of the mapping itself and its image. Numerical results demonstrate the substantial improvement of the Mscale-DeepOnet for the problem of wave scattering in the high-frequency regime over the normal DeepOnet with a similar number of network parameters.","authors":["Bo Wang","Lizuo Liu","Wei Cai"],"url":"https://arxiv.org/abs/2504.10932"}
{"created":"2025-04-16","title":"Towards Robust Trajectory Embedding for Similarity Computation: When Triangle Inequality Violations in Distance Metrics Matter","abstract":"Trajectory similarity is a cornerstone of trajectory data management and analysis. Traditional similarity functions often suffer from high computational complexity and a reliance on specific distance metrics, prompting a shift towards deep representation learning in Euclidean space. However, existing Euclidean-based trajectory embeddings often face challenges due to the triangle inequality constraints that do not universally hold for trajectory data. To address this issue, this paper introduces a novel approach by incorporating non-Euclidean geometry, specifically hyperbolic space, into trajectory representation learning. We present the first-ever integration of hyperbolic space to resolve the inherent limitations of the triangle inequality in Euclidean embeddings. In particular, we achieve it by designing a Lorentz distance measure, which is proven to overcome triangle inequality constraints. Additionally, we design a model-agnostic framework LH-plugin to seamlessly integrate hyperbolic embeddings into existing representation learning pipelines. This includes a novel projection method optimized with the Cosh function to prevent the diminishment of distances, supported by a theoretical foundation. Furthermore, we propose a dynamic fusion distance that intelligently adapts to variations in triangle inequality constraints across different trajectory pairs, blending Lorentzian and Euclidean distances for more robust similarity calculations. Comprehensive experimental evaluations demonstrate that our approach effectively enhances the accuracy of trajectory similarity measures in state-of-the-art models across multiple real-world datasets. The LH-plugin not only addresses the triangle inequality issues but also significantly refines the precision of trajectory similarity computations, marking a substantial advancement in the field of trajectory representation learning.","authors":["Jianing Si","Haitao Yuan","Nan Jiang","Minxiao Chen","Xiao Ma","Shangguang Wang"],"url":"https://arxiv.org/abs/2504.10933"}
{"created":"2025-04-16","title":"Can LLMs Leverage Observational Data? Towards Data-Driven Causal Discovery with LLMs","abstract":"Causal discovery traditionally relies on statistical methods applied to observational data, often requiring large datasets and assumptions about underlying causal structures. Recent advancements in Large Language Models (LLMs) have introduced new possibilities for causal discovery by providing domain expert knowledge. However, it remains unclear whether LLMs can effectively process observational data for causal discovery. In this work, we explore the potential of LLMs for data-driven causal discovery by integrating observational data for LLM-based reasoning. Specifically, we examine whether LLMs can effectively utilize observational data through two prompting strategies: pairwise prompting and breadth first search (BFS)-based prompting. In both approaches, we incorporate the observational data directly into the prompt to assess LLMs' ability to infer causal relationships from such data. Experiments on benchmark datasets show that incorporating observational data enhances causal discovery, boosting F1 scores by up to 0.11 point using both pairwise and BFS LLM-based prompting, while outperforming traditional statistical causal discovery baseline by up to 0.52 points. Our findings highlight the potential and limitations of LLMs for data-driven causal discovery, demonstrating their ability to move beyond textual metadata and effectively interpret and utilize observational data for more informed causal reasoning. Our studies lays the groundwork for future advancements toward fully LLM-driven causal discovery.","authors":["Yuni Susanti","Michael F\\\"arber"],"url":"https://arxiv.org/abs/2504.10936"}
{"created":"2025-04-16","title":"Finding Locally Densest Subgraphs: Convex Programming with Edge and Triangle Density","abstract":"Finding the densest subgraph (DS) from a graph is a fundamental problem in graph databases. The DS obtained, which reveals closely related entities, has been found to be useful in various application domains such as e-commerce, social science, and biology. However, in a big graph that contains billions of edges, it is desirable to find more than one subgraph cluster that is not necessarily the densest, yet they reveal closely related vertices. In this paper, we study the locally densest subgraph (LDS), a recently proposed variant of DS. An LDS is a subgraph which is the densest among the ``local neighbors''. Given a graph $G$, a number of LDSs can be returned, which reflect different dense regions of $G$ and thus give more information than DS. The existing LDS solution suffers from low efficiency. We thus develop a convex-programming-based solution that enables powerful pruning. We also extend our algorithm to triangle-based density to solve LTDS problem. Based on current algorithms, we propose a unified framework for the LDS and LTDS problems. Extensive experiments on thirteen real large graph datasets show that our proposed algorithm is up to four orders of magnitude faster than the state-of-the-art.","authors":["Yi Yang","Chenhao Ma","Reynold Cheng","Laks V. S. Lakshmanan","Xiaolin Han"],"url":"https://arxiv.org/abs/2504.10937"}
{"created":"2025-04-16","title":"Cartesian Merkle Tree","abstract":"This paper introduces the Cartesian Merkle Tree, a deterministic data structure that combines the properties of a Binary Search Tree, a Heap, and a Merkle tree. The Cartesian Merkle Tree supports insertions, updates, and removals of elements in $O(\\log n)$ time, requires $n$ space, and enables membership and non-membership proofs via Merkle-based authentication paths. This structure is particularly suitable for zero-knowledge applications, blockchain systems, and other protocols that require efficient and verifiable data structures.","authors":["Artem Chystiakov","Oleh Komendant","Kyrylo Riabov"],"url":"https://arxiv.org/abs/2504.10944"}
{"created":"2025-04-16","title":"Tighter Bounds on Non-clairvoyant Parallel Machine Scheduling with Prediction to Minimize Makespan","abstract":"This paper investigates the non-clairvoyant parallel machine scheduling problem with prediction, with the objective of minimizing the makespan. Improved lower bounds for the problem and competitive ratios of online algorithms with respect to the prediction error are presented for both the non-preemptive and preemptive cases on m identical machines.","authors":["Tianqi Chen","Zhiyi Tan"],"url":"https://arxiv.org/abs/2504.10945"}
{"created":"2025-04-16","title":"Improved MST3 Encryption scheme based on small Ree groups","abstract":"This article presents an encryption scheme based on the small Ree groups. We propose utilizing the small Ree group structure to enhance the overall security parameters of the encryption scheme. By extending the logarithmic signature to encompass the entire group and modifying the encryption algorithm, we have developed robust protection against sequential key recovery attacks.","authors":["Gennady Khalimov","Yevgen Kotukh"],"url":"https://arxiv.org/abs/2504.10947"}
{"created":"2025-04-16","title":"BEACON: A Benchmark for Efficient and Accurate Counting of Subgraphs","abstract":"Subgraph counting the task of determining the number of instances of a query pattern within a large graph lies at the heart of many critical applications, from analyzing financial networks and transportation systems to understanding biological interactions. Despite decades of work yielding efficient algorithmic (AL) solutions and, more recently, machine learning (ML) approaches, a clear comparative understanding is elusive. This gap stems from the absence of a unified evaluation framework, standardized datasets, and accessible ground truths, all of which hinder systematic analysis and fair benchmarking. To overcome these barriers, we introduce BEACON: a comprehensive benchmark designed to rigorously evaluate both AL and ML-based subgraph counting methods. BEACON provides a standardized dataset with verified ground truths, an integrated evaluation environment, and a public leaderboard, enabling reproducible and transparent comparisons across diverse approaches. Our extensive experiments reveal that while AL methods excel in efficiently counting subgraphs on very large graphs, they struggle with complex patterns (e.g., those exceeding six nodes). In contrast, ML methods are capable of handling larger patterns but demand massive graph data inputs and often yield suboptimal accuracy on small, dense graphs. These insights not only highlight the unique strengths and limitations of each approach but also pave the way for future advancements in subgraph counting techniques. Overall, BEACON represents a significant step towards unifying and accelerating research in subgraph counting, encouraging innovative solutions and fostering a deeper understanding of the trade-offs between algorithmic and machine learning paradigms.","authors":["Mohammad Matin Najafi","Xianju Zhu","Chrysanthi Kosyfaki","Laks V. S. Lakshmanan","Reynold Cheng"],"url":"https://arxiv.org/abs/2504.10948"}
{"created":"2025-04-16","title":"A Primer on Orthogonal Delay-Doppler Division Multiplexing (ODDM)","abstract":"As a new type of multicarrier (MC) scheme built upon the recently discovered delay-Doppler domain orthogonal pulse (DDOP), orthogonal delay-Doppler division multiplexing (ODDM) aims to address the challenges of waveform design in linear time-varying channels. In this paper, we explore the design principles of ODDM and clarify the key ideas underlying the DDOP. We then derive an alternative representation of the DDOP and highlight the fundamental differences between ODDM and conventional MC schemes. Finally, we discuss and compare two implementation methods for ODDM.","authors":["Hai Lin"],"url":"https://arxiv.org/abs/2504.10949"}
{"created":"2025-04-16","title":"Unveiling Challenges for LLMs in Enterprise Data Engineering","abstract":"Large Language Models (LLMs) have demonstrated significant potential for automating data engineering tasks on tabular data, giving enterprises a valuable opportunity to reduce the high costs associated with manual data handling. However, the enterprise domain introduces unique challenges that existing LLM-based approaches for data engineering often overlook, such as large table sizes, more complex tasks, and the need for internal knowledge. To bridge these gaps, we identify key enterprise-specific challenges related to data, tasks, and background knowledge and conduct a comprehensive study of their impact on recent LLMs for data engineering. Our analysis reveals that LLMs face substantial limitations in real-world enterprise scenarios, resulting in significant accuracy drops. Our findings contribute to a systematic understanding of LLMs for enterprise data engineering to support their adoption in industry.","authors":["Jan-Micha Bodensohn","Ulf Brackmann","Liane Vogel","Anupam Sanghi","Carsten Binnig"],"url":"https://arxiv.org/abs/2504.10950"}
{"created":"2025-04-16","title":"Offset-free Nonlinear MPC with Koopman-based Surrogate Models","abstract":"In this paper, we design offset-free nonlinear Model Predictive Control (MPC) for surrogate models based on Extended Dynamic Mode Decomposition (EDMD). The model used for prediction in MPC is augmented with a disturbance term, that is estimated by an observer. If the full information about the equilibrium of the real system is not available, a reference calculator is introduced in the algorithm to compute the MPC state and input references. The control algorithm guarantees offset-free tracking of the controlled output under the assumption that the modeling errors are asymptotically constant. The effectiveness of the proposed approach is showcased with numerical simulations for two popular benchmark systems: the van-der-Pol oscillator and the four-tanks process.","authors":["Irene Schimperna","Lea Bold","Karl Worthmann"],"url":"https://arxiv.org/abs/2504.10954"}
{"created":"2025-04-16","title":"When is Task Vector Provably Effective for Model Editing? A Generalization Analysis of Nonlinear Transformers","abstract":"Task arithmetic refers to editing the pre-trained model by adding a weighted sum of task vectors, each of which is the weight update from the pre-trained model to fine-tuned models for certain tasks. This approach recently gained attention as a computationally efficient inference method for model editing, e.g., multi-task learning, forgetting, and out-of-domain generalization capabilities. However, the theoretical understanding of why task vectors can execute various conceptual operations remains limited, due to the highly non-convexity of training Transformer-based models. To the best of our knowledge, this paper provides the first theoretical characterization of the generalization guarantees of task vector methods on nonlinear Transformers. We consider a conceptual learning setting, where each task is a binary classification problem based on a discriminative pattern. We theoretically prove the effectiveness of task addition in simultaneously learning a set of irrelevant or aligned tasks, as well as the success of task negation in unlearning one task from irrelevant or contradictory tasks. Moreover, we prove the proper selection of linear coefficients for task arithmetic to achieve guaranteed generalization to out-of-domain tasks. All of our theoretical results hold for both dense-weight parameters and their low-rank approximations. Although established in a conceptual setting, our theoretical findings were validated on a practical machine unlearning task using the large language model Phi-1.5 (1.3B).","authors":["Hongkang Li","Yihua Zhang","Shuai Zhang","Meng Wang","Sijia Liu","Pin-Yu Chen"],"url":"https://arxiv.org/abs/2504.10957"}
{"created":"2025-04-16","title":"Recognition of Geometrical Shapes by Dictionary Learning","abstract":"Dictionary learning is a versatile method to produce an overcomplete set of vectors, called atoms, to represent a given input with only a few atoms. In the literature, it has been used primarily for tasks that explore its powerful representation capabilities, such as for image reconstruction. In this work, we present a first approach to make dictionary learning work for shape recognition, considering specifically geometrical shapes. As we demonstrate, the choice of the underlying optimization method has a significant impact on recognition quality. Experimental results confirm that dictionary learning may be an interesting method for shape recognition tasks.","authors":["Alexander K\\\"ohler","Michael Breu{\\ss}"],"url":"https://arxiv.org/abs/2504.10958"}
{"created":"2025-04-16","title":"Learning-Based User Association for MmWave Vehicular Networks With Kernelized Contextual Bandits","abstract":"Vehicles require timely channel conditions to determine the base station (BS) to communicate with, but it is costly to estimate the fast-fading mmWave channels frequently. Without additional channel estimations, the proposed Distributed Kernelized Upper Confidence Bound (DK-UCB) algorithm estimates the current instantaneous transmission rates utilizing past contexts, such as the vehicle's location and velocity, along with past instantaneous transmission rates. To capture the nonlinear mapping from a context to the instantaneous transmission rate, DK-UCB maps a context into the reproducing kernel Hilbert space (RKHS) where a linear mapping becomes observable. To improve estimation accuracy, we propose a novel kernel function in RKHS which incorporates the propagation characteristics of the mmWave signals. Moreover, DK-UCB encourages a vehicle to share necessary information when it has conducted significant explorations, which speeds up the learning process while maintaining affordable communication costs.","authors":["Xiaoyang He","Xiaoxia Huang"],"url":"https://arxiv.org/abs/2504.10959"}
{"created":"2025-04-16","title":"A Linear Push-Pull Average Consensus Algorithm for Delay-Prone Networks","abstract":"In this paper, we address the average consensus problem of multi-agent systems for possibly unbalanced and delay-prone networks with directional information flow. We propose a linear distributed algorithm (referred to as RPPAC) that handles asynchronous updates and time-varying heterogeneous information delays. Our proposed distributed algorithm utilizes a surplus-consensus mechanism and information regarding the number of incoming and outgoing links to guarantee state averaging, despite the imbalanced and delayed information flow in directional networks. The convergence of the RPPAC algorithm is examined using key properties of the backward product of time-varying matrices that correspond to different snapshots of the directional augmented network.","authors":["Evagoras Makridis","Themistoklis Charalambous"],"url":"https://arxiv.org/abs/2504.10960"}
{"created":"2025-04-16","title":"Evaluating Trust in AI, Human, and Co-produced Feedback Among Undergraduate Students","abstract":"As generative AI transforms educational feedback practices, understanding students' perceptions of different feedback providers becomes crucial for effective implementation. This study addresses a critical gap by comparing undergraduate students' trust in AI-generated, human-created, and human-AI co-produced feedback, informing how institutions can adapt feedback practices in this new era. Through a within-subject experiment with 91 participants, we investigated factors predicting students' ability to distinguish between feedback types, perception of feedback quality, and potential biases to AI involvement. Findings revealed that students generally preferred AI and co-produced feedback over human feedback in terms of perceived usefulness and objectivity. Only AI feedback suffered a decline in perceived genuineness when feedback sources were revealed, while co-produced feedback maintained its positive perception. Educational AI experience improved students' ability to identify AI feedback and increased their trust in all feedback types, while general AI experience decreased perceived usefulness and credibility. Male students consistently rated all feedback types as less valuable than their female and non-binary counterparts. These insights inform evidence-based guidelines for integrating AI into higher education feedback systems while addressing trust concerns and fostering AI literacy among students.","authors":["Audrey Zhang","Yifei Gao","Wannapon Suraworachet","Tanya Nazaretsky","Mutlu Cukurova"],"url":"https://arxiv.org/abs/2504.10961"}
{"created":"2025-04-16","title":"$\\pi$-MPPI: A Projection-based Model Predictive Path Integral Scheme for Smooth Optimal Control of Fixed-Wing Aerial Vehicles","abstract":"Model Predictive Path Integral (MPPI) is a popular sampling-based Model Predictive Control (MPC) algorithm for nonlinear systems. It optimizes trajectories by sampling control sequences and averaging them. However, a key issue with MPPI is the non-smoothness of the optimal control sequence, leading to oscillations in systems like fixed-wing aerial vehicles (FWVs). Existing solutions use post-hoc smoothing, which fails to bound control derivatives. This paper introduces a new approach: we add a projection filter $\\pi$ to minimally correct control samples, ensuring bounds on control magnitude and higher-order derivatives. The filtered samples are then averaged using MPPI, leading to our $\\pi$-MPPI approach. We minimize computational overhead by using a neural accelerated custom optimizer for the projection filter. $\\pi$-MPPI offers a simple way to achieve arbitrary smoothness in control sequences. While we focus on FWVs, this projection filter can be integrated into any MPPI pipeline. Applied to FWVs, $\\pi$-MPPI is easier to tune than the baseline, resulting in smoother, more robust performance.","authors":["Edvin Martin Andrejev","Amith Manoharan","Karl-Eerik Unt","Arun Kumar Singh"],"url":"https://arxiv.org/abs/2504.10962"}
{"created":"2025-04-16","title":"Distributed Optimization with Gradient Tracking over Heterogeneous Delay-Prone Directed Networks","abstract":"In this paper, we address the distributed optimization problem over unidirectional networks with possibly time-invariant heterogeneous bounded transmission delays. In particular, we propose a modified version of the Accelerated Distributed Directed OPTimization (ADD-OPT) algorithm, herein called Robustified ADD-OPT (R-ADD-OPT), which is able to solve the distributed optimization problem, even when the communication links suffer from heterogeneous but bounded transmission delays. We show that if the gradient step-size of the R-ADD-OPT algorithm is within a certain range, which also depends on the maximum time delay in the network, then the nodes are guaranteed to converge to the optimal solution of the distributed optimization problem. The range of the gradient step-size that guarantees convergence can be computed a priori based on the maximum time delay in the network.","authors":["Evagoras Makridis","Gabriele Oliva","Kasagatta Ramesh Narahari","Mohammadreza Doostmohammadian","Usman A. Khan","Themistoklis Charalambous"],"url":"https://arxiv.org/abs/2504.10964"}
{"created":"2025-04-16","title":"An Efficient and Mixed Heterogeneous Model for Image Restoration","abstract":"Image restoration~(IR), as a fundamental multimedia data processing task, has a significant impact on downstream visual applications. In recent years, researchers have focused on developing general-purpose IR models capable of handling diverse degradation types, thereby reducing the cost and complexity of model development. Current mainstream approaches are based on three architectural paradigms: CNNs, Transformers, and Mambas. CNNs excel in efficient inference, whereas Transformers and Mamba excel at capturing long-range dependencies and modeling global contexts. While each architecture has demonstrated success in specialized, single-task settings, limited efforts have been made to effectively integrate heterogeneous architectures to jointly address diverse IR challenges. To bridge this gap, we propose RestorMixer, an efficient and general-purpose IR model based on mixed-architecture fusion. RestorMixer adopts a three-stage encoder-decoder structure, where each stage is tailored to the resolution and feature characteristics of the input. In the initial high-resolution stage, CNN-based blocks are employed to rapidly extract shallow local features. In the subsequent stages, we integrate a refined multi-directional scanning Mamba module with a multi-scale window-based self-attention mechanism. This hierarchical and adaptive design enables the model to leverage the strengths of CNNs in local feature extraction, Mamba in global context modeling, and attention mechanisms in dynamic feature refinement. Extensive experimental results demonstrate that RestorMixer achieves leading performance across multiple IR tasks while maintaining high inference efficiency. The official code can be accessed at https://github.com/ClimBin/RestorMixer.","authors":["Yubin Gu","Yuan Meng","Kaihang Zheng","Xiaoshuai Sun","Jiayi Ji","Weijian Ruan","Liujuan Cao","Rongrong Ji"],"url":"https://arxiv.org/abs/2504.10967"}
{"created":"2025-04-16","title":"RF Sensing Security and Malicious Exploitation: A Comprehensive Survey","abstract":"Radio Frequency (RF) sensing technologies have experienced significant growth due to the widespread adoption of RF devices and the Internet of Things (IoT). These technologies enable numerous applications across healthcare, smart homes, industrial automation, and human-computer interaction. However, the non-intrusive and ubiquitous nature of RF sensing - combined with its environmental sensitivity and data dependency - makes these systems inherently vulnerable not only as attack targets, but also as powerful attack vectors. This survey presents a comprehensive analysis of RF sensing security, covering both system-level vulnerabilities - such as signal spoofing, adversarial perturbations, and model poisoning - and the misuse of sensing capabilities for attacks like cross-boundary surveillance, side-channel inference, and semantic privacy breaches. We propose unified threat models to structure these attack vectors and further conduct task-specific vulnerability assessments across key RF sensing applications, identifying their unique attack surfaces and risk profiles. In addition, we systematically review defense strategies across system layers and threat-specific scenarios, incorporating both active and passive paradigms to provide a structured and practical view of protection mechanisms. Compared to prior surveys, our work distinguishes itself by offering a multi-dimensional classification framework based on task type, threat vector, and sensing modality, and by providing fine-grained, scenario-driven analysis that bridges theoretical models and real-world implications. This survey aims to serve as a comprehensive reference for researchers and practitioners seeking to understand, evaluate, and secure the evolving landscape of RF sensing technologies.","authors":["Mingda Han","Huanqi Yang","Wenhao Li","Weitao Xu","Xiuzhen Cheng","Prasant Mohapatra","Pengfei Hu"],"url":"https://arxiv.org/abs/2504.10969"}
{"created":"2025-04-16","title":"The Effectiveness of Business Process Visualisations: a Systematic Literature Review","abstract":"Business Process Visualisations (BPVs) have become indispensable tools for organisations seeking to enhance their operational efficiency, decision-making capabilities, and overall performance. The burgeoning interest in process modeling and tool development, coupled with the rise of data visualisation field, underscores the significant role of visual tools in leveraging human cognition. Unlike traditional models, data visualisation approaches graphics from a novel angle, emphasising the potency of visual representations. This review aims to integrate the domains of BPV and data visualisation to assess their combined influence on organisational effectiveness comprehensively. Through a meticulous analysis of existing literature, this study aims to amalgamate insights on BPVs impact from a data visualisation standpoint, advocating for a design philosophy that prioritises user engagement to bolster organisational outcomes. Additionally, our systematic review has unveiled promising avenues for future research, identifying underexplored variables that influence the efficacy of BPVs, thereby charting a path for forthcoming scholarly inquiries.","authors":["E. C. Overes","F. M. Santoro"],"url":"https://arxiv.org/abs/2504.10971"}
{"created":"2025-04-16","title":"AFiRe: Anatomy-Driven Self-Supervised Learning for Fine-Grained Representation in Radiographic Images","abstract":"Current self-supervised methods, such as contrastive learning, predominantly focus on global discrimination, neglecting the critical fine-grained anatomical details required for accurate radiographic analysis. To address this challenge, we propose an Anatomy-driven self-supervised framework for enhancing Fine-grained Representation in radiographic image analysis (AFiRe). The core idea of AFiRe is to align the anatomical consistency with the unique token-processing characteristics of Vision Transformer. Specifically, AFiRe synergistically performs two self-supervised schemes: (i) Token-wise anatomy-guided contrastive learning, which aligns image tokens based on structural and categorical consistency, thereby enhancing fine-grained spatial-anatomical discrimination; (ii) Pixel-level anomaly-removal restoration, which particularly focuses on local anomalies, thereby refining the learned discrimination with detailed geometrical information. Additionally, we propose Synthetic Lesion Mask to enhance anatomical diversity while preserving intra-consistency, which is typically corrupted by traditional data augmentations, such as Cropping and Affine transformations. Experimental results show that AFiRe: (i) provides robust anatomical discrimination, achieving more cohesive feature clusters compared to state-of-the-art contrastive learning methods; (ii) demonstrates superior generalization, surpassing 7 radiography-specific self-supervised methods in multi-label classification tasks with limited labeling; and (iii) integrates fine-grained information, enabling precise anomaly detection using only image-level annotations.","authors":["Yihang Liu","Lianghua He","Ying Wen","Longzhen Yang","Hongzhou Chen"],"url":"https://arxiv.org/abs/2504.10972"}
{"created":"2025-04-16","title":"Self-Supervised Enhancement of Forward-Looking Sonar Images: Bridging Cross-Modal Degradation Gaps through Feature Space Transformation and Multi-Frame Fusion","abstract":"Enhancing forward-looking sonar images is critical for accurate underwater target detection. Current deep learning methods mainly rely on supervised training with simulated data, but the difficulty in obtaining high-quality real-world paired data limits their practical use and generalization. Although self-supervised approaches from remote sensing partially alleviate data shortages, they neglect the cross-modal degradation gap between sonar and remote sensing images. Directly transferring pretrained weights often leads to overly smooth sonar images, detail loss, and insufficient brightness. To address this, we propose a feature-space transformation that maps sonar images from the pixel domain to a robust feature domain, effectively bridging the degradation gap. Additionally, our self-supervised multi-frame fusion strategy leverages complementary inter-frame information to naturally remove speckle noise and enhance target-region brightness. Experiments on three self-collected real-world forward-looking sonar datasets show that our method significantly outperforms existing approaches, effectively suppressing noise, preserving detailed edges, and substantially improving brightness, demonstrating strong potential for underwater target detection applications.","authors":["Zhisheng Zhang","Peng Zhang","Fengxiang Wang","Liangli Ma","Fuchun Sun"],"url":"https://arxiv.org/abs/2504.10974"}
{"created":"2025-04-16","title":"Adaptive Decision Boundary for Few-Shot Class-Incremental Learning","abstract":"Few-Shot Class-Incremental Learning (FSCIL) aims to continuously learn new classes from a limited set of training samples without forgetting knowledge of previously learned classes. Conventional FSCIL methods typically build a robust feature extractor during the base training session with abundant training samples and subsequently freeze this extractor, only fine-tuning the classifier in subsequent incremental phases. However, current strategies primarily focus on preventing catastrophic forgetting, considering only the relationship between novel and base classes, without paying attention to the specific decision spaces of each class. To address this challenge, we propose a plug-and-play Adaptive Decision Boundary Strategy (ADBS), which is compatible with most FSCIL methods. Specifically, we assign a specific decision boundary to each class and adaptively adjust these boundaries during training to optimally refine the decision spaces for the classes in each session. Furthermore, to amplify the distinctiveness between classes, we employ a novel inter-class constraint loss that optimizes the decision boundaries and prototypes for each class. Extensive experiments on three benchmarks, namely CIFAR100, miniImageNet, and CUB200, demonstrate that incorporating our ADBS method with existing FSCIL techniques significantly improves performance, achieving overall state-of-the-art results.","authors":["Linhao Li","Yongzhang Tan","Siyuan Yang","Hao Cheng","Yongfeng Dong","Liang Yang"],"url":"https://arxiv.org/abs/2504.10976"}
{"created":"2025-04-16","title":"Deep Learning in Concealed Dense Prediction","abstract":"Deep learning is developing rapidly and handling common computer vision tasks well. It is time to pay attention to more complex vision tasks, as model size, knowledge, and reasoning capabilities continue to improve. In this paper, we introduce and review a family of complex tasks, termed Concealed Dense Prediction (CDP), which has great value in agriculture, industry, etc. CDP's intrinsic trait is that the targets are concealed in their surroundings, thus fully perceiving them requires fine-grained representations, prior knowledge, auxiliary reasoning, etc. The contributions of this review are three-fold: (i) We introduce the scope, characteristics, and challenges specific to CDP tasks and emphasize their essential differences from generic vision tasks. (ii) We develop a taxonomy based on concealment counteracting to summarize deep learning efforts in CDP through experiments on three tasks. We compare 25 state-of-the-art methods across 12 widely used concealed datasets. (iii) We discuss the potential applications of CDP in the large model era and summarize 6 potential research directions. We offer perspectives for the future development of CDP by constructing a large-scale multimodal instruction fine-tuning dataset, CvpINST, and a concealed visual perception agent, CvpAgent.","authors":["Pancheng Zhao","Deng-Ping Fan","Shupeng Cheng","Salman Khan","Fahad Shahbaz Khan","David Clifton","Peng Xu","Jufeng Yang"],"url":"https://arxiv.org/abs/2504.10979"}
{"created":"2025-04-16","title":"Exploring the Role of KG-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs","abstract":"Large language models (LLMs) perform well in medical QA, but their effectiveness in Japanese contexts is limited due to privacy constraints that prevent the use of commercial models like GPT-4 in clinical settings. As a result, recent efforts focus on instruction-tuning open-source LLMs, though the potential of combining them with retrieval-augmented generation (RAG) remains underexplored. To bridge this gap, we are the first to explore a knowledge graph-based (KG) RAG framework for Japanese medical QA small-scale open-source LLMs. Experimental results show that KG-based RAG has only a limited impact on Japanese medical QA using small-scale open-source LLMs. Further case studies reveal that the effectiveness of the RAG is sensitive to the quality and relevance of the external retrieved content. These findings offer valuable insights into the challenges and potential of applying RAG in Japanese medical QA, while also serving as a reference for other low-resource languages.","authors":["Yingjian Chen","Feiyang Li","Xingyu Song","Tianxiao Li","Issey Sudeka","Irene Li"],"url":"https://arxiv.org/abs/2504.10982"}
{"created":"2025-04-16","title":"ProtFlow: Fast Protein Sequence Design via Flow Matching on Compressed Protein Language Model Embeddings","abstract":"The design of protein sequences with desired functionalities is a fundamental task in protein engineering. Deep generative methods, such as autoregressive models and diffusion models, have greatly accelerated the discovery of novel protein sequences. However, these methods mainly focus on local or shallow residual semantics and suffer from low inference efficiency, large modeling space and high training cost. To address these challenges, we introduce ProtFlow, a fast flow matching-based protein sequence design framework that operates on embeddings derived from semantically meaningful latent space of protein language models. By compressing and smoothing the latent space, ProtFlow enhances performance while training on limited computational resources. Leveraging reflow techniques, ProtFlow enables high-quality single-step sequence generation. Additionally, we develop a joint design pipeline for the design scene of multichain proteins. We evaluate ProtFlow across diverse protein design tasks, including general peptides and long-chain proteins, antimicrobial peptides, and antibodies. Experimental results demonstrate that ProtFlow outperforms task-specific methods in these applications, underscoring its potential and broad applicability in computational protein sequence design and analysis.","authors":["Zitai Kong","Yiheng Zhu","Yinlong Xu","Hanjing Zhou","Mingzhe Yin","Jialu Wu","Hongxia Xu","Chang-Yu Hsieh","Tingjun Hou","Jian Wu"],"url":"https://arxiv.org/abs/2504.10983"}
{"created":"2025-04-16","title":"Seeing like a Cephalopod: Colour Vision with a Monochrome Event Camera","abstract":"Cephalopods exhibit unique colour discrimination capabilities despite having one type of photoreceptor, relying instead on chromatic aberration induced by their ocular optics and pupil shapes to perceive spectral information. We took inspiration from this biological mechanism to design a spectral imaging system that combines a ball lens with an event-based camera. Our approach relies on a motorised system that shifts the focal position, mirroring the adaptive lens motion in cephalopods. This approach has enabled us to achieve wavelength-dependent focusing across the visible light and near-infrared spectrum, making the event a spectral sensor. We characterise chromatic aberration effects, using both event-based and conventional frame-based sensors, validating the effectiveness of bio-inspired spectral discrimination both in simulation and in a real setup as well as assessing the spectral discrimination performance. Our proposed approach provides a robust spectral sensing capability without conventional colour filters or computational demosaicing. This approach opens new pathways toward new spectral sensing systems inspired by nature's evolutionary solutions. Code and analysis are available at: https://samiarja.github.io/neuromorphic_octopus_eye/","authors":["Sami Arja","Nimrod Kruger","Alexandre Marcireau","Nicholas Owen Ralph","Saeed Afshar","Gregory Cohen"],"url":"https://arxiv.org/abs/2504.10984"}
{"created":"2025-04-16","title":"DMPT: Decoupled Modality-aware Prompt Tuning for Multi-modal Object Re-identification","abstract":"Current multi-modal object re-identification approaches based on large-scale pre-trained backbones (i.e., ViT) have displayed remarkable progress and achieved excellent performance. However, these methods usually adopt the standard full fine-tuning paradigm, which requires the optimization of considerable backbone parameters, causing extensive computational and storage requirements. In this work, we propose an efficient prompt-tuning framework tailored for multi-modal object re-identification, dubbed DMPT, which freezes the main backbone and only optimizes several newly added decoupled modality-aware parameters. Specifically, we explicitly decouple the visual prompts into modality-specific prompts which leverage prior modality knowledge from a powerful text encoder and modality-independent semantic prompts which extract semantic information from multi-modal inputs, such as visible, near-infrared, and thermal-infrared. Built upon the extracted features, we further design a Prompt Inverse Bind (PromptIBind) strategy that employs bind prompts as a medium to connect the semantic prompt tokens of different modalities and facilitates the exchange of complementary multi-modal information, boosting final re-identification results. Experimental results on multiple common benchmarks demonstrate that our DMPT can achieve competitive results to existing state-of-the-art methods while requiring only 6.5% fine-tuning of the backbone parameters.","authors":["Minghui Lin","Shu Wang","Xiang Wang","Jianhua Tang","Longbin Fu","Zhengrong Zuo","Nong Sang"],"url":"https://arxiv.org/abs/2504.10985"}
{"created":"2025-04-16","title":"PraNet-V2: Dual-Supervised Reverse Attention for Medical Image Segmentation","abstract":"Accurate medical image segmentation is essential for effective diagnosis and treatment. Previously, PraNet-V1 was proposed to enhance polyp segmentation by introducing a reverse attention (RA) module that utilizes background information. However, PraNet-V1 struggles with multi-class segmentation tasks. To address this limitation, we propose PraNet-V2, which, compared to PraNet-V1, effectively performs a broader range of tasks including multi-class segmentation. At the core of PraNet-V2 is the Dual-Supervised Reverse Attention (DSRA) module, which incorporates explicit background supervision, independent background modeling, and semantically enriched attention fusion. Our PraNet-V2 framework demonstrates strong performance on four polyp segmentation datasets. Additionally, by integrating DSRA to iteratively enhance foreground segmentation results in three state-of-the-art semantic segmentation models, we achieve up to a 1.36% improvement in mean Dice score. Code is available at: https://github.com/ai4colonoscopy/PraNet-V2/tree/main/binary_seg/jittor.","authors":["Bo-Cheng Hu","Ge-Peng Ji","Dian Shao","Deng-Ping Fan"],"url":"https://arxiv.org/abs/2504.10986"}
{"created":"2025-04-16","title":"Leveraging Vertical Public-Private Split for Improved Synthetic Data Generation","abstract":"Differentially Private Synthetic Data Generation (DP-SDG) is a key enabler of private and secure tabular-data sharing, producing artificial data that carries through the underlying statistical properties of the input data. This typically involves adding carefully calibrated statistical noise to guarantee individual privacy, at the cost of synthetic data quality. Recent literature has explored scenarios where a small amount of public data is used to help enhance the quality of synthetic data. These methods study a horizontal public-private partitioning which assumes access to a small number of public rows that can be used for model initialization, providing a small utility gain. However, realistic datasets often naturally consist of public and private attributes, making a vertical public-private partitioning relevant for practical synthetic data deployments. We propose a novel framework that adapts horizontal public-assisted methods into the vertical setting. We compare this framework against our alternative approach that uses conditional generation, highlighting initial limitations of public-data assisted methods and proposing future research directions to address these challenges.","authors":["Samuel Maddock","Shripad Gade","Graham Cormode","Will Bullock"],"url":"https://arxiv.org/abs/2504.10987"}
{"created":"2025-04-16","title":"A broken Hardy inequality on finite element space and application to strain gradient elasticity","abstract":"We illustrate a broken Hardy inequality on discontinuous finite element spaces, blowing up with a logarithmic factor with respect to the meshes size. This is motivated by numerical analysis for the strain gradient elasticity with natural boundary conditions. A mixed finite element pair is employed to solve this model with nearly incompressible materials. This pair is quasi-stable with a logarithmic factor, which is not significant in the approximation error, and converges robustly in the incompressible limit and uniformly in the microscopic material parameter. Numerical results back up that the theoretical predictions are nearly optimal. Moreover, the regularity estimates for the model over a smooth domain have been proved with the aid of the Agmon-Douglis-Nirenberg theory.","authors":["Yulei Liao","Pingbing Ming"],"url":"https://arxiv.org/abs/2504.10993"}
{"created":"2025-04-16","title":"TMCIR: Token Merge Benefits Composed Image Retrieval","abstract":"Composed Image Retrieval (CIR) retrieves target images using a multi-modal query that combines a reference image with text describing desired modifications. The primary challenge is effectively fusing this visual and textual information. Current cross-modal feature fusion approaches for CIR exhibit an inherent bias in intention interpretation. These methods tend to disproportionately emphasize either the reference image features (visual-dominant fusion) or the textual modification intent (text-dominant fusion through image-to-text conversion). Such an imbalanced representation often fails to accurately capture and reflect the actual search intent of the user in the retrieval results. To address this challenge, we propose TMCIR, a novel framework that advances composed image retrieval through two key innovations: 1) Intent-Aware Cross-Modal Alignment. We first fine-tune CLIP encoders contrastively using intent-reflecting pseudo-target images, synthesized from reference images and textual descriptions via a diffusion model. This step enhances the encoder ability of text to capture nuanced intents in textual descriptions. 2) Adaptive Token Fusion. We further fine-tune all encoders contrastively by comparing adaptive token-fusion features with the target image. This mechanism dynamically balances visual and textual representations within the contrastive learning pipeline, optimizing the composed feature for retrieval. Extensive experiments on Fashion-IQ and CIRR datasets demonstrate that TMCIR significantly outperforms state-of-the-art methods, particularly in capturing nuanced user intent.","authors":["Chaoyang Wang","Zeyu Zhang","Long Teng","Zijun Li","Shichao Kan"],"url":"https://arxiv.org/abs/2504.10995"}
{"created":"2025-04-16","title":"Denoising Application Performance Models with Noise-Resilient Priors","abstract":"When scaling parallel codes to larger machines, performance models help identify potential bottlenecks. Since analytically designing these mathematical representations is usually challenging, empirical models based on performance measurements offer a practical alternative. Yet, measurements on HPC systems are typically affected by noise, leading to potentially misleading model predictions. To reduce the influence of noise, we introduce application-specific dynamic priors into the modeling process, which we derive from noise-resilient measurements of computational effort and knowledge of typical algorithms used in communication routines. These priors then narrow the search space for our performance models, excluding complexity classes that reflect noise rather than performance. Our approach keeps the models much closer to theoretical expectations and significantly improves their predictive power. Finally, it cuts experimental costs in half by minimizing the number of repeated measurements.","authors":["Gustavo de Morais","Alexander Gei{\\ss}","Alexandru Calotoiu","Gregor Corbin","Ahmad Tarraf","Torsten Hoefler","Bernd Mohr","Felix Wolf"],"url":"https://arxiv.org/abs/2504.10996"}
{"created":"2025-04-16","title":"Why am I seeing this? Towards recognizing social media recommender systems with missing recommendations","abstract":"Social media plays a crucial role in shaping society, often amplifying polarization and spreading misinformation. These effects stem from complex dynamics involving user interactions, individual traits, and recommender algorithms driving content selection. Recommender systems, which significantly shape the content users see and decisions they make, offer an opportunity for intervention and regulation. However, assessing their impact is challenging due to algorithmic opacity and limited data availability. To effectively model user decision-making, it is crucial to recognize the recommender system adopted by the platform.","authors":["Sabrina Guidotti","Sabrina Patania","Giuseppe Vizzari","Dimitri Ognibene","Gregor Donabauer","Udo Kruschwitz","Davide Taibi"],"url":"https://arxiv.org/abs/2504.11000"}
{"created":"2025-04-16","title":"ReZero: Enhancing LLM search ability by trying one-more-time","abstract":"Retrieval-Augmented Generation (RAG) improves Large Language Model (LLM) performance on knowledge-intensive tasks but depends heavily on initial search query quality. Current methods, often using Reinforcement Learning (RL), typically focus on query formulation or reasoning over results, without explicitly encouraging persistence after a failed search. We introduce ReZero (Retry-Zero), a novel RL framework that directly rewards the act of retrying a search query following an initial unsuccessful attempt. This incentivizes the LLM to explore alternative queries rather than prematurely halting. ReZero demonstrates significant improvement, achieving 46.88% accuracy compared to a 25% baseline. By rewarding persistence, ReZero enhances LLM robustness in complex information-seeking scenarios where initial queries may prove insufficient.","authors":["Alan Dao (Gia Tuan Dao)","Thinh Le"],"url":"https://arxiv.org/abs/2504.11001"}
{"created":"2025-04-16","title":"Dopamine Audiobook: A Training-free MLLM Agent for Emotional and Human-like Audiobook Generation","abstract":"Audiobook generation, which creates vivid and emotion-rich audio works, faces challenges in conveying complex emotions, achieving human-like qualities, and aligning evaluations with human preferences. Existing text-to-speech (TTS) methods are often limited to specific scenarios, struggle with emotional transitions, and lack automatic human-aligned evaluation benchmarks, instead relying on either misaligned automated metrics or costly human assessments. To address these issues, we propose Dopamine Audiobook, a new unified training-free system leveraging a multimodal large language model (MLLM) as an AI agent for emotional and human-like audiobook generation and evaluation. Specifically, we first design a flow-based emotion-enhanced framework that decomposes complex emotional speech synthesis into controllable sub-tasks. Then, we propose an adaptive model selection module that dynamically selects the most suitable TTS methods from a set of existing state-of-the-art (SOTA) TTS methods for diverse scenarios. We further enhance emotional expressiveness through paralinguistic augmentation and prosody retrieval at word and utterance levels. For evaluation, we propose a novel GPT-based evaluation framework incorporating self-critique, perspective-taking, and psychological MagicEmo prompts to ensure human-aligned and self-aligned assessments. Experiments show that our method generates long speech with superior emotional expression to SOTA TTS models in various metrics. Importantly, our evaluation framework demonstrates better alignment with human preferences and transferability across audio tasks. Project website with audio samples can be found at https://dopamine-audiobook.github.io.","authors":["Yan Rong","Shan Yang","Guangzhi Lei","Li Liu"],"url":"https://arxiv.org/abs/2504.11002"}
{"created":"2025-04-16","title":"3D Gabor Splatting: Reconstruction of High-frequency Surface Texture using Gabor Noise","abstract":"3D Gaussian splatting has experienced explosive popularity in the past few years in the field of novel view synthesis. The lightweight and differentiable representation of the radiance field using the Gaussian enables rapid and high-quality reconstruction and fast rendering. However, reconstructing objects with high-frequency surface textures (e.g., fine stripes) requires many skinny Gaussian kernels because each Gaussian represents only one color if viewed from one direction. Thus, reconstructing the stripes pattern, for example, requires Gaussians for at least the number of stripes. We present 3D Gabor splatting, which augments the Gaussian kernel to represent spatially high-frequency signals using Gabor noise. The Gabor kernel is a combination of a Gaussian term and spatially fluctuating wave functions, making it suitable for representing spatial high-frequency texture. We demonstrate that our 3D Gabor splatting can reconstruct various high-frequency textures on the objects.","authors":["Haato Watanabe","Kenji Tojo","Nobuyuki Umetani"],"url":"https://arxiv.org/abs/2504.11003"}
{"created":"2025-04-16","title":"Dynamic Compressing Prompts for Efficient Inference of Large Language Models","abstract":"Large Language Models (LLMs) have shown outstanding performance across a variety of tasks, partly due to advanced prompting techniques. However, these techniques often require lengthy prompts, which increase computational costs and can hinder performance because of the limited context windows of LLMs. While prompt compression is a straightforward solution, existing methods confront the challenges of retaining essential information, adapting to context changes, and remaining effective across different tasks. To tackle these issues, we propose a task-agnostic method called Dynamic Compressing Prompts (LLM-DCP). Our method reduces the number of prompt tokens while aiming to preserve the performance as much as possible. We model prompt compression as a Markov Decision Process (MDP), enabling the DCP-Agent to sequentially remove redundant tokens by adapting to dynamic contexts and retaining crucial content. We develop a reward function for training the DCP-Agent that balances the compression rate, the quality of the LLM output, and the retention of key information. This allows for prompt token reduction without needing an external black-box LLM. Inspired by the progressive difficulty adjustment in curriculum learning, we introduce a Hierarchical Prompt Compression (HPC) training strategy that gradually increases the compression difficulty, enabling the DCP-Agent to learn an effective compression method that maintains information integrity. Experiments demonstrate that our method outperforms state-of-the-art techniques, especially at higher compression rates. The code for our approach will be available at https://github.com/Fhujinwu/DCP.","authors":["Jinwu Hu","Wei Zhang","Yufeng Wang","Yu Hu","Bin Xiao","Mingkui Tan","Qing Du"],"url":"https://arxiv.org/abs/2504.11004"}
{"created":"2025-04-16","title":"Kubernetes in the Cloud vs. Bare Metal: A Comparative Study of Network Costs","abstract":"Modern cloud-native applications increasingly utilise managed cloud services and containerisation technologies, such as Kubernetes, to achieve rapid time-to-market and scalable deployments. Organisations must consider various factors, including cost implications when deciding on a hosting platform for containerised applications as the usage grows. An emerging discipline called FinOps combines financial management and cloud operations to optimise costs in cloud-based applications. While prior research has explored system-level optimisation strategies for cost and resource efficiency in containerized systems, analysing network costs in Kubernetes clusters remains underexplored. This paper investigates the network usage and cost implications of containerised applications running on Kubernetes clusters. Using a methodology that combines measurement analysis, experimentation, and cost modelling, we aim to provide organisations with actionable insights into network cost optimisation. Our findings highlight key considerations for analysing network expenditures and evaluating the potential cost benefits of deploying applications on cloud providers. Overall, this paper contributes to the emerging FinOps discipline by addressing the financial and operational aspects of managing network costs in cloud-native environments.","authors":["Rodrigo Mompo Redoli","Amjad Ullah"],"url":"https://arxiv.org/abs/2504.11007"}
{"created":"2025-04-16","title":"MediSee: Reasoning-based Pixel-level Perception in Medical Images","abstract":"Despite remarkable advancements in pixel-level medical image perception, existing methods are either limited to specific tasks or heavily rely on accurate bounding boxes or text labels as input prompts. However, the medical knowledge required for input is a huge obstacle for general public, which greatly reduces the universality of these methods. Compared with these domain-specialized auxiliary information, general users tend to rely on oral queries that require logical reasoning. In this paper, we introduce a novel medical vision task: Medical Reasoning Segmentation and Detection (MedSD), which aims to comprehend implicit queries about medical images and generate the corresponding segmentation mask and bounding box for the target object. To accomplish this task, we first introduce a Multi-perspective, Logic-driven Medical Reasoning Segmentation and Detection (MLMR-SD) dataset, which encompasses a substantial collection of medical entity targets along with their corresponding reasoning. Furthermore, we propose MediSee, an effective baseline model designed for medical reasoning segmentation and detection. The experimental results indicate that the proposed method can effectively address MedSD with implicit colloquial queries and outperform traditional medical referring segmentation methods.","authors":["Qinyue Tong","Ziqian Lu","Jun Liu","Yangming Zheng","Zheming Lu"],"url":"https://arxiv.org/abs/2504.11008"}
{"created":"2025-04-16","title":"MMC: Iterative Refinement of VLM Reasoning via MCTS-based Multimodal Critique","abstract":"Visual language models (VLMs) have demonstrated strong performance across diverse multimodal reasoning tasks but still face challenges such as hallucinations, resulting in incorrect reasoning outcomes. Inspired by recent research on external feedback mechanisms in large language models (LLMs), we propose a multimodal actor-critic framework to enhance VLM reasoning capabilities. Specifically, the actor model generates step-by-step reasoning paths based on image and text inputs, while the critic model evaluates these reasoning paths and provides corrective feedback. The actor model iteratively refines its reasoning based on the feedback until the reasoning outcome is deemed satisfactory by the critic model. To reduce reliance on costly manual annotations, we introduce an automated method for constructing multimodal critique datasets. By leveraging Monte Carlo Tree Search (MCTS), we systematically guide the actor model to explore diverse reasoning paths. To obtain critique data for correcting erroneous reasoning steps, we prompt an annotator model to compare pairs of reasoning paths diverging from a shared ancestor node - one leading to a correct conclusion and the other to an incorrect one. This approach enables us to construct the MMC (MCTS-based Multimodal Critique) dataset, upon which we further develop a comprehensive training and inference pipeline. Extensive experiments conducted on several public benchmark datasets and mainstream VLMs demonstrate that our approach significantly improves the performance of VLM on complex multimodal reasoning tasks, underscoring its effectiveness and wide applicability.","authors":["Shuhang Liu","Zhenrong Zhang","Pengfei Hu","Jiefeng Ma","Jun Du","Qing Wang","Jianshu Zhang","Quan Liu","Jianqing Gao","Feng Ma"],"url":"https://arxiv.org/abs/2504.11009"}
{"created":"2025-04-16","title":"New Constructions of Binary Cyclic Codes with Both Relatively Large Minimum Distance and Dual Distance","abstract":"Binary cyclic codes are worth studying due to their applications and theoretical importance. It is an important problem to construct an infinite family of cyclic codes with large minimum distance $d$ and dual distance $d^{\\perp}$. In recent years, much research has been devoted to improving the lower bound on $d$, some of which have exceeded the square-root bound. The constructions presented recently seem to indicate that when the minimum distance increases, the minimum distance of its dual code decreases. In this paper, we focus on the new constructions of binary cyclic codes with length $n=2^m-1$, dimension near $n/2$, and both relatively large minimum distance and dual distance. For $m$ is even, we construct a family of binary cyclic codes with parameters $[2^m-1,2^{m-1}\\pm1,d]$, where $d\\ge 2^{m/2}-1$ and $d^\\perp\\ge2^{m/2}$. Both the minimum distance and the dual distance are significantly better than the previous results. When $m$ is the product of two distinct primes, we construct some cyclic codes with dimensions $k=(n+1)/2$ and $d>\\frac{n}{\\log_2n},$ where the lower bound on the minimum distance is much larger than the square-root bound. For $m$ is odd, we present two families of binary $[2^m-1,2^{m-1},d]$ cyclic codes with $d\\ge2^{(m+1)/2}-1$, $d^\\perp\\ge2^{(m+1)/2}$ and $d\\ge2^{(m+3)/2}-15$, $d^\\perp\\ge2^{(m-1)/2}$ respectively, which leads that $d\\cdot d^\\perp$ can reach $2n$ asymptotically. To the best of our knowledge, except for the punctured binary Reed-Muller codes, there is no other construction of binary cyclic codes that reaches this bound.","authors":["Lingqi Zheng","Weijun Fang","Rongxing Qiu"],"url":"https://arxiv.org/abs/2504.11010"}
{"created":"2025-04-16","title":"Document Quality Scoring for Web Crawling","abstract":"The internet contains large amounts of low-quality content, yet users expect web search engines to deliver high-quality, relevant results. The abundant presence of low-quality pages can negatively impact retrieval and crawling processes by wasting resources on these documents. Therefore, search engines can greatly benefit from techniques that leverage efficient quality estimation methods to mitigate these negative impacts. Quality scoring methods for web pages are useful for many processes typical for web search systems, including static index pruning, index tiering, and crawling. Building on work by Chang et al.~\\cite{chang2024neural}, who proposed using neural estimators of semantic quality for static index pruning, we extend their approach and apply their neural quality scorers to assess the semantic quality of web pages in crawling prioritisation tasks. In our experimental analysis, we found that prioritising semantically high-quality pages over low-quality ones can improve downstream search effectiveness. Our software contribution consists of a Docker container that computes an effective quality score for a given web page, allowing the quality scorer to be easily included and used in other components of web search systems.","authors":["Francesca Pezzuti","Ariane Mueller","Sean MacAvaney","Nicola Tonellotto"],"url":"https://arxiv.org/abs/2504.11011"}
{"created":"2025-04-16","title":"GATE3D: Generalized Attention-based Task-synergized Estimation in 3D*","abstract":"The emerging trend in computer vision emphasizes developing universal models capable of simultaneously addressing multiple diverse tasks. Such universality typically requires joint training across multi-domain datasets to ensure effective generalization. However, monocular 3D object detection presents unique challenges in multi-domain training due to the scarcity of datasets annotated with accurate 3D ground-truth labels, especially beyond typical road-based autonomous driving contexts. To address this challenge, we introduce a novel weakly supervised framework leveraging pseudo-labels. Current pretrained models often struggle to accurately detect pedestrians in non-road environments due to inherent dataset biases. Unlike generalized image-based 2D object detection models, achieving similar generalization in monocular 3D detection remains largely unexplored. In this paper, we propose GATE3D, a novel framework designed specifically for generalized monocular 3D object detection via weak supervision. GATE3D effectively bridges domain gaps by employing consistency losses between 2D and 3D predictions. Remarkably, our model achieves competitive performance on the KITTI benchmark as well as on an indoor-office dataset collected by us to evaluate the generalization capabilities of our framework. Our results demonstrate that GATE3D significantly accelerates learning from limited annotated data through effective pre-training strategies, highlighting substantial potential for broader impacts in robotics, augmented reality, and virtual reality applications. Project page: https://ies0411.github.io/GATE3D/","authors":["Eunsoo Im","Jung Kwon Lee","Changhyun Jee"],"url":"https://arxiv.org/abs/2504.11014"}
{"created":"2025-04-16","title":"AnimeDL-2M: Million-Scale AI-Generated Anime Image Detection and Localization in Diffusion Era","abstract":"Recent advances in image generation, particularly diffusion models, have significantly lowered the barrier for creating sophisticated forgeries, making image manipulation detection and localization (IMDL) increasingly challenging. While prior work in IMDL has focused largely on natural images, the anime domain remains underexplored-despite its growing vulnerability to AI-generated forgeries. Misrepresentations of AI-generated images as hand-drawn artwork, copyright violations, and inappropriate content modifications pose serious threats to the anime community and industry. To address this gap, we propose AnimeDL-2M, the first large-scale benchmark for anime IMDL with comprehensive annotations. It comprises over two million images including real, partially manipulated, and fully AI-generated samples. Experiments indicate that models trained on existing IMDL datasets of natural images perform poorly when applied to anime images, highlighting a clear domain gap between anime and natural images. To better handle IMDL tasks in anime domain, we further propose AniXplore, a novel model tailored to the visual characteristics of anime imagery. Extensive evaluations demonstrate that AniXplore achieves superior performance compared to existing methods. Dataset and code can be found in https://flytweety.github.io/AnimeDL2M/.","authors":["Chenyang Zhu","Xing Zhang","Yuyang Sun","Ching-Chun Chang","Isao Echizen"],"url":"https://arxiv.org/abs/2504.11015"}
{"created":"2025-04-16","title":"Heating reduction as collective action: Impact on attitudes, behavior and energy consumption in a Polish field experiment","abstract":"Heating and hot water usage account for nearly 80% of household energy consumption in the European Union. In order to reach the EU New Deal goals, new policies to reduce heat energy consumption are indispensable. However, research targeting reductions concentrates either on technical building interventions without considerations of people's behavior, or psychological interventions with no technical interference. Such interventions can be promising, but their true potential for scaling up can only be realized by testing approaches that integrate behavioral and technical solutions in tandem rather than in isolation. In this research, we study a mix of psychological and technical interventions targeting heating and hot water demand among students in Polish university dormitories. We evaluate effects on building energy consumption, behavioral spillovers and on social beliefs and attitudes in a pre-post quasi-experimental mixed-method field study in three student dormitories. Our findings reveal that the most effective approaches to yield energy savings were a direct, collectively framed request to students to reduce thermostat settings for the environment, and an automated technical adjustment of the heating curve temperature. Conversely, interventions targeting domestic hot water had unintended effects, including increased energy use and negative spillovers, such as higher water consumption. Further, we find that informing students about their active, collective participation had a positive impact on perceived social norms. Our findings highlight the importance of trialing interventions in controlled real-world settings to understand the interplay between technical systems, behaviors, and social impacts to enable scalable, evidence-based policies driving an effective and sustainable energy transition.","authors":["Mona Bielig","Lukasz Malewski","Karol Bandurski","Florian Kutzner","Melanie Vogel","Sonja Klingert","Radoslaw Gorzenski","Celina Kacperski"],"url":"https://arxiv.org/abs/2504.11016"}
{"created":"2025-04-16","title":"DRIFT open dataset: A drone-derived intelligence for traffic analysis in urban environmen","abstract":"Reliable traffic data are essential for understanding urban mobility and developing effective traffic management strategies. This study introduces the DRone-derived Intelligence For Traffic analysis (DRIFT) dataset, a large-scale urban traffic dataset collected systematically from synchronized drone videos at approximately 250 meters altitude, covering nine interconnected intersections in Daejeon, South Korea. DRIFT provides high-resolution vehicle trajectories that include directional information, processed through video synchronization and orthomap alignment, resulting in a comprehensive dataset of 81,699 vehicle trajectories. Through our DRIFT dataset, researchers can simultaneously analyze traffic at multiple scales - from individual vehicle maneuvers like lane-changes and safety metrics such as time-to-collision to aggregate network flow dynamics across interconnected urban intersections. The DRIFT dataset is structured to enable immediate use without additional preprocessing, complemented by open-source models for object detection and trajectory extraction, as well as associated analytical tools. DRIFT is expected to significantly contribute to academic research and practical applications, such as traffic flow analysis and simulation studies. The dataset and related resources are publicly accessible at https://github.com/AIxMobility/The-DRIFT.","authors":["Hyejin Lee","Seokjun Hong","Jeonghoon Song","Haechan Cho","Zhixiong Jin","Byeonghun Kim","Joobin Jin","Jaegyun Im","Byeongjoon Noh","Hwasoo Yeo"],"url":"https://arxiv.org/abs/2504.11019"}
{"created":"2025-04-16","title":"\"Even explanations will not help in trusting [this] fundamentally biased system\": A Predictive Policing Case-Study","abstract":"In today's society, where Artificial Intelligence (AI) has gained a vital role, concerns regarding user's trust have garnered significant attention. The use of AI systems in high-risk domains have often led users to either under-trust it, potentially causing inadequate reliance or over-trust it, resulting in over-compliance. Therefore, users must maintain an appropriate level of trust. Past research has indicated that explanations provided by AI systems can enhance user understanding of when to trust or not trust the system. However, the utility of presentation of different explanations forms still remains to be explored especially in high-risk domains. Therefore, this study explores the impact of different explanation types (text, visual, and hybrid) and user expertise (retired police officers and lay users) on establishing appropriate trust in AI-based predictive policing. While we observed that the hybrid form of explanations increased the subjective trust in AI for expert users, it did not led to better decision-making. Furthermore, no form of explanations helped build appropriate trust. The findings of our study emphasize the importance of re-evaluating the use of explanations to build [appropriate] trust in AI based systems especially when the system's use is questionable. Finally, we synthesize potential challenges and policy recommendations based on our results to design for appropriate trust in high-risk based AI-based systems.","authors":["Siddharth Mehrotra","Ujwal Gadiraju","Eva Bittner","Folkert van Delden","Catholijn M. Jonker","Myrthe L. Tielman"],"url":"https://arxiv.org/abs/2504.11020"}
{"created":"2025-04-16","title":"Meta-learning For Few-Shot Time Series Crop Type Classification: A Benchmark On The EuroCropsML Dataset","abstract":"Spatial imbalances in crop type data pose significant challenges for accurate classification in remote sensing applications. Algorithms aiming at transferring knowledge from data-rich to data-scarce tasks have thus surged in popularity. However, despite their effectiveness in previous evaluations, their performance in challenging real-world applications is unclear and needs to be evaluated. This study benchmarks transfer learning and several meta-learning algorithms, including (First-Order) Model-Agnostic Meta-Learning ((FO)-MAML), Almost No Inner Loop (ANIL), and Task-Informed Meta-Learning (TIML), on the real-world EuroCropsML time series dataset, which combines farmer-reported crop data with Sentinel-2 satellite observations from Estonia, Latvia, and Portugal. Our findings indicate that MAML-based meta-learning algorithms achieve slightly higher accuracy compared to simpler transfer learning methods when applied to crop type classification tasks in Estonia after pre-training on data from Latvia. However, this improvement comes at the cost of increased computational demands and training time. Moreover, we find that the transfer of knowledge between geographically disparate regions, such as Estonia and Portugal, poses significant challenges to all investigated algorithms. These insights underscore the trade-offs between accuracy and computational resource requirements in selecting machine learning methods for real-world crop type classification tasks and highlight the difficulties of transferring knowledge between different regions of the Earth. To facilitate future research in this domain, we present the first comprehensive benchmark for evaluating transfer and meta-learning methods for crop type classification under real-world conditions. The corresponding code is publicly available at https://github.com/dida-do/eurocrops-meta-learning.","authors":["Joana Reuss","Jan Macdonald","Simon Becker","Konrad Schultka","Lorenz Richter","Marco K\\\"orner"],"url":"https://arxiv.org/abs/2504.11022"}
{"created":"2025-04-16","title":"Easy3D: A Simple Yet Effective Method for 3D Interactive Segmentation","abstract":"The increasing availability of digital 3D environments, whether through image-based 3D reconstruction, generation, or scans obtained by robots, is driving innovation across various applications. These come with a significant demand for 3D interaction, such as 3D Interactive Segmentation, which is useful for tasks like object selection and manipulation. Additionally, there is a persistent need for solutions that are efficient, precise, and performing well across diverse settings, particularly in unseen environments and with unfamiliar objects. In this work, we introduce a 3D interactive segmentation method that consistently surpasses previous state-of-the-art techniques on both in-domain and out-of-domain datasets. Our simple approach integrates a voxel-based sparse encoder with a lightweight transformer-based decoder that implements implicit click fusion, achieving superior performance and maximizing efficiency. Our method demonstrates substantial improvements on benchmark datasets, including ScanNet, ScanNet++, S3DIS, and KITTI-360, and also on unseen geometric distributions such as the ones obtained by Gaussian Splatting. The project web-page is available at https://simonelli-andrea.github.io/easy3d.","authors":["Andrea Simonelli","Norman M\\\"uller","Peter Kontschieder"],"url":"https://arxiv.org/abs/2504.11024"}
{"created":"2025-04-16","title":"A PyTorch-Compatible Spike Encoding Framework for Energy-Efficient Neuromorphic Applications","abstract":"Spiking Neural Networks (SNNs) offer promising energy efficiency advantages, particularly when processing sparse spike trains. However, their incompatibility with traditional datasets, which consist of batches of input vectors rather than spike trains, necessitates the development of efficient encoding methods. This paper introduces a novel, open-source PyTorch-compatible Python framework for spike encoding, designed for neuromorphic applications in machine learning and reinforcement learning. The framework supports a range of encoding algorithms, including Leaky Integrate-and-Fire (LIF), Step Forward (SF), Pulse Width Modulation (PWM), and Ben's Spiker Algorithm (BSA), as well as specialized encoding strategies covering population coding and reinforcement learning scenarios. Furthermore, we investigate the performance trade-offs of each method on embedded hardware using C/C++ implementations, considering energy consumption, computation time, spike sparsity, and reconstruction accuracy. Our findings indicate that SF typically achieves the lowest reconstruction error and offers the highest energy efficiency and fastest encoding speed, achieving the second-best spike sparsity. At the same time, other methods demonstrate particular strengths depending on the signal characteristics. This framework and the accompanying empirical analysis provide valuable resources for selecting optimal encoding strategies for energy-efficient SNN applications.","authors":["Alexandru Vasilache","Jona Scholz","Vincent Schilling","Sven Nitzsche","Florian Kaelber","Johannes Korsch","Juergen Becker"],"url":"https://arxiv.org/abs/2504.11026"}
{"created":"2025-04-16","title":"Acquisition of high-quality images for camera calibration in robotics applications via speech prompts","abstract":"Accurate intrinsic and extrinsic camera calibration can be an important prerequisite for robotic applications that rely on vision as input. While there is ongoing research on enabling camera calibration using natural images, many systems in practice still rely on using designated calibration targets with e.g. checkerboard patterns or April tag grids. Once calibration images from different perspectives have been acquired and feature descriptors detected, those are typically used in an optimization process to minimize the geometric reprojection error. For this optimization to converge, input images need to be of sufficient quality and particularly sharpness; they should neither contain motion blur nor rolling-shutter artifacts that can arise when the calibration board was not static during image capture. In this work, we present a novel calibration image acquisition technique controlled via voice commands recorded with a clip-on microphone, that can be more robust and user-friendly than e.g. triggering capture with a remote control, or filtering out blurry frames from a video sequence in postprocessing. To achieve this, we use a state-of-the-art speech-to-text transcription model with accurate per-word timestamping to capture trigger words with precise temporal alignment. Our experiments show that the proposed method improves user experience by being fast and efficient, allowing us to successfully calibrate complex multi-camera setups.","authors":["Timm Linder","Kadir Yilmaz","David B. Adrian","Bastian Leibe"],"url":"https://arxiv.org/abs/2504.11031"}
{"created":"2025-04-16","title":"Defending Against Frequency-Based Attacks with Diffusion Models","abstract":"Adversarial training is a common strategy for enhancing model robustness against adversarial attacks. However, it is typically tailored to the specific attack types it is trained on, limiting its ability to generalize to unseen threat models. Adversarial purification offers an alternative by leveraging a generative model to remove perturbations before classification. Since the purifier is trained independently of both the classifier and the threat models, it is better equipped to handle previously unseen attack scenarios. Diffusion models have proven highly effective for noise purification, not only in countering pixel-wise adversarial perturbations but also in addressing non-adversarial data shifts. In this study, we broaden the focus beyond pixel-wise robustness to explore the extent to which purification can mitigate both spectral and spatial adversarial attacks. Our findings highlight its effectiveness in handling diverse distortion patterns across low- to high-frequency regions.","authors":["Fatemeh Amerehi","Patrick Healy"],"url":"https://arxiv.org/abs/2504.11034"}
{"created":"2025-04-16","title":"QAVA: Query-Agnostic Visual Attack to Large Vision-Language Models","abstract":"In typical multimodal tasks, such as Visual Question Answering (VQA), adversarial attacks targeting a specific image and question can lead large vision-language models (LVLMs) to provide incorrect answers. However, it is common for a single image to be associated with multiple questions, and LVLMs may still answer other questions correctly even for an adversarial image attacked by a specific question. To address this, we introduce the query-agnostic visual attack (QAVA), which aims to create robust adversarial examples that generate incorrect responses to unspecified and unknown questions. Compared to traditional adversarial attacks focused on specific images and questions, QAVA significantly enhances the effectiveness and efficiency of attacks on images when the question is unknown, achieving performance comparable to attacks on known target questions. Our research broadens the scope of visual adversarial attacks on LVLMs in practical settings, uncovering previously overlooked vulnerabilities, particularly in the context of visual adversarial threats. The code is available at https://github.com/btzyd/qava.","authors":["Yudong Zhang","Ruobing Xie","Jiansheng Chen","Xingwu Sun","Zhanhui Kang","Yu Wang"],"url":"https://arxiv.org/abs/2504.11038"}
{"created":"2025-04-16","title":"LazyReview A Dataset for Uncovering Lazy Thinking in NLP Peer Reviews","abstract":"Peer review is a cornerstone of quality control in scientific publishing. With the increasing workload, the unintended use of `quick' heuristics, referred to as lazy thinking, has emerged as a recurring issue compromising review quality. Automated methods to detect such heuristics can help improve the peer-reviewing process. However, there is limited NLP research on this issue, and no real-world dataset exists to support the development of detection tools. This work introduces LazyReview, a dataset of peer-review sentences annotated with fine-grained lazy thinking categories. Our analysis reveals that Large Language Models (LLMs) struggle to detect these instances in a zero-shot setting. However, instruction-based fine-tuning on our dataset significantly boosts performance by 10-20 performance points, highlighting the importance of high-quality training data. Furthermore, a controlled experiment demonstrates that reviews revised with lazy thinking feedback are more comprehensive and actionable than those written without such feedback. We will release our dataset and the enhanced guidelines that can be used to train junior reviewers in the community. (Code available here: https://github.com/UKPLab/arxiv2025-lazy-review)","authors":["Sukannya Purkayastha","Zhuang Li","Anne Lauscher","Lizhen Qu","Iryna Gurevych"],"url":"https://arxiv.org/abs/2504.11042"}
{"created":"2025-04-16","title":"Neural Control Barrier Functions from Physics Informed Neural Networks","abstract":"As autonomous systems become increasingly prevalent in daily life, ensuring their safety is paramount. Control Barrier Functions (CBFs) have emerged as an effective tool for guaranteeing safety; however, manually designing them for specific applications remains a significant challenge. With the advent of deep learning techniques, recent research has explored synthesizing CBFs using neural networks-commonly referred to as neural CBFs. This paper introduces a novel class of neural CBFs that leverages a physics-inspired neural network framework by incorporating Zubov's Partial Differential Equation (PDE) within the context of safety. This approach provides a scalable methodology for synthesizing neural CBFs applicable to high-dimensional systems. Furthermore, by utilizing reciprocal CBFs instead of zeroing CBFs, the proposed framework allows for the specification of flexible, user-defined safe regions. To validate the effectiveness of the approach, we present case studies on three different systems: an inverted pendulum, autonomous ground navigation, and aerial navigation in obstacle-laden environments.","authors":["Shreenabh Agrawal","Manan Tayal","Aditya Singh","Shishir Kolathaya"],"url":"https://arxiv.org/abs/2504.11045"}
{"created":"2025-04-16","title":"Leveraging LLMs and attention-mechanism for automatic annotation of historical maps","abstract":"Historical maps are essential resources that provide insights into the geographical landscapes of the past. They serve as valuable tools for researchers across disciplines such as history, geography, and urban studies, facilitating the reconstruction of historical environments and the analysis of spatial transformations over time. However, when constrained to analogue or scanned formats, their interpretation is limited to humans and therefore not scalable. Recent advancements in machine learning, particularly in computer vision and large language models (LLMs), have opened new avenues for automating the recognition and classification of features and objects in historical maps. In this paper, we propose a novel distillation method that leverages LLMs and attention mechanisms for the automatic annotation of historical maps. LLMs are employed to generate coarse classification labels for low-resolution historical image patches, while attention mechanisms are utilized to refine these labels to higher resolutions. Experimental results demonstrate that the refined labels achieve a high recall of more than 90%. Additionally, the intersection over union (IoU) scores--84.2% for Wood and 72.0% for Settlement--along with precision scores of 87.1% and 79.5%, respectively, indicate that most labels are well-aligned with ground-truth annotations. Notably, these results were achieved without the use of fine-grained manual labels during training, underscoring the potential of our approach for efficient and scalable historical map analysis.","authors":["Yunshuang Yuan","Monika Sester"],"url":"https://arxiv.org/abs/2504.11050"}
{"created":"2025-04-16","title":"QualiTagger: Automating software quality detection in issue trackers","abstract":"A systems quality is a major concern for development teams when it evolve. Understanding the effects of a loss of quality in the codebase is crucial to avoid side effects like the appearance of technical debt. Although the identification of these qualities in software requirements described in natural language has been investigated, most of the results are often not applicable in practice, and rely on having been validated on small datasets and limited amount of projects. For many years, machine learning (ML) techniques have been proved as a valid technique to identify and tag terms described in natural language. In order to advance previous works, in this research we use cutting edge models like Transformers, together with a vast dataset mined and curated from GitHub, to identify what text is usually associated with different quality properties. We also study the distribution of such qualities in issue trackers from openly accessible software repositories, and we evaluate our approach both with students from a software engineering course and with its application to recognize security labels in industry.","authors":["Karthik Shivashankar","Rafael Capilla","Maren Maritsdatter Kruke","Mili Orucevic","Antonio Martini"],"url":"https://arxiv.org/abs/2504.11053"}
{"created":"2025-04-16","title":"Zero-Shot Whole-Body Humanoid Control via Behavioral Foundation Models","abstract":"Unsupervised reinforcement learning (RL) aims at pre-training agents that can solve a wide range of downstream tasks in complex environments. Despite recent advancements, existing approaches suffer from several limitations: they may require running an RL process on each downstream task to achieve a satisfactory performance, they may need access to datasets with good coverage or well-curated task-specific samples, or they may pre-train policies with unsupervised losses that are poorly correlated with the downstream tasks of interest. In this paper, we introduce a novel algorithm regularizing unsupervised RL towards imitating trajectories from unlabeled behavior datasets. The key technical novelty of our method, called Forward-Backward Representations with Conditional-Policy Regularization, is to train forward-backward representations to embed the unlabeled trajectories to the same latent space used to represent states, rewards, and policies, and use a latent-conditional discriminator to encourage policies to ``cover'' the states in the unlabeled behavior dataset. As a result, we can learn policies that are well aligned with the behaviors in the dataset, while retaining zero-shot generalization capabilities for reward-based and imitation tasks. We demonstrate the effectiveness of this new approach in a challenging humanoid control problem: leveraging observation-only motion capture datasets, we train Meta Motivo, the first humanoid behavioral foundation model that can be prompted to solve a variety of whole-body tasks, including motion tracking, goal reaching, and reward optimization. The resulting model is capable of expressing human-like behaviors and it achieves competitive performance with task-specific methods while outperforming state-of-the-art unsupervised RL and model-based baselines.","authors":["Andrea Tirinzoni","Ahmed Touati","Jesse Farebrother","Mateusz Guzek","Anssi Kanervisto","Yingchen Xu","Alessandro Lazaric","Matteo Pirotta"],"url":"https://arxiv.org/abs/2504.11054"}
{"created":"2025-04-16","title":"Crane: Context-Guided Prompt Learning and Attention Refinement for Zero-Shot Anomaly Detections","abstract":"Anomaly Detection (AD) involves identifying deviations from normal data distributions and is critical in fields such as medical diagnostics and industrial defect detection. Traditional AD methods typically require the availability of normal training samples; however, this assumption is not always feasible, as collecting such data can be impractical. Additionally, these methods often struggle to generalize across different domains. Recent advancements, such as AnomalyCLIP and AdaCLIP, utilize the zero-shot generalization capabilities of CLIP but still face a performance gap between image-level and pixel-level anomaly detection. To address this gap, we propose a novel approach that conditions the prompts of the text encoder based on image context extracted from the vision encoder. Also, to capture fine-grained variations more effectively, we have modified the CLIP vision encoder and altered the extraction of dense features. These changes ensure that the features retain richer spatial and structural information for both normal and anomalous prompts. Our method achieves state-of-the-art performance, improving performance by 2% to 29% across different metrics on 14 datasets. This demonstrates its effectiveness in both image-level and pixel-level anomaly detection.","authors":["Alireza Salehi","Mohammadreza Salehi","Reshad Hosseini","Cees G. M. Snoek","Makoto Yamada","Mohammad Sabokrou"],"url":"https://arxiv.org/abs/2504.11055"}
{"created":"2025-04-16","title":"A study of troubled-cell indicators applied to finite volume methods using a novel monotonicity parameter","abstract":"We adapt a troubled-cell indicator developed for discontinuous Galerkin (DG) methods to the finite volume method (FVM) framework for solving hyperbolic conservation laws. This indicator depends solely on the cell-average data of the target cell and its immediate neighbours. Once the troubled-cells are identified, we apply the limiter only in these cells instead of applying in all computational cells. We introduce a novel technique to quantify the quality of the solution in the neighbourhood of the shock by defining a monotonicity parameter $\\mu$. Numerical results from various two-dimensional simulations on the hyperbolic systems of Euler equations using a finite volume solver employing MUSCL reconstruction validate the performance of the troubled-cell indicator and the approach of limiting only in the troubled-cells. These results show that limiting only in the troubled-cells is preferable to limiting everywhere as it improves convergence without compromising on the solution accuracy.","authors":["R. Shivananda Rao","M. Ramakrishna"],"url":"https://arxiv.org/abs/2504.11056"}
{"created":"2025-04-16","title":"Quantifying Group Fairness in Community Detection","abstract":"Understanding community structures is crucial for analyzing networks, as nodes join communities that collectively shape large-scale networks. In real-world settings, the formation of communities is often impacted by several social factors, such as ethnicity, gender, wealth, or other attributes. These factors may introduce structural inequalities; for instance, real-world networks can have a few majority groups and many minority groups. Community detection algorithms, which identify communities based on network topology, may generate unfair outcomes if they fail to account for existing structural inequalities, particularly affecting underrepresented groups. In this work, we propose a set of novel group fairness metrics to assess the fairness of community detection methods. Additionally, we conduct a comparative evaluation of the most common community detection methods, analyzing the trade-off between performance and fairness. Experiments are performed on synthetic networks generated using LFR, ABCD, and HICH-BA benchmark models, as well as on real-world networks. Our results demonstrate that the fairness-performance trade-off varies widely across methods, with no single class of approaches consistently excelling in both aspects. We observe that Infomap and Significance methods are high-performing and fair with respect to different types of communities across most networks. The proposed metrics and findings provide valuable insights for designing fair and effective community detection algorithms.","authors":["Elze de Vink","Frank W. Takes","Akrati Saxena"],"url":"https://arxiv.org/abs/2504.11059"}
{"created":"2025-04-16","title":"UKDM: Underwater keypoint detection and matching using underwater image enhancement techniques","abstract":"The purpose of this paper is to explore the use of underwater image enhancement techniques to improve keypoint detection and matching. By applying advanced deep learning models, including generative adversarial networks and convolutional neural networks, we aim to find the best method which improves the accuracy of keypoint detection and the robustness of matching algorithms. We evaluate the performance of these techniques on various underwater datasets, demonstrating significant improvements over traditional methods.","authors":["Pedro Diaz-Garcia","Felix Escalona","Miguel Cazorla"],"url":"https://arxiv.org/abs/2504.11063"}
{"created":"2025-04-16","title":"A Multi-UAV Formation Obstacle Avoidance Method Combined Improved Simulated Annealing and Adaptive Artificial Potential Field","abstract":"The traditional Artificial Potential Field (APF) method exhibits limitations in its force distribution: excessive attraction when UAVs are far from the target may cause collisions with obstacles, while insufficient attraction near the goal often results in failure to reach the target. Furthermore, APF is highly susceptible to local minima, compromising motion reliability in complex environments. To address these challenges, this paper presents a novel hybrid obstacle avoidance algorithm-Deflected Simulated Annealing-Adaptive Artificial Potential Field (DSA-AAPF)-which combines an improved simulated annealing mechanism with an enhanced APF model. The proposed approach integrates a Leader-Follower distributed formation strategy with the APF framework, where the resultant force formulation is redefined to smooth UAV trajectories. An adaptive gravitational gain function is introduced to dynamically adjust UAV velocity based on environmental context, and a fast-converging controller ensures accurate and efficient convergence to the target. Moreover, a directional deflection mechanism is embedded within the simulated annealing process, enabling UAVs to escape local minima caused by semi-enclosed obstacles through continuous rotational motion. The simulation results, covering formation reconfiguration, complex obstacle avoidance, and entrapment escape, demonstrate the feasibility, robustness, and superiority of the proposed DSA-AAPF algorithm.","authors":["Bo Ma","Yi Ji","Liyong Fang"],"url":"https://arxiv.org/abs/2504.11064"}
{"created":"2025-04-16","title":"Improving fingerprint presentation attack detection by an approach integrated into the personal verification stage","abstract":"Presentation Attack Detection (PAD) systems are usually designed independently of the fingerprint verification system. While this can be acceptable for use cases where specific user templates are not predetermined, it represents a missed opportunity to enhance security in scenarios where integrating PAD with the fingerprint verification system could significantly leverage users' templates, which are the real target of a potential presentation attack. This does not mean that a PAD should be specifically designed for such users; that would imply the availability of many enrolled users' PAI and, consequently, complexity, time, and cost increase. On the contrary, we propose to equip a basic PAD, designed according to the state of the art, with an innovative add-on module called the Closeness Binary Code (CC) module. The term \"closeness\" refers to a peculiar property of the bona fide-related features: in an Euclidean feature space, genuine fingerprints tend to cluster in a specific pattern. First, samples from the same finger are close to each other, then samples from other fingers of the same user and finally, samples from fingers of other users. This property is statistically verified in our previous publication, and further confirmed in this paper. It is independent of the user population and the feature set class, which can be handcrafted or deep network-based (embeddings). Therefore, the add-on can be designed without the need for the targeted user samples; moreover, it exploits her/his samples' \"closeness\" property during the verification stage. Extensive experiments on benchmark datasets and state-of-the-art PAD methods confirm the benefits of the proposed add-on, which can be easily coupled with the main PAD module integrated into the fingerprint verification system.","authors":["Marco Micheletto","Giulia Orr\\`u","Luca Ghiani","Gian Luca Marcialis"],"url":"https://arxiv.org/abs/2504.11066"}
{"created":"2025-04-16","title":"Morphing-based Compression for Data-centric ML Pipelines","abstract":"Data-centric ML pipelines extend traditional machine learning (ML) pipelines -- of feature transformations and ML model training -- by outer loops for data cleaning, augmentation, and feature engineering to create high-quality input data. Existing lossless matrix compression applies lightweight compression schemes to numeric matrices and performs linear algebra operations such as matrix-vector multiplications directly on the compressed representation but struggles to efficiently rediscover structural data redundancy. Compressed operations are effective at fitting data in available memory, reducing I/O across the storage-memory-cache hierarchy, and improving instruction parallelism. The applied data cleaning, augmentation, and feature transformations provide a rich source of information about data characteristics such as distinct items, column sparsity, and column correlations. In this paper, we introduce BWARE -- an extension of AWARE for workload-aware lossless matrix compression -- that pushes compression through feature transformations and engineering to leverage information about structural transformations. Besides compressed feature transformations, we introduce a novel technique for lightweight morphing of a compressed representation into workload-optimized compressed representations without decompression. BWARE shows substantial end-to-end runtime improvements, reducing the execution time for training data-centric ML pipelines from days to hours.","authors":["Sebastian Baunsgaard","Matthias Boehm"],"url":"https://arxiv.org/abs/2504.11067"}
{"created":"2025-04-16","title":"Uma extens\\~ao de Raft com propaga\\c{c}\\~ao epid\\'emica","abstract":"The Raft agreement algorithm is recognized for its ease of understanding and practical implementation, and is currently adopted in systems such as Kubernetes. However, it has some limitations in terms of scalability and performance as it concentrates effort on the leader. In this paper we present a new algorithm that expands Raft by incorporating epidemic propagation mechanisms to decentralize the replication effort. Our proposal is evaluated experimentally with a Go implementation and tested with a significant number of processes. -- --","authors":["Andr\\'e Gon\\c{c}alves","Ana Nunes Alonso","Jos\\'e Pereira","Rui Oliveira"],"url":"https://arxiv.org/abs/2504.11068"}
{"created":"2025-04-16","title":"FreeDOM: Online Dynamic Object Removal Framework for Static Map Construction Based on Conservative Free Space Estimation","abstract":"Online map construction is essential for autonomous robots to navigate in unknown environments. However, the presence of dynamic objects may introduce artifacts into the map, which can significantly degrade the performance of localization and path planning. To tackle this problem, a novel online dynamic object removal framework for static map construction based on conservative free space estimation (FreeDOM) is proposed, consisting of a scan-removal front-end and a map-refinement back-end. First, we propose a multi-resolution map structure for fast computation and effective map representation. In the scan-removal front-end, we employ raycast enhancement to improve free space estimation and segment the LiDAR scan based on the estimated free space. In the map-refinement back-end, we further eliminate residual dynamic objects in the map by leveraging incremental free space information. As experimentally verified on SemanticKITTI, HeLiMOS, and indoor datasets with various sensors, our proposed framework overcomes the limitations of visibility-based methods and outperforms state-of-the-art methods with an average F1-score improvement of 9.7%.","authors":["Chen Li","Wanlei Li","Wenhao Liu","Yixiang Shu","Yunjiang Lou"],"url":"https://arxiv.org/abs/2504.11073"}
{"created":"2025-04-16","title":"Dynamical errors in machine learning forecasts","abstract":"In machine learning forecasting, standard error metrics such as mean absolute error (MAE) and mean squared error (MSE) quantify discrepancies between predictions and target values. However, these metrics do not directly evaluate the physical and/or dynamical consistency of forecasts, an increasingly critical concern in scientific and engineering applications.","authors":["Zhou Fang","Gianmarco Mengaldo"],"url":"https://arxiv.org/abs/2504.11074"}
{"created":"2025-04-16","title":"Emergence of Goal-Directed Behaviors via Active Inference with Self-Prior","abstract":"Infants often exhibit goal-directed behaviors, such as reaching for a sensory stimulus, even when no external reward criterion is provided. These intrinsically motivated behaviors facilitate spontaneous exploration and learning of the body and environment during early developmental stages. Although computational modeling can offer insight into the mechanisms underlying such behaviors, many existing studies on intrinsic motivation focus primarily on how exploration contributes to acquiring external rewards. In this paper, we propose a novel density model for an agent's own multimodal sensory experiences, called the \"self-prior,\" and investigate whether it can autonomously induce goal-directed behavior. Integrated within an active inference framework based on the free energy principle, the self-prior generates behavioral references purely from an intrinsic process that minimizes mismatches between average past sensory experiences and current observations. This mechanism is also analogous to the acquisition and utilization of a body schema through continuous interaction with the environment. We examine this approach in a simulated environment and confirm that the agent spontaneously reaches toward a tactile stimulus. Our study implements intrinsically motivated behavior shaped by the agent's own sensory experiences, demonstrating the spontaneous emergence of intentional behavior during early development.","authors":["Dongmin Kim","Hoshinori Kanazawa","Naoto Yoshida","Yasuo Kuniyoshi"],"url":"https://arxiv.org/abs/2504.11075"}
{"created":"2025-04-16","title":"Scalability and Maintainability Challenges and Solutions in Machine Learning: Systematic Literature Review","abstract":"This systematic literature review examines the critical challenges and solutions related to scalability and maintainability in Machine Learning (ML) systems. As ML applications become increasingly complex and widespread across industries, the need to balance system scalability with long-term maintainability has emerged as a significant concern. This review synthesizes current research and practices addressing these dual challenges across the entire ML life-cycle, from data engineering to model deployment in production. We analyzed 124 papers to identify and categorize 41 maintainability challenges and 13 scalability challenges, along with their corresponding solutions. Our findings reveal intricate inter dependencies between scalability and maintainability, where improvements in one often impact the other.","authors":["Karthik Shivashankar","Ghadi S. Al Hajj","Antonio Martini"],"url":"https://arxiv.org/abs/2504.11079"}
{"created":"2025-04-16","title":"Change State Space Models for Remote Sensing Change Detection","abstract":"Despite their frequent use for change detection, both ConvNets and Vision transformers (ViT) exhibit well-known limitations, namely the former struggle to model long-range dependencies while the latter are computationally inefficient, rendering them challenging to train on large-scale datasets. Vision Mamba, an architecture based on State Space Models has emerged as an alternative addressing the aforementioned deficiencies and has been already applied to remote sensing change detection, though mostly as a feature extracting backbone. In this article the Change State Space Model is introduced, that has been specifically designed for change detection by focusing on the relevant changes between bi-temporal images, effectively filtering out irrelevant information. By concentrating solely on the changed features, the number of network parameters is reduced, enhancing significantly computational efficiency while maintaining high detection performance and robustness against input degradation. The proposed model has been evaluated via three benchmark datasets, where it outperformed ConvNets, ViTs, and Mamba-based counterparts at a fraction of their computational complexity. The implementation will be made available at https://github.com/Elman295/CSSM upon acceptance.","authors":["Elman Ghazaei","Erchan Aptoula"],"url":"https://arxiv.org/abs/2504.11080"}
{"created":"2025-04-16","title":"DPS: Design Pattern Summarisation Using Code Features","abstract":"Automatic summarisation has been used efficiently in recent years to condense texts, conversations, audio, code, and various other artefacts. A range of methods, from simple template-based summaries to complex machine learning techniques -- and more recently, large language models -- have been employed to generate these summaries. Summarising software design patterns is important because it helps developers quickly understand and reuse complex design concepts, thereby improving software maintainability and development efficiency. However, the generation of summaries for software design patterns has not yet been explored. Our approach utilises code features and JavaParser to parse the code and create a JSON representation. Using an NLG library on this JSON representation, we convert it into natural language text that acts as a summary of the code, capturing the contextual information of the design pattern. Our empirical results indicate that the summaries generated by our approach capture the context in which patterns are applied in the codebase. Statistical evaluations demonstrate that our summaries closely align with human-written summaries, as evident from high values in the ROUGE-L, BLEU-4, NIST, and FrugalScore metrics. A follow-up survey further shows that DPS summaries were rated as capturing context better than human-generated summaries.","authors":["Najam Nazar","Sameer Sikka","Christoph Treude"],"url":"https://arxiv.org/abs/2504.11081"}
{"created":"2025-04-16","title":"DeepMLF: Multimodal language model with learnable tokens for deep fusion in sentiment analysis","abstract":"While multimodal fusion has been extensively studied in Multimodal Sentiment Analysis (MSA), the role of fusion depth and multimodal capacity allocation remains underexplored. In this work, we position fusion depth, scalability, and dedicated multimodal capacity as primary factors for effective fusion. We introduce DeepMLF, a novel multimodal language model (LM) with learnable tokens tailored toward deep fusion. DeepMLF leverages an audiovisual encoder and a pretrained decoder LM augmented with multimodal information across its layers. We append learnable tokens to the LM that: 1) capture modality interactions in a controlled fashion and 2) preserve independent information flow for each modality. These fusion tokens gather linguistic information via causal self-attention in LM Blocks and integrate with audiovisual information through cross-attention MM Blocks. Serving as dedicated multimodal capacity, this design enables progressive fusion across multiple layers, providing depth in the fusion process. Our training recipe combines modality-specific losses and language modelling loss, with the decoder LM tasked to predict ground truth polarity. Across three MSA benchmarks with varying dataset characteristics, DeepMLF achieves state-of-the-art performance. Our results confirm that deeper fusion leads to better performance, with optimal fusion depths (5-7) exceeding those of existing approaches. Additionally, our analysis on the number of fusion tokens reveals that small token sets ($\\sim$20) achieve optimal performance. We examine the importance of representation learning order (fusion curriculum) through audiovisual encoder initialization experiments. Our ablation studies demonstrate the superiority of the proposed fusion design and gating while providing a holistic examination of DeepMLF's scalability to LLMs, and the impact of each training objective and embedding regularization.","authors":["Efthymios Georgiou","Vassilis Katsouros","Yannis Avrithis","Alexandros Potamianos"],"url":"https://arxiv.org/abs/2504.11082"}
{"created":"2025-04-16","title":"TD-Suite: All Batteries Included Framework for Technical Debt Classification","abstract":"Recognizing that technical debt is a persistent and significant challenge requiring sophisticated management tools, TD-Suite offers a comprehensive software framework specifically engineered to automate the complex task of its classification within software projects. It leverages the advanced natural language understanding of state-of-the-art transformer models to analyze textual artifacts, such as developer discussions in issue reports, where subtle indicators of debt often lie hidden.","authors":["Karthik Shivashankar","Antonio Martini"],"url":"https://arxiv.org/abs/2504.11085"}
{"created":"2025-04-16","title":"FLSSM: A Federated Learning Storage Security Model with Homomorphic Encryption","abstract":"Federated learning based on homomorphic encryption has received widespread attention due to its high security and enhanced protection of user data privacy. However, the characteristics of encrypted computation lead to three challenging problems: ``computation-efficiency\", ``attack-tracing\" and ``contribution-assessment\". The first refers to the efficiency of encrypted computation during model aggregation, the second refers to tracing malicious attacks in an encrypted state, and the third refers to the fairness of contribution assessment for local models after encryption. This paper proposes a federated learning storage security model with homomorphic encryption (FLSSM) to protect federated learning model privacy and address the three issues mentioned above. First, we utilize different nodes to aggregate local models in parallel, thereby improving encrypted models' aggregation efficiency. Second, we introduce trusted supervise nodes to examine local models when the global model is attacked, enabling the tracing of malicious attacks under homomorphic encryption. Finally, we fairly reward local training nodes with encrypted local models based on trusted training time. Experiments on multiple real-world datasets show that our model significantly outperforms baseline models in terms of both efficiency and security metrics.","authors":["Yang Li","Chunhe Xia","Chang Li","Xiaojian Li","Tianbo Wang"],"url":"https://arxiv.org/abs/2504.11088"}
{"created":"2025-04-16","title":"InfoClus: Informative Clustering of High-dimensional Data Embeddings","abstract":"Developing an understanding of high-dimensional data can be facilitated by visualizing that data using dimensionality reduction. However, the low-dimensional embeddings are often difficult to interpret. To facilitate the exploration and interpretation of low-dimensional embeddings, we introduce a new concept named partitioning with explanations. The idea is to partition the data shown through the embedding into groups, each of which is given a sparse explanation using the original high-dimensional attributes. We introduce an objective function that quantifies how much we can learn through observing the explanations of the data partitioning, using information theory, and also how complex the explanations are. Through parameterization of the complexity, we can tune the solutions towards the desired granularity. We propose InfoClus, which optimizes the partitioning and explanations jointly, through greedy search constrained over a hierarchical clustering. We conduct a qualitative and quantitative analysis of InfoClus on three data sets. We contrast the results on the Cytometry data with published manual analysis results, and compare with two other recent methods for explaining embeddings (RVX and VERA). These comparisons highlight that InfoClus has distinct advantages over existing procedures and methods. We find that InfoClus can automatically create good starting points for the analysis of dimensionality-reduction-based scatter plots.","authors":["Fuyin Lai","Edith Heiter","Guillaume Bied","Jefrey Lijffijt"],"url":"https://arxiv.org/abs/2504.11089"}
{"created":"2025-04-16","title":"Towards global equity in political polarization research","abstract":"With a folk understanding that political polarization refers to socio-political divisions within a society, many have proclaimed that we are more divided than ever. In this account, polarization has been blamed for populism, the erosion of social cohesion, the loss of trust in the institutions of democracy, legislative dysfunction, and the collective failure to address existential risks such as Covid-19 or climate change. However, at a global scale there is surprisingly little academic literature which conclusively supports these claims, with half of all studies being U.S.-focused. Here, we provide an overview of the global state of research on polarization, highlighting insights that are robust across countries, those unique to specific contexts, and key gaps in the literature. We argue that addressing these gaps is urgent, but has been hindered thus far by systemic and cultural barriers, such as regionally stratified restrictions on data access and misaligned research incentives. If continued cross-disciplinary inertia means that these disparities are left unaddressed, we see a substantial risk that countries will adopt policies to tackle polarization based on inappropriate evidence, risking flawed decision-making and the weakening of democratic institutions.","authors":["Max Falkenberg","Matteo Cinelli","Alessandro Galeazzi","Christopher A. Bail","Rosa M Benito","Axel Bruns","Anatoliy Gruzd","David Lazer","Jae K Lee","Jennifer McCoy","Kikuko Nagayoshi","David G Rand","Antonio Scala","Alexandra Siegel","Sander van der Linden","Onur Varol","Ingmar Weber","Magdalena Wojcieszak","Fabiana Zollo","Andrea Baronchelli","Walter Quattrociocchi"],"url":"https://arxiv.org/abs/2504.11090"}
{"created":"2025-04-16","title":"Vivid4D: Improving 4D Reconstruction from Monocular Video by Video Inpainting","abstract":"Reconstructing 4D dynamic scenes from casually captured monocular videos is valuable but highly challenging, as each timestamp is observed from a single viewpoint. We introduce Vivid4D, a novel approach that enhances 4D monocular video synthesis by augmenting observation views - synthesizing multi-view videos from a monocular input. Unlike existing methods that either solely leverage geometric priors for supervision or use generative priors while overlooking geometry, we integrate both. This reformulates view augmentation as a video inpainting task, where observed views are warped into new viewpoints based on monocular depth priors. To achieve this, we train a video inpainting model on unposed web videos with synthetically generated masks that mimic warping occlusions, ensuring spatially and temporally consistent completion of missing regions. To further mitigate inaccuracies in monocular depth priors, we introduce an iterative view augmentation strategy and a robust reconstruction loss. Experiments demonstrate that our method effectively improves monocular 4D scene reconstruction and completion.","authors":["Jiaxin Huang","Sheng Miao","BangBnag Yang","Yuewen Ma","Yiyi Liao"],"url":"https://arxiv.org/abs/2504.11092"}
{"created":"2025-04-16","title":"Evaluation Report on MCP Servers","abstract":"With the rise of LLMs, a large number of Model Context Protocol (MCP) services have emerged since the end of 2024. However, the effectiveness and efficiency of MCP servers have not been well studied. To study these questions, we propose an evaluation framework, called MCPBench. We selected several widely used MCP server and conducted an experimental evaluation on their accuracy, time, and token usage. Our experiments showed that the most effective MCP, Bing Web Search, achieved an accuracy of 64%. Importantly, we found that the accuracy of MCP servers can be substantially enhanced by involving declarative interface. This research paves the way for further investigations into optimized MCP implementations, ultimately leading to better AI-driven applications and data retrieval solutions.","authors":["Zhiling Luo","Xiaorong Shi","Xuanrui Lin","Jinyang Gao"],"url":"https://arxiv.org/abs/2504.11094"}
{"created":"2025-04-16","title":"A fully variational numerical method for structural topology optimization based on a Cahn-Hilliard model","abstract":"We formulate a novel numerical method suitable for the solution of topology optimization problems in solid mechanics. The most salient feature of the new approach is that the space and time discrete equations of the numerical method can be obtained as the optimality conditions of a single incremental potential. The governing equations define a gradient flow of the mass in the domain that maximizes the stiffness of the proposed solid, while exactly preserving the mass of the allocated material. Moreover, we propose a change of variables in the model equations that constrains the value of the density within admissible bounds and a continuation strategy that speeds up the evolution of the flow. The proposed strategy results in a robust and efficient topology optimization method that is exactly mass-preserving, does not employ Lagrange multipliers, and is fully variational.","authors":["Edmund Bell-Navas","David Portillo","Ignacio Romero"],"url":"https://arxiv.org/abs/2504.11096"}
{"created":"2025-04-16","title":"Steering Feedback in Dynamic Driving Simulators: Road-Induced and Non-Road-Induced Harshness","abstract":"Steering feedback plays a substantial role in the validity of driving simulators for the virtual development of modern vehicles. Established objective steering characteristics typically assess the feedback behavior in the frequency range of up to 30 Hz while factors such as steering wheel and vehicle body vibrations at higher frequencies are mainly approached as comfort issues. This work investigates the influence of steering wheel and vehicle body excitations in the frequency range between 30 and 100 Hz on the subjective evaluation of steering feedback in a dynamic driving simulator. A controlled subject study with 42 participants was performed to compare a reference vehicle with an electrical power steering system to four variants of its virtual representation on a dynamic driving simulator. The effects of road-induced excitations were investigated by comparing a semi-empirical and a physics-based tire model, while the influence of non-road-induced excitations was investigated by implementing engine and wheel orders. The simulator variants were evaluated in comparison to the reference vehicle during closed-loop driving on a country road in a single-blind within-subjects design. The subjective evaluation focused on the perception of road feedback compared to the reference vehicle. The statistical analysis of subjective results shows that there is a strong effect of non-road-induced steering and vehicle body excitations, while the effect of road-induced excitations is considerably less pronounced.","authors":["Maximilian B\\\"ohle","Bernhard Schick","Steffen M\\\"uller"],"url":"https://arxiv.org/abs/2504.11097"}
{"created":"2025-04-16","title":"Consensus Entropy: Harnessing Multi-VLM Agreement for Self-Verifying and Self-Improving OCR","abstract":"The Optical Character Recognition (OCR) task is important for evaluating Vision-Language Models (VLMs) and providing high-quality data sources for LLM training data. While state-of-the-art VLMs show improved average OCR accuracy, they still struggle with sample-level quality degradation and lack reliable automatic detection of low-quality outputs. We introduce Consensus Entropy (CE), a training-free post-inference method that quantifies OCR uncertainty by aggregating outputs from multiple VLMs. Our approach exploits a key insight: correct VLM OCR predictions converge in output space while errors diverge. We develop a lightweight multi-model framework that effectively identifies problematic samples, selects the best outputs and combines model strengths. Experiments across multiple OCR benchmarks and VLMs demonstrate that CE outperforms VLM-as-judge approaches and single-model baselines at the same cost and achieves state-of-the-art results across multiple metrics. For instance, our solution demonstrates: achieving 15.2\\% higher F1 scores than VLM-as-judge methods in quality verification, delivering 6.0\\% accuracy gains on mathematical calculation tasks, and requiring rephrasing only 7.3\\% of inputs while maintaining overall performance. Notably, the entire process requires neither training nor supervision while maintaining plug-and-play functionality throughout.","authors":["Yulong Zhang","Tianyi Liang","Xinyue Huang","Erfei Cui","Xu Guo","Pei Chu","Chenhui Li","Ru Zhang","Wenhai Wang","Gongshen Liu"],"url":"https://arxiv.org/abs/2504.11101"}
{"created":"2025-04-16","title":"Using LLMs as prompt modifier to avoid biases in AI image generators","abstract":"This study examines how Large Language Models (LLMs) can reduce biases in text-to-image generation systems by modifying user prompts. We define bias as a model's unfair deviation from population statistics given neutral prompts. Our experiments with Stable Diffusion XL, 3.5 and Flux demonstrate that LLM-modified prompts significantly increase image diversity and reduce bias without the need to change the image generators themselves. While occasionally producing results that diverge from original user intent for elaborate prompts, this approach generally provides more varied interpretations of underspecified requests rather than superficial variations. The method works particularly well for less advanced image generators, though limitations persist for certain contexts like disability representation. All prompts and generated images are available at https://iisys-hof.github.io/llm-prompt-img-gen/","authors":["Ren\\'e Peinl"],"url":"https://arxiv.org/abs/2504.11104"}
{"created":"2025-04-16","title":"Token-Level Constraint Boundary Search for Jailbreaking Text-to-Image Models","abstract":"Recent advancements in Text-to-Image (T2I) generation have significantly enhanced the realism and creativity of generated images. However, such powerful generative capabilities pose risks related to the production of inappropriate or harmful content. Existing defense mechanisms, including prompt checkers and post-hoc image checkers, are vulnerable to sophisticated adversarial attacks. In this work, we propose TCBS-Attack, a novel query-based black-box jailbreak attack that searches for tokens located near the decision boundaries defined by text and image checkers. By iteratively optimizing tokens near these boundaries, TCBS-Attack generates semantically coherent adversarial prompts capable of bypassing multiple defensive layers in T2I models. Extensive experiments demonstrate that our method consistently outperforms state-of-the-art jailbreak attacks across various T2I models, including securely trained open-source models and commercial online services like DALL-E 3. TCBS-Attack achieves an ASR-4 of 45\\% and an ASR-1 of 21\\% on jailbreaking full-chain T2I models, significantly surpassing baseline methods.","authors":["Jiangtao Liu","Zhaoxin Wang","Handing Wang","Cong Tian","Yaochu Jin"],"url":"https://arxiv.org/abs/2504.11106"}
{"created":"2025-04-16","title":"Benchmarking Vision Language Models on German Factual Data","abstract":"Similar to LLMs, the development of vision language models is mainly driven by English datasets and models trained in English and Chinese language, whereas support for other languages, even those considered high-resource languages such as German, remains significantly weaker. In this work we present an analysis of open-weight VLMs on factual knowledge in the German and English language. We disentangle the image-related aspects from the textual ones by analyzing accu-racy with jury-as-a-judge in both prompt languages and images from German and international contexts. We found that for celebrities and sights, VLMs struggle because they are lacking visual cognition of German image contents. For animals and plants, the tested models can often correctly identify the image contents ac-cording to the scientific name or English common name but fail in German lan-guage. Cars and supermarket products were identified equally well in English and German images across both prompt languages.","authors":["Ren\\'e Peinl","Vincent Tischler"],"url":"https://arxiv.org/abs/2504.11108"}
{"created":"2025-04-16","title":"Helper-Friendly Latency-Bounded Mitigation Strategies against Reactive Jamming Adversaries","abstract":"Due to the recent developments in the field of full-duplex radios and cognitive radios, a new class of reactive jamming attacks has gained attention wherein an adversary transmits jamming energy over the victim's frequency band and also monitors various energy statistics in the network so as to detect countermeasures, thereby trapping the victim. Although cooperative mitigation strategies against such security threats exist, they are known to incur spectral-efficiency loss on the helper node, and are also not robust to variable latency-constraints on victim's messages. Identifying these research gaps in existing countermeasures against reactive jamming attacks, we propose a family of helper-friendly cooperative mitigation strategies that are applicable for a wide-range of latency-requirements on the victim's messages as well as practical radio hardware at the helper nodes. The proposed strategies are designed to facilitate reliable communication for the victim, without compromising the helper's spectral efficiency and also minimally disturbing the various energy statistics in the network. For theoretical guarantees on their efficacy, interesting optimization problems are formulated on the choice of the underlying parameters, followed by extensive mathematical analyses on their error-performance and covertness. Experimental results indicate that the proposed strategies should be preferred over the state-of-the-art methods when the helper node is unwilling to compromise on its error performance for assisting the victim.","authors":["Soumita Hazra","J. Harshan"],"url":"https://arxiv.org/abs/2504.11110"}
{"created":"2025-04-16","title":"S$^2$Teacher: Step-by-step Teacher for Sparsely Annotated Oriented Object Detection","abstract":"Although fully-supervised oriented object detection has made significant progress in multimodal remote sensing image understanding, it comes at the cost of labor-intensive annotation. Recent studies have explored weakly and semi-supervised learning to alleviate this burden. However, these methods overlook the difficulties posed by dense annotations in complex remote sensing scenes. In this paper, we introduce a novel setting called sparsely annotated oriented object detection (SAOOD), which only labels partial instances, and propose a solution to address its challenges. Specifically, we focus on two key issues in the setting: (1) sparse labeling leading to overfitting on limited foreground representations, and (2) unlabeled objects (false negatives) confusing feature learning. To this end, we propose the S$^2$Teacher, a novel method that progressively mines pseudo-labels for unlabeled objects, from easy to hard, to enhance foreground representations. Additionally, it reweights the loss of unlabeled objects to mitigate their impact during training. Extensive experiments demonstrate that S$^2$Teacher not only significantly improves detector performance across different sparse annotation levels but also achieves near-fully-supervised performance on the DOTA dataset with only 10% annotation instances, effectively balancing detection accuracy with annotation efficiency. The code will be public.","authors":["Yu Lin","Jianghang Lin","Kai Ye","You Shen","Yan Zhang","Shengchuan Zhang","Liujuan Cao","Rongrong Ji"],"url":"https://arxiv.org/abs/2504.11111"}
{"created":"2025-04-16","title":"Flyweight FLIM Networks for Salient Object Detection in Biomedical Images","abstract":"Salient Object Detection (SOD) with deep learning often requires substantial computational resources and large annotated datasets, making it impractical for resource-constrained applications. Lightweight models address computational demands but typically strive in complex and scarce labeled-data scenarios. Feature Learning from Image Markers (FLIM) learns an encoder's convolutional kernels among image patches extracted from discriminative regions marked on a few representative images, dismissing large annotated datasets, pretraining, and backpropagation. Such a methodology exploits information redundancy commonly found in biomedical image applications. This study presents methods to learn dilated-separable convolutional kernels and multi-dilation layers without backpropagation for FLIM networks. It also proposes a novel network simplification method to reduce kernel redundancy and encoder size. By combining a FLIM encoder with an adaptive decoder, a concept recently introduced to estimate a pointwise convolution per image, this study presents very efficient (named flyweight) SOD models for biomedical images. Experimental results in challenging datasets demonstrate superior efficiency and effectiveness to lightweight models. By requiring significantly fewer parameters and floating-point operations, the results show competitive effectiveness to heavyweight models. These advances highlight the potential of FLIM networks for data-limited and resource-constrained applications with information redundancy.","authors":["Leonardo M. Joao","Jancarlo F. Gomes","Silvio J. F. Guimaraes","Ewa Kijak","Alexandre X. Falcao"],"url":"https://arxiv.org/abs/2504.11112"}
{"created":"2025-04-16","title":"Revealing Covert Attention by Analyzing Human and Reinforcement Learning Agent Gameplay","abstract":"This study introduces a novel method for revealing human covert attention patterns using gameplay data alone, utilizing offline attention techniques from reinforcement learning (RL). We propose the contextualized, task-relevant (CTR) attention network, which generates attention maps from both human and RL agent gameplay in Atari environments. These maps are sparse yet retain the necessary information for the current player's decision making. We compare the CTR-derived attention maps with a temporally integrated overt attention (TIOA) model based on eye-tracking data, serving as a point of comparison and discussion. Visual inspection reveals distinct attention patterns: human CTR maps focus on the player and rather nearby opponents, occasionally shifting between stronger focus and broader views - sometimes even attending to empty space ahead. In contrast, agent maps maintain a consistent broad focus on most objects, including distant ones and the player. Quantitative analysis further demonstrates that human CTR maps align more closely with TIOA than agent maps do. Our findings indicate that the CTR attention network can effectively reveal human covert attention patterns from gameplay alone, without the need for additional data like brain activity recordings. This work contributes to understanding human-agent attention differences and enables the development of RL agents augmented with human covert attention.","authors":["Henrik Krauss","Takehisa Yairi"],"url":"https://arxiv.org/abs/2504.11118"}
{"created":"2025-04-16","title":"A Unified Hardware Accelerator for Fast Fourier Transform and Number Theoretic Transform","abstract":"The Number Theoretic Transform (NTT) is an indispensable tool for computing efficient polynomial multiplications in post-quantum lattice-based cryptography. It has strong resemblance with the Fast Fourier Transform (FFT), which is the most widely used algorithm in digital signal processing. In this work, we demonstrate a unified hardware accelerator supporting both 512-point complex FFT as well as 256-point NTT for the recently standardized NIST post-quantum key encapsulation and digital signature algorithms ML-KEM and ML-DSA respectively. Our proposed architecture effectively utilizes the arithmetic circuitry required for complex FFT, and the only additional circuits required are for modular reduction along with modifications in the control logic. Our implementation achieves performance comparable to state-of-the-art ML-KEM / ML-DSA NTT accelerators on FPGA, thus demonstrating how an FFT accelerator can be augmented to support NTT and the unified hardware can be used for both digital signal processing and post-quantum lattice-based cryptography applications.","authors":["Rishabh Shrivastava","Chaitanya Prasad Ratnala","Durga Manasa Puli","Utsav Banerjee"],"url":"https://arxiv.org/abs/2504.11124"}
{"created":"2025-04-16","title":"A mixed-integer framework for analyzing neural network-based controllers for piecewise affine systems with bounded disturbances","abstract":"We present a method for representing the closed-loop dynamics of piecewise affine (PWA) systems with bounded additive disturbances and neural network-based controllers as mixed-integer (MI) linear constraints. We show that such representations enable the computation of robustly positively invariant (RPI) sets for the specified system class by solving MI linear programs. These RPI sets can subsequently be used to certify stability and constraint satisfaction. Furthermore, the approach allows to handle non-linear systems based on suitable PWA approximations and corresponding error bounds, which can be interpreted as the bounded disturbances from above.","authors":["Dieter Teichrib","Moritz Schulze Darup"],"url":"https://arxiv.org/abs/2504.11125"}
{"created":"2025-04-16","title":"KubeFence: Security Hardening of the Kubernetes Attack Surface","abstract":"Kubernetes (K8s) is widely used to orchestrate containerized applications, including critical services in domains such as finance, healthcare, and government. However, its extensive and feature-rich API interface exposes a broad attack surface, making K8s vulnerable to exploits of software vulnerabilities and misconfigurations. Even if K8s adopts role-based access control (RBAC) to manage access to K8s APIs, this approach lacks the granularity needed to protect specification attributes within API requests. This paper proposes a novel solution, KubeFence, which implements finer-grain API filtering tailored to specific client workloads. KubeFence analyzes Kubernetes Operators from trusted repositories and leverages their configuration files to restrict unnecessary features of the K8s API, to mitigate misconfigurations and vulnerabilities exploitable through the K8s API. The experimental results show that KubeFence can significantly reduce the attack surface and prevent attacks compared to RBAC.","authors":["Carmine Cesarano","Roberto Natella"],"url":"https://arxiv.org/abs/2504.11126"}
{"created":"2025-04-16","title":"K-means Enhanced Density Gradient Analysis for Urban and Transport Metrics Using Multi-Modal Satellite Imagery","abstract":"This paper presents a novel computational approach for evaluating urban metrics through density gradient analysis using multi-modal satellite imagery, with applications including public transport and other urban systems. By combining optical and Synthetic Aperture Radar (SAR) data, we develop a method to segment urban areas, identify urban centers, and quantify density gradients. Our approach calculates two key metrics: the density gradient coefficient ($\\alpha$) and the minimum effective distance (LD) at which density reaches a target threshold. We further employ machine learning techniques, specifically K-means clustering, to objectively identify uniform and high-variability regions within density gradient plots. We demonstrate that these metrics provide an effective screening tool for public transport analyses by revealing the underlying urban structure. Through comparative analysis of two representative cities with contrasting urban morphologies (monocentric vs polycentric), we establish relationships between density gradient characteristics and public transport network topologies. Cities with clear density peaks in their gradient plots indicate distinct urban centers requiring different transport strategies than those with more uniform density distributions. This methodology offers urban planners a cost-effective, globally applicable approach to preliminary public transport assessment using freely available satellite data. The complete implementation, with additional examples and documentation, is available in an open-source repository under the MIT license at https://github.com/nexri/Satellite-Imagery-Urban-Analysis.","authors":["P. Tomkiewicz","J. Jaworski","P. Zielonka","A. Wilinski"],"url":"https://arxiv.org/abs/2504.11128"}
{"created":"2025-04-16","title":"Divergence of Empirical Neural Tangent Kernel in Classification Problems","abstract":"This paper demonstrates that in classification problems, fully connected neural networks (FCNs) and residual neural networks (ResNets) cannot be approximated by kernel logistic regression based on the Neural Tangent Kernel (NTK) under overtraining (i.e., when training time approaches infinity). Specifically, when using the cross-entropy loss, regardless of how large the network width is (as long as it is finite), the empirical NTK diverges from the NTK on the training samples as training time increases. To establish this result, we first demonstrate the strictly positive definiteness of the NTKs for multi-layer FCNs and ResNets. Then, we prove that during training, % with the cross-entropy loss, the neural network parameters diverge if the smallest eigenvalue of the empirical NTK matrix (Gram matrix) with respect to training samples is bounded below by a positive constant. This behavior contrasts sharply with the lazy training regime commonly observed in regression problems. Consequently, using a proof by contradiction, we show that the empirical NTK does not uniformly converge to the NTK across all times on the training samples as the network width increases. We validate our theoretical results through experiments on both synthetic data and the MNIST classification task. This finding implies that NTK theory is not applicable in this context, with significant theoretical implications for understanding neural networks in classification problems.","authors":["Zixiong Yu","Songtao Tian","Guhan Chen"],"url":"https://arxiv.org/abs/2504.11130"}
{"created":"2025-04-16","title":"Visual Re-Ranking with Non-Visual Side Information","abstract":"The standard approach for visual place recognition is to use global image descriptors to retrieve the most similar database images for a given query image. The results can then be further improved with re-ranking methods that re-order the top scoring images. However, existing methods focus on re-ranking based on the same image descriptors that were used for the initial retrieval, which we argue provides limited additional signal.","authors":["Gustav Hanning","Gabrielle Flood","Viktor Larsson"],"url":"https://arxiv.org/abs/2504.11134"}
{"created":"2025-04-16","title":"BrickSmart: Leveraging Generative AI to Support Children's Spatial Language Learning in Family Block Play","abstract":"Block-building activities are crucial for developing children's spatial reasoning and mathematical skills, yet parents often lack the expertise to guide these activities effectively. BrickSmart, a pioneering system, addresses this gap by providing spatial language guidance through a structured three-step process: Discovery & Design, Build & Learn, and Explore & Expand. This system uniquely supports parents in 1) generating personalized block-building instructions, 2) guiding parents to teach spatial language during building and interactive play, and 3) tracking children's learning progress, altogether enhancing children's engagement and cognitive development. In a comparative study involving 12 parent-child pairs children aged 6-8 years) for both experimental and control groups, BrickSmart demonstrated improvements in supportiveness, efficiency, and innovation, with a significant increase in children's use of spatial vocabularies during block play, thereby offering an effective framework for fostering spatial language skills in children.","authors":["Yujia Liu","Siyu Zha","Yuewen Zhang","Yanjin Wang","Yangming Zhang","Qi Xin","Lunyiu Nie","Chao Zhang","Yingqing Xu"],"url":"https://arxiv.org/abs/2504.11138"}
{"created":"2025-04-16","title":"An Unsupervised Network Architecture Search Method for Solving Partial Differential Equations","abstract":"Solving partial differential equations (PDEs) has been indispensable in scientific and engineering applications. Recently, deep learning methods have been widely used to solve high-dimensional problems, one of which is the physics-informed neural network (PINN). Typically, a deep learning method has three main components: a neural network, a loss function, and an optimizer. While the construction of the loss function is rooted in the definition of solution space, how to choose a optimal neural network is somewhat ad hoc, leaving much room for improvement. In the framework of PINN, we propose an unsupervised network architecture search method for solving PDEs, termed PINN-DARTS, which applies the differentiable architecture search (DARTS) to find the optimal network architecture structure in a given set of neural networks. In this set, the number of layers and the number of neurons in each layer can change. In the searching phase, both network and architecture parameters are updated simultaneously, so the running time is close to that of PINN with a pre-determined network structure. Unlike available works, our approach is unsupervised and purely based on the PDE residual without any prior usage of solutions. PINN-DARTS outputs the optimal network structure as well as the associated numerical solution. The performance of PINN-DARTS is verified on several benchmark PDEs, including elliptic, parabolic, wave, and Burgers' equations. Compared to traditional architecture search methods, PINN-DARTS achieves significantly higher architectural accuracy. Another interesting observation is that both the solution complexity and the PDE type have a prominent impact on the optimal network architecture. Our study suggests that architectures with uneven widths from layer to layer may have superior performance across different solution complexities and different PDE types.","authors":["Qing Li","Jingrun Chen"],"url":"https://arxiv.org/abs/2504.11140"}
{"created":"2025-04-16","title":"Taming Consistency Distillation for Accelerated Human Image Animation","abstract":"Recent advancements in human image animation have been propelled by video diffusion models, yet their reliance on numerous iterative denoising steps results in high inference costs and slow speeds. An intuitive solution involves adopting consistency models, which serve as an effective acceleration paradigm through consistency distillation. However, simply employing this strategy in human image animation often leads to quality decline, including visual blurring, motion degradation, and facial distortion, particularly in dynamic regions. In this paper, we propose the DanceLCM approach complemented by several enhancements to improve visual quality and motion continuity at low-step regime: (1) segmented consistency distillation with an auxiliary light-weight head to incorporate supervision from real video latents, mitigating cumulative errors resulting from single full-trajectory generation; (2) a motion-focused loss to centre on motion regions, and explicit injection of facial fidelity features to improve face authenticity. Extensive qualitative and quantitative experiments demonstrate that DanceLCM achieves results comparable to state-of-the-art video diffusion models with a mere 2-4 inference steps, significantly reducing the inference burden without compromising video quality. The code and models will be made publicly available.","authors":["Xiang Wang","Shiwei Zhang","Hangjie Yuan","Yujie Wei","Yingya Zhang","Changxin Gao","Yuehuan Wang","Nong Sang"],"url":"https://arxiv.org/abs/2504.11143"}
{"created":"2025-04-16","title":"Exploring Student Behaviors and Motivations using AI TAs with Optional Guardrails","abstract":"AI-powered chatbots and digital teaching assistants (AI TAs) are gaining popularity in programming education, offering students timely and personalized feedback. Despite their potential benefits, concerns about student over-reliance and academic misconduct have prompted the introduction of \"guardrails\" into AI TAs - features that provide scaffolded support rather than direct solutions. However, overly restrictive guardrails may lead students to bypass these tools and use unconstrained AI models, where interactions are not observable, thus limiting our understanding of students' help-seeking behaviors. To investigate this, we designed and deployed a novel AI TA tool with optional guardrails in one lab of a large introductory programming course. As students completed three code writing and debugging tasks, they had the option to receive guardrailed help or use a \"See Solution\" feature which disabled the guardrails and generated a verbatim response from the underlying model. We investigate students' motivations and use of this feature and examine the association between usage and their course performance. We found that 50% of the 885 students used the \"See Solution\" feature for at least one problem and 14% used it for all three problems. Additionally, low-performing students were more likely to use this feature and use it close to the deadline as they started assignments later. The predominant factors that motivated students to disable the guardrails were assistance in solving problems, time pressure, lack of self-regulation, and curiosity. Our work provides insights into students' solution-seeking motivations and behaviors, which has implications for the design of AI TAs that balance pedagogical goals with student preferences.","authors":["Amanpreet Kapoor","Marc Diaz","Stephen MacNeil","Leo Porter","Paul Denny"],"url":"https://arxiv.org/abs/2504.11146"}
{"created":"2025-04-16","title":"An Application of Membrane Computing to Humanitarian Relief via Generalized Nash Equilibrium","abstract":"Natural and political disasters, including earthquakes, hurricanes, and tsunamis, but also migration and refugees crisis, need quick and coordinated responses in order to support vulnerable populations. In such disasters, nongovernmental organizations compete with each other for financial donations, while people who need assistance suffer a lack of coordination, congestion in terms of logistics, and duplication of services. From a theoretical point of view, this problem can be formalized as a Generalized Nash Equilibrium (GNE) problem. This is a generalization of the Nash equilibrium problem, where the agents' strategies are not fixed but depend on the other agents' strategies. In this paper, we show that Membrane Computing can model humanitarian relief as a GNE problem. We propose a family of P systems that compute GNE in this context, and we illustrate their capabilities with Hurricane Katrina in 2005 as a case study.","authors":["Alejandro Luque-Cerpa","David Orellana-Mart\\'in","Miguel A. Guti\\'errez-Naranjo"],"url":"https://arxiv.org/abs/2504.11149"}
{"created":"2025-04-16","title":"GC-GAT: Multimodal Vehicular Trajectory Prediction using Graph Goal Conditioning and Cross-context Attention","abstract":"Predicting future trajectories of surrounding vehicles heavily relies on what contextual information is given to a motion prediction model. The context itself can be static (lanes, regulatory elements, etc) or dynamic (traffic participants). This paper presents a lane graph-based motion prediction model that first predicts graph-based goal proposals and later fuses them with cross attention over multiple contextual elements. We follow the famous encoder-interactor-decoder architecture where the encoder encodes scene context using lightweight Gated Recurrent Units, the interactor applies cross-context attention over encoded scene features and graph goal proposals, and the decoder regresses multimodal trajectories via Laplacian Mixture Density Network from the aggregated encodings. Using cross-attention over graph-based goal proposals gives robust trajectory estimates since the model learns to attend to future goal-relevant scene elements for the intended agent. We evaluate our work on nuScenes motion prediction dataset, achieving state-of-the-art results.","authors":["Mahir Gulzar","Yar Muhammad","Naveed Muhammad"],"url":"https://arxiv.org/abs/2504.11150"}
{"created":"2025-04-16","title":"SAR-to-RGB Translation with Latent Diffusion for Earth Observation","abstract":"Earth observation satellites like Sentinel-1 (S1) and Sentinel-2 (S2) provide complementary remote sensing (RS) data, but S2 images are often unavailable due to cloud cover or data gaps. To address this, we propose a diffusion model (DM)-based approach for SAR-to-RGB translation, generating synthetic optical images from SAR inputs. We explore three different setups: two using Standard Diffusion, which reconstruct S2 images by adding and removing noise (one without and one with class conditioning), and one using Cold Diffusion, which blends S2 with S1 before removing the SAR signal. We evaluate the generated images in downstream tasks, including land cover classification and cloud removal. While generated images may not perfectly replicate real S2 data, they still provide valuable information. Our results show that class conditioning improves classification accuracy, while cloud removal performance remains competitive despite our approach not being optimized for it. Interestingly, despite exhibiting lower perceptual quality, the Cold Diffusion setup performs well in land cover classification, suggesting that traditional quantitative evaluation metrics may not fully reflect the practical utility of generated images. Our findings highlight the potential of DMs for SAR-to-RGB translation in RS applications where RGB images are missing.","authors":["Kaan Aydin","Joelle Hanna","Damian Borth"],"url":"https://arxiv.org/abs/2504.11154"}
{"created":"2025-04-16","title":"C-SHAP for time series: An approach to high-level temporal explanations","abstract":"Time series are ubiquitous in domains such as energy forecasting, healthcare, and industry. Using AI systems, some tasks within these domains can be efficiently handled. Explainable AI (XAI) aims to increase the reliability of AI solutions by explaining model reasoning. For time series, many XAI methods provide point- or sequence-based attribution maps. These methods explain model reasoning in terms of low-level patterns. However, they do not capture high-level patterns that may also influence model reasoning. We propose a concept-based method to provide explanations in terms of these high-level patterns. In this paper, we present C-SHAP for time series, an approach which determines the contribution of concepts to a model outcome. We provide a general definition of C-SHAP and present an example implementation using time series decomposition. Additionally, we demonstrate the effectiveness of the methodology through a use case from the energy domain.","authors":["Annemarie Jutte","Faizan Ahmed","Jeroen Linssen","Maurice van Keulen"],"url":"https://arxiv.org/abs/2504.11159"}
{"created":"2025-04-16","title":"DMAGaze: Gaze Estimation Based on Feature Disentanglement and Multi-Scale Attention","abstract":"Gaze estimation, which predicts gaze direction, commonly faces the challenge of interference from complex gaze-irrelevant information in face images. In this work, we propose DMAGaze, a novel gaze estimation framework that exploits information from facial images in three aspects: gaze-relevant global features (disentangled from facial image), local eye features (extracted from cropped eye patch), and head pose estimation features, to improve overall performance. Firstly, we design a new continuous mask-based Disentangler to accurately disentangle gaze-relevant and gaze-irrelevant information in facial images by achieving the dual-branch disentanglement goal through separately reconstructing the eye and non-eye regions. Furthermore, we introduce a new cascaded attention module named Multi-Scale Global Local Attention Module (MS-GLAM). Through a customized cascaded attention structure, it effectively focuses on global and local information at multiple scales, further enhancing the information from the Disentangler. Finally, the global gaze-relevant features disentangled by the upper face branch, combined with head pose and local eye features, are passed through the detection head for high-precision gaze estimation. Our proposed DMAGaze has been extensively validated on two mainstream public datasets, achieving state-of-the-art performance.","authors":["Haohan Chen","Hongjia Liu","Shiyong Lan","Wenwu Wang","Yixin Qiao","Yao Li","Guonan Deng"],"url":"https://arxiv.org/abs/2504.11160"}
{"created":"2025-04-16","title":"The Robotability Score: Enabling Harmonious Robot Navigation on Urban Streets","abstract":"This paper introduces the Robotability Score ($R$), a novel metric that quantifies the suitability of urban environments for autonomous robot navigation. Through expert interviews and surveys, we identify and weigh key features contributing to R for wheeled robots on urban streets. Our findings reveal that pedestrian density, crowd dynamics and pedestrian flow are the most critical factors, collectively accounting for 28% of the total score. Computing robotability across New York City yields significant variation; the area of highest R is 3.0 times more \"robotable\" than the area of lowest R. Deployments of a physical robot on high and low robotability areas show the adequacy of the score in anticipating the ease of robot navigation. This new framework for evaluating urban landscapes aims to reduce uncertainty in robot deployment while respecting established mobility patterns and urban planning principles, contributing to the discourse on harmonious human-robot environments.","authors":["Matt Franchi","Maria Teresa Parreira","Fanjun Bu","Wendy Ju"],"url":"https://arxiv.org/abs/2504.11163"}
{"created":"2025-04-16","title":"TSAL: Few-shot Text Segmentation Based on Attribute Learning","abstract":"Recently supervised learning rapidly develops in scene text segmentation. However, the lack of high-quality datasets and the high cost of pixel annotation greatly limit the development of them. Considering the well-performed few-shot learning methods for downstream tasks, we investigate the application of the few-shot learning method to scene text segmentation. We propose TSAL, which leverages CLIP's prior knowledge to learn text attributes for segmentation. To fully utilize the semantic and texture information in the image, a visual-guided branch is proposed to separately extract text and background features. To reduce data dependency and improve text detection accuracy, the adaptive prompt-guided branch employs effective adaptive prompt templates to capture various text attributes. To enable adaptive prompts capture distinctive text features and complex background distribution, we propose Adaptive Feature Alignment module(AFA). By aligning learnable tokens of different attributes with visual features and prompt prototypes, AFA enables adaptive prompts to capture both general and distinctive attribute information. TSAL can capture the unique attributes of text and achieve precise segmentation using only few images. Experiments demonstrate that our method achieves SOTA performance on multiple text segmentation datasets under few-shot settings and show great potential in text-related domains.","authors":["Chenming Li","Chengxu Liu","Yuanting Fan","Xiao Jin","Xingsong Hou","Xueming Qian"],"url":"https://arxiv.org/abs/2504.11164"}
{"created":"2025-04-16","title":"YOLO-RS: Remote Sensing Enhanced Crop Detection Methods","abstract":"With the rapid development of remote sensing technology, crop classification and health detection based on deep learning have gradually become a research hotspot. However, the existing target detection methods show poor performance when dealing with small targets in remote sensing images, especially in the case of complex background and image mixing, which is difficult to meet the practical application requirementsite. To address this problem, a novel target detection model YOLO-RS is proposed in this paper. The model is based on the latest Yolov11 which significantly enhances the detection of small targets by introducing the Context Anchor Attention (CAA) mechanism and an efficient multi-field multi-scale feature fusion network. YOLO-RS adopts a bidirectional feature fusion strategy in the feature fusion process, which effectively enhances the model's performance in the detection of small targets. Small target detection. Meanwhile, the ACmix module at the end of the model backbone network solves the category imbalance problem by adaptively adjusting the contrast and sample mixing, thus enhancing the detection accuracy in complex scenes. In the experiments on the PDT remote sensing crop health detection dataset and the CWC crop classification dataset, YOLO-RS improves both the recall and the mean average precision (mAP) by about 2-3\\% or so compared with the existing state-of-the-art methods, while the F1-score is also significantly improved. Moreover, the computational complexity of the model only increases by about 5.2 GFLOPs, indicating its significant advantages in both performance and efficiency. The experimental results validate the effectiveness and application potential of YOLO-RS in the task of detecting small targets in remote sensing images.","authors":["Linlin Xiao","Zhang Tiancong","Yutong Jia","Xinyu Nie","Mengyao Wang","Xiaohang Shao"],"url":"https://arxiv.org/abs/2504.11165"}
{"created":"2025-04-16","title":"Low-Rank SPIKE Framework for Solving Large Sparse Linear Systems with Applications","abstract":"The SPIKE family of linear system solvers provides parallelism using a block tridiagonal partitioning. Typically SPIKE-based solvers are applied to banded systems, resulting in structured off-diagonal blocks with non-zeros elements restricted to relatively small submatrices comprising the band of the original matrix. In this work, a low-rank SVD based approximation of the off-diagonal blocks is investigated. This produces a representation which more effectively handles matrices with large, sparse bands. A set of flexible distributed solvers, the LR-SPIKE variants, are implemented. There are applicable to a wide range of applications -- from use as a \"black-box\" preconditioner which straightforwardly improves upon the classic Block Jacobi preconditioner, to use as a specialized \"approximate direct solver.\" An investigation of the effectiveness of the new preconditioners for a selection of SuiteSparse matrices is performed, particularly focusing on matrices derived from 3D finite element simulations. In addition, the SPIKE approximate linear system solvers are also paired with the FEAST eigenvalue solver, where they are shown to be particularly effective due to the former's rapid convergence, and the latter's acceptance of loose linear system solver convergence, resulting in a combination which requires very few solver iterations.","authors":["Braegan S. Spring","Eric Polizzi","Ahmed H. Sameh"],"url":"https://arxiv.org/abs/2504.11167"}
{"created":"2025-04-16","title":"Bypassing Prompt Injection and Jailbreak Detection in LLM Guardrails","abstract":"Large Language Models (LLMs) guardrail systems are designed to protect against prompt injection and jailbreak attacks. However, they remain vulnerable to evasion techniques. We demonstrate two approaches for bypassing LLM prompt injection and jailbreak detection systems via traditional character injection methods and algorithmic Adversarial Machine Learning (AML) evasion techniques. Through testing against six prominent protection systems, including Microsoft's Azure Prompt Shield and Meta's Prompt Guard, we show that both methods can be used to evade detection while maintaining adversarial utility achieving in some instances up to 100% evasion success. Furthermore, we demonstrate that adversaries can enhance Attack Success Rates (ASR) against black-box targets by leveraging word importance ranking computed by offline white-box models. Our findings reveal vulnerabilities within current LLM protection mechanisms and highlight the need for more robust guardrail systems.","authors":["William Hackett","Lewis Birch","Stefan Trawicki","Neeraj Suri","Peter Garraghan"],"url":"https://arxiv.org/abs/2504.11168"}
{"created":"2025-04-16","title":"MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos","abstract":"Sexism is generally defined as prejudice and discrimination based on sex or gender, affecting every sector of society, from social institutions to relationships and individual behavior. Social media platforms amplify the impact of sexism by conveying discriminatory content not only through text but also across multiple modalities, highlighting the critical need for a multimodal approach to the analysis of sexism online. With the rise of social media platforms where users share short videos, sexism is increasingly spreading through video content. Automatically detecting sexism in videos is a challenging task, as it requires analyzing the combination of verbal, audio, and visual elements to identify sexist content. In this study, (1) we introduce MuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of $\\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose an innovative annotation framework for analyzing the contribution of textual and multimodal labels in the classification of sexist and non-sexist content; and (3) we evaluate a range of large language models (LLMs) and multimodal LLMs on the task of sexism detection. We find that visual information plays a key role in labeling sexist content for both humans and models. Models effectively detect explicit sexism; however, they struggle with implicit cases, such as stereotypes, instances where annotators also show low agreement. This highlights the inherent difficulty of the task, as identifying implicit sexism depends on the social and cultural context.","authors":["Laura De Grazia","Pol Pastells","Mauro V\\'azquez Chas","Desmond Elliott","Danae S\\'anchez Villegas","Mireia Farr\\'us","Mariona Taul\\'e"],"url":"https://arxiv.org/abs/2504.11169"}
{"created":"2025-04-16","title":"A Real-time Anomaly Detection Method for Robots based on a Flexible and Sparse Latent Space","abstract":"The growing demand for robots to operate effectively in diverse environments necessitates the need for robust real-time anomaly detection techniques during robotic operations. However, deep learning-based models in robotics face significant challenges due to limited training data and highly noisy signal features. In this paper, we present Sparse Masked Autoregressive Flow-based Adversarial AutoEncoders model to address these problems. This approach integrates Masked Autoregressive Flow model into Adversarial AutoEncoders to construct a flexible latent space and utilize Sparse autoencoder to efficiently focus on important features, even in scenarios with limited feature space. Our experiments demonstrate that the proposed model achieves a 4.96% to 9.75% higher area under the receiver operating characteristic curve for pick-and-place robotic operations with randomly placed cans, compared to existing state-of-the-art methods. Notably, it showed up to 19.67% better performance in scenarios involving collisions with lightweight objects. Additionally, unlike the existing state-of-the-art model, our model performs inferences within 1 millisecond, ensuring real-time anomaly detection. These capabilities make our model highly applicable to machine learning-based robotic safety systems in dynamic environments. The code will be made publicly available after acceptance.","authors":["Taewook Kang","Bum-Jae You","Juyoun Park","Yisoo Lee"],"url":"https://arxiv.org/abs/2504.11170"}
{"created":"2025-04-16","title":"TerraMind: Large-Scale Generative Multimodality for Earth Observation","abstract":"We present TerraMind, the first any-to-any generative, multimodal foundation model for Earth observation (EO). Unlike other multimodal models, TerraMind is pretrained on dual-scale representations combining both token-level and pixel-level data across modalities. On a token level, TerraMind encodes high-level contextual information to learn cross-modal relationships, while on a pixel level, TerraMind leverages fine-grained representations to capture critical spatial nuances. We pretrained TerraMind on nine geospatial modalities of a global, large-scale dataset. In this paper, we demonstrate that (i) TerraMind's dual-scale early fusion approach unlocks a range of zero-shot and few-shot applications for Earth observation, (ii) TerraMind introduces \"Thinking-in-Modalities\" (TiM) -- the capability of generating additional artificial data during finetuning and inference to improve the model output -- and (iii) TerraMind achieves beyond state-of-the-art performance in community-standard benchmarks for EO like PANGAEA. The pretraining dataset, the model weights, and our code is open-sourced under a permissive license.","authors":["Johannes Jakubik","Felix Yang","Benedikt Blumenstiel","Erik Scheurer","Rocco Sedona","Stefano Maurogiovanni","Jente Bosmans","Nikolaos Dionelis","Valerio Marsocci","Niklas Kopp","Rahul Ramachandran","Paolo Fraccaro","Thomas Brunschwiler","Gabriele Cavallaro","Juan Bernabe-Moreno","Nicolas Long\\'ep\\'e"],"url":"https://arxiv.org/abs/2504.11171"}
{"created":"2025-04-16","title":"TerraMesh: A Planetary Mosaic of Multimodal Earth Observation Data","abstract":"Large-scale foundation models in Earth Observation can learn versatile, label-efficient representations by leveraging massive amounts of unlabeled data. However, existing public datasets are often limited in scale, geographic coverage, or sensor variety. We introduce TerraMesh, a new globally diverse, multimodal dataset combining optical, synthetic aperture radar, elevation, and land-cover modalities in an Analysis-Ready Data format. TerraMesh includes over 9 million samples with eight spatiotemporal aligned modalities, enabling large-scale pre-training and fostering robust cross-modal correlation learning. We provide detailed data processing steps, comprehensive statistics, and empirical evidence demonstrating improved model performance when pre-trained on TerraMesh. The dataset will be made publicly available with a permissive license.","authors":["Benedikt Blumenstiel","Paolo Fraccaro","Valerio Marsocci","Johannes Jakubik","Stefano Maurogiovanni","Mikolaj Czerkawski","Rocco Sedona","Gabriele Cavallaro","Thomas Brunschwiler","Juan Bernabe-Moreno","Nicolas Long\\'ep\\'e"],"url":"https://arxiv.org/abs/2504.11172"}
{"created":"2025-04-16","title":"Exploring Backdoor Attack and Defense for LLM-empowered Recommendations","abstract":"The fusion of Large Language Models (LLMs) with recommender systems (RecSys) has dramatically advanced personalized recommendations and drawn extensive attention. Despite the impressive progress, the safety of LLM-based RecSys against backdoor attacks remains largely under-explored. In this paper, we raise a new problem: Can a backdoor with a specific trigger be injected into LLM-based Recsys, leading to the manipulation of the recommendation responses when the backdoor trigger is appended to an item's title? To investigate the vulnerabilities of LLM-based RecSys under backdoor attacks, we propose a new attack framework termed Backdoor Injection Poisoning for RecSys (BadRec). BadRec perturbs the items' titles with triggers and employs several fake users to interact with these items, effectively poisoning the training set and injecting backdoors into LLM-based RecSys. Comprehensive experiments reveal that poisoning just 1% of the training data with adversarial examples is sufficient to successfully implant backdoors, enabling manipulation of recommendations. To further mitigate such a security threat, we propose a universal defense strategy called Poison Scanner (P-Scanner). Specifically, we introduce an LLM-based poison scanner to detect the poisoned items by leveraging the powerful language understanding and rich knowledge of LLMs. A trigger augmentation agent is employed to generate diverse synthetic triggers to guide the poison scanner in learning domain-specific knowledge of the poisoned item detection task. Extensive experiments on three real-world datasets validate the effectiveness of the proposed P-Scanner.","authors":["Liangbo Ning","Wenqi Fan","Qing Li"],"url":"https://arxiv.org/abs/2504.11182"}
{"created":"2025-04-16","title":"Bias Beyond English: Evaluating Social Bias and Debiasing Methods in a Low-Resource Setting","abstract":"Social bias in language models can potentially exacerbate social inequalities. Despite it having garnered wide attention, most research focuses on English data. In a low-resource scenario, the models often perform worse due to insufficient training data. This study aims to leverage high-resource language corpora to evaluate bias and experiment with debiasing methods in low-resource languages. We evaluated the performance of recent multilingual models in five languages: English (\\textsc{eng}), Chinese (\\textsc{zho}), Russian (\\textsc{rus}), Indonesian (\\textsc{ind}) and Thai (\\textsc{tha}), and analyzed four bias dimensions: \\textit{gender}, \\textit{religion}, \\textit{nationality}, and \\textit{race-color}. By constructing multilingual bias evaluation datasets, this study allows fair comparisons between models across languages. We have further investigated three debiasing methods-\\texttt{CDA}, \\texttt{Dropout}, \\texttt{SenDeb}-and demonstrated that debiasing methods from high-resource languages can be effectively transferred to low-resource ones, providing actionable insights for fairness research in multilingual NLP.","authors":["Ej Zhou","Weiming Lu"],"url":"https://arxiv.org/abs/2504.11183"}
{"created":"2025-04-16","title":"Benchmarking Next-Generation Reasoning-Focused Large Language Models in Ophthalmology: A Head-to-Head Evaluation on 5,888 Items","abstract":"Recent advances in reasoning-focused large language models (LLMs) mark a shift from general LLMs toward models designed for complex decision-making, a crucial aspect in medicine. However, their performance in specialized domains like ophthalmology remains underexplored. This study comprehensively evaluated and compared the accuracy and reasoning capabilities of four newly developed reasoning-focused LLMs, namely DeepSeek-R1, OpenAI o1, o3-mini, and Gemini 2.0 Flash-Thinking. Each model was assessed using 5,888 multiple-choice ophthalmology exam questions from the MedMCQA dataset in zero-shot setting. Quantitative evaluation included accuracy, Macro-F1, and five text-generation metrics (ROUGE-L, METEOR, BERTScore, BARTScore, and AlignScore), computed against ground-truth reasonings. Average inference time was recorded for a subset of 100 randomly selected questions. Additionally, two board-certified ophthalmologists qualitatively assessed clarity, completeness, and reasoning structure of responses to differential diagnosis questions.O1 (0.902) and DeepSeek-R1 (0.888) achieved the highest accuracy, with o1 also leading in Macro-F1 (0.900). The performance of models across the text-generation metrics varied: O3-mini excelled in ROUGE-L (0.151), o1 in METEOR (0.232), DeepSeek-R1 and o3-mini tied for BERTScore (0.673), DeepSeek-R1 (-4.105) and Gemini 2.0 Flash-Thinking (-4.127) performed best in BARTScore, while o3-mini (0.181) and o1 (0.176) led AlignScore. Inference time across the models varied, with DeepSeek-R1 being slowest (40.4 seconds) and Gemini 2.0 Flash-Thinking fastest (6.7 seconds). Qualitative evaluation revealed that DeepSeek-R1 and Gemini 2.0 Flash-Thinking tended to provide detailed and comprehensive intermediate reasoning, whereas o1 and o3-mini displayed concise and summarized justifications.","authors":["Minjie Zou","Sahana Srinivasan","Thaddaeus Wai Soon Lo","Ke Zou","Gabriel Dawei Yang","Xuguang Ai","Hyunjae Kim","Maxwell Singer","Fares Antaki","Kelvin Li","Robert Chang","Marcus Tan","David Ziyou Chen","Dianbo Liu","Qingyu Chen","Yih Chung Tham"],"url":"https://arxiv.org/abs/2504.11186"}
{"created":"2025-04-16","title":"Enhancing multimodal analogical reasoning with Logic Augmented Generation","abstract":"Recent advances in Large Language Models have demonstrated their capabilities across a variety of tasks. However, automatically extracting implicit knowledge from natural language remains a significant challenge, as machines lack active experience with the physical world. Given this scenario, semantic knowledge graphs can serve as conceptual spaces that guide the automated text generation reasoning process to achieve more efficient and explainable results. In this paper, we apply a logic-augmented generation (LAG) framework that leverages the explicit representation of a text through a semantic knowledge graph and applies it in combination with prompt heuristics to elicit implicit analogical connections. This method generates extended knowledge graph triples representing implicit meaning, enabling systems to reason on unlabeled multimodal data regardless of the domain. We validate our work through three metaphor detection and understanding tasks across four datasets, as they require deep analogical reasoning capabilities. The results show that this integrated approach surpasses current baselines, performs better than humans in understanding visual metaphors, and enables more explainable reasoning processes, though still has inherent limitations in metaphor understanding, especially for domain-specific metaphors. Furthermore, we propose a thorough error analysis, discussing issues with metaphorical annotations and current evaluation methods.","authors":["Anna Sofia Lippolis","Andrea Giovanni Nuzzolese","Aldo Gangemi"],"url":"https://arxiv.org/abs/2504.11190"}
{"created":"2025-04-16","title":"Magnetic Field Conforming Formulations for Foil Windings","abstract":"We extend the foil winding homogenization method to magnetic field conforming formulations. We first propose a full magnetic field foil winding formulation by analogy with magnetic flux density conforming formulations. We then introduce the magnetic scalar potential in non-conducting regions to improve the efficiency of the model. This leads to a significant reduction in the number of degrees of freedom, particularly in 3-D applications. The proposed models are verified on two frequency-domain benchmark problems: a 2-D axisymmetric problem and a 3-D problem. They reproduce results obtained with magnetic flux density conforming formulations and with resolved conductor models that explicitly discretize all turns. Moreover, the models are applied in the transient simulation of a high-temperature superconducting coil. In all investigated configurations, the proposed models provide reliable results while considerably reducing the size of the numerical problem to be solved.","authors":["Louis Denis","Elias Paakkunainen","Paavo Rasilo","Sebastian Sch\\\"ops","Beno\\^it Vanderheyden","Christophe Geuzaine"],"url":"https://arxiv.org/abs/2504.11191"}
{"created":"2025-04-16","title":"R-TPT: Improving Adversarial Robustness of Vision-Language Models through Test-Time Prompt Tuning","abstract":"Vision-language models (VLMs), such as CLIP, have gained significant popularity as foundation models, with numerous fine-tuning methods developed to enhance performance on downstream tasks. However, due to their inherent vulnerability and the common practice of selecting from a limited set of open-source models, VLMs suffer from a higher risk of adversarial attacks than traditional vision models. Existing defense techniques typically rely on adversarial fine-tuning during training, which requires labeled data and lacks of flexibility for downstream tasks. To address these limitations, we propose robust test-time prompt tuning (R-TPT), which mitigates the impact of adversarial attacks during the inference stage. We first reformulate the classic marginal entropy objective by eliminating the term that introduces conflicts under adversarial conditions, retaining only the pointwise entropy minimization. Furthermore, we introduce a plug-and-play reliability-based weighted ensembling strategy, which aggregates useful information from reliable augmented views to strengthen the defense. R-TPT enhances defense against adversarial attacks without requiring labeled training data while offering high flexibility for inference tasks. Extensive experiments on widely used benchmarks with various attacks demonstrate the effectiveness of R-TPT. The code is available in https://github.com/TomSheng21/R-TPT.","authors":["Lijun Sheng","Jian Liang","Zilei Wang","Ran He"],"url":"https://arxiv.org/abs/2504.11195"}
{"created":"2025-04-16","title":"The Lifetime of the Covid Memorial Wall: Modelling with Collections Demography, Social Media Data and Citizen Science","abstract":"The National Covid Memorial Wall in London, featuring over 240,000 hand-painted red hearts, faces significant conservation challenges due to the rapid fading of the paint. This study evaluates the transition to a better-quality paint and its implications for the wall's long-term preservation. The rapid fading of the initial materials required an unsustainable repainting rate, burdening volunteers. Lifetime simulations based on a collections demography framework suggest that repainting efforts must continue at a rate of some hundreds of hearts per week to maintain a stable percentage of hearts in good condition. This finding highlights the need for a sustainable management strategy that includes regular maintenance or further reduction of the fading rate.","authors":["Josep Grau-Bov\\'e","Mara Cruz","Pakhee Kumar"],"url":"https://arxiv.org/abs/2504.11196"}
{"created":"2025-04-16","title":"Efficient Distributed Retrieval-Augmented Generation for Enhancing Language Model Performance","abstract":"Small language models (SLMs) support efficient deployments on resource-constrained edge devices, but their limited capacity compromises inference performance. Retrieval-augmented generation (RAG) is a promising solution to enhance model performance by integrating external databases, without requiring intensive on-device model retraining. However, large-scale public databases and user-specific private contextual documents are typically located on the cloud and the device separately, while existing RAG implementations are primarily centralized. To bridge this gap, we propose DRAGON, a distributed RAG framework to enhance on-device SLMs through both general and personal knowledge without the risk of leaking document privacy. Specifically, DRAGON decomposes multi-document RAG into multiple parallel token generation processes performed independently and locally on the cloud and the device, and employs a newly designed Speculative Aggregation, a dual-side speculative algorithm to avoid frequent output synchronization between the cloud and device. A new scheduling algorithm is further introduced to identify the optimal aggregation side based on real-time network conditions. Evaluations on real-world hardware testbed demonstrate a significant performance improvement of DRAGON-up to 1.9x greater gains over standalone SLM compared to the centralized RAG, substantial reduction in per-token latency, and negligible Time to First Token (TTFT) overhead.","authors":["Shangyu Liu","Zhenzhe Zheng","Xiaoyao Huang","Fan Wu","Jie Wu"],"url":"https://arxiv.org/abs/2504.11197"}
{"created":"2025-04-16","title":"Video Summarization with Large Language Models","abstract":"The exponential increase in video content poses significant challenges in terms of efficient navigation, search, and retrieval, thus requiring advanced video summarization techniques. Existing video summarization methods, which heavily rely on visual features and temporal dynamics, often fail to capture the semantics of video content, resulting in incomplete or incoherent summaries. To tackle the challenge, we propose a new video summarization framework that leverages the capabilities of recent Large Language Models (LLMs), expecting that the knowledge learned from massive data enables LLMs to evaluate video frames in a manner that better aligns with diverse semantics and human judgments, effectively addressing the inherent subjectivity in defining keyframes. Our method, dubbed LLM-based Video Summarization (LLMVS), translates video frames into a sequence of captions using a Muti-modal Large Language Model (M-LLM) and then assesses the importance of each frame using an LLM, based on the captions in its local context. These local importance scores are refined through a global attention mechanism in the entire context of video captions, ensuring that our summaries effectively reflect both the details and the overarching narrative. Our experimental results demonstrate the superiority of the proposed method over existing ones in standard benchmarks, highlighting the potential of LLMs in the processing of multimedia content.","authors":["Min Jung Lee","Dayoung Gong","Minsu Cho"],"url":"https://arxiv.org/abs/2504.11199"}
{"created":"2025-04-16","title":"Mutual Understanding between People and Systems via Neurosymbolic AI and Knowledge Graphs","abstract":"This chapter investigates the concept of mutual understanding between humans and systems, positing that Neuro-symbolic Artificial Intelligence (NeSy AI) methods can significantly enhance this mutual understanding by leveraging explicit symbolic knowledge representations with data-driven learning models. We start by introducing three critical dimensions to characterize mutual understanding: sharing knowledge, exchanging knowledge, and governing knowledge. Sharing knowledge involves aligning the conceptual models of different agents to enable a shared understanding of the domain of interest. Exchanging knowledge relates to ensuring the effective and accurate communication between agents. Governing knowledge concerns establishing rules and processes to regulate the interaction between agents. Then, we present several different use case scenarios that demonstrate the application of NeSy AI and Knowledge Graphs to aid meaningful exchanges between human, artificial, and robotic agents. These scenarios highlight both the potential and the challenges of combining top-down symbolic reasoning with bottom-up neural learning, guiding the discussion of the coverage provided by current solutions along the dimensions of sharing, exchanging, and governing knowledge. Concurrently, this analysis facilitates the identification of gaps and less developed aspects in mutual understanding to address in future research.","authors":["Irene Celino","Mario Scrocca","Agnese Chiatti"],"url":"https://arxiv.org/abs/2504.11200"}
{"created":"2025-04-16","title":"Focal Split: Untethered Snapshot Depth from Differential Defocus","abstract":"We introduce Focal Split, a handheld, snapshot depth camera with fully onboard power and computing based on depth-from-differential-defocus (DfDD). Focal Split is passive, avoiding power consumption of light sources. Its achromatic optical system simultaneously forms two differentially defocused images of the scene, which can be independently captured using two photosensors in a snapshot. The data processing is based on the DfDD theory, which efficiently computes a depth and a confidence value for each pixel with only 500 floating point operations (FLOPs) per pixel from the camera measurements. We demonstrate a Focal Split prototype, which comprises a handheld custom camera system connected to a Raspberry Pi 5 for real-time data processing. The system consumes 4.9 W and is powered on a 5 V, 10,000 mAh battery. The prototype can measure objects with distances from 0.4 m to 1.2 m, outputting 480$\\times$360 sparse depth maps at 2.1 frames per second (FPS) using unoptimized Python scripts. Focal Split is DIY friendly. A comprehensive guide to building your own Focal Split depth camera, code, and additional data can be found at https://focal-split.qiguo.org.","authors":["Junjie Luo","John Mamish","Alan Fu","Thomas Concannon","Josiah Hester","Emma Alexander","Qi Guo"],"url":"https://arxiv.org/abs/2504.11202"}
{"created":"2025-04-16","title":"Braiding vineyards","abstract":"Vineyards are a common way to study persistence diagrams of a data set which is changing, as strong stability means that it is possible to pair points in ``nearby'' persistence diagrams, yielding a family of point sets which connect into curves when stacked. Recent work has also studied monodromy in the persistent homology transform, demonstrating some interesting connections between an input shape and monodromy in the persistent homology transform for 0-dimensional homology embedded in $\\mathbb{R}^2$. In this work, we re-characterize monodromy in terms of periodicity of the associated vineyard of persistence diagrams.","authors":["Erin Chambers","Christopher Fillmore","Elizabeth Stephenson","Mathijs Wintraecken"],"url":"https://arxiv.org/abs/2504.11203"}
{"created":"2025-04-16","title":"Slice+Slice Baby: Generating Last-Level Cache Eviction Sets in the Blink of an Eye","abstract":"An essential step for mounting cache attacks is finding eviction sets, collections of memory locations that contend on cache space. On Intel processors, one of the main challenges for identifying contending addresses is the sliced cache design, where the processor hashes the physical address to determine where in the cache a memory location is stored. While past works have demonstrated that the hash function can be reversed, they also showed that it depends on physical address bits that the adversary does not know.","authors":["Bradley Morgan","Gal Horowitz","Sioli O'Connell","Stephan van Schaik","Chitchanok Chuengsatiansup","Daniel Genkin","Olaf Maennel","Paul Montague","Eyal Ronen","Yuval Yarom"],"url":"https://arxiv.org/abs/2504.11208"}
{"created":"2025-04-16","title":"SDFs from Unoriented Point Clouds using Neural Variational Heat Distances","abstract":"We propose a novel variational approach for computing neural Signed Distance Fields (SDF) from unoriented point clouds. To this end, we replace the commonly used eikonal equation with the heat method, carrying over to the neural domain what has long been standard practice for computing distances on discrete surfaces. This yields two convex optimization problems for whose solution we employ neural networks: We first compute a neural approximation of the gradients of the unsigned distance field through a small time step of heat flow with weighted point cloud densities as initial data. Then we use it to compute a neural approximation of the SDF. We prove that the underlying variational problems are well-posed. Through numerical experiments, we demonstrate that our method provides state-of-the-art surface reconstruction and consistent SDF gradients. Furthermore, we show in a proof-of-concept that it is accurate enough for solving a PDE on the zero-level set.","authors":["Samuel Weidemaier","Florine Hartwig","Josua Sassen","Sergio Conti","Mirela Ben-Chen","Martin Rumpf"],"url":"https://arxiv.org/abs/2504.11212"}
{"created":"2025-04-16","title":"Diversity-Driven Learning: Tackling Spurious Correlations and Data Heterogeneity in Federated Models","abstract":"Federated Learning (FL) enables decentralized training of machine learning models on distributed data while preserving privacy. However, in real-world FL settings, client data is often non-identically distributed and imbalanced, resulting in statistical data heterogeneity which impacts the generalization capabilities of the server's model across clients, slows convergence and reduces performance. In this paper, we address this challenge by first proposing a characterization of statistical data heterogeneity by means of 6 metrics of global and client attribute imbalance, class imbalance, and spurious correlations. Next, we create and share 7 computer vision datasets for binary and multiclass image classification tasks in Federated Learning that cover a broad range of statistical data heterogeneity and hence simulate real-world situations. Finally, we propose FedDiverse, a novel client selection algorithm in FL which is designed to manage and leverage data heterogeneity across clients by promoting collaboration between clients with complementary data distributions. Experiments on the seven proposed FL datasets demonstrate FedDiverse's effectiveness in enhancing the performance and robustness of a variety of FL methods while having low communication and computational overhead.","authors":["Gergely D. N\\'emeth","Eros Fan\\`i","Yeat Jeng Ng","Barbara Caputo","Miguel \\'Angel Lozano","Nuria Oliver","Novi Quadrianto"],"url":"https://arxiv.org/abs/2504.11216"}
{"created":"2025-04-16","title":"3DAffordSplat: Efficient Affordance Reasoning with 3D Gaussians","abstract":"3D affordance reasoning is essential in associating human instructions with the functional regions of 3D objects, facilitating precise, task-oriented manipulations in embodied AI. However, current methods, which predominantly depend on sparse 3D point clouds, exhibit limited generalizability and robustness due to their sensitivity to coordinate variations and the inherent sparsity of the data. By contrast, 3D Gaussian Splatting (3DGS) delivers high-fidelity, real-time rendering with minimal computational overhead by representing scenes as dense, continuous distributions. This positions 3DGS as a highly effective approach for capturing fine-grained affordance details and improving recognition accuracy. Nevertheless, its full potential remains largely untapped due to the absence of large-scale, 3DGS-specific affordance datasets. To overcome these limitations, we present 3DAffordSplat, the first large-scale, multi-modal dataset tailored for 3DGS-based affordance reasoning. This dataset includes 23,677 Gaussian instances, 8,354 point cloud instances, and 6,631 manually annotated affordance labels, encompassing 21 object categories and 18 affordance types. Building upon this dataset, we introduce AffordSplatNet, a novel model specifically designed for affordance reasoning using 3DGS representations. AffordSplatNet features an innovative cross-modal structure alignment module that exploits structural consistency priors to align 3D point cloud and 3DGS representations, resulting in enhanced affordance recognition accuracy. Extensive experiments demonstrate that the 3DAffordSplat dataset significantly advances affordance learning within the 3DGS domain, while AffordSplatNet consistently outperforms existing methods across both seen and unseen settings, highlighting its robust generalization capabilities.","authors":["Zeming wei","Junyi Lin","Yang Liu","Weixing Chen","Jingzhou Luo","Guanbin Li","Liang Lin"],"url":"https://arxiv.org/abs/2504.11218"}
{"created":"2025-04-16","title":"Directed First-Order Logic","abstract":"We present a first-order logic equipped with an \"asymmetric\" directed notion of equality, which can be thought of as transitions/rewrites between terms, allowing for types to be interpreted as preorders. We then provide a universal property to such \"directed equalities\" by describing introduction and elimination rules that allows them to be contracted only with certain syntactic restrictions, based on polarity, which do not allow for symmetry to be derived. We give a characterization of such directed equality as a relative left adjoint, generalizing the idea by Lawvere of equality as left adjoint. The logic is equipped with a precise syntactic system of polarities, inspired by dinaturality, that keeps track of the occurrence of variables (positive/negative/both). The semantics of this logic and its system of variances is then captured categorically using the notion of directed doctrine, which we prove sound and complete with respect to the syntax.","authors":["Andrea Laretto","Fosco Loregian","Niccol\\`o Veltri"],"url":"https://arxiv.org/abs/2504.11225"}
{"created":"2025-04-16","title":"VEXP: A Low-Cost RISC-V ISA Extension for Accelerated Softmax Computation in Transformers","abstract":"While Transformers are dominated by Floating-Point (FP) Matrix-Multiplications, their aggressive acceleration through dedicated hardware or many-core programmable systems has shifted the performance bottleneck to non-linear functions like Softmax. Accelerating Softmax is challenging due to its non-pointwise, non-linear nature, with exponentiation as the most demanding step. To address this, we design a custom arithmetic block for Bfloat16 exponentiation leveraging a novel approximation algorithm based on Schraudolph's method, and we integrate it into the Floating-Point Unit (FPU) of the RISC-V cores of a compute cluster, through custom Instruction Set Architecture (ISA) extensions, with a negligible area overhead of 1\\%. By optimizing the software kernels to leverage the extension, we execute Softmax with 162.7$\\times$ less latency and 74.3$\\times$ less energy compared to the baseline cluster, achieving an 8.2$\\times$ performance improvement and 4.1$\\times$ higher energy efficiency for the FlashAttention-2 kernel in GPT-2 configuration. Moreover, the proposed approach enables a multi-cluster system to efficiently execute end-to-end inference of pre-trained Transformer models, such as GPT-2, GPT-3 and ViT, achieving up to 5.8$\\times$ and 3.6$\\times$ reduction in latency and energy consumption, respectively, without requiring re-training and with negligible accuracy loss.","authors":["Run Wang","Gamze Islamoglu","Andrea Belano","Viviane Potocnik","Francesco Conti","Angelo Garofalo","Luca Benini"],"url":"https://arxiv.org/abs/2504.11227"}
{"created":"2025-04-16","title":"The Forward-Forward Algorithm: Characterizing Training Behavior","abstract":"The Forward-Forward algorithm is an alternative learning method which consists of two forward passes rather than a forward and backward pass employed by backpropagation. Forward-Forward networks employ layer local loss functions which are optimized based on the layer activation for each forward pass rather than a single global objective function. This work explores the dynamics of model and layer accuracy changes in Forward-Forward networks as training progresses in pursuit of a mechanistic understanding of their internal behavior. Treatments to various system characteristics are applied to investigate changes in layer and overall model accuracy as training progresses, how accuracy is impacted by layer depth, and how strongly individual layer accuracy is correlated with overall model accuracy. The empirical results presented suggest that layers deeper within Forward-Forward networks experience a delay in accuracy improvement relative to shallower layers and that shallower layer accuracy is strongly correlated with overall model accuracy.","authors":["Reece Adamson"],"url":"https://arxiv.org/abs/2504.11229"}
{"created":"2025-04-16","title":"CAP-Net: A Unified Network for 6D Pose and Size Estimation of Categorical Articulated Parts from a Single RGB-D Image","abstract":"This paper tackles category-level pose estimation of articulated objects in robotic manipulation tasks and introduces a new benchmark dataset. While recent methods estimate part poses and sizes at the category level, they often rely on geometric cues and complex multi-stage pipelines that first segment parts from the point cloud, followed by Normalized Part Coordinate Space (NPCS) estimation for 6D poses. These approaches overlook dense semantic cues from RGB images, leading to suboptimal accuracy, particularly for objects with small parts. To address these limitations, we propose a single-stage Network, CAP-Net, for estimating the 6D poses and sizes of Categorical Articulated Parts. This method combines RGB-D features to generate instance segmentation and NPCS representations for each part in an end-to-end manner. CAP-Net uses a unified network to simultaneously predict point-wise class labels, centroid offsets, and NPCS maps. A clustering algorithm then groups points of the same predicted class based on their estimated centroid distances to isolate each part. Finally, the NPCS region of each part is aligned with the point cloud to recover its final pose and size. To bridge the sim-to-real domain gap, we introduce the RGBD-Art dataset, the largest RGB-D articulated dataset to date, featuring photorealistic RGB images and depth noise simulated from real sensors. Experimental evaluations on the RGBD-Art dataset demonstrate that our method significantly outperforms the state-of-the-art approach. Real-world deployments of our model in robotic tasks underscore its robustness and exceptional sim-to-real transfer capabilities, confirming its substantial practical utility. Our dataset, code and pre-trained models are available on the project page.","authors":["Jingshun Huang","Haitao Lin","Tianyu Wang","Yanwei Fu","Xiangyang Xue","Yi Zhu"],"url":"https://arxiv.org/abs/2504.11230"}
{"created":"2025-04-16","title":"Leveraging multimodal explanatory annotations for video interpretation with Modality Specific Dataset","abstract":"We examine the impact of concept-informed supervision on multimodal video interpretation models using MOByGaze, a dataset containing human-annotated explanatory concepts. We introduce Concept Modality Specific Datasets (CMSDs), which consist of data subsets categorized by the modality (visual, textual, or audio) of annotated concepts. Models trained on CMSDs outperform those using traditional legacy training in both early and late fusion approaches. Notably, this approach enables late fusion models to achieve performance close to that of early fusion models. These findings underscore the importance of modality-specific annotations in developing robust, self-explainable video models and contribute to advancing interpretable multimodal learning in complex video analysis.","authors":["Elisa Ancarani","Julie Tores","Lucile Sassatelli","R\\'emy Sun","Hui-Yin Wu","Fr\\'ed\\'eric Precioso"],"url":"https://arxiv.org/abs/2504.11232"}
{"created":"2025-04-16","title":"AutoRAN: Automated and Zero-Touch Open RAN Systems","abstract":"[...] This paper presents AutoRAN, an automated, intent-driven framework for zero-touch provisioning of open, programmable cellular networks. Leveraging cloud-native principles, AutoRAN employs virtualization, declarative infrastructure-as-code templates, and disaggregated micro-services to abstract physical resources and protocol stacks. Its orchestration engine integrates Language Models (LLMs) to translate high-level intents into machine-readable configurations, enabling closed-loop control via telemetry-driven observability. Implemented on a multi-architecture OpenShift cluster with heterogeneous compute (x86/ARM CPUs, NVIDIA GPUs) and multi-vendor Radio Access Network (RAN) hardware (Foxconn, NI), AutoRAN automates deployment of O-RAN-compliant stacks-including OpenAirInterface, NVIDIA ARC RAN, Open5GS core, and O-RAN Software Community (OSC) RIC components-using CI/CD pipelines. Experimental results demonstrate that AutoRAN is capable of deploying an end-to-end Private 5G network in less than 60 seconds with 1.6 Gbps throughput, validating its ability to streamline configuration, accelerate testing, and reduce manual intervention with similar performance than non cloud-based implementations. With its novel LLM-assisted intent translation mechanism, and performance-optimized automation workflow for multi-vendor environments, AutoRAN has the potential of advancing the robustness of next-generation cellular supply chains through reproducible, intent-based provisioning across public and private deployments.","authors":["Stefano Maxenti","Ravis Shirkhani","Maxime Elkael","Leonardo Bonati","Salvatore D'Oro","Tommaso Melodia","Michele Polese"],"url":"https://arxiv.org/abs/2504.11233"}
{"created":"2025-04-16","title":"Nondeterministic Polynomial-time Problem Challenge: An Ever-Scaling Reasoning Benchmark for LLMs","abstract":"Reasoning is the fundamental capability of large language models (LLMs). Due to the rapid progress of LLMs, there are two main issues of current benchmarks: i) these benchmarks can be crushed in a short time (less than 1 year), and ii) these benchmarks may be easily hacked. To handle these issues, we propose the ever-scalingness for building the benchmarks which are uncrushable, unhackable, auto-verifiable and general. This paper presents Nondeterministic Polynomial-time Problem Challenge (NPPC), an ever-scaling reasoning benchmark for LLMs. Specifically, the NPPC has three main modules: i) npgym, which provides a unified interface of 25 well-known NP-complete problems and can generate any number of instances with any levels of complexities, ii) npsolver: which provides a unified interface to evaluate the problem instances with both online and offline models via APIs and local deployments, respectively, and iii) npeval: which provides the comprehensive and ready-to-use tools to analyze the performances of LLMs over different problems, the number of tokens, the aha moments, the reasoning errors and the solution errors. Extensive experiments over widely-used LLMs demonstrate: i) NPPC can successfully decrease the performances of advanced LLMs' performances to below 10%, demonstrating that NPPC is uncrushable, ii) DeepSeek-R1, Claude-3.7-Sonnet, and o1/o3-mini are the most powerful LLMs, where DeepSeek-R1 outperforms Claude-3.7-Sonnet and o1/o3-mini in most NP-complete problems considered, and iii) the numbers of tokens, aha moments in the advanced LLMs, e.g., Claude-3.7-Sonnet and DeepSeek-R1, are observed first to increase and then decrease when the problem instances become more and more difficult. We believe that NPPC is the first ever-scaling reasoning benchmark, serving as the uncrushable and unhackable testbed for LLMs toward artificial general intelligence (AGI).","authors":["Chang Yang","Ruiyu Wang","Junzhe Jiang","Qi Jiang","Qinggang Zhang","Yanchen Deng","Shuxin Li","Shuyue Hu","Bo Li","Florian T. Pokorny","Xiao Huang","Xinrun Wang"],"url":"https://arxiv.org/abs/2504.11239"}
{"created":"2025-04-16","title":"Towards Automated Safety Requirements Derivation Using Agent-based RAG","abstract":"We study the automated derivation of safety requirements in a self-driving vehicle use case, leveraging LLMs in combination with agent-based retrieval-augmented generation. Conventional approaches that utilise pre-trained LLMs to assist in safety analyses typically lack domain-specific knowledge. Existing RAG approaches address this issue, yet their performance deteriorates when handling complex queries and it becomes increasingly harder to retrieve the most relevant information. This is particularly relevant for safety-relevant applications. In this paper, we propose the use of agent-based RAG to derive safety requirements and show that the retrieved information is more relevant to the queries. We implement an agent-based approach on a document pool of automotive standards and the Apollo case study, as a representative example of an automated driving perception system. Our solution is tested on a data set of safety requirement questions and answers, extracted from the Apollo data. Evaluating a set of selected RAG metrics, we present and discuss advantages of a agent-based approach compared to default RAG methods.","authors":["Balahari Vignesh Balu","Florian Geissler","Francesco Carella","Joao-Vitor Zacchi","Josef Jiru","Nuria Mata","Reinhard Stolle"],"url":"https://arxiv.org/abs/2504.11243"}
{"created":"2025-04-16","title":"Influence Maximization in Temporal Social Networks with a Cold-Start Problem: A Supervised Approach","abstract":"Influence Maximization (IM) in temporal graphs focuses on identifying influential \"seeds\" that are pivotal for maximizing network expansion. We advocate defining these seeds through Influence Propagation Paths (IPPs), which is essential for scaling up the network. Our focus lies in efficiently labeling IPPs and accurately predicting these seeds, while addressing the often-overlooked cold-start issue prevalent in temporal networks. Our strategy introduces a motif-based labeling method and a tensorized Temporal Graph Network (TGN) tailored for multi-relational temporal graphs, bolstering prediction accuracy and computational efficiency. Moreover, we augment cold-start nodes with new neighbors from historical data sharing similar IPPs. The recommendation system within an online team-based gaming environment presents subtle impact on the social network, forming multi-relational (i.e., weak and strong) temporal graphs for our empirical IM study. We conduct offline experiments to assess prediction accuracy and model training efficiency, complemented by online A/B testing to validate practical network growth and the effectiveness in addressing the cold-start issue.","authors":["Laixin Xie","Ying Zhang","Xiyuan Wang","Shiyi Liu","Shenghan Gao","Xingxing Xing","Wei Wan","Haipeng Zhang","Quan Li"],"url":"https://arxiv.org/abs/2504.11245"}
{"created":"2025-04-16","title":"Next-Future: Sample-Efficient Policy Learning for Robotic-Arm Tasks","abstract":"Hindsight Experience Replay (HER) is widely regarded as the state-of-the-art algorithm for achieving sample-efficient multi-goal reinforcement learning (RL) in robotic manipulation tasks with binary rewards. HER facilitates learning from failed attempts by replaying trajectories with redefined goals. However, it relies on a heuristic-based replay method that lacks a principled framework. To address this limitation, we introduce a novel replay strategy, \"Next-Future\", which focuses on rewarding single-step transitions. This approach significantly enhances sample efficiency and accuracy in learning multi-goal Markov decision processes (MDPs), particularly under stringent accuracy requirements -- a critical aspect for performing complex and precise robotic-arm tasks. We demonstrate the efficacy of our method by highlighting how single-step learning enables improved value approximation within the multi-goal RL framework. The performance of the proposed replay strategy is evaluated across eight challenging robotic manipulation tasks, using ten random seeds for training. Our results indicate substantial improvements in sample efficiency for seven out of eight tasks and higher success rates in six tasks. Furthermore, real-world experiments validate the practical feasibility of the learned policies, demonstrating the potential of \"Next-Future\" in solving complex robotic-arm tasks.","authors":["Fikrican \\\"Ozg\\\"ur","Ren\\'e Zurbr\\\"ugg","Suryansh Kumar"],"url":"https://arxiv.org/abs/2504.11247"}
{"created":"2025-04-16","title":"A Rollout-Based Algorithm and Reward Function for Efficient Resource Allocation in Business Processes","abstract":"Resource allocation plays a critical role in minimizing cycle time and improving the efficiency of business processes. Recently, Deep Reinforcement Learning (DRL) has emerged as a powerful tool to optimize resource allocation policies in business processes. In the DRL framework, an agent learns a policy through interaction with the environment, guided solely by reward signals that indicate the quality of its decisions. However, existing algorithms are not suitable for dynamic environments such as business processes. Furthermore, existing DRL-based methods rely on engineered reward functions that approximate the desired objective, but a misalignment between reward and objective can lead to undesired decisions or suboptimal policies. To address these issues, we propose a rollout-based DRL algorithm and a reward function to optimize the objective directly. Our algorithm iteratively improves the policy by evaluating execution trajectories following different actions. Our reward function directly decomposes the objective function of minimizing the mean cycle time. Maximizing our reward function guarantees that the objective function is minimized without requiring extensive reward engineering. The results show that our method consistently learns the optimal policy in all six evaluated business processes, outperforming the state-of-the-art algorithm that can only learn the optimal policy in two of the evaluated processes.","authors":["Jeroen Middelhuis","Zaharah Bukhsh","Ivo Adan","Remco Dijkman"],"url":"https://arxiv.org/abs/2504.11250"}
{"created":"2025-04-16","title":"Easy repair via codes with simplex locality","abstract":"In the context of distributed storage systems, locally repairable codes have become important. In this paper we focus on codes that allow for multi-erasure pattern decoding with low computational effort. Different optimality requirements, measured by the code's rate, minimum distance, locality, availability as well as field size, influence each other and can not all be maximized at the same time. We focus on the notion of easy repair, more specifically on the construction of codes that can repair correctable erasure patterns with minimal computational effort. In particular, we introduce the easy repair property and then present codes of different rates that possess this property. The presented codes are all in some way related to simplex codes and comprise block codes as well as unit-memory convolutional codes. We also formulate conditions under which the easy repairs can be performed in parallel, thus improving access speed of the distributed storage system.","authors":["Margreta Kuijper","Julia Lieb","Diego Napp"],"url":"https://arxiv.org/abs/2504.11251"}
{"created":"2025-04-16","title":"Reconstructing Fine-Grained Network Data using Autoencoder Architectures with Domain Knowledge Penalties","abstract":"The ability to reconstruct fine-grained network session data, including individual packets, from coarse-grained feature vectors is crucial for improving network security models. However, the large-scale collection and storage of raw network traffic pose significant challenges, particularly for capturing rare cyberattack samples. These challenges hinder the ability to retain comprehensive datasets for model training and future threat detection. To address this, we propose a machine learning approach guided by formal methods to encode and reconstruct network data. Our method employs autoencoder models with domain-informed penalties to impute PCAP session headers from structured feature representations. Experimental results demonstrate that incorporating domain knowledge through constraint-based loss terms significantly improves reconstruction accuracy, particularly for categorical features with session-level encodings. By enabling efficient reconstruction of detailed network sessions, our approach facilitates data-efficient model training while preserving privacy and storage efficiency.","authors":["Mark Cheung","Sridhar Venkatesan"],"url":"https://arxiv.org/abs/2504.11255"}
{"created":"2025-04-16","title":"Covering Approximate Shortest Paths with DAGs","abstract":"We define and study analogs of probabilistic tree embedding and tree cover for directed graphs. We define the notion of a DAG cover of a general directed graph $G$: a small collection $D_1,\\dots D_g$ of DAGs so that for all pairs of vertices $s,t$, some DAG $D_i$ provides low distortion for $dist(s,t)$; i.e. $ dist_G(s, t) \\le \\min_{i \\in [g]} dist_{D_i}(s, t) \\leq \\alpha \\cdot dist_G(s, t)$, where $\\alpha$ is the distortion.","authors":["Sepehr Assadi","Gary Hoppenworth","Nicole Wein"],"url":"https://arxiv.org/abs/2504.11256"}
{"created":"2025-04-16","title":"UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis","abstract":"Recent advancements in Large Vision-Language Models are accelerating the development of Graphical User Interface (GUI) agents that utilize human-like vision perception capabilities to enhance productivity on digital devices. Compared to approaches predicated on GUI metadata, which are platform-dependent and vulnerable to implementation variations, vision-based approaches offer broader applicability. In this vision-based paradigm, the GUI instruction grounding, which maps user instruction to the location of corresponding element on the given screenshot, remains a critical challenge, particularly due to limited public training dataset and resource-intensive manual instruction data annotation.In this paper, we delve into unexplored challenges in this task including element-to-screen ratio, unbalanced element type, and implicit instruction. To address these challenges, we introduce a large-scale data synthesis pipeline UI-E2I-Synth for generating varying complex instruction datasets using GPT-4o instead of human annotators. Furthermore, we propose a new GUI instruction grounding benchmark UI-I2E-Bench, which is designed to address the limitations of existing benchmarks by incorporating diverse annotation aspects. Our model, trained on the synthesized data, achieves superior performance in GUI instruction grounding, demonstrating the advancements of proposed data synthesis pipeline. The proposed benchmark, accompanied by extensive analyses, provides practical insights for future research in GUI grounding. We will release corresponding artifacts at https://colmon46.github.io/i2e-bench-leaderboard/","authors":["Xinyi Liu","Xiaoyi Zhang","Ziyun Zhang","Yan Lu"],"url":"https://arxiv.org/abs/2504.11257"}
{"created":"2025-04-16","title":"The Cambridge Report on Database Research","abstract":"On October 19 and 20, 2023, the authors of this report convened in Cambridge, MA, to discuss the state of the database research field, its recent accomplishments and ongoing challenges, and future directions for research and community engagement. This gathering continues a long standing tradition in the database community, dating back to the late 1980s, in which researchers meet roughly every five years to produce a forward looking report.","authors":["Anastasia Ailamaki","Samuel Madden","Daniel Abadi","Gustavo Alonso","Sihem Amer-Yahia","Magdalena Balazinska","Philip A. Bernstein","Peter Boncz","Michael Cafarella","Surajit Chaudhuri","Susan Davidson","David DeWitt","Yanlei Diao","Xin Luna Dong","Michael Franklin","Juliana Freire","Johannes Gehrke","Alon Halevy","Joseph M. Hellerstein","Mark D. Hill","Stratos Idreos","Yannis Ioannidis","Christoph Koch","Donald Kossmann","Tim Kraska","Arun Kumar","Guoliang Li","Volker Markl","Ren\\'ee Miller","C. Mohan","Thomas Neumann","Beng Chin Ooi","Fatma Ozcan","Aditya Parameswaran","Ippokratis Pandis","Jignesh M. Patel","Andrew Pavlo","Danica Porobic","Viktor Sanca","Michael Stonebraker","Julia Stoyanovich","Dan Suciu","Wang-Chiew Tan","Shiv Venkataraman","Matei Zaharia","Stanley B. Zdonik"],"url":"https://arxiv.org/abs/2504.11259"}
{"created":"2025-04-16","title":"Robust MPC for Uncertain Linear Systems -- Combining Model Adaptation and Iterative Learning","abstract":"This paper presents a robust adaptive learning Model Predictive Control (MPC) framework for linear systems with parametric uncertainties and additive disturbances performing iterative tasks. The approach iteratively refines the parameter estimates using set membership estimation. Performance enhancement over iterations is achieved by learning the terminal cost from data. Safety is enforced using a terminal set, which is also learned iteratively. The proposed method guarantees recursive feasibility, constraint satisfaction, and a robust bound on the closed-loop cost. Numerical simulations on a mass-spring-damper system demonstrate improved computational efficiency and control performance compared to an existing robust adaptive MPC approach.","authors":["Hannes Petrenz","Johannes K\\\"ohler","Francesco Borrelli"],"url":"https://arxiv.org/abs/2504.11261"}
{"created":"2025-04-16","title":"Enhanced Small Target Detection via Multi-Modal Fusion and Attention Mechanisms: A YOLOv5 Approach","abstract":"With the rapid development of information technology, modern warfare increasingly relies on intelligence, making small target detection critical in military applications. The growing demand for efficient, real-time detection has created challenges in identifying small targets in complex environments due to interference. To address this, we propose a small target detection method based on multi-modal image fusion and attention mechanisms. This method leverages YOLOv5, integrating infrared and visible light data along with a convolutional attention module to enhance detection performance. The process begins with multi-modal dataset registration using feature point matching, ensuring accurate network training. By combining infrared and visible light features with attention mechanisms, the model improves detection accuracy and robustness. Experimental results on anti-UAV and Visdrone datasets demonstrate the effectiveness and practicality of our approach, achieving superior detection results for small and dim targets.","authors":["Xiaoxiao Ma","Junxiong Tong"],"url":"https://arxiv.org/abs/2504.11262"}
{"created":"2025-04-16","title":"DeepSelective: Feature Gating and Representation Matching for Interpretable Clinical Prediction","abstract":"The rapid accumulation of Electronic Health Records (EHRs) has transformed healthcare by providing valuable data that enhance clinical predictions and diagnoses. While conventional machine learning models have proven effective, they often lack robust representation learning and depend heavily on expert-crafted features. Although deep learning offers powerful solutions, it is often criticized for its lack of interpretability. To address these challenges, we propose DeepSelective, a novel end to end deep learning framework for predicting patient prognosis using EHR data, with a strong emphasis on enhancing model interpretability. DeepSelective combines data compression techniques with an innovative feature selection approach, integrating custom-designed modules that work together to improve both accuracy and interpretability. Our experiments demonstrate that DeepSelective not only enhances predictive accuracy but also significantly improves interpretability, making it a valuable tool for clinical decision-making. The source code is freely available at http://www.healthinformaticslab.org/supp/resources.php .","authors":["Ruochi Zhang","Qian Yang","Xiaoyang Wang","Haoran Wu","Qiong Zhou","Yu Wang","Kewei Li","Yueying Wang","Yusi Fan","Jiale Zhang","Lan Huang","Chang Liu","Fengfeng Zhou"],"url":"https://arxiv.org/abs/2504.11264"}
{"created":"2025-04-16","title":"Single-Input Multi-Output Model Merging: Leveraging Foundation Models for Dense Multi-Task Learning","abstract":"Model merging is a flexible and computationally tractable approach to merge single-task checkpoints into a multi-task model. Prior work has solely focused on constrained multi-task settings where there is a one-to-one mapping between a sample and a task, overlooking the paradigm where multiple tasks may operate on the same sample, e.g., scene understanding. In this paper, we focus on the multi-task setting with single-input-multiple-outputs (SIMO) and show that it qualitatively differs from the single-input-single-output model merging settings studied in the literature due to the existence of task-specific decoders and diverse loss objectives. We identify that existing model merging methods lead to significant performance degradation, primarily due to representation misalignment between the merged encoder and task-specific decoders. We propose two simple and efficient fixes for the SIMO setting to re-align the feature representation after merging. Compared to joint fine-tuning, our approach is computationally effective and flexible, and sheds light into identifying task relationships in an offline manner. Experiments on NYUv2, Cityscapes, and a subset of the Taskonomy dataset demonstrate: (1) task arithmetic suffices to enable multi-task capabilities; however, the representations generated by the merged encoder has to be re-aligned with the task-specific heads; (2) the proposed architecture rivals traditional multi-task learning in performance but requires fewer samples and training steps by leveraging the existence of task-specific models.","authors":["Juan Garcia Giraldo","Nikolaos Dimitriadis","Ke Wang","Pascal Frossard"],"url":"https://arxiv.org/abs/2504.11268"}
{"created":"2025-04-16","title":"Distillation-Supervised Convolutional Low-Rank Adaptation for Efficient Image Super-Resolution","abstract":"Convolutional neural networks (CNNs) have been widely used in efficient image super-resolution. However, for CNN-based methods, performance gains often require deeper networks and larger feature maps, which increase complexity and inference costs. Inspired by LoRA's success in fine-tuning large language models, we explore its application to lightweight models and propose Distillation-Supervised Convolutional Low-Rank Adaptation (DSCLoRA), which improves model performance without increasing architectural complexity or inference costs. Specifically, we integrate ConvLoRA into the efficient SR network SPAN by replacing the SPAB module with the proposed SConvLB module and incorporating ConvLoRA layers into both the pixel shuffle block and its preceding convolutional layer. DSCLoRA leverages low-rank decomposition for parameter updates and employs a spatial feature affinity-based knowledge distillation strategy to transfer second-order statistical information from teacher models (pre-trained SPAN) to student models (ours). This method preserves the core knowledge of lightweight models and facilitates optimal solution discovery under certain conditions. Experiments on benchmark datasets show that DSCLoRA improves PSNR and SSIM over SPAN while maintaining its efficiency and competitive image quality. Notably, DSCLoRA ranked first in the Overall Performance Track of the NTIRE 2025 Efficient Super-Resolution Challenge. Our code and models are made publicly available at https://github.com/Yaozzz666/DSCF-SR.","authors":["Xinning Chai","Yao Zhang","Yuxuan Zhang","Zhengxue Cheng","Yingsheng Qin","Yucai Yang","Li Song"],"url":"https://arxiv.org/abs/2504.11271"}
{"created":"2025-04-16","title":"From Misleading Queries to Accurate Answers: A Three-Stage Fine-Tuning Method for LLMs","abstract":"Large language models (LLMs) exhibit excellent performance in natural language processing (NLP), but remain highly sensitive to the quality of input queries, especially when these queries contain misleading or inaccurate information. Existing methods focus on correcting the output, but they often overlook the potential of improving the ability of LLMs to detect and correct misleading content in the input itself. In this paper, we propose a novel three-stage fine-tuning method that enhances the ability of LLMs to detect and correct misleading information in the input, further improving response accuracy and reducing hallucinations. Specifically, the three stages include (1) training LLMs to identify misleading information, (2) training LLMs to correct the misleading information using built-in or external knowledge, and (3) training LLMs to generate accurate answers based on the corrected queries. To evaluate our method, we conducted experiments on three datasets for the hallucination detection task and the question answering (QA) task, as well as two datasets containing misleading information that we constructed. The experimental results demonstrate that our method significantly improves the accuracy and factuality of LLM responses, while also enhancing the ability to detect hallucinations and reducing the generation of hallucinations in the output, particularly when the query contains misleading information. We will publicly release our code upon acceptance.","authors":["Guocong Li","Weize Liu","Yihang Wu","Ping Wang","Shuaihan Huang","Hongxia Xu","Jian Wu"],"url":"https://arxiv.org/abs/2504.11277"}
{"created":"2025-04-16","title":"Towards dimensions and granularity in a unified workflow and data provenance framework","abstract":"Provenance information are essential for the traceability of scientific studies or experiments and thus crucial for ensuring the credibility and reproducibility of research findings. This paper discusses a comprehensive provenance framework combining the two types 1. workflow provenance, and 2. data provenance as well as their dimensions and granularity, which enables the answering of W7+1 provenance questions. We demonstrate the applicability by employing a biomedical research use case, that can be easily transferred into other scientific fields. An integration of these concepts into a unified framework enables credibility and reproducibility of the research findings.","authors":["Tanja Auge","Sascha Genehr","Meike Klettke and","Frank Kr\\\"uger","Max Schr\\\"oder"],"url":"https://arxiv.org/abs/2504.11278"}
{"created":"2025-04-16","title":"PGU-SGP: A Pheno-Geno Unified Surrogate Genetic Programming For Real-life Container Terminal Truck Scheduling","abstract":"Data-driven genetic programming (GP) has proven highly effective in solving combinatorial optimization problems under dynamic and uncertain environments. A central challenge lies in fast fitness evaluations on large training datasets, especially for complex real-world problems involving time-consuming simulations. Surrogate models, like phenotypic characterization (PC)-based K-nearest neighbors (KNN), have been applied to reduce computational cost. However, the PC-based similarity measure is confined to behavioral characteristics, overlooking genotypic differences, which can limit surrogate quality and impair performance. To address these issues, this paper proposes a pheno-geno unified surrogate GP algorithm, PGU-SGP, integrating phenotypic and genotypic characterization (GC) to enhance surrogate sample selection and fitness prediction. A novel unified similarity metric combining PC and GC distances is proposed, along with an effective and efficient GC representation. Experimental results of a real-life vehicle scheduling problem demonstrate that PGU-SGP reduces training time by approximately 76% while achieving comparable performance to traditional GP. With the same training time, PGU-SGP significantly outperforms traditional GP and the state-of-the-art algorithm on most datasets. Additionally, PGU-SGP shows faster convergence and improved surrogate quality by maintaining accurate fitness rankings and appropriate selection pressure, further validating its effectiveness.","authors":["Leshan Tan","Chenwei Jin","Xinan Chen","Rong Qu","Ruibin Bai"],"url":"https://arxiv.org/abs/2504.11280"}
{"created":"2025-04-16","title":"The Obvious Invisible Threat: LLM-Powered GUI Agents' Vulnerability to Fine-Print Injections","abstract":"A Large Language Model (LLM) powered GUI agent is a specialized autonomous system that performs tasks on the user's behalf according to high-level instructions. It does so by perceiving and interpreting the graphical user interfaces (GUIs) of relevant apps, often visually, inferring necessary sequences of actions, and then interacting with GUIs by executing the actions such as clicking, typing, and tapping. To complete real-world tasks, such as filling forms or booking services, GUI agents often need to process and act on sensitive user data. However, this autonomy introduces new privacy and security risks. Adversaries can inject malicious content into the GUIs that alters agent behaviors or induces unintended disclosures of private information. These attacks often exploit the discrepancy between visual saliency for agents and human users, or the agent's limited ability to detect violations of contextual integrity in task automation. In this paper, we characterized six types of such attacks, and conducted an experimental study to test these attacks with six state-of-the-art GUI agents, 234 adversarial webpages, and 39 human participants. Our findings suggest that GUI agents are highly vulnerable, particularly to contextually embedded threats. Moreover, human users are also susceptible to many of these attacks, indicating that simple human oversight may not reliably prevent failures. This misalignment highlights the need for privacy-aware agent design. We propose practical defense strategies to inform the development of safer and more reliable GUI agents.","authors":["Chaoran Chen","Zhiping Zhang","Bingcan Guo","Shang Ma","Ibrahim Khalilov","Simret A Gebreegziabher","Yanfang Ye","Ziang Xiao","Yaxing Yao","Tianshi Li","Toby Jia-Jun Li"],"url":"https://arxiv.org/abs/2504.11281"}
{"created":"2025-04-16","title":"Bipartite Ranking From Multiple Labels: On Loss Versus Label Aggregation","abstract":"Bipartite ranking is a fundamental supervised learning problem, with the goal of learning a ranking over instances with maximal area under the ROC curve (AUC) against a single binary target label. However, one may often observe multiple binary target labels, e.g., from distinct human annotators. How can one synthesize such labels into a single coherent ranking? In this work, we formally analyze two approaches to this problem -- loss aggregation and label aggregation -- by characterizing their Bayes-optimal solutions. Based on this, we show that while both methods can yield Pareto-optimal solutions, loss aggregation can exhibit label dictatorship: one can inadvertently (and undesirably) favor one label over others. This suggests that label aggregation can be preferable to loss aggregation, which we empirically verify.","authors":["Michal Lukasik","Lin Chen","Harikrishna Narasimhan","Aditya Krishna Menon","Wittawat Jitkrittum","Felix X. Yu","Sashank J. Reddi","Gang Fu","Mohammadhossein Bateni","Sanjiv Kumar"],"url":"https://arxiv.org/abs/2504.11284"}
{"created":"2025-04-16","title":"Balancing hydrogen delivery in national energy systems: impact of the temporal flexibility of hydrogen delivery on export prices","abstract":"Hydrogen is expected to play a key role in the energy transition. Analyses exploring the price of hydrogen usually calculate average or marginal production costs regardless of the time of delivery. A key factor that affects the price of hydrogen is the balancing costs, which we define as the expense of ensuring a steady schedule of hydrogen delivery. We explore the effect of delivering hydrogen to the export ports at different schedules, ranging from fully flexible to moderately stable with a daily and weekly buffer, to fully stable. We quantify the rise in hydrogen price with strict balancing constraint in three countries: Brazil, Morocco and Turkey, and three export volumes: 10, 50 and 200 TWh. The price difference between the flexible and stable schedules was found to reach a maximum of 36% in Brazil, 47% in Morocco and 18% in Turkey across the different export volumes.","authors":["Hazem Abdel-Khalek","Eddy Jalbout","Caspar Schau{\\ss}","Benjamin Pfluger"],"url":"https://arxiv.org/abs/2504.11285"}
{"created":"2025-04-16","title":"UniAnimate-DiT: Human Image Animation with Large-Scale Video Diffusion Transformer","abstract":"This report presents UniAnimate-DiT, an advanced project that leverages the cutting-edge and powerful capabilities of the open-source Wan2.1 model for consistent human image animation. Specifically, to preserve the robust generative capabilities of the original Wan2.1 model, we implement Low-Rank Adaptation (LoRA) technique to fine-tune a minimal set of parameters, significantly reducing training memory overhead. A lightweight pose encoder consisting of multiple stacked 3D convolutional layers is designed to encode motion information of driving poses. Furthermore, we adopt a simple concatenation operation to integrate the reference appearance into the model and incorporate the pose information of the reference image for enhanced pose alignment. Experimental results show that our approach achieves visually appearing and temporally consistent high-fidelity animations. Trained on 480p (832x480) videos, UniAnimate-DiT demonstrates strong generalization capabilities to seamlessly upscale to 720P (1280x720) during inference. The training and inference code is publicly available at https://github.com/ali-vilab/UniAnimate-DiT.","authors":["Xiang Wang","Shiwei Zhang","Longxiang Tang","Yingya Zhang","Changxin Gao","Yuehuan Wang","Nong Sang"],"url":"https://arxiv.org/abs/2504.11289"}
{"created":"2025-04-16","title":"Automated Python Translation","abstract":"Python is one of the most commonly used programming languages in industry and education. Its English keywords and built-in functions/modules allow it to come close to pseudo-code in terms of its readability and ease of writing. However, those who do not speak English may not experience these advantages. In fact, they may even be hindered in their ability to understand Python code, as the English nature of its terms creates an additional layer of overhead. To that end, we introduce the task of automatically translating Python's natural modality (keywords, error types, identifiers, etc.) into other human languages. This presents a unique challenge, considering the abbreviated nature of these forms, as well as potential untranslatability of advanced mathematical/programming concepts across languages. We therefore create an automated pipeline to translate Python into other human languages, comparing strategies using machine translation and large language models. We then use this pipeline to acquire translations from five common Python libraries (pytorch, pandas, tensorflow, numpy, and random) in seven languages, and do a quality test on a subset of these terms in French, Greek, and Bengali. We hope this will provide a clearer path forward towards creating a universal Python, accessible to anyone regardless of nationality or language background.","authors":["Joshua Otten","Antonios Anastasopoulos","Kevin Moran"],"url":"https://arxiv.org/abs/2504.11290"}
{"created":"2025-04-16","title":"Optimal finite element approximations of monotone semilinear elliptic PDE with subcritical nonlinearities","abstract":"We study iterative finite element approximations for the numerical approximation of semilinear elliptic boundary value problems with monotone nonlinear reactions of subcritical growth. The focus of our contribution is on an optimal a priori error estimate for a contractive Picard type iteration scheme on meshes that are locally refined towards possible corner singularities in polygonal domains. Our analysis involves, in particular, an elliptic regularity result in weighted Sobolev spaces and the use of the Trudinger inequality, which is instrumental in dealing with subcritically growing nonlinearities. A series of numerical experiments confirm the accuracy and efficiency of our method.","authors":["Florian Spicher","Thomas P. Wihler"],"url":"https://arxiv.org/abs/2504.11292"}
{"created":"2025-04-16","title":"Autoregressive Distillation of Diffusion Transformers","abstract":"Diffusion models with transformer architectures have demonstrated promising capabilities in generating high-fidelity images and scalability for high resolution. However, iterative sampling process required for synthesis is very resource-intensive. A line of work has focused on distilling solutions to probability flow ODEs into few-step student models. Nevertheless, existing methods have been limited by their reliance on the most recent denoised samples as input, rendering them susceptible to exposure bias. To address this limitation, we propose AutoRegressive Distillation (ARD), a novel approach that leverages the historical trajectory of the ODE to predict future steps. ARD offers two key benefits: 1) it mitigates exposure bias by utilizing a predicted historical trajectory that is less susceptible to accumulated errors, and 2) it leverages the previous history of the ODE trajectory as a more effective source of coarse-grained information. ARD modifies the teacher transformer architecture by adding token-wise time embedding to mark each input from the trajectory history and employs a block-wise causal attention mask for training. Furthermore, incorporating historical inputs only in lower transformer layers enhances performance and efficiency. We validate the effectiveness of ARD in a class-conditioned generation on ImageNet and T2I synthesis. Our model achieves a $5\\times$ reduction in FID degradation compared to the baseline methods while requiring only 1.1\\% extra FLOPs on ImageNet-256. Moreover, ARD reaches FID of 1.84 on ImageNet-256 in merely 4 steps and outperforms the publicly available 1024p text-to-image distilled models in prompt adherence score with a minimal drop in FID compared to the teacher. Project page: https://github.com/alsdudrla10/ARD.","authors":["Yeongmin Kim","Sotiris Anagnostidis","Yuming Du","Edgar Sch\\\"onfeld","Jonas Kohler","Markos Georgopoulos","Albert Pumarola","Ali Thabet","Artsiom Sanakoyeu"],"url":"https://arxiv.org/abs/2504.11295"}
{"created":"2025-04-16","title":"Learning to Be A Doctor: Searching for Effective Medical Agent Architectures","abstract":"Large Language Model (LLM)-based agents have demonstrated strong capabilities across a wide range of tasks, and their application in the medical domain holds particular promise due to the demand for high generalizability and reliance on interdisciplinary knowledge. However, existing medical agent systems often rely on static, manually crafted workflows that lack the flexibility to accommodate diverse diagnostic requirements and adapt to emerging clinical scenarios. Motivated by the success of automated machine learning (AutoML), this paper introduces a novel framework for the automated design of medical agent architectures. Specifically, we define a hierarchical and expressive agent search space that enables dynamic workflow adaptation through structured modifications at the node, structural, and framework levels. Our framework conceptualizes medical agents as graph-based architectures composed of diverse, functional node types and supports iterative self-improvement guided by diagnostic feedback. Experimental results on skin disease diagnosis tasks demonstrate that the proposed method effectively evolves workflow structures and significantly enhances diagnostic accuracy over time. This work represents the first fully automated framework for medical agent architecture design and offers a scalable, adaptable foundation for deploying intelligent agents in real-world clinical environments.","authors":["Yangyang Zhuang","Wenjia Jiang","Jiayu Zhang","Ze Yang","Joey Tianyi Zhou","Chi Zhang"],"url":"https://arxiv.org/abs/2504.11301"}
{"created":"2025-04-16","title":"CFIS-YOLO: A Lightweight Multi-Scale Fusion Network for Edge-Deployable Wood Defect Detection","abstract":"Wood defect detection is critical for ensuring quality control in the wood processing industry. However, current industrial applications face two major challenges: traditional methods are costly, subjective, and labor-intensive, while mainstream deep learning models often struggle to balance detection accuracy and computational efficiency for edge deployment. To address these issues, this study proposes CFIS-YOLO, a lightweight object detection model optimized for edge devices. The model introduces an enhanced C2f structure, a dynamic feature recombination module, and a novel loss function that incorporates auxiliary bounding boxes and angular constraints. These innovations improve multi-scale feature fusion and small object localization while significantly reducing computational overhead. Evaluated on a public wood defect dataset, CFIS-YOLO achieves a mean Average Precision (mAP@0.5) of 77.5\\%, outperforming the baseline YOLOv10s by 4 percentage points. On SOPHON BM1684X edge devices, CFIS-YOLO delivers 135 FPS, reduces power consumption to 17.3\\% of the original implementation, and incurs only a 0.5 percentage point drop in mAP. These results demonstrate that CFIS-YOLO is a practical and effective solution for real-world wood defect detection in resource-constrained environments.","authors":["Jincheng Kang","Yi Cen","Yigang Cen","Ke Wang","Yuhan Liu"],"url":"https://arxiv.org/abs/2504.11305"}
{"created":"2025-04-16","title":"Context-Aware Palmprint Recognition via a Relative Similarity Metric","abstract":"We propose a new approach to matching mechanism for palmprint recognition by introducing a Relative Similarity Metric (RSM) that enhances the robustness and discriminability of existing matching frameworks. While conventional systems rely on direct pairwise similarity measures, such as cosine or Euclidean distances, these metrics fail to capture how a pairwise similarity compares within the context of the entire dataset. Our method addresses this by evaluating the relative consistency of similarity scores across up to all identities, allowing for better suppression of false positives and negatives. Applied atop the CCNet architecture, our method achieves a new state-of-the-art 0.000036% Equal Error Rate (EER) on the Tongji dataset, outperforming previous methods and demonstrating the efficacy of incorporating relational structure into the palmprint matching process.","authors":["Trinnhallen Brisley","Aryan Gandhi","Joseph Magen"],"url":"https://arxiv.org/abs/2504.11306"}
{"created":"2025-04-16","title":"Uncertainty Estimation for Trust Attribution to Speed-of-Sound Reconstruction with Variational Networks","abstract":"Speed-of-sound (SoS) is a biomechanical characteristic of tissue, and its imaging can provide a promising biomarker for diagnosis. Reconstructing SoS images from ultrasound acquisitions can be cast as a limited-angle computed-tomography problem, with Variational Networks being a promising model-based deep learning solution. Some acquired data frames may, however, get corrupted by noise due to, e.g., motion, lack of contact, and acoustic shadows, which in turn negatively affects the resulting SoS reconstructions. We propose to use the uncertainty in SoS reconstructions to attribute trust to each individual acquired frame. Given multiple acquisitions, we then use an uncertainty based automatic selection among these retrospectively, to improve diagnostic decisions. We investigate uncertainty estimation based on Monte Carlo Dropout and Bayesian Variational Inference. We assess our automatic frame selection method for differential diagnosis of breast cancer, distinguishing between benign fibroadenoma and malignant carcinoma. We evaluate 21 lesions classified as BI-RADS~4, which represents suspicious cases for probable malignancy. The most trustworthy frame among four acquisitions of each lesion was identified using uncertainty based criteria. Selecting a frame informed by uncertainty achieved an area under curve of 76% and 80% for Monte Carlo Dropout and Bayesian Variational Inference, respectively, superior to any uncertainty-uninformed baselines with the best one achieving 64%. A novel use of uncertainty estimation is proposed for selecting one of multiple data acquisitions for further processing and decision making.","authors":["Sonia Laguna","Lin Zhang","Can Deniz Bezek","Monika Farkas","Dieter Schweizer","Rahel A. Kubik-Huch","Orcun Goksel"],"url":"https://arxiv.org/abs/2504.11307"}
{"created":"2025-04-16","title":"Big Brother is Watching: Proactive Deepfake Detection via Learnable Hidden Face","abstract":"As deepfake technologies continue to advance, passive detection methods struggle to generalize with various forgery manipulations and datasets. Proactive defense techniques have been actively studied with the primary aim of preventing deepfake operation effectively working. In this paper, we aim to bridge the gap between passive detection and proactive defense, and seek to solve the detection problem utilizing a proactive methodology. Inspired by several watermarking-based forensic methods, we explore a novel detection framework based on the concept of ``hiding a learnable face within a face''. Specifically, relying on a semi-fragile invertible steganography network, a secret template image is embedded into a host image imperceptibly, acting as an indicator monitoring for any malicious image forgery when being restored by the inverse steganography process. Instead of being manually specified, the secret template is optimized during training to resemble a neutral facial appearance, just like a ``big brother'' hidden in the image to be protected. By incorporating a self-blending mechanism and robustness learning strategy with a simulative transmission channel, a robust detector is built to accurately distinguish if the steganographic image is maliciously tampered or benignly processed. Finally, extensive experiments conducted on multiple datasets demonstrate the superiority of the proposed approach over competing passive and proactive detection methods.","authors":["Hongbo Li","Shangchao Yang","Ruiyang Xia","Lin Yuan","Xinbo Gao"],"url":"https://arxiv.org/abs/2504.11309"}
{"created":"2025-04-16","title":"Intelligent driving vehicle front multi-target tracking and detection based on YOLOv5 and point cloud 3D projection","abstract":"In multi-target tracking and detection tasks, it is necessary to continuously track multiple targets, such as vehicles, pedestrians, etc. To achieve this goal, the system must be able to continuously acquire and process image frames containing these targets. These consecutive frame images enable the algorithm to update the position and state of the target in real-time in each frame of the image. How to accurately associate the detected target with the target in the previous or next frame to form a stable trajectory is a complex problem. Therefore, a multi object tracking and detection method for intelligent driving vehicles based on YOLOv5 and point cloud 3D projection is proposed. Using Retinex algorithm to enhance the image of the environment in front of the vehicle, remove light interference in the image, and build an intelligent detection model based on YOLOv5 network structure. The enhanced image is input into the model, and multiple targets in front of the vehicle are identified through feature extraction and target localization. By combining point cloud 3D projection technology, the correlation between the position changes of adjacent frame images in the projection coordinate system can be inferred. By sequentially projecting the multi-target recognition results of multiple consecutive frame images into the 3D laser point cloud environment, effective tracking of the motion trajectories of all targets in front of the vehicle can be achieved. The experimental results show that the application of this method for intelligent driving vehicle front multi-target tracking and detection yields a MOTA (Tracking Accuracy) value greater than 30, demonstrating its superior tracking and detection performance.","authors":["Dayong Liu","Qingrui Zhang","Zeyang Meng"],"url":"https://arxiv.org/abs/2504.11310"}
{"created":"2025-04-16","title":"Sensitivity Analysis of State Space Models for Scrap Composition Estimation in EAF and BOF","abstract":"This study develops and analyzes linear and nonlinear state space models for estimating the elemental composition of scrap steel used in steelmaking, with applications to Electric Arc Furnace (EAF) and Basic Oxygen Furnace (BOF) processes. The models incorporate mass balance equations and are fitted using a modified Kalman filter for linear cases and the Unscented Kalman Filter (UKF) for nonlinear cases. Using Cu and Cr as representative elements, we assess the sensitivity of model predictions to measurement noise in key process variables, including steel mass, steel composition, scrap input mass, slag mass, and iron oxide fraction in slag. Results show that the models are robust to moderate noise levels in most variables, particularly when errors are below $10\\%$. However, accuracy significantly deteriorates with noise in slag mass estimation. These findings highlight the practical feasibility and limitations of applying state space models for real-time scrap composition estimation in industrial settings.","authors":["Yiqing Zhou","Karsten Naert","Dirk Nuyens"],"url":"https://arxiv.org/abs/2504.11319"}
{"created":"2025-04-16","title":"Optimizing LLM Inference: Fluid-Guided Online Scheduling with Memory Constraints","abstract":"Large Language Models (LLMs) are indispensable in today's applications, but their inference procedure -- generating responses by processing text in segments and using a memory-heavy Key-Value (KV) cache -- demands significant computational resources, particularly under memory constraints. This paper formulates LLM inference optimization as a multi-stage online scheduling problem where sequential prompt arrivals and KV cache growth render conventional scheduling ineffective. We develop a fluid dynamics approximation to provide a tractable benchmark that guides algorithm design. Building on this, we propose the Waiting for Accumulated Inference Threshold (WAIT) algorithm, which uses multiple thresholds to schedule incoming prompts optimally when output lengths are known, and extend it to Nested WAIT for cases with unknown output lengths. Theoretical analysis shows that both algorithms achieve near-optimal performance against the fluid benchmark in heavy traffic conditions, balancing throughput, latency, and Time to First Token (TTFT). Experiments with the Llama-7B model on an A100 GPU using both synthetic and real-world datasets demonstrate improved throughput and reduced latency relative to established baselines like vLLM and Sarathi. This work bridges operations research and machine learning, offering a rigorous framework for the efficient deployment of LLMs under memory constraints.","authors":["Ruicheng Ao","Gan Luo","David Simchi-Levi","Xinshang Wang"],"url":"https://arxiv.org/abs/2504.11320"}
{"created":"2025-04-16","title":"Subset-Contrastive Multi-Omics Network Embedding","abstract":"Motivation: Network-based analyses of omics data are widely used, and while many of these methods have been adapted to single-cell scenarios, they often remain memory- and space-intensive. As a result, they are better suited to batch data or smaller datasets. Furthermore, the application of network-based methods in multi-omics often relies on similarity-based networks, which lack structurally-discrete topologies. This limitation may reduce the effectiveness of graph-based methods that were initially designed for topologies with better defined structures. Results: We propose Subset-Contrastive multi-Omics Network Embedding (SCONE), a method that employs contrastive learning techniques on large datasets through a scalable subgraph contrastive approach. By exploiting the pairwise similarity basis of many network-based omics methods, we transformed this characteristic into a strength, developing an approach that aims to achieve scalable and effective analysis. Our method demonstrates synergistic omics integration for cell type clustering in single-cell data. Additionally, we evaluate its performance in a bulk multi-omics integration scenario, where SCONE performs comparable to the state-of-the-art despite utilising limited views of the original data. We anticipate that our findings will motivate further research into the use of subset contrastive methods for omics data.","authors":["Pedro Henrique da Costa Avelar","Min Wu","Sophia Tsoka"],"url":"https://arxiv.org/abs/2504.11321"}
{"created":"2025-04-16","title":"PVUW 2025 Challenge Report: Advances in Pixel-level Understanding of Complex Videos in the Wild","abstract":"This report provides a comprehensive overview of the 4th Pixel-level Video Understanding in the Wild (PVUW) Challenge, held in conjunction with CVPR 2025. It summarizes the challenge outcomes, participating methodologies, and future research directions. The challenge features two tracks: MOSE, which focuses on complex scene video object segmentation, and MeViS, which targets motion-guided, language-based video segmentation. Both tracks introduce new, more challenging datasets designed to better reflect real-world scenarios. Through detailed evaluation and analysis, the challenge offers valuable insights into the current state-of-the-art and emerging trends in complex video segmentation. More information can be found on the workshop website: https://pvuw.github.io/.","authors":["Henghui Ding","Chang Liu","Nikhila Ravi","Shuting He","Yunchao Wei","Song Bai","Philip Torr","Kehuan Song","Xinglin Xie","Kexin Zhang","Licheng Jiao","Lingling Li","Shuyuan Yang","Xuqiang Cao","Linnan Zhao","Jiaxuan Zhao","Fang Liu","Mengjiao Wang","Junpei Zhang","Xu Liu","Yuting Yang","Mengru Ma","Hao Fang","Runmin Cong","Xiankai Lu","Zhiyang Che","Wei Zhan","Tianming Liang","Haichao Jiang","Wei-Shi Zheng","Jian-Fang Hu","Haobo Yuan","Xiangtai Li","Tao Zhang","Lu Qi","Ming-Hsuan Yang"],"url":"https://arxiv.org/abs/2504.11326"}
{"created":"2025-04-16","title":"Dependency Structure Augmented Contextual Scoping Framework for Multimodal Aspect-Based Sentiment Analysis","abstract":"Multimodal Aspect-Based Sentiment Analysis (MABSA) seeks to extract fine-grained information from image-text pairs to identify aspect terms and determine their sentiment polarity. However, existing approaches often fall short in simultaneously addressing three core challenges: Sentiment Cue Perception (SCP), Multimodal Information Misalignment (MIM), and Semantic Noise Elimination (SNE). To overcome these limitations, we propose DASCO (\\textbf{D}ependency Structure \\textbf{A}ugmented \\textbf{Sco}ping Framework), a fine-grained scope-oriented framework that enhances aspect-level sentiment reasoning by leveraging dependency parsing trees. First, we designed a multi-task pretraining strategy for MABSA on our base model, combining aspect-oriented enhancement, image-text matching, and aspect-level sentiment-sensitive cognition. This improved the model's perception of aspect terms and sentiment cues while achieving effective image-text alignment, addressing key challenges like SCP and MIM. Furthermore, we incorporate dependency trees as syntactic branch combining with semantic branch, guiding the model to selectively attend to critical contextual elements within a target-specific scope while effectively filtering out irrelevant noise for addressing SNE problem. Extensive experiments on two benchmark datasets across three subtasks demonstrate that DASCO achieves state-of-the-art performance in MABSA, with notable gains in JMASA (+3.1\\% F1 and +5.4\\% precision on Twitter2015).","authors":["Hao Liu","Lijun He","Jiaxi Liang","Zhihan Ren","Fan Li"],"url":"https://arxiv.org/abs/2504.11331"}
{"created":"2025-04-16","title":"Implicit dual time-stepping positivity-preserving entropy-stable schemes for the compressible Navier-Stokes equations","abstract":"We generalize the explicit high-order positivity-preserving entropy-stable spectral collocation schemes developed in [30, 34] for the three-dimensional (3D) compressible Navier Stokes equations to a time implicit formulation. The time derivative terms are discretized by using the first- and second-order implicit backward difference formulas (BDF1 and BDF2) that are well suited for solving steady-state and time-dependent viscous flows at high Reynolds numbers, respectively. The nonlinear system of discrete equations at each physical timestep is solved by using a dual time-stepping technique. The proposed scheme is provably entropy-stable and positivity-preserving and provides unconditional stability properties in the physical time. Numerical results demonstrating accuracy and positivity-preserving properties of the new dual time-stepping scheme are presented for supersonic viscous flows with strong shock waves and contact discontinuities.","authors":["Mohammed Sayyari","Nail K. Yamaleev"],"url":"https://arxiv.org/abs/2504.11333"}
{"created":"2025-04-16","title":"A Mathematical Framework of Semantic Communication based on Category Theory","abstract":"While semantic communication (SemCom) has recently demonstrated great potential to enhance transmission efficiency and reliability by leveraging machine learning (ML) and knowledge base (KB), there is a lack of mathematical modeling to rigorously characterize SemCom system and quantify the performance gain obtained from ML and KB. In this paper, we develop a mathematical framework for SemCom based on category theory, rigorously model the concepts of semantic entities and semantic probability space. Within this framework, semantic entropy is introduced to quantify the uncertainty of semantic entities. We theoretically prove that semantic entropy can be effectively reduced by exploiting KB, which capture semantic dependencies. Specifically, semantic entities can be combined based on semantic ambiguity, and are encoded based on contextual relationships among them. Then we refine semantic channel capacity modeling, which considers the mutual information contained in KB to better reflect SemCom efficiency. Numerical simulations validate the effectiveness of the proposed framework, showing that SemCom with KB integration outperforms traditional communication in both entropy reduction and coding efficiency.","authors":["Shuheng Hua","Yao Sun","Kairong Ma","Muhammad Ali Imran"],"url":"https://arxiv.org/abs/2504.11334"}
{"created":"2025-04-16","title":"Code Reborn AI-Driven Legacy Systems Modernization from COBOL to Java","abstract":"This study investigates AI-driven modernization of legacy COBOL code into Java, addressing a critical challenge in aging software systems. Leveraging the Legacy COBOL 2024 Corpus -- 50,000 COBOL files from public and enterprise sources -- Java parses the code, AI suggests upgrades, and React visualizes gains. Achieving 93% accuracy, complexity drops 35% (from 18 to 11.7) and coupling 33% (from 8 to 5.4), surpassing manual efforts (75%) and rule-based tools (82%). The approach offers a scalable path to rejuvenate COBOL systems, vital for industries like banking and insurance.","authors":["Gopichand Bandarupalli"],"url":"https://arxiv.org/abs/2504.11335"}
{"created":"2025-04-16","title":"Looking beyond the next token","abstract":"The structure of causal language model training assumes that each token can be accurately predicted from the previous context. This contrasts with humans' natural writing and reasoning process, where goals are typically known before the exact argument or phrasings. While this mismatch has been well studied in the literature, the working assumption has been that architectural changes are needed to address this mismatch. We argue that rearranging and processing the training data sequences can allow models to more accurately imitate the true data-generating process, and does not require any other changes to the architecture or training infrastructure. We demonstrate that this technique, Trelawney, and the inference algorithms derived from it allow us to improve performance on several key benchmarks that span planning, algorithmic reasoning, and story generation tasks. Finally, our method naturally enables the generation of long-term goals at no additional cost. We investigate how using the model's goal-generation capability can further improve planning and reasoning. Additionally, we believe Trelawney could potentially open doors to new capabilities beyond the current language modeling paradigm.","authors":["Abitha Thankaraj","Yiding Jiang","J. Zico Kolter","Yonatan Bisk"],"url":"https://arxiv.org/abs/2504.11336"}
{"created":"2025-04-16","title":"REWARD CONSISTENCY: Improving Multi-Objective Alignment from a Data-Centric Perspective","abstract":"Multi-objective preference alignment in language models often encounters a challenging trade-off: optimizing for one human preference (e.g., helpfulness) frequently compromises others (e.g., harmlessness) due to the inherent conflicts between competing objectives. While prior work mainly focuses on algorithmic solutions, we explore a novel data-driven approach to uncover the types of data that can effectively mitigate these conflicts. Specifically, we propose the concept of Reward Consistency (RC), which identifies samples that align with multiple preference objectives, thereby reducing conflicts during training. Through gradient-based analysis, we demonstrate that RC-compliant samples inherently constrain performance degradation during multi-objective optimization. Building on these insights, we further develop Reward Consistency Sampling, a framework that automatically constructs preference datasets that effectively mitigate conflicts during multi-objective alignment. Our generated data achieves an average improvement of 13.37% in both the harmless rate and helpfulness win rate when optimizing harmlessness and helpfulness, and can consistently resolve conflicts in varying multi-objective scenarios.","authors":["Zhihao Xu","Yongqi Tong","Xin Zhang","Jun Zhou","Xiting Wang"],"url":"https://arxiv.org/abs/2504.11337"}
{"created":"2025-04-16","title":"Transformer-Based Model for Cold Start Mitigation in FaaS Architecture","abstract":"Serverless architectures, particularly the Function as a Service (FaaS) model, have become a cornerstone of modern cloud computing due to their ability to simplify resource management and enhance application deployment agility. However, a significant challenge remains: the cold start problem. This phenomenon occurs when an idle FaaS function is invoked, requiring a full initialization process, which increases latency and degrades user experience. Existing solutions for cold start mitigation are limited in terms of invocation pattern generalization and implementation complexity. In this study, we propose an innovative approach leveraging Transformer models to mitigate the impact of cold starts in FaaS architectures. Our solution excels in accurately modeling function initialization delays and optimizing serverless system performance. Experimental evaluation using a public dataset provided by Azure demonstrates a significant reduction in cold start times, reaching up to 79\\% compared to conventional methods.","authors":["Alexandre Savi Fayam Mbala Mouen","Jerry Lacmou Zeutouo","Vianney Kengne Tchendji"],"url":"https://arxiv.org/abs/2504.11338"}
{"created":"2025-04-16","title":"Optimal and Scalable Augmented Lagrangian preconditioners for Fictitious Domain problems","abstract":"We present optimal and scalable preconditioning techniques to solve linear systems of equations with a block two-by-two and three-by-three structure arising from fictitious domain problems and from finite element discretizations of immersed boundary methods. In particular, we propose two augmented Lagrangian-based preconditioners to accelerate the convergence of iterative solvers for these two classes of linear. We consider two relevant examples to illustrate the performance of these preconditioners when used in conjunction with flexible GMRES: the Poisson and the Stokes fictitious domain problems. A spectral analysis is established for both exact and inexact versions of these preconditioners. We show the effectiveness of the proposed approach and the robustness of our preconditioning strategy through extensive numerical tests in both two and three dimensions.","authors":["Michele Benzi","Marco Feder","Luca Heltai","Federica Mugnaioni"],"url":"https://arxiv.org/abs/2504.11339"}
{"created":"2025-04-16","title":"Evaluating DAO Sustainability and Longevity Through On-Chain Governance Metrics","abstract":"Decentralised Autonomous Organisations (DAOs) automate governance and resource allocation through smart contracts, aiming to shift decision-making to distributed token holders. However, many DAOs face sustainability challenges linked to limited user participation, concentrated voting power, and technical design constraints. This paper addresses these issues by identifying research gaps in DAO evaluation and introducing a framework of Key Performance Indicators (KPIs) that capture governance efficiency, financial robustness, decentralisation, and community engagement. We apply the framework to a custom-built dataset of real-world DAOs constructed from on-chain data and analysed using non-parametric methods. The results reveal recurring governance patterns, including low participation rates and high proposer concentration, which may undermine long-term viability. The proposed KPIs offer a replicable, data-driven method for assessing DAO governance structures and identifying potential areas for improvement. These findings support a multidimensional approach to evaluating decentralised systems and provide practical tools for researchers and practitioners working to improve the resilience and effectiveness of DAO-based governance models.","authors":["Silvio Meneguzzo","Claudio Schifanella","Valentina Gatteschi","Giuseppe Destefanis"],"url":"https://arxiv.org/abs/2504.11341"}
{"created":"2025-04-16","title":"A Minimalist Approach to LLM Reasoning: from Rejection Sampling to Reinforce","abstract":"Reinforcement learning (RL) has become a prevailing approach for fine-tuning large language models (LLMs) on complex reasoning tasks. Among recent methods, GRPO stands out for its empirical success in training models such as DeepSeek-R1, yet the sources of its effectiveness remain poorly understood. In this work, we revisit GRPO from a reinforce-like algorithm perspective and analyze its core components. Surprisingly, we find that a simple rejection sampling baseline, RAFT, which trains only on positively rewarded samples, yields competitive performance than GRPO and PPO. Our ablation studies reveal that GRPO's main advantage arises from discarding prompts with entirely incorrect responses, rather than from its reward normalization. Motivated by this insight, we propose Reinforce-Rej, a minimal extension of policy gradient that filters both entirely incorrect and entirely correct samples. Reinforce-Rej improves KL efficiency and stability, serving as a lightweight yet effective alternative to more complex RL algorithms. We advocate RAFT as a robust and interpretable baseline, and suggest that future advances should focus on more principled designs for incorporating negative samples, rather than relying on them indiscriminately. Our findings provide guidance for future work in reward-based LLM post-training.","authors":["Wei Xiong","Jiarui Yao","Yuhui Xu","Bo Pang","Lei Wang","Doyen Sahoo","Junnan Li","Nan Jiang","Tong Zhang","Caiming Xiong","Hanze Dong"],"url":"https://arxiv.org/abs/2504.11343"}
{"created":"2025-04-16","title":"Interpretable Hybrid-Rule Temporal Point Processes","abstract":"Temporal Point Processes (TPPs) are widely used for modeling event sequences in various medical domains, such as disease onset prediction, progression analysis, and clinical decision support. Although TPPs effectively capture temporal dynamics, their lack of interpretability remains a critical challenge. Recent advancements have introduced interpretable TPPs. However, these methods fail to incorporate numerical features, thereby limiting their ability to generate precise predictions. To address this issue, we propose Hybrid-Rule Temporal Point Processes (HRTPP), a novel framework that integrates temporal logic rules with numerical features, improving both interpretability and predictive accuracy in event modeling. HRTPP comprises three key components: basic intensity for intrinsic event likelihood, rule-based intensity for structured temporal dependencies, and numerical feature intensity for dynamic probability modulation. To effectively discover valid rules, we introduce a two-phase rule mining strategy with Bayesian optimization. To evaluate our method, we establish a multi-criteria assessment framework, incorporating rule validity, model fitting, and temporal predictive accuracy. Experimental results on real-world medical datasets demonstrate that HRTPP outperforms state-of-the-art interpretable TPPs in terms of predictive performance and clinical interpretability. In case studies, the rules extracted by HRTPP explain the disease progression, offering valuable contributions to medical diagnosis.","authors":["Yunyang Cao","Juekai Lin","Hongye Wang","Wenhao Li","Bo Jin"],"url":"https://arxiv.org/abs/2504.11344"}
{"created":"2025-04-16","title":"Erzeugunsgrad, VC-Dimension and Neural Networks with rational activation function","abstract":"The notion of Erzeugungsgrad was introduced by Joos Heintz in 1983 to bound the number of non-empty cells occurring after a process of quantifier elimination. We extend this notion and the combinatorial bounds of Theorem 2 in Heintz (1983) using the degree for constructible sets defined in Pardo-Sebasti\\'an (2022). We show that the Erzeugungsgrad is the key ingredient to connect affine Intersection Theory over algebraically closed fields and the VC-Theory of Computational Learning Theory for families of classifiers given by parameterized families of constructible sets. In particular, we prove that the VC-dimension and the Krull dimension are linearly related up to logarithmic factors based on Intersection Theory. Using this relation, we study the density of correct test sequences in evasive varieties. We apply these ideas to analyze parameterized families of neural networks with rational activation function.","authors":["Luis Miguel Pardo","Daniel Sebasti\\'an"],"url":"https://arxiv.org/abs/2504.11345"}
{"created":"2025-04-16","title":"Seedream 3.0 Technical Report","abstract":"We present Seedream 3.0, a high-performance Chinese-English bilingual image generation foundation model. We develop several technical improvements to address existing challenges in Seedream 2.0, including alignment with complicated prompts, fine-grained typography generation, suboptimal visual aesthetics and fidelity, and limited image resolutions. Specifically, the advancements of Seedream 3.0 stem from improvements across the entire pipeline, from data construction to model deployment. At the data stratum, we double the dataset using a defect-aware training paradigm and a dual-axis collaborative data-sampling framework. Furthermore, we adopt several effective techniques such as mixed-resolution training, cross-modality RoPE, representation alignment loss, and resolution-aware timestep sampling in the pre-training phase. During the post-training stage, we utilize diversified aesthetic captions in SFT, and a VLM-based reward model with scaling, thereby achieving outputs that well align with human preferences. Furthermore, Seedream 3.0 pioneers a novel acceleration paradigm. By employing consistent noise expectation and importance-aware timestep sampling, we achieve a 4 to 8 times speedup while maintaining image quality. Seedream 3.0 demonstrates significant improvements over Seedream 2.0: it enhances overall capabilities, in particular for text-rendering in complicated Chinese characters which is important to professional typography generation. In addition, it provides native high-resolution output (up to 2K), allowing it to generate images with high visual quality.","authors":["Yu Gao","Lixue Gong","Qiushan Guo","Xiaoxia Hou","Zhichao Lai","Fanshi Li","Liang Li","Xiaochen Lian","Chao Liao","Liyang Liu","Wei Liu","Yichun Shi","Shiqi Sun","Yu Tian","Zhi Tian","Peng Wang","Rui Wang","Xuanda Wang","Xun Wang","Ye Wang","Guofeng Wu","Jie Wu","Xin Xia","Xuefeng Xiao","Zhonghua Zhai","Xinyu Zhang","Qi Zhang","Yuwei Zhang","Shijia Zhao","Jianchao Yang","Weilin Huang"],"url":"https://arxiv.org/abs/2504.11346"}
{"created":"2025-04-16","title":"DeepWheel: Generating a 3D Synthetic Wheel Dataset for Design and Performance Evaluation","abstract":"Data-driven design is emerging as a powerful strategy to accelerate engineering innovation. However, its application to vehicle wheel design remains limited due to the lack of large-scale, high-quality datasets that include 3D geometry and physical performance metrics. To address this gap, this study proposes a synthetic design-performance dataset generation framework using generative AI. The proposed framework first generates 2D rendered images using Stable Diffusion, and then reconstructs the 3D geometry through 2.5D depth estimation. Structural simulations are subsequently performed to extract engineering performance data. To further expand the design and performance space, topology optimization is applied, enabling the generation of a more diverse set of wheel designs. The final dataset, named DeepWheel, consists of over 6,000 photo-realistic images and 900 structurally analyzed 3D models. This multi-modal dataset serves as a valuable resource for surrogate model training, data-driven inverse design, and design space exploration. The proposed methodology is also applicable to other complex design domains. The dataset is released under the Creative Commons Attribution-NonCommercial 4.0 International(CC BY-NC 4.0) and is available on the https://www.smartdesignlab.org/datasets","authors":["Soyoung Yoo","Namwoo Kang"],"url":"https://arxiv.org/abs/2504.11347"}
{"created":"2025-04-16","title":"Circuit metaconstruction in logspace for Rice-like complexity lower bounds in ANs and SGRs","abstract":"A new proof technique combining finite model theory and dynamical systems has recently been introduced to obtain general complexity lower bounds on any question one may formulate on the dynamics (seen as a graph) of a given automata network (AN). ANs are abstract finite dynamical systems of interacting entities whose evolution rules are encoded as circuits, hence the study also applies to succinct graph representations (SGRs). In this article, we detail the construction of circuits to obtain general complexity lower bounds (metareduction) and show that the reduction is feasible in logarithmic space.","authors":["Ali\\'enor Goubault-Larrecq","K\\'evin Perrot"],"url":"https://arxiv.org/abs/2504.11348"}
{"created":"2025-04-16","title":"Explicit and Implicit Representations in AI-based 3D Reconstruction for Radiology: A systematic literature review","abstract":"The demand for high-quality medical imaging in clinical practice and assisted diagnosis has made 3D reconstruction in radiological imaging a key research focus. Artificial intelligence (AI) has emerged as a promising approach to enhancing reconstruction accuracy while reducing acquisition and processing time, thereby minimizing patient radiation exposure and discomfort and ultimately benefiting clinical diagnosis. This review explores state-of-the-art AI-based 3D reconstruction algorithms in radiological imaging, categorizing them into explicit and implicit approaches based on their underlying principles. Explicit methods include point-based, volume-based, and Gaussian representations, while implicit methods encompass implicit prior embedding and neural radiance fields. Additionally, we examine commonly used evaluation metrics and benchmark datasets. Finally, we discuss the current state of development, key challenges, and future research directions in this evolving field. Our project available on: https://github.com/Bean-Young/AI4Med.","authors":["Yuezhe Yang","Boyu Yang","Yaqian Wang","Yang He","Xingbo Dong","Zhe Jin"],"url":"https://arxiv.org/abs/2504.11349"}
{"created":"2025-04-16","title":"An Adaptive Dropout Approach for High-Dimensional Bayesian Optimization","abstract":"Bayesian optimization (BO) is a widely used algorithm for solving expensive black-box optimization problems. However, its performance decreases significantly on high-dimensional problems due to the inherent high-dimensionality of the acquisition function. In the proposed algorithm, we adaptively dropout the variables of the acquisition function along the iterations. By gradually reducing the dimension of the acquisition function, the proposed approach has less and less difficulty to optimize the acquisition function. Numerical experiments demonstrate that AdaDropout effectively tackle high-dimensional challenges and improve solution quality where standard Bayesian optimization methods often struggle. Moreover, it achieves superior results when compared with state-of-the-art high-dimensional Bayesian optimization approaches. This work provides a simple yet efficient solution for high-dimensional expensive optimization.","authors":["Jundi Huang","Dawei Zhan"],"url":"https://arxiv.org/abs/2504.11353"}
{"created":"2025-04-16","title":"Kimina-Prover Preview: Towards Large Formal Reasoning Models with Reinforcement Learning","abstract":"We introduce Kimina-Prover Preview, a large language model that pioneers a novel reasoning-driven exploration paradigm for formal theorem proving, as showcased in this preview release. Trained with a large-scale reinforcement learning pipeline from Qwen2.5-72B, Kimina-Prover demonstrates strong performance in Lean 4 proof generation by employing a structured reasoning pattern we term \\textit{formal reasoning pattern}. This approach allows the model to emulate human problem-solving strategies in Lean, iteratively generating and refining proof steps. Kimina-Prover sets a new state-of-the-art on the miniF2F benchmark, reaching 80.7% with pass@8192. Beyond improved benchmark performance, our work yields several key insights: (1) Kimina-Prover exhibits high sample efficiency, delivering strong results even with minimal sampling (pass@1) and scaling effectively with computational budget, stemming from its unique reasoning pattern and RL training; (2) we demonstrate clear performance scaling with model size, a trend previously unobserved for neural theorem provers in formal mathematics; (3) the learned reasoning style, distinct from traditional search algorithms, shows potential to bridge the gap between formal verification and informal mathematical intuition. We open source distilled versions with 1.5B and 7B parameters of Kimina-Prover","authors":["Haiming Wang","Mert Unsal","Xiaohan Lin","Mantas Baksys","Junqi Liu","Marco Dos Santos","Flood Sung","Marina Vinyes","Zhenzhe Ying","Zekai Zhu","Jianqiao Lu","Hugues de Saxc\\'e","Bolton Bailey","Chendong Song","Chenjun Xiao","Dehao Zhang","Ebony Zhang","Frederick Pu","Han Zhu","Jiawei Liu","Jonas Bayer","Julien Michel","Longhui Yu","L\\'eo Dreyfus-Schmidt","Lewis Tunstall","Luigi Pagani","Moreira Machado","Pauline Bourigault","Ran Wang","Stanislas Polu","Thibaut Barroyer","Wen-Ding Li","Yazhe Niu","Yann Fleureau","Yangyang Hu","Zhouliang Yu","Zihan Wang","Zhilin Yang","Zhengying Liu","Jia Li"],"url":"https://arxiv.org/abs/2504.11354"}
{"created":"2025-04-16","title":"Neural Networks for on-chip Model Predictive Control: a Method to Build Optimized Training Datasets and its application to Type-1 Diabetes","abstract":"Training Neural Networks (NNs) to behave as Model Predictive Control (MPC) algorithms is an effective way to implement them in constrained embedded devices. By collecting large amounts of input-output data, where inputs represent system states and outputs are MPC-generated control actions, NNs can be trained to replicate MPC behavior at a fraction of the computational cost. However, although the composition of the training data critically influences the final NN accuracy, methods for systematically optimizing it remain underexplored. In this paper, we introduce the concept of Optimally-Sampled Datasets (OSDs) as ideal training sets and present an efficient algorithm for generating them. An OSD is a parametrized subset of all the available data that (i) preserves existing MPC information up to a certain numerical resolution, (ii) avoids duplicate or near-duplicate states, and (iii) becomes saturated or complete. We demonstrate the effectiveness of OSDs by training NNs to replicate the University of Virginia's MPC algorithm for automated insulin delivery in Type-1 Diabetes, achieving a four-fold improvement in final accuracy. Notably, two OSD-trained NNs received regulatory clearance for clinical testing as the first NN-based control algorithm for direct human insulin dosing. This methodology opens new pathways for implementing advanced optimizations on resource-constrained embedded platforms, potentially revolutionizing how complex algorithms are deployed.","authors":["Alberto Castillo","Elliot Pryor","Anas El Fathi","Boris Kovatchev","Marc Breton"],"url":"https://arxiv.org/abs/2504.11355"}
{"created":"2025-04-16","title":"DataSentinel: A Game-Theoretic Detection of Prompt Injection Attacks","abstract":"LLM-integrated applications and agents are vulnerable to prompt injection attacks, where an attacker injects prompts into their inputs to induce attacker-desired outputs. A detection method aims to determine whether a given input is contaminated by an injected prompt. However, existing detection methods have limited effectiveness against state-of-the-art attacks, let alone adaptive ones. In this work, we propose DataSentinel, a game-theoretic method to detect prompt injection attacks. Specifically, DataSentinel fine-tunes an LLM to detect inputs contaminated with injected prompts that are strategically adapted to evade detection. We formulate this as a minimax optimization problem, with the objective of fine-tuning the LLM to detect strong adaptive attacks. Furthermore, we propose a gradient-based method to solve the minimax optimization problem by alternating between the inner max and outer min problems. Our evaluation results on multiple benchmark datasets and LLMs show that DataSentinel effectively detects both existing and adaptive prompt injection attacks.","authors":["Yupei Liu","Yuqi Jia","Jinyuan Jia","Dawn Song","Neil Zhenqiang Gong"],"url":"https://arxiv.org/abs/2504.11358"}
{"created":"2025-04-16","title":"Teaching Large Language Models to Reason through Learning and Forgetting","abstract":"Leveraging inference-time search in large language models has proven effective in further enhancing a trained model's capability to solve complex mathematical and reasoning problems. However, this approach significantly increases computational costs and inference time, as the model must generate and evaluate multiple candidate solutions to identify a viable reasoning path. To address this, we propose an effective approach that integrates search capabilities directly into the model by fine-tuning it using both successful (learning) and failed reasoning paths (forgetting) derived from diverse search methods. While fine-tuning the model with these data might seem straightforward, we identify a critical issue: the model's search capability tends to degrade rapidly if fine-tuning is performed naively. We show that this degradation can be substantially mitigated by employing a smaller learning rate. Extensive experiments on the challenging Game-of-24 and Countdown mathematical reasoning benchmarks show that our approach not only outperforms both standard fine-tuning and inference-time search baselines but also significantly reduces inference time by 180$\\times$.","authors":["Tianwei Ni","Allen Nie","Sapana Chaudhary","Yao Liu","Huzefa Rangwala","Rasool Fakoor"],"url":"https://arxiv.org/abs/2504.11364"}
{"created":"2025-04-16","title":"A Decade of Wheat Mapping for Lebanon","abstract":"Wheat accounts for approximately 20% of the world's caloric intake, making it a vital component of global food security. Given this importance, mapping wheat fields plays a crucial role in enabling various stakeholders, including policy makers, researchers, and agricultural organizations, to make informed decisions regarding food security, supply chain management, and resource allocation. In this paper, we tackle the problem of accurately mapping wheat fields out of satellite images by introducing an improved pipeline for winter wheat segmentation, as well as presenting a case study on a decade-long analysis of wheat mapping in Lebanon. We integrate a Temporal Spatial Vision Transformer (TSViT) with Parameter-Efficient Fine Tuning (PEFT) and a novel post-processing pipeline based on the Fields of The World (FTW) framework. Our proposed pipeline addresses key challenges encountered in existing approaches, such as the clustering of small agricultural parcels in a single large field. By merging wheat segmentation with precise field boundary extraction, our method produces geometrically coherent and semantically rich maps that enable us to perform in-depth analysis such as tracking crop rotation pattern over years. Extensive evaluations demonstrate improved boundary delineation and field-level precision, establishing the potential of the proposed framework in operational agricultural monitoring and historical trend analysis. By allowing for accurate mapping of wheat fields, this work lays the foundation for a range of critical studies and future advances, including crop monitoring and yield estimation.","authors":["Hasan Wehbi","Hasan Nasrallah","Mohamad Hasan Zahweh","Zeinab Takach","Veera Ganesh Yalla","Ali J. Ghandour"],"url":"https://arxiv.org/abs/2504.11366"}
{"created":"2025-04-16","title":"From Gaze to Insight: Bridging Human Visual Attention and Vision Language Model Explanation for Weakly-Supervised Medical Image Segmentation","abstract":"Medical image segmentation remains challenging due to the high cost of pixel-level annotations for training. In the context of weak supervision, clinician gaze data captures regions of diagnostic interest; however, its sparsity limits its use for segmentation. In contrast, vision-language models (VLMs) provide semantic context through textual descriptions but lack the explanation precision required. Recognizing that neither source alone suffices, we propose a teacher-student framework that integrates both gaze and language supervision, leveraging their complementary strengths. Our key insight is that gaze data indicates where clinicians focus during diagnosis, while VLMs explain why those regions are significant. To implement this, the teacher model first learns from gaze points enhanced by VLM-generated descriptions of lesion morphology, establishing a foundation for guiding the student model. The teacher then directs the student through three strategies: (1) Multi-scale feature alignment to fuse visual cues with textual semantics; (2) Confidence-weighted consistency constraints to focus on reliable predictions; (3) Adaptive masking to limit error propagation in uncertain areas. Experiments on the Kvasir-SEG, NCI-ISBI, and ISIC datasets show that our method achieves Dice scores of 80.78%, 80.53%, and 84.22%, respectively-improving 3-5% over gaze baselines without increasing the annotation burden. By preserving correlations among predictions, gaze data, and lesion descriptions, our framework also maintains clinical interpretability. This work illustrates how integrating human visual attention with AI-generated semantic context can effectively overcome the limitations of individual weak supervision signals, thereby advancing the development of deployable, annotation-efficient medical AI systems. Code is available at: https://github.com/jingkunchen/FGI.git.","authors":["Jingkun Chen","Haoran Duan","Xiao Zhang","Boyan Gao","Tao Tan","Vicente Grau","Jungong Han"],"url":"https://arxiv.org/abs/2504.11368"}
{"created":"2025-04-16","title":"OpenTuringBench: An Open-Model-based Benchmark and Framework for Machine-Generated Text Detection and Attribution","abstract":"Open Large Language Models (OLLMs) are increasingly leveraged in generative AI applications, posing new challenges for detecting their outputs. We propose OpenTuringBench, a new benchmark based on OLLMs, designed to train and evaluate machine-generated text detectors on the Turing Test and Authorship Attribution problems. OpenTuringBench focuses on a representative set of OLLMs, and features a number of challenging evaluation tasks, including human/machine-manipulated texts, out-of-domain texts, and texts from previously unseen models. We also provide OTBDetector, a contrastive learning framework to detect and attribute OLLM-based machine-generated texts. Results highlight the relevance and varying degrees of difficulty of the OpenTuringBench tasks, with our detector achieving remarkable capabilities across the various tasks and outperforming most existing detectors. Resources are available on the OpenTuringBench Hugging Face repository at https://huggingface.co/datasets/MLNTeam-Unical/OpenTuringBench","authors":["Lucio La Cava","Andrea Tagarelli"],"url":"https://arxiv.org/abs/2504.11369"}
{"created":"2025-04-16","title":"Cancer-Myth: Evaluating AI Chatbot on Patient Questions with False Presuppositions","abstract":"Cancer patients are increasingly turning to large language models (LLMs) as a new form of internet search for medical information, making it critical to assess how well these models handle complex, personalized questions. However, current medical benchmarks focus on medical exams or consumer-searched questions and do not evaluate LLMs on real patient questions with detailed clinical contexts. In this paper, we first evaluate LLMs on cancer-related questions drawn from real patients, reviewed by three hematology oncology physicians. While responses are generally accurate, with GPT-4-Turbo scoring 4.13 out of 5, the models frequently fail to recognize or address false presuppositions in the questions-posing risks to safe medical decision-making. To study this limitation systematically, we introduce Cancer-Myth, an expert-verified adversarial dataset of 585 cancer-related questions with false presuppositions. On this benchmark, no frontier LLM -- including GPT-4o, Gemini-1.Pro, and Claude-3.5-Sonnet -- corrects these false presuppositions more than 30% of the time. Even advanced medical agentic methods do not prevent LLMs from ignoring false presuppositions. These findings expose a critical gap in the clinical reliability of LLMs and underscore the need for more robust safeguards in medical AI systems.","authors":["Wang Bill Zhu","Tianqi Chen","Ching Ying Lin","Jade Law","Mazen Jizzini","Jorge J. Nieva","Ruishan Liu","Robin Jia"],"url":"https://arxiv.org/abs/2504.11373"}
{"created":"2025-04-16","title":"A Winner-Takes-All Mechanism for Event Generation","abstract":"We present a novel framework for central pattern generator design that leverages the intrinsic rebound excitability of neurons in combination with winner-takes-all computation. Our approach unifies decision-making and rhythmic pattern generation within a simple yet powerful network architecture that employs all-to-all inhibitory connections enhanced by designable excitatory interactions. This design offers significant advantages regarding ease of implementation, adaptability, and robustness. We demonstrate its efficacy through a ring oscillator model, which exhibits adaptive phase and frequency modulation, making the framework particularly promising for applications in neuromorphic systems and robotics.","authors":["Yongkang Huo","Fuvio Forni","Rodolphe Sepulchre"],"url":"https://arxiv.org/abs/2504.11374"}
{"created":"2025-04-16","title":"A Multi-Stage Potts Machine based on Coupled CMOS Ring Oscillators","abstract":"This work presents a multi-stage coupled ring oscillator","authors":["Yilmaz Ege Gonul","Baris Taskin"],"url":"https://arxiv.org/abs/2504.11376"}
{"created":"2025-04-16","title":"Improving Swimming Performance in Soft Robotic Fish with Distributed Muscles and Embedded Kinematic Sensing","abstract":"Bio-inspired underwater vehicles could yield improved efficiency, maneuverability, and environmental compatibility over conventional propeller-driven underwater vehicles. However, to realize the swimming performance of biology, there is a need for soft robotic swimmers with both distributed muscles and kinematic feedback. This study presents the design and swimming performance of a soft robotic fish with independently controllable muscles and embedded kinematic sensing distributed along the body. The soft swimming robot consists of an interior flexible spine, three axially distributed sets of HASEL artificial muscles, embedded strain gauges, a streamlined silicone body, and off-board electronics. In a fixed configuration, the soft robot generates a maximum thrust of 7.9 mN when excited near its first resonant frequency (2 Hz) with synchronized antagonistic actuation of all muscles. When excited near its second resonant frequency (8 Hz), synchronized muscle actuation generates 5.0 mN of thrust. By introducing a sequential phase offset into the muscle actuation, the thrust at the second resonant frequency increases to 7.2 mN, a 44% increase from simple antagonistic activation. The sequential muscle activation improves the thrust by increasing 1) the tail-beat velocity and 2) traveling wave content in the swimming kinematics by four times. Further, the second resonant frequency (8 Hz) generates nearly as much thrust as the first resonance (2 Hz) while requiring only $\\approx25$% of the tail displacement, indicating that higher resonant frequencies have benefits for swimming in confined environments where a smaller kinematic envelope is necessary. These results demonstrate the performance benefits of independently controllable muscles and distributed kinematic sensing, and this type of soft robotic swimmer provides a platform to address the open challenge of sensorimotor control.","authors":["Kevin Soto","Isabel Hess","Brandon Schrader","Shan He","Patrick Musgrave"],"url":"https://arxiv.org/abs/2504.11377"}
{"created":"2025-04-16","title":"Omni$^2$: Unifying Omnidirectional Image Generation and Editing in an Omni Model","abstract":"$360^{\\circ}$ omnidirectional images (ODIs) have gained considerable attention recently, and are widely used in various virtual reality (VR) and augmented reality (AR) applications. However, capturing such images is expensive and requires specialized equipment, making ODI synthesis increasingly important. While common 2D image generation and editing methods are rapidly advancing, these models struggle to deliver satisfactory results when generating or editing ODIs due to the unique format and broad 360$^{\\circ}$ Field-of-View (FoV) of ODIs. To bridge this gap, we construct \\textbf{\\textit{Any2Omni}}, the first comprehensive ODI generation-editing dataset comprises 60,000+ training data covering diverse input conditions and up to 9 ODI generation and editing tasks. Built upon Any2Omni, we propose an \\textbf{\\underline{Omni}} model for \\textbf{\\underline{Omni}}-directional image generation and editing (\\textbf{\\textit{Omni$^2$}}), with the capability of handling various ODI generation and editing tasks under diverse input conditions using one model. Extensive experiments demonstrate the superiority and effectiveness of the proposed Omni$^2$ model for both the ODI generation and editing tasks.","authors":["Liu Yang","Huiyu Duan","Yucheng Zhu","Xiaohong Liu","Lu Liu","Zitong Xu","Guangji Ma","Xiongkuo Min","Guangtao Zhai","Patrick Le Callet"],"url":"https://arxiv.org/abs/2504.11379"}
{"created":"2025-04-16","title":"Speak with Confidence: Designing an Augmented Reality Training Tool for Public Speaking","abstract":"Public speaking anxiety affects many individuals, yet opportunities for real-world practice remain limited. This study explores how augmented reality (AR) can provide an accessible training environment for public speaking. Drawing from literature on public speaking, VR-based training, self-efficacy, and behavioral feedback mechanisms, we designed SpeakAR, an AR-based tool that simulates audience interaction through virtual models. SpeakAR was evaluated with five participants of varying anxiety levels, each completing six speaking tasks. Results indicate that AR exposure can enhance confidence, with participants finding the system useful for practice. Feedback highlighted the importance of dynamic facial expressions and idle animations in virtual models to improve realism and engagement. Our findings contribute to the design of AR-based training tools for public speaking, offering insights into how immersive environments can support skill development and anxiety reduction.","authors":["Mark Edison Jim","Jan Benjamin Yap","Gian Chill Laolao","Andrei Zachary Lim","Jordan Aiko Deja"],"url":"https://arxiv.org/abs/2504.11380"}
{"created":"2025-04-16","title":"RankAlign: A Ranking View of the Generator-Validator Gap in Large Language Models","abstract":"Although large language models (LLMs) have become generally more capable and accurate across many tasks, some fundamental sources of unreliability remain in their behavior. One key limitation is their inconsistency at reporting the the same information when prompts are changed. In this paper, we consider the discrepancy between a model's generated answer and their own verification of that answer, the generator-validator gap. We define this gap in a more stringent way than prior work: we expect correlation of scores from a generator and a validator over the entire set of candidate answers. We show that according to this measure, a large gap exists in various settings, including question answering, lexical semantics tasks, and next-word prediction. We then propose RankAlign, a ranking-based training method, and show that it significantly closes the gap by 31.8% on average, surpassing all baseline methods. Moreover, this approach generalizes well to out-of-domain tasks and lexical items.","authors":["Juan Diego Rodriguez","Wenxuan Ding","Katrin Erk","Greg Durrett"],"url":"https://arxiv.org/abs/2504.11381"}
{"created":"2025-04-16","title":"Accelerating Multiscale Modeling with Hybrid Solvers: Coupling FEM and Neural Operators with Domain Decomposition","abstract":"Numerical solvers for partial differential equations (PDEs) face challenges balancing computational cost and accuracy, especially in multiscale and dynamic systems. Neural operators can significantly speed up simulations; however, they often face challenges such as error accumulation and limited generalization in multiphysics problems. This work introduces a novel hybrid framework that integrates physics-informed DeepONet with FEM through domain decomposition. The core innovation lies in adaptively coupling FEM and DeepONet subdomains via a Schwarz alternating method. This methodology strategically allocates computationally demanding regions to a pre-trained Deep Operator Network, while the remaining computational domain is solved through FEM. To address dynamic systems, we integrate the Newmark time-stepping scheme directly into the DeepONet, significantly mitigating error accumulation in long-term simulations. Furthermore, an adaptive subdomain evolution enables the ML-resolved region to expand dynamically, capturing emerging fine-scale features without remeshing. The framework's efficacy has been validated across a range of solid mechanics problems, including static, quasi-static, and dynamic regimes, demonstrating accelerated convergence rates (up to 20% improvement compared to FE-FE approaches), while preserving solution fidelity with error < 1%. Our case studies show that our proposed hybrid solver: (1) maintains solution continuity across subdomain interfaces, (2) reduces computational costs by eliminating fine mesh requirements, (3) mitigates error accumulation in time-dependent simulations, and (4) enables automatic adaptation to evolving physical phenomena. This work bridges the gap between numerical methods and AI-driven surrogates, offering a scalable pathway for high-fidelity simulations in engineering and scientific applications.","authors":["Wei Wanga","Maryam Hakimzadeh","Haihui Ruan","Somdatta Goswami"],"url":"https://arxiv.org/abs/2504.11383"}
{"created":"2025-04-16","title":"Trajectory Encoding Temporal Graph Networks","abstract":"Temporal Graph Networks (TGNs) have demonstrated significant success in dynamic graph tasks such as link prediction and node classification. Both tasks comprise transductive settings, where the model predicts links among known nodes, and in inductive settings, where it generalises learned patterns to previously unseen nodes. Existing TGN designs face a dilemma under these dual scenarios. Anonymous TGNs, which rely solely on temporal and structural information, offer strong inductive generalisation but struggle to distinguish known nodes. In contrast, non-anonymous TGNs leverage node features to excel in transductive tasks yet fail to adapt to new nodes. To address this challenge, we propose Trajectory Encoding TGN (TETGN). Our approach introduces automatically expandable node identifiers (IDs) as learnable temporal positional features and performs message passing over these IDs to capture each node's historical context. By integrating this trajectory-aware module with a standard TGN using multi-head attention, TETGN effectively balances transductive accuracy with inductive generalisation. Experimental results on three real-world datasets show that TETGN significantly outperforms strong baselines on both link prediction and node classification tasks, demonstrating its ability to unify the advantages of anonymous and non-anonymous models for dynamic graph learning.","authors":["Jiafeng Xiong","Rizos Sakellariou"],"url":"https://arxiv.org/abs/2504.11386"}
{"created":"2025-04-16","title":"VideoPanda: Video Panoramic Diffusion with Multi-view Attention","abstract":"High resolution panoramic video content is paramount for immersive experiences in Virtual Reality, but is non-trivial to collect as it requires specialized equipment and intricate camera setups. In this work, we introduce VideoPanda, a novel approach for synthesizing 360$^\\circ$ videos conditioned on text or single-view video data. VideoPanda leverages multi-view attention layers to augment a video diffusion model, enabling it to generate consistent multi-view videos that can be combined into immersive panoramic content. VideoPanda is trained jointly using two conditions: text-only and single-view video, and supports autoregressive generation of long-videos. To overcome the computational burden of multi-view video generation, we randomly subsample the duration and camera views used during training and show that the model is able to gracefully generalize to generating more frames during inference. Extensive evaluations on both real-world and synthetic video datasets demonstrate that VideoPanda generates more realistic and coherent 360$^\\circ$ panoramas across all input conditions compared to existing methods. Visit the project website at https://research-staging.nvidia.com/labs/toronto-ai/VideoPanda/ for results.","authors":["Kevin Xie","Amirmojtaba Sabour","Jiahui Huang","Despoina Paschalidou","Greg Klar","Umar Iqbal","Sanja Fidler","Xiaohui Zeng"],"url":"https://arxiv.org/abs/2504.11389"}
{"created":"2025-04-16","title":"DataDecide: How to Predict Best Pretraining Data with Small Experiments","abstract":"Because large language models are expensive to pretrain on different datasets, using smaller-scale experiments to decide on data is crucial for reducing costs. Which benchmarks and methods of making decisions from observed performance at small scale most accurately predict the datasets that yield the best large models? To empower open exploration of this question, we release models, data, and evaluations in DataDecide -- the most extensive open suite of models over differences in data and scale. We conduct controlled pretraining experiments across 25 corpora with differing sources, deduplication, and filtering up to 100B tokens, model sizes up to 1B parameters, and 3 random seeds. We find that the ranking of models at a single, small size (e.g., 150M parameters) is a strong baseline for predicting best models at our larger target scale (1B) (~80% of com parisons correct). No scaling law methods among 8 baselines exceed the compute-decision frontier of single-scale predictions, but DataDecide can measure improvement in future scaling laws. We also identify that using continuous likelihood metrics as proxies in small experiments makes benchmarks including MMLU, ARC, HellaSwag, MBPP, and HumanEval >80% predictable at the target 1B scale with just 0.01% of the compute.","authors":["Ian Magnusson","Nguyen Tai","Ben Bogin","David Heineman","Jena D. Hwang","Luca Soldaini","Akshita Bhagia","Jiacheng Liu","Dirk Groeneveld","Oyvind Tafjord","Noah A. Smith","Pang Wei Koh","Jesse Dodge"],"url":"https://arxiv.org/abs/2504.11393"}
{"created":"2025-04-16","title":"Property Inheritance for Subtensors in Tensor Train Decompositions","abstract":"Tensor dimensionality reduction is one of the fundamental tools for modern data science. To address the high computational overhead, fiber-wise sampled subtensors that preserve the original tensor rank are often used in designing efficient and scalable tensor dimensionality reduction. However, the theory of property inheritance for subtensors is still underdevelopment, that is, how the essential properties of the original tensor will be passed to its subtensors. This paper theoretically studies the property inheritance of the two key tensor properties, namely incoherence and condition number, under the tensor train setting. We also show how tensor train rank is preserved through fiber-wise sampling. The key parameters introduced in theorems are numerically evaluated under various settings. The results show that the properties of interest can be well preserved to the subtensors formed via fiber-wise sampling. Overall, this paper provides several handy analytic tools for developing efficient tensor analysis","authors":["HanQin Cai","Longxiu Huang"],"url":"https://arxiv.org/abs/2504.11396"}
{"created":"2025-04-16","title":"MLPs and KANs for data-driven learning in physical problems: A performance comparison","abstract":"There is increasing interest in solving partial differential equations (PDEs) by casting them as machine learning problems. Recently, there has been a spike in exploring Kolmogorov-Arnold Networks (KANs) as an alternative to traditional neural networks represented by Multi-Layer Perceptrons (MLPs). While showing promise, their performance advantages in physics-based problems remain largely unexplored. Several critical questions persist: Can KANs capture complex physical dynamics and under what conditions might they outperform traditional architectures? In this work, we present a comparative study of KANs and MLPs for learning physical systems governed by PDEs. We assess their performance when applied in deep operator networks (DeepONet) and graph network-based simulators (GNS), and test them on physical problems that vary significantly in scale and complexity. Drawing inspiration from the Kolmogorov Representation Theorem, we examine the behavior of KANs and MLPs across shallow and deep network architectures. Our results reveal that although KANs do not consistently outperform MLPs when configured as deep neural networks, they demonstrate superior expressiveness in shallow network settings, significantly outpacing MLPs in accuracy over our test cases. This suggests that KANs are a promising choice, offering a balance of efficiency and accuracy in applications involving physical systems.","authors":["Raghav Pant","Sikan Li","Xingjian Li","Hassan Iqbal","Krishna Kumar"],"url":"https://arxiv.org/abs/2504.11397"}
{"created":"2025-04-16","title":"Breaking a Long-Standing Barrier: 2-$\\varepsilon$ Approximation for Steiner Forest","abstract":"The Steiner Forest problem, also known as the Generalized Steiner Tree problem, is a fundamental optimization problem on edge-weighted graphs where, given a set of vertex pairs, the goal is to select a minimum-cost subgraph such that each pair is connected. This problem generalizes the Steiner Tree problem, first introduced in 1811, for which the best known approximation factor is 1.39 [Byrka, Grandoni, Rothvo{\\ss}, and Sanit\\`a, 2010] (Best Paper award, STOC 2010).","authors":["Ali Ahmadi","Iman Gholami","MohammadTaghi Hajiaghayi","Peyman Jabbarzade","Mohammad Mahdavi"],"url":"https://arxiv.org/abs/2504.11398"}
{"created":"2025-04-16","title":"FlowUnits: Extending Dataflow for the Edge-to-Cloud Computing Continuum","abstract":"This paper introduces FlowUnits, a novel programming and deployment model that extends the traditional dataflow paradigm to address the unique challenges of edge-to-cloud computing environments. While conventional dataflow systems offer significant advantages for large-scale data processing in homogeneous cloud settings, they fall short when deployed across distributed, heterogeneous infrastructures. FlowUnits addresses three critical limitations of current approaches: lack of locality awareness, insufficient resource adaptation, and absence of dynamic update mechanisms. FlowUnits organize processing operators into cohesive, independently manageable components that can be transparently replicated across different regions, efficiently allocated on nodes with appropriate hardware capabilities, and dynamically updated without disrupting ongoing computations. We implement and evaluate the FlowUnits model within Renoir, an existing dataflow system, demonstrating significant improvements in deployment flexibility and resource utilization across the computing continuum. Our approach maintains the simplicity of dataflow while enabling seamless integration of edge and cloud resources into unified data processing pipelines.","authors":["Fabio Chini","Luca De Martini","Alessandro Margara","Gianpaolo Cugola"],"url":"https://arxiv.org/abs/2504.11400"}
{"created":"2025-04-16","title":"Multi-level Cellular Automata for FLIM networks","abstract":"The necessity of abundant annotated data and complex network architectures presents a significant challenge in deep-learning Salient Object Detection (deep SOD) and across the broader deep-learning landscape. This challenge is particularly acute in medical applications in developing countries with limited computational resources. Combining modern and classical techniques offers a path to maintaining competitive performance while enabling practical applications. Feature Learning from Image Markers (FLIM) methodology empowers experts to design convolutional encoders through user-drawn markers, with filters learned directly from these annotations. Recent findings demonstrate that coupling a FLIM encoder with an adaptive decoder creates a flyweight network suitable for SOD, requiring significantly fewer parameters than lightweight models and eliminating the need for backpropagation. Cellular Automata (CA) methods have proven successful in data-scarce scenarios but require proper initialization -- typically through user input, priors, or randomness. We propose a practical intersection of these approaches: using FLIM networks to initialize CA states with expert knowledge without requiring user interaction for each image. By decoding features from each level of a FLIM network, we can initialize multiple CAs simultaneously, creating a multi-level framework. Our method leverages the hierarchical knowledge encoded across different network layers, merging multiple saliency maps into a high-quality final output that functions as a CA ensemble. Benchmarks across two challenging medical datasets demonstrate the competitiveness of our multi-level CA approach compared to established models in the deep SOD literature.","authors":["Felipe Crispim Salvagnini","Jancarlo F. Gomes","Cid A. N. Santos","Silvio Jamil F. Guimar\\~aes","Alexandre X. Falc\\~ao"],"url":"https://arxiv.org/abs/2504.11406"}
{"created":"2025-04-16","title":"Efficient Hybrid Language Model Compression through Group-Aware SSM Pruning","abstract":"Hybrid LLM architectures that combine Attention and State Space Models (SSMs) achieve state-of-the-art accuracy and runtime performance. Recent work has demonstrated that applying compression and distillation to Attention-only models yields smaller, more accurate models at a fraction of the training cost. In this work, we explore the effectiveness of compressing Hybrid architectures. We introduce a novel group-aware pruning strategy that preserves the structural integrity of SSM blocks and their sequence modeling capabilities. Furthermore, we demonstrate the necessity of such SSM pruning to achieve improved accuracy and inference speed compared to traditional approaches. Our compression recipe combines SSM, FFN, embedding dimension, and layer pruning, followed by knowledge distillation-based retraining, similar to the MINITRON technique. Using this approach, we compress the Nemotron-H 8B Hybrid model down to 4B parameters with up to 40x fewer training tokens. The resulting model surpasses the accuracy of similarly-sized models while achieving 2x faster inference, significantly advancing the Pareto frontier.","authors":["Ali Taghibakhshi","Sharath Turuvekere Sreenivas","Saurav Muralidharan","Marcin Chochowski","Yashaswi Karnati","Raviraj Joshi","Ameya Sunil Mahabaleshwarkar","Zijia Chen","Yoshi Suhara","Oluwatobi Olabiyi","Daniel Korzekwa","Mostofa Patwary","Mohammad Shoeybi","Jan Kautz","Bryan Catanzaro","Ashwath Aithal","Nima Tajbakhsh","Pavlo Molchanov"],"url":"https://arxiv.org/abs/2504.11409"}
{"created":"2025-04-16","title":"Breaking the TDD Flow for Over-the-Air Phase Synchronization in Distributed Antenna Systems","abstract":"Phase synchronization between distributed antenna arrays requires measurements that break the standard time-division duplex (TDD) operation. We present a feasibility study on implementing such synchronization and analyze its impact on the quality of service. Considering two antenna arrays with independent local oscillators (LOs), we propose a modified TDD flow to accommodate the transmission of phase synchronization signals, formulate the phase estimation and compensation problem, and derive the achievable downlink spectral efficiency (SE). Numerical results show that frequent re-estimation of the interarray phase disparity is essential for maximizing SE in systems with low-quality LOs. Furthermore, applying a Kalman filter for phase tracking substantially improves the SE, especially if phase estimation errors are large compared to LOs phase drifts.","authors":["Khac-Hoang Ngo","Erik G. Larsson"],"url":"https://arxiv.org/abs/2504.11411"}
{"created":"2025-04-16","title":"Measures of Variability for Risk-averse Policy Gradient","abstract":"Risk-averse reinforcement learning (RARL) is critical for decision-making under uncertainty, which is especially valuable in high-stake applications. However, most existing works focus on risk measures, e.g., conditional value-at-risk (CVaR), while measures of variability remain underexplored. In this paper, we comprehensively study nine common measures of variability, namely Variance, Gini Deviation, Mean Deviation, Mean-Median Deviation, Standard Deviation, Inter-Quantile Range, CVaR Deviation, Semi_Variance, and Semi_Standard Deviation. Among them, four metrics have not been previously studied in RARL. We derive policy gradient formulas for these unstudied metrics, improve gradient estimation for Gini Deviation, analyze their gradient properties, and incorporate them with the REINFORCE and PPO frameworks to penalize the dispersion of returns.","authors":["Yudong Luo","Yangchen Pan","Jiaqi Tan","Pascal Poupart"],"url":"https://arxiv.org/abs/2504.11412"}
{"created":"2025-04-16","title":"Robustness and sex differences in skin cancer detection: logistic regression vs CNNs","abstract":"Deep learning has been reported to achieve high performances in the detection of skin cancer, yet many challenges regarding the reproducibility of results and biases remain. This study is a replication (different data, same analysis) of a study on Alzheimer's disease [28] which studied robustness of logistic regression (LR) and convolutional neural networks (CNN) across patient sexes. We explore sex bias in skin cancer detection, using the PAD-UFES-20 dataset with LR trained on handcrafted features reflecting dermatological guidelines (ABCDE and the 7-point checklist), and a pre-trained ResNet-50 model. We evaluate these models in alignment with [28]: across multiple training datasets with varied sex composition to determine their robustness. Our results show that both the LR and the CNN were robust to the sex distributions, but the results also revealed that the CNN had a significantly higher accuracy (ACC) and area under the receiver operating characteristics (AUROC) for male patients than for female patients. We hope these findings to contribute to the growing field of investigating potential bias in popular medical machine learning methods. The data and relevant scripts to reproduce our results can be found in our Github.","authors":["Nikolette Pedersen","Regitze Sydendal","Andreas Wulff","Ralf Raumanns","Eike Petersen","Veronika Cheplygina"],"url":"https://arxiv.org/abs/2504.11415"}
{"created":"2025-04-16","title":"Deep Learning-based Bathymetry Retrieval without In-situ Depths using Remote Sensing Imagery and SfM-MVS DSMs with Data Gaps","abstract":"Accurate, detailed, and high-frequent bathymetry is crucial for shallow seabed areas facing intense climatological and anthropogenic pressures. Current methods utilizing airborne or satellite optical imagery to derive bathymetry primarily rely on either SfM-MVS with refraction correction or Spectrally Derived Bathymetry (SDB). However, SDB methods often require extensive manual fieldwork or costly reference data, while SfM-MVS approaches face challenges even after refraction correction. These include depth data gaps and noise in environments with homogeneous visual textures, which hinder the creation of accurate and complete Digital Surface Models (DSMs) of the seabed. To address these challenges, this work introduces a methodology that combines the high-fidelity 3D reconstruction capabilities of the SfM-MVS methods with state-of-the-art refraction correction techniques, along with the spectral analysis capabilities of a new deep learning-based method for bathymetry prediction. This integration enables a synergistic approach where SfM-MVS derived DSMs with data gaps are used as training data to generate complete bathymetric maps. In this context, we propose Swin-BathyUNet that combines U-Net with Swin Transformer self-attention layers and a cross-attention mechanism, specifically tailored for SDB. Swin-BathyUNet is designed to improve bathymetric accuracy by capturing long-range spatial relationships and can also function as a standalone solution for standard SDB with various training depth data, independent of the SfM-MVS output. Experimental results in two completely different test sites in the Mediterranean and Baltic Seas demonstrate the effectiveness of the proposed approach through extensive experiments that demonstrate improvements in bathymetric accuracy, detail, coverage, and noise reduction in the predicted DSM. The code is available at https://github.com/pagraf/Swin-BathyUNet.","authors":["Panagiotis Agrafiotis","Beg\\\"um Demir"],"url":"https://arxiv.org/abs/2504.11416"}
{"created":"2025-04-16","title":"Leveraging Point Transformers for Detecting Anatomical Landmarks in Digital Dentistry","abstract":"The increasing availability of intraoral scanning devices has heightened their importance in modern clinical orthodontics. Clinicians utilize advanced Computer-Aided Design techniques to create patient-specific treatment plans that include laboriously identifying crucial landmarks such as cusps, mesial-distal locations, facial axis points, and tooth-gingiva boundaries. Detecting such landmarks automatically presents challenges, including limited dataset sizes, significant anatomical variability among subjects, and the geometric nature of the data. We present our experiments from the 3DTeethLand Grand Challenge at MICCAI 2024. Our method leverages recent advancements in point cloud learning through transformer architectures. We designed a Point Transformer v3 inspired module to capture meaningful geometric and anatomical features, which are processed by a lightweight decoder to predict per-point distances, further processed by graph-based non-minima suppression. We report promising results and discuss insights on learned feature interpretability.","authors":["Tibor Kub\\'ik","Old\\v{r}ich Kodym","Petr \\v{S}illing","Kate\\v{r}ina Tr\\'avn\\'i\\v{c}kov\\'a","Tom\\'a\\v{s} Moj\\v{z}i\\v{s}","Jan Matula"],"url":"https://arxiv.org/abs/2504.11418"}
{"created":"2025-04-16","title":"Embodied World Models Emerge from Navigational Task in Open-Ended Environments","abstract":"Understanding how artificial systems can develop spatial awareness and reasoning has long been a challenge in AI research. Traditional models often rely on passive observation, but embodied cognition theory suggests that deeper understanding emerges from active interaction with the environment. This study investigates whether neural networks can autonomously internalize spatial concepts through interaction, focusing on planar navigation tasks. Using Gated Recurrent Units (GRUs) combined with Meta-Reinforcement Learning (Meta-RL), we show that agents can learn to encode spatial properties like direction, distance, and obstacle avoidance. We introduce Hybrid Dynamical Systems (HDS) to model the agent-environment interaction as a closed dynamical system, revealing stable limit cycles that correspond to optimal navigation strategies. Ridge Representation allows us to map navigation paths into a fixed-dimensional behavioral space, enabling comparison with neural states. Canonical Correlation Analysis (CCA) confirms strong alignment between these representations, suggesting that the agent's neural states actively encode spatial knowledge. Intervention experiments further show that specific neural dimensions are causally linked to navigation performance. This work provides an approach to bridging the gap between action and perception in AI, offering new insights into building adaptive, interpretable models that can generalize across complex environments. The causal validation of neural representations also opens new avenues for understanding and controlling the internal mechanisms of AI systems, pushing the boundaries of how machines learn and reason in dynamic, real-world scenarios.","authors":["Li Jin","Liu Jia"],"url":"https://arxiv.org/abs/2504.11419"}
{"created":"2025-04-16","title":"Reinforcing Compositional Retrieval: Retrieving Step-by-Step for Composing Informative Contexts","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across numerous tasks, yet they often rely on external context to handle complex tasks. While retrieval-augmented frameworks traditionally focus on selecting top-ranked documents in a single pass, many real-world scenarios demand compositional retrieval, where multiple sources must be combined in a coordinated manner. In this work, we propose a tri-encoder sequential retriever that models this process as a Markov Decision Process (MDP), decomposing the probability of retrieving a set of elements into a sequence of conditional probabilities and allowing each retrieval step to be conditioned on previously selected examples. We train the retriever in two stages: first, we efficiently construct supervised sequential data for initial policy training; we then refine the policy to align with the LLM's preferences using a reward grounded in the structural correspondence of generated programs. Experimental results show that our method consistently and significantly outperforms baselines, underscoring the importance of explicitly modeling inter-example dependencies. These findings highlight the potential of compositional retrieval for tasks requiring multiple pieces of evidence or examples.","authors":["Quanyu Long","Jianda Chen","Zhengyuan Liu","Nancy F. Chen","Wenya Wang","Sinno Jialin Pan"],"url":"https://arxiv.org/abs/2504.11420"}
{"created":"2025-04-16","title":"HeatSense: Intelligent Thermal Anomaly Detection for Securing NoC-Enabled MPSoCs","abstract":"Multi-Processor System-on-Chips (MPSoCs) are highly vulnerable to thermal attacks that manipulate dynamic thermal management systems. To counter this, we propose an adaptive real-time monitoring mechanism that detects abnormal thermal patterns in chip tiles. Our design space exploration helped identify key thermal features for an efficient anomaly detection module to be implemented at routers of network-enabled MPSoCs. To minimize hardware overhead, we employ weighted moving average (WMA) calculations and bit-shift operations, ensuring a lightweight yet effective implementation. By defining a spectrum of abnormal behaviors, our system successfully detects and mitigates malicious temperature fluctuations, reducing severe cases from 3.00{\\deg}C to 1.9{\\deg}C. The anomaly detection module achieves up to 82% of accuracy in detecting thermal attacks, which is only 10-15% less than top-performing machine learning (ML) models like Random Forest. However, our approach reduces hardware usage by up to 75% for logic resources and 100% for specialized resources, making it significantly more efficient than ML-based solutions. This method provides a practical, low-cost solution for resource-constrained environments, ensuring resilience against thermal attacks while maintaining system performance.","authors":["Mahdi Hasanzadeh","Kasem Khalil","Cynthia Sturton","Ahmad Patooghy"],"url":"https://arxiv.org/abs/2504.11421"}
{"created":"2025-04-16","title":"ADT: Tuning Diffusion Models with Adversarial Supervision","abstract":"Diffusion models have achieved outstanding image generation by reversing a forward noising process to approximate true data distributions. During training, these models predict diffusion scores from noised versions of true samples in a single forward pass, while inference requires iterative denoising starting from white noise. This training-inference divergences hinder the alignment between inference and training data distributions, due to potential prediction biases and cumulative error accumulation. To address this problem, we propose an intuitive but effective fine-tuning framework, called Adversarial Diffusion Tuning (ADT), by stimulating the inference process during optimization and aligning the final outputs with training data by adversarial supervision. Specifically, to achieve robust adversarial training, ADT features a siamese-network discriminator with a fixed pre-trained backbone and lightweight trainable parameters, incorporates an image-to-image sampling strategy to smooth discriminative difficulties, and preserves the original diffusion loss to prevent discriminator hacking. In addition, we carefully constrain the backward-flowing path for back-propagating gradients along the inference path without incurring memory overload or gradient explosion. Finally, extensive experiments on Stable Diffusion models (v1.5, XL, and v3), demonstrate that ADT significantly improves both distribution alignment and image quality.","authors":["Dazhong Shen","Guanglu Song","Yi Zhang","Bingqi Ma","Lujundong Li","Dongzhi Jiang","Zhuofan Zong","Yu Liu"],"url":"https://arxiv.org/abs/2504.11423"}
{"created":"2025-04-16","title":"A Dual-Space Framework for General Knowledge Distillation of Large Language Models","abstract":"Knowledge distillation (KD) is a promising solution to compress large language models (LLMs) by transferring their knowledge to smaller models. During this process, white-box KD methods usually minimize the distance between the output distributions of the teacher model and the student model to transfer more information. However, we reveal that the current white-box KD framework exhibits two limitations: a) bridging probability distributions from different output spaces will limit the similarity between the teacher model and the student model; b) this framework cannot be applied to LLMs with different vocabularies. One of the root causes for these limitations is that the distributions from the teacher and the student for KD are output by different prediction heads, which yield distributions in different output spaces and dimensions. Therefore, in this paper, we propose a dual-space knowledge distillation (DSKD) framework that unifies the prediction heads of the teacher and the student models for KD. Specifically, we first introduce two projectors with ideal initialization to project the teacher/student hidden states into the student/teacher representation spaces. After this, the hidden states from different models can share the same head and unify the output spaces of the distributions. Furthermore, we develop an exact token alignment (ETA) algorithm to align the same tokens in two differently-tokenized sequences. Based on the above, our DSKD framework is a general KD framework that supports both off-policy and on-policy KD, and KD between any two LLMs regardless of their vocabularies. Extensive experiments on instruction-following, mathematical reasoning, and code generation benchmarks show that DSKD significantly outperforms existing methods based on the current white-box KD framework and surpasses other cross-tokenizer KD methods for LLMs with different vocabularies.","authors":["Xue Zhang","Songming Zhang","Yunlong Liang","Fandong Meng","Yufeng Chen","Jinan Xu","Jie Zhou"],"url":"https://arxiv.org/abs/2504.11426"}
{"created":"2025-04-16","title":"NormalCrafter: Learning Temporally Consistent Normals from Video Diffusion Priors","abstract":"Surface normal estimation serves as a cornerstone for a spectrum of computer vision applications. While numerous efforts have been devoted to static image scenarios, ensuring temporal coherence in video-based normal estimation remains a formidable challenge. Instead of merely augmenting existing methods with temporal components, we present NormalCrafter to leverage the inherent temporal priors of video diffusion models. To secure high-fidelity normal estimation across sequences, we propose Semantic Feature Regularization (SFR), which aligns diffusion features with semantic cues, encouraging the model to concentrate on the intrinsic semantics of the scene. Moreover, we introduce a two-stage training protocol that leverages both latent and pixel space learning to preserve spatial accuracy while maintaining long temporal context. Extensive evaluations demonstrate the efficacy of our method, showcasing a superior performance in generating temporally consistent normal sequences with intricate details from diverse videos.","authors":["Yanrui Bin","Wenbo Hu","Haoyuan Wang","Xinya Chen","Bing Wang"],"url":"https://arxiv.org/abs/2504.11427"}
{"created":"2025-04-16","title":"Improving Statistical Privacy by Subsampling","abstract":"Differential privacy (DP) considers a scenario, where an adversary has almost complete information about the entries of a database This worst-case assumption is likely to overestimate the privacy thread for an individual in real life. Statistical privacy (SP) denotes a setting where only the distribution of the database entries is known to an adversary, but not their exact values. In this case one has to analyze the interaction between noiseless privacy based on the entropy of distributions and privacy mechanisms that distort the answers of queries, which can be quite complex.","authors":["Dennis Breutigam","R\\\"udiger Reischuk"],"url":"https://arxiv.org/abs/2504.11429"}
{"created":"2025-04-16","title":"Masculine Defaults via Gendered Discourse in Podcasts and Large Language Models","abstract":"Masculine defaults are widely recognized as a significant type of gender bias, but they are often unseen as they are under-researched. Masculine defaults involve three key parts: (i) the cultural context, (ii) the masculine characteristics or behaviors, and (iii) the reward for, or simply acceptance of, those masculine characteristics or behaviors. In this work, we study discourse-based masculine defaults, and propose a twofold framework for (i) the large-scale discovery and analysis of gendered discourse words in spoken content via our Gendered Discourse Correlation Framework (GDCF); and (ii) the measurement of the gender bias associated with these gendered discourse words in LLMs via our Discourse Word-Embedding Association Test (D-WEAT). We focus our study on podcasts, a popular and growing form of social media, analyzing 15,117 podcast episodes. We analyze correlations between gender and discourse words -- discovered via LDA and BERTopic -- to automatically form gendered discourse word lists. We then study the prevalence of these gendered discourse words in domain-specific contexts, and find that gendered discourse-based masculine defaults exist in the domains of business, technology/politics, and video games. Next, we study the representation of these gendered discourse words from a state-of-the-art LLM embedding model from OpenAI, and find that the masculine discourse words have a more stable and robust representation than the feminine discourse words, which may result in better system performance on downstream tasks for men. Hence, men are rewarded for their discourse patterns with better system performance by one of the state-of-the-art language models -- and this embedding disparity is a representational harm and a masculine default.","authors":["Maria Teleki","Xiangjue Dong","Haoran Liu","James Caverlee"],"url":"https://arxiv.org/abs/2504.11431"}
{"created":"2025-04-16","title":"Predicting Wave Dynamics using Deep Learning with Multistep Integration Inspired Attention and Physics-Based Loss Decomposition","abstract":"In this paper, we present a physics-based deep learning framework for data-driven prediction of wave propagation in fluid media. The proposed approach, termed Multistep Integration-Inspired Attention (MI2A), combines a denoising-based convolutional autoencoder for reduced latent representation with an attention-based recurrent neural network with long-short-term memory cells for time evolution of reduced coordinates. This proposed architecture draws inspiration from classical linear multistep methods to enhance stability and long-horizon accuracy in latent-time integration. Despite the efficiency of hybrid neural architectures in modeling wave dynamics, autoregressive predictions are often prone to accumulating phase and amplitude errors over time. To mitigate this issue within the MI2A framework, we introduce a novel loss decomposition strategy that explicitly separates the training loss function into distinct phase and amplitude components. We assess the performance of MI2A against two baseline reduced-order models trained with standard mean-squared error loss: a sequence-to-sequence recurrent neural network and a variant using Luong-style attention. To demonstrate the effectiveness of the MI2A model, we consider three benchmark wave propagation problems of increasing complexity, namely one-dimensional linear convection, the nonlinear viscous Burgers equation, and the two-dimensional Saint-Venant shallow water system. Our results demonstrate that the MI2A framework significantly improves the accuracy and stability of long-term predictions, accurately preserving wave amplitude and phase characteristics. Compared to the standard long-short term memory and attention-based models, MI2A-based deep learning exhibits superior generalization and temporal accuracy, making it a promising tool for real-time wave modeling.","authors":["Indu Kant Deo","Rajeev K. Jaiman"],"url":"https://arxiv.org/abs/2504.11433"}
{"created":"2025-04-16","title":"Enhancing Out-of-Distribution Detection with Extended Logit Normalization","abstract":"Out-of-distribution (OOD) detection is essential for the safe deployment of machine learning models. Recent advances have explored improved classification losses and representation learning strategies to enhance OOD detection. However, these methods are often tailored to specific post-hoc detection techniques, limiting their generalizability. In this work, we identify a critical issue in Logit Normalization (LogitNorm), which inhibits its effectiveness in improving certain post-hoc OOD detection methods. To address this, we propose Extended Logit Normalization ($\\textbf{ELogitNorm}$), a novel hyperparameter-free formulation that significantly benefits a wide range of post-hoc detection methods. By incorporating feature distance-awareness to LogitNorm, $\\textbf{ELogitNorm}$ shows more robust OOD separability and in-distribution (ID) confidence calibration than its predecessor. Extensive experiments across standard benchmarks demonstrate that our approach outperforms state-of-the-art training-time methods in OOD detection while maintaining strong ID classification accuracy.","authors":["Yifan Ding","Xixi Liu","Jonas Unger","Gabriel Eilertsen"],"url":"https://arxiv.org/abs/2504.11434"}
{"created":"2025-04-16","title":"Robust Containment Queries over Collections of Trimmed NURBS Surfaces via Generalized Winding Numbers","abstract":"Efficient and accurate evaluation of containment queries for regions bound by trimmed NURBS surfaces is important in many graphics and engineering applications. However, the algebraic complexity of surface-surface intersections makes gaps and overlaps between surfaces difficult to avoid for in-the-wild surface models. By considering this problem through the lens of the generalized winding number (GWN), a mathematical construction that is indifferent to the arrangement of surfaces in the shape, we can define a containment query that is robust to model watertightness. Applying contemporary techniques for the 3D GWN on arbitrary curved surfaces would require some form of geometric discretization, potentially inducing containment misclassifications near boundary components. In contrast, our proposed method computes an accurate GWN directly on the curved geometry of the input model. We accomplish this using a novel reformulation of the relevant surface integral using Stokes' theorem, which in turn permits an efficient adaptive quadrature calculation on the boundary and trimming curves of the model. While this is sufficient for \"far-field\" query points that are distant from the surface, we augment this approach for \"near-field\" query points (i.e., within a bounding box) and even those coincident to the surface patches via a strategy that directly identifies and accounts for the jump discontinuity in the scalar field. We demonstrate that our method of evaluating the GWN field is robust to complex trimming geometry in a CAD model, and is accurate up to arbitrary precision at arbitrary distances from the surface. Furthermore, the derived containment query is robust to non-watertightness while respecting all curved features of the input shape.","authors":["Jacob Spainhour","Kenneth Weiss"],"url":"https://arxiv.org/abs/2504.11435"}
{"created":"2025-04-16","title":"Mamba-Based Ensemble learning for White Blood Cell Classification","abstract":"White blood cell (WBC) classification assists in assessing immune health and diagnosing various diseases, yet manual classification is labor-intensive and prone to inconsistencies. Recent advancements in deep learning have shown promise over traditional methods; however, challenges such as data imbalance and the computational demands of modern technologies, such as Transformer-based models which do not scale well with input size, limit their practical application. This paper introduces a novel framework that leverages Mamba models integrated with ensemble learning to improve WBC classification. Mamba models, known for their linear complexity, provide a scalable alternative to Transformer-based approaches, making them suitable for deployment in resource-constrained environments. Additionally, we introduce a new WBC dataset, Chula-WBC-8, for benchmarking. Our approach not only validates the effectiveness of Mamba models in this domain but also demonstrates their potential to significantly enhance classification efficiency without compromising accuracy. The source code can be found at https://github.com/LewisClifton/Mamba-WBC-Classification.","authors":["Lewis Clifton","Xin Tian","Duangdao Palasuwan","Phandee Watanaboonyongcharoen","Ponlapat Rojnuckarin","Nantheera Anantrasirichai"],"url":"https://arxiv.org/abs/2504.11438"}
{"created":"2025-04-16","title":"TADACap: Time-series Adaptive Domain-Aware Captioning","abstract":"While image captioning has gained significant attention, the potential of captioning time-series images, prevalent in areas like finance and healthcare, remains largely untapped. Existing time-series captioning methods typically offer generic, domain-agnostic descriptions of time-series shapes and struggle to adapt to new domains without substantial retraining. To address these limitations, we introduce TADACap, a retrieval-based framework to generate domain-aware captions for time-series images, capable of adapting to new domains without retraining. Building on TADACap, we propose a novel retrieval strategy that retrieves diverse image-caption pairs from a target domain database, namely TADACap-diverse. We benchmarked TADACap-diverse against state-of-the-art methods and ablation variants. TADACap-diverse demonstrates comparable semantic accuracy while requiring significantly less annotation effort.","authors":["Elizabeth Fons","Rachneet Kaur","Zhen Zeng","Soham Palande","Tucker Balch","Svitlana Vyetrenko","Manuela Veloso"],"url":"https://arxiv.org/abs/2504.11441"}
{"created":"2025-04-16","title":"TextArena","abstract":"TextArena is an open-source collection of competitive text-based games for training and evaluation of agentic behavior in Large Language Models (LLMs). It spans 57+ unique environments (including single-player, two-player, and multi-player setups) and allows for easy evaluation of model capabilities via an online-play system (against humans and other submitted models) with real-time TrueSkill scores. Traditional benchmarks rarely assess dynamic social skills such as negotiation, theory of mind, and deception, creating a gap that TextArena addresses. Designed with research, community and extensibility in mind, TextArena emphasizes ease of adding new games, adapting the framework, testing models, playing against the models, and training models. Detailed documentation of environments, games, leaderboard, and examples are available on https://github.com/LeonGuertler/TextArena and https://www.textarena.ai/.","authors":["Leon Guertler","Bobby Cheng","Simon Yu","Bo Liu","Leshem Choshen","Cheston Tan"],"url":"https://arxiv.org/abs/2504.11442"}
{"created":"2025-04-16","title":"eXplainable AI for data driven control: an inverse optimal control approach","abstract":"Understanding the behavior of black-box data-driven controllers is a key challenge in modern control design. In this work, we propose an eXplainable AI (XAI) methodology based on Inverse Optimal Control (IOC) to obtain local explanations for the behavior of a controller operating around a given region. Specifically, we extract the weights assigned to tracking errors and control effort in the implicit cost function that a black-box controller is optimizing, offering a more transparent and interpretable representation of the controller's underlying objectives. This approach presents connections with well-established XAI techniques, such as Local Interpretable Model-agnostic Explanations (LIME) since it is still based on a local approximation of the control policy. However, rather being limited to a standard sensitivity analysis, the explanation provided by our method relies on the solution of an inverse Linear Quadratic (LQ) problem, offering a structured and more control-relevant perspective. Numerical examples demonstrate that the inferred cost function consistently provides a deeper understanding of the controller's decision-making process, shedding light on otherwise counterintuitive or unexpected phenomena.","authors":["Federico Porcari","Donatello Materassi","Simone Formentin"],"url":"https://arxiv.org/abs/2504.11446"}
{"created":"2025-04-16","title":"Diffusion Distillation With Direct Preference Optimization For Efficient 3D LiDAR Scene Completion","abstract":"The application of diffusion models in 3D LiDAR scene completion is limited due to diffusion's slow sampling speed. Score distillation accelerates diffusion sampling but with performance degradation, while post-training with direct policy optimization (DPO) boosts performance using preference data. This paper proposes Distillation-DPO, a novel diffusion distillation framework for LiDAR scene completion with preference aligment. First, the student model generates paired completion scenes with different initial noises. Second, using LiDAR scene evaluation metrics as preference, we construct winning and losing sample pairs. Such construction is reasonable, since most LiDAR scene metrics are informative but non-differentiable to be optimized directly. Third, Distillation-DPO optimizes the student model by exploiting the difference in score functions between the teacher and student models on the paired completion scenes. Such procedure is repeated until convergence. Extensive experiments demonstrate that, compared to state-of-the-art LiDAR scene completion diffusion models, Distillation-DPO achieves higher-quality scene completion while accelerating the completion speed by more than 5-fold. Our method is the first to explore adopting preference learning in distillation to the best of our knowledge and provide insights into preference-aligned distillation. Our code is public available on https://github.com/happyw1nd/DistillationDPO.","authors":["An Zhaol","Shengyuan Zhang","Ling Yang","Zejian Li","Jiale Wu","Haoran Xu","AnYang Wei","Perry Pengyun GU Lingyun Sun"],"url":"https://arxiv.org/abs/2504.11447"}
{"created":"2025-04-16","title":"Full-Diversity Construction-D Lattices: Design and Decoding Perspective on Block-Fading Channels","abstract":"This paper introduces a novel framework for constructing algebraic lattices based on Construction-D, leveraging nested linear codes and prime ideals from algebraic number fields. We focus on the application of these lattices in block-fading (BF) channels, which are characterized by piecewise-constant fading across blocks of transmitted symbols. This approach results in a semi-systematic generator matrix, providing a structured foundation for high-dimensional lattice design for BF channels. The proposed Construction-D lattices exhibit the full diversity property, making them highly effective for error performance improvement. To address this, we develop an efficient decoding algorithm designed specifically for full-diversity Construction-D lattices.","authors":["Maryam Sadeghi","Hassan Khodaiemehr","Chen Feng"],"url":"https://arxiv.org/abs/2504.11448"}
{"created":"2025-04-16","title":"Optimal Hardness of Online Algorithms for Large Independent Sets","abstract":"We study the algorithmic problem of finding a large independent set in an Erd\\\"{o}s-R\\'{e}nyi random graph $\\mathbb{G}(n,p)$. For constant $p$ and $b=1/(1-p)$, the largest independent set has size $2\\log_b n$, while a simple greedy algorithm revealing vertices sequentially and making decisions based only on previously seen vertices finds an independent set of size $\\log_b n$. In his seminal 1976 paper, Karp challenged to either improve this guarantee or establish its hardness. Decades later, this problem remains open, one of the most prominent algorithmic problems in the theory of random graphs.","authors":["David Gamarnik","Eren C. K{\\i}z{\\i}lda\\u{g}","Lutz Warnke"],"url":"https://arxiv.org/abs/2504.11450"}
{"created":"2025-04-16","title":"PARTFIELD: Learning 3D Feature Fields for Part Segmentation and Beyond","abstract":"We propose PartField, a feedforward approach for learning part-based 3D features, which captures the general concept of parts and their hierarchy without relying on predefined templates or text-based names, and can be applied to open-world 3D shapes across various modalities. PartField requires only a 3D feedforward pass at inference time, significantly improving runtime and robustness compared to prior approaches. Our model is trained by distilling 2D and 3D part proposals from a mix of labeled datasets and image segmentations on large unsupervised datasets, via a contrastive learning formulation. It produces a continuous feature field which can be clustered to yield a hierarchical part decomposition. Comparisons show that PartField is up to 20% more accurate and often orders of magnitude faster than other recent class-agnostic part-segmentation methods. Beyond single-shape part decomposition, consistency in the learned field emerges across shapes, enabling tasks such as co-segmentation and correspondence, which we demonstrate in several applications of these general-purpose, hierarchical, and consistent 3D feature fields. Check our Webpage! https://research.nvidia.com/labs/toronto-ai/partfield-release/","authors":["Minghua Liu","Mikaela Angelina Uy","Donglai Xiang","Hao Su","Sanja Fidler","Nicholas Sharp","Jun Gao"],"url":"https://arxiv.org/abs/2504.11451"}
{"created":"2025-04-16","title":"A Clean Slate for Offline Reinforcement Learning","abstract":"Progress in offline reinforcement learning (RL) has been impeded by ambiguous problem definitions and entangled algorithmic designs, resulting in inconsistent implementations, insufficient ablations, and unfair evaluations. Although offline RL explicitly avoids environment interaction, prior methods frequently employ extensive, undocumented online evaluation for hyperparameter tuning, complicating method comparisons. Moreover, existing reference implementations differ significantly in boilerplate code, obscuring their core algorithmic contributions. We address these challenges by first introducing a rigorous taxonomy and a transparent evaluation protocol that explicitly quantifies online tuning budgets. To resolve opaque algorithmic design, we provide clean, minimalistic, single-file implementations of various model-free and model-based offline RL methods, significantly enhancing clarity and achieving substantial speed-ups. Leveraging these streamlined implementations, we propose Unifloral, a unified algorithm that encapsulates diverse prior approaches within a single, comprehensive hyperparameter space, enabling algorithm development in a shared hyperparameter space. Using Unifloral with our rigorous evaluation protocol, we develop two novel algorithms - TD3-AWR (model-free) and MoBRAC (model-based) - which substantially outperform established baselines. Our implementation is publicly available at https://github.com/EmptyJackson/unifloral.","authors":["Matthew Thomas Jackson","Uljad Berdica","Jarek Liesen","Shimon Whiteson","Jakob Nicolaus Foerster"],"url":"https://arxiv.org/abs/2504.11453"}
{"created":"2025-04-16","title":"Elucidating the Design Space of Multimodal Protein Language Models","abstract":"Multimodal protein language models (PLMs) integrate sequence and token-based structural information, serving as a powerful foundation for protein modeling, generation, and design. However, the reliance on tokenizing 3D structures into discrete tokens causes substantial loss of fidelity about fine-grained structural details and correlations. In this paper, we systematically elucidate the design space of multimodal PLMs to overcome their limitations. We identify tokenization loss and inaccurate structure token predictions by the PLMs as major bottlenecks. To address these, our proposed design space covers improved generative modeling, structure-aware architectures and representation learning, and data exploration. Our advancements approach finer-grained supervision, demonstrating that token-based multimodal PLMs can achieve robust structural modeling. The effective design methods dramatically improve the structure generation diversity, and notably, folding abilities of our 650M model by reducing the RMSD from 5.52 to 2.36 on PDB testset, even outperforming 3B baselines and on par with the specialized folding models.","authors":["Cheng-Yen (Wesley)","Hsieh","Xinyou Wang","Daiheng Zhang","Dongyu Xue","Fei Ye","Shujian Huang","Zaixiang Zheng","Quanquan Gu"],"url":"https://arxiv.org/abs/2504.11454"}
{"created":"2025-04-16","title":"SimpleAR: Pushing the Frontier of Autoregressive Visual Generation through Pretraining, SFT, and RL","abstract":"This work presents SimpleAR, a vanilla autoregressive visual generation framework without complex architecure modifications. Through careful exploration of training and inference optimization, we demonstrate that: 1) with only 0.5B parameters, our model can generate 1024x1024 resolution images with high fidelity, and achieve competitive results on challenging text-to-image benchmarks, e.g., 0.59 on GenEval and 79.66 on DPG; 2) both supervised fine-tuning (SFT) and Group Relative Policy Optimization (GRPO) training could lead to significant improvements on generation aesthectics and prompt alignment; and 3) when optimized with inference acceleraton techniques like vLLM, the time for SimpleAR to generate an 1024x1024 image could be reduced to around 14 seconds. By sharing these findings and open-sourcing the code, we hope to reveal the potential of autoregressive visual generation and encourage more participation in this research field. Code is available at https://github.com/wdrink/SimpleAR.","authors":["Junke Wang","Zhi Tian","Xun Wang","Xinyu Zhang","Weilin Huang","Zuxuan Wu","Yu-Gang Jiang"],"url":"https://arxiv.org/abs/2504.11455"}
{"created":"2025-04-16","title":"DeepMath-103K: A Large-Scale, Challenging, Decontaminated, and Verifiable Mathematical Dataset for Advancing Reasoning","abstract":"The capacity for complex mathematical reasoning is a key benchmark for artificial intelligence. While reinforcement learning (RL) applied to LLMs shows promise, progress is significantly hindered by the lack of large-scale training data that is sufficiently challenging, possesses verifiable answer formats suitable for RL, and is free from contamination with evaluation benchmarks. To address these limitations, we introduce DeepMath-103K, a new, large-scale dataset comprising approximately 103K mathematical problems, specifically designed to train advanced reasoning models via RL. DeepMath-103K is curated through a rigorous pipeline involving source analysis, stringent decontamination against numerous benchmarks, and filtering for high difficulty (primarily Levels 5-9), significantly exceeding existing open resources in challenge. Each problem includes a verifiable final answer, enabling rule-based RL, and three distinct R1-generated solutions suitable for diverse training paradigms like supervised fine-tuning or distillation. Spanning a wide range of mathematical topics, DeepMath-103K promotes the development of generalizable reasoning. We demonstrate that models trained on DeepMath-103K achieve significant improvements on challenging mathematical benchmarks, validating its effectiveness. We release DeepMath-103K publicly to facilitate community progress in building more capable AI reasoning systems: https://github.com/zwhe99/DeepMath.","authors":["Zhiwei He","Tian Liang","Jiahao Xu","Qiuzhi Liu","Xingyu Chen","Yue Wang","Linfeng Song","Dian Yu","Zhenwen Liang","Wenxuan Wang","Zhuosheng Zhang","Rui Wang","Zhaopeng Tu","Haitao Mi","Dong Yu"],"url":"https://arxiv.org/abs/2504.11456"}
{"created":"2025-04-16","title":"Aligning Generative Denoising with Discriminative Objectives Unleashes Diffusion for Visual Perception","abstract":"With the success of image generation, generative diffusion models are increasingly adopted for discriminative tasks, as pixel generation provides a unified perception interface. However, directly repurposing the generative denoising process for discriminative objectives reveals critical gaps rarely addressed previously. Generative models tolerate intermediate sampling errors if the final distribution remains plausible, but discriminative tasks require rigorous accuracy throughout, as evidenced in challenging multi-modal tasks like referring image segmentation. Motivated by this gap, we analyze and enhance alignment between generative diffusion processes and perception tasks, focusing on how perception quality evolves during denoising. We find: (1) earlier denoising steps contribute disproportionately to perception quality, prompting us to propose tailored learning objectives reflecting varying timestep contributions; (2) later denoising steps show unexpected perception degradation, highlighting sensitivity to training-denoising distribution shifts, addressed by our diffusion-tailored data augmentation; and (3) generative processes uniquely enable interactivity, serving as controllable user interfaces adaptable to correctional prompts in multi-round interactions. Our insights significantly improve diffusion-based perception models without architectural changes, achieving state-of-the-art performance on depth estimation, referring image segmentation, and generalist perception tasks. Code available at https://github.com/ziqipang/ADDP.","authors":["Ziqi Pang","Xin Xu","Yu-Xiong Wang"],"url":"https://arxiv.org/abs/2504.11457"}
{"created":"2025-04-16","title":"Focal Loss based Residual Convolutional Neural Network for Speech Emotion Recognition","abstract":"This paper proposes a Residual Convolutional Neural Network (ResNet) based on speech features and trained under Focal Loss to recognize emotion in speech. Speech features such as Spectrogram and Mel-frequency Cepstral Coefficients (MFCCs) have shown the ability to characterize emotion better than just plain text. Further Focal Loss, first used in One-Stage Object Detectors, has shown the ability to focus the training process more towards hard-examples and down-weight the loss assigned to well-classified examples, thus preventing the model from being overwhelmed by easily classifiable examples.","authors":["Suraj Tripathi","Abhay Kumar","Abhiram Ramesh","Chirag Singh","Promod Yenigalla"],"url":"https://arxiv.org/abs/1906.05682"}
{"created":"2025-04-16","title":"MTCNET: Multi-task Learning Paradigm for Crowd Count Estimation","abstract":"We propose a Multi-Task Learning (MTL) paradigm based deep neural network architecture, called MTCNet (Multi-Task Crowd Network) for crowd density and count estimation. Crowd count estimation is challenging due to the non-uniform scale variations and the arbitrary perspective of an individual image. The proposed model has two related tasks, with Crowd Density Estimation as the main task and Crowd-Count Group Classification as the auxiliary task. The auxiliary task helps in capturing the relevant scale-related information to improve the performance of the main task. The main task model comprises two blocks: VGG-16 front-end for feature extraction and a dilated Convolutional Neural Network for density map generation. The auxiliary task model shares the same front-end as the main task, followed by a CNN classifier. Our proposed network achieves 5.8% and 14.9% lower Mean Absolute Error (MAE) than the state-of-the-art methods on ShanghaiTech dataset without using any data augmentation. Our model also outperforms with 10.5% lower MAE on UCF_CC_50 dataset.","authors":["Abhay Kumar","Nishant Jain","Suraj Tripathi","Chirag Singh","Kamal Krishna"],"url":"https://arxiv.org/abs/1908.08652"}
{"created":"2025-04-16","title":"MARVIS: Motion & Geometry Aware Real and Virtual Image Segmentation","abstract":"Tasks such as autonomous navigation, 3D reconstruction, and object recognition near the water surfaces are crucial in marine robotics applications. However, challenges arise due to dynamic disturbances, e.g., light reflections and refraction from the random air-water interface, irregular liquid flow, and similar factors, which can lead to potential failures in perception and navigation systems. Traditional computer vision algorithms struggle to differentiate between real and virtual image regions, significantly complicating tasks. A virtual image region is an apparent representation formed by the redirection of light rays, typically through reflection or refraction, creating the illusion of an object's presence without its actual physical location. This work proposes a novel approach for segmentation on real and virtual image regions, exploiting synthetic images combined with domain-invariant information, a Motion Entropy Kernel, and Epipolar Geometric Consistency. Our segmentation network does not need to be re-trained if the domain changes. We show this by deploying the same segmentation network in two different domains: simulation and the real world. By creating realistic synthetic images that mimic the complexities of the water surface, we provide fine-grained training data for our network (MARVIS) to discern between real and virtual images effectively. By motion & geometry-aware design choices and through comprehensive experimental analysis, we achieve state-of-the-art real-virtual image segmentation performance in unseen real world domain, achieving an IoU over 78% and a F1-Score over 86% while ensuring a small computational footprint. MARVIS offers over 43 FPS (8 FPS) inference rates on a single GPU (CPU core). Our code and dataset are available here https://github.com/jiayi-wu-umd/MARVIS.","authors":["Jiayi Wu","Xiaomin Lin","Shahriar Negahdaripour","Cornelia Ferm\\\"uller","Yiannis Aloimonos"],"url":"https://arxiv.org/abs/2403.09850"}
{"created":"2025-04-16","title":"Evaluating Semantic Variation in Text-to-Image Synthesis: A Causal Perspective","abstract":"Accurate interpretation and visualization of human instructions are crucial for text-to-image (T2I) synthesis. However, current models struggle to capture semantic variations from word order changes, and existing evaluations, relying on indirect metrics like text-image similarity, fail to reliably assess these challenges. This often obscures poor performance on complex or uncommon linguistic patterns by the focus on frequent word combinations. To address these deficiencies, we propose a novel metric called SemVarEffect and a benchmark named SemVarBench, designed to evaluate the causality between semantic variations in inputs and outputs in T2I synthesis. Semantic variations are achieved through two types of linguistic permutations, while avoiding easily predictable literal variations. Experiments reveal that the CogView-3-Plus and Ideogram 2 performed the best, achieving a score of 0.2/1. Semantic variations in object relations are less understood than attributes, scoring 0.07/1 compared to 0.17-0.19/1. We found that cross-modal alignment in UNet or Transformers plays a crucial role in handling semantic variations, a factor previously overlooked by a focus on textual encoders. Our work establishes an effective evaluation framework that advances the T2I synthesis community's exploration of human instruction understanding. Our benchmark and code are available at https://github.com/zhuxiangru/SemVarBench .","authors":["Xiangru Zhu","Penglei Sun","Yaoxian Song","Yanghua Xiao","Zhixu Li","Chengyu Wang","Jun Huang","Bei Yang","Xiaoxiao Xu"],"url":"https://arxiv.org/abs/2410.10291"}
{"created":"2025-04-16","title":"Integrating electrocardiogram and fundus images for early detection of cardiovascular diseases","abstract":"Cardiovascular diseases (CVD) are a predominant health concern globally, emphasizing the need for advanced diagnostic techniques. In our research, we present an avant-garde methodology that synergistically integrates ECG readings and retinal fundus images to facilitate the early disease tagging as well as triaging of the CVDs in the order of disease priority. Recognizing the intricate vascular network of the retina as a reflection of the cardiovascular system, alongwith the dynamic cardiac insights from ECG, we sought to provide a holistic diagnostic perspective. Initially, a Fast Fourier Transform (FFT) was applied to both the ECG and fundus images, transforming the data into the frequency domain. Subsequently, the Earth Mover's Distance (EMD) was computed for the frequency-domain features of both modalities. These EMD values were then concatenated, forming a comprehensive feature set that was fed into a Neural Network classifier. This approach, leveraging the FFT's spectral insights and EMD's capability to capture nuanced data differences, offers a robust representation for CVD classification. Preliminary tests yielded a commendable accuracy of 84 percent, underscoring the potential of this combined diagnostic strategy. As we continue our research, we anticipate refining and validating the model further to enhance its clinical applicability in resource limited healthcare ecosystems prevalent across the Indian sub-continent and also the world at large.","authors":["K. A. Muthukumar","Dhruva Nandi","Priya Ranjan","Krithika Ramachandran","Shiny PJ","Anirban Ghosh","Ashwini M","Aiswaryah Radhakrishnan","V. E. Dhandapani","Rajiv Janardhanan"],"url":"https://arxiv.org/abs/2504.10493"}
{"created":"2025-04-16","title":"Artificial Intelligence and the Dual Paradoxes: Examining the Interplay of Efficiency, Resource Consumption, and Labor Dynamics","abstract":"Artificial Intelligence's (AI) rapid development and growth not only transformed industries but also fired up important debates about its impacts on employment, resource allocation, and the ethics involved in decision-making. It serves to understand how changes within an industry will be able to influence society with that change. Advancing AI technologies will create a dual paradox of efficiency, greater resource consumption, and displacement of traditional labor. In this context, we explore the impact of AI on energy consumption, human labor roles, and hybrid roles widespread human labor replacement. We used mixed methods involving qualitative and quantitative analyses of data identified from various sources. Findings suggest that AI increases energy consumption and has impacted human labor roles to a minimal extent, considering that its applicability is limited to some tasks that require human judgment. In this context, the","authors":["Mfon Akpan","Adeyemi Adebayo"],"url":"https://arxiv.org/abs/2504.10503"}
{"created":"2025-04-16","title":"Assessing the Elephant in the Room in Scheduling for Current Hybrid HPC-QC Clusters","abstract":"Quantum computing resources are among the most promising candidates for extending the computational capabilities of High-Performance Computing (HPC) systems. As a result, HPC-quantum integration has become an increasingly active area of research. While much of the existing literature has focused on software stack integration and quantum circuit compilation, key challenges such as hybrid resource allocation and job scheduling-especially relevant in the current Noisy Intermediate-Scale Quantum era-have received less attention. In this work, we highlight these critical issues in the context of integrating quantum computers with operational HPC environments, taking into account the current maturity and heterogeneity of quantum technologies. We then propose a set of conceptual strategies aimed at addressing these challenges and paving the way for practical HPC-QC integration in the near future.","authors":["Paolo Viviani","Roberto Rocco","Matteo Barbieri","Gabriella Bettonte","Elisabetta Boella","Marco Cipollini","Jonathan Frassineti","Fulvio Ganz","Sara Marzella","Daniele Ottaviani","Simone Rizzo","Alberto Scionti","Chiara Vercellino","Giacomo Vitali","Olivier Terzo","Bartolomeo Montrucchio","Daniele Gregori"],"url":"https://arxiv.org/abs/2504.10520"}
{"created":"2025-04-16","title":"PathSeqSAM: Sequential Modeling for Pathology Image Segmentation with SAM2","abstract":"Current methods for pathology image segmentation typically treat 2D slices independently, ignoring valuable cross-slice information. We present PathSeqSAM, a novel approach that treats 2D pathology slices as sequential video frames using SAM2's memory mechanisms. Our method introduces a distance-aware attention mechanism that accounts for variable physical distances between slices and employs LoRA for domain adaptation. Evaluated on the KPI Challenge 2024 dataset for glomeruli segmentation, PathSeqSAM demonstrates improved segmentation quality, particularly in challenging cases that benefit from cross-slice context. We have publicly released our code at https://github.com/JackyyyWang/PathSeqSAM.","authors":["Mingyang Zhu","Yinting Liu","Mingyu Li","Jiacheng Wang"],"url":"https://arxiv.org/abs/2504.10526"}
{"created":"2025-04-16","title":"Physics-Informed Neural Networks for Enhanced Interface Preservation in Lattice Boltzmann Multiphase Simulations","abstract":"This paper presents an improved approach for preserving sharp interfaces in multiphase Lattice Boltzmann Method (LBM) simulations using Physics-Informed Neural Networks (PINNs). Interface diffusion is a common challenge in multiphase LBM, leading to reduced accuracy in simulating phenomena where interfacial dynamics are critical. We propose a coupled PINN-LBM framework that maintains interface sharpness while preserving the physical accuracy of the simulation. Our approach is validated through droplet simulations, with quantitative metrics measuring interface width, maximum gradient, phase separation, effective interface width, and interface energy. The enhanced visualization techniques employed in this work clearly demonstrate the superior performance of PINN-LBM over standard LBM for multiphase simulations, particularly in maintaining well-defined interfaces throughout the simulation. We provide a comprehensive analysis of the results, showcasing how the neural network integration effectively counteracts numerical diffusion, while maintaining physical consistency with the underlying fluid dynamics.","authors":["Yue Li"],"url":"https://arxiv.org/abs/2504.10539"}
{"created":"2025-04-16","title":"AB-Cache: Training-Free Acceleration of Diffusion Models via Adams-Bashforth Cached Feature Reuse","abstract":"Diffusion models have demonstrated remarkable success in generative tasks, yet their iterative denoising process results in slow inference, limiting their practicality. While existing acceleration methods exploit the well-known U-shaped similarity pattern between adjacent steps through caching mechanisms, they lack theoretical foundation and rely on simplistic computation reuse, often leading to performance degradation. In this work, we provide a theoretical understanding by analyzing the denoising process through the second-order Adams-Bashforth method, revealing a linear relationship between the outputs of consecutive steps. This analysis explains why the outputs of adjacent steps exhibit a U-shaped pattern. Furthermore, extending Adams-Bashforth method to higher order, we propose a novel caching-based acceleration approach for diffusion models, instead of directly reusing cached results, with a truncation error bound of only \\(O(h^k)\\) where $h$ is the step size. Extensive validation across diverse image and video diffusion models (including HunyuanVideo and FLUX.1-dev) with various schedulers demonstrates our method's effectiveness in achieving nearly $3\\times$ speedup while maintaining original performance levels, offering a practical real-time solution without compromising generation quality.","authors":["Zichao Yu","Zhen Zou","Guojiang Shao","Chengwei Zhang","Shengze Xu","Jie Huang","Feng Zhao","Xiaodong Cun","Wenyi Zhang"],"url":"https://arxiv.org/abs/2504.10540"}
{"created":"2025-04-16","title":"An Efficient Quantum Classifier Based on Hamiltonian Representations","abstract":"Quantum machine learning (QML) is a discipline that seeks to transfer the advantages of quantum computing to data-driven tasks. However, many studies rely on toy datasets or heavy feature reduction, raising concerns about their scalability. Progress is further hindered by hardware limitations and the significant costs of encoding dense vector representations on quantum devices. To address these challenges, we propose an efficient approach called Hamiltonian classifier that circumvents the costs associated with data encoding by mapping inputs to a finite set of Pauli strings and computing predictions as their expectation values. In addition, we introduce two classifier variants with different scaling in terms of parameters and sample complexity. We evaluate our approach on text and image classification tasks, against well-established classical and quantum models. The Hamiltonian classifier delivers performance comparable to or better than these methods. Notably, our method achieves logarithmic complexity in both qubits and quantum gates, making it well-suited for large-scale, real-world applications. We make our implementation available on GitHub.","authors":["Federico Tiblias","Anna Schroeder","Yue Zhang","Mariami Gachechiladze","Iryna Gurevych"],"url":"https://arxiv.org/abs/2504.10542"}
{"created":"2025-04-16","title":"LCDC: Bridging Science and Machine Learning for Light Curve Analysis","abstract":"The characterization and analysis of light curves are vital for understanding the physical and rotational properties of artificial space objects such as satellites, rocket stages, and space debris. This paper introduces the Light Curve Dataset Creator (LCDC), a Python-based toolkit designed to facilitate the preprocessing, analysis, and machine learning applications of light curve data. LCDC enables seamless integration with publicly available datasets, such as the newly introduced Mini Mega Tortora (MMT) database. Moreover, it offers data filtering, transformation, as well as feature extraction tooling. To demonstrate the toolkit's capabilities, we created the first standardized dataset for rocket body classification, RoBo6, which was used to train and evaluate several benchmark machine learning models, addressing the lack of reproducibility and comparability in recent studies. Furthermore, the toolkit enables advanced scientific analyses, such as surface characterization of the Atlas 2AS Centaur and the rotational dynamics of the Delta 4 rocket body, by streamlining data preprocessing, feature extraction, and visualization. These use cases highlight LCDC's potential to advance space debris characterization and promote sustainable space exploration. Additionally, they highlight the toolkit's ability to enable AI-focused research within the space debris community.","authors":["Daniel Kyselica","Tom\\'a\\v{s} Hrob\\'ar","Ji\\v{r}\\'i \\v{S}ilha","Roman \\v{D}urikovi\\v{c}","Marek \\v{S}uppa"],"url":"https://arxiv.org/abs/2504.10550"}
{"created":"2025-04-16","title":"Inferring the Hubble Constant Using Simulated Strongly Lensed Supernovae and Neural Network Ensembles","abstract":"Strongly lensed supernovae are a promising new probe to obtain independent measurements of the Hubble constant (${H_0}$). In this work, we employ simulated gravitationally lensed Type Ia supernovae (glSNe Ia) to train our machine learning (ML) pipeline to constrain $H_0$. We simulate image time-series of glSNIa, as observed with the upcoming Nancy Grace Roman Space Telescope, that we employ for training an ensemble of five convolutional neural networks (CNNs). The outputs of this ensemble network are combined with a simulation-based inference (SBI) framework to quantify the uncertainties on the network predictions and infer full posteriors for the $H_0$ estimates. We illustrate that the combination of multiple glSN systems enhances constraint precision, providing a $4.4\\%$ estimate of $H_0$ based on 100 simulated systems, which is in agreement with the ground truth. This research highlights the potential of leveraging the capabilities of ML with glSNe systems to obtain a pipeline capable of fast and automated $H_0$ measurements.","authors":["Gon\\c{c}alo Gon\\c{c}alves","Nikki Arendse","Doogesh Kodi Ramanah","Rados{\\l}aw Wojtak"],"url":"https://arxiv.org/abs/2504.10553"}
{"created":"2025-04-16","title":"Molecular Learning Dynamics","abstract":"We apply the physics-learning duality to molecular systems by complementing the physical description of interacting particles with a dual learning description, where each particle is modeled as an agent minimizing a loss function. In the traditional physics framework, the equations of motion are derived from the Lagrangian function, while in the learning framework, the same equations emerge from learning dynamics driven by the agent loss function. The loss function depends on scalar quantities that describe invariant properties of all other agents or particles. To demonstrate this approach, we first infer the loss functions of oxygen and hydrogen directly from a dataset generated by the CP2K physics-based simulation of water molecules. We then employ the loss functions to develop a learning-based simulation of water molecules, which achieves comparable accuracy while being significantly more computationally efficient than standard physics-based simulations.","authors":["Yaroslav Gusev","Vitaly Vanchurin"],"url":"https://arxiv.org/abs/2504.10560"}
{"created":"2025-04-16","title":"FLOWR: Flow Matching for Structure-Aware De Novo, Interaction- and Fragment-Based Ligand Generation","abstract":"We introduce FLOWR, a novel structure-based framework for the generation and optimization of three-dimensional ligands. FLOWR integrates continuous and categorical flow matching with equivariant optimal transport, enhanced by an efficient protein pocket conditioning. Alongside FLOWR, we present SPINDR, a thoroughly curated dataset comprising ligand-pocket co-crystal complexes specifically designed to address existing data quality issues. Empirical evaluations demonstrate that FLOWR surpasses current state-of-the-art diffusion- and flow-based methods in terms of PoseBusters-validity, pose accuracy, and interaction recovery, while offering a significant inference speedup, achieving up to 70-fold faster performance. In addition, we introduce FLOWR.multi, a highly accurate multi-purpose model allowing for the targeted sampling of novel ligands that adhere to predefined interaction profiles and chemical substructures for fragment-based design without the need of re-training or any re-sampling strategies","authors":["Julian Cremer","Ross Irwin","Alessandro Tibot","Jon Paul Janet","Simon Olsson","Djork-Arn\\'e Clevert"],"url":"https://arxiv.org/abs/2504.10564"}
{"created":"2025-04-16","title":"Visual anemometry of natural vegetation from their leaf motion","abstract":"High-resolution, near-ground wind-speed data are critical for improving the accuracy of weather predictions and climate models,$^{1-3}$ supporting wildfire control efforts,$^{4-7}$ and ensuring the safe passage of airplanes during takeoff and landing maneouvers.$^{8,9}$ Quantitative wind speed anemometry generally employs on-site instrumentation for accurate single-position data or sophisticated remote techniques such as Doppler radar for quantitative field measurements. It is widely recognized that the wind-induced motion of vegetation depends in a complex manner on their structure and mechanical properties, obviating their use in quantitative anemometry.$^{10-14}$ We analyze measurements on a host of different vegetation showing that leaf motion can be decoupled from the leaf's branch and support structure, at low-to-moderate wind speed, $U_{wind}$. This wind speed range is characterized by a leaf Reynolds number, enabling the development of a remote, quantitative anemometry method based on the formula, $U_{wind}\\approx740\\sqrt{{\\mu}U_{leaf}/{\\rho}D}$, that relies only on the leaf size $D$, its measured fluctuating (RMS) speed $U_{leaf}$, the air viscosity $\\mu$, and its mass density $\\rho$. This formula is corroborated by a first-principles model and validated using a host of laboratory and field tests on diverse vegetation types, ranging from oak, olive, and magnolia trees through to camphor and bullgrass. The findings of this study open the door to a new paradigm in anemometry, using natural vegetation to enable remote and rapid quantitative field measurements at global locations with minimal cost.","authors":["Roni H. Goldshmid","John O. Dabiri","John E. Sader"],"url":"https://arxiv.org/abs/2504.10584"}
{"created":"2025-04-16","title":"Lattice Surgery Compilation Beyond the Surface Code","abstract":"Large-scale fault-tolerant quantum computation requires compiling logical circuits into physical operations tailored to a given architecture. Prior work addressing this challenge has mostly focused on the surface code and lattice surgery schemes. In this work, we broaden the scope by considering lattice surgery compilation for topological codes beyond the surface code. We begin by defining a code substrate - a blueprint for implementing topological codes and lattice surgery. We then abstract from the microscopic details and rephrase the compilation task as a mapping and routing problem on a macroscopic routing graph, potentially subject to substrate-specific constraints. We explore specific substrates and codes, including the color code and the folded surface code, providing detailed microscopic constructions. For the color code, we present numerical simulations analyzing how design choices at the microscopic and macroscopic levels affect the depth of compiled logical $\\mathrm{CNOT}+\\mathrm{T}$ circuits. An open-source code is available on GitHub https://github.com/cda-tum/mqt-qecc.","authors":["Laura S. Herzog","Lucas Berent","Aleksander Kubica","Robert Wille"],"url":"https://arxiv.org/abs/2504.10591"}
{"created":"2025-04-16","title":"Beyond Worst-Case Online Classification: VC-Based Regret Bounds for Relaxed Benchmarks","abstract":"We revisit online binary classification by shifting the focus from competing with the best-in-class binary loss to competing against relaxed benchmarks that capture smoothed notions of optimality. Instead of measuring regret relative to the exact minimal binary error -- a standard approach that leads to worst-case bounds tied to the Littlestone dimension -- we consider comparing with predictors that are robust to small input perturbations, perform well under Gaussian smoothing, or maintain a prescribed output margin. Previous examples of this were primarily limited to the hinge loss. Our algorithms achieve regret guarantees that depend only on the VC dimension and the complexity of the instance space (e.g., metric entropy), and notably, they incur only an $O(\\log(1/\\gamma))$ dependence on the generalized margin $\\gamma$. This stands in contrast to most existing regret bounds, which typically exhibit a polynomial dependence on $1/\\gamma$. We complement this with matching lower bounds. Our analysis connects recent ideas from adversarial robustness and smoothed online learning.","authors":["Omar Montasser","Abhishek Shetty","Nikita Zhivotovskiy"],"url":"https://arxiv.org/abs/2504.10598"}
{"created":"2025-04-16","title":"Re-imagining Spectral Graph Theory","abstract":"We propose a Laplacian based on general inner product spaces, which we call the inner product Laplacian. We show the combinatorial and normalized graph Laplacians, as well as other Laplacians for hypergraphs and directed graphs, are special cases of the inner product Laplacian. After developing the necessary basic theory for the inner product Laplacian, we establish generalized analogs of key isoperimetric inequalities, including the Cheeger inequality and expander mixing lemma. Dirichlet and Neumann subgraph eigenvalues may also be recovered as appropriate limit points of a sequence of inner product Laplacians. In addition to suggesting a new context through which to examine existing Laplacians, this generalized framework is also flexible in applications: through choice of an inner product on the vertices and edges of a graph, the inner product Laplacian naturally encodes both combinatorial structure and domain-knowledge.","authors":["Sinan G. Aksoy","Stephen J. Young"],"url":"https://arxiv.org/abs/2504.10624"}
{"created":"2025-04-16","title":"Who is More Bayesian: Humans or ChatGPT?","abstract":"We compare the performance of human and artificially intelligent (AI) decision makers in simple binary classification tasks where the optimal decision rule is given by Bayes Rule. We reanalyze choices of human subjects gathered from laboratory experiments conducted by El-Gamal and Grether and Holt and Smith. We confirm that while overall, Bayes Rule represents the single best model for predicting human choices, subjects are heterogeneous and a significant share of them make suboptimal choices that reflect judgement biases described by Kahneman and Tversky that include the ``representativeness heuristic'' (excessive weight on the evidence from the sample relative to the prior) and ``conservatism'' (excessive weight on the prior relative to the sample). We compare the performance of AI subjects gathered from recent versions of large language models (LLMs) including several versions of ChatGPT. These general-purpose generative AI chatbots are not specifically trained to do well in narrow decision making tasks, but are trained instead as ``language predictors'' using a large corpus of textual data from the web. We show that ChatGPT is also subject to biases that result in suboptimal decisions. However we document a rapid evolution in the performance of ChatGPT from sub-human performance for early versions (ChatGPT 3.5) to superhuman and nearly perfect Bayesian classifications in the latest versions (ChatGPT 4o).","authors":["Tianshi Mu","Pranjal Rawat","John Rust","Chengjun Zhang","Qixuan Zhong"],"url":"https://arxiv.org/abs/2504.10636"}
{"created":"2025-04-16","title":"On the Asymptotics of the Connectivity Probability of Random Bipartite Graphs","abstract":"In this paper, we analyze the exact asymptotic behavior of the connectivity probability in a random binomial bipartite graph $G(n,m,p)$ under various regimes of the edge probability $p=p(n)$. To determine this probability, a method based on the analysis of inhomogeneous random walks is proposed.","authors":["Boris Chinyaev"],"url":"https://arxiv.org/abs/2504.10640"}
{"created":"2025-04-16","title":"Modeling and solving an integrated periodic vehicle routing and capacitated facility location problem in the context of solid waste collection","abstract":"Few activities are as crucial in urban environments as waste management. Mismanagement of waste can cause significant economic, social, and environmental damage. However, waste management is often a complex system to manage and therefore where computational decision-support tools can play a pivotal role in assisting managers to make faster and better decisions. In this sense, this article proposes, on the one hand, a unified optimization model to address two common waste management system optimization problem: the determination of the capacity of waste bins in the collection network and the design and scheduling of collection routes. The integration of these two problems is not usual in the literature since each of them separately is already a major computational challenge. On the other hand, two improved exact formulations based on mathematical programming and a genetic algorithm (GA) are provided to solve this proposed unified optimization model. It should be noted that the GA considers a mixed chromosome representation of the solutions combining binary and integer alleles, in order to solve realistic instances of this complex problem. Also, different genetic operators have been tested to study which combination of them obtained better results in execution times on the order of that of the exact solvers. The obtained results show that the proposed GA is able to match the results of exact solvers on small instances and, in addition, can obtain feasible solutions on large instances, where exact formulations are not applicable, in reasonable computation times.","authors":["Bego\\~na Gonz\\'alez","Diego Rossit","Mariano Frutos","M\\'aximo M\\'endez"],"url":"https://arxiv.org/abs/2504.10648"}
{"created":"2025-04-16","title":"On the Contractivity of Stochastic Interpolation Flow","abstract":"We investigate stochastic interpolation, a recently introduced framework for high dimensional sampling which bears many similarities to diffusion modeling. Stochastic interpolation generates a data sample by first randomly initializing a particle drawn from a simple base distribution, then simulating deterministic or stochastic dynamics such that in finite time the particle's distribution converges to the target. We show that for a Gaussian base distribution and a strongly log-concave target distribution, the stochastic interpolation flow map is Lipschitz with a sharp constant which matches that of Caffarelli's theorem for optimal transport maps. We are further able to construct Lipschitz transport maps between non-Gaussian distributions, generalizing some recent constructions in the literature on transport methods for establishing functional inequalities. We discuss the practical implications of our theorem for the sampling and estimation problems required by stochastic interpolation.","authors":["Max Daniels"],"url":"https://arxiv.org/abs/2504.10653"}
{"created":"2025-04-16","title":"MatterTune: An Integrated, User-Friendly Platform for Fine-Tuning Atomistic Foundation Models to Accelerate Materials Simulation and Discovery","abstract":"Geometric machine learning models such as graph neural networks have achieved remarkable success in recent years in chemical and materials science research for applications such as high-throughput virtual screening and atomistic simulations. The success of these models can be attributed to their ability to effectively learn latent representations of atomic structures directly from the training data. Conversely, this also results in high data requirements for these models, hindering their application to problems which are data sparse which are common in this domain. To address this limitation, there is a growing development in the area of pre-trained machine learning models which have learned general, fundamental, geometric relationships in atomistic data, and which can then be fine-tuned to much smaller application-specific datasets. In particular, models which are pre-trained on diverse, large-scale atomistic datasets have shown impressive generalizability and flexibility to downstream applications, and are increasingly referred to as atomistic foundation models. To leverage the untapped potential of these foundation models, we introduce MatterTune, a modular and extensible framework that provides advanced fine-tuning capabilities and seamless integration of atomistic foundation models into downstream materials informatics and simulation workflows, thereby lowering the barriers to adoption and facilitating diverse applications in materials science. In its current state, MatterTune supports a number of state-of-the-art foundation models such as ORB, MatterSim, JMP, and EquformerV2, and hosts a wide range of features including a modular and flexible design, distributed and customizable fine-tuning, broad support for downstream informatics tasks, and more.","authors":["Lingyu Kong","Nima Shoghi","Guoxiang Hu","Pan Li","Victor Fung"],"url":"https://arxiv.org/abs/2504.10655"}
{"created":"2025-04-16","title":"Distinct hydrologic response patterns and trends worldwide revealed by physics-embedded learning","abstract":"To track rapid changes within our water sector, Global Water Models (GWMs) need to realistically represent hydrologic systems' response patterns - such as baseflow fraction - but are hindered by their limited ability to learn from data. Here we introduce a high-resolution physics-embedded big-data-trained model as a breakthrough in reliably capturing characteristic hydrologic response patterns ('signatures') and their shifts. By realistically representing the long-term water balance, the model revealed widespread shifts - up to ~20% over 20 years - in fundamental green-blue-water partitioning and baseflow ratios worldwide. Shifts in these response patterns, previously considered static, contributed to increasing flood risks in northern mid-latitudes, heightening water supply stresses in southern subtropical regions, and declining freshwater inputs to many European estuaries, all with ecological implications. With more accurate simulations at monthly and daily scales than current operational systems, this next-generation model resolves large, nonlinear seasonal runoff responses to rainfall ('elasticity') and streamflow flashiness in semi-arid and arid regions. These metrics highlight regions with management challenges due to large water supply variability and high climate sensitivity, but also provide tools to forecast seasonal water availability. This capability newly enables global-scale models to deliver reliable and locally relevant insights for water management.","authors":["Haoyu Ji","Yalan Song","Tadd Bindas","Chaopeng Shen","Yuan Yang","Ming Pan","Jiangtao Liu","Farshid Rahmani","Ather Abbas","Hylke Beck","Yoshihide Wada","Kathryn Lawson"],"url":"https://arxiv.org/abs/2504.10707"}
{"created":"2025-04-16","title":"Improved approximation algorithms for the EPR Hamiltonian","abstract":"The EPR Hamiltonian is a family of 2-local quantum Hamiltonians introduced by King (arXiv:2209.02589). We introduce a polynomial time $\\frac{1+\\sqrt{5}}{4}\\approx 0.809$-approximation algorithm for the problem of computing the ground energy of the EPR Hamiltonian, improving upon the previous state of the art of $0.72$ (arXiv:2410.15544). As a special case, this also implies a $\\frac{1+\\sqrt{5}}{4}$-approximation for Quantum Max Cut on bipartite instances, improving upon the approximation ratio of $3/4$ that one can infer in a relatively straightforward manner from the work of Lee and Parekh (arXiv:2401.03616).","authors":["Nathan Ju","Ansh Nagda"],"url":"https://arxiv.org/abs/2504.10712"}
{"created":"2025-04-16","title":"Cross-Problem Parameter Transfer in Quantum Approximate Optimization Algorithm: A Machine Learning Approach","abstract":"Quantum Approximate Optimization Algorithm (QAOA) is one of the most promising candidates to achieve the quantum advantage in solving combinatorial optimization problems. The process of finding a good set of variational parameters in the QAOA circuit has proven to be challenging due to multiple factors, such as barren plateaus. As a result, there is growing interest in exploiting parameter transferability, where parameter sets optimized for one problem instance are transferred to another that could be more complex either to estimate the solution or to serve as a warm start for further optimization. But can we transfer parameters from one class of problems to another? Leveraging parameter sets learned from a well-studied class of problems could help navigate the less studied one, reducing optimization overhead and mitigating performance pitfalls. In this paper, we study whether pretrained QAOA parameters of MaxCut can be used as is or to warm start the Maximum Independent Set (MIS) circuits. Specifically, we design machine learning models to find good donor candidates optimized on MaxCut and apply their parameters to MIS acceptors. Our experimental results show that such parameter transfer can significantly reduce the number of optimization iterations required while achieving comparable approximation ratios.","authors":["Kien X. Nguyen","Bao Bach","Ilya Safro"],"url":"https://arxiv.org/abs/2504.10733"}
{"created":"2025-04-16","title":"Adaptive Synaptogenesis Implemented on a Nanomagnetic Platform","abstract":"The human brain functions very differently from artificial neural networks (ANN) and possesses unique features that are absent in ANN. An important one among them is \"adaptive synaptogenesis\" that modifies synaptic weights when needed to avoid catastrophic forgetting and promote lifelong learning. The key aspect of this algorithm is supervised Hebbian learning, where weight modifications in the neocortex driven by temporal coincidence are further accepted or vetoed by an added control mechanism from the hippocampus during the training cycle, to make distant synaptic connections highly sparse and strategic. In this work, we discuss various algorithmic aspects of adaptive synaptogenesis tailored to edge computing, demonstrate its function using simulations, and design nanomagnetic hardware accelerators for specific functions of synaptogenesis.","authors":["Faiyaz Elahi Mullick","Supriyo Bandyopadhyay","Rob Baxter","Tony J. Ragucci","Avik W. Ghosh"],"url":"https://arxiv.org/abs/2504.10767"}
{"created":"2025-04-16","title":"Simon's Period Finding on a Quantum Annealer","abstract":"Dating to 1994, Simon's period-finding algorithm is among the earliest and most fragile of quantum algorithms. The algorithm's fragility arises from the requirement that, to solve an n qubit problem, one must fault-tolerantly sample O(n) linearly independent values from a solution space. In this paper, we study an adiabatic implementation of Simon's algorithm that requires a constant number of successful samples regardless of problem size. We implement this algorithm on D-Wave hardware and solve problems with up to 298 qubits. We compare the runtime of classical algorithms to the D-Wave solution to analyze any potential advantage.","authors":["Reece Robertson","Emery Doucet","Zakaria Mzaouali","Krzysztof Domino","Bart{\\l}omiej Gardas","Sebastian Deffner"],"url":"https://arxiv.org/abs/2504.10771"}
{"created":"2025-04-16","title":"Neural Network Emulation of the Classical Limit in Quantum Systems via Learned Observable Mappings","abstract":"The classical limit of quantum mechanics, formally investigated through frameworks like strict deformation quantization, remains a profound area of inquiry in the philosophy of physics. This paper explores a computational approach employing a neural network to emulate the emergence of classical behavior from the quantum harmonic oscillator as Planck's constant $\\hbar$ approaches zero. We develop and train a neural network architecture to learn the mapping from initial expectation values and $\\hbar$ to the time evolution of the expectation value of position. By analyzing the network's predictions across different regimes of hbar, we aim to provide computational insights into the nature of the quantum-classical transition. This work demonstrates the potential of machine learning as a complementary tool for exploring foundational questions in quantum mechanics and its classical limit.","authors":["Kamran Majid"],"url":"https://arxiv.org/abs/2504.10781"}
{"created":"2025-04-16","title":"Wasserstein Distributionally Regret Optimization","abstract":"Distributionally Robust Optimization (DRO) is a popular framework for decision-making under uncertainty, but its adversarial nature can lead to overly conservative solutions. To address this, we study ex-ante Distributionally Robust Regret Optimization (DRRO), focusing on Wasserstein-based ambiguity sets which are popular due to their links to regularization and machine learning. We provide a systematic analysis of Wasserstein DRRO, paralleling known results for Wasserstein DRO. Under smoothness and regularity conditions, we show that Wasserstein DRRO coincides with Empirical Risk Minimization (ERM) up to first-order terms, and exactly so in convex quadratic settings. We revisit the Wasserstein DRRO newsvendor problem, where the loss is the maximum of two linear functions of demand and decision. Extending [25], we show that the regret can be computed by maximizing two one-dimensional concave functions. For more general loss functions involving the maximum of multiple linear terms in multivariate random variables and decision vectors, we prove that computing the regret and thus also the DRRO policy is NP-hard. We then propose a convex relaxation for these more general Wasserstein DRRO problems and demonstrate its strong empirical performance. Finally, we provide an upper bound on the optimality gap of our relaxation and show it improves over recent alternatives.","authors":["Lukas-Benedikt Fiechtner","Jose Blanchet"],"url":"https://arxiv.org/abs/2504.10796"}
{"created":"2025-04-16","title":"Efficient and Robust Remote Sensing Image Denoising Using Randomized Approximation of Geodesics' Gramian on the Manifold Underlying the Patch Space","abstract":"Remote sensing images are widely utilized in many disciplines such as feature recognition and scene semantic segmentation. However, due to environmental factors and the issues of the imaging system, the image quality is often degraded which may impair subsequent visual tasks. Even though denoising remote sensing images plays an essential role before applications, the current denoising algorithms fail to attain optimum performance since these images possess complex features in the texture. Denoising frameworks based on artificial neural networks have shown better performance; however, they require exhaustive training with heterogeneous samples that extensively consume resources like power, memory, computation, and latency. Thus, here we present a computationally efficient and robust remote sensing image denoising method that doesn't require additional training samples. This method partitions patches of a remote-sensing image in which a low-rank manifold, representing the noise-free version of the image, underlies the patch space. An efficient and robust approach to revealing this manifold is a randomized approximation of the singular value spectrum of the geodesics' Gramian matrix of the patch space. The method asserts a unique emphasis on each color channel during denoising so the three denoised channels are merged to produce the final image.","authors":["Kelum Gajamannage","Dilhani I. Jayathilake","Maria Vasilyeva"],"url":"https://arxiv.org/abs/2504.10820"}
{"created":"2025-04-16","title":"Uplink Assisted Joint Channel Estimation and CSI Feedback: An Approach Based on Deep Joint Source-Channel Coding","abstract":"In frequency division duplex (FDD) multiple-input multiple-output (MIMO) wireless communication systems, the acquisition of downlink channel state information (CSI) is essential for maximizing spatial resource utilization and improving system spectral efficiency. The separate design of modules in AI-based CSI feedback architectures under traditional modular communication frameworks, including channel estimation (CE), CSI compression and feedback, leads to sub-optimal performance. In this paper, we propose an uplink assisted joint CE and and CSI feedback approach via deep learning for downlink CSI acquisition, which mitigates performance degradation caused by distribution bias across separately trained modules in traditional modular communication frameworks. The proposed network adopts a deep joint source-channel coding (DJSCC) architecture to mitigate the cliff effect encountered in the conventional separate source-channel coding. Furthermore, we exploit the uplink CSI as auxiliary information to enhance CSI reconstruction accuracy by leveraging the partial reciprocity between the uplink and downlink channels in FDD systems, without introducing additional overhead. The effectiveness of uplink CSI as assisted information and the necessity of an end-toend multi-module joint training architecture is validated through comprehensive ablation and scalability experiments.","authors":["Yiran Guo","Wei Chen","Bo Ai"],"url":"https://arxiv.org/abs/2504.10836"}
{"created":"2025-04-16","title":"A Quantum Advantage in Localizing Transmission Loss Change in Optical Networks","abstract":"The ability to localize transmission loss change to a subset of links in optical networks is crucial for maintaining network reliability, performance and security. \\emph{Quantum probes}, implemented by sending blocks of $n$ coherent-state pulses augmented with continuous-variable (CV) squeezing ($n=1$) or weak temporal-mode entanglement ($n>1$) over a lossy channel to a receiver with homodyne detection capabilities, are known to be more sensitive than their quasi-classical counterparts in detecting a sudden increase in channel loss. The enhanced sensitivity can be characterized by the increased Kullback-Leibler (KL) divergence of the homodyne output, before and after the loss change occurs. When combined with the theory of quickest change detection (QCD), the increase in KL divergence translates into a decrease in detection latency.","authors":["Yufei Zheng","Yu-Zhen Janice Chen","Prithwish Basu","Don Towsley"],"url":"https://arxiv.org/abs/2504.10882"}
{"created":"2025-04-16","title":"Low-Overhead Channel Estimation Framework for Beyond Diagonal Reconfigurable Intelligent Surface Assisted Multi-User MIMO Communication","abstract":"Beyond diagonal reconfigurable intelligent surface (BD-RIS) refers to a family of RIS architectures characterized by scattering matrices not limited to being diagonal and enables higher wave manipulation flexibility and large performance gains over conventional (diagonal) RIS. To achieve those promising gains, accurate channel state information (CSI) needs to be acquired in BD-RIS assisted communication systems. However, the number of coefficients in the cascaded channels to be estimated in BD-RIS assisted systems is significantly larger than that in conventional RIS assisted systems, because the channels associated with the off-diagonal elements of the scattering matrix have to be estimated as well. Surprisingly, for the first time in the literature, this paper rigorously shows that the uplink channel estimation overhead in BD-RIS assisted systems is actually of the same order as that in the conventional RIS assisted systems. This amazing result stems from a key observation: for each user antenna, its cascaded channel matrix associated with one reference BD-RIS element is a scaled version of that associated with any other BD-RIS element due to the common RIS-base station (BS) channel. In other words, the number of independent unknown variables is far less than it would seem at first glance. Building upon this property, this paper manages to characterize the minimum overhead to perfectly estimate all the channels in the ideal case without noise at the BS, and propose a twophase estimation framework for the practical case with noise at the BS. Numerical results demonstrate outstanding channel estimation overhead reduction over existing schemes in BD-RIS assisted systems.","authors":["Rui Wang","Shuowen Zhang","Bruno Clerckx","Liang Liu"],"url":"https://arxiv.org/abs/2504.10911"}
{"created":"2025-04-16","title":"Embedding Radiomics into Vision Transformers for Multimodal Medical Image Classification","abstract":"Background: Deep learning has significantly advanced medical image analysis, with Vision Transformers (ViTs) offering a powerful alternative to convolutional models by modeling long-range dependencies through self-attention. However, ViTs are inherently data-intensive and lack domain-specific inductive biases, limiting their applicability in medical imaging. In contrast, radiomics provides interpretable, handcrafted descriptors of tissue heterogeneity but suffers from limited scalability and integration into end-to-end learning frameworks. In this work, we propose the Radiomics-Embedded Vision Transformer (RE-ViT) that combines radiomic features with data-driven visual embeddings within a ViT backbone.","authors":["Zhenyu Yang","Haiming Zhu","Rihui Zhang","Haipeng Zhang","Jianliang Wang","Chunhao Wang","Minbin Chen","Fang-Fang Yin"],"url":"https://arxiv.org/abs/2504.10916"}
{"created":"2025-04-16","title":"Convergence rate for a semidiscrete approximation of scalar conservation laws","abstract":"We propose a semidiscrete scheme for approximation of entropy solutions of one-dimensional scalar conservation laws with nonnegative initial data. The scheme is based on the concept of particle paths for conservation laws and can be interpreted as a finite-particle discretization. A convergence rate of order $1/2$ with respect to initial particle spacing is proved. As a special case, this covers the convergence of the Follow--the--Leader model to the Lighthill--Whitham--Richards model for traffic flow.","authors":["Magnus C. {\\O}rke"],"url":"https://arxiv.org/abs/2504.10951"}
{"created":"2025-04-16","title":"Early Detection of Cognitive Impairment in Elderly using a Passive FPVS-EEG BCI and Machine Learning -- Extended Version","abstract":"Early dementia diagnosis requires biomarkers sensitive to both structural and functional brain changes. While structural neuroimaging biomarkers have progressed significantly, objective functional biomarkers of early cognitive decline remain a critical unmet need. Current cognitive assessments often rely on behavioral responses, making them susceptible to factors like effort, practice effects, and educational background, thereby hindering early and accurate detection. This work introduces a novel approach, leveraging a lightweight convolutional neural network (CNN) to infer cognitive impairment levels directly from electroencephalography (EEG) data. Critically, this method employs a passive fast periodic visual stimulation (FPVS) paradigm, eliminating the need for explicit behavioral responses or task comprehension from the participant. This passive approach provides an objective measure of working memory function, independent of confounding factors inherent in active cognitive tasks, and offers a promising new avenue for early and unbiased detection of cognitive decline.","authors":["Tomasz M. Rutkowski","Stanis{\\l}aw Nar\\k{e}bski","Mihoko Otake-Matsuura","Tomasz Komendzi\\'nski"],"url":"https://arxiv.org/abs/2504.10973"}
{"created":"2025-04-16","title":"AgentPolyp: Accurate Polyp Segmentation via Image Enhancement Agent","abstract":"Since human and environmental factors interfere, captured polyp images usually suffer from issues such as dim lighting, blur, and overexposure, which pose challenges for downstream polyp segmentation tasks. To address the challenges of noise-induced degradation in polyp images, we present AgentPolyp, a novel framework integrating CLIP-based semantic guidance and dynamic image enhancement with a lightweight neural network for segmentation. The agent first evaluates image quality using CLIP-driven semantic analysis (e.g., identifying ``low-contrast polyps with vascular textures\") and adapts reinforcement learning strategies to dynamically apply multi-modal enhancement operations (e.g., denoising, contrast adjustment). A quality assessment feedback loop optimizes pixel-level enhancement and segmentation focus in a collaborative manner, ensuring robust preprocessing before neural network segmentation. This modular architecture supports plug-and-play extensions for various enhancement algorithms and segmentation networks, meeting deployment requirements for endoscopic devices.","authors":["Pu Wang","Zhihua Zhang","Dianjie Lu","Guijuan Zhang","Youshan Zhang","Zhuoran Zheng"],"url":"https://arxiv.org/abs/2504.10978"}
{"created":"2025-04-16","title":"Using Time Structure to Estimate Causal Effects","abstract":"There exist several approaches for estimating causal effects in time series when latent confounding is present. Many of these approaches rely on additional auxiliary observed variables or time series such as instruments, negative controls or time series that satisfy the front- or backdoor criterion in certain graphs. In this paper, we present a novel approach for estimating direct (and via Wright's path rule total) causal effects in a time series setup which does not rely on additional auxiliary observed variables or time series. This approach assumes that the underlying time series is a Structural Vector Autoregressive (SVAR) process and estimates direct causal effects by solving certain linear equation systems made up of different covariances and model parameters. We state sufficient graphical criteria in terms of the so-called full time graph under which these linear equations systems are uniquely solvable and under which their solutions contain the to-be-identified direct causal effects as components. We also state sufficient lag-based criteria under which the previously mentioned graphical conditions are satisfied and, thus, under which direct causal effects are identifiable. Several numerical experiments underline the correctness and applicability of our results.","authors":["Tom Hochsprung","Jakob Runge","Andreas Gerhardus"],"url":"https://arxiv.org/abs/2504.11076"}
{"created":"2025-04-16","title":"QAMA: Quantum annealing multi-head attention operator with classical deep learning framework","abstract":"As large language models scale up, the conventional attention mechanism faces critical challenges of exponential growth in memory consumption and energy costs. Quantum annealing computing, with its inherent advantages in computational efficiency and low energy consumption, offers an innovative direction for constructing novel deep learning architectures. This study proposes the first Quantum Annealing-based Multi-head Attention (QAMA) mechanism, achieving seamless compatibility with classical attention architectures through quadratic unconstrained binary optimization (QUBO) modeling of forward propagation and energy-based backpropagation. The method innovatively leverages the quantum bit interaction characteristics of Ising models to optimize the conventional $O(n^2)$ spatiotemporal complexity into linear resource consumption. Integrated with the optical computing advantages of coherent Ising machines (CIM), the system maintains millisecond-level real-time responsiveness while significantly reducing energy consumption. Our key contributions include: Theoretical proofs establish QAMA mathematical equivalence to classical attention mechanisms; Dual optimization of multi-head specificity and long-range information capture via QUBO constraints; Explicit gradient proofs for the Ising energy equation are utilized to implement gradient conduction as the only path in the computational graph as a layer; Proposed soft selection mechanism overcoming traditional binary attention limitations to approximate continuous weights. Experiments on QBoson CPQC quantum computer show QAMA achieves comparable accuracy to classical operators while reducing inference time to millisecond level and improving solution quality. This work pioneers architectural-level integration of quantum computing and deep learning, applicable to any attention-based model, driving paradigm innovation in AI foundational computing.","authors":["Peng Du","Shuolei Wang","Shicheng Li","Jinjing Shi"],"url":"https://arxiv.org/abs/2504.11083"}
{"created":"2025-04-16","title":"AI-guided Antibiotic Discovery Pipeline from Target Selection to Compound Identification","abstract":"Antibiotic resistance presents a growing global health crisis, demanding new therapeutic strategies that target novel bacterial mechanisms. Recent advances in protein structure prediction and machine learning-driven molecule generation offer a promising opportunity to accelerate drug discovery. However, practical guidance on selecting and integrating these models into real-world pipelines remains limited. In this study, we develop an end-to-end, artificial intelligence-guided antibiotic discovery pipeline that spans target identification to compound realization. We leverage structure-based clustering across predicted proteomes of multiple pathogens to identify conserved, essential, and non-human-homologous targets. We then systematically evaluate six leading 3D-structure-aware generative models$\\unicode{x2014}$spanning diffusion, autoregressive, graph neural network, and language model architectures$\\unicode{x2014}$on their usability, chemical validity, and biological relevance. Rigorous post-processing filters and commercial analogue searches reduce over 100 000 generated compounds to a focused, synthesizable set. Our results highlight DeepBlock and TamGen as top performers across diverse criteria, while also revealing critical trade-offs between model complexity, usability, and output quality. This work provides a comparative benchmark and blueprint for deploying artificial intelligence in early-stage antibiotic development.","authors":["Maximilian G. Schuh","Joshua Hesse","Stephan A. Sieber"],"url":"https://arxiv.org/abs/2504.11091"}
{"created":"2025-04-16","title":"Fine-Tuning Large Language Models on Quantum Optimization Problems for Circuit Generation","abstract":"Large language models (LLM) have achieved remarkable outcomes in addressing complex problems, including math, coding, and analyzing large amounts of scientific reports. Yet few works have explored the potential of LLM in quantum computing. The most challenging problem is how to leverage LLMs to automatically generate quantum circuits at a large scale. In this paper, we address such a challenge by fine-tuning LLMs and injecting the domain-specific knowledge of quantum computing. In particular, we investigate the mechanisms to generate training data sets and construct the end-to-end pipeline to fine-tune pre-trained LLMs that produce parameterized quantum circuits for optimization problems. We have prepared 14,000 quantum circuits covering a substantial part of the quantum optimization landscape: 12 optimization problem instances and their optimized QAOA, VQE, and adaptive VQE circuits. The fine-tuned LLMs can construct syntactically correct parametrized quantum circuits in the most recent OpenQASM 3.0. We have evaluated the quality of the parameters by comparing them to the optimized expectation values and distributions. Our evaluation shows that the fine-tuned LLM outperforms state-of-the-art models and that the parameters are better than random. The LLM-generated parametrized circuits and initial parameters can be used as a starting point for further optimization, \\emph{e.g.,} templates in quantum machine learning and the benchmark for compilers and hardware.","authors":["Linus Jern","Valter Uotila","Cong Yu","Bo Zhao"],"url":"https://arxiv.org/abs/2504.11109"}
{"created":"2025-04-16","title":"Scalable Transceiver Design for Multi-User Communication in FDD Massive MIMO Systems via Deep Learning","abstract":"This paper addresses the joint transceiver design, including pilot transmission, channel feature extraction and feedback, as well as precoding, for low-overhead downlink massive multiple-input multiple-output (MIMO) communication in frequency-division duplex (FDD) systems. Although deep learning (DL) has shown great potential in tackling this problem, existing methods often suffer from poor scalability in practical systems, as the solution obtained in the training phase merely works for a fixed feedback capacity and a fixed number of users in the deployment phase. To address this limitation, we propose a novel DL-based framework comprised of choreographed neural networks, which can utilize one training phase to generate all the transceiver solutions used in the deployment phase with varying sizes of feedback codebooks and numbers of users. The proposed framework includes a residual vector-quantized variational autoencoder (RVQ-VAE) for efficient channel feedback and an edge graph attention network (EGAT) for robust multiuser precoding. It can adapt to different feedback capacities by flexibly adjusting the RVQ codebook sizes using the hierarchical codebook structure, and scale with the number of users through a feedback module sharing scheme and the inherent scalability of EGAT. Moreover, a progressive training strategy is proposed to further enhance data transmission performance and generalization capability. Numerical results on a real-world dataset demonstrate the superior scalability and performance of our approach over existing methods.","authors":["Lin Zhu","Weifeng Zhu","Shuowen Zhang","Shuguang Cui","Liang Liu"],"url":"https://arxiv.org/abs/2504.11162"}
{"created":"2025-04-16","title":"Characterizing High Schmidt Number Witnesses in Arbitrary Dimensions System","abstract":"A profound comprehension of quantum entanglement is crucial for the progression of quantum technologies. The degree of entanglement can be assessed by enumerating the entangled degrees of freedom, leading to the determination of a parameter known as the Schmidt number. In this paper, we develop an efficient analytical tool for characterizing high Schmidt number witnesses for bipartite quantum states in arbitrary dimensions. Our methods not only offer viable mathematical methods for constructing high-dimensional Schmidt number witnesses in theory but also simplify the quantification of entanglement and dimensionality. Most notably, we develop high-dimensional Schmidt number witnesses within arbitrary-dimensional systems, with our Schmidt witness coefficients relying solely on the operator Schmidt coefficient. Subsequently, we demonstrate our theoretical advancements and computational superiority by constructing Schmidt number witnesses in arbitrary dimensional bipartite quantum systems with Schmidt numbers four and five.","authors":["Liang Xiong","Nung-sing Sze"],"url":"https://arxiv.org/abs/2504.11213"}
{"created":"2025-04-16","title":"Respiratory Inhaler Sound Event Classification Using Self-Supervised Learning","abstract":"Asthma is a chronic respiratory condition that affects millions of people worldwide. While this condition can be managed by administering controller medications through handheld inhalers, clinical studies have shown low adherence to the correct inhaler usage technique. Consequently, many patients may not receive the full benefit of their medication. Automated classification of inhaler sounds has recently been studied to assess medication adherence. However, the existing classification models were typically trained using data from specific inhaler types, and their ability to generalize to sounds from different inhalers remains unexplored. In this study, we adapted the wav2vec 2.0 self-supervised learning model for inhaler sound classification by pre-training and fine-tuning this model on inhaler sounds. The proposed model shows a balanced accuracy of 98% on a dataset collected using a dry powder inhaler and smartwatch device. The results also demonstrate that re-finetuning this model on minimal data from a target inhaler is a promising approach to adapting a generic inhaler sound classification model to a different inhaler device and audio capture hardware. This is the first study in the field to demonstrate the potential of smartwatches as assistive technologies for the personalized monitoring of inhaler adherence using machine learning models.","authors":["Davoud Shariat Panah","Alessandro N Franciosi","Cormac McCarthy","Andrew Hines"],"url":"https://arxiv.org/abs/2504.11246"}
{"created":"2025-04-16","title":"Cryo-em images are intrinsically low dimensional","abstract":"Simulation-based inference provides a powerful framework for cryo-electron microscopy, employing neural networks in methods like CryoSBI to infer biomolecular conformations via learned latent representations. This latent space represents a rich opportunity, encoding valuable information about the physical system and the inference process. Harnessing this potential hinges on understanding the underlying geometric structure of these representations. We investigate this structure by applying manifold learning techniques to CryoSBI representations of hemagglutinin (simulated and experimental). We reveal that these high-dimensional data inherently populate low-dimensional, smooth manifolds, with simulated data effectively covering the experimental counterpart. By characterizing the manifold's geometry using Diffusion Maps and identifying its principal axes of variation via coordinate interpretation methods, we establish a direct link between the latent structure and key physical parameters. Discovering this intrinsic low-dimensionality and interpretable geometric organization not only validates the CryoSBI approach but enables us to learn more from the data structure and provides opportunities for improving future inference strategies by exploiting this revealed manifold geometry.","authors":["Luke Evans","Octavian-Vlad Murad","Lars Dingeldein","Pilar Cossio","Roberto Covino","Marina Meila"],"url":"https://arxiv.org/abs/2504.11249"}
{"created":"2025-04-16","title":"Multi-Agent Reinforcement Learning for Greenhouse Gas Offset Credit Markets","abstract":"Climate change is a major threat to the future of humanity, and its impacts are being intensified by excess man-made greenhouse gas emissions. One method governments can employ to control these emissions is to provide firms with emission limits and penalize any excess emissions above the limit. Excess emissions may also be offset by firms who choose to invest in carbon reducing and capturing projects. These projects generate offset credits which can be submitted to a regulating agency to offset a firm's excess emissions, or they can be traded with other firms. In this work, we characterize the finite-agent Nash equilibrium for offset credit markets. As computing Nash equilibria is an NP-hard problem, we utilize the modern reinforcement learning technique Nash-DQN to efficiently estimate the market's Nash equilibria. We demonstrate not only the validity of employing reinforcement learning methods applied to climate themed financial markets, but also the significant financial savings emitting firms may achieve when abiding by the Nash equilibria through numerical experiments.","authors":["Liam Welsh","Udit Grover","Sebastian Jaimungal"],"url":"https://arxiv.org/abs/2504.11258"}
{"created":"2025-04-16","title":"Efficient Medical Image Restoration via Reliability Guided Learning in Frequency Domain","abstract":"Medical image restoration tasks aim to recover high-quality images from degraded observations, exhibiting emergent desires in many clinical scenarios, such as low-dose CT image denoising, MRI super-resolution, and MRI artifact removal. Despite the success achieved by existing deep learning-based restoration methods with sophisticated modules, they struggle with rendering computationally-efficient reconstruction results. Moreover, they usually ignore the reliability of the restoration results, which is much more urgent in medical systems. To alleviate these issues, we present LRformer, a Lightweight Transformer-based method via Reliability-guided learning in the frequency domain. Specifically, inspired by the uncertainty quantification in Bayesian neural networks (BNNs), we develop a Reliable Lesion-Semantic Prior Producer (RLPP). RLPP leverages Monte Carlo (MC) estimators with stochastic sampling operations to generate sufficiently-reliable priors by performing multiple inferences on the foundational medical image segmentation model, MedSAM. Additionally, instead of directly incorporating the priors in the spatial domain, we decompose the cross-attention (CA) mechanism into real symmetric and imaginary anti-symmetric parts via fast Fourier transform (FFT), resulting in the design of the Guided Frequency Cross-Attention (GFCA) solver. By leveraging the conjugated symmetric property of FFT, GFCA reduces the computational complexity of naive CA by nearly half. Extensive experimental results in various tasks demonstrate the superiority of the proposed LRformer in both effectiveness and efficiency.","authors":["Pengcheng Zheng","Kecheng Chen","Jiaxin Huang","Bohao Chen","Ju Liu","Yazhou Ren","Xiaorong Pu"],"url":"https://arxiv.org/abs/2504.11286"}
{"created":"2025-04-16","title":"Efficient and Stable Multi-Dimensional Kolmogorov-Smirnov Distance","abstract":"We revisit extending the Kolmogorov-Smirnov distance between probability distributions to the multidimensional setting and make new arguments about the proper way to approach this generalization. Our proposed formulation maximizes the difference over orthogonal dominating rectangular ranges (d-sided rectangles in R^d), and is an integral probability metric. We also prove that the distance between a distribution and a sample from the distribution converges to 0 as the sample size grows, and bound this rate. Moreover, we show that one can, up to this same approximation error, compute the distance efficiently in 4 or fewer dimensions; specifically the runtime is near-linear in the size of the sample needed for that error. With this, we derive a delta-precision two-sample hypothesis test using this distance. Finally, we show these metric and approximation properties do not hold for other popular variants.","authors":["Peter Matthew Jacobs","Foad Namjoo","Jeff M. Phillips"],"url":"https://arxiv.org/abs/2504.11299"}
{"created":"2025-04-16","title":"Limits of Discrete Energy of Families of Increasing Sets","abstract":"The Hausdorff dimension of a set can be detected using the Riesz energy. Here, we consider situations where a sequence of points, $\\{x_n\\}$, ``fills in'' a set $E \\subset \\mathbb{R}^d$ in an appropriate sense and investigate the degree to which the discrete analog to the Riesz energy of these sets can be used to bound the Hausdorff dimension of $E$. We also discuss applications to data science and Erd\\H{o}s/Falconer type problems.","authors":["Hari Nathan"],"url":"https://arxiv.org/abs/2504.11302"}
{"created":"2025-04-16","title":"Differentially Private Geodesic and Linear Regression","abstract":"In statistical applications it has become increasingly common to encounter data structures that live on non-linear spaces such as manifolds. Classical linear regression, one of the most fundamental methodologies of statistical learning, captures the relationship between an independent variable and a response variable which both are assumed to live in Euclidean space. Thus, geodesic regression emerged as an extension where the response variable lives on a Riemannian manifold. The parameters of geodesic regression, as with linear regression, capture the relationship of sensitive data and hence one should consider the privacy protection practices of said parameters. We consider releasing Differentially Private (DP) parameters of geodesic regression via the K-Norm Gradient (KNG) mechanism for Riemannian manifolds. We derive theoretical bounds for the sensitivity of the parameters showing they are tied to their respective Jacobi fields and hence the curvature of the space. This corroborates recent findings of differential privacy for the Fr\\'echet mean. We demonstrate the efficacy of our methodology on the sphere, $\\mbS^2\\subset\\mbR^3$ and, since it is general to Riemannian manifolds, the manifold of Euclidean space which simplifies geodesic regression to a case of linear regression. Our methodology is general to any Riemannian manifold and thus it is suitable for data in domains such as medical imaging and computer vision.","authors":["Aditya Kulkarni","Carlos Soto"],"url":"https://arxiv.org/abs/2504.11304"}
{"created":"2025-04-16","title":"Mildly-Interacting Fermionic Unitaries are Efficiently Learnable","abstract":"Recent work has shown that one can efficiently learn fermionic Gaussian unitaries, also commonly known as nearest-neighbor matchcircuits or non-interacting fermionic unitaries. However, one could ask a similar question about unitaries that are near Gaussian: for example, unitaries prepared with a small number of non-Gaussian circuit elements. These operators find significance in quantum chemistry and many-body physics, yet no algorithm exists to learn them.","authors":["Vishnu Iyer"],"url":"https://arxiv.org/abs/2504.11318"}
{"created":"2025-04-16","title":"Network Alignment","abstract":"Complex networks are frequently employed to model physical or virtual complex systems. When certain entities exist across multiple systems simultaneously, unveiling their corresponding relationships across the networks becomes crucial. This problem, known as network alignment, holds significant importance. It enhances our understanding of complex system structures and behaviours, facilitates the validation and extension of theoretical physics research about studying complex systems, and fosters diverse practical applications across various fields. However, due to variations in the structure, characteristics, and properties of complex networks across different fields, the study of network alignment is often isolated within each domain, with even the terminologies and concepts lacking uniformity. This review comprehensively summarizes the latest advancements in network alignment research, focusing on analyzing network alignment characteristics and progress in various domains such as social network analysis, bioinformatics, computational linguistics and privacy protection. It provides a detailed analysis of various methods' implementation principles, processes, and performance differences, including structure consistency-based methods, network embedding-based methods, and graph neural network-based (GNN-based) methods. Additionally, the methods for network alignment under different conditions, such as in attributed networks, heterogeneous networks, directed networks, and dynamic networks, are presented. Furthermore, the challenges and the open issues for future studies are also discussed.","authors":["Rui Tang","Ziyun Yong","Shuyu Jiang","Xingshu Chen","Yaofang Liu","Yi-Cheng Zhang","Gui-Quan Sun","Wei Wang"],"url":"https://arxiv.org/abs/2504.11367"}
{"created":"2025-04-16","title":"Taxonomy of Prediction","abstract":"A prediction makes a claim about a system's future given knowledge of its past. A retrodiction makes a claim about its past given knowledge of its future. We introduce the ambidextrous hidden Markov chain that does both optimally -- the bidirectional machine whose state structure makes explicit all statistical correlations in a stochastic process. We introduce an informational taxonomy to profile these correlations via a suite of multivariate information measures. While prior results laid out the different kinds of information contained in isolated measurements, in addition to being limited to single measurements the associated informations were challenging to calculate explicitly. Overcoming these via bidirectional machine states, we expand that analysis to information embedded across sequential measurements. The result highlights fourteen new interpretable and calculable information measures that fully characterize a process' informational structure. Additionally, we introduce a labeling and indexing scheme that systematizes information-theoretic analyses of highly complex multivariate systems. Operationalizing this, we provide algorithms to directly calculate all of these quantities in closed form for finitely-modeled processes.","authors":["Alexandra Jurgens","James P. Crutchfield"],"url":"https://arxiv.org/abs/2504.11371"}
{"created":"2025-04-16","title":"A Review of Traffic Wave Suppression Strategies: Variable Speed Limit vs. Jam-Absorption Driving","abstract":"The main form of freeway traffic congestion is the familiar stop-and-go wave, characterized by wide moving jams that propagate indefinitely upstream provided enough traffic demand. They cause severe, long-lasting adverse effects, such as reduced traffic efficiency, increased driving risks, and higher vehicle emissions. This underscores the crucial importance of artificial intervention in the propagation of stop-and-go waves. Over the past two decades, two prominent strategies for stop-and-go wave suppression have emerged: variable speed limit (VSL) and jam-absorption driving (JAD). Although they share similar research motivations, objectives, and theoretical foundations, the development of these strategies has remained relatively disconnected. To synthesize fragmented advances and drive the field forward, this paper first provides a comprehensive review of the achievements in the stop-and-go wave suppression-oriented VSL and JAD, respectively. It then focuses on bridging the two areas and identifying research opportunities from the following perspectives: fundamental diagrams, traffic dynamics modeling, traffic state estimation and prediction, stochasticity, scenarios for strategy validation, and field tests and practical deployment. We expect that through this review, one area can effectively address its limitations by identifying and leveraging the strengths of the other, thus promoting the overall research goal of freeway stop-and-go wave suppression.","authors":["Zhengbing He","Jorge Laval","Yu Han","Ryosuke Nishi","Cathy Wu"],"url":"https://arxiv.org/abs/2504.11372"}
{"created":"2025-04-16","title":"Shifting Work Patterns with Generative AI","abstract":"We present evidence on how generative AI changes the work patterns of knowledge workers using data from a 6-month-long, cross-industry, randomized field experiment. Half of the 6,000 workers in the study received access to a generative AI tool integrated into the applications they already used for emails, document creation, and meetings. We find that access to the AI tool during the first year of its release primarily impacted behaviors that could be changed independently and not behaviors that required coordination to change: workers who used the tool spent 3 fewer hours, or 25% less time on email each week (intent to treat estimate is 1.4 hours) and seemed to complete documents moderately faster, but did not significantly change time spent in meetings.","authors":["Eleanor Wiske Dillon","Sonia Jaffe","Nicole Immorlica","Christopher T. Stanton"],"url":"https://arxiv.org/abs/2504.11436"}
{"created":"2025-04-16","title":"Greedy Restart Schedules: A Baseline for Dynamic Algorithm Selection on Numerical Black-box Optimization Problems","abstract":"In many optimization domains, there are multiple different solvers that contribute to the overall state-of-the-art, each performing better on some, and worse on other types of problem instances. Meta-algorithmic approaches, such as instance-based algorithm selection, configuration and scheduling, aim to close this gap by extracting the most performance possible from a set of (configurable) optimizers. In this context, the best performing individual algorithms are often hand-crafted hybrid heuristics which perform many restarts of fast local optimization approaches. However, data-driven techniques to create optimized restart schedules have not yet been extensively studied.","authors":["Lennart Sch\\\"apermeier"],"url":"https://arxiv.org/abs/2504.11440"}
{"created":"2025-04-16","title":"Early Impacts of M365 Copilot","abstract":"Advances in generative AI have rapidly expanded the potential of computers to perform or assist in a wide array of tasks traditionally performed by humans. We analyze a large, real-world randomized experiment of over 6,000 workers at 56 firms to present some of the earliest evidence on how these technologies are changing the way knowledge workers do their jobs. We find substantial time savings on common core tasks across a wide range of industries and occupations: workers who make use of this technology spent half an hour less reading email each week and completed documents 12% faster. Despite the newness of the technology, nearly 40% of workers who were given access to the tool used it regularly in their work throughout the 6-month study.","authors":["Eleanor Wiske Dillon","Sonia Jaffe","Sida Peng","Alexia Cambon"],"url":"https://arxiv.org/abs/2504.11443"}
{"created":"2025-04-16","title":"Fault Tolerant Quantum Simulation via Symplectic Transvections","abstract":"Conventional approaches to fault-tolerant quantum computing realize logical circuits gate-by-gate, synthesizing each gate independently on one or more code blocks. This incurs excess overhead and doesn't leverage common structures in quantum algorithms. In contrast, we propose a framework that enables the execution of entire logical circuit blocks at once, preserving their global structure. This whole-block approach allows for the direct implementation of logical Trotter circuits - of arbitrary rotation angles - on any stabilizer code, providing a powerful new method for fault tolerant Hamiltonian simulation within a single code block. At the heart of our approach lies a deep structural correspondence between symplectic transvections and Trotter circuits. This connection enables both logical and physical circuits to share the Trotter structure while preserving stabilizer centralization and circuit symmetry even in the presence of non-Clifford rotations. We discuss potential approaches to fault tolerance via biased noise and code concatenation. While we illustrate the key principles using a $[[8,3,3]]$ code, our simulations show that the framework applies to Hamiltonian simulation on even good quantum LDPC codes. These results open the door to new algorithm-tailored, block-level strategies for fault tolerant circuit design, especially in quantum simulation.","authors":["Zhuangzhuang Chen","Jack Owen Weinberg","Narayanan Rengaswamy"],"url":"https://arxiv.org/abs/2504.11444"}
{"created":"2025-04-16","title":"Nodal auxiliary a posteriori error estimates","abstract":"We introduce and explain key relations between a posteriori error estimates and subspace correction methods viewed as preconditioners for problems in infinite dimensional Hilbert spaces. We set the stage using the Finite Element Exterior Calculus and Nodal Auxiliary Space Preconditioning. This framework provides a systematic way to derive explicit residual estimators and estimators based on local problems which are upper and lower bounds of the true error. We show the applications to discretizations of $\\delta d$, curl-curl, grad-div, Hodge Laplacian problems, and linear elasticity with weak symmetry. We also provide a new regular decomposition for singularly perturbed H(d) norms and parameter-independent error estimators. The only ingredients needed are: well-posedness of the problem and the existence of regular decomposition on continuous level.","authors":["Yuwen Li","Ludmil T. Zikatanov"],"url":"https://arxiv.org/abs/2010.06774"}
{"created":"2025-04-16","title":"Longitudinal Analysis of Privacy Labels in the Apple App Store","abstract":"In December of 2020, Apple started to require app developers to self-report privacy label annotations on their apps indicating what data is collected and how it is used.To understand the adoption and shifts in privacy labels in the App Store, we collected nearly weekly snapshots of over 1.6 million apps for over a year (July 15, 2021 -- October 25, 2022) to understand the dynamics of privacy label ecosystem. Nearly two years after privacy labels launched, only 70.1% of apps have privacy labels, but we observed an increase of 28% during the measurement period. Privacy label adoption rates are mostly driven by new apps rather than older apps coming into compliance. Of apps with labels, 18.1% collect data used to track users, 38.1% collect data that is linked to a user identity, and 42.0% collect data that is not linked. A surprisingly large share (41.8%) of apps with labels indicate that they do not collect any data, and while we do not perform direct analysis of the apps to verify this claim, we observe that it is likely that many of these apps are choosing a Does Not Collect label due to being forced to select a label, rather than this being the true behavior of the app. Moreover, for apps that have assigned labels during the measurement period nearly all do not change their labels, and when they do, the new labels indicate more data collection than less. This suggests that privacy labels may be a ``set once'' mechanism for developers that may not actually provide users with the clarity needed to make informed privacy decisions.","authors":["David G. Balash","Mir Masood Ali","Monica Kodwani","Xiaoyuan Wu","Chris Kanich","Adam J. Aviv"],"url":"https://arxiv.org/abs/2206.02658"}
{"created":"2025-04-16","title":"Experiential Explanations for Reinforcement Learning","abstract":"Reinforcement learning (RL) systems can be complex and non-interpretable, making it challenging for non-AI experts to understand or intervene in their decisions. This is due in part to the sequential nature of RL in which actions are chosen because of their likelihood of obtaining future rewards. However, RL agents discard the qualitative features of their training, making it difficult to recover user-understandable information for \"why\" an action is chosen. We propose a technique Experiential Explanations to generate counterfactual explanations by training influence predictors along with the RL policy. Influence predictors are models that learn how different sources of reward affect the agent in different states, thus restoring information about how the policy reflects the environment. Two human evaluation studies revealed that participants presented with Experiential Explanations were better able to correctly guess what an agent would do than those presented with other standard types of explanation. Participants also found that Experiential Explanations are more understandable, satisfying, complete, useful, and accurate. Qualitative analysis provides information on the factors of Experiential Explanations that are most useful and the desired characteristics that participants seek from the explanations.","authors":["Amal Alabdulkarim","Madhuri Singh","Gennie Mansi","Kaely Hall","Upol Ehsan","Mark O. Riedl"],"url":"https://arxiv.org/abs/2210.04723"}
{"created":"2025-04-16","title":"FairPy: A Toolkit for Evaluation of Prediction Biases and their Mitigation in Large Language Models","abstract":"Recent studies have demonstrated that large pretrained language models (LLMs) such as BERT and GPT-2 exhibit biases in token prediction, often inherited from the data distributions present in their training corpora. In response, a number of mathematical frameworks have been proposed to quantify, identify, and mitigate these the likelihood of biased token predictions. In this paper, we present a comprehensive survey of such techniques tailored towards widely used LLMs such as BERT, GPT-2, etc. We additionally introduce Fairpy, a modular and extensible toolkit that provides plug-and-play interfaces for integrating these mathematical tools, enabling users to evaluate both pretrained and custom language models. Fairpy supports the implementation of existing debiasing algorithms. The toolkit is open-source and publicly available at: \\href{https://github.com/HrishikeshVish/Fairpy}{https://github.com/HrishikeshVish/Fairpy}","authors":["Hrishikesh Viswanath","Tianyi Zhang"],"url":"https://arxiv.org/abs/2302.05508"}
{"created":"2025-04-16","title":"GarmentTracking: Category-Level Garment Pose Tracking","abstract":"Garments are important to humans. A visual system that can estimate and track the complete garment pose can be useful for many downstream tasks and real-world applications. In this work, we present a complete package to address the category-level garment pose tracking task: (1) A recording system VR-Garment, with which users can manipulate virtual garment models in simulation through a VR interface. (2) A large-scale dataset VR-Folding, with complex garment pose configurations in manipulation like flattening and folding. (3) An end-to-end online tracking framework GarmentTracking, which predicts complete garment pose both in canonical space and task space given a point cloud sequence. Extensive experiments demonstrate that the proposed GarmentTracking achieves great performance even when the garment has large non-rigid deformation. It outperforms the baseline approach on both speed and accuracy. We hope our proposed solution can serve as a platform for future research. Codes and datasets are available in https://garment-tracking.robotflow.ai.","authors":["Han Xue","Wenqiang Xu","Jieyi Zhang","Tutian Tang","Yutong Li","Wenxin Du","Ruolin Ye","Cewu Lu"],"url":"https://arxiv.org/abs/2303.13913"}
{"created":"2025-04-16","title":"Privacy-Preserving CNN Training with Transfer Learning: Multiclass Logistic Regression","abstract":"In this paper, we present a practical solution to implement privacy-preserving CNN training based on mere Homomorphic Encryption (HE) technique. To our best knowledge, this is the first attempt successfully to crack this nut and no work ever before has achieved this goal. Several techniques combine to accomplish the task:: (1) with transfer learning, privacy-preserving CNN training can be reduced to homomorphic neural network training, or even multiclass logistic regression (MLR) training; (2) via a faster gradient variant called $\\texttt{Quadratic Gradient}$, an enhanced gradient method for MLR with a state-of-the-art performance in convergence speed is applied in this work to achieve high performance; (3) we employ the thought of transformation in mathematics to transform approximating Softmax function in the encryption domain to the approximation of the Sigmoid function. A new type of loss function termed $\\texttt{Squared Likelihood Error}$ has been developed alongside to align with this change.; and (4) we use a simple but flexible matrix-encoding method named $\\texttt{Volley Revolver}$ to manage the data flow in the ciphertexts, which is the key factor to complete the whole homomorphic CNN training. The complete, runnable C++ code to implement our work can be found at: \\href{https://github.com/petitioner/HE.CNNtraining}{$\\texttt{https://github.com/petitioner/HE.CNNtraining}$}. We select $\\texttt{REGNET\\_X\\_400MF}$ as our pre-trained model for transfer learning. We use the first 128 MNIST training images as training data and the whole MNIST testing dataset as the testing data. The client only needs to upload 6 ciphertexts to the cloud and it takes $\\sim 21$ mins to perform 2 iterations on a cloud with 64 vCPUs, resulting in a precision of $21.49\\%$.","authors":["John Chiang"],"url":"https://arxiv.org/abs/2304.03807"}
{"created":"2025-04-16","title":"AwesomeMeta+: A Mixed-Prototyping Meta-Learning System Supporting AI Application Design Anywhere","abstract":"Meta-learning, also known as ``learning to learn'', enables models to acquire great generalization abilities by learning from various tasks. Recent advancements have made these models applicable across various fields without data constraints, offering new opportunities for general artificial intelligence. However, applying these models can be challenging due to their often task-specific, standalone nature and the technical barriers involved. To address this challenge, we develop AwesomeMeta+, a prototyping and learning system designed to standardize the key components of meta-learning within the context of systems engineering. It standardizes different components of meta-learning and uses a building block metaphor to assist in model construction. By employing a modular, building-block approach, AwesomeMeta+ facilitates the construction of meta-learning models that can be adapted and optimized for specific application needs in real-world systems. The system is developed to support the full lifecycle of meta-learning system engineering, from design to deployment, by enabling users to assemble compatible algorithmic modules. We evaluate AwesomeMeta+ through feedback from 50 researchers and a series of machine-based tests and user studies. The results demonstrate that AwesomeMeta+ enhances users' understanding of meta-learning principles, accelerates system engineering processes, and provides valuable decision-making support for efficient deployment of meta-learning systems in complex application scenarios.","authors":["Jingyao Wang","Yuxuan Yang","Wenwen Qiang","Changwen Zheng","Fuchun Sun"],"url":"https://arxiv.org/abs/2304.12921"}
{"created":"2025-04-16","title":"Inferring Communities of Interest in Collaborative Learning-based Recommender Systems","abstract":"Collaborative-learning-based recommender systems, such as those employing Federated Learning (FL) and Gossip Learning (GL), allow users to train models while keeping their history of liked items on their devices. While these methods were seen as promising for enhancing privacy, recent research has shown that collaborative learning can be vulnerable to various privacy attacks. In this paper, we propose a novel attack called Community Inference Attack (CIA), which enables an adversary to identify community members based on a set of target items. What sets CIA apart is its efficiency: it operates at low computational cost by eliminating the need for training surrogate models. Instead, it uses a comparison-based approach, inferring sensitive information by comparing users' models rather than targeting any specific individual model. To evaluate the effectiveness of CIA, we conduct experiments on three real-world recommendation datasets using two recommendation models under both Federated and Gossip-like settings. The results demonstrate that CIA can be up to 10 times more accurate than random guessing. Additionally, we evaluate two mitigation strategies: Differentially Private Stochastic Gradient Descent (DP-SGD) and a Share less policy, which involves sharing fewer, less sensitive model parameters. Our findings suggest that the Share less strategy offers a better privacy-utility trade-off, especially in GL.","authors":["Yacine Belal","Sonia Ben Mokhtar","Mohamed Maouche","Anthony Simonet-Boulogne"],"url":"https://arxiv.org/abs/2306.08929"}
{"created":"2025-04-16","title":"Finite-Time Analysis of Temporal Difference Learning with Experience Replay","abstract":"Temporal-difference (TD) learning is widely regarded as one of the most popular algorithms in reinforcement learning (RL). Despite its widespread use, it has only been recently that researchers have begun to actively study its finite time behavior, including the finite time bound on mean squared error and sample complexity. On the empirical side, experience replay has been a key ingredient in the success of deep RL algorithms, but its theoretical effects on RL have yet to be fully understood. In this paper, we present a simple decomposition of the Markovian noise terms and provide finite-time error bounds for TD-learning with experience replay. Specifically, under the Markovian observation model, we demonstrate that for both the averaged iterate and final iterate cases, the error term induced by a constant step-size can be effectively controlled by the size of the replay buffer and the mini-batch sampled from the experience replay buffer.","authors":["Han-Dong Lim","Donghwan Lee"],"url":"https://arxiv.org/abs/2306.09746"}
{"created":"2025-04-16","title":"Visualizing Geophylogenies -- Internal and External Labeling with Phylogenetic Tree Constraints","abstract":"A geophylogeny is a phylogenetic tree (or dendrogram) where each leaf (e.g. biological taxon) has an associated geographic location (site). To clearly visualize a geophylogeny, the tree is typically represented as a crossing-free drawing next to a map. The correspondence between the taxa and the sites is either shown with matching labels on the map (internal labeling) or with leaders that connect each site to the corresponding leaf of the tree (external labeling). In both cases, a good order of the leaves is paramount for understanding the association between sites and taxa. We define several quality measures for internal labeling and give an efficient algorithm for optimizing them. In contrast, minimizing the number of leader crossings in an external labeling is NP-hard. On the positive side, we show that crossing-free instances can be solved in polynomial time and give a fixed-parameter tractable (FPT) algorithm. Furthermore, optimal solutions can be found in a matter of seconds on realistic instances using integer linear programming. Finally, we provide several efficient heuristic algorithms and experimentally show them to be near optimal on real-world and synthetic instances.","authors":["Jonathan Klawitter","Felix Klesen","Joris Y. Scholl","Thomas C. van Dijk","Alexander Zaft"],"url":"https://arxiv.org/abs/2306.17348"}
{"created":"2025-04-16","title":"Deep-seeded Clustering for Emotion Recognition from Wearable Physiological Sensors","abstract":"According to the circumplex model of affect, an emotional response could characterized by a level of pleasure (valence) and intensity (arousal). As it reflects on the autonomic nervous system (ANS) activity, modern wearable wristbands can record non-invasively and during our everyday lives peripheral end-points of this response. While emotion recognition from physiological signals is usually achieved using supervised machine learning algorithms that require ground truth labels for training, collecting it is cumbersome and particularly unfeasible in naturalistic settings, and extracting meaningful insights from these signals requires domain knowledge and might be prone to bias. Here, we propose and test a deep-seeded clustering algorithm that automatically extracts and classifies features from those physiological signals with minimal supervision - combining an autoencoder (AE) for unsupervised feature representation and c-means clustering for fine-grained classification. We also show that the model obtains good performance results across three different datasets frequently used in affective computing studies (accuracies of 80.7% on WESAD, 64.2% on Stress-Predict and 61.0% on CEAP360-VR).","authors":["Marta A. Concei\\c{c}\\~ao","Antoine Dubois","Sonja Haustein","Bruno Miranda","Carlos Lima Azevedo"],"url":"https://arxiv.org/abs/2308.09013"}
{"created":"2025-04-16","title":"Privacy-Preserving 3-Layer Neural Network Training","abstract":"In this manuscript, we consider the problem of privacy-preserving training of neural networks in the mere homomorphic encryption setting. We combine several exsiting techniques available, extend some of them, and finally enable the training of 3-layer neural networks for both the regression and classification problems using mere homomorphic encryption technique.","authors":["John Chiang"],"url":"https://arxiv.org/abs/2308.09531"}
{"created":"2025-04-16","title":"ESTA: An Efficient Spatial-Temporal Range Aggregation Query Processing Algorithm for UAV Networks","abstract":"Unmanned Aerial Vehicle (UAV) networks are increasingly deployed in military and civilian applications, serving as critical platforms for data collection. Users frequently require aggregated statistical information derived from historical sensory data within specific spatial and temporal boundaries. To address this, users submit aggregation query requests with spatial-temporal constraints to target UAVs that store the relevant data. These UAVs process and return the query results, which can be aggregated within the network during transmission to conserve energy and bandwidth-resources that are inherently limited in UAV networks. However,the dynamic topology caused by UAV mobility, coupled with these resource constraints, makes efficient in-network aggregation challenging without compromising user query delay. To the best of our knowledge, existing research has yet to adequately explore spatial-temporal range aggregation queries in the context of UAV networks. In this paper, we propose ESTA, an Efficient Spatial-Temporal range Aggregation query processing algorithm tailored for UAV networks. ESTA leverages pre-planned UAV trajectories to construct a topology change graph that models the network's evolving connectivity. It then employs an efficient shortest path algorithm to determine the minimum query response delay. Subsequently, while adhering to user-specified delay constraints, ESTA transforms the in-network aggregation process into a series of set cover problems, which are solved recursively to build a Spatial-Temporal Aggregation Tree (STAT). This tree enables the identification of an energy-efficient routing path for aggregating and delivering query results. Extensive simulations demonstrate that ESTA reduces energy consumption by more than 50% compared to a baseline algorithm, all while satisfying the required query delay.","authors":["Liang Liu","Wenbin Zhai","Xin Li","Youwei Ding","Wanying Lu"],"url":"https://arxiv.org/abs/2308.11977"}
{"created":"2025-04-16","title":"The Index and Core of a Relation. With Applications to the Axiomatics of Relation Algebra","abstract":"We introduce the general notions of an index and a core of a relation. We postulate a limited form of the axiom of choice -- specifically that all partial equivalence relations have an index -- and explore the consequences of adding the axiom to standard axiom systems for point-free reasoning. Examples of the theorems we prove are that a core/index of a difunction is a bijection, and that the so-called ``all or nothing'' axiom used to facilitate pointwise reasoning is derivable from our axiom of choice.","authors":["Roland Backhouse (University of Nottingham","UK)","Ed Voermans (Independent Researcher)"],"url":"https://arxiv.org/abs/2309.02017"}
{"created":"2025-04-16","title":"Fundamental Limits of Deep Learning-Based Binary Classifiers Trained with Hinge Loss","abstract":"Although deep learning (DL) has led to several breakthroughs in many disciplines, the fundamental understanding on why and how DL is empirically successful remains elusive. To attack this fundamental problem and unravel the mysteries behind DL's empirical successes, significant innovations toward a unified theory of DL have been made. Although these innovations encompass nearly fundamental advances in optimization, generalization, and approximation, no work has quantified the testing performance of a DL-based algorithm employed to solve a pattern classification problem. To overcome this fundamental challenge in part, this paper exposes the fundamental testing performance limits of DL-based binary classifiers trained with hinge loss. For binary classifiers that are based on deep rectified linear unit (ReLU) feedforward neural networks (FNNs) and deep FNNs with ReLU and Tanh activation, we derive their respective novel asymptotic testing performance limits, which are validated by extensive computer experiments.","authors":["Tilahun M. Getu","Georges Kaddoum","M. Bennis"],"url":"https://arxiv.org/abs/2309.06774"}
{"created":"2025-04-16","title":"HAS-RRT: RRT-based Motion Planning using Topological Guidance","abstract":"We present a hierarchical RRT-based motion planning strategy, Hierarchical Annotated-Skeleton Guided RRT (HAS-RRT), guided by a workspace skeleton, to solve motion planning problems. HAS-RRT provides up to a 91% runtime reduction and builds a tree at least 30% smaller than competitors while still finding competitive-cost paths. This is because our strategy prioritizes paths indicated by the workspace guidance to efficiently find a valid motion plan for the robot. Existing methods either rely too heavily on workspace guidance or have difficulty finding narrow passages. By taking advantage of the assumptions that the workspace skeleton provides, HAS-RRT is able to build a smaller tree and find a path faster than its competitors. Additionally, we show that HAS-RRT is robust to the quality of workspace guidance provided and that, in a worst-case scenario where the workspace skeleton provides no additional insight, our method performs comparably to an unguided method.","authors":["Diane Uwacu","Ananya Yammanuru","Keerthana Nallamotu","Vasu Chalasani","Marco Morales","Nancy M. Amato"],"url":"https://arxiv.org/abs/2309.10801"}
{"created":"2025-04-16","title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","abstract":"Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability. We aspire for it to serve as inspiration for future research in this field. Project page: https://sites.google.com/view/llm-mpc","authors":["Hao Sha","Yao Mu","Yuxuan Jiang","Li Chen","Chenfeng Xu","Ping Luo","Shengbo Eben Li","Masayoshi Tomizuka","Wei Zhan","Mingyu Ding"],"url":"https://arxiv.org/abs/2310.03026"}
{"created":"2025-04-16","title":"On the Rational Degree of Boolean Functions and Applications","abstract":"We study a natural complexity measure of Boolean functions known as the rational degree. Denoted $\\textrm{rdeg}(f)$, it is the minimal degree of a rational function that is equal to $f$ on the Boolean hypercube. For total functions $f$, it is conjectured that $\\textrm{rdeg}(f)$ is polynomially related to the Fourier degree of $f$, $\\textrm{deg}(f)$. Towards this conjecture, we show that:","authors":["Vishnu Iyer","Siddhartha Jain","Robin Kothari","Matt Kovacs-Deak","Vinayak M. Kumar","Luke Schaeffer","Daochen Wang","Michael Whitmeyer"],"url":"https://arxiv.org/abs/2310.08004"}
{"created":"2025-04-16","title":"PonderV2: Pave the Way for 3D Foundation Model with A Universal Pre-training Paradigm","abstract":"In contrast to numerous NLP and 2D vision foundational models, learning a 3D foundational model poses considerably greater challenges. This is primarily due to the inherent data variability and diversity of downstream tasks. In this paper, we introduce a novel universal 3D pre-training framework designed to facilitate the acquisition of efficient 3D representation, thereby establishing a pathway to 3D foundational models. Considering that informative 3D features should encode rich geometry and appearance cues that can be utilized to render realistic images, we propose to learn 3D representations by differentiable neural rendering. We train a 3D backbone with a devised volumetric neural renderer by comparing the rendered with the real images. Notably, our approach seamlessly integrates the learned 3D encoder into various downstream tasks. These tasks encompass not only high-level challenges such as 3D detection and segmentation but also low-level objectives like 3D reconstruction and image synthesis, spanning both indoor and outdoor scenarios. Besides, we also illustrate the capability of pre-training a 2D backbone using the proposed methodology, surpassing conventional pre-training methods by a large margin. For the first time, PonderV2 achieves state-of-the-art performance on 11 indoor and outdoor benchmarks, implying its effectiveness. Code and models are available at https://github.com/OpenGVLab/PonderV2.","authors":["Haoyi Zhu","Honghui Yang","Xiaoyang Wu","Di Huang","Sha Zhang","Xianglong He","Hengshuang Zhao","Chunhua Shen","Yu Qiao","Tong He","Wanli Ouyang"],"url":"https://arxiv.org/abs/2310.08586"}
{"created":"2025-04-16","title":"Making informed decisions in cutting tool maintenance in milling: A KNN-based model agnostic approach","abstract":"Tool Condition Monitoring (TCM) is vital for maintaining productivity and product quality in machining. This study leverages machine learning to analyze real-time force signals collected from experiments under various tool wear conditions. Statistical analysis and feature selection using decision trees were followed by classification using a K-Nearest Neighbors (KNN) algorithm, with hyperparameter tuning to enhance performance. While machine learning has been widely applied in TCM, interpretability remains limited. This work introduces a KNN-based white-box model that enhances transparency in decision-making by revealing how features influence classification. The model not only detects tool wear but also provides insights into the reasoning behind each decision, enabling manufacturers to make informed maintenance choices.","authors":["Revati M. Wahul","Aditya M. Rahalkar","Om M. Khare","Abhishek D. Patange","Rohan N. Soman"],"url":"https://arxiv.org/abs/2310.14629"}
{"created":"2025-04-16","title":"Disambiguation for Video Frame Interpolation","abstract":"Existing video frame interpolation (VFI) methods blindly predict where each object is at a specific timestep t (\"time indexing\"), which struggles to predict precise object movements. Given two images of a baseball, there are infinitely many possible trajectories: accelerating or decelerating, straight or curved. This often results in blurry frames as the method averages out these possibilities. Instead of forcing the network to learn this complicated time-to-location mapping implicitly, we provide the network with an explicit hint on how far the object has traveled between start and end frames, a novel approach termed \"distance indexing\". This method offers a clearer learning goal for models, reducing the uncertainty tied to object speeds. Moreover, even with this extra guidance, objects can still be blurry especially when they are equally far from both input frames, due to the directional ambiguity in long-range motion. To solve this, we propose an iterative reference-based estimation strategy that breaks down a long-range prediction into several short-range steps. When integrating our plug-and-play strategies into state-of-the-art learning-based models, they exhibit markedly superior perceptual quality in arbitrary time interpolations, using a uniform distance indexing map in the same format as time indexing without requiring extra computation. Furthermore, we demonstrate that if additional latency is acceptable, a continuous map estimator can be employed to compute a pixel-wise dense distance indexing using multiple nearby frames. Combined with efficient multi-frame refinement, this extension can further disambiguate complex motion, thus enhancing performance both qualitatively and quantitatively. Additionally, the ability to manually specify distance indexing allows for independent temporal manipulation of each object, providing a novel tool for video editing tasks such as re-timing.","authors":["Zhihang Zhong","Yiming Zhang","Wei Wang","Xiao Sun","Yu Qiao","Gurunandan Krishnan","Sizhuo Ma","Jian Wang"],"url":"https://arxiv.org/abs/2311.08007"}
{"created":"2025-04-16","title":"Enhancing autonomous vehicle acceptance with age and education sensitive simulation interventions: An experimental trial","abstract":"The familiarity principle posits that acceptance increases with exposure, which has previously been shown with in vivo and simulated experiences with connected and autonomous vehicles (CAVs). We investigate the impact of a simulated video-based first-person drive on CAV acceptance, as well as the impact of information customization, with a particular focus on acceptance by older individuals and those with lower education. Findings from an online experiment with N=799 German residents reveal that the simulated experience improved acceptance across response variables such as intention to use and ease of use, particularly among older individuals. However, the opportunity to customize navigation information decreased acceptance of older individuals and those with university degrees and increased acceptance for younger individuals and those with lower educational levels.","authors":["Celina Kacperski","Roberto Ulloa","Jeremy Wautelet","Tobias Vogel","Florian Kutzner"],"url":"https://arxiv.org/abs/2311.16780"}
{"created":"2025-04-16","title":"GaussianHead: High-fidelity Head Avatars with Learnable Gaussian Derivation","abstract":"Constructing vivid 3D head avatars for given subjects and realizing a series of animations on them is valuable yet challenging. This paper presents GaussianHead, which models the actional human head with anisotropic 3D Gaussians. In our framework, a motion deformation field and multi-resolution tri-plane are constructed respectively to deal with the head's dynamic geometry and complex texture. Notably, we impose an exclusive derivation scheme on each Gaussian, which generates its multiple doppelgangers through a set of learnable parameters for position transformation. With this design, we can compactly and accurately encode the appearance information of Gaussians, even those fitting the head's particular components with sophisticated structures. In addition, an inherited derivation strategy for newly added Gaussians is adopted to facilitate training acceleration. Extensive experiments show that our method can produce high-fidelity renderings, outperforming state-of-the-art approaches in reconstruction, cross-identity reenactment, and novel view synthesis tasks. Our code is available at: https://github.com/chiehwangs/gaussian-head.","authors":["Jie Wang","Jiu-Cheng Xie","Xianyan Li","Feng Xu","Chi-Man Pun","Hao Gao"],"url":"https://arxiv.org/abs/2312.01632"}
{"created":"2025-04-16","title":"MIMIR: Masked Image Modeling for Mutual Information-based Adversarial Robustness","abstract":"Vision Transformers (ViTs) have emerged as a fundamental architecture and serve as the backbone of modern vision-language models. Despite their impressive performance, ViTs exhibit notable vulnerability to evasion attacks, necessitating the development of specialized Adversarial Training (AT) strategies tailored to their unique architecture. While a direct solution might involve applying existing AT methods to ViTs, our analysis reveals significant incompatibilities, particularly with state-of-the-art (SOTA) approaches such as Generalist (CVPR 2023) and DBAT (USENIX Security 2024). This paper presents a systematic investigation of adversarial robustness in ViTs and provides a novel theoretical Mutual Information (MI) analysis in its autoencoder-based self-supervised pre-training. Specifically, we show that MI between the adversarial example and its latent representation in ViT-based autoencoders should be constrained via derived MI bounds. Building on this insight, we propose a self-supervised AT method, MIMIR, that employs an MI penalty to facilitate adversarial pre-training by masked image modeling with autoencoders. Extensive experiments on CIFAR-10, Tiny-ImageNet, and ImageNet-1K show that MIMIR can consistently provide improved natural and robust accuracy, where MIMIR outperforms SOTA AT results on ImageNet-1K. Notably, MIMIR demonstrates superior robustness against unforeseen attacks and common corruption data and can also withstand adaptive attacks where the adversary possesses full knowledge of the defense mechanism.","authors":["Xiaoyun Xu","Shujian Yu","Zhuoran Liu","Stjepan Picek"],"url":"https://arxiv.org/abs/2312.04960"}
{"created":"2025-04-16","title":"Leveraging Driver Field-of-View for Multimodal Ego-Trajectory Prediction","abstract":"Understanding drivers' decision-making is crucial for road safety. Although predicting the ego-vehicle's path is valuable for driver-assistance systems, existing methods mainly focus on external factors like other vehicles' motions, often neglecting the driver's attention and intent. To address this gap, we infer the ego-trajectory by integrating the driver's gaze and the surrounding scene. We introduce RouteFormer, a novel multimodal ego-trajectory prediction network combining GPS data, environmental context, and the driver's field-of-view, comprising first-person video and gaze fixations. We also present the Path Complexity Index (PCI), a new metric for trajectory complexity that enables a more nuanced evaluation of challenging scenarios. To tackle data scarcity and enhance diversity, we introduce GEM, a comprehensive dataset of urban driving scenarios enriched with synchronized driver field-of-view and gaze data. Extensive evaluations on GEM and DR(eye)VE demonstrate that RouteFormer significantly outperforms state-of-the-art methods, achieving notable improvements in prediction accuracy across diverse conditions. Ablation studies reveal that incorporating driver field-of-view data yields significantly better average displacement error, especially in challenging scenarios with high PCI scores, underscoring the importance of modeling driver attention. All data and code are available at https://meakbiyik.github.io/routeformer.","authors":["M. Eren Akbiyik","Nedko Savov","Danda Pani Paudel","Nikola Popovic","Christian Vater","Otmar Hilliges","Luc Van Gool","Xi Wang"],"url":"https://arxiv.org/abs/2312.08558"}
{"created":"2025-04-16","title":"Multicut Problems in Embedded Graphs: The Dependency of Complexity on the Demand Pattern","abstract":"The Multicut problem asks for a minimum cut separating certain pairs of vertices: formally, given a graph $G$ and demand graph $H$ on a set $T\\subseteq V(G)$ of terminals, the task is to find a minimum-weight set $C$ of edges of $G$ such that whenever two vertices of $T$ are adjacent in $H$, they are in different components of $G\\setminus C$. Colin de Verdi\\`{e}re [Algorithmica, 2017] showed that Multicut with $t$ terminals on a graph $G$ of genus $g$ can be solved in time $f(t,g)n^{O(\\sqrt{g^2+gt+t})}$. Cohen-Addad et al. [JACM, 2021] proved a matching lower bound showing that the exponent of $n$ is essentially best possible (for every fixed value of $t$ and $g$), even in the special case of Multiway Cut, where the demand graph $H$ is a complete graph.","authors":["Jacob Focke","Florian H\\\"orsch","Shaohua Li","D\\'aniel Marx"],"url":"https://arxiv.org/abs/2312.11086"}
{"created":"2025-04-16","title":"Moving a Derivation Along a Derivation Preserves the Spine in Adhesive Categories","abstract":"In this paper, we investigate the relationship between two elementary operations on derivations in the framework of graph transformation based on adhesive categories: moving a derivation along a derivation based on parallel and sequential independence on one hand and restriction of a derivation with respect to a monomorphism into the start object on the other hand. Intuitively, a restriction clips off parts of the start object that are never matched by a rule application throughout the derivation on the other hand. As main result, it is shown that moving a derivation preserves its spine being the minimal restriction.","authors":["Hans-J\\\"org Kreowski","Aaron Lye","Aljoscha Windhorst"],"url":"https://arxiv.org/abs/2312.13510"}
{"created":"2025-04-16","title":"Collaborative Perception for Connected and Autonomous Driving: Challenges, Possible Solutions and Opportunities","abstract":"Autonomous driving has attracted significant attention from both academia and industries, which is expected to offer a safer and more efficient driving system. However, current autonomous driving systems are mostly based on a single vehicle, which has significant limitations which still poses threats to driving safety. Collaborative perception with connected and autonomous vehicles (CAVs) shows a promising solution to overcoming these limitations. In this article, we first identify the challenges of collaborative perception, such as data sharing asynchrony, data volume, and pose errors. Then, we discuss the possible solutions to address these challenges with various technologies, where the research opportunities are also elaborated. Furthermore, we propose a scheme to deal with communication efficiency and latency problems, which is a channel-aware collaborative perception framework to dynamically adjust the communication graph and minimize latency, thereby improving perception performance while increasing communication efficiency. Finally, we conduct experiments to demonstrate the effectiveness of our proposed scheme.","authors":["Senkang Hu","Zhengru Fang","Yiqin Deng","Xianhao Chen","Yuguang Fang"],"url":"https://arxiv.org/abs/2401.01544"}
{"created":"2025-04-16","title":"Lateral Phishing With Large Language Models: A Large Organization Comparative Study","abstract":"The emergence of Large Language Models (LLMs) has heightened the threat of phishing emails by enabling the generation of highly targeted, personalized, and automated attacks. Traditionally, many phishing emails have been characterized by typos, errors, and poor language. These errors can be mitigated by LLMs, potentially lowering the barrier for attackers. Despite this, there is a lack of large-scale studies comparing the effectiveness of LLM-generated lateral phishing emails to those crafted by humans. Current literature does not adequately address the comparative effectiveness of LLM and human-generated lateral phishing emails in a real-world, large-scale organizational setting, especially considering the potential for LLMs to generate more convincing and error-free phishing content. To address this gap, we conducted a pioneering study within a large university, targeting its workforce of approximately 9,000 individuals including faculty, staff, administrators, and student workers. Our results indicate that LLM-generated lateral phishing emails are as effective as those written by communications professionals, emphasizing the critical threat posed by LLMs in leading phishing campaigns. We break down the results of the overall phishing experiment, comparing vulnerability between departments and job roles. Furthermore, to gather qualitative data, we administered a detailed questionnaire, revealing insights into the reasons and motivations behind vulnerable employee's actions. This study contributes to the understanding of cyber security threats in educational institutions and provides a comprehensive comparison of LLM and human-generated phishing emails' effectiveness, considering the potential for LLMs to generate more convincing content. The findings highlight the need for enhanced user education and system defenses to mitigate the growing threat of AI-powered phishing attacks.","authors":["Mazal Bethany","Athanasios Galiopoulos","Emet Bethany","Mohammad Bahrami Karkevandi","Nicole Beebe","Nishant Vishwamitra","Peyman Najafirad"],"url":"https://arxiv.org/abs/2401.09727"}
{"created":"2025-04-16","title":"Out-of-Distribution Detection & Applications With Ablated Learned Temperature Energy","abstract":"As deep neural networks become adopted in high-stakes domains, it is crucial to identify when inference inputs are Out-of-Distribution (OOD) so that users can be alerted of likely drops in performance and calibration despite high confidence -- ultimately to know when networks' decisions (and their uncertainty in those decisions) should be trusted. In this paper we introduce Ablated Learned Temperature Energy (or \"AbeT\" for short), an OOD detection method which lowers the False Positive Rate at 95\\% True Positive Rate (FPR@95) by $43.43\\%$ in classification compared to state of the art without training networks in multiple stages or requiring hyperparameters or test-time backward passes. We additionally provide empirical insights as to why our model learns to distinguish between In-Distribution (ID) and OOD samples while only being explicitly trained on ID samples via exposure to misclassified ID examples at training time. Lastly, we show the efficacy of our method in identifying predicted bounding boxes and pixels corresponding to OOD objects in object detection and semantic segmentation, respectively -- with an AUROC increase of $5.15\\%$ in object detection and both a decrease in FPR@95 of $41.48\\%$ and an increase in AUPRC of $34.20\\%$ in semantic segmentation compared to previous state of the art.","authors":["Will LeVine","Benjamin Pikus","Jacob Phillips","Berk Norman","Fernando Amat Gil","Sean Hendryx"],"url":"https://arxiv.org/abs/2401.12129"}
{"created":"2025-04-16","title":"Optimal Intraday Power Trading for Single-Price Balancing Markets: An Adaptive Risk-Averse Strategy using Mixture Models","abstract":"Efficient markets are characterised by profit-driven participants continuously refining their positions towards the latest insights. Margins for profit generation are generally small, shaping a difficult landscape for automated trading strategies. This paper introduces a novel intraday power trading strategy tailored for single-price balancing markets. The strategy relies on a strategically devised mixture model to forecast future system imbalance prices and is formulated as a stochastic optimization problem with decision-dependent distributions to address two primary challenges: (i) the impact of trading positions on the system imbalance price and (ii) the uncertainty inherent in the model. The first challenge is tackled by adjusting the model to account for price changes after taking a position. For the second challenge, a coherent risk measure is added to the cost function to take additional uncertainties into account. This paper introduces a methodology to select the tuning parameter of this risk measure adaptively by continuously quantifying the performance of the strategy on a window of recently observed data. The strategy is validated with a simulation on the Belgian electricity market using real-time market data. The adaptive tuning approach leads to higher absolute profits, while also reducing the number of trades.","authors":["Robin Bruneel","Mathijs Schuurmans","Panagiotis Patrinos"],"url":"https://arxiv.org/abs/2402.01215"}
{"created":"2025-04-16","title":"Stochastic theta methods for free stochastic differential equations","abstract":"We introduce free probability analogues of the stochastic theta methods for free stochastic differential equations in this work. Assume that the drift coefficient of the free stochastic differential equations is operator Lipschitz and the diffusion coefficients are locally operator Lipschitz, we prove the strong convergence of the numerical methods. Moreover, we investigate the exponential stability in mean square of the equations and the numerical methods. In particular, the free stochastic theta methods with $\\theta \\in [1/2, 1]$ can inherit the exponential stability of original equations for any given step size. Our methods offer better stability than the free Euler-Maruyama method. Numerical results are reported to confirm these theoretical findings and show the efficiency of our methods compared with the free Euler-Maruyama method.","authors":["Yuanling Niu","Jiaxin Wei","Zhi Yin","Dan Zeng"],"url":"https://arxiv.org/abs/2402.04094"}
{"created":"2025-04-16","title":"Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes","abstract":"Structured pruning is a promising approach to create smaller, faster LLMs. However, existing methods typically rely on backward passes, which can inflate memory requirements and compute costs. In this work we introduce Bonsai, a gradient-free structured pruning method that eliminates the need for backpropagation, significantly reducing memory requirements and compute costs while achieving state-of-the-art pruning performance. Bonsai uses forward-pass-only perturbative pruning to enable efficient compression of large models on a broader range of hardware configurations. Unlike existing structured pruning approaches, Bonsai not only achieves better compression with fewer resources, but also produces models that are twice as fast as those generated by semi-structured pruning. As a concrete demonstration, we use Bonsai to prune an 8B LLaMA-3 model to 50% sparsity on a single A6000 GPU -- a task infeasible with backprop-based methods, which require 2-3x memory. Our results show that removing backprop as a requirement not only enables pruning larger models on constrained hardware but can also lead to state-of-the-art efficiency and performance.","authors":["Lucio Dery","Steven Kolawole","Jean-Fran\\c{c}ois Kagy","Virginia Smith","Graham Neubig","Ameet Talwalkar"],"url":"https://arxiv.org/abs/2402.05406"}
{"created":"2025-04-16","title":"A Hierarchical Dataflow-Driven Heterogeneous Architecture for Wireless Baseband Processing","abstract":"Wireless baseband processing (WBP) is a key element of wireless communications, with a series of signal processing modules to improve data throughput and counter channel fading. Conventional hardware solutions, such as digital signal processors (DSPs) and more recently, graphic processing units (GPUs), provide various degrees of parallelism, yet they both fail to take into account the cyclical and consecutive character of WBP. Furthermore, the large amount of data in WBPs cannot be processed quickly in symmetric multiprocessors (SMPs) due to the unpredictability of memory latency. To address this issue, we propose a hierarchical dataflow-driven architecture to accelerate WBP. A pack-and-ship approach is presented under a non-uniform memory access (NUMA) architecture to allow the subordinate tiles to operate in a bundled access and execute manner. We also propose a multi-level dataflow model and the related scheduling scheme to manage and allocate the heterogeneous hardware resources. Experiment results demonstrate that our prototype achieves $2\\times$ and $2.3\\times$ speedup in terms of normalized throughput and single-tile clock cycles compared with GPU and DSP counterparts in several critical WBP benchmarks. Additionally, a link-level throughput of $288$ Mbps can be achieved with a $45$-core configuration.","authors":["Limin Jiang","Yi Shi","Yintao Liu","Qingyu Deng","Siyi Xu","Yihao Shen","Fangfang Ye","Shan Cao","Zhiyuan Jiang"],"url":"https://arxiv.org/abs/2402.18070"}
{"created":"2025-04-16","title":"Cramming Contextual Bandits for On-policy Statistical Evaluation","abstract":"We introduce the cram method as a general statistical framework for evaluating the final learned policy from a multi-armed contextual bandit algorithm, using the dataset generated by the same bandit algorithm. The proposed on-policy evaluation methodology differs from most existing methods that focus on off-policy performance evaluation of contextual bandit algorithms. Cramming utilizes an entire bandit sequence through a single pass of data, leading to both statistically and computationally efficient evaluation. We prove that if a bandit algorithm satisfies a certain stability condition, the resulting crammed evaluation estimator is consistent and asymptotically normal under mild regularity conditions. Furthermore, we show that this stability condition holds for commonly used linear contextual bandit algorithms, including epsilon-greedy, Thompson Sampling, and Upper Confidence Bound algorithms. Using both synthetic and publicly available datasets, we compare the empirical performance of cramming with the state-of-the-art methods. The results demonstrate that the proposed cram method reduces the evaluation standard error by approximately 40% relative to off-policy evaluation methods while preserving unbiasedness and valid confidence interval coverage.","authors":["Zeyang Jia","Kosuke Imai","Michael Lingzhi Li"],"url":"https://arxiv.org/abs/2403.07031"}
{"created":"2025-04-16","title":"MedMerge: Merging Models for Effective Transfer Learning to Medical Imaging Tasks","abstract":"Transfer learning has become a powerful tool to initialize deep learning models to achieve faster convergence and higher performance. This is especially useful in the medical imaging analysis domain, where data scarcity limits possible performance gains for deep learning models. Some advancements have been made in boosting the transfer learning performance gain by merging models starting from the same initialization. However, in the medical imaging analysis domain, there is an opportunity to merge models starting from different initializations, thus combining the features learned from different tasks. In this work, we propose MedMerge, a method whereby the weights of different models can be merged, and their features can be effectively utilized to boost performance on a new task. With MedMerge, we learn kernel-level weights that can later be used to merge the models into a single model, even when starting from different initializations. Testing on various medical imaging analysis tasks, we show that our merged model can achieve significant performance gains, with up to 7% improvement on the F1 score. The code implementation of this work is available at github.com/BioMedIA-MBZUAI/MedMerge.","authors":["Ibrahim Almakky","Santosh Sanjeev","Anees Ur Rehman Hashmi","Mohammad Areeb Qazi","Hu Wang","Mohammad Yaqub"],"url":"https://arxiv.org/abs/2403.11646"}
{"created":"2025-04-16","title":"Kernel Modelling of Fading Memory Systems","abstract":"The paper is a follow-up of the recently introduced kernel-based framework to identify nonlinear input-output systems regularized by desirable input-output incremental properties. Assuming that the system has fading memory, we propose to learn the functional that maps the past input to the present output rather than the operator mapping input trajectories to output trajectories. While retaining the benefits of the previously proposed framework, this modification simplifies the selection of the kernel, enforces causality, and enables temporal simulation.","authors":["Yongkang Huo","Thomas Chaffey","Rodolphe Sepulchre"],"url":"https://arxiv.org/abs/2403.11945"}
{"created":"2025-04-16","title":"Contextual AD Narration with Interleaved Multimodal Sequence","abstract":"The Audio Description (AD) task aims to generate descriptions of visual elements for visually impaired individuals to help them access long-form video content, like movies. With video feature, text, character bank and context information as inputs, the generated ADs are able to correspond to the characters by name and provide reasonable, contextual descriptions to help audience understand the storyline of movie. To achieve this goal, we propose to leverage pre-trained foundation models through a simple and unified framework to generate ADs with interleaved multimodal sequence as input, termed as Uni-AD. To enhance the alignment of features across various modalities with finer granularity, we introduce a simple and lightweight module that maps video features into the textual feature space. Moreover, we also propose a character-refinement module to provide more precise information by identifying the main characters who play more significant roles in the video context. With these unique designs, we further incorporate contextual information and a contrastive loss into our architecture to generate smoother and more contextually appropriate ADs. Experiments on multiple AD datasets show that Uni-AD performs well on AD generation, which demonstrates the effectiveness of our approach. Our code is available at: https://github.com/ant-research/UniAD.","authors":["Hanlin Wang","Zhan Tong","Kecheng Zheng","Yujun Shen","Limin Wang"],"url":"https://arxiv.org/abs/2403.12922"}
{"created":"2025-04-16","title":"Uncertainty Quantification for Gradient-based Explanations in Neural Networks","abstract":"Explanation methods help understand the reasons for a model's prediction. These methods are increasingly involved in model debugging, performance optimization, and gaining insights into the workings of a model. With such critical applications of these methods, it is imperative to measure the uncertainty associated with the explanations generated by these methods. In this paper, we propose a pipeline to ascertain the explanation uncertainty of neural networks by combining uncertainty estimation methods and explanation methods. We use this pipeline to produce explanation distributions for the CIFAR-10, FER+, and California Housing datasets. By computing the coefficient of variation of these distributions, we evaluate the confidence in the explanation and determine that the explanations generated using Guided Backpropagation have low uncertainty associated with them. Additionally, we compute modified pixel insertion/deletion metrics to evaluate the quality of the generated explanations.","authors":["Mihir Mulye","Matias Valdenegro-Toro"],"url":"https://arxiv.org/abs/2403.17224"}
{"created":"2025-04-16","title":"Steering Feedback in Dynamic Driving Simulators: The Influence of Steering Wheel Vibration and Vehicle Motion Frequency","abstract":"The validity of the subjective evaluation of steering feedback in driving simulators is crucial for modern vehicle development. Although there are established objective steering characteristics for the assessment of both stationary and dynamic feedback behaviour, factors such as steering wheel vibrations and vehicle body motion, particularly in high-frequency ranges, present challenges in simulator fidelity. This work investigates the influence of steering wheel vibration and vehicle body motion frequency content on the subjective evaluation of steering feedback during closed-loop driving in a dynamic driving simulator. A controlled subject study with 30 participants consisting of a back-to-back comparison of a reference vehicle with an electrical power steering system and three variants of its virtual representation on a dynamic driving simulator was performed. Subjective evaluation focused on the representation of road feedback in comparison to the reference vehicle. The statistical analysis of subjective results show that there is a significant influence of the frequency content of both steering wheel torque and vehicle motion on the subjective evaluation of steering feedback in a dynamic driving simulator. The results suggest an influence of frequency content on the subjective evaluation quality of steering feedback characteristics that are not associated with the dynamic feedback behaviour in the context of established performance indicators.","authors":["Maximilian B\\\"ohle","Bernhard Schick","Steffen M\\\"uller"],"url":"https://arxiv.org/abs/2403.17800"}
{"created":"2025-04-16","title":"Reference-Based 3D-Aware Image Editing with Triplanes","abstract":"Generative Adversarial Networks (GANs) have emerged as powerful tools for high-quality image generation and real image editing by manipulating their latent spaces. Recent advancements in GANs include 3D-aware models such as EG3D, which feature efficient triplane-based architectures capable of reconstructing 3D geometry from single images. However, limited attention has been given to providing an integrated framework for 3D-aware, high-quality, reference-based image editing. This study addresses this gap by exploring and demonstrating the effectiveness of the triplane space for advanced reference-based edits. Our novel approach integrates encoding, automatic localization, spatial disentanglement of triplane features, and fusion learning to achieve the desired edits. We demonstrate how our approach excels across diverse domains, including human faces, 360-degree heads, animal faces, partially stylized edits like cartoon faces, full-body clothing edits, and edits on class-agnostic samples. Our method shows state-of-the-art performance over relevant latent direction, text, and image-guided 2D and 3D-aware diffusion and GAN methods, both qualitatively and quantitatively.","authors":["Bahri Batuhan Bilecen","Yigit Yalin","Ning Yu","Aysegul Dundar"],"url":"https://arxiv.org/abs/2404.03632"}
{"created":"2025-04-16","title":"Automatic BLAS Offloading on Unified Memory Architecture: A Study on NVIDIA Grace-Hopper","abstract":"Porting codes to GPU often requires major efforts. While several tools exist for automatically offload numerical libraries such as BLAS and LAPACK, they often prove impractical due to the high cost of mandatory data transfer. The new unified memory architecture in NVIDIA Grace-Hopper allows high bandwidth cache-coherent memory access of all memory from both CPU and GPU, potentially eliminating bottleneck faced in conventional architecture. This breakthrough opens up new avenues for application development and porting strategies. In this study, we introduce a new tool for automatic BLAS offload, the tool leverages the high speed cache coherent NVLink C2C interconnect in Grace-Hopper, and enables performant GPU offload for BLAS heavy applications with no code changes or recompilation. The tool was tested on two quantum chemistry or physics codes, great performance benefits were observed.","authors":["Junjie Li","Yinzhi Wang","Xiao Liang","Hang Liu"],"url":"https://arxiv.org/abs/2404.13195"}
{"created":"2025-04-16","title":"UniRGB-IR: A Unified Framework for Visible-Infrared Semantic Tasks via Adapter Tuning","abstract":"Semantic analysis on visible (RGB) and infrared (IR) images has gained significant attention due to their enhanced accuracy and robustness under challenging conditions including low-illumination and adverse weather. However, due to the lack of pre-trained foundation models on the large-scale infrared image datasets, existing methods prefer to design task-specific frameworks and directly fine-tune them with pre-trained foundation models on their RGB-IR semantic relevance datasets, which results in poor scalability and limited generalization. To address these limitations, we propose UniRGB-IR, a scalable and efficient framework for RGB-IR semantic tasks that introduces a novel adapter mechanism to effectively incorporate rich multi-modal features into pre-trained RGB-based foundation models. Our framework comprises three key components: a vision transformer (ViT) foundation model, a Multi-modal Feature Pool (MFP) module, and a Supplementary Feature Injector (SFI) module. The MFP and SFI modules cooperate with each other as an adpater to effectively complement the ViT features with the contextual multi-scale features. During training process, we freeze the entire foundation model to inherit prior knowledge and only optimize the MFP and SFI modules. Furthermore, to verify the effectiveness of our framework, we utilize the ViT-Base as the pre-trained foundation model to perform extensive experiments. Experimental results on various RGB-IR semantic tasks demonstrate that our method can achieve state-of-the-art performance. The source code and results are available at https://github.com/PoTsui99/UniRGB-IR.git.","authors":["Maoxun Yuan","Bo Cui","Tianyi Zhao","Jiayi Wang","Shan Fu","Xue Yang","Xingxing Wei"],"url":"https://arxiv.org/abs/2404.17360"}
{"created":"2025-04-16","title":"A velocity-based moving mesh Discontinuous Galerkin method for the advection-diffusion equation","abstract":"In convection-dominated flows, robustness of the spatial discretisation is a key property. While Interior Penalty Galerkin (IPG) methods already proved efficient in the situation of large mesh Peclet numbers, Arbitrary Lagrangian-Eulerian (ALE) methods are able to reduce the convection-dominance by moving the mesh. In this paper, we introduce and analyse a velocity-based moving mesh discontinuous Galerkin (DG) method for the solution of the linear advection-diffusion equation. By introducing a smooth parameterized velocity $\\Tilde{V}$ that separates the flow into a mean flow, also called moving mesh velocity, and a remaining advection field $V-\\Tilde{V}$, we made a convergence analysis based on the smoothness of the mesh velocity. Furthermore, the reduction of the advection speed improves the stability of an explicit time-stepping. Finally, by adapting the existing robust error criteria to this moving mesh situation, we derived robust \\textit{a posteriori} error criteria that describe the potentially small deviation to the mean flow and include the information of a transition towards $V=\\Tilde{V}$.","authors":["Ezra Rozier","J\\\"orn Behrens"],"url":"https://arxiv.org/abs/2405.09408"}
{"created":"2025-04-16","title":"Enhancing Bayesian model updating in structural health monitoring via learnable mappings","abstract":"In the context of structural health monitoring (SHM), the selection and extraction of damage-sensitive features from raw sensor recordings represent a critical step towards solving the inverse problem underlying the identification of structural health conditions. This work introduces a novel approach that employs deep neural networks to enhance stochastic SHM methods. A learnable feature extractor and a feature-oriented surrogate model are synergistically exploited to evaluate a likelihood function within a Markov chain Monte Carlo sampling algorithm. The feature extractor undergoes pairwise supervised training to map sensor recordings onto a low-dimensional metric space, which encapsulates the sensitivity to structural health parameters. The surrogate model maps structural health parameters to their feature representation. The procedure enables the updating of beliefs about structural health parameters, eliminating the need for computationally expensive numerical models. A preliminary offline phase involves the generation of a labeled dataset to train both the feature extractor and the surrogate model. Within a simulation-based SHM framework, training vibration responses are efficiently generated using a multi-fidelity surrogate modeling strategy to approximate sensor recordings under varying damage and operational conditions. The multi-fidelity surrogate exploits model order reduction and artificial neural networks to speed up the data generation phase while ensuring the damage-sensitivity of the approximated signals. The proposed strategy is assessed through three synthetic case studies, demonstrating high accuracy in the estimated parameters and strong computational efficiency.","authors":["Matteo Torzoni","Andrea Manzoni","Stefano Mariani"],"url":"https://arxiv.org/abs/2405.13648"}
{"created":"2025-04-16","title":"Belief-State Query Policies for User-Aligned POMDPs","abstract":"Planning in real-world settings often entails addressing partial observability while aligning with users' requirements. We present a novel framework for expressing users' constraints and preferences about agent behavior in a partially observable setting using parameterized belief-state query (BSQ) policies in the setting of goal-oriented partially observable Markov decision processes (gPOMDPs). We present the first formal analysis of such constraints and prove that while the expected cost function of a parameterized BSQ policy w.r.t its parameters is not convex, it is piecewise constant and yields an implicit discrete parameter search space that is finite for finite horizons. This theoretical result leads to novel algorithms that optimize gPOMDP agent behavior with guaranteed user alignment. Analysis proves that our algorithms converge to the optimal user-aligned behavior in the limit. Empirical results show that parameterized BSQ policies provide a computationally feasible approach for user-aligned planning in partially observable settings.","authors":["Daniel Bramblett","Siddharth Srivastava"],"url":"https://arxiv.org/abs/2405.15907"}
{"created":"2025-04-16","title":"Verifying Properties of Binary Neural Networks Using Sparse Polynomial Optimization","abstract":"This paper explores methods for verifying the properties of Binary Neural Networks (BNNs), focusing on robustness against adversarial attacks. Despite their lower computational and memory needs, BNNs, like their full-precision counterparts, are also sensitive to input perturbations. Established methods for solving this problem are predominantly based on Satisfiability Modulo Theories and Mixed-Integer Linear Programming techniques, which are characterized by NP complexity and often face scalability issues.","authors":["Jianting Yang","Sre\\'cko {\\DH}ura\\v{s}inovi\\'c","Jean-Bernard Lasserre","Victor Magron","Jun Zhao"],"url":"https://arxiv.org/abs/2405.17049"}
{"created":"2025-04-16","title":"Rank-Refining Seed Selection Methods for Budget Constrained Influence Maximisation in Multilayer Networks under Linear Threshold Model","abstract":"The problem of selecting an optimal seed set to maximise influence in networks has been a subject of intense research in recent years. However, despite numerous works addressing this area, it remains a topic that requires further elaboration. Most often, it is considered within the scope of classically defined graphs with a spreading model in the form of Independent Cascades. In this work, we focus on the problem of budget-constrained influence maximisation in multilayer networks using a Linear Threshold Model. Both the graph model and the spreading process we employ are less prevalent in the literature, even though their application allows for a more precise representation of the opinion dynamics in social networks. This paper aims to answer which of the sixteen evaluated seed selection methods is the most effective and how similar they are. Additionally, we focus our analysis on the impact of spreading model parameters, network characteristics, a budget, and the seed selection methods on the diffusion effectiveness in multilayer networks. Our contribution also includes extending several centrality measures and heuristics to the case of such graphs. The results indicate that all the factors mentioned above collectively contribute to the effectiveness of influence maximisation. Moreover, there is no seed selection method which always provides the best results. However, the seeds chosen with VoteRank-based methods (especially with the $v-rnk-m$ variant we propose) usually provide the most extensive diffusion.","authors":["Micha{\\l} Czuba","Piotr Br\\'odka"],"url":"https://arxiv.org/abs/2405.18059"}
{"created":"2025-04-16","title":"Unsupervised Model Tree Heritage Recovery","abstract":"The number of models shared online has recently skyrocketed, with over one million public models available on Hugging Face. Sharing models allows other users to build on existing models, using them as initialization for fine-tuning, improving accuracy, and saving compute and energy. However, it also raises important intellectual property issues, as fine-tuning may violate the license terms of the original model or that of its training data. A Model Tree, i.e., a tree data structure rooted at a foundation model and having directed edges between a parent model and other models directly fine-tuned from it (children), would settle such disputes by making the model heritage explicit. Unfortunately, current models are not well documented, with most model metadata (e.g., \"model cards\") not providing accurate information about heritage. In this paper, we introduce the task of Unsupervised Model Tree Heritage Recovery (Unsupervised MoTHer Recovery) for collections of neural networks. For each pair of models, this task requires: i) determining if they are directly related, and ii) establishing the direction of the relationship. Our hypothesis is that model weights encode this information, the challenge is to decode the underlying tree structure given the weights. We discover several properties of model weights that allow us to perform this task. By using these properties, we formulate the MoTHer Recovery task as finding a directed minimal spanning tree. In extensive experiments we demonstrate that our method successfully reconstructs complex Model Trees.","authors":["Eliahu Horwitz","Asaf Shul","Yedid Hoshen"],"url":"https://arxiv.org/abs/2405.18432"}
{"created":"2025-04-16","title":"Face processing emerges from object-trained convolutional neural networks","abstract":"Whether face processing depends on unique, domain-specific neurocognitive mechanisms or domain-general object recognition mechanisms has long been debated. Directly testing these competing hypotheses in humans has proven challenging due to extensive exposure to both faces and objects. Here, we systematically test these hypotheses by capitalizing on recent progress in convolutional neural networks (CNNs) that can be trained without face exposure (i.e., pre-trained weights). Domain-general mechanism accounts posit that face processing can emerge from a neural network without specialized pre-training on faces. Consequently, we trained CNNs solely on objects and tested their ability to recognize and represent faces as well as objects that look like faces (face pareidolia stimuli).... Due to the character limits, for more details see in attached pdf","authors":["Zhenhua Zhao","Ji Chen","Zhicheng Lin","Haojiang Ying"],"url":"https://arxiv.org/abs/2405.18800"}
{"created":"2025-04-16","title":"Towards Hierarchical Multi-Agent Workflows for Zero-Shot Prompt Optimization","abstract":"Large language models (LLMs) have shown great progress in responding to user questions, allowing for a multitude of diverse applications. Yet, the quality of LLM outputs heavily depends on the prompt design, where a good prompt might enable the LLM to answer a very challenging question correctly. Therefore, recent works have developed many strategies for improving the prompt, including both manual crafting and in-domain optimization. However, their efficacy in unrestricted scenarios remains questionable, as the former depends on human design for specific questions and the latter usually generalizes poorly to unseen scenarios. To address these problems, we give LLMs the freedom to design the best prompts according to themselves. Specifically, we include a hierarchy of LLMs, first constructing a prompt with precise instructions and accurate wording in a hierarchical manner, and then using this prompt to generate the final answer to the user query. We term this pipeline Hierarchical Multi-Agent Workflow, or HMAW. In contrast with prior works, HMAW imposes no human restriction and requires no training, and is completely task-agnostic while capable of adjusting to the nuances of the underlying task. Through both quantitative and qualitative experiments across multiple benchmarks, we verify that despite its simplicity, the proposed approach can create detailed and suitable prompts, further boosting the performance of current LLMs.","authors":["Yuchi Liu","Jaskirat Singh","Gaowen Liu","Ali Payani","Liang Zheng"],"url":"https://arxiv.org/abs/2405.20252"}
{"created":"2025-04-16","title":"On a perturbation analysis of Higham squared maximum Gaussian elimination growth matrices","abstract":"Gaussian elimination is the most popular technique for solving a dense linear system. Large errors in this procedure can occur in floating point arithmetic when the matrix's growth factor is large. In the study of numerical linear algebra, it is often valuable to study and characterize the worst case examples. To this end, in their 1989 paper, Higham and Higham characterized the complete set of real n by n matrices that achieves the maximum growth factor under partial pivoting. Left undone is a sensitivity analysis for these matrices under perturbations. The growth factor of these and nearby matrices is the subject of this work. Through theoretical insights and empirical results, we illustrate the high sensitivity of the growth factor of these matrices to perturbations and show how subtle changes can be strategically applied to matrix entries to significantly reduce the growth.","authors":["Alan Edelman","John Urschel","Bowen Zhu"],"url":"https://arxiv.org/abs/2406.00737"}
{"created":"2025-04-16","title":"Adaptive Multi-Scale Decomposition Framework for Time Series Forecasting","abstract":"Transformer-based and MLP-based methods have emerged as leading approaches in time series forecasting (TSF). While Transformer-based methods excel in capturing long-range dependencies, they suffer from high computational complexities and tend to overfit. Conversely, MLP-based methods offer computational efficiency and adeptness in modeling temporal dynamics, but they struggle with capturing complex temporal patterns effectively. To address these challenges, we propose a novel MLP-based Adaptive Multi-Scale Decomposition (AMD) framework for TSF. Our framework decomposes time series into distinct temporal patterns at multiple scales, leveraging the Multi-Scale Decomposable Mixing (MDM) block to dissect and aggregate these patterns in a residual manner. Complemented by the Dual Dependency Interaction (DDI) block and the Adaptive Multi-predictor Synthesis (AMS) block, our approach effectively models both temporal and channel dependencies and utilizes autocorrelation to refine multi-scale data integration. Comprehensive experiments demonstrate that our AMD framework not only overcomes the limitations of existing methods but also consistently achieves state-of-the-art performance in both long-term and short-term forecasting tasks across various datasets, showcasing superior efficiency. Code is available at https://github.com/TROUBADOUR000/AMD","authors":["Yifan Hu","Peiyuan Liu","Peng Zhu","Dawei Cheng","Tao Dai"],"url":"https://arxiv.org/abs/2406.03751"}
{"created":"2025-04-16","title":"WildlifeReID-10k: Wildlife re-identification dataset with 10k individual animals","abstract":"This paper introduces WildlifeReID-10k, a new large-scale re-identification benchmark with more than 10k animal identities of around 33 species across more than 140k images, re-sampled from 37 existing datasets. WildlifeReID-10k covers diverse animal species and poses significant challenges for SoTA methods, ensuring fair and robust evaluation through its time-aware and similarity-aware split protocol. The latter is designed to address the common issue of training-to-test data leakage caused by visually similar images appearing in both training and test sets. The WildlifeReID-10k dataset and benchmark are publicly available on Kaggle, along with strong baselines for both closed-set and open-set evaluation, enabling fair, transparent, and standardized evaluation of not just multi-species animal re-identification models.","authors":["Luk\\'a\\v{s} Adam","Vojt\\v{e}ch \\v{C}erm\\'ak","Kostas Papafitsoros","Lukas Picek"],"url":"https://arxiv.org/abs/2406.09211"}
{"created":"2025-04-16","title":"Enhancing Commentary Strategies for Imperfect Information Card Games: A Study of Large Language Models in Guandan Commentary","abstract":"Recent advancements in large language models (LLMs) have unlocked the potential for generating high-quality game commentary. However, producing insightful and engaging commentary for complex games with incomplete information remains a significant challenge. In this paper, we introduce a novel commentary method that combine Reinforcement Learning (RL) and LLMs, tailored specifically for the Chinese card game \\textit{Guandan}. Our system leverages RL to generate intricate card-playing scenarios and employs LLMs to generate corresponding commentary text, effectively emulating the strategic analysis and narrative prowess of professional commentators. The framework comprises a state commentary guide, a Theory of Mind (ToM)-based strategy analyzer, and a style retrieval module, which seamlessly collaborate to deliver detailed and context-relevant game commentary in the Chinese language environment. We empower LLMs with ToM capabilities and refine both retrieval and information filtering mechanisms. This facilitates the generation of personalized commentary content. Our experimental results showcase the substantial enhancement in performance achieved by the proposed commentary framework when applied to open-source LLMs, surpassing the performance of GPT-4 across multiple evaluation metrics.","authors":["Meiling Tao","Xuechen Liang","Xinyuan Song","Yangfan He","Yiling Tao","Jianhui Wang","Sun Li Tianyu Shi"],"url":"https://arxiv.org/abs/2406.17807"}
{"created":"2025-04-16","title":"SABLE: Staging Blocked Evaluation of Sparse Matrix Computations","abstract":"Structured sparsity, like regions of non-zero elements in sparse matrices, can offer optimization opportunities often overlooked by existing solutions that treat matrices as entirely dense or sparse. Block-based approaches, such as BCSR, partially address this issue by choosing between fixed-size blocks which results in wasted computation on zero elements. On the other hand, variable-sized blocks introduce overheads due to variable loop bounds unknown at compile time.","authors":["Pratyush Das","Amirhossein Basareh","Adhitha Dias","Artem Pelenitsyn","Kirshanthan Sundararajah","Milind Kulkarni"],"url":"https://arxiv.org/abs/2407.00829"}
{"created":"2025-04-16","title":"LPViT: Low-Power Semi-structured Pruning for Vision Transformers","abstract":"Vision transformers have emerged as a promising alternative to convolutional neural networks for various image analysis tasks, offering comparable or superior performance. However, one significant drawback of ViTs is their resource-intensive nature, leading to increased memory footprint, computation complexity, and power consumption. To democratize this high-performance technology and make it more environmentally friendly, it is essential to compress ViT models, reducing their resource requirements while maintaining high performance. In this paper, we introduce a new block-structured pruning to address the resource-intensive issue for ViTs, offering a balanced trade-off between accuracy and hardware acceleration. Unlike unstructured pruning or channel-wise structured pruning, block pruning leverages the block-wise structure of linear layers, resulting in more efficient matrix multiplications. To optimize this pruning scheme, our paper proposes a novel hardware-aware learning objective that simultaneously maximizes speedup and minimizes power consumption during inference, tailored to the block sparsity structure. This objective eliminates the need for empirical look-up tables and focuses solely on reducing parametrized layer connections. Moreover, our paper provides a lightweight algorithm to achieve post-training pruning for ViTs, utilizing second-order Taylor approximation and empirical optimization to solve the proposed hardware-aware objective. Extensive experiments on ImageNet are conducted across various ViT architectures, including DeiT-B and DeiT-S, demonstrating competitive performance with other pruning methods and achieving a remarkable balance between accuracy preservation and power savings. Especially, we achieve 3.93x speedup on dedicated hardware and GPUs respectively for DeiT-B, and a power reduction by 1.4x on GPUs. Code released to https://github.com/Akimoto-Cris/LPViT.","authors":["Kaixin Xu","Zhe Wang","Chunyun Chen","Xue Geng","Jie Lin","Mohamed M. Sabry Aly","Xulei Yang","Min Wu","Xiaoli Li","Weisi Lin"],"url":"https://arxiv.org/abs/2407.02068"}
{"created":"2025-04-16","title":"When big data actually are low-rank, or entrywise approximation of certain function-generated matrices","abstract":"The article concerns low-rank approximation of matrices generated by sampling a smooth function of two $m$-dimensional variables. We identify several misconceptions surrounding a claim that, for a specific class of analytic functions, such $n \\times n$ matrices admit accurate entrywise approximation of rank that is independent of $m$ and grows as $\\log(n)$ -- colloquially known as ''big-data matrices are approximately low-rank''. We provide a theoretical explanation of the numerical results presented in support of this claim, describing three narrower classes of functions for which function-generated matrices can be approximated within an entrywise error of order $\\varepsilon$ with rank $\\mathcal{O}(\\log(n) \\varepsilon^{-2} \\log(\\varepsilon^{-1}))$ that is independent of the dimension $m$: (i) functions of the inner product of the two variables, (ii) functions of the Euclidean distance between the variables, and (iii) shift-invariant positive-definite kernels. We extend our argument to tensor-train approximation of tensors generated with functions of the ''higher-order inner product'' of their multiple variables. We discuss our results in the context of low-rank approximation of (a) growing datasets and (b) attention in transformer neural networks.","authors":["Stanislav Budzinskiy"],"url":"https://arxiv.org/abs/2407.03250"}
{"created":"2025-04-16","title":"Balancing Forecast Accuracy and Switching Costs in Online Optimization of Energy Management Systems","abstract":"This study investigates the integration of forecasting and optimization in energy management systems, with a focus on the role of switching costs -- penalties incurred from frequent operational adjustments. We develop a theoretical and empirical framework to examine how forecast accuracy and stability interact with switching costs in online decision-making settings. Our analysis spans both deterministic and stochastic optimization approaches, using point and probabilistic forecasts. A novel metric for measuring temporal consistency in probabilistic forecasts is introduced, and the framework is validated in a real-world battery scheduling case based on the CityLearn 2022 challenge. Results show that switching costs significantly alter the trade-off between forecast accuracy and stability, and that more stable forecasts can reduce the performance loss due to switching. Contrary to common practice, the findings suggest that, under non-negligible switching costs, longer commitment periods may lead to better overall outcomes. These insights have practical implications for the design of intelligent, forecast-aware energy management systems.","authors":["Evgenii Genov","Julian Ruddick","Christoph Bergmeir","Majid Vafaeipour","Thierry Coosemans","Salvador Garcia","Maarten Messagie"],"url":"https://arxiv.org/abs/2407.03368"}
{"created":"2025-04-16","title":"Flying Calligrapher: Contact-Aware Motion and Force Planning and Control for Aerial Manipulation","abstract":"Aerial manipulation has gained interest in completing high-altitude tasks that are challenging for human workers, such as contact inspection and defect detection, etc. Previous research has focused on maintaining static contact points or forces. This letter addresses a more general and dynamic task: simultaneously tracking time-varying contact force in the surface normal direction and motion trajectories on tangential surfaces. We propose a pipeline that includes a contact-aware trajectory planner to generate dynamically feasible trajectories, and a hybrid motion-force controller to track such trajectories. We demonstrate the approach in an aerial calligraphy task using a novel sponge pen design as the end-effector, whose stroke width is positively related to the contact force. Additionally, we develop a touchscreen interface for flexible user input. Experiments show our method can effectively draw diverse letters, achieving an IoU of 0.59 and an end-effector position (force) tracking RMSE of 2.9 cm (0.7 N). Website: https://xiaofeng-guo.github.io/flying-calligrapher/","authors":["Xiaofeng Guo","Guanqi He","Jiahe Xu","Mohammadreza Mousaei","Junyi Geng","Sebastian Scherer","Guanya Shi"],"url":"https://arxiv.org/abs/2407.05587"}
{"created":"2025-04-16","title":"An Attempt to Devise a Pairwise Ising-Type Maximum Entropy Model Integrated Cost Function for Optimizing SNN Deployment","abstract":"Spiking Neural Networks (SNNs) emulate the spiking behavior of biological neurons and are typically deployed on distributed-memory neuromorphic hardware. The deployment of a SNN usually requires partitioning the network and mapping these partitions onto the hardware's processing units.","authors":["Wanhong Huang"],"url":"https://arxiv.org/abs/2407.07014"}
{"created":"2025-04-16","title":"Can Learned Optimization Make Reinforcement Learning Less Difficult?","abstract":"While reinforcement learning (RL) holds great potential for decision making in the real world, it suffers from a number of unique difficulties which often need specific consideration. In particular: it is highly non-stationary; suffers from high degrees of plasticity loss; and requires exploration to prevent premature convergence to local optima and maximize return. In this paper, we consider whether learned optimization can help overcome these problems. Our method, Learned Optimization for Plasticity, Exploration and Non-stationarity (OPEN), meta-learns an update rule whose input features and output structure are informed by previously proposed solutions to these difficulties. We show that our parameterization is flexible enough to enable meta-learning in diverse learning contexts, including the ability to use stochasticity for exploration. Our experiments demonstrate that when meta-trained on single and small sets of environments, OPEN outperforms or equals traditionally used optimizers. Furthermore, OPEN shows strong generalization characteristics across a range of environments and agent architectures.","authors":["Alexander David Goldie","Chris Lu","Matthew Thomas Jackson","Shimon Whiteson","Jakob Nicolaus Foerster"],"url":"https://arxiv.org/abs/2407.07082"}
{"created":"2025-04-16","title":"Teaching Transformers Causal Reasoning through Axiomatic Training","abstract":"For text-based AI systems to interact in the real world, causal reasoning is an essential skill. Since active interventions are costly, we study to what extent a system can learn causal reasoning from symbolic demonstrations of causal axioms. Specifically, we present an axiomatic training method where the system learns from multiple demonstrations of a causal axiom (or rule), rather than incorporating the axiom as an inductive bias or inferring it from data values. A key question is whether the system would learn to generalize from the axiom demonstrations to more complex scenarios. Our results, based on applying axiomatic training to learn the transitivity axiom and d-separation rule, indicate that such generalization is possible. To avoid data contamination issues, we start with a 67 million parameter transformer model and train it from scratch. On both tasks, we find that a model trained on linear causal chains (along with some noisy variations) can generalize well to complex graphs, including longer causal chains, causal chains with reversed order, and graphs with branching.To handle diverse text inputs, the same method is extended to finetune language models. Finetuning Llama-3.1 8B model on our axiomatic data leads to significant gains on causal benchmarks such as Corr2Cause and CLEAR, in some cases providing state-of-the-art performance surpassing GPT-4.","authors":["Aniket Vashishtha","Abhinav Kumar","Atharva Pandey","Abbavaram Gowtham Reddy","Kabir Ahuja","Vineeth N Balasubramanian","Amit Sharma"],"url":"https://arxiv.org/abs/2407.07612"}
{"created":"2025-04-16","title":"MARTSIA: A Tool for Confidential Data Exchange via Public Blockchain","abstract":"Blockchain technology streamlines multi-party collaborations in decentralized settings, especially when trust is limited or difficult to establish. While public blockchains enhance transparency and reliability by replicating data across all network nodes, they also conflict with confidentiality. Here, we introduce Multi-Authority Approach to Transaction Systems for Interoperating Applications (MARTSIA) to address this challenge. MARTSIA provides fine-grained read-access control at the message-part level by combining user-defined policies with certifier-declared attributes. The approach guarantees that even though data is replicated across the network to maintain consistency, fault tolerance, and availability, its confidentiality is securely preserved through encryption. To this end, MARTSIA integrates blockchain technologies, Multi-Authority Attribute-Based Encryption, and distributed hash-table file storages. This architecture effectively balances the transparency inherent in public blockchains with the privacy required for sensitive applications. We present the tool and its applicability in a business scenario.","authors":["Michele Kryston","Edoardo Marangone","Claudio Di Ciccio","Daniele Friolo","Eugenio Nerio Nemmi","Mattia Samory","Michele Spina","Daniele Venturi","Ingo Weber"],"url":"https://arxiv.org/abs/2407.10684"}
{"created":"2025-04-16","title":"System-1.x: Learning to Balance Fast and Slow Planning with Language Models","abstract":"Language models can be used to solve long-horizon planning problems in two distinct modes: a fast 'System-1' mode, directly generating plans without any explicit search or backtracking, and a slow 'System-2' mode, planning step-by-step by explicitly searching over possible actions. While System-2 is typically more effective, it is also more computationally expensive, making it infeasible for long plans or large action spaces. Moreover, isolated System-1 or 2 ignores the user's end goals, failing to provide ways to control the model's behavior. To this end, we propose the System-1.x Planner, a controllable planning framework with LLMs that is capable of generating hybrid plans and balancing between the two planning modes based on the difficulty of the problem at hand. System-1.x consists of (i) a controller, (ii) a System-1 Planner, and (iii) a System-2 Planner. Based on a user-specified hybridization factor (x) governing the mixture between System-1 and 2, the controller decomposes a problem into sub-goals, and classifies them as easy or hard to be solved by either System-1 or 2, respectively. We fine-tune all three components on top of a single base LLM, requiring only search traces as supervision. Experiments with two diverse planning tasks -- Maze Navigation and Blocksworld -- show that our System-1.x Planner outperforms a System-1 Planner, a System-2 Planner trained to approximate A* search, and also a symbolic planner (A*). We demonstrate the following key properties of our planner: (1) controllability: increasing the hybridization factor (e.g., System-1.75 vs 1.5) performs more search, improving performance, (2) flexibility: by building a neuro-symbolic variant with a neural System-1 and a symbolic System-2, we can use existing symbolic methods, and (3) generalizability: by being able to learn from different search algorithms, our method is robust to the choice of search algorithm.","authors":["Swarnadeep Saha","Archiki Prasad","Justin Chih-Yao Chen","Peter Hase","Elias Stengel-Eskin","Mohit Bansal"],"url":"https://arxiv.org/abs/2407.14414"}
{"created":"2025-04-16","title":"A DeepONet for inverting the Neumann-to-Dirichlet Operator in Electrical Impedance Tomography: An approximation theoretic perspective and numerical results","abstract":"In this work, we consider the non-invasive medical imaging modality of Electrical Impedance Tomography, where the problem is to recover the conductivity in a medium from a set of data that arises out of a current-to-voltage map (Neumann-to-Dirichlet operator) defined on the boundary of the medium. We formulate this inverse problem as an operator-learning problem where the goal is to learn the implicitly defined operator-to-function map between the space of Neumann-to-Dirichlet operators to the space of admissible conductivities. Subsequently, we use an operator-learning architecture, popularly called DeepONets, to learn this operator-to-function map. Thus far, most of the operator learning architectures have been implemented to learn operators between function spaces. In this work, we generalize the earlier works and use a DeepONet to actually {learn an operator-to-function} map. We provide a Universal Approximation Theorem type result which guarantees that this implicitly defined operator-to-function map between the space of Neumann-to-Dirichlet operator to the space of conductivity function can be approximated to an arbitrary degree using such a DeepONet. Furthermore, we provide a computational implementation of our proposed approach and compare it against a standard baseline. We show that the proposed approach achieves good reconstructions and outperforms the baseline method in our experiments.","authors":["Anuj Abhishek","Thilo Strauss"],"url":"https://arxiv.org/abs/2407.17182"}
{"created":"2025-04-16","title":"Towards the Terminator Economy: Assessing Job Exposure to AI through LLMs","abstract":"AI and related technologies are reshaping jobs and tasks, either by automating or augmenting human skills in the workplace. Many researchers have been working on estimating if and to what extent jobs and tasks are exposed to the risk of being automatized by AI-related technologies. Our work tackles this issue through a data-driven approach by: (i) developing a reproducible framework that uses cutting-edge open-source large language models to assess the current capabilities of AI and robotics in performing job-related tasks; (ii) formalizing and computing a measure of AI exposure by occupation, the Task Exposure to AI (TEAI) index, and a measure of Task Replacement by AI (TRAI), both validated through a human user evaluation and compared with the state of the art.","authors":["Emilio Colombo","Fabio Mercorio","Mario Mezzanzanica","Antonio Serino"],"url":"https://arxiv.org/abs/2407.19204"}
{"created":"2025-04-16","title":"\"A Good Bot Always Knows Its Limitations\": Assessing Autonomous System Decision-making Competencies through Factorized Machine Self-confidence","abstract":"How can intelligent machines assess their competency to complete a task? This question has come into focus for autonomous systems that algorithmically make decisions under uncertainty. We argue that machine self-confidence -- a form of meta-reasoning based on self-assessments of system knowledge about the state of the world, itself, and ability to reason about and execute tasks -- leads to many computable and useful competency indicators for such agents. This paper presents our body of work, so far, on this concept in the form of the Factorized Machine Self-confidence (FaMSeC) framework, which holistically considers several major factors driving competency in algorithmic decision-making: outcome assessment, solver quality, model quality, alignment quality, and past experience. In FaMSeC, self-confidence indicators are derived via 'problem-solving statistics' embedded in Markov decision process solvers and related approaches. These statistics come from evaluating probabilistic exceedance margins in relation to certain outcomes and associated competency standards specified by an evaluator. Once designed, and evaluated, the statistics can be easily incorporated into autonomous agents and serve as indicators of competency. We include detailed descriptions and examples for Markov decision process agents, and show how outcome assessment and solver quality factors can be found for a range of tasking contexts through novel use of meta-utility functions, behavior simulations, and surrogate prediction models. Numerical evaluations are performed to demonstrate that FaMSeC indicators perform as desired (references to human subject studies beyond the scope of this paper are provided).","authors":["Brett W. Israelsen","Nisar R. Ahmed","Matthew Aitken","Eric W. Frew","Dale A. Lawrence","Brian M. Argrow"],"url":"https://arxiv.org/abs/2407.19631"}
{"created":"2025-04-16","title":"Integer-Valued Training and Spike-Driven Inference Spiking Neural Network for High-performance and Energy-efficient Object Detection","abstract":"Brain-inspired Spiking Neural Networks (SNNs) have bio-plausibility and low-power advantages over Artificial Neural Networks (ANNs). Applications of SNNs are currently limited to simple classification tasks because of their poor performance. In this work, we focus on bridging the performance gap between ANNs and SNNs on object detection. Our design revolves around network architecture and spiking neuron. First, the overly complex module design causes spike degradation when the YOLO series is converted to the corresponding spiking version. We design a SpikeYOLO architecture to solve this problem by simplifying the vanilla YOLO and incorporating meta SNN blocks. Second, object detection is more sensitive to quantization errors in the conversion of membrane potentials into binary spikes by spiking neurons. To address this challenge, we design a new spiking neuron that activates Integer values during training while maintaining spike-driven by extending virtual timesteps during inference. The proposed method is validated on both static and neuromorphic object detection datasets. On the static COCO dataset, we obtain 66.2% mAP@50 and 48.9% mAP@50:95, which is +15.0% and +18.7% higher than the prior state-of-the-art SNN, respectively. On the neuromorphic Gen1 dataset, we achieve 67.2% mAP@50, which is +2.5% greater than the ANN with equivalent architecture, and the energy efficiency is improved by 5.7*. Code: https://github.com/BICLab/SpikeYOLO","authors":["Xinhao Luo","Man Yao","Yuhong Chou","Bo Xu","Guoqi Li"],"url":"https://arxiv.org/abs/2407.20708"}
{"created":"2025-04-16","title":"Deep Learning-Based Longitudinal Prediction of Childhood Myopia Progression Using Fundus Image Sequences and Baseline Refraction Data","abstract":"Childhood myopia constitutes a significant global health concern. It exhibits an escalating prevalence and has the potential to evolve into severe, irreversible conditions that detrimentally impact familial well-being and create substantial economic costs. Contemporary research underscores the importance of precisely predicting myopia progression to enable timely and effective interventions, thereby averting severe visual impairment in children. Such predictions predominantly rely on subjective clinical assessments, which are inherently biased and resource-intensive, thus hindering their widespread application. In this study, we introduce a novel, high-accuracy method for quantitatively predicting the myopic trajectory and myopia risk in children using only fundus images and baseline refraction data. This approach was validated through a six-year longitudinal study of 3,408 children in Henan, utilizing 16,211 fundus images and corresponding refractive data. Our method based on deep learning demonstrated predictive accuracy with an error margin of 0.311D per year and AUC scores of 0.944 and 0.995 for forecasting the risks of developing myopia and high myopia, respectively. These findings confirm the utility of our model in supporting early intervention strategies and in significantly reducing healthcare costs, particularly by obviating the need for additional metadata and repeated consultations. Furthermore, our method was designed to rely only on fundus images and refractive error data, without the need for meta data or multiple inquiries from doctors, strongly reducing the associated medical costs and facilitating large-scale screening. Our model can even provide good predictions based on only a single time measurement. Consequently, the proposed method is an important means to reduce medical inequities caused by economic disparities.","authors":["Mengtian Kang","Yansong Hu","Shuo Gao","Yuanyuan Liu","Hongbei Meng","Xuemeng Li","Xuhang Chen","Hubin Zhao","Jing Fu","Guohua Hu","Wei Wang","Yanning Dai","Arokia Nathan","Peter Smielewski","Ningli Wang","Shiming Li"],"url":"https://arxiv.org/abs/2407.21467"}
{"created":"2025-04-16","title":"An augmented Lagrangian preconditioner for the control of the Navier--Stokes equations","abstract":"We address the solution of the distributed control problem for the steady, incompressible Navier--Stokes equations. We propose an inexact Newton linearization of the optimality conditions. Upon discretization by a finite element scheme, we obtain a sequence of large symmetric linear systems of saddle-point type. We use an augmented Lagrangian-based block triangular preconditioner in combination with the flexible GMRES method at each Newton step. The preconditioner is applied inexactly via a suitable multigrid solver. Numerical experiments indicate that the resulting method appears to be fairly robust with respect to viscosity, mesh size, and the choice of regularization parameter when applied to 2D problems.","authors":["Santolo Leveque","Michele Benzi","Patrick E. Farrell"],"url":"https://arxiv.org/abs/2408.05095"}
{"created":"2025-04-16","title":"DeNOTS: Stable Deep Neural ODEs for Time Series","abstract":"Neural ODEs are a prominent branch of methods designed to capture the temporal evolution of complex time-stamped data. Their idea is to solve an ODE with Neural Network-defined dynamics, which take the immediate parameters of the observed system into account. However, larger integration intervals cause instability, which forces most modern methods to normalize time to $[0, 1]$. We provably stabilize these models by introducing an adaptive negative feedback mechanism. This modification allows for longer integration, which in turn implies higher expressiveness, mirroring the behaviour of increasing depth in conventional Neural Networks.Additionally, it provides intriguing theoretical properties: forgetfulness and missing-value robustness. For three open datasets, our method obtains up to 20\\% improvements in downstream quality if compared to existing baselines, including State Space Models and Neural~CDEs.","authors":["Ilya Kuleshov","Evgenia Romanenkova","Galina Boeva","Vladislav Zhuzhel","Evgeni Vorsin","Alexey Zaytsev"],"url":"https://arxiv.org/abs/2408.08055"}
{"created":"2025-04-16","title":"Weakly Supervised Lymph Nodes Segmentation Based on Partial Instance Annotations with Pre-trained Dual-branch Network and Pseudo Label Learning","abstract":"Assessing the presence of potentially malignant lymph nodes aids in estimating cancer progression, and identifying surrounding benign lymph nodes can assist in determining potential metastatic pathways for cancer. For quantitative analysis, automatic segmentation of lymph nodes is crucial. However, due to the labor-intensive and time-consuming manual annotation process required for a large number of lymph nodes, it is more practical to annotate only a subset of the lymph node instances to reduce annotation costs. In this study, we propose a pre-trained Dual-Branch network with Dynamically Mixed Pseudo label (DBDMP) to learn from partial instance annotations for lymph nodes segmentation. To obtain reliable pseudo labels for lymph nodes that are not annotated, we employ a dual-decoder network to generate different outputs that are then dynamically mixed. We integrate the original weak partial annotations with the mixed pseudo labels to supervise the network. To further leverage the extensive amount of unannotated voxels, we apply a self-supervised pre-training strategy to enhance the model's feature extraction capability. Experiments on the mediastinal Lymph Node Quantification (LNQ) dataset demonstrate that our method, compared to directly learning from partial instance annotations, significantly improves the Dice Similarity Coefficient (DSC) from 11.04% to 54.10% and reduces the Average Symmetric Surface Distance (ASSD) from 20.83 $mm$ to 8.72 $mm$. The code is available at https://github.com/WltyBY/LNQ2023_training_code.git","authors":["Litingyu Wang (University of Electronic Science and Technology of China","Chengdu","China)","Yijie Qu (University of Electronic Science and Technology of China","Chengdu","China)","Xiangde Luo (University of Electronic Science and Technology of China","Chengdu","China","Shanghai AI Laboratory","Shanghai","China)","Wenjun Liao (University of Electronic Science and Technology of China","Chengdu","China","Department of Radiation Oncology","Sichuan Cancer Hospital & Institute","Sichuan Cancer Center","Chengdu","China)","Shichuan Zhang (University of Electronic Science and Technology of China","Chengdu","China","Department of Radiation Oncology","Sichuan Cancer Hospital & Institute","Sichuan Cancer Center","Chengdu","China)","Guotai Wang (University of Electronic Science and Technology of China","Chengdu","China","Shanghai AI Laboratory","Shanghai","China)"],"url":"https://arxiv.org/abs/2408.09411"}
{"created":"2025-04-16","title":"IAA: Inner-Adaptor Architecture Empowers Frozen Large Language Model with Multimodal Capabilities","abstract":"In the field of multimodal large language models (MLLMs), common methods typically involve unfreezing the language model during training to foster profound visual understanding. However, the fine-tuning of such models with vision-language data often leads to a diminution of their natural language processing (NLP) capabilities. To avoid this performance degradation, a straightforward solution is to freeze the language model while developing multimodal competencies. Unfortunately, previous works have not attained satisfactory outcomes. Building on the strategy of freezing the language model, we conduct thorough structural exploration and introduce the Inner-Adaptor Architecture (IAA). Specifically, the architecture incorporates multiple multimodal adaptors at varying depths within the large language model to facilitate direct interaction with the inherently text-oriented transformer layers, thereby enabling the frozen language model to acquire multimodal capabilities. Unlike previous approaches of freezing language models that require large-scale aligned data, our proposed architecture is able to achieve superior performance on small-scale datasets. We conduct extensive experiments to improve the general multimodal capabilities and visual grounding abilities of the MLLM. Our approach remarkably outperforms previous state-of-the-art methods across various vision-language benchmarks without sacrificing performance on NLP tasks. Code and models are available at https://github.com/360CVGroup/Inner-Adaptor-Architecture.","authors":["Bin Wang","Chunyu Xie","Dawei Leng","Yuhui Yin"],"url":"https://arxiv.org/abs/2408.12902"}
{"created":"2025-04-16","title":"PointDGMamba: Domain Generalization of Point Cloud Classification via Generalized State Space Model","abstract":"Domain Generalization (DG) has been recently explored to improve the generalizability of point cloud classification (PCC) models toward unseen domains. However, they often suffer from limited receptive fields or quadratic complexity due to using convolution neural networks or vision Transformers. In this paper, we present the first work that studies the generalizability of state space models (SSMs) in DG PCC and find that directly applying SSMs into DG PCC will encounter several challenges: the inherent topology of the point cloud tends to be disrupted and leads to noise accumulation during the serialization stage. Besides, the lack of designs in domain-agnostic feature learning and data scanning will introduce unanticipated domain-specific information into the 3D sequence data. To this end, we propose a novel framework, PointDGMamba, that excels in strong generalizability toward unseen domains and has the advantages of global receptive fields and efficient linear complexity. PointDGMamba consists of three innovative components: Masked Sequence Denoising (MSD), Sequence-wise Cross-domain Feature Aggregation (SCFA), and Dual-level Domain Scanning (DDS). In particular, MSD selectively masks out the noised point tokens of the point cloud sequences, SCFA introduces cross-domain but same-class point cloud features to encourage the model to learn how to extract more generalized features. DDS includes intra-domain scanning and cross-domain scanning to facilitate information exchange between features. In addition, we propose a new and more challenging benchmark PointDG-3to1 for multi-domain generalization. Extensive experiments demonstrate the effectiveness and state-of-the-art performance of PointDGMamba.","authors":["Hao Yang","Qianyu Zhou","Haijia Sun","Xiangtai Li","Fengqi Liu","Xuequan Lu","Lizhuang Ma","Shuicheng Yan"],"url":"https://arxiv.org/abs/2408.13574"}
{"created":"2025-04-16","title":"Unlocking the Wisdom of Large Language Models: An Introduction to The Path to Artificial General Intelligence","abstract":"This booklet, Unlocking the Wisdom of Multi-LLM Collaborative Intelligence, serves as an accessible introduction to the full volume The Path to Artificial General Intelligence. Through fourteen aphorisms, it distills the core principles of Multi-LLM Agent Collaborative Intelligence (MACI), a framework designed to coordinate multiple LLMs toward reasoning, planning, and decision-making that surpasses the capabilities of any single model. The booklet includes titles, abstracts, and introductions from each main chapter, along with the full content of the first two. The newly released third edition features significant enhancements to Chapters 6 through 9 and a revised preface responding to Yann LeCun's critique of AGI feasibility. While LeCun argues that LLMs lack grounding, memory, and planning, we propose that MACI's collaborative architecture, featuring multimodal agents in executive, legislative, and judicial roles, directly addresses these limitations. Chapters on SocraSynth, EVINCE, consciousness modeling, and behavior regulation demonstrate that reasoning systems grounded in structured interaction and checks and balances can produce more reliable, interpretable, and adaptive intelligence. By integrating complementary model strengths, including world modeling and multimodal perception, MACI enables a system-level intelligence that exceeds the sum of its parts. Like human institutions, progress in AI may depend less on isolated performance and more on coordinated judgment. Collaborative LLMs, not just larger ones, may chart the path toward artificial general intelligence.","authors":["Edward Y. Chang"],"url":"https://arxiv.org/abs/2409.01007"}
{"created":"2025-04-16","title":"Nested Bregman Iterations for Decomposition Problems","abstract":"We consider the task of image reconstruction while simultaneously decomposing the reconstructed image into components with different features. A commonly used tool for this is a variational approach with an infimal convolution of appropriate functions as a regularizer. Especially for noise corrupted observations, incorporating these functionals into the classical method of Bregman iterations provides a robust method for obtaining an overall good approximation of the true image, by stopping early the iteration according to a discrepancy principle. However, crucially, the quality of the separate components depends further on the proper choice of the regularization weights associated to the infimally convoluted functionals. Here, we propose the method of Nested Bregman iterations to improve a decomposition in a structured way. This allows to transform the task of choosing the weights into the problem of stopping the iteration according to a meaningful criterion based on normalized cross-correlation. We discuss the well-definedness and the convergence behavior of the proposed method, and illustrate its strength numerically with various image decomposition tasks employing infimal convolution functionals.","authors":["Tobias Wolf","Derek Driggs","Kostas Papafitsoros","Elena Resmerita","Carola-Bibiane Sch\\\"onlieb"],"url":"https://arxiv.org/abs/2409.01097"}
{"created":"2025-04-16","title":"ResiLogic: Leveraging Composability and Diversity to Design Fault and Intrusion Resilient Chips","abstract":"A long-standing challenge is the design of chips resilient to faults and glitches. Both fine-grained gate diversity and coarse-grained modular redundancy have been used in the past. However, these approaches have not been well-studied under other threat models where some stakeholders in the supply chain are untrusted. Increasing digital sovereignty tensions raise concerns regarding the use of foreign off-the-shelf tools and IPs, or off-sourcing fabrication, driving research into the design of resilient chips under this threat model. This paper addresses a threat model considering three pertinent attacks to resilience: distribution, zonal, and compound attacks. To mitigate these attacks, we introduce the \\texttt{ResiLogic} framework that exploits \\textit{Diversity by Composability}: constructing diverse circuits composed of smaller diverse ones by design. This gives designer the capability to create circuits at design time without requiring extra redundancy in space or cost. Using this approach at different levels of granularity is shown to improve the resilience of circuit design in \\texttt{ResiLogic} against the three considered attacks by a factor of five. Additionally, we also make a case to show how E-Graphs can be utilized to generate diverse circuits under given rewrite rules.","authors":["Ahmad T. Sheikh","Ali Shoker","Suhaib A. Fahmy","Paulo Esteves-Verissimo"],"url":"https://arxiv.org/abs/2409.02553"}
{"created":"2025-04-16","title":"What is the Role of Small Models in the LLM Era: A Survey","abstract":"Large Language Models (LLMs) have made significant progress in advancing artificial general intelligence (AGI), leading to the development of increasingly large models such as GPT-4 and LLaMA-405B. However, scaling up model sizes results in exponentially higher computational costs and energy consumption, making these models impractical for academic researchers and businesses with limited resources. At the same time, Small Models (SMs) are frequently used in practical settings, although their significance is currently underestimated. This raises important questions about the role of small models in the era of LLMs, a topic that has received limited attention in prior research. In this work, we systematically examine the relationship between LLMs and SMs from two key perspectives: Collaboration and Competition. We hope this survey provides valuable insights for practitioners, fostering a deeper understanding of the contribution of small models and promoting more efficient use of computational resources. The code is available at https://github.com/tigerchen52/role_of_small_models","authors":["Lihu Chen","Ga\\\"el Varoquaux"],"url":"https://arxiv.org/abs/2409.06857"}
{"created":"2025-04-16","title":"Gaussian Differentially Private Human Faces Under a Face Radial Curve Representation","abstract":"In this paper we consider the problem of releasing a Gaussian Differentially Private (GDP) 3D human face. The human face is a complex structure with many features and inherently tied to one's identity. Protecting this data, in a formally private way, is important yet challenging given the dimensionality of the problem. We extend approximate DP techniques for functional data to the GDP framework. We further propose a novel representation, face radial curves, of a 3D face as a set of functions and then utilize our proposed GDP functional data mechanism. To preserve the shape of the face while injecting noise we rely on tools from shape analysis for our novel representation of the face. We show that our method preserves the shape of the average face and injects less noise than traditional methods for the same privacy budget. Our mechanism consists of two primary components, the first is generally applicable to function value summaries (as are commonly found in nonparametric statistics or functional data analysis) while the second is general to disk-like surfaces and hence more applicable than just to human faces.","authors":["Carlos Soto","Matthew Reimherr","Aleksandra Slavkovic","Mark Shriver"],"url":"https://arxiv.org/abs/2409.08301"}
{"created":"2025-04-16","title":"FedProphet: Memory-Efficient Federated Adversarial Training via Robust and Consistent Cascade Learning","abstract":"Federated Adversarial Training (FAT) can supplement robustness against adversarial examples to Federated Learning (FL), promoting a meaningful step toward trustworthy AI. However, FAT requires large models to preserve high accuracy while achieving strong robustness, incurring high memory-swapping latency when training on memory-constrained edge devices. Existing memory-efficient FL methods suffer from poor accuracy and weak robustness due to inconsistent local and global models. In this paper, we propose FedProphet, a novel FAT framework that can achieve memory efficiency, robustness, and consistency simultaneously. FedProphget reduces the memory requirement in local training while guaranteeing adversarial robustness by adversarial cascade learning with strong convexity regularization, and we show that the strong robustness also implies low inconsistency in FedProphet. We also develop a training coordinator on the server of FL, with Adaptive Perturbation Adjustment for utility-robustness balance and Differentiated Module Assignment for objective inconsistency mitigation. FedPeophet significantly outperforms other baselines under different experimental settings, maintaining the accuracy and robustness of end-to-end FAT with 80% memory reduction and up to 10.8x speedup in training time.","authors":["Minxue Tang","Yitu Wang","Jingyang Zhang","Louis DiValentin","Aolin Ding","Amin Hass","Yiran Chen","Hai \"Helen\" Li"],"url":"https://arxiv.org/abs/2409.08372"}
{"created":"2025-04-16","title":"PIP-Loco: A Proprioceptive Infinite Horizon Planning Framework for Quadrupedal Robot Locomotion","abstract":"A core strength of Model Predictive Control (MPC) for quadrupedal locomotion has been its ability to enforce constraints and provide interpretability of the sequence of commands over the horizon. However, despite being able to plan, MPC struggles to scale with task complexity, often failing to achieve robust behavior on rapidly changing surfaces. On the other hand, model-free Reinforcement Learning (RL) methods have outperformed MPC on multiple terrains, showing emergent motions but inherently lack any ability to handle constraints or perform planning. To address these limitations, we propose a framework that integrates proprioceptive planning with RL, allowing for agile and safe locomotion behaviors through the horizon. Inspired by MPC, we incorporate an internal model that includes a velocity estimator and a Dreamer module. During training, the framework learns an expert policy and an internal model that are co-dependent, facilitating exploration for improved locomotion behaviors. During deployment, the Dreamer module solves an infinite-horizon MPC problem, adapting actions and velocity commands to respect the constraints. We validate the robustness of our training framework through ablation studies on internal model components and demonstrate improved robustness to training noise. Finally, we evaluate our approach across multi-terrain scenarios in both simulation and hardware.","authors":["Aditya Shirwatkar","Naman Saxena","Kishore Chandra","Shishir Kolathaya"],"url":"https://arxiv.org/abs/2409.09441"}
{"created":"2025-04-16","title":"Robust Reinforcement Learning with Dynamic Distortion Risk Measures","abstract":"In a reinforcement learning (RL) setting, the agent's optimal strategy heavily depends on her risk preferences and the underlying model dynamics of the training environment. These two aspects influence the agent's ability to make well-informed and time-consistent decisions when facing testing environments. In this work, we devise a framework to solve robust risk-aware RL problems where we simultaneously account for environmental uncertainty and risk with a class of dynamic robust distortion risk measures. Robustness is introduced by considering all models within a Wasserstein ball around a reference model. We estimate such dynamic robust risk measures using neural networks by making use of strictly consistent scoring functions, derive policy gradient formulae using the quantile representation of distortion risk measures, and construct an actor-critic algorithm to solve this class of robust risk-aware RL problems. We demonstrate the performance of our algorithm on a portfolio allocation example.","authors":["Anthony Coache","Sebastian Jaimungal"],"url":"https://arxiv.org/abs/2409.10096"}
{"created":"2025-04-16","title":"Algorithmic Behaviors Across Regions: A Geolocation Audit of YouTube Search for COVID-19 Misinformation Between the United States and South Africa","abstract":"Despite being an integral tool for finding health-related information online, YouTube has faced criticism for disseminating COVID-19 misinformation globally to its users. Yet, prior audit studies have predominantly investigated YouTube within the Global North contexts, often overlooking the Global South. To address this gap, we conducted a comprehensive 10-day geolocation-based audit on YouTube to compare the prevalence of COVID-19 misinformation in search results between the United States (US) and South Africa (SA), the countries heavily affected by the pandemic in the Global North and the Global South, respectively. For each country, we selected 3 geolocations and placed sock-puppets, or bots emulating \"real\" users, that collected search results for 48 search queries sorted by 4 search filters for 10 days, yielding a dataset of 915K results. We found that 31.55% of the top-10 search results contained COVID-19 misinformation. Among the top-10 search results, bots in SA faced significantly more misinformative search results than their US counterparts. Overall, our study highlights the contrasting algorithmic behaviors of YouTube search between two countries, underscoring the need for the platform to regulate algorithmic behavior consistently across different regions of the Globe.","authors":["Hayoung Jung","Prerna Juneja","Tanushree Mitra"],"url":"https://arxiv.org/abs/2409.10168"}
{"created":"2025-04-16","title":"Protecting Copyright of Medical Pre-trained Language Models: Training-Free Backdoor Model Watermarking","abstract":"With the advancement of intelligent healthcare, medical pre-trained language models (Med-PLMs) have emerged and demonstrated significant effectiveness in downstream medical tasks. While these models are valuable assets, they are vulnerable to misuse and theft, requiring copyright protection. However, existing watermarking methods for pre-trained language models (PLMs) cannot be directly applied to Med-PLMs due to domain-task mismatch and inefficient watermark embedding. To fill this gap, we propose the first training-free backdoor model watermarking for Med-PLMs. Our method employs low-frequency words as triggers, embedding the watermark by replacing their embeddings in the model's word embedding layer with those of specific medical terms. The watermarked Med-PLMs produce the same output for triggers as for the corresponding specified medical terms. We leverage this unique mapping to design tailored watermark extraction schemes for different downstream tasks, thereby addressing the challenge of domain-task mismatch in previous methods. Experiments demonstrate superior effectiveness of our watermarking method across medical downstream tasks. Moreover, the method exhibits robustness against model extraction, pruning, fusion-based backdoor removal attacks, while maintaining high efficiency with 10-second watermark embedding.","authors":["Cong Kong","Rui Xu","Weixi Chen","Jiawei Chen","Zhaoxia Yin"],"url":"https://arxiv.org/abs/2409.10570"}
{"created":"2025-04-16","title":"Clustering with Non-adaptive Subset Queries","abstract":"Recovering the underlying clustering of a set $U$ of $n$ points by asking pair-wise same-cluster queries has garnered significant interest in the last decade. Given a query $S \\subset U$, $|S|=2$, the oracle returns yes if the points are in the same cluster and no otherwise. For adaptive algorithms with pair-wise queries, the number of required queries is known to be $\\Theta(nk)$, where $k$ is the number of clusters. However, non-adaptive schemes require $\\Omega(n^2)$ queries, which matches the trivial $O(n^2)$ upper bound attained by querying every pair of points.","authors":["Hadley Black","Euiwoong Lee","Arya Mazumdar","Barna Saha"],"url":"https://arxiv.org/abs/2409.10908"}
{"created":"2025-04-16","title":"MalMixer: Few-Shot Malware Classification with Retrieval-Augmented Semi-Supervised Learning","abstract":"Recent growth and proliferation of malware has tested practitioners' ability to promptly classify new samples according to malware families. In contrast to labor-intensive reverse engineering efforts, machine learning approaches have demonstrated increased speed and accuracy. However, most existing deep-learning malware family classifiers must be calibrated using a large number of samples that are painstakingly manually analyzed before training. Furthermore, as novel malware samples arise that are beyond the scope of the training set, additional reverse engineering effort must be employed to update the training set. The sheer volume of new samples found in the wild creates substantial pressure on practitioners' ability to reverse engineer enough malware to adequately train modern classifiers. In this paper, we present MalMixer, a malware family classifier using semi-supervised learning that achieves high accuracy with sparse training data. We present a novel domain-knowledge-aware technique for augmenting malware feature representations, enhancing few-shot performance of semi-supervised malware family classification. We show that MalMixer achieves state-of-the-art performance in few-shot malware family classification settings. Our research confirms the feasibility and effectiveness of lightweight, domain-knowledge-aware feature augmentation methods and highlights the capabilities of similar semi-supervised classifiers in addressing malware classification issues.","authors":["Jiliang Li","Yifan Zhang","Yu Huang","Kevin Leach"],"url":"https://arxiv.org/abs/2409.13213"}
{"created":"2025-04-16","title":"Why Is Anything Conscious?","abstract":"We tackle the hard problem of consciousness taking the naturally selected, embodied organism as our starting point. We provide a formalism describing how biological systems self-organise to hierarchically interpret unlabelled sensory information according to valence. Such interpretations imply behavioural policies which are differentiated from each other only by the qualitative aspect of information processing. Natural selection favours systems that intervene in the world to achieve homeostatic and reproductive goals. Quality is a property arising in such systems to link cause to affect to motivate interventions. This produces interoceptive and exteroceptive classifiers and determines priorities. In formalising the seminal distinction between access and phenomenal consciousness, we claim that access consciousness at the human level requires the ability to hierarchically model i) the self, ii) the world/others and iii) the self as modelled by others, and that this requires phenomenal consciousness. Phenomenal without access consciousness is likely common, but the reverse is implausible. To put it provocatively: death grounds meaning, and Nature does not like zombies. We then describe the multilayered architecture of self-organisation from rocks to Einstein, illustrating how our argument applies. Our proposal lays the foundation of a formal science of consciousness, closer to human fact than zombie fiction.","authors":["Michael Timothy Bennett","Sean Welsh","Anna Ciaunica"],"url":"https://arxiv.org/abs/2409.14545"}
{"created":"2025-04-16","title":"ToxiCraft: A Novel Framework for Synthetic Generation of Harmful Information","abstract":"In different NLP tasks, detecting harmful content is crucial for online environments, especially with the growing influence of social media. However, previous research has two main issues: 1) a lack of data in low-resource settings, and 2) inconsistent definitions and criteria for judging harmful content, requiring classification models to be robust to spurious features and diverse. We propose Toxicraft, a novel framework for synthesizing datasets of harmful information to address these weaknesses. With only a small amount of seed data, our framework can generate a wide variety of synthetic, yet remarkably realistic, examples of toxic information. Experimentation across various datasets showcases a notable enhancement in detection model robustness and adaptability, surpassing or close to the gold labels.","authors":["Zheng Hui","Zhaoxiao Guo","Hang Zhao","Juanyong Duan","Congrui Huang"],"url":"https://arxiv.org/abs/2409.14740"}
{"created":"2025-04-16","title":"SpoofCeleb: Speech Deepfake Detection and SASV In The Wild","abstract":"This paper introduces SpoofCeleb, a dataset designed for Speech Deepfake Detection (SDD) and Spoofing-robust Automatic Speaker Verification (SASV), utilizing source data from real-world conditions and spoofing attacks generated by Text-To-Speech (TTS) systems also trained on the same real-world data. Robust recognition systems require speech data recorded in varied acoustic environments with different levels of noise to be trained. However, current datasets typically include clean, high-quality recordings (bona fide data) due to the requirements for TTS training; studio-quality or well-recorded read speech is typically necessary to train TTS models. Current SDD datasets also have limited usefulness for training SASV models due to insufficient speaker diversity. SpoofCeleb leverages a fully automated pipeline we developed that processes the VoxCeleb1 dataset, transforming it into a suitable form for TTS training. We subsequently train 23 contemporary TTS systems. SpoofCeleb comprises over 2.5 million utterances from 1,251 unique speakers, collected under natural, real-world conditions. The dataset includes carefully partitioned training, validation, and evaluation sets with well-controlled experimental protocols. We present the baseline results for both SDD and SASV tasks. All data, protocols, and baselines are publicly available at https://jungjee.github.io/spoofceleb.","authors":["Jee-weon Jung","Yihan Wu","Xin Wang","Ji-Hoon Kim","Soumi Maiti","Yuta Matsunaga","Hye-jin Shim","Jinchuan Tian","Nicholas Evans","Joon Son Chung","Wangyou Zhang","Seyun Um","Shinnosuke Takamichi","Shinji Watanabe"],"url":"https://arxiv.org/abs/2409.17285"}
{"created":"2025-04-16","title":"CurricuLLM: Automatic Task Curricula Design for Learning Complex Robot Skills using Large Language Models","abstract":"Curriculum learning is a training mechanism in reinforcement learning (RL) that facilitates the achievement of complex policies by progressively increasing the task difficulty during training. However, designing effective curricula for a specific task often requires extensive domain knowledge and human intervention, which limits its applicability across various domains. Our core idea is that large language models (LLMs), with their extensive training on diverse language data and ability to encapsulate world knowledge, present significant potential for efficiently breaking down tasks and decomposing skills across various robotics environments. Additionally, the demonstrated success of LLMs in translating natural language into executable code for RL agents strengthens their role in generating task curricula. In this work, we propose CurricuLLM, which leverages the high-level planning and programming capabilities of LLMs for curriculum design, thereby enhancing the efficient learning of complex target tasks. CurricuLLM consists of: (Step 1) Generating sequence of subtasks that aid target task learning in natural language form, (Step 2) Translating natural language description of subtasks in executable task code, including the reward code and goal distribution code, and (Step 3) Evaluating trained policies based on trajectory rollout and subtask description. We evaluate CurricuLLM in various robotics simulation environments, ranging from manipulation, navigation, and locomotion, to show that CurricuLLM can aid learning complex robot control tasks. In addition, we validate humanoid locomotion policy learned through CurricuLLM in real-world. Project website is https://iconlab.negarmehr.com/CurricuLLM/","authors":["Kanghyun Ryu","Qiayuan Liao","Zhongyu Li","Payam Delgosha","Koushil Sreenath","Negar Mehr"],"url":"https://arxiv.org/abs/2409.18382"}
{"created":"2025-04-16","title":"Trading Determinism for Time: The k-Reach Problem","abstract":"Kallampally and Tewari showed in 2016 that there can be a trade-off between determinism and time in space-bounded computations. This they did by describing an unambiguous non-deterministic algorithm to solve Directed Graph Reachability that requires O(log^2 n) space and simultaneously runs in polynomial time. Savitch's 1970 algorithm that solves the same problem deterministically also requires O(log^2 n) space but doesn't guarantee polynomial running time and hence the trade off. We describe a new problem for which we can show a similar trade off between determinism and time.","authors":["Ronak Bhadra","Raghunath Tewari"],"url":"https://arxiv.org/abs/2409.18469"}
{"created":"2025-04-16","title":"SCA: Highly Efficient Semantic-Consistent Unrestricted Adversarial Attack","abstract":"Deep neural network based systems deployed in sensitive environments are vulnerable to adversarial attacks. Unrestricted adversarial attacks typically manipulate the semantic content of an image (e.g., color or texture) to create adversarial examples that are both effective and photorealistic. Recent works have utilized the diffusion inversion process to map images into a latent space, where high-level semantics are manipulated by introducing perturbations. However, they often results in substantial semantic distortions in the denoised output and suffers from low efficiency. In this study, we propose a novel framework called Semantic-Consistent Unrestricted Adversarial Attacks (SCA), which employs an inversion method to extract edit-friendly noise maps and utilizes Multimodal Large Language Model (MLLM) to provide semantic guidance throughout the process. Under the condition of rich semantic information provided by MLLM, we perform the DDPM denoising process of each step using a series of edit-friendly noise maps, and leverage DPM Solver++ to accelerate this process, enabling efficient sampling with semantic consistency. Compared to existing methods, our framework enables the efficient generation of adversarial examples that exhibit minimal discernible semantic changes. Consequently, we for the first time introduce Semantic-Consistent Adversarial Examples (SCAE). Extensive experiments and visualizations have demonstrated the high efficiency of SCA, particularly in being on average 12 times faster than the state-of-the-art attacks. Our research can further draw attention to the security of multimedia information.","authors":["Zihao Pan","Weibin Wu","Yuhang Cao","Zibin Zheng"],"url":"https://arxiv.org/abs/2410.02240"}
{"created":"2025-04-16","title":"MROSS: Multi-Round Region-based Optimization for Scene Sketching","abstract":"Scene sketching is to convert a scene into a simplified, abstract representation that captures the essential elements and composition of the original scene. It requires a semantic understanding of the scene and consideration of different regions within the scene. Since scenes often contain diverse visual information across various regions, such as foreground objects, background elements, and spatial divisions, dealing with these different regions poses unique difficulties. In this paper, we define a sketch as some sets of B\\'ezier curves because of their smooth and versatile characteristics. We optimize different regions of input scene in multiple rounds. In each optimization round, the strokes sampled from the next region can seamlessly be integrated into the sketch generated in the previous optimization round. We propose an additional stroke initialization method to ensure the integrity of the scene and the convergence of optimization. A novel CLIP-based Semantic Loss and a VGG-based Feature Loss are utilized to guide our multi-round optimization. Extensive experimental results on the quality and quantity of the generated sketches confirm the effectiveness of our method.","authors":["Yiqi Liang","Ying Liu","Dandan Long","Ruihui Li"],"url":"https://arxiv.org/abs/2410.04072"}
{"created":"2025-04-16","title":"TIS-DPO: Token-level Importance Sampling for Direct Preference Optimization With Estimated Weights","abstract":"Direct Preference Optimization (DPO) has been widely adopted for preference alignment of Large Language Models (LLMs) due to its simplicity and effectiveness. However, DPO is derived as a bandit problem in which the whole response is treated as a single arm, ignoring the importance differences between tokens, which may affect optimization efficiency and make it difficult to achieve optimal results. In this work, we propose that the optimal data for DPO has equal expected rewards for each token in winning and losing responses, as there is no difference in token importance. However, since the optimal dataset is unavailable in practice, we propose using the original dataset for importance sampling to achieve unbiased optimization. Accordingly, we propose a token-level importance sampling DPO objective named TIS-DPO that assigns importance weights to each token based on its reward. Inspired by previous works, we estimate the token importance weights using the difference in prediction probabilities from a pair of contrastive LLMs. We explore three methods to construct these contrastive LLMs: (1) guiding the original LLM with contrastive prompts, (2) training two separate LLMs using winning and losing responses, and (3) performing forward and reverse DPO training with winning and losing responses. Experiments show that TIS-DPO significantly outperforms various baseline methods on harmlessness and helpfulness alignment and summarization tasks. We also visualize the estimated weights, demonstrating their ability to identify key token positions.","authors":["Aiwei Liu","Haoping Bai","Zhiyun Lu","Yanchao Sun","Xiang Kong","Simon Wang","Jiulong Shan","Albin Madappally Jose","Xiaojiang Liu","Lijie Wen","Philip S. Yu","Meng Cao"],"url":"https://arxiv.org/abs/2410.04350"}
{"created":"2025-04-16","title":"Why am I seeing this: Democratizing End User Auditing for Online Content Recommendations","abstract":"Personalized recommendation systems tailor content based on user attributes, which are either provided or inferred from private data. Research suggests that users often hypothesize about reasons behind contents they encounter (e.g., \"I see this jewelry ad because I am a woman\"), but they lack the means to confirm these hypotheses due to the opaqueness of these systems. This hinders informed decision-making about privacy and system use and contributes to the lack of algorithmic accountability. To address these challenges, we introduce a new interactive sandbox approach. This approach creates sets of synthetic user personas and corresponding personal data that embody realistic variations in personal attributes, allowing users to test their hypotheses by observing how a website's algorithms respond to these personas. We tested the sandbox in the context of targeted advertisement. Our user study demonstrates its usability, usefulness, and effectiveness in empowering end-user auditing in a case study of targeting ads.","authors":["Chaoran Chen","Leyang Li","Luke Cao","Yanfang Ye","Tianshi Li","Yaxing Yao","Toby Jia-jun Li"],"url":"https://arxiv.org/abs/2410.04917"}
{"created":"2025-04-16","title":"Securing HHL Quantum Algorithm against Quantum Computer Attacks","abstract":"As the quantum research community expands and new quantum algorithms are created and implemented, it is essential to consider the security implications and potential threats that could lead to the compromise the information processed by them. This work focuses on securing the HHL quantum algorithm against attacks while it executes on a quantum computer. Specifically, two types of potential attacks could be deployed on a cloud-based quantum computer by an attacker circuit attempting to interfere with the victim HHL circuit: the Improper Initialization Attack (IIA) and the Higher Energy Attack (HEA). To protect the HHL algorithm from IIA and HEA, this work proposes first-of-a-kind defense strategies against these attacks on the HHL quantum algorithm. Next, this work demonstrates an implementation of a new quantum circuit for the HHL quantum algorithm that incorporates these defenses. The redesigned quantum circuit is necessary to successfully apply and realize all proposed defense strategies. Finally, this work illustrates how these defense strategies function in practice in the redesigned circuit, specifically how they can protect the HHL quantum algorithm from both IIA and HEA across multiple qubits involving all three types of qubits used in the HHL algorithms: ancilla, clock, and b. The defense requires minimal modification to the circuit, and has only a very small effect on the fidelity of the circuits. The circuits have been tested and validated in both simulation, and also on real IBM quantum computer hardware. The work further analyzes how the modified HHL circuit with the defenses is affected by noise during quantum computation. This work in the end demonstrates that it is practical to add protections to quantum circuits so that they not only perform correct computation, but also self-detect if an attack has occured during the execution.","authors":["Yizhuo Tan","Hrvoje Kukina","Jakub Szefer"],"url":"https://arxiv.org/abs/2410.08010"}
{"created":"2025-04-16","title":"Adam Exploits $\\ell_\\infty$-geometry of Loss Landscape via Coordinate-wise Adaptivity","abstract":"Adam outperforms SGD when training language models. Yet this advantage is not well-understood theoretically -- previous convergence analysis for Adam and SGD mainly focuses on the number of steps $T$ and is already minimax-optimal in non-convex cases, which are both $\\widetilde{O}(T^{-1/4})$. In this work, we argue that the exploitation of nice $\\ell_\\infty$-geometry is the key advantage of Adam over SGD. More specifically, we give a new convergence analysis for Adam under novel assumptions that loss is smooth under $\\ell_\\infty$-geometry rather than the more common $\\ell_2$-geometry, which yields a much better empirical smoothness constant for GPT-2 and ResNet models. Our experiments confirm that Adam performs much worse when the favorable $\\ell_\\infty$-geometry is changed while SGD provably remains unaffected. We also extend the convergence analysis to blockwise Adam under novel blockwise smoothness assumptions.","authors":["Shuo Xie","Mohamad Amin Mohamadi","Zhiyuan Li"],"url":"https://arxiv.org/abs/2410.08198"}
{"created":"2025-04-16","title":"AFlow: Automating Agentic Workflow Generation","abstract":"Large language models (LLMs) have demonstrated remarkable potential in solving complex tasks across diverse domains, typically by employing agentic workflows that follow detailed instructions and operational sequences. However, constructing these workflows requires significant human effort, limiting scalability and generalizability. Recent research has sought to automate the generation and optimization of these workflows, but existing methods still rely on initial manual setup and fall short of achieving fully automated and effective workflow generation. To address this challenge, we reformulate workflow optimization as a search problem over code-represented workflows, where LLM-invoking nodes are connected by edges. We introduce AFlow, an automated framework that efficiently explores this space using Monte Carlo Tree Search, iteratively refining workflows through code modification, tree-structured experience, and execution feedback. Empirical evaluations across six benchmark datasets demonstrate AFlow's efficacy, yielding a 5.7% average improvement over state-of-the-art baselines. Furthermore, AFlow enables smaller models to outperform GPT-4o on specific tasks at 4.55% of its inference cost in dollars. The code is available at https://github.com/FoundationAgents/AFlow.","authors":["Jiayi Zhang","Jinyu Xiang","Zhaoyang Yu","Fengwei Teng","Xionghui Chen","Jiaqi Chen","Mingchen Zhuge","Xin Cheng","Sirui Hong","Jinlin Wang","Bingnan Zheng","Bang Liu","Yuyu Luo","Chenglin Wu"],"url":"https://arxiv.org/abs/2410.10762"}
{"created":"2025-04-16","title":"Reward-free World Models for Online Imitation Learning","abstract":"Imitation learning (IL) enables agents to acquire skills directly from expert demonstrations, providing a compelling alternative to reinforcement learning. However, prior online IL approaches struggle with complex tasks characterized by high-dimensional inputs and complex dynamics. In this work, we propose a novel approach to online imitation learning that leverages reward-free world models. Our method learns environmental dynamics entirely in latent spaces without reconstruction, enabling efficient and accurate modeling. We adopt the inverse soft-Q learning objective, reformulating the optimization process in the Q-policy space to mitigate the instability associated with traditional optimization in the reward-policy space. By employing a learned latent dynamics model and planning for control, our approach consistently achieves stable, expert-level performance in tasks with high-dimensional observation or action spaces and intricate dynamics. We evaluate our method on a diverse set of benchmarks, including DMControl, MyoSuite, and ManiSkill2, demonstrating superior empirical performance compared to existing approaches.","authors":["Shangzhe Li","Zhiao Huang","Hao Su"],"url":"https://arxiv.org/abs/2410.14081"}
{"created":"2025-04-16","title":"Accelerate Coastal Ocean Circulation Model with AI Surrogate","abstract":"Nearly 900 million people live in low-lying coastal zones around the world and bear the brunt of impacts from more frequent and severe hurricanes and storm surges. Oceanographers simulate ocean current circulation along the coasts to develop early warning systems that save lives and prevent loss and damage to property from coastal hazards. Traditionally, such simulations are conducted using coastal ocean circulation models such as the Regional Ocean Modeling System (ROMS), which usually runs on an HPC cluster with multiple CPU cores. However, the process is time-consuming and energy expensive. While coarse-grained ROMS simulations offer faster alternatives, they sacrifice detail and accuracy, particularly in complex coastal environments. Recent advances in deep learning and GPU architecture have enabled the development of faster AI (neural network) surrogates. This paper introduces an AI surrogate based on a 4D Swin Transformer to simulate coastal tidal wave propagation in an estuary for both hindcast and forecast (up to 12 days). Our approach not only accelerates simulations but also incorporates a physics-based constraint to detect and correct inaccurate results, ensuring reliability while minimizing manual intervention. We develop a fully GPU-accelerated workflow, optimizing the model training and inference pipeline on NVIDIA DGX-2 A100 GPUs. Our experiments demonstrate that our AI surrogate reduces the time cost of 12-day forecasting of traditional ROMS simulations from 9,908 seconds (on 512 CPU cores) to 22 seconds (on one A100 GPU), achieving over 450$\\times$ speedup while maintaining high-quality simulation results. This work contributes to oceanographic modeling by offering a fast, accurate, and physically consistent alternative to traditional simulation models, particularly for real-time forecasting in rapid disaster response.","authors":["Zelin Xu","Jie Ren","Yupu Zhang","Jose Maria Gonzalez Ondina","Maitane Olabarrieta","Tingsong Xiao","Wenchong He","Zibo Liu","Shigang Chen","Kaleb Smith","Zhe Jiang"],"url":"https://arxiv.org/abs/2410.14952"}
{"created":"2025-04-16","title":"Automated Proof Generation for Rust Code via Self-Evolution","abstract":"Ensuring correctness is crucial for code generation. Formal verification offers a definitive assurance of correctness, but demands substantial human effort in proof construction and hence raises a pressing need for automation. The primary obstacle lies in the severe lack of data-there is much fewer proofs than code snippets for Large Language Models (LLMs) to train upon. In this paper, we introduce SAFE, a framework that overcomes the lack of human-written proofs to enable automated proof generation of Rust code. SAFE establishes a self-evolving cycle where data synthesis and fine-tuning collaborate to enhance the model capability, leveraging the definitive power of a symbolic verifier in telling correct proofs from incorrect ones. SAFE also re-purposes the large number of synthesized incorrect proofs to train the self-debugging capability of the fine-tuned models, empowering them to fix incorrect proofs based on the verifier's feedback. SAFE demonstrates superior efficiency and precision compared to GPT-4o. Through tens of thousands of synthesized proofs and the self-debugging mechanism, we improve the capability of open-source models, initially unacquainted with formal verification, to automatically write proofs for Rust code. This advancement leads to a significant improvement in performance, achieving a 52.52% accuracy rate in a benchmark crafted by human experts, a significant leap over GPT-4o's performance of 14.39%.","authors":["Tianyu Chen","Shuai Lu","Shan Lu","Yeyun Gong","Chenyuan Yang","Xuheng Li","Md Rakib Hossain Misu","Hao Yu","Nan Duan","Peng Cheng","Fan Yang","Shuvendu K Lahiri","Tao Xie","Lidong Zhou"],"url":"https://arxiv.org/abs/2410.15756"}
{"created":"2025-04-16","title":"Molecular Signal Reception in Complex Vessel Networks: The Role of the Network Topology","abstract":"The notion of synthetic molecular communication (MC) refers to the transmission of information via molecules and is largely foreseen for use within the human body, where traditional electromagnetic wave (EM)-based communication is impractical. MC is anticipated to enable innovative medical applications, such as early-stage tumor detection, targeted drug delivery, and holistic approaches like the Internet of Bio-Nano Things (IoBNT). Many of these applications involve parts of the human cardiovascular system (CVS), here referred to as networks, posing challenges for MC due to their complex, highly branched vessel structures. To gain a better understanding of how the topology of such branched vessel networks affects the reception of a molecular signal at a target location, e.g., the network outlet, we present a generic analytical end-to-end model that characterizes molecule propagation and reception in linear branched vessel networks (LBVNs). We specialize this generic model to any MC system employing superparamagnetic iron-oxide nanoparticles (SPIONs) as signaling molecules and a planar coil as receiver (RX). By considering components that have been previously established in testbeds, we effectively isolate the impact of the network topology and validate our theoretical model with testbed data. Additionally, we propose two metrics, namely the molecule delay and the multi-path spread, that relate the LBVN topology to the molecule dispersion induced by the network, thereby linking the network structure to the signal-to-noise ratio (SNR) at the target location. This allows the characterization of the SNR at any point in the network solely based on the network topology. Consequently, our framework can, e.g., be exploited for optimal sensor placement in the CVS or identification of suitable testbed topologies for given SNR requirements.","authors":["Timo Jakumeit","Lukas Brand","Jens Kirchner","Robert Schober","Sebastian Lotter"],"url":"https://arxiv.org/abs/2410.15943"}
{"created":"2025-04-16","title":"A Neural Network Alternative to Tree-based Models","abstract":"Tabular datasets are widely used in scientific disciplines such as biology. While these disciplines have already adopted AI methods to enhance their findings and analysis, they mainly use tree-based methods due to their interpretability. At the same time, artificial neural networks have been shown to offer superior flexibility and depth for rich and complex non-tabular problems, but they are falling behind tree-based models for tabular data in terms of performance and interpretability. Although sparsity has been shown to improve the interpretability and performance of ANN models for complex non-tabular datasets, enforcing sparsity structurally and formatively for tabular data before training the model, remains an open question. To address this question, we establish a method that infuses sparsity in neural networks by utilising attention mechanisms to capture the features' importance in tabular datasets. We show that our models, Sparse TABular NET or sTAB-Net with attention mechanisms, are more effective than tree-based models, reaching the state-of-the-art on biological datasets. They further permit the extraction of insights from these datasets and achieve better performance than post-hoc methods like SHAP.","authors":["Salvatore Raieli","Nathalie Jeanray","St\\'ephane Gerart","Sebastien Vachenc","Abdulrahman Altahhan"],"url":"https://arxiv.org/abs/2410.17758"}
{"created":"2025-04-16","title":"VR-Splatting: Foveated Radiance Field Rendering via 3D Gaussian Splatting and Neural Points","abstract":"Recent advances in novel view synthesis have demonstrated impressive results in fast photorealistic scene rendering through differentiable point rendering, either via Gaussian Splatting (3DGS) [Kerbl and Kopanas et al. 2023] or neural point rendering [Aliev et al. 2020]. Unfortunately, these directions require either a large number of small Gaussians or expensive per-pixel post-processing for reconstructing fine details, which negatively impacts rendering performance. To meet the high performance demands of virtual reality (VR) systems, primitive or pixel counts therefore must be kept low, affecting visual quality.","authors":["Linus Franke","Laura Fink","Marc Stamminger"],"url":"https://arxiv.org/abs/2410.17932"}
{"created":"2025-04-16","title":"MoGe: Unlocking Accurate Monocular Geometry Estimation for Open-Domain Images with Optimal Training Supervision","abstract":"We present MoGe, a powerful model for recovering 3D geometry from monocular open-domain images. Given a single image, our model directly predicts a 3D point map of the captured scene with an affine-invariant representation, which is agnostic to true global scale and shift. This new representation precludes ambiguous supervision in training and facilitate effective geometry learning. Furthermore, we propose a set of novel global and local geometry supervisions that empower the model to learn high-quality geometry. These include a robust, optimal, and efficient point cloud alignment solver for accurate global shape learning, and a multi-scale local geometry loss promoting precise local geometry supervision. We train our model on a large, mixed dataset and demonstrate its strong generalizability and high accuracy. In our comprehensive evaluation on diverse unseen datasets, our model significantly outperforms state-of-the-art methods across all tasks, including monocular estimation of 3D point map, depth map, and camera field of view. Code and models can be found on our project page.","authors":["Ruicheng Wang","Sicheng Xu","Cassie Dai","Jianfeng Xiang","Yu Deng","Xin Tong","Jiaolong Yang"],"url":"https://arxiv.org/abs/2410.19115"}
{"created":"2025-04-16","title":"Graph Linearization Methods for Reasoning on Graphs with Large Language Models","abstract":"Large language models have evolved to process multiple modalities beyond text, such as images and audio, which motivates us to explore how to effectively leverage them for graph reasoning tasks. The key question, therefore, is how to transform graphs into linear sequences of tokens, a process we term \"graph linearization\", so that LLMs can handle graphs naturally. We consider that graphs should be linearized meaningfully to reflect certain properties of natural language text, such as local dependency and global alignment, in order to ease contemporary LLMs, trained on trillions of textual tokens, better understand graphs. To achieve this, we developed several graph linearization methods based on graph centrality and degeneracy. These methods are further enhanced using node relabeling techniques. The experimental results demonstrate the effectiveness of our methods compared to the random linearization baseline. Our work introduces novel graph representations suitable for LLMs, contributing to the potential integration of graph machine learning with the trend of multimodal processing using a unified transformer model.","authors":["Christos Xypolopoulos","Guokan Shang","Xiao Fei","Giannis Nikolentzos","Hadi Abdine","Iakovos Evdaimon","Michail Chatzianastasis","Giorgos Stamou","Michalis Vazirgiannis"],"url":"https://arxiv.org/abs/2410.19494"}
{"created":"2025-04-16","title":"DivShift: Exploring Domain-Specific Distribution Shifts in Large-Scale, Volunteer-Collected Biodiversity Datasets","abstract":"Large-scale, volunteer-collected datasets of community-identified natural world imagery like iNaturalist have enabled marked performance gains for fine-grained visual classification of species using machine learning methods. However, such data -- sometimes referred to as citizen science data -- are opportunistic and lack a structured sampling strategy. This volunteer-collected biodiversity data contains geographic, temporal, taxonomic, observers, and sociopolitical biases that can have significant effects on biodiversity model performance, but whose impacts are unclear for fine-grained species recognition performance. Here we introduce Diversity Shift (DivShift), a framework for quantifying the effects of domain-specific distribution shifts on machine learning model performance. To diagnose the performance effects of biases specific to volunteer-collected biodiversity data, we also introduce DivShift - North American West Coast (DivShift-NAWC), a curated dataset of almost 7.5 million iNaturalist images across the western coast of North America partitioned across five types of expert-verified bias. We compare species recognition performance across these bias partitions using a diverse variety of species- and ecosystem-focused accuracy metrics. We observe that these biases confound model performance less than expected from the underlying label distribution shift, and that more data leads to better model performance but the magnitude of these improvements are bias-specific. These findings imply that while the structure within natural world images provides generalization improvements for biodiversity monitoring tasks, the biases present in volunteer-collected biodiversity data can also affect model performance; thus these models should be used with caution in downstream biodiversity monitoring tasks.","authors":["Elena Sierra","Lauren E. Gillespie","Salim Soltani","Moises Exposito-Alonso","Teja Kattenborn"],"url":"https://arxiv.org/abs/2410.19816"}
{"created":"2025-04-16","title":"Air Quality Prediction with Physics-Guided Dual Neural ODEs in Open Systems","abstract":"Air pollution significantly threatens human health and ecosystems, necessitating effective air quality prediction to inform public policy. Traditional approaches are generally categorized into physics-based and data-driven models. Physics-based models usually struggle with high computational demands and closed-system assumptions, while data-driven models may overlook essential physical dynamics, confusing the capturing of spatiotemporal correlations. Although some physics-guided approaches combine the strengths of both models, they often face a mismatch between explicit physical equations and implicit learned representations. To address these challenges, we propose Air-DualODE, a novel physics-guided approach that integrates dual branches of Neural ODEs for air quality prediction. The first branch applies open-system physical equations to capture spatiotemporal dependencies for learning physics dynamics, while the second branch identifies the dependencies not addressed by the first in a fully data-driven way. These dual representations are temporally aligned and fused to enhance prediction accuracy. Our experimental results demonstrate that Air-DualODE achieves state-of-the-art performance in predicting pollutant concentrations across various spatial scales, thereby offering a promising solution for real-world air quality challenges.","authors":["Jindong Tian","Yuxuan Liang","Ronghui Xu","Peng Chen","Chenjuan Guo","Aoying Zhou","Lujia Pan","Zhongwen Rao","Bin Yang"],"url":"https://arxiv.org/abs/2410.19892"}
{"created":"2025-04-16","title":"ARLON: Boosting Diffusion Transformers with Autoregressive Models for Long Video Generation","abstract":"Text-to-video models have recently undergone rapid and substantial advancements. Nevertheless, due to limitations in data and computational resources, achieving efficient generation of long videos with rich motion dynamics remains a significant challenge. To generate high-quality, dynamic, and temporally consistent long videos, this paper presents ARLON, a novel framework that boosts diffusion Transformers with autoregressive models for long video generation, by integrating the coarse spatial and long-range temporal information provided by the AR model to guide the DiT model. Specifically, ARLON incorporates several key innovations: 1) A latent Vector Quantized Variational Autoencoder (VQ-VAE) compresses the input latent space of the DiT model into compact visual tokens, bridging the AR and DiT models and balancing the learning complexity and information density; 2) An adaptive norm-based semantic injection module integrates the coarse discrete visual units from the AR model into the DiT model, ensuring effective guidance during video generation; 3) To enhance the tolerance capability of noise introduced from the AR inference, the DiT model is trained with coarser visual latent tokens incorporated with an uncertainty sampling module. Experimental results demonstrate that ARLON significantly outperforms the baseline OpenSora-V1.2 on eight out of eleven metrics selected from VBench, with notable improvements in dynamic degree and aesthetic quality, while delivering competitive results on the remaining three and simultaneously accelerating the generation process. In addition, ARLON achieves state-of-the-art performance in long video generation. Detailed analyses of the improvements in inference efficiency are presented, alongside a practical application that demonstrates the generation of long videos using progressive text prompts. See demos of ARLON at http://aka.ms/arlon.","authors":["Zongyi Li","Shujie Hu","Shujie Liu","Long Zhou","Jeongsoo Choi","Lingwei Meng","Xun Guo","Jinyu Li","Hefei Ling","Furu Wei"],"url":"https://arxiv.org/abs/2410.20502"}
{"created":"2025-04-16","title":"Constant Approximation for Weighted Nash Social Welfare with Submodular Valuations","abstract":"We study the problem of assigning items to agents so as to maximize the \\emph{weighted} Nash Social Welfare (NSW) under submodular valuations. The best-known result for the problem is an $O(nw_{\\max})$-approximation due to Garg, Husic, Li, Vega, and Vondrak~\\cite{GHL23}, where $w_{\\max}$ is the maximum weight over all agents. Obtaining a constant approximation algorithm is an open problem in the field that has recently attracted considerable attention.","authors":["Yuda Feng","Yang Hu","Shi Li","Ruilong Zhang"],"url":"https://arxiv.org/abs/2411.02942"}
{"created":"2025-04-16","title":"MonoRollBot: 3-DOF Spherical Robot with Underactuated Single Compliant Actuator Design","abstract":"Spherical rolling robots have garnered significant attention in the field of mobile robotics for applications such as inspection and space exploration. Designing underactuated rolling robots poses challenges in achieving multi-directional propulsion with high degrees of freedom while utilizing a limited number of actuators. This paper presents the MonoRollBot, a novel 3-degree-of-freedom (DOF) spherical robot that utilizes an underactuated mechanism driven by only a single spring-motor system. Unlike conventional spherical robots, MonoRollBot employs a minimalist actuation approach, relying on only one motor and a passive spring to control its locomotion. The robot achieves 3-DOF motion through an innovative coupling of spring dynamics and motor control. In this work, we detail the design of the MonoRollBot and evaluate its motion capabilities through design studies. We also do studies on its locomotion behaviours based on changes in rotating mass and stiffness properties.","authors":["Zhiwei Liu","Seyed Amir Tafrishi"],"url":"https://arxiv.org/abs/2411.04264"}
{"created":"2025-04-16","title":"Lightning IR: Straightforward Fine-tuning and Inference of Transformer-based Language Models for Information Retrieval","abstract":"A wide range of transformer-based language models have been proposed for information retrieval tasks. However, including transformer-based models in retrieval pipelines is often complex and requires substantial engineering effort. In this paper, we introduce Lightning IR, an easy-to-use PyTorch Lightning-based framework for applying transformer-based language models in retrieval scenarios. Lightning IR provides a modular and extensible architecture that supports all stages of a retrieval pipeline: from fine-tuning and indexing to searching and re-ranking. Designed to be scalable and reproducible, Lightning IR is available as open-source: https://github.com/webis-de/lightning-ir.","authors":["Ferdinand Schlatt","Maik Fr\\\"obe","Matthias Hagen"],"url":"https://arxiv.org/abs/2411.04677"}
{"created":"2025-04-16","title":"Snippet-based Conversational Recommender System","abstract":"Conversational Recommender Systems (CRS) engage users in interactive dialogues to gather preferences and provide personalized recommendations. While existing studies have advanced conversational strategies, they often rely on predefined attributes or expensive, domain-specific annotated datasets, which limits their flexibility in handling diverse user preferences and adaptability across domains. We propose SnipRec, a novel resource-efficient approach that leverages user-generated content, such as customer reviews, to capture a broader range of user expressions. By employing large language models to map reviews and user responses into concise snippets, SnipRec represents user preferences and retrieves relevant items without the need for intensive manual data collection or fine-tuning. Experiments across the restaurant, book, and clothing domains show that snippet-based representations outperform document- and sentence-based representations, achieving Hits@10 of 0.25-0.55 with 3,000 to 10,000 candidate items while successfully handling free-form user responses.","authors":["Haibo Sun","Naoki Otani","Hannah Kim","Dan Zhang","Nikita Bhutani"],"url":"https://arxiv.org/abs/2411.06064"}
{"created":"2025-04-16","title":"Re-anchoring Quantum Monte Carlo with Tensor-Train Sketching","abstract":"We propose a novel algorithm for calculating the ground-state energy of quantum many-body systems by combining auxiliary-field quantum Monte Carlo (AFQMC) with tensor-train sketching. In AFQMC, a good trial wavefunction to guide the random walk is crucial for improving the sampling efficiency and controlling the sign problem. Our proposed method iterates between determining a new trial wavefunction in the form of a tensor train, derived from the current walkers, and using this updated trial wavefunction to anchor the next phase of AFQMC. Numerical results demonstrate that the algorithm is highly accurate for large spin systems. The overlap between the estimated trial wavefunction and the ground-state wavefunction also achieves high fidelity. We additionally provide a convergence analysis, highlighting how an effective trial wavefunction can reduce the variance in the AFQMC energy estimation. From a complementary perspective, our algorithm also extends the reach of tensor-train methods for studying quantum many-body systems.","authors":["Ziang Yu","Shiwei Zhang","Yuehaw Khoo"],"url":"https://arxiv.org/abs/2411.07194"}
{"created":"2025-04-16","title":"Short note on the mapping of heritage sites impacted by the 2024 floods in Valencia, Spain","abstract":"This short note presents preliminary findings on the impact of the October 2024 floods on cultural heritage sites in Valencia, Spain. Using publicly available data, we assess the extent of potential damage by overlaying flood maps with heritage site coordinates. We identify that 3.3% of heritage sites in the region have been potentially impacted, with churches and shrines (81), outdoor religious iconography (78), and historic irrigation features (45) being the most heavily affected. Our analysis utilizes data from OpenStreetMap and listings from the Generalitat Valenciana, suggesting that while OpenStreetMap's crowd-sourced data can provide useful estimates of the proportion of impacted sites, it may not be suitable for a detailed damage assessment. By sharing this data openly, we aim to contribute to international efforts in preserving cultural heritage after the disaster and provide a foundation for future assessments of heritage site vulnerability to climate-related events.","authors":["Josep Grau-Bove","Richard Higham","Scott Orr","Pakhee Kumar"],"url":"https://arxiv.org/abs/2411.08717"}
{"created":"2025-04-16","title":"A characterization of positive spanning sets with ties to strongly connected digraphs","abstract":"Positive spanning sets (PSSs) are families of vectors that span a given linear space through non-negative linear combinations. Despite certain classes of PSSs being well understood, a complete characterization of PSSs remains elusive. In this paper, we explore a relatively understudied relationship between positive spanning sets and strongly edge-connected digraphs, in that the former can be viewed as a generalization of the latter. We leverage this connection to define a decomposition structure for positive spanning sets inspired by the ear decomposition from digraph theory.","authors":["Denis Cornaz","S\\'ebastien Kerleau","Cl\\'ement W. Royer"],"url":"https://arxiv.org/abs/2411.08994"}
{"created":"2025-04-16","title":"Safe Text-to-Image Generation: Simply Sanitize the Prompt Embedding","abstract":"In recent years, text-to-image (T2I) generation models have made significant progress in generating high-quality images that align with text descriptions. However, these models also face the risk of unsafe generation, potentially producing harmful content that violates usage policies, such as explicit material. Existing safe generation methods typically focus on suppressing inappropriate content by erasing undesired concepts from visual representations, while neglecting to sanitize the textual representation. Although these methods help mitigate the risk of misuse to some extent, their robustness remains insufficient when dealing with adversarial attacks.","authors":["Huming Qiu","Guanxu Chen","Mi Zhang","Xiaohan Zhang","Xiaoyu You","Min Yang"],"url":"https://arxiv.org/abs/2411.10329"}
{"created":"2025-04-16","title":"Limitations of Automatic Relevance Assessments with Large Language Models for Fair and Reliable Retrieval Evaluation","abstract":"Offline evaluation of search systems depends on test collections. These benchmarks provide the researchers with a corpus of documents, topics and relevance judgements indicating which documents are relevant for each topic. While test collections are an integral part of Information Retrieval (IR) research, their creation involves significant efforts in manual annotation. Large language models (LLMs) are gaining much attention as tools for automatic relevance assessment. Recent research has shown that LLM-based assessments yield high systems ranking correlation with human-made judgements. These correlations are helpful in large-scale experiments but less informative if we want to focus on top-performing systems. Moreover, these correlations ignore whether and how LLM-based judgements impact the statistically significant differences among systems with respect to human assessments. In this work, we look at how LLM-generated judgements preserve ranking differences among top-performing systems and also how they preserve pairwise significance evaluation as human judgements. Our results show that LLM-based judgements are unfair at ranking top-performing systems. Moreover, we observe an exceedingly high rate of false positives regarding statistical differences. Our work represents a step forward in the evaluation of the reliability of using LLMs-based judgements for IR evaluation. We hope this will serve as a basis for other researchers to develop more reliable models for automatic relevance assessment.","authors":["David Otero","Javier Parapar","\\'Alvaro Barreiro"],"url":"https://arxiv.org/abs/2411.13212"}
{"created":"2025-04-16","title":"ICODE: Modeling Dynamical Systems with Extrinsic Input Information","abstract":"Learning models of dynamical systems with external inputs, which may be, for example, nonsmooth or piecewise, is crucial for studying complex phenomena and predicting future state evolution, which is essential for applications such as safety guarantees and decision-making. In this work, we introduce \\emph{Input Concomitant Neural ODEs (ICODEs)}, which incorporate precise real-time input information into the learning process of the models, rather than treating the inputs as hidden parameters to be learned. The sufficient conditions to ensure the model's contraction property are provided to guarantee that system trajectories of the trained model converge to a fixed point, regardless of initial conditions across different training processes. We validate our method through experiments on several representative real dynamics: Single-link robot, DC-to-DC converter, motion dynamics of a rigid body, Rabinovich-Fabrikant equation, Glycolytic-glycogenolytic pathway model, and heat conduction equation. The experimental results demonstrate that our proposed ICODEs efficiently learn the ground truth systems, achieving superior prediction performance under both typical and atypical inputs. This work offers a valuable class of neural ODE models for understanding physical systems with explicit external input information, with potentially promising applications in fields such as physics and robotics. Our code is available online at https://github.com/EEE-ai59/ICODE.git.","authors":["Zhaoyi Li","Wenjie Mei","Ke Yu","Yang Bai","Shihua Li"],"url":"https://arxiv.org/abs/2411.13914"}
{"created":"2025-04-16","title":"Tulu 3: Pushing Frontiers in Open Language Model Post-Training","abstract":"Language model post-training is applied to refine behaviors and unlock new skills across a wide range of recent language models, but open recipes for applying these techniques lag behind proprietary ones. The underlying training data and recipes for post-training are simultaneously the most important pieces of the puzzle and the portion with the least transparency. To bridge this gap, we introduce Tulu 3, a family of fully-open state-of-the-art post-trained models, alongside its data, code, and training recipes, serving as a comprehensive guide for modern post-training techniques. Tulu 3, which builds on Llama 3.1 base models, achieves results surpassing the instruct versions of Llama 3.1, Qwen 2.5, Mistral, and even closed models such as GPT-4o-mini and Claude 3.5-Haiku. The training algorithms for our models include supervised finetuning (SFT), Direct Preference Optimization (DPO), and a novel method we call Reinforcement Learning with Verifiable Rewards (RLVR). With Tulu 3, we introduce a multi-task evaluation scheme for post-training recipes with development and unseen evaluations, standard benchmark implementations, and substantial decontamination of existing open datasets on said benchmarks. We conclude with analysis and discussion of training methods that did not reliably improve performance.","authors":["Nathan Lambert","Jacob Morrison","Valentina Pyatkin","Shengyi Huang","Hamish Ivison","Faeze Brahman","Lester James V. Miranda","Alisa Liu","Nouha Dziri","Shane Lyu","Yuling Gu","Saumya Malik","Victoria Graf","Jena D. Hwang","Jiangjiang Yang","Ronan Le Bras","Oyvind Tafjord","Chris Wilhelm","Luca Soldaini","Noah A. Smith","Yizhong Wang","Pradeep Dasigi","Hannaneh Hajishirzi"],"url":"https://arxiv.org/abs/2411.15124"}
{"created":"2025-04-16","title":"IterIS: Iterative Inference-Solving Alignment for LoRA Merging","abstract":"Low-rank adaptations (LoRA) are widely used to fine-tune large models across various domains for specific downstream tasks. While task-specific LoRAs are often available, concerns about data privacy and intellectual property can restrict access to training data, limiting the acquisition of a multi-task model through gradient-based training. In response, LoRA merging presents an effective solution by combining multiple LoRAs into a unified adapter while maintaining data privacy. Prior works on LoRA merging primarily frame it as an optimization problem, yet these approaches face several limitations, including the rough assumption about input features utilized in optimization, massive sample requirements, and the unbalanced optimization objective. These limitations can significantly degrade performance. To address these, we propose a novel optimization-based method, named IterIS: 1) We formulate LoRA merging as an advanced optimization problem to mitigate the rough assumption. Additionally, we employ an iterative inference-solving framework in our algorithm. It can progressively refine the optimization objective for improved performance. 2) We introduce an efficient regularization term to reduce the need for massive sample requirements (requiring only 1-5% of the unlabeled samples compared to prior methods). 3) We utilize adaptive weights in the optimization objective to mitigate potential unbalances in LoRA merging process. Our method demonstrates significant improvements over multiple baselines and state-of-the-art methods in composing tasks for text-to-image diffusion, vision-language models, and large language models. Furthermore, our layer-wise algorithm can achieve convergence with minimal steps, ensuring efficiency in both memory and computation.","authors":["Hongxu Chen","Runshi Li","Bowei Zhu","Zhen Wang","Long Chen"],"url":"https://arxiv.org/abs/2411.15231"}
{"created":"2025-04-16","title":"Adversarial Prompt Distillation for Vision-Language Models","abstract":"Large pre-trained Vision-Language Models (VLMs) such as Contrastive Language-Image Pre-training (CLIP) have been shown to be susceptible to adversarial attacks, raising concerns about their deployment in safety-critical applications like autonomous driving and medical diagnosis. One promising approach for robustifying pre-trained VLMs is Adversarial Prompt Tuning (APT), which applies adversarial training during the process of prompt tuning. However, existing APT methods are mostly single-modal methods that design prompt(s) for only the visual or textual modality, limiting their effectiveness in either robustness or clean accuracy. In this work, we propose Adversarial Prompt Distillation (APD), a bimodal knowledge distillation framework that enhances APT by integrating it with multi-modal knowledge transfer. APD optimizes prompts for both visual and textual modalities while distilling knowledge from a clean pre-trained teacher CLIP model. Extensive experiments on multiple benchmark datasets demonstrate the superiority of our APD method over the current state-of-the-art APT methods in terms of both adversarial robustness and clean accuracy. The effectiveness of APD also validates the possibility of using a non-robust teacher to improve the generalization and robustness of fine-tuned VLMs.","authors":["Lin Luo","Xin Wang","Bojia Zi","Shihao Zhao","Xingjun Ma","Yu-Gang Jiang"],"url":"https://arxiv.org/abs/2411.15244"}
{"created":"2025-04-16","title":"OPMOS: Ordered Parallel Algorithm for Multi-Objective Shortest-Paths","abstract":"The Multi-Objective Shortest-Path (MOS) problem finds a set of Pareto-optimal solutions from a start node to a destination node in a multi-attribute graph. The literature explores multi-objective A*-style algorithmic approaches to solving the NP-hard MOS problem. These approaches use consistent heuristics to compute an exact set of solutions for the goal node. A generalized MOS algorithm maintains a \"frontier\" of partial paths at each node and performs ordered processing to ensure that Pareto-optimal paths are generated to reach the goal node. The algorithm becomes computationally intractable at a higher number of objectives due to a rapid increase in the search space for non-dominated paths and the significant increase in Pareto-optimal solutions. While prior works have focused on algorithmic methods to reduce the complexity, we tackle this challenge by exploiting parallelism to accelerate the MOS problem. The key insight is that MOS algorithms rely on the ordered execution of partial paths to maintain high work efficiency. The proposed parallel algorithm (OPMOS) unlocks ordered parallelism and efficiently exploits the concurrent execution of multiple paths in MOS. Experimental evaluation using the NVIDIA GH200 Superchip's 72-core Arm-based CPU shows the performance scaling potential of OPMOS on work efficiency and parallelism using a real-world application to ship routing.","authors":["Leo Gold","Adam Bienkowski","David Sidoti","Krishna Pattipati","Omer Khan"],"url":"https://arxiv.org/abs/2411.16667"}
{"created":"2025-04-16","title":"An AI-driven multimodal smart home platform for continuous monitoring and intelligent assistance in post-stroke patients","abstract":"At-home rehabilitation for post-stroke patients presents significant challenges, as continuous, personalized care is often limited outside clinical settings. Additionally, the absence of comprehensive solutions addressing diverse monitoring and assistance needs in home environments complicates recovery efforts. Here, we present a multimodal smart home platform designed for continuous, at-home rehabilitation of post-stroke patients, integrating wearable sensing, ambient monitoring, and adaptive automation. A plantar pressure insole equipped with a machine learning pipeline classifies users into motor recovery stages with up to 94% accuracy, enabling quantitative tracking of walking patterns. A head-mounted eye-tracking module supports cognitive assessments and hands-free control of household devices, while ambient sensors ensure sub-second response times for interaction. These data streams are fused locally via a hierarchical Internet of Things (IoT) architecture, protecting privacy and minimizing latency. An embedded large language model (LLM) agent, Auto-Care, continuously interprets multimodal data to provide real-time interventions-issuing personalized reminders, adjusting environmental conditions, and notifying caregivers. Implemented in a post-stroke context, this integrated smart home platform increases overall user satisfaction by an average of 115% (p<0.01) compared to traditional home environment. Beyond stroke, the system offers a scalable framework for patient-centered, long-term care in broader neurorehabilitation and aging-in-place applications.","authors":["Chenyu Tang","Ruizhi Zhang","Shuo Gao","Zihe Zhao","Zibo Zhang","Jiaqi Wang","Cong Li","Junliang Chen","Yanning Dai","Shengbo Wang","Ruoyu Juan","Qiaoying Li","Ruimou Xie","Xuhang Chen","Xinkai Zhou","Yunjia Xia","Jianan Chen","Fanghao Lu","Xin Li","Ninglli Wang","Peter Smielewski","Yu Pan","Hubin Zhao","Luigi G. Occhipinti"],"url":"https://arxiv.org/abs/2411.19000"}
{"created":"2025-04-16","title":"InstanceGaussian: Appearance-Semantic Joint Gaussian Representation for 3D Instance-Level Perception","abstract":"3D scene understanding has become an essential area of research with applications in autonomous driving, robotics, and augmented reality. Recently, 3D Gaussian Splatting (3DGS) has emerged as a powerful approach, combining explicit modeling with neural adaptability to provide efficient and detailed scene representations. However, three major challenges remain in leveraging 3DGS for scene understanding: 1) an imbalance between appearance and semantics, where dense Gaussian usage for fine-grained texture modeling does not align with the minimal requirements for semantic attributes; 2) inconsistencies between appearance and semantics, as purely appearance-based Gaussians often misrepresent object boundaries; and 3) reliance on top-down instance segmentation methods, which struggle with uneven category distributions, leading to over- or under-segmentation. In this work, we propose InstanceGaussian, a method that jointly learns appearance and semantic features while adaptively aggregating instances. Our contributions include: i) a novel Semantic-Scaffold-GS representation balancing appearance and semantics to improve feature representations and boundary delineation; ii) a progressive appearance-semantic joint training strategy to enhance stability and segmentation accuracy; and iii) a bottom-up, category-agnostic instance aggregation approach that addresses segmentation challenges through farthest point sampling and connected component analysis. Our approach achieves state-of-the-art performance in category-agnostic, open-vocabulary 3D point-level segmentation, highlighting the effectiveness of the proposed representation and training strategies. Project page: https://lhj-git.github.io/InstanceGaussian/","authors":["Haijie Li","Yanmin Wu","Jiarui Meng","Qiankun Gao","Zhiyao Zhang","Ronggang Wang","Jian Zhang"],"url":"https://arxiv.org/abs/2411.19235"}
{"created":"2025-04-16","title":"SAT-HMR: Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive Tokens","abstract":"We propose a one-stage framework for real-time multi-person 3D human mesh estimation from a single RGB image. While current one-stage methods, which follow a DETR-style pipeline, achieve state-of-the-art (SOTA) performance with high-resolution inputs, we observe that this particularly benefits the estimation of individuals in smaller scales of the image (e.g., those far from the camera), but at the cost of significantly increased computation overhead. To address this, we introduce scale-adaptive tokens that are dynamically adjusted based on the relative scale of each individual in the image within the DETR framework. Specifically, individuals in smaller scales are processed at higher resolutions, larger ones at lower resolutions, and background regions are further distilled. These scale-adaptive tokens more efficiently encode the image features, facilitating subsequent decoding to regress the human mesh, while allowing the model to allocate computational resources more effectively and focus on more challenging cases. Experiments show that our method preserves the accuracy benefits of high-resolution processing while substantially reducing computational cost, achieving real-time inference with performance comparable to SOTA methods.","authors":["Chi Su","Xiaoxuan Ma","Jiajun Su","Yizhou Wang"],"url":"https://arxiv.org/abs/2411.19824"}
{"created":"2025-04-16","title":"Resilience Against Soft Faults through Adaptivity in Spectral Deferred Correction","abstract":"As supercomputers grow in hardware complexity, their susceptibility to faults increases and measures need to be taken to ensure the correctness of results. Some numerical algorithms have certain characteristics that allow them to recover from some types of faults. It has been demonstrated that adaptive Runge-Kutta methods provide resilience against transient faults without adding computational cost. Using recent advances in adaptive step size selection for spectral deferred correction (SDC), an iterative numerical time stepping scheme that can produce methods of arbitrary order, we show that adaptive SDC can also detect and correct transient faults. Its performance is found to be comparable to that of the dedicated resilience strategy Hot Rod.","authors":["Thomas Baumann","Sebastian G\\\"otschel","Thibaut Lunet","Daniel Ruprecht","Robert Speck"],"url":"https://arxiv.org/abs/2412.00529"}
{"created":"2025-04-16","title":"Using Cooperative Co-evolutionary Search to Generate Metamorphic Test Cases for Autonomous Driving Systems","abstract":"Autonomous Driving Systems (ADSs) rely on Deep Neural Networks, allowing vehicles to navigate complex, open environments. However, the unpredictability of these scenarios highlights the need for rigorous system-level testing to ensure safety, a task usually performed with a simulator in the loop. Though one important goal of such testing is to detect safety violations, there are many undesirable system behaviors, that may not immediately lead to violations, that testing should also be focusing on, thus detecting more subtle problems and enabling a finer-grained analysis. This paper introduces Cooperative Co-evolutionary MEtamorphic test Generator for Autonomous systems (CoCoMEGA), a novel automated testing framework aimed at advancing system-level safety assessments of ADSs. CoCoMEGA combines Metamorphic Testing (MT) with a search-based approach utilizing Cooperative Co-Evolutionary Algorithms (CCEA) to efficiently generate a diverse set of test cases. CoCoMEGA emphasizes the identification of test scenarios that present undesirable system behavior, that may eventually lead to safety violations, captured by Metamorphic Relations (MRs). When evaluated within the CARLA simulation environment on the Interfuser ADS, CoCoMEGA consistently outperforms baseline methods, demonstrating enhanced effectiveness and efficiency in generating severe, diverse MR violations and achieving broader exploration of the test space. These results underscore CoCoMEGA as a promising, more scalable solution to the inherent challenges in ADS testing with a simulator in the loop. Future research directions may include extending the approach to additional simulation platforms, applying it to other complex systems, and exploring methods for further improving testing efficiency such as surrogate modeling.","authors":["Hossein Yousefizadeh","Shenghui Gu","Lionel C. Briand","Ali Nasr"],"url":"https://arxiv.org/abs/2412.03843"}
{"created":"2025-04-16","title":"The Complexity of Tullock Contests","abstract":"This paper studies the algorithmic complexity for computing the Nash equilibrium in the extensively studied Tullock Contest game. The (possibly heterogeneous) elasticity parameter $r_i$ determines whether a contestant $i$'s cost function is convex or concave. Our core conceptual insight is that the domains of $r_i$ governs the complexity for solving Tullock contents. This is illustrated by our following complete set of algorithmic results.","authors":["Yu He","Fan Yao","Yang Yu","Xiaoyun Qiu","Minming Li","Haifeng Xu"],"url":"https://arxiv.org/abs/2412.06444"}
{"created":"2025-04-16","title":"Challenges and Opportunities for Visual Analytics in Jurisprudence","abstract":"Legal exploration, analysis, and interpretation remain complex and demanding tasks, even for experienced legal scholars, due to the domain-specific language, tacit legal concepts, and intentional ambiguities embedded in legal texts. In related, text-based domains, Visual Analytics (VA) and Large Language Models (LLMs) have become indispensable tools for navigating documents, representing knowledge, and supporting analytical reasoning. However, legal scholarship presents distinct challenges: it requires managing formal legal structure, drawing on tacit domain knowledge, and documenting intricate and accurate reasoning processes - needs that current VA systems designs and LLMs fail to address adequately. We identify previously unexamined key challenges and underexplored opportunities in applying VA to jurisprudence to explore how these technologies might better serve the legal domain. Based on semi-structured interviews with nine legal experts, we find a significant gap in tools and means that can externalize tacit legal knowledge in a form that is both explicit and machine-interpretable. Hence, we propose leveraging interactive visualization for this articulation, teaching the machine relevant semantic relationships between legal documents that inform the predictions of LLMs, facilitating the enhanced navigation between hierarchies of legal collections. This work introduces a user-centered VA workflow to the jurisprudential context, recognizing tacit legal knowledge and expert experience as vital components in deriving legal insight, comparing it with established practices in other text-based domains, and outlining a research agenda that offers future guidance for researchers in Visual Analytics for law and beyond.","authors":["Daniel F\\\"urst","Mennatallah El-Assady","Daniel A. Keim","Maximilian T. Fischer"],"url":"https://arxiv.org/abs/2412.06543"}
{"created":"2025-04-16","title":"Towards Predictive Communication with Brain-Computer Interfaces integrating Large Language Models","abstract":"This perspective article aims at providing an outline of the state of the art and future developments towards the integration of cutting-edge predictive language models with BCI. A synthetic overview of early and more recent linguistic models, from natural language processing (NLP) models to recent LLM, that to a varying extent improved predictive writing systems, is first provided. Second, a summary of previous BCI implementations integrating language models is presented. The few preliminary studies investigating the possible combination of LLM with BCI spellers to efficiently support fast communication and control are then described. Finally, current challenges and limitations towards the full integration of LLM with BCI systems are discussed. Recent investigations suggest that the combination of LLM with BCI might drastically improve human-computer interaction in patients with motor or language disorders as well as in healthy individuals. In particular, the pretrained autoregressive transformer models, such as GPT, that capitalize from parallelization, learning through pre-training and fine-tuning, promise a substantial improvement of BCI for communication with respect to previous systems incorporating simpler language models. Indeed, among various models, the GPT-2 was shown to represent an excellent candidate for its integration into BCI although testing was only perfomed on simulated conversations and not on real BCI scenarios. Prospectively, the full integration of LLM with advanced BCI systems might lead to a big leap forward towards fast, efficient and user-adaptive neurotechnology.","authors":["Andrea Caria"],"url":"https://arxiv.org/abs/2412.07355"}
{"created":"2025-04-16","title":"Causal Graphical Models for Vision-Language Compositional Understanding","abstract":"Recent work has empirically shown that Vision-Language Models (VLMs) struggle to fully understand the compositional properties of the human language, usually modeling an image caption as a \"bag of words\". As a result, they perform poorly on compositional tasks, which require a deeper understanding of the different entities of a sentence (subject, verb, etc.) jointly with their mutual relationships in order to be solved. In this paper, we model the dependency relations among textual and visual tokens using a Causal Graphical Model (CGM), built using a dependency parser, and we train a decoder conditioned by the VLM visual encoder. Differently from standard autoregressive or parallel predictions, our decoder's generative process is partially-ordered following the CGM structure. This structure encourages the decoder to learn only the main causal dependencies in a sentence discarding spurious correlations. Using extensive experiments on five compositional benchmarks, we show that our method significantly outperforms all the state-of-the-art compositional approaches by a large margin, and it also improves over methods trained using much larger datasets.","authors":["Fiorenzo Parascandolo","Nicholas Moratelli","Enver Sangineto","Lorenzo Baraldi","Rita Cucchiara"],"url":"https://arxiv.org/abs/2412.09353"}
{"created":"2025-04-16","title":"Constraint-Aware Zero-Shot Vision-Language Navigation in Continuous Environments","abstract":"We address the task of Vision-Language Navigation in Continuous Environments (VLN-CE) under the zero-shot setting. Zero-shot VLN-CE is particularly challenging due to the absence of expert demonstrations for training and minimal environment structural prior to guide navigation. To confront these challenges, we propose a Constraint-Aware Navigator (CA-Nav), which reframes zero-shot VLN-CE as a sequential, constraint-aware sub-instruction completion process. CA-Nav continuously translates sub-instructions into navigation plans using two core modules: the Constraint-Aware Sub-instruction Manager (CSM) and the Constraint-Aware Value Mapper (CVM). CSM defines the completion criteria for decomposed sub-instructions as constraints and tracks navigation progress by switching sub-instructions in a constraint-aware manner. CVM, guided by CSM's constraints, generates a value map on the fly and refines it using superpixel clustering to improve navigation stability. CA-Nav achieves the state-of-the-art performance on two VLN-CE benchmarks, surpassing the previous best method by 12 percent and 13 percent in Success Rate on the validation unseen splits of R2R-CE and RxR-CE, respectively. Moreover, CA-Nav demonstrates its effectiveness in real-world robot deployments across various indoor scenes and instructions.","authors":["Kehan Chen","Dong An","Yan Huang","Rongtao Xu","Yifei Su","Yonggen Ling","Ian Reid","Liang Wang"],"url":"https://arxiv.org/abs/2412.10137"}
{"created":"2025-04-16","title":"Who's the (Multi-)Fairest of Them All: Rethinking Interpolation-Based Data Augmentation Through the Lens of Multicalibration","abstract":"Data augmentation methods, especially SoTA interpolation-based methods such as Fair Mixup, have been widely shown to increase model fairness. However, this fairness is evaluated on metrics that do not capture model uncertainty and on datasets with only one, relatively large, minority group. As a remedy, multicalibration has been introduced to measure fairness while accommodating uncertainty and accounting for multiple minority groups. However, existing methods of improving multicalibration involve reducing initial training data to create a holdout set for post-processing, which is not ideal when minority training data is already sparse. This paper uses multicalibration to more rigorously examine data augmentation for classification fairness. We stress-test four versions of Fair Mixup on two structured data classification problems with up to 81 marginalized groups, evaluating multicalibration violations and balanced accuracy. We find that on nearly every experiment, Fair Mixup \\textit{worsens} baseline performance and fairness, but the simple vanilla Mixup \\textit{outperforms} both Fair Mixup and the baseline, especially when calibrating on small groups. \\textit{Combining} vanilla Mixup with multicalibration post-processing, which enforces multicalibration through post-processing on a holdout set, further increases fairness.","authors":["Karina Halevy","Karly Hou","Charumathi Badrinath"],"url":"https://arxiv.org/abs/2412.10575"}
{"created":"2025-04-16","title":"Facial Surgery Preview Based on the Orthognathic Treatment Prediction","abstract":"Orthognathic surgery consultation is essential to help patients understand the changes to their facial appearance after surgery. However, current visualization methods are often inefficient and inaccurate due to limited pre- and post-treatment data and the complexity of the treatment. To overcome these challenges, this study aims to develop a fully automated pipeline that generates accurate and efficient 3D previews of postsurgical facial appearances for patients with orthognathic treatment without requiring additional medical images. The study introduces novel aesthetic losses, such as mouth-convexity and asymmetry losses, to improve the accuracy of facial surgery prediction. Additionally, it proposes a specialized parametric model for 3D reconstruction of the patient, medical-related losses to guide latent code prediction network optimization, and a data augmentation scheme to address insufficient data. The study additionally employs FLAME, a parametric model, to enhance the quality of facial appearance previews by extracting facial latent codes and establishing dense correspondences between pre- and post-surgery geometries. Quantitative comparisons showed the algorithm's effectiveness, and qualitative results highlighted accurate facial contour and detail predictions. A user study confirmed that doctors and the public could not distinguish between machine learning predictions and actual postoperative results. This study aims to offer a practical, effective solution for orthognathic surgery consultations, benefiting doctors and patients.","authors":["Huijun Han","Congyi Zhang","Lifeng Zhu","Pradeep Singh","Richard Tai Chiu Hsung","Yiu Yan Leung","Taku Komura","Wenping Wang","Min Gu"],"url":"https://arxiv.org/abs/2412.11045"}
{"created":"2025-04-16","title":"Establishing a Foundation for Tetun Ad-Hoc Text Retrieval: Stemming, Indexing, Retrieval, and Ranking","abstract":"Searching for information on the internet and digital platforms to satisfy an information need requires effective retrieval solutions. However, such solutions are not yet available for Tetun, making it challenging to find relevant documents for text-based search queries in this language. To address these challenges, this study investigates Tetun text retrieval with a focus on the ad-hoc retrieval task. It begins by developing essential language resources -- including a list of stopwords, a stemmer, and a test collection -- which serve as foundational components for solutions tailored to Tetun text retrieval. Various strategies are then explored using both document titles and content to evaluate retrieval effectiveness. The results show that retrieving document titles, after removing hyphens and apostrophes without applying stemming, significantly improves retrieval performance compared to the baseline. Efficiency increases by 31.37%, while effectiveness achieves an average gain of 9.40% in MAP@10 and 30.35% in nDCG@10 with DFR BM25. Beyond the top-10 cutoff point, Hiemstra LM demonstrates strong performance across various retrieval strategies and evaluation metrics. Contributions of this work include the development of Labadain-Stopwords (a list of 160 Tetun stopwords), Labadain-Stemmer (a Tetun stemmer with three variants), and Labadain-Avaliad\\'or (a Tetun test collection containing 59 topics, 33,550 documents, and 5,900 qrels).","authors":["Gabriel de Jesus","S\\'ergio Nunes"],"url":"https://arxiv.org/abs/2412.11758"}
{"created":"2025-04-16","title":"Automatic Item Generation for Personality Situational Judgment Tests with Large Language Models","abstract":"Personality assessment, particularly through situational judgment tests (SJTs), is a vital tool for psychological research, talent selection, and educational evaluation. This study explores the potential of GPT-4, a state-of-the-art large language model (LLM), to automate the generation of personality situational judgment tests (PSJTs) in Chinese. Traditional SJT development is labor-intensive and prone to biases, while GPT-4 offers a scalable, efficient alternative. Two studies were conducted: Study 1 evaluated the impact of prompt design and temperature settings on content validity, finding that optimized prompts with a temperature of 1.0 produced creative and accurate items. Study 2 assessed the psychometric properties of GPT-4-generated PSJTs, revealing that they demonstrated satisfactory reliability and validity, surpassing the performance of manually developed tests in measuring the Big Five personality traits. This research highlights GPT-4's effectiveness in developing high-quality PSJTs, providing a scalable and innovative method for psychometric test development. These findings expand the possibilities of automatic item generation and the application of LLMs in psychology, and offer practical implications for streamlining test development processes in resource-limited settings.","authors":["Chang-Jin Li","Jiyuan Zhang","Yun Tang","Jian Li"],"url":"https://arxiv.org/abs/2412.12144"}
{"created":"2025-04-16","title":"Prototypical Calibrating Ambiguous Samples for Micro-Action Recognition","abstract":"Micro-Action Recognition (MAR) has gained increasing attention due to its crucial role as a form of non-verbal communication in social interactions, with promising potential for applications in human communication and emotion analysis. However, current approaches often overlook the inherent ambiguity in micro-actions, which arises from the wide category range and subtle visual differences between categories. This oversight hampers the accuracy of micro-action recognition. In this paper, we propose a novel Prototypical Calibrating Ambiguous Network (PCAN) to unleash and mitigate the ambiguity of MAR. Firstly, we employ a hierarchical action-tree to identify the ambiguous sample, categorizing them into distinct sets of ambiguous samples of false negatives and false positives, considering both body- and action-level categories. Secondly, we implement an ambiguous contrastive refinement module to calibrate these ambiguous samples by regulating the distance between ambiguous samples and their corresponding prototypes. This calibration process aims to pull false negative (FN) samples closer to their respective prototypes and push false positive (FP) samples apart from their affiliated prototypes. In addition, we propose a new prototypical diversity amplification loss to strengthen the model's capacity by amplifying the differences between different prototypes. Finally, we propose a prototype-guided rectification to rectify prediction by incorporating the representability of prototypes. Extensive experiments conducted on the benchmark dataset demonstrate the superior performance of our method compared to existing approaches. The code is available at https://github.com/kunli-cs/PCAN.","authors":["Kun Li","Dan Guo","Guoliang Chen","Chunxiao Fan","Jingyuan Xu","Zhiliang Wu","Hehe Fan","Meng Wang"],"url":"https://arxiv.org/abs/2412.14719"}
{"created":"2025-04-16","title":"Fine-tuning Whisper on Low-Resource Languages for Real-World Applications","abstract":"This paper presents a new approach to fine-tuning OpenAI's Whisper model for low-resource languages by introducing a novel data generation method that converts sentence-level data into a long-form corpus, using Swiss German as a case study. Non-sentence-level data, which could improve the performance of long-form audio, is difficult to obtain and often restricted by copyright laws. Our method bridges this gap by transforming more accessible sentence-level data into a format that preserves the model's ability to handle long-form audio and perform segmentation without requiring non-sentence-level data. Our data generation process improves performance in several real-world applications and leads to the development of a new state-of-the-art speech-to-text (STT) model for Swiss German. We compare our model with a non-fine-tuned Whisper and our previous state-of-the-art Swiss German STT models, where our new model achieves higher BLEU scores. Our results also indicate that the proposed method is adaptable to other low-resource languages, supported by written guidance and code that allows the creation of fine-tuned Whisper models, which keep segmentation capabilities and allow the transcription of longer audio files using only sentence-level data with high quality.","authors":["Vincenzo Timmel","Claudio Paonessa","Reza Kakooee","Manfred Vogel","Daniel Perruchoud"],"url":"https://arxiv.org/abs/2412.15726"}
{"created":"2025-04-16","title":"Unveiling the Threat of Fraud Gangs to Graph Neural Networks: Multi-Target Graph Injection Attacks Against GNN-Based Fraud Detectors","abstract":"Graph neural networks (GNNs) have emerged as an effective tool for fraud detection, identifying fraudulent users, and uncovering malicious behaviors. However, attacks against GNN-based fraud detectors and their risks have rarely been studied, thereby leaving potential threats unaddressed. Recent findings suggest that frauds are increasingly organized as gangs or groups. In this work, we design attack scenarios where fraud gangs aim to make their fraud nodes misclassified as benign by camouflaging their illicit activities in collusion. Based on these scenarios, we study adversarial attacks against GNN-based fraud detectors by simulating attacks of fraud gangs in three real-world fraud cases: spam reviews, fake news, and medical insurance frauds. We define these attacks as multi-target graph injection attacks and propose MonTi, a transformer-based Multi-target one-Time graph injection attack model. MonTi simultaneously generates attributes and edges of all attack nodes with a transformer encoder, capturing interdependencies between attributes and edges more effectively than most existing graph injection attack methods that generate these elements sequentially. Additionally, MonTi adaptively allocates the degree budget for each attack node to explore diverse injection structures involving target, candidate, and attack nodes, unlike existing methods that fix the degree budget across all attack nodes. Experiments show that MonTi outperforms the state-of-the-art graph injection attack methods on five real-world graphs.","authors":["Jinhyeok Choi","Heehyeon Kim","Joyce Jiyoung Whang"],"url":"https://arxiv.org/abs/2412.18370"}
{"created":"2025-04-16","title":"Muse: A Multimodal Conversational Recommendation Dataset with Scenario-Grounded User Profiles","abstract":"Current conversational recommendation systems focus predominantly on text. However, real-world recommendation settings are generally multimodal, causing a significant gap between existing research and practical applications. To address this issue, we propose Muse, the first multimodal conversational recommendation dataset. Muse comprises 83,148 utterances from 7,000 conversations centered around the Clothing domain. Each conversation contains comprehensive multimodal interactions, rich elements, and natural dialogues. Data in Muse are automatically synthesized by a multi-agent framework powered by multimodal large language models (MLLMs). It innovatively derives user profiles from real-world scenarios rather than depending on manual design and history data for better scalability, and then it fulfills conversation simulation and optimization. Both human and LLM evaluations demonstrate the high quality of conversations in Muse. Additionally, fine-tuning experiments on three MLLMs demonstrate Muse's learnable patterns for recommendations and responses, confirming its value for multimodal conversational recommendation. Our dataset and codes are available at https://anonymous.4open.science/r/Muse-0086.","authors":["Zihan Wang","Xiaocui Yang","Yongkang Liu","Shi Feng","Daling Wang","Yifei Zhang"],"url":"https://arxiv.org/abs/2412.18416"}
{"created":"2025-04-16","title":"Toward Intelligent and Secure Cloud: Large Language Model Empowered Proactive Defense","abstract":"The rapid evolution of cloud computing technologies and the increasing number of cloud applications have provided a large number of benefits in daily lives. However, the diversity and complexity of different components pose a significant challenge to cloud security, especially when dealing with sophisticated and advanced cyberattacks. Recent advancements in generative foundation models (GFMs), particularly in the large language models (LLMs), offer promising solutions for security intelligence. By exploiting the powerful abilities in language understanding, data analysis, task inference, action planning, and code generation, we present LLM-PD, a novel proactive defense architecture that defeats various threats in a proactive manner. LLM-PD can efficiently make a decision through comprehensive data analysis and sequential reasoning, as well as dynamically creating and deploying actionable defense mechanisms on the target cloud. Furthermore, it can flexibly self-evolve based on experience learned from previous interactions and adapt to new attack scenarios without additional training. The experimental results demonstrate its remarkable ability in terms of defense effectiveness and efficiency, particularly highlighting an outstanding success rate when compared with other existing methods.","authors":["Yuyang Zhou","Guang Cheng","Kang Du","Zihan Chen","Yuyu Zhao"],"url":"https://arxiv.org/abs/2412.21051"}
{"created":"2025-04-16","title":"Performant Automatic BLAS Offloading on Unified Memory Architecture with OpenMP First-Touch Style Data Movement","abstract":"BLAS is a fundamental building block of advanced linear algebra libraries and many modern scientific computing applications. GPUs are known for their strong arithmetic computing capabilities and are highly suited for BLAS operations. However, porting code to GPUs often requires significant effort, especially for large, complex codes or legacy codes, even for BLAS-heavy applications. While various tools exist to automatically offload BLAS to GPUs, they are often impractical due to the high costs associated with mandatory data transfers. The advent of unified memory architectures in recent GPU designs, such as the NVIDIA Grace-Hopper, allows cache-coherent memory access across all types of memory for both CPU and GPU, potentially eliminating the bottlenecks faced in conventional architectures. This breakthrough paves the way for innovative application developments and porting strategies. Building on our preliminary work demonstrating the potential of automatic *gemm offload, this paper extends the framework to all level-3 BLAS operations and introduces SCILIB-Accel, a novel tool for automatic BLAS offload. SCILIB-Accel leverages the memory coherency in Grace-Hopper and introduces a Device First-Use data movement policy inspired by the OpenMP First-Touch approach in multi-socket CPU programming, minimizing CPU-GPU data transfers for typical scientific computing codes. Additionally, utilizing dynamic binary instrumentation, the tool intercepts BLAS symbols directly from a CPU binary, requiring no code modifications or recompilation. SCILIB-Accel has been evaluated using multiple quantum physics codes on up to a few hundred GPU nodes, yielding promising speedups. Notably, for the LSMS method in the MuST suite, a 3x speedup was achieved on Grace-Hopper compared to Grace-Grace.","authors":["Junjie Li"],"url":"https://arxiv.org/abs/2501.00279"}
{"created":"2025-04-16","title":"RORem: Training a Robust Object Remover with Human-in-the-Loop","abstract":"Despite the significant advancements, existing object removal methods struggle with incomplete removal, incorrect content synthesis and blurry synthesized regions, resulting in low success rates. Such issues are mainly caused by the lack of high-quality paired training data, as well as the self-supervised training paradigm adopted in these methods, which forces the model to in-paint the masked regions, leading to ambiguity between synthesizing the masked objects and restoring the background. To address these issues, we propose a semi-supervised learning strategy with human-in-the-loop to create high-quality paired training data, aiming to train a Robust Object Remover (RORem). We first collect 60K training pairs from open-source datasets to train an initial object removal model for generating removal samples, and then utilize human feedback to select a set of high-quality object removal pairs, with which we train a discriminator to automate the following training data generation process. By iterating this process for several rounds, we finally obtain a substantial object removal dataset with over 200K pairs. Fine-tuning the pre-trained stable diffusion model with this dataset, we obtain our RORem, which demonstrates state-of-the-art object removal performance in terms of both reliability and image quality. Particularly, RORem improves the object removal success rate over previous methods by more than 18\\%. The dataset, source code and trained model are available at https://github.com/leeruibin/RORem.","authors":["Ruibin Li","Tao Yang","Song Guo","Lei Zhang"],"url":"https://arxiv.org/abs/2501.00740"}
{"created":"2025-04-16","title":"Differential Properties of Information in Jump-diffusion Channels","abstract":"We propose a channel modeling using jump-diffusion processes, and study the differential properties of entropy and mutual information. By utilizing the Kramers-Moyal and Kolmogorov-Feller equations, we express the mutual information between the input and the output in series and integral forms, presented by Fisher-type information and mismatched KL divergence. We extend de Bruijn's identity and the I-MMSE relation to encompass general Markov processes.","authors":["Luyao Fan","Jiayang Zou","Jiayang Gao","Jia Wang"],"url":"https://arxiv.org/abs/2501.05708"}
{"created":"2025-04-16","title":"A theoretical analysis on the resolution of parametric PDEs via Neural Networks designed with Strassen algorithm","abstract":"We construct a Neural Network that approximates the matrix multiplication operator for any activation function such that there exists a Neural Network which can approximate the scalar multiplication function. In particular, we use the Strassen algorithm to reduce the number of weights and layers needed for such Neural Networks. This allows us to define another Neural Network for approximating the inverse matrix operator. Also, by relying on the Galerkin method, we apply those Neural Networks to solve parametric elliptic PDEs for a whole set of parameters. Finally, we discuss improvements with respect to the prior results.","authors":["Gonzalo Romera","Jon Asier B\\'arcena-Petisco"],"url":"https://arxiv.org/abs/2501.06539"}
{"created":"2025-04-16","title":"Automated Retrosynthesis Planning of Macromolecules Using Large Language Models and Knowledge Graphs","abstract":"Identifying reliable synthesis pathways in materials chemistry is a complex task, particularly in polymer science, due to the intricate and often non-unique nomenclature of macromolecules. To address this challenge, we propose an agent system that integrates large language models (LLMs) and knowledge graphs. By leveraging LLMs' powerful capabilities for extracting and recognizing chemical substance names, and storing the extracted data in a structured knowledge graph, our system fully automates the retrieval of relevant literatures, extraction of reaction data, database querying, construction of retrosynthetic pathway trees, further expansion through the retrieval of additional literature and recommendation of optimal reaction pathways. By considering the complex interdependencies among chemical reactants, a novel Multi-branched Reaction Pathway Search Algorithm (MBRPS) is proposed to help identify all valid multi-branched reaction pathways, which arise when a single product decomposes into multiple reaction intermediates. In contrast, previous studies were limited to cases where a product decomposes into at most one reaction intermediate. This work represents the first attempt to develop a fully automated retrosynthesis planning agent tailored specially for macromolecules powered by LLMs. Applied to polyimide synthesis, our new approach constructs a retrosynthetic pathway tree with hundreds of pathways and recommends optimized routes, including both known and novel pathways. This demonstrates utilizing LLMs for literature consultation to accomplish specific tasks is possible and crucial for future materials research, given the vast amount of materials-related literature.","authors":["Qinyu Ma","Yuhao Zhou","Jianfeng Li"],"url":"https://arxiv.org/abs/2501.08897"}
{"created":"2025-04-16","title":"Non-Reversible Langevin Algorithms for Constrained Sampling","abstract":"We consider the constrained sampling problem where the goal is to sample from a target distribution on a constrained domain. We propose skew-reflected non-reversible Langevin dynamics (SRNLD), a continuous-time stochastic differential equation with skew-reflected boundary. We obtain non-asymptotic convergence rate of SRNLD to the target distribution in both total variation and 1-Wasserstein distances. By breaking reversibility, we show that the convergence is faster than the special case of the reversible dynamics. Based on the discretization of SRNLD, we propose skew-reflected non-reversible Langevin Monte Carlo (SRNLMC), and obtain non-asymptotic discretization error from SRNLD, and convergence guarantees to the target distribution in 1-Wasserstein distance. We show better performance guarantees than the projected Langevin Monte Carlo in the literature that is based on the reversible dynamics. Numerical experiments are provided for both synthetic and real datasets to show efficiency of the proposed algorithms.","authors":["Hengrong Du","Qi Feng","Changwei Tu","Xiaoyu Wang","Lingjiong Zhu"],"url":"https://arxiv.org/abs/2501.11743"}
{"created":"2025-04-16","title":"Reinforcement Learning Platform for Adversarial Black-box Attacks with Custom Distortion Filters","abstract":"We present a Reinforcement Learning Platform for Adversarial Black-box untargeted and targeted attacks, RLAB, that allows users to select from various distortion filters to create adversarial examples. The platform uses a Reinforcement Learning agent to add minimum distortion to input images while still causing misclassification by the target model. The agent uses a novel dual-action method to explore the input image at each step to identify sensitive regions for adding distortions while removing noises that have less impact on the target model. This dual action leads to faster and more efficient convergence of the attack. The platform can also be used to measure the robustness of image classification models against specific distortion types. Also, retraining the model with adversarial samples significantly improved robustness when evaluated on benchmark datasets. The proposed platform outperforms state-of-the-art methods in terms of the average number of queries required to cause misclassification. This advances trustworthiness with a positive social impact.","authors":["Soumyendu Sarkar","Ashwin Ramesh Babu","Sajad Mousavi","Vineet Gundecha","Sahand Ghorbanpour","Avisek Naug","Ricardo Luna Gutierrez","Antonio Guillen"],"url":"https://arxiv.org/abs/2501.14122"}
{"created":"2025-04-16","title":"Reddit Rules and Rulers: Quantifying the Link Between Rules and Perceptions of Governance across Thousands of Communities","abstract":"Rules are a critical component of the functioning of nearly every online community, yet it is challenging for community moderators to make data-driven decisions about what rules to set for their communities. The connection between a community's rules and how its membership feels about its governance is not well understood. In this work, we conduct the largest-to-date analysis of rules on Reddit, collecting a set of 67,545 unique rules across 5,225 communities which collectively account for more than 67% of all content on Reddit. More than just a point-in-time study, our work measures how communities change their rules over a 5+ year period. We develop a method to classify these rules using a taxonomy of 17 key attributes extended from previous work. We assess what types of rules are most prevalent, how rules are phrased, and how they vary across communities of different types. Using a dataset of communities' discussions about their governance, we are the first to identify the rules most strongly associated with positive community perceptions of governance: rules addressing who participates, how content is formatted and tagged, and rules about commercial activities. We conduct a longitudinal study to quantify the impact of adding new rules to communities, finding that after a rule is added, community perceptions of governance immediately improve, yet this effect diminishes after six months. Our results have important implications for platforms, moderators, and researchers. We make our classification model and rules datasets public to support future research on this topic.","authors":["Leon Leibmann","Galen Weld","Amy X. Zhang","Tim Althoff"],"url":"https://arxiv.org/abs/2501.14163"}
{"created":"2025-04-16","title":"Point Cloud Neural Operator for Parametric PDEs on Complex and Variable Geometries","abstract":"Surrogate models are critical for accelerating computationally expensive simulations in science and engineering, particularly for solving parametric partial differential equations (PDEs). Developing practical surrogate models poses significant challenges, particularly in handling geometrically complex and variable domains, which are often discretized as point clouds. In this work, we systematically investigate the formulation of neural operators -- maps between infinite-dimensional function spaces -- on point clouds to better handle complex and variable geometries while mitigating discretization effects. We introduce the Point Cloud Neural Operator (PCNO), designed to efficiently approximate solution maps of parametric PDEs on such domains. We evaluate the performance of PCNO on a range of pedagogical PDE problems, focusing on aspects such as boundary layers, adaptively meshed point clouds, and variable domains with topological variations. Its practicality is further demonstrated through three-dimensional applications, such as predicting pressure loads on various vehicle types and simulating the inflation process of intricate parachute structures.","authors":["Chenyu Zeng","Yanshu Zhang","Jiayi Zhou","Yuhan Wang","Zilin Wang","Yuhao Liu","Lei Wu","Daniel Zhengyu Huang"],"url":"https://arxiv.org/abs/2501.14475"}
{"created":"2025-04-16","title":"Breaking the Pre-Planning Barrier: Adaptive Real-Time Coordination of Heterogeneous UAVs","abstract":"Unmanned Aerial Vehicles (UAVs) offer significant potential in dynamic, perception-intensive tasks such as search and rescue and environmental monitoring; however, their effectiveness is severely restricted by conventional pre-planned routing methods, which lack the flexibility to respond in real-time to evolving task demands, unexpected disturbances, and localized view limitations in real-world scenarios. To address this fundamental limitation, we introduce a novel multi-agent reinforcement learning framework named \\textbf{H}eterogeneous \\textbf{G}raph \\textbf{A}ttention \\textbf{M}ulti-agent Deep Deterministic Policy Gradient (HGAM), uniquely designed to enable adaptive real-time coordination between mission UAVs (MUAVs) and charging UAVs (CUAVs). HGAM specifically addresses the previously unsolved challenge of enabling precise, decentralized continuous-action coordination solely based on local, heterogeneous graph-based observations. Extensive simulations demonstrate that HGAM substantially surpasses existing methods, achieving, for example, a 30\\% improvement in data collection coverage and a 20\\% increase in charging efficiency, providing crucial insights and foundations for the future deployment of intelligent, flexible UAV networks in complex, dynamic environments.","authors":["Yuhan Hu","Yirong Sun","Yanjun Chen","Xinghao Chen","Xiaoyu Shen","Wei Zhang"],"url":"https://arxiv.org/abs/2501.14488"}
{"created":"2025-04-16","title":"Information Age and Correctness for Energy Harvesting Devices with Random Access","abstract":"We investigate accuracy and freshness of status updates from a large number of energy-harvesting devices that monitor two-state Markov processes and access the medium using the slotted ALOHA protocol without feedback. Using a Markovian framework, we analyze the average value of a generic state-dependent penalty function that grows whenever there is a state estimation error. The age of incorrect information (AoII) is an example of such penalty function. We propose an accurate and easy-to-compute approximation for the average penalty. Numerical results demonstrate the benefits of optimizing the transmission probabilities according to the process state transitions and current battery levels to minimize the average penalty. The average-AoII-minimizing strategy can be highly suboptimal in terms of average penalty when one of the process states is critical, i.e., entails a high penalty if wrongly estimated. Furthermore, minimizing the average penalty does not guarantee a low probability of misdetecting a critical state period.","authors":["Khac-Hoang Ngo","Giuseppe Durisi","Petar Popovski"],"url":"https://arxiv.org/abs/2501.14522"}
{"created":"2025-04-16","title":"Faster Configuration Performance Bug Testing with Neural Dual-level Prioritization","abstract":"As software systems become more complex and configurable, more performance problems tend to arise from the configuration designs. This has caused some configuration options to unexpectedly degrade performance which deviates from their original expectations designed by the developers. Such discrepancies, namely configuration performance bugs (CPBugs), are devastating and can be deeply hidden in the source code. Yet, efficiently testing CPBugs is difficult, not only due to the test oracle is hard to set, but also because the configuration measurement is expensive and there are simply too many possible configurations to test. As such, existing testing tools suffer from lengthy runtime or have been ineffective in detecting CPBugs when the budget is limited, compounded by inaccurate test oracle. In this paper, we seek to achieve significantly faster CPBug testing by neurally prioritizing the testing at both the configuration option and value range levels with automated oracle estimation. Our proposed tool, dubbed NDP, is a general framework that works with different heuristic generators. The idea is to leverage two neural language models: one to estimate the CPBug types that serve as the oracle while, more vitally, the other to infer the probabilities of an option being CPBug-related, based on which the options and the value ranges to be searched can be prioritized. Experiments on several widely-used systems of different versions reveal that NDP can, in general, better predict CPBug type in 87% cases and find more CPBugs with up to 88.88x testing efficiency speedup over the state-of-the-art tools.","authors":["Youpeng Ma","Tao Chen","Ke Li"],"url":"https://arxiv.org/abs/2501.15392"}
{"created":"2025-04-16","title":"Which Optimizer Works Best for Physics-Informed Neural Networks and Kolmogorov-Arnold Networks?","abstract":"Physics-Informed Neural Networks (PINNs) have revolutionized the computation of PDE solutions by integrating partial differential equations (PDEs) into the neural network's training process as soft constraints, becoming an important component of the scientific machine learning (SciML) ecosystem. More recently, physics-informed Kolmogorv-Arnold networks (PIKANs) have also shown to be effective and comparable in accuracy with PINNs. In their current implementation, both PINNs and PIKANs are mainly optimized using first-order methods like Adam, as well as quasi-Newton methods such as BFGS and its low-memory variant, L-BFGS. However, these optimizers often struggle with highly non-linear and non-convex loss landscapes, leading to challenges such as slow convergence, local minima entrapment, and (non)degenerate saddle points. In this study, we investigate the performance of Self-Scaled BFGS (SSBFGS), Self-Scaled Broyden (SSBroyden) methods and other advanced quasi-Newton schemes, including BFGS and L-BFGS with different line search strategies approaches. These methods dynamically rescale updates based on historical gradient information, thus enhancing training efficiency and accuracy. We systematically compare these optimizers -- using both PINNs and PIKANs -- on key challenging linear, stiff, multi-scale and non-linear PDEs, including the Burgers, Allen-Cahn, Kuramoto-Sivashinsky, and Ginzburg-Landau equations. Our findings provide state-of-the-art results with orders-of-magnitude accuracy improvements without the use of adaptive weights or any other enhancements typically employed in PINNs. More broadly, our results reveal insights into the effectiveness of second-order optimization strategies in significantly improving the convergence and accurate generalization of PINNs and PIKANs.","authors":["Elham Kiyani","Khemraj Shukla","Jorge F. Urb\\'an","J\\'er\\^ome Darbon","George Em Karniadakis"],"url":"https://arxiv.org/abs/2501.16371"}
{"created":"2025-04-16","title":"Codd's Theorem for Databases over Semirings","abstract":"Codd's Theorem, a fundamental result of database theory, asserts that relational algebra and relational calculus have the same expressive power on relational databases. We explore Codd's Theorem for databases over semirings and establish two different versions of this result for such databases: the first version involves the five basic operations of relational algebra, while in the second version the division operation is added to the five basic operations of relational algebra. In both versions, the difference operation of relations is given semantics using semirings with monus, while on the side of relational calculus a limited form of negation is used. The reason for considering these two different versions of Codd's theorem is that, unlike the case of ordinary relational databases, the division operation need not be expressible in terms of the five basic operations of relational algebra for databases over an arbitrary positive semiring; in fact, we show that this inexpressibility result holds even for bag databases.","authors":["Guillermo Badia","Phokion G. Kolaitis","Carles Noguera"],"url":"https://arxiv.org/abs/2501.16543"}
{"created":"2025-04-16","title":"SHIELD: Secure Host-Independent Extensible Logging for Tamper-Proof Detection and Real-Time Mitigation of Ransomware Threats","abstract":"Ransomware's escalating sophistication necessitates tamper-resistant, off-host detection solutions that capture deep disk activity beyond the reach of a compromised operating system while overcoming evasion and obfuscation techniques. To address this, we introduce SHIELD: a metric acquisition framework leveraging low-level filesystem monitoring and Network Block Device (NBD) technology to provide off-host, tamper-proof measurements for continuous observation of disk activity exhibited by software executing on a target device. We employ Shield within a detection architecture leveraging deep filesystem features along with simplified metrics aggregated based on frequency of disk actions, making the metrics impervious to obfuscation while avoiding reliance on vulnerable host-based logs. We evaluate the efficacy of these metrics through extensive experiments with both binary (benign vs. malicious behavior) and multiclass (ransomware strain identification) classifiers and confirm that our metrics yield high accuracy across diverse threat profiles, including intermittent or partial encryption. In a proof-of-concept deployment, we demonstrate real-time mitigation using models trained on these metrics by halting malicious disk operations after ransomware detection with minimum file loss and memory corruption. We also show that hardware-only features collected independently of OS or network stack retain high detection effectiveness, verifying feasibility of embedding the proposed pipeline in a SATA controller ASIC or FPGA for next-generation, disk-centric defenses that combine filesystem insight with inherent off-host isolation.","authors":["Md Raz","P. V. Sai Charan","Prashanth Krishnamurthy","Farshad Khorrami","Ramesh Karri"],"url":"https://arxiv.org/abs/2501.16619"}
{"created":"2025-04-16","title":"ContourFormer: Real-Time Contour-Based End-to-End Instance Segmentation Transformer","abstract":"This paper presents Contourformer, a real-time contour-based instance segmentation algorithm. The method is fully based on the DETR paradigm and achieves end-to-end inference through iterative and progressive mechanisms to optimize contours. To improve efficiency and accuracy, we develop two novel techniques: sub-contour decoupling mechanisms and contour fine-grained distribution refinement. In the sub-contour decoupling mechanism, we propose a deformable attention-based module that adaptively selects sampling regions based on the current predicted contour, enabling more effective capturing of object boundary information. Additionally, we design a multi-stage optimization process to enhance segmentation precision by progressively refining sub-contours. The contour fine-grained distribution refinement technique aims to further improve the ability to express fine details of contours. These innovations enable Contourformer to achieve stable and precise segmentation for each instance while maintaining real-time performance. Extensive experiments demonstrate the superior performance of Contourformer on multiple benchmark datasets, including SBD, COCO, and KINS. We conduct comprehensive evaluations and comparisons with existing state-of-the-art methods, showing significant improvements in both accuracy and inference speed. This work provides a new solution for contour-based instance segmentation tasks and lays a foundation for future research, with the potential to become a strong baseline method in this field.","authors":["Weiwei Yao","Chen Li","Minjun Xiong","Wenbo Dong","Hao Chen","Xiong Xiao"],"url":"https://arxiv.org/abs/2501.17688"}
{"created":"2025-04-16","title":"Advanced Architectures Integrated with Agentic AI for Next-Generation Wireless Networks","abstract":"This paper investigates a range of cutting-edge technologies and architectural innovations aimed at simplifying network operations, reducing operational expenditure (OpEx), and enabling the deployment of new service models. The focus is on (i) Proposing novel, more efficient 6G architectures, with both Control and User planes enabling the seamless expansion of services, while addressing long-term 6G network evolution. (ii) Exploring advanced techniques for constrained artificial intelligence (AI) operations, particularly the design of AI agents for real-time learning, optimizing energy consumption, and the allocation of computational resources. (iii) Identifying technologies and architectures that support the orchestration of backend services using serverless computing models across multiple domains, particularly for vertical industries. (iv) Introducing optically-based, ultra-high-speed, low-latency network architectures, with fast optical switching and real-time control, replacing conventional electronic switching to reduce power consumption by an order of magnitude.","authors":["Kapal Dev","Sunder Ali Khowaja","Keshav Singh","Engin Zeydan","Merouane Debbah"],"url":"https://arxiv.org/abs/2502.01089"}
{"created":"2025-04-16","title":"SE Arena: An Interactive Platform for Evaluating Foundation Models in Software Engineering","abstract":"Foundation models (FMs), particularly large language models (LLMs), have shown significant promise in various software engineering (SE) tasks, including code generation, debugging, and requirement refinement. Despite these advances, existing evaluation frameworks are insufficient for assessing model performance in iterative, context-rich workflows characteristic of SE activities. To address this limitation, we introduce SE Arena, an interactive platform designed to evaluate SE-focused chatbots. SE Arena provides a transparent, open-source leaderboard, supports multi-round conversational workflows, and enables end-to-end model comparisons. The platform introduces novel metrics, including the consistency score that measures model consistency through self-play matches. Moreover, SE Arena incorporates a new feature called RepoChat, which automatically injects repository-related context (e.g., issues, commits, pull requests) into the conversation, further aligning evaluations with real-world development processes. This paper outlines the design and capabilities of SE Arena, emphasizing its potential to advance the evaluation and practical application of FMs in software engineering.","authors":["Zhimin Zhao"],"url":"https://arxiv.org/abs/2502.01860"}
{"created":"2025-04-16","title":"Mosaic3D: Foundation Dataset and Model for Open-Vocabulary 3D Segmentation","abstract":"We tackle open-vocabulary 3D scene understanding by introducing a novel data generation pipeline and training framework. Our method addresses three critical requirements for effective training: precise 3D region segmentation, comprehensive textual descriptions, and sufficient dataset scale. By leveraging state-of-the-art open-vocabulary image segmentation models and region-aware Vision-Language Models, we develop an automatic pipeline that generates high-quality 3D mask-text pairs. Applying this pipeline to multiple 3D scene datasets, we create Mosaic3D-5.6M, a dataset of over 30K annotated scenes with 5.6M mask-text pairs, significantly larger than existing datasets. Building upon this data, we propose Mosaic3D, a foundation model combining a 3D encoder trained with contrastive learning and a lightweight mask decoder for open-vocabulary 3D semantic and instance segmentation. Our approach achieves state-of-the-art results on open-vocabulary 3D semantic and instance segmentation tasks including ScanNet200, Matterport3D, and ScanNet++, with ablation studies validating the effectiveness of our large-scale training data.","authors":["Junha Lee","Chunghyun Park","Jaesung Choe","Yu-Chiang Frank Wang","Jan Kautz","Minsu Cho","Chris Choy"],"url":"https://arxiv.org/abs/2502.02548"}
{"created":"2025-04-16","title":"Kozax: Flexible and Scalable Genetic Programming in JAX","abstract":"Genetic programming is an optimization algorithm inspired by evolution which automatically evolves the structure of interpretable computer programs. The fitness evaluation in genetic programming suffers from high computational requirements, limiting the performance on difficult problems. Consequently, there is no efficient genetic programming framework that is usable for a wide range of tasks. To this end, we developed Kozax, a genetic programming framework that evolves symbolic expressions for arbitrary problems. We implemented Kozax using JAX, a framework for high-performance and scalable machine learning, which allows the fitness evaluation to scale efficiently to large populations or datasets on GPU. Furthermore, Kozax offers constant optimization, custom operator definition and simultaneous evolution of multiple trees. We demonstrate successful applications of Kozax to discover equations of natural laws, recover equations of hidden dynamic variables, evolve a control policy and optimize an objective function. Overall, Kozax provides a general, fast, and scalable library to optimize white-box solutions in the realm of scientific computing.","authors":["Sigur de Vries","Sander W. Keemink","Marcel A. J. van Gerven"],"url":"https://arxiv.org/abs/2502.03047"}
{"created":"2025-04-16","title":"A Critical Analysis of Deployed Use Cases for Quantum Key Distribution and Comparison with Post-Quantum Cryptography","abstract":"Quantum Key Distribution (QKD) is currently being discussed as a technology to safeguard communication in a future where quantum computers compromise traditional public-key cryptosystems. In this paper, we conduct a comprehensive security evaluation of QKD-based solutions, focusing on real-world use cases sourced from academic literature and industry reports. We analyze these use cases, assess their security and identify the possible advantages of deploying QKD-based solutions. We further compare QKD-based solutions with Post-Quantum Cryptography (PQC), the alternative approach to achieving security when quantum computers compromise traditional public-key cryptosystems, evaluating their respective suitability for each scenario. Based on this comparative analysis, we critically discuss and comment on which use cases QKD is suited for, considering factors such as implementation complexity, scalability, and long-term security. Our findings contribute to a better understanding of the role QKD could play in future cryptographic infrastructures and offer guidance to decision-makers considering the deployment of QKD.","authors":["Nick Aquina","Bruno Cimoli","Soumya Das","Kathrin H\\\"ovelmanns","Fiona Johanna Weber","Chigo Okonkwo","Simon Rommel","Boris \\v{S}kori\\'c","Idelfonso Tafur Monroy","Sebastian Verschoor"],"url":"https://arxiv.org/abs/2502.04009"}
{"created":"2025-04-16","title":"Interpretable Failure Detection with Human-Level Concepts","abstract":"Reliable failure detection holds paramount importance in safety-critical applications. Yet, neural networks are known to produce overconfident predictions for misclassified samples. As a result, it remains a problematic matter as existing confidence score functions rely on category-level signals, the logits, to detect failures. This research introduces an innovative strategy, leveraging human-level concepts for a dual purpose: to reliably detect when a model fails and to transparently interpret why. By integrating a nuanced array of signals for each category, our method enables a finer-grained assessment of the model's confidence. We present a simple yet highly effective approach based on the ordinal ranking of concept activation to the input image. Without bells and whistles, our method significantly reduce the false positive rate across diverse real-world image classification benchmarks, specifically by 3.7% on ImageNet and 9% on EuroSAT.","authors":["Kien X. Nguyen","Tang Li","Xi Peng"],"url":"https://arxiv.org/abs/2502.05275"}
{"created":"2025-04-16","title":"Explainable and Class-Revealing Signal Feature Extraction via Scattering Transform and Constrained Zeroth-Order Optimization","abstract":"We propose a new method to extract discriminant and explainable features from a particular machine learning model, i.e., a combination of the scattering transform and the multiclass logistic regression. Although this model is well-known for its ability to learn various signal classes with high classification rate, it remains elusive to understand why it can generate such successful classification, mainly due to the nonlinearity of the scattering transform. In order to uncover the meaning of the scattering transform coefficients selected by the multiclass logistic regression (with the Lasso penalty), we adopt zeroth-order optimization algorithms to search an input pattern that maximizes the class probability of a class of interest given the learned model. In order to do so, it turns out that imposing sparsity and smoothness of input patterns is important. We demonstrate the effectiveness of our proposed method using a couple of synthetic time-series classification problems.","authors":["Naoki Saito","David Weber"],"url":"https://arxiv.org/abs/2502.05722"}
{"created":"2025-04-16","title":"Rethinking Fine-Tuning when Scaling Test-Time Compute: Limiting Confidence Improves Mathematical Reasoning","abstract":"Recent progress in large language models (LLMs) highlights the power of scaling test-time compute to achieve strong performance on complex tasks, such as mathematical reasoning and code generation. This raises a critical question: how should model training be modified to optimize performance under a subsequent test-time compute strategy and budget? To explore this, we focus on pass@N, a simple test-time strategy that searches for a correct answer in $N$ independent samples. We show, surprisingly, that training with cross-entropy (CE) loss can be ${\\it misaligned}$ with pass@N in that pass@N accuracy ${\\it decreases}$ with longer training. We explain the origins of this misalignment in terms of model overconfidence induced by CE, and experimentally verify our prediction of overconfidence as an impediment to scaling test-time compute via pass@N. Furthermore we suggest a principled, modified training loss that is better aligned to pass@N by limiting model confidence and rescuing pass@N test performance. Our algorithm demonstrates improved mathematical reasoning on MATH and MiniF2F benchmarks under several scenarios: (1) providing answers to math questions; and (2) proving theorems by searching over proof trees of varying shapes. Overall our work underscores the importance of co-designing two traditionally separate phases of LLM development: training-time protocols and test-time search and reasoning strategies.","authors":["Feng Chen","Allan Raventos","Nan Cheng","Surya Ganguli","Shaul Druckmann"],"url":"https://arxiv.org/abs/2502.07154"}
{"created":"2025-04-16","title":"SS4Rec: Continuous-Time Sequential Recommendation with State Space Models","abstract":"Sequential recommendation is a key area in the field of recommendation systems aiming to model user interest based on historical interaction sequences with irregular intervals. While previous recurrent neural network-based and attention-based approaches have achieved significant results, they have limitations in capturing system continuity due to the discrete characteristics. In the context of continuous-time modeling, state space model (SSM) offers a potential solution, as it can effectively capture the dynamic evolution of user interest over time. However, existing SSM-based approaches ignore the impact of irregular time intervals within historical user interactions, making it difficult to model complexed user-item transitions in sequences. To address this issue, we propose a hybrid SSM-based model called SS4Rec for continuous-time sequential recommendation. SS4Rec integrates a time-aware SSM to handle irregular time intervals and a relation-aware SSM to model contextual dependencies, enabling it to infer user interest from both temporal and sequential perspectives. In the training process, the time-aware SSM and the relation-aware SSM are discretized by variable stepsizes according to user interaction time intervals and input data, respectively. This helps capture the continuous dependency from irregular time intervals and provides time-specific personalized recommendations. Experimental studies on five benchmark datasets demonstrate the superiority and effectiveness of SS4Rec.","authors":["Wei Xiao","Huiying Wang","Qifeng Zhou"],"url":"https://arxiv.org/abs/2502.08132"}
{"created":"2025-04-16","title":"MERGE$^3$: Efficient Evolutionary Merging on Consumer-grade GPUs","abstract":"Evolutionary model merging enables the creation of high-performing multi-task models but remains computationally prohibitive for consumer hardware. We introduce MERGE$^3$, an efficient framework that makes evolutionary merging feasible on a single GPU by reducing fitness computation costs 50$\\times$ while preserving performance. MERGE$^3$ achieves this by Extracting a reduced dataset for evaluation, Estimating model abilities using Item Response Theory (IRT), and Evolving optimal merges via IRT-based performance estimators. Our method enables state-of-the-art multilingual and cross-lingual merging, transferring knowledge across languages with significantly lower computational overhead. We provide theoretical guarantees and an open-source library, democratizing high-quality model merging.","authors":["Tommaso Mencattini","Adrian Robert Minut","Donato Crisostomi","Andrea Santilli","Emanuele Rodol\\`a"],"url":"https://arxiv.org/abs/2502.10436"}
{"created":"2025-04-16","title":"Evolving Hard Maximum Cut Instances for Quantum Approximate Optimization Algorithms","abstract":"Variational quantum algorithms, such as the Recursive Quantum Approximate Optimization Algorithm (RQAOA), have become increasingly popular, offering promising avenues for employing Noisy Intermediate-Scale Quantum devices to address challenging combinatorial optimization tasks like the maximum cut problem. In this study, we utilize an evolutionary algorithm equipped with a unique fitness function. This approach targets hard maximum cut instances within the latent space of a Graph Autoencoder, identifying those that pose significant challenges or are particularly tractable for RQAOA, in contrast to the classic Goemans and Williamson algorithm. Our findings not only delineate the distinct capabilities and limitations of each algorithm but also expand our understanding of RQAOA's operational limits. Furthermore, the diverse set of graphs we have generated serves as a crucial benchmarking asset, emphasizing the need for more advanced algorithms to tackle combinatorial optimization challenges. Additionally, our results pave the way for new avenues in graph generation research, offering exciting opportunities for future explorations.","authors":["Shuaiqun Pan","Yash J. Patel","Aneta Neumann","Frank Neumann","Thomas B\\\"ack","Hao Wang"],"url":"https://arxiv.org/abs/2502.12012"}
{"created":"2025-04-16","title":"What Is a Good Caption? A Comprehensive Visual Caption Benchmark for Evaluating Both Correctness and Thoroughness","abstract":"Visual captioning benchmarks have become outdated with the emergence of modern multimodal large language models (MLLMs), as the brief ground-truth sentences and traditional metrics fail to assess detailed captions effectively. While recent benchmarks attempt to address this by focusing on keyword extraction or object-centric evaluation, they remain limited to vague-view or object-view analyses and incomplete visual element coverage. In this paper, we introduce CAPability, a comprehensive multi-view benchmark for evaluating visual captioning across 12 dimensions spanning six critical views. We curate nearly 11K human-annotated images and videos with visual element annotations to evaluate the generated captions. CAPability stably assesses both the correctness and thoroughness of captions using F1-score. By converting annotations to QA pairs, we further introduce a heuristic metric, \\textit{know but cannot tell} ($K\\bar{T}$), indicating a significant performance gap between QA and caption capabilities. Our work provides the first holistic analysis of MLLMs' captioning abilities, as we identify their strengths and weaknesses across various dimensions, guiding future research to enhance specific aspects of capabilities.","authors":["Zhihang Liu","Chen-Wei Xie","Bin Wen","Feiwu Yu","Jixuan Chen","Boqiang Zhang","Nianzu Yang","Pandeng Li","Yinglu Li","Zuan Gao","Yun Zheng","Hongtao Xie"],"url":"https://arxiv.org/abs/2502.14914"}
{"created":"2025-04-16","title":"Verified Parameterized Choreographies Technical Report","abstract":"This technical report contains the full set of definitions and projection rules of the paper ``Verified Parameterized Choreographies'' by Rubbens et al. It also supplements the artefact.","authors":["Robert Rubbens","Petra van den Bos","Marieke Huisman"],"url":"https://arxiv.org/abs/2502.15382"}
{"created":"2025-04-16","title":"AI Mismatches: Identifying Potential Algorithmic Harms Before AI Development","abstract":"AI systems are often introduced with high expectations, yet many fail to deliver, resulting in unintended harm and missed opportunities for benefit. We frequently observe significant \"AI Mismatches\", where the system's actual performance falls short of what is needed to ensure safety and co-create value. These mismatches are particularly difficult to address once development is underway, highlighting the need for early-stage intervention. Navigating complex, multi-dimensional risk factors that contribute to AI Mismatches is a persistent challenge. To address it, we propose an AI Mismatch approach to anticipate and mitigate risks early on, focusing on the gap between realistic model performance and required task performance. Through an analysis of 774 AI cases, we extracted a set of critical factors, which informed the development of seven matrices that map the relationships between these factors and highlight high-risk areas. Through case studies, we demonstrate how our approach can help reduce risks in AI development.","authors":["Devansh Saxena","Ji-Youn Jung","Jodi Forlizzi","Kenneth Holstein","John Zimmerman"],"url":"https://arxiv.org/abs/2502.18682"}
{"created":"2025-04-16","title":"Trustworthy Answers, Messier Data: Bridging the Gap in Low-Resource Retrieval-Augmented Generation for Domain Expert Systems","abstract":"RAG has become a key technique for enhancing LLMs by reducing hallucinations, especially in domain expert systems where LLMs may lack sufficient inherent knowledge. However, developing these systems in low-resource settings introduces several challenges: (1) handling heterogeneous data sources, (2) optimizing retrieval phase for trustworthy answers, and (3) evaluating generated answers across diverse aspects. To address these, we introduce a data generation pipeline that transforms raw multi-modal data into structured corpus and Q&amp;A pairs, an advanced re-ranking phase improving retrieval precision, and a reference matching algorithm enhancing answer traceability. Applied to the automotive engineering domain, our system improves factual correctness (+1.94), informativeness (+1.16), and helpfulness (+1.67) over a non-RAG baseline, based on a 1-5 scale by an LLM judge. These results highlight the effectiveness of our approach across distinct aspects, with strong answer grounding and transparency.","authors":["Nayoung Choi","Grace Byun","Andrew Chung","Ellie S. Paek","Shinsun Lee","Jinho D. Choi"],"url":"https://arxiv.org/abs/2502.19596"}
{"created":"2025-04-16","title":"A Deep Learning Framework for Medium-Term Covariance Forecasting in Multi-Asset Portfolios","abstract":"Accurate covariance forecasting is central to portfolio allocation, risk management, and asset pricing, yet many existing methods struggle at medium-term horizons, where shifting market regimes and slower dynamics predominate. We propose a deep learning framework that combines three-dimensional convolutional neural networks, bidirectional long short-term memory layers, and multi-head attention to capture complex spatio-temporal dependencies. Using daily data on 14 exchange-traded funds from 2017 through 2023, we find that our model reduces Euclidean and Frobenius distance metrics by up to 20\\% relative to classical benchmarks (e.g., shrinkage and GARCH approaches) and remains robust across distinct market regimes. Our portfolio experiments demonstrate significant economic value through lower volatility and moderate turnover. These findings highlight the potential of advanced deep learning architectures to improve medium-term covariance forecasts, offering practical benefits for institutional investors and risk managers.","authors":["Pedro Reis","Ana Paula Serra","Jo\\~ao Gama"],"url":"https://arxiv.org/abs/2503.01581"}
{"created":"2025-04-16","title":"LangGas: Introducing Language in Selective Zero-Shot Background Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset","abstract":"Gas leakage poses a significant hazard that requires prevention. Traditionally, human inspection has been used for detection, a slow and labour-intensive process. Recent research has applied machine learning techniques to this problem, yet there remains a shortage of high-quality, publicly available datasets. This paper introduces a synthetic dataset, SimGas, featuring diverse backgrounds, interfering foreground objects, diverse leak locations, and precise segmentation ground truth. We propose a zero-shot method that combines background subtraction, zero-shot object detection, filtering, and segmentation to leverage this dataset. Experimental results indicate that our approach significantly outperforms baseline methods based solely on background subtraction and zero-shot object detection with segmentation, reaching an IoU of 69%. We also present an analysis of various prompt configurations and threshold settings to provide deeper insights into the performance of our method. Finally, we qualitatively (because of the lack of ground truth) tested our performance on GasVid and reached decent results on the real-world dataset. The dataset, code, and full qualitative results are available at https://github.com/weathon/Lang-Gas.","authors":["Wenqi Guo","Yiyang Du","Shan Du"],"url":"https://arxiv.org/abs/2503.02910"}
{"created":"2025-04-16","title":"O-RAN xApps Conflict Management using Graph Convolutional Networks","abstract":"The lack of a unified mechanism to coordinate and prioritize the actions of different applications can create three types of conflicts (direct, indirect, and implicit). Conflict management in O-RAN refers to the process of identifying and resolving conflicts between network applications. In our paper, we introduce a novel data-driven GCN-based method called GRAPH-based Intelligent xApp Conflict Prediction and Analysis (GRAPHICA) based on Graph Convolutional Network (GCN). It predicts three types of conflicts (direct, indirect, and implicit) and pinpoints the root causes (xApps). GRAPHICA captures the complex and hidden dependencies among the xApps, controlled parameters, and KPIs in O-RAN to predict possible conflicts. Then, it identifies the root causes (xApps) contributing to the predicted conflicts. The proposed method was tested on highly imbalanced synthesized datasets where conflict instances range from 40% to 10%. The model is tested in a setting that simulates real-world scenarios where conflicts are rare to assess its performance. Experimental results demonstrate a high F1-score over 98% for the synthesized datasets with different levels of class imbalance.","authors":["Maryam Al Shami","Jun Yan","Emmanuel Thepie Fapi"],"url":"https://arxiv.org/abs/2503.03523"}
{"created":"2025-04-16","title":"Fine-Tuning Florence2 for Enhanced Object Detection in Un-constructed Environments: Vision-Language Model Approach","abstract":"Vision-Language Models (VLMs) have emerged as powerful tools in artificial intelli-gence, capable of integrating textual and visual data for a unified understanding of complex scenes. While models such as Florence2, built on transformer architectures, have shown promise across general tasks, their performance in object detection within unstructured or cluttered environments remains underexplored. In this study, we fi-ne-tuned the Florence2 model for object detection tasks in non-constructed, complex environments. A comprehensive experimental framework was established involving multiple hardware configurations (NVIDIA T4, L4, and A100 GPUs), optimizers (AdamW, SGD), and varied hyperparameters including learning rates and LoRA (Low-Rank Adaptation) setups. Model training and evaluation were conducted on challenging datasets representative of real-world, disordered settings. The optimized Florence2 models exhibited significant improvements in object detection accuracy, with Mean Average Precision (mAP) metrics approaching or matching those of estab-lished models such as YOLOv8, YOLOv9, and YOLOv10. The integration of LoRA and careful fine-tuning of transformer layers contributed notably to these gains. Our find-ings highlight the adaptability of transformer-based VLMs like Florence2 for do-main-specific tasks, particularly in visually complex environments. The study under-scores the potential of fine-tuned VLMs to rival traditional convolution-based detec-tors, offering a flexible and scalable approach for advanced vision applications in re-al-world, unstructured settings.","authors":["Aysegul Ucar","Soumyadeep Ro","Sanapala Satwika","Pamarthi Yasoda Gayathri","Mohmmad Ghaith Balsha"],"url":"https://arxiv.org/abs/2503.04918"}
{"created":"2025-04-16","title":"Linear-MoE: Linear Sequence Modeling Meets Mixture-of-Experts","abstract":"Linear Sequence Modeling (LSM) like linear attention, state space models and linear RNNs, and Mixture-of-Experts (MoE) have recently emerged as significant architectural improvements. In this paper, we introduce Linear-MoE, a production-level system for modeling and training large-scale models that integrate LSM with MoE. Linear-MoE leverages the advantages of both LSM modules for linear-complexity sequence modeling and MoE layers for sparsely activation, aiming to offer high performance with efficient training. The Linear-MoE system comprises: 1) Modeling subsystem, which provides a unified framework supporting all instances of LSM. and 2) Training subsystem, which facilitates efficient training by incorporating various advanced parallelism technologies, particularly Sequence Parallelism designed for Linear-MoE models. Additionally, we explore hybrid models that combine Linear-MoE layers with standard Transformer-MoE layers with its Sequence Parallelism to further enhance model flexibility and performance. Evaluations on two model series, A0.3B-2B and A1B-7B, demonstrate Linear-MoE achieves efficiency gains while maintaining competitive performance on various benchmarks, showcasing its potential as a next-generation foundational model architecture. Code: https://github.com/OpenSparseLLMs/Linear-MoE.","authors":["Weigao Sun","Disen Lan","Tong Zhu","Xiaoye Qu","Yu Cheng"],"url":"https://arxiv.org/abs/2503.05447"}
{"created":"2025-04-16","title":"Free Your Hands: Lightweight Relightable Turntable Capture Pipeline","abstract":"Novel view synthesis (NVS) from multiple captured photos of an object is a widely studied problem. Achieving high quality typically requires dense sampling of input views, which can lead to frustrating and tedious manual labor. Manually positioning cameras to maintain an optimal desired distribution can be difficult for humans, and if a good distribution is found, it is not easy to replicate. Additionally, the captured data can suffer from motion blur and defocus due to human error. In this paper, we present a lightweight object capture pipeline to reduce the manual workload and standardize the acquisition setup. We use a consumer turntable to carry the object and a tripod to hold the camera. As the turntable rotates, we automatically capture dense samples from various views and lighting conditions; we can repeat this for several camera positions. This way, we can easily capture hundreds of valid images in several minutes without hands-on effort. However, in the object reference frame, the light conditions vary; this is harmful to a standard NVS method like 3D Gaussian splatting (3DGS) which assumes fixed lighting. We design a neural radiance representation conditioned on light rotations, which addresses this issue and allows relightability as an additional benefit. We demonstrate our pipeline using 3DGS as the underlying framework, achieving competitive quality compared to previous methods with exhaustive acquisition and showcasing its potential for relighting and harmonization tasks.","authors":["Jiahui Fan","Fujun Luan","Jian Yang","Milo\\v{s} Ha\\v{s}an","Beibei Wang"],"url":"https://arxiv.org/abs/2503.05511"}
{"created":"2025-04-16","title":"Superintelligence Strategy: Expert Version","abstract":"Rapid advances in AI are beginning to reshape national security. Destabilizing AI developments could rupture the balance of power and raise the odds of great-power conflict, while widespread proliferation of capable AI hackers and virologists would lower barriers for rogue actors to cause catastrophe. Superintelligence -- AI vastly better than humans at nearly all cognitive tasks -- is now anticipated by AI researchers. Just as nations once developed nuclear strategies to secure their survival, we now need a coherent superintelligence strategy to navigate a new period of transformative change. We introduce the concept of Mutual Assured AI Malfunction (MAIM): a deterrence regime resembling nuclear mutual assured destruction (MAD) where any state's aggressive bid for unilateral AI dominance is met with preventive sabotage by rivals. Given the relative ease of sabotaging a destabilizing AI project -- through interventions ranging from covert cyberattacks to potential kinetic strikes on datacenters -- MAIM already describes the strategic picture AI superpowers find themselves in. Alongside this, states can increase their competitiveness by bolstering their economies and militaries through AI, and they can engage in nonproliferation to rogue actors to keep weaponizable AI capabilities out of their hands. Taken together, the three-part framework of deterrence, nonproliferation, and competitiveness outlines a robust strategy to superintelligence in the years ahead.","authors":["Dan Hendrycks","Eric Schmidt","Alexandr Wang"],"url":"https://arxiv.org/abs/2503.05628"}
{"created":"2025-04-16","title":"MM-Eureka: Exploring the Frontiers of Multimodal Reasoning with Rule-based Reinforcement Learning","abstract":"DeepSeek R1, and o1 have demonstrated powerful reasoning capabilities in the text domain through stable large-scale reinforcement learning. To enable broader applications, some works have attempted to transfer these capabilities to multimodal reasoning. However, these efforts have been limited by the limited difficulty of selected tasks and relatively small training scales, making it challenging to demonstrate strong multimodal reasoning abilities. To address this gap, we introduce the MMK12 dataset and MM-EUREKA with 7B and 32B parameters. The former is a high-quality multimodal mathematics reasoning dataset featuring diverse knowledge domains with human-verified answers and solution processes. The latter is a multimodal model employing rule-based reinforcement learning on MMK12, utilizing online filtering and two-stage training strategy to enhance training stability. MM-EUREKA demonstrates remarkable performance gains in multimodal mathematical reasoning, outperforming previous powerful models like InternVL2.5-78B or InternVL2.5-38B-MPO. In particular, MM-EUREKA achieves competitive or superior performance compared to both open-source and closed-source models, and trails slightly behind o1 in multidisciplinary reasoning tasks. We open-source our complete pipeline to foster further research in this area. We release all our codes, models, data, etc. at https://github.com/ModalMinds/MM-EUREKA","authors":["Fanqing Meng","Lingxiao Du","Zongkai Liu","Zhixiang Zhou","Quanfeng Lu","Daocheng Fu","Tiancheng Han","Botian Shi","Wenhai Wang","Junjun He","Kaipeng Zhang","Ping Luo","Yu Qiao","Qiaosheng Zhang","Wenqi Shao"],"url":"https://arxiv.org/abs/2503.07365"}
{"created":"2025-04-16","title":"Split-n-Chain: Privacy-Preserving Multi-Node Split Learning with Blockchain-Based Auditability","abstract":"Deep learning, when integrated with a large amount of training data, has the potential to outperform machine learning in terms of high accuracy. Recently, privacy-preserving deep learning has drawn significant attention of the research community. Different privacy notions in deep learning include privacy of data provided by data-owners and privacy of parameters and/or hyperparameters of the underlying neural network. Federated learning is a popular privacy-preserving execution environment where data-owners participate in learning the parameters collectively without leaking their respective data to other participants. However, federated learning suffers from certain security/privacy issues. In this paper, we propose Split-n-Chain, a variant of split learning where the layers of the network are split among several distributed nodes. Split-n-Chain achieves several privacy properties: data-owners need not share their training data with other nodes, and no nodes have access to the parameters and hyperparameters of the neural network (except that of the respective layers they hold). Moreover, Split-n-Chain uses blockchain to audit the computation done by different nodes. Our experimental results show that: Split-n-Chain is efficient, in terms of time required to execute different phases, and the training loss trend is similar to that for the same neural network when implemented in a monolithic fashion.","authors":["Mukesh Sahani","Binanda Sengupta"],"url":"https://arxiv.org/abs/2503.07570"}
{"created":"2025-04-16","title":"Reasoning in visual navigation of end-to-end trained agents: a dynamical systems approach","abstract":"Progress in Embodied AI has made it possible for end-to-end-trained agents to navigate in photo-realistic environments with high-level reasoning and zero-shot or language-conditioned behavior, but benchmarks are still dominated by simulation. In this work, we focus on the fine-grained behavior of fast-moving real robots and present a large-scale experimental study involving \\numepisodes{} navigation episodes in a real environment with a physical robot, where we analyze the type of reasoning emerging from end-to-end training. In particular, we study the presence of realistic dynamics which the agent learned for open-loop forecasting, and their interplay with sensing. We analyze the way the agent uses latent memory to hold elements of the scene structure and information gathered during exploration. We probe the planning capabilities of the agent, and find in its memory evidence for somewhat precise plans over a limited horizon. Furthermore, we show in a post-hoc analysis that the value function learned by the agent relates to long-term planning. Put together, our experiments paint a new picture on how using tools from computer vision and sequential decision making have led to new capabilities in robotics and control. An interactive tool is available at europe.naverlabs.com/research/publications/reasoning-in-visual-navigation-of-end-to-end-trained-agents.","authors":["Steeven Janny","Herv\\'e Poirier","Leonid Antsfeld","Guillaume Bono","Gianluca Monaci","Boris Chidlovskii","Francesco Giuliari","Alessio Del Bue","Christian Wolf"],"url":"https://arxiv.org/abs/2503.08306"}
{"created":"2025-04-16","title":"Comparing Next-Day Wildfire Predictability of MODIS and VIIRS Satellite Data","abstract":"Multiple studies have performed next-day fire prediction using satellite imagery. Two main satellites are used to detect wildfires: MODIS and VIIRS. Both satellites provide fire mask products, called MOD14 and VNP14, respectively. Studies have used one or the other, but there has been no comparison between them to determine which might be more suitable for next-day fire prediction. In this paper, we first evaluate how well VIIRS and MODIS data can be used to forecast wildfire spread one day ahead. We find that the model using VIIRS as input and VNP14 as target achieves the best results. Interestingly, the model using MODIS as input and VNP14 as target performs significantly better than using VNP14 as input and MOD14 as target. Next, we discuss why MOD14 might be harder to use for predicting next-day fires. We find that the MOD14 fire mask is highly stochastic and does not correlate with reasonable fire spread patterns. This is detrimental for machine learning tasks, as the model learns irrational patterns. Therefore, we conclude that MOD14 is unsuitable for next-day fire prediction and that VNP14 is a much better option. However, using MODIS input and VNP14 as target, we achieve a significant improvement in predictability. This indicates that an improved fire detection model is possible for MODIS. The full code and dataset is available online: https://github.com/justuskarlsson/wildfire-mod14-vnp14","authors":["Justus Karlsson","Yonghao Xu","Amanda Berg","Leif Haglund"],"url":"https://arxiv.org/abs/2503.08580"}
{"created":"2025-04-16","title":"Steering No-Regret Agents in MFGs under Model Uncertainty","abstract":"Incentive design is a popular framework for guiding agents' learning dynamics towards desired outcomes by providing additional payments beyond intrinsic rewards. However, most existing works focus on a finite, small set of agents or assume complete knowledge of the game, limiting their applicability to real-world scenarios involving large populations and model uncertainty. To address this gap, we study the design of steering rewards in Mean-Field Games (MFGs) with density-independent transitions, where both the transition dynamics and intrinsic reward functions are unknown. This setting presents non-trivial challenges, as the mediator must incentivize the agents to explore for its model learning under uncertainty, while simultaneously steer them to converge to desired behaviors without incurring excessive incentive payments. Assuming agents exhibit no(-adaptive) regret behaviors, we contribute novel optimistic exploration algorithms. Theoretically, we establish sub-linear regret guarantees for the cumulative gaps between the agents' behaviors and the desired ones. In terms of the steering cost, we demonstrate that our total incentive payments incur only sub-linear excess, competing with a baseline steering strategy that stabilizes the target policy as an equilibrium. Our work presents an effective framework for steering agents behaviors in large-population systems under uncertainty.","authors":["Leo Widmer","Jiawei Huang","Niao He"],"url":"https://arxiv.org/abs/2503.09309"}
{"created":"2025-04-16","title":"The Pitfalls of Imitation Learning when Actions are Continuous","abstract":"We study the problem of imitating an expert demonstrator in a discrete-time, continuous state-and-action control system. We show that, even if the dynamics satisfy a control-theoretic property called exponentially stability (i.e. the effects of perturbations decay exponentially quickly), and the expert is smooth and deterministic, any smooth, deterministic imitator policy necessarily suffers error on execution that is exponentially larger, as a function of problem horizon, than the error under the distribution of expert training data. Our negative result applies to any algorithm which learns solely from expert data, including both behavior cloning and offline-RL algorithms, unless the algorithm produces highly \"improper\" imitator policies--those which are non-smooth, non-Markovian, or which exhibit highly state-dependent stochasticity--or unless the expert trajectory distribution is sufficiently \"spread.\" We provide experimental evidence of the benefits of these more complex policy parameterizations, explicating the benefits of today's popular policy parameterizations in robot learning (e.g. action-chunking and Diffusion Policies). We also establish a host of complementary negative and positive results for imitation in control systems.","authors":["Max Simchowitz","Daniel Pfrommer","Ali Jadbabaie"],"url":"https://arxiv.org/abs/2503.09722"}
{"created":"2025-04-16","title":"Strong normalization through idempotent intersection types: a new syntactical approach","abstract":"It is well-known that intersection type assignment systems can be used to characterize strong normalization (SN). Typical proofs that typable lambda-terms are SN in these systems rely on semantical techniques. In this work, we study $\\Lambda_\\cap^e$, a variant of Coppo and Dezani's (Curry-style) intersection type system, and we propose a syntactical proof of strong normalization for it. We first design $\\Lambda_\\cap^i$, a Church-style version, in which terms closely correspond to typing derivations. Then we prove that typability in $\\Lambda_\\cap^i$ implies SN through a measure that, given a term, produces a natural number that decreases along with reduction. Finally, the result is extended to $\\Lambda_\\cap^e$, since the two systems simulate each other.","authors":["Pablo Barenbaum","Simona Ronchi Della Rocca","Cristian Sottile"],"url":"https://arxiv.org/abs/2503.09831"}
{"created":"2025-04-16","title":"Approximation technique for preserving the minimum principle on the entropy for the compressible Euler Equations","abstract":"This paper is concerned with constructing an invariant-domain preserving approximation technique for the compressible Euler equations that preserves the minimum principle on the physical entropy. We show that any numerical method that can be written as a convex combination of good auxiliary states will satisfy the minimum principle on the physical entropy provided the equation of state satisfies some mild assumptions. Furthermore, we derive a wave speed estimate in an extended Riemann problem necessary for constructing the auxiliary states with desired properties. Finally, we numerically illustrate the proposed methodology.","authors":["Bennett Clayton","Eric J. Tovar"],"url":"https://arxiv.org/abs/2503.10612"}
{"created":"2025-04-16","title":"AI Enabled User-Specific Cyberbullying Severity Detection with Explainability","abstract":"The rise of social media has significantly increased the prevalence of cyberbullying (CB), posing serious risks to both mental and physical well-being. Effective detection systems are essential for mitigating its impact. While several machine learning (ML) models have been developed, few incorporate victims' psychological, demographic, and behavioral factors alongside bullying comments to assess severity. In this study, we propose an AI model intregrating user-specific attributes, including psychological factors (self-esteem, anxiety, depression), online behavior (internet usage, disciplinary history), and demographic attributes (race, gender, ethnicity), along with social media comments. Additionally, we introduce a re-labeling technique that categorizes social media comments into three severity levels: Not Bullying, Mild Bullying, and Severe Bullying, considering user-specific factors.Our LSTM model is trained using 146 features, incorporating emotional, topical, and word2vec representations of social media comments as well as user-level attributes and it outperforms existing baseline models, achieving the highest accuracy of 98\\% and an F1-score of 0.97. To identify key factors influencing the severity of cyberbullying, we employ explainable AI techniques (SHAP and LIME) to interpret the model's decision-making process. Our findings reveal that, beyond hate comments, victims belonging to specific racial and gender groups are more frequently targeted and exhibit higher incidences of depression, disciplinary issues, and low self-esteem. Additionally, individuals with a prior history of bullying are at a greater risk of becoming victims of cyberbullying.","authors":["Tabia Tanzin Prama","Jannatul Ferdaws Amrin","Md. Mushfique Anwar","Iqbal H. Sarker"],"url":"https://arxiv.org/abs/2503.10650"}
{"created":"2025-04-16","title":"CyclePose -- Leveraging Cycle-Consistency for Annotation-Free Nuclei Segmentation in Fluorescence Microscopy","abstract":"In recent years, numerous neural network architectures specifically designed for the instance segmentation of nuclei in microscopic images have been released. These models embed nuclei-specific priors to outperform generic architectures like U-Nets; however, they require large annotated datasets, which are often not available. Generative models (GANs, diffusion models) have been used to compensate for this by synthesizing training data. These two-stage approaches are computationally expensive, as first a generative model and then a segmentation model has to be trained. We propose CyclePose, a hybrid framework integrating synthetic data generation and segmentation training. CyclePose builds on a CycleGAN architecture, which allows unpaired translation between microscopy images and segmentation masks. We embed a segmentation model into CycleGAN and leverage a cycle consistency loss for self-supervision. Without annotated data, CyclePose outperforms other weakly or unsupervised methods on two public datasets. Code is available at https://github.com/jonasutz/CyclePose","authors":["Jonas Utz","Stefan Vocht","Anne Tjorven Buessen","Dennis Possart","Fabian Wagner","Mareike Thies","Mingxuan Gu","Stefan Uderhardt","Katharina Breininger"],"url":"https://arxiv.org/abs/2503.11266"}
{"created":"2025-04-16","title":"Cognitive Disentanglement for Referring Multi-Object Tracking","abstract":"As a significant application of multi-source information fusion in intelligent transportation perception systems, Referring Multi-Object Tracking (RMOT) involves localizing and tracking specific objects in video sequences based on language references. However, existing RMOT approaches often treat language descriptions as holistic embeddings and struggle to effectively integrate the rich semantic information contained in language expressions with visual features. This limitation is especially apparent in complex scenes requiring comprehensive understanding of both static object attributes and spatial motion information. In this paper, we propose a Cognitive Disentanglement for Referring Multi-Object Tracking (CDRMT) framework that addresses these challenges. It adapts the \"what\" and \"where\" pathways from the human visual processing system to RMOT tasks. Specifically, our framework first establishes cross-modal connections while preserving modality-specific characteristics. It then disentangles language descriptions and hierarchically injects them into object queries, refining object understanding from coarse to fine-grained semantic levels. Finally, we reconstruct language representations based on visual features, ensuring that tracked objects faithfully reflect the referring expression. Extensive experiments on different benchmark datasets demonstrate that CDRMT achieves substantial improvements over state-of-the-art methods, with average gains of 6.0% in HOTA score on Refer-KITTI and 3.2% on Refer-KITTI-V2. Our approach advances the state-of-the-art in RMOT while simultaneously providing new insights into multi-source information fusion.","authors":["Shaofeng Liang","Runwei Guan","Wangwang Lian","Daizong Liu","Xiaolou Sun","Dongming Wu","Yutao Yue","Weiping Ding","Hui Xiong"],"url":"https://arxiv.org/abs/2503.11496"}
{"created":"2025-04-16","title":"Train Robots in a JIF: Joint Inverse and Forward Dynamics with Human and Robot Demonstrations","abstract":"Pre-training on large datasets of robot demonstrations is a powerful technique for learning diverse manipulation skills but is often limited by the high cost and complexity of collecting robot-centric data, especially for tasks requiring tactile feedback. This work addresses these challenges by introducing a novel method for pre-training with multi-modal human demonstrations. Our approach jointly learns inverse and forward dynamics to extract latent state representations, towards learning manipulation specific representations. This enables efficient fine-tuning with only a small number of robot demonstrations, significantly improving data efficiency. Furthermore, our method allows for the use of multi-modal data, such as combination of vision and touch for manipulation. By leveraging latent dynamics modeling and tactile sensing, this approach paves the way for scalable robot manipulation learning based on human demonstrations.","authors":["Gagan Khandate","Boxuan Wang","Sarah Park","Weizhe Ni","Jaoquin Palacious","Kate Lampo","Philippe Wu","Rosh Ho","Eric Chang","Matei Ciocarlie"],"url":"https://arxiv.org/abs/2503.12297"}
{"created":"2025-04-16","title":"Enhancing Code LLM Training with Programmer Attention","abstract":"Human attention provides valuable yet underexploited signals for code LLM training, offering a perspective beyond purely machine-driven attention. Despite the complexity and cost of collecting eye-tracking data, there has also been limited progress in systematically using these signals for code LLM training. To address both issues, we propose a cohesive pipeline spanning augmentation and reward-based fine-tuning. Specifically, we introduce (1) an eye-tracking path augmentation method to expand programmer attention datasets, (2) a pattern abstraction step that refines raw fixations into learnable attention motifs, and (3) a reward-guided strategy for integrating these insights directly into a CodeT5 supervised fine-tuning process. Our experiments yield +7.16 in CodeBLEU on the CodeXGlue benchmark for code summarization, underscoring how uniting human and machine attention can boost code intelligence. We hope this work encourages broader exploration of human-centric methods in next-generation AI4SE.","authors":["Yifan Zhang","Chen Huang","Zachary Karas","Dung Thuy Nguyen","Kevin Leach","Yu Huang"],"url":"https://arxiv.org/abs/2503.14936"}
{"created":"2025-04-16","title":"ELTEX: A Framework for Domain-Driven Synthetic Data Generation","abstract":"We introduce Efficient LLM Token Extraction (ELTEX), a framework addressing the critical challenge of LLM domain specialization by systematically extracting and integrating domain indicators throughout synthetic data generation. Unlike approaches relying on implicit knowledge transfer, ELTEX explicitly leverages domain signals to maintain specialized knowledge integrity. In our cybersecurity case study, ELTEX-enhanced data enables a fine-tuned Gemma-2B model to achieve performance competitive with GPT-4o on blockchain cyberattack classification while reducing computational requirements. Our Google Sheets implementation makes ELTEX accessible to non-technical users. Our contributions include: (1) the ELTEX framework; (2) Google Sheets Add-on implementation; (3) empirical validation showing how ELTEX bridges performance gaps between small and large models; and (4) a synthetic dataset of 11,448 texts for blockchain cyberattack detection.","authors":["Arina Razmyslovich","Kseniia Murasheva","Sofia Sedlova","Julien Capitaine","Eugene Dmitriev"],"url":"https://arxiv.org/abs/2503.15055"}
{"created":"2025-04-16","title":"Cube: A Roblox View of 3D Intelligence","abstract":"Foundation models trained on vast amounts of data have demonstrated remarkable reasoning and generation capabilities in the domains of text, images, audio and video. Our goal at Roblox is to build such a foundation model for 3D intelligence, a model that can support developers in producing all aspects of a Roblox experience, from generating 3D objects and scenes to rigging characters for animation to producing programmatic scripts describing object behaviors. We discuss three key design requirements for such a 3D foundation model and then present our first step towards building such a model. We expect that 3D geometric shapes will be a core data type and describe our solution for 3D shape tokenizer. We show how our tokenization scheme can be used in applications for text-to-shape generation, shape-to-text generation and text-to-scene generation. We demonstrate how these applications can collaborate with existing large language models (LLMs) to perform scene analysis and reasoning. We conclude with a discussion outlining our path to building a fully unified foundation model for 3D intelligence.","authors":["Foundation AI Team","Kiran Bhat","Nishchaie Khanna","Karun Channa","Tinghui Zhou","Yiheng Zhu","Xiaoxia Sun","Charles Shang","Anirudh Sudarshan","Maurice Chu","Daiqing Li","Kangle Deng","Jean-Philippe Fauconnier","Tijmen Verhulsdonck","Maneesh Agrawala","Kayvon Fatahalian","Alexander Weiss","Christian Reiser","Ravi Kiran Chirravuri","Ravali Kandur","Alejandro Pelaez","Akash Garg","Michael Palleschi","Jessica Wang","Skylar Litz","Leon Liu","Anying Li","David Harmon","Derek Liu","Liangjun Feng","Denis Goupil","Lukas Kuczynski","Jihyun Yoon","Naveen Marri","Peiye Zhuang","Yinan Zhang","Brian Yin","Haomiao Jiang","Marcel van Workum","Thomas Lane","Bryce Erickson","Salil Pathare","Kyle Price","Steve Han","Yiqing Wang","Anupam Singh","David Baszucki"],"url":"https://arxiv.org/abs/2503.15475"}
{"created":"2025-04-16","title":"Think or Not Think: A Study of Explicit Thinking in Rule-Based Visual Reinforcement Fine-Tuning","abstract":"This paper investigates the thinking process in rule-based reinforcement learning fine-tuning (RFT) for multi-modal large language models (MLLMs). We first propose CLS-RL for classification, using verifiable rewards to encourage MLLM thinking. Experiments show CLS-RL significantly outperforms SFT and yields a 'free-lunch' generalization effect (improving performance on unseen datasets after training on one dataset). We then question if this explicit thinking is always necessary for RFT. Challenging convention that explicit thinking is crucial for RFT, we introduce No-Thinking-RL, minimizing thinking via a simple equality accuracy reward. Experiments show No-Thinking-RL surpasses CLS-RL in in-domain and generalization abilities, with significantly less fine-tuning time. This suggests reducing thinking can improve MLLM fine-tuning efficiency and effectiveness for certain visual tasks. We hypothesize explicit thinking negatively impacts reward convergence during RFT. To test this, we propose the Think-After-Answerwer method to let models first output the answer and then generate thinking process to alliviate the negative impact of thinking. We further test No-Thinking-RL on diverse tasks (including math, spatial, puzzles) with 2B and 7B models. For 2B models, No-Thinking-RL outperforms thinking-based RFT for all tasks, even on math, with Think-After-Answerwer performing intermediately. For 7B models, performance is comparable on simple visual tasks, but RFT with thinking excels on complex reasoning (math). This implies when dealing with complex math problems, smaller models struggle with generating effective reasoning, hurting performance on complex tasks. Conversely, for simple visual tasks, thinking is not indispensable, and its removal can boost performance and reduce training time. We hope our findings offer insights for better understanding the effect of the thinking process in RFT.","authors":["Ming Li","Jike Zhong","Shitian Zhao","Yuxiang Lai","Kaipeng Zhang"],"url":"https://arxiv.org/abs/2503.16188"}
{"created":"2025-04-16","title":"Bridging Technology and Humanities: Evaluating the Impact of Large Language Models on Social Sciences Research with DeepSeek-R1","abstract":"In recent years, the development of Large Language Models (LLMs) has made significant breakthroughs in the field of natural language processing and has gradually been applied to the field of humanities and social sciences research. LLMs have a wide range of application value in the field of humanities and social sciences because of its strong text understanding, generation and reasoning capabilities. In humanities and social sciences research, LLMs can analyze large-scale text data and make inferences.","authors":["Peiran Gu","Fuhao Duan","Wenhao Li","Bochen Xu","Ying Cai","Teng Yao","Chenxun Zhuo","Tianming Liu","Bao Ge"],"url":"https://arxiv.org/abs/2503.16304"}
{"created":"2025-04-16","title":"Text-Driven 3D Lidar Place Recognition for Autonomous Driving","abstract":"Environment description-based localization in large-scale point cloud maps constructed through remote sensing is critically significant for the advancement of large-scale autonomous systems, such as delivery robots operating in the last mile. However, current approaches encounter challenges due to the inability of point cloud encoders to effectively capture local details and long-range spatial relationships, as well as a significant modality gap between text and point cloud representations. To address these challenges, we present Des4Pos, a novel two-stage text-driven remote sensing localization framework. In the coarse stage, the point-cloud encoder utilizes the Multi-scale Fusion Attention Mechanism (MFAM) to enhance local geometric features, followed by a bidirectional Long Short-Term Memory (LSTM) module to strengthen global spatial relationships. Concurrently, the Stepped Text Encoder (STE) integrates cross-modal prior knowledge from CLIP [1] and aligns text and point-cloud features using this prior knowledge, effectively bridging modality discrepancies. In the fine stage, we introduce a Cascaded Residual Attention (CRA) module to fuse cross-modal features and predict relative localization offsets, thereby achieving greater localization precision. Experiments on the KITTI360Pose test set demonstrate that Des4Pos achieves state-of-the-art performance in text-to-point-cloud place recognition. Specifically, it attains a top-1 accuracy of 40% and a top-10 accuracy of 77% under a 5-meter radius threshold, surpassing the best existing methods by 7% and 7%, respectively.","authors":["Tianyi Shang","Zhenyu Li","Pengjie Xu","Zhaojun Deng","Ruirui Zhang"],"url":"https://arxiv.org/abs/2503.18035"}
{"created":"2025-04-16","title":"Unmasking Deceptive Visuals: Benchmarking Multimodal Large Language Models on Misleading Chart Question Answering","abstract":"Misleading chart visualizations, which intentionally manipulate data representations to support specific claims, can distort perceptions and lead to incorrect conclusions. Despite decades of research, misleading visualizations remain a widespread and pressing issue. Recent advances in multimodal large language models (MLLMs) have demonstrated strong chart comprehension capabilities, yet no existing work has systematically evaluated their ability to detect and interpret misleading charts. This paper introduces the Misleading Chart Question Answering (Misleading ChartQA) Benchmark, a large-scale multimodal dataset designed to assess MLLMs in identifying and reasoning about misleading charts. It contains over 3,000 curated examples, covering 21 types of misleaders and 10 chart types. Each example includes standardized chart code, CSV data, and multiple-choice questions with labeled explanations, validated through multi-round MLLM checks and exhausted expert human review. We benchmark 16 state-of-the-art MLLMs on our dataset, revealing their limitations in identifying visually deceptive practices. We also propose a novel pipeline that detects and localizes misleaders, enhancing MLLMs' accuracy in misleading chart interpretation. Our work establishes a foundation for advancing MLLM-driven misleading chart comprehension. We publicly release the sample dataset to support further research in this critical area.","authors":["Zixin Chen","Sicheng Song","Kashun Shum","Yanna Lin","Rui Sheng","Huamin Qu"],"url":"https://arxiv.org/abs/2503.18172"}
{"created":"2025-04-16","title":"Feature Calibration enhanced Parameter Synthesis for CLIP-based Class-incremental Learning","abstract":"Class-Incremental Learning (CIL) enables models to continuously learn new class knowledge while retaining previous classes, facilitating adaptation and evolution in dynamic, real-world environments. Traditional CIL methods primarily rely on visual features, which limits their effectiveness in complex, multimodal scenarios. In contrast, VLMs show promising potential for enhancing CIL by leveraging pre-trained knowledge and integrating multi-modal semantic cues such as text and vision. However, existing approaches struggle to mitigate catastrophic forgetting while preserving the generalization strengths of VLMs across diverse modalities. To address these challenges, we propose a Feature Calibration Enhanced Parameter Synthesis (FCPS) framework. Specifically, FCPS introduces a dynamic parameter adjustment mechanism that iteratively calibrates the contribution of original visual features to the final class decision, thus preserving the model's intrinsic generalization capability across modalities. Simultaneously, parameter integration enables effective knowledge transfer, maintaining a balance between acquiring new class representations and preserving old knowledge. Experimental results on popular benchmarks (e.g., CIFAR100 and ImageNet100) validate the superiority of the proposed method.","authors":["Juncen Guo","Yang Liu","Xiaoguang Zhu","Lianlong Sun","Liangyu Teng","Jingyi Wu","Di Li","Wei Zhou","Liang Song"],"url":"https://arxiv.org/abs/2503.18672"}
{"created":"2025-04-16","title":"AI threats to national security can be countered through an incident regime","abstract":"Recent progress in AI capabilities has heightened concerns that AI systems could pose a threat to national security, for example, by making it easier for malicious actors to perform cyberattacks on critical national infrastructure, or through loss of control of autonomous AI systems. In parallel, federal legislators in the US have proposed nascent 'AI incident regimes' to identify and counter similar threats. In this paper, we consolidate these two trends and present a timely proposal for a legally mandated post-deployment AI incident regime that aims to counter potential national security threats from AI systems. We start the paper by introducing the concept of 'security-critical' to describe doctors that pose extreme risks to national security, before arguing that 'security-critical' describes civilian nuclear power, aviation, life science dual-use research of concern, and frontier AI development. We then present in detail our AI incident regime proposal, justifying each component of the proposal by demonstrating its similarity to US domestic incident regimes in other 'security-critical' sectors. Finally, we sketch a hypothetical scenario where our proposed AI incident regime deals with an AI cyber incident. Our proposed AI incident regime is split into three phases. The first phase revolves around a novel operationalization of what counts as an 'AI incident' and we suggest that AI providers must create a 'national security case' before deploying a frontier AI system. The second and third phases spell out that AI providers should notify a government agency about incidents, and that the government agency should be involved in amending AI providers' security and safety procedures, in order to counter future threats to national security.","authors":["Alejandro Ortega"],"url":"https://arxiv.org/abs/2503.19887"}
{"created":"2025-04-16","title":"Are We Solving a Well-Defined Problem? A Task-Centric Perspective on Recommendation Tasks","abstract":"Recommender systems (RecSys) leverage user interaction history to predict and suggest relevant items, shaping user experiences across various domains. While many studies adopt a general problem definition, i.e., to recommend preferred items to users based on past interactions, such abstraction often lacks the domain-specific nuances necessary for practical deployment. However, models are frequently evaluated using datasets from online recommender platforms, which inherently reflect these specificities. In this paper, we analyze RecSys task formulations, emphasizing key components such as input-output structures, temporal dynamics, and candidate item selection. All these factors directly impact offline evaluation. We further examine the complexities of user-item interactions, including decision-making costs, multi-step engagements, and unobservable interactions, which may influence model design and loss functions. Additionally, we explore the balance between task specificity and model generalizability, highlighting how well-defined task formulations serve as the foundation for robust evaluation and effective solution development. By clarifying task definitions and their implications, this work provides a structured perspective on RecSys research. The goal is to help researchers better navigate the field, particularly in understanding specificities of the RecSys tasks and ensuring fair and meaningful evaluations.","authors":["Aixin Sun"],"url":"https://arxiv.org/abs/2503.21188"}
{"created":"2025-04-16","title":"SC-NeRF: NeRF-based Point Cloud Reconstruction using a Stationary Camera for Agricultural Applications","abstract":"This paper presents a NeRF-based framework for point cloud (PCD) reconstruction, specifically designed for indoor high-throughput plant phenotyping facilities. Traditional NeRF-based reconstruction methods require cameras to move around stationary objects, but this approach is impractical for high-throughput environments where objects are rapidly imaged while moving on conveyors or rotating pedestals. To address this limitation, we develop a variant of NeRF-based PCD reconstruction that uses a single stationary camera to capture images as the object rotates on a pedestal. Our workflow comprises COLMAP-based pose estimation, a straightforward pose transformation to simulate camera movement, and subsequent standard NeRF training. A defined Region of Interest (ROI) excludes irrelevant scene data, enabling the generation of high-resolution point clouds (10M points). Experimental results demonstrate excellent reconstruction fidelity, with precision-recall analyses yielding an F-score close to 100.00 across all evaluated plant objects. Although pose estimation remains computationally intensive with a stationary camera setup, overall training and reconstruction times are competitive, validating the method's feasibility for practical high-throughput indoor phenotyping applications. Our findings indicate that high-quality NeRF-based 3D reconstructions are achievable using a stationary camera, eliminating the need for complex camera motion or costly imaging equipment. This approach is especially beneficial when employing expensive and delicate instruments, such as hyperspectral cameras, for 3D plant phenotyping. Future work will focus on optimizing pose estimation techniques and further streamlining the methodology to facilitate seamless integration into automated, high-throughput 3D phenotyping pipelines.","authors":["Kibon Ku","Talukder Z Jubery","Elijah Rodriguez","Aditya Balu","Soumik Sarkar","Adarsh Krishnamurthy","Baskar Ganapathysubramanian"],"url":"https://arxiv.org/abs/2503.21958"}
{"created":"2025-04-16","title":"CIMPool: Scalable Neural Network Acceleration for Compute-In-Memory using Weight Pools","abstract":"Compute-in-memory (CIM) based neural network accelerators offer a promising solution to the Von Neumann bottleneck by computing directly within memory arrays. However, SRAM CIM faces limitations in executing larger models due to its cell size and on-chip memory constraints. This work proposes CIMPool, a CIM-aware compression and acceleration framework that counters this limitation through a weight sharing-based compression technique, aptly named `Weight Pool,' enabling significantly larger neural networks to be accommodated within on-chip memory constraints. This method minimizes the accuracy trade-off typically associated with parameter compression, allowing CIMPool to achieve a significantly larger compression ratio compared to the traditional quantization method with iso-accuracy.","authors":["Shurui Li","Puneet Gupta"],"url":"https://arxiv.org/abs/2503.22044"}
{"created":"2025-04-16","title":"Preference-based Learning with Retrieval Augmented Generation for Conversational Question Answering","abstract":"Conversational Question Answering (ConvQA) involves multiple subtasks, i) to understand incomplete questions in their context, ii) to retrieve relevant information, and iii) to generate answers. This work presents PRAISE, a pipeline-based approach for ConvQA that trains LLM adapters for each of the three subtasks. As labeled training data for individual subtasks is unavailable in practice, PRAISE learns from its own generations using the final answering performance as feedback signal without human intervention and treats intermediate information, like relevant evidence, as weakly labeled data. We apply Direct Preference Optimization by contrasting successful and unsuccessful samples for each subtask. In our experiments, we show the effectiveness of this training paradigm: PRAISE shows improvements per subtask and achieves new state-of-the-art performance on a popular ConvQA benchmark, by gaining 15.5 percentage points increase in precision over baselines.","authors":["Magdalena Kaiser","Gerhard Weikum"],"url":"https://arxiv.org/abs/2503.22303"}
{"created":"2025-04-16","title":"Buyer-Initiated Auction Mechanism for Data Redemption in Machine Unlearning","abstract":"The rapid growth of artificial intelligence (AI) has raised privacy concerns over user data, leading to regulations like the General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA). With the essential toolbox provided by machine unlearning, AI service providers are now able to remove user data from their trained models as well as the training datasets, so as to comply with such regulations. However, extensive data redemption can be costly and degrade model accuracy. To balance the cost of unlearning and the privacy protection, we propose a buyer-initiated auction mechanism for data redemption, enabling the service provider to purchase data from willing users with appropriate compensation. This approach does not require the server to have any a priori knowledge about the users' privacy preference, and provides an efficient solution for maximizing the social welfare in the investigated problem.","authors":["Bin Han","Di Feng","Jie Wang","Hans D. Schotten"],"url":"https://arxiv.org/abs/2503.23001"}
{"created":"2025-04-16","title":"Do We Truly Need So Many Samples? Multi-LLM Repeated Sampling Efficiently Scales Test-Time Compute","abstract":"This paper presents a simple, effective, and cost-efficient strategy to improve LLM performance by scaling test-time compute. Our strategy builds upon the repeated-sampling-then-voting framework, with a novel twist: incorporating multiple models, even weaker ones, to leverage their complementary strengths that potentially arise from diverse training data and paradigms. By using consistency as a signal, our strategy dynamically switches between models. Theoretical analysis highlights the efficiency and performance advantages of our strategy. Extensive experiments on six datasets demonstrate that our strategy not only outperforms self-consistency and state-of-the-art multi-agent debate approaches, but also significantly reduces inference costs. Additionally, ModelSwitch requires only a few comparable LLMs to achieve optimal performance and can be extended with verification methods, demonstrating the potential of leveraging multiple LLMs in the generation-verification paradigm.","authors":["Jianhao Chen","Zishuo Xun","Bocheng Zhou","Han Qi","Qiaosheng Zhang","Yang Chen","Wei Hu","Yuzhong Qu","Wanli Ouyang","Shuyue Hu"],"url":"https://arxiv.org/abs/2504.00762"}
{"created":"2025-04-16","title":"Improved Visual-Spatial Reasoning via R1-Zero-Like Training","abstract":"Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.","authors":["Zhenyi Liao","Qingsong Xie","Yanhao Zhang","Zijian Kong","Haonan Lu","Zhenyu Yang","Zhijie Deng"],"url":"https://arxiv.org/abs/2504.00883"}
{"created":"2025-04-16","title":"AutoML Benchmark with shorter time constraints and early stopping","abstract":"Automated Machine Learning (AutoML) automatically builds machine learning (ML) models on data. The de facto standard for evaluating new AutoML frameworks for tabular data is the AutoML Benchmark (AMLB). AMLB proposed to evaluate AutoML frameworks using 1- and 4-hour time budgets across 104 tasks. We argue that shorter time constraints should be considered for the benchmark because of their practical value, such as when models need to be retrained with high frequency, and to make AMLB more accessible. This work considers two ways in which to reduce the overall computation used in the benchmark: smaller time constraints and the use of early stopping. We conduct evaluations of 11 AutoML frameworks on 104 tasks with different time constraints and find the relative ranking of AutoML frameworks is fairly consistent across time constraints, but that using early-stopping leads to a greater variety in model performance.","authors":["Israel Campero Jurado","Pieter Gijsbers","Joaquin Vanschoren"],"url":"https://arxiv.org/abs/2504.01222"}
{"created":"2025-04-16","title":"Niche Dynamics in Complex Online Community Ecosystems","abstract":"Online communities are important organizational forms where members socialize and share information. Curiously, different online communities often overlap considerably in topic and membership. Recent research has investigated competition and mutualism among overlapping online communities through the lens of organizational ecology; however, it has not accounted for how the nonlinear dynamics of online attention may lead to episodic competition and mutualism. Neither has it explored the origins of competition and mutualism in the processes by which online communities select or adapt to their niches. This paper presents a large-scale study of 8,806 Reddit communities belonging to 1,919 clusters of high user overlap over a 5-year period. The method uses nonlinear time series methods to infer bursty, often short-lived ecological dynamics. Results reveal that mutualism episodes are longer lived and slightly more frequent than competition episodes. Next, it tests whether online communities find their niches by specializing to avoid competition using panel regression models. It finds that competitive ecological interactions lead to decreasing topic and user overlaps; however, changes that decrease such niche overlaps do not lead to mutualism. The discussion proposes that future designs may enable online community ecosystem management by informing online community leaders to organize \"spin-off\" communities or via feeds and recommendations.","authors":["Nathan TeBlunthuis"],"url":"https://arxiv.org/abs/2504.02153"}
{"created":"2025-04-16","title":"DeepResearcher: Scaling Deep Research via Reinforcement Learning in Real-world Environments","abstract":"Large Language Models (LLMs) equipped with web search capabilities have demonstrated impressive potential for deep research tasks. However, current approaches predominantly rely on either manually engineered prompts (prompt engineering-based) with brittle performance or reinforcement learning within controlled Retrieval-Augmented Generation (RAG) environments (RAG-based) that fail to capture the complexities of real-world interaction. In this paper, we introduce DeepResearcher, the first comprehensive framework for end-to-end training of LLM-based deep research agents through scaling reinforcement learning (RL) in real-world environments with authentic web search interactions. Unlike RAG-based approaches that assume all necessary information exists within a fixed corpus, our method trains agents to navigate the noisy, unstructured, and dynamic nature of the open web. We implement a specialized multi-agent architecture where browsing agents extract relevant information from various webpage structures and overcoming significant technical challenges. Extensive experiments on open-domain research tasks demonstrate that DeepResearcher achieves substantial improvements of up to 28.9 points over prompt engineering-based baselines and up to 7.2 points over RAG-based RL agents. Our qualitative analysis reveals emergent cognitive behaviors from end-to-end RL training, including the ability to formulate plans, cross-validate information from multiple sources, engage in self-reflection to redirect research, and maintain honesty when unable to find definitive answers. Our results highlight that end-to-end training in real-world web environments is not merely an implementation detail but a fundamental requirement for developing robust research capabilities aligned with real-world applications. We release DeepResearcher at https://github.com/GAIR-NLP/DeepResearcher.","authors":["Yuxiang Zheng","Dayuan Fu","Xiangkun Hu","Xiaojie Cai","Lyumanshan Ye","Pengrui Lu","Pengfei Liu"],"url":"https://arxiv.org/abs/2504.03160"}
{"created":"2025-04-16","title":"Mamba as a Bridge: Where Vision Foundation Models Meet Vision Language Models for Domain-Generalized Semantic Segmentation","abstract":"Vision Foundation Models (VFMs) and Vision-Language Models (VLMs) have gained traction in Domain Generalized Semantic Segmentation (DGSS) due to their strong generalization capabilities. However, existing DGSS methods often rely exclusively on either VFMs or VLMs, overlooking their complementary strengths. VFMs (e.g., DINOv2) excel at capturing fine-grained features, while VLMs (e.g., CLIP) provide robust text alignment but struggle with coarse granularity. Despite their complementary strengths, effectively integrating VFMs and VLMs with attention mechanisms is challenging, as the increased patch tokens complicate long-sequence modeling. To address this, we propose MFuser, a novel Mamba-based fusion framework that efficiently combines the strengths of VFMs and VLMs while maintaining linear scalability in sequence length. MFuser consists of two key components: MVFuser, which acts as a co-adapter to jointly fine-tune the two models by capturing both sequential and spatial dynamics; and MTEnhancer, a hybrid attention-Mamba module that refines text embeddings by incorporating image priors. Our approach achieves precise feature locality and strong text alignment without incurring significant computational overhead. Extensive experiments demonstrate that MFuser significantly outperforms state-of-the-art DGSS methods, achieving 68.20 mIoU on synthetic-to-real and 71.87 mIoU on real-to-real benchmarks. The code is available at https://github.com/devinxzhang/MFuser.","authors":["Xin Zhang","Robby T. Tan"],"url":"https://arxiv.org/abs/2504.03193"}
{"created":"2025-04-16","title":"Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models","abstract":"As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\\times$ faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.","authors":["NVIDIA",":","Aaron Blakeman","Aarti Basant","Abhinav Khattar","Adithya Renduchintala","Akhiad Bercovich","Aleksander Ficek","Alexis Bjorlin","Ali Taghibakhshi","Amala Sanjay Deshmukh","Ameya Sunil Mahabaleshwarkar","Andrew Tao","Anna Shors","Ashwath Aithal","Ashwin Poojary","Ayush Dattagupta","Balaram Buddharaju","Bobby Chen","Boris Ginsburg","Boxin Wang","Brandon Norick","Brian Butterfield","Bryan Catanzaro","Carlo del Mundo","Chengyu Dong","Christine Harvey","Christopher Parisien","Dan Su","Daniel Korzekwa","Danny Yin","Daria Gitman","David Mosallanezhad","Deepak Narayanan","Denys Fridman","Dima Rekesh","Ding Ma","Dmytro Pykhtar","Dong Ahn","Duncan Riach","Dusan Stosic","Eileen Long","Elad Segal","Ellie Evans","Eric Chung","Erick Galinkin","Evelina Bakhturina","Ewa Dobrowolska","Fei Jia","Fuxiao Liu","Gargi Prasad","Gerald Shen","Guilin Liu","Guo Chen","Haifeng Qian","Helen Ngo","Hongbin Liu","Hui Li","Igor Gitman","Ilia Karmanov","Ivan Moshkov","Izik Golan","Jan Kautz","Jane Polak Scowcroft","Jared Casper","Jarno Seppanen","Jason Lu","Jason Sewall","Jiaqi Zeng","Jiaxuan You","Jimmy Zhang","Jing Zhang","Jining Huang","Jinze Xue","Jocelyn Huang","Joey Conway","John Kamalu","Jon Barker","Jonathan Cohen","Joseph Jennings","Jupinder Parmar","Karan Sapra","Kari Briski","Kateryna Chumachenko","Katherine Luna","Keshav Santhanam","Kezhi Kong","Kirthi Sivamani","Krzysztof Pawelec","Kumar Anik","Kunlun Li","Lawrence McAfee","Leon Derczynski","Lindsey Pavao","Luis Vega","Lukas Voegtle","Maciej Bala","Maer Rodrigues de Melo","Makesh Narsimhan Sreedhar","Marcin Chochowski","Markus Kliegl","Marta Stepniewska-Dziubinska","Matthieu Le","Matvei Novikov","Mehrzad Samadi","Michael Andersch","Michael Evans","Miguel Martinez","Mike Chrzanowski","Mike Ranzinger","Mikolaj Blaz","Misha Smelyanskiy","Mohamed Fawzy","Mohammad Shoeybi","Mostofa Patwary","Nayeon Lee","Nima Tajbakhsh","Ning Xu","Oleg Rybakov","Oleksii Kuchaiev","Olivier Delalleau","Osvald Nitski","Parth Chadha","Pasha Shamis","Paulius Micikevicius","Pavlo Molchanov","Peter Dykas","Philipp Fischer","Pierre-Yves Aquilanti","Piotr Bialecki","Prasoon Varshney","Pritam Gundecha","Przemek Tredak","Rabeeh Karimi","Rahul Kandu","Ran El-Yaniv","Raviraj Joshi","Roger Waleffe","Ruoxi Zhang","Sabrina Kavanaugh","Sahil Jain","Samuel Kriman","Sangkug Lym","Sanjeev Satheesh","Saurav Muralidharan","Sean Narenthiran","Selvaraj Anandaraj","Seonmyeong Bak","Sergey Kashirsky","Seungju Han","Shantanu Acharya","Shaona Ghosh","Sharath Turuvekere Sreenivas","Sharon Clay","Shelby Thomas","Shrimai Prabhumoye","Shubham Pachori","Shubham Toshniwal","Shyamala Prayaga","Siddhartha Jain","Sirshak Das","Slawek Kierat","Somshubra Majumdar","Song Han","Soumye Singhal","Sriharsha Niverty","Stefania Alborghetti","Suseella Panguluri","Swetha Bhendigeri","Syeda Nahida Akter","Szymon Migacz","Tal Shiri","Terry Kong","Timo Roman","Tomer Ronen","Trisha Saar","Tugrul Konuk","Tuomas Rintamaki","Tyler Poon","Ushnish De","Vahid Noroozi","Varun Singh","Vijay Korthikanti","Vitaly Kurin","Wasi Uddin Ahmad","Wei Du","Wei Ping","Wenliang Dai","Wonmin Byeon","Xiaowei Ren","Yao Xu","Yejin Choi","Yian Zhang","Ying Lin","Yoshi Suhara","Zhiding Yu","Zhiqi Li","Zhiyu Li","Zhongbo Zhu","Zhuolin Yang","Zijia Chen"],"url":"https://arxiv.org/abs/2504.03624"}
{"created":"2025-04-16","title":"Reinforcing Clinical Decision Support through Multi-Agent Systems and Ethical AI Governance","abstract":"Recent advances in the data-driven medicine approach, which integrates ethically managed and explainable artificial intelligence into clinical decision support systems (CDSS), are critical to ensure reliable and effective patient care. This paper focuses on comparing novel agent system designs that use modular agents to analyze laboratory results, vital signs, and clinical context, and to predict and validate results. We implement our agent system with the eICU database, including running lab analysis, vitals-only interpreters, and contextual reasoners agents first, then sharing the memory into the integration agent, prediction agent, transparency agent, and a validation agent. Our results suggest that the multi-agent system (MAS) performed better than the single-agent system (SAS) with mortality prediction accuracy (59%, 56%) and the mean error for length of stay (LOS)(4.37 days, 5.82 days), respectively. However, the transparency score for the SAS (86.21) is slightly better than the transparency score for MAS (85.5). Finally, this study suggests that our agent-based framework not only improves process transparency and prediction accuracy but also strengthens trustworthy AI-assisted decision support in an intensive care setting.","authors":["Ying-Jung Chen","Ahmad Albarqawi","Chi-Sheng Chen"],"url":"https://arxiv.org/abs/2504.03699"}
{"created":"2025-04-16","title":"TrafficLLM: Enhancing Large Language Models for Network Traffic Analysis with Generic Traffic Representation","abstract":"Machine learning (ML) powered network traffic analysis has been widely used for the purpose of threat detection. Unfortunately, their generalization across different tasks and unseen data is very limited. Large language models (LLMs), known for their strong generalization capabilities, have shown promising performance in various domains. However, their application to the traffic analysis domain is limited due to significantly different characteristics of network traffic. To address the issue, in this paper, we propose TrafficLLM, which introduces a dual-stage fine-tuning framework to learn generic traffic representation from heterogeneous raw traffic data. The framework uses traffic-domain tokenization, dual-stage tuning pipeline, and extensible adaptation to help LLM release generalization ability on dynamic traffic analysis tasks, such that it enables traffic detection and traffic generation across a wide range of downstream tasks. We evaluate TrafficLLM across 10 distinct scenarios and 229 types of traffic. TrafficLLM achieves F1-scores of 0.9875 and 0.9483, with up to 80.12% and 33.92% better performance than existing detection and generation methods. It also shows strong generalization on unseen traffic with an 18.6% performance improvement. We further evaluate TrafficLLM in real-world scenarios. The results confirm that TrafficLLM is easy to scale and achieves accurate detection performance on enterprise traffic.","authors":["Tianyu Cui","Xinjie Lin","Sijia Li","Miao Chen","Qilei Yin","Qi Li","Ke Xu"],"url":"https://arxiv.org/abs/2504.04222"}
{"created":"2025-04-16","title":"Retro-Search: Exploring Untaken Paths for Deeper and Efficient Reasoning","abstract":"Large reasoning models exhibit remarkable reasoning capabilities via long, elaborate reasoning trajectories. Supervised fine-tuning on such reasoning traces, also known as distillation, can be a cost-effective way to boost reasoning capabilities of student models. However, empirical observations reveal that these reasoning trajectories are often suboptimal, switching excessively between different lines of thought, resulting in under-thinking, over-thinking, and even degenerate responses. We introduce Retro-Search, an MCTS-inspired search algorithm, for distilling higher quality reasoning paths from large reasoning models. Retro-Search retrospectively revises reasoning paths to discover better, yet shorter traces, which can then lead to student models with enhanced reasoning capabilities with shorter, thus faster inference. Our approach can enable two use cases: self-improvement, where models are fine-tuned on their own Retro-Search-ed thought traces, and weak-to-strong improvement, where a weaker model revises stronger model's thought traces via Retro-Search. For self-improving, R1-distill-7B, fine-tuned on its own Retro-Search-ed traces, reduces the average reasoning length by 31.2% while improving performance by 7.7% across seven math benchmarks. For weak-to-strong improvement, we retrospectively revise R1-671B's traces from the OpenThoughts dataset using R1-distill-32B as the Retro-Search-er, a model 20x smaller. Qwen2.5-32B, fine-tuned on this refined data, achieves performance comparable to R1-distill-32B, yielding an 11.3% reduction in reasoning length and a 2.4% performance improvement compared to fine-tuning on the original OpenThoughts data. Our work counters recently emergent viewpoints that question the relevance of search algorithms in the era of large reasoning models, by demonstrating that there are still opportunities for algorithmic advancements, even for frontier models.","authors":["Ximing Lu","Seungju Han","David Acuna","Hyunwoo Kim","Jaehun Jung","Shrimai Prabhumoye","Niklas Muennighoff","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro","Yejin Choi"],"url":"https://arxiv.org/abs/2504.04383"}
{"created":"2025-04-16","title":"ELT-Bench: An End-to-End Benchmark for Evaluating AI Agents on ELT Pipelines","abstract":"Practitioners are increasingly turning to Extract-Load-Transform (ELT) pipelines with the widespread adoption of cloud data warehouses. However, designing these pipelines often involves significant manual work to ensure correctness. Recent advances in AI-based methods, which have shown strong capabilities in data tasks, such as text-to-SQL, present an opportunity to alleviate manual efforts in developing ELT pipelines. Unfortunately, current benchmarks in data engineering only evaluate isolated tasks, such as using data tools and writing data transformation queries, leaving a significant gap in evaluating AI agents for generating end-to-end ELT pipelines.","authors":["Tengjun Jin","Yuxuan Zhu","Daniel Kang"],"url":"https://arxiv.org/abs/2504.04808"}
{"created":"2025-04-16","title":"Transforming Future Data Center Operations and Management via Physical AI","abstract":"Data centers (DCs) as mission-critical infrastructures are pivotal in powering the growth of artificial intelligence (AI) and the digital economy. The evolution from Internet DC to AI DC has introduced new challenges in operating and managing data centers for improved business resilience and reduced total cost of ownership. As a result, new paradigms, beyond the traditional approaches based on best practices, must be in order for future data centers. In this research, we propose and develop a novel Physical AI (PhyAI) framework for advancing DC operations and management. Our system leverages the emerging capabilities of state-of-the-art industrial products and our in-house research and development. Specifically, it presents three core modules, namely: 1) an industry-grade in-house simulation engine to simulate DC operations in a highly accurate manner, 2) an AI engine built upon NVIDIA PhysicsNemo for the training and evaluation of physics-informed machine learning (PIML) models, and 3) a digital twin platform built upon NVIDIA Omniverse for our proposed 5-tier digital twin framework. This system presents a scalable and adaptable solution to digitalize, optimize, and automate future data center operations and management, by enabling real-time digital twins for future data centers. To illustrate its effectiveness, we present a compelling case study on building a surrogate model for predicting the thermal and airflow profiles of a large-scale DC in a real-time manner. Our results demonstrate its superior performance over traditional time-consuming Computational Fluid Dynamics/Heat Transfer (CFD/HT) simulation, with a median absolute temperature prediction error of 0.18 {\\deg}C. This emerging approach would open doors to several potential research directions for advancing Physical AI in future DC operations.","authors":["Zhiwei Cao","Minghao Li","Feng Lin","Jimin Jia","Yonggang Wen","Jianxiong Yin","Simon See"],"url":"https://arxiv.org/abs/2504.04982"}
{"created":"2025-04-16","title":"LDGNet: A Lightweight Difference Guiding Network for Remote Sensing Change Detection","abstract":"With the rapid advancement of deep learning, the field of change detection (CD) in remote sensing imagery has achieved remarkable progress. Existing change detection methods primarily focus on achieving higher accuracy with increased computational costs and parameter sizes, leaving development of lightweight methods for rapid real-world processing an underexplored challenge. To address this challenge, we propose a Lightweight Difference Guiding Network (LDGNet), leveraging absolute difference image to guide optical remote sensing change detection. First, to enhance the feature representation capability of the lightweight backbone network, we propose the Difference Guiding Module (DGM), which leverages multi-scale features extracted from the absolute difference image to progressively influence the original image encoder at each layer, thereby reinforcing feature extraction. Second, we propose the Difference-Aware Dynamic Fusion (DADF) module with Visual State Space Model (VSSM) for lightweight long-range dependency modeling. The module first uses feature absolute differences to guide VSSM's global contextual modeling of change regions, then employs difference attention to dynamically fuse these long-range features with feature differences, enhancing change semantics while suppressing noise and background. Extensive experiments on multiple datasets demonstrate that our method achieves comparable or superior performance to current state-of-the-art (SOTA) methods requiring several times more computation, while maintaining only 3.43M parameters and 1.12G FLOPs.","authors":["Chenfeng Xu"],"url":"https://arxiv.org/abs/2504.05062"}
{"created":"2025-04-16","title":"CARE: Aligning Language Models for Regional Cultural Awareness","abstract":"Existing language models (LMs) often exhibit a Western-centric bias and struggle to represent diverse cultural knowledge. Previous attempts to address this rely on synthetic data and express cultural knowledge only in English. In this work, we study whether a small amount of human-written, multilingual cultural preference data can improve LMs across various model families and sizes. We first introduce CARE, a multilingual resource of 24.1k responses with human preferences on 2,580 questions about Chinese and Arab cultures, all carefully annotated by native speakers and offering more balanced coverage. Using CARE, we demonstrate that cultural alignment improves existing LMs beyond generic resources without compromising general capabilities. Moreover, we evaluate the cultural awareness of LMs, native speakers, and retrieved web content when queried in different languages. Our experiment reveals regional disparities among LMs, which may also be reflected in the documentation gap: native speakers often take everyday cultural commonsense and social norms for granted, while non-natives are more likely to actively seek out and document them. CARE is publicly available at https://github.com/Guochry/CARE (we plan to add Japanese data in the near future).","authors":["Geyang Guo","Tarek Naous","Hiromi Wakaki","Yukiko Nishimura","Yuki Mitsufuji","Alan Ritter","Wei Xu"],"url":"https://arxiv.org/abs/2504.05154"}
{"created":"2025-04-16","title":"Frontier AI's Impact on the Cybersecurity Landscape","abstract":"As frontier AI advances rapidly, understanding its impact on cybersecurity and inherent risks is essential to ensuring safe AI evolution (e.g., guiding risk mitigation and informing policymakers). While some studies review AI applications in cybersecurity, none of them comprehensively discuss AI's future impacts or provide concrete recommendations for navigating its safe and secure usage. This paper presents an in-depth analysis of frontier AI's impact on cybersecurity and establishes a systematic framework for risk assessment and mitigation. To this end, we first define and categorize the marginal risks of frontier AI in cybersecurity and then systemically analyze the current and future impacts of frontier AI in cybersecurity, qualitatively and quantitatively. We also discuss why frontier AI likely benefits attackers more than defenders in the short term from equivalence classes, asymmetry, and economic impact. Next, we explore frontier AI's impact on future software system development, including enabling complex hybrid systems while introducing new risks. Based on our findings, we provide security recommendations, including constructing fine-grained benchmarks for risk assessment, designing AI agents for defenses, building security mechanisms and provable defenses for hybrid systems, enhancing pre-deployment security testing and transparency, and strengthening defenses for users. Finally, we present long-term research questions essential for understanding AI's future impacts and unleashing its defensive capabilities.","authors":["Wenbo Guo","Yujin Potter","Tianneng Shi","Zhun Wang","Andy Zhang","Dawn Song"],"url":"https://arxiv.org/abs/2504.05408"}
{"created":"2025-04-16","title":"LLM$\\times$MapReduce-V2: Entropy-Driven Convolutional Test-Time Scaling for Generating Long-Form Articles from Extremely Long Resources","abstract":"Long-form generation is crucial for a wide range of practical applications, typically categorized into short-to-long and long-to-long generation. While short-to-long generations have received considerable attention, generating long texts from extremely long resources remains relatively underexplored. The primary challenge in long-to-long generation lies in effectively integrating and analyzing relevant information from extensive inputs, which remains difficult for current large language models (LLMs). In this paper, we propose LLM$\\times$MapReduce-V2, a novel test-time scaling strategy designed to enhance the ability of LLMs to process extremely long inputs. Drawing inspiration from convolutional neural networks, which iteratively integrate local features into higher-level global representations, LLM$\\times$MapReduce-V2 utilizes stacked convolutional scaling layers to progressively expand the understanding of input materials. Both quantitative and qualitative experimental results demonstrate that our approach substantially enhances the ability of LLMs to process long inputs and generate coherent, informative long-form articles, outperforming several representative baselines. Both LLM$\\times$MapReduce-V2 and SurveyEval are publicly available at https://github.com/thunlp/LLMxMapReduce .","authors":["Haoyu Wang","Yujia Fu","Zhu Zhang","Shuo Wang","Zirui Ren","Xiaorong Wang","Zhili Li","Chaoqun He","Bo An","Zhiyuan Liu","Maosong Sun"],"url":"https://arxiv.org/abs/2504.05732"}
{"created":"2025-04-16","title":"SEA-LION: Southeast Asian Languages in One Network","abstract":"Recently, Large Language Models (LLMs) have dominated much of the artificial intelligence scene with their ability to process and generate natural languages. However, the majority of LLM research and development remains English-centric, leaving low-resource languages such as those in the Southeast Asian (SEA) region under-represented. To address this representation gap, we introduce Llama-SEA-LION-v3-8B-IT and Gemma-SEA-LION-v3-9B-IT, two cutting-edge multilingual LLMs designed for SEA languages. The SEA-LION family of LLMs supports 11 SEA languages, namely English, Chinese, Indonesian, Vietnamese, Malay, Thai, Burmese, Lao, Filipino, Tamil, and Khmer. Our work leverages large-scale multilingual continued pre-training with a comprehensive post-training regime involving multiple stages of instruction fine-tuning, alignment, and model merging. Evaluation results on multilingual benchmarks indicate that our models achieve state-of-the-art performance across LLMs supporting SEA languages. We open-source the models to benefit the wider SEA community.","authors":["Raymond Ng","Thanh Ngan Nguyen","Yuli Huang","Ngee Chia Tai","Wai Yi Leong","Wei Qi Leong","Xianbin Yong","Jian Gang Ngui","Yosephine Susanto","Nicholas Cheng","Hamsawardhini Rengarajan","Peerat Limkonchotiwat","Adithya Venkatadri Hulagadri","Kok Wai Teng","Yeo Yeow Tong","Bryan Siow","Wei Yi Teo","Wayne Lau","Choon Meng Tan","Brandon Ong","Zhi Hao Ong","Jann Railey Montalan","Adwin Chan","Sajeban Antonyrex","Ren Lee","Esther Choa","David Ong Tat-Wee","Bing Jie Darius Liu","William Chandra Tjhi","Erik Cambria","Leslie Teo"],"url":"https://arxiv.org/abs/2504.05747"}
{"created":"2025-04-16","title":"PaMi-VDPO: Mitigating Video Hallucinations by Prompt-Aware Multi-Instance Video Preference Learning","abstract":"Direct Preference Optimization (DPO) helps reduce hallucinations in Video Multimodal Large Language Models (VLLMs), but its reliance on offline preference data limits adaptability and fails to capture true video-response misalignment. We propose Video Direct Preference Optimization (VDPO), an online preference learning framework that eliminates the need for preference annotation by leveraging video augmentations to generate rejected samples while keeping responses fixed. However, selecting effective augmentations is non-trivial, as some clips may be semantically identical to the original under specific prompts, leading to false rejections and disrupting alignment. To address this, we introduce Prompt-aware Multi-instance Learning VDPO (PaMi-VDPO), which selects augmentations based on prompt context. Instead of a single rejection, we construct a candidate set of augmented clips and apply a close-to-far selection strategy, initially ensuring all clips are semantically relevant while then prioritizing the most prompt-aware distinct clip. This allows the model to better capture meaningful visual differences, mitigating hallucinations, while avoiding false rejections, and improving alignment. PaMi-VDPOseamlessly integrates into existing VLLMs without additional parameters, GPT-4/human supervision. With only 10k SFT data, it improves the base model by 5.3% on VideoHallucer, surpassing GPT-4o, while maintaining stable performance on general video benchmarks.","authors":["Xinpeng Ding","Kui Zhang","Jianhua Han","Lanqing Hong","Hang Xu","Xiaomeng Li"],"url":"https://arxiv.org/abs/2504.05810"}
{"created":"2025-04-16","title":"Are Generative AI Agents Effective Personalized Financial Advisors?","abstract":"Large language model-based agents are becoming increasingly popular as a low-cost mechanism to provide personalized, conversational advice, and have demonstrated impressive capabilities in relatively simple scenarios, such as movie recommendations. But how do these agents perform in complex high-stakes domains, where domain expertise is essential and mistakes carry substantial risk? This paper investigates the effectiveness of LLM-advisors in the finance domain, focusing on three distinct challenges: (1) eliciting user preferences when users themselves may be unsure of their needs, (2) providing personalized guidance for diverse investment preferences, and (3) leveraging advisor personality to build relationships and foster trust. Via a lab-based user study with 64 participants, we show that LLM-advisors often match human advisor performance when eliciting preferences, although they can struggle to resolve conflicting user needs. When providing personalized advice, the LLM was able to positively influence user behavior, but demonstrated clear failure modes. Our results show that accurate preference elicitation is key, otherwise, the LLM-advisor has little impact, or can even direct the investor toward unsuitable assets. More worryingly, users appear insensitive to the quality of advice being given, or worse these can have an inverse relationship. Indeed, users reported a preference for and increased satisfaction as well as emotional trust with LLMs adopting an extroverted persona, even though those agents provided worse advice.","authors":["Takehiro Takayanagi","Kiyoshi Izumi","Javier Sanz-Cruzado","Richard McCreadie","Iadh Ounis"],"url":"https://arxiv.org/abs/2504.05862"}
{"created":"2025-04-16","title":"A Metropolis-Adjusted Langevin Algorithm for Sampling Jeffreys Prior","abstract":"Inference and estimation are fundamental aspects of statistics, system identification and machine learning. For most inference problems, prior knowledge is available on the system to be modeled, and Bayesian analysis is a natural framework to impose such prior information in the form of a prior distribution. However, in many situations, coming out with a fully specified prior distribution is not easy, as prior knowledge might be too vague, so practitioners prefer to use a prior distribution that is as `ignorant' or `uninformative' as possible, in the sense of not imposing subjective beliefs, while still supporting reliable statistical analysis. Jeffreys prior is an appealing uninformative prior because it offers two important benefits: (i) it is invariant under any re-parameterization of the model, (ii) it encodes the intrinsic geometric structure of the parameter space through the Fisher information matrix, which in turn enhances the diversity of parameter samples. Despite these benefits, drawing samples from Jeffreys prior is a challenging task. In this paper, we propose a general sampling scheme using the Metropolis-Adjusted Langevin Algorithm that enables sampling of parameter values from Jeffreys prior, and provide numerical illustrations of our approach through several examples.","authors":["Yibo Shi","Braghadeesh Lakshminarayanan","Cristian R. Rojas"],"url":"https://arxiv.org/abs/2504.06372"}
{"created":"2025-04-16","title":"CAFA: a Controllable Automatic Foley Artist","abstract":"Foley is a key element in video production, refers to the process of adding an audio signal to a silent video while ensuring semantic and temporal alignment. In recent years, the rise of personalized content creation and advancements in automatic video-to-audio models have increased the demand for greater user control in the process. One possible approach is to incorporate text to guide audio generation. While supported by existing methods, challenges remain in ensuring compatibility between modalities, particularly when the text introduces additional information or contradicts the sounds naturally inferred from the visuals. In this work, we introduce CAFA (Controllable Automatic Foley Artist) a video-and-text-to-audio model that generates semantically and temporally aligned audio for a given video, guided by text input. CAFA is built upon a text-to-audio model and integrates video information through a modality adapter mechanism. By incorporating text, users can refine semantic details and introduce creative variations, guiding the audio synthesis beyond the expected video contextual cues. Experiments show that besides its superior quality in terms of semantic alignment and audio-visual synchronization the proposed method enable high textual controllability as demonstrated in subjective and objective evaluations.","authors":["Roi Benita","Michael Finkelson","Tavi Halperin","Gleb Sterkin","Yossi Adi"],"url":"https://arxiv.org/abs/2504.06778"}
{"created":"2025-04-16","title":"GAAPO: Genetic Algorithmic Applied to Prompt Optimization","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, with their performance heavily dependent on the quality of input prompts. While prompt engineering has proven effective, it typically relies on manual adjustments, making it time-consuming and potentially suboptimal. This paper introduces GAAPO (Genetic Algorithm Applied to Prompt Optimization), a novel hybrid optimization framework that leverages genetic algorithm principles to evolve prompts through successive generations. Unlike traditional genetic approaches that rely solely on mutation and crossover operations, GAAPO integrates multiple specialized prompt generation strategies within its evolutionary framework. Through extensive experimentation on diverse datasets including ETHOS, MMLU-Pro, and GPQA, our analysis reveals several important point for the future development of automatic prompt optimization methods: importance of the tradeoff between the population size and the number of generations, effect of selection methods on stability results, capacity of different LLMs and especially reasoning models to be able to automatically generate prompts from similar queries... Furthermore, we provide insights into the relative effectiveness of different prompt generation strategies and their evolution across optimization phases. These findings contribute to both the theoretical understanding of prompt optimization and practical applications in improving LLM performance.","authors":["Xavier S\\'echeresse","Jacques-Yves Guilbert--Ly","Antoine Villedieu de Torcy"],"url":"https://arxiv.org/abs/2504.07157"}
{"created":"2025-04-16","title":"Cryptographic Strengthening of MST3 cryptosystem via Automorphism Group of Suzuki Function Fields","abstract":"The article describes a new implementation of MST3 cryptosystems based on the automorphism group of the Suzuki function field. The main difference in the presented implementation is to use the logarithmic signature for encryption not only in the center of the group, as in the well-known implementation of MST3 for Suzuki groups but also for coordinates outside the center of the group. The present implementation of a cryptosystem has higher reliability. The complexity of cryptanalysis and the size of the message for encryption squared is higher than that of the MST3 cryptosystem in the Suzuki group.","authors":["Gennady Khalimov","Yevgen Kotukh"],"url":"https://arxiv.org/abs/2504.07318"}
{"created":"2025-04-16","title":"Kimi-VL Technical Report","abstract":"We present Kimi-VL, an efficient open-source Mixture-of-Experts (MoE) vision-language model (VLM) that offers advanced multimodal reasoning, long-context understanding, and strong agent capabilities - all while activating only 2.8B parameters in its language decoder (Kimi-VL-A3B). Kimi-VL demonstrates strong performance across challenging domains: as a general-purpose VLM, Kimi-VL excels in multi-turn agent tasks (e.g., OSWorld), matching flagship models. Furthermore, it exhibits remarkable capabilities across diverse challenging vision language tasks, including college-level image and video comprehension, OCR, mathematical reasoning, and multi-image understanding. In comparative evaluations, it effectively competes with cutting-edge efficient VLMs such as GPT-4o-mini, Qwen2.5-VL-7B, and Gemma-3-12B-IT, while surpassing GPT-4o in several key domains. Kimi-VL also advances in processing long contexts and perceiving clearly. With a 128K extended context window, Kimi-VL can process diverse long inputs, achieving impressive scores of 64.5 on LongVideoBench and 35.1 on MMLongBench-Doc. Its native-resolution vision encoder, MoonViT, further allows it to see and understand ultra-high-resolution visual inputs, achieving 83.2 on InfoVQA and 34.5 on ScreenSpot-Pro, while maintaining lower computational cost for common tasks. Building upon Kimi-VL, we introduce an advanced long-thinking variant: Kimi-VL-Thinking. Developed through long chain-of-thought (CoT) supervised fine-tuning (SFT) and reinforcement learning (RL), this model exhibits strong long-horizon reasoning capabilities. It achieves scores of 61.7 on MMMU, 36.8 on MathVision, and 71.3 on MathVista while maintaining the compact 2.8B activated LLM parameters, setting a new standard for efficient multimodal thinking models. Code and models are publicly accessible at https://github.com/MoonshotAI/Kimi-VL.","authors":["Kimi Team","Angang Du","Bohong Yin","Bowei Xing","Bowen Qu","Bowen Wang","Cheng Chen","Chenlin Zhang","Chenzhuang Du","Chu Wei","Congcong Wang","Dehao Zhang","Dikang Du","Dongliang Wang","Enming Yuan","Enzhe Lu","Fang Li","Flood Sung","Guangda Wei","Guokun Lai","Han Zhu","Hao Ding","Hao Hu","Hao Yang","Hao Zhang","Haoning Wu","Haotian Yao","Haoyu Lu","Heng Wang","Hongcheng Gao","Huabin Zheng","Jiaming Li","Jianlin Su","Jianzhou Wang","Jiaqi Deng","Jiezhong Qiu","Jin Xie","Jinhong Wang","Jingyuan Liu","Junjie Yan","Kun Ouyang","Liang Chen","Lin Sui","Longhui Yu","Mengfan Dong","Mengnan Dong","Nuo Xu","Pengyu Cheng","Qizheng Gu","Runjie Zhou","Shaowei Liu","Sihan Cao","Tao Yu","Tianhui Song","Tongtong Bai","Wei Song","Weiran He","Weixiao Huang","Weixin Xu","Xiaokun Yuan","Xingcheng Yao","Xingzhe Wu","Xinxing Zu","Xinyu Zhou","Xinyuan Wang","Y. Charles","Yan Zhong","Yang Li","Yangyang Hu","Yanru Chen","Yejie Wang","Yibo Liu","Yibo Miao","Yidao Qin","Yimin Chen","Yiping Bao","Yiqin Wang","Yongsheng Kang","Yuanxin Liu","Yulun Du","Yuxin Wu","Yuzhi Wang","Yuzi Yan","Zaida Zhou","Zhaowei Li","Zhejun Jiang","Zheng Zhang","Zhilin Yang","Zhiqi Huang","Zihao Huang","Zijia Zhao","Ziwei Chen","Zongyu Lin"],"url":"https://arxiv.org/abs/2504.07491"}
{"created":"2025-04-16","title":"Deep Learning Based Service Composition in Integrated Aerial-Terrestrial Networks","abstract":"The explosive growth of user devices and emerging applications is driving unprecedented traffic demands, accompanied by stringent Quality of Service (QoS) requirements. Addressing these challenges necessitates innovative service orchestration methods capable of seamless integration across the edge-cloud continuum. Terrestrial network-based service orchestration methods struggle to deliver timely responses to growing traffic demands or support users with poor or lack of access to terrestrial infrastructure. Exploiting both aerial and terrestrial resources in service composition increases coverage and facilitates the use of full computing and communication potentials. This paper proposes a service placement and composition mechanism for integrated aerial-terrestrial networks over the edge-cloud continuum while considering the dynamic nature of the network. The service function placement and service orchestration are modeled in an optimization framework. Considering the dynamicity, the Aerial Base Station (ABS) trajectory might not be deterministic, and their mobility pattern might not be known as assumed knowledge. Also, service requests can traverse through access nodes due to users' mobility. By incorporating predictive algorithms, including Deep Reinforcement Learning (DRL) approaches, the proposed method predicts ABS locations and service requests. Subsequently, a heuristic isomorphic graph matching approach is proposed to enable efficient, latency-aware service orchestration. Simulation results demonstrate the efficiency of the proposed prediction and service composition schemes in terms of accuracy, cost optimization, scalability, and responsiveness, ensuring timely and reliable service delivery under diverse network conditions.","authors":["Mohammad Farhoudi","Masoud Shokrnezhad","Somayeh Kianpisheh","Tarik Taleb"],"url":"https://arxiv.org/abs/2504.07528"}
{"created":"2025-04-16","title":"SafeChat: A Framework for Building Trustworthy Collaborative Assistants and a Case Study of its Usefulness","abstract":"Collaborative assistants, or chatbots, are data-driven decision support systems that enable natural interaction for task completion. While they can meet critical needs in modern society, concerns about their reliability and trustworthiness persist. In particular, Large Language Model (LLM)-based chatbots like ChatGPT, Gemini, and DeepSeek are becoming more accessible. However, such chatbots have limitations, including their inability to explain response generation, the risk of generating problematic content, the lack of standardized testing for reliability, and the need for deep AI expertise and extended development times. These issues make chatbots unsuitable for trust-sensitive applications like elections or healthcare. To address these concerns, we introduce SafeChat, a general architecture for building safe and trustworthy chatbots, with a focus on information retrieval use cases. Key features of SafeChat include: (a) safety, with a domain-agnostic design where responses are grounded and traceable to approved sources (provenance), and 'do-not-respond' strategies to prevent harmful answers; (b) usability, with automatic extractive summarization of long responses, traceable to their sources, and automated trust assessments to communicate expected chatbot behavior, such as sentiment; and (c) fast, scalable development, including a CSV-driven workflow, automated testing, and integration with various devices. We implemented SafeChat in an executable framework using the open-source chatbot platform Rasa. A case study demonstrates its application in building ElectionBot-SC, a chatbot designed to safely disseminate official election information. SafeChat is being used in many domains, validating its potential, and is available at: https://github.com/ai4society/trustworthy-chatbot.","authors":["Biplav Srivastava","Kausik Lakkaraju","Nitin Gupta","Vansh Nagpal","Bharath C. Muppasani","Sara E. Jones"],"url":"https://arxiv.org/abs/2504.07995"}
{"created":"2025-04-16","title":"Improving Multiresource Job Scheduling with Markovian Service Rate Policies","abstract":"Modern cloud computing workloads are composed of multiresource jobs that require a variety of computational resources in order to run, such as CPU cores, memory, disk space, or hardware accelerators. A single cloud server can typically run many multiresource jobs in parallel, but only if the server has sufficient resources to satisfy the demands of every job. A scheduling policy must therefore select sets of multiresource jobs to run in parallel in order to minimize the mean response time across jobs -- the average time from when a job arrives to the system until it is completed. Unfortunately, achieving low response times by selecting sets of jobs that fully utilize the available server resources has proven to be a difficult problem.","authors":["Zhongrui Chen","Isaac Grosof","Benjamin Berg"],"url":"https://arxiv.org/abs/2504.08094"}
{"created":"2025-04-16","title":"DrivAer Transformer: A high-precision and fast prediction method for vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset","abstract":"At the current stage, deep learning-based methods have demonstrated excellent capabilities in evaluating aerodynamic performance, significantly reducing the time and cost required for traditional computational fluid dynamics (CFD) simulations. However, when faced with the task of processing extremely complex three-dimensional (3D) vehicle models, the lack of large-scale datasets and training resources, coupled with the inherent diversity and complexity of the geometry of different vehicle models, means that the prediction accuracy and versatility of these networks are still not up to the level required for current production. In view of the remarkable success of Transformer models in the field of natural language processing and their strong potential in the field of image processing, this study innovatively proposes a point cloud learning framework called DrivAer Transformer (DAT). The DAT structure uses the DrivAerNet++ dataset, which contains high-fidelity CFD data of industrial-standard 3D vehicle shapes. enabling accurate estimation of air drag directly from 3D meshes, thus avoiding the limitations of traditional methods such as 2D image rendering or signed distance fields (SDF). DAT enables fast and accurate drag prediction, driving the evolution of the aerodynamic evaluation process and laying the critical foundation for introducing a data-driven approach to automotive design. The framework is expected to accelerate the vehicle design process and improve development efficiency.","authors":["Jiaqi He","Xiangwen Luo","Yiping Wang"],"url":"https://arxiv.org/abs/2504.08217"}
{"created":"2025-04-16","title":"F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos","abstract":"Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a significant challenge in video analytics and multi-modal LLMs. Current methods struggle to identify events that satisfy all the F$^3$ criteria with high accuracy due to challenges such as motion blur and subtle visual discrepancies. To advance research in video understanding, we introduce F$^3$Set, a benchmark that consists of video datasets for precise F$^3$ event detection. Datasets in F$^3$Set are characterized by their extensive scale and comprehensive detail, usually encompassing over 1,000 event types with precise timestamps and supporting multi-level granularity. Currently, F$^3$Set contains several sports datasets, and this framework may be extended to other applications as well. We evaluated popular temporal action understanding methods on F$^3$Set, revealing substantial challenges for existing techniques. Additionally, we propose a new method, F$^3$ED, for F$^3$ event detections, achieving superior performance. The dataset, model, and benchmark code are available at https://github.com/F3Set/F3Set.","authors":["Zhaoyu Liu","Kan Jiang","Murong Ma","Zhe Hou","Yun Lin","Jin Song Dong"],"url":"https://arxiv.org/abs/2504.08222"}
{"created":"2025-04-16","title":"Large language models could be rote learners","abstract":"Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of learning and seek to disentangle genuine capability acquisition from superficial memorization in LLM evaluation. First, by analyzing model performance under different memorization conditions, we uncover a counterintuitive trend: LLMs perform worse on memorized MCQs than on non-memorized ones, indicating the coexistence of two distinct learning phenomena, i.e., rote memorization and genuine capability learning. To disentangle them, we propose TrinEval, a novel evaluation framework that reformulates MCQs into an alternative trinity format, reducing memorization while preserving knowledge assessment. Experiments validate TrinEval's effectiveness in reformulation, and its evaluation reveals that common LLMs may memorize by rote 20.5% of knowledge points (in MMLU on average).","authors":["Yuyang Xu","Renjun Hu","Haochao Ying","Jian Wu","Xing Shi","Wei Lin"],"url":"https://arxiv.org/abs/2504.08300"}
{"created":"2025-04-16","title":"Efficient Architecture for RISC-V Vector Memory Access","abstract":"Vector processors frequently suffer from inefficient memory accesses, particularly for strided and segment patterns. While coalescing strided accesses is a natural solution, effectively gathering or scattering elements at fixed strides remains challenging. Naive approaches rely on high-overhead crossbars that remap any byte between memory and registers, leading to physical design issues. Segment operations require row-column transpositions, typically handled using either element-level in-place transposition (degrading performance) or large buffer-based bulk transposition (incurring high area overhead). In this paper, we present EARTH, a novel vector memory access architecture designed to overcome these challenges through shifting-based optimizations. For strided accesses, EARTH integrates specialized shift networks for gathering and scattering elements. After coalescing multiple accesses within the same cache line, data is routed between memory and registers through the shifting network with minimal overhead. For segment operations, EARTH employs a shifted register bank enabling direct column-wise access, eliminating dedicated segment buffers while providing high-performance, in-place bulk transposition. Implemented on FPGA with Chisel HDL based on an open-source RISC-V vector unit, EARTH enhances performance for strided memory accesses, achieving 4x-8x speedups in benchmarks dominated by strided operations. Compared to conventional designs, EARTH reduces hardware area by 9% and power consumption by 41%, significantly advancing both performance and efficiency of vector processors.","authors":["Hongyi Guan","Yichuan Gao","Chenlu Miao","Haoyang Wu","Hang Zhu","Mingfeng Lin","Huayue Liang"],"url":"https://arxiv.org/abs/2504.08334"}
{"created":"2025-04-16","title":"Light-YOLOv8-Flame: A Lightweight High-Performance Flame Detection Algorithm","abstract":"Fire detection algorithms, particularly those based on computer vision, encounter significant challenges such as high computational costs and delayed response times, which hinder their application in real-time systems. To address these limitations, this paper introduces Light-YOLOv8-Flame, a lightweight flame detection algorithm specifically designed for fast and efficient real-time deployment. The proposed model enhances the YOLOv8 architecture through the substitution of the original C2f module with the FasterNet Block module. This new block combines Partial Convolution (PConv) and Convolution (Conv) layers, reducing both computational complexity and model size. A dataset comprising 7,431 images, representing both flame and non-flame scenarios, was collected and augmented for training purposes. Experimental findings indicate that the modified YOLOv8 model achieves a 0.78% gain in mean average precision (mAP) and a 2.05% boost in recall, while reducing the parameter count by 25.34%, with only a marginal decrease in precision by 0.82%. These findings highlight that Light-YOLOv8-Flame offers enhanced detection performance and speed, making it well-suited for real-time fire detection on resource-constrained devices.","authors":["Jiawei Lan","Ye Tao","Zhibiao Wang","Haoyang Yu","Wenhua Cui"],"url":"https://arxiv.org/abs/2504.08389"}
{"created":"2025-04-16","title":"COP-GEN-Beta: Unified Generative Modelling of COPernicus Imagery Thumbnails","abstract":"In remote sensing, multi-modal data from various sensors capturing the same scene offers rich opportunities, but learning a unified representation across these modalities remains a significant challenge. Traditional methods have often been limited to single or dual-modality approaches. In this paper, we introduce COP-GEN-Beta, a generative diffusion model trained on optical, radar, and elevation data from the Major TOM dataset. What sets COP-GEN-Beta apart is its ability to map any subset of modalities to any other, enabling zero-shot modality translation after training. This is achieved through a sequence-based diffusion transformer, where each modality is controlled by its own timestep embedding. We extensively evaluate COP-GEN-Beta on thumbnail images from the Major TOM dataset, demonstrating its effectiveness in generating high-quality samples. Qualitative and quantitative evaluations validate the model's performance, highlighting its potential as a powerful pre-trained model for future remote sensing tasks.","authors":["Miguel Espinosa","Valerio Marsocci","Yuru Jia","Elliot J. Crowley","Mikolaj Czerkawski"],"url":"https://arxiv.org/abs/2504.08548"}
{"created":"2025-04-16","title":"Analyzing 16,193 LLM Papers for Fun and Profits","abstract":"Large Language Models (LLMs) are reshaping the landscape of computer science research, driving significant shifts in research priorities across diverse conferences and fields. This study provides a comprehensive analysis of the publication trend of LLM-related papers in 77 top-tier computer science conferences over the past six years (2019-2024). We approach this analysis from four distinct perspectives: (1) We investigate how LLM research is driving topic shifts within major conferences. (2) We adopt a topic modeling approach to identify various areas of LLM-related topic growth and reveal the topics of concern at different conferences. (3) We explore distinct contribution patterns of academic and industrial institutions. (4) We study the influence of national origins on LLM development trajectories. Synthesizing the findings from these diverse analytical angles, we derive ten key insights that illuminate the dynamics and evolution of the LLM research ecosystem.","authors":["Zhiqiu Xia","Lang Zhu","Bingzhe Li","Feng Chen","Qiannan Li","Chunhua Liao","Feiyi Wang","Hang Liu"],"url":"https://arxiv.org/abs/2504.08619"}
{"created":"2025-04-16","title":"Task-conditioned Ensemble of Expert Models for Continuous Learning","abstract":"One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed model with new data could be one remedy. However, the question arises as to how we should update the model with new training data so that it retains its accuracy on the old data while adapting to the new data. In this work, we propose a task-conditioned ensemble of models to maintain the performance of the existing model. The method involves an ensemble of expert models based on task membership information. The in-domain models-based on the local outlier concept (different from the expert models) provide task membership information dynamically at run-time to each probe sample. To evaluate the proposed method, we experiment with three setups: the first represents distribution shift between tasks (LivDet-Iris-2017), the second represents distribution shift both between and within tasks (LivDet-Iris-2020), and the third represents disjoint distribution between tasks (Split MNIST). The experiments highlight the benefits of the proposed method. The source code is available at https://github.com/iPRoBe-lab/Continuous_Learning_FE_DM.","authors":["Renu Sharma","Debasmita Pal","Arun Ross"],"url":"https://arxiv.org/abs/2504.08626"}
{"created":"2025-04-16","title":"Designing Child-Friendly AI Interfaces: Six Developmentally-Appropriate Design Insights from Analysing Disney Animation","abstract":"To build AI interfaces that children can intuitively understand and use, designers need a design grammar that truly serves children's developmental needs. This paper bridges Artificial Intelligence design for children -- an emerging field still defining its best practices -- and children's animation, a well-established field with decades of experience in engaging young viewers through emotionally resonant, cognitively accessible storytelling. Pairing Piagetian developmental theory with design pattern extraction from 52 works of Disney animation, the paper presents six design insights transferable to child-centred AI interface design: (1) emotional expressiveness and visual clarity, (2) musical and auditory scaffolding, (3) audiovisual synchrony for emotional comfort, (4) sidekick-style personas, (5) support for symbolic play and imaginative exploration, and (6) predictable and scaffolded interaction structures. These strategies -- long refined in Disney animation -- function as multimodal scaffolds for attention, understanding, and emotional attunement, thereby forming a structured design grammar familiar to children and transferable to AI interface design. By reframing cinematic storytelling as design logic for AI, the paper offers heuristics for crafting intuitive AI interfaces that align with children's cognitive stages and emotional needs. The work contributes to design theory by showing how sensory, affective and narrative techniques can inform developmentally attuned AI design for children. Future directions include empirical testing, cultural adaptation, and participatory co-design.","authors":["Nomisha Kurian"],"url":"https://arxiv.org/abs/2504.08670"}
{"created":"2025-04-16","title":"SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents","abstract":"Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at: https://github.com/amazon-science/SWE-PolyBench","authors":["Muhammad Shihab Rashid","Christian Bock","Yuan Zhuang","Alexander Buccholz","Tim Esler","Simon Valentin","Luca Franceschi","Martin Wistuba","Prabhu Teja Sivaprasad","Woo Jung Kim","Anoop Deoras","Giovanni Zappella","Laurent Callot"],"url":"https://arxiv.org/abs/2504.08703"}
{"created":"2025-04-16","title":"Towards Personalized Conversational Sales Agents : Contextual User Profiling for Strategic Action","abstract":"Conversational Recommender Systems (CRSs) aim to engage users in dialogue to provide tailored recommendations. While traditional CRSs focus on eliciting preferences and retrieving items, real-world e-commerce interactions involve more complex decision-making, where users consider multiple factors beyond simple attributes. To bridge this gap, we introduce Conversational Sales (CSales), a novel task that unifies preference elicitation, recommendation, and persuasion to better support user decision-making. For a realistic evaluation of CSales, we present CSUser, an LLM-based user simulator constructed from real-world data, modeling diverse user profiles with needs and personalities. Additionally, we propose CSI, a conversational sales agent that proactively infers contextual profiles through dialogue for personalized action planning. Extensive experiments demonstrate that CSUser effectively replicates real-world users and emphasize the importance of contextual profiling for strategic action selection, ultimately driving successful purchases in e-commerce.","authors":["Tongyoung Kim","Jeongeun Lee","Soojin Yoon","Seonghwan Kim","Dongha Lee"],"url":"https://arxiv.org/abs/2504.08754"}
{"created":"2025-04-16","title":"BlockGaussian: Efficient Large-Scale Scene Novel View Synthesis via Adaptive Block-Based Gaussian Splatting","abstract":"The recent advancements in 3D Gaussian Splatting (3DGS) have demonstrated remarkable potential in novel view synthesis tasks. The divide-and-conquer paradigm has enabled large-scale scene reconstruction, but significant challenges remain in scene partitioning, optimization, and merging processes. This paper introduces BlockGaussian, a novel framework incorporating a content-aware scene partition strategy and visibility-aware block optimization to achieve efficient and high-quality large-scale scene reconstruction. Specifically, our approach considers the content-complexity variation across different regions and balances computational load during scene partitioning, enabling efficient scene reconstruction. To tackle the supervision mismatch issue during independent block optimization, we introduce auxiliary points during individual block optimization to align the ground-truth supervision, which enhances the reconstruction quality. Furthermore, we propose a pseudo-view geometry constraint that effectively mitigates rendering degradation caused by airspace floaters during block merging. Extensive experiments on large-scale scenes demonstrate that our approach achieves state-of-the-art performance in both reconstruction efficiency and rendering quality, with a 5x speedup in optimization and an average PSNR improvement of 1.21 dB on multiple benchmarks. Notably, BlockGaussian significantly reduces computational requirements, enabling large-scale scene reconstruction on a single 24GB VRAM device. The project page is available at https://github.com/SunshineWYC/BlockGaussian","authors":["Yongchang Wu","Zipeng Qi","Zhenwei Shi","Zhengxia Zou"],"url":"https://arxiv.org/abs/2504.09048"}
{"created":"2025-04-16","title":"GAMA: High-Performance GEMM Acceleration on AMD Versal ML-Optimized AI Engines","abstract":"General matrix-matrix multiplication (GEMM) is a fundamental operation in machine learning (ML) applications. We present the first comprehensive performance acceleration of GEMM workloads on AMD's second-generation AIE-ML (AIE2) architecture, which is specifically optimized for ML applications. Compared to AI-Engine (AIE1), AIE offers increased compute throughput and larger on-chip memory capacity. We propose a novel design that maximizes AIE memory utilization, incorporates custom buffer placement within the AIE2 and staggered kernel placement across the AIE2 array, significantly reducing performance bottlenecks such as memory stalls and routing congestion, resulting in improved performance and efficiency compared to the default compiler provided by AMD. We evaluate the performance benefits of our design at three levels: single AIE, pack of AIEs and the complete AIE array. GAMA achieves state-of-the-art performance, delivering up to 165 TOPS (85% of peak) for int8 precision and 83 TBFLOPS (86% of peak) for bfloat16 precision GEMM workloads. Our solution achieves 8.7%, 9%, 39% and 53.6% higher peak throughput efficiency compared to the state-of-the-art AIE1 frameworks AMA, MAXEVA, ARIES and CHARM, respectively.","authors":["Kaustubh Mhatre","Endri Taka","Aman Arora"],"url":"https://arxiv.org/abs/2504.09688"}
{"created":"2025-04-16","title":"Connecting 3-manifold triangulations with unimodal sequences of elementary moves","abstract":"A key result in computational 3-manifold topology is that any two triangulations of the same 3-manifold are connected by a finite sequence of bistellar flips, also known as Pachner moves. One limitation of this result is that little is known about the structure of this sequence; knowing more about the structure could help both proofs and algorithms. Motivated by this, we consider sequences of moves that are \"unimodal\" in the sense that they break up into two parts: first, a sequence that monotonically increases the size of the triangulation; and second, a sequence that monotonically decreases the size. We prove that any two one-vertex triangulations of the same 3-manifold, each with at least two tetrahedra, are connected by a unimodal sequence of 2-3 and 2-0 moves. We also study the practical utility of unimodal sequences; specifically, we implement an algorithm to find such sequences, and use this algorithm to perform some detailed computational experiments.","authors":["Benjamin A. Burton","Alexander He"],"url":"https://arxiv.org/abs/2012.02398"}
{"created":"2025-04-16","title":"Algorithmic correspondence and analytic rules","abstract":"We introduce the algorithm MASSA which takes classical modal formulas in input, and, when successful, effectively generates: (a) (analytic) geometric rules of the labelled calculus G3K, and (b) cut-free derivations (of a certain `canonical' shape) of each given input formula in the geometric labelled calculus obtained by adding the rule in output to G3K. We show that MASSA successfully terminates whenever its input formula is a (definite) analytic inductive formula, in which case, the geometric axiom corresponding to the output rule is, modulo logical equivalence, the first-order correspondent of the input formula. In proving the correctness of MASSA, we also show that the algorithm for the elimination of second-order quantifiers SCAN is complete with respect to the class of inductive analytic formulas. Finally, we show how our algorithm can be extended to the class of inductive formulas and to modal logic with quantifiers.","authors":["Andrea De Domenico","Giuseppe Greco","Alessandra Palmigiano"],"url":"https://arxiv.org/abs/2203.14147"}
{"created":"2025-04-16","title":"Classical Distributive Restriction Categories","abstract":"In the category of sets and partial functions, $\\mathsf{PAR}$, while the disjoint union $\\sqcup$ is the usual categorical coproduct, the Cartesian product $\\times$ becomes a restriction categorical analogue of the categorical product: a restriction product. Nevertheless, $\\mathsf{PAR}$ does have a usual categorical product as well in the form $A \\& B := A \\sqcup B \\sqcup (A \\times B)$. Surprisingly, asking that a distributive restriction category (a restriction category with restriction products $\\times$ and coproducts $\\oplus$) has $A \\& B$ a categorical product is enough to imply that the category is a classical restriction category. This is a restriction category which has joins and relative complements and, thus, supports classical Boolean reasoning. The first and main observation of the paper is that a distributive restriction category is classical if and only if $A \\& B := A \\oplus B \\oplus (A \\times B)$ is a categorical product in which case we call $\\&$ the ''classical'' product.","authors":["Robin Cockett","Jean-Simon Pacaud Lemay"],"url":"https://arxiv.org/abs/2305.16524"}
{"created":"2025-04-16","title":"Interval Decomposition of Persistence Modules over a Principal Ideal Domain","abstract":"The study of persistent homology has contributed new insights and perspectives into a variety of interesting problems in science and engineering. Work in this domain relies on the result that any finitely-indexed persistence module of finite-dimensional vector spaces admits an interval decomposition -- that is, a decomposition as a direct sum of simpler components called interval modules. This result fails if we replace vector spaces with modules over more general coefficient rings.","authors":["Jiajie Luo","Gregory Henselman-Petrusek"],"url":"https://arxiv.org/abs/2310.07971"}
{"created":"2025-04-16","title":"The Existence and Structure of Universal Partial Cycles","abstract":"A universal partial cycle (or upcycle) for $\\mathcal{A}^n$ is a cyclic sequence that covers each word of length $n$ over the alphabet $\\mathcal{A}$ exactly once -- like a De Bruijn cycle, except that we also allow a wildcard symbol $\\mathord{\\diamond}$ that can represent any letter of $\\mathcal{A}$. Chen et al. in 2017 and Goeckner et al. in 2018 showed that the existence and structure of upcycles are highly constrained, unlike those of De Bruijn cycles, which exist for every alphabet size and word length. Moreover, it was not known whether any upcycles existed for $n \\ge 5$. We present several examples of upcycles over both binary and non-binary alphabets for $n = 8$. We generalize two graph-theoretic representations of De Bruijn cycles to upcycles. We then introduce novel approaches to constructing new upcycles from old ones. Notably, given any upcycle for an alphabet of size $a$, we show how to construct an upcycle for an alphabet of size $ak$ for any $k \\in \\mathbb{N}$, so each example generates an infinite family of upcycles. We also define folds and lifts of upcycles, which relate upcycles with differing densities of $\\mathord{\\diamond}$ characters. In particular, we show that every upcycle lifts to a De Bruijn cycle. Our constructions rely on a different generalization of De Bruijn cycles known as perfect necklaces, and we introduce several new examples of perfect necklaces. We extend the definitions of certain pseudorandomness properties to partial words and determine which are satisfied by all upcycles, then draw a conclusion about linear feedback shift registers. Finally, we prove new nonexistence results based on the word length $n$, alphabet size, and $\\mathord{\\diamond}$ density.","authors":["Dylan Fillmore","Bennet Goeckner","Rachel Kirsch","Kirin Martin","Daniel McGinnis"],"url":"https://arxiv.org/abs/2310.13067"}
{"created":"2025-04-16","title":"Simplifying modular lattices by removing doubly irreducible elements","abstract":"Lattices are simplified by removing some of their doubly irreducible elements, resulting in smaller lattices called racks. All vertically indecomposable modular racks of $n \\le 40$ elements are listed, and the numbers of all modular lattices of $n \\le 40$ elements are obtained by P\\'olya counting. SageMath code is provided that allows easy access both to the listed racks, and to the modular lattices that were not listed. More than 3000-fold savings in storage space are demonstrated.","authors":["Jukka Kohonen"],"url":"https://arxiv.org/abs/2311.16643"}
{"created":"2025-04-16","title":"Point Processes and spatial statistics in time-frequency analysis","abstract":"A finite-energy signal is represented by a square-integrable, complex-valued function $t\\mapsto s(t)$ of a real variable $t$, interpreted as time. Similarly, a noisy signal is represented by a random process. Time-frequency analysis, a subfield of signal processing, amounts to describing the temporal evolution of the frequency content of a signal. Loosely speaking, if $s$ is the audio recording of a musical piece, time-frequency analysis somehow consists in writing the musical score of the piece. Mathematically, the operation is performed through a transform $\\mathcal{V}$, mapping $s \\in L^2(\\mathbb{R})$ onto a complex-valued function $\\mathcal{V}s \\in L^2(\\mathbb{R}^2)$ of time $t$ and angular frequency $\\omega$. The squared modulus $(t, \\omega) \\mapsto \\vert\\mathcal{V}s(t,\\omega)\\vert^2$ of the time-frequency representation is known as the spectrogram of $s$; in the musical score analogy, a peaked spectrogram at $(t_0,\\omega_0)$ corresponds to a musical note at angular frequency $\\omega_0$ localized at time $t_0$. More generally, the intuition is that upper level sets of the spectrogram contain relevant information about in the original signal. Hence, many signal processing algorithms revolve around identifying maxima of the spectrogram. In contrast, zeros of the spectrogram indicate perfect silence, that is, a time at which a particular frequency is absent. Assimilating $\\mathbb{R}^2$ to $\\mathbb{C}$ through $z = \\omega + \\mathrm{i}t$, this chapter focuses on time-frequency transforms $\\mathcal{V}$ that map signals to analytic functions. The zeros of the spectrogram of a noisy signal are then the zeros of a random analytic function, hence forming a Point Process in $\\mathbb{C}$. This chapter is devoted to the study of these Point Processes, to their links with zeros of Gaussian Analytic Functions, and to designing signal detection and denoising algorithms using spatial statistics.","authors":["Barbara Pascal","R\\'emi Bardenet"],"url":"https://arxiv.org/abs/2402.19172"}
{"created":"2025-04-16","title":"Posterior and variational inference for deep neural networks with heavy-tailed weights","abstract":"We consider deep neural networks in a Bayesian framework with a prior distribution sampling the network weights at random. Following a recent idea of Agapiou and Castillo (2023), who show that heavy-tailed prior distributions achieve automatic adaptation to smoothness, we introduce a simple Bayesian deep learning prior based on heavy-tailed weights and ReLU activation. We show that the corresponding posterior distribution achieves near-optimal minimax contraction rates, simultaneously adaptive to both intrinsic dimension and smoothness of the underlying function, in a variety of contexts including nonparametric regression, geometric data and Besov spaces. While most works so far need a form of model selection built-in within the prior distribution, a key aspect of our approach is that it does not require to sample hyperparameters to learn the architecture of the network. We also provide variational Bayes counterparts of the results, that show that mean-field variational approximations still benefit from near-optimal theoretical support.","authors":["Isma\\\"el Castillo","Paul Egels"],"url":"https://arxiv.org/abs/2406.03369"}
{"created":"2025-04-16","title":"Epidemic-induced local awareness behavior inferred from surveys and genetic sequence data","abstract":"Behavior-disease models suggest that pandemics can be contained cost-effectively if individuals take preventive actions when disease prevalence rises among their close contacts. However, assessing local awareness behavior in real-world datasets remains a challenge. Through the analysis of mutation patterns in clinical genetic sequence data, we propose an efficient approach to quantify the impact of local awareness by identifying superspreading events and assigning containment scores to them.","authors":["Gergely \\'Odor","M\\'arton Karsai"],"url":"https://arxiv.org/abs/2406.09983"}
{"created":"2025-04-16","title":"QOS: A Quantum Operating System","abstract":"Quantum computers face challenges due to hardware constraints, noise errors, and heterogeneity, and face fundamental design tradeoffs between key performance metrics such as \\textit{quantum fidelity} and system utilization. This substantially complicates managing quantum resources to scale the size and number of quantum algorithms that can be executed reliably in a given time.","authors":["Emmanouil Giortamis","Francisco Rom\\~ao","Nathaniel Tornow","Pramod Bhatotia"],"url":"https://arxiv.org/abs/2406.19120"}
{"created":"2025-04-16","title":"Multicriteria Optimization and Decision Making: Principles, Algorithms and Case Studies","abstract":"Real-world decision and optimization problems, often involve constraints and conflicting criteria. For example, choosing a travel method must balance speed, cost, environmental footprint, and convenience. Similarly, designing an industrial process must consider safety, environmental impact, and cost efficiency. Ideal solutions where all objectives are optimally met are rare; instead, we seek good compromises and aim to avoid lose-lose scenarios. Multicriteria optimization offers computational techniques to compute Pareto optimal solutions, aiding decision analysis and decision making. This reader offers an introduction to this topic and has been developed on the basis of the revised edition of the reader for the MSc computer science course \"Multicriteria Optimization and Decision Analysis\" at the Leiden Institute of Advanced Computer Science, Leiden University, The Netherlands. This course was taught annually by the first author from 2007 to 2023 as a single semester course with lectures and practicals. Our aim was to make the material accessible to MSc students who do not study mathematics as their core discipline by introducing basic numerical analysis concepts when necessary and providing numerical examples for interesting cases. The introduction is organized in a unique didactic manner developed by the authors, starting from more simple concepts such as linear programming and single-point methods, and advancing from these to more difficult concepts such as optimality conditions for nonlinear optimization and set-oriented solution algorithms. Besides, we focus on the mathematical modeling and foundations rather than on specific algorithms, though not excluding the discussion of some representative examples of solution algorithms.","authors":["Michael Emmerich","Andr\\'e Deutz"],"url":"https://arxiv.org/abs/2407.00359"}
{"created":"2025-04-16","title":"Topology-enhanced machine learning model (Top-ML) for anticancer peptide prediction","abstract":"Recently, therapeutic peptides have demonstrated great promise for cancer treatment. To explore powerful anticancer peptides, artificial intelligence (AI)-based approaches have been developed to systematically screen potential candidates. However, the lack of efficient featurization of peptides has become a bottleneck for these machine-learning models. In this paper, we propose a topology-enhanced machine learning model (Top-ML) for anticancer peptides prediction. Our Top-ML employs peptide topological features derived from its sequence \"connection\" information characterized by vector and spectral descriptors. Our Top-ML model, employing an Extra-Trees classifier, has been validated on the AntiCP 2.0 and mACPpred 2.0 benchmark datasets, achieving state-of-the-art performance or results comparable to existing deep learning models, while providing greater interpretability. Our results highlight the potential of leveraging novel topology-based featurization to accelerate the identification of anticancer peptides.","authors":["Joshua Zhi En Tan","JunJie Wee","Xue Gong","Kelin Xia"],"url":"https://arxiv.org/abs/2407.08974"}
{"created":"2025-04-16","title":"Over-the-Air Diagnosis of Defective Elements in Intelligent Reflecting Surface","abstract":"Due to circuit failures, defective elements that cannot adaptively adjust the phase shifts of their impinging signals in a desired manner may exist on an intelligent reflecting surface (IRS). Traditional way to locate these defective IRS elements requires a thorough diagnosis of all the circuits belonging to a huge number of IRS elements, which is practically challenging. In this paper, we will devise novel approaches under which a transmitter sends known pilot signals and a receiver localizes all the defective IRS elements just based on its over-the-air measurements reflected from the IRS. Specifically, given any set of IRS elements, we propose an efficient method to process the received signals to determine whether this cluster contains defective elements or not with a very high accuracy probability. Based on this method, we show that the over-the-air diagnosis problem belongs to the 20 questions problem, where we can adaptively change the query set at the IRS so as to localize all the defective elements as quickly as possible. Along this line, we first propose a sorted posterior matching (sortPM) based method according to the noisy 20 questions technique, which enables accurate diagnosis even if the answers about the existence of defective elements in some sets of interest are wrong at certain question and answer (Q&amp;A) rounds due to the noisy received signals. Next, to reduce the complexity, we propose a bisection based method according to the noiseless 20 questions technique, which totally trusts the answer at each Q&amp;A round and keeps removing half of the remaining region based on such answers. Via numerical results, we show that our proposed methods can exploit the over-the-air measurements to localize all the defective IRS elements quickly and accurately.","authors":["Ziyi Zhao","Zhaorui Wang","Lin Zhou","Chunsong Sun","Shuowen Zhang","Naofal Al-Dhahir","Liang Liu"],"url":"https://arxiv.org/abs/2408.00379"}
{"created":"2025-04-16","title":"A Policy Iteration Method for Inverse Mean Field Games","abstract":"We propose a policy iteration method to solve an inverse problem for a mean-field game (MFG) model, specifically to reconstruct the obstacle function in the game from the partial observation data of value functions, which represent the optimal costs for agents. The proposed approach decouples this complex inverse problem, which is an optimization problem constrained by a coupled nonlinear forward and backward PDE system in the MFG, into several iterations of solving linear PDEs and linear inverse problems. This method can also be viewed as a fixed-point iteration that simultaneously solves the MFG system and inversion. We prove its linear rate of convergence. In addition, numerical examples in 1D and 2D, along with performance comparisons to a direct least-squares method, demonstrate the superior efficiency and accuracy of the proposed method for solving inverse MFGs.","authors":["Kui Ren","Nathan Soedjak","Shanyin Tong"],"url":"https://arxiv.org/abs/2409.06184"}
{"created":"2025-04-16","title":"OmniXAS: A Universal Deep-Learning Framework for Materials X-ray Absorption Spectra","abstract":"X-ray absorption spectroscopy (XAS) is a powerful characterization technique for probing the local chemical environment of absorbing atoms. However, analyzing XAS data presents significant challenges, often requiring extensive, computationally intensive simulations, as well as significant domain expertise. These limitations hinder the development of fast, robust XAS analysis pipelines that are essential in high-throughput studies and for autonomous experimentation. We address these challenges with OmniXAS, a framework that contains a suite of transfer learning approaches for XAS prediction, each contributing to improved accuracy and efficiency, as demonstrated on K-edge spectra database covering eight 3d transition metals (Ti-Cu). The OmniXAS framework is built upon three distinct strategies. First, we use M3GNet to derive latent representations of the local chemical environment of absorption sites as input for XAS prediction, achieving up to order-of-magnitude improvements over conventional featurization techniques. Second, we employ a hierarchical transfer learning strategy, training a universal multi-task model across elements before fine-tuning for element-specific predictions. Models based on this cascaded approach after element-wise fine-tuning outperform element-specific models by up to 69%. Third, we implement cross-fidelity transfer learning, adapting a universal model to predict spectra generated by simulation of a different fidelity with a higher computational cost. This approach improves prediction accuracy by up to 11% over models trained on the target fidelity alone. Our approach boosts the throughput of XAS modeling by orders of magnitude versus first-principles simulations and is extendable to XAS prediction for a broader range of elements. This transfer learning framework is generalizable to enhance deep-learning models that target other properties in materials research.","authors":["Shubha R. Kharel","Fanchen Meng","Xiaohui Qu","Matthew R. Carbone","Deyu Lu"],"url":"https://arxiv.org/abs/2409.19552"}
{"created":"2025-04-16","title":"Second-Order Min-Max Optimization with Lazy Hessians","abstract":"This paper studies second-order methods for convex-concave minimax optimization. Monteiro and Svaiter (2012) proposed a method to solve the problem with an optimal iteration complexity of $\\mathcal{O}(\\epsilon^{-3/2})$ to find an $\\epsilon$-saddle point. However, it is unclear whether the computational complexity, $\\mathcal{O}((N+ d^2) d \\epsilon^{-2/3})$, can be improved. In the above, we follow Doikov et al. (2023) and assume the complexity of obtaining a first-order oracle as $N$ and the complexity of obtaining a second-order oracle as $dN$. In this paper, we show that the computation cost can be reduced by reusing Hessian across iterations. Our methods take the overall computational complexity of $ \\tilde{\\mathcal{O}}( (N+d^2)(d+ d^{2/3}\\epsilon^{-2/3}))$, which improves those of previous methods by a factor of $d^{1/3}$. Furthermore, we generalize our method to strongly-convex-strongly-concave minimax problems and establish the complexity of $\\tilde{\\mathcal{O}}((N+d^2) (d + d^{2/3} \\kappa^{2/3}) )$ when the condition number of the problem is $\\kappa$, enjoying a similar speedup upon the state-of-the-art method. Numerical experiments on both real and synthetic datasets also verify the efficiency of our method.","authors":["Lesi Chen","Chengchang Liu","Jingzhao Zhang"],"url":"https://arxiv.org/abs/2410.09568"}
{"created":"2025-04-16","title":"Code Drift: Towards Idempotent Neural Audio Codecs","abstract":"Neural codecs have demonstrated strong performance in high-fidelity compression of audio signals at low bitrates. The token-based representations produced by these codecs have proven particularly useful for generative modeling. While much research has focused on improvements in compression ratio and perceptual transparency, recent works have largely overlooked another desirable codec property -- idempotence, the stability of compressed outputs under multiple rounds of encoding. We find that state-of-the-art neural codecs exhibit varied degrees of idempotence, with some degrading audio outputs significantly after as few as three encodings. We investigate possible causes of low idempotence and devise a method for improving idempotence through fine-tuning a codec model. We then examine the effect of idempotence on a simple conditional generative modeling task, and find that increased idempotence can be achieved without negatively impacting downstream modeling performance -- potentially extending the usefulness of neural codecs for practical file compression and iterative generative modeling workflows.","authors":["Patrick O'Reilly","Prem Seetharaman","Jiaqi Su","Zeyu Jin","Bryan Pardo"],"url":"https://arxiv.org/abs/2410.11025"}
{"created":"2025-04-16","title":"Revocable Encryption, Programs, and More: The Case of Multi-Copy Security","abstract":"Fundamental principles of quantum mechanics have inspired many new research directions, particularly in quantum cryptography. One such principle is quantum no-cloning which has led to the emerging field of revocable cryptography. Roughly speaking, in a revocable cryptographic primitive, a cryptographic object (such as a ciphertext or program) is represented as a quantum state in such a way that surrendering it effectively translates into losing the capability to use this cryptographic object. All of the revocable cryptographic systems studied so far have a major drawback: the recipient only receives one copy of the quantum state. Worse yet, the schemes become completely insecure if the recipient receives many identical copies of the same quantum state -- a property that is clearly much more desirable in practice. While multi-copy security has been extensively studied for a number of other quantum cryptographic primitives, it has so far received only little treatment in context of unclonable primitives. Our work, for the first time, shows the feasibility of revocable primitives, such as revocable encryption and revocable programs, which satisfy multi-copy security in oracle models. This suggest that the stronger notion of multi-copy security is within reach in unclonable cryptography more generally, and therefore could lead to a new research direction in the field.","authors":["Prabhanjan Ananth","Saachi Mutreja","Alexander Poremba"],"url":"https://arxiv.org/abs/2410.13163"}
{"created":"2025-04-16","title":"Fully smooth one shot multipartite soft covering of quantum states without pairwise independence","abstract":"We provide a powerful machinery to prove fully smooth one shot multipartite covering, aka convex split, type results for quantum states. In the important case of smooth multipartite convex split for classical quantum states, aka smooth multipartite soft covering, our machinery works even when certain marginals of these states do not satisfy pairwise independence. The recent paper (arXiv:2410.17893) gave the first proof of fully smooth multipartite convex split by simplifying and extending a technique called telescoping, developed originally for convex split by (arXiv:2304.12056). However, that work as well as all earlier works on convex split assumed pairwise or even more independence amongst suitable marginals of the quantum states.","authors":["Pranab Sen"],"url":"https://arxiv.org/abs/2410.17908"}
{"created":"2025-04-16","title":"The Learning Stabilizers with Noise problem","abstract":"Random classical codes have good error correcting properties, and yet they are notoriously hard to decode in practice. Despite many decades of extensive study, the fastest known algorithms still run in exponential time. The Learning Parity with Noise (LPN) problem, which can be seen as the task of decoding a random linear code in the presence of noise, has thus emerged as a prominent hardness assumption with numerous applications in both cryptography and learning theory.","authors":["Alexander Poremba","Yihui Quek","Peter Shor"],"url":"https://arxiv.org/abs/2410.18953"}
{"created":"2025-04-16","title":"Large problems are not necessarily hard: A case study on distributed NMPC paying off","abstract":"A key motivation in the development of Distributed Model Predictive Control (DMPC) is to accelerate centralized Model Predictive Control (MPC) for large-scale systems. DMPC has the prospect of scaling well by parallelizing computations among subsystems. However, communication delays may deteriorate the performance of decentralized optimization, if excessively many iterations are required per control step. Moreover, centralized solvers often exhibit faster asymptotic convergence rates and, by parallelizing costly linear algebra operations, they can also benefit from modern multicore computing architectures. On this canvas, we study the computational performance of cooperative DMPC for linear and nonlinear systems. To this end, we apply a tailored decentralized real-time iteration scheme to frequency control for power systems. DMPC scales well for the considered linear and nonlinear benchmarks, as the iteration number does not depend on the number of subsystems. Comparisons with multi-threaded centralized solvers demonstrate competitive performance of the proposed decentralized optimization algorithms.","authors":["G\\\"osta Stomberg","Maurice Raetsch","Alexander Engelmann","Timm Faulwasser"],"url":"https://arxiv.org/abs/2411.05627"}
{"created":"2025-04-16","title":"Predicting ionic conductivity in solids from the machine-learned potential energy landscape","abstract":"Discovering new superionic materials is essential for advancing solid-state batteries, which offer improved energy density and safety compared to the traditional lithium-ion batteries with liquid electrolytes. Conventional computational methods for identifying such materials are resource-intensive and not easily scalable. Recently, universal interatomic potential models have been developed using equivariant graph neural networks. These models are trained on extensive datasets of first-principles force and energy calculations. One can achieve significant computational advantages by leveraging them as the foundation for traditional methods of assessing the ionic conductivity, such as molecular dynamics or nudged elastic band techniques. However, the generalization error from model inference on diverse atomic structures arising in such calculations can compromise the reliability of the results. In this work, we propose an approach for the quick and reliable screening of ionic conductors through the analysis of a universal interatomic potential. Our method incorporates a set of heuristic structure descriptors that effectively employ the rich knowledge of the underlying model while requiring minimal generalization capabilities. Using our descriptors, we rank lithium-containing materials in the Materials Project database according to their expected ionic conductivity. Eight out of the ten highest-ranked materials are confirmed to be superionic at room temperature in first-principles calculations. Notably, our method achieves a speed-up factor of approximately 50 compared to molecular dynamics driven by a machine-learning potential, and is at least 3,000 times faster compared to first-principles molecular dynamics.","authors":["Artem Maevskiy","Alexandra Carvalho","Emil Sataev","Volha Turchyna","Keian Noori","Aleksandr Rodin","A. H. Castro Neto","Andrey Ustyuzhanin"],"url":"https://arxiv.org/abs/2411.06804"}
{"created":"2025-04-16","title":"Generation of Cycle Permutation Graphs and Permutation Snarks","abstract":"We present an algorithm for the efficient generation of all pairwise non-isomorphic cycle permutation graphs, i.e. cubic graphs with a $2$-factor consisting of two chordless cycles, non-hamiltonian cycle permutation graphs and permutation snarks, i.e. cycle permutation graphs that do not admit a $3$-edge-colouring. This allows us to generate all cycle permutation graphs up to order $34$ and all permutation snarks up to order $46$, improving upon previous computational results by Brinkmann et al. Moreover, we give several improved lower bounds for interesting permutation snarks, such as for a smallest permutation snark of order $6 \\bmod 8$ or a smallest permutation snark of girth at least $6$. These computational results also allow us to complete a characterisation of the orders for which non-hamiltonian cycle permutation graphs exist, answering an open question by Klee from 1972, and yield many more counterexamples to a conjecture by Zhang.","authors":["Jan Goedgebeur","Jarne Renders","Steven Van Overberghe"],"url":"https://arxiv.org/abs/2411.12606"}
{"created":"2025-04-16","title":"Generative AI for Brane Configurations and Coamoeba","abstract":"We introduce a generative AI model to obtain Type IIB brane configurations that realize toric phases of a family of 4d N=1 supersymmetric gauge theories. These 4d N=1 quiver gauge theories are worldvolume theories of a D3-brane probing a toric Calabi-Yau 3-fold. The Type IIB brane configurations are given by the coamoeba projection of the mirror curve associated with the toric Calabi-Yau 3-fold. The shape of the mirror curve and its coamoeba projection, as well as the corresponding Type IIB brane configuration and the toric phase of the 4d N=1 theory, all depend on the complex structure moduli parameterizing the mirror curve. We train a generative AI model, a conditional variational autoencoder (CVAE), that takes a choice of complex structure moduli as input and generates the corresponding coamoeba. This enables us not only to obtain a high-resolution representation of the entire phase space for a family of 4d N=1 theories corresponding to the same toric Calabi-Yau 3-fold, but also to continuously track the movements of the mirror curve and the branes wrapping the curve in the corresponding Type IIB brane configurations during phase transitions associated with Seiberg duality.","authors":["Rak-Kyeong Seong"],"url":"https://arxiv.org/abs/2411.16033"}
{"created":"2025-04-16","title":"Super-Polynomial Growth of the Generalized Persistence Diagram","abstract":"The Generalized Persistence Diagram (GPD) for multi-parameter persistence naturally extends the classical notion of persistence diagram for one-parameter persistence. However, unlike its classical counterpart, computing the GPD remains a significant challenge. The main hurdle is that, while the GPD is defined as the M\\\"obius inversion of the Generalized Rank Invariant (GRI), computing the GRI is intractable due to the formidable size of its domain, i.e., the set of all connected and convex subsets in a finite grid in $\\mathbb{R}^d$ with $d \\geq 2$. This computational intractability suggests seeking alternative approaches to computing the GPD.","authors":["Donghan Kim","Woojin Kim","Wonjun Lee"],"url":"https://arxiv.org/abs/2412.04889"}
{"created":"2025-04-16","title":"IgCraft: A versatile sequence generation framework for antibody discovery and engineering","abstract":"Designing antibody sequences to better resemble those observed in natural human repertoires is a key challenge in biologics development. We introduce IgCraft: a multi-purpose model for paired human antibody sequence generation, built on Bayesian Flow Networks. IgCraft presents one of the first unified generative modeling frameworks capable of addressing multiple antibody sequence design tasks with a single model, including unconditional sampling, sequence inpainting, inverse folding, and CDR motif scaffolding. Our approach achieves competitive results across the full spectrum of these tasks while constraining generation to the space of human antibody sequences, exhibiting particular strengths in CDR motif scaffolding (grafting) where we achieve state-of-the-art performance in terms of humanness and preservation of structural properties. By integrating previously separate tasks into a single scalable generative model, IgCraft provides a versatile platform for sampling human antibody sequences under a variety of contexts relevant to antibody discovery and engineering. Model code and weights are publicly available at https://github.com/mgreenig/IgCraft.","authors":["Matthew Greenig","Haowen Zhao","Vladimir Radenkovic","Aubin Ramon","Pietro Sormanni"],"url":"https://arxiv.org/abs/2503.19821"}
{"created":"2025-04-16","title":"JanusDDG: A Thermodynamics-Compliant Model for Sequence-Based Protein Stability via Two-Fronts Multi-Head Attention","abstract":"Understanding how residue variations affect protein stability is crucial for designing functional proteins and deciphering the molecular mechanisms underlying disease-related mutations. Recent advances in protein language models (PLMs) have revolutionized computational protein analysis, enabling, among other things, more accurate predictions of mutational effects. In this work, we introduce JanusDDG, a deep learning framework that leverages PLM-derived embeddings and a bidirectional cross-attention transformer architecture to predict $\\Delta \\Delta G$ of single and multiple-residue mutations while simultaneously being constrained to respect fundamental thermodynamic properties, such as antisymmetry and transitivity. Unlike conventional self-attention, JanusDDG computes queries (Q) and values (V) as the difference between wild-type and mutant embeddings, while keys (K) alternate between the two. This cross-interleaved attention mechanism enables the model to capture mutation-induced perturbations while preserving essential contextual information. Experimental results show that JanusDDG achieves state-of-the-art performance in predicting $\\Delta \\Delta G$ from sequence alone, matching or exceeding the accuracy of structure-based methods for both single and multiple mutations. Code Availability:https://github.com/compbiomed-unito/JanusDDG","authors":["Guido Barducci","Ivan Rossi","Francesco Codic\\`e","Cesare Rollo","Valeria Repetto","Corrado Pancotti","Virginia Iannibelli","Tiziana Sanavia","Piero Fariselli"],"url":"https://arxiv.org/abs/2504.03278"}
{"created":"2025-04-16","title":"Robust Reinforcement Learning from Human Feedback for Large Language Models Fine-Tuning","abstract":"Reinforcement learning from human feedback (RLHF) has emerged as a key technique for aligning the output of large language models (LLMs) with human preferences. To learn the reward function, most existing RLHF algorithms use the Bradley-Terry model, which relies on assumptions about human preferences that may not reflect the complexity and variability of real-world judgments. In this paper, we propose a robust algorithm to enhance the performance of existing approaches under such reward model misspecifications. Theoretically, our algorithm reduces the variance of reward and policy estimators, leading to improved regret bounds. Empirical evaluations on LLM benchmark datasets demonstrate that the proposed algorithm consistently outperforms existing methods, with 77-81% of responses being favored over baselines on the Anthropic Helpful and Harmless dataset.","authors":["Kai Ye","Hongyi Zhou","Jin Zhu","Francesco Quinzan","Chengchung Shi"],"url":"https://arxiv.org/abs/2504.03784"}
{"created":"2025-04-16","title":"Arnold Diffusion in the Full Three-Body Problem","abstract":"We show the existence of Arnold diffusion in the planar full three-body problem, which is expressed as a perturbation of a Kepler problem and a planar circular restricted three-body problem, with the perturbation parameter being the mass of the smallest body. In this context, we obtain Arnold diffusion in terms of a transfer of energy, in an amount independent of the perturbation parameter, between the Kepler problem and the restricted three-body problem. Our argument is based on a topological method based on correctly aligned windows which is implemented into a computer assisted proof. This approach can be applied to physically relevant masses of the bodies, such as those in a Neptune-Triton-asteroid system. In this case, we obtain explicit estimates for the range of the perturbation parameter and for the diffusion time.","authors":["Maciej J. Capinski","Marian Gidea"],"url":"https://arxiv.org/abs/2504.09273"}
