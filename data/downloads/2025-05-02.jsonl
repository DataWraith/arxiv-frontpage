{"created":"2025-05-02","title":"Rosetta-PL: Propositional Logic as a Benchmark for Large Language Model Reasoning","abstract":"Large Language Models (LLMs) are primarily trained on high-resource natural languages, limiting their effectiveness in low-resource settings and in tasks requiring deep logical reasoning. This research introduces Rosetta-PL, a benchmark designed to evaluate LLMs' logical reasoning and generalization capabilities in a controlled environment. We construct Rosetta-PL by translating a dataset of logical propositions from Lean into a custom logical language, which is then used to fine-tune an LLM (e.g., GPT-4o). Our experiments analyze the impact of the size of the dataset and the translation methodology on the performance of the model. Our results indicate that preserving logical relationships in the translation process significantly boosts precision, with accuracy plateauing beyond roughly 20,000 training samples. These insights provide valuable guidelines for optimizing LLM training in formal reasoning tasks and improving performance in various low-resource language applications.","authors":["Shaun Baek","Shaun Esua-Mensah","Cyrus Tsui","Sejan Vigneswaralingam","Abdullah Alali","Michael Lu","Vasu Sharma","Kevin Zhu"],"url":"https://arxiv.org/abs/2505.00001"}
{"created":"2025-05-02","title":"Symbol grounding in computational systems: A paradox of intentions","abstract":"The paper presents a paradoxical feature of computational systems that suggests that computationalism cannot explain symbol grounding. If the mind is a digital computer, as computationalism claims, then it can be computing either over meaningful symbols or over meaningless symbols. If it is computing over meaningful symbols its functioning presupposes the existence of meaningful symbols in the system, i.e. it implies semantic nativism. If the mind is computing over meaningless symbols, no intentional cognitive processes are available prior to symbol grounding. In this case, no symbol grounding could take place since any grounding presupposes intentional cognitive processes. So, whether computing in the mind is over meaningless or over meaningful symbols, computationalism implies semantic nativism.","authors":["Vincent C. M\\\"uller"],"url":"https://arxiv.org/abs/2505.00002"}
{"created":"2025-05-02","title":"The Mind in the Machine: A Survey of Incorporating Psychological Theories in LLMs","abstract":"Psychological insights have long shaped pivotal NLP breakthroughs, including the cognitive underpinnings of attention mechanisms, formative reinforcement learning, and Theory of Mind-inspired social modeling. As Large Language Models (LLMs) continue to grow in scale and complexity, there is a rising consensus that psychology is essential for capturing human-like cognition, behavior, and interaction. This paper reviews how psychological theories can inform and enhance stages of LLM development, including data, pre-training, post-training, and evaluation\\&amp;application. Our survey integrates insights from cognitive, developmental, behavioral, social, personality psychology, and psycholinguistics. Our analysis highlights current trends and gaps in how psychological theories are applied. By examining both cross-domain connections and points of tension, we aim to bridge disciplinary divides and promote more thoughtful integration of psychology into future NLP research.","authors":["Zizhou Liu","Ziwei Gong","Lin Ai","Zheng Hui","Run Chen","Colin Wayne Leach","Michelle R. Greene","Julia Hirschberg"],"url":"https://arxiv.org/abs/2505.00003"}
{"created":"2025-05-02","title":"LangVAE and LangSpace: Building and Probing for Language Model VAEs","abstract":"We present LangVAE, a novel framework for modular construction of variational autoencoders (VAEs) on top of pre-trained large language models (LLMs). Such language model VAEs can encode the knowledge of their pre-trained components into more compact and semantically disentangled representations. The representations obtained in this way can be analysed with the LangVAE companion framework: LangSpace, which implements a collection of probing methods, such as vector traversal and interpolation, disentanglement measures, and cluster visualisations. LangVAE and LangSpace offer a flexible, efficient and scalable way of building and analysing textual representations, with simple integration for models available on the HuggingFace Hub. Additionally, we conducted a set of experiments with different encoder and decoder combinations, as well as annotated inputs, revealing a wide range of interactions across architectural families and sizes w.r.t. generalisation and disentanglement. Our findings demonstrate a promising framework for systematising the experimentation and understanding of textual representations.","authors":["Danilo S. Carvalho","Yingji Zhang","Harriet Unsworth","Andr\\'e Freitas"],"url":"https://arxiv.org/abs/2505.00004"}
{"created":"2025-05-02","title":"Belief System Dynamics as Network of Single Layered Neural Network","abstract":"As problems in political polarization and the spread of misinformation become serious, belief propagation on a social network becomes an important question to explore. Previous breakthroughs have been made in algorithmic approaches to understanding how group consensus or polarization can occur in a population. This paper proposed a modified model of the Friedkin-Johnsen model that tries to explain the underlying stubbornness of individual as well as possible back fire effect by treating each individual as a single layer neural network on a set of evidence for a particular statement with input being confidence level on each evidence, and belief of the statement is the output of this neural network.","authors":["Yujian Fu"],"url":"https://arxiv.org/abs/2505.00005"}
{"created":"2025-05-02","title":"Toward a digital twin of U.S. Congress","abstract":"In this paper we provide evidence that a virtual model of U.S. congresspersons based on a collection of language models satisfies the definition of a digital twin. In particular, we introduce and provide high-level descriptions of a daily-updated dataset that contains every Tweet from every U.S. congressperson during their respective terms. We demonstrate that a modern language model equipped with congressperson-specific subsets of this data are capable of producing Tweets that are largely indistinguishable from actual Tweets posted by their physical counterparts. We illustrate how generated Tweets can be used to predict roll-call vote behaviors and to quantify the likelihood of congresspersons crossing party lines, thereby assisting stakeholders in allocating resources and potentially impacting real-world legislative dynamics. We conclude with a discussion of the limitations and important extensions of our analysis.","authors":["Hayden Helm","Tianyi Chen","Harvey McGuinness","Paige Lee","Brandon Duderstadt","Carey E. Priebe"],"url":"https://arxiv.org/abs/2505.00006"}
{"created":"2025-05-02","title":"A Scoping Review of Natural Language Processing in Addressing Medically Inaccurate Information: Errors, Misinformation, and Hallucination","abstract":"Objective: This review aims to explore the potential and challenges of using Natural Language Processing (NLP) to detect, correct, and mitigate medically inaccurate information, including errors, misinformation, and hallucination. By unifying these concepts, the review emphasizes their shared methodological foundations and their distinct implications for healthcare. Our goal is to advance patient safety, improve public health communication, and support the development of more reliable and transparent NLP applications in healthcare.","authors":["Zhaoyi Sun","Wen-Wai Yim","Ozlem Uzuner","Fei Xia","Meliha Yetisgen"],"url":"https://arxiv.org/abs/2505.00008"}
{"created":"2025-05-02","title":"Efficient Knowledge Transfer in Multi-Task Learning through Task-Adaptive Low-Rank Representation","abstract":"Pre-trained language models (PLMs) demonstrate remarkable intelligence but struggle with emerging tasks unseen during training in real-world applications. Training separate models for each new task is usually impractical. Multi-task learning (MTL) addresses this challenge by transferring shared knowledge from source tasks to target tasks. As an dominant parameter-efficient fine-tuning method, prompt tuning (PT) enhances MTL by introducing an adaptable vector that captures task-specific knowledge, which acts as a prefix to the original prompt that preserves shared knowledge, while keeping PLM parameters frozen. However, PT struggles to effectively capture the heterogeneity of task-specific knowledge due to its limited representational capacity. To address this challenge, we propose Task-Adaptive Low-Rank Representation (TA-LoRA), an MTL method built on PT, employing the low-rank representation to model task heterogeneity and a fast-slow weights mechanism where the slow weight encodes shared knowledge, while the fast weight captures task-specific nuances, avoiding the mixing of shared and task-specific knowledge, caused by training low-rank representations from scratch. Moreover, a zero-initialized attention mechanism is introduced to minimize the disruption of immature low-rank components on original prompts during warm-up epochs. Experiments on 16 tasks demonstrate that TA-LoRA achieves state-of-the-art performance in full-data and few-shot settings while maintaining superior parameter efficiency.","authors":["Xiao Zhang","Kangsheng Wang","Tianyu Hu","Huimin Ma"],"url":"https://arxiv.org/abs/2505.00009"}
{"created":"2025-05-02","title":"Jailbreak Detection in Clinical Training LLMs Using Feature-Based Predictive Models","abstract":"Jailbreaking in Large Language Models (LLMs) threatens their safe use in sensitive domains like education by allowing users to bypass ethical safeguards. This study focuses on detecting jailbreaks in 2-Sigma, a clinical education platform that simulates patient interactions using LLMs. We annotated over 2,300 prompts across 158 conversations using four linguistic variables shown to correlate strongly with jailbreak behavior. The extracted features were used to train several predictive models, including Decision Trees, Fuzzy Logic-based classifiers, Boosting methods, and Logistic Regression. Results show that feature-based predictive models consistently outperformed Prompt Engineering, with the Fuzzy Decision Tree achieving the best overall performance. Our findings demonstrate that linguistic-feature-based models are effective and explainable alternatives for jailbreak detection. We suggest future work explore hybrid frameworks that integrate prompt-based flexibility with rule-based robustness for real-time, spectrum-based jailbreak monitoring in educational LLMs.","authors":["Tri Nguyen","Lohith Srikanth Pentapalli","Magnus Sieverding","Laurah Turner","Seth Overla","Weibing Zheng","Chris Zhou","David Furniss","Danielle Weber","Michael Gharib","Matt Kelleher","Michael Shukis","Cameron Pawlik","Kelly Cohen"],"url":"https://arxiv.org/abs/2505.00010"}
{"created":"2025-05-02","title":"Computing with Printed and Flexible Electronics","abstract":"Printed and flexible electronics (PFE) have emerged as the ubiquitous solution for application domains at the extreme edge, where the demands for low manufacturing and operational cost cannot be met by silicon-based computing. Built on mechanically flexible substrates, printed and flexible devices offer unparalleled advantages in terms of form factor, bio-compatibility and sustainability, making them ideal for emerging and uncharted applications, such as wearable healthcare products or fast-moving consumer goods. Their desirable attributes stem from specialized fabrication technologies, e.g., Pragmatic's FlexIC, where advancements like ultra-thin substrates and specialized printing methods expand their hardware efficiency, and enable penetration to previously unexplored application domains. In recent years, significant focus has been on machine learning (ML) circuits for resource-constrained on-sensor and near-sensor processing, both in the digital and analog domains, as they meet the requirements of target applications by PFE. Despite their advancements, challenges like reliability, device integration and efficient memory design are still prevalent in PFE, spawning several research efforts towards cross-layer optimization and co-design, whilst showing promise for advancing printed and flexible electronics to new domains.","authors":["Mehdi B. Tahoori","Emre Ozer","Georgios Zervakis","Konstantinos Balaskas","Priyanjana Pal"],"url":"https://arxiv.org/abs/2505.00011"}
{"created":"2025-05-02","title":"The AI Co-Ethnographer: How Far Can Automation Take Qualitative Research?","abstract":"Qualitative research often involves labor-intensive processes that are difficult to scale while preserving analytical depth. This paper introduces The AI Co-Ethnographer (AICoE), a novel end-to-end pipeline developed for qualitative research and designed to move beyond the limitations of simply automating code assignments, offering a more integrated approach. AICoE organizes the entire process, encompassing open coding, code consolidation, code application, and even pattern discovery, leading to a comprehensive analysis of qualitative data.","authors":["Fabian Retkowski","Andreas Sudmann","Alexander Waibel"],"url":"https://arxiv.org/abs/2505.00012"}
{"created":"2025-05-02","title":"Performance Evaluation of Emotion Classification in Japanese Using RoBERTa and DeBERTa","abstract":"Background Practical applications such as social media monitoring and customer-feedback analysis require accurate emotion detection for Japanese text, yet resource scarcity and class imbalance hinder model performance.","authors":["Yoichi Takenaka"],"url":"https://arxiv.org/abs/2505.00013"}
{"created":"2025-05-02","title":"Manifold-Constrained Sentence Embeddings via Triplet Loss: Projecting Semantics onto Spheres, Tori, and M\\\"obius Strips","abstract":"Recent advances in representation learning have emphasized the role of embedding geometry in capturing semantic structure. Traditional sentence embeddings typically reside in unconstrained Euclidean spaces, which may limit their ability to reflect complex relationships in language. In this work, we introduce a novel framework that constrains sentence embeddings to lie on continuous manifolds -- specifically the unit sphere, torus, and M\\\"obius strip -- using triplet loss as the core training objective. By enforcing differential geometric constraints on the output space, our approach encourages the learning of embeddings that are both discriminative and topologically structured.","authors":["Vinit K. Chavan"],"url":"https://arxiv.org/abs/2505.00014"}
{"created":"2025-05-02","title":"Design and Application of Multimodal Large Language Model Based System for End to End Automation of Accident Dataset Generation","abstract":"Road traffic accidents remain a major public safety and socio-economic issue in developing countries like Bangladesh. Existing accident data collection is largely manual, fragmented, and unreliable, resulting in underreporting and inconsistent records. This research proposes a fully automated system using Large Language Models (LLMs) and web scraping techniques to address these challenges. The pipeline consists of four components: automated web scraping code generation, news collection from online sources, accident news classification with structured data extraction, and duplicate removal. The system uses the multimodal generative LLM Gemini-2.0-Flash for seamless automation. The code generation module classifies webpages into pagination, dynamic, or infinite scrolling categories and generates suitable Python scripts for scraping. LLMs also classify and extract key accident information such as date, time, location, fatalities, injuries, road type, vehicle types, and pedestrian involvement. A deduplication algorithm ensures data integrity by removing duplicate reports. The system scraped 14 major Bangladeshi news sites over 111 days (Oct 1, 2024 - Jan 20, 2025), processing over 15,000 news articles and identifying 705 unique accidents. The code generation module achieved 91.3% calibration and 80% validation accuracy. Chittagong reported the highest number of accidents (80), fatalities (70), and injuries (115), followed by Dhaka, Faridpur, Gazipur, and Cox's Bazar. Peak accident times were morning (8-9 AM), noon (12-1 PM), and evening (6-7 PM). A public repository was also developed with usage instructions. This study demonstrates the viability of an LLM-powered, scalable system for accurate, low-effort accident data collection, providing a foundation for data-driven road safety policymaking in Bangladesh.","authors":["MD Thamed Bin Zaman Chowdhury","Moazzem Hossain"],"url":"https://arxiv.org/abs/2505.00015"}
{"created":"2025-05-02","title":"Sparks of Tabular Reasoning via Text2SQL Reinforcement Learning","abstract":"This work reframes the Text-to-SQL task as a pathway for teaching large language models (LLMs) to reason over and manipulate tabular data--moving beyond the traditional focus on query generation. We propose a two-stage framework that leverages SQL supervision to develop transferable table reasoning capabilities. First, we synthesize detailed chain-of-thought (CoT) traces from real-world SQL queries, providing step-by-step, clause-level supervision that teaches the model how to traverse, filter, and aggregate table fields. Second, we introduce a Group Relative Policy Optimization (GRPO) reinforcement learning objective that connects SQL execution accuracy to generalizable reasoning by encouraging steps that extend beyond task-specific syntax and transfer across datasets. Empirically, our approach improves performance on standard Text-to-SQL benchmarks and achieves substantial gains on reasoning-intensive datasets such as BIRD and CRT-QA, demonstrating enhanced generalization and interpretability. Specifically, the distilled-quantized LLaMA model achieved a 20\\% increase in accuracy when trained on Text-to-SQL tasks, while Qwen achieved a 5\\% increase. These results suggest that SQL can serve not only as a target formalism but also as an effective scaffold for learning robust, transferable reasoning over structured data.","authors":["Josefa Lia Stoisser","Marc Boubnovski Martell","Julien Fauqueur"],"url":"https://arxiv.org/abs/2505.00016"}
{"created":"2025-05-02","title":"ReCellTy: Domain-specific knowledge graph retrieval-augmented LLMs workflow for single-cell annotation","abstract":"To enable precise and fully automated cell type annotation with large language models (LLMs), we developed a graph structured feature marker database to retrieve entities linked to differential genes for cell reconstruction. We further designed a multi task workflow to optimize the annotation process. Compared to general purpose LLMs, our method improves human evaluation scores by up to 0.21 and semantic similarity by 6.1% across 11 tissue types, while more closely aligning with the cognitive logic of manual annotation.","authors":["Dezheng Han","Yibin Jia","Ruxiao Chen","Wenjie Han","Shuaishuai Guo","Jianbo Wang"],"url":"https://arxiv.org/abs/2505.00017"}
{"created":"2025-05-02","title":"Position Paper: Towards Open Complex Human-AI Agents Collaboration System for Problem-Solving and Knowledge Management","abstract":"This position paper critically surveys a broad spectrum of recent empirical developments on human-AI agents collaboration, highlighting both their technical achievements and persistent gaps. We observe a lack of a unifying theoretical framework that can coherently integrate these varied studies, especially when tackling open-ended, complex tasks. To address this, we propose a novel conceptual architecture: one that systematically interlinks the technical details of multi-agent coordination, knowledge management, cybernetic feedback loops, and higher-level control mechanisms. By mapping existing contributions, from symbolic AI techniques and connectionist LLM-based agents to hybrid organizational practices, onto this proposed framework (Hierarchical Exploration-Exploitation Net), our approach facilitates revision of legacy methods and inspires new work that fuses qualitative and quantitative paradigms. The paper's structure allows it to be read from any section, serving equally as a critical review of technical implementations and as a forward-looking reference for designing or extending human-AI symbioses. Together, these insights offer a stepping stone toward deeper co-evolution of human cognition and AI capability.","authors":["Ju Wu","Calvin K. L. Or"],"url":"https://arxiv.org/abs/2505.00018"}
{"created":"2025-05-02","title":"An Empirical Study on Prompt Compression for Large Language Models","abstract":"Prompt engineering enables Large Language Models (LLMs) to perform a variety of tasks. However, lengthy prompts significantly increase computational complexity and economic costs. To address this issue, we study six prompt compression methods for LLMs, aiming to reduce prompt length while maintaining LLM response quality. In this paper, we present a comprehensive analysis covering aspects such as generation performance, model hallucinations, efficacy in multimodal tasks, word omission analysis, and more. We evaluate these methods across 13 datasets, including news, scientific articles, commonsense QA, math QA, long-context QA, and VQA datasets. Our experiments reveal that prompt compression has a greater impact on LLM performance in long contexts compared to short ones. In the Longbench evaluation, moderate compression even enhances LLM performance. Our code and data is available at https://github.com/3DAgentWorld/Toolkit-for-Prompt-Compression.","authors":["Zheng Zhang","Jinyi Li","Yihuai Lan","Xiang Wang","Hao Wang"],"url":"https://arxiv.org/abs/2505.00019"}
{"created":"2025-05-02","title":"Beyond Public Access in LLM Pre-Training Data","abstract":"Using a legally obtained dataset of 34 copyrighted O'Reilly Media books, we apply the DE-COP membership inference attack method to investigate whether OpenAI's large language models were trained on copyrighted content without consent. Our AUROC scores show that GPT-4o, OpenAI's more recent and capable model, demonstrates strong recognition of paywalled O'Reilly book content (AUROC = 82\\%), compared to OpenAI's earlier model GPT-3.5 Turbo. In contrast, GPT-3.5 Turbo shows greater relative recognition of publicly accessible O'Reilly book samples. GPT-4o Mini, as a much smaller model, shows no knowledge of public or non-public O'Reilly Media content when tested (AUROC $\\approx$ 50\\%). Testing multiple models, with the same cutoff date, helps us account for potential language shifts over time that might bias our findings. These results highlight the urgent need for increased corporate transparency regarding pre-training data sources as a means to develop formal licensing frameworks for AI content training","authors":["Sruly Rosenblat","Tim O'Reilly","Ilan Strauss"],"url":"https://arxiv.org/abs/2505.00020"}
{"created":"2025-05-02","title":"Ustnlp16 at SemEval-2025 Task 9: Improving Model Performance through Imbalance Handling and Focal Loss","abstract":"Classification tasks often suffer from imbal- anced data distribution, which presents chal- lenges in food hazard detection due to severe class imbalances, short and unstructured text, and overlapping semantic categories. In this paper, we present our system for SemEval- 2025 Task 9: Food Hazard Detection, which ad- dresses these issues by applying data augmenta- tion techniques to improve classification perfor- mance. We utilize transformer-based models, BERT and RoBERTa, as backbone classifiers and explore various data balancing strategies, including random oversampling, Easy Data Augmentation (EDA), and focal loss. Our ex- periments show that EDA effectively mitigates class imbalance, leading to significant improve- ments in accuracy and F1 scores. Furthermore, combining focal loss with oversampling and EDA further enhances model robustness, par- ticularly for hard-to-classify examples. These findings contribute to the development of more effective NLP-based classification models for food hazard detection.","authors":["Zhuoang Cai","Zhenghao Li","Yang Liu","Liyuan Guo","Yangqiu Song"],"url":"https://arxiv.org/abs/2505.00021"}
{"created":"2025-05-02","title":"Aleph-Alpha-GermanWeb: Improving German-language LLM pre-training with model-based data curation and synthetic data generation","abstract":"Scaling data quantity is essential for large language models (LLMs), yet recent findings show that data quality can significantly boost performance and training efficiency. We introduce a German-language dataset curation pipeline that combines heuristic and model-based filtering techniques with synthetic data generation. We use our pipeline to create Aleph-Alpha-GermanWeb, a large-scale German pre-training dataset which draws from: (1) Common Crawl web data, (2) FineWeb2, and (3) synthetically-generated data conditioned on actual, organic web data. We evaluate our dataset by pre-training both a 1B Llama-style model and an 8B tokenizer-free hierarchical autoregressive transformer (HAT). A comparison on German-language benchmarks, including MMMLU, shows significant performance gains of Aleph-Alpha-GermanWeb over FineWeb2 alone. This advantage holds at the 8B scale even when FineWeb2 is enriched by human-curated high-quality data sources such as Wikipedia. Our findings support the growing body of evidence that model-based data curation and synthetic data generation can significantly enhance LLM pre-training datasets.","authors":["Thomas F Burns","Letitia Parcalabescu","Stephan W\\\"aldchen","Michael Barlow","Gregor Ziegltrum","Volker Stampa","Bastian Harren","Bj\\\"orn Deiseroth"],"url":"https://arxiv.org/abs/2505.00022"}
{"created":"2025-05-02","title":"CORG: Generating Answers from Complex, Interrelated Contexts","abstract":"In a real-world corpus, knowledge frequently recurs across documents but often contains inconsistencies due to ambiguous naming, outdated information, or errors, leading to complex interrelationships between contexts. Previous research has shown that language models struggle with these complexities, typically focusing on single factors in isolation. We classify these relationships into four types: distracting, ambiguous, counterfactual, and duplicated. Our analysis reveals that no single approach effectively addresses all these interrelationships simultaneously. Therefore, we introduce Context Organizer (CORG), a framework that organizes multiple contexts into independently processed groups. This design allows the model to efficiently find all relevant answers while ensuring disambiguation. CORG consists of three key components: a graph constructor, a reranker, and an aggregator. Our results demonstrate that CORG balances performance and efficiency effectively, outperforming existing grouping methods and achieving comparable results to more computationally intensive, single-context approaches.","authors":["Hyunji Lee","Franck Dernoncourt","Trung Bui","Seunghyun Yoon"],"url":"https://arxiv.org/abs/2505.00023"}
{"created":"2025-05-02","title":"Nemotron-Research-Tool-N1: Tool-Using Language Models with Reinforced Reasoning","abstract":"Enabling large language models with external tools has become a pivotal strategy for extending their functionality beyond text generation tasks. Prior work typically enhances tool-use abilities by either applying supervised fine-tuning (SFT) to enforce tool-call correctness or distilling reasoning traces from stronger models for SFT. However, both approaches fall short, either omitting reasoning entirely or producing imitative reasoning that limits generalization. Inspired by the success of DeepSeek-R1 in eliciting reasoning through rule-based reinforcement learning, we develop the Nemotron-Research-Tool-N1 series of tool-using language models using a similar training paradigm. Instead of restrictively supervising intermediate reasoning traces distilled from stronger models, Nemotron-Research-Tool-N1 is optimized with a binary reward that evaluates only the structural validity and functional correctness of tool invocations. This lightweight supervision allows the model to autonomously internalize reasoning strategies, without the need for annotated reasoning trajectories. Experiments on the BFCL and API-Bank benchmarks show that Nemotron-Research-Tool-N1-7B and Nemotron-Research-Tool-N1-14B, built on Qwen-2.5-7B/14B-Instruct, achieve state-of-the-art results, outperforming GPT-4o on both evaluations.","authors":["Shaokun Zhang","Yi Dong","Jieyu Zhang","Jan Kautz","Bryan Catanzaro","Andrew Tao","Qingyun Wu","Zhiding Yu","Guilin Liu"],"url":"https://arxiv.org/abs/2505.00024"}
{"created":"2025-05-02","title":"A Method for the Architecture of a Medical Vertical Large Language Model Based on Deepseek R1","abstract":"In recent years, despite foundation models like DeepSeek-R1 and ChatGPT demonstrating significant capabilities in general tasks, professional knowledge barriers, computational resource requirements, and deployment environment limitations have severely hindered their application in actual medical scenarios. Addressing these challenges, this paper proposes an efficient lightweight medical vertical large language model architecture method, systematically solving the lightweight problem of medical large models from three dimensions: knowledge acquisition, model compression, and computational optimization. At the knowledge acquisition level, a knowledge transfer pipeline is designed from the fine-tuned DeepSeek-R1-Distill-70B teacher model to the DeepSeek-R1-Distill-7B student model, and Low-Rank Adaptation (LoRA) technology is adopted to precisely adjust key attention layers. At the model compression level, compression techniques including 4-bit weight quantization are implemented while preserving the core representation ability for medical reasoning. At the computational optimization level, inference optimization techniques such as Flash Attention acceleration and continuous batching are integrated, and a professional prompt template system is constructed to adapt to different types of medical problems. Experimental results on medical question-answering datasets show that the method proposed in this paper maintains professional accuracy while reducing memory consumption by 64.7\\% and inference latency by 12.4\\%, providing an effective solution for the application of medical large models in resource-constrained environments such as edge computing devices.","authors":["Mingda Zhang","Jianglong Qin"],"url":"https://arxiv.org/abs/2505.00025"}
{"created":"2025-05-02","title":"Theory of Mind in Large Language Models: Assessment and Enhancement","abstract":"Theory of Mind (ToM)-the ability to infer and reason about others' mental states-is fundamental to human social intelligence. As Large Language Models (LLMs) become increasingly integrated into daily life, it is crucial to assess and enhance their capacity to interpret and respond to human mental states. In this paper, we review LLMs' ToM capabilities by examining both evaluation benchmarks and the strategies designed to improve them. We focus on widely adopted story-based benchmarks and provide an in-depth analysis of methods aimed at enhancing ToM in LLMs. Furthermore, we outline promising future research directions informed by recent benchmarks and state-of-the-art approaches. Our survey serves as a valuable resource for researchers interested in advancing LLMs' ToM capabilities.","authors":["Ruirui Chen","Weifeng Jiang","Chengwei Qin","Cheston Tan"],"url":"https://arxiv.org/abs/2505.00026"}
{"created":"2025-05-02","title":"Extracting Abstraction Dimensions by Identifying Syntax Pattern from Texts","abstract":"This paper proposed an approach to automatically discovering subject dimension, action dimension, object dimension and adverbial dimension from texts to efficiently operate texts and support query in natural language. The high quality of trees guarantees that all subjects, actions, objects and adverbials and their subclass relations within texts can be represented. The independency of trees ensures that there is no redundant representation between trees. The expressiveness of trees ensures that the majority of sentences can be accessed from each tree and the rest of sentences can be accessed from at least one tree so that the tree-based search mechanism can support querying in natural language. Experiments show that the average precision, recall and F1-score of the abstraction trees constructed by the subclass relations of subject, action, object and adverbial are all greater than 80%. The application of the proposed approach to supporting query in natural language demonstrates that different types of question patterns for querying subject or object have high coverage of texts, and searching multiple trees on subject, action, object and adverbial according to the question pattern can quickly reduce search space to locate target sentences, which can support precise operation on texts.","authors":["Jian Zhou","Jiazheng Li","Sirui Zhuge","Hai Zhuge"],"url":"https://arxiv.org/abs/2505.00027"}
{"created":"2025-05-02","title":"Enhancing Speech-to-Speech Dialogue Modeling with End-to-End Retrieval-Augmented Generation","abstract":"In recent years, end-to-end speech-to-speech (S2S) dialogue systems have garnered increasing research attention due to their advantages over traditional cascaded systems, including achieving lower latency and more natural integration of nonverbal cues such as emotion and speaker identity. However, these end-to-end systems face key challenges, particularly in incorporating external knowledge, a capability commonly addressed by Retrieval-Augmented Generation (RAG) in text-based large language models (LLMs). The core difficulty lies in the modality gap between input speech and retrieved textual knowledge, which hinders effective integration. To address this issue, we propose a novel end-to-end RAG framework that directly retrieves relevant textual knowledge from speech queries, eliminating the need for intermediate speech-to-text conversion via techniques like ASR. Experimental results demonstrate that our method significantly improves the performance of end-to-end S2S dialogue systems while achieving higher retrieval efficiency. Although the overall performance still lags behind cascaded models, our framework offers a promising direction for enhancing knowledge integration in end-to-end S2S systems. We will release the code and dataset to support reproducibility and promote further research in this area.","authors":["Pengchao Feng","Ziyang Ma","Wenxi Chen","Yao Li","Sheng Wang","Kai Yu","Xie Chen"],"url":"https://arxiv.org/abs/2505.00028"}
{"created":"2025-05-02","title":"Keep the General, Inject the Specific: Structured Dialogue Fine-Tuning for Knowledge Injection without Catastrophic Forgetting","abstract":"Large Vision Language Models have demonstrated impressive versatile capabilities through extensive multimodal pre-training, but face significant limitations when incorporating specialized knowledge domains beyond their training distribution. These models struggle with a fundamental dilemma: direct adaptation approaches that inject domain-specific knowledge often trigger catastrophic forgetting of foundational visual-linguistic abilities. We introduce Structured Dialogue Fine-Tuning (SDFT), an effective approach that effectively injects domain-specific knowledge while minimizing catastrophic forgetting. Drawing inspiration from supervised fine-tuning in LLMs and subject-driven personalization in text-to-image diffusion models, our method employs a three-phase dialogue structure: Foundation Preservation reinforces pre-trained visual-linguistic alignment through caption tasks; Contrastive Disambiguation introduces carefully designed counterfactual examples to maintain semantic boundaries; and Knowledge Specialization embeds specialized information through chain-of-thought reasoning. Experimental results across multiple domains confirm SDFT's effectiveness in balancing specialized knowledge acquisition with general capability retention. Our key contributions include a data-centric dialogue template that balances foundational alignment with targeted knowledge integration, a weighted multi-turn supervision framework, and comprehensive evaluation across diverse knowledge types.","authors":["Yijie Hong","Xiaofei Yin","Xinzhong Wang","Yi Tu","Ya Guo","Sufeng Duan","Weiqiang Wang","Lingyong Fang","Depeng Wang","Huijia Zhu"],"url":"https://arxiv.org/abs/2505.00029"}
{"created":"2025-05-02","title":"Can Language Models Represent the Past without Anachronism?","abstract":"Before researchers can use language models to simulate the past, they need to understand the risk of anachronism. We find that prompting a contemporary model with examples of period prose does not produce output consistent with period style. Fine-tuning produces results that are stylistically convincing enough to fool an automated judge, but human evaluators can still distinguish fine-tuned model outputs from authentic historical text. We tentatively conclude that pretraining on period prose may be required in order to reliably simulate historical perspectives for social research.","authors":["Ted Underwood","Laura K. Nelson","Matthew Wilkens"],"url":"https://arxiv.org/abs/2505.00030"}
{"created":"2025-05-02","title":"Learning to Plan Before Answering: Self-Teaching LLMs to Learn Abstract Plans for Problem Solving","abstract":"In the field of large language model (LLM) post-training, the effectiveness of utilizing synthetic data generated by the LLM itself has been well-presented. However, a key question remains unaddressed: what essential information should such self-generated data encapsulate? Existing approaches only produce step-by-step problem solutions, and fail to capture the abstract meta-knowledge necessary for generalization across similar problems. Drawing insights from cognitive science, where humans employ high-level abstraction to simplify complex problems before delving into specifics, we introduce a novel self-training algorithm: LEarning to Plan before Answering (LEPA). LEPA trains the LLM to formulate anticipatory plans, which serve as abstract meta-knowledge for problem-solving, before engaging with the intricacies of problems. This approach not only outlines the solution generation path but also shields the LLM from the distraction of irrelevant details. During data generation, LEPA first crafts an anticipatory plan based on the problem, and then generates a solution that aligns with both the plan and the problem. LEPA refines the plan through self-reflection, aiming to acquire plans that are instrumental in yielding correct solutions. During model optimization, the LLM is trained to predict both the refined plans and the corresponding solutions. By efficiently extracting and utilizing the anticipatory plans, LEPA demonstrates remarkable superiority over conventional algorithms on various challenging natural language reasoning benchmarks.","authors":["Jin Zhang","Flood Sung","Zhilin Yang","Yang Gao","Chongjie Zhang"],"url":"https://arxiv.org/abs/2505.00031"}
{"created":"2025-05-02","title":"MDD-LLM: Towards Accuracy Large Language Models for Major Depressive Disorder Diagnosis","abstract":"Major depressive disorder (MDD) impacts more than 300 million people worldwide, highlighting a significant public health issue. However, the uneven distribution of medical resources and the complexity of diagnostic methods have resulted in inadequate attention to this disorder in numerous countries and regions. This paper introduces a high-performance MDD diagnosis tool named MDD-LLM, an AI-driven framework that utilizes fine-tuned large language models (LLMs) and extensive real-world samples to tackle challenges in MDD diagnosis. Therefore, we select 274,348 individual information from the UK Biobank cohort to train and evaluate the proposed method. Specifically, we select 274,348 individual records from the UK Biobank cohort and design a tabular data transformation method to create a large corpus for training and evaluating the proposed approach. To illustrate the advantages of MDD-LLM, we perform comprehensive experiments and provide several comparative analyses against existing model-based solutions across multiple evaluation metrics. Experimental results show that MDD-LLM (70B) achieves an accuracy of 0.8378 and an AUC of 0.8919 (95% CI: 0.8799 - 0.9040), significantly outperforming existing machine learning and deep learning frameworks for MDD diagnosis. Given the limited exploration of LLMs in MDD diagnosis, we examine numerous factors that may influence the performance of our proposed method, such as tabular data transformation techniques and different fine-tuning strategies.","authors":["Yuyang Sha","Hongxin Pan","Wei Xu","Weiyu Meng","Gang Luo","Xinyu Du","Xiaobing Zhai","Henry H. Y. Tong","Caijuan Shi","Kefeng Li"],"url":"https://arxiv.org/abs/2505.00032"}
{"created":"2025-05-02","title":"From Attention to Atoms: Spectral Dictionary Learning for Fast, Interpretable Language Models","abstract":"We propose a novel spectral generative modeling framework for natural language processing that jointly learns a global time varying Fourier dictionary and per token mixing coefficients, replacing the ubiquitous self attention mechanism in transformer architectures. By enforcing reconstruction losses in both the time domain (embedding reconstruction) and the frequency domain (via Short Time Fourier Transform magnitude matching) alongside a standard language modeling objective, and fitting a Gaussian Mixture Model (GMM) prior over the learned mixing vectors, our approach achieves competitive perplexity and generation quality on standard benchmarks such as WikiText2 and Penn Treebank. In contrast to the quadratic computation complexity of self attention, our method operates with linear complexity, delivering substantial efficiency gains. We demonstrate that spectral dictionary models can achieve competitive performance compared to transformer baselines while significantly reducing inference latency and memory footprint, offering a compelling alternative for scalable language modeling.","authors":["Andrew Kiruluta"],"url":"https://arxiv.org/abs/2505.00033"}
{"created":"2025-05-02","title":"Improving Phishing Email Detection Performance of Small Large Language Models","abstract":"Large language models(LLMs) have demonstrated remarkable performance on many natural language processing(NLP) tasks and have been employed in phishing email detection research. However, in current studies, well-performing LLMs typically contain billions or even tens of billions of parameters, requiring enormous computational resources. To reduce computational costs, we investigated the effectiveness of small-parameter LLMs for phishing email detection. These LLMs have around 3 billion parameters and can run on consumer-grade GPUs. However, small LLMs often perform poorly in phishing email detection task. To address these issues, we designed a set of methods including Prompt Engineering, Explanation Augmented Fine-tuning, and Model Ensemble to improve phishing email detection capabilities of small LLMs. We validated the effectiveness of our approach through experiments, significantly improving accuracy on the SpamAssassin dataset from around 0.5 for baseline models like Qwen2.5-1.5B-Instruct to 0.976.","authors":["Zijie Lin","Zikang Liu","Hanbo Fan"],"url":"https://arxiv.org/abs/2505.00034"}
{"created":"2025-05-02","title":"Linguistic Complexity and Socio-cultural Patterns in Hip-Hop Lyrics","abstract":"This paper presents a comprehensive computational framework for analyzing linguistic complexity and socio-cultural trends in hip-hop lyrics. Using a dataset of 3,814 songs from 146 influential artists spanning four decades (1980-2020), we employ natural language processing techniques to quantify multiple dimensions of lyrical complexity. Our analysis reveals a 23.7% increase in vocabulary diversity over the study period, with East Coast artists demonstrating 17.3% higher lexical variation than other regions. Rhyme density increased by 34.2% across all regions, with Midwest artists exhibiting the highest technical complexity (3.04 rhymes per line). Topic modeling identified significant shifts in thematic content, with social justice themes decreasing from 28.5% to 13.8% of content while introspective themes increased from 7.6% to 26.3%. Sentiment analysis demon- strated that lyrics became significantly more negative during sociopolitical crises, with polarity decreasing by 0.31 following major social unrest. Multi-dimensional analysis revealed four dis- tinct stylistic approaches that correlate strongly with geographic origin (r=0.68, p!0.001) and time period (r=0.59, p<0.001). These findings establish quantitative evidence for the evolution of hip- hop as both an art form and a reflection of societal dynamics, providing insights into the interplay between linguistic innovation and cultural context in popular music.","authors":["Aayam Bansal","Raghav Agarwal","Kaashvi Jain"],"url":"https://arxiv.org/abs/2505.00035"}
{"created":"2025-05-02","title":"A Framework to Assess the Persuasion Risks Large Language Model Chatbots Pose to Democratic Societies","abstract":"In recent years, significant concern has emerged regarding the potential threat that Large Language Models (LLMs) pose to democratic societies through their persuasive capabilities. We expand upon existing research by conducting two survey experiments and a real-world simulation exercise to determine whether it is more cost effective to persuade a large number of voters using LLM chatbots compared to standard political campaign practice, taking into account both the \"receive\" and \"accept\" steps in the persuasion process (Zaller 1992). These experiments improve upon previous work by assessing extended interactions between humans and LLMs (instead of using single-shot interactions) and by assessing both short- and long-run persuasive effects (rather than simply asking users to rate the persuasiveness of LLM-produced content). In two survey experiments (N = 10,417) across three distinct political domains, we find that while LLMs are about as persuasive as actual campaign ads once voters are exposed to them, political persuasion in the real-world depends on both exposure to a persuasive message and its impact conditional on exposure. Through simulations based on real-world parameters, we estimate that LLM-based persuasion costs between \\$48-\\$74 per persuaded voter compared to \\$100 for traditional campaign methods, when accounting for the costs of exposure. However, it is currently much easier to scale traditional campaign persuasion methods than LLM-based persuasion. While LLMs do not currently appear to have substantially greater potential for large-scale political persuasion than existing non-LLM methods, this may change as LLM capabilities continue to improve and it becomes easier to scalably encourage exposure to persuasive LLMs.","authors":["Zhongren Chen","Joshua Kalla","Quan Le","Shinpei Nakamura-Sakai","Jasjeet Sekhon","Ruixiao Wang"],"url":"https://arxiv.org/abs/2505.00036"}
{"created":"2025-05-02","title":"HyPerAlign: Hypotheses-driven Personalized Alignment","abstract":"Alignment algorithms are widely used to align large language models (LLMs) to human users based on preference annotations that reflect their intended real-world use cases. Typically these (often divergent) preferences are aggregated over a diverse set of users, resulting in fine-tuned models that are aligned to the ``average-user'' preference. Nevertheless, current models are used by individual users in very specific contexts and situations, emphasizing the need for user-dependent preference control. In this work we address the problem of personalizing LLM outputs to their users, aiming to generate customized responses tailored to individual users, instead of generic outputs that emulate the collective voices of diverse populations. We propose a novel interpretable and sample-efficient hypotheses-driven personalization approach (HyPerAlign) where given few-shot examples written by a particular user, we first infer hypotheses about their communication strategies, personality and writing style, then prompt LLM models with these hypotheses and user specific attributes to generate customized outputs. We conduct experiments on two different personalization tasks, authorship attribution and deliberative alignment, with datasets from diverse domains (news articles, blog posts, emails, jailbreaking benchmarks), and demonstrate the superiority of hypotheses-driven personalization approach when compared to preference-based fine-tuning methods. For deliberative alignment, the helpfulness of LLM models is improved by up to $70\\%$ on average. For authorship attribution, results indicate consistently high win-rates (commonly $>90\\%$) against state-of-the-art preference fine-tuning approaches for LLM personalization across diverse user profiles and LLM models. Overall, our approach represents an interpretable and sample-efficient strategy for the personalization of LLM models to individual users.","authors":["Cristina Garbacea","Chenhao Tan"],"url":"https://arxiv.org/abs/2505.00038"}
{"created":"2025-05-02","title":"Graph RAG for Legal Norms: A Hierarchical and Temporal Approach","abstract":"This article proposes an adaptation of Graph Retrieval Augmented Generation (Graph RAG) specifically designed for the analysis and comprehension of legal norms, which are characterized by their predefined hierarchical structure, extensive network of internal and external references and multiple temporal versions. By combining structured knowledge graphs with contextually enriched text segments, Graph RAG offers a promising solution to address the inherent complexity and vast volume of legal data. The integration of hierarchical structure and temporal evolution into knowledge graphs - along with the concept of comprehensive Text Units - facilitates the construction of richer, interconnected representations of legal knowledge. Through a detailed analysis of Graph RAG and its application to legal norm datasets, this article aims to significantly advance the field of Artificial Intelligence applied to Law, creating opportunities for more effective systems in legal research, legislative analysis, and decision support.","authors":["Hudson de Martim"],"url":"https://arxiv.org/abs/2505.00039"}
{"created":"2025-05-02","title":"MCMComm: Hardware-Software Co-Optimization for End-to-End Communication in Multi-Chip-Modules","abstract":"Increasing AI computing demands and slowing transistor scaling have led to the advent of Multi-Chip-Module (MCMs) based accelerators. MCMs enable cost-effective scalability, higher yield, and modular reuse by partitioning large chips into smaller chiplets. However, MCMs come at an increased communication cost, which requires critical analysis and optimization. This paper makes three main contributions: (i) an end-to-end, off-chip congestion-aware and packaging-adaptive analytical framework for detailed analysis, (ii) hardware software co-optimization incorporating diagonal links, on-chip redistribution, and non-uniform workload partitioning to optimize the framework, and (iii) using metaheuristics (genetic algorithms, GA) and mixed integer quadratic programming (MIQP) to solve the optimized framework. Experimental results demonstrate significant performance improvements for CNNs and Vision Transformers, showcasing up to 1.58x and 2.7x EdP (Energy delay Product) improvement using GA and MIQP, respectively.","authors":["Ritik Raj","Shengjie Lin","Willam Won","Tushar Krishna"],"url":"https://arxiv.org/abs/2505.00041"}
{"created":"2025-05-02","title":"Learning to Borrow Features for Improved Detection of Small Objects in Single-Shot Detectors","abstract":"Detecting small objects remains a significant challenge in single-shot object detectors due to the inherent trade-off between spatial resolution and semantic richness in convolutional feature maps. To address this issue, we propose a novel framework that enables small object representations to \"borrow\" discriminative features from larger, semantically richer instances within the same class. Our architecture introduces three key components: the Feature Matching Block (FMB) to identify semantically similar descriptors across layers, the Feature Representing Block (FRB) to generate enhanced shallow features through weighted aggregation, and the Feature Fusion Block (FFB) to refine feature maps by integrating original, borrowed, and context information. Built upon the SSD framework, our method improves the descriptive capacity of shallow layers while maintaining real-time detection performance. Experimental results demonstrate that our approach significantly boosts small object detection accuracy over baseline methods, offering a promising direction for robust object detection in complex visual environments.","authors":["Richard Schmit"],"url":"https://arxiv.org/abs/2505.00044"}
{"created":"2025-05-02","title":"Base Models Beat Aligned Models at Randomness and Creativity","abstract":"Alignment has quickly become a default ingredient in LLM development, with techniques such as reinforcement learning from human feedback making models act safely, follow instructions, and perform ever-better on complex tasks. While these techniques are certainly useful, we propose that they should not be universally applied and demonstrate a range of tasks on which base language models consistently outperform their popular aligned forms. Particularly, we study tasks that require unpredictable outputs, such as random number generation, mixed strategy games (rock-paper-scissors and hide-and-seek), and creative writing. In each case, aligned models tend towards narrow behaviors that result in distinct disadvantages, for instance, preferring to generate \"7\" over other uniformly random numbers, becoming almost fully predictable in some game states, or prioritizing pleasant writing over creative originality. Across models tested, better performance on common benchmarks tends to correlate with worse performance on our tasks, suggesting an effective trade-off in the required capabilities.","authors":["Peter West","Christopher Potts"],"url":"https://arxiv.org/abs/2505.00047"}
{"created":"2025-05-02","title":"Humanizing LLMs: A Survey of Psychological Measurements with Tools, Datasets, and Human-Agent Applications","abstract":"As large language models (LLMs) are increasingly used in human-centered tasks, assessing their psychological traits is crucial for understanding their social impact and ensuring trustworthy AI alignment. While existing reviews have covered some aspects of related research, several important areas have not been systematically discussed, including detailed discussions of diverse psychological tests, LLM-specific psychological datasets, and the applications of LLMs with psychological traits. To address this gap, we systematically review six key dimensions of applying psychological theories to LLMs: (1) assessment tools; (2) LLM-specific datasets; (3) evaluation metrics (consistency and stability); (4) empirical findings; (5) personality simulation methods; and (6) LLM-based behavior simulation. Our analysis highlights both the strengths and limitations of current methods. While some LLMs exhibit reproducible personality patterns under specific prompting schemes, significant variability remains across tasks and settings. Recognizing methodological challenges such as mismatches between psychological tools and LLMs' capabilities, as well as inconsistencies in evaluation practices, this study aims to propose future directions for developing more interpretable, robust, and generalizable psychological assessment frameworks for LLMs.","authors":["Wenhan Dong","Yuemeng Zhao","Zhen Sun","Yule Liu","Zifan Peng","Jingyi Zheng","Zongmin Zhang","Ziyi Zhang","Jun Wu","Ruiming Wang","Shengmin Xu","Xinyi Huang","Xinlei He"],"url":"https://arxiv.org/abs/2505.00049"}
{"created":"2025-05-02","title":"Emotional Analysis of Fashion Trends Using Social Media and AI: Sentiment Analysis on Twitter for Fashion Trend Forecasting","abstract":"This study explores the intersection of fashion trends and social media sentiment through computational analysis of Twitter data using the T4SA (Twitter for Sentiment Analysis) dataset. By applying natural language processing and machine learning techniques, we examine how sentiment patterns in fashion-related social media conversations can serve as predictors for emerging fashion trends. Our analysis involves the identification and categorization of fashion-related content, sentiment classification with improved normalization techniques, time series decomposition, statistically validated causal relationship modeling, cross-platform sentiment comparison, and brand-specific sentiment analysis. Results indicate correlations between sentiment patterns and fashion theme popularity, with accessories and streetwear themes showing statistically significant rising trends. The Granger causality analysis establishes sustainability and streetwear as primary trend drivers, showing bidirectional relationships with several other themes. The findings demonstrate that social media sentiment analysis can serve as an effective early indicator of fashion trend trajectories when proper statistical validation is applied. Our improved predictive model achieved 78.35% balanced accuracy in sentiment classification, establishing a reliable foundation for trend prediction across positive, neutral, and negative sentiment categories.","authors":["Aayam Bansal","Agneya Tharun"],"url":"https://arxiv.org/abs/2505.00050"}
{"created":"2025-05-02","title":"Computational Complexity of UAP Reverse Engineering: A Formal Analysis of Automaton Identification and Data Complexity","abstract":"This white paper demonstrates that reverse engineering Unidentified Aerial Phenomena (UAP) is NP-complete under classical computational paradigms. By modeling UAP reconstruction as an automaton identification problem with a state characterization matrix M(D, T, E) and examining the inherent challenges in data gathering as well as unknown physics, we show that inferring internal mechanisms (such as Isotopically-Engineered-Materials or unconventional propulsion systems) from finite observational data is computationally intractable. Data D, comprising both operational non-reproducible observations and reproducible analysis data from purported crash retrievals, remains inherently fragmentary. Even if UAP observables were reproducible, the absence of a comprehensive theoretical framework ensures that reverse engineering remains NP-complete, and may escalate to PSPACE-hard or to an Entscheidungsproblem. This intractability challenges current UAP reverse engineering efforts and has profound implications for transparency on UAP technology and related venture investments. Hence, UAP are as analogous to modern smartphones in the hands of Neanderthals.","authors":["Karim Daghbouche"],"url":"https://arxiv.org/abs/2505.00051"}
{"created":"2025-05-02","title":"Algorithmic Addiction by Design: Big Tech's Leverage of Dark Patterns to Maintain Market Dominance and its Challenge for Content Moderation","abstract":"Today's largest technology corporations, especially ones with consumer-facing products such as social media platforms, use a variety of unethical and often outright illegal tactics to maintain their dominance. One tactic that has risen to the level of the public consciousness is the concept of addictive design, evidenced by the fact that excessive social media use has become a salient problem, particularly in the mental and social development of adolescents and young adults. As tech companies have developed more and more sophisticated artificial intelligence (AI) models to power their algorithmic recommender systems, they will become more successful at their goal of ensuring addiction to their platforms. This paper explores how online platforms intentionally cultivate addictive user behaviors and the broad societal implications, including on the health and well-being of children and adolescents. It presents the usage of addictive design - including the usage of dark patterns, persuasive design elements, and recommender algorithms - as a tool leveraged by technology corporations to maintain their dominance. Lastly, it describes the challenge of content moderation to address the problem and gives an overview of solutions at the policy level to counteract addictive design.","authors":["Michelle Nie"],"url":"https://arxiv.org/abs/2505.00054"}
{"created":"2025-05-02","title":"TinyMA-IEI-PPO: Exploration Incentive-Driven Multi-Agent DRL with Self-Adaptive Pruning for Vehicular Embodied AI Agent Twins Migration","abstract":"Embodied Artificial Intelligence (EAI) addresses autonomous driving challenges in Vehicular Embodied AI Networks (VEANETs) through multi-modal perception, adaptive decision-making, and hardware-software co-scheduling. However, the computational demands of virtual services and the inherent mobility of autonomous vehicles (AVs) necessitate real-time migration of Vehicular Embodied Agent AI Twins (VEAATs) between resource-constrained Roadside Units (RSUs). This paper proposes a novel framework for efficient VEAAT migration in VEANETs, combining a multi-leader multi-follower (MLMF) Stackelberg game-theoretic incentive mechanism with a tiny multi-agent deep reinforcement learning (MADRL) algorithm. First, We propose an virtual immersive experience-driven utility model that captures AV-RSU dynamic interactions by integrating AVs' social influence, service complementarity and substitutability, and RSUs' resource allocation strategies to optimize VEAAT migration decisions. Second, to enhance training efficiency and enable efficient deployment on computation-constrained AVs while preserving exploration-exploitation performance, we propose TinyMA-IEI-PPO, a self-adaptive dynamic structured pruning algorithm that dynamically adjusts neuron importance based on agents' exploration incentives. Numerical results demonstrate that our approach achieves convergence comparable to baseline models and closely approximates the Stackelberg equilibrium.","authors":["Zhuoqi Zeng","Yuxiang Wei","Jiawen Kang"],"url":"https://arxiv.org/abs/2505.00055"}
{"created":"2025-05-02","title":"Clustering Internet Memes Through Template Matching and Multi-Dimensional Similarity","abstract":"Meme clustering is critical for toxicity detection, virality modeling, and typing, but it has received little attention in previous research. Clustering similar Internet memes is challenging due to their multimodality, cultural context, and adaptability. Existing approaches rely on databases, overlook semantics, and struggle to handle diverse dimensions of similarity. This paper introduces a novel method that uses template-based matching with multi-dimensional similarity features, thus eliminating the need for predefined databases and supporting adaptive matching. Memes are clustered using local and global features across similarity categories such as form, visual content, text, and identity. Our combined approach outperforms existing clustering methods, producing more consistent and coherent clusters, while similarity-based feature sets enable adaptability and align with human intuition. We make all supporting code publicly available to support subsequent research. Code: https://github.com/tygobl/meme-clustering","authors":["Tygo Bloem","Filip Ilievski"],"url":"https://arxiv.org/abs/2505.00056"}
{"created":"2025-05-02","title":"A Report on the llms evaluating the high school questions","abstract":"This report aims to evaluate the performance of large language models (LLMs) in solving high school science questions and to explore their potential applications in the educational field. With the rapid development of LLMs in the field of natural language processing, their application in education has attracted widespread attention. This study selected mathematics exam questions from the college entrance examinations (2019-2023) as evaluation data and utilized at least eight LLM APIs to provide answers. A comprehensive assessment was conducted based on metrics such as accuracy, response time, logical reasoning, and creativity. Through an in-depth analysis of the evaluation results, this report reveals the strengths and weaknesses of LLMs in handling high school science questions and discusses their implications for educational practice. The findings indicate that although LLMs perform excellently in certain aspects, there is still room for improvement in logical reasoning and creative problem-solving. This report provides an empirical foundation for further research and application of LLMs in the educational field and offers suggestions for improvement.","authors":["Zhu Jiawei","Chen Wei"],"url":"https://arxiv.org/abs/2505.00057"}
{"created":"2025-05-02","title":"BERSting at the Screams: A Benchmark for Distanced, Emotional and Shouted Speech Recognition","abstract":"Some speech recognition tasks, such as automatic speech recognition (ASR), are approaching or have reached human performance in many reported metrics. Yet, they continue to struggle in complex, real-world, situations, such as with distanced speech. Previous challenges have released datasets to address the issue of distanced ASR, however, the focus remains primarily on distance, specifically relying on multi-microphone array systems. Here we present the B(asic) E(motion) R(andom phrase) S(hou)t(s) (BERSt) dataset. The dataset contains almost 4 hours of English speech from 98 actors with varying regional and non-native accents. The data was collected on smartphones in the actors homes and therefore includes at least 98 different acoustic environments. The data also includes 7 different emotion prompts and both shouted and spoken utterances. The smartphones were places in 19 different positions, including obstructions and being in a different room than the actor. This data is publicly available for use and can be used to evaluate a variety of speech recognition tasks, including: ASR, shout detection, and speech emotion recognition (SER). We provide initial benchmarks for ASR and SER tasks, and find that ASR degrades both with an increase in distance and shout level and shows varied performance depending on the intended emotion. Our results show that the BERSt dataset is challenging for both ASR and SER tasks and continued work is needed to improve the robustness of such systems for more accurate real-world use.","authors":["Paige Tutt\\\"os\\'i","Mantaj Dhillon","Luna Sang","Shane Eastwood","Poorvi Bhatia","Quang Minh Dinh","Avni Kapoor","Yewon Jin","Angelica Lim"],"url":"https://arxiv.org/abs/2505.00059"}
{"created":"2025-05-02","title":"Fact-Consistency Evaluation of Text-to-SQL Generation for Business Intelligence Using Exaone 3.5","abstract":"Large Language Models (LLMs) have shown promise in enabling natural language interfaces for structured data querying through text-to-SQL generation. However, their application in real-world Business Intelligence (BI) contexts remains limited due to semantic hallucinations, structural errors, and a lack of domain-specific evaluation frameworks. In this study, we propose a Fact-Consistency Evaluation Framework for assessing the semantic accuracy of LLM-generated SQL outputs using Exaone 3.5--an instruction-tuned, bilingual LLM optimized for enterprise tasks. We construct a domain-specific benchmark comprising 219 natural language business questions across five SQL complexity levels, derived from actual sales data in LG Electronics' internal BigQuery environment. Each question is paired with a gold-standard SQL query and a validated ground-truth answer. We evaluate model performance using answer accuracy, execution success rate, semantic error rate, and non-response rate. Experimental results show that while Exaone 3.5 performs well on simple aggregation tasks (93% accuracy in L1), it exhibits substantial degradation in arithmetic reasoning (4% accuracy in H1) and grouped ranking tasks (31% in H4), with semantic errors and non-responses concentrated in complex cases. Qualitative error analysis further identifies common failure types such as misapplied arithmetic logic, incomplete filtering, and incorrect grouping operations. Our findings highlight the current limitations of LLMs in business-critical environments and underscore the need for fact-consistency validation layers and hybrid reasoning approaches. This work contributes a reproducible benchmark and evaluation methodology for advancing reliable natural language interfaces to structured enterprise data systems.","authors":["Jeho Choi"],"url":"https://arxiv.org/abs/2505.00060"}
{"created":"2025-05-02","title":"Enhancing Security and Strengthening Defenses in Automated Short-Answer Grading Systems","abstract":"This study examines vulnerabilities in transformer-based automated short-answer grading systems used in medical education, with a focus on how these systems can be manipulated through adversarial gaming strategies. Our research identifies three main types of gaming strategies that exploit the system's weaknesses, potentially leading to false positives. To counteract these vulnerabilities, we implement several adversarial training methods designed to enhance the systems' robustness. Our results indicate that these methods significantly reduce the susceptibility of grading systems to such manipulations, especially when combined with ensemble techniques like majority voting and ridge regression, which further improve the system's defense against sophisticated adversarial inputs. Additionally, employing large language models such as GPT-4 with varied prompting techniques has shown promise in recognizing and scoring gaming strategies effectively. The findings underscore the importance of continuous improvements in AI-driven educational tools to ensure their reliability and fairness in high-stakes settings.","authors":["Sahar Yarmohammadtoosky","Yiyun Zhou","Victoria Yaneva","Peter Baldwin","Saed Rezayi","Brian Clauser","Polina Harikeo"],"url":"https://arxiv.org/abs/2505.00061"}
{"created":"2025-05-02","title":"GDI-Bench: A Benchmark for General Document Intelligence with Vision and Reasoning Decoupling","abstract":"The rapid advancement of multimodal large language models (MLLMs) has profoundly impacted the document domain, creating a wide array of application scenarios. This progress highlights the need for a comprehensive benchmark to evaluate these models' capabilities across various document-specific tasks. However, existing benchmarks often fail to locate specific model weaknesses or guide systematic improvements. To bridge this gap, we introduce a General Document Intelligence Benchmark (GDI-Bench), featuring 1.9k images across 9 key scenarios and 19 document-specific tasks. By decoupling visual complexity and reasoning complexity, the GDI-Bench structures graded tasks that allow performance assessment by difficulty, aiding in model weakness identification and optimization guidance. We evaluate the GDI-Bench on various open-source and closed-source models, conducting decoupled analyses in the visual and reasoning domains. For instance, the GPT-4o model excels in reasoning tasks but exhibits limitations in visual capabilities. To address the diverse tasks and domains in the GDI-Bench, we propose a GDI Model that mitigates the issue of catastrophic forgetting during the supervised fine-tuning (SFT) process through a intelligence-preserving training strategy. Our model achieves state-of-the-art performance on previous benchmarks and the GDI-Bench. Both our benchmark and model will be open source.","authors":["Siqi Li","Yufan Shen","Xiangnan Chen","Jiayi Chen","Hengwei Ju","Haodong Duan","Song Mao","Hongbin Zhou","Bo Zhang","Pinlong Cai","Licheng Wen","Botian Shi","Yong Liu","Xinyu Cai","Yu Qiao"],"url":"https://arxiv.org/abs/2505.00063"}
{"created":"2025-05-02","title":"ConSens: Assessing context grounding in open-book question answering","abstract":"Large Language Models (LLMs) have demonstrated considerable success in open-book question answering (QA), where the task requires generating answers grounded in a provided external context. A critical challenge in open-book QA is to ensure that model responses are based on the provided context rather than its parametric knowledge, which can be outdated, incomplete, or incorrect. Existing evaluation methods, primarily based on the LLM-as-a-judge approach, face significant limitations, including biases, scalability issues, and dependence on costly external systems. To address these challenges, we propose a novel metric that contrasts the perplexity of the model response under two conditions: when the context is provided and when it is not. The resulting score quantifies the extent to which the model's answer relies on the provided context. The validity of this metric is demonstrated through a series of experiments that show its effectiveness in identifying whether a given answer is grounded in the provided context. Unlike existing approaches, this metric is computationally efficient, interpretable, and adaptable to various use cases, offering a scalable and practical solution to assess context utilization in open-book QA systems.","authors":["Ivan Vankov","Matyo Ivanov","Adriana Correia","Victor Botev"],"url":"https://arxiv.org/abs/2505.00065"}
{"created":"2025-05-02","title":"CoordField: Coordination Field for Agentic UAV Task Allocation In Low-altitude Urban Scenarios","abstract":"With the increasing demand for heterogeneous Unmanned Aerial Vehicle (UAV) swarms to perform complex tasks in urban environments, system design now faces major challenges, including efficient semantic understanding, flexible task planning, and the ability to dynamically adjust coordination strategies in response to evolving environmental conditions and continuously changing task requirements. To address the limitations of existing approaches, this paper proposes coordination field agentic system for coordinating heterogeneous UAV swarms in complex urban scenarios. In this system, large language models (LLMs) is responsible for interpreting high-level human instructions and converting them into executable commands for the UAV swarms, such as patrol and target tracking. Subsequently, a Coordination field mechanism is proposed to guide UAV motion and task selection, enabling decentralized and adaptive allocation of emergent tasks. A total of 50 rounds of comparative testing were conducted across different models in a 2D simulation space to evaluate their performance. Experimental results demonstrate that the proposed system achieves superior performance in terms of task coverage, response time, and adaptability to dynamic changes.","authors":["Tengchao Zhang","Yonglin Tian","Fei Lin","Jun Huang","Rui Qin","Fei-Yue Wang"],"url":"https://arxiv.org/abs/2505.00091"}
{"created":"2025-05-02","title":"Evaluating the AI-Lab Intervention: Impact on Student Perception and Use of Generative AI in Early Undergraduate Computer Science Courses","abstract":"Generative AI (GenAI) is rapidly entering computer science education, yet its effects on student learning, skill development, and perceptions remain underexplored. Concerns about overreliance coexist with a gap in research on structured scaffolding to guide tool use in formal courses. This study examines the impact of a dedicated \"AI-Lab\" intervention -- emphasizing guided scaffolding and mindful engagement -- on undergraduate students in Data Structures and Algorithms, Competitive Programming, and first-year engineering courses at Purdue University.","authors":["Ethan Dickey","Andres Bejarano","Rhianna Kuperus","B\\'arbara Fagundes"],"url":"https://arxiv.org/abs/2505.00100"}
{"created":"2025-05-02","title":"From Lab to Wrist: Bridging Metabolic Monitoring and Consumer Wearables for Heart Rate and Oxygen Consumption Modeling","abstract":"Understanding physiological responses during running is critical for performance optimization, tailored training prescriptions, and athlete health management. We introduce a comprehensive framework -- what we believe to be the first capable of predicting instantaneous oxygen consumption (VO$_{2}$) trajectories exclusively from consumer-grade wearable data. Our approach employs two complementary physiological models: (1) accurate modeling of heart rate (HR) dynamics via a physiologically constrained ordinary differential equation (ODE) and neural Kalman filter, trained on over 3 million HR observations, achieving 1-second interval predictions with mean absolute errors as low as 2.81\\,bpm (correlation 0.87); and (2) leveraging the principles of precise HR modeling, a novel VO$_{2}$ prediction architecture requiring only the initial second of VO$_{2}$ data for calibration, enabling robust, sequence-to-sequence metabolic demand estimation. Despite relying solely on smartwatch and chest-strap data, our method achieves mean absolute percentage errors of approximately 13\\%, effectively capturing rapid physiological transitions and steady-state conditions across diverse running intensities. Our synchronized dataset, complemented by blood lactate measurements, further lays the foundation for future noninvasive metabolic zone identification. By embedding physiological constraints within modern machine learning, this framework democratizes advanced metabolic monitoring, bridging laboratory-grade accuracy and everyday accessibility, thus empowering both elite athletes and recreational fitness enthusiasts.","authors":["Barak Gahtan","Sanketh Vedula","Gil Samuelly Leichtag","Einat Kodesh","Alex M. Bronstein"],"url":"https://arxiv.org/abs/2505.00101"}
{"created":"2025-05-02","title":"Optimization of embeddings storage for RAG systems using quantization and dimensionality reduction techniques","abstract":"Retrieval-Augmented Generation enhances language models by retrieving relevant information from external knowledge bases, relying on high-dimensional vector embeddings typically stored in float32 precision. However, storing these embeddings at scale presents significant memory challenges. To address this issue, we systematically investigate on MTEB benchmark two complementary optimization strategies: quantization, evaluating standard formats (float16, int8, binary) and low-bit floating-point types (float8), and dimensionality reduction, assessing methods like PCA, Kernel PCA, UMAP, Random Projections and Autoencoders. Our results show that float8 quantization achieves a 4x storage reduction with minimal performance degradation (<0.3%), significantly outperforming int8 quantization at the same compression level, being simpler to implement. PCA emerges as the most effective dimensionality reduction technique. Crucially, combining moderate PCA (e.g., retaining 50% dimensions) with float8 quantization offers an excellent trade-off, achieving 8x total compression with less performance impact than using int8 alone (which provides only 4x compression). To facilitate practical application, we propose a methodology based on visualizing the performance-storage trade-off space to identify the optimal configuration that maximizes performance within their specific memory constraints.","authors":["Naam\\'an Huerga-P\\'erez","Rub\\'en \\'Alvarez","Rub\\'en Ferrero-Guill\\'en","Alberto Mart\\'inez-Guti\\'errez","Javier D\\'iez-Gonz\\'alez"],"url":"https://arxiv.org/abs/2505.00105"}
{"created":"2025-05-02","title":"Security-by-Design at the Telco Edge with OSS: Challenges and Lessons Learned","abstract":"This paper presents our experience, in the context of an industrial R&amp;D project, on securing GENIO, a platform for edge computing on Passive Optical Network (PON) infrastructures, and based on Open-Source Software (OSS). We identify threats and related mitigations through hardening, vulnerability management, digital signatures, and static and dynamic analysis. In particular, we report lessons learned in applying these mitigations using OSS, and share our findings about the maturity and limitations of these security solutions in an industrial context.","authors":["Carmine Cesarano","Alessio Foggia","Gianluca Roscigno","Luca Andreani","Roberto Natella"],"url":"https://arxiv.org/abs/2505.00111"}
{"created":"2025-05-02","title":"Fine-Tuning LLMs for Low-Resource Dialect Translation: The Case of Lebanese","abstract":"This paper examines the effectiveness of Large Language Models (LLMs) in translating the low-resource Lebanese dialect, focusing on the impact of culturally authentic data versus larger translated datasets. We compare three fine-tuning approaches: Basic, contrastive, and grammar-hint tuning, using open-source Aya23 models. Experiments reveal that models fine-tuned on a smaller but culturally aware Lebanese dataset (LW) consistently outperform those trained on larger, non-native data. The best results were achieved through contrastive fine-tuning paired with contrastive prompting, which indicates the benefits of exposing translation models to bad examples. In addition, to ensure authentic evaluation, we introduce LebEval, a new benchmark derived from native Lebanese content, and compare it to the existing FLoRes benchmark. Our findings challenge the \"More Data is Better\" paradigm and emphasize the crucial role of cultural authenticity in dialectal translation. We made our datasets and code available on Github.","authors":["Silvana Yakhni","Ali Chehab"],"url":"https://arxiv.org/abs/2505.00114"}
{"created":"2025-05-02","title":"Between Underthinking and Overthinking: An Empirical Study of Reasoning Length and correctness in LLMs","abstract":"Large language models (LLMs) are increasingly optimized for long reasoning, under the assumption that more reasoning leads to better performance. However, emerging evidence suggests that longer responses can sometimes degrade accuracy rather than improve it. In this paper, we conduct a systematic empirical study of the relationship between reasoning length and answer correctness. We find that LLMs tend to overthink simple problems, generating unnecessarily long outputs, and underthink harder ones, failing to extend their reasoning when it is most needed. This indicates that models might misjudge problem difficulty and fail to calibrate their response length appropriately. Furthermore, we investigate the effects of length reduction with a preference optimization algorithm when simply preferring the shorter responses regardless of answer correctness. Experiments show that the generation length can be significantly reduced while maintaining acceptable accuracy. Our findings highlight generation length as a meaningful signal for reasoning behavior and motivate further exploration into LLMs' self-awareness in reasoning length adaptation.","authors":["Jinyan Su","Jennifer Healey","Preslav Nakov","Claire Cardie"],"url":"https://arxiv.org/abs/2505.00127"}
{"created":"2025-05-02","title":"Extension operators and geometric decompositions","abstract":"Geometric decomposition is a widely used tool for constructing local bases for finite element spaces. For finite element spaces of differential forms on simplicial meshes, Arnold, Falk, and Winther showed that geometric decompositions can be constructed from extension operators satisfying certain properties. In this paper, we generalize their results to function spaces and meshes satisfying very minimal hypotheses, while at the same time reducing the conditions that must hold for the extension operators. In particular, the geometry of the mesh and the mesh elements can be completely arbitrary, and the function spaces need only have well-defined restrictions to subelements. In this general context, we show that extension operators yield geometric decompositions for both the primal and dual function spaces. Later, we specialize to simplicial meshes, and we show that, to obtain geometric decompositions, one needs only to construct extension operators on the reference simplex in each dimension. In particular, for simplicial meshes, the existence of geometric decompositions depends only on the dimension of the mesh.","authors":["Yakov Berchenko-Kogan"],"url":"https://arxiv.org/abs/2505.00129"}
{"created":"2025-05-02","title":"Kernel-Based Ensemble Gaussian Mixture Probability Hypothesis Density Filter","abstract":"In this work, a kernel-based Ensemble Gaussian Mixture Probability Hypothesis Density (EnGM-PHD) filter is presented for multi-target filtering applications. The EnGM-PHD filter combines the Gaussian-mixture-based techniques of the Gaussian Mixture Probability Hypothesis Density (GM-PHD) filter with the particle-based techniques of the Sequential Monte Carlo Probability Hypothesis Density (SMC-PHD) filter. It achieves this by obtaining particles from the posterior intensity function, propagating them through the system dynamics, and then using Kernel Density Estimation (KDE) techniques to approximate the Gaussian mixture of the prior intensity function. This approach guarantees convergence to the true intensity function in the limit of the number of components. Moreover, in the special case of a single target with no births, deaths, clutter, and perfect detection probability, the EnGM-PHD filter reduces to the standard Ensemble Gaussian Mixture Filter (EnGMF). In the presented experiment, the results indicate that the EnGM-PHD filter achieves better multi-target filtering performance than both the GM-PHD and SMC-PHD filters while using the same number of components or particles.","authors":["Dalton Durant","Renato Zanetti"],"url":"https://arxiv.org/abs/2505.00131"}
{"created":"2025-05-02","title":"Investigating Zero-Shot Diagnostic Pathology in Vision-Language Models with Efficient Prompt Design","abstract":"Vision-language models (VLMs) have gained significant attention in computational pathology due to their multimodal learning capabilities that enhance big-data analytics of giga-pixel whole slide image (WSI). However, their sensitivity to large-scale clinical data, task formulations, and prompt design remains an open question, particularly in terms of diagnostic accuracy. In this paper, we present a systematic investigation and analysis of three state of the art VLMs for histopathology, namely Quilt-Net, Quilt-LLAVA, and CONCH, on an in-house digestive pathology dataset comprising 3,507 WSIs, each in giga-pixel form, across distinct tissue types. Through a structured ablative study on cancer invasiveness and dysplasia status, we develop a comprehensive prompt engineering framework that systematically varies domain specificity, anatomical precision, instructional framing, and output constraints. Our findings demonstrate that prompt engineering significantly impacts model performance, with the CONCH model achieving the highest accuracy when provided with precise anatomical references. Additionally, we identify the critical importance of anatomical context in histopathological image analysis, as performance consistently degraded when reducing anatomical precision. We also show that model complexity alone does not guarantee superior performance, as effective domain alignment and domain-specific training are critical. These results establish foundational guidelines for prompt engineering in computational pathology and highlight the potential of VLMs to enhance diagnostic accuracy when properly instructed with domain-appropriate prompts.","authors":["Vasudev Sharma","Ahmed Alagha","Abdelhakim Khellaf","Vincent Quoc-Huy Trinh","Mahdi S. Hosseini"],"url":"https://arxiv.org/abs/2505.00134"}
{"created":"2025-05-02","title":"Eye2Eye: A Simple Approach for Monocular-to-Stereo Video Synthesis","abstract":"The rising popularity of immersive visual experiences has increased interest in stereoscopic 3D video generation. Despite significant advances in video synthesis, creating 3D videos remains challenging due to the relative scarcity of 3D video data. We propose a simple approach for transforming a text-to-video generator into a video-to-stereo generator. Given an input video, our framework automatically produces the video frames from a shifted viewpoint, enabling a compelling 3D effect. Prior and concurrent approaches for this task typically operate in multiple phases, first estimating video disparity or depth, then warping the video accordingly to produce a second view, and finally inpainting the disoccluded regions. This approach inherently fails when the scene involves specular surfaces or transparent objects. In such cases, single-layer disparity estimation is insufficient, resulting in artifacts and incorrect pixel shifts during warping. Our work bypasses these restrictions by directly synthesizing the new viewpoint, avoiding any intermediate steps. This is achieved by leveraging a pre-trained video model's priors on geometry, object materials, optics, and semantics, without relying on external geometry models or manually disentangling geometry from the synthesis process. We demonstrate the advantages of our approach in complex, real-world scenarios featuring diverse object materials and compositions. See videos on https://video-eye2eye.github.io","authors":["Michal Geyer","Omer Tov","Linyi Jin","Richard Tucker","Inbar Mosseri","Tali Dekel","Noah Snavely"],"url":"https://arxiv.org/abs/2505.00135"}
{"created":"2025-05-02","title":"GPRat: Gaussian Process Regression with Asynchronous Tasks","abstract":"Python is the de-facto language for software development in artificial intelligence (AI). Commonly used libraries, such as PyTorch and TensorFlow, rely on parallelization built into their BLAS backends to achieve speedup on CPUs. However, only applying parallelization in a low-level backend can lead to performance and scaling degradation. In this work, we present a novel way of binding task-based C++ code built on the asynchronous runtime model HPX to a high-level Python API using pybind11. We develop a parallel Gaussian process (GP) li- brary as an application. The resulting Python library GPRat combines the ease of use of commonly available GP libraries with the performance and scalability of asynchronous runtime systems. We evaluate the per- formance on a mass-spring-damper system, a standard benchmark from control theory, for varying numbers of regressors (features). The results show almost no binding overhead when binding the asynchronous HPX code using pybind11. Compared to GPyTorch and GPflow, GPRat shows superior scaling on up to 64 cores on an AMD EPYC 7742 CPU for train- ing. Furthermore, our library achieves a prediction speedup of 7.63 over GPyTorch and 25.25 over GPflow. If we increase the number of features from eight to 128, we observe speedups of 29.62 and 21.19, respectively. These results showcase the potential of using asynchronous tasks within Python-based AI applications.","authors":["Maksim Helmann","Alexander Strack","Dirk Pfl\\\"uger"],"url":"https://arxiv.org/abs/2505.00136"}
{"created":"2025-05-02","title":"Q Cells in Wireless Networks","abstract":"For a given set of transmitters such as cellular base stations or WiFi access points, is it possible to analytically characterize the set of locations that are \"covered\" in the sense that users at these locations experience a certain minimum quality of service? In this paper, we affirmatively answer this question, by providing explicit simple outer bounds and estimates for the coverage manifold. The key geometric elements of our analytical method are the Q cells, defined as the intersections of a small number of disks. The Q cell of a transmitter is an outer bound to the service region of the transmitter, and, in turn, the union of Q cells is an outer bound to the coverage manifold. In infinite networks, connections to the meta distribution of the signal-to-interference ratio allow for a scaling of the Q cells to obtain accurate estimates of the coverage manifold.","authors":["Martin Haenggi"],"url":"https://arxiv.org/abs/2505.00138"}
{"created":"2025-05-02","title":"Counting Specific Classes of Relations Regarding Fixed Points and Reflexive Points","abstract":"Given a finite and non-empty set $X$ and randomly selected specific functions and relations on $X$, we investigate the existence and non-existence of fixed points and reflexive points, respectively. First, we consider the class of functions, weaken it to the classes of partial functions, total relations and general relations and also strengthen it to the class of permutations. Then we investigate the class of involutions and the subclass of proper involutions. Finally, we treat idempotent functions, partial idempotent functions and related concepts. We count relations, calculate corresponding probabilities and also calculate the limiting values of the latter in case that the cardinality of $X$ tends to infinity. All these results have been motivated and also supported by numerous experiments performed with the RelView tool.","authors":["Rudolf Berghammer","Jules Desharnais","Michael Winter"],"url":"https://arxiv.org/abs/2505.00140"}
{"created":"2025-05-02","title":"When Deep Learning Meets Information Retrieval-based Bug Localization: A Survey","abstract":"Bug localization is a crucial aspect of software maintenance, running through the entire software lifecycle. Information retrieval-based bug localization (IRBL) identifies buggy code based on bug reports, expediting the bug resolution process for developers. Recent years have witnessed significant achievements in IRBL, propelled by the widespread adoption of deep learning (DL). To provide a comprehensive overview of the current state of the art and delve into key issues, we conduct a survey encompassing 61 IRBL studies leveraging DL. We summarize best practices in each phase of the IRBL workflow, undertake a meta-analysis of prior studies, and suggest future research directions. This exploration aims to guide further advancements in the field, fostering a deeper understanding and refining practices for effective bug localization. Our study suggests that the integration of DL in IRBL enhances the model's capacity to extract semantic and syntactic information from both bug reports and source code, addressing issues such as lexical gaps, neglect of code structure information, and cold-start problems. Future research avenues for IRBL encompass exploring diversity in programming languages, adopting fine-grained granularity, and focusing on real-world applications. Most importantly, although some studies have started using large language models for IRBL, there is still a need for more in-depth exploration and thorough investigation in this area.","authors":["Feifei Niu","Chuanyi Li","Kui Liu","Xin Xia","David Lo"],"url":"https://arxiv.org/abs/2505.00144"}
{"created":"2025-05-02","title":"AdaptMI: Adaptive Skill-based In-context Math Instruction for Small Language Models","abstract":"In-context learning (ICL) allows a language model to improve its problem-solving capability when provided with suitable information in context. Since the choice of in-context information can be determined based on the problem itself, in-context learning is analogous to human learning from teachers in a classroom. Recent works (Didolkar et al., 2024a; 2024b) show that ICL performance can be improved by leveraging a frontier large language model's (LLM) ability to predict required skills to solve a problem, popularly referred to as an LLM's metacognition, and using the recommended skills to construct necessary in-context examples. While this skill-based strategy boosts ICL performance in larger models, its gains on small language models (SLMs) have been minimal, highlighting a performance gap in ICL capabilities. We investigate this gap and show that skill-based prompting can hurt SLM performance on easy questions by introducing unnecessary information, akin to cognitive overload. To address this, we introduce AdaptMI, an adaptive approach to selecting skill-based in-context Math Instructions for SLMs. Inspired by cognitive load theory from human pedagogy, our method only introduces skill-based examples when the model performs poorly. We further propose AdaptMI+, which adds examples targeted to the specific skills missing from the model's responses. On 5-shot evaluations across popular math benchmarks and five SLMs (1B--7B; Qwen, Llama), AdaptMI+ improves accuracy by up to 6% over naive skill-based strategies.","authors":["Yinghui He","Abhishek Panigrahi","Yong Lin","Sanjeev Arora"],"url":"https://arxiv.org/abs/2505.00147"}
{"created":"2025-05-02","title":"Detecting and Mitigating Hateful Content in Multimodal Memes with Vision-Language Models","abstract":"The rapid evolution of social media has provided enhanced communication channels for individuals to create online content, enabling them to express their thoughts and opinions. Multimodal memes, often utilized for playful or humorous expressions with visual and textual elements, are sometimes misused to disseminate hate speech against individuals or groups. While the detection of hateful memes is well-researched, developing effective methods to transform hateful content in memes remains a significant challenge. Leveraging the powerful generation and reasoning capabilities of Vision-Language Models (VLMs), we address the tasks of detecting and mitigating hateful content. This paper presents two key contributions: first, a definition-guided prompting technique for detecting hateful memes, and second, a unified framework for mitigating hateful content in memes, named UnHateMeme, which works by replacing hateful textual and/or visual components. With our definition-guided prompts, VLMs achieve impressive performance on hateful memes detection task. Furthermore, our UnHateMeme framework, integrated with VLMs, demonstrates a strong capability to convert hateful memes into non-hateful forms that meet human-level criteria for hate speech and maintain multimodal coherence between image and text. Through empirical experiments, we show the effectiveness of state-of-the-art pretrained VLMs such as LLaVA, Gemini and GPT-4o on the proposed tasks, providing a comprehensive analysis of their respective strengths and limitations for these tasks. This paper aims to shed light on important applications of VLMs for ensuring safe and respectful online environments.","authors":["Minh-Hao Van","Xintao Wu"],"url":"https://arxiv.org/abs/2505.00150"}
{"created":"2025-05-02","title":"Audo-Sight: Enabling Ambient Interaction For Blind And Visually Impaired Individuals","abstract":"Visually impaired people face significant challenges when attempting to interact with and understand complex environments, and traditional assistive technologies often struggle to quickly provide necessary contextual understanding and interactive intelligence. This thesis presents Audo-Sight, a state-of-the-art assistive system that seamlessly integrates Multimodal Large Language Models (MLLMs) to provide expedient, context-aware interactions for Blind and Visually Impaired (BVI) individuals. The system operates in two different modalities: personalized interaction through user identification and public access in common spaces like museums and shopping malls. In tailored environments, the system adjusts its output to conform to the preferences of individual users, thus enhancing accessibility through a user-aware form of interaction. In shared environments, Audo-Sight employs a shared architecture that adapts to its current user with no manual reconfiguration required. To facilitate appropriate interactions with the LLM, the public Audo-Sight solution includes an Age-Range Determiner and Safe Query Filter. Additionally, the system ensures that responses are respectful to BVI users through NeMo Guardrails. By utilizing multimodal reasoning, BVI-cognizant response editing, and safeguarding features, this work represents a major leap in AI-driven accessibility technology capable of increasing autonomy, safety, and interaction for people with visual impairments in social settings. Finally, we present the integration of Audo-Sight and SmartSight, which enables enhanced situational awareness for BVI individuals. This integration takes advantage of the real-time visual analysis of SmartSight, combined with the extensive reasoning and interactive capabilities of Audo-Sight, and goes beyond object identification to provide context-driven, voice-controlled assistance in dynamic environments.","authors":["Bhanuja Ainary"],"url":"https://arxiv.org/abs/2505.00153"}
{"created":"2025-05-02","title":"V3LMA: Visual 3D-enhanced Language Model for Autonomous Driving","abstract":"Large Vision Language Models (LVLMs) have shown strong capabilities in understanding and analyzing visual scenes across various domains. However, in the context of autonomous driving, their limited comprehension of 3D environments restricts their effectiveness in achieving a complete and safe understanding of dynamic surroundings. To address this, we introduce V3LMA, a novel approach that enhances 3D scene understanding by integrating Large Language Models (LLMs) with LVLMs. V3LMA leverages textual descriptions generated from object detections and video inputs, significantly boosting performance without requiring fine-tuning. Through a dedicated preprocessing pipeline that extracts 3D object data, our method improves situational awareness and decision-making in complex traffic scenarios, achieving a score of 0.56 on the LingoQA benchmark. We further explore different fusion strategies and token combinations with the goal of advancing the interpretation of traffic scenes, ultimately enabling safer autonomous driving systems.","authors":["Jannik L\\\"ubberstedt","Esteban Rivera","Nico Uhlemann","Markus Lienkamp"],"url":"https://arxiv.org/abs/2505.00156"}
{"created":"2025-05-02","title":"Optimized Lattice-Structured Flexible EIT Sensor for Tactile Reconstruction and Classification","abstract":"Flexible electrical impedance tomography (EIT) offers a promising alternative to traditional tactile sensing approaches, enabling low-cost, scalable, and deformable sensor designs. Here, we propose an optimized lattice-structured flexible EIT tactile sensor incorporating a hydrogel-based conductive layer, systematically designed through three-dimensional coupling field simulations to optimize structural parameters for enhanced sensitivity and robustness. By tuning the lattice channel width and conductive layer thickness, we achieve significant improvements in tactile reconstruction quality and classification performance. Experimental results demonstrate high-quality tactile reconstruction with correlation coefficients up to 0.9275, peak signal-to-noise ratios reaching 29.0303 dB, and structural similarity indexes up to 0.9660, while maintaining low relative errors down to 0.3798. Furthermore, the optimized sensor accurately classifies 12 distinct tactile stimuli with an accuracy reaching 99.6%. These results highlight the potential of simulation-guided structural optimization for advancing flexible EIT-based tactile sensors toward practical applications in wearable systems, robotics, and human-machine interfaces.","authors":["Huazhi Dong","Sihao Teng","Xu Han","Xiaopeng Wu","Francesco Giorgio-Serchi","Yunjie Yang"],"url":"https://arxiv.org/abs/2505.00161"}
{"created":"2025-05-02","title":"Stochastic Subspace Descent Accelerated via Bi-fidelity Line Search","abstract":"Efficient optimization remains a fundamental challenge across numerous scientific and engineering domains, especially when objective function and gradient evaluations are computationally expensive. While zeroth-order optimization methods offer effective approaches when gradients are inaccessible, their practical performance can be limited by the high cost associated with function queries. This work introduces the bi-fidelity stochastic subspace descent (BF-SSD) algorithm, a novel zeroth-order optimization method designed to reduce this computational burden. BF-SSD leverages a bi-fidelity framework, constructing a surrogate model from a combination of computationally inexpensive low-fidelity (LF) and accurate high-fidelity (HF) function evaluations. This surrogate model facilitates an efficient backtracking line search for step size selection, for which we provide theoretical convergence guarantees under standard assumptions. We perform a comprehensive empirical evaluation of BF-SSD across four distinct problems: a synthetic optimization benchmark, dual-form kernel ridge regression, black-box adversarial attacks on machine learning models, and transformer-based black-box language model fine-tuning. Numerical results demonstrate that BF-SSD consistently achieves superior optimization performance while requiring significantly fewer HF function evaluations compared to relevant baseline methods. This study highlights the efficacy of integrating bi-fidelity strategies within zeroth-order optimization, positioning BF-SSD as a promising and computationally efficient approach for tackling large-scale, high-dimensional problems encountered in various real-world applications.","authors":["Nuojin Cheng","Alireza Doostan","Stephen Becker"],"url":"https://arxiv.org/abs/2505.00162"}
{"created":"2025-05-02","title":"One-way Communication Complexity of Minimum Vertex Cover in General Graphs","abstract":"We study the communication complexity of the Minimum Vertex Cover (MVC) problem on general graphs within the \\(k\\)-party one-way communication model. Edges of an arbitrary \\(n\\)-vertex graph are distributed among \\(k\\) parties. The objective is for the parties to collectively find a small vertex cover of the graph while adhering to a communication protocol where each party sequentially sends a message to the next until the last party outputs a valid vertex cover of the whole graph. We are particularly interested in the trade-off between the size of the messages sent and the approximation ratio of the output solution.","authors":["Mahsa Derakhshan","Andisheh Ghasemi","Rajmohan Rajaraman"],"url":"https://arxiv.org/abs/2505.00164"}
{"created":"2025-05-02","title":"Deep Reinforcement Learning Policies for Underactuated Satellite Attitude Control","abstract":"Autonomy is a key challenge for future space exploration endeavours. Deep Reinforcement Learning holds the promises for developing agents able to learn complex behaviours simply by interacting with their environment. This paper investigates the use of Reinforcement Learning for the satellite attitude control problem, namely the angular reorientation of a spacecraft with respect to an in- ertial frame of reference. In the proposed approach, a set of control policies are implemented as neural networks trained with a custom version of the Proximal Policy Optimization algorithm to maneuver a small satellite from a random starting angle to a given pointing target. In particular, we address the problem for two working conditions: the nominal case, in which all the actuators (a set of 3 reac- tion wheels) are working properly, and the underactuated case, where an actuator failure is simulated randomly along with one of the axes. We show that the agents learn to effectively perform large-angle slew maneuvers with fast convergence and industry-standard pointing accuracy. Furthermore, we test the proposed method on representative hardware, showing that by taking adequate measures controllers trained in simulation can perform well in real systems.","authors":["Matteo El Hariry","Andrea Cini","Giacomo Mellone","Alessandro Balossino"],"url":"https://arxiv.org/abs/2505.00165"}
{"created":"2025-05-02","title":"Guidance and Control of Unmanned Surface Vehicles via HEOL","abstract":"This work presents a new approach to the guidance and control of marine craft via HEOL, i.e., a new way of combining flatness-based and model-free controllers. Its goal is to develop a general regulator for Unmanned Surface Vehicles (USV). To do so, the well-known USV maneuvering model is simplified into a nominal Hovercraft model which is flat. A flatness-based controller is derived for the simplified USV model and the loop is closed via an intelligent proportional-derivative (iPD) regulator. We thus associate the well-documented natural robustness of flatness-based control and adaptivity of iPDs. The controller is applied in simulation to two surface vessels, one meeting the simplifying hypotheses, the other one being a generic USV of the literature. It is shown to stabilize both systems even in the presence of unmodeled environmental disturbances.","authors":["Lo\\\"ick Degorre","Emmanuel Delaleau","C\\'edric Join","Michel Fliess"],"url":"https://arxiv.org/abs/2505.00168"}
{"created":"2025-05-02","title":"GEOM-Drugs Revisited: Toward More Chemically Accurate Benchmarks for 3D Molecule Generation","abstract":"Deep generative models have shown significant promise in generating valid 3D molecular structures, with the GEOM-Drugs dataset serving as a key benchmark. However, current evaluation protocols suffer from critical flaws, including incorrect valency definitions, bugs in bond order calculations, and reliance on force fields inconsistent with the reference data. In this work, we revisit GEOM-Drugs and propose a corrected evaluation framework: we identify and fix issues in data preprocessing, construct chemically accurate valency tables, and introduce a GFN2-xTB-based geometry and energy benchmark. We retrain and re-evaluate several leading models under this framework, providing updated performance metrics and practical recommendations for future benchmarking. Our results underscore the need for chemically rigorous evaluation practices in 3D molecular generation. Our recommended evaluation methods and GEOM-Drugs processing scripts are available at https://github.com/isayevlab/geom-drugs-3dgen-evaluation.","authors":["Filipp Nikitin","Ian Dunn","David Ryan Koes","Olexandr Isayev"],"url":"https://arxiv.org/abs/2505.00169"}
{"created":"2025-05-02","title":"Attention-enabled Explainable AI for Bladder Cancer Recurrence Prediction","abstract":"Non-muscle-invasive bladder cancer (NMIBC) is a relentless challenge in oncology, with recurrence rates soaring as high as 70-80%. Each recurrence triggers a cascade of invasive procedures, lifelong surveillance, and escalating healthcare costs - affecting 460,000 individuals worldwide. However, existing clinical prediction tools remain fundamentally flawed, often overestimating recurrence risk and failing to provide personalized insights for patient management. In this work, we propose an interpretable deep learning framework that integrates vector embeddings and attention mechanisms to improve NMIBC recurrence prediction performance. We incorporate vector embeddings for categorical variables such as smoking status and intravesical treatments, allowing the model to capture complex relationships between patient attributes and recurrence risk. These embeddings provide a richer representation of the data, enabling improved feature interactions and enhancing prediction performance. Our approach not only enhances performance but also provides clinicians with patient-specific insights by highlighting the most influential features contributing to recurrence risk for each patient. Our model achieves accuracy of 70% with tabular data, outperforming conventional statistical methods while providing clinician-friendly patient-level explanations through feature attention. Unlike previous studies, our approach identifies new important factors influencing recurrence, such as surgical duration and hospital stay, which had not been considered in existing NMIBC prediction models.","authors":["Saram Abbas","Naeem Soomro","Rishad Shafik","Rakesh Heer","Kabita Adhikari"],"url":"https://arxiv.org/abs/2505.00171"}
{"created":"2025-05-02","title":"First Order Logic with Fuzzy Semantics for Describing and Recognizing Nerves in Medical Images","abstract":"This article deals with the description and recognition of fiber bundles, in particular nerves, in medical images, based on the anatomical description of the fiber trajectories. To this end, we propose a logical formalization of this anatomical knowledge. The intrinsically imprecise description of nerves, as found in anatomical textbooks, leads us to propose fuzzy semantics combined with first-order logic. We define a language representing spatial entities, relations between these entities and quantifiers. A formula in this language is then a formalization of the natural language description. The semantics are given by fuzzy representations in a concrete domain and satisfaction degrees of relations. Based on this formalization, a spatial reasoning algorithm is proposed for segmentation and recognition of nerves from anatomical and diffusion magnetic resonance images, which is illustrated on pelvic nerves in pediatric imaging, enabling surgeons to plan surgery.","authors":["Isabelle Bloch","Enzo Bonnot","Pietro Gori","Giammarco La Barbera","Sabine Sarnacki"],"url":"https://arxiv.org/abs/2505.00173"}
{"created":"2025-05-02","title":"Real-World Gaps in AI Governance Research","abstract":"Drawing on 1,178 safety and reliability papers from 9,439 generative AI papers (January 2020 - March 2025), we compare research outputs of leading AI companies (Anthropic, Google DeepMind, Meta, Microsoft, and OpenAI) and AI universities (CMU, MIT, NYU, Stanford, UC Berkeley, and University of Washington). We find that corporate AI research increasingly concentrates on pre-deployment areas -- model alignment and testing & evaluation -- while attention to deployment-stage issues such as model bias has waned. Significant research gaps exist in high-risk deployment domains, including healthcare, finance, misinformation, persuasive and addictive features, hallucinations, and copyright. Without improved observability into deployed AI, growing corporate concentration could deepen knowledge deficits. We recommend expanding external researcher access to deployment data and systematic observability of in-market AI behaviors.","authors":["Ilan Strauss","Isobel Moure","Tim O'Reilly","Sruly Rosenblat"],"url":"https://arxiv.org/abs/2505.00174"}
{"created":"2025-05-02","title":"Generative Multimodal Multiscale Data Fusion for Digital Twins in Aerosol Jet Electronics Printing","abstract":"The rising demand for high-value electronics necessitates advanced manufacturing techniques capable of meeting stringent specifications for precise, complex, and compact devices, driving the shift toward innovative additive manufacturing (AM) solutions. Aerosol Jet Printing (AJP) is a versatile AM technique that utilizes aerosolized functional materials to accurately print intricate patterns onto diverse substrates. Machine learning (ML)- based Process-Structure-Property (PSP) modeling is essential for enhancing AJP manufacturing, as it quantitatively connects process parameters, structural features, and resulting material properties. However, current ML approaches for modeling PSP relationships in AJP face significant limitations in handling multimodal and multiscale data, underscoring a critical need for generative methods capable of comprehensive analysis through multimodal and multiscale fusion. To address this challenge, this study introduces a novel generative modeling methodology leveraging diffusion models for PSP data fusion in AJP. The proposed method integrates multimodal, multiscale PSP features in two phases: (1) registering the features, and (2) fusing them to generate causal relationships between PSP attributes. A case study demonstrates the registration and fusion of optical microscopy (OM) images and confocal profilometry (CP) data from AJP, along with the fine-tuning of the fusion step. The results effectively capture complex PSP relationships, offering deeper insights into digital twins of dynamic manufacturing systems.","authors":["Fatemeh Elhambakhsh","Suk Ki Lee","Hyunwoong Ko"],"url":"https://arxiv.org/abs/2505.00176"}
{"created":"2025-05-02","title":"On the Space Complexity of Online Convolution","abstract":"We study a discrete convolution streaming problem. An input arrives as a stream of numbers $z = (z_0,z_1,z_2,\\ldots)$, and at time $t$ our goal is to output $(Tz)_t$ where $T$ is a lower-triangular Toeplitz matrix. We focus on space complexity; the algorithm can store a buffer of $\\beta(t)$ numbers in order to achieve this goal.","authors":["Joel Daniel Andersson","Amir Yehudayoff"],"url":"https://arxiv.org/abs/2505.00181"}
{"created":"2025-05-02","title":"Neuroevolution of Self-Attention Over Proto-Objects","abstract":"Proto-objects - image regions that share common visual properties - offer a promising alternative to traditional attention mechanisms based on rectangular-shaped image patches in neural networks. Although previous work demonstrated that evolving a patch-based hard-attention module alongside a controller network could achieve state-of-the-art performance in visual reinforcement learning tasks, our approach leverages image segmentation to work with higher-level features. By operating on proto-objects rather than fixed patches, we significantly reduce the representational complexity: each image decomposes into fewer proto-objects than regular patches, and each proto-object can be efficiently encoded as a compact feature vector. This enables a substantially smaller self-attention module that processes richer semantic information. Our experiments demonstrate that this proto-object-based approach matches or exceeds the state-of-the-art performance of patch-based implementations with 62% less parameters and 2.6 times less training time.","authors":["Rafael C. Pinto","Anderson R. Tavares"],"url":"https://arxiv.org/abs/2505.00186"}
{"created":"2025-05-02","title":"Chronic Diseases Prediction using Machine Learning and Deep Learning Methods","abstract":"Chronic diseases, such as cardiovascular disease, diabetes, chronic kidney disease, and thyroid disorders, are the leading causes of premature mortality worldwide. Early detection and intervention are crucial for improving patient outcomes, yet traditional diagnostic methods often fail due to the complex nature of these conditions. This study explores the application of machine learning (ML) and deep learning (DL) techniques to predict chronic disease and thyroid disorders. We used a variety of models, including Logistic Regression (LR), Random Forest (RF), Gradient Boosted Trees (GBT), Neural Networks (NN), Decision Trees (DT) and Native Bayes (NB), to analyze and predict disease outcomes. Our methodology involved comprehensive data pre-processing, including handling missing values, categorical encoding, and feature aggregation, followed by model training and evaluation. Performance metrics such ad precision, recall, accuracy, F1-score, and Area Under the Curve (AUC) were used to assess the effectiveness of each model. The results demonstrated that ensemble methods like Random Forest and Gradient Boosted Trees consistently outperformed. Neutral Networks also showed superior performance, particularly in capturing complex data patterns. The findings highlight the potential of ML and DL in revolutionizing chronic disease prediction, enabling early diagnosis and personalized treatment strategies. However, challenges such as data quality, model interpretability, and the need for advanced computational techniques in healthcare to improve patient outcomes and reduce the burden of chronic diseases. This study was conducted as part of Big Data class project under the supervision of our professors Mr. Abderrahmane EZ-ZAHOUT and Mr. Abdessamad ESSAIDI.","authors":["Houda Belhad","Asmae Bourbia","Salma Boughanja"],"url":"https://arxiv.org/abs/2505.00189"}
{"created":"2025-05-02","title":"Empirical Evaluation of Progressive Coding for Sparse Autoencoders","abstract":"Sparse autoencoders (SAEs) \\citep{bricken2023monosemanticity,gao2024scalingevaluatingsparseautoencoders} rely on dictionary learning to extract interpretable features from neural networks at scale in an unsupervised manner, with applications to representation engineering and information retrieval. SAEs are, however, computationally expensive \\citep{lieberum2024gemmascopeopensparse}, especially when multiple SAEs of different sizes are needed. We show that dictionary importance in vanilla SAEs follows a power law. We compare progressive coding based on subset pruning of SAEs -- to jointly training nested SAEs, or so-called {\\em Matryoshka} SAEs \\citep{bussmann2024learning,nabeshima2024Matryoshka} -- on a language modeling task. We show Matryoshka SAEs exhibit lower reconstruction loss and recaptured language modeling loss, as well as higher representational similarity. Pruned vanilla SAEs are more interpretable, however. We discuss the origins and implications of this trade-off.","authors":["Hans Peter","Anders S{\\o}gaard"],"url":"https://arxiv.org/abs/2505.00190"}
{"created":"2025-05-02","title":"IP-CRR: Information Pursuit for Interpretable Classification of Chest Radiology Reports","abstract":"The development of AI-based methods for analyzing radiology reports could lead to significant advances in medical diagnosis--from improving diagnostic accuracy to enhancing efficiency and reducing workload. However, the lack of interpretability in these methods has hindered their adoption in clinical settings. In this paper, we propose an interpretable-by-design framework for classifying radiology reports. The key idea is to extract a set of most informative queries from a large set of reports and use these queries and their corresponding answers to predict a diagnosis. Thus, the explanation for a prediction is, by construction, the set of selected queries and answers. We use the Information Pursuit framework to select informative queries, the Flan-T5 model to determine if facts are present in the report, and a classifier to predict the disease. Experiments on the MIMIC-CXR dataset demonstrate the effectiveness of the proposed method, highlighting its potential to enhance trust and usability in medical AI.","authors":["Yuyan Ge","Kwan Ho Ryan Chan","Pablo Messina","Ren\\'e Vidal"],"url":"https://arxiv.org/abs/2505.00191"}
{"created":"2025-05-02","title":"Algorithmic Collective Action with Two Collectives","abstract":"Given that data-dependent algorithmic systems have become impactful in more domains of life, the need for individuals to promote their own interests and hold algorithms accountable has grown. To have meaningful influence, individuals must band together to engage in collective action. Groups that engage in such algorithmic collective action are likely to vary in size, membership characteristics, and crucially, objectives. In this work, we introduce a first of a kind framework for studying collective action with two or more collectives that strategically behave to manipulate data-driven systems. With more than one collective acting on a system, unexpected interactions may occur. We use this framework to conduct experiments with language model-based classifiers and recommender systems where two collectives each attempt to achieve their own individual objectives. We examine how differing objectives, strategies, sizes, and homogeneity can impact a collective's efficacy. We find that the unintentional interactions between collectives can be quite significant; a collective acting in isolation may be able to achieve their objective (e.g., improve classification outcomes for themselves or promote a particular item), but when a second collective acts simultaneously, the efficacy of the first group drops by as much as $75\\%$. We find that, in the recommender system context, neither fully heterogeneous nor fully homogeneous collectives stand out as most efficacious and that heterogeneity's impact is secondary compared to collective size. Our results signal the need for more transparency in both the underlying algorithmic models and the different behaviors individuals or collectives may take on these systems. This approach also allows collectives to hold algorithmic system developers accountable and provides a framework for people to actively use their own data to promote their own interests.","authors":["Aditya Karan","Nicholas Vincent","Karrie Karahalios","Hari Sundaram"],"url":"https://arxiv.org/abs/2505.00195"}
{"created":"2025-05-02","title":"Mapping minds not averages: a scalable subject-specific manifold learning framework for neuroimaging data","abstract":"Mental and cognitive representations are believed to reside on low-dimensional, non-linear manifolds embedded within high-dimensional brain activity. Uncovering these manifolds is key to understanding individual differences in brain function, yet most existing machine learning methods either rely on population-level spatial alignment or assume data that is temporally structured, either because data is aligned among subjects or because event timings are known. We introduce a manifold learning framework that can capture subject-specific spatial variations across both structured and temporally unstructured neuroimaging data. On simulated data and two naturalistic fMRI datasets (Sherlock and Forrest Gump), our framework outperforms group-based baselines by recovering more accurate and individualized representations. We further show that the framework scales efficiently to large datasets and generalizes well to new subjects. To test this, we apply the framework to temporally unstructured resting-state fMRI data from individuals with schizophrenia and healthy controls. We further apply our method to a large resting-state fMRI dataset comprising individuals with schizophrenia and controls. In this setting, we demonstrate that the framework scales efficiently to large populations and generalizes robustly to unseen subjects. The learned subject-specific spatial maps our model finds reveal clinically relevant patterns, including increased activation in the basal ganglia, visual, auditory, and somatosensory regions, and decreased activation in the insula, inferior frontal gyrus, and angular gyrus. These findings suggest that our framework can uncover clinically relevant subject-specific brain activity patterns. Our approach thus provides a scalable and individualized framework for modeling brain activity, with applications in computational neuroscience and clinical research.","authors":["Eloy Geenjaar","Vince Calhoun"],"url":"https://arxiv.org/abs/2505.00196"}
{"created":"2025-05-02","title":"Characterizing gaussian mixture of motion modes for skid-steer state estimation","abstract":"Skid-steered wheel mobile robots (SSWMRs) are characterized by the unique domination of the tire-terrain skidding for the robot to move. The lack of reliable friction models cascade into unreliable motion models, especially the reduced ordered variants used for state estimation and robot control. Ensemble modeling is an emerging research direction where the overall motion model is broken down into a family of local models to distribute the performance and resource requirement and provide a fast real-time prediction. To this end, a gaussian mixture model based modeling identification of model clusters is adopted and implemented within an interactive multiple model (IMM) based state estimation. The framework is adopted and implemented for angular velocity as the estimated state for a mid scaled skid-steered wheel mobile robot platform.","authors":["Ameya Salvi","Mark Brudnak","Jonathon M. Smereka","Matthias Schmid","Venkat Krovi"],"url":"https://arxiv.org/abs/2505.00200"}
{"created":"2025-05-02","title":"Investigating Adaptive Tuning of Assistive Exoskeletons Using Offline Reinforcement Learning: Challenges and Insights","abstract":"Assistive exoskeletons have shown great potential in enhancing mobility for individuals with motor impairments, yet their effectiveness relies on precise parameter tuning for personalized assistance. In this study, we investigate the potential of offline reinforcement learning for optimizing effort thresholds in upper-limb assistive exoskeletons, aiming to reduce reliance on manual calibration. Specifically, we frame the problem as a multi-agent system where separate agents optimize biceps and triceps effort thresholds, enabling a more adaptive and data-driven approach to exoskeleton control. Mixed Q-Functionals (MQF) is employed to efficiently handle continuous action spaces while leveraging pre-collected data, thereby mitigating the risks associated with real-time exploration. Experiments were conducted using the MyoPro 2 exoskeleton across two distinct tasks involving horizontal and vertical arm movements. Our results indicate that the proposed approach can dynamically adjust threshold values based on learned patterns, potentially improving user interaction and control, though performance evaluation remains challenging due to dataset limitations.","authors":["Yasin Findik","Christopher Coco","Reza Azadeh"],"url":"https://arxiv.org/abs/2505.00201"}
{"created":"2025-05-02","title":"RAIL in the Wild: Operationalizing Responsible AI Evaluation Using Anthropic's Value Dataset","abstract":"As AI systems become embedded in real-world applications, ensuring they meet ethical standards is crucial. While existing AI ethics frameworks emphasize fairness, transparency, and accountability, they often lack actionable evaluation methods. This paper introduces a systematic approach using the Responsible AI Labs (RAIL) framework, which includes eight measurable dimensions to assess the normative behavior of large language models (LLMs). We apply this framework to Anthropic's \"Values in the Wild\" dataset, containing over 308,000 anonymized conversations with Claude and more than 3,000 annotated value expressions. Our study maps these values to RAIL dimensions, computes synthetic scores, and provides insights into the ethical behavior of LLMs in real-world use.","authors":["Sumit Verma","Pritam Prasun","Arpit Jaiswal","Pritish Kumar"],"url":"https://arxiv.org/abs/2505.00204"}
{"created":"2025-05-02","title":"The Planted Orthogonal Vectors Problem","abstract":"In the $k$-Orthogonal Vectors ($k$-OV) problem we are given $k$ sets, each containing $n$ binary vectors of dimension $d=n^{o(1)}$, and our goal is to pick one vector from each set so that at each coordinate at least one vector has a zero. It is a central problem in fine-grained complexity, conjectured to require $n^{k-o(1)}$ time in the worst case.","authors":["David K\\\"uhnemann","Adam Polak","Alon Rosen"],"url":"https://arxiv.org/abs/2505.00206"}
{"created":"2025-05-02","title":"Direct Motion Models for Assessing Generated Videos","abstract":"A current limitation of video generative video models is that they generate plausible looking frames, but poor motion -- an issue that is not well captured by FVD and other popular methods for evaluating generated videos. Here we go beyond FVD by developing a metric which better measures plausible object interactions and motion. Our novel approach is based on auto-encoding point tracks and yields motion features that can be used to not only compare distributions of videos (as few as one generated and one ground truth, or as many as two datasets), but also for evaluating motion of single videos. We show that using point tracks instead of pixel reconstruction or action recognition features results in a metric which is markedly more sensitive to temporal distortions in synthetic data, and can predict human evaluations of temporal consistency and realism in generated videos obtained from open-source models better than a wide range of alternatives. We also show that by using a point track representation, we can spatiotemporally localize generative video inconsistencies, providing extra interpretability of generated video errors relative to prior work. An overview of the results and link to the code can be found on the project page: http://trajan-paper.github.io.","authors":["Kelsey Allen","Carl Doersch","Guangyao Zhou","Mohammed Suhail","Danny Driess","Ignacio Rocco","Yulia Rubanova","Thomas Kipf","Mehdi S. M. Sajjadi","Kevin Murphy","Joao Carreira","Sjoerd van Steenkiste"],"url":"https://arxiv.org/abs/2505.00209"}
{"created":"2025-05-02","title":"Generative Machine Learning in Adaptive Control of Dynamic Manufacturing Processes: A Review","abstract":"Dynamic manufacturing processes exhibit complex characteristics defined by time-varying parameters, nonlinear behaviors, and uncertainties. These characteristics require sophisticated in-situ monitoring techniques utilizing multimodal sensor data and adaptive control systems that can respond to real-time feedback while maintaining product quality. Recently, generative machine learning (ML) has emerged as a powerful tool for modeling complex distributions and generating synthetic data while handling these manufacturing uncertainties. However, adopting these generative technologies in dynamic manufacturing systems lacks a functional control-oriented perspective to translate their probabilistic understanding into actionable process controls while respecting constraints. This review presents a functional classification of Prediction-Based, Direct Policy, Quality Inference, and Knowledge-Integrated approaches, offering a perspective for understanding existing ML-enhanced control systems and incorporating generative ML. The analysis of generative ML architectures within this framework demonstrates control-relevant properties and potential to extend current ML-enhanced approaches where conventional methods prove insufficient. We show generative ML's potential for manufacturing control through decision-making applications, process guidance, simulation, and digital twins, while identifying critical research gaps: separation between generation and control functions, insufficient physical understanding of manufacturing phenomena, and challenges adapting models from other domains. To address these challenges, we propose future research directions aimed at developing integrated frameworks that combine generative ML and control technologies to address the dynamic complexities of modern manufacturing systems.","authors":["Suk Ki Lee","Hyunwoong Ko"],"url":"https://arxiv.org/abs/2505.00210"}
{"created":"2025-05-02","title":"Which Agent Causes Task Failures and When? On Automated Failure Attribution of LLM Multi-Agent Systems","abstract":"Failure attribution in LLM multi-agent systems-identifying the agent and step responsible for task failures-provides crucial clues for systems debugging but remains underexplored and labor-intensive. In this paper, we propose and formulate a new research area: automated failure attribution for LLM multi-agent systems. To support this initiative, we introduce the Who&amp;When dataset, comprising extensive failure logs from 127 LLM multi-agent systems with fine-grained annotations linking failures to specific agents and decisive error steps. Using the Who&amp;When, we develop and evaluate three automated failure attribution methods, summarizing their corresponding pros and cons. The best method achieves 53.5% accuracy in identifying failure-responsible agents but only 14.2% in pinpointing failure steps, with some methods performing below random. Even SOTA reasoning models, such as OpenAI o1 and DeepSeek R1, fail to achieve practical usability. These results highlight the task's complexity and the need for further research in this area. Code and dataset are available at https://github.com/mingyin1/Agents_Failure_Attribution","authors":["Shaokun Zhang","Ming Yin","Jieyu Zhang","Jiale Liu","Zhiguang Han","Jingyang Zhang","Beibin Li","Chi Wang","Huazheng Wang","Yiran Chen","Qingyun Wu"],"url":"https://arxiv.org/abs/2505.00212"}
{"created":"2025-05-02","title":"PSN Game: Game-theoretic Planning via a Player Selection Network","abstract":"While game-theoretic planning frameworks are effective at modeling multi-agent interactions, they require solving optimization problems with hundreds or thousands of variables, resulting in long computation times that limit their use in large-scale, real-time systems. To address this issue, we propose PSN Game: a novel game-theoretic planning framework that reduces runtime by learning a Player Selection Network (PSN). A PSN outputs a player selection mask that distinguishes influential players from less relevant ones, enabling the ego player to solve a smaller, masked game involving only selected players. By reducing the number of variables in the optimization problem, PSN directly lowers computation time. The PSN Game framework is more flexible than existing player selection methods as it i) relies solely on observations of players' past trajectories, without requiring full state, control, or other game-specific information; and ii) requires no online parameter tuning. We train PSNs in an unsupervised manner using a differentiable dynamic game solver, with reference trajectories from full-player games guiding the learning. Experiments in both simulated scenarios and human trajectory datasets demonstrate that i) PSNs outperform baseline selection methods in trajectory smoothness and length, while maintaining comparable safety and achieving a 10x speedup in runtime; and ii) PSNs generalize effectively to real-world scenarios without fine-tuning. By selecting only the most relevant players for decision-making, PSNs offer a general mechanism for reducing planning complexity that can be seamlessly integrated into existing multi-agent planning frameworks.","authors":["Tianyu Qiu","Eric Ouano","Fernando Palafox","Christian Ellis","David Fridovich-Keil"],"url":"https://arxiv.org/abs/2505.00213"}
{"created":"2025-05-02","title":"Online Federation For Mixtures of Proprietary Agents with Black-Box Encoders","abstract":"Most industry-standard generative AIs and feature encoders are proprietary, offering only black-box access: their outputs are observable, but their internal parameters and architectures remain hidden from the end-user. This black-box access is especially limiting when constructing mixture-of-expert type ensemble models since the user cannot optimize each proprietary AI's internal parameters. Our problem naturally lends itself to a non-competitive game-theoretic lens where each proprietary AI (agent) is inherently competing against the other AI agents, with this competition arising naturally due to their obliviousness of the AI's to their internal structure. In contrast, the user acts as a central planner trying to synchronize the ensemble of competing AIs.","authors":["Xuwei Yang","Fatemeh Tavakoli","David B. Emerson","Anastasis Kratsios"],"url":"https://arxiv.org/abs/2505.00216"}
{"created":"2025-05-02","title":"Towards Robust and Generalizable Gerchberg Saxton based Physics Inspired Neural Networks for Computer Generated Holography: A Sensitivity Analysis Framework","abstract":"Computer-generated holography (CGH) enables applications in holographic augmented reality (AR), 3D displays, systems neuroscience, and optical trapping. The fundamental challenge in CGH is solving the inverse problem of phase retrieval from intensity measurements. Physics-inspired neural networks (PINNs), especially Gerchberg-Saxton-based PINNs (GS-PINNs), have advanced phase retrieval capabilities. However, their performance strongly depends on forward models (FMs) and their hyperparameters (FMHs), limiting generalization, complicating benchmarking, and hindering hardware optimization. We present a systematic sensitivity analysis framework based on Saltelli's extension of Sobol's method to quantify FMH impacts on GS-PINN performance. Our analysis demonstrates that SLM pixel-resolution is the primary factor affecting neural network sensitivity, followed by pixel-pitch, propagation distance, and wavelength. Free space propagation forward models demonstrate superior neural network performance compared to Fourier holography, providing enhanced parameterization and generalization. We introduce a composite evaluation metric combining performance consistency, generalization capability, and hyperparameter perturbation resilience, establishing a unified benchmarking standard across CGH configurations. Our research connects physics-inspired deep learning theory with practical CGH implementations through concrete guidelines for forward model selection, neural network architecture, and performance evaluation. Our contributions advance the development of robust, interpretable, and generalizable neural networks for diverse holographic applications, supporting evidence-based decisions in CGH research and implementation.","authors":["Ankit Amrutkar","Bj\\\"orn Kampa","Volkmar Schulz","Johannes Stegmaier","Markus Rothermel","Dorit Merhof"],"url":"https://arxiv.org/abs/2505.00220"}
{"created":"2025-05-02","title":"AI-Enhanced Automatic Design of Efficient Underwater Gliders","abstract":"The development of novel autonomous underwater gliders has been hindered by limited shape diversity, primarily due to the reliance on traditional design tools that depend heavily on manual trial and error. Building an automated design framework is challenging due to the complexities of representing glider shapes and the high computational costs associated with modeling complex solid-fluid interactions. In this work, we introduce an AI-enhanced automated computational framework designed to overcome these limitations by enabling the creation of underwater robots with non-trivial hull shapes. Our approach involves an algorithm that co-optimizes both shape and control signals, utilizing a reduced-order geometry representation and a differentiable neural-network-based fluid surrogate model. This end-to-end design workflow facilitates rapid iteration and evaluation of hydrodynamic performance, leading to the discovery of optimal and complex hull shapes across various control settings. We validate our method through wind tunnel experiments and swimming pool gliding tests, demonstrating that our computationally designed gliders surpass manually designed counterparts in terms of energy efficiency. By addressing challenges in efficient shape representation and neural fluid surrogate models, our work paves the way for the development of highly efficient underwater gliders, with implications for long-range ocean exploration and environmental monitoring.","authors":["Peter Yichen Chen","Pingchuan Ma","Niklas Hagemann","John Romanishin","Wei Wang","Daniela Rus","Wojciech Matusik"],"url":"https://arxiv.org/abs/2505.00222"}
{"created":"2025-05-02","title":"Predicting Estimated Times of Restoration for Electrical Outages Using Longitudinal Tabular Transformers","abstract":"As climate variability increases, the ability of utility providers to deliver precise Estimated Times of Restoration (ETR) during natural disasters has become increasingly critical. Accurate and timely ETRs are essential for enabling customer preparedness during extended power outages, where informed decision-making can be crucial, particularly in severe weather conditions. Nonetheless, prevailing utility practices predominantly depend on manual assessments or traditional statistical methods, which often fail to achieve the level of precision required for reliable and actionable predictions. To address these limitations, we propose a Longitudinal Tabular Transformer (LTT) model that leverages historical outage event data along with sequential updates of these events to improve the accuracy of ETR predictions. The model's performance was evaluated over 34,000 storm-related outage events from three major utility companies, collectively serving over 3 million customers over a 2-year period. Results demonstrate that the LTT model improves the Customer Satisfaction Impact (CSI) metric by an average of 19.08% (p > 0.001) compared to existing methods. Additionally, we introduce customer-informed regression metrics that align model evaluation with real-world satisfaction, ensuring the outcomes resonate with customer expectations. Furthermore, we employ interpretability techniques to analyze the temporal significance of incorporating sequential updates in modeling outage events and to identify the contributions of predictive features to a given ETR. This comprehensive approach not only improves predictive accuracy but also enhances transparency, fostering greater trust in the model's capabilities.","authors":["Bogireddy Sai Prasanna Teja","Valliappan Muthukaruppan","Carls Benjamin"],"url":"https://arxiv.org/abs/2505.00225"}
{"created":"2025-05-02","title":"HP-MDR: High-performance and Portable Data Refactoring and Progressive Retrieval with Advanced GPUs","abstract":"Scientific applications produce vast amounts of data, posing grand challenges in the underlying data management and analytic tasks. Progressive compression is a promising way to address this problem, as it allows for on-demand data retrieval with significantly reduced data movement cost. However, most existing progressive methods are designed for CPUs, leaving a gap for them to unleash the power of today's heterogeneous computing systems with GPUs. In this work, we propose HP-MDR, a high-performance and portable data refactoring and progressive retrieval framework for GPUs. Our contributions are three-fold: (1) We carefully optimize the bitplane encoding and lossless encoding, two key stages in progressive methods, to achieve high performance on GPUs; (2) We propose pipeline optimization and incorporate it with data refactoring and progressive retrieval workflows to further enhance the performance for large data process; (3) We leverage our framework to enable high-performance data retrieval with guaranteed error control for common Quantities of Interest; (4) We evaluate HP-MDR and compare it with state of the arts using five real-world datasets. Experimental results demonstrate that HP-MDR delivers up to 6.6x throughput in data refactoring and progressive retrieval tasks. It also leads to 10.4x throughput for recomposing required data representations under Quantity-of-Interest error control and 4.2x performance for the corresponding end-to-end data retrieval, when compared with state-of-the-art solutions.","authors":["Yanliang Li","Wenbo Li","Qian Gong","Qing Liu","Norbert Podhorszki","Scott Klasky","Xin Liang","Jieyang Chen"],"url":"https://arxiv.org/abs/2505.00227"}
{"created":"2025-05-02","title":"ReXGradient-160K: A Large-Scale Publicly Available Dataset of Chest Radiographs with Free-text Reports","abstract":"We present ReXGradient-160K, representing the largest publicly available chest X-ray dataset to date in terms of the number of patients. This dataset contains 160,000 chest X-ray studies with paired radiological reports from 109,487 unique patients across 3 U.S. health systems (79 medical sites). This comprehensive dataset includes multiple images per study and detailed radiology reports, making it particularly valuable for the development and evaluation of AI systems for medical imaging and automated report generation models. The dataset is divided into training (140,000 studies), validation (10,000 studies), and public test (10,000 studies) sets, with an additional private test set (10,000 studies) reserved for model evaluation on the ReXrank benchmark. By providing this extensive dataset, we aim to accelerate research in medical imaging AI and advance the state-of-the-art in automated radiological analysis. Our dataset will be open-sourced at https://huggingface.co/datasets/rajpurkarlab/ReXGradient-160K.","authors":["Xiaoman Zhang","Juli\\'an N. Acosta","Josh Miller","Ouwen Huang","Pranav Rajpurkar"],"url":"https://arxiv.org/abs/2505.00228"}
{"created":"2025-05-02","title":"Scaling On-Device GPU Inference for Large Generative Models","abstract":"Driven by the advancements in generative AI, large machine learning models have revolutionized domains such as image processing, audio synthesis, and speech recognition. While server-based deployments remain the locus of peak performance, the imperative for on-device inference, necessitated by privacy and efficiency considerations, persists. Recognizing GPUs as the on-device ML accelerator with the widest reach, we present ML Drift--an optimized framework that extends the capabilities of state-of-the-art GPU-accelerated inference engines. ML Drift enables on-device execution of generative AI workloads which contain 10 to 100x more parameters than existing on-device generative AI models. ML Drift addresses intricate engineering challenges associated with cross-GPU API development, and ensures broad compatibility across mobile and desktop/laptop platforms, thereby facilitating the deployment of significantly more complex models on resource-constrained devices. Our GPU-accelerated ML/AI inference engine achieves an order-of-magnitude performance improvement relative to existing open-source GPU inference engines.","authors":["Jiuqiang Tang","Raman Sarokin","Ekaterina Ignasheva","Grant Jensen","Lin Chen","Juhyun Lee","Andrei Kulik","Matthias Grundmann"],"url":"https://arxiv.org/abs/2505.00232"}
{"created":"2025-05-02","title":"Self-Generated In-Context Examples Improve LLM Agents for Sequential Decision-Making Tasks","abstract":"Many methods for improving Large Language Model (LLM) agents for sequential decision-making tasks depend on task-specific knowledge engineering--such as prompt tuning, curated in-context examples, or customized observation and action spaces. Using these approaches, agent performance improves with the quality or amount of knowledge engineering invested. Instead, we investigate how LLM agents can automatically improve their performance by learning in-context from their own successful experiences on similar tasks. Rather than relying on task-specific knowledge engineering, we focus on constructing and refining a database of self-generated examples. We demonstrate that even a naive accumulation of successful trajectories across training tasks boosts test performance on three benchmarks: ALFWorld (73% to 89%), Wordcraft (55% to 64%), and InterCode-SQL (75% to 79%)--matching the performance the initial agent achieves if allowed two to three attempts per task. We then introduce two extensions: (1) database-level selection through population-based training to identify high-performing example collections, and (2) exemplar-level selection that retains individual trajectories based on their empirical utility as in-context examples. These extensions further enhance performance, achieving 91% on ALFWorld--matching more complex approaches that employ task-specific components and prompts. Our results demonstrate that automatic trajectory database construction offers a compelling alternative to labor-intensive knowledge engineering.","authors":["Vishnu Sarukkai","Zhiqiang Xie","Kayvon Fatahalian"],"url":"https://arxiv.org/abs/2505.00234"}
{"created":"2025-05-02","title":"Node2Vec-DGI-EL: A Hierarchical Graph Representation Learning Model for Ingredient-Disease Association Prediction","abstract":"Traditional Chinese medicine, as an essential component of traditional medicine, contains active ingredients that serve as a crucial source for modern drug development, holding immense therapeutic potential and development value. A multi-layered and complex network is formed from Chinese medicine to diseases and used to predict the potential associations between Chinese medicine ingredients and diseases. This study proposes an ingredient-disease association prediction model (Node2Vec-DGI-EL) based on hierarchical graph representation learning. First, the model uses the Node2Vec algorithm to extract node embedding vectors from the network as the initial features of the nodes. Next, the network nodes are deeply represented and learned using the DGI algorithm to enhance the model's expressive power. To improve prediction accuracy and robustness, an ensemble learning method is incorporated to achieve more accurate ingredient-disease association predictions. The effectiveness of the model is then evaluated through a series of theoretical verifications. The results demonstrated that the proposed model significantly outperformed existing methods, achieving an AUC of 0.9987 and an AUPR of 0.9545, thereby indicating superior predictive capability. Ablation experiments further revealed the contribution and importance of each module. Additionally, case studies explored potential associations, such as triptonide with hypertensive retinopathy and methyl ursolate with colorectal cancer. Molecular docking experiments validated these findings, showing the triptonide-PGR interaction and the methyl ursolate-NFE2L2 interaction can bind stable. In conclusion, the Node2Vec-DGI-EL model focuses on TCM datasets and effectively predicts ingredient-disease associations, overcoming the reliance on node semantic information.","authors":["Leifeng Zhang","Xin Dong","Shuaibing Jia","Jianhua Zhang"],"url":"https://arxiv.org/abs/2505.00236"}
{"created":"2025-05-02","title":"Future-Oriented Navigation: Dynamic Obstacle Avoidance with One-Shot Energy-Based Multimodal Motion Prediction","abstract":"This paper proposes an integrated approach for the safe and efficient control of mobile robots in dynamic and uncertain environments. The approach consists of two key steps: one-shot multimodal motion prediction to anticipate motions of dynamic obstacles and model predictive control to incorporate these predictions into the motion planning process. Motion prediction is driven by an energy-based neural network that generates high-resolution, multi-step predictions in a single operation. The prediction outcomes are further utilized to create geometric shapes formulated as mathematical constraints. Instead of treating each dynamic obstacle individually, predicted obstacles are grouped by proximity in an unsupervised way to improve performance and efficiency. The overall collision-free navigation is handled by model predictive control with a specific design for proactive dynamic obstacle avoidance. The proposed approach allows mobile robots to navigate effectively in dynamic environments. Its performance is accessed across various scenarios that represent typical warehouse settings. The results demonstrate that the proposed approach outperforms other existing dynamic obstacle avoidance methods.","authors":["Ze Zhang","Georg Hess","Junjie Hu","Emmanuel Dean","Lennart Svensson","Knut {\\AA}kesson"],"url":"https://arxiv.org/abs/2505.00237"}
{"created":"2025-05-02","title":"LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems","abstract":"The increasing complexity and scale of the Internet of Things (IoT) have made security a critical concern. This paper presents a novel Large Language Model (LLM)-based framework for comprehensive threat detection and prevention in IoT environments. The system integrates lightweight LLMs fine-tuned on IoT-specific datasets (IoT-23, TON_IoT) for real-time anomaly detection and automated, context-aware mitigation strategies optimized for resource-constrained devices. A modular Docker-based deployment enables scalable and reproducible evaluation across diverse network conditions. Experimental results in simulated IoT environments demonstrate significant improvements in detection accuracy, response latency, and resource efficiency over traditional security methods. The proposed framework highlights the potential of LLM-driven, autonomous security solutions for future IoT ecosystems.","authors":["Yazan Otoum","Arghavan Asad","Amiya Nayak"],"url":"https://arxiv.org/abs/2505.00240"}
{"created":"2025-05-02","title":"D-Tracker: Modeling Interest Diffusion in Social Activity Tensor Data Streams","abstract":"Large quantities of social activity data, such as weekly web search volumes and the number of new infections with infectious diseases, reflect peoples' interests and activities. It is important to discover temporal patterns from such data and to forecast future activities accurately. However, modeling and forecasting social activity data streams is difficult because they are high-dimensional and composed of multiple time-varying dynamics such as trends, seasonality, and interest diffusion. In this paper, we propose D-Tracker, a method for continuously capturing time-varying temporal patterns within social activity tensor data streams and forecasting future activities. Our proposed method has the following properties: (a) Interpretable: it incorporates the partial differential equation into a tensor decomposition framework and captures time-varying temporal patterns such as trends, seasonality, and interest diffusion between locations in an interpretable manner; (b) Automatic: it has no hyperparameters and continuously models tensor data streams fully automatically; (c) Scalable: the computation time of D-Tracker is independent of the time series length. Experiments using web search volume data obtained from GoogleTrends, and COVID-19 infection data obtained from COVID-19 Open Data Repository show that our method can achieve higher forecasting accuracy in less computation time than existing methods while extracting the interest diffusion between locations. Our source code and datasets are available at {https://github.com/Higashiguchi-Shingo/D-Tracker.","authors":["Shingo Higashiguchi","Yasuko Matsubara","Koki Kawabata","Taichi Murayama","Yasushi Sakurai"],"url":"https://arxiv.org/abs/2505.00242"}
{"created":"2025-05-02","title":"Multi-start Optimization Method via Scalarization based on Target Point-based Tchebycheff Distance for Multi-objective Optimization","abstract":"Multi-objective optimization is crucial in scientific and industrial applications where solutions must balance trade-offs among conflicting objectives. State-of-the-art methods, such as NSGA-III and MOEA/D, can handle many objectives but struggle with coverage issues, particularly in cases involving inverted triangular Pareto fronts or strong nonlinearity. Moreover, NSGA-III often relies on simulated binary crossover, which deteriorates in problems with variable dependencies. In this study, we propose a novel multi-start optimization method that addresses these challenges. Our approach introduces a newly introduced scalarization technique, the Target Point-based Tchebycheff Distance (TPTD) method, which significantly improves coverage on problems with inverted triangular Pareto fronts. For efficient multi-start optimization, TPTD leverages a target point defined in the objective space, which plays a critical role in shaping the scalarized function. The position of the target point is adaptively determined according to the shape of the Pareto front, ensuring improvement in coverage. Furthermore, the flexibility of this scalarization allows seamless integration with powerful single-objective optimization methods, such as natural evolution strategies, to efficiently handle variable dependencies. Experimental results on benchmark problems, including those with inverted triangular Pareto fronts, demonstrate that our method outperforms NSGA-II, NSGA-III, and MOEA/D-DE in terms of the Hypervolume indicator. Notably, our approach achieves computational efficiency improvements of up to 474 times over these baselines.","authors":["Kota Nagakane","Masahiro Nomura","Isao Ono"],"url":"https://arxiv.org/abs/2505.00251"}
{"created":"2025-05-02","title":"Empowering Agentic Video Analytics Systems with Video Language Models","abstract":"AI-driven video analytics has become increasingly pivotal across diverse domains. However, existing systems are often constrained to specific, predefined tasks, limiting their adaptability in open-ended analytical scenarios. The recent emergence of Video-Language Models (VLMs) as transformative technologies offers significant potential for enabling open-ended video understanding, reasoning, and analytics. Nevertheless, their limited context windows present challenges when processing ultra-long video content, which is prevalent in real-world applications. To address this, we introduce AVA, a VLM-powered system designed for open-ended, advanced video analytics. AVA incorporates two key innovations: (1) the near real-time construction of Event Knowledge Graphs (EKGs) for efficient indexing of long or continuous video streams, and (2) an agentic retrieval-generation mechanism that leverages EKGs to handle complex and diverse queries. Comprehensive evaluations on public benchmarks, LVBench and VideoMME-Long, demonstrate that AVA achieves state-of-the-art performance, attaining 62.3% and 64.1% accuracy, respectively, significantly surpassing existing VLM and video Retrieval-Augmented Generation (RAG) systems. Furthermore, to evaluate video analytics in ultra-long and open-world video scenarios, we introduce a new benchmark, AVA-100. This benchmark comprises 8 videos, each exceeding 10 hours in duration, along with 120 manually annotated, diverse, and complex question-answer pairs. On AVA-100, AVA achieves top-tier performance with an accuracy of 75.8%.","authors":["Yuxuan Yan","Shiqi Jiang","Ting Cao","Yifan Yang","Qianqian Yang","Yuanchao Shu","Yuqing Yang","Lili Qiu"],"url":"https://arxiv.org/abs/2505.00254"}
{"created":"2025-05-02","title":"Graph Privacy: A Heterogeneous Federated GNN for Trans-Border Financial Data Circulation","abstract":"The sharing of external data has become a strong demand of financial institutions, but the privacy issue has led to the difficulty of interconnecting different platforms and the low degree of data openness. To effectively solve the privacy problem of financial data in trans-border flow and sharing, to ensure that the data is available but not visible, to realize the joint portrait of all kinds of heterogeneous data of business organizations in different industries, we propose a Heterogeneous Federated Graph Neural Network (HFGNN) approach. In this method, the distribution of heterogeneous business data of trans-border organizations is taken as subgraphs, and the sharing and circulation process among subgraphs is constructed as a statistically heterogeneous global graph through a central server. Each subgraph learns the corresponding personalized service model through local training to select and update the relevant subset of subgraphs with aggregated parameters, and effectively separates and combines topological and feature information among subgraphs. Finally, our simulation experimental results show that the proposed method has higher accuracy performance and faster convergence speed than existing methods.","authors":["Zhizhong Tan","Jiexin Zheng","Kevin Qi Zhang","Wenyong Wang"],"url":"https://arxiv.org/abs/2505.00257"}
{"created":"2025-05-02","title":"Quantile-RK and Double Quantile-RK Error Horizon Analysis","abstract":"In solving linear systems of equations of the form $Ax=b$, corruptions present in $b$ affect stochastic iterative algorithms' ability to reach the true solution $x^\\ast$ to the uncorrupted linear system. The randomized Kaczmarz method converges in expectation to $x^\\ast$ up to an error horizon dependent on the conditioning of $A$ and the supremum norm of the corruption in $b$. To avoid this error horizon in the sparse corruption setting, previous works have proposed quantile-based adaptations that make iterative methods robust. Our work first establishes a new convergence rate for the quantile-based random Kaczmarz (qRK) and double quantile-based random Kaczmarz (dqRK) methods, which, under mild conditions, improves upon known bounds. We further consider the more practical setting in which the vector $b$ includes both non-sparse \"noise\" and sparse \"corruption\". Error horizon bounds for qRK and dqRK are derived and shown to produce a smaller error horizon compared to their non-quantile-based counterparts, further demonstrating the advantages of quantile-based methods.","authors":["Emeric Battaglia","Anna Ma"],"url":"https://arxiv.org/abs/2505.00258"}
{"created":"2025-05-02","title":"Pack-PTQ: Advancing Post-training Quantization of Neural Networks by Pack-wise Reconstruction","abstract":"Post-training quantization (PTQ) has evolved as a prominent solution for compressing complex models, which advocates a small calibration dataset and avoids end-to-end retraining. However, most existing PTQ methods employ block-wise reconstruction, which neglects cross-block dependency and exhibits a notable accuracy drop in low-bit cases. To address these limitations, this paper presents a novel PTQ method, dubbed Pack-PTQ. First, we design a Hessian-guided adaptive packing mechanism to partition blocks into non-overlapping packs, which serve as the base unit for reconstruction, thereby preserving the cross-block dependency and enabling accurate quantization parameters estimation. Second, based on the pack configuration, we propose a mixed-precision quantization approach to assign varied bit-widths to packs according to their distinct sensitivities, thereby further enhancing performance. Extensive experiments on 2D image and 3D point cloud classification tasks, using various network architectures, demonstrate the superiority of our method over the state-of-the-art PTQ methods.","authors":["Changjun Li","Runqing Jiang","Zhuo Song","Pengpeng Yu","Ye Zhang","Yulan Guo"],"url":"https://arxiv.org/abs/2505.00259"}
{"created":"2025-05-02","title":"Enriching the Korean Learner Corpus with Multi-reference Annotations and Rubric-Based Scoring","abstract":"Despite growing global interest in Korean language education, there remains a significant lack of learner corpora tailored to Korean L2 writing. To address this gap, we enhance the KoLLA Korean learner corpus by adding multiple grammatical error correction (GEC) references, thereby enabling more nuanced and flexible evaluation of GEC systems, and reflects the variability of human language. Additionally, we enrich the corpus with rubric-based scores aligned with guidelines from the Korean National Language Institute, capturing grammatical accuracy, coherence, and lexical diversity. These enhancements make KoLLA a robust and standardized resource for research in Korean L2 education, supporting advancements in language learning, assessment, and automated error correction.","authors":["Jayoung Song","KyungTae Lim","Jungyeul Park"],"url":"https://arxiv.org/abs/2505.00261"}
{"created":"2025-05-02","title":"EnronQA: Towards Personalized RAG over Private Documents","abstract":"Retrieval Augmented Generation (RAG) has become one of the most popular methods for bringing knowledge-intensive context to large language models (LLM) because of its ability to bring local context at inference time without the cost or data leakage risks associated with fine-tuning. A clear separation of private information from the LLM training has made RAG the basis for many enterprise LLM workloads as it allows the company to augment LLM's understanding using customers' private documents. Despite its popularity for private documents in enterprise deployments, current RAG benchmarks for validating and optimizing RAG pipelines draw their corpora from public data such as Wikipedia or generic web pages and offer little to no personal context. Seeking to empower more personal and private RAG we release the EnronQA benchmark, a dataset of 103,638 emails with 528,304 question-answer pairs across 150 different user inboxes. EnronQA enables better benchmarking of RAG pipelines over private data and allows for experimentation on the introduction of personalized retrieval settings over realistic data. Finally, we use EnronQA to explore the tradeoff in memorization and retrieval when reasoning over private documents.","authors":["Michael J. Ryan","Danmei Xu","Chris Nivera","Daniel Campos"],"url":"https://arxiv.org/abs/2505.00263"}
{"created":"2025-05-02","title":"Integrating VR tours in online language learning:A design-based research study","abstract":"This paper presents an investigation into the integration of virtual reality (VR) tours in online English lessons tailored for adult learners. The study utilised a design-based research approach to evaluate the effectiveness of VR tours in this context. It specifically examined the responses of adult learners to this instructional strategy by collecting data through surveys, observation notes and interviews with four learners in Japan and five learners in France, most of whom completed 10 lessons over 4 months. The research findings highlight the effectiveness of VR tours in enhancing learner motivation. Additionally, they demonstrate that perceived learning outcomes are influenced not only by the immersive experience of spatial presence but also by the novelty of technological and scenery-related aspects within the VR environment, as well as factors related to lesson design and individual learner characteristics.","authors":["Roberto B. Figueroa Jr.","Insung Jung"],"url":"https://arxiv.org/abs/2505.00264"}
{"created":"2025-05-02","title":"Field-scale soil moisture estimated from Sentinel-1 SAR data using a knowledge-guided deep learning approach","abstract":"Soil moisture (SM) estimation from active microwave data remains challenging due to the complex interactions between radar backscatter and surface characteristics. While the water cloud model (WCM) provides a semi-physical approach for understanding these interactions, its empirical component often limits performance across diverse agricultural landscapes. This research presents preliminary efforts for developing a knowledge-guided deep learning approach, which integrates WCM principles into a long short-term memory (LSTM) model, to estimate field SM using Sentinel-1 Synthetic Aperture Radar (SAR) data. Our proposed approach leverages LSTM's capacity to capture spatiotemporal dependencies while maintaining physical consistency through a modified dual-component loss function, including a WCM-based semi-physical component and a boundary condition regularisation. The proposed approach is built upon the soil backscatter coefficients isolated from the total backscatter, together with Landsat-resolution vegetation information and surface characteristics. A four-fold spatial cross-validation was performed against in-situ SM data to assess the model performance. Results showed the proposed approach reduced SM retrieval uncertainties by 0.02 m$^3$/m$^3$ and achieved correlation coefficients (R) of up to 0.64 in areas with varying vegetation cover and surface conditions, demonstrating the potential to address the over-simplification in WCM.","authors":["Yi Yu","Patrick Filippi","Thomas F. A. Bishop"],"url":"https://arxiv.org/abs/2505.00265"}
{"created":"2025-05-02","title":"Consistency in Language Models: Current Landscape, Challenges, and Future Directions","abstract":"The hallmark of effective language use lies in consistency -- expressing similar meanings in similar contexts and avoiding contradictions. While human communication naturally demonstrates this principle, state-of-the-art language models struggle to maintain reliable consistency across different scenarios. This paper examines the landscape of consistency research in AI language systems, exploring both formal consistency (including logical rule adherence) and informal consistency (such as moral and factual coherence). We analyze current approaches to measure aspects of consistency, identify critical research gaps in standardization of definitions, multilingual assessment, and methods to improve consistency. Our findings point to an urgent need for robust benchmarks to measure and interdisciplinary approaches to ensure consistency in the application of language models on domain-specific tasks while preserving the utility and adaptability.","authors":["Jekaterina Novikova","Carol Anderson","Borhane Blili-Hamelin","Subhabrata Majumdar"],"url":"https://arxiv.org/abs/2505.00268"}
{"created":"2025-05-02","title":"Weighted-Scenario Optimisation for the Chance Constrained Travelling Thief Problem","abstract":"The chance constrained travelling thief problem (chance constrained TTP) has been introduced as a stochastic variation of the classical travelling thief problem (TTP) in an attempt to embody the effect of uncertainty in the problem definition. In this work, we characterise the chance constrained TTP using a limited number of weighted scenarios. Each scenario represents a similar TTP instance, differing slightly in the weight profile of the items and associated with a certain probability of occurrence. Collectively, the weighted scenarios represent a relaxed form of a stochastic TTP instance where the objective is to maximise the expected benefit while satisfying the knapsack constraint with a larger probability. We incorporate a set of evolutionary algorithms and heuristic procedures developed for the classical TTP, and formulate adaptations that apply to the weighted scenario-based representation of the problem. The analysis focuses on the performance of the algorithms on different settings and examines the impact of uncertainty on the quality of the solutions.","authors":["Thilina Pathirage Don","Aneta Neumann","Frank Neumann"],"url":"https://arxiv.org/abs/2505.00269"}
{"created":"2025-05-02","title":"AdCare-VLM: Leveraging Large Vision Language Model (LVLM) to Monitor Long-Term Medication Adherence and Care","abstract":"Chronic diseases, including diabetes, hypertension, asthma, HIV-AIDS, epilepsy, and tuberculosis, necessitate rigorous adherence to medication to avert disease progression, manage symptoms, and decrease mortality rates. Adherence is frequently undermined by factors including patient behavior, caregiver support, elevated medical costs, and insufficient healthcare infrastructure. We propose AdCare-VLM, a specialized Video-LLaVA-based multimodal large vision language model (LVLM) aimed at visual question answering (VQA) concerning medication adherence through patient videos. We employ a private dataset comprising 806 custom-annotated tuberculosis (TB) medication monitoring videos, which have been labeled by clinical experts, to fine-tune the model for adherence pattern detection. We present LLM-TB-VQA, a detailed medical adherence VQA dataset that encompasses positive, negative, and ambiguous adherence cases. Our method identifies correlations between visual features, such as the clear visibility of the patient's face, medication, water intake, and the act of ingestion, and their associated medical concepts in captions. This facilitates the integration of aligned visual-linguistic representations and improves multimodal interactions. Experimental results indicate that our method surpasses parameter-efficient fine-tuning (PEFT) enabled VLM models, such as LLaVA-V1.5 and Chat-UniVi, with absolute improvements ranging from 3.1% to 3.54% across pre-trained, regular, and low-rank adaptation (LoRA) configurations. Comprehensive ablation studies and attention map visualizations substantiate our approach, enhancing interpretability.","authors":["Md Asaduzzaman Jabin","Hanqi Jiang","Yiwei Li","Patrick Kaggwa","Eugene Douglass","Juliet N. Sekandi","Tianming Liu"],"url":"https://arxiv.org/abs/2505.00275"}
{"created":"2025-05-02","title":"DeCo: Defect-Aware Modeling with Contrasting Matching for Optimizing Task Assignment in Online IC Testing","abstract":"In the semiconductor industry, integrated circuit (IC) processes play a vital role, as the rising complexity and market expectations necessitate improvements in yield. Identifying IC defects and assigning IC testing tasks to the right engineers improves efficiency and reduces losses. While current studies emphasize fault localization or defect classification, they overlook the integration of defect characteristics, historical failures, and the insights from engineer expertise, which restrains their effectiveness in improving IC handling. To leverage AI for these challenges, we propose DeCo, an innovative approach for optimizing task assignment in IC testing. DeCo constructs a novel defect-aware graph from IC testing reports, capturing co-failure relationships to enhance defect differentiation, even with scarce defect data. Additionally, it formulates defect-aware representations for engineers and tasks, reinforced by local and global structure modeling on the defect-aware graph. Finally, a contrasting-based assignment mechanism pairs testing tasks with QA engineers by considering their skill level and current workload, thus promoting an equitable and efficient job dispatch. Experiments on a real-world dataset demonstrate that DeCo achieves the highest task-handling success rates in different scenarios, exceeding 80\\%, while also maintaining balanced workloads on both scarce or expanded defect data. Moreover, case studies reveal that DeCo can assign tasks to potentially capable engineers, even for their unfamiliar defects, highlighting its potential as an AI-driven solution for the real-world IC failure analysis and task handling.","authors":["Lo Pang-Yun Ting","Yu-Hao Chiang","Yi-Tung Tsai","Hsu-Chao Lai","Kun-Ta Chuang"],"url":"https://arxiv.org/abs/2505.00278"}
{"created":"2025-05-02","title":"Policies of Multiple Skill Levels for Better Strength Estimation in Games","abstract":"Accurately estimating human skill levels is crucial for designing effective human-AI interactions so that AI can provide appropriate challenges or guidance. In games where AI players have beaten top human professionals, strength estimation plays a key role in adapting AI behavior to match human skill levels. In a previous state-of-the-art study, researchers have proposed a strength estimator trained using human players' match data. Given some matches, the strength estimator computes strength scores and uses them to estimate player ranks (skill levels). In this paper, we focus on the observation that human players' behavior tendency varies according to their strength and aim to improve the accuracy of strength estimation by taking this into account. Specifically, in addition to strength scores, we obtain policies for different skill levels from neural networks trained using human players' match data. We then combine features based on these policies with the strength scores to estimate strength. We conducted experiments on Go and chess. For Go, our method achieved an accuracy of 80% in strength estimation when given 10 matches, which increased to 92% when given 20 matches. In comparison, the previous state-of-the-art method had an accuracy of 71% with 10 matches and 84% with 20 matches, demonstrating improvements of 8-9%. We observed similar improvements in chess. These results contribute to developing a more accurate strength estimation method and to improving human-AI interaction.","authors":["Kyota Kuboki","Tatsuyoshi Ogawa","Chu-Hsuan Hsueh","Shi-Jim Yen","Kokolo Ikeda"],"url":"https://arxiv.org/abs/2505.00279"}
{"created":"2025-05-02","title":"Mixed Precision Orthogonalization-Free Projection Methods for Eigenvalue and Singular Value Problems","abstract":"Mixed-precision arithmetic offers significant computational advantages for large-scale matrix computation tasks, yet preserving accuracy and stability in eigenvalue problems and the singular value decomposition (SVD) remains challenging. This paper introduces an approach that eliminates orthogonalization requirements in traditional Rayleigh-Ritz projection methods. The proposed method employs non-orthogonal bases computed at reduced precision, resulting in bases computed without inner-products. A primary focus is on maintaining the linear independence of the basis vectors. Through extensive evaluation with both synthetic test cases and real-world applications, we demonstrate that the proposed approach achieves the desired accuracy while fully taking full advantage of mixed-precision arithmetic.","authors":["Tianshi Xu","Zechen Zhang","Jie Chen","Yousef Saad","Yuanzhe Xi"],"url":"https://arxiv.org/abs/2505.00281"}
{"created":"2025-05-02","title":"LightEMMA: Lightweight End-to-End Multimodal Model for Autonomous Driving","abstract":"Vision-Language Models (VLMs) have demonstrated significant potential for end-to-end autonomous driving. However, fully exploiting their capabilities for safe and reliable vehicle control remains an open research challenge. To systematically examine advances and limitations of VLMs in driving tasks, we introduce LightEMMA, a Lightweight End-to-End Multimodal Model for Autonomous driving. LightEMMA provides a unified, VLM-based autonomous driving framework without ad hoc customizations, enabling easy integration and evaluation of evolving state-of-the-art commercial and open-source models. We construct twelve autonomous driving agents using various VLMs and evaluate their performance on the nuScenes prediction task, comprehensively assessing metrics such as inference time, computational cost, and predictive accuracy. Illustrative examples highlight that, despite their strong scenario interpretation capabilities, VLMs' practical performance in autonomous driving tasks remains concerning, emphasizing the need for further improvements. The code is available at https://github.com/michigan-traffic-lab/LightEMMA.","authors":["Zhijie Qiao","Haowei Li","Zhong Cao","Henry X. Liu"],"url":"https://arxiv.org/abs/2505.00284"}
{"created":"2025-05-02","title":"Avatar Communication Provides More Efficient Online Social Support Than Text Communication","abstract":"Online communication via avatars provides a richer online social experience than text communication. This reinforces the importance of online social support. Online social support is effective for people who lack social resources because of the anonymity of online communities. We aimed to understand online social support via avatars and their social relationships to provide better social support to avatar users. Therefore, we administered a questionnaire to three avatar communication service users (Second Life, ZEPETO, and Pigg Party) and three text communication service users (Facebook, X, and Instagram) (N=8,947). There was no duplication of users for each service. By comparing avatar and text communication users, we examined the amount of online social support, stability of online relationships, and the relationships between online social support and offline social resources (e.g., offline social support). We observed that avatar communication service users received more online social support, had more stable relationships, and had fewer offline social resources than text communication service users. However, the positive association between online and offline social support for avatar communication users was more substantial than for text communication users. These findings highlight the significance of realistic online communication experiences through avatars, including nonverbal and real-time interactions with co-presence. The findings also highlighted avatar communication service users' problems in the physical world, such as the lack of offline social resources. This study suggests that enhancing online social support through avatars can address these issues. This could help resolve social resource problems, both online and offline in future metaverse societies.","authors":["Masanori Takano","Kenji Yokotani","Takahiro Kato","Nobuhito Abe","Fumiaki Taka"],"url":"https://arxiv.org/abs/2505.00287"}
{"created":"2025-05-02","title":"PatchFuzz: Patch Fuzzing for JavaScript Engines","abstract":"Patch fuzzing is a technique aimed at identifying vulnerabilities that arise from newly patched code. While researchers have made efforts to apply patch fuzzing to testing JavaScript engines with considerable success, these efforts have been limited to using ordinary test cases or publicly available vulnerability PoCs (Proof of Concepts) as seeds, and the sustainability of these approaches is hindered by the challenges associated with automating the PoC collection. To address these limitations, we propose an end-to-end sustainable approach for JavaScript engine patch fuzzing, named PatchFuzz. It automates the collection of PoCs of a broader range of historical vulnerabilities and leverages both the PoCs and their corresponding patches to uncover new vulnerabilities more effectively. PatchFuzz starts by recognizing git commits which intend to fix security bugs. Subsequently, it extracts and processes PoCs from these commits to form the seeds for fuzzing, while utilizing code revisions to focus limited fuzzing resources on the more vulnerable code areas through selective instrumentation. The mutation strategy of PatchFuzz is also optimized to maximize the potential of the PoCs. Experimental results demonstrate the effectiveness of PatchFuzz. Notably, 54 bugs across six popular JavaScript engines have been exposed and a total of $62,500 bounties has been received.","authors":["Junjie Wang","Yuhan Ma","Xiaofei Xie","Xiaoning Du","Xiangwei Zhang"],"url":"https://arxiv.org/abs/2505.00289"}
{"created":"2025-05-02","title":"Multi-Hierarchical Fine-Grained Feature Mapping Driven by Feature Contribution for Molecular Odor Prediction","abstract":"Molecular odor prediction is the process of using a molecule's structure to predict its smell. While accurate prediction remains challenging, AI models can suggest potential odors. Existing methods, however, often rely on basic descriptors or handcrafted fingerprints, which lack expressive power and hinder effective learning. Furthermore, these methods suffer from severe class imbalance, limiting the training effectiveness of AI models. To address these challenges, we propose a Feature Contribution-driven Hierarchical Multi-Feature Mapping Network (HMFNet). Specifically, we introduce a fine-grained, Local Multi-Hierarchy Feature Extraction module (LMFE) that performs deep feature extraction at the atomic level, capturing detailed features crucial for odor prediction. To enhance the extraction of discriminative atomic features, we integrate a Harmonic Modulated Feature Mapping (HMFM). This module dynamically learns feature importance and frequency modulation, improving the model's capability to capture relevant patterns. Additionally, a Global Multi-Hierarchy Feature Extraction module (GMFE) is designed to learn global features from the molecular graph topology, enabling the model to fully leverage global information and enhance its discriminative power for odor prediction. To further mitigate the issue of class imbalance, we propose a Chemically-Informed Loss (CIL). Experimental results demonstrate that our approach significantly improves performance across various deep learning models, highlighting its potential to advance molecular structure representation and accelerate the development of AI-driven technologies.","authors":["Hong Xin Xie","Jian De Sun","Fan Fu Xue","Zi Fei Han","Shan Shan Feng","Qi Chen"],"url":"https://arxiv.org/abs/2505.00290"}
{"created":"2025-05-02","title":"Repetition Makes Perfect: Recurrent Sum-GNNs Match Message Passing Limit","abstract":"We provide first tight bounds for the expressivity of Recurrent Graph Neural Networks (recurrent GNNs) with finite-precision parameters. We prove that recurrent GNNs, with sum aggregation and ReLU activation, can emulate any graph algorithm that respects the natural message-passing invariance induced by the color refinement (or Weisfeiler-Leman) algorithm. While it is well known that the expressive power of GNNs is limited by this invariance [Morris et al., AAAI 2019; Xu et al., ICLR 2019], we establish that recurrent GNNs can actually reach this limit. This is in contrast to non-recurrent GNNs, which have the power of Weisfeiler-Leman only in a very weak, \"non-uniform\", sense where every graph size requires a different GNN model to compute with. The emulation we construct introduces only a polynomial overhead in both time and space.","authors":["Eran Rosenbluth","Martin Grohe"],"url":"https://arxiv.org/abs/2505.00291"}
{"created":"2025-05-02","title":"Reducing Sexual Predation and Victimization Through Warnings and Awareness among High-Risk Users","abstract":"Online sexual predators target children by building trust, creating dependency, and arranging meetings for sexual purposes. This poses a significant challenge for online communication platforms that strive to monitor and remove such content and terminate predators' accounts. However, these platforms can only take such actions if sexual predators explicitly violate the terms of service, not during the initial stages of relationship-building. This study designed and evaluated a strategy to prevent sexual predation and victimization by delivering warnings and raising awareness among high-risk individuals based on the routine activity theory in criminal psychology. We identified high-risk users as those with a high probability of committing or being subjected to violations, using a machine learning model that analyzed social networks and monitoring data from the platform. We conducted a randomized controlled trial on a Japanese avatar-based communication application, Pigg Party. High-risk players in the intervention group received warnings and awareness-building messages, while those in the control group did not receive the messages, regardless of their risk level. The trial involved 12,842 high-risk players in the intervention group and 12,844 in the control group for 138 days. The intervention successfully reduced violations and being violated among women for 12 weeks, although the impact on men was limited. These findings contribute to efforts to combat online sexual abuse and advance understanding of criminal psychology.","authors":["Masanori Takano","Mao Nishiguchi","Fujio Toriumi"],"url":"https://arxiv.org/abs/2505.00293"}
{"created":"2025-05-02","title":"Fine-grained spatial-temporal perception for gas leak segmentation","abstract":"Gas leaks pose significant risks to human health and the environment. Despite long-standing concerns, there are limited methods that can efficiently and accurately detect and segment leaks due to their concealed appearance and random shapes. In this paper, we propose a Fine-grained Spatial-Temporal Perception (FGSTP) algorithm for gas leak segmentation. FGSTP captures critical motion clues across frames and integrates them with refined object features in an end-to-end network. Specifically, we first construct a correlation volume to capture motion information between consecutive frames. Then, the fine-grained perception progressively refines the object-level features using previous outputs. Finally, a decoder is employed to optimize boundary segmentation. Because there is no highly precise labeled dataset for gas leak segmentation, we manually label a gas leak video dataset, GasVid. Experimental results on GasVid demonstrate that our model excels in segmenting non-rigid objects such as gas leaks, generating the most accurate mask compared to other state-of-the-art (SOTA) models.","authors":["Xinlong Zhao","Shan Du"],"url":"https://arxiv.org/abs/2505.00295"}
{"created":"2025-05-02","title":"The Complexity of Minimum-Envy House Allocation Over Graphs","abstract":"In this paper, we study a generalization of the House Allocation problem. In our problem, agents are represented by vertices of a graph $\\GG_{\\mathcal{A}} = (\\AA, E_\\AA)$, and each agent $a \\in \\AA$ is associated with a set of preferred houses $\\PP_a \\subseteq \\HH$, where $\\AA$ is the set of agents and $\\HH$ is the set of houses. A house allocation is an injective function $\\phi: \\AA \\rightarrow \\HH$, and an agent $a$ envies a neighbour $a' \\in N_{\\GG_\\AA}(a)$ under $\\phi$ if $\\phi(a) \\notin \\PP_a$ and $\\phi(a') \\in \\PP_a$. We study two natural objectives: the first problem called \\ohaa, aims to compute an allocation that minimizes the number of envious agents; the second problem called \\ohaah aims to maximize, among all minimum-envy allocations, the number of agents who are assigned a house they prefer. These two objectives capture complementary notions of fairness and individual satisfaction.","authors":["Palash Dey","Anubhav Dhar","Ashlesha Hota","Sudeshna Kolay"],"url":"https://arxiv.org/abs/2505.00296"}
{"created":"2025-05-02","title":"Intelligent Task Scheduling for Microservices via A3C-Based Reinforcement Learning","abstract":"To address the challenges of high resource dynamism and intensive task concurrency in microservice systems, this paper proposes an adaptive resource scheduling method based on the A3C reinforcement learning algorithm. The scheduling problem is modeled as a Markov Decision Process, where policy and value networks are jointly optimized to enable fine-grained resource allocation under varying load conditions. The method incorporates an asynchronous multi-threaded learning mechanism, allowing multiple agents to perform parallel sampling and synchronize updates to the global network parameters. This design improves both policy convergence efficiency and model stability. In the experimental section, a real-world dataset is used to construct a scheduling scenario. The proposed method is compared with several typical approaches across multiple evaluation metrics, including task delay, scheduling success rate, resource utilization, and convergence speed. The results show that the proposed method delivers high scheduling performance and system stability in multi-task concurrent environments. It effectively alleviates the resource allocation bottlenecks faced by traditional methods under heavy load, demonstrating its practical value for intelligent scheduling in microservice systems.","authors":["Yang Wang","Tengda Tang","Zhou Fang","Yingnan Deng","Yifei Duan"],"url":"https://arxiv.org/abs/2505.00299"}
{"created":"2025-05-02","title":"Temporal Attention Evolutional Graph Convolutional Network for Multivariate Time Series Forecasting","abstract":"Multivariate time series forecasting enables the prediction of future states by leveraging historical data, thereby facilitating decision-making processes. Each data node in a multivariate time series encompasses a sequence of multiple dimensions. These nodes exhibit interdependent relationships, forming a graph structure. While existing prediction methods often assume a fixed graph structure, many real-world scenarios involve dynamic graph structures. Moreover, interactions among time series observed at different time scales vary significantly. To enhance prediction accuracy by capturing precise temporal and spatial features, this paper introduces the Temporal Attention Evolutional Graph Convolutional Network (TAEGCN). This novel method not only integrates causal temporal convolution and a multi-head self-attention mechanism to learn temporal features of nodes, but also construct the dynamic graph structure based on these temporal features to keep the consistency of the changing in spatial feature with temporal series. TAEGCN adeptly captures temporal causal relationships and hidden spatial dependencies within the data. Furthermore, TAEGCN incorporates a unified neural network that seamlessly integrates these components to generate final predictions. Experimental results conducted on two public transportation network datasets, METR-LA and PEMS-BAY, demonstrate the superior performance of the proposed model.","authors":["Xinlong Zhao","Liying Zhang","Tianbo Zou","Yan Zhang"],"url":"https://arxiv.org/abs/2505.00302"}
{"created":"2025-05-02","title":"J-PARSE: Jacobian-based Projection Algorithm for Resolving Singularities Effectively in Inverse Kinematic Control of Serial Manipulators","abstract":"J-PARSE is a method for smooth first-order inverse kinematic control of a serial manipulator near kinematic singularities. The commanded end-effector velocity is interpreted component-wise, according to the available mobility in each dimension of the task space. First, a substitute \"Safety\" Jacobian matrix is created, keeping the aspect ratio of the manipulability ellipsoid above a threshold value. The desired motion is then projected onto non-singular and singular directions, and the latter projection scaled down by a factor informed by the threshold value. A right-inverse of the non-singular Safety Jacobian is applied to the modified command. In the absence of joint limits and collisions, this ensures smooth transition into and out of low-rank poses, guaranteeing asymptotic stability for target poses within the workspace, and stability for those outside. Velocity control with J-PARSE is benchmarked against the Least-Squares and Damped Least-Squares inversions of the Jacobian, and shows high accuracy in reaching and leaving singular target poses. By expanding the available workspace of manipulators, the method finds applications in servoing, teleoperation, and learning.","authors":["Shivani Guptasarma","Matthew Strong","Honghao Zhen","Monroe Kennedy III"],"url":"https://arxiv.org/abs/2505.00306"}
{"created":"2025-05-02","title":"Gateformer: Advancing Multivariate Time Series Forecasting through Temporal and Variate-Wise Attention with Gated Representations","abstract":"There has been a recent surge of interest in time series modeling using the Transformer architecture. However, forecasting multivariate time series with Transformer presents a unique challenge as it requires modeling both temporal (cross-time) and variate (cross-variate) dependencies. While Transformer-based models have gained popularity for their flexibility in capturing both sequential and cross-variate relationships, it is unclear how to best integrate these two sources of information in the context of the Transformer architecture while optimizing for both performance and efficiency. We re-purpose the Transformer architecture to effectively model both cross-time and cross-variate dependencies. Our approach begins by embedding each variate independently into a variate-wise representation that captures its cross-time dynamics, and then models cross-variate dependencies through attention mechanisms on these learned embeddings. Gating operations in both cross-time and cross-variate modeling phases regulate information flow, allowing the model to focus on the most relevant features for accurate predictions. Our method achieves state-of-the-art performance across 13 real-world datasets and can be seamlessly integrated into other Transformer-based and LLM-based forecasters, delivering performance improvements up to 20.7\\% over original models. Code is available at this repository: https://github.com/nyuolab/Gateformer.","authors":["Yu-Hsiang Lan","Anton Alyakin","Eric K. Oermann"],"url":"https://arxiv.org/abs/2505.00307"}
{"created":"2025-05-02","title":"AI-Assisted Decision-Making for Clinical Assessment of Auto-Segmented Contour Quality","abstract":"Purpose: This study presents a Deep Learning (DL)-based quality assessment (QA) approach for evaluating auto-generated contours (auto-contours) in radiotherapy, with emphasis on Online Adaptive Radiotherapy (OART). Leveraging Bayesian Ordinal Classification (BOC) and calibrated uncertainty thresholds, the method enables confident QA predictions without relying on ground truth contours or extensive manual labeling. Methods: We developed a BOC model to classify auto-contour quality and quantify prediction uncertainty. A calibration step was used to optimize uncertainty thresholds that meet clinical accuracy needs. The method was validated under three data scenarios: no manual labels, limited labels, and extensive labels. For rectum contours in prostate cancer, we applied geometric surrogate labels when manual labels were absent, transfer learning when limited, and direct supervision when ample labels were available. Results: The BOC model delivered robust performance across all scenarios. Fine-tuning with just 30 manual labels and calibrating with 34 subjects yielded over 90% accuracy on test data. Using the calibrated threshold, over 93% of the auto-contours' qualities were accurately predicted in over 98% of cases, reducing unnecessary manual reviews and highlighting cases needing correction. Conclusion: The proposed QA model enhances contouring efficiency in OART by reducing manual workload and enabling fast, informed clinical decisions. Through uncertainty quantification, it ensures safer, more reliable radiotherapy workflows.","authors":["Biling Wang","Austen Maniscalco","Ti Bai","Siqiu Wang","Michael Dohopolski","Mu-Han Lin","Chenyang Shen","Dan Nguyen","Junzhou Huang","Steve Jiang","Xinlei Wang"],"url":"https://arxiv.org/abs/2505.00308"}
{"created":"2025-05-02","title":"AWARE-NET: Adaptive Weighted Averaging for Robust Ensemble Network in Deepfake Detection","abstract":"Deepfake detection has become increasingly important due to the rise of synthetic media, which poses significant risks to digital identity and cyber presence for security and trust. While multiple approaches have improved detection accuracy, challenges remain in achieving consistent performance across diverse datasets and manipulation types. In response, we propose a novel two-tier ensemble framework for deepfake detection based on deep learning that hierarchically combines multiple instances of three state-of-the-art architectures: Xception, Res2Net101, and EfficientNet-B7. Our framework employs a unique approach where each architecture is instantiated three times with different initializations to enhance model diversity, followed by a learnable weighting mechanism that dynamically combines their predictions. Unlike traditional fixed-weight ensembles, our first-tier averages predictions within each architecture family to reduce model variance, while the second tier learns optimal contribution weights through backpropagation, automatically adjusting each architecture's influence based on their detection reliability. Our experiments achieved state-of-the-art intra-dataset performance with AUC scores of 99.22% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.06% (FF++) and 99.94% (CelebDF-v2) without augmentation. With augmentation, we achieve AUC scores of 99.47% (FF++) and 100.00% (CelebDF-v2), and F1 scores of 98.43% (FF++) and 99.95% (CelebDF-v2). The framework demonstrates robust cross-dataset generalization, achieving AUC scores of 88.20% and 72.52%, and F1 scores of 93.16% and 80.62% in cross-dataset evaluations.","authors":["Muhammad Salman","Iqra Tariq","Mishal Zulfiqar","Muqadas Jalal","Sami Aujla","Sumbal Fatima"],"url":"https://arxiv.org/abs/2505.00312"}
{"created":"2025-05-02","title":"Mixture of Sparse Attention: Content-Based Learnable Sparse Attention via Expert-Choice Routing","abstract":"Recent advances in large language models highlighted the excessive quadratic cost of self-attention. Despite the significant research efforts, subquadratic attention methods still suffer from inferior performance in practice. We hypothesize that dynamic, learned content-based sparsity can lead to more efficient attention mechanisms. We present Mixture of Sparse Attention (MoSA), a novel approach inspired by Mixture of Experts (MoE) with expert choice routing. MoSA dynamically selects tokens for each attention head, allowing arbitrary sparse attention patterns. By selecting $k$ tokens from a sequence of length $T$, MoSA reduces the computational complexity of each attention head from $O(T^2)$ to $O(k^2 + T)$. This enables using more heads within the same computational budget, allowing higher specialization. We show that among the tested sparse attention variants, MoSA is the only one that can outperform the dense baseline, sometimes with up to 27% better perplexity for an identical compute budget. MoSA can also reduce the resource usage compared to dense self-attention. Despite using torch implementation without an optimized kernel, perplexity-matched MoSA models are simultaneously faster in wall-clock time, require less memory for training, and drastically reduce the size of the KV-cache compared to the dense transformer baselines.","authors":["Piotr Pi\\k{e}kos","R\\'obert Csord\\'as","J\\\"urgen Schmidhuber"],"url":"https://arxiv.org/abs/2505.00315"}
{"created":"2025-05-02","title":"Surrogate modeling of Cellular-Potts Agent-Based Models as a segmentation task using the U-Net neural network architecture","abstract":"The Cellular-Potts model is a powerful and ubiquitous framework for developing computational models for simulating complex multicellular biological systems. Cellular-Potts models (CPMs) are often computationally expensive due to the explicit modeling of interactions among large numbers of individual model agents and diffusive fields described by partial differential equations (PDEs). In this work, we develop a convolutional neural network (CNN) surrogate model using a U-Net architecture that accounts for periodic boundary conditions. We use this model to accelerate the evaluation of a mechanistic CPM previously used to investigate \\textit{in vitro} vasculogenesis. The surrogate model was trained to predict 100 computational steps ahead (Monte-Carlo steps, MCS), accelerating simulation evaluations by a factor of 590 times compared to CPM code execution. Over multiple recursive evaluations, our model effectively captures the emergent behaviors demonstrated by the original Cellular-Potts model of such as vessel sprouting, extension and anastomosis, and contraction of vascular lacunae. This approach demonstrates the potential for deep learning to serve as efficient surrogate models for CPM simulations, enabling faster evaluation of computationally expensive CPM of biological processes at greater spatial and temporal scales.","authors":["Tien Comlekoglu","J. Quetzalc\\'oatl Toledo-Mar\\'in","Tina Comlekoglu","Douglas W. DeSimone","Shayn M. Peirce","Geoffrey Fox","James A. Glazier"],"url":"https://arxiv.org/abs/2505.00316"}
{"created":"2025-05-02","title":"Beyond Quadratic Costs in LQR: Bregman Divergence Control","abstract":"In the past couple of decades, the use of ``non-quadratic\" convex cost functions has revolutionized signal processing, machine learning, and statistics, allowing one to customize solutions to have desired structures and properties. However, the situation is not the same in control where the use of quadratic costs still dominates, ostensibly because determining the ``value function\", i.e., the optimal expected cost-to-go, which is critical to the construction of the optimal controller, becomes computationally intractable as soon as one considers general convex costs. As a result, practitioners often resort to heuristics and approximations, such as model predictive control that only looks a few steps into the future. In the quadratic case, the value function is easily determined by solving Riccati equations. In this work, we consider a special class of convex cost functions constructed from Bregman divergence and show how, with appropriate choices, they can be used to fully extend the framework developed for the quadratic case. The resulting optimal controllers are infinite horizon, come with stability guarantees, and have state-feedback, or estimated state-feedback, laws. They exhibit a much wider range of behavior than their quadratic counterparts since the feedback laws are nonlinear. The approach can be applied to several cases of interest, including safety control, sparse control, and bang-bang control.","authors":["Babak Hassibi","Joudi Hajar","Reza Ghane"],"url":"https://arxiv.org/abs/2505.00317"}
{"created":"2025-05-02","title":"FedEMA: Federated Exponential Moving Averaging with Negative Entropy Regularizer in Autonomous Driving","abstract":"Street Scene Semantic Understanding (denoted as S3U) is a crucial but complex task for autonomous driving (AD) vehicles. Their inference models typically face poor generalization due to domain-shift. Federated Learning (FL) has emerged as a promising paradigm for enhancing the generalization of AD models through privacy-preserving distributed learning. However, these FL AD models face significant temporal catastrophic forgetting when deployed in dynamically evolving environments, where continuous adaptation causes abrupt erosion of historical knowledge. This paper proposes Federated Exponential Moving Average (FedEMA), a novel framework that addresses this challenge through two integral innovations: (I) Server-side model's historical fitting capability preservation via fusing current FL round's aggregation model and a proposed previous FL round's exponential moving average (EMA) model; (II) Vehicle-side negative entropy regularization to prevent FL models' possible overfitting to EMA-introduced temporal patterns. Above two strategies empower FedEMA a dual-objective optimization that balances model generalization and adaptability. In addition, we conduct theoretical convergence analysis for the proposed FedEMA. Extensive experiments both on Cityscapes dataset and Camvid dataset demonstrate FedEMA's superiority over existing approaches, showing 7.12% higher mean Intersection-over-Union (mIoU).","authors":["Wei-Bin Kou","Guangxu Zhu","Bingyang Cheng","Shuai Wang","Ming Tang","Yik-Chung Wu"],"url":"https://arxiv.org/abs/2505.00318"}
{"created":"2025-05-02","title":"Beyond Quadratic Costs: A Bregman Divergence Approach to H$_\\infty$ Control","abstract":"This paper presents a novel extension of the H$_\\infty$ control framework that generalizes the traditional quadratic cost formulation to accommodate strictly convex, nonquadratic functions for the state, control input, and disturbance. This new formulation not only captures additional noise characteristics but also supports a range of performance objectives-including sparse control, safety constraints, and other tailored behaviors-beyond what is possible with quadratic costs. We derive a closed-form solution of a central controller that minimizes the worst-case performance ratio under the proposed cost structure. Furthermore, we develop Riccati-like equations that impose necessary and sufficient conditions on the nonquadratic cost functions, thereby ensuring the existence of a robust solution. Finally, we rigorously establish Lyapunov stability for the closed-loop system. The proposed framework bridges robust control theory with modern approaches in machine learning and signal processing, offering enhanced flexibility and improved performance in complex control scenarios.","authors":["Joudi Hajar","Reza Ghane","Babak Hassibi"],"url":"https://arxiv.org/abs/2505.00319"}
{"created":"2025-05-02","title":"Edge Large AI Models: Revolutionizing 6G Networks","abstract":"Large artificial intelligence models (LAMs) possess human-like abilities to solve a wide range of real-world problems, exemplifying the potential of experts in various domains and modalities. By leveraging the communication and computation capabilities of geographically dispersed edge devices, edge LAM emerges as an enabling technology to empower the delivery of various real-time intelligent services in 6G. Unlike traditional edge artificial intelligence (AI) that primarily supports a single task using small models, edge LAM is featured by the need of the decomposition and distributed deployment of large models, and the ability to support highly generalized and diverse tasks. However, due to limited communication, computation, and storage resources over wireless networks, the vast number of trainable neurons and the substantial communication overhead pose a formidable hurdle to the practical deployment of edge LAMs. In this paper, we investigate the opportunities and challenges of edge LAMs from the perspectives of model decomposition and resource management. Specifically, we propose collaborative fine-tuning and full-parameter training frameworks, alongside a microservice-assisted inference architecture, to enhance the deployment of edge LAM over wireless networks. Additionally, we investigate the application of edge LAM in air-interface designs, focusing on channel prediction and beamforming. These innovative frameworks and applications offer valuable insights and solutions for advancing 6G technology.","authors":["Zixin Wang","Yuanming Shi","Yong Zhou","Jingyang Zhu","Khaled. B. Letaief"],"url":"https://arxiv.org/abs/2505.00321"}
{"created":"2025-05-02","title":"AI2-Active Safety: AI-enabled Interaction-aware Active Safety Analysis with Vehicle Dynamics","abstract":"This paper introduces an AI-enabled, interaction-aware active safety analysis framework that accounts for groupwise vehicle interactions. Specifically, the framework employs a bicycle model-augmented with road gradient considerations-to accurately capture vehicle dynamics. In parallel, a hypergraph-based AI model is developed to predict probabilistic trajectories of ambient traffic. By integrating these two components, the framework derives vehicle intra-spacing over a 3D road surface as the solution of a stochastic ordinary differential equation, yielding high-fidelity surrogate safety measures such as time-to-collision (TTC). To demonstrate its effectiveness, the framework is analyzed using stochastic numerical methods comprising 4th-order Runge-Kutta integration and AI inference, generating probability-weighted high-fidelity TTC (HF-TTC) distributions that reflect complex multi-agent maneuvers and behavioral uncertainties. Evaluated with HF-TTC against traditional constant-velocity TTC and non-interaction-aware approaches on highway datasets, the proposed framework offers a systematic methodology for active safety analysis with enhanced potential for improving safety perception in complex traffic environments.","authors":["Keshu Wu","Zihao Li","Sixu Li","Xinyue Ye","Dominique Lord","Yang Zhou"],"url":"https://arxiv.org/abs/2505.00322"}
{"created":"2025-05-02","title":"Recursive Algorithms for Sparse Parameter Identification of Multivariate Stochastic Systems with Non-stationary Observations","abstract":"The classical sparse parameter identification methods are usually based on the iterative basis selection such as greedy algorithms, or the numerical optimization of regularized cost functions such as LASSO and Bayesian posterior probability distribution, etc., which, however, are not suitable for online sparsity inference when data arrive sequentially. This paper presents recursive algorithms for sparse parameter identification of multivariate stochastic systems with non-stationary observations. First, a new bivariate criterion function is presented by introducing an auxiliary variable matrix into a weighted $L_1$ regularization criterion. The new criterion function is subsequently decomposed into two solvable subproblems via alternating optimization of the two variable matrices, for which the optimizers can be explicitly formulated into recursive equations. Second, under the non-stationary and non-persistent excitation conditions on the systems, theoretical properties of the recursive algorithms are established. That is, the estimates are proved to be with (i) set convergence, i.e., the accurate estimation of the sparse index set of the unknown parameter matrix, and (ii) parameter convergence, i.e., the consistent estimation for values of the non-zero elements of the unknown parameter matrix. Finally, numerical examples are given to support the theoretical analysis.","authors":["Yanxin Fu","Wenxiao Zhao"],"url":"https://arxiv.org/abs/2505.00323"}
{"created":"2025-05-02","title":"CognitionNet: A Collaborative Neural Network for Play Style Discovery in Online Skill Gaming Platform","abstract":"Games are one of the safest source of realizing self-esteem and relaxation at the same time. An online gaming platform typically has massive data coming in, e.g., in-game actions, player moves, clickstreams, transactions etc. It is rather interesting, as something as simple as data on gaming moves can help create a psychological imprint of the user at that moment, based on her impulsive reactions and response to a situation in the game. Mining this knowledge can: (a) immediately help better explain observed and predicted player behavior; and (b) consequently propel deeper understanding towards players' experience, growth and protection. To this effect, we focus on discovery of the \"game behaviours\" as micro-patterns formed by continuous sequence of games and the persistent \"play styles\" of the players' as a sequence of such sequences on an online skill gaming platform for Rummy. We propose a two stage deep neural network, CognitionNet. The first stage focuses on mining game behaviours as cluster representations in a latent space while the second aggregates over these micro patterns to discover play styles via a supervised classification objective around player engagement. The dual objective allows CognitionNet to reveal several player psychology inspired decision making and tactics. To our knowledge, this is the first and one-of-its-kind research to fully automate the discovery of: (i) player psychology and game tactics from telemetry data; and (ii) relevant diagnostic explanations to players' engagement predictions. The collaborative training of the two networks with differential input dimensions is enabled using a novel formulation of \"bridge loss\". The network plays pivotal role in obtaining homogeneous and consistent play style definitions and significantly outperforms the SOTA baselines wherever applicable.","authors":["Rukma Talwadker","Surajit Chakrabarty","Aditya Pareek","Tridib Mukherjee","Deepak Saini"],"url":"https://arxiv.org/abs/2505.00325"}
{"created":"2025-05-02","title":"Optimal Vector Compressed Sensing Using James Stein Shrinkage","abstract":"The trend in modern science and technology is to take vector measurements rather than scalars, ruthlessly scaling to ever higher dimensional vectors. For about two decades now, traditional scalar Compressed Sensing has been synonymous with a Convex Optimization based procedure called Basis Pursuit. In the vector recovery case, the natural tendency is to return to a straightforward vector extension of Basis Pursuit, also based on Convex Optimization. However, Convex Optimization is provably suboptimal, particularly when $B$ is large. In this paper, we propose SteinSense, a lightweight iterative algorithm, which is provably optimal when $B$ is large. It does not have any tuning parameter, does not need any training data, requires zero knowledge of sparsity, is embarrassingly simple to implement, and all of this makes it easily scalable to high vector dimensions. We conduct a massive volume of both real and synthetic experiments that confirm the efficacy of SteinSense, and also provide theoretical justification based on ideas from Approximate Message Passing. Fascinatingly, we discover that SteinSense is quite robust, delivering the same quality of performance on real data, and even under substantial departures from conditions under which existing theory holds.","authors":["Apratim Dey","David Donoho"],"url":"https://arxiv.org/abs/2505.00326"}
{"created":"2025-05-02","title":"Active Contact Engagement for Aerial Navigation in Unknown Environments with Glass","abstract":"Autonomous aerial robots are increasingly being deployed in real-world scenarios, where transparent glass obstacles present significant challenges to reliable navigation. Researchers have investigated the use of non-contact sensors and passive contact-resilient aerial vehicle designs to detect glass surfaces, which are often limited in terms of robustness and efficiency. In this work, we propose a novel approach for robust autonomous aerial navigation in unknown environments with transparent glass obstacles, combining the strengths of both sensor-based and contact-based glass detection. The proposed system begins with the incremental detection and information maintenance about potential glass surfaces using visual sensor measurements. The vehicle then actively engages in touch actions with the visually detected potential glass surfaces using a pair of lightweight contact-sensing modules to confirm or invalidate their presence. Following this, the volumetric map is efficiently updated with the glass surface information and safe trajectories are replanned on the fly to circumvent the glass obstacles. We validate the proposed system through real-world experiments in various scenarios, demonstrating its effectiveness in enabling efficient and robust autonomous aerial navigation in complex real-world environments with glass obstacles.","authors":["Xinyi Chen","Yichen Zhang","Hetai Zou","Junzhe Wang","Shaojie Shen"],"url":"https://arxiv.org/abs/2505.00332"}
{"created":"2025-05-02","title":"Communication-Efficient Wireless Federated Fine-Tuning for Large-Scale AI Models","abstract":"Transformer-based large language models (LLMs) have achieved remarkable success across various tasks. Yet, fine-tuning such massive models in federated learning (FL) settings poses significant challenges due to resource constraints and communication overhead. Low-Rank Adaptation (LoRA) addresses these issues by training compact, low-rank matrices instead of fully fine-tuning large models. This paper introduces a wireless federated LoRA fine-tuning framework that optimizes both learning performance and communication efficiency. We provide a novel convergence analysis, revealing how LoRA rank and covariance effects influence FL training dynamics. Leveraging these insights, we propose Sparsified Orthogonal Fine-Tuning (\\textbf{SOFT}), an adaptive sparsification method that streamlines parameter updates without expensive matrix multiplications and singular value decomposition (SVD) operations. Additionally, we present a Two Stage Federated Algorithm (\\textbf{TSFA}) algorithm that pre-determines key parameters offline and dynamically adjusts bandwidth and sparsification online, ensuring efficient training under latency constraints. Experiments on benchmark datasets show that our approach achieves accuracy comparable to ideal scenario models while significantly reducing communication overhead. Our framework thus enables scalable, resource-efficient deployment of large models in real-world wireless FL scenarios.","authors":["Bumjun Kim","Wan Choi"],"url":"https://arxiv.org/abs/2505.00333"}
{"created":"2025-05-02","title":"Quaternion Wavelet-Conditioned Diffusion Models for Image Super-Resolution","abstract":"Image Super-Resolution is a fundamental problem in computer vision with broad applications spacing from medical imaging to satellite analysis. The ability to reconstruct high-resolution images from low-resolution inputs is crucial for enhancing downstream tasks such as object detection and segmentation. While deep learning has significantly advanced SR, achieving high-quality reconstructions with fine-grained details and realistic textures remains challenging, particularly at high upscaling factors. Recent approaches leveraging diffusion models have demonstrated promising results, yet they often struggle to balance perceptual quality with structural fidelity. In this work, we introduce ResQu a novel SR framework that integrates a quaternion wavelet preprocessing framework with latent diffusion models, incorporating a new quaternion wavelet- and time-aware encoder. Unlike prior methods that simply apply wavelet transforms within diffusion models, our approach enhances the conditioning process by exploiting quaternion wavelet embeddings, which are dynamically integrated at different stages of denoising. Furthermore, we also leverage the generative priors of foundation models such as Stable Diffusion. Extensive experiments on domain-specific datasets demonstrate that our method achieves outstanding SR results, outperforming in many cases existing approaches in perceptual quality and standard evaluation metrics. The code will be available after the revision process.","authors":["Luigi Sigillo","Christian Bianchi","Danilo Comminiello"],"url":"https://arxiv.org/abs/2505.00334"}
{"created":"2025-05-02","title":"Efficient Neural Video Representation with Temporally Coherent Modulation","abstract":"Implicit neural representations (INR) has found successful applications across diverse domains. To employ INR in real-life, it is important to speed up training. In the field of INR for video applications, the state-of-the-art approach employs grid-type parametric encoding and successfully achieves a faster encoding speed in comparison to its predecessors. However, the grid usage, which does not consider the video's dynamic nature, leads to redundant use of trainable parameters. As a result, it has significantly lower parameter efficiency and higher bitrate compared to NeRV-style methods that do not use a parametric encoding. To address the problem, we propose Neural Video representation with Temporally coherent Modulation (NVTM), a novel framework that can capture dynamic characteristics of video. By decomposing the spatio-temporal 3D video data into a set of 2D grids with flow information, NVTM enables learning video representation rapidly and uses parameter efficiently. Our framework enables to process temporally corresponding pixels at once, resulting in the fastest encoding speed for a reasonable video quality, especially when compared to the NeRV-style method, with a speed increase of over 3 times. Also, it remarks an average of 1.54dB/0.019 improvements in PSNR/LPIPS on UVG (Dynamic) (even with 10% fewer parameters) and an average of 1.84dB/0.013 improvements in PSNR/LPIPS on MCL-JCV (Dynamic), compared to previous grid-type works. By expanding this to compression tasks, we demonstrate comparable performance to video compression standards (H.264, HEVC) and recent INR approaches for video compression. Additionally, we perform extensive experiments demonstrating the superior performance of our algorithm across diverse tasks, encompassing super resolution, frame interpolation and video inpainting. Project page is https://sujiikim.github.io/NVTM/.","authors":["Seungjun Shin","Suji Kim","Dokwan Oh"],"url":"https://arxiv.org/abs/2505.00335"}
{"created":"2025-05-02","title":"T2VPhysBench: A First-Principles Benchmark for Physical Consistency in Text-to-Video Generation","abstract":"Text-to-video generative models have made significant strides in recent years, producing high-quality videos that excel in both aesthetic appeal and accurate instruction following, and have become central to digital art creation and user engagement online. Yet, despite these advancements, their ability to respect fundamental physical laws remains largely untested: many outputs still violate basic constraints such as rigid-body collisions, energy conservation, and gravitational dynamics, resulting in unrealistic or even misleading content. Existing physical-evaluation benchmarks typically rely on automatic, pixel-level metrics applied to simplistic, life-scenario prompts, and thus overlook both human judgment and first-principles physics. To fill this gap, we introduce \\textbf{T2VPhysBench}, a first-principled benchmark that systematically evaluates whether state-of-the-art text-to-video systems, both open-source and commercial, obey twelve core physical laws including Newtonian mechanics, conservation principles, and phenomenological effects. Our benchmark employs a rigorous human evaluation protocol and includes three targeted studies: (1) an overall compliance assessment showing that all models score below 0.60 on average in each law category; (2) a prompt-hint ablation revealing that even detailed, law-specific hints fail to remedy physics violations; and (3) a counterfactual robustness test demonstrating that models often generate videos that explicitly break physical rules when so instructed. The results expose persistent limitations in current architectures and offer concrete insights for guiding future research toward truly physics-aware video generation.","authors":["Xuyang Guo","Jiayan Huo","Zhenmei Shi","Zhao Song","Jiahao Zhang","Jiale Zhao"],"url":"https://arxiv.org/abs/2505.00337"}
{"created":"2025-05-02","title":"New Distributed Interactive Proofs for Planarity: A Matter of Left and Right","abstract":"We provide new distributed interactive proofs (DIP) for planarity and related graph families. The notion of a \\emph{distributed interactive proof} (DIP) was introduced by Kol, Oshman, and Saxena (PODC 2018). In this setting, the verifier consists of $n$ nodes connected by a communication graph $G$. The prover is a single entity that communicates with all nodes by short messages. The goal is to verify that the graph $G$ satisfies a certain property (e.g., planarity) in a small number of rounds, and with a small communication bound, denoted as the \\emph{proof size}.","authors":["Yuval Gil","Merav Parter"],"url":"https://arxiv.org/abs/2505.00338"}
{"created":"2025-05-02","title":"Enhancing AI-Driven Education: Integrating Cognitive Frameworks, Linguistic Feedback Analysis, and Ethical Considerations for Improved Content Generation","abstract":"Artificial intelligence (AI) is rapidly transforming education, presenting unprecedented opportunities for personalized learning and streamlined content creation. However, realizing the full potential of AI in educational settings necessitates careful consideration of the quality, cognitive depth, and ethical implications of AI-generated materials. This paper synthesizes insights from four related studies to propose a comprehensive framework for enhancing AI-driven educational tools. We integrate cognitive assessment frameworks (Bloom's Taxonomy and SOLO Taxonomy), linguistic analysis of AI-generated feedback, and ethical design principles to guide the development of effective and responsible AI tools. We outline a structured three-phase approach encompassing cognitive alignment, linguistic feedback integration, and ethical safeguards. The practical application of this framework is demonstrated through its integration into OneClickQuiz, an AI-powered Moodle plugin for quiz generation. This work contributes a comprehensive and actionable guide for educators, researchers, and developers aiming to harness AI's potential while upholding pedagogical and ethical standards in educational content generation.","authors":["Antoun Yaacoub","Sansiri Tarnpradab","Phattara Khumprom","Zainab Assaghir","Lionel Prevost","J\\'er\\^ome Da-Rugna"],"url":"https://arxiv.org/abs/2505.00339"}
{"created":"2025-05-02","title":"Vehicular Communication Security: Multi-Channel and Multi-Factor Authentication","abstract":"Secure and reliable communications are crucial for Intelligent Transportation Systems (ITSs), where Vehicle-to-Infrastructure (V2I) communication plays a key role in enabling mobility-enhancing and safety-critical services. Current V2I authentication relies on credential-based methods over wireless Non-Line-of-Sight (NLOS) channels, leaving them exposed to remote impersonation and proximity attacks. To mitigate these risks, we propose a unified Multi-Channel, Multi-Factor Authentication (MFA) scheme that combines NLOS cryptographic credentials with a Line-of-Sight (LOS) visual channel. Our approach leverages a challenge-response security paradigm: the infrastructure issues challenges and the vehicle's headlights respond by flashing a structured sequence containing encoded security data. Deep learning models on the infrastructure side then decode the embedded information to authenticate the vehicle. Real-world experimental evaluations demonstrate high test accuracy, reaching an average of 95% and 96.6%, respectively, under various lighting, weather, speed, and distance conditions. Additionally, we conducted extensive experiments on three state-of-the-art deep learning models, including detailed ablation studies for decoding the flashing sequence. Our results indicate that the optimal architecture employs a dual-channel design, enabling simultaneous decoding of the flashing sequence and extraction of vehicle spatial and locational features for robust authentication.","authors":["Marco De Vincenzi","Shuyang Sun","Chen Bo Calvin Zhang","Manuel Garcia","Shaozu Ding","Chiara Bodei","Ilaria Matteucci","Dajiang Suo"],"url":"https://arxiv.org/abs/2505.00340"}
{"created":"2025-05-02","title":"LLMPrism: Black-box Performance Diagnosis for Production LLM Training Platforms","abstract":"Large Language Models (LLMs) have brought about revolutionary changes in diverse fields, rendering LLM training of utmost importance for modern enterprises. To meet this demand, multi-tenant large-scale LLM training platforms have been built to offer LLM training services. Nevertheless, due to the complexity and synchronous nature of LLM training process, performance issues occur frequently and can result in substantial resource wastage. The limited visibility from the perspective of platform providers impedes existing profiling methods and poses challenges to the monitoring and diagnosis of the performance of LLM training jobs. For the first time, this paper proposes the utilization of underlying network flow data to reconstruct the training timelines of jobs based on the distinct characteristics in the LLM training procedure. We design LLMPrism, the first black-box performance diagnosis system for LLM training platforms. By progressively recognizing LLM training jobs, identifying their parallelism strategies, and reconstructing the training timelines, LLMPrism achieves non-intrusive, lightweight, and continuous monitoring of LLM training systems. Leveraging this monitoring capability, it further effectively diagnoses potential performance issues. Since Oct. 2024, LLMPrism has been deployed on our large-scale production Platform-X, in which the evaluations and deployment experiences demonstrate that LLMPrism can achieve accurate timeline reconstruction with an error within 0.3% and effectively diagnose various performance issues.","authors":["Zhihan Jiang","Rui Ren","Guangba Yu","Yulun Wu","Wenwei Gu","Yichen Li","Yujie Huang","Cong Feng","Zengyin Yang","Yongqiang Yang","Michael R. Lyu"],"url":"https://arxiv.org/abs/2505.00342"}
{"created":"2025-05-02","title":"Pushing the Limits of Low-Bit Optimizers: A Focus on EMA Dynamics","abstract":"The explosion in model sizes leads to continued growth in prohibitive training/fine-tuning costs, particularly for stateful optimizers which maintain auxiliary information of even 2x the model size to achieve optimal convergence. We therefore present in this work a novel type of optimizer that carries with extremely lightweight state overloads, achieved through ultra-low-precision quantization. While previous efforts have achieved certain success with 8-bit or 4-bit quantization, our approach enables optimizers to operate at precision as low as 3 bits, or even 2 bits per state element. This is accomplished by identifying and addressing two critical challenges: the signal swamping problem in unsigned quantization that results in unchanged state dynamics, and the rapidly increased gradient variance in signed quantization that leads to incorrect descent directions. The theoretical analysis suggests a tailored logarithmic quantization for the former and a precision-specific momentum value for the latter. Consequently, the proposed SOLO achieves substantial memory savings (approximately 45 GB when training a 7B model) with minimal accuracy loss. We hope that SOLO can contribute to overcoming the bottleneck in computational resources, thereby promoting greater accessibility in fundamental research.","authors":["Cong Xu","Wenbin Liang","Mo Yu","Anan Liu","Ke-Yue Zhang","Lizhuang Ma","Jianyong Wang","Jun Wang","Wei Zhang"],"url":"https://arxiv.org/abs/2505.00347"}
{"created":"2025-05-02","title":"Validation of a 24-hour-ahead Prediction model for a Residential Electrical Load under diverse climate","abstract":"Accurate household electrical energy demand prediction is essential for effectively managing sustainable Energy Communities. Integrated with the Energy Management System, these communities aim to optimise operational costs. However, most existing forecasting models are region-specific and depend on large datasets, limiting their applicability across different climates and geographical areas. These models often lack flexibility and may not perform well in regions with limited historical data, leading to inaccurate predictions. This paper proposes a global model for 24-hour-ahead hourly electrical energy demand prediction that is designed to perform effectively across diverse climate conditions and datasets. The model's efficiency is demonstrated using data from two distinct regions: Ireland, with a maritime climate and Vietnam, with a tropical climate. Remarkably, the model achieves high accuracy even with a limited dataset spanning only nine months. Its robustness is further validated across different seasons in Ireland (summer and winter) and Vietnam (dry and wet). The proposed model is evaluated against state-of-the-art machine learning and deep learning methods. Simulation results indicate that the model consistently outperforms benchmark models, showcasing its capability to provide reliable forecasts globally, regardless of varying climatic conditions and data availability. This research underscores the model's potential to enhance the efficiency and sustainability of Energy Communities worldwide. The proposed model achieves a Mean Absolute Percentage Error of 8.0% and 4.0% on the full Irish and Vietnamese datasets.","authors":["Ehtisham Asghar","Martin Hill","Ibrahim Sengor","Conor Lynch","Phan Quang An"],"url":"https://arxiv.org/abs/2505.00348"}
{"created":"2025-05-02","title":"Optimizing Deep Neural Networks using Safety-Guided Self Compression","abstract":"The deployment of deep neural networks on resource-constrained devices necessitates effective model com- pression strategies that judiciously balance the reduction of model size with the preservation of performance. This study introduces a novel safety-driven quantization framework that leverages preservation sets to systematically prune and quantize neural network weights, thereby optimizing model complexity without compromising accuracy. The proposed methodology is rigorously evaluated on both a convolutional neural network (CNN) and an attention-based language model, demonstrating its applicability across diverse architectural paradigms. Experimental results reveal that our framework achieves up to a 2.5% enhancement in test accuracy relative to the original unquantized models while maintaining 60% of the initial model size. In comparison to conventional quantization techniques, our approach not only augments generalization by eliminating parameter noise and retaining essential weights but also reduces variance, thereby ensuring the retention of critical model features. These findings underscore the efficacy of safety-driven quantization as a robust and reliable strategy for the efficient optimization of deep learn- ing models. The implementation and comprehensive experimental evaluations of our framework are publicly accessible at GitHub.","authors":["Mohammad Zbeeb","Mariam Salman","Mohammad Bazzi","Ammar Mohanna"],"url":"https://arxiv.org/abs/2505.00350"}
{"created":"2025-05-02","title":"Integral Representations of Sobolev Spaces via ReLU$^k$ Activation Function and Optimal Error Estimates for Linearized Networks","abstract":"This paper presents two main theoretical results concerning shallow neural networks with ReLU$^k$ activation functions. We establish a novel integral representation for Sobolev spaces, showing that every function in $H^{\\frac{d+2k+1}{2}}(\\Omega)$ can be expressed as an $L^2$-weighted integral of ReLU$^k$ ridge functions over the unit sphere. This result mirrors the known representation of Barron spaces and highlights a fundamental connection between Sobolev regularity and neural network representations. Moreover, we prove that linearized shallow networks -- constructed by fixed inner parameters and optimizing only the linear coefficients -- achieve optimal approximation rates $O(n^{-\\frac{1}{2}-\\frac{2k+1}{2d}})$ in Sobolev spaces.","authors":["Xinliang Liu","Tong Mao","Jinchao Xu"],"url":"https://arxiv.org/abs/2505.00351"}
{"created":"2025-05-02","title":"Multi-segment Soft Robot Control via Deep Koopman-based Model Predictive Control","abstract":"Soft robots, compared to regular rigid robots, as their multiple segments with soft materials bring flexibility and compliance, have the advantages of safe interaction and dexterous operation in the environment. However, due to its characteristics of high dimensional, nonlinearity, time-varying nature, and infinite degree of freedom, it has been challenges in achieving precise and dynamic control such as trajectory tracking and position reaching. To address these challenges, we propose a framework of Deep Koopman-based Model Predictive Control (DK-MPC) for handling multi-segment soft robots. We first employ a deep learning approach with sampling data to approximate the Koopman operator, which therefore linearizes the high-dimensional nonlinear dynamics of the soft robots into a finite-dimensional linear representation. Secondly, this linearized model is utilized within a model predictive control framework to compute optimal control inputs that minimize the tracking error between the desired and actual state trajectories. The real-world experiments on the soft robot \"Chordata\" demonstrate that DK-MPC could achieve high-precision control, showing the potential of DK-MPC for future applications to soft robots.","authors":["Lei Lv","Lei Liu","Lei Bao","Fuchun Sun","Jiahong Dong","Jianwei Zhang","Xuemei Shan","Kai Sun","Hao Huang","Yu Luo"],"url":"https://arxiv.org/abs/2505.00354"}
{"created":"2025-05-02","title":"R&B: Domain Regrouping and Data Mixture Balancing for Efficient Foundation Model Training","abstract":"Data mixing strategies have successfully reduced the costs involved in training language models. While promising, such methods suffer from two flaws. First, they rely on predetermined data domains (e.g., data sources, task types), which may fail to capture critical semantic nuances, leaving performance on the table. Second, these methods scale with the number of domains in a computationally prohibitive way. We address these challenges via R&amp;B, a framework that re-partitions training data based on semantic similarity (Regroup) to create finer-grained domains, and efficiently optimizes the data composition (Balance) by leveraging a Gram matrix induced by domain gradients obtained throughout training. Unlike prior works, it removes the need for additional compute to obtain evaluation information such as losses or gradients. We analyze this technique under standard regularity conditions and provide theoretical insights that justify R&amp;B's effectiveness compared to non-adaptive mixing approaches. Empirically, we demonstrate the effectiveness of R&amp;B on five diverse datasets ranging from natural language to reasoning and multimodal tasks. With as little as 0.01% additional compute overhead, R&amp;B matches or exceeds the performance of state-of-the-art data mixing strategies.","authors":["Albert Ge","Tzu-Heng Huang","John Cooper","Avi Trost","Ziyi Chu","Satya Sai Srinath Namburi GNVV","Ziyang Cai","Kendall Park","Nicholas Roberts","Frederic Sala"],"url":"https://arxiv.org/abs/2505.00358"}
{"created":"2025-05-02","title":"TNStream: Applying Tightest Neighbors to Micro-Clusters to Define Multi-Density Clusters in Streaming Data","abstract":"In data stream clustering, systematic theory of stream clustering algorithms remains relatively scarce. Recently, density-based methods have gained attention. However, existing algorithms struggle to simultaneously handle arbitrarily shaped, multi-density, high-dimensional data while maintaining strong outlier resistance. Clustering quality significantly deteriorates when data density varies complexly. This paper proposes a clustering algorithm based on the novel concept of Tightest Neighbors and introduces a data stream clustering theory based on the Skeleton Set. Based on these theories, this paper develops a new method, TNStream, a fully online algorithm. The algorithm adaptively determines the clustering radius based on local similarity, summarizing the evolution of multi-density data streams in micro-clusters. It then applies a Tightest Neighbors-based clustering algorithm to form final clusters. To improve efficiency in high-dimensional cases, Locality-Sensitive Hashing (LSH) is employed to structure micro-clusters, addressing the challenge of storing k-nearest neighbors. TNStream is evaluated on various synthetic and real-world datasets using different clustering metrics. Experimental results demonstrate its effectiveness in improving clustering quality for multi-density data and validate the proposed data stream clustering theory.","authors":["Qifen Zeng","Haomin Bao","Yuanzhuo Hu","Zirui Zhang","Yuheng Zheng","Luosheng Wen"],"url":"https://arxiv.org/abs/2505.00359"}
{"created":"2025-05-02","title":"From GNNs to Trees: Multi-Granular Interpretability for Graph Neural Networks","abstract":"Interpretable Graph Neural Networks (GNNs) aim to reveal the underlying reasoning behind model predictions, attributing their decisions to specific subgraphs that are informative. However, existing subgraph-based interpretable methods suffer from an overemphasis on local structure, potentially overlooking long-range dependencies within the entire graphs. Although recent efforts that rely on graph coarsening have proven beneficial for global interpretability, they inevitably reduce the graphs to a fixed granularity. Such an inflexible way can only capture graph connectivity at a specific level, whereas real-world graph tasks often exhibit relationships at varying granularities (e.g., relevant interactions in proteins span from functional groups, to amino acids, and up to protein domains). In this paper, we introduce a novel Tree-like Interpretable Framework (TIF) for graph classification, where plain GNNs are transformed into hierarchical trees, with each level featuring coarsened graphs of different granularity as tree nodes. Specifically, TIF iteratively adopts a graph coarsening module to compress original graphs (i.e., root nodes of trees) into increasingly coarser ones (i.e., child nodes of trees), while preserving diversity among tree nodes within different branches through a dedicated graph perturbation module. Finally, we propose an adaptive routing module to identify the most informative root-to-leaf paths, providing not only the final prediction but also the multi-granular interpretability for the decision-making process. Extensive experiments on the graph classification benchmarks with both synthetic and real-world datasets demonstrate the superiority of TIF in interpretability, while also delivering a competitive prediction performance akin to the state-of-the-art counterparts.","authors":["Jie Yang","Yuwen Wang","Kaixuan Chen","Tongya Zheng","Yihe Zhou","Zhenbang Xiao","Ji Cao","Mingli Song","Shunyu Liu"],"url":"https://arxiv.org/abs/2505.00364"}
{"created":"2025-05-02","title":"SacFL: Self-Adaptive Federated Continual Learning for Resource-Constrained End Devices","abstract":"The proliferation of end devices has led to a distributed computing paradigm, wherein on-device machine learning models continuously process diverse data generated by these devices. The dynamic nature of this data, characterized by continuous changes or data drift, poses significant challenges for on-device models. To address this issue, continual learning (CL) is proposed, enabling machine learning models to incrementally update their knowledge and mitigate catastrophic forgetting. However, the traditional centralized approach to CL is unsuitable for end devices due to privacy and data volume concerns. In this context, federated continual learning (FCL) emerges as a promising solution, preserving user data locally while enhancing models through collaborative updates. Aiming at the challenges of limited storage resources for CL, poor autonomy in task shift detection, and difficulty in coping with new adversarial tasks in FCL scenario, we propose a novel FCL framework named SacFL. SacFL employs an Encoder-Decoder architecture to separate task-robust and task-sensitive components, significantly reducing storage demands by retaining lightweight task-sensitive components for resource-constrained end devices. Moreover, $\\rm{SacFL}$ leverages contrastive learning to introduce an autonomous data shift detection mechanism, enabling it to discern whether a new task has emerged and whether it is a benign task. This capability ultimately allows the device to autonomously trigger CL or attack defense strategy without additional information, which is more practical for end devices. Comprehensive experiments conducted on multiple text and image datasets, such as Cifar100 and THUCNews, have validated the effectiveness of $\\rm{SacFL}$ in both class-incremental and domain-incremental scenarios. Furthermore, a demo system has been developed to verify its practicality.","authors":["Zhengyi Zhong","Weidong Bao","Ji Wang","Jianguo Chen","Lingjuan Lyu","Wei Yang Bryan Lim"],"url":"https://arxiv.org/abs/2505.00365"}
{"created":"2025-05-02","title":"KoACD: The First Korean Adolescent Dataset for Cognitive Distortion Analysis","abstract":"Cognitive distortion refers to negative thinking patterns that can lead to mental health issues like depression and anxiety in adolescents. Previous studies using natural language processing (NLP) have focused mainly on small-scale adult datasets, with limited research on adolescents. This study introduces KoACD, the first large-scale dataset of cognitive distortions in Korean adolescents, containing 108,717 instances. We applied a multi-Large Language Model (LLM) negotiation method to refine distortion classification and generate synthetic data using two approaches: cognitive clarification for textual clarity and cognitive balancing for diverse distortion representation. Validation through LLMs and expert evaluations showed that while LLMs classified distortions with explicit markers, they struggled with context-dependent reasoning, where human evaluators demonstrated higher accuracy. KoACD aims to enhance future research on cognitive distortion detection.","authors":["JunSeo Kim","HyeHyeon Kim"],"url":"https://arxiv.org/abs/2505.00367"}
{"created":"2025-05-02","title":"Urban Air Mobility as a System of Systems: An LLM-Enhanced Holonic Approach","abstract":"Urban Air Mobility (UAM) is an emerging System of System (SoS) that faces challenges in system architecture, planning, task management, and execution. Traditional architectural approaches struggle with scalability, adaptability, and seamless resource integration within dynamic and complex environments. This paper presents an intelligent holonic architecture that incorporates Large Language Model (LLM) to manage the complexities of UAM. Holons function semi autonomously, allowing for real time coordination among air taxis, ground transport, and vertiports. LLMs process natural language inputs, generate adaptive plans, and manage disruptions such as weather changes or airspace closures.Through a case study of multimodal transportation with electric scooters and air taxis, we demonstrate how this architecture enables dynamic resource allocation, real time replanning, and autonomous adaptation without centralized control, creating more resilient and efficient urban transportation networks. By advancing decentralized control and AI driven adaptability, this work lays the groundwork for resilient, human centric UAM ecosystems, with future efforts targeting hybrid AI integration and real world validation.","authors":["Ahmed R. Sadik","Muhammad Ashfaq","Niko M\\\"akitalo","Tommi Mikkonen"],"url":"https://arxiv.org/abs/2505.00368"}
{"created":"2025-05-02","title":"Automated segmenta-on of pediatric neuroblastoma on multi-modal MRI: Results of the SPPIN challenge at MICCAI 2023","abstract":"Surgery plays an important role within the treatment for neuroblastoma, a common pediatric cancer. This requires careful planning, often via magnetic resonance imaging (MRI)-based anatomical 3D models. However, creating these models is often time-consuming and user dependent. We organized the Surgical Planning in Pediatric Neuroblastoma (SPPIN) challenge, to stimulate developments on this topic, and set a benchmark for fully automatic segmentation of neuroblastoma on multi-model MRI. The challenge started with a training phase, where teams received 78 sets of MRI scans from 34 patients, consisting of both diagnostic and post-chemotherapy MRI scans. The final test phase, consisting of 18 MRI sets from 9 patients, determined the ranking of the teams. Ranking was based on the Dice similarity coefficient (Dice score), the 95th percentile of the Hausdorff distance (HD95) and the volumetric similarity (VS). The SPPIN challenge was hosted at MICCAI 2023. The final leaderboard consisted of 9 teams. The highest-ranking team achieved a median Dice score 0.82, a median HD95 of 7.69 mm and a VS of 0.91, utilizing a large, pretrained network called STU-Net. A significant difference for the segmentation results between diagnostic and post-chemotherapy MRI scans was observed (Dice = 0.89 vs Dice = 0.59, P = 0.01) for the highest-ranking team. SPPIN is the first medical segmentation challenge in extracranial pediatric oncology. The highest-ranking team used a large pre-trained network, suggesting that pretraining can be of use in small, heterogenous datasets. Although the results of the highest-ranking team were high for most patients, segmentation especially in small, pre-treated tumors were insufficient. Therefore, more reliable segmentation methods are needed to create clinically applicable models to aid surgical planning in pediatric neuroblastoma.","authors":["M. A. D. Buser","D. C. Simons","M. Fitski","M. H. W. A. Wijnen","A. S. Littooij","A. H. ter Brugge","I. N. Vos","M. H. A. Janse","M. de Boer","R. ter Maat","J. Sato","S. Kido","S. Kondo","S. Kasai","M. Wodzinski","H. Muller","J. Ye","J. He","Y. Kirchhoff","M. R. Rokkus","G. Haokai","S. Zitong","M. Fern\\'andez-Pat\\'on","D. Veiga-Canuto","D. G. Ellis","M. R. Aizenberg","B. H. M. van der Velden","H. Kuijf","A. De Luca","A. F. W. van der Steeg"],"url":"https://arxiv.org/abs/2505.00369"}
{"created":"2025-05-02","title":"On the Schr\\\"odingerization method for linear non-unitary dynamics with optimal dependence on matrix queries","abstract":"The Schr\\\"odingerization method converts linear partial and ordinary differential equations with non-unitary dynamics into systems of Schr\\\"odinger-type equations with unitary evolution. It does so via the so-called warped phase transformation that maps the original equation into a Schr\\\"odinger-type equation in one higher dimension \\cite{Schrshort,JLY22SchrLong}. We show that by employing a smooth initialization of the warped phase transform \\cite{JLM24SchrBackward}, Schr\\\"odingerization can in fact achieve optimal scaling in matrix queries. This paper presents the detailed implementation of three smooth initializations for the Schr\\\"odingerization method: (a) the cut-off function, (b) the higher-order polynomial interpolation, and (c) the Fourier transform methods, that achieve optimality for (a) and near-optimality for (b) and (c). A detailed analysis of key parameters affecting time complexity is conducted.","authors":["Shi Jin","Nana Liu","Chuwen Ma","Yue Yu"],"url":"https://arxiv.org/abs/2505.00370"}
{"created":"2025-05-02","title":"Learning to Estimate Package Delivery Time in Mixed Imbalanced Delivery and Pickup Logistics Services","abstract":"Accurately estimating package delivery time is essential to the logistics industry, which enables reasonable work allocation and on-time service guarantee. This becomes even more necessary in mixed logistics scenarios where couriers handle a high volume of delivery and a smaller number of pickup simultaneously. However, most of the related works treat the pickup and delivery patterns on couriers' decision behavior equally, neglecting that the pickup has a greater impact on couriers' decision-making compared to the delivery due to its tighter time constraints. In such context, we have three main challenges: 1) multiple spatiotemporal factors are intricately interconnected, significantly affecting couriers' delivery behavior; 2) pickups have stricter time requirements but are limited in number, making it challenging to model their effects on couriers' delivery process; 3) couriers' spatial mobility patterns are critical determinants of their delivery behavior, but have been insufficiently explored. To deal with these, we propose TransPDT, a Transformer-based multi-task package delivery time prediction model. We first employ the Transformer encoder architecture to capture the spatio-temporal dependencies of couriers' historical travel routes and pending package sets. Then we design the pattern memory to learn the patterns of pickup in the imbalanced dataset via attention mechanism. We also set the route prediction as an auxiliary task of delivery time prediction, and incorporate the prior courier spatial movement regularities in prediction. Extensive experiments on real industry-scale datasets demonstrate the superiority of our method. A system based on TransPDT is deployed internally in JD Logistics to track more than 2000 couriers handling hundreds of thousands of packages per day in Beijing.","authors":["Jinhui Yi","Huan Yan","Haotian Wang","Jian Yuan","Yong Li"],"url":"https://arxiv.org/abs/2505.00375"}
{"created":"2025-05-02","title":"Cues3D: Unleashing the Power of Sole NeRF for Consistent and Unique Instances in Open-Vocabulary 3D Panoptic Segmentation","abstract":"Open-vocabulary 3D panoptic segmentation has recently emerged as a significant trend. Top-performing methods currently integrate 2D segmentation with geometry-aware 3D primitives. However, the advantage would be lost without high-fidelity 3D point clouds, such as methods based on Neural Radiance Field (NeRF). These methods are limited by the insufficient capacity to maintain consistency across partial observations. To address this, recent works have utilized contrastive loss or cross-view association pre-processing for view consensus. In contrast to them, we present Cues3D, a compact approach that relies solely on NeRF instead of pre-associations. The core idea is that NeRF's implicit 3D field inherently establishes a globally consistent geometry, enabling effective object distinction without explicit cross-view supervision. We propose a three-phase training framework for NeRF, initialization-disambiguation-refinement, whereby the instance IDs are corrected using the initially-learned knowledge. Additionally, an instance disambiguation method is proposed to match NeRF-rendered 3D masks and ensure globally unique 3D instance identities. With the aid of Cues3D, we obtain highly consistent and unique 3D instance ID for each object across views with a balanced version of NeRF. Our experiments are conducted on ScanNet v2, ScanNet200, ScanNet++, and Replica datasets for 3D instance, panoptic, and semantic segmentation tasks. Cues3D outperforms other 2D image-based methods and competes with the latest 2D-3D merging based methods, while even surpassing them when using additional 3D point clouds. The code link could be found in the appendix and will be released on \\href{https://github.com/mRobotit/Cues3D}{github}","authors":["Feng Xue","Wenzhuang Xu","Guofeng Zhong","Anlong Minga","Nicu Sebe"],"url":"https://arxiv.org/abs/2505.00378"}
{"created":"2025-05-02","title":"The Invisible Threat: Evaluating the Vulnerability of Cross-Spectral Face Recognition to Presentation Attacks","abstract":"Cross-spectral face recognition systems are designed to enhance the performance of facial recognition systems by enabling cross-modal matching under challenging operational conditions. A particularly relevant application is the matching of near-infrared (NIR) images to visible-spectrum (VIS) images, enabling the verification of individuals by comparing NIR facial captures acquired with VIS reference images. The use of NIR imaging offers several advantages, including greater robustness to illumination variations, better visibility through glasses and glare, and greater resistance to presentation attacks. Despite these claimed benefits, the robustness of NIR-based systems against presentation attacks has not been systematically studied in the literature. In this work, we conduct a comprehensive evaluation into the vulnerability of NIR-VIS cross-spectral face recognition systems to presentation attacks. Our empirical findings indicate that, although these systems exhibit a certain degree of reliability, they remain vulnerable to specific attacks, emphasizing the need for further research in this area.","authors":["Anjith George","Sebastien Marcel"],"url":"https://arxiv.org/abs/2505.00380"}
{"created":"2025-05-02","title":"Approximation to Deep Q-Network by Stochastic Delay Differential Equations","abstract":"Despite the significant breakthroughs that the Deep Q-Network (DQN) has brought to reinforcement learning, its theoretical analysis remains limited. In this paper, we construct a stochastic differential delay equation (SDDE) based on the DQN algorithm and estimate the Wasserstein-1 distance between them. We provide an upper bound for the distance and prove that the distance between the two converges to zero as the step size approaches zero. This result allows us to understand DQN's two key techniques, the experience replay and the target network, from the perspective of continuous systems. Specifically, the delay term in the equation, corresponding to the target network, contributes to the stability of the system. Our approach leverages a refined Lindeberg principle and an operator comparison to establish these results.","authors":["Jianya Lu","Yingjun Mo"],"url":"https://arxiv.org/abs/2505.00382"}
{"created":"2025-05-02","title":"Improving the scalability of a high-order atmospheric dynamics solver based on the deal.II library","abstract":"We present recent advances on the massively parallel performance of a numerical scheme for atmosphere dynamics applications based on the deal.II library. The implicit-explicit discontinuous finite element scheme is based on a matrix-free approach, meaning that no global sparse matrix is built and only the action of the linear operators on a vector is actually implemented. Following a profiling analysis, we focus on the performance optimization of the numerical method and describe the impact of different preconditioning and solving techniques in this framework. Moreover, we show how the use of the latest version of the \\texttt{deal.II} library and of suitable execution flags can improve the parallel performance.","authors":["Giuseppe Orlando","Tommaso Benacchio","Luca Bonaventura"],"url":"https://arxiv.org/abs/2505.00384"}
{"created":"2025-05-02","title":"CSE-SFP: Enabling Unsupervised Sentence Representation Learning via a Single Forward Pass","abstract":"As a fundamental task in Information Retrieval and Computational Linguistics, sentence representation has profound implications for a wide range of practical applications such as text clustering, content analysis, question-answering systems, and web search. Recent advances in pre-trained language models (PLMs) have driven remarkable progress in this field, particularly through unsupervised embedding derivation methods centered on discriminative PLMs like BERT. However, due to time and computational constraints, few efforts have attempted to integrate unsupervised sentence representation with generative PLMs, which typically possess much larger parameter sizes. Given that state-of-the-art models in both academia and industry are predominantly based on generative architectures, there is a pressing need for an efficient unsupervised text representation framework tailored to decoder-only PLMs. To address this concern, we propose CSE-SFP, an innovative method that exploits the structural characteristics of generative models. Compared to existing strategies, CSE-SFP requires only a single forward pass to perform effective unsupervised contrastive learning. Rigorous experimentation demonstrates that CSE-SFP not only produces higher-quality embeddings but also significantly reduces both training time and memory consumption. Furthermore, we introduce two ratio metrics that jointly assess alignment and uniformity, thereby providing a more robust means for evaluating the semantic spatial properties of encoding models.","authors":["Bowen Zhang","Zixin Song","Chunping Li"],"url":"https://arxiv.org/abs/2505.00389"}
{"created":"2025-05-02","title":"S3AND: Efficient Subgraph Similarity Search Under Aggregated Neighbor Difference Semantics (Technical Report)","abstract":"For the past decades, the \\textit{subgraph similarity search} over a large-scale data graph has become increasingly important and crucial in many real-world applications, such as social network analysis, bioinformatics network analytics, knowledge graph discovery, and many others. While previous works on subgraph similarity search used various graph similarity metrics such as the graph isomorphism, graph edit distance, and so on, in this paper, we propose a novel problem, namely \\textit{subgraph similarity search under aggregated neighbor difference semantics} (S$^3$AND), which identifies subgraphs $g$ in a data graph $G$ that are similar to a given query graph $q$ by considering both keywords and graph structures (under new keyword/structural matching semantics). To efficiently tackle the S$^3$AND problem, we design two effective pruning methods, \\textit{keyword set} and \\textit{aggregated neighbor difference lower bound pruning}, which rule out false alarms of candidate vertices/subgraphs to reduce the S$^3$AND search space. Furthermore, we construct an effective indexing mechanism to facilitate our proposed efficient S$^3$AND query answering algorithm. Through extensive experiments, we demonstrate the effectiveness and efficiency of our S$^3$AND approach over both real and synthetic graphs under various parameter settings.","authors":["Qi Wen","Yutong Ye","Xiang Lian","Mingsong Chen"],"url":"https://arxiv.org/abs/2505.00393"}
{"created":"2025-05-02","title":"SOTA: Spike-Navigated Optimal TrAnsport Saliency Region Detection in Composite-bias Videos","abstract":"Existing saliency detection methods struggle in real-world scenarios due to motion blur and occlusions. In contrast, spike cameras, with their high temporal resolution, significantly enhance visual saliency maps. However, the composite noise inherent to spike camera imaging introduces discontinuities in saliency detection. Low-quality samples further distort model predictions, leading to saliency bias. To address these challenges, we propose Spike-navigated Optimal TrAnsport Saliency Region Detection (SOTA), a framework that leverages the strengths of spike cameras while mitigating biases in both spatial and temporal dimensions. Our method introduces Spike-based Micro-debias (SM) to capture subtle frame-to-frame variations and preserve critical details, even under minimal scene or lighting changes. Additionally, Spike-based Global-debias (SG) refines predictions by reducing inconsistencies across diverse conditions. Extensive experiments on real and synthetic datasets demonstrate that SOTA outperforms existing methods by eliminating composite noise bias. Our code and dataset will be released at https://github.com/lwxfight/sota.","authors":["Wenxuan Liu","Yao Deng","Kang Chen","Xian Zhong","Zhaofei Yu","Tiejun Huang"],"url":"https://arxiv.org/abs/2505.00394"}
{"created":"2025-05-02","title":"GAN-based Generator of Adversarial Attack on Intelligent End-to-End Autoencoder-based Communication System","abstract":"Deep neural networks have been applied in wireless communications system to intelligently adapt to dynamically changing channel conditions, while the users are still under the threat of the malicious attacks due to the broadcasting property of wireless channels. However, most attack models require the knowledge of the target details, which is difficult to be implemented in real systems. Our objective is to develop an attack model with no requirement for the target information, while enhancing the block error rate. In our design, we propose a novel Generative Adversarial Networks(GANs) based attack architecture, which exploits the property of deep learning models being vulnerable to perturbations induced by dynamically changing channel conditions. In the proposed generator, the attack network is composed of convolution layer, convolution transpose layer and linear layer. Then we present the training strategy and the details of the training algorithm. Subsequently, we propose the validation strategy to evaluate the performance of the generator. Simulations are conducted and the results show that our proposed adversarial attack generator achieve better block error rate attack performance than that of benchmark schemes over Additive White Gaussian Noise (AWGN) channel, Rayleigh channel and High-Speed Railway channel.","authors":["Jianyuan Chen","Lin Zhang","Zuwei Chen","Yawen Chen","Hongcheng Zhuang"],"url":"https://arxiv.org/abs/2505.00395"}
{"created":"2025-05-02","title":"Safety in the Face of Adversity: Achieving Zero Constraint Violation in Online Learning with Slowly Changing Constraints","abstract":"We present the first theoretical guarantees for zero constraint violation in Online Convex Optimization (OCO) across all rounds, addressing dynamic constraint changes. Unlike existing approaches in constrained OCO, which allow for occasional safety breaches, we provide the first approach for maintaining strict safety under the assumption of gradually evolving constraints, namely the constraints change at most by a small amount between consecutive rounds. This is achieved through a primal-dual approach and Online Gradient Ascent in the dual space. We show that employing a dichotomous learning rate enables ensuring both safety, via zero constraint violation, and sublinear regret. Our framework marks a departure from previous work by providing the first provable guarantees for maintaining absolute safety in the face of changing constraints in OCO.","authors":["Bassel Hamoud","Ilnura Usmanova","Kfir Y. Levy"],"url":"https://arxiv.org/abs/2505.00398"}
{"created":"2025-05-02","title":"Holistic Optimization of Modular Robots","abstract":"Modular robots have the potential to revolutionize automation as one can optimize their composition for any given task. However, finding optimal compositions is non-trivial. In addition, different compositions require different base positions and trajectories to fully use the potential of modular robots. We address this problem holistically for the first time by jointly optimizing the composition, base placement, and trajectory, to minimize the cycle time of a given task. Our approach is evaluated on over 300 industrial benchmarks requiring point-to-point movements. Overall, we reduce cycle time by up to 25% and find feasible solutions in twice as many benchmarks compared to optimizing the module composition alone. In the first real-world validation of modular robots optimized for point-to-point movement, we find that the optimized robot is successfully deployed in nine out of ten cases in less than an hour.","authors":["Matthias Mayer","Matthias Althoff"],"url":"https://arxiv.org/abs/2505.00400"}
{"created":"2025-05-02","title":"DeepSTA: A Spatial-Temporal Attention Network for Logistics Delivery Timely Rate Prediction in Anomaly Conditions","abstract":"Prediction of couriers' delivery timely rates in advance is essential to the logistics industry, enabling companies to take preemptive measures to ensure the normal operation of delivery services. This becomes even more critical during anomaly conditions like the epidemic outbreak, during which couriers' delivery timely rate will decline markedly and fluctuates significantly. Existing studies pay less attention to the logistics scenario. Moreover, many works focusing on prediction tasks in anomaly scenarios fail to explicitly model abnormal events, e.g., treating external factors equally with other features, resulting in great information loss. Further, since some anomalous events occur infrequently, traditional data-driven methods perform poorly in these scenarios. To deal with them, we propose a deep spatial-temporal attention model, named DeepSTA. To be specific, to avoid information loss, we design an anomaly spatio-temporal learning module that employs a recurrent neural network to model incident information. Additionally, we utilize Node2vec to model correlations between road districts, and adopt graph neural networks and long short-term memory to capture the spatial-temporal dependencies of couriers. To tackle the issue of insufficient training data in abnormal circumstances, we propose an anomaly pattern attention module that adopts a memory network for couriers' anomaly feature patterns storage via attention mechanisms. The experiments on real-world logistics datasets during the COVID-19 outbreak in 2022 show the model outperforms the best baselines by 12.11% in MAE and 13.71% in MSE, demonstrating its superior performance over multiple competitive baselines.","authors":["Jinhui Yi","Huan Yan","Haotian Wang","Jian Yuan","Yong Li"],"url":"https://arxiv.org/abs/2505.00402"}
{"created":"2025-05-02","title":"iMacSR: Intermediate Multi-Access Supervision and Regularization in Training Autonomous Driving Models","abstract":"Deep Learning (DL)-based street scene semantic understanding has become a cornerstone of autonomous driving (AD). DL model performance heavily relies on network depth. Specifically, deeper DL architectures yield better segmentation performance. However, as models grow deeper, traditional one-point supervision at the final layer struggles to optimize intermediate feature representations, leading to subpar training outcomes. To address this, we propose an intermediate Multi-access Supervision and Regularization (iMacSR) strategy. The proposed iMacSR introduces two novel components: (I) mutual information between latent features and ground truth as intermediate supervision loss ensures robust feature alignment at multiple network depths; and (II) negative entropy regularization on hidden features discourages overconfident predictions and mitigates overfitting. These intermediate terms are combined into the original final-layer training loss to form a unified optimization objective, enabling comprehensive optimization across the network hierarchy. The proposed iMacSR provides a robust framework for training deep AD architectures, advancing the performance of perception systems in real-world driving scenarios. In addition, we conduct theoretical convergence analysis for the proposed iMacSR. Extensive experiments on AD benchmarks (i.e., Cityscapes, CamVid, and SynthiaSF datasets) demonstrate that iMacSR outperforms conventional final-layer single-point supervision method up to 9.19% in mean Intersection over Union (mIoU).","authors":["Wei-Bin Kou","Guangxu Zhu","Yichen Jin","Shuai Wang","Ming Tang","Yik-Chung Wu"],"url":"https://arxiv.org/abs/2505.00404"}
{"created":"2025-05-02","title":"Selling Information in Games with Externalities","abstract":"A competitive market is modeled as a game of incomplete information. One player observes some payoff-relevant state and can sell (possibly noisy) messages thereof to the other, whose willingness to pay is contingent on their own beliefs. We frame the decision of what information to sell, and at what price, as a product versioning problem. The optimal menu screens buyer types to maximize profit, which is the payment minus the externality induced by selling information to a competitor, that is, the cost of refining a competitor's beliefs. For a class of games with binary actions and states, we derive the following insights: (i) payments are necessary to provide incentives for information sharing amongst competing firms; (ii) the optimal menu benefits both the buyer and the seller; (iii) the seller cannot steer the buyer's actions at the expense of social welfare; (iv) as such, as competition grows fiercer it can be optimal to sell no information at all.","authors":["Thomas Falconer","Anubhav Ratha","Jalal Kazempour","Pierre Pinson","Maryam Kamgarpour"],"url":"https://arxiv.org/abs/2505.00405"}
{"created":"2025-05-02","title":"Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis","abstract":"The present research tackles the difficulty of predicting osteoporosis risk via machine learning (ML) approaches, emphasizing the use of explainable artificial intelligence (XAI) to improve model transparency. Osteoporosis is a significant public health concern, sometimes remaining untreated owing to its asymptomatic characteristics, and early identification is essential to avert fractures. The research assesses six machine learning classifiers: Random Forest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting and utilizes a dataset based on clinical, demographic, and lifestyle variables. The models are refined using GridSearchCV to calibrate hyperparameters, with the objective of enhancing predictive efficacy. XGBoost had the greatest accuracy (91%) among the evaluated models, surpassing others in precision (0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI approaches, such as SHAP, LIME, and Permutation Feature Importance, to elucidate the decision-making process of the optimal model. The study indicates that age is the primary determinant in forecasting osteoporosis risk, followed by hormonal alterations and familial history. These results corroborate clinical knowledge and affirm the models' therapeutic significance. The research underscores the significance of explainability in machine learning models for healthcare applications, guaranteeing that physicians can rely on the system's predictions. The report ultimately proposes directions for further research, such as validation across varied populations and the integration of supplementary biomarkers for enhanced predictive accuracy.","authors":["Farhana Elias","Md Shihab Reza","Muhammad Zawad Mahmud","Samiha Islam"],"url":"https://arxiv.org/abs/2505.00410"}
{"created":"2025-05-02","title":"Affine matrix scrambling achieves smoothness-dependent convergence rates","abstract":"We study the convergence rate of the median estimator for affine matrix scrambled digital nets applied to integrands over the unit hypercube $[0, 1]^s$. By taking the median of $(2r-1)$ independent randomized quasi-Monte Carlo (RQMC) samples, we demonstrate that the desired convergence rates can be achieved without increasing the number of randomizations $r$ as the quadrature size $N$ grows for both bounded and unbounded integrands. For unbounded integrands, our analysis assumes a boundary growth condition on the weak derivatives and also considers singularities such as kinks and jump discontinuities. Notably, when $r = 1$, the median estimator reduces to the standard RQMC estimator. By applying analytical techniques developed for median estimators, we prove that the affine matrix scrambled estimator achieves a convergence rate depending on the integrand's smoothness, and is therefore not limited by the canonical rate $\\mathcal{O}(N^{-3/2})$. However, this smoothness-dependent theoretical rate is not observed empirically in numerical experiments when the affine matrix scrambling yields a heavy-tailed sampling distribution. In contrast, the median estimator consistently reveals the theoretical rates and yields smaller integration errors than mean estimators, further highlighting its advantages.","authors":["Yang Liu"],"url":"https://arxiv.org/abs/2505.00411"}
{"created":"2025-05-02","title":"CICADA: Cross-Domain Interpretable Coding for Anomaly Detection and Adaptation in Multivariate Time Series","abstract":"Unsupervised Time series anomaly detection plays a crucial role in applications across industries. However, existing methods face significant challenges due to data distributional shifts across different domains, which are exacerbated by the non-stationarity of time series over time. Existing models fail to generalize under multiple heterogeneous source domains and emerging unseen new target domains. To fill the research gap, we introduce CICADA (Cross-domain Interpretable Coding for Anomaly Detection and Adaptation), with four key innovations: (1) a mixture of experts (MOE) framework that captures domain-agnostic anomaly features with high flexibility and interpretability; (2) a novel selective meta-learning mechanism to prevent negative transfer between dissimilar domains, (3) an adaptive expansion algorithm for emerging heterogeneous domain expansion, and (4) a hierarchical attention structure that quantifies expert contributions during fusion to enhance interpretability further.Extensive experiments on synthetic and real-world industrial datasets demonstrate that CICADA outperforms state-of-the-art methods in both cross-domain detection performance and interpretability.","authors":["Tian Lan","Yifei Gao","Yimeng Lu","Chen Zhang"],"url":"https://arxiv.org/abs/2505.00415"}
{"created":"2025-05-02","title":"ScaleTrack: Scaling and back-tracking Automated GUI Agents","abstract":"Automated GUI agents aims to facilitate user interaction by automatically performing complex tasks in digital environments, such as web, mobile, desktop devices. It receives textual task instruction and GUI description to generate executable actions (\\emph{e.g.}, click) and operation boxes step by step. Training a GUI agent mainly involves grounding and planning stages, in which the GUI grounding focuses on finding the execution coordinates according to the task, while the planning stage aims to predict the next action based on historical actions. However, previous work suffers from the limitations of insufficient training data for GUI grounding, as well as the ignorance of backtracking historical behaviors for GUI planning. To handle the above challenges, we propose ScaleTrack, a training framework by scaling grounding and backtracking planning for automated GUI agents. We carefully collected GUI samples of different synthesis criterions from a wide range of sources, and unified them into the same template for training GUI grounding models. Moreover, we design a novel training strategy that predicts the next action from the current GUI image, while also backtracking the historical actions that led to the GUI image. In this way, ScaleTrack explains the correspondence between GUI images and actions, which effectively describes the evolution rules of the GUI environment. Extensive experimental results demonstrate the effectiveness of ScaleTrack. Data and code will be available at url.","authors":["Jing Huang","Zhixiong Zeng","Wenkang Han","Yufeng Zhong","Liming Zheng","Shuai Fu","Jingyuan Chen","Lin Ma"],"url":"https://arxiv.org/abs/2505.00416"}
{"created":"2025-05-02","title":"Real-Time Animatable 2DGS-Avatars with Detail Enhancement from Monocular Videos","abstract":"High-quality, animatable 3D human avatar reconstruction from monocular videos offers significant potential for reducing reliance on complex hardware, making it highly practical for applications in game development, augmented reality, and social media. However, existing methods still face substantial challenges in capturing fine geometric details and maintaining animation stability, particularly under dynamic or complex poses. To address these issues, we propose a novel real-time framework for animatable human avatar reconstruction based on 2D Gaussian Splatting (2DGS). By leveraging 2DGS and global SMPL pose parameters, our framework not only aligns positional and rotational discrepancies but also enables robust and natural pose-driven animation of the reconstructed avatars. Furthermore, we introduce a Rotation Compensation Network (RCN) that learns rotation residuals by integrating local geometric features with global pose parameters. This network significantly improves the handling of non-rigid deformations and ensures smooth, artifact-free pose transitions during animation. Experimental results demonstrate that our method successfully reconstructs realistic and highly animatable human avatars from monocular videos, effectively preserving fine-grained details while ensuring stable and natural pose variation. Our approach surpasses current state-of-the-art methods in both reconstruction quality and animation robustness on public benchmarks.","authors":["Xia Yuan","Hai Yuan","Wenyi Ge","Ying Fu","Xi Wu","Guanyu Xing"],"url":"https://arxiv.org/abs/2505.00421"}
{"created":"2025-05-02","title":"Toward Automated Regulatory Decision-Making: Trustworthy Medical Device Risk Classification with Multimodal Transformers and Self-Training","abstract":"Accurate classification of medical device risk levels is essential for regulatory oversight and clinical safety. We present a Transformer-based multimodal framework that integrates textual descriptions and visual information to predict device regulatory classification. The model incorporates a cross-attention mechanism to capture intermodal dependencies and employs a self-training strategy for improved generalization under limited supervision. Experiments on a real-world regulatory dataset demonstrate that our approach achieves up to 90.4% accuracy and 97.9% AUROC, significantly outperforming text-only (77.2%) and image-only (54.8%) baselines. Compared to standard multimodal fusion, the self-training mechanism improved SVM performance by 3.3 percentage points in accuracy (from 87.1% to 90.4%) and 1.4 points in macro-F1, suggesting that pseudo-labeling can effectively enhance generalization under limited supervision. Ablation studies further confirm the complementary benefits of both cross-modal attention and self-training.","authors":["Yu Han","Aaron Ceross","Jeroen H. M. Bergmann"],"url":"https://arxiv.org/abs/2505.00422"}
{"created":"2025-05-02","title":"Leveraging Pretrained Diffusion Models for Zero-Shot Part Assembly","abstract":"3D part assembly aims to understand part relationships and predict their 6-DoF poses to construct realistic 3D shapes, addressing the growing demand for autonomous assembly, which is crucial for robots. Existing methods mainly estimate the transformation of each part by training neural networks under supervision, which requires a substantial quantity of manually labeled data. However, the high cost of data collection and the immense variability of real-world shapes and parts make traditional methods impractical for large-scale applications. In this paper, we propose first a zero-shot part assembly method that utilizes pre-trained point cloud diffusion models as discriminators in the assembly process, guiding the manipulation of parts to form realistic shapes. Specifically, we theoretically demonstrate that utilizing a diffusion model for zero-shot part assembly can be transformed into an Iterative Closest Point (ICP) process. Then, we propose a novel pushing-away strategy to address the overlap parts, thereby further enhancing the robustness of the method. To verify our work, we conduct extensive experiments and quantitative comparisons to several strong baseline methods, demonstrating the effectiveness of the proposed approach, which even surpasses the supervised learning method. The code has been released on https://github.com/Ruiyuan-Zhang/Zero-Shot-Assembly.","authors":["Ruiyuan Zhang","Qi Wang","Jiaxiang Liu","Yu Zhang","Yuchi Huo","Chao Wu"],"url":"https://arxiv.org/abs/2505.00426"}
{"created":"2025-05-02","title":"A Neural Network Mode for PX4 on Embedded Flight Controllers","abstract":"This paper contributes an open-sourced implementation of a neural-network based controller framework within the PX4 stack. We develop a custom module for inference on the microcontroller while retaining all of the functionality of the PX4 autopilot. Policies trained in the Aerial Gym Simulator are converted to the TensorFlow Lite format and then built together with PX4 and flashed to the flight controller. The policies substitute the control-cascade within PX4 to offer an end-to-end position-setpoint tracking controller directly providing normalized motor RPM setpoints. Experiments conducted in simulation and the real-world show similar tracking performance. We thus provide a flight-ready pipeline for testing neural control policies in the real world. The pipeline simplifies the deployment of neural networks on embedded flight controller hardware thereby accelerating research on learning-based control. Both the Aerial Gym Simulator and the PX4 module are open-sourced at https://github.com/ntnu-arl/aerial_gym_simulator and https://github.com/SindreMHegre/PX4-Autopilot-public/tree/for_paper. Video: https://youtu.be/lY1OKz_UOqM?si=VtzL243BAY3lblTJ.","authors":["Sindre M. Hegre","Welf Rehberg","Mihir Kulkarni","Kostas Alexis"],"url":"https://arxiv.org/abs/2505.00432"}
{"created":"2025-05-02","title":"Stability of the first-order unified gas-kinetic scheme based on a linear kinetic model","abstract":"The unified gas-kinetic scheme (UGKS) is becoming increasingly popular for multiscale simulations in all flow regimes. This paper provides the first analytical study on the stability of the UGKS applied to a linear kinetic model, which is able to reproduce the one-dimensional linear scalar advection-diffusion equation via the Chapman-Enskog expansion method. Adopting periodic boundary conditions and neglecting the error from numerical integration, this paper rigorously proves the weighted $L^2$-stability of the first-order UGKS under the Courant-Friedrichs-Lewy (CFL) conditions. It is shown that the time step of the method is not constrained by being less than the particle collision time, nor is it limited by parabolic type CFL conditions typically applied in solving diffusion equations. The novelty of the proof lies in that based on the ratio of the time step to the particle collision time, the update of distribution functions is viewed as a convex combinations of sub-methods related to various physics processes, such as the particle free transport and collisions. The weighted $L^2$-stability of the sub-methods is obtained by considering them as discretizations to corresponding linear hyperbolic systems and utilizing the associated Riemann invariants. Finally, the strong stability preserving property of the UGKS leads to the desired weighted $L^2$-stability.","authors":["Tuowei Chen","Kun Xu"],"url":"https://arxiv.org/abs/2505.00434"}
{"created":"2025-05-02","title":"Per-Domain Generalizing Policies: On Validation Instances and Scaling Behavior","abstract":"Recent work has shown that successful per-domain generalizing action policies can be learned. Scaling behavior, from small training instances to large test instances, is the key objective; and the use of validation instances larger than training instances is one key to achieve it. Prior work has used fixed validation sets. Here, we introduce a method generating the validation set dynamically, on the fly, increasing instance size so long as informative and feasible.We also introduce refined methodology for evaluating scaling behavior, generating test instances systematically to guarantee a given confidence in coverage performance for each instance size. In experiments, dynamic validation improves scaling behavior of GNN policies in all 9 domains used.","authors":["Timo P. Gros","Nicola J. M\\\"uller","Daniel Fiser","Isabel Valera","Verena Wolf","J\\\"org Hoffmann"],"url":"https://arxiv.org/abs/2505.00439"}
{"created":"2025-05-02","title":"Error bounds for function approximation using generated sets","abstract":"This paper explores the use of \"generated sets\" $\\{ \\{ k \\boldsymbol{\\zeta} \\} : k = 1, \\ldots, n \\}$ for function approximation in reproducing kernel Hilbert spaces which consist of multi-dimensional functions with an absolutely convergent Fourier series. The algorithm is a least squares algorithm that samples the function at the points of a generated set. We show that there exist $\\boldsymbol{\\zeta} \\in [0,1]^d$ for which the worst-case $L_2$ error has the optimal order of convergence if the space has polynomially converging approximation numbers. In fact, this holds for a significant portion of the generators. Additionally we show that a restriction to rational generators is possible with a slight increase of the bound. Furthermore, we specialise the results to the weighted Korobov space, where we derive a bound applicable to low values of sample points, and state tractability results.","authors":["Ronald Cools","Dirk Nuyens","Laurence Wilkes"],"url":"https://arxiv.org/abs/2505.00440"}
{"created":"2025-05-02","title":"Decentralised, Self-Organising Drone Swarms using Coupled Oscillators","abstract":"The problem of robotic synchronisation and coordination is a long-standing one. Combining autonomous, computerised systems with unpredictable real-world conditions can have consequences ranging from poor performance to collisions and damage. This paper proposes using coupled oscillators to create a drone swarm that is decentralised and self organising. This allows for greater flexibility and adaptiveness than a hard-coded swarm, with more resilience and scalability than a centralised system. Our method allows for a variable number of drones to spontaneously form a swarm and react to changing swarm conditions. Additionally, this method includes provisions to prevent communication interference between drones, and signal processing techniques to ensure a smooth and cohesive swarm.","authors":["Kevin Quinn","Cormac Molloy","Harun \\v{S}iljak"],"url":"https://arxiv.org/abs/2505.00442"}
{"created":"2025-05-02","title":"Distributed Retrieval-Augmented Generation","abstract":"As large language models (LLMs) become increasingly adopted on edge devices, Retrieval-Augmented Generation (RAG) is gaining prominence as a solution to address factual deficiencies and hallucinations by integrating external knowledge. However, centralized RAG architectures face significant challenges in data privacy and scalability. For instance, smart healthcare services often rely on collecting sensitive patient data and building a centralized knowledge base to provide better diagnosis and treatment advice, while privacy concerns significantly impede this process. Besides, maintaining a comprehensive and continuously updated knowledge base is costly, particularly in response to regional epidemics and rapidly mutating viruses. To address these challenges, this paper introduces Distributed Retrieval-Augmented Generation (DRAG), a novel framework that improves data privacy by eliminating the need for a centralized knowledge base and restoring data control to owners. DRAG incorporates a Topic-Aware Random Walk (TARW) algorithm that leverages LLMs to extract query topics and facilitate targeted peer discovery within a peer-to-peer network, enabling efficient knowledge retrieval in decentralized environments. Extensive experiments across three diverse datasets and LLMs demonstrate that DRAG with TARW achieves near-centralized RAG performance by using half as many messages as flooding. The code is available at https://github.com/xuchenhao001/DRAG.","authors":["Chenhao Xu","Longxiang Gao","Yuan Miao","Xi Zheng"],"url":"https://arxiv.org/abs/2505.00443"}
{"created":"2025-05-02","title":"Deterministic Scheduling over Wi-Fi 6 using Target Wake Time: An Experimental Approach","abstract":"Wi-Fi networks traditionally use Distributed Coordination Function (DCF) that employs CSMA/CA along with the binary backoff mechanism for channel access. This causes unavoidable contention overheads and does not provide performance guarantees. In this work, we outline some issues that occur with the probabilistic channel access in highly congested scenarios and how those can be mitigated using deterministic scheduling. Towards this, we propose to use Target Wake Time (TWT) - a feature introduced in Wi-Fi 6 as a power-saving mechanism, to improve the performance of Wi-Fi. To gain insights into the workings of the TWT over commercially available off-the-shelf components and to analyze the factors that affect its performance, we carry out various experiments with it over our Wi-Fi 6 testbed. Using these insights and analysis, we formulate and solve an optimization problem to synthesize deterministic schedules and obtain the optimal values of various system parameters. Lastly, we configure our testbed with these optimal parameter values and show that the TWT based deterministic scheduling consistently results in better performance of the TWT-capable clients and overall system performance compared to traditional CSMA/CA based scheduling.","authors":["Govind Rajendran","Samar Agnihotri"],"url":"https://arxiv.org/abs/2505.00447"}
{"created":"2025-05-02","title":"NApy: Efficient Statistics in Python for Large-Scale Heterogeneous Data with Enhanced Support for Missing Data","abstract":"Existing Python libraries and tools lack the ability to efficiently compute statistical test results for large datasets in the presence of missing values. This presents an issue as soon as constraints on runtime and memory availability become essential considerations for a particular usecase. Relevant research areas where such limitations arise include interactive tools and databases for exploratory analysis of biomedical data. To address this problem, we present the Python package NApy, which relies on a Numba and C++ backend with OpenMP parallelization to enable scalable statistical testing for mixed-type datasets in the presence of missing values. Both with respect to runtime and memory consumption, NApy outperforms competitor tools and baseline implementations with naive Python-based parallelization by orders of magnitude, thereby enabling on-the-fly analyses in interactive applications. NApy is publicly available at https://github.com/DyHealthNet/NApy.","authors":["Fabian Woller","Lis Arend","Christian Fuchsberger","Markus List","David B. Blumenthal"],"url":"https://arxiv.org/abs/2505.00448"}
{"created":"2025-05-02","title":"An approach for modularly verifying the core of Rust's atomic reference counting algorithm against the (X)C20 memory consistency model","abstract":"We propose an approach for modular verification of programs that use relaxed-consistency atomic memory access primitives and fences, sufficient for verifying the core of Rust's Atomic Reference Counting (ARC) algorithm, and we argue its soundness, when combined with a simple static analysis and admitting an open sub-problem, with respect to the C20 memory consistency model, as well as, even in the absence of any static analysis and without any assumptions, with respect to XC20, a recently proposed minor strengthening of C20 that rules out out-of-thin-air behaviors but allows load buffering. In contrast to existing work on verifying ARC, we do not assume acyclicity of the union of the program-order and reads-from relations. We define an interleaving operational semantics, prove its soundness with respect to (X)C20's axiomatic semantics, and then apply any existing program logic for fine-grained interleaving concurrency, such as Iris.","authors":["Bart Jacobs","Justus Fasse"],"url":"https://arxiv.org/abs/2505.00449"}
{"created":"2025-05-02","title":"ClearLines - Camera Calibration from Straight Lines","abstract":"The problem of calibration from straight lines is fundamental in geometric computer vision, with well-established theoretical foundations. However, its practical applicability remains limited, particularly in real-world outdoor scenarios. These environments pose significant challenges due to diverse and cluttered scenes, interrupted reprojections of straight 3D lines, and varying lighting conditions, making the task notoriously difficult. Furthermore, the field lacks a dedicated dataset encouraging the development of respective detection algorithms. In this study, we present a small dataset named \"ClearLines\", and by detailing its creation process, provide practical insights that can serve as a guide for developing and refining straight 3D line detection algorithms.","authors":["Gregory Schroeder","Mohamed Sabry","Cristina Olaverri-Monreal"],"url":"https://arxiv.org/abs/2505.00452"}
{"created":"2025-05-02","title":"Data Therapist: Eliciting Domain Knowledge from Subject Matter Experts Using Large Language Models","abstract":"Effective data visualization requires not only technical proficiency but also a deep understanding of the domain-specific context in which data exists. This context often includes tacit knowledge about data provenance, quality, and intended use, which is rarely explicit in the dataset itself. We present the Data Therapist, a web-based tool that helps domain experts externalize this implicit knowledge through a mixed-initiative process combining iterative Q&amp;A with interactive annotation. Powered by a large language model, the system analyzes user-supplied datasets, prompts users with targeted questions, and allows annotation at varying levels of granularity. The resulting structured knowledge base can inform both human and automated visualization design. We evaluated the tool in a qualitative study involving expert pairs from Molecular Biology, Accounting, Political Science, and Usable Security. The study revealed recurring patterns in how experts reason about their data and highlights areas where AI support can improve visualization design.","authors":["Sungbok Shin","Hyeon Jeon","Sanghyun Hong","Niklas Elmqvist"],"url":"https://arxiv.org/abs/2505.00455"}
{"created":"2025-05-02","title":"Memory-Centric Computing: Solving Computing's Memory Problem","abstract":"Computing has a huge memory problem. The memory system, consisting of multiple technologies at different levels, is responsible for most of the energy consumption, performance bottlenecks, robustness problems, monetary cost, and hardware real estate of a modern computing system. All this becomes worse as modern and emerging applications become more data-intensive (as we readily witness in e.g., machine learning, genome analysis, graph processing, and data analytics), making the memory system an even larger bottleneck. In this paper, we discuss two major challenges that greatly affect computing system performance and efficiency: 1) memory technology & capacity scaling (at the lower device and circuit levels) and 2) system and application performance & energy scaling (at the higher levels of the computing stack). We demonstrate that both types of scaling have become extremely difficult, wasteful, and costly due to the dominant processor-centric design & execution paradigm of computers, which treats memory as a dumb and inactive component that cannot perform any computation. We show that moving to a memory-centric design & execution paradigm can solve the major challenges, while enabling multiple other potential benefits. In particular, we demonstrate that: 1) memory technology scaling problems (e.g., RowHammer, RowPress, Variable Read Disturbance, data retention, and other issues awaiting to be discovered) can be much more easily and efficiently handled by enabling memory to autonomously manage itself; 2) system and application performance & energy efficiency can, at the same time, be improved by orders of magnitude by enabling computation capability in memory chips and structures (i.e., processing in memory). We discuss adoption challenges against enabling memory-centric computing, and describe how we can get there step-by-step via an evolutionary path.","authors":["Onur Mutlu","Ataberk Olgun","Ismail Emir Yuksel"],"url":"https://arxiv.org/abs/2505.00458"}
{"created":"2025-05-02","title":"Subspace-Distance-Enabled Active Learning for Efficient Data-Driven Model Reduction of Parametric Dynamical Systems","abstract":"In situations where the solution of a high-fidelity dynamical system needs to be evaluated repeatedly, over a vast pool of parametric configurations and in absence of access to the underlying governing equations, data-driven model reduction techniques are preferable. We propose a novel active learning approach to build a parametric data-driven reduced-order model (ROM) by greedily picking the most important parameter samples from the parameter domain. As a result, during the ROM construction phase, the number of high-fidelity solutions dynamically grow in a principled fashion. The high-fidelity solution snapshots are expressed in several parameter-specific linear subspaces, with the help of proper orthogonal decomposition (POD), and the relative distance between these subspaces is used as a guiding mechanism to perform active learning. For successfully achieving this, we provide a distance measure to evaluate the similarity between pairs of linear subspaces with different dimensions, and also show that this distance measure is a metric. The usability of the proposed subspace-distance-enabled active learning (SDE-AL) framework is demonstrated by augmenting two existing non-intrusive reduced-order modeling approaches, and providing their active-learning-driven (ActLearn) extensions, namely, SDE-ActLearn-POD-KSNN, and SDE-ActLearn-POD-NN. Furthermore, we report positive results for two parametric physical models, highlighting the efficiency of the proposed SDE-AL approach.","authors":["Harshit Kapadia","Peter Benner","Lihong Feng"],"url":"https://arxiv.org/abs/2505.00460"}
{"created":"2025-05-02","title":"HoneyWin: High-Interaction Windows Honeypot in Enterprise Environment","abstract":"Windows operating systems (OS) are ubiquitous in enterprise Information Technology (IT) and operational technology (OT) environments. Due to their widespread adoption and known vulnerabilities, they are often the primary targets of malware and ransomware attacks. With 93% of the ransomware targeting Windows-based systems, there is an urgent need for advanced defensive mechanisms to detect, analyze, and mitigate threats effectively. In this paper, we propose HoneyWin a high-interaction Windows honeypot that mimics an enterprise IT environment. The HoneyWin consists of three Windows 11 endpoints and an enterprise-grade gateway provisioned with comprehensive network traffic capturing, host-based logging, deceptive tokens, endpoint security and real-time alerts capabilities. The HoneyWin has been deployed live in the wild for 34 days and receives more than 5.79 million unsolicited connections, 1.24 million login attempts, 5 and 354 successful logins via remote desktop protocol (RDP) and secure shell (SSH) respectively. The adversary interacted with the deceptive token in one of the RDP sessions and exploited the public-facing endpoint to initiate the Simple Mail Transfer Protocol (SMTP) brute-force bot attack via SSH sessions. The adversary successfully harvested 1,250 SMTP credentials after attempting 151,179 credentials during the attack.","authors":["Yan Lin Aung","Yee Loon Khoo","Davis Yang Zheng","Bryan Swee Duo","Sudipta Chattopadhyay","Jianying Zhou","Liming Lu","Weihan Goh"],"url":"https://arxiv.org/abs/2505.00465"}
{"created":"2025-05-02","title":"A Generalised Framework for Property-Driven Machine Learning","abstract":"Neural networks have been shown to frequently fail to satisfy critical safety and correctness properties after training, highlighting the pressing need for training methods that incorporate such properties directly. While adversarial training can be used to improve robustness to small perturbations within $\\epsilon$-cubes, domains other than computer vision -- such as control systems and natural language processing -- may require more flexible input region specifications via generalised hyper-rectangles. Meanwhile, differentiable logics offer a way to encode arbitrary logical constraints as additional loss terms that guide the learning process towards satisfying these constraints. In this paper, we investigate how these two complementary approaches can be unified within a single framework for property-driven machine learning. We show that well-known properties from the literature are subcases of this general approach, and we demonstrate its practical effectiveness on a case study involving a neural network controller for a drone system. Our framework is publicly available at https://github.com/tflinkow/property-driven-ml.","authors":["Thomas Flinkow","Marco Casadio","Colin Kessler","Rosemary Monahan","Ekaterina Komendantskaya"],"url":"https://arxiv.org/abs/2505.00466"}
{"created":"2025-05-02","title":"Red Teaming Large Language Models for Healthcare","abstract":"We present the design process and findings of the pre-conference workshop at the Machine Learning for Healthcare Conference (2024) entitled Red Teaming Large Language Models for Healthcare, which took place on August 15, 2024. Conference participants, comprising a mix of computational and clinical expertise, attempted to discover vulnerabilities -- realistic clinical prompts for which a large language model (LLM) outputs a response that could cause clinical harm. Red-teaming with clinicians enables the identification of LLM vulnerabilities that may not be recognised by LLM developers lacking clinical expertise. We report the vulnerabilities found, categorise them, and present the results of a replication study assessing the vulnerabilities across all LLMs provided.","authors":["Vahid Balazadeh","Michael Cooper","David Pellow","Atousa Assadi","Jennifer Bell","Jim Fackler","Gabriel Funingana","Spencer Gable-Cook","Anirudh Gangadhar","Abhishek Jaiswal","Sumanth Kaja","Christopher Khoury","Randy Lin","Kaden McKeen","Sara Naimimohasses","Khashayar Namdar","Aviraj Newatia","Allan Pang","Anshul Pattoo","Sameer Peesapati","Diana Prepelita","Bogdana Rakova","Saba Sadatamin","Rafael Schulman","Ajay Shah","Syed Azhar Shah","Syed Ahmar Shah","Babak Taati","Balagopal Unnikrishnan","Stephanie Williams","Rahul G Krishnan"],"url":"https://arxiv.org/abs/2505.00467"}
{"created":"2025-05-02","title":"Evaluation of Thermal Control Based on Spatial Thermal Comfort with Reconstructed Environmental Data","abstract":"Achieving thermal comfort while maintaining energy efficiency is a critical objective in building system control. Conventional thermal comfort models, such as the Predicted Mean Vote (PMV), rely on both environmental and personal variables. However, the use of fixed-location sensors limits the ability to capture spatial variability, which reduces the accuracy of occupant-specific comfort estimation. To address this limitation, this study proposes a new PMV estimation method that incorporates spatial environmental data reconstructed using the Gappy Proper Orthogonal Decomposition (Gappy POD) algorithm. In addition, a group PMV-based control framework is developed to account for the thermal comfort of multiple occupants. The Gappy POD method enables fast and accurate reconstruction of indoor temperature fields from sparse sensor measurements. Using these reconstructed fields and occupant location data, spatially resolved PMV values are calculated. Group-level thermal conditions are then derived through statistical aggregation methods and used to control indoor temperature in a multi-occupant living lab environment. Experimental results show that the Gappy POD algorithm achieves an average relative error below 3\\% in temperature reconstruction. PMV distributions varied by up to 1.26 scale units depending on occupant location. Moreover, thermal satisfaction outcomes varied depending on the group PMV method employed. These findings underscore the importance for adaptive thermal control strategies that incorporate both spatial and individual variability, offering valuable insights for future occupant-centric building operations.","authors":["Youngkyu Kim","Byounghyun Yoo","Ji Young Yun","Hyeokmin Lee","Sehyeon Park","Jin Woo Moon","Eun Ji Choi"],"url":"https://arxiv.org/abs/2505.00468"}
{"created":"2025-05-02","title":"UserCentrix: An Agentic Memory-augmented AI Framework for Smart Spaces","abstract":"Agentic AI, with its autonomous and proactive decision-making, has transformed smart environments. By integrating Generative AI (GenAI) and multi-agent systems, modern AI frameworks can dynamically adapt to user preferences, optimize data management, and improve resource allocation. This paper introduces UserCentrix, an agentic memory-augmented AI framework designed to enhance smart spaces through dynamic, context-aware decision-making. This framework integrates personalized Large Language Model (LLM) agents that leverage user preferences and LLM memory management to deliver proactive and adaptive assistance. Furthermore, it incorporates a hybrid hierarchical control system, balancing centralized and distributed processing to optimize real-time responsiveness while maintaining global situational awareness. UserCentrix achieves resource-efficient AI interactions by embedding memory-augmented reasoning, cooperative agent negotiation, and adaptive orchestration strategies. Our key contributions include (i) a self-organizing framework with proactive scaling based on task urgency, (ii) a Value of Information (VoI)-driven decision-making process, (iii) a meta-reasoning personal LLM agent, and (iv) an intelligent multi-agent coordination system for seamless environment adaptation. Experimental results across various models confirm the effectiveness of our approach in enhancing response accuracy, system efficiency, and computational resource management in real-world application.","authors":["Alaa Saleh","Sasu Tarkoma","Praveen Kumar Donta","Naser Hossein Motlagh","Schahram Dustdar","Susanna Pirttikangas","Lauri Lov\\'en"],"url":"https://arxiv.org/abs/2505.00472"}
{"created":"2025-05-02","title":"Interpretable Spatial-Temporal Fusion Transformers: Multi-Output Prediction for Parametric Dynamical Systems with Time-Varying Inputs","abstract":"We explore the promising performance of a transformer model in predicting outputs of parametric dynamical systems with external time-varying input signals. The outputs of such systems vary not only with physical parameters but also with external time-varying input signals. Accurately catching the dynamics of such systems is challenging. We have adapted and extended an existing transformer model for single output prediction to a multiple-output transformer that is able to predict multiple output responses of these systems. The multiple-output transformer generalizes the interpretability of the original transformer. The generalized interpretable attention weight matrix explores not only the temporal correlations in the sequence, but also the interactions between the multiple outputs, providing explanation for the spatial correlation in the output domain. This multiple-output transformer accurately predicts the sequence of multiple outputs, regardless of the nonlinearity of the system and the dimensionality of the parameter space.","authors":["Shuwen Sun","Lihong Feng","Peter Benner"],"url":"https://arxiv.org/abs/2505.00473"}
{"created":"2025-05-02","title":"Rule-based Classifier Models","abstract":"We extend the formal framework of classifier models used in the legal domain. While the existing classifier framework characterises cases solely through the facts involved, legal reasoning fundamentally relies on both facts and rules, particularly the ratio decidendi. This paper presents an initial approach to incorporating sets of rules within a classifier. Our work is built on the work of Canavotto et al. (2023), which has developed the rule-based reason model of precedential constraint within a hierarchy of factors. We demonstrate how decisions for new cases can be inferred using this enriched rule-based classifier framework. Additionally, we provide an example of how the time element and the hierarchy of courts can be used in the new classifier framework.","authors":["Cecilia Di Florio","Huimin Dong","Antonino Rotolo"],"url":"https://arxiv.org/abs/2505.00474"}
{"created":"2025-05-02","title":"Computational Identification of Regulatory Statements in EU Legislation","abstract":"Identifying regulatory statements in legislation is useful for developing metrics to measure the regulatory density and strictness of legislation. A computational method is valuable for scaling the identification of such statements from a growing body of EU legislation, constituting approximately 180,000 published legal acts between 1952 and 2023. Past work on extraction of these statements varies in the permissiveness of their definitions for what constitutes a regulatory statement. In this work, we provide a specific definition for our purposes based on the institutional grammar tool. We develop and compare two contrasting approaches for automatically identifying such statements in EU legislation, one based on dependency parsing, and the other on a transformer-based machine learning model. We found both approaches performed similarly well with accuracies of 80% and 84% respectively and a K alpha of 0.58. The high accuracies and not exceedingly high agreement suggests potential for combining strengths of both approaches.","authors":["Gijs Jan Brandsma","Jens Blom-Hansen","Christiaan Meijer","Kody Moodley"],"url":"https://arxiv.org/abs/2505.00479"}
{"created":"2025-05-02","title":"Decentralized Vulnerability Disclosure via Permissioned Blockchain: A Secure, Transparent Alternative to Centralized CVE Management","abstract":"This paper proposes a decentralized, blockchain-based system for the publication of Common Vulnerabilities and Exposures (CVEs), aiming to mitigate the limitations of the current centralized model primarily overseen by MITRE. The proposed architecture leverages a permissioned blockchain, wherein only authenticated CVE Numbering Authorities (CNAs) are authorized to submit entries. This ensures controlled write access while preserving public transparency. By incorporating smart contracts, the system supports key features such as embargoed disclosures and decentralized governance. We evaluate the proposed model in comparison with existing practices, highlighting its advantages in transparency, trust decentralization, and auditability. A prototype implementation using Hyperledger Fabric is presented to demonstrate the feasibility of the approach, along with a discussion of its implications for the future of vulnerability disclosure.","authors":["Novruz Amirov","Kemal Bicakci"],"url":"https://arxiv.org/abs/2505.00480"}
{"created":"2025-05-02","title":"Stabilization by Controllers Having Integer Coefficients","abstract":"The system property of ``having integer coefficients,'' that is, a transfer function has an integer monic polynomial as its denominator, is significant in the field of encrypted control as it is required for a dynamic controller to be realized over encrypted data. This paper shows that there always exists a controller with integer coefficients stabilizing a given discrete-time linear time-invariant plant. A constructive algorithm to obtain such a controller is provided, along with numerical examples. Furthermore, the proposed method is applied to converting a pre-designed controller to have integer coefficients, while the original performance is preserved in the sense that the transfer function of the closed-loop system remains unchanged.","authors":["Joowon Lee","Donggil Lee","Junsoo Kim"],"url":"https://arxiv.org/abs/2505.00481"}
{"created":"2025-05-02","title":"JointDiT: Enhancing RGB-Depth Joint Modeling with Diffusion Transformers","abstract":"We present JointDiT, a diffusion transformer that models the joint distribution of RGB and depth. By leveraging the architectural benefit and outstanding image prior of the state-of-the-art diffusion transformer, JointDiT not only generates high-fidelity images but also produces geometrically plausible and accurate depth maps. This solid joint distribution modeling is achieved through two simple yet effective techniques that we propose, i.e., adaptive scheduling weights, which depend on the noise levels of each modality, and the unbalanced timestep sampling strategy. With these techniques, we train our model across all noise levels for each modality, enabling JointDiT to naturally handle various combinatorial generation tasks, including joint generation, depth estimation, and depth-conditioned image generation by simply controlling the timestep of each branch. JointDiT demonstrates outstanding joint generation performance. Furthermore, it achieves comparable results in depth estimation and depth-conditioned image generation, suggesting that joint distribution modeling can serve as a replaceable alternative to conditional generation. The project page is available at https://byungki-k.github.io/JointDiT/.","authors":["Kwon Byung-Ki","Qi Dai","Lee Hyoseok","Chong Luo","Tae-Hyun Oh"],"url":"https://arxiv.org/abs/2505.00482"}
{"created":"2025-05-02","title":"Analysis of the vulnerability of machine learning regression models to adversarial attacks using data from 5G wireless networks","abstract":"This article describes the process of creating a script and conducting an analytical study of a dataset using the DeepMIMO emulator. An advertorial attack was carried out using the FGSM method to maximize the gradient. A comparison is made of the effectiveness of binary classifiers in the task of detecting distorted data. The dynamics of changes in the quality indicators of the regression model were analyzed in conditions without adversarial attacks, during an adversarial attack and when the distorted data was isolated. It is shown that an adversarial FGSM attack with gradient maximization leads to an increase in the value of the MSE metric by 33% and a decrease in the R2 indicator by 10% on average. The LightGBM binary classifier effectively identifies data with adversarial anomalies with 98% accuracy. Regression machine learning models are susceptible to adversarial attacks, but rapid analysis of network traffic and data transmitted over the network makes it possible to identify malicious activity","authors":["Leonid Legashev","Artur Zhigalov","Denis Parfenov"],"url":"https://arxiv.org/abs/2505.00487"}
{"created":"2025-05-02","title":"MULE: Multi-terrain and Unknown Load Adaptation for Effective Quadrupedal Locomotion","abstract":"Quadrupedal robots are increasingly deployed for load-carrying tasks across diverse terrains. While Model Predictive Control (MPC)-based methods can account for payload variations, they often depend on predefined gait schedules or trajectory generators, limiting their adaptability in unstructured environments. To address these limitations, we propose an Adaptive Reinforcement Learning (RL) framework that enables quadrupedal robots to dynamically adapt to both varying payloads and diverse terrains. The framework consists of a nominal policy responsible for baseline locomotion and an adaptive policy that learns corrective actions to preserve stability and improve command tracking under payload variations. We validate the proposed approach through large-scale simulation experiments in Isaac Gym and real-world hardware deployment on a Unitree Go1 quadruped. The controller was tested on flat ground, slopes, and stairs under both static and dynamic payload changes. Across all settings, our adaptive controller consistently outperformed the controller in tracking body height and velocity commands, demonstrating enhanced robustness and adaptability without requiring explicit gait design or manual tuning.","authors":["Vamshi Kumar Kurva","Shishir Kolathaya"],"url":"https://arxiv.org/abs/2505.00488"}
{"created":"2025-05-02","title":"Optimal Interactive Learning on the Job via Facility Location Planning","abstract":"Collaborative robots must continually adapt to novel tasks and user preferences without overburdening the user. While prior interactive robot learning methods aim to reduce human effort, they are typically limited to single-task scenarios and are not well-suited for sustained, multi-task collaboration. We propose COIL (Cost-Optimal Interactive Learning) -- a multi-task interaction planner that minimizes human effort across a sequence of tasks by strategically selecting among three query types (skill, preference, and help). When user preferences are known, we formulate COIL as an uncapacitated facility location (UFL) problem, which enables bounded-suboptimal planning in polynomial time using off-the-shelf approximation algorithms. We extend our formulation to handle uncertainty in user preferences by incorporating one-step belief space planning, which uses these approximation algorithms as subroutines to maintain polynomial-time performance. Simulated and physical experiments on manipulation tasks show that our framework significantly reduces the amount of work allocated to the human while maintaining successful task completion.","authors":["Shivam Vats","Michelle Zhao","Patrick Callaghan","Mingxi Jia","Maxim Likhachev","Oliver Kroemer","George Konidaris"],"url":"https://arxiv.org/abs/2505.00490"}
{"created":"2025-05-02","title":"Enhancing Tropical Cyclone Path Forecasting with an Improved Transformer Network","abstract":"A storm is a type of extreme weather. Therefore, forecasting the path of a storm is extremely important for protecting human life and property. However, storm forecasting is very challenging because storm trajectories frequently change. In this study, we propose an improved deep learning method using a Transformer network to predict the movement trajectory of a storm over the next 6 hours. The storm data used to train the model was obtained from the National Oceanic and Atmospheric Administration (NOAA) [1]. Simulation results show that the proposed method is more accurate than traditional methods. Moreover, the proposed method is faster and more cost-effective","authors":["Nguyen Van Thanh","Nguyen Dang Huynh","Nguyen Ngoc Tan","Nguyen Thai Minh","Nguyen Nam Hoang"],"url":"https://arxiv.org/abs/2505.00495"}
{"created":"2025-05-02","title":"Out of the Loop Again: How Dangerous is Weaponizing Automated Nuclear Systems?","abstract":"Are nuclear weapons useful for coercion, and, if so, what factors increase the credibility and effectiveness of nuclear threats? While prominent scholars like Thomas Schelling argue that nuclear brinkmanship, or the manipulation of nuclear risk, can effectively coerce adversaries, others contend nuclear weapons are not effective tools of coercion, especially coercion designed to achieve offensive and revisionist objectives. Simultaneously, there is broad debate about the incorporation of artificial intelligence (AI) into military systems, especially nuclear command and control. We develop a theoretical argument that explicit nuclear threats implemented with automated nuclear launch systems are potentially more credible compared to ambiguous nuclear threats or explicit nuclear threats implemented via non-automated means. By reducing human control over nuclear use, leaders can more effectively tie their hands and thus signal resolve. While automated nuclear weapons launch systems may seem like something out of science fiction, the Soviet Union deployed such a system during the Cold War and the technology necessary to automate the use of force has developed considerably in recent years due to advances in AI. Preregistered survey experiments on an elite sample of United Kingdom Members of Parliament and two public samples of UK citizens provide support for these expectations, showing that, in a limited set of circumstances, nuclear threats backed by AI integration have credibility advantages, no matter how dangerous they may be. Our findings contribute to the literatures on coercive bargaining, weapons of mass destruction, and emerging technology.","authors":["Joshua A. Schwartz","Michael C. Horowitz"],"url":"https://arxiv.org/abs/2505.00496"}
{"created":"2025-05-02","title":"KeySync: A Robust Approach for Leakage-free Lip Synchronization in High Resolution","abstract":"Lip synchronization, known as the task of aligning lip movements in an existing video with new input audio, is typically framed as a simpler variant of audio-driven facial animation. However, as well as suffering from the usual issues in talking head generation (e.g., temporal consistency), lip synchronization presents significant new challenges such as expression leakage from the input video and facial occlusions, which can severely impact real-world applications like automated dubbing, but are often neglected in existing works. To address these shortcomings, we present KeySync, a two-stage framework that succeeds in solving the issue of temporal consistency, while also incorporating solutions for leakage and occlusions using a carefully designed masking strategy. We show that KeySync achieves state-of-the-art results in lip reconstruction and cross-synchronization, improving visual quality and reducing expression leakage according to LipLeak, our novel leakage metric. Furthermore, we demonstrate the effectiveness of our new masking approach in handling occlusions and validate our architectural choices through several ablation studies. Code and model weights can be found at https://antonibigata.github.io/KeySync.","authors":["Antoni Bigata","Rodrigo Mira","Stella Bounareli","Micha{\\l} Stypu{\\l}kowski","Konstantinos Vougioukas","Stavros Petridis","Maja Pantic"],"url":"https://arxiv.org/abs/2505.00497"}
{"created":"2025-05-02","title":"Implicit Neural-Representation Learning for Elastic Deformable-Object Manipulations","abstract":"We aim to solve the problem of manipulating deformable objects, particularly elastic bands, in real-world scenarios. However, deformable object manipulation (DOM) requires a policy that works on a large state space due to the unlimited degree of freedom (DoF) of deformable objects. Further, their dense but partial observations (e.g., images or point clouds) may increase the sampling complexity and uncertainty in policy learning. To figure it out, we propose a novel implicit neural-representation (INR) learning for elastic DOMs, called INR-DOM. Our method learns consistent state representations associated with partially observable elastic objects reconstructing a complete and implicit surface represented as a signed distance function. Furthermore, we perform exploratory representation fine-tuning through reinforcement learning (RL) that enables RL algorithms to effectively learn exploitable representations while efficiently obtaining a DOM policy. We perform quantitative and qualitative analyses building three simulated environments and real-world manipulation studies with a Franka Emika Panda arm. Videos are available at http://inr-dom.github.io.","authors":["Minseok Song","JeongHo Ha","Bonggyeong Park","Daehyung Park"],"url":"https://arxiv.org/abs/2505.00500"}
{"created":"2025-05-02","title":"Towards Scalable Human-aligned Benchmark for Text-guided Image Editing","abstract":"A variety of text-guided image editing models have been proposed recently. However, there is no widely-accepted standard evaluation method mainly due to the subjective nature of the task, letting researchers rely on manual user study. To address this, we introduce a novel Human-Aligned benchmark for Text-guided Image Editing (HATIE). Providing a large-scale benchmark set covering a wide range of editing tasks, it allows reliable evaluation, not limited to specific easy-to-evaluate cases. Also, HATIE provides a fully-automated and omnidirectional evaluation pipeline. Particularly, we combine multiple scores measuring various aspects of editing so as to align with human perception. We empirically verify that the evaluation of HATIE is indeed human-aligned in various aspects, and provide benchmark results on several state-of-the-art models to provide deeper insights on their performance.","authors":["Suho Ryu","Kihyun Kim","Eugene Baek","Dongsoo Shin","Joonseok Lee"],"url":"https://arxiv.org/abs/2505.00502"}
{"created":"2025-05-02","title":"Variational OOD State Correction for Offline Reinforcement Learning","abstract":"The performance of Offline reinforcement learning is significantly impacted by the issue of state distributional shift, and out-of-distribution (OOD) state correction is a popular approach to address this problem. In this paper, we propose a novel method named Density-Aware Safety Perception (DASP) for OOD state correction. Specifically, our method encourages the agent to prioritize actions that lead to outcomes with higher data density, thereby promoting its operation within or the return to in-distribution (safe) regions. To achieve this, we optimize the objective within a variational framework that concurrently considers both the potential outcomes of decision-making and their density, thus providing crucial contextual information for safe decision-making. Finally, we validate the effectiveness and feasibility of our proposed method through extensive experimental evaluations on the offline MuJoCo and AntMaze suites.","authors":["Ke Jiang","Wen Jiang","Xiaoyang Tan"],"url":"https://arxiv.org/abs/2505.00503"}
{"created":"2025-05-02","title":"HalluMix: A Task-Agnostic, Multi-Domain Benchmark for Real-World Hallucination Detection","abstract":"As large language models (LLMs) are increasingly deployed in high-stakes domains, detecting hallucinated content$\\unicode{x2013}$text that is not grounded in supporting evidence$\\unicode{x2013}$has become a critical challenge. Existing benchmarks for hallucination detection are often synthetically generated, narrowly focused on extractive question answering, and fail to capture the complexity of real-world scenarios involving multi-document contexts and full-sentence outputs. We introduce the HalluMix Benchmark, a diverse, task-agnostic dataset that includes examples from a range of domains and formats. Using this benchmark, we evaluate seven hallucination detection systems$\\unicode{x2013}$both open and closed source$\\unicode{x2013}$highlighting differences in performance across tasks, document lengths, and input representations. Our analysis highlights substantial performance disparities between short and long contexts, with critical implications for real-world Retrieval Augmented Generation (RAG) implementations. Quotient Detections achieves the best overall performance, with an accuracy of 0.82 and an F1 score of 0.84.","authors":["Deanna Emery","Michael Goitia","Freddie Vargus","Iulia Neagu"],"url":"https://arxiv.org/abs/2505.00506"}
{"created":"2025-05-02","title":"HeAL3D: Heuristical-enhanced Active Learning for 3D Object Detection","abstract":"Active Learning has proved to be a relevant approach to perform sample selection for training models for Autonomous Driving. Particularly, previous works on active learning for 3D object detection have shown that selection of samples in uncontrolled scenarios is challenging. Furthermore, current approaches focus exclusively on the theoretical aspects of the sample selection problem but neglect the practical insights that can be obtained from the extensive literature and application of 3D detection models. In this paper, we introduce HeAL (Heuristical-enhanced Active Learning for 3D Object Detection) which integrates those heuristical features together with Localization and Classification to deliver the most contributing samples to the model's training. In contrast to previous works, our approach integrates heuristical features such as object distance and point-quantity to estimate the uncertainty, which enhance the usefulness of selected samples to train detection models. Our quantitative evaluation on KITTI shows that HeAL presents competitive mAP with respect to the State-of-the-Art, and achieves the same mAP as the full-supervised baseline with only 24% of the samples.","authors":["Esteban Rivera","Surya Prabhakaran","Markus Lienkamp"],"url":"https://arxiv.org/abs/2505.00507"}
{"created":"2025-05-02","title":"Weak Random Feature Method for Solving Partial Differential Equations","abstract":"The random feature method (RFM) has demonstrated great potential in bridging traditional numerical methods and machine learning techniques for solving partial differential equations (PDEs). It retains the advantages of mesh-free approaches while achieving spectral accuracy for smooth solutions, without the need for iterative procedures. However, the implementation of RFM in the identification of weak solutions remains a subject of limited comprehension, despite crucial role of weak solutions in addressing numerous applied problems. While the direct application of RFM to problems without strong solutions is fraught with potential challenges, we propose an enhancement to the original random feature method that is specifically suited for finding weak solutions and is termed as Weak RFM. Essentially, Weak RFM reformulates the original RFM by adopting the weak form of the governing equations and constructing a new linear system through the use of carefully designed test functions, ensuring that the resulting solution satisfies the weak form by default. To rigorously evaluate the performance of the proposed method, we conduct extensive experiments on a variety of benchmark problems, including challenging three-dimensional cases, and compare its performance with state of the art machine learning-based approaches. The results demonstrate that Weak RFM achieves comparable or superior accuracy while significantly reducing computational time and memory consumption, highlighting its potential as a highly efficient and robust tool for finding weak solutions to various PDE problems.","authors":["Mikhail Kuvakin","Zijian Mei","Jingrun Chen"],"url":"https://arxiv.org/abs/2505.00508"}
{"created":"2025-05-02","title":"Self-Ablating Transformers: More Interpretability, Less Sparsity","abstract":"A growing intuition in machine learning suggests a link between sparsity and interpretability. We introduce a novel self-ablation mechanism to investigate this connection ante-hoc in the context of language transformers. Our approach dynamically enforces a k-winner-takes-all constraint, forcing the model to demonstrate selective activation across neuron and attention units. Unlike post-hoc methods that analyze already-trained models, our approach integrates interpretability directly into model training, promoting feature localization from inception. Training small models on the TinyStories dataset and employing interpretability tests, we find that self-ablation leads to more localized circuits, concentrated feature representations, and increased neuron specialization without compromising language modelling performance. Surprisingly, our method also decreased overall sparsity, indicating that self-ablation promotes specialization rather than widespread inactivity. This reveals a complex interplay between sparsity and interpretability, where decreased global sparsity can coexist with increased local specialization, leading to enhanced interpretability. To facilitate reproducibility, we make our code available at https://github.com/keenanpepper/self-ablating-transformers.","authors":["Jeremias Ferrao","Luhan Mikaelson","Keenan Pepper","Natalia Perez-Campanero Antolin"],"url":"https://arxiv.org/abs/2505.00509"}
{"created":"2025-05-02","title":"Inconsistency-based Active Learning for LiDAR Object Detection","abstract":"Deep learning models for object detection in autonomous driving have recently achieved impressive performance gains and are already being deployed in vehicles worldwide. However, current models require increasingly large datasets for training. Acquiring and labeling such data is costly, necessitating the development of new strategies to optimize this process. Active learning is a promising approach that has been extensively researched in the image domain. In our work, we extend this concept to the LiDAR domain by developing several inconsistency-based sample selection strategies and evaluate their effectiveness in various settings. Our results show that using a naive inconsistency approach based on the number of detected boxes, we achieve the same mAP as the random sampling strategy with 50% of the labeled data.","authors":["Esteban Rivera","Loic Stratil","Markus Lienkamp"],"url":"https://arxiv.org/abs/2505.00511"}
{"created":"2025-05-02","title":"InterLoc: LiDAR-based Intersection Localization using Road Segmentation with Automated Evaluation Method","abstract":"Intersections are geometric and functional key points in every road network. They offer strong landmarks to correct GNSS dropouts and anchor new sensor data in up-to-date maps. Despite that importance, intersection detectors either ignore the rich semantic information already computed onboard or depend on scarce, hand-labeled intersection datasets. To close that gap, this paper presents a LiDAR-based method for intersection detection that (i) fuses semantic road segmentation with vehicle localization to detect intersection candidates in a bird's eye view (BEV) representation and (ii) refines those candidates by analyzing branch topology with a least squares formulation. To evaluate our method, we introduce an automated benchmarking pipeline that pairs detections with OpenStreetMap (OSM) intersection nodes using precise GNSS/INS ground-truth poses. Tested on eight SemanticKITTI sequences, the approach achieves a mean localization error of 1.9 m, 89% precision, and 77% recall at a 5 m tolerance, outperforming the latest learning-based baseline. Moreover, the method is robust to segmentation errors higher than those of the benchmark model, demonstrating its applicability in the real world.","authors":["Nguyen Hoang Khoi Tran","Julie Stephany Berrio","Mao Shan","Zhenxing Ming","Stewart Worrall"],"url":"https://arxiv.org/abs/2505.00512"}
{"created":"2025-05-02","title":"Safety-Critical Traffic Simulation with Guided Latent Diffusion Model","abstract":"Safety-critical traffic simulation plays a crucial role in evaluating autonomous driving systems under rare and challenging scenarios. However, existing approaches often generate unrealistic scenarios due to insufficient consideration of physical plausibility and suffer from low generation efficiency. To address these limitations, we propose a guided latent diffusion model (LDM) capable of generating physically realistic and adversarial safety-critical traffic scenarios. Specifically, our model employs a graph-based variational autoencoder (VAE) to learn a compact latent space that captures complex multi-agent interactions while improving computational efficiency. Within this latent space, the diffusion model performs the denoising process to produce realistic trajectories. To enable controllable and adversarial scenario generation, we introduce novel guidance objectives that drive the diffusion process toward producing adversarial and behaviorally realistic driving behaviors. Furthermore, we develop a sample selection module based on physical feasibility checks to further enhance the physical plausibility of the generated scenarios. Extensive experiments on the nuScenes dataset demonstrate that our method achieves superior adversarial effectiveness and generation efficiency compared to existing baselines while maintaining a high level of realism. Our work provides an effective tool for realistic safety-critical scenario simulation, paving the way for more robust evaluation of autonomous driving systems.","authors":["Mingxing Peng","Ruoyu Yao","Xusen Guo","Yuting Xie","Xianda Chen","Jun Ma"],"url":"https://arxiv.org/abs/2505.00515"}
{"created":"2025-05-02","title":"Linear Phase Balancing Scheme using Voltage Unbalance Sensitivities in Multi-phase Power Distribution Grids","abstract":"Power distribution networks, especially in North America, are often unbalanced due to the mix of single-, two- and three-phase networks as well as due to the high penetration of single-phase devices at the distribution level such as electric vehicle (EV) chargers and single-phase solar plants. However, the network operator must adhere to the voltage unbalance levels within the limits specified by IEEE, IEC, and NEMA standards for the safety of the equipment as well as the efficiency of the network operation. Existing works have proposed active and reactive power control in the network to minimize imbalances. However, these optimization problems are highly nonlinear and nonconvex due to the inherent non-linearity of unbalanced metrics and power-flow equations. In this work, we propose a linearization approach of unbalance metrics such as voltage unbalance factors (VUF), phase voltage unbalance rate (PVUR), and line voltage unbalance rate (LVUR) using the first order Taylor's approximation. This linearization is then applied to the phase balancing control scheme; it is formulated as a feedback approach where the linearization is updated successively after the active/reactive control setpoint has been actuated and shows improvement in voltage imbalances. We demonstrate the application of the proposed scheme on a standard IEEE benchmark test case, demonstrating its effectiveness.","authors":["Rahul K. Gupta"],"url":"https://arxiv.org/abs/2505.00519"}
{"created":"2025-05-02","title":"Proportionality in Practice: Quantifying Proportionality in Ordinal Elections","abstract":"Proportional representation plays a crucial role in electoral systems. In ordinal elections, where voters rank candidates based on their preferences, the Single Transferable Vote (STV) is the most widely used proportional voting method. STV is considered proportional because it satisfies an axiom requiring that large enough solid coalitions of voters are adequately represented. Using real-world data from local Scottish elections, we observe that solid coalitions of the required size rarely occur in practice. This observation challenges the importance of proportionality axioms and raises the question of how the proportionality of voting methods can be assessed beyond their axiomatic performance. We address these concerns by developing quantitative measures of proportionality. We apply these measures to evaluate the proportionality of voting rules on real-world election data. Besides STV, we consider SNTV, the Expanding Approvals Rule, and Sequential Ranked-Choice Voting. We also study the effects of ballot truncation by artificially completing truncated ballots and comparing the proportionality of outcomes under complete and truncated ballots.","authors":["Tuva Bardal","Markus Brill","David McCune","Jannik Peters"],"url":"https://arxiv.org/abs/2505.00520"}
{"created":"2025-05-02","title":"DeCo: Task Decomposition and Skill Composition for Zero-Shot Generalization in Long-Horizon 3D Manipulation","abstract":"Generalizing language-conditioned multi-task imitation learning (IL) models to novel long-horizon 3D manipulation tasks remains a significant challenge. To address this, we propose DeCo (Task Decomposition and Skill Composition), a model-agnostic framework compatible with various multi-task IL models, designed to enhance their zero-shot generalization to novel, compositional, long-horizon 3D manipulation tasks. DeCo first decomposes IL demonstrations into a set of modular atomic tasks based on the physical interaction between the gripper and objects, and constructs an atomic training dataset that enables models to learn a diverse set of reusable atomic skills during imitation learning. At inference time, DeCo leverages a vision-language model (VLM) to parse high-level instructions for novel long-horizon tasks, retrieve the relevant atomic skills, and dynamically schedule their execution; a spatially-aware skill-chaining module then ensures smooth, collision-free transitions between sequential skills. We evaluate DeCo in simulation using DeCoBench, a benchmark specifically designed to assess zero-shot generalization of multi-task IL models in compositional long-horizon 3D manipulation. Across three representative multi-task IL models (RVT-2, 3DDA, and ARP), DeCo achieves success rate improvements of 66.67%, 21.53%, and 57.92%, respectively, on 12 novel compositional tasks. Moreover, in real-world experiments, a DeCo-enhanced model trained on only 6 atomic tasks successfully completes 9 novel long-horizon tasks, yielding an average success rate improvement of 53.33% over the base multi-task IL model. Video demonstrations are available at: https://deco226.github.io.","authors":["Zixuan Chen","Junhui Yin","Yangtao Chen","Jing Huo","Pinzhuo Tian","Jieqi Shi","Yiwen Hou","Yinchuan Li","Yang Gao"],"url":"https://arxiv.org/abs/2505.00527"}
{"created":"2025-05-02","title":"Leveraging Partial SMILES Validation Scheme for Enhanced Drug Design in Reinforcement Learning Frameworks","abstract":"SMILES-based molecule generation has emerged as a powerful approach in drug discovery. Deep reinforcement learning (RL) using large language model (LLM) has been incorporated into the molecule generation process to achieve high matching score in term of likelihood of desired molecule candidates. However, a critical challenge in this approach is catastrophic forgetting during the RL phase, where knowledge such as molecule validity, which often exceeds 99\\% during pretraining, significantly deteriorates. Current RL algorithms applied in drug discovery, such as REINVENT, use prior models as anchors to retian pretraining knowledge, but these methods lack robust exploration mechanisms. To address these issues, we propose Partial SMILES Validation-PPO (PSV-PPO), a novel RL algorithm that incorporates real-time partial SMILES validation to prevent catastrophic forgetting while encouraging exploration. Unlike traditional RL approaches that validate molecule structures only after generating entire sequences, PSV-PPO performs stepwise validation at each auto-regressive step, evaluating not only the selected token candidate but also all potential branches stemming from the prior partial sequence. This enables early detection of invalid partial SMILES across all potential paths. As a result, PSV-PPO maintains high validity rates even during aggressive exploration of the vast chemical space. Our experiments on the PMO and GuacaMol benchmark datasets demonstrate that PSV-PPO significantly reduces the number of invalid generated structures while maintaining competitive exploration and optimization performance. While our work primarily focuses on maintaining validity, the framework of PSV-PPO can be extended in future research to incorporate additional forms of valuable domain knowledge, further enhancing reinforcement learning applications in drug discovery.","authors":["Xinyu Wang","Jinbo Bi","Minghu Song"],"url":"https://arxiv.org/abs/2505.00530"}
{"created":"2025-05-02","title":"Test-time Correlation Alignment","abstract":"Deep neural networks often experience performance drops due to distribution shifts between training and test data. Although domain adaptation offers a solution, privacy concerns restrict access to training data in many real-world scenarios. This restriction has spurred interest in Test-Time Adaptation (TTA), which adapts models using only unlabeled test data. However, current TTA methods still face practical challenges: (1) a primary focus on instance-wise alignment, overlooking CORrelation ALignment (CORAL) due to missing source correlations; (2) complex backpropagation operations for model updating, resulting in overhead computation and (3) domain forgetting.","authors":["Linjing You","Jiabao Lu","Xiayuan Huang"],"url":"https://arxiv.org/abs/2505.00533"}
{"created":"2025-05-02","title":"A Robust Deep Networks based Multi-Object MultiCamera Tracking System for City Scale Traffic","abstract":"Vision sensors are becoming more important in Intelligent Transportation Systems (ITS) for traffic monitoring, management, and optimization as the number of network cameras continues to rise. However, manual object tracking and matching across multiple non-overlapping cameras pose significant challenges in city-scale urban traffic scenarios. These challenges include handling diverse vehicle attributes, occlusions, illumination variations, shadows, and varying video resolutions. To address these issues, we propose an efficient and cost-effective deep learning-based framework for Multi-Object Multi-Camera Tracking (MO-MCT). The proposed framework utilizes Mask R-CNN for object detection and employs Non-Maximum Suppression (NMS) to select target objects from overlapping detections. Transfer learning is employed for re-identification, enabling the association and generation of vehicle tracklets across multiple cameras. Moreover, we leverage appropriate loss functions and distance measures to handle occlusion, illumination, and shadow challenges. The final solution identification module performs feature extraction using ResNet-152 coupled with Deep SORT based vehicle tracking. The proposed framework is evaluated on the 5th AI City Challenge dataset (Track 3), comprising 46 camera feeds. Among these 46 camera streams, 40 are used for model training and validation, while the remaining six are utilized for model testing. The proposed framework achieves competitive performance with an IDF1 score of 0.8289, and precision and recall scores of 0.9026 and 0.8527 respectively, demonstrating its effectiveness in robust and accurate vehicle tracking.","authors":["Muhammad Imran Zaman","Usama Ijaz Bajwa","Gulshan Saleem","Rana Hammad Raza"],"url":"https://arxiv.org/abs/2505.00534"}
{"created":"2025-05-02","title":"Emergence of Roles in Robotic Teams with Model Sharing and Limited Communication","abstract":"We present a reinforcement learning strategy for use in multi-agent foraging systems in which the learning is centralised to a single agent and its model is periodically disseminated among the population of non-learning agents. In a domain where multi-agent reinforcement learning (MARL) is the common approach, this approach aims to significantly reduce the computational and energy demands compared to approaches such as MARL and centralised learning models. By developing high performing foraging agents, these approaches can be translated into real-world applications such as logistics, environmental monitoring, and autonomous exploration. A reward function was incorporated into this approach that promotes role development among agents, without explicit directives. This led to the differentiation of behaviours among the agents. The implicit encouragement of role differentiation allows for dynamic actions in which agents can alter roles dependent on their interactions with the environment without the need for explicit communication between agents.","authors":["Ian O'Flynn","Harun \\v{S}iljak"],"url":"https://arxiv.org/abs/2505.00540"}
{"created":"2025-05-02","title":"KnowEEG: Explainable Knowledge Driven EEG Classification","abstract":"Electroencephalography (EEG) is a method of recording brain activity that shows significant promise in applications ranging from disease classification to emotion detection and brain-computer interfaces. Recent advances in deep learning have improved EEG classification performance yet model explainability remains an issue. To address this key limitation of explainability we introduce KnowEEG; a novel explainable machine learning approach for EEG classification. KnowEEG extracts a comprehensive set of per-electrode features, filters them using statistical tests, and integrates between-electrode connectivity statistics. These features are then input to our modified Random Forest model (Fusion Forest) that balances per electrode statistics with between electrode connectivity features in growing the trees of the forest. By incorporating knowledge from both the generalized time-series and EEG-specific domains, KnowEEG achieves performance comparable to or exceeding state-of-the-art deep learning models across five different classification tasks: emotion detection, mental workload classification, eyes open/closed detection, abnormal EEG classification, and event detection. In addition to high performance, KnowEEG provides inherent explainability through feature importance scores for understandable features. We demonstrate by example on the eyes closed/open classification task that this explainability can be used to discover knowledge about the classes. This discovered knowledge for eyes open/closed classification was proven to be correct by current neuroscience literature. Therefore, the impact of KnowEEG will be significant for domains where EEG explainability is critical such as healthcare.","authors":["Amarpal Sahota","Navid Mohammadi Foumani","Raul Santos-Rodriguez","Zahraa S. Abdallah"],"url":"https://arxiv.org/abs/2505.00541"}
{"created":"2025-05-02","title":"Reducing Student Distraction Through Fuzzy Logic Based Seating Arrangements","abstract":"A crucial skill for primary school teachers is maintaining efficient classroom management. Teachers use classroom seating arrangements to help maintain this efficiency. However, developing classroom seating arrangements is both time-consuming and often non-optimal for distraction mitigation. Fuzzy logic-based approaches for the development of classroom seating arrangements can reduce development time and minimize classroom distraction. In this study, an original fuzzy logic-based software package named \"CUB\" is introduced and applied to a modern classroom using \"cluster\" seating arrangements. The combination of fuzzy inference systems, fuzzy c-means clustering, sequential, and iterative processes produce ready-to-use seating arrangements for the classroom in this study. The seating arrangements are compared with an existing set of seating arrangements to validate the results. The author's findings show that CUB is successful in generating applicable seating arrangements with a small liklihood of replicating arrangements. The findings also suggest that fuzz logic-based approaches may be successful in other styles of classroom arrangement.","authors":["Garrett Olges","Kelly Cohen"],"url":"https://arxiv.org/abs/2505.00545"}
{"created":"2025-05-02","title":"Directly Forecasting Belief for Reinforcement Learning with Delays","abstract":"Reinforcement learning (RL) with delays is challenging as sensory perceptions lag behind the actual events: the RL agent needs to estimate the real state of its environment based on past observations. State-of-the-art (SOTA) methods typically employ recursive, step-by-step forecasting of states. This can cause the accumulation of compounding errors. To tackle this problem, our novel belief estimation method, named Directly Forecasting Belief Transformer (DFBT), directly forecasts states from observations without incrementally estimating intermediate states step-by-step. We theoretically demonstrate that DFBT greatly reduces compounding errors of existing recursively forecasting methods, yielding stronger performance guarantees. In experiments with D4RL offline datasets, DFBT reduces compounding errors with remarkable prediction accuracy. DFBT's capability to forecast state sequences also facilitates multi-step bootstrapping, thus greatly improving learning efficiency. On the MuJoCo benchmark, our DFBT-based method substantially outperforms SOTA baselines.","authors":["Qingyuan Wu","Yuhui Wang","Simon Sinong Zhan","Yixuan Wang","Chung-Wei Lin","Chen Lv","Qi Zhu","J\\\"urgen Schmidhuber","Chao Huang"],"url":"https://arxiv.org/abs/2505.00546"}
{"created":"2025-05-02","title":"From Requirements to Test Cases: An NLP-Based Approach for High-Performance ECU Test Case Automation","abstract":"Automating test case specification generation is vital for improving the efficiency and accuracy of software testing, particularly in complex systems like high-performance Electronic Control Units (ECUs). This study investigates the use of Natural Language Processing (NLP) techniques, including Rule-Based Information Extraction and Named Entity Recognition (NER), to transform natural language requirements into structured test case specifications. A dataset of 400 feature element documents from the Polarion tool was used to evaluate both approaches for extracting key elements such as signal names and values. The results reveal that the Rule-Based method outperforms the NER method, achieving 95% accuracy for more straightforward requirements with single signals, while the NER method, leveraging SVM and other machine learning algorithms, achieved 77.3% accuracy but struggled with complex scenarios. Statistical analysis confirmed that the Rule-Based approach significantly enhances efficiency and accuracy compared to manual methods. This research highlights the potential of NLP-driven automation in improving quality assurance, reducing manual effort, and expediting test case generation, with future work focused on refining NER and hybrid models to handle greater complexity.","authors":["Nikitha Medeshetty","Ahmad Nauman Ghazi","Sadi Alawadi","Fahed Alkhabbas"],"url":"https://arxiv.org/abs/2505.00547"}
{"created":"2025-05-02","title":"Model order reduction of hemodynamics by space-time reduced basis and reduced fluid-structure interaction","abstract":"In this work, we apply the space-time Galerkin reduced basis (ST-GRB) method to a reduced fluid-structure interaction model, for the numerical simulation of hemodynamics in arteries. In essence, ST-GRB extends the classical reduced basis (RB) method, exploiting a data-driven low-dimensional linear encoding of the temporal dynamics to further cut the computational costs. The current investigation brings forth two key enhancements, compared to previous works on the topic. On the one side, we model blood flow through the Navier-Stokes equations, hence accounting for convection. In this regard, we implement a hyper-reduction scheme, based on approximate space-time reduced affine decompositions, to deal with nonlinearities effectively. On the other side, we move beyond the constraint of modelling blood vessels as rigid structures, acknowledging the importance of elasticity for the accurate simulation of complex blood flow patterns. To limit computational complexity, we adopt the Coupled Momentum model, incorporating the effect of wall compliance in the fluid's equations through a generalized Robin boundary condition. In particular, we propose an efficient strategy for handling the spatio-temporal projection of the structural displacement, which ultimately configures as a by-product. The performances of ST-GRB are assessed in three different numerical experiments. The results confirm that the proposed approach can outperform the classical RB method, yielding precise approximations of high-fidelity solutions at more convenient costs. However, the computational gains of ST-GRB vanish if the number of retained temporal modes is too large, which occurs either when complex dynamics arise or if very precise solutions are sought.","authors":["Riccardo Tenderini","Simone Deparis"],"url":"https://arxiv.org/abs/2505.00548"}
{"created":"2025-05-02","title":"Sum Rate Maximization for NOMA-Assisted Uplink Pinching-Antenna Systems","abstract":"In this paper, we investigate an uplink communication scenario in which multiple users communicate with an access point (AP) employing non-orthogonal multiple access (NOMA). A pinching antenna, which can be activated at an arbitrary point along a dielectric waveguide, is deployed at the AP to dynamically reconfigure user channels. The objective is to maximize the system sum rate by jointly optimizing the pinching-antenna's position and the users' transmit powers. The formulated optimization problem is non-convex, and addressed using the particle swarm optimization (PSO) algorithm. For performance benchmarking, two time division multiple access (TDMA) schemes are considered: one based on the pinching antenna individually activated for each user, and the other based on the single-pinching-antenna configuration serving all users. Numerical results demonstrate that the use of the pinching antenna significantly enhances the system sum rate compared to conventional antenna architectures. Moreover, the NOMA-based scheme outperforms the TDMA-based scheme with a single pinching antenna but is outperformed by the TDMA-based approach when the pinching antenna is adaptively configured for each user. Finally, the proposed PSO-based method is shown to achieve near-optimal performance for both NOMA and TDMA with a common pinching-antenna configuration.","authors":["Ming Zeng","Ji Wang","Xingwang Li","Gongpu Wang","Octavia A. Dobre","Zhiguo Ding"],"url":"https://arxiv.org/abs/2505.00549"}
{"created":"2025-05-02","title":"Bridging Cultural and Digital Divides: A Low-Latency JackTrip Framework for Equitable Music Education in the Global South","abstract":"The rapid expansion of digital technologies has transformed educational landscapes worldwide, yet significant infrastructural and cultural challenges persist in the Global South. This paper introduces a low-latency JackTrip framework designed to bridge both the cultural and digital divides in music education. By leveraging an open-source, UDP-based audio streaming protocol originally developed at Stanford's CCRMA, the framework is tailored to address technical constraints such as intermittent connectivity, limited bandwidth, and high latency that characterize many rural and underserved regions. The study systematically compares the performance of JackTrip with conventional platforms like Zoom, demonstrating that JackTrip achieves sub-30~ms latency under simulated low-resource conditions while preserving the intricate audio details essential for non-Western musical traditions. Spectral analysis confirms that JackTrip's superior handling of microtonal scales, complex rhythms, and harmonic textures provides a culturally authentic medium for real-time ensemble performance and music education. These findings underscore the transformative potential of decentralized, edge-computing solutions in empowering educators and musicians across the Global South, promoting both technological equity and cultural preservation.","authors":["Tiange Zhou","Marco Bidin"],"url":"https://arxiv.org/abs/2505.00550"}
{"created":"2025-05-02","title":"100 Days After DeepSeek-R1: A Survey on Replication Studies and More Directions for Reasoning Language Models","abstract":"The recent development of reasoning language models (RLMs) represents a novel evolution in large language models. In particular, the recent release of DeepSeek-R1 has generated widespread social impact and sparked enthusiasm in the research community for exploring the explicit reasoning paradigm of language models. However, the implementation details of the released models have not been fully open-sourced by DeepSeek, including DeepSeek-R1-Zero, DeepSeek-R1, and the distilled small models. As a result, many replication studies have emerged aiming to reproduce the strong performance achieved by DeepSeek-R1, reaching comparable performance through similar training procedures and fully open-source data resources. These works have investigated feasible strategies for supervised fine-tuning (SFT) and reinforcement learning from verifiable rewards (RLVR), focusing on data preparation and method design, yielding various valuable insights. In this report, we provide a summary of recent replication studies to inspire future research. We primarily focus on SFT and RLVR as two main directions, introducing the details for data construction, method design and training procedure of current replication studies. Moreover, we conclude key findings from the implementation details and experimental results reported by these studies, anticipating to inspire future research. We also discuss additional techniques of enhancing RLMs, highlighting the potential of expanding the application scope of these models, and discussing the challenges in development. By this survey, we aim to help researchers and developers of RLMs stay updated with the latest advancements, and seek to inspire new ideas to further enhance RLMs.","authors":["Chong Zhang","Yue Deng","Xiang Lin","Bin Wang","Dianwen Ng","Hai Ye","Xingxuan Li","Yao Xiao","Zhanfeng Mo","Qi Zhang","Lidong Bing"],"url":"https://arxiv.org/abs/2505.00551"}
{"created":"2025-05-02","title":"Graph Spectral Filtering with Chebyshev Interpolation for Recommendation","abstract":"Graph convolutional networks have recently gained prominence in collaborative filtering (CF) for recommendations. However, we identify potential bottlenecks in two foundational components. First, the embedding layer leads to a latent space with limited capacity, overlooking locally observed but potentially valuable preference patterns. Also, the widely-used neighborhood aggregation is limited in its ability to leverage diverse preference patterns in a fine-grained manner. Building on spectral graph theory, we reveal that these limitations stem from graph filtering with a cut-off in the frequency spectrum and a restricted linear form. To address these issues, we introduce ChebyCF, a CF framework based on graph spectral filtering. Instead of a learned embedding, it takes a user's raw interaction history to utilize the full spectrum of signals contained in it. Also, it adopts Chebyshev interpolation to effectively approximate a flexible non-linear graph filter, and further enhances it by using an additional ideal pass filter and degree-based normalization. Through extensive experiments, we verify that ChebyCF overcomes the aforementioned bottlenecks and achieves state-of-the-art performance across multiple benchmarks and reasonably fast inference. Our code is available at https://github.com/chanwoo0806/ChebyCF.","authors":["Chanwoo Kim","Jinkyu Sung","Yebonn Han","Joonseok Lee"],"url":"https://arxiv.org/abs/2505.00552"}
{"created":"2025-05-02","title":"Notes on Univariate Sumcheck","abstract":"These notes describe an adaptation of the multivariate sumcheck protocol to univariate polynomials interpolated over roots of unity.","authors":["Malcom Mohamed"],"url":"https://arxiv.org/abs/2505.00554"}
{"created":"2025-05-02","title":"Triggering Hallucinations in LLMs: A Quantitative Study of Prompt-Induced Hallucination in Large Language Models","abstract":"Hallucinations in large language models (LLMs) present a growing challenge across real-world applications, from healthcare to law, where factual reliability is essential. Despite advances in alignment and instruction tuning, LLMs can still generate outputs that are fluent yet fundamentally untrue. Understanding the cognitive dynamics that underlie these hallucinations remains an open problem. In this study, we propose a prompt-based framework to systematically trigger and quantify hallucination: a Hallucination-Inducing Prompt (HIP), which synthetically fuses semantically distant concepts (e.g., periodic table of elements and tarot divination) in a misleading way, and a Hallucination Quantifying Prompt (HQP), which scores the plausibility, confidence, and coherence of the output. Controlled experiments across multiple LLMs revealed that HIPs consistently produced less coherent and more hallucinated responses than their null-fusion controls. These effects varied across models, with reasoning-oriented LLMs showing distinct profiles from general-purpose ones. Our framework provides a reproducible testbed for studying hallucination vulnerability, and opens the door to developing safer, more introspective LLMs that can detect and self-regulate the onset of conceptual instability.","authors":["Makoto Sato"],"url":"https://arxiv.org/abs/2505.00557"}
{"created":"2025-05-02","title":"Exponentially Consistent Low Complexity Tests for Outlier Hypothesis Testing with Distribution Uncertainty","abstract":"We revisit the outlier hypothesis testing (OHT) problem of Li et al. (TIT 2024) and propose exponentially consistent tests when there is distribution uncertainty for both nominal samples and outliers. In original OHT, one is given a list of sequences, most of which are generated i.i.d. from a distribution called the nominal distribution while the rest are generated i.i.d. from another distribution named the anomalous distribution. The task of OHT is to identify outliers when both the nominal and anomalous distributions are unknown. Motivated by the study for classification with distribution uncertainty by Hsu and Wang (ISIT 2020), we consider OHT with distribution uncertainty, where each nominal sample is generated from a distribution centered around the unknown nominal distribution and each outlier is generated from a distribution centered around the unknown anomalous distribution. With a further step towards practical applications, in the spirit of Bu et al. (TSP 2019), we propose low-complexity tests when the number of outliers is known and unknown, and show that our proposed tests are exponentially consistent. Furthermore, we demonstrate that there is a penalty for not knowing the number of outliers in the error exponent when outliers exist. Our results strengthen Bu et al. in three aspects: i) our tests allow distribution uncertainty and reveal the impact of distribution uncertainty on the performance of low-complexity tests; ii) when the number of outliers is known and there is no distribution uncertainty, our test achieves the same asymptotic performance with lower complexity; and iii) when the number of outliers is unknown, we characterize the tradeoff among the three error probabilities, while two of these error probabilities were not analyzed by Bu et al. even when there is no distribution uncertainty. Finally, we illustrate our theoretical results using numerical examples.","authors":["Jun Diao","Jingjing Wang","Lin Zhou"],"url":"https://arxiv.org/abs/2505.00558"}
{"created":"2025-05-02","title":"Efficient Recommendation with Millions of Items by Dynamic Pruning of Sub-Item Embeddings","abstract":"A large item catalogue is a major challenge for deploying modern sequential recommender models, since it makes the memory footprint of the model large and increases inference latency. One promising approach to address this is RecJPQ, which replaces item embeddings with sub-item embeddings. However, slow inference remains problematic because finding the top highest-scored items usually requires scoring all items in the catalogue, which may not be feasible for large catalogues. By adapting dynamic pruning concepts from document retrieval, we propose the RecJPQPrune dynamic pruning algorithm to efficiently find the top highest-scored items without computing the scores of all items in the catalogue. Our RecJPQPrune algorithm is safe-up-to-rank K since it theoretically guarantees that no potentially high-scored item is excluded from the final top K recommendation list, thereby ensuring no impact on effectiveness. Our experiments on two large datasets and three recommendation models demonstrate the efficiency achievable using RecJPQPrune: for instance, on the Tmall dataset with 2.2M items, we can reduce the median model scoring time by 64 times compared to the Transformer Default baseline, and 5.3 times compared to a recent scoring approach called PQTopK. Overall, this paper demonstrates the effective and efficient inference of Transformer-based recommendation models at catalogue scales not previously reported in the literature. Indeed, our RecJPQPrune algorithm can score 2 million items in under 10 milliseconds without GPUs, and without relying on Approximate Nearest Neighbour (ANN) techniques.","authors":["Aleksandr V. Petrov","Craig Macdonald","Nicola Tonellotto"],"url":"https://arxiv.org/abs/2505.00560"}
{"created":"2025-05-02","title":"TeLoGraF: Temporal Logic Planning via Graph-encoded Flow Matching","abstract":"Learning to solve complex tasks with signal temporal logic (STL) specifications is crucial to many real-world applications. However, most previous works only consider fixed or parametrized STL specifications due to the lack of a diverse STL dataset and encoders to effectively extract temporal logic information for downstream tasks. In this paper, we propose TeLoGraF, Temporal Logic Graph-encoded Flow, which utilizes Graph Neural Networks (GNN) encoder and flow-matching to learn solutions for general STL specifications. We identify four commonly used STL templates and collect a total of 200K specifications with paired demonstrations. We conduct extensive experiments in five simulation environments ranging from simple dynamical models in the 2D space to high-dimensional 7DoF Franka Panda robot arm and Ant quadruped navigation. Results show that our method outperforms other baselines in the STL satisfaction rate. Compared to classical STL planning algorithms, our approach is 10-100X faster in inference and can work on any system dynamics. Besides, we show our graph-encoding method's capability to solve complex STLs and robustness to out-distribution STL specifications. Code is available at https://github.com/mengyuest/TeLoGraF","authors":["Yue Meng","Chuchu Fan"],"url":"https://arxiv.org/abs/2505.00562"}
{"created":"2025-05-02","title":"X-ray illicit object detection using hybrid CNN-transformer neural network architectures","abstract":"In the field of X-ray security applications, even the smallest details can significantly impact outcomes. Objects that are heavily occluded or intentionally concealed pose a great challenge for detection, whether by human observation or through advanced technological applications. While certain Deep Learning (DL) architectures demonstrate strong performance in processing local information, such as Convolutional Neural Networks (CNNs), others excel in handling distant information, e.g., transformers. In X-ray security imaging the literature has been dominated by the use of CNN-based methods, while the integration of the two aforementioned leading architectures has not been sufficiently explored. In this paper, various hybrid CNN-transformer architectures are evaluated against a common CNN object detection baseline, namely YOLOv8. In particular, a CNN (HGNetV2) and a hybrid CNN-transformer (Next-ViT-S) backbone are combined with different CNN/transformer detection heads (YOLOv8 and RT-DETR). The resulting architectures are comparatively evaluated on three challenging public X-ray inspection datasets, namely EDS, HiXray, and PIDray. Interestingly, while the YOLOv8 detector with its default backbone (CSP-DarkNet53) is generally shown to be advantageous on the HiXray and PIDray datasets, when a domain distribution shift is incorporated in the X-ray images (as happens in the EDS datasets), hybrid CNN-transformer architectures exhibit increased robustness. Detailed comparative evaluation results, including object-level detection performance and object-size error analysis, demonstrate the strengths and weaknesses of each architectural combination and suggest guidelines for future research. The source code and network weights of the models employed in this study are available at https://github.com/jgenc/xray-comparative-evaluation.","authors":["Jorgen Cani","Christos Diou","Spyridon Evangelatos","Panagiotis Radoglou-Grammatikis","Vasileios Argyriou","Panagiotis Sarigiannidis","Iraklis Varlamis","Georgios Th. Papadopoulos"],"url":"https://arxiv.org/abs/2505.00564"}
{"created":"2025-05-02","title":"Error Exponents for Oblivious Relaying and Connections to Source Coding with a Helper","abstract":"The information bottleneck channel, also known as oblivious relaying, is a two-hop channel where a transmitter sends messages to a remote receiver via an intermediate relay node. A codeword sent by the transmitter passes through a discrete memoryless channel to reach the relay, and then the relay processes the noisy channel output and forwards it to the receiver through a noiseless rate-limited link. The relay is oblivious, in the sense that it has no knowledge of the channel codebook used in transmission. Past works on oblivious relaying are focused on characterizing achievable rates. In this work, we study error exponents and explore connections to loseless source coding with a helper, also known as the Wyner-Ahlswede-K\\\"orner (WAK) problem. We first establish an achievable error exponent for oblivious relaying under constant compositions codes. A key feature of our analysis is the use of the type covering lemma to design the relay's compress-forward scheme. We then show that employing constant composition code ensembles does not improve the rates achieved with their IID counterparts. We also derive a sphere packing upper bound for the error exponent. In the second part of this paper, we establish a connection between the information bottleneck channel and the WAK problem. We show that good codes for the latter can be produced through permuting codes designed for the former. This is accomplished by revisiting Ahlswede's covering lemma, and extending it to achieve simultaneous covering of a type class by several distinct sets using the same sequence of permutations. We then apply our approach to attain the best known achievable error exponent for the WAK problem, previously established by Kelly and Wagner. As a byproduct of our derivations, we also establish error exponents and achievable rates under mismatched decoding rules.","authors":["Han Wu","Hamdi Joudeh"],"url":"https://arxiv.org/abs/2505.00567"}
{"created":"2025-05-02","title":"Multimodal Masked Autoencoder Pre-training for 3D MRI-Based Brain Tumor Analysis with Missing Modalities","abstract":"Multimodal magnetic resonance imaging (MRI) constitutes the first line of investigation for clinicians in the care of brain tumors, providing crucial insights for surgery planning, treatment monitoring, and biomarker identification. Pre-training on large datasets have been shown to help models learn transferable representations and adapt with minimal labeled data. This behavior is especially valuable in medical imaging, where annotations are often scarce. However, applying this paradigm to multimodal medical data introduces a challenge: most existing approaches assume that all imaging modalities are available during both pre-training and fine-tuning. In practice, missing modalities often occur due to acquisition issues, specialist unavailability, or specific experimental designs on small in-house datasets. Consequently, a common approach involves training a separate model for each desired modality combination, making the process both resource-intensive and impractical for clinical use. Therefore, we introduce BM-MAE, a masked image modeling pre-training strategy tailored for multimodal MRI data. The same pre-trained model seamlessly adapts to any combination of available modalities, extracting rich representations that capture both intra- and inter-modal information. This allows fine-tuning on any subset of modalities without requiring architectural changes, while still benefiting from a model pre-trained on the full set of modalities. Extensive experiments show that the proposed pre-training strategy outperforms or remains competitive with baselines that require separate pre-training for each modality subset, while substantially surpassing training from scratch on several downstream tasks. Additionally, it can quickly and efficiently reconstruct missing modalities, highlighting its practical value. Code and trained models are available at: https://github.com/Lucas-rbnt/bmmae","authors":["Lucas Robinet","Ahmad Berjaoui","Elizabeth Cohen-Jonathan Moyal"],"url":"https://arxiv.org/abs/2505.00568"}
{"created":"2025-05-02","title":"AnimalMotionCLIP: Embedding motion in CLIP for Animal Behavior Analysis","abstract":"Recently, there has been a surge of interest in applying deep learning techniques to animal behavior recognition, particularly leveraging pre-trained visual language models, such as CLIP, due to their remarkable generalization capacity across various downstream tasks. However, adapting these models to the specific domain of animal behavior recognition presents two significant challenges: integrating motion information and devising an effective temporal modeling scheme. In this paper, we propose AnimalMotionCLIP to address these challenges by interleaving video frames and optical flow information in the CLIP framework. Additionally, several temporal modeling schemes using an aggregation of classifiers are proposed and compared: dense, semi dense, and sparse. As a result, fine temporal actions can be correctly recognized, which is of vital importance in animal behavior analysis. Experiments on the Animal Kingdom dataset demonstrate that AnimalMotionCLIP achieves superior performance compared to state-of-the-art approaches.","authors":["Enmin Zhong","Carlos R. del-Blanco","Daniel Berj\\'on","Fernando Jaureguizar","Narciso Garc\\'ia"],"url":"https://arxiv.org/abs/2505.00569"}
{"created":"2025-05-02","title":"FreqKV: Frequency Domain Key-Value Compression for Efficient Context Window Extension","abstract":"Extending the context window in large language models (LLMs) is essential for applications involving long-form content generation. However, the linear increase in key-value (KV) cache memory requirements and the quadratic complexity of self-attention with respect to sequence length present significant challenges during fine-tuning and inference. Existing methods suffer from performance degradation when extending to longer contexts. In this work, we introduce a novel context extension method that optimizes both fine-tuning and inference efficiency. Our method exploits a key observation: in the frequency domain, the energy distribution of the KV cache is primarily concentrated in low-frequency components. By filtering out the high-frequency components, the KV cache can be effectively compressed with minimal information loss. Building on this insight, we propose an efficient compression technique, FreqKV, that iteratively compresses the increasing KV cache to a fixed size in the frequency domain, applicable to both fine-tuning and inference. FreqKV introduces no additional parameters or architectural modifications. With minimal fine-tuning, LLMs can learn to leverage the limited cache that is compressed in the frequency domain and extend the context window efficiently. Experiments on various long context language modeling and understanding tasks demonstrate the efficiency and efficacy of the proposed method.","authors":["Jushi Kai","Boyi Zeng","Yixuan Wang","Haoli Bai","Bo Jiang","Zhouhan Lin"],"url":"https://arxiv.org/abs/2505.00570"}
{"created":"2025-05-02","title":"Voice Cloning: Comprehensive Survey","abstract":"Voice Cloning has rapidly advanced in today's digital world, with many researchers and corporations working to improve these algorithms for various applications. This article aims to establish a standardized terminology for voice cloning and explore its different variations. It will cover speaker adaptation as the fundamental concept and then delve deeper into topics such as few-shot, zero-shot, and multilingual TTS within that context. Finally, we will explore the evaluation metrics commonly used in voice cloning research and related datasets. This survey compiles the available voice cloning algorithms to encourage research toward its generation and detection to limit its misuse.","authors":["Hussam Azzuni","Abdulmotaleb El Saddik"],"url":"https://arxiv.org/abs/2505.00579"}
{"created":"2025-05-02","title":"Parameter-Efficient Fine-Tuning with Circulant and Diagonal Vectors","abstract":"Foundation models have achieved tremendous success in different domains. However, their huge computation and storage complexity make these models difficult to fine-tune and also less applicable in practice. Recent study shows training in Fourier domain can be an effective fine-tuning method in terms of both model performance and number of training parameters. In this work, we propose to further reduce the complexity by the factorization through the product of interleaved circulant and diagonal matrices. In addition, we address the case of non-square fine-tuning weights by partitioning the circulant matrix into blocks. Our method avoids the construction of weight change matrix and utilizes 1D fast Fourier transform (FFT) instead of 2D FFT. Experimental results show that our method achieves similar or better performance across various tasks with much less floating-point operations (FLOPs) and the number of trainable parameters.","authors":["Xinyu Ding","Lexuan Chen","Siyu Liao","Zhongfeng Wang"],"url":"https://arxiv.org/abs/2505.00580"}
{"created":"2025-05-02","title":"Block Circulant Adapter for Large Language Models","abstract":"Fine-tuning large language models (LLMs) is difficult due to their huge model size. Recent Fourier domain-based methods show potential for reducing fine-tuning costs. We propose a block circulant matrix-based fine-tuning method with a stable training heuristic to leverage the properties of circulant matrices and one-dimensional Fourier transforms to reduce storage and computation costs. Experiments show that our method uses $14\\times$ less number of parameters than VeRA, $16\\times$ smaller than LoRA and $32\\times$ less FLOPs than FourierFT, while maintaining close or better task performance. Our approach presents a promising way in frequency domain to fine-tune large models on downstream tasks.","authors":["Xinyu Ding","Meiqi Wang","Siyu Liao","Zhongfeng Wang"],"url":"https://arxiv.org/abs/2505.00582"}
{"created":"2025-05-02","title":"Synthesizing and Identifying Noise Levels in Autonomous Vehicle Camera Radar Datasets","abstract":"Detecting and tracking objects is a crucial component of any autonomous navigation method. For the past decades, object detection has yielded promising results using neural networks on various datasets. While many methods focus on performance metrics, few projects focus on improving the robustness of these detection and tracking pipelines, notably to sensor failures. In this paper we attempt to address this issue by creating a realistic synthetic data augmentation pipeline for camera-radar Autonomous Vehicle (AV) datasets. Our goal is to accurately simulate sensor failures and data deterioration due to real-world interferences. We also present our results of a baseline lightweight Noise Recognition neural network trained and tested on our augmented dataset, reaching an overall recognition accuracy of 54.4\\% on 11 categories across 10086 images and 2145 radar point-clouds.","authors":["Mathis Morales","Golnaz Habibi"],"url":"https://arxiv.org/abs/2505.00584"}
{"created":"2025-05-02","title":"Dimension-reduced Optimization of Multi-zone Thermostatically Controlled Loads","abstract":"This study proposes a computationally efficient method for optimizing multi-zone thermostatically controlled loads (TCLs) by leveraging dimensionality reduction through an auto-encoder. We develop a multi-task learning framework to jointly represent latent variables and formulate a state-space model based on observed TCL operation data. This significantly reduces the dimensionality of TCL variables and states while preserving critical nonlinear interdependencies in TCL control. To address various application scenarios, we introduce optimization algorithms based on system identification (OptIden) and system simulation (OptSim) tailored to the latent variable representation. These approaches employ automatic differentiation and zeroth-order techniques, respectively, for efficient implementation. We evaluate the proposed method using a 90-zone apartment prototype, comparing its performance to traditional high-dimensional optimization. Results demonstrate that our approach effectively reduces control costs while achieving significantly higher computational efficiency.","authors":["Xueyuan Cui","Yi Wang","Bolun Xu"],"url":"https://arxiv.org/abs/2505.00585"}
{"created":"2025-05-02","title":"ParkDiffusion: Heterogeneous Multi-Agent Multi-Modal Trajectory Prediction for Automated Parking using Diffusion Models","abstract":"Automated parking is a critical feature of Advanced Driver Assistance Systems (ADAS), where accurate trajectory prediction is essential to bridge perception and planning modules. Despite its significance, research in this domain remains relatively limited, with most existing studies concentrating on single-modal trajectory prediction of vehicles. In this work, we propose ParkDiffusion, a novel approach that predicts the trajectories of both vehicles and pedestrians in automated parking scenarios. ParkDiffusion employs diffusion models to capture the inherent uncertainty and multi-modality of future trajectories, incorporating several key innovations. First, we propose a dual map encoder that processes soft semantic cues and hard geometric constraints using a two-step cross-attention mechanism. Second, we introduce an adaptive agent type embedding module, which dynamically conditions the prediction process on the distinct characteristics of vehicles and pedestrians. Third, to ensure kinematic feasibility, our model outputs control signals that are subsequently used within a kinematic framework to generate physically feasible trajectories. We evaluate ParkDiffusion on the Dragon Lake Parking (DLP) dataset and the Intersections Drone (inD) dataset. Our work establishes a new baseline for heterogeneous trajectory prediction in parking scenarios, outperforming existing methods by a considerable margin.","authors":["Jiarong Wei","Niclas V\\\"odisch","Anna Rehr","Christian Feist","Abhinav Valada"],"url":"https://arxiv.org/abs/2505.00586"}
{"created":"2025-05-02","title":"Unlocking the Potential of Linear Networks for Irregular Multivariate Time Series Forecasting","abstract":"Time series forecasting holds significant importance across various industries, including finance, transportation, energy, healthcare, and climate. Despite the widespread use of linear networks due to their low computational cost and effectiveness in modeling temporal dependencies, most existing research has concentrated on regularly sampled and fully observed multivariate time series. However, in practice, we frequently encounter irregular multivariate time series characterized by variable sampling intervals and missing values. The inherent intra-series inconsistency and inter-series asynchrony in such data hinder effective modeling and forecasting with traditional linear networks relying on static weights. To tackle these challenges, this paper introduces a novel model named AiT. AiT utilizes an adaptive linear network capable of dynamically adjusting weights according to observation time points to address intra-series inconsistency, thereby enhancing the accuracy of temporal dependencies modeling. Furthermore, by incorporating the Transformer module on variable semantics embeddings, AiT efficiently captures variable correlations, avoiding the challenge of inter-series asynchrony. Comprehensive experiments across four benchmark datasets demonstrate the superiority of AiT, improving prediction accuracy by 11% and decreasing runtime by 52% compared to existing state-of-the-art methods.","authors":["Chengsen Wang","Qi Qi","Jingyu Wang","Haifeng Sun","Zirui Zhuang","Jianxin Liao"],"url":"https://arxiv.org/abs/2505.00590"}
{"created":"2025-05-02","title":"Explainable AI in Spatial Analysis","abstract":"This chapter discusses the opportunities of eXplainable Artificial Intelligence (XAI) within the realm of spatial analysis. A key objective in spatial analysis is to model spatial relationships and infer spatial processes to generate knowledge from spatial data, which has been largely based on spatial statistical methods. More recently, machine learning offers scalable and flexible approaches that complement traditional methods and has been increasingly applied in spatial data science. Despite its advantages, machine learning is often criticized for being a black box, which limits our understanding of model behavior and output. Recognizing this limitation, XAI has emerged as a pivotal field in AI that provides methods to explain the output of machine learning models to enhance transparency and understanding. These methods are crucial for model diagnosis, bias detection, and ensuring the reliability of results obtained from machine learning models. This chapter introduces key concepts and methods in XAI with a focus on Shapley value-based approaches, which is arguably the most popular XAI method, and their integration with spatial analysis. An empirical example of county-level voting behaviors in the 2020 Presidential election is presented to demonstrate the use of Shapley values and spatial analysis with a comparison to multi-scale geographically weighted regression. The chapter concludes with a discussion on the challenges and limitations of current XAI techniques and proposes new directions.","authors":["Ziqi Li"],"url":"https://arxiv.org/abs/2505.00591"}
{"created":"2025-05-02","title":"Uncertainty-Aware Multi-Expert Knowledge Distillation for Imbalanced Disease Grading","abstract":"Automatic disease image grading is a significant application of artificial intelligence for healthcare, enabling faster and more accurate patient assessments. However, domain shifts, which are exacerbated by data imbalance, introduce bias into the model, posing deployment difficulties in clinical applications. To address the problem, we propose a novel \\textbf{U}ncertainty-aware \\textbf{M}ulti-experts \\textbf{K}nowledge \\textbf{D}istillation (UMKD) framework to transfer knowledge from multiple expert models to a single student model. Specifically, to extract discriminative features, UMKD decouples task-agnostic and task-specific features with shallow and compact feature alignment in the feature space. At the output space, an uncertainty-aware decoupled distillation (UDD) mechanism dynamically adjusts knowledge transfer weights based on expert model uncertainties, ensuring robust and reliable distillation. Additionally, UMKD also tackles the problems of model architecture heterogeneity and distribution discrepancies between source and target domains, which are inadequately tackled by previous KD approaches. Extensive experiments on histology prostate grading (\\textit{SICAPv2}) and fundus image grading (\\textit{APTOS}) demonstrate that UMKD achieves a new state-of-the-art in both source-imbalanced and target-imbalanced scenarios, offering a robust and practical solution for real-world disease image grading.","authors":["Shuo Tong","Shangde Gao","Ke Liu","Zihang Huang","Hongxia Xu","Haochao Ying","Jian Wu"],"url":"https://arxiv.org/abs/2505.00592"}
{"created":"2025-05-02","title":"A Novel Feature-Aware Chaotic Image Encryption Scheme For Data Security and Privacy in IoT and Edge Networks","abstract":"The security of image data in the Internet of Things (IoT) and edge networks is crucial due to the increasing deployment of intelligent systems for real-time decision-making. Traditional encryption algorithms such as AES and RSA are computationally expensive for resource-constrained IoT devices and ineffective for large-volume image data, leading to inefficiencies in privacy-preserving distributed learning applications. To address these concerns, this paper proposes a novel Feature-Aware Chaotic Image Encryption scheme that integrates Feature-Aware Pixel Segmentation (FAPS) with Chaotic Chain Permutation and Confusion mechanisms to enhance security while maintaining efficiency. The proposed scheme consists of three stages: (1) FAPS, which extracts and reorganizes pixels based on high and low edge intensity features for correlation disruption; (2) Chaotic Chain Permutation, which employs a logistic chaotic map with SHA-256-based dynamically updated keys for block-wise permutation; and (3) Chaotic chain Confusion, which utilises dynamically generated chaotic seed matrices for bitwise XOR operations. Extensive security and performance evaluations demonstrate that the proposed scheme significantly reduces pixel correlation -- almost zero, achieves high entropy values close to 8, and resists differential cryptographic attacks. The optimum design of the proposed scheme makes it suitable for real-time deployment in resource-constrained environments.","authors":["Muhammad Shahbaz Khan","Ahmed Al-Dubai","Jawad Ahmad","Nikolaos Pitropakis","Baraq Ghaleb"],"url":"https://arxiv.org/abs/2505.00593"}
{"created":"2025-05-02","title":"A Finite-State Controller Based Offline Solver for Deterministic POMDPs","abstract":"Deterministic partially observable Markov decision processes (DetPOMDPs) often arise in planning problems where the agent is uncertain about its environmental state but can act and observe deterministically. In this paper, we propose DetMCVI, an adaptation of the Monte Carlo Value Iteration (MCVI) algorithm for DetPOMDPs, which builds policies in the form of finite-state controllers (FSCs). DetMCVI solves large problems with a high success rate, outperforming existing baselines for DetPOMDPs. We also verify the performance of the algorithm in a real-world mobile robot forest mapping scenario.","authors":["Alex Schutz","Yang You","Matias Mattamala","Ipek Caliskanelli","Bruno Lacerda","Nick Hawes"],"url":"https://arxiv.org/abs/2505.00596"}
{"created":"2025-05-02","title":"Fast and Low-Cost Genomic Foundation Models via Outlier Removal","abstract":"We propose the first unified adversarial attack benchmark for Genomic Foundation Models (GFMs), named GERM. Unlike existing GFM benchmarks, GERM offers the first comprehensive evaluation framework to systematically assess the vulnerability of GFMs to adversarial attacks. Methodologically, we evaluate the adversarial robustness of five state-of-the-art GFMs using four widely adopted attack algorithms and three defense strategies. Importantly, our benchmark provides an accessible and comprehensive framework to analyze GFM vulnerabilities with respect to model architecture, quantization schemes, and training datasets. Empirically, transformer-based models exhibit greater robustness to adversarial perturbations compared to HyenaDNA, highlighting the impact of architectural design on vulnerability. Moreover, adversarial attacks frequently target biologically significant genomic regions, suggesting that these models effectively capture meaningful sequence features.","authors":["Haozheng Luo","Chenghao Qiu","Maojiang Su","Zhihan Zhou","Zoe Mehta","Guo Ye","Jerry Yao-Chieh Hu","Han Liu"],"url":"https://arxiv.org/abs/2505.00598"}
{"created":"2025-05-02","title":"Visual Trajectory Prediction of Vessels for Inland Navigation","abstract":"The future of inland navigation increasingly relies on autonomous systems and remote operations, emphasizing the need for accurate vessel trajectory prediction. This study addresses the challenges of video-based vessel tracking and prediction by integrating advanced object detection methods, Kalman filters, and spline-based interpolation. However, existing detection systems often misclassify objects in inland waterways due to complex surroundings. A comparative evaluation of tracking algorithms, including BoT-SORT, Deep OC-SORT, and ByeTrack, highlights the robustness of the Kalman filter in providing smoothed trajectories. Experimental results from diverse scenarios demonstrate improved accuracy in predicting vessel movements, which is essential for collision avoidance and situational awareness. The findings underline the necessity of customized datasets and models for inland navigation. Future work will expand the datasets and incorporate vessel classification to refine predictions, supporting both autonomous systems and human operators in complex environments.","authors":["Alexander Puzicha","Konstantin W\\\"ustefeld","Kathrin Wilms","Frank Weichert"],"url":"https://arxiv.org/abs/2505.00599"}
{"created":"2025-05-02","title":"Can LLMs Help Improve Analogical Reasoning For Strategic Decisions? Experimental Evidence from Humans and GPT-4","abstract":"This study investigates whether large language models, specifically GPT4, can match human capabilities in analogical reasoning within strategic decision making contexts. Using a novel experimental design involving source to target matching, we find that GPT4 achieves high recall by retrieving all plausible analogies but suffers from low precision, frequently applying incorrect analogies based on superficial similarities. In contrast, human participants exhibit high precision but low recall, selecting fewer analogies yet with stronger causal alignment. These findings advance theory by identifying matching, the evaluative phase of analogical reasoning, as a distinct step that requires accurate causal mapping beyond simple retrieval. While current LLMs are proficient in generating candidate analogies, humans maintain a comparative advantage in recognizing deep structural similarities across domains. Error analysis reveals that AI errors arise from surface level matching, whereas human errors stem from misinterpretations of causal structure. Taken together, the results suggest a productive division of labor in AI assisted organizational decision making where LLMs may serve as broad analogy generators, while humans act as critical evaluators, applying the most contextually appropriate analogies to strategic problems.","authors":["Phanish Puranam","Prothit Sen","Maciej Workiewicz"],"url":"https://arxiv.org/abs/2505.00603"}
{"created":"2025-05-02","title":"Surviving the Storm: The Impacts of Open RAN Disaggregation on Latency and Resilience","abstract":"The development of Open Radio Access Networks (Open RAN), with their disaggregated architectures and virtualization of network functions, has brought considerable flexibility and cost savings to mobile networks. However, these architectural advancements introduce additional latency during the initial attachment procedure of User Equipment (UE), increasing the risk of signaling storms. This paper investigates the latency impact due to disaggregation of the Base-band Unit (BBU) into the Central Unit (CU) and Distributed Unit (DU). Specifically, we model the delays induced due to disaggregation on UE attachment, analyzing the performance under varying load conditions, and sensitivity to processing times. We demonstrate that while both monolithic and Open RAN architectures experience performance degradation under high-load conditions, Open RAN's added overheads can increase its susceptibility to congestion and signaling storms. However, Open RAN's inherent flexibility, enabled by disaggregation and virtualization, allows efficient deployment of resources, faster service deployment, and adaptive congestion control mechanisms to mitigate these risks and enhance overall system resilience. Thereby, we quantify resilience by introducing a new utility function and propose a novel adaptation mechanism to reinforce Open RAN's robustness against signaling storms. Our results show that the proposed adaptive mechanism significantly enhances resilience, achieving improvements of up to 286% over fixed configurations, with resilience scores approaching 0.96 under optimal conditions. While simulation results show that Open RAN disaggregation increases attachment latency and susceptibility to signaling congestion, they also highlight that its architectural flexibility can mitigate these effects, improving resilience under high-load conditions.","authors":["Sotiris Chatzimiltis","Mohammad Shojafar","Mahdi Boloursaz Mashhadi","Rahim Tafazolli"],"url":"https://arxiv.org/abs/2505.00605"}
{"created":"2025-05-02","title":"Dietary Intake Estimation via Continuous 3D Reconstruction of Food","abstract":"Monitoring dietary habits is crucial for preventing health risks associated with overeating and undereating, including obesity, diabetes, and cardiovascular diseases. Traditional methods for tracking food intake rely on self-reported data before or after the eating, which are prone to inaccuracies. This study proposes an approach to accurately monitor ingest behaviours by leveraging 3D food models constructed from monocular 2D video. Using COLMAP and pose estimation algorithms, we generate detailed 3D representations of food, allowing us to observe changes in food volume as it is consumed. Experiments with toy models and real food items demonstrate the approach's potential. Meanwhile, we have proposed a new methodology for automated state recognition challenges to accurately detect state changes and maintain model fidelity. The 3D reconstruction approach shows promise in capturing comprehensive dietary behaviour insights, ultimately contributing to the development of automated and accurate dietary monitoring tools.","authors":["Wallace Lee","YuHao Chen"],"url":"https://arxiv.org/abs/2505.00606"}
{"created":"2025-05-02","title":"Combining LLMs with Logic-Based Framework to Explain MCTS","abstract":"In response to the lack of trust in Artificial Intelligence (AI) for sequential planning, we design a Computational Tree Logic-guided large language model (LLM)-based natural language explanation framework designed for the Monte Carlo Tree Search (MCTS) algorithm. MCTS is often considered challenging to interpret due to the complexity of its search trees, but our framework is flexible enough to handle a wide range of free-form post-hoc queries and knowledge-based inquiries centered around MCTS and the Markov Decision Process (MDP) of the application domain. By transforming user queries into logic and variable statements, our framework ensures that the evidence obtained from the search tree remains factually consistent with the underlying environmental dynamics and any constraints in the actual stochastic control process. We evaluate the framework rigorously through quantitative assessments, where it demonstrates strong performance in terms of accuracy and factual consistency.","authors":["Ziyan An","Xia Wang","Hendrik Baier","Zirong Chen","Abhishek Dubey","Taylor T. Johnson","Jonathan Sprinkle","Ayan Mukhopadhyay","Meiyi Ma"],"url":"https://arxiv.org/abs/2505.00610"}
{"created":"2025-05-02","title":"Position: AI Competitions Provide the Gold Standard for Empirical Rigor in GenAI Evaluation","abstract":"In this position paper, we observe that empirical evaluation in Generative AI is at a crisis point since traditional ML evaluation and benchmarking strategies are insufficient to meet the needs of evaluating modern GenAI models and systems. There are many reasons for this, including the fact that these models typically have nearly unbounded input and output spaces, typically do not have a well defined ground truth target, and typically exhibit strong feedback loops and prediction dependence based on context of previous model outputs. On top of these critical issues, we argue that the problems of {\\em leakage} and {\\em contamination} are in fact the most important and difficult issues to address for GenAI evaluations. Interestingly, the field of AI Competitions has developed effective measures and practices to combat leakage for the purpose of counteracting cheating by bad actors within a competition setting. This makes AI Competitions an especially valuable (but underutilized) resource. Now is time for the field to view AI Competitions as the gold standard for empirical rigor in GenAI evaluation, and to harness and harvest their results with according value.","authors":["D. Sculley","Will Cukierski","Phil Culliton","Sohier Dane","Maggie Demkin","Ryan Holbrook","Addison Howard","Paul Mooney","Walter Reade","Megan Risdal","Nate Keating"],"url":"https://arxiv.org/abs/2505.00612"}
{"created":"2025-05-02","title":"Pixel3DMM: Versatile Screen-Space Priors for Single-Image 3D Face Reconstruction","abstract":"We address the 3D reconstruction of human faces from a single RGB image. To this end, we propose Pixel3DMM, a set of highly-generalized vision transformers which predict per-pixel geometric cues in order to constrain the optimization of a 3D morphable face model (3DMM). We exploit the latent features of the DINO foundation model, and introduce a tailored surface normal and uv-coordinate prediction head. We train our model by registering three high-quality 3D face datasets against the FLAME mesh topology, which results in a total of over 1,000 identities and 976K images. For 3D face reconstruction, we propose a FLAME fitting opitmization that solves for the 3DMM parameters from the uv-coordinate and normal estimates. To evaluate our method, we introduce a new benchmark for single-image face reconstruction, which features high diversity facial expressions, viewing angles, and ethnicities. Crucially, our benchmark is the first to evaluate both posed and neutral facial geometry. Ultimately, our method outperforms the most competitive baselines by over 15% in terms of geometric accuracy for posed facial expressions.","authors":["Simon Giebenhain","Tobias Kirschstein","Martin R\\\"unz","Lourdes Agapito","Matthias Nie{\\ss}ner"],"url":"https://arxiv.org/abs/2505.00615"}
{"created":"2025-05-02","title":"Catastrophic Liability: Managing Systemic Risks in Frontier AI Development","abstract":"As artificial intelligence systems grow more capable and autonomous, frontier AI development poses potential systemic risks that could affect society at a massive scale. Current practices at many AI labs developing these systems lack sufficient transparency around safety measures, testing procedures, and governance structures. This opacity makes it challenging to verify safety claims or establish appropriate liability when harm occurs. Drawing on liability frameworks from nuclear energy, aviation software, and healthcare, we propose a comprehensive approach to safety documentation and accountability in frontier AI development.","authors":["Aidan Kierans","Kaley Rittichier","Utku Sonsayar"],"url":"https://arxiv.org/abs/2505.00616"}
{"created":"2025-05-02","title":"RevealNet: Distributed Traffic Correlation for Attack Attribution on Programmable Networks","abstract":"Network attackers have increasingly resorted to proxy chains, VPNs, and anonymity networks to conceal their activities. To tackle this issue, past research has explored the applicability of traffic correlation techniques to perform attack attribution, i.e., to identify an attacker's true network location. However, current traffic correlation approaches rely on well-provisioned and centralized systems that ingest flows from multiple network probes to compute correlation scores. Unfortunately, this makes correlation efforts scale poorly for large high-speed networks.","authors":["Gurjot Singh","Alim Dhanani","Diogo Barradas"],"url":"https://arxiv.org/abs/2505.00618"}
{"created":"2025-05-02","title":"Diverse Semantics-Guided Feature Alignment and Decoupling for Visible-Infrared Person Re-Identification","abstract":"Visible-Infrared Person Re-Identification (VI-ReID) is a challenging task due to the large modality discrepancy between visible and infrared images, which complicates the alignment of their features into a suitable common space. Moreover, style noise, such as illumination and color contrast, reduces the identity discriminability and modality invariance of features. To address these challenges, we propose a novel Diverse Semantics-guided Feature Alignment and Decoupling (DSFAD) network to align identity-relevant features from different modalities into a textual embedding space and disentangle identity-irrelevant features within each modality. Specifically, we develop a Diverse Semantics-guided Feature Alignment (DSFA) module, which generates pedestrian descriptions with diverse sentence structures to guide the cross-modality alignment of visual features. Furthermore, to filter out style information, we propose a Semantic Margin-guided Feature Decoupling (SMFD) module, which decomposes visual features into pedestrian-related and style-related components, and then constrains the similarity between the former and the textual embeddings to be at least a margin higher than that between the latter and the textual embeddings. Additionally, to prevent the loss of pedestrian semantics during feature decoupling, we design a Semantic Consistency-guided Feature Restitution (SCFR) module, which further excavates useful information for identification from the style-related features and restores it back into the pedestrian-related features, and then constrains the similarity between the features after restitution and the textual embeddings to be consistent with that between the features before decoupling and the textual embeddings. Extensive experiments on three VI-ReID datasets demonstrate the superiority of our DSFAD.","authors":["Neng Dong","Shuanglin Yan","Liyan Zhang","Jinhui Tang"],"url":"https://arxiv.org/abs/2505.00619"}
{"created":"2025-05-02","title":"Beyond Affine Loops: A Geometric Approach to Program Synthesis","abstract":"Ensuring software correctness remains a fundamental challenge in formal program verification. One promising approach relies on finding polynomial invariants for loops. Polynomial invariants are properties of a program loop that hold before and after each iteration. Generating polynomial invariants is a crucial task for loops, but it is an undecidable problem in the general case. Recently, an alternative approach to this problem has emerged, focusing on synthesizing loops from invariants. However, existing methods only synthesize affine loops without guard conditions from polynomial invariants. In this paper, we address a more general problem, allowing loops to have polynomial update maps with a given structure, inequations in the guard condition, and polynomial invariants of arbitrary form.","authors":["Erdenebayar Bayarmagnai","Fatemeh Mohammadi","R\\'emi Pr\\'ebet"],"url":"https://arxiv.org/abs/2505.00620"}
{"created":"2025-05-02","title":"Neural Network Verification for Gliding Drone Control: A Case Study","abstract":"As machine learning is increasingly deployed in autonomous systems, verification of neural network controllers is becoming an active research domain. Existing tools and annual verification competitions suggest that soon this technology will become effective for real-world applications. Our application comes from the emerging field of microflyers that are passively transported by the wind, which may have various uses in weather or pollution monitoring. Specifically, we investigate centimetre-scale bio-inspired gliding drones that resemble Alsomitra macrocarpa diaspores. In this paper, we propose a new case study on verifying Alsomitra-inspired drones with neural network controllers, with the aim of adhering closely to a target trajectory. We show that our system differs substantially from existing VNN and ARCH competition benchmarks, and show that a combination of tools holds promise for verifying such systems in the future, if certain shortcomings can be overcome. We propose a novel method for robust training of regression networks, and investigate formalisations of this case study in Vehicle and CORA. Our verification results suggest that the investigated training methods do improve performance and robustness of neural network controllers in this application, but are limited in scope and usefulness. This is due to systematic limitations of both Vehicle and CORA, and the complexity of our system reducing the scale of reachability, which we investigate in detail. If these limitations can be overcome, it will enable engineers to develop safe and robust technologies that improve people's lives and reduce our impact on the environment.","authors":["Colin Kessler","Ekaterina Komendantskaya","Marco Casadio","Ignazio Maria Viola","Thomas Flinkow","Albaraa Ammar Othman","Alistair Malhotra","Robbie McPherson"],"url":"https://arxiv.org/abs/2505.00622"}
{"created":"2025-05-02","title":"FineScope : Precision Pruning for Domain-Specialized Large Language Models Using SAE-Guided Self-Data Cultivation","abstract":"Training large language models (LLMs) from scratch requires significant computational resources, driving interest in developing smaller, domain-specific LLMs that maintain both efficiency and strong task performance. Medium-sized models such as LLaMA, llama} have served as starting points for domain-specific adaptation, but they often suffer from accuracy degradation when tested on specialized datasets. We introduce FineScope, a framework for deriving compact, domain-optimized LLMs from larger pretrained models. FineScope leverages the Sparse Autoencoder (SAE) framework, inspired by its ability to produce interpretable feature representations, to extract domain-specific subsets from large datasets. We apply structured pruning with domain-specific constraints, ensuring that the resulting pruned models retain essential knowledge for the target domain. To further enhance performance, these pruned models undergo self-data distillation, leveraging SAE-curated datasets to restore key domain-specific information lost during pruning. Extensive experiments and ablation studies demonstrate that FineScope achieves highly competitive performance, outperforming several large-scale state-of-the-art LLMs in domain-specific tasks. Additionally, our results show that FineScope enables pruned models to regain a substantial portion of their original performance when fine-tuned with SAE-curated datasets. Furthermore, applying these datasets to fine-tune pretrained LLMs without pruning also improves their domain-specific accuracy, highlighting the robustness of our approach. The code will be released.","authors":["Chaitali Bhattacharyya","Yeseong Kim"],"url":"https://arxiv.org/abs/2505.00624"}
{"created":"2025-05-02","title":"The Illusion of Role Separation: Hidden Shortcuts in LLM Role Learning (and How to Fix Them)","abstract":"Large language models (LLMs) that integrate multiple input roles (e.g., system instructions, user queries, external tool outputs) are increasingly prevalent in practice. Ensuring that the model accurately distinguishes messages from each role -- a concept we call \\emph{role separation} -- is crucial for consistent multi-role behavior. Although recent work often targets state-of-the-art prompt injection defenses, it remains unclear whether such methods truly teach LLMs to differentiate roles or merely memorize known triggers. In this paper, we examine \\emph{role-separation learning}: the process of teaching LLMs to robustly distinguish system and user tokens. Through a \\emph{simple, controlled experimental framework}, we find that fine-tuned models often rely on two proxies for role identification: (1) task type exploitation, and (2) proximity to begin-of-text. Although data augmentation can partially mitigate these shortcuts, it generally leads to iterative patching rather than a deeper fix. To address this, we propose reinforcing \\emph{invariant signals} that mark role boundaries by adjusting token-wise cues in the model's input encoding. In particular, manipulating position IDs helps the model learn clearer distinctions and reduces reliance on superficial proxies. By focusing on this mechanism-centered perspective, our work illuminates how LLMs can more reliably maintain consistent multi-role behavior without merely memorizing known prompts or triggers.","authors":["Zihao Wang","Yibo Jiang","Jiahao Yu","Heqing Huang"],"url":"https://arxiv.org/abs/2505.00626"}
{"created":"2025-05-02","title":"Brain Foundation Models with Hypergraph Dynamic Adapter for Brain Disease Analysis","abstract":"Brain diseases, such as Alzheimer's disease and brain tumors, present profound challenges due to their complexity and societal impact. Recent advancements in brain foundation models have shown significant promise in addressing a range of brain-related tasks. However, current brain foundation models are limited by task and data homogeneity, restricted generalization beyond segmentation or classification, and inefficient adaptation to diverse clinical tasks. In this work, we propose SAM-Brain3D, a brain-specific foundation model trained on over 66,000 brain image-label pairs across 14 MRI sub-modalities, and Hypergraph Dynamic Adapter (HyDA), a lightweight adapter for efficient and effective downstream adaptation. SAM-Brain3D captures detailed brain-specific anatomical and modality priors for segmenting diverse brain targets and broader downstream tasks. HyDA leverages hypergraphs to fuse complementary multi-modal data and dynamically generate patient-specific convolutional kernels for multi-scale feature fusion and personalized patient-wise adaptation. Together, our framework excels across a broad spectrum of brain disease segmentation and classification tasks. Extensive experiments demonstrate that our method consistently outperforms existing state-of-the-art approaches, offering a new paradigm for brain disease analysis through multi-modal, multi-scale, and dynamic foundation modeling.","authors":["Zhongying Deng","Haoyu Wang","Ziyan Huang","Lipei Zhang","Angelica I. Aviles-Rivero","Chaoyu Liu","Junjun He","Zoe Kourtzi","Carola-Bibiane Sch\\\"onlieb"],"url":"https://arxiv.org/abs/2505.00627"}
{"created":"2025-05-02","title":"Vision Mamba in Remote Sensing: A Comprehensive Survey of Techniques, Applications and Outlook","abstract":"Deep learning has profoundly transformed remote sensing, yet prevailing architectures like Convolutional Neural Networks (CNNs) and Vision Transformers (ViTs) remain constrained by critical trade-offs: CNNs suffer from limited receptive fields, while ViTs grapple with quadratic computational complexity, hindering their scalability for high-resolution remote sensing data. State Space Models (SSMs), particularly the recently proposed Mamba architecture, have emerged as a paradigm-shifting solution, combining linear computational scaling with global context modeling. This survey presents a comprehensive review of Mamba-based methodologies in remote sensing, systematically analyzing about 120 studies to construct a holistic taxonomy of innovations and applications. Our contributions are structured across five dimensions: (i) foundational principles of vision Mamba architectures, (ii) micro-architectural advancements such as adaptive scan strategies and hybrid SSM formulations, (iii) macro-architectural integrations, including CNN-Transformer-Mamba hybrids and frequency-domain adaptations, (iv) rigorous benchmarking against state-of-the-art methods in multiple application tasks, such as object detection, semantic segmentation, change detection, etc. and (v) critical analysis of unresolved challenges with actionable future directions. By bridging the gap between SSM theory and remote sensing practice, this survey establishes Mamba as a transformative framework for remote sensing analysis. To our knowledge, this paper is the first systematic review of Mamba architectures in remote sensing. Our work provides a structured foundation for advancing research in remote sensing systems through SSM-based methods. We curate an open-source repository (https://github.com/BaoBao0926/Awesome-Mamba-in-Remote-Sensing) to foster community-driven advancements.","authors":["Muyi Bao","Shuchang Lyu","Zhaoyang Xu","Huiyu Zhou","Jinchang Ren","Shiming Xiang","Xiangtai Li","Guangliang Cheng"],"url":"https://arxiv.org/abs/2505.00630"}
{"created":"2025-05-02","title":"Forward kinematics of a general Stewart-Gough platform by elimination templates","abstract":"The paper proposes an efficient algebraic solution to the problem of forward kinematics for a general Stewart-Gough platform. The problem involves determining all possible postures of a mobile platform connected to a fixed base by six legs, given the leg lengths and the internal geometries of the platform and base. The problem is known to have 40 solutions (whether real or complex). The proposed algorithm consists of three main steps: (i) a specific sparse matrix of size 293x362 (the elimination template) is constructed from the coefficients of the polynomial system describing the platform's kinematics; (ii) the PLU decomposition of this matrix is used to construct a pair of 69x69 matrices; (iii) all 40 solutions (including complex ones) are obtained by computing the generalized eigenvectors of this matrix pair. The proposed algorithm is numerically robust, computationally efficient, and straightforward to implement - requiring only standard linear algebra decompositions. MATLAB, Julia, and Python implementations of the algorithm will be made publicly available.","authors":["Evgeniy Martyushev"],"url":"https://arxiv.org/abs/2505.00634"}
{"created":"2025-05-02","title":"GeoDEx: A Unified Geometric Framework for Tactile Dexterous and Extrinsic Manipulation under Force Uncertainty","abstract":"Sense of touch that allows robots to detect contact and measure interaction forces enables them to perform challenging tasks such as grasping fragile objects or using tools. Tactile sensors in theory can equip the robots with such capabilities. However, accuracy of the measured forces is not on a par with those of the force sensors due to the potential calibration challenges and noise. This has limited the values these sensors can offer in manipulation applications that require force control. In this paper, we introduce GeoDEx, a unified estimation, planning, and control framework using geometric primitives such as plane, cone and ellipsoid, which enables dexterous as well as extrinsic manipulation in the presence of uncertain force readings. Through various experimental results, we show that while relying on direct inaccurate and noisy force readings from tactile sensors results in unstable or failed manipulation, our method enables successful grasping and extrinsic manipulation of different objects. Additionally, compared to directly running optimization using SOCP (Second Order Cone Programming), planning and force estimation using our framework achieves a 14x speed-up.","authors":["Sirui Chen","Sergio Aguilera Marinovic","Soshi Iba","Rana Soltani Zarrin"],"url":"https://arxiv.org/abs/2505.00647"}
{"created":"2025-05-02","title":"Adaptive Nonoverlapping Preconditioners for the Helmholtz Equation","abstract":"The Helmholtz equation poses significant computational challenges due to its oscillatory solutions, particularly for large wavenumbers. Inspired by the Schur complement system for elliptic problems, this paper presents a novel substructuring approach to mitigate the potential ill-posedness of local Dirichlet problems for the Helmholtz equation. We propose two types of preconditioners within the framework of nonoverlapping spectral additive Schwarz (NOSAS) methods. The first type of preconditioner focuses on the real part of the Helmholtz problem, while the second type addresses both the real and imaginary components, providing a comprehensive strategy to enhance scalability and reduce computational cost. Our approach is purely algebraic, which allows for adaptability to various discretizations and heterogeneous Helmholtz coefficients while maintaining theoretical convergence for thresholds close to zero. Numerical experiments confirm the effectiveness of the proposed preconditioners, demonstrating robust convergence rates and scalability, even for large wavenumbers.","authors":["Yi Yu","Marcus Sarkis","Guanglian Li","Zhiwen Zhang"],"url":"https://arxiv.org/abs/2505.00648"}
{"created":"2025-05-02","title":"Investigating Task Arithmetic for Zero-Shot Information Retrieval","abstract":"Large Language Models (LLMs) have shown impressive zero-shot performance across a variety of Natural Language Processing tasks, including document re-ranking. However, their effectiveness degrades on unseen tasks and domains, largely due to shifts in vocabulary and word distributions. In this paper, we investigate Task Arithmetic, a technique that combines the weights of LLMs pre-trained on different tasks or domains via simple mathematical operations, such as addition or subtraction, to adapt retrieval models without requiring additional fine-tuning. Our method is able to synthesize diverse tasks and domain knowledge into a single model, enabling effective zero-shot adaptation in different retrieval contexts. Extensive experiments on publicly available scientific, biomedical, and multilingual datasets show that our method improves state-of-the-art re-ranking performance by up to 18% in NDCG@10 and 15% in P@10. In addition to these empirical gains, our analysis provides insights into the strengths and limitations of Task Arithmetic as a practical strategy for zero-shot learning and model adaptation. We make our code publicly available at https://github.com/DetectiveMB/Task-Arithmetic-for-ZS-IR.","authors":["Marco Braga","Pranav Kasela","Alessandro Raganato","Gabriella Pasi"],"url":"https://arxiv.org/abs/2505.00649"}
{"created":"2025-05-02","title":"OmicsCL: Unsupervised Contrastive Learning for Cancer Subtype Discovery and Survival Stratification","abstract":"Unsupervised learning of disease subtypes from multi-omics data presents a significant opportunity for advancing personalized medicine. We introduce OmicsCL, a modular contrastive learning framework that jointly embeds heterogeneous omics modalities-such as gene expression, DNA methylation, and miRNA expression-into a unified latent space. Our method incorporates a survival-aware contrastive loss that encourages the model to learn representations aligned with survival-related patterns, without relying on labeled outcomes. Evaluated on the TCGA BRCA dataset, OmicsCL uncovers clinically meaningful clusters and achieves strong unsupervised concordance with patient survival. The framework demonstrates robustness across hyperparameter configurations and can be tuned to prioritize either subtype coherence or survival stratification. Ablation studies confirm that integrating survival-aware loss significantly enhances the predictive power of learned embeddings. These results highlight the promise of contrastive objectives for biological insight discovery in high-dimensional, heterogeneous omics data.","authors":["Atahan Karagoz"],"url":"https://arxiv.org/abs/2505.00650"}
{"created":"2025-05-02","title":"Open-Source LLM-Driven Federated Transformer for Predictive IoV Management","abstract":"The proliferation of connected vehicles within the Internet of Vehicles (IoV) ecosystem presents critical challenges in ensuring scalable, real-time, and privacy-preserving traffic management. Existing centralized IoV solutions often suffer from high latency, limited scalability, and reliance on proprietary Artificial Intelligence (AI) models, creating significant barriers to widespread deployment, particularly in dynamic and privacy-sensitive environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular systems remains underexplored, especially concerning prompt optimization and effective utilization in federated contexts. To address these challenges, we propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel framework that leverages open-source LLMs for predictive IoV management. FPoTT introduces a dynamic prompt optimization mechanism that iteratively refines textual prompts to enhance trajectory prediction. The architecture employs a dual-layer federated learning paradigm, combining lightweight edge models for real-time inference with cloud-based LLMs to retain global intelligence. A Transformer-driven synthetic data generator is incorporated to augment training with diverse, high-fidelity traffic scenarios in the Next Generation Simulation (NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data while maintaining high performance on synthetic datasets. These results underscore the potential of open-source LLMs in enabling secure, adaptive, and scalable IoV management, offering a promising alternative to proprietary solutions in smart mobility ecosystems.","authors":["Yazan Otoum","Arghavan Asad","Ishtiaq Ahmad"],"url":"https://arxiv.org/abs/2505.00651"}
{"created":"2025-05-02","title":"Large Language Models Understanding: an Inherent Ambiguity Barrier","abstract":"A lively ongoing debate is taking place, since the extraordinary emergence of Large Language Models (LLMs) with regards to their capability to understand the world and capture the meaning of the dialogues in which they are involved. Arguments and counter-arguments have been proposed based upon thought experiments, anecdotal conversations between LLMs and humans, statistical linguistic analysis, philosophical considerations, and more. In this brief paper we present a counter-argument based upon a thought experiment and semi-formal considerations leading to an inherent ambiguity barrier which prevents LLMs from having any understanding of what their amazingly fluent dialogues mean.","authors":["Daniel N. Nissani (Nissensohn)"],"url":"https://arxiv.org/abs/2505.00654"}
{"created":"2025-05-02","title":"RIS Partitioning and User Clustering for Resilient Non-Orthogonal Multiple Access UAV Networks","abstract":"The integration of reconfigurable intelligent surfaces (RISs) and unmanned aerial vehicles (UAVs) has emerged as a promising solution for enhancing connectivity in future wireless networks. This paper designs well-connected and resilient UAV networks by deploying and virtually partitioning multiple RISs to create multiple RIS-aided links, focusing on a link-layer perspective. The RIS-aided links are created to connect user equipment (UE) to blocked and reliable UAVs, where multiple UEs can transmit to same UAV via RIS using non-orthogonal multiple access (NOMA), granting access to UEs and maximizing network connectivity. We first derive exact and approximated closed-form expressions for signal-to-interference plus noise ratio (SINR) based on aligned and non-aligned RIS-aided beams. Then, we propose to formulate the problem of maximizing network connectivity that jointly considers (i) UE NOMA clustering, (ii) RIS-aided link selection, and (ii) virtual RIS partitioning. This problem is a computationally expensive combinatorial optimization. To tackle this problem, a two-step iterative approach, called RIS-aided NOMA, is proposed. In the first step, the UEs are clustered to the RISs according to their channel gains, while UAVs are associated to those generated clusters based on their reliability, which measures the criticality of UAVs. The second step optimally partitions the RISs to support each of the cluster members. In this step, we derive the closed-form equations for the optimal partitioning of RISs within the clusters. Simulation results demonstrate that the proposed RIS-aided NOMA yields a gain of 30% to 40%, respectively, compared to UAV traditional scheme. The finding emphasizes the potential of integrating RIS with UAV communications as a robust and reliable connectivity solution for future wireless communication systems.","authors":["Mohammed Saif","Shahrokh Valaee"],"url":"https://arxiv.org/abs/2505.00658"}
{"created":"2025-05-02","title":"AI-based CSI Feedback with Digital Twins: Real-World Validation and Insights","abstract":"Deep learning (DL) has shown great potential for enhancing channel state information (CSI) feedback in multiple-input multiple-output (MIMO) communication systems, a subject currently under study by the 3GPP standards body. Digital twins (DTs) have emerged as an effective means to generate site-specific datasets for training DL-based CSI feedback models. However, most existing studies rely solely on simulations, leaving the effectiveness of DTs in reducing DL training costs yet to be validated through realistic experimental setups. This paper addresses this gap by establishing a real-world (RW) environment and corresponding virtual channels using ray tracing with replicated 3D models and accurate antenna properties. We evaluate whether models trained in DT environments can effectively operate in RW scenarios and quantify the benefits of online learning (OL) for performance enhancement. Results show that a dedicated DT remains essential even with OL to achieve satisfactory performance in RW scenarios.","authors":["Tzu-Hao Huang","Chao-Kai Wen","Shang-Ho Tsai","Trung Q. Duong"],"url":"https://arxiv.org/abs/2505.00660"}
{"created":"2025-05-02","title":"On the generalization of language models from in-context learning and finetuning: a controlled study","abstract":"Large language models exhibit exciting capabilities, yet can show surprisingly narrow generalization from finetuning -- from failing to generalize to simple reversals of relations they are trained on, to missing logical deductions that can be made from trained information. These failures to generalize from fine-tuning can hinder practical application of these models. However, language models' in-context learning shows different inductive biases, and can generalize better in some of these cases. Here, we explore these differences in generalization between in-context- and fine-tuning-based learning. To do so, we constructed several novel datasets to evaluate and improve models' ability to generalize from finetuning data. The datasets are constructed to isolate the knowledge in the dataset from that in pretraining, to create clean tests of generalization. We expose pretrained large models to controlled subsets of the information in these datasets -- either in context, or through fine-tuning -- and evaluate their performance on test sets that require various types of generalization. We find overall that in data-matched settings, in-context learning can generalize more flexibly than fine-tuning (though we also find some qualifications of prior findings, such as cases when fine-tuning can generalize to reversals embedded in a larger structure of knowledge). We build on these findings to propose a method to enable improved generalization from fine-tuning: adding in-context inferences to finetuning data. We show that this method improves generalization across various splits of our datasets and other benchmarks. Our results have implications for understanding the inductive biases of different modes of learning in language models, and practically improving their performance.","authors":["Andrew K. Lampinen","Arslan Chaudhry","Stephanie C. Y. Chan","Cody Wild","Diane Wan","Alex Ku","J\\\"org Bornschein","Razvan Pascanu","Murray Shanahan","James L. McClelland"],"url":"https://arxiv.org/abs/2505.00661"}
{"created":"2025-05-02","title":"DeepCritic: Deliberate Critique with Large Language Models","abstract":"As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback.","authors":["Wenkai Yang","Jingwen Chen","Yankai Lin","Ji-Rong Wen"],"url":"https://arxiv.org/abs/2505.00662"}
{"created":"2025-05-02","title":"Wasserstein Policy Optimization","abstract":"We introduce Wasserstein Policy Optimization (WPO), an actor-critic algorithm for reinforcement learning in continuous action spaces. WPO can be derived as an approximation to Wasserstein gradient flow over the space of all policies projected into a finite-dimensional parameter space (e.g., the weights of a neural network), leading to a simple and completely general closed-form update. The resulting algorithm combines many properties of deterministic and classic policy gradient methods. Like deterministic policy gradients, it exploits knowledge of the gradient of the action-value function with respect to the action. Like classic policy gradients, it can be applied to stochastic policies with arbitrary distributions over actions -- without using the reparameterization trick. We show results on the DeepMind Control Suite and a magnetic confinement fusion task which compare favorably with state-of-the-art continuous control methods.","authors":["David Pfau","Ian Davies","Diana Borsa","Joao G. M. Araujo","Brendan Tracey","Hado van Hasselt"],"url":"https://arxiv.org/abs/2505.00663"}
{"created":"2025-05-02","title":"Auditing without Leaks Despite Curiosity","abstract":"\\textit{Auditing} data accesses helps preserve privacy and ensures accountability by allowing one to determine who accessed (potentially sensitive) information. A prior formal definition of register auditability was based on the values returned by read operations, \\emph{without accounting for cases where a reader might learn a value without explicitly reading it or gain knowledge of data access without being an auditor}.","authors":["Hagit Attiya","Antonio Fern\\'andez Anta","Alessia Milani","Alexandre Rapetti","Corentin Travers"],"url":"https://arxiv.org/abs/2505.00665"}
{"created":"2025-05-02","title":"Deep Reinforcement Learning for Urban Air Quality Management: Multi-Objective Optimization of Pollution Mitigation Booth Placement in Metropolitan Environments","abstract":"Urban air pollution remains a pressing global concern, particularly in densely populated and traffic-intensive metropolitan areas like Delhi, where exposure to harmful pollutants severely impacts public health. Delhi, being one of the most polluted cities globally, experiences chronic air quality issues due to vehicular emissions, industrial activities, and construction dust, which exacerbate its already fragile atmospheric conditions. Traditional pollution mitigation strategies, such as static air purifying installations, often fail to maximize their impact due to suboptimal placement and limited adaptability to dynamic urban environments. This study presents a novel deep reinforcement learning (DRL) framework to optimize the placement of air purification booths to improve the air quality index (AQI) in the city of Delhi. We employ Proximal Policy Optimization (PPO), a state-of-the-art reinforcement learning algorithm, to iteratively learn and identify high-impact locations based on multiple spatial and environmental factors, including population density, traffic patterns, industrial influence, and green space constraints. Our approach is benchmarked against conventional placement strategies, including random and greedy AQI-based methods, using multi-dimensional performance evaluation metrics such as AQI improvement, spatial coverage, population and traffic impact, and spatial entropy. Experimental results demonstrate that the RL-based approach outperforms baseline methods by achieving a balanced and effective distribution of air purification infrastructure. Notably, the DRL framework achieves an optimal trade-off between AQI reduction and high-coverage deployment, ensuring equitable environmental benefits across urban regions. The findings underscore the potential of AI-driven spatial optimization in advancing smart city initiatives and data-driven urban air quality management.","authors":["Kirtan Rajesh","Suvidha Rupesh Kumar"],"url":"https://arxiv.org/abs/2505.00668"}
{"created":"2025-05-02","title":"Multi-Constraint Safe Reinforcement Learning via Closed-form Solution for Log-Sum-Exp Approximation of Control Barrier Functions","abstract":"The safety of training task policies and their subsequent application using reinforcement learning (RL) methods has become a focal point in the field of safe RL. A central challenge in this area remains the establishment of theoretical guarantees for safety during both the learning and deployment processes. Given the successful implementation of Control Barrier Function (CBF)-based safety strategies in a range of control-affine robotic systems, CBF-based safe RL demonstrates significant promise for practical applications in real-world scenarios. However, integrating these two approaches presents several challenges. First, embedding safety optimization within the RL training pipeline requires that the optimization outputs be differentiable with respect to the input parameters, a condition commonly referred to as differentiable optimization, which is non-trivial to solve. Second, the differentiable optimization framework confronts significant efficiency issues, especially when dealing with multi-constraint problems. To address these challenges, this paper presents a CBF-based safe RL architecture that effectively mitigates the issues outlined above. The proposed approach constructs a continuous AND logic approximation for the multiple constraints using a single composite CBF. By leveraging this approximation, a close-form solution of the quadratic programming is derived for the policy network in RL, thereby circumventing the need for differentiable optimization within the end-to-end safe RL pipeline. This strategy significantly reduces computational complexity because of the closed-form solution while maintaining safety guarantees. Simulation results demonstrate that, in comparison to existing approaches relying on differentiable optimization, the proposed method significantly reduces training computational costs while ensuring provable safety throughout the training process.","authors":["Chenggang Wang","Xinyi Wang","Yutong Dong","Lei Song","Xinping Guan"],"url":"https://arxiv.org/abs/2505.00671"}
{"created":"2025-05-02","title":"Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future Directions","abstract":"Memory is a fundamental component of AI systems, underpinning large language models (LLMs) based agents. While prior surveys have focused on memory applications with LLMs, they often overlook the atomic operations that underlie memory dynamics. In this survey, we first categorize memory representations into parametric, contextual structured, and contextual unstructured and then introduce six fundamental memory operations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and Compression. We systematically map these operations to the most relevant research topics across long-term, long-context, parametric modification, and multi-source memory. By reframing memory systems through the lens of atomic operations and representation types, this survey provides a structured and dynamic perspective on research, benchmark datasets, and tools related to memory in AI, clarifying the functional interplay in LLMs based agents while outlining promising directions for future research\\footnote{The paper list, datasets, methods and tools are available at \\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{https://github.com/Elvin-Yiming-Du/Survey\\_Memory\\_in\\_AI}.}.","authors":["Yiming Du","Wenyu Huang","Danna Zheng","Zhaowei Wang","Sebastien Montella","Mirella Lapata","Kam-Fai Wong","Jeff Z. Pan"],"url":"https://arxiv.org/abs/2505.00675"}
{"created":"2025-05-02","title":"Linear Parameter Varying Attitude Control For CubeSats Using Electrospray Thrusters","abstract":"This paper proposes the design of a single linear parameter-varying (LPV) controller for the attitude control of CubeSats using electro spray thrusters. CubeSat attitude control based on electro spray thrusters faces two main challenges. Firstly, the thruster can only generate a small control torque leading to easily saturating the actuation system. Secondly, CubeSats need to operate multiple different maneuvers from large to small slews to pointing tasks. LPV control is ideally suitable to address these challenges. The proposed design follows a mixed-sensitivity control scheme. The parameter-varying weights depend on the attitude error and are derived from the performance and robustness requirements of individual typical CubeSat maneuvers. The controller is synthesized by minimizing the induced L2-norm of the closed-loop interconnections between the controller and weighted plant. The performance and robustness of the controller is demonstrated on a simulation of the MIT Space Propulsion Lab's Magnetic Levitation CubeSat Testbed.","authors":["Felix Biert\\\"umpfel","Emily Burgin","Hanna-Lee Harjono","Paulo Lozano","Harald Pfifer"],"url":"https://arxiv.org/abs/2505.00677"}
{"created":"2025-05-02","title":"Steering Large Language Models with Register Analysis for Arbitrary Style Transfer","abstract":"Large Language Models (LLMs) have demonstrated strong capabilities in rewriting text across various styles. However, effectively leveraging this ability for example-based arbitrary style transfer, where an input text is rewritten to match the style of a given exemplar, remains an open challenge. A key question is how to describe the style of the exemplar to guide LLMs toward high-quality rewrites. In this work, we propose a prompting method based on register analysis to guide LLMs to perform this task. Empirical evaluations across multiple style transfer tasks show that our prompting approach enhances style transfer strength while preserving meaning more effectively than existing prompting strategies.","authors":["Xinchen Yang","Marine Carpuat"],"url":"https://arxiv.org/abs/2505.00679"}
{"created":"2025-05-02","title":"MINERVA: Evaluating Complex Video Reasoning","abstract":"Multimodal LLMs are turning their focus to video benchmarks, however most video benchmarks only provide outcome supervision, with no intermediate or interpretable reasoning steps. This makes it challenging to assess if models are truly able to combine perceptual and temporal information to reason about videos, or simply get the correct answer by chance or by exploiting linguistic biases. To remedy this, we provide a new video reasoning dataset called MINERVA for modern multimodal models. Each question in the dataset comes with 5 answer choices, as well as detailed, hand-crafted reasoning traces. Our dataset is multimodal, diverse in terms of video domain and length, and consists of complex multi-step questions. Extensive benchmarking shows that our dataset provides a challenge for frontier open-source and proprietary models. We perform fine-grained error analysis to identify common failure modes across various models, and create a taxonomy of reasoning errors. We use this to explore both human and LLM-as-a-judge methods for scoring video reasoning traces, and find that failure modes are primarily related to temporal localization, followed by visual perception errors, as opposed to logical or completeness errors. The dataset, along with questions, answer candidates and reasoning traces will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\\#minerva.","authors":["Arsha Nagrani","Sachit Menon","Ahmet Iscen","Shyamal Buch","Ramin Mehran","Nilpa Jha","Anja Hauth","Yukun Zhu","Carl Vondrick","Mikhail Sirotenko","Cordelia Schmid","Tobias Weyand"],"url":"https://arxiv.org/abs/2505.00681"}
{"created":"2025-05-02","title":"Visual Test-time Scaling for GUI Agent Grounding","abstract":"We introduce RegionFocus, a visual test-time scaling approach for Vision Language Model Agents. Understanding webpages is challenging due to the visual complexity of GUI images and the large number of interface elements, making accurate action selection difficult. Our approach dynamically zooms in on relevant regions, reducing background clutter and improving grounding accuracy. To support this process, we propose an image-as-map mechanism that visualizes key landmarks at each step, providing a transparent action record and enables the agent to effectively choose among action candidates. Even with a simple region selection strategy, we observe significant performance gains of 28+\\% on Screenspot-pro and 24+\\% on WebVoyager benchmarks on top of two state-of-the-art open vision language model agents, UI-TARS and Qwen2.5-VL, highlighting the effectiveness of visual test-time scaling in interactive settings. We achieve a new state-of-the-art grounding performance of 61.6\\% on the ScreenSpot-Pro benchmark by applying RegionFocus to a Qwen2.5-VL-72B model. Our code will be released publicly at https://github.com/tiangeluo/RegionFocus.","authors":["Tiange Luo","Lajanugen Logeswaran","Justin Johnson","Honglak Lee"],"url":"https://arxiv.org/abs/2505.00684"}
{"created":"2025-05-02","title":"On the Importance of Gaussianizing Representations","abstract":"The normal distribution plays a central role in information theory - it is at the same time the best-case signal and worst-case noise distribution, has the greatest representational capacity of any distribution, and offers an equivalence between uncorrelatedness and independence for joint distributions. Accounting for the mean and variance of activations throughout the layers of deep neural networks has had a significant effect on facilitating their effective training, but seldom has a prescription for precisely what distribution these activations should take, and how this might be achieved, been offered. Motivated by the information-theoretic properties of the normal distribution, we address this question and concurrently present normality normalization: a novel normalization layer which encourages normality in the feature representations of neural networks using the power transform and employs additive Gaussian noise during training. Our experiments comprehensively demonstrate the effectiveness of normality normalization, in regards to its generalization performance on an array of widely used model and dataset combinations, its strong performance across various common factors of variation such as model width, depth, and training minibatch size, its suitability for usage wherever existing normalization layers are conventionally used, and as a means to improving model robustness to random perturbations.","authors":["Daniel Eftekhari","Vardan Papyan"],"url":"https://arxiv.org/abs/2505.00685"}
{"created":"2025-05-02","title":"The Architecture Tradeoff and Risk Analysis Framework (ATRAF): A Unified Approach for Evaluating Software Architectures, Reference Architectures, and Architectural Frameworks","abstract":"Modern software systems are guided by hierarchical architectural concepts -- software architectures, reference architectures, and architectural frameworks -- each operating at a distinct level of abstraction. These artifacts promote reuse, scalability, and consistency, but also embed tradeoffs that shape critical quality attributes such as modifiability, performance, and security. Existing evaluation methods, such as the Architecture Tradeoff Analysis Method (ATAM), focus on system-specific architectures and are not designed to address the broader generality and variability of higher-level architectural forms. To close this gap, we introduce the Architecture Tradeoff and Risk Analysis Framework (ATRAF) -- a unified, scenario-driven framework for evaluating tradeoffs and risks across architectural levels. ATRAF encompasses three methods: the Architecture Tradeoff and Risk Analysis Method (ATRAM), extending ATAM with enhanced risk identification for concrete systems; the Reference Architecture Tradeoff and Risk Analysis Method (RATRAM), adapting ATRAM to the evaluation of domain-level reference architectures; and the Architectural Framework Tradeoff and Risk Analysis Method (AFTRAM), supporting the evaluation of architectural frameworks that guide entire system families. All three methods follow an iterative spiral process that enables the identification of sensitivities, tradeoffs, and risks while supporting continuous refinement of architectural artifacts. We demonstrate ATRAF through progressively abstracted examples derived from the Remote Temperature Sensor (RTS) case, originally introduced in the ATAM literature. ATRAF equips architects, reference modelers, and framework designers with a practical, systematic approach for analyzing design alternatives and managing quality attribute tradeoffs early in the lifecycle and across all levels of architectural abstraction.","authors":["Amine Ben Hassouna"],"url":"https://arxiv.org/abs/2505.00688"}
{"created":"2025-05-02","title":"Towards Autonomous Micromobility through Scalable Urban Simulation","abstract":"Micromobility, which utilizes lightweight mobile machines moving in urban public spaces, such as delivery robots and mobility scooters, emerges as a promising alternative to vehicular mobility. Current micromobility depends mostly on human manual operation (in-person or remote control), which raises safety and efficiency concerns when navigating busy urban environments full of unpredictable obstacles and pedestrians. Assisting humans with AI agents in maneuvering micromobility devices presents a viable solution for enhancing safety and efficiency. In this work, we present a scalable urban simulation solution to advance autonomous micromobility. First, we build URBAN-SIM - a high-performance robot learning platform for large-scale training of embodied agents in interactive urban scenes. URBAN-SIM contains three critical modules: Hierarchical Urban Generation pipeline, Interactive Dynamics Generation strategy, and Asynchronous Scene Sampling scheme, to improve the diversity, realism, and efficiency of robot learning in simulation. Then, we propose URBAN-BENCH - a suite of essential tasks and benchmarks to gauge various capabilities of the AI agents in achieving autonomous micromobility. URBAN-BENCH includes eight tasks based on three core skills of the agents: Urban Locomotion, Urban Navigation, and Urban Traverse. We evaluate four robots with heterogeneous embodiments, such as the wheeled and legged robots, across these tasks. Experiments on diverse terrains and urban structures reveal each robot's strengths and limitations.","authors":["Wayne Wu","Honglin He","Chaoyuan Zhang","Jack He","Seth Z. Zhao","Ran Gong","Quanyi Li","Bolei Zhou"],"url":"https://arxiv.org/abs/2505.00690"}
{"created":"2025-05-02","title":"Robotic Visual Instruction","abstract":"Recently, natural language has been the primary medium for human-robot interaction. However, its inherent lack of spatial precision for robotic control introduces challenges such as ambiguity and verbosity. To address these limitations, we introduce the Robotic Visual Instruction (RoVI), a novel paradigm to guide robotic tasks through an object-centric, hand-drawn symbolic representation. RoVI effectively encodes spatial-temporal information into human-interpretable visual instructions through 2D sketches, utilizing arrows, circles, colors, and numbers to direct 3D robotic manipulation. To enable robots to understand RoVI better and generate precise actions based on RoVI, we present Visual Instruction Embodied Workflow (VIEW), a pipeline formulated for RoVI-conditioned policies. This approach leverages Vision-Language Models (VLMs) to interpret RoVI inputs, decode spatial and temporal constraints from 2D pixel space via keypoint extraction, and then transform them into executable 3D action sequences. We additionally curate a specialized dataset of 15K instances to fine-tune small VLMs for edge deployment, enabling them to effectively learn RoVI capabilities. Our approach is rigorously validated across 11 novel tasks in both real and simulated environments, demonstrating significant generalization capability. Notably, VIEW achieves an 87.5% success rate in real-world scenarios involving unseen tasks that feature multi-step actions, with disturbances, and trajectory-following requirements. Code and Datasets in this paper will be released soon.","authors":["Yanbang Li","Ziyang Gong","Haoyang Li","Haoyang Li","Xiaoqi Huang","Haolan Kang","Guangping Bai","Xianzheng Ma"],"url":"https://arxiv.org/abs/2505.00693"}
{"created":"2025-05-02","title":"RayZer: A Self-supervised Large View Synthesis Model","abstract":"We present RayZer, a self-supervised multi-view 3D Vision model trained without any 3D supervision, i.e., camera poses and scene geometry, while exhibiting emerging 3D awareness. Concretely, RayZer takes unposed and uncalibrated images as input, recovers camera parameters, reconstructs a scene representation, and synthesizes novel views. During training, RayZer relies solely on its self-predicted camera poses to render target views, eliminating the need for any ground-truth camera annotations and allowing RayZer to be trained with 2D image supervision. The emerging 3D awareness of RayZer is attributed to two key factors. First, we design a self-supervised framework, which achieves 3D-aware auto-encoding of input images by disentangling camera and scene representations. Second, we design a transformer-based model in which the only 3D prior is the ray structure, connecting camera, pixel, and scene simultaneously. RayZer demonstrates comparable or even superior novel view synthesis performance than ``oracle'' methods that rely on pose annotations in both training and testing. Project: https://hwjiang1510.github.io/RayZer/","authors":["Hanwen Jiang","Hao Tan","Peng Wang","Haian Jin","Yue Zhao","Sai Bi","Kai Zhang","Fujun Luan","Kalyan Sunkavalli","Qixing Huang","Georgios Pavlakos"],"url":"https://arxiv.org/abs/2505.00702"}
{"created":"2025-05-02","title":"T2I-R1: Reinforcing Image Generation with Collaborative Semantic-level and Token-level CoT","abstract":"Recent advancements in large language models have demonstrated how chain-of-thought (CoT) and reinforcement learning (RL) can improve performance. However, applying such reasoning strategies to the visual generation domain remains largely unexplored. In this paper, we present T2I-R1, a novel reasoning-enhanced text-to-image generation model, powered by RL with a bi-level CoT reasoning process. Specifically, we identify two levels of CoT that can be utilized to enhance different stages of generation: (1) the semantic-level CoT for high-level planning of the prompt and (2) the token-level CoT for low-level pixel processing during patch-by-patch generation. To better coordinate these two levels of CoT, we introduce BiCoT-GRPO with an ensemble of generation rewards, which seamlessly optimizes both generation CoTs within the same training step. By applying our reasoning strategies to the baseline model, Janus-Pro, we achieve superior performance with 13% improvement on T2I-CompBench and 19% improvement on the WISE benchmark, even surpassing the state-of-the-art model FLUX.1. Code is available at: https://github.com/CaraJ7/T2I-R1","authors":["Dongzhi Jiang","Ziyu Guo","Renrui Zhang","Zhuofan Zong","Hao Li","Le Zhuo","Shilin Yan","Pheng-Ann Heng","Hongsheng Li"],"url":"https://arxiv.org/abs/2505.00703"}
{"created":"2025-05-02","title":"Controllable Weather Synthesis and Removal with Video Diffusion Models","abstract":"Generating realistic and controllable weather effects in videos is valuable for many applications. Physics-based weather simulation requires precise reconstructions that are hard to scale to in-the-wild videos, while current video editing often lacks realism and control. In this work, we introduce WeatherWeaver, a video diffusion model that synthesizes diverse weather effects -- including rain, snow, fog, and clouds -- directly into any input video without the need for 3D modeling. Our model provides precise control over weather effect intensity and supports blending various weather types, ensuring both realism and adaptability. To overcome the scarcity of paired training data, we propose a novel data strategy combining synthetic videos, generative image editing, and auto-labeled real-world videos. Extensive evaluations show that our method outperforms state-of-the-art methods in weather simulation and removal, providing high-quality, physically plausible, and scene-identity-preserving results over various real-world videos.","authors":["Chih-Hao Lin","Zian Wang","Ruofan Liang","Yuxuan Zhang","Sanja Fidler","Shenlong Wang","Zan Gojcic"],"url":"https://arxiv.org/abs/2505.00704"}
{"created":"2025-05-02","title":"Deep Learning for automated multi-scale functional field boundaries extraction using multi-date Sentinel-2 and PlanetScope imagery: Case Study of Netherlands and Pakistan","abstract":"This study explores the effectiveness of multi-temporal satellite imagery for better functional field boundary delineation using deep learning semantic segmentation architecture on two distinct geographical and multi-scale farming systems of Netherlands and Pakistan. Multidate images of April, August and October 2022 were acquired for PlanetScope and Sentinel-2 in sub regions of Netherlands and November 2022, February and March 2023 for selected area of Dunyapur in Pakistan. For Netherlands, Basic registration crop parcels (BRP) vector layer was used as labeled training data. while self-crafted field boundary vector data were utilized for Pakistan. Four deep learning models with UNET architecture were evaluated using different combinations of multi-date images and NDVI stacks in the Netherlands subregions. A comparative analysis of IoU scores assessed the effectiveness of the proposed multi-date NDVI stack approach. These findings were then applied for transfer learning, using pre-trained models from the Netherlands on the selected area in Pakistan. Additionally, separate models were trained using self-crafted field boundary data for Pakistan, and combined models were developed using data from both the Netherlands and Pakistan. Results indicate that multi-date NDVI stacks provide additional temporal context, reflecting crop growth over different times of the season. The study underscores the critical role of multi-scale ground information from diverse geographical areas in developing robust and universally applicable models for field boundary delineation. The results also highlight the importance of fine spatial resolution for extraction of field boundaries in regions with small scale framing. The findings can be extended to multi-scale implementations for improved automatic field boundary delineation in heterogeneous agricultural environments.","authors":["Saba Zahid","Sajid Ghuffar","Obaid-ur-Rehman","Syed Roshaan Ali Shah"],"url":"https://arxiv.org/abs/2411.15923"}
{"created":"2025-05-02","title":"Can a Quantum Support Vector Machine algorithm be utilized to identify Key Biomarkers from Multi-Omics data of COVID19 patients?","abstract":"Identifying key biomarkers for COVID-19 from high-dimensional multi-omics data is critical for advancing both diagnostic and pathogenesis research. In this study, we evaluated the applicability of the Quantum Support Vector Machine (QSVM) algorithm for biomarker-based classification of COVID-19. Proteomic and metabolomic biomarkers from two independent datasets were ranked by importance using ridge regression and grouped accordingly. The top- and bottom-ranked biomarker sets were then used to train and evaluate both classical SVM (CSVM) and QSVM models, serving as predictive and negative control inputs, respectively. The QSVM was implemented with multiple quantum kernels, including amplitude encoding, angle encoding, the ZZ feature map, and the projected quantum kernel. Across various experimental settings, QSVM consistently achieved classification performance that was comparable to or exceeded that of CSVM, while reflecting the importance rankings by ridge regression. Although the experiments were conducted in numerical simulation, our findings highlight the potential of QSVM as a promising approach for multi-omics data analysis in biomedical research.","authors":["Junggu Choi","Chansu Yu","Kyle L. Jung","Suan-Sin Foo","Weiqiang Chen","Suzy AA Comhair","Serpil C. Erzurum","Lara Jehi","Jae U. Jung"],"url":"https://arxiv.org/abs/2505.00037"}
{"created":"2025-05-02","title":"Convolutional Autoencoders for Data Compression and Anomaly Detection in Small Satellite Technologies","abstract":"Small satellite technologies have enhanced the potential and feasibility of geodesic missions, through simplification of design and decreased costs allowing for more frequent launches. On-satellite data acquisition systems can benefit from the implementation of machine learning (ML), for better performance and greater efficiency on tasks such as image processing or feature extraction. This work presents convolutional autoencoders for implementation on the payload of small satellites, designed to achieve dual functionality of data compression for more efficient off-satellite transmission, and at-source anomaly detection to inform satellite data-taking. This capability is demonstrated for a use case of disaster monitoring using aerial image datasets of the African continent, offering avenues for both novel ML-based approaches in small satellite applications along with the expansion of space technology and artificial intelligence in Africa.","authors":["Dishanand Jayeprokash","Julia Gonski"],"url":"https://arxiv.org/abs/2505.00040"}
{"created":"2025-05-02","title":"SR-NeRV: Improving Embedding Efficiency of Neural Video Representation via Super-Resolution","abstract":"Implicit Neural Representations (INRs) have garnered significant attention for their ability to model complex signals across a variety of domains. Recently, INR-based approaches have emerged as promising frameworks for neural video compression. While conventional methods primarily focus on embedding video content into compact neural networks for efficient representation, they often struggle to reconstruct high-frequency details under stringent model size constraints, which are critical in practical compression scenarios. To address this limitation, we propose an INR-based video representation method that integrates a general-purpose super-resolution (SR) network. Motivated by the observation that high-frequency components exhibit low temporal redundancy across frames, our method entrusts the reconstruction of fine details to the SR network. Experimental results demonstrate that the proposed method outperforms conventional INR-based baselines in terms of reconstruction quality, while maintaining comparable model sizes.","authors":["Taiga Hayami","Kakeru Koizumi","Hiroshi Watanabe"],"url":"https://arxiv.org/abs/2505.00046"}
{"created":"2025-05-02","title":"On the expressivity of deep Heaviside networks","abstract":"We show that deep Heaviside networks (DHNs) have limited expressiveness but that this can be overcome by including either skip connections or neurons with linear activation. We provide lower and upper bounds for the Vapnik-Chervonenkis (VC) dimensions and approximation rates of these network classes. As an application, we derive statistical convergence rates for DHN fits in the nonparametric regression model.","authors":["Insung Kong","Juntong Chen","Sophie Langer","Johannes Schmidt-Hieber"],"url":"https://arxiv.org/abs/2505.00110"}
{"created":"2025-05-02","title":"Rootlets-based registration to the spinal cord PAM50 template","abstract":"Spinal cord functional MRI studies require precise localization of spinal levels for reliable voxelwise group analyses. Traditional template-based registration of the spinal cord uses intervertebral discs for alignment. However, substantial anatomical variability across individuals exists between vertebral and spinal levels. This study proposes a novel registration approach that leverages spinal nerve rootlets to improve alignment accuracy and reproducibility across individuals. We developed a registration method leveraging dorsal cervical rootlets segmentation and aligning them non-linearly with the PAM50 spinal cord template. Validation was performed on a multi-subject, multi-site dataset (n=267, 44 sites) and a multi-subject dataset with various neck positions (n=10, 3 sessions). We further validated the method on task-based functional MRI (n=23) to compare group-level activation maps using rootlet-based registration to traditional disc-based methods. Rootlet-based registration showed superior alignment across individuals compared to the traditional disc-based method. Notably, rootlet positions were more stable across neck positions. Group-level analysis of task-based functional MRI using rootlet-based increased Z scores and activation cluster size compared to disc-based registration (number of active voxels from 3292 to 7978). Rootlet-based registration enhances both inter- and intra-subject anatomical alignment and yields better spatial normalization for group-level fMRI analyses. Our findings highlight the potential of rootlet-based registration to improve the precision and reliability of spinal cord neuroimaging group analysis.","authors":["Sandrine B\\'edard","Jan Valo\\v{s}ek","Valeria Oliva","Kenneth A. Weber II","Julien Cohen-Adad"],"url":"https://arxiv.org/abs/2505.00115"}
{"created":"2025-05-02","title":"Efficient and robust 3D blind harmonization for large domain gaps","abstract":"Blind harmonization has emerged as a promising technique for MR image harmonization to achieve scale-invariant representations, requiring only target domain data (i.e., no source domain data necessary). However, existing methods face limitations such as inter-slice heterogeneity in 3D, moderate image quality, and limited performance for a large domain gap. To address these challenges, we introduce BlindHarmonyDiff, a novel blind 3D harmonization framework that leverages an edge-to-image model tailored specifically to harmonization. Our framework employs a 3D rectified flow trained on target domain images to reconstruct the original image from an edge map, then yielding a harmonized image from the edge of a source domain image. We propose multi-stride patch training for efficient 3D training and a refinement module for robust inference by suppressing hallucination. Extensive experiments demonstrate that BlindHarmonyDiff outperforms prior arts by harmonizing diverse source domain images to the target domain, achieving higher correspondence to the target domain characteristics. Downstream task-based quality assessments such as tissue segmentation and age prediction on diverse MR scanners further confirm the effectiveness of our approach and demonstrate the capability of our robust and generalizable blind harmonization.","authors":["Hwihun Jeong","Hayeon Lee","Se Young Chun","Jongho Lee"],"url":"https://arxiv.org/abs/2505.00133"}
{"created":"2025-05-02","title":"Toward Practical Quantum Machine Learning: A Novel Hybrid Quantum LSTM for Fraud Detection","abstract":"We present a novel hybrid quantum-classical neural network architecture for fraud detection that integrates a classical Long Short-Term Memory (LSTM) network with a variational quantum circuit. By leveraging quantum phenomena such as superposition and entanglement, our model enhances the feature representation of sequential transaction data, capturing complex non-linear patterns that are challenging for purely classical models. A comprehensive data preprocessing pipeline is employed to clean, encode, balance, and normalize a credit card fraud dataset, ensuring a fair comparison with baseline models. Notably, our hybrid approach achieves per-epoch training times in the range of 45-65 seconds, which is significantly faster than similar architectures reported in the literature, where training typically requires several minutes per epoch. Both classical and quantum gradients are jointly optimized via a unified backpropagation procedure employing the parameter-shift rule for the quantum parameters. Experimental evaluations demonstrate competitive improvements in accuracy, precision, recall, and F1 score relative to a conventional LSTM baseline. These results underscore the promise of hybrid quantum-classical techniques in advancing the efficiency and performance of fraud detection systems.","authors":["Rushikesh Ubale","Sujan K. K.","Sangram Deshpande","Gregory T. Byrd"],"url":"https://arxiv.org/abs/2505.00137"}
{"created":"2025-05-02","title":"A Bayesian approach to inverse problems in spaces of measures","abstract":"In this work, we develop a Bayesian framework for solving inverse problems in which the unknown parameter belongs to a space of Radon measures taking values in a separable Hilbert space. The inherent ill-posedness of such problems is addressed by introducing suitable measure-valued priors that encode prior information and promote desired sparsity properties of the parameter. Under appropriate assumptions on the forward operator and noise model, we establish the well-posedness of the Bayesian formulation by proving the existence, uniqueness, and stability of the posterior with respect to perturbations in the observed data. In addition, we also discuss computational strategies for approximating the posterior distribution. Finally, we present some examples that demonstrate the effectiveness of the proposed approach.","authors":["Phuoc-Truong Huynh"],"url":"https://arxiv.org/abs/2505.00151"}
{"created":"2025-05-02","title":"Pinching-Antenna Systems (PASS): Power Radiation Model and Optimal Beamforming Design","abstract":"Pinching-antenna systems (PASS) improve wireless links by configuring the locations of activated pinching antennas along dielectric waveguides, namely pinching beamforming. In this paper, a novel adjustable power radiation model is proposed for PASS, where power radiation ratios of pinching antennas can be flexibly controlled by tuning the spacing between pinching antennas and waveguides. A closed-form pinching antenna spacing arrangement strategy is derived to achieve the commonly assumed equal-power radiation. Based on this, a practical PASS framework relying on discrete activation is considered, where pinching antennas can only be activated among a set of predefined locations. A transmit power minimization problem is formulated, which jointly optimizes the transmit beamforming, pinching beamforming, and the numbers of activated pinching antennas, subject to each user's minimum rate requirement. (1) To solve the resulting highly coupled mixed-integer nonlinear programming (MINLP) problem, branch-and-bound (BnB)-based algorithms are proposed for both single-user and multi-user scenarios, which is guaranteed to converge to globally optimal solutions. (2) A low-complexity many-to-many matching algorithm is further developed. Combined with the Karush-Kuhn-Tucker (KKT) theory, locally optimal and pairwise-stable solutions are obtained within polynomial-time complexity. Simulation results demonstrate that: (i) PASS significantly outperforms conventional multi-antenna architectures, particularly when the number of users and the spatial range increase; and (ii) The proposed matching-based algorithm achieves near-optimal performance, resulting in only a slight performance loss while significantly reducing computational overheads. Code is available at https://github.com/xiaoxiaxusummer/PASS_Discrete","authors":["Xiaoxia Xu","Xidong Mu","Zhaolin Wang","Yuanwei Liu","Arumugam Nallanathan"],"url":"https://arxiv.org/abs/2505.00218"}
{"created":"2025-05-02","title":"Real-Time Brain-Computer Interface Control of Walking Exoskeleton with Bilateral Sensory Feedback","abstract":"Invasive brain-computer interface (BCI) technology has demonstrated the possibility of restoring brain-controlled walking in paraplegic spinal cord injury patients. However, current implementations of BCI-controlled walking still have significant drawbacks. In particular, prior systems are unidirectional and lack sensory feedback for insensate patients, have suboptimal reliance on brain signals from the bilateral arm areas of the motor cortex, and depend on external systems for signal processing. Motivated by these shortcomings, this study is the first time a bidirectional brain-computer interface (BDBCI) has demonstrated the restoration of both brain-controlled walking and leg sensory feedback while utilizing the bilateral leg motor and sensory cortices. Here, a subject undergoing subdural electrocorticogram electrode implantation for epilepsy surgery evaluation leveraged the leg representation areas of the bilateral interhemispheric primary motor and sensory cortices to operate a BDBCI with high performance. Although electrode implantation in the interhemispheric region is uncommon, electrodes can be safely implanted in this region to access rich leg motor information and deliver bilateral leg sensory feedback. Finally, we demonstrated that all BDBCI operations can be executed on a dedicated, portable embedded system. These results indicate that BDBCIs can potentially provide brain-controlled ambulation and artificial leg sensation to people with paraplegia after spinal cord injury in a manner that emulates full-implantability and is untethered from any external systems.","authors":["Jeffrey Lim","Po T. Wang","Won Joon Sohn","Derrick Lin","Shravan Thaploo","Luke Bashford","David Bjanes","Angelica Nguyen","Hui Gong","Michelle Armacost","Susan J. Shaw","Spencer Kellis","Brian Lee","Darrin Lee","Payam Heydari","Richard A. Andersen","Zoran Nenadic","Charles Y. Liu","An H. Do"],"url":"https://arxiv.org/abs/2505.00219"}
{"created":"2025-05-02","title":"Inference for max-linear Bayesian networks with noise","abstract":"Max-Linear Bayesian Networks (MLBNs) provide a powerful framework for causal inference in extreme-value settings; we consider MLBNs with noise parameters with a given topology in terms of the max-plus algebra by taking its logarithm. Then, we show that an estimator of a parameter for each edge in a directed acyclic graph (DAG) is distributed normally. We end this paper with computational experiments with the expectation and maximization (EM) algorithm and quadratic optimization.","authors":["Mark Adams","Kamillo Ferry","Ruriko Yoshida"],"url":"https://arxiv.org/abs/2505.00229"}
{"created":"2025-05-02","title":"Explorative Curriculum Learning for Strongly Correlated Electron Systems","abstract":"Recent advances in neural network quantum states (NQS) have enabled high-accuracy predictions for complex quantum many-body systems such as strongly correlated electron systems. However, the computational cost remains prohibitive, making exploration of the diverse parameters of interaction strengths and other physical parameters inefficient. While transfer learning has been proposed to mitigate this challenge, achieving generalization to large-scale systems and diverse parameter regimes remains difficult. To address this limitation, we propose a novel curriculum learning framework based on transfer learning for NQS. This facilitates efficient and stable exploration across a vast parameter space of quantum many-body systems. In addition, by interpreting NQS transfer learning through a perturbative lens, we demonstrate how prior physical knowledge can be flexibly incorporated into the curriculum learning process. We also propose Pairing-Net, an architecture to practically implement this strategy for strongly correlated electron systems, and empirically verify its effectiveness. Our results show an approximately 200-fold speedup in computation and a marked improvement in optimization stability compared to conventional methods.","authors":["Kimihiro Yamazaki","Takuya Konishi","Yoshinobu Kawahara"],"url":"https://arxiv.org/abs/2505.00233"}
{"created":"2025-05-02","title":"Topological State Space Inference for Dynamical Systems","abstract":"We present a computational pipe aiming at recovery of the topology of the underlying phase space from observation of an output function along a sample of trajectories of a dynamical system.","authors":["Mishal Assif P K","Yuliy Baryshnikov"],"url":"https://arxiv.org/abs/2505.00276"}
{"created":"2025-05-02","title":"A Unifying Framework for Robust and Efficient Inference with Unstructured Data","abstract":"This paper presents a general framework for conducting efficient and robust inference on parameters derived from unstructured data, which include text, images, audio, and video. Economists have long incorporated data extracted from texts and images into their analyses, a practice that has accelerated with advancements in deep neural networks. However, neural networks do not generically produce unbiased predictions, potentially propagating bias to estimators that use their outputs. To address this challenge, we reframe inference with unstructured data as a missing structured data problem, where structured data are imputed from unstructured inputs using deep neural networks. This perspective allows us to apply classic results from semiparametric inference, yielding valid, efficient, and robust estimators based on unstructured data. We formalize this approach with MARS (Missing At Random Structured Data), a unifying framework that integrates and extends existing methods for debiased inference using machine learning predictions, linking them to a variety of older, familiar problems such as causal inference. We develop robust and efficient estimators for both descriptive and causal estimands and address challenges such as inference using aggregated and transformed predictions from unstructured data. Importantly, MARS applies to common empirical settings that have received limited attention in the existing literature. Finally, we reanalyze prominent studies that use unstructured data, demonstrating the practical value of MARS.","authors":["Jacob Carlson","Melissa Dell"],"url":"https://arxiv.org/abs/2505.00282"}
{"created":"2025-05-02","title":"A polytopal discrete de Rham scheme for the exterior calculus Einstein's equations","abstract":"In this work, based on the $3+1$ decomposition in [23, 32], we present a fully exterior calculus breakdown of spacetime and Einstein's equations. Links to the orthonormal frame approach [37] are drawn to help understand the variables in this context. Two formulations are derived, discretised and tested using the exterior calculus discrete de Rham complex [12], and some discrete quantities are shown to be conserved in one of the cases.","authors":["Todd A. Oliynyk","Jia Jia Qian"],"url":"https://arxiv.org/abs/2505.00286"}
{"created":"2025-05-02","title":"Nystr\\\"om Type Exponential Integrators for Strongly Magnetized Charged Particle Dynamics","abstract":"Calculating the dynamics of charged particles in electromagnetic fields (i.e. the particle pushing problem) is one of the most computationally intensive components of particle-in-cell (PIC) methods for plasma physics simulations. This task is especially challenging when the plasma is strongly magnetized, since in this case the particle motion consists of a wide range of temporal scales from highly oscillatory fast gyromotion to slow macroscopic behavior and the resulting numerical model is very stiff. Current state-of-the-art time integrators used to simulate particle motion have limitations given the severe numerical stiffness of the problem and more efficient methods are of interest. Recently, exponential integrators have been proposed as a promising new approach for these simulations and shown to offer computational advantages over commonly used schemes. Exponential methods can solve linear problems exactly and are $A$-stable. In this paper, the standard exponential algorithms framework is extended to derive Nystr\\\"om-type exponential methods that integrate the Newtonian equations of motion as a second-order differential equation. Specific Nystr\\\"om-type schemes of second and third orders are derived and applied to strongly magnetized particle pushing problems. Numerical experiments are presented to demonstrate that the Nystr\\\"om-type exponential integrators can provide significant improvement in computational efficiency over the standard exponential methods.","authors":["Tri P. Nguyen","Ilon Joseph","Mayya Tokman"],"url":"https://arxiv.org/abs/2505.00288"}
{"created":"2025-05-02","title":"Reinforcement Learning with Continuous Actions Under Unmeasured Confounding","abstract":"This paper addresses the challenge of offline policy learning in reinforcement learning with continuous action spaces when unmeasured confounders are present. While most existing research focuses on policy evaluation within partially observable Markov decision processes (POMDPs) and assumes discrete action spaces, we advance this field by establishing a novel identification result to enable the nonparametric estimation of policy value for a given target policy under an infinite-horizon framework. Leveraging this identification, we develop a minimax estimator and introduce a policy-gradient-based algorithm to identify the in-class optimal policy that maximizes the estimated policy value. Furthermore, we provide theoretical results regarding the consistency, finite-sample error bound, and regret bound of the resulting optimal policy. Extensive simulations and a real-world application using the German Family Panel data demonstrate the effectiveness of our proposed methodology.","authors":["Yuhan Li","Eugene Han","Yifan Hu","Wenzhuo Zhou","Zhengling Qi","Yifan Cui","Ruoqing Zhu"],"url":"https://arxiv.org/abs/2505.00304"}
{"created":"2025-05-02","title":"Statistical Learning for Heterogeneous Treatment Effects: Pretraining, Prognosis, and Prediction","abstract":"Robust estimation of heterogeneous treatment effects is a fundamental challenge for optimal decision-making in domains ranging from personalized medicine to educational policy. In recent years, predictive machine learning has emerged as a valuable toolbox for causal estimation, enabling more flexible effect estimation. However, accurately estimating conditional average treatment effects (CATE) remains a major challenge, particularly in the presence of many covariates. In this article, we propose pretraining strategies that leverages a phenomenon in real-world applications: factors that are prognostic of the outcome are frequently also predictive of treatment effect heterogeneity. In medicine, for example, components of the same biological signaling pathways frequently influence both baseline risk and treatment response. Specifically, we demonstrate our approach within the R-learner framework, which estimates the CATE by solving individual prediction problems based on a residualized loss. We use this structure to incorporate \"side information\" and develop models that can exploit synergies between risk prediction and causal effect estimation. In settings where these synergies are present, this cross-task learning enables more accurate signal detection: yields lower estimation error, reduced false discovery rates, and higher power for detecting heterogeneity.","authors":["Maximilian Schuessler","Erik Sverdrup","Robert Tibshirani"],"url":"https://arxiv.org/abs/2505.00310"}
{"created":"2025-05-02","title":"PDCS: A Primal-Dual Large-Scale Conic Programming Solver with GPU Enhancements","abstract":"In this paper, we introduce the \"Primal-Dual Conic Programming Solver\" (PDCS), a large-scale conic programming solver with GPU enhancements. Problems that PDCS currently supports include linear programs, second-order cone programs, convex quadratic programs, and exponential cone programs. PDCS achieves scalability to large-scale problems by leveraging sparse matrix-vector multiplication as its core computational operation, which is both memory-efficient and well-suited for GPU acceleration. The solver is based on the restarted primal-dual hybrid gradient method but further incorporates several enhancements, including adaptive reflected Halpern restarts, adaptive step-size selection, adaptive weight adjustment, and diagonal rescaling. Additionally, PDCS employs a bijection-based method to compute projections onto rescaled cones. Furthermore, cuPDCS is a GPU implementation of PDCS and it implements customized computational schemes that utilize different levels of GPU architecture to handle cones of different types and sizes. Numerical experiments demonstrate that cuPDCS is generally more efficient than state-of-the-art commercial solvers and other first-order methods on large-scale conic program applications, including Fisher market equilibrium problems, Lasso regression, and multi-period portfolio optimization. Furthermore, cuPDCS also exhibits better scalability, efficiency, and robustness compared to other first-order methods on the conic program benchmark dataset CBLIB. These advantages are more pronounced in large-scale, lower-accuracy settings.","authors":["Zhenwei Lin","Zikai Xiong","Dongdong Ge","Yinyu Ye"],"url":"https://arxiv.org/abs/2505.00311"}
{"created":"2025-05-02","title":"Towards Lightweight Hyperspectral Image Super-Resolution with Depthwise Separable Dilated Convolutional Network","abstract":"Deep neural networks have demonstrated highly competitive performance in super-resolution (SR) for natural images by learning mappings from low-resolution (LR) to high-resolution (HR) images. However, hyperspectral super-resolution remains an ill-posed problem due to the high spectral dimensionality of the data and the scarcity of available training samples. Moreover, existing methods often rely on large models with a high number of parameters or require the fusion with panchromatic or RGB images, both of which are often impractical in real-world scenarios. Inspired by the MobileNet architecture, we introduce a lightweight depthwise separable dilated convolutional network (DSDCN) to address the aforementioned challenges. Specifically, our model leverages multiple depthwise separable convolutions, similar to the MobileNet architecture, and further incorporates a dilated convolution fusion block to make the model more flexible for the extraction of both spatial and spectral features. In addition, we propose a custom loss function that combines mean squared error (MSE), an L2 norm regularization-based constraint, and a spectral angle-based loss, ensuring the preservation of both spectral and spatial details. The proposed model achieves very competitive performance on two publicly available hyperspectral datasets, making it well-suited for hyperspectral image super-resolution tasks. The source codes are publicly available at: \\href{https://github.com/Usman1021/lightweight}{https://github.com/Usman1021/lightweight}.","authors":["Usman Muhammad","Jorma Laaksonen","Lyudmila Mihaylova"],"url":"https://arxiv.org/abs/2505.00374"}
{"created":"2025-05-02","title":"Perceptual Implications of Automatic Anonymization in Pathological Speech","abstract":"Automatic anonymization techniques are essential for ethical sharing of pathological speech data, yet their perceptual consequences remain understudied. This study presents the first comprehensive human-centered analysis of anonymized pathological speech, using a structured perceptual protocol involving ten native and non-native German listeners with diverse linguistic, clinical, and technical backgrounds. Listeners evaluated anonymized-original utterance pairs from 180 speakers spanning Cleft Lip and Palate, Dysarthria, Dysglossia, Dysphonia, and age-matched healthy controls. Speech was anonymized using state-of-the-art automatic methods (equal error rates in the range of 30-40%). Listeners completed Turing-style discrimination and quality rating tasks under zero-shot (single-exposure) and few-shot (repeated-exposure) conditions. Discrimination accuracy was high overall (91% zero-shot; 93% few-shot), but varied by disorder (repeated-measures ANOVA: p=0.007), ranging from 96% (Dysarthria) to 86% (Dysphonia). Anonymization consistently reduced perceived quality (from 83% to 59%, p<0.001), with pathology-specific degradation patterns (one-way ANOVA: p=0.005). Native listeners rated original speech slightly higher than non-native listeners (Delta=4%, p=0.199), but this difference nearly disappeared after anonymization (Delta=1%, p=0.724). No significant gender-based bias was observed. Critically, human perceptual outcomes did not correlate with automatic privacy or clinical utility metrics. These results underscore the need for listener-informed, disorder- and context-specific anonymization strategies that preserve privacy while maintaining interpretability, communicative functions, and diagnostic utility, especially for vulnerable populations such as children.","authors":["Soroosh Tayebi Arasteh","Saba Afza","Tri-Thien Nguyen","Lukas Buess","Maryam Parvin","Tomas Arias-Vergara","Paula Andrea Perez-Toro","Hiu Ching Hung","Mahshad Lotfinia","Thomas Gorges","Elmar Noeth","Maria Schuster","Seung Hee Yang","Andreas Maier"],"url":"https://arxiv.org/abs/2505.00409"}
{"created":"2025-05-02","title":"Maximum list $r$-colorable induced subgraphs in $kP_3$-free graphs","abstract":"We show that, for every fixed positive integers $r$ and $k$, \\textsc{Max-Weight List $r$-Colorable Induced Subgraph} admits a polynomial-time algorithm on $kP_3$-free graphs. This problem is a common generalization of \\textsc{Max-Weight Independent Set}, \\textsc{Odd Cycle Transversal} and \\textsc{List $r$-Coloring}, among others. Our result has several consequences.","authors":["Esther Galby","Paloma T. Lima","Andrea Munaro","Amir Nikabadi"],"url":"https://arxiv.org/abs/2505.00412"}
{"created":"2025-05-02","title":"Over-the-Air Inference over Multi-hop MIMO Networks","abstract":"A novel over-the-air machine learning framework over multi-hop multiple-input and multiple-output (MIMO) networks is proposed. The core idea is to imitate fully connected (FC) neural network layers using multiple MIMO channels by carefully designing the precoding matrices at the transmitting nodes. A neural network dubbed PrototypeNet is employed consisting of multiple FC layers, with the number of neurons of each layer equal to the number of antennas of the corresponding terminal. To achieve satisfactory performance, we train PrototypeNet based on a customized loss function consisting of classification error and the power of latent vectors to satisfy transmit power constraints, with noise injection during training. Precoding matrices for each hop are then obtained by solving an optimization problem. We also propose a multiple-block extension when the number of antennas is limited. Numerical results verify that the proposed over-the-air transmission scheme can achieve satisfactory classification accuracy under a power constraint. The results also show that higher classification accuracy can be achieved with an increasing number of hops at a modest signal-to-noise ratio (SNR).","authors":["Chenghong Bian","Meng Hua","Deniz Gunduz"],"url":"https://arxiv.org/abs/2505.00430"}
{"created":"2025-05-02","title":"Success probability in Shor's Algorithm","abstract":"This paper aims to determine the exact success probability at each step of Shor's algorithm. Although the literature usually provides a lower bound on this probability, we present an improved bound. The derived formulas enable the identification of all failure cases in Shor's algorithm, which correspond to a success probability of zero. A simulation routine is provided to evaluate the theoretical success probability for a given integer when its prime factorization is known with potential applications in quantum resource estimation and algorithm benchmarking.","authors":["Ali Abbassi","Lionel Bayle"],"url":"https://arxiv.org/abs/2505.00433"}
{"created":"2025-05-02","title":"Analysis of evolution equation with variable-exponent memory modeling multiscale viscoelasticity","abstract":"We investigate the well-posedness and solution regularity of an evolution equation with non-positive type variable-exponent memory, which describes multiscale viscoelasticity in materials with memory. The perturbation method is applied for model transformation, based on which the well-posedness is proved. Then the weighted solution regularity is derived, where the initial singularity is characterized by the initial value of variable exponent.","authors":["Yiqun Li","Xiangcheng Zheng"],"url":"https://arxiv.org/abs/2505.00446"}
{"created":"2025-05-02","title":"On estimating the quantum $\\ell_{\\alpha}$ distance","abstract":"We study the computational complexity of estimating the quantum $\\ell_{\\alpha}$ distance ${\\mathrm{T}_\\alpha}(\\rho_0,\\rho_1)$, defined via the Schatten $\\alpha$-norm $\\|A\\|_{\\alpha} = \\mathrm{tr}(|A|^{\\alpha})^{1/\\alpha}$, given $\\operatorname{poly}(n)$-size state-preparation circuits of $n$-qubit quantum states $\\rho_0$ and $\\rho_1$. This quantity serves as a lower bound on the trace distance for $\\alpha > 1$. For any constant $\\alpha > 1$, we develop an efficient rank-independent quantum estimator for ${\\mathrm{T}_\\alpha}(\\rho_0,\\rho_1)$ with time complexity $\\operatorname{poly}(n)$, achieving an exponential speedup over the prior best results of $\\exp(n)$ due to Wang, Guan, Liu, Zhang, and Ying (TIT 2024). Our improvement leverages efficiently computable uniform polynomial approximations of signed positive power functions within quantum singular value transformation, thereby eliminating the dependence on the rank of the quantum states.","authors":["Yupan Liu","Qisheng Wang"],"url":"https://arxiv.org/abs/2505.00457"}
{"created":"2025-05-02","title":"CORSTITCH - A free, open source software for stitching and georeferencing underwater coral reef videos","abstract":"CorStitch is an open-source software developed to automate the creation of accurate georeferenced reef mosaics from video transects obtained through Automated Rapid Reef Assessment System surveys. We utilized a Fourier-based image correlation algorithm to stitch sequential video frames, aligning them with synchronized GNSS timestamps. The resulting compressed Keyhole Markup Language files, compatible with geographic information systems such as Google Earth, enable detailed spatial analysis. Validation through comparative analysis of mosaics from two temporally distinct surveys of the same reef demonstrated the software's consistent and reliable performance.","authors":["Julian Christopher L. Maya","Johnenn R. Manalang","Maricor N. Soriano"],"url":"https://arxiv.org/abs/2505.00462"}
{"created":"2025-05-02","title":"Orbit-blocking words in free groups","abstract":"By strengthening known results about primitivity-blocking words in free groups, we prove that for any nontrivial element w of a free group of finite rank, there are words that cannot be subwords of any cyclically reduced automorphic image of w. This has implications for the average-case complexity of Whitehead's problem.","authors":["Lucy Koch-Hyde","Siobhan O'Connor","Eamonn Olive","Vladimir Shpilrain"],"url":"https://arxiv.org/abs/2505.00477"}
{"created":"2025-05-02","title":"A Methodological and Structural Review of Parkinsons Disease Detection Across Diverse Data Modalities","abstract":"Parkinsons Disease (PD) is a progressive neurological disorder that primarily affects motor functions and can lead to mild cognitive impairment (MCI) and dementia in its advanced stages. With approximately 10 million people diagnosed globally 1 to 1.8 per 1,000 individuals, according to reports by the Japan Times and the Parkinson Foundation early and accurate diagnosis of PD is crucial for improving patient outcomes. While numerous studies have utilized machine learning (ML) and deep learning (DL) techniques for PD recognition, existing surveys are limited in scope, often focusing on single data modalities and failing to capture the potential of multimodal approaches. To address these gaps, this study presents a comprehensive review of PD recognition systems across diverse data modalities, including Magnetic Resonance Imaging (MRI), gait-based pose analysis, gait sensory data, handwriting analysis, speech test data, Electroencephalography (EEG), and multimodal fusion techniques. Based on over 347 articles from leading scientific databases, this review examines key aspects such as data collection methods, settings, feature representations, and system performance, with a focus on recognition accuracy and robustness. This survey aims to serve as a comprehensive resource for researchers, providing actionable guidance for the development of next generation PD recognition systems. By leveraging diverse data modalities and cutting-edge machine learning paradigms, this work contributes to advancing the state of PD diagnostics and improving patient care through innovative, multimodal approaches.","authors":["Abu Saleh Musa Miah","taro Suzuki","Jungpil Shin"],"url":"https://arxiv.org/abs/2505.00525"}
{"created":"2025-05-02","title":"Pre-Training Estimators for Structural Models: Application to Consumer Search","abstract":"We explore pretraining estimators for structural econometric models. The estimator is \"pretrained\" in the sense that the bulk of the computational cost and researcher effort occur during the construction of the estimator. Subsequent applications of the estimator to different datasets require little computational cost or researcher effort. The estimation leverages a neural net to recognize the structural model's parameter from data patterns. As an initial trial, this paper builds a pretrained estimator for a sequential search model that is known to be difficult to estimate. We evaluate the pretrained estimator on 14 real datasets. The estimation takes seconds to run and shows high accuracy. We provide the estimator at pnnehome.github.io. More generally, pretrained, off-the-shelf estimators can make structural models more accessible to researchers and practitioners.","authors":["Yanhao 'Max' Wei","Zhenling Jiang"],"url":"https://arxiv.org/abs/2505.00526"}
{"created":"2025-05-02","title":"On the Mechanistic Interpretability of Neural Networks for Causality in Bio-statistics","abstract":"Interpretable insights from predictive models remain critical in bio-statistics, particularly when assessing causality, where classical statistical and machine learning methods often provide inherent clarity. While Neural Networks (NNs) offer powerful capabilities for modeling complex biological data, their traditional \"black-box\" nature presents challenges for validation and trust in high-stakes health applications. Recent advances in Mechanistic Interpretability (MI) aim to decipher the internal computations learned by these networks. This work investigates the application of MI techniques to NNs within the context of causal inference for bio-statistics.","authors":["Jean-Baptiste A. Conan"],"url":"https://arxiv.org/abs/2505.00555"}
{"created":"2025-05-02","title":"Learning to Learn with Quantum Optimization via Quantum Neural Networks","abstract":"Quantum Approximate Optimization Algorithms (QAOA) promise efficient solutions to classically intractable combinatorial optimization problems by harnessing shallow-depth quantum circuits. Yet, their performance and scalability often hinge on effective parameter optimization, which remains nontrivial due to rugged energy landscapes and hardware noise. In this work, we introduce a quantum meta-learning framework that combines quantum neural networks, specifically Quantum Long Short-Term Memory (QLSTM) architectures, with QAOA. By training the QLSTM optimizer on smaller graph instances, our approach rapidly generalizes to larger, more complex problems, substantially reducing the number of iterations required for convergence. Through comprehensive benchmarks on Max-Cut and Sherrington-Kirkpatrick model instances, we demonstrate that QLSTM-based optimizers converge faster and achieve higher approximation ratios compared to classical baselines, thereby offering a robust pathway toward scalable quantum optimization in the NISQ era.","authors":["Kuan-Cheng Chen","Hiromichi Matsuyama","Wei-Hao Huang"],"url":"https://arxiv.org/abs/2505.00561"}
{"created":"2025-05-02","title":"Hypothesis-free discovery from epidemiological data by automatic detection and local inference for tree-based nonlinearities and interactions","abstract":"In epidemiological settings, Machine Learning (ML) is gaining popularity for hypothesis-free discovery of risk (or protective) factors. Although ML is strong at discovering non-linearities and interactions, this power is currently compromised by a lack of reliable inference. Although local measures of feature effect can be combined with tree ensembles, uncertainty quantifications for these measures remain only partially available and oftentimes unsatisfactory. We propose RuleSHAP, a framework for using rule-based, hypothesis-free discovery that combines sparse Bayesian regression, tree ensembles and Shapley values in a one-step procedure that both detects and tests complex patterns at the individual level. To ease computation, we derive a formula that computes marginal Shapley values more efficiently for our setting. We demonstrate the validity of our framework on simulated data. To illustrate, we apply our machinery to data from an epidemiological cohort to detect and infer several effects for high cholesterol and blood pressure, such as nonlinear interaction effects between features like age, sex, ethnicity, BMI and glucose level.","authors":["Giorgio Spadaccini","Marjolein Fokkema","Mark A. van de Wiel"],"url":"https://arxiv.org/abs/2505.00571"}
{"created":"2025-05-02","title":"Transition States Energies from Machine Learning: An Application to Reverse Water-Gas Shift on Single-Atom Alloys","abstract":"Obtaining accurate transition state (TS) energies is a bottleneck in computational screening of complex materials and reaction networks due to the high cost of TS search methods and first-principles methods such as density functional theory (DFT). Here we propose a machine learning (ML) model for predicting TS energies based on Gaussian process regression with the Wasserstein Weisfeiler-Lehman graph kernel (WWL-GPR). Applying the model to predict adsorption and TS energies for the reverse water-gas shift (RWGS) reaction on single-atom alloy (SAA) catalysts, we show that it can significantly improve the accuracy compared to traditional approaches based on scaling relations or ML models without a graph representation. Further benefitting from the low cost of model training, we train an ensemble of WWL-GPR models to obtain uncertainties through subsampling of the training data and show how these uncertainties propagate to turnover frequency (TOF) predictions through the construction of an ensemble of microkinetic models. Comparing the errors in model-based vs DFT-based TOF predictions, we show that the WWL-GPR model reduces errors by almost an order of magnitude compared to scaling relations. This demonstrates the critical impact of accurate energy predictions on catalytic activity estimation. Finally, we apply our model to screen new materials, identifying promising catalysts for RWGS. This work highlights the power of combining advanced ML techniques with DFT and microkinetic modeling for screening catalysts for complex reactions like RWGS, providing a robust framework for future catalyst design.","authors":["Raffaele Cheula","Mie Andersen"],"url":"https://arxiv.org/abs/2505.00574"}
{"created":"2025-05-02","title":"Decomposing graphs into stable and ordered parts","abstract":"Connections between structural graph theory and finite model theory recently gained a lot of attention. In this setting, many interesting question remain on the properties of hereditary dependent (NIP) classes of graphs, in particular related to first-order transductions.","authors":["Hector Buffi\\`ere","Patrice Ossona de Mendez"],"url":"https://arxiv.org/abs/2505.00594"}
{"created":"2025-05-02","title":"SA-GAT-SR: Self-Adaptable Graph Attention Networks with Symbolic Regression for high-fidelity material property prediction","abstract":"Recent advances in machine learning have demonstrated an enormous utility of deep learning approaches, particularly Graph Neural Networks (GNNs) for materials science. These methods have emerged as powerful tools for high-throughput prediction of material properties, offering a compelling enhancement and alternative to traditional first-principles calculations. While the community has predominantly focused on developing increasingly complex and universal models to enhance predictive accuracy, such approaches often lack physical interpretability and insights into materials behavior. Here, we introduce a novel computational paradigm, Self-Adaptable Graph Attention Networks integrated with Symbolic Regression (SA-GAT-SR), that synergistically combines the predictive capability of GNNs with the interpretative power of symbolic regression. Our framework employs a self-adaptable encoding algorithm that automatically identifies and adjust attention weights so as to screen critical features from an expansive 180-dimensional feature space while maintaining O(n) computational scaling. The integrated SR module subsequently distills these features into compact analytical expressions that explicitly reveal quantum-mechanically meaningful relationships, achieving 23 times acceleration compared to conventional SR implementations that heavily rely on first principle calculations-derived features as input. This work suggests a new framework in computational materials science, bridging the gap between predictive accuracy and physical interpretability, offering valuable physical insights into material behavior.","authors":["Liu Junchi","Tang Ying","Tretiak Sergei","Duan Wenhui","Zhou Liujiang"],"url":"https://arxiv.org/abs/2505.00625"}
{"created":"2025-05-02","title":"Bayes-Optimal Fair Classification with Multiple Sensitive Features","abstract":"Existing theoretical work on Bayes-optimal fair classifiers usually considers a single (binary) sensitive feature. In practice, individuals are often defined by multiple sensitive features. In this paper, we characterize the Bayes-optimal fair classifier for multiple sensitive features under general approximate fairness measures, including mean difference and mean ratio. We show that these approximate measures for existing group fairness notions, including Demographic Parity, Equal Opportunity, Predictive Equality, and Accuracy Parity, are linear transformations of selection rates for specific groups defined by both labels and sensitive features. We then characterize that Bayes-optimal fair classifiers for multiple sensitive features become instance-dependent thresholding rules that rely on a weighted sum of these group membership probabilities. Our framework applies to both attribute-aware and attribute-blind settings and can accommodate composite fairness notions like Equalized Odds. Building on this, we propose two practical algorithms for Bayes-optimal fair classification via in-processing and post-processing. We show empirically that our methods compare favorably to existing methods.","authors":["Yi Yang","Yinghui Huang","Xiangyu Chang"],"url":"https://arxiv.org/abs/2505.00631"}
{"created":"2025-05-02","title":"Deep Learning Assisted Outer Volume Removal for Highly-Accelerated Real-Time Dynamic MRI","abstract":"Real-time (RT) dynamic MRI plays a vital role in capturing rapid physiological processes, offering unique insights into organ motion and function. Among these applications, RT cine MRI is particularly important for functional assessment of the heart with high temporal resolution. RT imaging enables free-breathing, ungated imaging of cardiac motion, making it a crucial alternative for patients who cannot tolerate conventional breath-hold, ECG-gated acquisitions. However, achieving high acceleration rates in RT cine MRI is challenging due to aliasing artifacts from extra-cardiac tissues, particularly at high undersampling factors. In this study, we propose a novel outer volume removal (OVR) method to address this challenge by eliminating aliasing contributions from non-cardiac regions in a post-processing framework. Our approach estimates the outer volume signal for each timeframe using composite temporal images from time-interleaved undersampling patterns, which inherently contain pseudo-periodic ghosting artifacts. A deep learning (DL) model is trained to identify and remove these artifacts, producing a clean outer volume estimate that is subsequently subtracted from the corresponding k-space data. The final reconstruction is performed with a physics-driven DL (PD-DL) method trained using an OVR-specific loss function to restore high spatio-temporal resolution images. Experimental results show that the proposed method at high accelerations achieves image quality that is visually comparable to clinical baseline images, while outperforming conventional reconstruction techniques, both qualitatively and quantitatively. The proposed approach provides a practical and effective solution for artifact reduction in RT cine MRI without requiring acquisition modifications, offering a pathway to higher acceleration rates while preserving diagnostic quality.","authors":["Merve G\\\"ulle","Sebastian Weing\\\"artner","Mehmet Ak\\c{c}akaya"],"url":"https://arxiv.org/abs/2505.00643"}
{"created":"2025-05-02","title":"The local coupling of noise technique and its application to lower error bounds for strong approximation of SDEs with irregular coefficients","abstract":"In recent years, interest in approximation methods for stochastic differential equations (SDEs) with non-Lipschitz continuous coefficients has increased. We show lower bounds for the $L^p$-error of such methods in the case of approximation at a single point in time or globally in time. On the one hand, we show that for a large class of piecewise Lipschitz continuous drifts and non-additive diffusions the best possible $L^p$-error rate for final time approximation that can be achieved by any method based on finitely many evaluations of the driving Brownian motion is at most $3/4$, which was previously known only for additive diffusions. Moreover, we show that the best $L^p$-error rate for global approximation that can be achieved by any method based on finitely many evaluations of the driving Brownian motion is at most $1/2$ when the drift is locally bounded and the diffusion is locally Lipschitz continuous.","authors":["Simon Ellinger"],"url":"https://arxiv.org/abs/2505.00656"}
{"created":"2025-05-02","title":"Key exchange protocol based on circulant matrix action over congruence-simple semiring","abstract":"We present a new key exchange protocol based on circulant matrices acting on matrices over a congruence-simple semiring. We describe how to compute matrices with the necessary properties for the implementation of the protocol. Additionally, we provide an analysis of its computational cost and its security against known attacks.","authors":["Alvaro Otero Sanchez"],"url":"https://arxiv.org/abs/2505.00664"}
{"created":"2025-05-02","title":"TumorTwin: A python framework for patient-specific digital twins in oncology","abstract":"Background: Advances in the theory and methods of computational oncology have enabled accurate characterization and prediction of tumor growth and treatment response on a patient-specific basis. This capability can be integrated into a digital twin framework in which bi-directional data-flow between the physical tumor and the digital tumor facilitate dynamic model re-calibration, uncertainty quantification, and clinical decision-support via recommendation of optimal therapeutic interventions. However, many digital twin frameworks rely on bespoke implementations tailored to each disease site, modeling choice, and algorithmic implementation.","authors":["Michael Kapteyn","Anirban Chaudhuri","Ernesto A. B. F. Lima","Graham Pash","Rafael Bravo","Karen Willcox","Thomas E. Yankeelov","David A. Hormuth II"],"url":"https://arxiv.org/abs/2505.00670"}
{"created":"2025-05-02","title":"GuideSR: Rethinking Guidance for One-Step High-Fidelity Diffusion-Based Super-Resolution","abstract":"In this paper, we propose GuideSR, a novel single-step diffusion-based image super-resolution (SR) model specifically designed to enhance image fidelity. Existing diffusion-based SR approaches typically adapt pre-trained generative models to image restoration tasks by adding extra conditioning on a VAE-downsampled representation of the degraded input, which often compromises structural fidelity. GuideSR addresses this limitation by introducing a dual-branch architecture comprising: (1) a Guidance Branch that preserves high-fidelity structures from the original-resolution degraded input, and (2) a Diffusion Branch, which a pre-trained latent diffusion model to enhance perceptual quality. Unlike conventional conditioning mechanisms, our Guidance Branch features a tailored structure for image restoration tasks, combining Full Resolution Blocks (FRBs) with channel attention and an Image Guidance Network (IGN) with guided attention. By embedding detailed structural information directly into the restoration pipeline, GuideSR produces sharper and more visually consistent results. Extensive experiments on benchmark datasets demonstrate that GuideSR achieves state-of-the-art performance while maintaining the low computational cost of single-step approaches, with up to 1.39dB PSNR gain on challenging real-world datasets. Our approach consistently outperforms existing methods across various reference-based metrics including PSNR, SSIM, LPIPS, DISTS and FID, further representing a practical advancement for real-world image restoration.","authors":["Aditya Arora","Zhengzhong Tu","Yufei Wang","Ruizheng Bai","Jian Wang","Sizhuo Ma"],"url":"https://arxiv.org/abs/2505.00687"}
{"created":"2025-05-02","title":"Physical Limits and Optimal Synthesis of Beyond Diagonal Anomalous Scatterers","abstract":"Realizing metasurfaces for anomalous scattering is fundamental to designing reflector arrays, reconfigurable intelligent surfaces, and metasurface antennas. However, the basic cost of steering scattering into non-specular directions is not fully understood. This paper derives tight physical bounds on anomalous scattering using antenna array systems equipped with non-local matching networks. The matching networks are explicitly synthesized based on the solutions of the optimization problems that define these bounds. Furthermore, we analyze fundamental limits for metasurface antennas implemented with metallic and dielectric materials exhibiting minimal loss within a finite design region. The results reveal a typical 6dB reduction in bistatic radar cross section (RCS) in anomalous directions compared to the forward direction. Numerical examples complement the theory and illustrate the inherent cost of achieving anomalous scattering relative to forward or specular scattering for canonical configurations.","authors":["Mats Gustafsson"],"url":"https://arxiv.org/abs/2505.00691"}
{"created":"2025-05-02","title":"Minimizing $\\ell_2$ Norm of Flow Time by Starvation Mitigation","abstract":"The assessment of a job's Quality of Service (QoS) often revolves around its flow time, also referred to as response time. This study delves into two fundamental objectives for scheduling jobs: the average flow time and the maximum flow time. While the Shortest Remaining Processing Time (SRPT) algorithm minimizes average flow time, it can result in job starvation, causing certain jobs to experience disproportionately long and unfair flow times. In contrast, the First-Come-First-Served (FCFS) algorithm minimizes the maximum flow time but may compromise the average flow time.","authors":["Tung-Wei Kuo"],"url":"https://arxiv.org/abs/2112.14403"}
{"created":"2025-05-02","title":"Characterizing Human Actions in the Digital Platform by Temporal Context","abstract":"Recent advances in digital platforms generate rich, high-dimensional logs of human behavior, and machine learning models have helped social scientists explain knowledge accumulation, communication, and information diffusion. Such models, however, almost always treat behavior as sequences of actions, abstracting the inter-temporal information among actions. To close this gap, we introduce a two-scale Action-Timing Context(ATC) framework that jointly embeds each action and its time interval. ATC obtains low-dimensional representations of actions and characterizes them with inter-temporal information. We provide three applications of ATC to real-world datasets and demonstrate that the method offers a unified view of human behavior. The presented qualitative findings demonstrate that explicitly modeling inter-temporal context is essential for a comprehensive, interpretable understanding of human activity on digital platforms.","authors":["Akira Matsui","Emilio Ferrara"],"url":"https://arxiv.org/abs/2206.09535"}
{"created":"2025-05-02","title":"Introduction to Online Control","abstract":"This text presents an introduction to an emerging paradigm in control of dynamical systems and differentiable reinforcement learning called online nonstochastic control. The new approach applies techniques from online convex optimization and convex relaxations to obtain new methods with provable guarantees for classical settings in optimal and robust control.","authors":["Elad Hazan","Karan Singh"],"url":"https://arxiv.org/abs/2211.09619"}
{"created":"2025-05-02","title":"Sim-Anchored Learning for On-the-Fly Adaptation","abstract":"Fine-tuning simulation-trained RL agents with real-world data often degrades crucial behaviors due to limited or skewed data distributions. We argue that designer priorities exist not just in reward functions, but also in simulation design choices like task selection and state initialization. When adapting to real-world data, agents can experience catastrophic forgetting in important but underrepresented scenarios. We propose framing live-adaptation as a multi-objective optimization problem, where policy objectives must be satisfied both in simulation and reality. Our approach leverages critics from simulation as \"anchors for design intent\" (anchor critics). By jointly optimizing policies against both anchor critics and critics trained on real-world experience, our method enables adaptation while preserving prioritized behaviors from simulation. Evaluations demonstrate robust behavior retention in sim-to-sim benchmarks and a sim-to-real scenario with a racing quadrotor, allowing for power consumption reductions of up to 50% without control loss. We also contribute SwaNNFlight, an open-source firmware for enabling live adaptation on similar robotic platforms.","authors":["Bassel El Mabsout","Shahin Roozkhosh","Siddharth Mysore","Kate Saenko","Renato Mancuso"],"url":"https://arxiv.org/abs/2301.06987"}
{"created":"2025-05-02","title":"Learning Against Distributional Uncertainty: On the Trade-off Between Robustness and Specificity","abstract":"Trustworthy machine learning aims at combating distributional uncertainties in training data distributions compared to population distributions. Typical treatment frameworks include the Bayesian approach, (min-max) distributionally robust optimization (DRO), and regularization. However, three issues have to be raised: 1) the prior distribution in the Bayesian method and the regularizer in the regularization method are difficult to specify; 2) the DRO method tends to be overly conservative; 3) all the three methods are biased estimators of the true optimal cost. This paper studies a new framework that unifies the three approaches and addresses the three challenges above. The asymptotic properties (e.g., consistencies and asymptotic normalities), non-asymptotic properties (e.g., generalization bounds and unbiasedness), and solution methods of the proposed model are studied. The new model reveals the trade-off between the robustness to the unseen data and the specificity to the training data. Experiments on various real-world tasks validate the superiority of the proposed learning framework.","authors":["Shixiong Wang","Haowei Wang","Xinke Li","Jean Honorio"],"url":"https://arxiv.org/abs/2301.13565"}
{"created":"2025-05-02","title":"Learning An Active Inference Model of Driver Perception and Control: Application to Vehicle Car-Following","abstract":"In this paper we introduce a general estimation methodology for learning a model of human perception and control in a sensorimotor control task based upon a finite set of demonstrations. The model's structure consists of i the agent's internal representation of how the environment and associated observations evolve as a result of control actions and ii the agent's preferences over observable outcomes. We consider a model's structure specification consistent with active inference, a theory of human perception and behavior from cognitive science. According to active inference, the agent acts upon the world so as to minimize surprise defined as a measure of the extent to which an agent's current sensory observations differ from its preferred sensory observations. We propose a bi-level optimization approach to estimation which relies on a structural assumption on prior distributions that parameterize the statistical accuracy of the human agent's model of the environment. To illustrate the proposed methodology, we present the estimation of a model for car-following behavior based upon a naturalistic dataset. Overall, the results indicate that learning active inference models of human perception and control from data is a promising alternative to black-box models of driving.","authors":["Ran Wei","Anthony D. McDonald","Alfredo Garcia","Gustav Markkula","Johan Engstrom","Matthew O'Kelly"],"url":"https://arxiv.org/abs/2303.15201"}
{"created":"2025-05-02","title":"Analysis and systematic discretization of a Fokker-Planck equation with Lorentz force","abstract":"The propagation of charged particles through a scattering medium in the presence of a magnetic field can be described by a Fokker-Planck equation with Lorentz force. This model is studied both, from a theoretical and a numerical point of view. A particular trace estimate is derived for the relevant function spaces to clarify the meaning of boundary values. Existence of a weak solution is then proven by the Rothe method. In the second step of our investigations, a fully practical discretization scheme is proposed based on an implicit Euler method for the energy variable and a spherical-harmonics finite-element discretization with respect to the remaining variables. A complete error analysis of the resulting scheme is given and numerical test are presented to illustrate the theoretical results and the performance of the proposed method.","authors":["Vincent Bosboom","Herbert Egger","Matthias Schlottbom"],"url":"https://arxiv.org/abs/2304.01937"}
{"created":"2025-05-02","title":"Interpretability-Aware Vision Transformer","abstract":"Vision Transformers (ViTs) have become prominent models for solving various vision tasks. However, the interpretability of ViTs has not kept pace with their promising performance. While there has been a surge of interest in developing {\\it post hoc} solutions to explain ViTs' outputs, these methods do not generalize to different downstream tasks and various transformer architectures. Furthermore, if ViTs are not properly trained with the given data and do not prioritize the region of interest, the {\\it post hoc} methods would be less effective. Instead of developing another {\\it post hoc} approach, we introduce a novel training procedure that inherently enhances model interpretability. Our interpretability-aware ViT (IA-ViT) draws inspiration from a fresh insight: both the class patch and image patches consistently generate predicted distributions and attention maps. IA-ViT is composed of a feature extractor, a predictor, and an interpreter, which are trained jointly with an interpretability-aware training objective. Consequently, the interpreter simulates the behavior of the predictor and provides a faithful explanation through its single-head self-attention mechanism. Our comprehensive experimental results demonstrate the effectiveness of IA-ViT in several image classification tasks, with both qualitative and quantitative evaluations of model performance and interpretability. Source code is available from: https://github.com/qiangyao1988/IA-ViT.","authors":["Yao Qiang","Chengyin Li","Prashant Khanduri","Dongxiao Zhu"],"url":"https://arxiv.org/abs/2309.08035"}
{"created":"2025-05-02","title":"EvoPrompt: Connecting LLMs with Evolutionary Algorithms Yields Powerful Prompt Optimizers","abstract":"Large Language Models (LLMs) excel in various tasks, but they rely on carefully crafted prompts that often demand substantial human effort. To automate this process, in this paper, we propose a novel framework for discrete prompt optimization, called EvoPrompt, which borrows the idea of evolutionary algorithms (EAs) as they exhibit good performance and fast convergence. To enable EAs to work on discrete prompts, which are natural language expressions that need to be coherent and human-readable, we connect LLMs with EAs. This approach allows us to simultaneously leverage the powerful language processing capabilities of LLMs and the efficient optimization performance of EAs. Specifically, abstaining from any gradients or parameters, EvoPrompt starts from a population of prompts and iteratively generates new prompts with LLMs based on the evolutionary operators, improving the population based on the development set. We optimize prompts for both closed- and open-source LLMs including GPT-3.5 and Alpaca, on 31 datasets covering language understanding, generation tasks, as well as BIG-Bench Hard (BBH) tasks. EvoPrompt significantly outperforms human-engineered prompts and existing methods for automatic prompt generation (e.g., up to 25% on BBH). Furthermore, EvoPrompt demonstrates that connecting LLMs with EAs creates synergies, which could inspire further research on the combination of LLMs and conventional algorithms.","authors":["Qingyan Guo","Rui Wang","Junliang Guo","Bei Li","Kaitao Song","Xu Tan","Guoqing Liu","Jiang Bian","Yujiu Yang"],"url":"https://arxiv.org/abs/2309.08532"}
{"created":"2025-05-02","title":"Omitted Labels Induce Nontransitive Paradoxes in Causality","abstract":"We explore \"omitted label contexts,\" in which training data is limited to a subset of the possible labels. This setting is standard among specialized human experts or specific, focused studies. By studying Simpson's paradox, we observe that ``correct'' adjustments sometimes require non-exchangeable treatment and control groups. A generalization of Simpson's paradox leads us to study networks of conclusions drawn from different contexts, within which a paradox of nontransitivity arises. We prove that the space of possible nontransitive structures in these networks exactly corresponds to structures that form from aggregating ranked-choice votes.","authors":["Bijan Mazaheri","Siddharth Jain","Matthew Cook","Jehoshua Bruck"],"url":"https://arxiv.org/abs/2311.06840"}
{"created":"2025-05-02","title":"Disentangle Before Anonymize: A Two-stage Framework for Attribute-preserved and Occlusion-robust De-identification","abstract":"In an era where personal photos are easily leaked and collected, face de-identification is a crucial method for protecting identity privacy. However, current face de-identification techniques face challenges in preserving attribute details and often produce anonymized results with reduced authenticity. These shortcomings are particularly evident when handling occlusions,frequently resulting in noticeable editing artifacts. Our primary finding in this work is that simultaneous training of identity disentanglement and anonymization hinders their respective effectiveness.Therefore, we propose \"Disentangle Before Anonymize\",a novel two-stage Framework(DBAF)designed for attributepreserved and occlusion-robust de-identification. This framework includes a Contrastive Identity Disentanglement (CID) module and a Key-authorized Reversible Identity Anonymization (KRIA) module, achieving faithful attribute preservation and high-quality identity anonymization edits. Additionally, we introduce a Multiscale Attentional Attribute Retention (MAAR) module to address the issue of reduced anonymization quality under occlusions.Extensive experiments demonstrate that our method outperforms state-of-the-art de-identification approaches, delivering superior quality, enhanced detail fidelity, improved attribute preservation performance, and greater robustness to occlusions.","authors":["Mingrui Zhu","Dongxin Chen","Xin Wei","Nannan Wang","Xinbo Gao"],"url":"https://arxiv.org/abs/2311.08786"}
{"created":"2025-05-02","title":"Class Uncertainty: A Measure to Mitigate Class Imbalance","abstract":"Class-wise characteristics of training examples affect the performance of deep classifiers. A well-studied example is when the number of training examples of classes follows a long-tailed distribution, a situation that is likely to yield sub-optimal performance for under-represented classes. This class imbalance problem is conventionally addressed by approaches relying on the class-wise cardinality of training examples, such as data resampling. In this paper, we demonstrate that considering solely the cardinality of classes does not cover all issues causing class imbalance. To measure class imbalance, we propose \"Class Uncertainty\" as the average predictive uncertainty of the training examples, and we show that this novel measure captures the differences across classes better than cardinality. We also curate SVCI-20 as a novel dataset in which the classes have equal number of training examples but they differ in terms of their hardness; thereby causing a type of class imbalance which cannot be addressed by the approaches relying on cardinality. We incorporate our \"Class Uncertainty\" measure into a diverse set of ten class imbalance mitigation methods to demonstrate its effectiveness on long-tailed datasets as well as on our SVCI-20. Code and datasets will be made available.","authors":["Z. S. Baltaci","K. Oksuz","S. Kuzucu","K. Tezoren","B. K. Konar","A. Ozkan","E. Akbas","S. Kalkan"],"url":"https://arxiv.org/abs/2311.14090"}
{"created":"2025-05-02","title":"Scene-Conditional 3D Object Stylization and Composition","abstract":"Recently, 3D generative models have made impressive progress, enabling the generation of almost arbitrary 3D assets from text or image inputs. However, these approaches generate objects in isolation without any consideration for the scene where they will eventually be placed. In this paper, we propose a framework that allows for the stylization of an existing 3D asset to fit into a given 2D scene, and additionally produce a photorealistic composition as if the asset was placed within the environment. This not only opens up a new level of control for object stylization, for example, the same assets can be stylized to reflect changes in the environment, such as summer to winter or fantasy versus futuristic settings-but also makes the object-scene composition more controllable. We achieve this by combining modeling and optimizing the object's texture and environmental lighting through differentiable ray tracing with image priors from pre-trained text-to-image diffusion models. We demonstrate that our method is applicable to a wide variety of indoor and outdoor scenes and arbitrary objects. Project page: https://jensenzhoujh.github.io/scene-cond-3d/.","authors":["Jinghao Zhou","Tomas Jakab","Philip Torr","Christian Rupprecht"],"url":"https://arxiv.org/abs/2312.12419"}
{"created":"2025-05-02","title":"AMM-based DEX on the XRP Ledger","abstract":"Automated Market Maker (AMM)-based Decentralized Exchanges (DEXs) are crucial in Decentralized Finance (DeFi), but Ethereum implementations suffer from high transaction costs and price synchronization challenges. To address these limitations, we compare the XRP Ledger (XRPL)-AMM-Decentralized Exchange (DEX), a protocol-level implementation, against a Generic AMM-based DEX (G-AMM-DEX) on Ethereum, akin to Uniswap's V2 AMM implementation, through agent-based simulations using real market data and multiple volatility scenarios generated via Geometric Brownian Motion (GBM). Results demonstrate that the XRPL-AMM-DEX achieves superior price synchronization, reduced slippage, and improved returns due to XRPL's lower fees and shorter block times, with benefits amplifying during market volatility. The integrated Continuous Auction Mechanism (CAM) further mitigates impermanent loss by redistributing arbitrage value to Liquidity Providers (LPs). To the best of our knowledge, this study represents the first comparative analysis between protocol-level and smart contract AMM-based DEX implementations and the first agent-based simulation validating theoretical auction mechanisms for AMM-based DEXs.","authors":["Walter Hernandez Cruz","Firas Dahi","Yebo Feng","Jiahua Xu","Aanchal Malhotra","Paolo Tasca"],"url":"https://arxiv.org/abs/2312.13749"}
{"created":"2025-05-02","title":"Latte: Latent Diffusion Transformer for Video Generation","abstract":"We propose Latte, a novel Latent Diffusion Transformer for video generation. Latte first extracts spatio-temporal tokens from input videos and then adopts a series of Transformer blocks to model video distribution in the latent space. In order to model a substantial number of tokens extracted from videos, four efficient variants are introduced from the perspective of decomposing the spatial and temporal dimensions of input videos. To improve the quality of generated videos, we determine the best practices of Latte through rigorous experimental analysis, including video clip patch embedding, model variants, timestep-class information injection, temporal positional embedding, and learning strategies. Our comprehensive evaluation demonstrates that Latte achieves state-of-the-art performance across four standard video generation datasets, i.e., FaceForensics, SkyTimelapse, UCF101, and Taichi-HD. In addition, we extend Latte to the text-to-video generation (T2V) task, where Latte achieves results that are competitive with recent T2V models. We strongly believe that Latte provides valuable insights for future research on incorporating Transformers into diffusion models for video generation.","authors":["Xin Ma","Yaohui Wang","Xinyuan Chen","Gengyun Jia","Ziwei Liu","Yuan-Fang Li","Cunjian Chen","Yu Qiao"],"url":"https://arxiv.org/abs/2401.03048"}
{"created":"2025-05-02","title":"LegalDuet: Learning Fine-grained Representations for Legal Judgment Prediction via a Dual-View Contrastive Learning","abstract":"Legal Judgment Prediction (LJP) is a fundamental task of legal artificial intelligence, aiming to automatically predict the judgment outcomes of legal cases. Existing LJP models primarily focus on identifying legal triggers within criminal fact descriptions by contrastively training language models. However, these LJP models overlook the importance of learning to effectively distinguish subtle differences among judgments, which is crucial for producing more accurate predictions. In this paper, we propose LegalDuet, which continuously pretrains language models to learn a more tailored embedding space for representing legal cases. Specifically, LegalDuet designs a dual-view mechanism to continuously pretrain language models: 1) Law Case Clustering retrieves similar cases as hard negatives and employs contrastive training to differentiate among confusing cases; 2) Legal Decision Matching aims to identify legal clues within criminal fact descriptions to align them with the chain of reasoning that contains the correct legal decision. Our experiments on the CAIL2018 dataset demonstrate the effectiveness of LegalDuet. Further analysis reveals that LegalDuet improves the ability of pretrained language models to distinguish confusing criminal charges by reducing prediction uncertainty and enhancing the separability of criminal charges. The experiments demonstrate that LegalDuet produces a more concentrated and distinguishable embedding space, effectively aligning criminal facts with corresponding legal decisions. The code is available at https://github.com/NEUIR/LegalDuet.","authors":["Buqiang Xu","Xin Dai","Zhenghao Liu","Huiyuan Xie","Xiaoyuan Yi","Shuo Wang","Yukun Yan","Liner Yang","Yu Gu","Ge Yu"],"url":"https://arxiv.org/abs/2401.15371"}
{"created":"2025-05-02","title":"CATO: End-to-End Optimization of ML-Based Traffic Analysis Pipelines","abstract":"Machine learning has shown tremendous potential for improving the capabilities of network traffic analysis applications, often outperforming simpler rule-based heuristics. However, ML-based solutions remain difficult to deploy in practice. Many existing approaches only optimize the predictive performance of their models, overlooking the practical challenges of running them against network traffic in real time. This is especially problematic in the domain of traffic analysis, where the efficiency of the serving pipeline is a critical factor in determining the usability of a model. In this work, we introduce CATO, a framework that addresses this problem by jointly optimizing the predictive performance and the associated systems costs of the serving pipeline. CATO leverages recent advances in multi-objective Bayesian optimization to efficiently identify Pareto-optimal configurations, and automatically compiles end-to-end optimized serving pipelines that can be deployed in real networks. Our evaluations show that compared to popular feature optimization techniques, CATO can provide up to 3600x lower inference latency and 3.7x higher zero-loss throughput while simultaneously achieving better model performance.","authors":["Gerry Wan","Shinan Liu","Francesco Bronzino","Nick Feamster","Zakir Durumeric"],"url":"https://arxiv.org/abs/2402.06099"}
{"created":"2025-05-02","title":"Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models","abstract":"Large Language Models (LLMs) with the Mixture-of-Experts (MoE) architectures have shown promising performance on various tasks. However, due to the huge model sizes, running them in resource-constrained environments where the GPU memory is not abundant is challenging. Some existing systems propose to use CPU resources to solve that, but they either suffer from the significant overhead of frequently moving data between CPU and GPU, or fail to consider distinct characteristics of CPUs and GPUs. This paper proposes Fiddler, a resource-efficient inference system for MoE models with limited GPU resources. Fiddler strategically utilizes CPU and GPU resources by determining the optimal execution strategy. Our evaluation shows that, unlike state-of-the-art systems that optimize for specific scenarios such as single batch inference or long prefill, Fiddler performs better in all scenarios. Compared against different baselines, Fiddler achieves 1.26 times speed up in single batch inference, 1.30 times in long prefill processing, and 11.57 times in beam search inference. The code of Fiddler is publicly available at https://github.com/efeslab/fiddler.","authors":["Keisuke Kamahori","Tian Tang","Yile Gu","Kan Zhu","Baris Kasikci"],"url":"https://arxiv.org/abs/2402.07033"}
{"created":"2025-05-02","title":"\"Reasoning\" with Rhetoric: On the Style-Evidence Tradeoff in LLM-Generated Counter-Arguments","abstract":"Large language models (LLMs) play a key role in generating evidence-based and stylistic counter-arguments, yet their effectiveness in real-world applications has been underexplored. Previous research often neglects the balance between evidentiality and style, which are crucial for persuasive arguments. To address this, we evaluated the effectiveness of stylized evidence-based counter-argument generation in Counterfire, a new dataset of 38,000 counter-arguments generated by revising counter-arguments to Reddit's ChangeMyView community to follow different discursive styles. We evaluated generic and stylized counter-arguments from basic and fine-tuned models such as GPT-3.5, PaLM-2, and Koala-13B, as well as newer models (GPT-4o, Claude Haiku, LLaMA-3.1) focusing on rhetorical quality and persuasiveness. Our findings reveal that humans prefer stylized counter-arguments over the original outputs, with GPT-3.5 Turbo performing well, though still not reaching human standards of rhetorical quality nor persuasiveness. Additionally, our work created a novel argument triplets dataset for studying style control, with human preference labels that provide insights into the tradeoffs between evidence integration and argument quality.","authors":["Preetika Verma","Kokil Jaidka","Svetlana Churina"],"url":"https://arxiv.org/abs/2402.08498"}
{"created":"2025-05-02","title":"Fairness Risks for Group-conditionally Missing Demographics","abstract":"Fairness-aware classification models have gained increasing attention in recent years as concerns grow on discrimination against some demographic groups. Most existing models require full knowledge of the sensitive features, which can be impractical due to privacy, legal issues, and an individual's fear of discrimination. The key challenge we will address is the group dependency of the unavailability, e.g., people of some age range may be more reluctant to reveal their age. Our solution augments general fairness risks with probabilistic imputations of the sensitive features, while jointly learning the group-conditionally missing probabilities in a variational auto-encoder. Our model is demonstrated effective on both image and tabular datasets, achieving an improved balance between accuracy and fairness.","authors":["Kaiqi Jiang","Wenzhe Fan","Mao Li","Xinhua Zhang"],"url":"https://arxiv.org/abs/2402.13393"}
{"created":"2025-05-02","title":"A new sociology of humans and machines","abstract":"From fake social media accounts and generative artificial intelligence chatbots to trading algorithms and self-driving vehicles, robots, bots and algorithms are proliferating and permeating our communication channels, social interactions, economic transactions and transportation arteries. Networks of multiple interdependent and interacting humans and intelligent machines constitute complex social systems for which the collective outcomes cannot be deduced from either human or machine behaviour alone. Under this paradigm, we review recent research and identify general dynamics and patterns in situations of competition, coordination, cooperation, contagion and collective decision-making, with context-rich examples from high-frequency trading markets, a social media platform, an open collaboration community and a discussion forum. To ensure more robust and resilient human-machine communities, we require a new sociology of humans and machines. Researchers should study these communities using complex system methods; engineers should explicitly design artificial intelligence for human-machine and machine-machine interactions; and regulators should govern the ecological diversity and social co-development of humans and machines.","authors":["Milena Tsvetkova","Taha Yasseri","Niccolo Pescetelli","Tobias Werner"],"url":"https://arxiv.org/abs/2402.14410"}
{"created":"2025-05-02","title":"LoRATK: LoRA Once, Backdoor Everywhere in the Share-and-Play Ecosystem","abstract":"Finetuning LLMs with LoRA has gained significant popularity due to its simplicity and effectiveness. Often, users may even find pluggable, community-shared LoRAs to enhance their base models for a specific downstream task of interest; enjoying a powerful, efficient, yet customized LLM experience with negligible investment. However, this convenient share-and-play ecosystem also introduces a new attack surface, where attackers can distribute malicious LoRAs to a community eager to try out shared assets. Despite the high-risk potential, no prior art has comprehensively explored LoRA's attack surface under the downstream-enhancing share-and-play context. In this paper, we investigate how backdoors can be injected into task-enhancing LoRAs and examine the mechanisms of such infections. We find that with a simple, efficient, yet specific recipe, a backdoor LoRA can be trained once and then seamlessly merged (in a training-free fashion) with multiple task-enhancing LoRAs, retaining both its malicious backdoor and benign downstream capabilities. This allows attackers to scale the distribution of compromised LoRAs with minimal effort by leveraging the rich pool of existing shared LoRA assets. We note that such merged LoRAs are particularly infectious -- because their malicious intent is cleverly concealed behind improved downstream capabilities, creating a strong incentive for voluntary download -- and dangerous -- because under local deployment, no safety measures exist to intervene when things go wrong. Our work is among the first to study this new threat model of training-free distribution of downstream-capable-yet-backdoor-injected LoRAs, highlighting the urgent need for heightened security awareness in the LoRA ecosystem. Warning: This paper contains offensive content and involves a real-life tragedy.","authors":["Hongyi Liu","Shaochen Zhong","Xintong Sun","Minghao Tian","Mohsen Hariri","Zirui Liu","Ruixiang Tang","Zhimeng Jiang","Jiayi Yuan","Yu-Neng Chuang","Li Li","Soo-Hyun Choi","Rui Chen","Vipin Chaudhary","Xia Hu"],"url":"https://arxiv.org/abs/2403.00108"}
{"created":"2025-05-02","title":"FOOL: Addressing the Downlink Bottleneck in Satellite Computing with Neural Feature Compression","abstract":"Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks. This work presents FOOL, an OEC-native and task-agnostic feature compression method that preserves prediction performance. FOOL partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While FOOL is a feature compressor, it can recover images with competitive scores on quality measures at lower bitrates. We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. Lastly, we test the feasibility of our system for standardized nanosatellite form factors. We demonstrate that FOOL permits downlinking over 100x the data volume without relying on prior information on the downstream tasks.","authors":["Alireza Furutanpey","Qiyang Zhang","Philipp Raith","Tobias Pfandzelter","Shangguang Wang","Schahram Dustdar"],"url":"https://arxiv.org/abs/2403.16677"}
{"created":"2025-05-02","title":"Large Language Model Agent as a Mechanical Designer","abstract":"Conventional mechanical design follows an iterative process in which initial concepts are refined through cycles of expert assessment and resource-intensive Finite Element Method (FEM) analysis to meet performance goals. While machine learning models have been developed to assist in parts of this process, they typically require large datasets, extensive training, and are often tailored to specific tasks, limiting their generalizability. To address these limitations, we propose a framework that leverages a pretrained Large Language Model (LLM) in conjunction with an FEM module to autonomously generate, evaluate, and refine structural designs based on performance specifications and numerical feedback. The LLM operates without domain-specific fine-tuning, using general reasoning to propose design candidates, interpret FEM-derived performance metrics, and apply structurally sound modifications. Using 2D truss structures as a testbed, we show that the LLM can effectively navigate highly discrete and multi-faceted design spaces, balance competing objectives, and identify convergence when further optimization yields diminishing returns. Compared to Non-dominated Sorting Genetic Algorithm II (NSGA-II), our method achieves faster convergence and fewer FEM evaluations. Experiments with varying temperature settings (0.5, 1.0, 1.2) and model sizes (GPT-4.1 and GPT-4.1-mini) indicate that smaller models yield higher constraint satisfaction with fewer steps, while lower temperatures enhance design consistency. These results establish LLMs as a promising new class of reasoning-based, natural language-driven optimizers for autonomous design and iterative structural refinement.","authors":["Yayati Jadhav","Amir Barati Farimani"],"url":"https://arxiv.org/abs/2404.17525"}
{"created":"2025-05-02","title":"F2M-Reg: Unsupervised RGB-D Point Cloud Registration with Frame-to-Model Optimization","abstract":"This work studies the problem of unsupervised RGB-D point cloud registration, which aims at training a robust registration model without ground-truth pose supervision. Existing methods usually leverages unposed RGB-D sequences and adopt a frame-to-frame framework based on differentiable rendering to train the registration model, which enforces the photometric and geometric consistency between the two frames for supervision. However, this frame-to-frame framework is vulnerable to inconsistent factors between different frames, e.g., lighting changes, geometry occlusion, and reflective materials, which leads to suboptimal convergence of the registration model. In this paper, we propose a novel frame-to-model optimization framework named F2M-Reg for unsupervised RGB-D point cloud registration. We leverage the neural implicit field as a global model of the scene and optimize the estimated poses of the frames by registering them to the global model, and the registration model is subsequently trained with the optimized poses. Thanks to the global encoding capability of neural implicit field, our frame-to-model framework is significantly more robust to inconsistent factors between different frames and thus can provide better supervision for the registration model. Besides, we demonstrate that F2M-Reg can be further enhanced by a simplistic synthetic warming-up strategy. To this end, we construct a photorealistic synthetic dataset named Sim-RGBD to initialize the registration model for the frame-to-model optimization on real-world RGB-D sequences. Extensive experiments on four challenging benchmarks have shown that our method surpasses the previous state-of-the-art counterparts by a large margin, especially under scenarios with severe lighting changes and low overlap. Our code and models are available at https://github.com/MrIsland/F2M_Reg.","authors":["Zhinan Yu","Zheng Qin","Yijie Tang","Yongjun Wang","Renjiao Yi","Chenyang Zhu","Kai Xu"],"url":"https://arxiv.org/abs/2405.00507"}
{"created":"2025-05-02","title":"S3Former: Self-supervised High-resolution Transformer for Solar PV Profiling","abstract":"As the impact of climate change escalates, the global necessity to transition to sustainable energy sources becomes increasingly evident. Renewable energies have emerged as a viable solution for users, with Photovoltaic energy being a favored choice for small installations due to its reliability and efficiency. Accurate mapping of PV installations is crucial for understanding the extension of its adoption and informing energy policy. To meet this need, we introduce S3Former, designed to segment solar panels from aerial imagery and provide size and location information critical for analyzing the impact of such installations on the grid. Solar panel identification is challenging due to factors such as varying weather conditions, roof characteristics, Ground Sampling Distance variations and lack of appropriate initialization weights for optimized training. To tackle these complexities, S3Former features a Masked Attention Mask Transformer incorporating a self-supervised learning pretrained backbone. Specifically, our model leverages low-level and high-level features extracted from the backbone and incorporates an instance query mechanism incorporated on the Transformer architecture to enhance the localization of solar PV installations. We introduce a self-supervised learning phase (pretext task) to improve the initialization weights on the backbone of S3Former. We evaluated S3Former using diverse datasets, demonstrate improvement state-of-the-art models.","authors":["Minh Tran","Adrian De Luis","Haitao Liao","Ying Huang","Roy McCann","Alan Mantooth","Jack Cothren","Ngan Le"],"url":"https://arxiv.org/abs/2405.04489"}
{"created":"2025-05-02","title":"QServe: W4A8KV4 Quantization and System Co-design for Efficient LLM Serving","abstract":"Quantization can accelerate large language model (LLM) inference. Going beyond INT8 quantization, the research community is actively exploring even lower precision, such as INT4. Nonetheless, state-of-the-art INT4 quantization techniques only accelerate low-batch, edge LLM inference, failing to deliver performance gains in large-batch, cloud-based LLM serving. We uncover a critical issue: existing INT4 quantization methods suffer from significant runtime overhead (20-90%) when dequantizing either weights or partial sums on GPUs. To address this challenge, we introduce QoQ, a W4A8KV4 quantization algorithm with 4-bit weight, 8-bit activation, and 4-bit KV cache. QoQ stands for quattuor-octo-quattuor, which represents 4-8-4 in Latin. QoQ is implemented by the QServe inference library that achieves measured speedup. The key insight driving QServe is that the efficiency of LLM serving on GPUs is critically influenced by operations on low-throughput CUDA cores. Building upon this insight, in QoQ algorithm, we introduce progressive quantization that can allow low dequantization overhead in W4A8 GEMM. Additionally, we develop SmoothAttention to effectively mitigate the accuracy degradation incurred by 4-bit KV quantization. In the QServe system, we perform compute-aware weight reordering and take advantage of register-level parallelism to reduce dequantization latency. We also make fused attention memory-bound, harnessing the performance gain brought by KV4 quantization. As a result, QServe improves the maximum achievable serving throughput of Llama-3-8B by 1.2x on A100, 1.4x on L40S; and Qwen1.5-72B by 2.4x on A100, 3.5x on L40S, compared to TensorRT-LLM. Remarkably, QServe on L40S GPU can achieve even higher throughput than TensorRT-LLM on A100. Thus, QServe effectively reduces the dollar cost of LLM serving by 3x. Code is available at https://github.com/mit-han-lab/omniserve.","authors":["Yujun Lin","Haotian Tang","Shang Yang","Zhekai Zhang","Guangxuan Xiao","Chuang Gan","Song Han"],"url":"https://arxiv.org/abs/2405.04532"}
{"created":"2025-05-02","title":"RobMOT: Robust 3D Multi-Object Tracking by Observational Noise and State Estimation Drift Mitigation on LiDAR PointCloud","abstract":"This paper addresses limitations in 3D tracking-by-detection methods, particularly in identifying legitimate trajectories and reducing state estimation drift in Kalman filters. Existing methods often use threshold-based filtering for detection scores, which can fail for distant and occluded objects, leading to false positives. To tackle this, we propose a novel track validity mechanism and multi-stage observational gating process, significantly reducing ghost tracks and enhancing tracking performance. Our method achieves a $29.47\\%$ improvement in Multi-Object Tracking Accuracy (MOTA) on the KITTI validation dataset with the Second detector. Additionally, a refined Kalman filter term reduces localization noise, improving higher-order tracking accuracy (HOTA) by $4.8\\%$. The online framework, RobMOT, outperforms state-of-the-art methods across multiple detectors, with HOTA improvements of up to $3.92\\%$ on the KITTI testing dataset and $8.7\\%$ on the validation dataset, while achieving low identity switch scores. RobMOT excels in challenging scenarios, tracking distant objects and prolonged occlusions, with a $1.77\\%$ MOTA improvement on the Waymo Open dataset, and operates at a remarkable 3221 FPS on a single CPU, proving its efficiency for real-time multi-object tracking.","authors":["Mohamed Nagy","Naoufel Werghi","Bilal Hassan","Jorge Dias","Majid Khonji"],"url":"https://arxiv.org/abs/2405.11536"}
{"created":"2025-05-02","title":"(Perhaps) Beyond Human Translation: Harnessing Multi-Agent Collaboration for Translating Ultra-Long Literary Texts","abstract":"Literary translation remains one of the most challenging frontiers in machine translation due to the complexity of capturing figurative language, cultural nuances, and unique stylistic elements. In this work, we introduce TransAgents, a novel multi-agent framework that simulates the roles and collaborative practices of a human translation company, including a CEO, Senior Editor, Junior Editor, Translator, Localization Specialist, and Proofreader. The translation process is divided into two stages: a preparation stage where the team is assembled and comprehensive translation guidelines are drafted, and an execution stage that involves sequential translation, localization, proofreading, and a final quality check. Furthermore, we propose two innovative evaluation strategies: Monolingual Human Preference (MHP), which evaluates translations based solely on target language quality and cultural appropriateness, and Bilingual LLM Preference (BLP), which leverages large language models like GPT-4} for direct text comparison. Although TransAgents achieves lower d-BLEU scores, due to the limited diversity of references, its translations are significantly better than those of other baselines and are preferred by both human evaluators and LLMs over traditional human references and GPT-4} translations. Our findings highlight the potential of multi-agent collaboration in enhancing translation quality, particularly for longer texts.","authors":["Minghao Wu","Jiahao Xu","Yulin Yuan","Gholamreza Haffari","Longyue Wang","Weihua Luo","Kaifu Zhang"],"url":"https://arxiv.org/abs/2405.11804"}
{"created":"2025-05-02","title":"Narrative Review of Emotional Expression Support in XR: Psychophysiology of Speech-to-Text Interfaces","abstract":"This narrative review examines recent advancements, limitations, and research gaps in integrating emotional expression into speech-to-text (STT) interfaces within extended reality (XR) environments. Drawing from 37 peer-reviewed studies published between 2020 and 2024, we synthesized literature across multiple domains, including affective computing, psychophysiology, captioning innovation, and immersive human-computer interaction. Thematic categories include communication enhancement technologies for Deaf and Hard of Hearing (DHH) users, emotive captioning strategies, visual and affective augmentation in AR/VR, speech emotion recognition, and the development of empathic systems. Despite the growing accessibility of real-time STT tools, such systems largely fail to convey affective nuance, limiting the richness of communication for DHH users and other caption consumers. This review highlights emerging approaches such as animated captions, emojilization, color-coded overlays, and avatar-based emotion visualization, but finds a persistent gap in real-time emotion-aware captioning within immersive XR contexts. We identify key research opportunities at the intersection of accessibility, XR, and emotional expression, and propose future directions for the development of affect-responsive, user-centered captioning interfaces.","authors":["Sunday David Ubur","Denis Gracanin"],"url":"https://arxiv.org/abs/2405.13924"}
{"created":"2025-05-02","title":"Infinite-dimensional Diffusion Bridge Simulation via Operator Learning","abstract":"The diffusion bridge, which is a diffusion process conditioned on hitting a specific state within a finite period, has found broad applications in various scientific and engineering fields. However, simulating diffusion bridges for modeling natural data can be challenging due to both the intractability of the drift term and continuous representations of the data. Although several methods are available to simulate finite-dimensional diffusion bridges, infinite-dimensional cases remain under explored. This paper presents a method that merges score matching techniques with operator learning, enabling a direct approach to learn the infinite-dimensional bridge and achieving a discretization equivariant bridge simulation. We conduct a series of experiments, ranging from synthetic examples with closed-form solutions to the stochastic nonlinear evolution of real-world biological shape data. Our method demonstrates high efficacy, particularly due to its ability to adapt to any resolution without extra training.","authors":["Gefan Yang","Elizabeth Louise Baker","Michael L. Severinsen","Christy Anna Hipsley","Stefan Sommer"],"url":"https://arxiv.org/abs/2405.18353"}
{"created":"2025-05-02","title":"3D StreetUnveiler with Semantic-aware 2DGS -- a simple baseline","abstract":"Unveiling an empty street from crowded observations captured by in-car cameras is crucial for autonomous driving. However, removing all temporarily static objects, such as stopped vehicles and standing pedestrians, presents a significant challenge. Unlike object-centric 3D inpainting, which relies on thorough observation in a small scene, street scene cases involve long trajectories that differ from previous 3D inpainting tasks. The camera-centric moving environment of captured videos further complicates the task due to the limited degree and time duration of object observation. To address these obstacles, we introduce StreetUnveiler to reconstruct an empty street. StreetUnveiler learns a 3D representation of the empty street from crowded observations. Our representation is based on the hard-label semantic 2D Gaussian Splatting (2DGS) for its scalability and ability to identify Gaussians to be removed. We inpaint rendered image after removing unwanted Gaussians to provide pseudo-labels and subsequently re-optimize the 2DGS. Given its temporal continuous movement, we divide the empty street scene into observed, partial-observed, and unobserved regions, which we propose to locate through a rendered alpha map. This decomposition helps us to minimize the regions that need to be inpainted. To enhance the temporal consistency of the inpainting, we introduce a novel time-reversal framework to inpaint frames in reverse order and use later frames as references for earlier frames to fully utilize the long-trajectory observations. Our experiments conducted on the street scene dataset successfully reconstructed a 3D representation of the empty street. The mesh representation of the empty street can be extracted for further applications. The project page and more visualizations can be found at: https://streetunveiler.github.io","authors":["Jingwei Xu","Yikai Wang","Yiqun Zhao","Yanwei Fu","Shenghua Gao"],"url":"https://arxiv.org/abs/2405.18416"}
{"created":"2025-05-02","title":"A Machine Learning-Based Framework for Assessing Cryptographic Indistinguishability of Lightweight Block Ciphers","abstract":"Indistinguishability is a fundamental principle of cryptographic security, crucial for securing data transmitted between Internet of Things (IoT) devices. This principle ensures that an attacker cannot distinguish between the encrypted data, also known as ciphertext, and random data or the ciphertexts of the two messages encrypted with the same key. This research investigates the ability of machine learning (ML) in assessing indistinguishability property in encryption systems, with a focus on lightweight ciphers. As our first case study, we consider the SPECK32/64 and SIMON32/64 lightweight block ciphers, designed for IoT devices operating under significant energy constraints.","authors":["Jimmy Dani","Kalyan Nakka","Nitesh Saxena"],"url":"https://arxiv.org/abs/2405.19683"}
{"created":"2025-05-02","title":"From Effectiveness to Efficiency: Uncovering Linguistic Bias in Large Language Model-based Code Generation","abstract":"Large Language Models (LLMs) have demonstrated promising capabilities for code generation. While existing benchmarks evaluate the correctness and efficiency of LLM-generated code, the potential linguistic bias - where code quality varies based on the natural language used to describe programming tasks - remains underexplored. In this paper, we aim to investigate this linguistic bias through the lens of English and Chinese. To facilitate our investigation, we present a unified evaluation framework comprising a curated dataset of 52 Python programming questions with parallel bilingual task descriptions, automated correctness verification, and efficiency quantification tools based on runtime complexity estimation. Based on this framework, we conduct the first empirical study towards the linguistic bias in LLM-generated code on eight popular LCGMs, as well as GPT-3.5-Turbo and GPT-4. We observe that these LCGM-generated code show different correctness on an average of 12% bilingual programming tasks, where 39% also exhibits diverse efficiency. Our findings indicate that LLMs commonly exhibit linguistic bias for code generation.","authors":["Weipeng Jiang","Xuanqi Gao","Juan Zhai","Shiqing Ma","Xiaoyu Zhang","Ziyan Lei","Chao Shen"],"url":"https://arxiv.org/abs/2406.00602"}
{"created":"2025-05-02","title":"Ouroboros3D: Image-to-3D Generation via 3D-aware Recursive Diffusion","abstract":"Existing single image-to-3D creation methods typically involve a two-stage process, first generating multi-view images, and then using these images for 3D reconstruction. However, training these two stages separately leads to significant data bias in the inference phase, thus affecting the quality of reconstructed results. We introduce a unified 3D generation framework, named Ouroboros3D, which integrates diffusion-based multi-view image generation and 3D reconstruction into a recursive diffusion process. In our framework, these two modules are jointly trained through a self-conditioning mechanism, allowing them to adapt to each other's characteristics for robust inference. During the multi-view denoising process, the multi-view diffusion model uses the 3D-aware maps rendered by the reconstruction module at the previous timestep as additional conditions. The recursive diffusion framework with 3D-aware feedback unites the entire process and improves geometric consistency.Experiments show that our framework outperforms separation of these two stages and existing methods that combine them at the inference phase. Project page: https://costwen.github.io/Ouroboros3D/","authors":["Hao Wen","Zehuan Huang","Yaohui Wang","Xinyuan Chen","Lu Sheng"],"url":"https://arxiv.org/abs/2406.03184"}
{"created":"2025-05-02","title":"Fast expansion into harmonics on the ball","abstract":"We devise fast and provably accurate algorithms to transform between an $N\\times N \\times N$ Cartesian voxel representation of a three-dimensional function and its expansion into the {ball harmonics}, that is, the eigenbasis of the Dirichlet Laplacian on the unit ball in $\\mathbb{R}^3$. Given $\\varepsilon > 0$, our algorithms achieve relative $\\ell^1$ - $\\ell^\\infty$ accuracy $\\varepsilon$ in time $O(N^3 (\\log N)^2 + N^3 |\\log \\varepsilon|^2)$, while the na\\\"{i}ve direct application of the expansion operators has time complexity $O(N^6)$. We illustrate our methods on numerical examples.","authors":["Joe Kileel","Nicholas F. Marshall","Oscar Mickelin","Amit Singer"],"url":"https://arxiv.org/abs/2406.05922"}
{"created":"2025-05-02","title":"Computational and Statistical Guarantees for Tensor-on-Tensor Regression with Tensor Train Decomposition","abstract":"Recently, a tensor-on-tensor (ToT) regression model has been proposed to generalize tensor recovery, encompassing scenarios like scalar-on-tensor regression and tensor-on-vector regression. However, the exponential growth in tensor complexity poses challenges for storage and computation in ToT regression. To overcome this hurdle, tensor decompositions have been introduced, with the tensor train (TT)-based ToT model proving efficient in practice due to reduced memory requirements, enhanced computational efficiency, and decreased sampling complexity. Despite these practical benefits, a disparity exists between theoretical analysis and real-world performance. In this paper, we delve into the theoretical and algorithmic aspects of the TT-based ToT regression model. Assuming the regression operator satisfies the restricted isometry property (RIP), we conduct an error analysis for the solution to a constrained least-squares optimization problem. This analysis includes upper error bound and minimax lower bound, revealing that such error bounds polynomially depend on the order $N+M$. To efficiently find solutions meeting such error bounds, we propose two optimization algorithms: the iterative hard thresholding (IHT) algorithm (employing gradient descent with TT-singular value decomposition (TT-SVD)) and the factorization approach using the Riemannian gradient descent (RGD) algorithm. When RIP is satisfied, spectral initialization facilitates proper initialization, and we establish the linear convergence rate of both IHT and RGD.","authors":["Zhen Qin","Zhihui Zhu"],"url":"https://arxiv.org/abs/2406.06002"}
{"created":"2025-05-02","title":"CombAlign: Enhancing Model Expressiveness in Unsupervised Graph Alignment","abstract":"Unsupervised graph alignment finds the node correspondence between a pair of attributed graphs by only exploiting graph structure and node features. One category of recent studies first computes the node representation and then matches nodes with the largest embedding-based similarity, while the other category reduces the problem to optimal transport (OT) via Gromov-Wasserstein learning. However, it remains largely unexplored in the model expressiveness, as well as how theoretical expressivity impacts prediction accuracy. We investigate the model expressiveness from two aspects. First, we characterize the model's discriminative power in distinguishing matched and unmatched node pairs across two graphs.Second, we study the model's capability of guaranteeing node matching properties such as one-to-one matching and mutual alignment. Motivated by our theoretical analysis, we put forward a hybrid approach named CombAlign with stronger expressive power. Specifically, we enable cross-dimensional feature interaction for OT-based learning and propose an embedding-based method inspired by the Weisfeiler-Lehman test. We also apply non-uniform marginals obtained from the embedding-based modules to OT as priors for more expressiveness. Based on that, we propose a traditional algorithm-based refinement, which combines our OT and embedding-based predictions using the ensemble learning strategy and reduces the problem to maximum weight matching. With carefully designed edge weights, we ensure those matching properties and further enhance prediction accuracy. By extensive experiments, we demonstrate a significant improvement of 14.5% in alignment accuracy compared to state-of-the-art approaches and confirm the soundness of our theoretical analysis.","authors":["Songyang Chen","Yu Liu","Lei Zou","Zexuan Wang","Youfang Lin"],"url":"https://arxiv.org/abs/2406.13216"}
{"created":"2025-05-02","title":"DRTR: Distance-Aware Graph Representation Learning","abstract":"We propose \\textbf{DRTR}, a novel graph learning framework that integrates distance-aware multi-hop message passing with dynamic topology refinement. Unlike standard GNNs that rely on shallow, fixed-hop aggregation, DRTR leverages both static preprocessing and dynamic resampling to capture deeper structural dependencies. A \\emph{Distance Recomputator} prunes semantically weak edges using adaptive attention, while a \\emph{Topology Reconstructor} establishes latent connections among distant but relevant nodes. This joint mechanism enables more expressive and robust representation learning across evolving graph structures. Extensive experiments demonstrate that DRTR outperforms baseline GNNs in both accuracy and scalability, especially in complex and noisy graph environments.","authors":["Dong Liu","Yanxuan Yu"],"url":"https://arxiv.org/abs/2406.17281"}
{"created":"2025-05-02","title":"Smoothed Analysis for Learning Concepts with Low Intrinsic Dimension","abstract":"In traditional models of supervised learning, the goal of a learner -- given examples from an arbitrary joint distribution on $\\mathbb{R}^d \\times \\{\\pm 1\\}$ -- is to output a hypothesis that is competitive (to within $\\epsilon$) of the best fitting concept from some class. In order to escape strong hardness results for learning even simple concept classes, we introduce a smoothed-analysis framework that requires a learner to compete only with the best classifier that is robust to small random Gaussian perturbation.","authors":["Gautam Chandrasekaran","Adam Klivans","Vasilis Kontonis","Raghu Meka","Konstantinos Stavropoulos"],"url":"https://arxiv.org/abs/2407.00966"}
{"created":"2025-05-02","title":"Commute Graph Neural Networks","abstract":"Graph Neural Networks (GNNs) have shown remarkable success in learning from graph-structured data. However, their application to directed graphs (digraphs) presents unique challenges, primarily due to the inherent asymmetry in node relationships. Traditional GNNs are adept at capturing unidirectional relations but fall short in encoding the mutual path dependencies between nodes, such as asymmetrical shortest paths typically found in digraphs. Recognizing this gap, we introduce Commute Graph Neural Networks (CGNN), an approach that seamlessly integrates node-wise commute time into the message passing scheme. The cornerstone of CGNN is an efficient method for computing commute time using a newly formulated digraph Laplacian. Commute time is then integrated into the neighborhood aggregation process, with neighbor contributions weighted according to their respective commute time to the central node in each layer. It enables CGNN to directly capture the mutual, asymmetric relationships in digraphs. Extensive experiments on 8 benchmarking datasets confirm the superiority of CGNN against 13 state-of-the-art methods.","authors":["Wei Zhuo","Han Yu","Guang Tan","Xiaoxiao Li"],"url":"https://arxiv.org/abs/2407.01635"}
{"created":"2025-05-02","title":"Enhancing clinical decision support with physiological waveforms -- a multimodal benchmark in emergency care","abstract":"Background: AI-driven prediction algorithms have the potential to enhance emergency medicine by enabling rapid and accurate decision-making regarding patient status and potential deterioration. However, the integration of multimodal data, including raw waveform signals, remains underexplored in clinical decision support. Methods: We present a dataset and benchmarking protocol designed to advance multimodal decision support in emergency care. Our models utilize demographics, biometrics, vital signs, laboratory values, and electrocardiogram (ECG) waveforms as inputs to predict both discharge diagnoses and patient deterioration. Results: The diagnostic model achieves area under the receiver operating curve (AUROC) scores above 0.8 for 609 out of 1,428 conditions, covering both cardiac (e.g., myocardial infarction) and non-cardiac (e.g., renal disease, diabetes) diagnoses. The deterioration model attains AUROC scores above 0.8 for 14 out of 15 targets, accurately predicting critical events such as cardiac arrest, mechanical ventilation, ICU admission, and mortality. Conclusions: Our study highlights the positive impact of incorporating raw waveform data into decision support models, improving predictive performance. By introducing a unique, publicly available dataset and baseline models, we provide a foundation for measurable progress in AI-driven decision support for emergency care.","authors":["Juan Miguel Lopez Alcaraz","Hjalmar Bouma","Nils Strodthoff"],"url":"https://arxiv.org/abs/2407.17856"}
{"created":"2025-05-02","title":"Automated Review Generation Method Based on Large Language Models","abstract":"Literature research, vital for scientific work, faces the challenge of surging information volumes exceeding researchers' processing capabilities. We present an automated review generation method based on large language models (LLMs) to overcome efficiency bottlenecks and reduce cognitive load. Our statistically validated evaluation framework demonstrates that the generated reviews match or exceed manual quality, offering broad applicability across research fields without requiring users' domain knowledge. Applied to propane dehydrogenation (PDH) catalysts, our method swiftly analyzed 343 articles, averaging seconds per article per LLM account, producing comprehensive reviews spanning 35 topics, with extended analysis of 1041 articles providing insights into catalysts' properties. Through multi-layered quality control, we effectively mitigated LLMs' hallucinations, with expert verification confirming accuracy and citation integrity while demonstrating hallucination risks reduced to below 0.5\\% with 95\\% confidence. Released Windows application enables one-click review generation, enhancing research productivity and literature recommendation efficiency while setting the stage for broader scientific explorations.","authors":["Shican Wu","Xiao Ma","Dehui Luo","Lulu Li","Xiangcheng Shi","Xin Chang","Xiaoyun Lin","Ran Luo","Chunlei Pei","Changying Du","Zhi-Jian Zhao","Jinlong Gong"],"url":"https://arxiv.org/abs/2407.20906"}
{"created":"2025-05-02","title":"Achieving Human Level Competitive Robot Table Tennis","abstract":"Achieving human-level speed and performance on real world tasks is a north star for the robotics research community. This work takes a step towards that goal and presents the first learned robot agent that reaches amateur human-level performance in competitive table tennis. Table tennis is a physically demanding sport which requires human players to undergo years of training to achieve an advanced level of proficiency. In this paper, we contribute (1) a hierarchical and modular policy architecture consisting of (i) low level controllers with their detailed skill descriptors which model the agent's capabilities and help to bridge the sim-to-real gap and (ii) a high level controller that chooses the low level skills, (2) techniques for enabling zero-shot sim-to-real including an iterative approach to defining the task distribution that is grounded in the real-world and defines an automatic curriculum, and (3) real time adaptation to unseen opponents. Policy performance was assessed through 29 robot vs. human matches of which the robot won 45% (13/29). All humans were unseen players and their skill level varied from beginner to tournament level. Whilst the robot lost all matches vs. the most advanced players it won 100% matches vs. beginners and 55% matches vs. intermediate players, demonstrating solidly amateur human-level performance. Videos of the matches can be viewed at https://sites.google.com/view/competitive-robot-table-tennis","authors":["David B. D'Ambrosio","Saminda Abeyruwan","Laura Graesser","Atil Iscen","Heni Ben Amor","Alex Bewley","Barney J. Reed","Krista Reymann","Leila Takayama","Yuval Tassa","Krzysztof Choromanski","Erwin Coumans","Deepali Jain","Navdeep Jaitly","Natasha Jaques","Satoshi Kataoka","Yuheng Kuang","Nevena Lazic","Reza Mahjourian","Sherry Moore","Kenneth Oslund","Anish Shankar","Vikas Sindhwani","Vincent Vanhoucke","Grace Vesom","Peng Xu","Pannag R. Sanketi"],"url":"https://arxiv.org/abs/2408.03906"}
{"created":"2025-05-02","title":"Steady-State Cascade Operators and their Role in Linear Control, Estimation, and Model Reduction Problems","abstract":"Certain linear matrix operators arise naturally in systems analysis and design problems involving cascade interconnections of linear time-invariant systems, including problems of stabilization, estimation, and model order reduction. We conduct here a comprehensive study of these operators and their relevant system-theoretic properties. The general theory is leveraged to delineate both known and new design methodologies for control and observation of cascades, and to characterize structural properties of reduced models. Several entirely new designs arise from this systematic categorization, including new recursive and low-gain design frameworks for observation of cascaded systems. The benefits of the results beyond the linear time-invariant setting are demonstrated through preliminary extensions for nonlinear systems, with an outlook towards the development of a similarly comprehensive nonlinear theory.","authors":["John W. Simpson-Porco","Daniele Astolfi","Giordano Scarciotti"],"url":"https://arxiv.org/abs/2408.07568"}
{"created":"2025-05-02","title":"Chemputer and Chemputation - A Universal Chemical Compound Synthesis Machine","abstract":"Chemputation, the execution of code controlled reaction pathways on universally reconfigurable hardware, offers a route to treating chemical synthesis as a formally computable i.e. chemputable process to produce chemical compounds or molecules. Here I present a proof that a chemputer endowed with a finite, but extensible reagent, catalyst, process condition sets, and a chempiler that maps reaction graphs to hardware graphs, and dynamic error correction handles, is universal for the synthesis of any stable, isolable molecule that respects conservation of matter and finite reaction time and can be produced in detectable amounts. In developing this proof, I also expanded the internationally accepted definition of a molecule, by requiring that the molecule must be reachable in an analytically accessible amount using concepts from assembly theory. This shows that error correction is a vital requirement for chemputation, and I formalise the universal chemical synthesis theorem, demonstrate a full worked example, and explore the practical limits imposed by vessel count and sensing bandwidth. In doing so, I show that chemical reactions are not implicit blackbox functions but emergent graph operations or chemical transformations over Reagents (R) x Process (P) x Catalyst (K). The role of universally configurable hardware is also highlighted, with the introduction of a chempiling function that translates synthesis pathways into executable hardware configurations.","authors":["Leroy Cronin"],"url":"https://arxiv.org/abs/2408.09171"}
{"created":"2025-05-02","title":"Level-set shape optimization via polytopic discontinuous Galerkin methods","abstract":"We introduce a new level-set shape optimization approach based on polytopic (i.e., polygonal in two and polyhedral in three spatial dimensions) discontinuous Galerkin methods. The approach benefits from the geometric mesh flexibility of polytopic discontinuous Galerkin methods to resolve the zero-level set accurately and efficiently. Additionally, we employ suitable Runge-Kutta discontinuous Galerkin methods to update the level-set function on a fine underlying simplicial mesh. We discuss the construction and implementation of the approach, explaining how to modify shape derivate formulas to compute consistent shape gradient approximations using discontinuous Galerkin methods, and how to recover dG functions into smoother ones. Numerical experiments on unconstrained and PDE-constrained test cases evidence the good properties of the proposed methodology.","authors":["Raphael E. Fernandes","Emmanuil H. Georgoulis","Alberto Paganini"],"url":"https://arxiv.org/abs/2408.13206"}
{"created":"2025-05-02","title":"AI-powered software testing tools: A systematic review and empirical assessment of their features and limitations","abstract":"Context: The rise of Artificial Intelligence (AI) in software engineering has led to the development of AI-powered test automation tools, promising improved efficiency, reduced maintenance effort, and enhanced defect-detection. However, a systematic evaluation of these tools is needed to understand their capabilities, benefits, and limitations. Objective: This study has two objectives: (1) A systematic review of AI-assisted test automation tools, categorizing their key AI features; (2) an empirical study of two selected AI-powered tools on two software under test, to investigate the effectiveness and limitations of the tools. Method: A systematic review of 55 AI-based test automation tools was conducted, classifying them based on their AI-assisted capabilities such as self-healing tests, visual testing, and AI-powered test generation. In the second phase, two representative tools were selected for the empirical study, in which we applied them to test two open-source software systems. Their performance was compared with traditional test automation approaches to evaluate efficiency and adaptability. Results: The review provides a comprehensive taxonomy of AI-driven testing tools, highlighting common features and trends. The empirical evaluation demonstrates that AI-powered automation enhances test execution efficiency and reduces maintenance effort but also exposes limitations such as handling complex UI changes and contextual understanding. Conclusion: AI-driven test automation tools show strong potential in improving software quality and reducing manual testing effort. However, their current limitations-such as false positives, lack of domain knowledge, and dependency on predefined models-indicate the need for further refinement. Future research should focus on advancing AI models to improve adaptability, reliability, and robustness in software testing.","authors":["Vahid Garousi","Nithin Joy","Zafar Jafarov","Alper Bu\\u{g}ra Kele\\c{s}","Sevde De\\u{g}irmenci","Ece \\\"Ozdemir","Ryan Zarringhalami"],"url":"https://arxiv.org/abs/2409.00411"}
{"created":"2025-05-02","title":"Leveraging LLMs for Influence Path Planning in Proactive Recommendation","abstract":"Recommender systems are pivotal in Internet social platforms, yet they often cater to users' historical interests, leading to critical issues like echo chambers. To broaden user horizons, proactive recommender systems aim to guide user interest to gradually like a target item beyond historical interests through an influence path,i.e., a sequence of recommended items. As a representative, Influential Recommender System (IRS) designs a sequential model for influence path planning but faces issues of lacking target item inclusion and path coherence. To address the issues, we leverage the advanced planning capabilities of Large Language Models (LLMs) and propose an LLM-based Influence Path Planning (LLM-IPP) method. LLM-IPP generates coherent and effective influence paths by capturing user interest shifts and item characteristics. We introduce novel evaluation metrics and user simulators to benchmark LLM-IPP against traditional methods. Our experiments demonstrate that LLM-IPP significantly enhances user acceptability and path coherence, outperforming existing approaches.","authors":["Mingze Wang","Shuxian Bi","Wenjie Wang","Chongming Gao","Yangyang Li","Fuli Feng"],"url":"https://arxiv.org/abs/2409.04827"}
{"created":"2025-05-02","title":"SoK: Security and Privacy Risks of Healthcare AI","abstract":"The integration of artificial intelligence (AI) and machine learning (ML) into healthcare systems holds great promise for enhancing patient care and care delivery efficiency; however, it also exposes sensitive data and system integrity to potential cyberattacks. Current security and privacy (S&amp;P) research on healthcare AI is highly unbalanced in terms of healthcare deployment scenarios and threat models, and has a disconnected focus with the biomedical research community. This hinders a comprehensive understanding of the risks that healthcare AI entails. To address this gap, this paper takes a thorough examination of existing healthcare AI S&amp;P research, providing a unified framework that allows the identification of under-explored areas. Our survey presents a systematic overview of healthcare AI attacks and defenses, and points out challenges and research opportunities for each AI-driven healthcare application domain. Through our experimental analysis of different threat models and feasibility studies on under-explored adversarial attacks, we provide compelling insights into the pressing need for cybersecurity research in the rapidly evolving field of healthcare AI.","authors":["Yuanhaur Chang","Han Liu","Chenyang Lu","Ning Zhang"],"url":"https://arxiv.org/abs/2409.07415"}
{"created":"2025-05-02","title":"EZIGen: Enhancing zero-shot personalized image generation with precise subject encoding and decoupled guidance","abstract":"Zero-shot personalized image generation models aim to produce images that align with both a given text prompt and subject image, requiring the model to incorporate both sources of guidance. Existing methods often struggle to capture fine-grained subject details and frequently prioritize one form of guidance over the other, resulting in suboptimal subject encoding and imbalanced generation. In this study, we uncover key insights into overcoming such drawbacks, notably that 1) the choice of the subject image encoder critically influences subject identity preservation and training efficiency, and 2) the text and subject guidance should take effect at different denoising stages. Building on these insights, we introduce a new approach, EZIGen, that employs two main components: leveraging a fixed pre-trained Diffusion UNet itself as subject encoder, following a process that balances the two guidances by separating their dominance stage and revisiting certain time steps to bootstrap subject transfer quality. Through these two components, EZIGen, initially built upon SD2.1-base, achieved state-of-the-art performances on multiple personalized generation benchmarks with a unified model, while using 100 times less training data. Moreover, by further migrating our design to SDXL, EZIGen is proven to be a versatile model-agnostic solution for personalized generation. Demo Page: zichengduan.github.io/pages/EZIGen/index.html","authors":["Zicheng Duan","Yuxuan Ding","Chenhui Gou","Ziqin Zhou","Ethan Smith","Lingqiao Liu"],"url":"https://arxiv.org/abs/2409.08091"}
{"created":"2025-05-02","title":"LT3SD: Latent Trees for 3D Scene Diffusion","abstract":"We present LT3SD, a novel latent diffusion model for large-scale 3D scene generation. Recent advances in diffusion models have shown impressive results in 3D object generation, but are limited in spatial extent and quality when extended to 3D scenes. To generate complex and diverse 3D scene structures, we introduce a latent tree representation to effectively encode both lower-frequency geometry and higher-frequency detail in a coarse-to-fine hierarchy. We can then learn a generative diffusion process in this latent 3D scene space, modeling the latent components of a scene at each resolution level. To synthesize large-scale scenes with varying sizes, we train our diffusion model on scene patches and synthesize arbitrary-sized output 3D scenes through shared diffusion generation across multiple scene patches. Through extensive experiments, we demonstrate the efficacy and benefits of LT3SD for large-scale, high-quality unconditional 3D scene generation and for probabilistic completion for partial scene observations.","authors":["Quan Meng","Lei Li","Matthias Nie{\\ss}ner","Angela Dai"],"url":"https://arxiv.org/abs/2409.08215"}
{"created":"2025-05-02","title":"Towards Global Localization using Multi-Modal Object-Instance Re-Identification","abstract":"Re-identification (ReID) is a critical challenge in computer vision, predominantly studied in the context of pedestrians and vehicles. However, robust object-instance ReID, which has significant implications for tasks such as autonomous exploration, long-term perception, and scene understanding, remains underexplored. In this work, we address this gap by proposing a novel dual-path object-instance re-identification transformer architecture that integrates multimodal RGB and depth information. By leveraging depth data, we demonstrate improvements in ReID across scenes that are cluttered or have varying illumination conditions. Additionally, we develop a ReID-based localization framework that enables accurate camera localization and pose identification across different viewpoints. We validate our methods using two custom-built RGB-D datasets, as well as multiple sequences from the open-source TUM RGB-D datasets. Our approach demonstrates significant improvements in both object instance ReID (mAP of 75.18) and localization accuracy (success rate of 83% on TUM-RGBD), highlighting the essential role of object ReID in advancing robotic perception. Our models, frameworks, and datasets have been made publicly available.","authors":["Aneesh Chavan","Vaibhav Agrawal","Vineeth Bhat","Sarthak Chittawar","Siddharth Srivastava","Chetan Arora","K Madhava Krishna"],"url":"https://arxiv.org/abs/2409.12002"}
{"created":"2025-05-02","title":"ProxFly: Robust Control for Close Proximity Quadcopter Flight via Residual Reinforcement Learning","abstract":"This paper proposes the ProxFly, a residual deep Reinforcement Learning (RL)-based controller for close proximity quadcopter flight. Specifically, we design a residual module on top of a cascaded controller (denoted as basic controller) to generate high-level control commands, which compensate for external disturbances and thrust loss caused by downwash effects from other quadcopters. First, our method takes only the ego state and controllers' commands as inputs and does not rely on any communication between quadcopters, thereby reducing the bandwidth requirement. Through domain randomization, our method relaxes the requirement for accurate system identification and fine-tuned controller parameters, allowing it to adapt to changing system models. Meanwhile, our method not only reduces the proportion of unexplainable signals from the black box in control commands but also enables the RL training to skip the time-consuming exploration from scratch via guidance from the basic controller. We validate the effectiveness of the residual module in the simulation with different proximities. Moreover, we conduct the real close proximity flight test to compare ProxFly with the basic controller and an advanced model-based controller with complex aerodynamic compensation. Finally, we show that ProxFly can be used for challenging quadcopter mid-air docking, where two quadcopters fly in extreme proximity, and strong airflow significantly disrupts flight. However, our method can stabilize the quadcopter in this case and accomplish docking. The resources are available at https://github.com/ruiqizhang99/ProxFly.","authors":["Ruiqi Zhang","Dingqi Zhang","Mark W. Mueller"],"url":"https://arxiv.org/abs/2409.13193"}
{"created":"2025-05-02","title":"Mitigating Covariate Shift in Imitation Learning for Autonomous Vehicles Using Latent Space Generative World Models","abstract":"We propose the use of latent space generative world models to address the covariate shift problem in autonomous driving. A world model is a neural network capable of predicting an agent's next state given past states and actions. By leveraging a world model during training, the driving policy effectively mitigates covariate shift without requiring an excessive amount of training data. During end-to-end training, our policy learns how to recover from errors by aligning with states observed in human demonstrations, so that at runtime it can recover from perturbations outside the training distribution. Additionally, we introduce a novel transformer-based perception encoder that employs multi-view cross-attention and a learned scene query. We present qualitative and quantitative results, demonstrating significant improvements upon prior state of the art in closed-loop testing in the CARLA simulator, as well as showing the ability to handle perturbations in both CARLA and NVIDIA's DRIVE Sim.","authors":["Alexander Popov","Alperen Degirmenci","David Wehr","Shashank Hegde","Ryan Oldja","Alexey Kamenev","Bertrand Douillard","David Nist\\'er","Urs Muller","Ruchi Bhargava","Stan Birchfield","Nikolai Smolyanskiy"],"url":"https://arxiv.org/abs/2409.16663"}
{"created":"2025-05-02","title":"Challenges and Future Directions of Data-Centric AI Alignment","abstract":"As AI systems become increasingly capable and influential, ensuring their alignment with human values, preferences, and goals has become a critical research focus. Current alignment methods primarily focus on designing algorithms and loss functions but often underestimate the crucial role of data. This paper advocates for a shift towards data-centric AI alignment, emphasizing the need to enhance the quality and representativeness of data used in aligning AI systems. In this position paper, we highlight key challenges associated with both human-based and AI-based feedback within the data-centric alignment framework. Through qualitative analysis, we identify multiple sources of unreliability in human feedback, as well as problems related to temporal drift, context dependence, and AI-based feedback failing to capture human values due to inherent model limitations. We propose future research directions, including improved feedback collection practices, robust data-cleaning methodologies, and rigorous feedback verification processes. We call for future research into these critical directions to ensure, addressing gaps that persist in understanding and improving data-centric alignment practices.","authors":["Min-Hsuan Yeh","Jeffrey Wang","Xuefeng Du","Seongheon Park","Leitian Tao","Shawn Im","Yixuan Li"],"url":"https://arxiv.org/abs/2410.01957"}
{"created":"2025-05-02","title":"TaeBench: Improving Quality of Toxic Adversarial Examples","abstract":"Toxicity text detectors can be vulnerable to adversarial examples - small perturbations to input text that fool the systems into wrong detection. Existing attack algorithms are time-consuming and often produce invalid or ambiguous adversarial examples, making them less useful for evaluating or improving real-world toxicity content moderators. This paper proposes an annotation pipeline for quality control of generated toxic adversarial examples (TAE). We design model-based automated annotation and human-based quality verification to assess the quality requirements of TAE. Successful TAE should fool a target toxicity model into making benign predictions, be grammatically reasonable, appear natural like human-generated text, and exhibit semantic toxicity. When applying these requirements to more than 20 state-of-the-art (SOTA) TAE attack recipes, we find many invalid samples from a total of 940k raw TAE attack generations. We then utilize the proposed pipeline to filter and curate a high-quality TAE dataset we call TaeBench (of size 264k). Empirically, we demonstrate that TaeBench can effectively transfer-attack SOTA toxicity content moderation models and services. Our experiments also show that TaeBench with adversarial training achieve significant improvements of the robustness of two toxicity detectors.","authors":["Xuan Zhu","Dmitriy Bespalov","Liwen You","Ninad Kulkarni","Yanjun Qi"],"url":"https://arxiv.org/abs/2410.05573"}
{"created":"2025-05-02","title":"Reward-Augmented Data Enhances Direct Preference Alignment of LLMs","abstract":"Preference alignment in Large Language Models (LLMs) has significantly improved their ability to adhere to human instructions and intentions. However, existing direct alignment algorithms primarily focus on relative preferences and often overlook the qualitative aspects of responses, despite having access to preference data that includes reward scores from judge models during AI feedback. Striving to maximize the implicit reward gap between the chosen and the slightly inferior rejected responses can cause overfitting and unnecessary unlearning of the high-quality rejected responses. The unawareness of the reward scores also drives the LLM to indiscriminately favor the low-quality chosen responses and fail to generalize to optimal responses that are sparse in data. To overcome these shortcomings, our study introduces reward-conditioned LLM policies that discern and learn from the entire spectrum of response quality within the dataset, helping extrapolate to more optimal regions. We propose an effective yet simple data relabeling method that conditions the preference pairs on quality scores to construct a reward-augmented dataset. The experiments across various benchmarks and diverse models demonstrate that our approach consistently boosts DPO by a considerable margin. Through comprehensive ablation studies, we demonstrate that our method not only maximizes the utility of preference data but also mitigates the issue of unlearning, demonstrating its broad effectiveness beyond mere data expansion. Our code is available at https://github.com/shenao-zhang/reward-augmented-preference.","authors":["Shenao Zhang","Zhihan Liu","Boyi Liu","Yufeng Zhang","Yingxiang Yang","Yongfei Liu","Liyu Chen","Tao Sun","Zhaoran Wang"],"url":"https://arxiv.org/abs/2410.08067"}
{"created":"2025-05-02","title":"MotionGlot: A Multi-Embodied Motion Generation Model","abstract":"This paper introduces MotionGlot, a model that can generate motion across multiple embodiments with different action dimensions, such as quadruped robots and human bodies. By leveraging the well-established training procedures commonly used in large language models (LLMs), we introduce an instruction-tuning template specifically designed for motionrelated tasks. Our approach demonstrates that the principles underlying LLM training can be successfully adapted to learn a wide range of motion generation tasks across multiple embodiments with different action dimensions. We demonstrate the various abilities of MotionGlot on a set of 6 tasks and report an average improvement of 35.3% across tasks. Additionally, we contribute two new datasets: (1) a dataset of expert-controlled quadruped locomotion with approximately 48,000 trajectories paired with direction-based text annotations, and (2) a dataset of over 23,000 situational text prompts for human motion generation tasks. Finally, we conduct hardware experiments to validate the capabilities of our system in real-world applications.","authors":["Sudarshan Harithas","Srinath Sridhar"],"url":"https://arxiv.org/abs/2410.16623"}
{"created":"2025-05-02","title":"Guiding-Based Importance Sampling for Walk on Stars","abstract":"Walk on stars (WoSt) has shown its power in being applied to Monte Carlo methods for solving partial differential equations, but the sampling techniques in WoSt are not satisfactory, leading to high variance. We propose a guiding-based importance sampling method to reduce the variance of WoSt. Drawing inspiration from path guiding in rendering, we approximate the directional distribution of the recursive term of WoSt using online-learned parametric mixture distributions, decoded by a lightweight neural field. This adaptive approach enables importance sampling the recursive term, which lacks shape information before computation. We introduce a reflection technique to represent guiding distributions at Neumann boundaries and incorporate multiple importance sampling with learnable selection probabilities to further reduce variance. We also present a practical GPU implementation of our method. Experiments show that our method effectively reduces variance compared to the original WoSt, given the same time or the same sample budget. Code and data for this paper are at https://github.com/tyanyuy3125/elaina.","authors":["Tianyu Huang","Jingwang Ling","Shuang Zhao","Feng Xu"],"url":"https://arxiv.org/abs/2410.18944"}
{"created":"2025-05-02","title":"Are LLM-Judges Robust to Expressions of Uncertainty? Investigating the effect of Epistemic Markers on LLM-based Evaluation","abstract":"In line with the principle of honesty, there has been a growing effort to train large language models (LLMs) to generate outputs containing epistemic markers. However, evaluation in the presence of epistemic markers has been largely overlooked, raising a critical question: Could the use of epistemic markers in LLM-generated outputs lead to unintended negative consequences? To address this, we present EMBER, a benchmark designed to assess the robustness of LLM-judges to epistemic markers in both single and pairwise evaluation settings. Our findings, based on evaluations using EMBER, reveal that all tested LLM-judges, including GPT-4o, show a notable lack of robustness in the presence of epistemic markers. Specifically, we observe a negative bias toward epistemic markers, with a stronger bias against markers expressing uncertainty. This suggests that LLM-judges are influenced by the presence of these markers and do not focus solely on the correctness of the content.","authors":["Dongryeol Lee","Yerin Hwang","Yongil Kim","Joonsuk Park","Kyomin Jung"],"url":"https://arxiv.org/abs/2410.20774"}
{"created":"2025-05-02","title":"Bridging Personalization and Control in Scientific Personalized Search","abstract":"Personalized search is a problem where models benefit from learning user preferences from per-user historical interaction data. The inferred preferences enable personalized ranking models to improve the relevance of documents for users. However, personalization is also seen as opaque in its use of historical interactions and is not amenable to users' control. Further, personalization limits the diversity of information users are exposed to. While search results may be automatically diversified this does little to address the lack of control over personalization. In response, we introduce a model for personalized search that enables users to control personalized rankings proactively. Our model, CtrlCE, is a novel cross-encoder model augmented with an editable memory built from users' historical interactions. The editable memory allows cross-encoders to be personalized efficiently and enables users to control personalized ranking. Next, because all queries do not require personalization, we introduce a calibrated mixing model which determines when personalization is necessary. This enables users to control personalization via their editable memory only when necessary. To thoroughly evaluate CtrlCE, we demonstrate its empirical performance in four domains of science, its ability to selectively request user control in a calibration evaluation of the mixing model, and the control provided by its editable memory in a user study.","authors":["Sheshera Mysore","Garima Dhanania","Kishor Patil","Surya Kallumadi","Andrew McCallum","Hamed Zamani"],"url":"https://arxiv.org/abs/2411.02790"}
{"created":"2025-05-02","title":"A Comprehensive Survey of Deep Learning for Time Series Forecasting: Architectural Diversity and Open Challenges","abstract":"Time series forecasting is a critical task that provides key information for decision-making. After traditional statistical and machine learning approaches, various fundamental deep learning architectures such as MLPs, CNNs, RNNs, and GNNs have been developed. However, the structural limitations caused by the inductive biases of each deep learning architecture constrained their performance. Transformer models, which excel at handling long-term dependencies, have become significant architectural components for time series forecasting. However, recent research has shown that alternatives such as simple linear layers can outperform Transformers. These findings have opened up new possibilities for using diverse architectures, ranging from fundamental deep learning models to emerging architectures and hybrid approaches. In this context, architectural modeling of time series forecasting has now entered a renaissance. This survey not only provides a historical context for time series forecasting but also offers comprehensive and timely analysis of the movement toward architectural diversification. By comparing and re-examining deep learning models, we uncover new perspectives and present recent trends, including hybrid, diffusion, Mamba, and foundation models. By focusing on the inherent characteristics of time series data, we also address open challenges that have gained attention in time series forecasting, such as channel dependency, distribution shift, causality, and feature extraction. These contributions help lower entry barriers for newcomers by providing a systematic understanding of the diverse research areas in time series forecasting (TSF), while offering seasoned researchers broader perspectives and new opportunities through in-depth exploration of TSF challenges. (Shortened due to arXiv's 1,920-character limit. Full version in the paper.)","authors":["Jongseon Kim","Hyungjoon Kim","HyunGi Kim","Dongjun Lee","Sungroh Yoon"],"url":"https://arxiv.org/abs/2411.05793"}
{"created":"2025-05-02","title":"Probabilistic approach to feedback control enhances multi-legged locomotion on rugged landscapes","abstract":"Achieving robust legged locomotion on complex terrains poses challenges due to the high uncertainty in robot-environment interactions. Recent advances in bipedal and quadrupedal robots demonstrate good mobility on rugged terrains but rely heavily on sensors for stability due to low static stability from a high center of mass and a narrow base of support. We hypothesize that a multi-legged robotic system can leverage morphological redundancy from additional legs to minimize sensing requirements when traversing challenging terrains. Studies suggest that a multi-legged system with sufficient legs can reliably navigate noisy landscapes without sensing and control, albeit at a low speed of up to 0.1 body lengths per cycle (BLC). However, the control framework to enhance speed on challenging terrains remains underexplored due to the complex environmental interactions, making it difficult to identify the key parameters to control in these high-degree-of-freedom systems. Here, we present a bio-inspired vertical body undulation wave as a novel approach to mitigate environmental disturbances affecting robot speed, supported by experiments and probabilistic models. Finally, we introduce a control framework which monitors foot-ground contact patterns on rugose landscapes using binary foot-ground contact sensors to estimate terrain rugosity. The controller adjusts the vertical body wave based on the deviation of the limb's averaged actual-to-ideal foot-ground contact ratio, achieving a significant enhancement of up to 0.235 BLC on rugose laboratory terrain. We observed a $\\sim$ 50\\% increase in speed and a $\\sim$ 40\\% reduction in speed variance compared to the open-loop controller. Additionally, the controller operates in complex terrains outside the lab, including pine straw, robot-sized rocks, mud, and leaves.","authors":["Juntao He","Baxi Chong","Jianfeng Lin","Zhaochen Xu","Hosain Bagheri","Esteban Flores","Daniel I. Goldman"],"url":"https://arxiv.org/abs/2411.07183"}
{"created":"2025-05-02","title":"Artificial Scientific Discovery","abstract":"Rooted in the explosion of deep learning over the past decade, this thesis spans from AlphaGo to ChatGPT to empirically examine the fundamental concepts needed to realize the vision of an artificial scientist: a machine with the capacity to autonomously generate original research and contribute to the expansion of human knowledge. The investigation begins with Olivaw, an AlphaGo Zero-like agent that discovers Othello knowledge from scratch but is unable to communicate it. This realization leads to the development of the Explanatory Learning (EL) framework, a formalization of the problem faced by a scientist when trying to explain a new phenomenon to their peers. The effective EL prescriptions allow us to crack Zendo, a popular board game simulating the scientific endeavor. This success comes with a fundamental insight: an artificial scientist must develop its own interpretation of the language used to explain its findings, and not rely on a rigid existing interpreter. Questioning the very process of learning an interpreter, we turn our attention to the inner functioning of modern multimodal models. This culminates in a simple idea to build CLIP-like models where interpretation and perception are explicitly disentangled: a cost-effective approach that couples two unimodal models using little multimodal data and no further training. Finally, we discuss what ChatGPT and its siblings are still missing to become artificial scientists, and introduce the Big-Bench Symbol Interpretation Task, a benchmark about interpreting Zendo-like explanations that sees LLMs going no further than random chance while being instead fully solved by humans.","authors":["Antonio Norelli"],"url":"https://arxiv.org/abs/2411.11672"}
{"created":"2025-05-02","title":"GLOVER: Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping","abstract":"Inferring affordable (i.e., graspable) parts of arbitrary objects based on human specifications is essential for robots advancing toward open-vocabulary manipulation. Current grasp planners, however, are hindered by limited vision-language comprehension and time-consuming 3D radiance modeling, restricting real-time, open-vocabulary interactions with objects. To address these limitations, we propose GLOVER, a unified Generalizable Open-Vocabulary Affordance Reasoning framework, which fine-tunes the Large Language Models (LLMs) to predict the visual affordance of graspable object parts within RGB feature space. We compile a dataset of over 10,000 images from human-object interactions, annotated with unified visual and linguistic affordance labels, to enable multi-modal fine-tuning. GLOVER inherits world knowledge and common-sense reasoning from LLMs, facilitating more fine-grained object understanding and sophisticated tool-use reasoning. To enable effective real-world deployment, we present Affordance-Aware Grasping Estimation (AGE), a non-parametric grasp planner that aligns the gripper pose with a superquadric surface derived from affordance data. In evaluations across 30 table-top real-world scenes, GLOVER achieves success rates of 86.0% in part identification and 76.3% in grasping, with speeds approximately 29 times faster in affordance reasoning and 40 times faster in grasping pose estimation than the previous state-of-the-art. We also validate the generalization across embodiments, showing effectiveness in humanoid robots with dexterous hands.","authors":["Teli Ma","Zifan Wang","Jiaming Zhou","Mengmeng Wang","Junwei Liang"],"url":"https://arxiv.org/abs/2411.12286"}
{"created":"2025-05-02","title":"Spatiotemporal Tubes for Temporal Reach-Avoid-Stay Tasks in Unknown Systems","abstract":"The paper considers the controller synthesis problem for general MIMO systems with unknown dynamics, aiming to fulfill the temporal reach-avoid-stay task, where the unsafe regions are time-dependent, and the target must be reached within a specified time frame. The primary aim of the paper is to construct the spatiotemporal tube (STT) using a sampling-based approach and thereby devise a closed-form approximation-free control strategy to ensure that system trajectory reaches the target set while avoiding time-dependent unsafe sets. The proposed scheme utilizes a novel method involving STTs to provide controllers that guarantee both system safety and reachability. In our sampling-based framework, we translate the requirements of STTs into a Robust optimization program (ROP). To address the infeasibility of ROP caused by infinite constraints, we utilize the sampling-based Scenario optimization program (SOP). Subsequently, we solve the SOP to generate the tube and closed-form controller for an unknown system, ensuring the temporal reach-avoid-stay specification. Finally, the effectiveness of the proposed approach is demonstrated through three case studies: an omnidirectional robot, a SCARA manipulator, and a magnetic levitation system.","authors":["Ratnangshu Das","Ahan Basu","Pushpak Jagtap"],"url":"https://arxiv.org/abs/2411.13834"}
{"created":"2025-05-02","title":"Updating Katz centrality by counting walks","abstract":"We develop efficient and effective strategies for the update of Katz centralities after node and edge removal in simple graphs. We provide explicit formulas for the ``loss of walks\" a network suffers when nodes/edges are removed, and use these to inform our algorithms. The theory builds on the newly introduced concept of $\\cF$-avoiding first-passage walks. Further, bounds on the change of total network communicability are also derived. Extensive numerical experiments on synthetic and real-world networks complement our theoretical results.","authors":["Francesca Arrigo","Daniele Bertaccini","Alessandro Filippo"],"url":"https://arxiv.org/abs/2411.19560"}
{"created":"2025-05-02","title":"Network-aided Efficient LLM Services With Denoising-inspired Prompt Compression","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in various tasks, leading to their increasing adoption in diverse services delivered through wireless networks. There is a growing trend toward longer prompts to better leverage LLMs' capabilities and address difficult tasks. However, longer prompts not only increase data transmission costs but also require more computing resources and processing time, which impacts overall system efficiency and user experience. To address this challenge, we propose Joint Power and Prompt Optimization (JPPO), a framework that combines Small Language Model (SLM)-based prompt compression with wireless power allocation optimization. By deploying SLM at edge devices for prompt compression and employing Deep Reinforcement Learning (DRL) for joint optimization of compression ratio and transmission power, JPPO effectively balances service quality with resource efficiency. Furthermore, inspired by denoising diffusion models, we design a denoising-inspired prompt compression approach that iteratively compresses prompts by gradually removing non-critical information, further enhancing the framework's performance. Experimental results with long prompt tokens demonstrate that our framework achieves high service fidelity while optimizing power usage in wireless LLM services, significantly reducing the total service response time. With our DRL-based JPPO, the framework maintains fidelity comparable to the no-compression baseline while still achieving a 17% service time reduction through adaptive compression.","authors":["Feiran You","Hongyang Du","Kaibin Huang","Abbas Jamalipour"],"url":"https://arxiv.org/abs/2412.03621"}
{"created":"2025-05-02","title":"A Unified Approach for Multi-Granularity Search over Spatial Datasets","abstract":"There has been increased interest in data search as a means to find relevant datasets or data points in data lakes and repositories. Although approaches have been proposed to support spatial dataset search and data point search, they consider the two types of searches independently. To enable search operations ranging from the coarse-grained dataset level to the fine-grained data point level, we provide an integrated one that supports diverse query types and distance metrics. In this paper, we focus on designing a multi-granularity spatial data search system, called Spadas, that supports both dataset and data point search operations. To address the challenges of the high cost of indexing and susceptibility to outliers, we propose a unified index that can drastically improve query efficiency in various scenarios by organizing data reasonably and removing outliers in datasets. Moreover, to accelerate all data search operations, we propose a set of pruning mechanisms based on the unified index, including fast bound estimation, approximation technique with error bound, and pruning in batch techniques, to effectively filter out non-relevant datasets and points. Finally, we report the results of a detailed experimental evaluation using six spatial data repositories, achieving orders of magnitude faster than the state-of-the-art algorithms and demonstrating the effectiveness by case study. An online spatial data search system of Spadas is also implemented and made accessible to users.","authors":["Wenzhe Yang","Sheng Wang","Shixun Huang","Yuyang Liao","Yuan Sun","Juliana Freire","Zhiyong Peng"],"url":"https://arxiv.org/abs/2412.04805"}
{"created":"2025-05-02","title":"Domain-Specific Translation with Open-Source Large Language Models: Resource-Oriented Analysis","abstract":"In this work, we compare the domain-specific translation performance of open-source autoregressive decoder-only large language models (LLMs) with task-oriented machine translation (MT) models. Our experiments focus on the medical domain and cover four language directions with varied resource availability: English-to-French, English-to-Portuguese, English-to-Swahili, and Swahili-to-English. Despite recent advancements, LLMs demonstrate a significant quality gap in specialized translation compared to multilingual encoder-decoder MT models such as NLLB-200. Our results indicate that NLLB-200 3.3B outperforms all evaluated LLMs in the 7-8B parameter range across three out of the four language directions. While fine-tuning improves the performance of LLMs such as Mistral and Llama, these models still underperform compared to fine-tuned NLLB-200 3.3B models. Our findings highlight the ongoing need for specialized MT models to achieve high-quality domain-specific translation, especially in medium-resource and low-resource settings. Moreover, the superior performance of larger LLMs over their 8B variants suggests potential value in pre-training domain-specific medium-sized language models, employing targeted data selection and knowledge distillation approaches to enhance both quality and efficiency in specialized translation tasks.","authors":["Aman Kassahun Wassie","Mahdi Molaei","Yasmin Moslem"],"url":"https://arxiv.org/abs/2412.05862"}
{"created":"2025-05-02","title":"PPT: Pretraining with Pseudo-Labeled Trajectories for Motion Forecasting","abstract":"Accurately predicting how agents move in dynamic scenes is essential for safe autonomous driving. State-of-the-art motion forecasting models rely on large curated datasets with manually annotated or heavily post-processed trajectories. However, building these datasets is costly, generally manual, hard to scale, and lacks reproducibility. They also introduce domain gaps that limit generalization across environments. We introduce PPT (Pretraining with Pseudo-labeled Trajectories), a simple and scalable alternative that uses unprocessed and diverse trajectories automatically generated from off-the-shelf 3D detectors and tracking. Unlike traditional pipelines aiming for clean, single-label annotations, PPT embraces noise and diversity as useful signals for learning robust representations. With optional finetuning on a small amount of labeled data, models pretrained with PPT achieve strong performance across standard benchmarks particularly in low-data regimes, and in cross-domain, end-to-end and multi-class settings. PPT is easy to implement and improves generalization in motion forecasting. Code and data will be released upon acceptance.","authors":["Yihong Xu","Yuan Yin","\\'Eloi Zablocki","Tuan-Hung Vu","Alexandre Boulch","Matthieu Cord"],"url":"https://arxiv.org/abs/2412.06491"}
{"created":"2025-05-02","title":"Use of diverse data sources to control which topics emerge in a science map","abstract":"Traditional science maps visualize topics by clustering documents, but they are inherently biased toward clustering certain topics over others. If these topics could be chosen, then the science maps could be tailored for different needs. In this paper, we explore the use of document networks from diverse data sources as a tool to control the topic clustering bias of a science map. We analyze this by evaluating the clustering effectiveness of several topic categories over two traditional and six non-traditional data sources. We found that the topics favored in each non-traditional data source are about: Health for Facebook users, biotechnology for patent families, government and social issues for policy documents, food for Twitter conversations, nursing for Twitter users, and geographical entities for document authors (the favoring in this latter source was particularly strong). Our results show that diverse data sources can be used to control topic bias, which opens up the possibility of creating science maps tailored for different needs.","authors":["Juan Pablo Bascur","Rodrigo Costas","Suzan Verberne"],"url":"https://arxiv.org/abs/2412.07550"}
{"created":"2025-05-02","title":"Non-Myopic Multi-Objective Bayesian Optimization","abstract":"We consider the problem of finite-horizon sequential experimental design to solve multi-objective optimization (MOO) of expensive black-box objective functions. This problem arises in many real-world applications, including materials design, where we have a small resource budget to make and evaluate candidate materials in the lab. We solve this problem using the framework of Bayesian optimization (BO) and propose the first set of non-myopic methods for MOO problems. Prior work on non-myopic BO for single-objective problems relies on the Bellman optimality principle to handle the lookahead reasoning process. However, this principle does not hold for most MOO problems because the reward function needs to satisfy some conditions: scalar variable, monotonicity, and additivity. We address this challenge by using hypervolume improvement (HVI) as our scalarization approach, which allows us to use a lower-bound on the Bellman equation to approximate the finite-horizon using a batch expected hypervolume improvement (EHVI) acquisition function (AF) for MOO. Our formulation naturally allows us to use other improvement-based scalarizations and compare their efficacy to HVI. We derive three non-myopic AFs for MOBO: 1) the Nested AF, which is based on the exact computation of the lower bound, 2) the Joint AF, which is a lower bound on the nested AF, and 3) the BINOM AF, which is a fast and approximate variant based on batch multi-objective acquisition functions. Our experiments on multiple diverse real-world MO problems demonstrate that our non-myopic AFs substantially improve performance over the existing myopic AFs for MOBO.","authors":["Syrine Belakaria","Alaleh Ahmadianshalchi","Barbara Engelhardt","Stefano Ermon","Janardhan Rao Doppa"],"url":"https://arxiv.org/abs/2412.08085"}
{"created":"2025-05-02","title":"Two-dimensional Constacyclic Codes over $\\mathbb{F}_q$","abstract":"We consider two-dimensional $(\\lambda_1, \\lambda_2)$-constacyclic codes over $\\mathbb{F}_{q}$ of area $M N$, where $q$ is some power of prime $p$ with $\\gcd(M,p)=1$ and $\\gcd(N,p)=1$. With the help of common zero (CZ) set, we characterize 2-D constacyclic codes. Further, we provide an algorithm to construct an ideal basis of these codes by using their essential common zero (ECZ) sets. We describe the dual of 2-D constacyclic codes. Finally, we provide an encoding scheme for generating 2-D constacyclic codes. We present an example to illustrate that 2-D constacyclic codes can have better minimum distance compared to their cyclic counterparts with the same code area and code rate.","authors":["Vidya Sagar","Shikha Patel","Shayan Srinivasa Garani"],"url":"https://arxiv.org/abs/2412.09915"}
{"created":"2025-05-02","title":"Deterministic Even-Cycle Detection in Broadcast CONGEST","abstract":"We show that, for every $k\\geq 2$, $C_{2k}$-freeness can be decided in $O(n^{1-1/k})$ rounds in the Broadcast CONGEST model, by a deterministic algorithm. This (deterministic) round-complexity is optimal for $k=2$ up to logarithmic factors thanks to the lower bound for $C_4$-freeness by Drucker et al. [PODC 2014], which holds even for randomized algorithms. Moreover it matches the round-complexity of the best known randomized algorithms by Censor-Hillel et al. [DISC 2020] for $k\\in\\{3,4,5\\}$, and by Fraigniaud et al. [PODC 2024] for $k\\geq 6$. Our algorithm uses parallel BFS-explorations with deterministic selections of the set of paths that are forwarded at each round, in a way similar to what was done for the detection of odd-length cycles, by Korhonen and Rybicki [OPODIS 2017]. However, the key element in the design and analysis of our algorithm is a new combinatorial result bounding the \"local density\" of graphs without $2k$-cycles, which we believe is interesting on its own.","authors":["Pierre Fraigniaud","Ma\\\"el Luce","Fr\\'ed\\'eric Magniez","Ioan Todinca"],"url":"https://arxiv.org/abs/2412.11195"}
{"created":"2025-05-02","title":"Seamless Optical Cloud Computing across Edge-Metro Network for Generative AI","abstract":"The rapid advancement of generative artificial intelligence (AI) in recent years has profoundly reshaped modern lifestyles, necessitating a revolutionary architecture to support the growing demands for computational power. Cloud computing has become the driving force behind this transformation. However, it consumes significant power and faces computation security risks due to the reliance on extensive data centers and servers in the cloud. Reducing power consumption while enhancing computational scale remains persistent challenges in cloud computing. Here, we propose and experimentally demonstrate an optical cloud computing system that can be seamlessly deployed across edge-metro network. By modulating inputs and models into light, a wide range of edge nodes can directly access the optical computing center via the edge-metro network. The experimental validations show an energy efficiency of 118.6 mW/TOPs (tera operations per second), reducing energy consumption by two orders of magnitude compared to traditional electronic-based cloud computing solutions. Furthermore, it is experimentally validated that this architecture can perform various complex generative AI models through parallel computing to achieve image generation tasks.","authors":["Sizhe Xing","Aolong Sun","Chengxi Wang","Yizhi Wang","Boyu Dong","Junhui Hu","Xuyu Deng","An Yan","Yingjun Liu","Fangchen Hu","Zhongya Li","Ouhan Huang","Junhao Zhao","Yingjun Zhou","Ziwei Li","Jianyang Shi","Xi Xiao","Richard Penty","Qixiang Cheng","Nan Chi","Junwen Zhang"],"url":"https://arxiv.org/abs/2412.12126"}
{"created":"2025-05-02","title":"Towards Physically Interpretable World Models: Meaningful Weakly Supervised Representations for Visual Trajectory Prediction","abstract":"Deep learning models are increasingly employed for perception, prediction, and control in robotic systems. For for achieving realistic and consistent outputs, it is crucial to embed physical knowledge into their learned representations. However, doing so is difficult due to high-dimensional observation data, such as images, particularly under conditions of incomplete system knowledge and imprecise state sensing. To address this, we propose Physically Interpretable World Models, a novel architecture that aligns learned latent representations with real-world physical quantities. To this end, our architecture combines three key elements: (1) a vector-quantized image autoencoder, (2) a transformer-based physically interpretable autoencoder, and (3) a partially known dynamical model. The training incorporates weak interval-based supervision to eliminate the impractical reliance on ground-truth physical knowledge. Three case studies demonstrate that our approach achieves physical interpretability and accurate state predictions, thus advancing representation learning for robotics.","authors":["Zhenjiang Mao","Ivan Ruchkin"],"url":"https://arxiv.org/abs/2412.12870"}
{"created":"2025-05-02","title":"Generating Traffic Scenarios via In-Context Learning to Learn Better Motion Planner","abstract":"Motion planning is a crucial component in autonomous driving. State-of-the-art motion planners are trained on meticulously curated datasets, which are not only expensive to annotate but also insufficient in capturing rarely seen critical scenarios. Failing to account for such scenarios poses a significant risk to motion planners and may lead to incidents during testing. An intuitive solution is to manually compose such scenarios by programming and executing a simulator (e.g., CARLA). However, this approach incurs substantial human costs. Motivated by this, we propose an inexpensive method for generating diverse critical traffic scenarios to train more robust motion planners. First, we represent traffic scenarios as scripts, which are then used by the simulator to generate traffic scenarios. Next, we develop a method that accepts user-specified text descriptions, which a Large Language Model translates into scripts using in-context learning. The output scripts are sent to the simulator that produces the corresponding traffic scenarios. As our method can generate abundant safety-critical traffic scenarios, we use them as synthetic training data for motion planners. To demonstrate the value of generated scenarios, we train existing motion planners on our synthetic data, real-world datasets, and a combination of both. Our experiments show that motion planners trained with our data significantly outperform those trained solely on real-world data, showing the usefulness of our synthetic data and the effectiveness of our data generation method. Our source code is available at https://ezharjan.github.io/AutoSceneGen.","authors":["Aizierjiang Aiersilan"],"url":"https://arxiv.org/abs/2412.18086"}
{"created":"2025-05-02","title":"Multimodal classification of forest biodiversity potential from 2D orthophotos and 3D airborne laser scanning point clouds","abstract":"Assessment of forest biodiversity is crucial for ecosystem management and conservation. While traditional field surveys provide high-quality assessments, they are labor-intensive and spatially limited. This study investigates whether deep learning-based fusion of close-range sensing data from 2D orthophotos and 3D airborne laser scanning (ALS) point clouds can reliable assess the biodiversity potential of forests. We introduce the BioVista dataset, comprising 44 378 paired samples of orthophotos and ALS point clouds from temperate forests in Denmark, designed to explore multimodal fusion approaches. Using deep neural networks (ResNet for orthophotos and PointVector for ALS point clouds), we investigate each data modality's ability to assess forest biodiversity potential, achieving overall accuracies of 76.7% and 75.8%, respectively. We explore various 2D and 3D fusion approaches: confidence-based ensembling, feature-level concatenation, and end-to-end training, achieving overall accuracies of 80.5%, 81.4% and 80.4% respectively. Our results demonstrate that spectral information from orthophotos and structural information from ALS point clouds effectively complement each other in forest biodiversity assessment.","authors":["Simon B. Jensen","Stefan Oehmcke","Andreas M{\\o}gelmose","Meysam Madadi","Christian Igel","Sergio Escalera","Thomas B. Moeslund"],"url":"https://arxiv.org/abs/2501.01728"}
{"created":"2025-05-02","title":"Distilling Calibration via Conformalized Credal Inference","abstract":"Deploying artificial intelligence (AI) models on edge devices involves a delicate balance between meeting stringent complexity constraints, such as limited memory and energy resources, and ensuring reliable performance in sensitive decision-making tasks. One way to enhance reliability is through uncertainty quantification via Bayesian inference. This approach, however, typically necessitates maintaining and running multiple models in an ensemble, which may exceed the computational limits of edge devices. This paper introduces a low-complexity methodology to address this challenge by distilling calibration information from a more complex model. In an offline phase, predictive probabilities generated by a high-complexity cloud-based model are leveraged to determine a threshold based on the typical divergence between the cloud and edge models. At run time, this threshold is used to construct credal sets -- ranges of predictive probabilities that are guaranteed, with a user-selected confidence level, to include the predictions of the cloud model. The credal sets are obtained through thresholding of a divergence measure in the simplex of predictive probabilities. Experiments on visual and language tasks demonstrate that the proposed approach, termed Conformalized Distillation for Credal Inference (CD-CI), significantly improves calibration performance compared to low-complexity Bayesian methods, such as Laplace approximation, making it a practical and efficient solution for edge AI deployments.","authors":["Jiayi Huang","Sangwoo Park","Nicola Paoletti","Osvaldo Simeone"],"url":"https://arxiv.org/abs/2501.06066"}
{"created":"2025-05-02","title":"AnyStory: Towards Unified Single and Multiple Subject Personalization in Text-to-Image Generation","abstract":"Recently, large-scale generative models have demonstrated outstanding text-to-image generation capabilities. However, generating high-fidelity personalized images with specific subjects still presents challenges, especially in cases involving multiple subjects. In this paper, we propose AnyStory, a unified approach for personalized subject generation. AnyStory not only achieves high-fidelity personalization for single subjects, but also for multiple subjects, without sacrificing subject fidelity. Specifically, AnyStory models the subject personalization problem in an \"encode-then-route\" manner. In the encoding step, AnyStory utilizes a universal and powerful image encoder, i.e., ReferenceNet, in conjunction with CLIP vision encoder to achieve high-fidelity encoding of subject features. In the routing step, AnyStory utilizes a decoupled instance-aware subject router to accurately perceive and predict the potential location of the corresponding subject in the latent space, and guide the injection of subject conditions. Detailed experimental results demonstrate the excellent performance of our method in retaining subject details, aligning text descriptions, and personalizing for multiple subjects. The project page is at https://aigcdesigngroup.github.io/AnyStory/ .","authors":["Junjie He","Yuxiang Tuo","Binghui Chen","Chongyang Zhong","Yifeng Geng","Liefeng Bo"],"url":"https://arxiv.org/abs/2501.09503"}
{"created":"2025-05-02","title":"A Comprehensive Survey on Integrating Large Language Models with Knowledge-Based Methods","abstract":"The rapid development of artificial intelligence has led to marked progress in the field. One interesting direction for research is whether Large Language Models (LLMs) can be integrated with structured knowledge-based systems. This approach aims to combine the generative language understanding of LLMs and the precise knowledge representation systems by which they are integrated. This article surveys the relationship between LLMs and knowledge bases, looks at how they can be applied in practice, and discusses related technical, operational, and ethical challenges. Utilizing a comprehensive examination of the literature, the study both identifies important issues and assesses existing solutions. It demonstrates the merits of incorporating generative AI into structured knowledge-base systems concerning data contextualization, model accuracy, and utilization of knowledge resources. The findings give a full list of the current situation of research, point out the main gaps, and propose helpful paths to take. These insights contribute to advancing AI technologies and support their practical deployment across various sectors.","authors":["Wenli Yang","Lilian Some","Michael Bain","Byeong Kang"],"url":"https://arxiv.org/abs/2501.13947"}
{"created":"2025-05-02","title":"The Redundancy of Non-Singular Channel Simulation","abstract":"Channel simulation is an alternative to quantization and entropy coding for performing lossy source coding. Recently, channel simulation has gained significant traction in both the machine learning and information theory communities, as it integrates better with machine learning-based data compression algorithms and has better rate-distortion-perception properties than quantization. As the practical importance of channel simulation increases, it is vital to understand its fundamental limitations. Recently, Sriramu and Wagner provided an almost complete characterisation of the redundancy of channel simulation algorithms. In this paper, we complete this characterisation. First, we significantly extend a result of Li and El Gamal, and show that the redundancy of any instance of a channel simulation problem is lower bounded by the channel simulation divergence. Second, we give two proofs that the asymptotic redundancy of simulating iid non-singular channels is lower-bounded by $1/2$: one using a direct approach based on the asymptotic expansion of the channel simulation divergence and one using large deviations theory.","authors":["Gergely Flamich","Sharang M. Sriramu","Aaron B. Wagner"],"url":"https://arxiv.org/abs/2501.14053"}
{"created":"2025-05-02","title":"Rate Distortion Approach to Joint Communication and Sensing With Markov States: Open Loop Case","abstract":"We investigate a joint communication and sensing (JCAS) framework in which a transmitter concurrently transmits information to a receiver and estimates a state of interest based on noisy observations. The state is assumed to evolve according to a known dynamical model. Past state estimates may then be used to inform current state estimates. We show that Bayesian filtering constitutes the optimal sensing strategy. We analyze JCAS performance under an open loop encoding strategy with results presented in terms of the tradeoff between asymptotic communication rate and expected per-block distortion of the state. We illustrate the general result by specializing the analysis to a beam-pointing model with mobile state tracking. Our results shed light on the relative performance of two beam control strategies, beam-switching and multi-beam.","authors":["Colton P. Lindstrom","Matthieu R. Bloch"],"url":"https://arxiv.org/abs/2501.15652"}
{"created":"2025-05-02","title":"Boli: A dataset for understanding stuttering experience and analyzing stuttered speech","abstract":"There is a growing need for diverse, high-quality stuttered speech data, particularly in the context of Indian languages. This paper introduces Project Boli, a multi-lingual stuttered speech dataset designed to advance scientific understanding and technology development for individuals who stutter, particularly in India. The dataset constitutes (a) anonymized metadata (gender, age, country, mother tongue) and responses to a questionnaire about how stuttering affects their daily lives, (b) captures both read speech (using the Rainbow Passage) and spontaneous speech (through image description tasks) for each participant and (c) includes detailed annotations of five stutter types: blocks, prolongations, interjections, sound repetitions and word repetitions. We present a comprehensive analysis of the dataset, including the data collection procedure, experience summarization of people who stutter, severity assessment of stuttering events and technical validation of the collected data. The dataset is released as an open access to further speech technology development.","authors":["Ashita Batra","Mannas Narang","Neeraj Kumar Sharma","Pradip K Das"],"url":"https://arxiv.org/abs/2501.15877"}
{"created":"2025-05-02","title":"Advanced Physics-Informed Neural Network with Residuals for Solving Complex Integral Equations","abstract":"In this paper, we present the Residual Integral Solver Network (RISN), a novel neural network architecture designed to solve a wide range of integral and integro-differential equations, including one-dimensional, multi-dimensional, ordinary and partial integro-differential, systems, fractional types, and Helmholtz-type integral equations involving oscillatory kernels. RISN integrates residual connections with high-accuracy numerical methods such as Gaussian quadrature and fractional derivative operational matrices, enabling it to achieve higher accuracy and stability than traditional Physics-Informed Neural Networks (PINN). The residual connections help mitigate vanishing gradient issues, allowing RISN to handle deeper networks and more complex kernels, particularly in multi-dimensional problems. Through extensive experiments, we demonstrate that RISN consistently outperforms not only classical PINNs but also advanced variants such as Auxiliary PINN (A-PINN) and Self-Adaptive PINN (SA-PINN), achieving significantly lower Mean Absolute Errors (MAE) across various types of equations. These results highlight RISN's robustness and efficiency in solving challenging integral and integro-differential problems, making it a valuable tool for real-world applications where traditional methods often struggle.","authors":["Mahdi Movahedian Moghaddam","Kourosh Parand","Saeed Reza Kheradpisheh"],"url":"https://arxiv.org/abs/2501.16370"}
{"created":"2025-05-02","title":"Instantiation-based Formalization of Logical Reasoning Tasks using Language Models and Logical Solvers","abstract":"Robustness of reasoning remains a significant challenge for large language models, and addressing it is essential for the practical applicability of AI-driven reasoning systems. We introduce Semantic Self-Verification (SSV), a novel approach that addresses the key challenge in combining language models with the rigor of logical solvers: to accurately formulate the reasoning problem from natural language to the formal language of the solver. SSV uses a consistency-based approach to produce strong abstract formalizations of problems using concrete instantiations that are generated by the model and verified by the solver. In addition to significantly advancing the overall reasoning accuracy over the state-of-the-art, a key novelty that this approach presents is a feature of verification that has near-perfect precision over a significant coverage of cases, as we demonstrate on open reasoning benchmarks. We propose such *near-certain reasoning* as a new approach to reduce the need for manual verification in many cases, taking us closer to more dependable and autonomous AI reasoning systems.","authors":["Mohammad Raza","Natasa Milic-Frayling"],"url":"https://arxiv.org/abs/2501.16961"}
{"created":"2025-05-02","title":"Belief Roadmaps with Uncertain Landmark Evanescence","abstract":"We would like a robot to navigate to a goal location while minimizing state uncertainty. To aid the robot in this endeavor, maps provide a prior belief over the location of objects and regions of interest. To localize itself within the map, a robot identifies mapped landmarks using its sensors. However, as the time between map creation and robot deployment increases, portions of the map can become stale, and landmarks, once believed to be permanent, may disappear. We refer to the propensity of a landmark to disappear as landmark evanescence. Reasoning about landmark evanescence during path planning, and the associated impact on localization accuracy, requires analyzing the presence or absence of each landmark, leading to an exponential number of possible outcomes of a given motion plan. To address this complexity, we develop BRULE, an extension of the Belief Roadmap. During planning, we replace the belief over future robot poses with a Gaussian mixture which is able to capture the effects of landmark evanescence. Furthermore, we show that belief updates can be made efficient, and that maintaining a random subset of mixture components is sufficient to find high quality solutions. We demonstrate performance in simulated and real-world experiments. Software is available at https://bit.ly/BRULE.","authors":["Erick Fuentes","Jared Strader","Ethan Fahnestock","Nicholas Roy"],"url":"https://arxiv.org/abs/2501.17982"}
{"created":"2025-05-02","title":"KNN and K-means in Gini Prametric Spaces","abstract":"This paper introduces innovative enhancements to the K-means and K-nearest neighbors (KNN) algorithms based on the concept of Gini prametric spaces. Unlike traditional distance metrics, Gini-based measures incorporate both value-based and rank-based information, improving robustness to noise and outliers. The main contributions of this work include: proposing a Gini-based measure that captures both rank information and value distances; presenting a Gini K-means algorithm that is proven to converge and demonstrates resilience to noisy data; and introducing a Gini KNN method that performs competitively with state-of-the-art approaches such as Hassanat's distance in noisy environments. Experimental evaluations on 14 datasets from the UCI repository demonstrate the superior performance and efficiency of Gini-based algorithms in clustering and classification tasks. This work opens new avenues for leveraging rank-based measures in machine learning and statistical analysis.","authors":["Cassandra Mussard","Arthur Charpentier","St\\'ephane Mussard"],"url":"https://arxiv.org/abs/2501.18028"}
{"created":"2025-05-02","title":"Efficiency and Effectiveness of LLM-Based Summarization of Evidence in Crowdsourced Fact-Checking","abstract":"Evaluating the truthfulness of online content is critical for combating misinformation. This study examines the efficiency and effectiveness of crowdsourced truthfulness assessments through a comparative analysis of two approaches: one involving full-length webpages as evidence for each claim, and another using summaries for each evidence document generated with a large language model. Using an A/B testing setting, we engage a diverse pool of participants tasked with evaluating the truthfulness of statements under these conditions. Our analysis explores both the quality of assessments and the behavioral patterns of participants. The results reveal that relying on summarized evidence offers comparable accuracy and error metrics to the Standard modality while significantly improving efficiency. Workers in the Summary setting complete a significantly higher number of assessments, reducing task duration and costs. Additionally, the Summary modality maximizes internal agreement and maintains consistent reliance on and perceived usefulness of evidence, demonstrating its potential to streamline large-scale truthfulness evaluations.","authors":["Kevin Roitero","Dustin Wright","Michael Soprano","Isabelle Augenstein","Stefano Mizzaro"],"url":"https://arxiv.org/abs/2501.18265"}
{"created":"2025-05-02","title":"Diversity By Design: Leveraging Distribution Matching for Offline Model-Based Optimization","abstract":"The goal of offline model-based optimization (MBO) is to propose new designs that maximize a reward function given only an offline dataset. However, an important desiderata is to also propose a diverse set of final candidates that capture many optimal and near-optimal design configurations. We propose Diversity in Adversarial Model-based Optimization (DynAMO) as a novel method to introduce design diversity as an explicit objective into any MBO problem. Our key insight is to formulate diversity as a distribution matching problem where the distribution of generated designs captures the inherent diversity contained within the offline dataset. Extensive experiments spanning multiple scientific domains show that DynAMO can be used with common optimization methods to significantly improve the diversity of proposed designs while still discovering high-quality candidates.","authors":["Michael S. Yao","James C. Gee","Osbert Bastani"],"url":"https://arxiv.org/abs/2501.18768"}
{"created":"2025-05-02","title":"Accelerating Diffusion Transformer via Error-Optimized Cache","abstract":"Diffusion Transformer (DiT) is a crucial method for content generation. However, it needs a lot of time to sample. Many studies have attempted to use caching to reduce the time consumption of sampling. Existing caching methods accelerate generation by reusing DiT features from the previous time step and skipping calculations in the next, but they tend to locate and cache low-error modules without focusing on reducing caching-induced errors, resulting in a sharp decline in generated content quality when increasing caching intensity. To solve this problem, we propose the Error-Optimized Cache (EOC). This method introduces three key improvements: (1) Prior knowledge extraction: Extract and process the caching differences; (2) A judgment method for cache optimization: Determine whether certain caching steps need to be optimized; (3) Cache optimization: reduce caching errors. Experiments show that this algorithm significantly reduces the error accumulation caused by caching, especially excessive caching. On the ImageNet dataset, without substantially increasing the computational load, this method improves the FID of the generated images when the rule-based model FORA has a caching level of 75%, 50%, and 25%, and the training-based model Learning-to-cache has a caching level of 22%. Specifically, the FID values change from 30.454 to 21.690 (28.8%), from 6.857 to 5.821 (15.1%), from 3.870 to 3.692 (4.6%), and from 3.539 to 3.451 (2.5%) respectively.","authors":["Junxiang Qiu","Shuo Wang","Jinda Lu","Lin Liu","Houcheng Jiang","Xingyu Zhu","Yanbin Hao"],"url":"https://arxiv.org/abs/2501.19243"}
{"created":"2025-05-02","title":"Multi-Objective Reinforcement Learning for Power Grid Topology Control","abstract":"Transmission grid congestion increases as the electrification of various sectors requires transmitting more power. Topology control, through substation reconfiguration, can reduce congestion but its potential remains under-exploited in operations. A challenge is modeling the topology control problem to align well with the objectives and constraints of operators. Addressing this challenge, this paper investigates the application of multi-objective reinforcement learning (MORL) to integrate multiple conflicting objectives for power grid topology control. We develop a MORL approach using deep optimistic linear support (DOL) and multi-objective proximal policy optimization (MOPPO) to generate a set of Pareto-optimal policies that balance objectives such as minimizing line loading, topological deviation, and switching frequency. Initial case studies show that the MORL approach can provide valuable insights into objective trade-offs and improve Pareto front approximation compared to a random search baseline. The generated multi-objective RL policies are 30% more successful in preventing grid failure under contingencies and 20% more effective when training budget is reduced - compared to the common single objective RL policy.","authors":["Thomas Lautenbacher","Ali Rajaei","Davide Barbieri","Jan Viebahn","Jochen L. Cremer"],"url":"https://arxiv.org/abs/2502.00040"}
{"created":"2025-05-02","title":"Robot localization aided by quantum algorithms","abstract":"Localization is a critical aspect of mobile robotics, enabling robots to navigate their environment efficiently and avoid obstacles. Current probabilistic localization methods, such as the Adaptive-Monte Carlo localization (AMCL) algorithm, are computationally intensive and may struggle with large maps or high-resolution sensor data. This paper explores the application of quantum computing in robotics, focusing on the use of Grover's search algorithm to improve the efficiency of localization in mobile robots. We propose a novel approach to utilize Grover's algorithm in a 2D map, enabling faster and more efficient localization. Despite the limitations of current physical quantum computers, our experimental results demonstrate a significant speedup over classical methods, highlighting the potential of quantum computing to improve robotic localization. This work bridges the gap between quantum computing and robotics, providing a practical solution for robotic localization and paving the way for future research in quantum robotics.","authors":["Unai Antero","Basilio Sierra","Jon O\\~nativia","Alejandra Ruiz","Eneko Osaba"],"url":"https://arxiv.org/abs/2502.00077"}
{"created":"2025-05-02","title":"On the Source Model Key Agreement Problem","abstract":"We consider the source model key agreement problem involving two legitimate parties and an eavesdropper who observe n i.i.d. samples of X, Y, and Z, respectively. In this paper, we focus on one of the simplest instances where the key capacity remains open, specifically when X and Y are binary random variables and Z is a function of the pair (X, Y). The best-known upper bound on the key capacity is characterized by an inf-max optimization problem that generally lacks a closed-form solution. We provide general conditions under which the upper bound reduces to I(X;Y). As an example, we consider the XOR setting in which X and Y are binary, and Z is the XOR of X and Y. The upper bound reduces to I(X;Y) for this source. Next, we conjecture that the rate I(X;Y) is not achievable for the XOR source and provide some ideas that might be useful for developing a new upper bound on the source model problem.","authors":["Hamidreza Abin","Amin Gohari"],"url":"https://arxiv.org/abs/2502.00294"}
{"created":"2025-05-02","title":"Generalizing Safety Beyond Collision-Avoidance via Latent-Space Reachability Analysis","abstract":"Hamilton-Jacobi (HJ) reachability is a rigorous mathematical framework that enables robots to simultaneously detect unsafe states and generate actions that prevent future failures. While in theory, HJ reachability can synthesize safe controllers for nonlinear systems and nonconvex constraints, in practice, it has been limited to hand-engineered collision-avoidance constraints modeled via low-dimensional state-space representations and first-principles dynamics. In this work, our goal is to generalize safe robot controllers to prevent failures that are hard--if not impossible--to write down by hand, but can be intuitively identified from high-dimensional observations: for example, spilling the contents of a bag. We propose Latent Safety Filters, a latent-space generalization of HJ reachability that tractably operates directly on raw observation data (e.g., RGB images) to automatically compute safety-preserving actions without explicit recovery demonstrations by performing safety analysis in the latent embedding space of a generative world model. Our method leverages diverse robot observation-action data of varying quality (including successes, random exploration, and unsafe demonstrations) to learn a world model. Constraint specification is then transformed into a classification problem in the latent space of the learned world model. In simulation and hardware experiments, we compute an approximation of Latent Safety Filters to safeguard arbitrary policies (from imitation- learned policies to direct teleoperation) from complex safety hazards, like preventing a Franka Research 3 manipulator from spilling the contents of a bag or toppling cluttered objects.","authors":["Kensuke Nakamura","Lasse Peters","Andrea Bajcsy"],"url":"https://arxiv.org/abs/2502.00935"}
{"created":"2025-05-02","title":"Non-minimality of minimal telescopers explained by residues","abstract":"Elaborating on an approach recently proposed by Mark van Hoeij, we continue to investigate why creative telescoping occasionally fails to find the minimal-order annihilating operator of a given definite sum or integral. We offer an explanation based on the consideration of residues.","authors":["Shaoshi Chen","Manuel Kauers","Christoph Koutschan","Xiuyun Li","Rong-Hua Wang","Yisen Wang"],"url":"https://arxiv.org/abs/2502.03757"}
{"created":"2025-05-02","title":"Hypencoder: Hypernetworks for Information Retrieval","abstract":"Existing information retrieval systems are largely constrained by their reliance on vector inner products to assess query-document relevance, which naturally limits the expressiveness of the relevance score they can produce. We propose a new paradigm; instead of representing a query as a vector, we use a small neural network that acts as a learned query-specific relevance function. This small neural network takes a document representation as input (in this work we use a single vector) and produces a scalar relevance score. To produce the small neural network we use a hypernetwork, a network that produces the weights of other networks, as our query encoder. We name this category of encoder models Hypencoders. Experiments on in-domain search tasks show that Hypencoders significantly outperform strong dense retrieval models and even surpass reranking models and retrieval models with an order of magnitude more parameters. To assess the extent of Hypencoders' capabilities, we evaluate on a set of hard retrieval tasks including tip-of-the-tongue and instruction-following retrieval tasks. On harder tasks, we find that the performance gap widens substantially compared to standard retrieval tasks. Furthermore, to demonstrate the practicality of our method, we implement an approximate search algorithm and show that our model is able to retrieve from a corpus of 8.8M documents in under 60 milliseconds.","authors":["Julian Killingback","Hansi Zeng","Hamed Zamani"],"url":"https://arxiv.org/abs/2502.05364"}
{"created":"2025-05-02","title":"HSI: Head-Specific Intervention Can Induce Misaligned AI Coordination in Large Language Models","abstract":"Robust alignment guardrails for large language models are becoming increasingly important with their widespread application. In contrast to previous studies, we demonstrate that inference-time activation interventions can bypass safety alignments and effectively steer model generations towards harmful AI coordination for Llama 2. Our method applies fine-grained interventions at specific model subcomponents, particularly attention heads, using a simple binary choice probing strategy. These interventions then generalise to the open-ended generation setting effectively circumventing safety guardrails. We show that probing single attention heads is more effective than intervening on full layers and intervening on only four attention heads is comparable to supervised fine-tuning. We further show that only a few example completions are needed to compute effective steering directions, which is an advantage over classical fine-tuning. Our findings highlight the shortcomings of current alignment techniques. In addition, our results suggest that, at the attention head level, activations encode fine-grained linearly separable behaviors. Practically, the approach offers a straightforward methodology to steer large language model behaviour, which could be extended to diverse domains beyond safety requiring fine-grained control over the model output. The code and datasets for this study can be found on https://github.com/PaulDrm/targeted_intervention.","authors":["Paul Darm","Annalisa Riccardi"],"url":"https://arxiv.org/abs/2502.05945"}
{"created":"2025-05-02","title":"Decision Making in Hybrid Environments: A Model Aggregation Approach","abstract":"Recent work by Foster et al. (2021, 2022, 2023b) and Xu and Zeevi (2023) developed the framework of decision estimation coefficient (DEC) that characterizes the complexity of general online decision making problems and provides a general algorithm design principle. These works, however, either focus on the pure stochastic regime where the world remains fixed over time, or the pure adversarial regime where the world arbitrarily changes over time. For the hybrid regime where the dynamics of the world is fixed while the reward arbitrarily changes, they only give pessimistic bounds on the decision complexity. In this work, we propose a general extension of DEC that more precisely characterizes this case. Besides applications in special cases, our framework leads to a flexible algorithm design where the learner learns over subsets of the hypothesis set, trading estimation complexity with decision complexity, which could be of independent interest. Our work covers model-based learning and model-free learning in the hybrid regime, with a newly proposed extension of the bilinear classes (Du et al., 2021) to the adversarial-reward case. In addition, our method improves the best-known regret bounds for linear Q*/V* MDPs in the pure stochastic regime.","authors":["Haolin Liu","Chen-Yu Wei","Julian Zimmert"],"url":"https://arxiv.org/abs/2502.05974"}
{"created":"2025-05-02","title":"On the Reliability of Information Retrieval From MDS Coded Data in DNA Storage","abstract":"This work presents a theoretical analysis of the probability of successfully retrieving data encoded with MDS codes (e.g., Reed-Solomon codes) in DNA storage systems. We study this probability under independent and identically distributed (i.i.d.) substitution errors, focusing on a common code design strategy that combines inner and outer MDS codes. Our analysis demonstrates how this probability depends on factors such as the total number of sequencing reads, their distribution across strands, the rates of the inner and outer codes, and the substitution error probabilities. These results provide actionable insights into optimizing DNA storage systems under reliability constraints, including determining the minimum number of sequencing reads needed for reliable data retrieval and identifying the optimal balance between the rates of inner and outer MDS codes.","authors":["Serge Kas Hanna"],"url":"https://arxiv.org/abs/2502.06618"}
{"created":"2025-05-02","title":"RoboBERT: An End-to-end Multimodal Robotic Manipulation Model","abstract":"Embodied intelligence seamlessly integrates vision, language, and action.~However, most multimodal robotic models rely on massive fine-tuning, incurring high time and hardware costs.~To address this, we introduce RoboBERT, an end-to-end multimodal manipulation model built around a novel two-stage training paradigm.~In the first stage, we freeze most of the vision encoder and train with a single \"standard\" instruction phrasing, allowing the model to focus on stable policy learning via a CNN-based diffusion policy.~In the second stage, we unfreeze all modules and inject diverse natural language variants, rapidly aligning varied instructions to the already-learned policy without destabilizing performance.~We further employ systematic data augmentations to enhance robustness against visual perturbations.~Without relying on auxiliary datasets, RoboBERT achieves new state-of-the-art (SOTA) mean episode lengths of 4.52 on the CALVIN ABCD-D benchmark and 3.79 on the ABC-D benchmark using only language-labeled expert demonstrations and a comparatively lightweight architecture.Real-robot trials on a 6-DOF manipulator confirm higher success rates than comparable methods trained on identical data.These results demonstrate that our data-augmentation-enhanced two-stage training paradigm delivers efficient, scalable, and broadly applicable performance for multimodal robotic systems.","authors":["Sicheng Wang","Sheng Liu","Weiheng Wang","Jianhua Shan","Bin Fang"],"url":"https://arxiv.org/abs/2502.07837"}
{"created":"2025-05-02","title":"Caught in the Web of Words: Do LLMs Fall for Spin in Medical Literature?","abstract":"Medical research faces well-documented challenges in translating novel treatments into clinical practice. Publishing incentives encourage researchers to present \"positive\" findings, even when empirical results are equivocal. Consequently, it is well-documented that authors often spin study results, especially in article abstracts. Such spin can influence clinician interpretation of evidence and may affect patient care decisions. In this study, we ask whether the interpretation of trial results offered by Large Language Models (LLMs) is similarly affected by spin. This is important since LLMs are increasingly being used to trawl through and synthesize published medical evidence. We evaluated 22 LLMs and found that they are across the board more susceptible to spin than humans. They might also propagate spin into their outputs: We find evidence, e.g., that LLMs implicitly incorporate spin into plain language summaries that they generate. We also find, however, that LLMs are generally capable of recognizing spin, and can be prompted in a way to mitigate spin's impact on LLM outputs.","authors":["Hye Sun Yun","Karen Y. C. Zhang","Ramez Kouzy","Iain J. Marshall","Junyi Jessy Li","Byron C. Wallace"],"url":"https://arxiv.org/abs/2502.07963"}
{"created":"2025-05-02","title":"Self-Explaining Hypergraph Neural Networks for Diagnosis Prediction","abstract":"The burgeoning volume of electronic health records (EHRs) has enabled deep learning models to excel in predictive healthcare. However, for high-stakes applications such as diagnosis prediction, model interpretability remains paramount. Existing deep learning diagnosis prediction models with intrinsic interpretability often assign attention weights to every past diagnosis or hospital visit, providing explanations lacking flexibility and succinctness. In this paper, we introduce SHy, a self-explaining hypergraph neural network model, designed to offer personalized, concise and faithful explanations that allow for interventions from clinical experts. By modeling each patient as a unique hypergraph and employing a message-passing mechanism, SHy captures higher-order disease interactions and extracts distinct temporal phenotypes as personalized explanations. It also addresses the incompleteness of the EHR data by accounting for essential false negatives in the original diagnosis record. A qualitative case study and extensive quantitative evaluations on two real-world EHR datasets demonstrate the superior predictive performance and interpretability of SHy over existing state-of-the-art models.","authors":["Leisheng Yu","Yanxiao Cai","Minxing Zhang","Xia Hu"],"url":"https://arxiv.org/abs/2502.10689"}
{"created":"2025-05-02","title":"Cognitive Neural Architecture Search Reveals Hierarchical Entailment","abstract":"Recent research has suggested that the brain is more shallow than previously thought, challenging the traditionally assumed hierarchical structure of the ventral visual pathway. Here, we demonstrate that optimizing convolutional network architectures for brain-alignment via evolutionary neural architecture search results in models with clear representational hierarchies. Despite having random weights, the identified models achieve brain-alignment scores surpassing even those of pretrained classification models - as measured by both regression and representational similarity analysis. Furthermore, through traditional supervised training, architectures optimized for alignment with late ventral regions become competitive classification models. These findings suggest that hierarchical structure is a fundamental mechanism of primate visual processing. Finally, this work demonstrates the potential of neural architecture search as a framework for computational cognitive neuroscience research that could reduce the field's reliance on manually designed convolutional networks.","authors":["Lukas Kuhn","Sari Saba-Sadiya","Gemma Roig"],"url":"https://arxiv.org/abs/2502.11141"}
{"created":"2025-05-02","title":"Generative Predictive Control: Flow Matching Policies for Dynamic and Difficult-to-Demonstrate Tasks","abstract":"Generative control policies have recently unlocked major progress in robotics. These methods produce action sequences via diffusion or flow matching, with training data provided by demonstrations. But existing methods come with two key limitations: they require expert demonstrations, which can be difficult to obtain, and they are limited to relatively slow, quasi-static tasks. In this paper, we leverage a tight connection between sampling-based predictive control and generative modeling to address each of these issues. In particular, we introduce generative predictive control, a supervised learning framework for tasks with fast dynamics that are easy to simulate but difficult to demonstrate. We then show how trained flow-matching policies can be warm-started at inference time, maintaining temporal consistency and enabling high-frequency feedback. We believe that generative predictive control offers a complementary approach to existing behavior cloning methods, and hope that it paves the way toward generalist policies that extend beyond quasi-static demonstration-oriented tasks.","authors":["Vince Kurtz","Joel W. Burdick"],"url":"https://arxiv.org/abs/2502.13406"}
{"created":"2025-05-02","title":"Reproducing NevIR: Negation in Neural Information Retrieval","abstract":"Negation is a fundamental aspect of human communication, yet it remains a challenge for Language Models (LMs) in Information Retrieval (IR). Despite the heavy reliance of modern neural IR systems on LMs, little attention has been given to their handling of negation. In this study, we reproduce and extend the findings of NevIR, a benchmark study that revealed most IR models perform at or below the level of random ranking when dealing with negation. We replicate NevIR's original experiments and evaluate newly developed state-of-the-art IR models. Our findings show that a recently emerging category-listwise Large Language Model (LLM) re-rankers-outperforms other models but still underperforms human performance. Additionally, we leverage ExcluIR, a benchmark dataset designed for exclusionary queries with extensive negation, to assess the generalisability of negation understanding. Our findings suggest that fine-tuning on one dataset does not reliably improve performance on the other, indicating notable differences in their data distributions. Furthermore, we observe that only cross-encoders and listwise LLM re-rankers achieve reasonable performance across both negation tasks.","authors":["Coen van den Elsen","Francien Barkhof","Thijmen Nijdam","Simon Lupart","Mohammad Aliannejadi"],"url":"https://arxiv.org/abs/2502.13506"}
{"created":"2025-05-02","title":"The illusion of households as entities in social networks","abstract":"Data recording connections between people in communities and villages are collected and analyzed in various ways, most often as either networks of individuals or as networks of households. These two networks can differ in substantial ways. The methodological choice of which network to study, therefore, is an important aspect in both study design and data analysis. In this work, we consider various key differences between household and individual social network structure, and ways in which the networks cannot be used interchangeably. In addition to formalizing the choices for representing each network, we explore the consequences of how the results of social network analysis change depending on the choice between studying the individual and household network -- from determining whether networks are assortative or disassortative to the ranking of influence-maximizing nodes. As our main contribution, we draw upon related work to propose a set of systematic recommendations for determining the relevant network representation to study. Our recommendations include assessing a series of entitativity criteria and relating these criteria to theories and observations about patterns and norms in social dynamics at the household level: notably, how information spreads within households and how power structures and gender roles affect this spread. We draw upon the definition of an illusion of entitativity to identify cases wherein grouping people into households does not satisfy these criteria or adequately represent given cultural or experimental contexts. Given the widespread use of social network data for studying communities, there is broad impact in understanding which network to study and the consequences of that decision. We hope that this work gives guidance to practitioners and researchers collecting and studying social network data.","authors":["Izabel Aguiar","Philip S. Chodrow","Johan Ugander"],"url":"https://arxiv.org/abs/2502.14764"}
{"created":"2025-05-02","title":"TerEffic: Highly Efficient Ternary LLM Inference on FPGA","abstract":"Deploying Large Language Models (LLMs) efficiently on edge devices is often constrained by limited memory capacity and high power consumption. Low-bit quantization methods, particularly ternary quantization, have demonstrated significant potential in preserving model accuracy while substantially decreasing memory footprint and computational costs. However, existing general-purpose architectures and accelerators have not fully exploited the advantages of low-bit quantization due to insufficient specialized hardware support. We introduce TerEffic, an FPGA-based architecture tailored for ternary-quantized LLM inference. The proposed system offers flexibility through reconfigurable hardware to meet various system requirements. We evaluated two representative configurations: a fully on-chip design that stores all weights within on-chip memories, scaling out using multiple FPGAs, and an HBM-assisted design capable of accommodating larger models on a single FPGA board. Experimental results demonstrate significant performance and energy efficiency improvements. For single-batch inference on a 370 M-parameter model, our fully on-chip architecture achieves 16,300 tokens/second, delivering a throughput 192 times higher than NVIDIA Jetson Orin Nano with a power efficiency of 455 tokens/second/W, marking a 19-fold improvement. The HBM-assisted architecture processes 727 tokens/second for a larger 2.7B-parameter model, which is 3 times of the throughput of NVIDIA A100, while consuming only 46W, resulting in a power efficiency of 16 tokens/second/W, an 8-fold improvement over the A100.","authors":["Chenyang Yin","Zhenyu Bai","Pranav Venkatram","Shivam Aggarwal","Zhaoying Li","Tulika Mitra"],"url":"https://arxiv.org/abs/2502.16473"}
{"created":"2025-05-02","title":"SpargeAttn: Accurate Sparse Attention Accelerating Any Model Inference","abstract":"An efficient attention implementation is essential for large models due to its quadratic time complexity. Fortunately, attention commonly exhibits sparsity, i.e., many values in the attention map are near zero, allowing for the omission of corresponding computations. Many studies have utilized the sparse pattern to accelerate attention. However, most existing works focus on optimizing attention within specific models by exploiting certain sparse patterns of the attention map. A universal sparse attention that guarantees both the speedup and end-to-end performance of diverse models remains elusive. In this paper, we propose SpargeAttn, a universal sparse and quantized attention for any model. Our method uses a two-stage online filter: in the first stage, we rapidly and accurately predict the attention map, enabling the skip of some matrix multiplications in attention. In the second stage, we design an online softmax-aware filter that incurs no extra overhead and further skips some matrix multiplications. Experiments show that our method significantly accelerates diverse models, including language, image, and video generation, without sacrificing end-to-end metrics. The codes are available at https://github.com/thu-ml/SpargeAttn.","authors":["Jintao Zhang","Chendong Xiang","Haofeng Huang","Jia Wei","Haocheng Xi","Jun Zhu","Jianfei Chen"],"url":"https://arxiv.org/abs/2502.18137"}
{"created":"2025-05-02","title":"MFSR-GAN: Multi-Frame Super-Resolution with Handheld Motion Modeling","abstract":"Smartphone cameras have become ubiquitous imaging tools, yet their small sensors and compact optics often limit spatial resolution and introduce distortions. Combining information from multiple low-resolution (LR) frames to produce a high-resolution (HR) image has been explored to overcome the inherent limitations of smartphone cameras. Despite the promise of multi-frame super-resolution (MFSR), current approaches are hindered by datasets that fail to capture the characteristic noise and motion patterns found in real-world handheld burst images. In this work, we address this gap by introducing a novel synthetic data engine that uses multi-exposure static images to synthesize LR-HR training pairs while preserving sensor-specific noise characteristics and image motion found during handheld burst photography. We also propose MFSR-GAN: a multi-scale RAW-to-RGB network for MFSR. Compared to prior approaches, MFSR-GAN emphasizes a \"base frame\" throughout its architecture to mitigate artifacts. Experimental results on both synthetic and real data demonstrates that MFSR-GAN trained with our synthetic engine yields sharper, more realistic reconstructions than existing methods for real-world MFSR.","authors":["Fadeel Sher Khan","Joshua Ebenezer","Hamid Sheikh","Seok-Jun Lee"],"url":"https://arxiv.org/abs/2502.20824"}
{"created":"2025-05-02","title":"UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models for Multilingual Multimodal Idiomaticity Representation","abstract":"SemEval-2025 Task 1 focuses on ranking images based on their alignment with a given nominal compound that may carry idiomatic meaning in both English and Brazilian Portuguese. To address this challenge, this work uses generative large language models (LLMs) and multilingual CLIP models to enhance idiomatic compound representations. LLMs generate idiomatic meanings for potentially idiomatic compounds, enriching their semantic interpretation. These meanings are then encoded using multilingual CLIP models, serving as representations for image ranking. Contrastive learning and data augmentation techniques are applied to fine-tune these embeddings for improved performance. Experimental results show that multimodal representations extracted through this method outperformed those based solely on the original nominal compounds. The fine-tuning approach shows promising outcomes but is less effective than using embeddings without fine-tuning. The source code used in this paper is available at https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.","authors":["Thanet Markchom","Tong Wu","Liting Huang","Huizhi Liang"],"url":"https://arxiv.org/abs/2502.20984"}
{"created":"2025-05-02","title":"AMUN: Adversarial Machine UNlearning","abstract":"Machine unlearning, where users can request the deletion of a forget dataset, is becoming increasingly important because of numerous privacy regulations. Initial works on ``exact'' unlearning (e.g., retraining) incur large computational overheads. However, while computationally inexpensive, ``approximate'' methods have fallen short of reaching the effectiveness of exact unlearning: models produced fail to obtain comparable accuracy and prediction confidence on both the forget and test (i.e., unseen) dataset. Exploiting this observation, we propose a new unlearning method, Adversarial Machine UNlearning (AMUN), that outperforms prior state-of-the-art (SOTA) methods for image classification. AMUN lowers the confidence of the model on the forget samples by fine-tuning the model on their corresponding adversarial examples. Adversarial examples naturally belong to the distribution imposed by the model on the input space; fine-tuning the model on the adversarial examples closest to the corresponding forget samples (a) localizes the changes to the decision boundary of the model around each forget sample and (b) avoids drastic changes to the global behavior of the model, thereby preserving the model's accuracy on test samples. Using AMUN for unlearning a random $10\\%$ of CIFAR-10 samples, we observe that even SOTA membership inference attacks cannot do better than random guessing.","authors":["Ali Ebrahimpour-Boroojeny","Hari Sundaram","Varun Chandrasekaran"],"url":"https://arxiv.org/abs/2503.00917"}
{"created":"2025-05-02","title":"A Bridge to Nowhere: A Healthcare Case Study for Non-Reformist Design","abstract":"In the face of intensified datafication and automation in public sector industries, frameworks like design justice and the feminist practice of refusal provide help to identify and mitigate structural harm and challenge inequities reproduced in digitized infrastructures. This paper applies those frameworks to emerging efforts across the U.S. healthcare industry to automate prior authorization -- a process whereby insurance companies determine whether a treatment or service is 'medically necessary' before agreeing to cover it. Federal regulatory interventions turn to datafication and automation to reduce the harms of this widely unpopular process shown to delay vital treatments and create immense administrative burden for healthcare providers and patients. This paper explores emerging prior authorization reforms as a case study, applying the frameworks of design justice and refusal to highlight the inherent conservatism of interventions oriented towards improving the user experience of extractive systems. I further explore how the abolitionist framework of non-reformist reform helps to clarify alternative interventions that would mitigate the harms of prior authorization in ways that do not reproduce or extend the power of insurance companies. I propose a set of four tenets for nonreformist design to mitigate structural harms and advance design justice in a broad set of domains.","authors":["Linda Huber"],"url":"https://arxiv.org/abs/2503.03849"}
{"created":"2025-05-02","title":"A cross-regional review of AI safety regulations in the commercial aviation","abstract":"In this paper we examine the existing artificial intelligence (AI) policy documents in aviation for the following three regions: the United States, European Union, and China. The aviation industry has always been a first mover in adopting technological advancements. This early adoption offers valuable insights because of its stringent regulations and safety-critical procedures. As a result, the aviation industry provides an optimal platform to counter AI vulnerabilities through its tight regulations, standardization processes, and certification of new technologies. Keywords: AI in aviation; aviation safety; standardization; certifiable AI; regulations","authors":["Penny A. Barr","Sohel M. Imroz"],"url":"https://arxiv.org/abs/2503.04767"}
{"created":"2025-05-02","title":"Process-Supervised LLM Recommenders via Flow-guided Tuning","abstract":"While large language models (LLMs) are increasingly adapted for recommendation systems via supervised fine-tuning (SFT), this approach amplifies popularity bias due to its likelihood maximization objective, compromising recommendation diversity and fairness. To address this, we present Flow-guided fine-tuning recommender (Flower), which replaces SFT with a Generative Flow Network (GFlowNet) framework that enacts process supervision through token-level reward propagation. Flower's key innovation lies in decomposing item-level rewards into constituent token rewards, enabling direct alignment between token generation probabilities and their reward signals. This mechanism achieves three critical advancements: (1) popularity bias mitigation and fairness enhancement through empirical distribution matching, (2) preservation of diversity through GFlowNet's proportional sampling, and (3) flexible integration of personalized preferences via adaptable token rewards. Experiments demonstrate Flower's superior distribution-fitting capability and its significant advantages over traditional SFT in terms of accuracy, fairness, and diversity, highlighting its potential to improve LLM-based recommendation systems. The implementation is available via https://github.com/MrPeach0301/Flower","authors":["Chongming Gao","Mengyao Gao","Chenxiao Fan","Shuai Yuan","Wentao Shi","Xiangnan He"],"url":"https://arxiv.org/abs/2503.07377"}
{"created":"2025-05-02","title":"KAP: MLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese Non-Narrative Documents","abstract":"Hybrid Retrieval systems, combining Sparse and Dense Retrieval methods, struggle with Traditional Chinese non-narrative documents due to their complex formatting, rich vocabulary, and the insufficient understanding of Chinese synonyms by common embedding models. Previous approaches inadequately address the dual needs of these systems, focusing mainly on general text quality improvement rather than optimizing for retrieval. We propose Knowledge-Aware Preprocessing (KAP), a novel framework that transforms noisy OCR outputs into retrieval-optimized text. KAP adopts a two-stage approach: it first extracts text using OCR, then employs Multimodal Large Language Models to refine the output by integrating visual information from the original documents. This design reduces OCR noise, reconstructs structural elements, and formats the text to satisfy the distinct requirements of sparse and dense retrieval. Empirical results demonstrate that KAP consistently and significantly outperforms conventional preprocessing approaches. Our code is available at https://github.com/JustinHsu1019/KAP.","authors":["Hsin-Ling Hsu","Ping-Sheng Lin","Jing-Di Lin","Jengnan Tzeng"],"url":"https://arxiv.org/abs/2503.08452"}
{"created":"2025-05-02","title":"Cooperative Deterministic Learning-Based Formation Control for a Group of Nonlinear Mechanical Systems Under Complete Uncertainty","abstract":"In this work we address the formation control problem for a group of nonlinear mechanical systems with complete uncertain dynamics under a virtual leader-following framework. We propose a novel cooperative deterministic learning-based adaptive formation control algorithm. This algorithm is designed by utilizing artificial neural networks to simultaneously achieve formation tracking control and locally-accurate identification/learning of the nonlinear uncertain dynamics of the considered group of mechanical systems. To demonstrate the practicality and verify the effectiveness of the proposed results, numerical simulations have been conducted.","authors":["Maryam Norouzi","Mingxi Zhou","Chengzhi Yuan"],"url":"https://arxiv.org/abs/2503.13688"}
{"created":"2025-05-02","title":"Streamlining SIMD ISA Extensions with Takum Arithmetic: A Case Study on Intel AVX10.2","abstract":"Modern microprocessors extend their instruction set architecture (ISA) with Single Instruction, Multiple Data (SIMD) operations to improve performance. The Intel Advanced Vector Extensions (AVX) enhance the x86 ISA and are widely supported in Intel and AMD processors. The latest version, AVX10.2, places a strong emphasis on low-precision, non-standard floating-point formats, including bfloat16 and E4M3/E5M2 float8 (OCP 8-bit Floating Point, OFP8), primarily catering to deep learning applications rather than general-purpose arithmetic. However, as these formats remain within the IEEE 754 framework, they inherit its limitations, introducing inconsistencies and added complexity into the ISA.","authors":["Laslo Hunhold"],"url":"https://arxiv.org/abs/2503.14067"}
{"created":"2025-05-02","title":"Real AI Agents with Fake Memories: Fatal Context Manipulation Attacks on Web3 Agents","abstract":"The integration of AI agents with Web3 ecosystems harnesses their complementary potential for autonomy and openness yet also introduces underexplored security risks, as these agents dynamically interact with financial protocols and immutable smart contracts. This paper investigates the vulnerabilities of AI agents within blockchain-based financial ecosystems when exposed to adversarial threats in real-world scenarios. We introduce the concept of context manipulation, a comprehensive attack vector that exploits unprotected context surfaces, including input channels, memory modules, and external data feeds.","authors":["Atharv Singh Patlan","Peiyao Sheng","S. Ashwin Hebbar","Prateek Mittal","Pramod Viswanath"],"url":"https://arxiv.org/abs/2503.16248"}
{"created":"2025-05-02","title":"Efficient IoT Intrusion Detection with an Improved Attention-Based CNN-BiLSTM Architecture","abstract":"The ever-increasing security vulnerabilities in the Internet-of-Things (IoT) systems require improved threat detection approaches. This paper presents a compact and efficient approach to detect botnet attacks by employing an integrated approach that consists of traffic pattern analysis, temporal support learning, and focused feature extraction. The proposed attention-based model benefits from a hybrid CNN-BiLSTM architecture and achieves 99% classification accuracy in detecting botnet attacks utilizing the N-BaIoT dataset, while maintaining high precision and recall across various scenarios. The proposed model's performance is further validated by key parameters, such as Mathews Correlation Coefficient and Cohen's kappa Correlation Coefficient. The close-to-ideal results for these parameters demonstrate the proposed model's ability to detect botnet attacks accurately and efficiently in practical settings and on unseen data. The proposed model proved to be a powerful defence mechanism for IoT networks to face emerging security challenges.","authors":["Amna Naeem","Muazzam A. Khan","Nada Alasbali","Jawad Ahmad","Aizaz Ahmad Khattak","Muhammad Shahbaz Khan"],"url":"https://arxiv.org/abs/2503.19339"}
{"created":"2025-05-02","title":"Uncertainty-aware Bayesian machine learning modelling of land cover classification","abstract":"Land cover classification involves the production of land cover maps, which determine the type of land through remote sensing imagery. Over recent years, such classification is being performed by machine learning classification models, which can give highly accurate predictions on land cover per pixel using large quantities of input training data. However, such models do not currently take account of input measurement uncertainty, which is vital for traceability in metrology. In this work we propose a Bayesian classification framework using generative modelling to take account of input measurement uncertainty. We take the specific case of Bayesian quadratic discriminant analysis, and apply it to land cover datasets from Copernicus Sentinel-2 in 2020 and 2021. We benchmark the performance of the model against more popular classification models used in land cover maps such as random forests and neural networks. We find that such Bayesian models are more trustworthy, in the sense that they are more interpretable, explicitly model the input measurement uncertainty, and maintain predictive performance of class probability outputs across datasets of different years and sizes, whilst also being computationally efficient.","authors":["Samuel Bilson","Anna Pustogvar"],"url":"https://arxiv.org/abs/2503.21510"}
{"created":"2025-05-02","title":"DWTRec: An Efficient Wavelet Enhanced Model for Sequential Recommendation","abstract":"Transformer-based sequential recommender systems have achieved notable successes. Despite their effectiveness, recent studies reveal that self-attention mechanism used in current Transformer-based sequential recommendation models is constantly a low-pass filter which results in several problems. These problems include being incompetent in learning evolving user patterns and capturing users' abrupt interests. To integrate low and high frequency information effectively and overcome these problems, we propose a novel Wavelet enhanced hybrid attention network called DWTRec for sequential recommendation. Due to excellent characteristics of Wavelet Transform in signal processing, we can perform fine-grained signals decomposition while removing noise and capturing trending signals behind non-stationary sequences of user-item interactions. To address the bound distortion problem of the reconstructed signal of Discrete Wavelet Transform, we design a learning algorithm which adapts to different datasets. Our findings suggest a promising approach to improve the modeling power of self-attention. We tested our method on six datasets with different domains, different sparsity levels, and different average sequence lengths. Experiments show that our method outperforms all eight baseline models in recommendation performance on all datasets.","authors":["Sheng Lu","Mingxi Ge","Jiuyi Zhang","Wanli Zhu","Guanjin Li","Fangming Gu"],"url":"https://arxiv.org/abs/2503.23436"}
{"created":"2025-05-02","title":"MolGround: A Benchmark for Molecular Grounding","abstract":"Current molecular understanding approaches predominantly focus on the descriptive aspect of human perception, providing broad, topic-level insights. However, the referential aspect -- linking molecular concepts to specific structural components -- remains largely unexplored. To address this gap, we propose a molecular grounding benchmark designed to evaluate a model's referential abilities. We align molecular grounding with established conventions in NLP, cheminformatics, and molecular science, showcasing the potential of NLP techniques to advance molecular understanding within the AI for Science movement. Furthermore, we constructed the largest molecular understanding benchmark to date, comprising 117k QA pairs, and developed a multi-agent grounding prototype as proof of concept. This system outperforms existing models, including GPT-4o, and its grounding outputs have been integrated to enhance traditional tasks such as molecular captioning and ATC (Anatomical, Therapeutic, Chemical) classification.","authors":["Jiaxin Wu","Ting Zhang","Rubing Chen","Wengyu Zhang","Chen Jason Zhang","Xiao-Yong Wei","Li Qing"],"url":"https://arxiv.org/abs/2503.23668"}
{"created":"2025-05-02","title":"Dynamic Parametric Retrieval Augmented Generation for Test-time Knowledge Enhancement","abstract":"Retrieval-augmented generation (RAG) enhances large language models (LLMs) by retrieving relevant documents from external sources and incorporating them into the context. While it improves reliability by providing factual texts, it significantly increases inference costs as context length grows and introduces challenging issue of RAG hallucination, primarily caused by the lack of corresponding parametric knowledge in LLMs. An efficient solution is to enhance the knowledge of LLMs at test-time. Parametric RAG (PRAG) addresses this by embedding document into LLMs parameters to perform test-time knowledge enhancement, effectively reducing inference costs through offline training. However, its high training and storage costs, along with limited generalization ability, significantly restrict its practical adoption. To address these challenges, we propose Dynamic Parametric RAG (DyPRAG), a novel framework that leverages a lightweight parameter translator model to efficiently convert documents into parametric knowledge. DyPRAG not only reduces inference, training, and storage costs but also dynamically generates parametric knowledge, seamlessly enhancing the knowledge of LLMs and resolving knowledge conflicts in a plug-and-play manner at test-time. Extensive experiments on multiple datasets demonstrate the effectiveness and generalization capabilities of DyPRAG, offering a powerful and practical RAG paradigm which enables superior knowledge fusion and mitigates RAG hallucination in real-world applications. Our code is available at https://github.com/Trae1ounG/DyPRAG.","authors":["Yuqiao Tan","Shizhu He","Huanxuan Liao","Jun Zhao","Kang Liu"],"url":"https://arxiv.org/abs/2503.23895"}
{"created":"2025-05-02","title":"Opioid Named Entity Recognition (ONER-2025) from Reddit","abstract":"The opioid overdose epidemic remains a critical public health crisis, particularly in the United States, leading to significant mortality and societal costs. Social media platforms like Reddit provide vast amounts of unstructured data that offer insights into public perceptions, discussions, and experiences related to opioid use. This study leverages Natural Language Processing (NLP), specifically Opioid Named Entity Recognition (ONER-2025), to extract actionable information from these platforms. Our research makes four key contributions. First, we created a unique, manually annotated dataset sourced from Reddit, where users share self-reported experiences of opioid use via different administration routes. This dataset contains 331,285 tokens and includes eight major opioid entity categories. Second, we detail our annotation process and guidelines while discussing the challenges of labeling the ONER-2025 dataset. Third, we analyze key linguistic challenges, including slang, ambiguity, fragmented sentences, and emotionally charged language, in opioid discussions. Fourth, we propose a real-time monitoring system to process streaming data from social media, healthcare records, and emergency services to identify overdose events. Using 5-fold cross-validation in 11 experiments, our system integrates machine learning, deep learning, and transformer-based language models with advanced contextual embeddings to enhance understanding. Our transformer-based models (bert-base-NER and roberta-base) achieved 97% accuracy and F1-score, outperforming baselines by 10.23% (RF=0.88).","authors":["Grigori Sidorov","Muhammad Ahmad","Iqra Ameer","Muhammad Usman","Ildar Batyrshin"],"url":"https://arxiv.org/abs/2504.00027"}
{"created":"2025-05-02","title":"Safe Navigation in Dynamic Environments Using Data-Driven Koopman Operators and Conformal Prediction","abstract":"We propose a novel framework for safe navigation in dynamic environments by integrating Koopman operator theory with conformal prediction. Our approach leverages data-driven Koopman approximation to learn nonlinear dynamics and employs conformal prediction to quantify uncertainty, providing statistical guarantees on approximation errors. This uncertainty is effectively incorporated into a Model Predictive Controller (MPC) formulation through constraint tightening, ensuring robust safety guarantees. We implement a layered control architecture with a reference generator providing waypoints for safe navigation. The effectiveness of our methods is validated in simulation.","authors":["Kaier Liang","Guang Yang","Mingyu Cai","Cristian-Ioan Vasile"],"url":"https://arxiv.org/abs/2504.00352"}
{"created":"2025-05-02","title":"GPG: A Simple and Strong Reinforcement Learning Baseline for Model Reasoning","abstract":"Reinforcement Learning (RL) can directly enhance the reasoning capabilities of large language models without extensive reliance on Supervised Fine-Tuning (SFT). In this work, we revisit the traditional Policy Gradient (PG) mechanism and propose a minimalist RL approach termed Group Policy Gradient (GPG). Unlike conventional methods, GPG directly optimize the original RL objective, thus obviating the need for surrogate loss functions. By eliminating the critic and reference models, avoiding KL divergence constraints, and addressing the advantage and gradient estimation bias, our approach significantly simplifies the training process compared to Group Relative Policy Optimization (GRPO). Our approach achieves superior performance without relying on auxiliary techniques or adjustments. As illustrated in Figure 1, extensive experiments demonstrate that our method not only reduces computational costs but also consistently outperforms GRPO across various unimodal and multimodal tasks. Our code is available at https://github.com/AMAP-ML/GPG.","authors":["Xiangxiang Chu","Hailang Huang","Xiao Zhang","Fei Wei","Yong Wang"],"url":"https://arxiv.org/abs/2504.02546"}
{"created":"2025-05-02","title":"GPT-ImgEval: A Comprehensive Benchmark for Diagnosing GPT4o in Image Generation","abstract":"The recent breakthroughs in OpenAI's GPT4o model have demonstrated surprisingly good capabilities in image generation and editing, resulting in significant excitement in the community. This technical report presents the first-look evaluation benchmark (named GPT-ImgEval), quantitatively and qualitatively diagnosing GPT-4o's performance across three critical dimensions: (1) generation quality, (2) editing proficiency, and (3) world knowledge-informed semantic synthesis. Across all three tasks, GPT-4o demonstrates strong performance, significantly surpassing existing methods in both image generation control and output quality, while also showcasing exceptional knowledge reasoning capabilities. Furthermore, based on the GPT-4o's generated data, we propose a classification-model-based approach to investigate the underlying architecture of GPT-4o, where our empirical results suggest the model consists of an auto-regressive (AR) combined with a diffusion-based head for image decoding, rather than the VAR-like architectures. We also provide a complete speculation on GPT-4o's overall architecture. In addition, we conduct a series of analyses to identify and visualize GPT-4o's specific limitations and the synthetic artifacts commonly observed in its image generation. We also present a comparative study of multi-round image editing between GPT-4o and Gemini 2.0 Flash, and discuss the safety implications of GPT-4o's outputs, particularly their detectability by existing image forensic models. We hope that our work can offer valuable insight and provide a reliable benchmark to guide future research, foster reproducibility, and accelerate innovation in the field of image generation and beyond. The codes and datasets used for evaluating GPT-4o can be found at https://github.com/PicoTrex/GPT-ImgEval.","authors":["Zhiyuan Yan","Junyan Ye","Weijia Li","Zilong Huang","Shenghai Yuan","Xiangyang He","Kaiqing Lin","Jun He","Conghui He","Li Yuan"],"url":"https://arxiv.org/abs/2504.02782"}
{"created":"2025-05-02","title":"A metrological framework for uncertainty evaluation in machine learning classification models","abstract":"Machine learning (ML) classification models are increasingly being used in a wide range of applications where it is important that predictions are accompanied by uncertainties, including in climate and earth observation, medical diagnosis and bioaerosol monitoring. The output of an ML classification model is a type of categorical variable known as a nominal property in the International Vocabulary of Metrology (VIM). However, concepts related to uncertainty evaluation for nominal properties are not defined in the VIM, nor is such evaluation addressed by the Guide to the Expression of Uncertainty in Measurement (GUM). In this paper we propose a metrological conceptual uncertainty evaluation framework for ML classification, and illustrate its use in the context of two applications that exemplify the issues and have significant societal impact, namely, climate and earth observation and medical diagnosis. Our framework would enable an extension of the VIM and GUM to uncertainty for nominal properties, which would make both applicable to ML classification models.","authors":["Samuel Bilson","Maurice Cox","Anna Pustogvar","Andrew Thompson"],"url":"https://arxiv.org/abs/2504.03359"}
{"created":"2025-05-02","title":"Variational Self-Supervised Learning","abstract":"We present Variational Self-Supervised Learning (VSSL), a novel framework that combines variational inference with self-supervised learning to enable efficient, decoder-free representation learning. Unlike traditional VAEs that rely on input reconstruction via a decoder, VSSL symmetrically couples two encoders with Gaussian outputs. A momentum-updated teacher network defines a dynamic, data-dependent prior, while the student encoder produces an approximate posterior from augmented views. The reconstruction term in the ELBO is replaced with a cross-view denoising objective, preserving the analytical tractability of Gaussian KL divergence. We further introduce cosine-based formulations of KL and log-likelihood terms to enhance semantic alignment in high-dimensional latent spaces. Experiments on CIFAR-10, CIFAR-100, and ImageNet-100 show that VSSL achieves competitive or superior performance to leading self-supervised methods, including BYOL and MoCo V3. VSSL offers a scalable, probabilistically grounded approach to learning transferable representations without generative reconstruction, bridging the gap between variational modeling and modern self-supervised techniques.","authors":["Mehmet Can Yavuz","Berrin Yanikoglu"],"url":"https://arxiv.org/abs/2504.04318"}
{"created":"2025-05-02","title":"Towards Source Mapping for Zero-Knowledge Smart Contracts: Design and Preliminary Evaluation","abstract":"Debugging and auditing zero-knowledge-compatible smart contracts remains a significant challenge due to the lack of source mapping in compilers such as zkSolc. In this work, we present a preliminary source mapping framework that establishes traceability between Solidity source code, LLVM IR, and zkEVM bytecode within the zkSolc compilation pipeline. Our approach addresses the traceability challenges introduced by non-linear transformations and proof-friendly optimizations in zero-knowledge compilation. To improve the reliability of mappings, we incorporate lightweight consistency checks based on static analysis and structural validation. We evaluate the framework on a dataset of 50 benchmark contracts and 500 real-world zkSync contracts, observing a mapping accuracy of approximately 97.2% for standard Solidity constructs. Expected limitations arise in complex scenarios such as inline assembly and deep inheritance hierarchies. The measured compilation overhead remains modest, at approximately 8.6%. Our initial results suggest that source mapping support in zero-knowledge compilation pipelines is feasible and can benefit debugging, auditing, and development workflows. We hope that this work serves as a foundation for further research and tool development aimed at improving developer experience in zk-Rollup environments.","authors":["Pei Xu","Yulei Sui","Mark Staples"],"url":"https://arxiv.org/abs/2504.04322"}
{"created":"2025-05-02","title":"TabRep: Training Tabular Diffusion Models with a Simple and Effective Continuous Representation","abstract":"Diffusion models have been the predominant generative model for tabular data generation. However, they face the conundrum of modeling under a separate versus a unified data representation. The former encounters the challenge of jointly modeling all multi-modal distributions of tabular data in one model. While the latter alleviates this by learning a single representation for all features, it currently leverages sparse suboptimal encoding heuristics and necessitates additional computation costs. In this work, we address the latter by presenting TabRep, a tabular diffusion architecture trained with a unified continuous representation. To motivate the design of our representation, we provide geometric insights into how the data manifold affects diffusion models. The key attributes of our representation are composed of its density, flexibility to provide ample separability for nominal features, and ability to preserve intrinsic relationships. Ultimately, TabRep provides a simple yet effective approach for training tabular diffusion models under a continuous data manifold. Our results showcase that TabRep achieves superior performance across a broad suite of evaluations. It is the first to synthesize tabular data that exceeds the downstream quality of the original datasets while preserving privacy and remaining computationally efficient.","authors":["Jacob Si","Zijing Ou","Mike Qu","Zhengrui Xiang","Yingzhen Li"],"url":"https://arxiv.org/abs/2504.04798"}
{"created":"2025-05-02","title":"Gaussian Mixture Flow Matching Models","abstract":"Diffusion models approximate the denoising distribution as a Gaussian and predict its mean, whereas flow matching models reparameterize the Gaussian mean as flow velocity. However, they underperform in few-step sampling due to discretization error and tend to produce over-saturated colors under classifier-free guidance (CFG). To address these limitations, we propose a novel Gaussian mixture flow matching (GMFlow) model: instead of predicting the mean, GMFlow predicts dynamic Gaussian mixture (GM) parameters to capture a multi-modal flow velocity distribution, which can be learned with a KL divergence loss. We demonstrate that GMFlow generalizes previous diffusion and flow matching models where a single Gaussian is learned with an $L_2$ denoising loss. For inference, we derive GM-SDE/ODE solvers that leverage analytic denoising distributions and velocity fields for precise few-step sampling. Furthermore, we introduce a novel probabilistic guidance scheme that mitigates the over-saturation issues of CFG and improves image generation quality. Extensive experiments demonstrate that GMFlow consistently outperforms flow matching baselines in generation quality, achieving a Precision of 0.942 with only 6 sampling steps on ImageNet 256$\\times$256.","authors":["Hansheng Chen","Kai Zhang","Hao Tan","Zexiang Xu","Fujun Luan","Leonidas Guibas","Gordon Wetzstein","Sai Bi"],"url":"https://arxiv.org/abs/2504.05304"}
{"created":"2025-05-02","title":"Efficient Reinforcement Finetuning via Adaptive Curriculum Learning","abstract":"Reinforcement finetuning (RFT) has shown great potential for enhancing the mathematical reasoning capabilities of large language models (LLMs), but it is often sample- and compute-inefficient, requiring extensive training. In this work, we introduce AdaRFT (Adaptive Curriculum Reinforcement Finetuning), a method that significantly improves both the efficiency and final accuracy of RFT through adaptive curriculum learning. AdaRFT dynamically adjusts the difficulty of training problems based on the model's recent reward signals, ensuring that the model consistently trains on tasks that are challenging but solvable. This adaptive sampling strategy accelerates learning by maintaining an optimal difficulty range, avoiding wasted computation on problems that are too easy or too hard. AdaRFT requires only a lightweight extension to standard RFT algorithms like Proximal Policy Optimization (PPO), without modifying the reward function or model architecture. Experiments on competition-level math datasets-including AMC, AIME, and IMO-style problems-demonstrate that AdaRFT significantly improves both training efficiency and reasoning performance. We evaluate AdaRFT across multiple data distributions and model sizes, showing that it reduces training time by up to 2x and improves accuracy by a considerable margin, offering a more scalable and effective RFT framework.","authors":["Taiwei Shi","Yiyang Wu","Linxin Song","Tianyi Zhou","Jieyu Zhao"],"url":"https://arxiv.org/abs/2504.05520"}
{"created":"2025-05-02","title":"\"Sorry for bugging you so much.\" Exploring Developers' Behavior Towards Privacy-Compliant Implementation","abstract":"While protecting user data is essential, software developers often fail to fulfill privacy requirements. However, the reasons why they struggle with privacy-compliant implementation remain unclear. Is it due to a lack of knowledge, or is it because of insufficient support? To provide foundational insights in this field, we conducted a qualitative 5-hour programming study with 30 professional software developers implementing 3 privacy-sensitive programming tasks that were designed with GDPR compliance in mind. To explore if and how developers implement privacy requirements, participants were divided into 3 groups: control, privacy prompted, and privacy expert-supported. After task completion, we conducted follow-up interviews. Alarmingly, almost all participants submitted non-GDPR-compliant solutions (79/90). In particular, none of the 3 tasks were solved privacy-compliant by all 30 participants, with the non-prompted group having the lowest number of 3 out of 30 privacy-compliant solution attempts. Privacy prompting and expert support only slightly improved participants' submissions, with 6/30 and 8/30 privacy-compliant attempts, respectively. In fact, all participants reported severe issues addressing common privacy requirements such as purpose limitation, user consent, or data minimization. Counterintuitively, although most developers exhibited minimal confidence in their solutions, they rarely sought online assistance or contacted the privacy expert, with only 4 out of 10 expert-supported participants explicitly asking for compliance confirmation. Instead, participants often relied on existing implementations and focused on implementing functionality and security first.","authors":["Stefan Albert Horstmann","Sandy Hong","David Klein","Raphael Serafini","Martin Degeling","Martin Johns","Veelasha Moonsamy","Alena Naiakshina"],"url":"https://arxiv.org/abs/2504.06697"}
{"created":"2025-05-02","title":"Bridging Deep Reinforcement Learning and Motion Planning for Model-Free Navigation in Cluttered Environments","abstract":"Deep Reinforcement Learning (DRL) has emerged as a powerful model-free paradigm for learning optimal policies. However, in navigation tasks with cluttered environments, DRL methods often suffer from insufficient exploration, especially under sparse rewards or complex dynamics with system disturbances. To address this challenge, we bridge general graph-based motion planning with DRL, enabling agents to explore cluttered spaces more effectively and achieve desired navigation performance. Specifically, we design a dense reward function grounded in a graph structure that spans the entire state space. This graph provides rich guidance, steering the agent toward optimal strategies. We validate our approach in challenging environments, demonstrating substantial improvements in exploration efficiency and task success rates.","authors":["Licheng Luo","Mingyu Cai"],"url":"https://arxiv.org/abs/2504.07283"}
{"created":"2025-05-02","title":"Probability Estimation and Scheduling Optimization for Battery Swap Stations via LRU-Enhanced Genetic Algorithm and Dual-Factor Decision System","abstract":"To address the challenges of limited Battery Swap Stations datasets, high operational costs, and fluctuating user charging demand, this research proposes a probability estimation model based on charging pile data and constructs nine scenario-specific battery swap demand datasets. In addition, this study combines Least Recently Used strategy with Genetic Algorithm and incorporates a guided search mechanism, which effectively enhances the global optimization capability. Thus, a dual-factor decision-making based charging schedule optimization system is constructed. Experimental results show that the constructed datasets exhibit stable trend characteristics, adhering to 24-hour and 168-hour periodicity patterns, with outlier ratios consistently below 3.26%, confirming data validity. Compared to baseline, the improved algorithm achieves better fitness individuals in 80% of test regions under the same iterations. When benchmarked against immediate swap-and-charge strategy, our algorithm achieves a peak cost reduction of 13.96%. Moreover, peak user satisfaction reaches 98.57%, while the average iteration time remains below 0.6 seconds, demonstrating good computational efficiency. The complete datasets and optimization algorithm are open-sourced at https://github.com/qingshufan/GA-EVLRU.","authors":["Anzhen Li","Shufan Qing","Xiaochang Li","Rui Mao","Mingchen Feng"],"url":"https://arxiv.org/abs/2504.07453"}
{"created":"2025-05-02","title":"Pychop: Emulating Low-Precision Arithmetic in Numerical Methods and Neural Networks","abstract":"Motivated by the growing demand for low-precision arithmetic in computational science, we exploit lower-precision emulation in Python -- widely regarded as the dominant programming language for numerical analysis and machine learning. Low-precision training has revolutionized deep learning by enabling more efficient computation and reduced memory and energy consumption while maintaining model fidelity. To better enable numerical experimentation with and exploration of low precision computation, we developed the Pychop library, which supports customizable floating-point formats and a comprehensive set of rounding modes in Python, allowing users to benefit from fast, low-precision emulation in numerous applications. Pychop also introduces interfaces for both PyTorch and JAX, enabling efficient low-precision emulation on GPUs for neural network training and inference with unparalleled flexibility.","authors":["Erin Carson","Xinye Chen"],"url":"https://arxiv.org/abs/2504.07835"}
{"created":"2025-05-02","title":"Hybrid discontinuous Galerkin discretizations for the damped time-harmonic Galbrun's equation","abstract":"In this article, we study the damped time-harmonic Galbrun's equation which models solar and stellar oscillations. We introduce and analyze hybrid discontinuous Galerkin discretizations (HDG) that are stable and optimally convergent for all polynomial degrees greater than or equal to one. The proposed methods are robust with respect to the drastic changes in the magnitude of the coefficients that naturally occur in stars. Our analysis is based on the concept of discrete approximation schemes and weak T-compatibility, which exploits the weakly T-coercive structure of the equation. Compared to the $H^1$-conforming discretization of [Halla, Lehrenfeld, Stocker, 2022], our method offers improved stability and robustness. Furthermore, it significantly reduces the computational costs compared to the $H(\\operatorname{div})$-conforming DG discretization of [Halla, 2023], which has similar stability properties. These advantages make the proposed HDG methods well-suited for astrophysical simulations.","authors":["Martin Halla","Christoph Lehrenfeld","Tim van Beeck"],"url":"https://arxiv.org/abs/2504.09547"}
{"created":"2025-05-02","title":"WG-IDENT: Weak Group Identification of PDEs with Varying Coefficients","abstract":"Partial Differential Equations (PDEs) identification is a data-driven method for mathematical modeling, and has received a lot of attentions recently. The stability and precision in identifying PDE from heavily noisy spatiotemporal data present significant difficulties. This problem becomes even more complex when the coefficients of the PDEs are subject to spatial variation. In this paper, we propose a Weak formulation of Group-sparsity-based framework for IDENTifying PDEs with varying coefficients, called WG-IDENT, to tackle this challenge. Our approach utilizes the weak formulation of PDEs to reduce the impact of noise. We represent test functions and unknown PDE coefficients using B-splines, where the knot vectors of test functions are optimally selected based on spectral analysis of the noisy data. To facilitate feature selection, we propose to integrate group sparse regression with a newly designed group feature trimming technique, called GF-trim, to eliminate unimportant features. Extensive and comparative ablation studies are conducted to validate our proposed method. The proposed method not only demonstrates greater robustness to high noise levels compared to state-of-the-art algorithms but also achieves superior performance while exhibiting reduced sensitivity to hyperparameter selection.","authors":["Cheng Tang","Roy Y. He","Hao Liu"],"url":"https://arxiv.org/abs/2504.10212"}
{"created":"2025-05-02","title":"Kernel Ridge Regression for Efficient Learning of High-Capacity Hopfield Networks","abstract":"Hopfield networks using Hebbian learning suffer from limited storage capacity. While supervised methods like Linear Logistic Regression (LLR) offer some improvement, kernel methods like Kernel Logistic Regression (KLR) significantly enhance capacity and noise robustness. However, KLR requires computationally expensive iterative learning. We propose Kernel Ridge Regression (KRR) as an efficient kernel-based alternative for learning high-capacity Hopfield networks. KRR utilizes the kernel trick and predicts bipolar states via regression, crucially offering a non-iterative, closed-form solution for learning dual variables. We evaluate KRR and compare its performance against Hebbian, LLR, and KLR. Our results demonstrate that KRR achieves state-of-the-art storage capacity (reaching $\\beta$=1.5) and noise robustness, comparable to KLR. Crucially, KRR drastically reduces training time, being orders of magnitude faster than LLR and significantly faster than KLR, especially at higher storage loads. This establishes KRR as a potent and highly efficient method for building high-performance associative memories, providing comparable performance to KLR with substantial training speed advantages. This work provides the first empirical comparison between KRR and KLR in the context of Hopfield network learning.","authors":["Akira Tamamori"],"url":"https://arxiv.org/abs/2504.12561"}
{"created":"2025-05-02","title":"Meta-rater: A Multi-dimensional Data Selection Method for Pre-training Language Models","abstract":"The composition of pre-training datasets for large language models (LLMs) remains largely undisclosed, hindering transparency and efforts to optimize data quality, a critical driver of model performance. Current data selection methods, such as natural language quality assessments, diversity-based filters, and classifier-based approaches, are limited by single-dimensional evaluation or redundancy-focused strategies. To address these gaps, we propose PRRC to evaluate data quality across Professionalism, Readability, Reasoning, and Cleanliness. We further introduce Meta-rater, a multi-dimensional data selection method that integrates these dimensions with existing quality metrics through learned optimal weightings. Meta-rater employs proxy models to train a regression model that predicts validation loss, enabling the identification of optimal combinations of quality scores. Experiments demonstrate that Meta-rater doubles convergence speed for 1.3B parameter models and improves downstream task performance by 3.23, with scalable benefits observed in 3.3B models trained on 100B tokens. Additionally, we release the annotated SlimPajama-627B dataset, labeled across 25 quality metrics (including PRRC), to advance research in data-centric LLM development. Our work establishes that holistic, multi-dimensional quality integration significantly outperforms conventional single-dimension approaches, offering a scalable paradigm for enhancing pre-training efficiency and model capability.","authors":["Xinlin Zhuang","Jiahui Peng","Ren Ma","Yinfan Wang","Tianyi Bai","Xingjian Wei","Jiantao Qiu","Chi Zhang","Ying Qian","Conghui He"],"url":"https://arxiv.org/abs/2504.14194"}
{"created":"2025-05-02","title":"LGD: Leveraging Generative Descriptions for Zero-Shot Referring Image Segmentation","abstract":"Zero-shot referring image segmentation aims to locate and segment the target region based on a referring expression, with the primary challenge of aligning and matching semantics across visual and textual modalities without training. Previous works address this challenge by utilizing Vision-Language Models and mask proposal networks for region-text matching. However, this paradigm may lead to incorrect target localization due to the inherent ambiguity and diversity of free-form referring expressions. To alleviate this issue, we present LGD (Leveraging Generative Descriptions), a framework that utilizes the advanced language generation capabilities of Multi-Modal Large Language Models to enhance region-text matching performance in Vision-Language Models. Specifically, we first design two kinds of prompts, the attribute prompt and the surrounding prompt, to guide the Multi-Modal Large Language Models in generating descriptions related to the crucial attributes of the referent object and the details of surrounding objects, referred to as attribute description and surrounding description, respectively. Secondly, three visual-text matching scores are introduced to evaluate the similarity between instance-level visual features and textual features, which determines the mask most associated with the referring expression. The proposed method achieves new state-of-the-art performance on three public datasets RefCOCO, RefCOCO+ and RefCOCOg, with maximum improvements of 9.97% in oIoU and 11.29% in mIoU compared to previous methods.","authors":["Jiachen Li","Qing Xie","Renshu Gu","Jinyu Xu","Yongjian Liu","Xiaohan Yu"],"url":"https://arxiv.org/abs/2504.14467"}
{"created":"2025-05-02","title":"ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model","abstract":"Large Language Models (LLMs) have advanced Verilog code generation significantly, yet face challenges in data quality, reasoning capabilities, and computational efficiency. This paper presents ReasoningV, a novel model employing a hybrid reasoning strategy that integrates trained intrinsic capabilities with dynamic inference adaptation for Verilog code generation. Our framework introduces three complementary innovations: (1) ReasoningV-5K, a high-quality dataset of 5,000 functionally verified instances with reasoning paths created through multi-dimensional filtering of PyraNet samples; (2) a two-stage training approach combining parameter-efficient fine-tuning for foundational knowledge with full-parameter optimization for enhanced reasoning; and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning depth based on problem complexity, reducing token consumption by up to 75\\% while preserving performance. Experimental results demonstrate ReasoningV's effectiveness with a pass@1 accuracy of 57.8\\% on VerilogEval-human, achieving performance competitive with leading commercial models like Gemini-2.0-flash (59.5\\%) and exceeding the previous best open-source model by 10.4 percentage points. ReasoningV offers a more reliable and accessible pathway for advancing AI-driven hardware design automation, with our model, data, and code available at https://github.com/BUAA-CLab/ReasoningV.","authors":["Haiyan Qin","Zhiwei Xie","Jingjing Li","Liangchen Li","Xiaotong Feng","Junzhan Liu","Wang Kang"],"url":"https://arxiv.org/abs/2504.14560"}
{"created":"2025-05-02","title":"Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence","abstract":"Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38\\%--1075\\% higher than human designs. We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency). To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem -- a competitive circuit design platform with hundreds of thousands of players. Experiments show CircuitMind enables 55.6\\% of model implementations to match or exceed top-tier human experts in composite efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency comparable to the top 25\\% of human experts without requiring specialized training. These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs. Our model, data, and code are open-source at https://github.com/BUAA-CLab/CircuitMind.","authors":["Haiyan Qin","Jiahao Feng","Xiaotong Feng","Wei W. Xing","Wang Kang"],"url":"https://arxiv.org/abs/2504.14625"}
{"created":"2025-05-02","title":"SQL-Factory: A Multi-Agent Framework for High-Quality and Large-Scale SQL Generation","abstract":"High quality SQL corpus is essential for intelligent database. For example, Text-to-SQL requires SQL queries and correspond natural language questions as training samples. However, collecting such query corpus remains challenging in practice due to the high cost of manual annotation, which highlights the importance of automatic SQL generation. Despite recent advances, existing generation methods still face limitations in achieving both diversity and cost-effectiveness. Besides, many methods also treat all tables equally, which overlooks schema complexity and leads to under-utilization of structurally rich tables. To address these issues, this paper proposes a multi-agent framework for high-quality and large-scale SQL generation, dubbed SQL-Factory. It decomposes the generation process into three collaborative teams: the Generation Team explores diverse query structures using a powerful language model, the Expansion Team scales promising patterns via a lightweight language model, and the Management Team adaptively schedules the workflow and evaluates the quality of synthesized queries. This modular framework ensures a balanced trade-off between diversity, scalability, and generation cost. We apply SQL-Factory to four widely used benchmarks and generate over 300,000 SQL queries with less than $200 API cost. Our generated queries achieve higher diversity compared to other methods, and extensive experiments demonstrate that the generated queries significantly improve the model performance in various downstream tasks.","authors":["Jiahui Li","Tongwang Wu","Yuren Mao","Yunjun Gao","Yajie Feng","Huaizhong Liu"],"url":"https://arxiv.org/abs/2504.14837"}
{"created":"2025-05-02","title":"A Framework for Testing and Adapting REST APIs as LLM Tools","abstract":"Large Language Models (LLMs) are enabling autonomous agents to perform complex workflows using external tools or functions, often provided via REST APIs in enterprise systems. However, directly utilizing these APIs as tools poses challenges due to their complex input schemas, elaborate responses, and often ambiguous documentation. Current benchmarks for tool testing do not adequately address these complexities, leading to a critical gap in evaluating API readiness for agent-driven automation. In this work, we present a novel testing framework aimed at evaluating and enhancing the readiness of REST APIs to function as tools for LLM-based agents. Our framework transforms apis as tools, generates comprehensive test cases for the APIs, translates tests cases into natural language instructions suitable for agents, enriches tool definitions and evaluates the agent's ability t correctly invoke the API and process its inputs and responses. To provide actionable insights, we analyze the outcomes of 750 test cases, presenting a detailed taxonomy of errors, including input misinterpretation, output handling inconsistencies, and schema mismatches. Additionally, we classify these test cases to streamline debugging and refinement of tool integrations. This work offers a foundational step toward enabling enterprise APIs as tools, improving their usability in agent-based applications.","authors":["Jayachandu Bandlamudi","Ritwik Chaudhuri","Neelamadhav Gantayat","Kushal Mukherjee","Prerna Agarwal","Renuka Sindhgatta","Sameep Mehta"],"url":"https://arxiv.org/abs/2504.15546"}
{"created":"2025-05-02","title":"Vision-Language Models Are Not Pragmatically Competent in Referring Expression Generation","abstract":"Referring Expression Generation (REG) is a core task for evaluating the pragmatic competence of vision-language systems, requiring not only accurate semantic grounding but also adherence to principles of cooperative communication (Grice, 1975). However, current evaluations of vision-language models (VLMs) often overlook the pragmatic dimension, reducing REG to a region-based captioning task and neglecting Gricean maxims. In this work, we revisit REG from a pragmatic perspective, introducing a new dataset (RefOI) of 1.5k images annotated with both written and spoken referring expressions. Through a systematic evaluation of state-of-the-art VLMs, we identify three key failures of pragmatic competence: (1) failure to uniquely identify the referent, (2) inclusion of excessive or irrelevant information, and (3) misalignment with human pragmatic preference, such as the underuse of minimal spatial cues. We also show that standard automatic evaluations fail to capture these pragmatic violations, reinforcing superficial cues rather than genuine referential success. Our findings call for a renewed focus on pragmatically informed models and evaluation frameworks that align with real human communication.","authors":["Ziqiao Ma","Jing Ding","Xuejun Zhang","Dezhi Luo","Jiahe Ding","Sihan Xu","Yuchen Huang","Run Peng","Joyce Chai"],"url":"https://arxiv.org/abs/2504.16060"}
{"created":"2025-05-02","title":"Enhancing the Security of Semantic Communication via Knowledge-Aided Coding and Jamming","abstract":"As semantic communication (SemCom) emerges as a promising communication paradigm, ensuring the security of semantic information over open wireless channels has become crucial. Traditional encryption methods introduce considerable communication overhead, while existing learning-based secure SemCom schemes often rely on a channel capacity advantage for the legitimate receiver, which is challenging to guarantee in practice. In this paper, we propose a coding-enhanced jamming approach that eliminates the need to transmit a secret key by utilizing shared knowledge between the legitimate receiver and the transmitter. We generate private codebooks with neural network (NN)-based encoders, using them to encode data into a sequence Y1, which is then superposed with a sequence Y2 drawn from the private codebook. By optimizing the power allocation between the two sequences, the legitimate receiver can successfully decode the data, while the eavesdropper' s performance is significantly degraded, potentially to the point of random guessing. Experimental results demonstrate that our method achieves comparable security to state-of-the-art approaches while significantly improving the reconstruction performance of the legitimate receiver by more than 1 dB across varying channel signal-to-noise ratios (SNRs) and compression ratios.","authors":["Weixuan Chen (Sherman)","Qianqian Yang (Sherman)","Shuo Shao (Sherman)","Zhiguo Shi (Sherman)","Jiming Chen (Sherman)","Xuemin (Sherman)","Shen"],"url":"https://arxiv.org/abs/2504.16960"}
{"created":"2025-05-02","title":"Whence Is A Model Fair? Fixing Fairness Bugs via Propensity Score Matching","abstract":"Fairness-aware learning aims to mitigate discrimination against specific protected social groups (e.g., those categorized by gender, ethnicity, age) while minimizing predictive performance loss. Despite efforts to improve fairness in machine learning, prior studies have shown that many models remain unfair when measured against various fairness metrics. In this paper, we examine whether the way training and testing data are sampled affects the reliability of reported fairness metrics. Since training and test sets are often randomly sampled from the same population, bias present in the training data may still exist in the test data, potentially skewing fairness assessments. To address this, we propose FairMatch, a post-processing method that applies propensity score matching to evaluate and mitigate bias. FairMatch identifies control and treatment pairs with similar propensity scores in the test set and adjusts decision thresholds for different subgroups accordingly. For samples that cannot be matched, we perform probabilistic calibration using fairness-aware loss functions. Experimental results demonstrate that our approach can (a) precisely locate subsets of the test data where the model is unbiased, and (b) significantly reduce bias on the remaining data. Overall, propensity score matching offers a principled way to improve both fairness evaluation and mitigation, without sacrificing predictive performance.","authors":["Kewen Peng","Yicheng Yang","Hao Zhuo"],"url":"https://arxiv.org/abs/2504.17066"}
{"created":"2025-05-02","title":"OmniSage: Large Scale, Multi-Entity Heterogeneous Graph Representation Learning","abstract":"Representation learning, a task of learning latent vectors to represent entities, is a key task in improving search and recommender systems in web applications. Various representation learning methods have been developed, including graph-based approaches for relationships among entities, sequence-based methods for capturing the temporal evolution of user activities, and content-based models for leveraging text and visual content. However, the development of a unifying framework that integrates these diverse techniques to support multiple applications remains a significant challenge. This paper presents OmniSage, a large-scale representation framework that learns universal representations for a variety of applications at Pinterest. OmniSage integrates graph neural networks with content-based models and user sequence models by employing multiple contrastive learning tasks to effectively process graph data, user sequence data, and content signals. To support the training and inference of OmniSage, we developed an efficient infrastructure capable of supporting Pinterest graphs with billions of nodes. The universal representations generated by OmniSage have significantly enhanced user experiences on Pinterest, leading to an approximate 2.5% increase in sitewide repins (saves) across five applications. This paper highlights the impact of unifying representation learning methods, and we will open source the OmniSage code by the time of publication.","authors":["Anirudhan Badrinath","Alex Yang","Kousik Rajesh","Prabhat Agarwal","Jaewon Yang","Haoyu Chen","Jiajing Xu","Charles Rosenberg"],"url":"https://arxiv.org/abs/2504.17811"}
{"created":"2025-05-02","title":"Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional","abstract":"Transition path sampling (TPS), which involves finding probable paths connecting two points on an energy landscape, remains a challenge due to the complexity of real-world atomistic systems. Current machine learning approaches use expensive, task-specific, and data-free training procedures, limiting their ability to benefit from recent advances in atomistic machine learning, such as high-quality datasets and large-scale pre-trained models. In this work, we address TPS by interpreting candidate paths as trajectories sampled from stochastic dynamics induced by the learned score function of pre-trained generative models, specifically denoising diffusion and flow matching. Under these dynamics, finding high-likelihood transition paths becomes equivalent to minimizing the Onsager-Machlup (OM) action functional. This enables us to repurpose pre-trained generative models for TPS in a zero-shot manner, in contrast with bespoke, task-specific TPS models trained in previous work. We demonstrate our approach on varied molecular systems, obtaining diverse, physically realistic transition pathways and generalizing beyond the pre-trained model's original training dataset. Our method can be easily incorporated into new generative models, making it practically relevant as models continue to scale and improve with increased data availability.","authors":["Sanjeev Raja","Martin \\v{S}\\'ipka","Michael Psenka","Tobias Kreiman","Michal Pavelka","Aditi S. Krishnapriyan"],"url":"https://arxiv.org/abs/2504.18506"}
{"created":"2025-05-02","title":"Practical Type-Based Taint Checking and Inference (Extended Version)","abstract":"Many important security properties can be formulated in terms of flows of tainted data, and improved taint analysis tools to prevent such flows are of critical need. Most existing taint analyses use whole-program static analysis, leading to scalability challenges. Type-based checking is a promising alternative, as it enables modular and incremental checking for fast performance. However, type-based approaches have not been widely adopted in practice, due to challenges with false positives and annotating existing codebases. In this paper, we present a new approach to type-based checking of taint properties that addresses these challenges, based on two key techniques. First, we present a new type-based tainting checker with significantly reduced false positives, via more practical handling of third-party libraries and other language constructs. Second, we present a novel technique to automatically infer tainting type qualifiers for existing code. Our technique supports inference of generic type argument annotations, crucial for tainting properties. We implemented our techniques in a tool TaintTyper and evaluated it on real-world benchmarks. TaintTyper exceeds the recall of a state-of-the-art whole-program taint analyzer, with comparable precision, and 2.93X-22.9X faster checking time. Further, TaintTyper infers annotations comparable to those written by hand, suitable for insertion into source code. TaintTyper is a promising new approach to efficient and practical taint checking.","authors":["Nima Karimipour","Kanak Das","Manu Sridharan","Behnaz Hassanshahi"],"url":"https://arxiv.org/abs/2504.18529"}
{"created":"2025-05-02","title":"WASP: Benchmarking Web Agent Security Against Prompt Injection Attacks","abstract":"Web navigation AI agents use language-and-vision foundation models to enhance productivity but these models are known to be susceptible to indirect prompt injections that get them to follow instructions different from the legitimate user's. Existing explorations of this threat applied to web agents often focus on a single isolated adversarial goal, test with injected instructions that are either too easy or not truly malicious, and often give the adversary unreasonable access. In order to better focus adversarial research, we construct a new benchmark called WASP (Web Agent Security against Prompt injection attacks) that introduces realistic web agent hijacking objectives and an isolated environment to test them in that does not affect real users or the live web. As part of WASP, we also develop baseline attacks against popular web agentic systems (VisualWebArena, Claude Computer Use, etc.) instantiated with various state-of-the-art models. Our evaluation shows that even AI agents backed by models with advanced reasoning capabilities and by models with instruction hierarchy mitigations are susceptible to low-effort human-written prompt injections. However, the realistic objectives in WASP also allow us to observe that agents are currently not capable enough to complete the goals of attackers end-to-end. Agents begin executing the adversarial instruction between 16 and 86% of the time but only achieve the goal between 0 and 17% of the time. Based on these findings, we argue that adversarial researchers should demonstrate stronger attacks that more consistently maintain control over the agent given realistic constraints on the adversary's power.","authors":["Ivan Evtimov","Arman Zharmagambetov","Aaron Grattafiori","Chuan Guo","Kamalika Chaudhuri"],"url":"https://arxiv.org/abs/2504.18575"}
{"created":"2025-05-02","title":"TransparentGS: Fast Inverse Rendering of Transparent Objects with Gaussians","abstract":"The emergence of neural and Gaussian-based radiance field methods has led to considerable advancements in novel view synthesis and 3D object reconstruction. Nonetheless, specular reflection and refraction continue to pose significant challenges due to the instability and incorrect overfitting of radiance fields to high-frequency light variations. Currently, even 3D Gaussian Splatting (3D-GS), as a powerful and efficient tool, falls short in recovering transparent objects with nearby contents due to the existence of apparent secondary ray effects. To address this issue, we propose TransparentGS, a fast inverse rendering pipeline for transparent objects based on 3D-GS. The main contributions are three-fold. Firstly, an efficient representation of transparent objects, transparent Gaussian primitives, is designed to enable specular refraction through a deferred refraction strategy. Secondly, we leverage Gaussian light field probes (GaussProbe) to encode both ambient light and nearby contents in a unified framework. Thirdly, a depth-based iterative probes query (IterQuery) algorithm is proposed to reduce the parallax errors in our probe-based framework. Experiments demonstrate the speed and accuracy of our approach in recovering transparent objects from complex environments, as well as several applications in computer graphics and vision.","authors":["Letian Huang","Dongwei Ye","Jialin Dan","Chengzhi Tao","Huiwen Liu","Kun Zhou","Bo Ren","Yuanqi Li","Yanwen Guo","Jie Guo"],"url":"https://arxiv.org/abs/2504.18768"}
{"created":"2025-05-02","title":"Advancing Face-to-Face Emotion Communication: A Multimodal Dataset (AFFEC)","abstract":"Emotion recognition has the potential to play a pivotal role in enhancing human-computer interaction by enabling systems to accurately interpret and respond to human affect. Yet, capturing emotions in face-to-face contexts remains challenging due to subtle nonverbal cues, variations in personal traits, and the real-time dynamics of genuine interactions. Existing emotion recognition datasets often rely on limited modalities or controlled conditions, thereby missing the richness and variability found in real-world scenarios.","authors":["Meisam J. Sekiavandi","Laurits Dixen","Jostein Fimland","Sree Keerthi Desu","Antonia-Bianca Zserai","Ye Sul Lee","Maria Barrett","Paolo Burelli"],"url":"https://arxiv.org/abs/2504.18969"}
{"created":"2025-05-02","title":"$PINN - a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks","abstract":"Physics-Informed Neural Networks (PINNs) are a novel computational approach for solving partial differential equations (PDEs) with noisy and sparse initial and boundary data. Although, efficient quantification of epistemic and aleatoric uncertainties in big multi-scale problems remains challenging. We propose \\$PINN a novel method of computing global uncertainty in PDEs using a Bayesian framework, by combining local Bayesian Physics-Informed Neural Networks (BPINN) with domain decomposition. The solution continuity across subdomains is obtained by imposing the flux continuity across the interface of neighboring subdomains. To demonstrate the effectiveness of \\$PINN, we conduct a series of computational experiments on PDEs in 1D and 2D spatial domains. Although we have adopted conservative PINNs (cPINNs), the method can be seamlessly extended to other domain decomposition techniques. The results infer that the proposed method recovers the global uncertainty by computing the local uncertainty exactly more efficiently as the uncertainty in each subdomain can be computed concurrently. The robustness of \\$PINN is verified by adding uncorrelated random noise to the training data up to 15% and testing for different domain sizes.","authors":["J\\'ulia Vicens Figueres","Juliette Vanderhaeghen","Federica Bragone","Kateryna Morozovska","Khemraj Shukla"],"url":"https://arxiv.org/abs/2504.19013"}
{"created":"2025-05-02","title":"CLR-Wire: Towards Continuous Latent Representations for 3D Curve Wireframe Generation","abstract":"We introduce CLR-Wire, a novel framework for 3D curve-based wireframe generation that integrates geometry and topology into a unified Continuous Latent Representation. Unlike conventional methods that decouple vertices, edges, and faces, CLR-Wire encodes curves as Neural Parametric Curves along with their topological connectivity into a continuous and fixed-length latent space using an attention-driven variational autoencoder (VAE). This unified approach facilitates joint learning and generation of both geometry and topology. To generate wireframes, we employ a flow matching model to progressively map Gaussian noise to these latents, which are subsequently decoded into complete 3D wireframes. Our method provides fine-grained modeling of complex shapes and irregular topologies, and supports both unconditional generation and generation conditioned on point cloud or image inputs. Experimental results demonstrate that, compared with state-of-the-art generative approaches, our method achieves substantial improvements in accuracy, novelty, and diversity, offering an efficient and comprehensive solution for CAD design, geometric reconstruction, and 3D content creation.","authors":["Xueqi Ma","Yilin Liu","Tianlong Gao","Qirui Huang","Hui Huang"],"url":"https://arxiv.org/abs/2504.19174"}
{"created":"2025-05-02","title":"BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese","abstract":"As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.","authors":["Peilin Zhou","Bruce Leon","Xiang Ying","Can Zhang","Yifan Shao","Qichen Ye","Dading Chong","Zhiling Jin","Chenxuan Xie","Meng Cao","Yuxin Gu","Sixin Hong","Jing Ren","Jian Chen","Chao Liu","Yining Hua"],"url":"https://arxiv.org/abs/2504.19314"}
{"created":"2025-05-02","title":"Follow Everything: A Leader-Following and Obstacle Avoidance Framework with Goal-Aware Adaptation","abstract":"Robust and flexible leader-following is a critical capability for robots to integrate into human society. While existing methods struggle to generalize to leaders of arbitrary form and often fail when the leader temporarily leaves the robot's field of view, this work introduces a unified framework addressing both challenges. First, traditional detection models are replaced with a segmentation model, allowing the leader to be anything. To enhance recognition robustness, a distance frame buffer is implemented that stores leader embeddings at multiple distances, accounting for the unique characteristics of leader-following tasks. Second, a goal-aware adaptation mechanism is designed to govern robot planning states based on the leader's visibility and motion, complemented by a graph-based planner that generates candidate trajectories for each state, ensuring efficient following with obstacle avoidance. Simulations and real-world experiments with a legged robot follower and various leaders (human, ground robot, UAV, legged robot, stop sign) in both indoor and outdoor environments show competitive improvements in follow success rate, reduced visual loss duration, lower collision rate, and decreased leader-follower distance.","authors":["Qianyi Zhang","Shijian Ma","Boyi Liu","Jingtai Liu","Jianhao Jiao","Dimitrios Kanoulas"],"url":"https://arxiv.org/abs/2504.19399"}
{"created":"2025-05-02","title":"BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text","abstract":"Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, current evaluations of LLMs in clinical contexts remain limited. Most existing benchmarks rely on medical exam-style questions or PubMed-derived text, failing to capture the complexity of real-world electronic health record (EHR) data. Others focus narrowly on specific application scenarios, limiting their generalizability across broader clinical use. To address this gap, we present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks sourced from real-world clinical data sources across nine languages. We systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1, GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total of 13,572 experiments, our results reveal substantial performance variation across model sizes, languages, natural language processing tasks, and clinical specialties. Notably, we demonstrate that open-source LLMs can achieve performance comparable to proprietary models, while medically fine-tuned LLMs based on older architectures often underperform versus updated general-purpose models. The BRIDGE and its corresponding leaderboard serve as a foundational resource and a unique reference for the development and evaluation of new LLMs in real-world clinical text understanding.","authors":["Jiageng Wu","Bowen Gu","Ren Zhou","Kevin Xie","Doug Snyder","Yixing Jiang","Valentina Carducci","Richard Wyss","Rishi J Desai","Emily Alsentzer","Leo Anthony Celi","Adam Rodman","Sebastian Schneeweiss","Jonathan H. Chen","Santiago Romero-Brufau","Kueiyu Joshua Lin","Jie Yang"],"url":"https://arxiv.org/abs/2504.19467"}
{"created":"2025-05-02","title":"Soft-Label Caching and Sharpening for Communication-Efficient Federated Distillation","abstract":"Federated Learning (FL) enables collaborative model training across decentralized clients, enhancing privacy by keeping data local. Yet conventional FL, relying on frequent parameter-sharing, suffers from high communication overhead and limited model heterogeneity. Distillation-based FL approaches address these issues by sharing predictions (soft-labels) instead, but they often involve redundant transmissions across communication rounds, reducing efficiency. We propose SCARLET, a novel framework integrating synchronized soft-label caching and an enhanced Entropy Reduction Aggregation (Enhanced ERA) mechanism. SCARLET minimizes redundant communication by reusing cached soft-labels, achieving up to 50% reduction in communication costs compared to existing methods while maintaining accuracy. Enhanced ERA can be tuned to adapt to non-IID data variations, ensuring robust aggregation and performance in diverse client scenarios. Experimental evaluations demonstrate that SCARLET consistently outperforms state-of-the-art distillation-based FL methods in terms of accuracy and communication efficiency. The implementation of SCARLET is publicly available at https://github.com/kitsuyaazuma/SCARLET.","authors":["Kitsuya Azuma","Takayuki Nishio","Yuichi Kitagawa","Wakako Nakano","Takahito Tanimura"],"url":"https://arxiv.org/abs/2504.19602"}
{"created":"2025-05-02","title":"Fitness Landscape of Large Language Model-Assisted Automated Algorithm Search","abstract":"Large Language Models (LLMs) have demonstrated significant potential in algorithm design. However, when integrated into search frameworks for iterative algorithm search, the underlying fitness landscape--critical for understanding search behaviou--remains underexplored. In this paper, we illustrate and analyze the fitness landscape of LLM-assisted Algorithm Search (LAS) using a graph-based approach, where nodes represent algorithms and edges denote transitions between them. We conduct extensive evaluations across six algorithm design tasks and six commonly used LLMs. Our findings reveal that LAS landscapes are highly multimodal and rugged, particularly in combinatorial optimization tasks, with distinct structural variations across tasks and LLMs. For instance, heuristic design tasks exhibit dense clusters of high-performing algorithms, while symbolic regression tasks show sparse, scattered distributions. Additionally, we demonstrate how population size influences exploration-exploitation trade-offs and the evolving trajectory of elite algorithms. These insights not only advance our understanding of LAS landscapes but also provide practical guidance for designing more effective LAS methods.","authors":["Fei Liu","Qingfu Zhang","Xialiang Tong","Kun Mao","Mingxuan Yuan"],"url":"https://arxiv.org/abs/2504.19636"}
{"created":"2025-05-02","title":"GenTorrent: Scaling Large Language Model Serving with An Overley Network","abstract":"While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.","authors":["Fei Fang","Yifan Hua","Shengze Wang","Ruilin Zhou","Yi Liu","Chen Qian","Xiaoxue Zhang"],"url":"https://arxiv.org/abs/2504.20101"}
{"created":"2025-05-02","title":"Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets","abstract":"Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.","authors":["Lorenz Brehme","Thomas Str\\\"ohle","Ruth Breu"],"url":"https://arxiv.org/abs/2504.20119"}
{"created":"2025-05-02","title":"PRISM-DP: Spatial Pose-based Observations for Diffusion-Policies via Segmentation, Mesh Generation, and Pose Tracking","abstract":"Diffusion-based visuomotor policies generate robot motions by learning to denoise action-space trajectories conditioned on observations. These observations are commonly streams of RGB images, whose high dimensionality includes substantial task-irrelevant information, requiring large models to extract relevant patterns. In contrast, using more structured observations, such as the spatial poses (positions and orientations) of key objects over time, enables training more compact policies that can recognize relevant patterns with fewer parameters. However, obtaining accurate object poses in open-set, real-world environments remains challenging. For instance, it is impractical to assume that all relevant objects are equipped with markers, and recent learning-based 6D pose estimation and tracking methods often depend on pre-scanned object meshes, requiring manual reconstruction. In this work, we propose PRISM-DP, an approach that leverages segmentation, mesh generation, pose estimation, and pose tracking models to enable compact diffusion policy learning directly from the spatial poses of task-relevant objects. Crucially, because PRISM-DP uses a mesh generation model, it eliminates the need for manual mesh processing or creation, improving scalability and usability in open-set, real-world environments. Experiments across a range of tasks in both simulation and real-world settings show that PRISM-DP outperforms high-dimensional image-based diffusion policies and achieves performance comparable to policies trained with ground-truth state information. We conclude with a discussion of the broader implications and limitations of our approach.","authors":["Xiatao Sun","Yinxing Chen","Daniel Rakita"],"url":"https://arxiv.org/abs/2504.20359"}
{"created":"2025-05-02","title":"GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting","abstract":"In this paper, we present a method for localizing a query image with respect to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the method uses 3DGS to render a synthetic RGBD image at some initial pose estimate. Second, it establishes 2D-2D correspondences between the query image and this synthetic image. Third, it uses the depth map to lift the 2D-2D correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP) problem to produce a final pose estimate. Results from evaluation across three existing datasets with 38 scenes and over 2,700 test images show that our method significantly reduces both inference time (by over two orders of magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation error compared to baseline methods that use photometric loss minimization. Results also show that our method tolerates large errors in the initial pose estimate of up to 55{\\deg} in rotation and 1.1 units in translation (normalized by scene scale), achieving final pose errors of less than 5{\\deg} in rotation and 0.05 units in translation on 90% of images from the Synthetic NeRF and Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and Temples dataset.","authors":["Jongwon Lee","Timothy Bretl"],"url":"https://arxiv.org/abs/2504.20379"}
{"created":"2025-05-02","title":"SNR-aware Semantic Image Transmission with Deep Learning-based Channel Estimation in Fading Channels","abstract":"Semantic communications (SCs) play a central role in shaping the future of the sixth generation (6G) wireless systems, which leverage rapid advances in deep learning (DL). In this regard, end-to-end optimized DL-based joint source-channel coding (JSCC) has been adopted to achieve SCs, particularly in image transmission. Utilizing vision transformers in the encoder/decoder design has enabled significant advancements in image semantic extraction, surpassing traditional convolutional neural networks (CNNs). In this paper, we propose a new JSCC paradigm for image transmission, namely Swin semantic image transmission (SwinSIT), based on the Swin transformer. The Swin transformer is employed to construct both the semantic encoder and decoder for efficient image semantic extraction and reconstruction. Inspired by the squeezing-and-excitation (SE) network, we introduce a signal-to-noise-ratio (SNR)-aware module that utilizes SNR feedback to adaptively perform a double-phase enhancement for the encoder-extracted semantic map and its noisy version at the decoder. Additionally, a CNN-based channel estimator and compensator (CEAC) module repurposes an image-denoising CNN to mitigate fading channel effects. To optimize deployment in resource-constrained IoT devices, a joint pruning and quantization scheme compresses the SwinSIT model. Simulations evaluate the SwinSIT performance against conventional benchmarks demonstrating its effectiveness. Moreover, the model's compressed version substantially reduces its size while maintaining favorable PSNR performance.","authors":["Mahmoud M. Salim","Mohamed S. Abdalzaher","Ali H. Muqaibel","Hussein A. Elsayed","Inkyu Lee"],"url":"https://arxiv.org/abs/2504.20557"}
{"created":"2025-05-02","title":"Natural Language Processing tools for Pharmaceutical Manufacturing Information Extraction from Patents","abstract":"Abundant and diverse data on medicines manufacturing and other lifecycle components has been made easily accessible in the last decades. However, a significant proportion of this information is characterised by not being tabulated and usable for machine learning purposes. Thus, natural language processing tools have been used to build databases in domains such as biomedical and chemical to address this limitation. This has allowed the development of artificial intelligence applications, which have improved drug discovery and treatments. In the pharmaceutical manufacturing context, some initiatives and datasets for primary processing can be found, but the manufacturing of drug products is an area which is still lacking, to the best of our knowledge. This works aims to explore and adapt NLP tools used in other domains to extract information on both primary and secondary manufacturing, employing patents as the main source of data. Thus, two independent, but complementary, models were developed comprising a method to select fragments of text that contain manufacturing data, and a named entity recognition system that enables extracting information on operations, materials, and conditions of a process. For the first model, the identification of relevant sections was achieved using an unsupervised approach combining Latent Dirichlet Allocation and k-Means clustering. The performance of this model measured as a Cohen's kappa between model output and manual revision was higher than 90%. NER model consisted of a deep neural network, and an f1-score micro average of 84.2% was obtained which is comparable to other works. Some considerations for these tools to be used in data extraction are discussed throughout this document.","authors":["Diego Alvarado-Maldonado","Blair Johnston","Cameron J. Brown"],"url":"https://arxiv.org/abs/2504.20598"}
{"created":"2025-05-02","title":"A Domain-Agnostic Scalable AI Safety Ensuring Framework","abstract":"Ensuring the safety of AI systems has recently emerged as a critical priority for real-world deployment, particularly in physical AI applications. Current approaches to AI safety typically address predefined domain-specific safety conditions, limiting their ability to generalize across contexts. We propose a novel AI safety framework that ensures AI systems comply with any user-defined constraint, with any desired probability, and across various domains. In this framework, we combine an AI component (e.g., neural network) with an optimization problem to produce responses that minimize objectives while satisfying user-defined constraints with probabilities exceeding user-defined thresholds. For credibility assessment of the AI component, we propose internal test data, a supplementary set of safety-labeled data, and a conservative testing methodology that provides statistical validity of using internal test data. We also present an approximation method of a loss function and how to compute its gradient for training. We mathematically prove that probabilistic constraint satisfaction is guaranteed under specific, mild conditions and prove a scaling law between safety and the number of internal test data. We demonstrate our framework's effectiveness through experiments in diverse domains: demand prediction for production decision, safe reinforcement learning within the SafetyGym simulator, and guarding AI chatbot outputs. Through these experiments, we demonstrate that our method guarantees safety for user-specified constraints, outperforms for up to several order of magnitudes existing methods in low safety threshold regions, and scales effectively with respect to the size of internal test data.","authors":["Beomjun Kim","Kangyeon Kim","Sunwoo Kim","Heejin Ahn"],"url":"https://arxiv.org/abs/2504.20924"}
{"created":"2025-05-02","title":"Trace-of-Thought Prompting: Investigating Prompt-Based Knowledge Distillation Through Question Decomposition","abstract":"Knowledge distillation allows smaller neural networks to emulate the performance of larger, teacher models with reduced computational demands. Traditional methods for Large Language Models (LLMs) often necessitate extensive fine-tuning, which limits their accessibility. To address this, we introduce Trace-of-Thought Prompting, a novel framework designed to distill critical reasoning capabilities from high-resource teacher models (over 8 billion parameters) to low-resource student models (up to 8 billion parameters). This approach leverages problem decomposition to enhance interpretability and facilitate human-in-the-loop interventions. Empirical evaluations on the GSM8K and MATH datasets show that student models achieve accuracy gains of up to 113% on GSM8K and 21% on MATH, with significant improvements particularly notable in smaller models like Llama 2 and Zephyr. Our results suggest a promising pathway for open-source, low-resource models to eventually serve both as both students and teachers, potentially reducing our reliance on high-resource, proprietary models.","authors":["Tyler McDonald","Ali Emami"],"url":"https://arxiv.org/abs/2504.20946"}
{"created":"2025-05-02","title":"The Development of Reflective Practice on a Work-Based Software Engineering Program: A Longitudinal Study","abstract":"This study examines the development of reflective practice among students on a four-year work-based Software Engineering program. Using two established models of reflection - Boud et al.'s Model of Reflective Process and Bain et al.'s 5R Framework for Reflection - we analyse a series of reflective assignments submitted by students over four years. Our longitudinal analysis reveals clear trends in how students' reflective abilities evolve over the course of the program. We find that more sophisticated forms of reflection, such as integration of knowledge, appropriation of skills, and reconstruction of practice, increase markedly in prevalence in later years. The complementary nature of workplace experience and university study is highlighted in students' reflections, demonstrating a key benefit of the work-based learning approach. By the final year, all students demonstrate the ability to reconstruct their experiences to inform future practice. Our findings provide insight into how reflective practice develops in Software Engineering education and suggest potential value in incorporating more structured reflection into traditional degree programs. The study also reveals instances of meta-reflection, where students reflect on the value of reflection itself, indicating a deep engagement with the reflective process. While acknowledging limitations, this work offers a unique longitudinal perspective on the development of reflective practice in work-based Software Engineering education.","authors":["Matthew Barr","Syed Waqar Nabi","Oana Andrei"],"url":"https://arxiv.org/abs/2504.20956"}
{"created":"2025-05-02","title":"TRIED: Truly Innovative and Effective AI Detection Benchmark, developed by WITNESS","abstract":"The proliferation of generative AI and deceptive synthetic media threatens the global information ecosystem, especially across the Global Majority. This report from WITNESS highlights the limitations of current AI detection tools, which often underperform in real-world scenarios due to challenges related to explainability, fairness, accessibility, and contextual relevance. In response, WITNESS introduces the Truly Innovative and Effective AI Detection (TRIED) Benchmark, a new framework for evaluating detection tools based on their real-world impact and capacity for innovation. Drawing on frontline experiences, deceptive AI cases, and global consultations, the report outlines how detection tools must evolve to become truly innovative and relevant by meeting diverse linguistic, cultural, and technological contexts. It offers practical guidance for developers, policy actors, and standards bodies to design accountable, transparent, and user-centered detection solutions, and incorporate sociotechnical considerations into future AI standards, procedures and evaluation frameworks. By adopting the TRIED Benchmark, stakeholders can drive innovation, safeguard public trust, strengthen AI literacy, and contribute to a more resilient global information credibility.","authors":["Shirin Anlen (WITNESS)","Zuzanna Wojciak (WITNESS)"],"url":"https://arxiv.org/abs/2504.21489"}
{"created":"2025-05-02","title":"Decomposition Problem in Process of Selective Identification and Localization of Voltage Fluctuations Sources in Power Grids","abstract":"Voltage fluctuations are common disturbances in power grids, therefore the effective and selective process of identification and localization of individual voltage fluctuations sources is necessary for the minimization of such disturbances. Selectivity in the process of identification and localization disturbing loads is possible by the use cascade of blocks: demodulation, decomposition and propagation assessment. The effectiveness of this approach is closely related to the used method of decomposition. The paper presents the problem of decomposition process for the selected method of selective identification and localization of voltage fluctuation sources, in which the algorithm of enhanced empirical wavelet transform (EEWT) is used as the decomposition method. The paper presents selected research results from the real power grid, for which the result of selected approach causes mistakes in the process of identification and localization of voltage fluctuations sources. The potential causes of such mistakes related to the decomposition process are discussed on the basis of obtained research results.","authors":["Piotr Kuwa{\\l}ek"],"url":"https://arxiv.org/abs/2202.05020"}
{"created":"2025-05-02","title":"Self-restricting Noise and Exponential Relative Entropy Decay Under Unital Quantum Markov Semigroups","abstract":"States of open quantum systems often decay continuously under environmental interactions. Quantum Markov semigroups model such processes in dissipative environments. It is known that finite-dimensional quantum Markov semigroups with GNS detailed balance universally obey complete modified logarithmic Sobolev inequalities (CMLSIs), yielding exponential decay of relative entropy to a subspace of fixed point states. We analyze continuous processes that combine dissipative with Hamiltonian time-evolution, precluding this notion of detailed balance. First, we find counterexamples to CMLSI-like decay for these processes and determine conditions under which it fails. In contrast, we prove that despite its absence at early times, exponential decay re-appears for unital, finite-dimensional quantum Markov semigroups at finite timescales. Finally, we show that when dissipation is much stronger than Hamiltonian time-evolution, the rate of eventual, exponential decay toward the semigroup's decoherence-free subspace is bounded inversely in the decay rate of the dissipative part alone. Dubbed self-restricting noise, this inverse relationship arises when strong damping suppresses effects that would otherwise spread noise beyond its initial subspace.","authors":["Nicholas LaRacuente"],"url":"https://arxiv.org/abs/2203.03745"}
{"created":"2025-05-02","title":"Accelerated First-Order Optimization under Nonlinear Constraints","abstract":"We exploit analogies between first-order algorithms for constrained optimization and non-smooth dynamical systems to design a new class of accelerated first-order algorithms for constrained optimization. Unlike Frank-Wolfe or projected gradients, these algorithms avoid optimization over the entire feasible set at each iteration. We prove convergence to stationary points even in a nonconvex setting and we derive accelerated rates for the convex setting both in continuous time, as well as in discrete time. An important property of these algorithms is that constraints are expressed in terms of velocities instead of positions, which naturally leads to sparse, local and convex approximations of the feasible set (even if the feasible set is nonconvex). Thus, the complexity tends to grow mildly in the number of decision variables and in the number of constraints, which makes the algorithms suitable for machine learning applications. We apply our algorithms to a compressed sensing and a sparse regression problem, showing that we can treat nonconvex $\\ell^p$ constraints ($p<1$) efficiently, while recovering state-of-the-art performance for $p=1$.","authors":["Michael Muehlebach","Michael I. Jordan"],"url":"https://arxiv.org/abs/2302.00316"}
{"created":"2025-05-02","title":"A Near-Optimal Single-Loop Stochastic Algorithm for Convex Finite-Sum Coupled Compositional Optimization","abstract":"This paper studies a class of convex Finite-sum Coupled Compositional Optimization (cFCCO) problems with applications including group distributionally robust optimization (GDRO) and learning with imbalanced data. To better address these problems, we introduce an efficient single-loop primal-dual block-coordinate stochastic algorithm called ALEXR. The algorithm employs block-coordinate stochastic mirror ascent with extrapolation for the dual variable and stochastic proximal gradient descent updates for the primal variable. We establish the convergence rates of ALEXR in both convex and strongly convex cases under smoothness and non-smoothness conditions of involved functions, which not only improve the best rates in previous works on smooth cFCCO problems but also expand the realm of cFCCO for solving more challenging non-smooth problems such as the dual form of GDRO. Finally, we derive lower complexity bounds, demonstrating the (near-)optimality of ALEXR within a broad class of stochastic algorithms for cFCCO. Experimental results on GDRO and partial Area Under the ROC Curve (pAUC) maximization demonstrate the promising performance of our algorithm.","authors":["Bokun Wang","Tianbao Yang"],"url":"https://arxiv.org/abs/2312.02277"}
{"created":"2025-05-02","title":"Folded Context Condensation in Path Integral Formalism for Infinite Context Transformers","abstract":"In this work, we present a generalized formulation of the Transformer algorithm by reinterpreting its core mechanisms within the framework of Path Integral formalism. In this perspective, the attention mechanism is recast as a process that integrates all possible transition paths leading to future token states, with temporal evolution governed by the Feed-Forward Network. By systematically mapping each component of the Transformer to its counterpart in the Path Integral formulation, we obtain a more compact and efficient representation, in which the contextual information of a sequence is condensed into memory-like segments. These segments are recurrently processed across Transformer layers, enabling more effective long-term information retention. We validate the effectiveness of this approach through the Passkey retrieval task and a summarization task, demonstrating that the proposed method preserves historical information while exhibiting memory usage that scales linearly with sequence length. This contrasts with the non-linear memory growth typically observed in standard attention mechanisms. We expect that this quantum-inspired generalization of the Transformer architecture will open new avenues for enhancing both the efficiency and expressiveness of future Transformer models.","authors":["Won-Gi Paeng","Daesuk Kwon","Kyungwon Jeong","Honggyo Suh"],"url":"https://arxiv.org/abs/2405.04620"}
{"created":"2025-05-02","title":"Orthogonal Causal Calibration","abstract":"Estimates of heterogeneous treatment effects such as conditional average treatment effects (CATEs) and conditional quantile treatment effects (CQTEs) play an important role in real-world decision making. Given this importance, one should ensure these estimates are calibrated. While there is a rich literature on calibrating estimators of non-causal parameters, very few methods have been derived for calibrating estimators of causal parameters, or more generally estimators of quantities involving nuisance parameters. In this work, we develop general algorithms for reducing the task of causal calibration to that of calibrating a standard (non-causal) predictive model.","authors":["Justin Whitehouse","Christopher Jung","Vasilis Syrgkanis","Bryan Wilder","Zhiwei Steven Wu"],"url":"https://arxiv.org/abs/2406.01933"}
{"created":"2025-05-02","title":"Sample-Efficient Quantum State Tomography for Structured Quantum States in One Dimension","abstract":"While quantum state tomography (QST) remains the gold standard for benchmarking and verifying quantum devices, it requires an exponentially large number of measurements and classical computational resources for generic quantum many-body systems, making it impractical even for intermediate-size quantum devices. Fortunately, many physical quantum states often exhibit certain low-dimensional structures that enable the development of efficient QST. A notable example is the class of states represented by matrix product operators (MPOs) with a finite matrix/bond dimension, which include most physical states in one dimension and where the number of independent parameters describing the states only grows linearly with the number of qubits. Whether a sample efficient quantum state tomography protocol, where the number of required state copies scales only linearly as the number of parameters describing the state, exists for a generic MPO state still remains an important open question.","authors":["Zhen Qin","Casey Jameson","Alireza Goldar","Michael B. Wakin","Zhexuan Gong","Zhihui Zhu"],"url":"https://arxiv.org/abs/2410.02583"}
{"created":"2025-05-02","title":"Integer linear programming for unsupervised training set selection in molecular machine learning","abstract":"Integer linear programming (ILP) is an elegant approach to solve linear optimization problems, naturally described using integer decision variables. Within the context of physics-inspired machine learning applied to chemistry, we demonstrate the relevance of an ILP formulation to select molecular training sets for predictions of size-extensive properties. We show that our algorithm outperforms existing unsupervised training set selection approaches, especially when predicting properties of molecules larger than those present in the training set. We argue that the reason for the improved performance is due to the selection that is based on the notion of local similarity (i.e., per-atom) and a unique ILP approach that finds optimal solutions efficiently. Altogether, this work provides a practical algorithm to improve the performance of physics-inspired machine learning models and offers insights into the conceptual differences with existing training set selection approaches.","authors":["Matthieu Haeberle","Puck van Gerwen","Ruben Laplaza","Ksenia R. Briling","Jan Weinreich","Friedrich Eisenbrand","Clemence Corminboeuf"],"url":"https://arxiv.org/abs/2410.16122"}
{"created":"2025-05-02","title":"Adversarial Data Poisoning Attacks on Quantum Machine Learning in the NISQ Era","abstract":"With the growing interest in Quantum Machine Learning (QML) and the increasing availability of quantum computers through cloud providers, addressing the potential security risks associated with QML has become an urgent priority. One key concern in the QML domain is the threat of data poisoning attacks in the current quantum cloud setting. Adversarial access to training data could severely compromise the integrity and availability of QML models. Classical data poisoning techniques require significant knowledge and training to generate poisoned data, and lack noise resilience, making them ineffective for QML models in the Noisy Intermediate Scale Quantum (NISQ) era. In this work, we first propose a simple yet effective technique to measure intra-class encoder state similarity (ESS) by analyzing the outputs of encoding circuits. Leveraging this approach, we introduce a \\underline{Qu}antum \\underline{I}ndiscriminate \\underline{D}ata Poisoning attack, QUID. Through extensive experiments conducted in both noiseless and noisy environments (e.g., IBM\\_Brisbane's noise), across various architectures and datasets, QUID achieves up to $92\\%$ accuracy degradation in model performance compared to baseline models and up to $75\\%$ accuracy degradation compared to random label-flipping. We also tested QUID against state-of-the-art classical defenses, with accuracy degradation still exceeding $50\\%$, demonstrating its effectiveness. This work represents the first attempt to reevaluate data poisoning attacks in the context of QML.","authors":["Satwik Kundu","Swaroop Ghosh"],"url":"https://arxiv.org/abs/2411.14412"}
{"created":"2025-05-02","title":"Segment-and-Classify: ROI-Guided Generalizable Contrast Phase Classification in CT Using XGBoost","abstract":"Purpose: To automate contrast phase classification in CT using organ-specific features extracted from a widely used segmentation tool with a lightweight decision tree classifier.","authors":["Benjamin Hou","Tejas Sudharshan Mathai","Pritam Mukherjee","Xinya Wang","Ronald M. Summers","Zhiyong Lu"],"url":"https://arxiv.org/abs/2501.14066"}
{"created":"2025-05-02","title":"Information geometry of tempered stable processes","abstract":"We find the information geometry of tempered stable processes. Beginning with the derivation of $\\alpha$-divergence between two tempered stable processes, we obtain the corresponding Fisher information matrices and the $\\alpha$-connections on their statistical manifolds. Furthermore, we explore statistical applications of this geometric framework. Various tempered stable processes such as generalized tempered stable processes, classical tempered stable processes, and rapidly-decreasing tempered stable processes are presented as illustrative examples.","authors":["Jaehyung Choi"],"url":"https://arxiv.org/abs/2502.12037"}
{"created":"2025-05-02","title":"Integrating Dynamical Systems Modeling with Spatiotemporal scRNA-seq Data Analysis","abstract":"Understanding the dynamic nature of biological systems is fundamental to deciphering cellular behavior, developmental processes, and disease progression. Single-cell RNA sequencing (scRNA-seq) has provided static snapshots of gene expression, offering valuable insights into cellular states at a single time point. Recent advancements in temporally resolved scRNA-seq, spatial transcriptomics (ST), and time-series spatial transcriptomics (temporal-ST) have further revolutionized our ability to study the spatiotemporal dynamics of individual cells. These technologies, when combined with computational frameworks such as Markov chains, stochastic differential equations (SDEs), and generative models like optimal transport and Schr\\\"odinger bridges, enable the reconstruction of dynamic cellular trajectories and cell fate decisions. This review discusses how these dynamical system approaches offer new opportunities to model and infer cellular dynamics from a systematic perspective.","authors":["Zhenyi Zhang","Yuhao Sun","Qiangwei Peng","Tiejun Li","Peijie Zhou"],"url":"https://arxiv.org/abs/2503.11347"}
{"created":"2025-05-02","title":"Recovering Small Communities in the Planted Partition Model","abstract":"We analyze community recovery in the planted partition model (PPM) in regimes where the number of communities is arbitrarily large. We examine the three standard recovery regimes: exact recovery, almost exact recovery, and weak recovery. When communities vary in size, traditional accuracy- or alignment-based metrics become unsuitable for assessing the correctness of a predicted partition. To address this, we redefine these recovery regimes using the correlation coefficient, a more versatile metric for comparing partitions. We then demonstrate that $\\textit{Diamond Percolation}$, an algorithm based on common-neighbors, successfully recovers communities under mild assumptions on edge probabilities, with minimal restrictions on the number and sizes of communities. As a key application, we consider the case where community sizes follow a power-law distribution, a characteristic frequently found in real-world networks. To the best of our knowledge, we provide the first recovery results for such unbalanced partitions.","authors":["Martijn G\\\"osgens","Maximilien Dreveton"],"url":"https://arxiv.org/abs/2504.01663"}
{"created":"2025-05-02","title":"Improvement of Clamonds solution of the Colebrook-White equation - highest accuracy for engineering purposes with one iteration","abstract":"The Colebrook-White equation is the widely used basis for the calculation of the friction factor lambda for flows in pipes and ducts. Because this equation is implicit in lambda, many solutions have been developed to ease the calculation in order to reduce the effort and to reach a sufficient accuracy. Clamond has proposed in 2008 an iterative solution that requires maximally two iterations to obtain the machine double precision. Here an improvement of this solution is presented, that achieves already with one iteration a maximal error of 2.79E-7, what is more than sufficient for most engineering purposes. This solution is compared in a chart of CPU time versus accuracy with 28 solutions from the literature and in the group of the fastest solutions, that require only two calls of the logarithm function, it proved to be by far the most accurate one.","authors":["Ernst Grosse-Dunker"],"url":"https://arxiv.org/abs/2504.03678"}
{"created":"2025-05-02","title":"Geometry-aware Active Learning of Spatiotemporal Dynamic Systems","abstract":"Rapid developments in advanced sensing and imaging have significantly enhanced information visibility, opening opportunities for predictive modeling of complex dynamic systems. However, sensing signals acquired from such complex systems are often distributed across 3D geometries and rapidly evolving over time, posing significant challenges in spatiotemporal predictive modeling. This paper proposes a geometry-aware active learning framework for modeling spatiotemporal dynamic systems. Specifically, we propose a geometry-aware spatiotemporal Gaussian Process (G-ST-GP) to effectively integrate the temporal correlations and geometric manifold features for reliable prediction of high-dimensional dynamic behaviors. In addition, we develop an adaptive active learning strategy to strategically identify informative spatial locations for data collection and further maximize the prediction accuracy. This strategy achieves the adaptive trade-off between the prediction uncertainty in the G-ST-GP model and the space-filling design guided by the geodesic distance across the 3D geometry. We implement the proposed framework to model the spatiotemporal electrodynamics in a 3D heart geometry. Numerical experiments show that our framework outperforms traditional methods lacking the mechanism of geometric information incorporation or effective data collection.","authors":["Xizhuo Zhang","Bing Yao"],"url":"https://arxiv.org/abs/2504.19012"}
{"created":"2025-05-02","title":"Two-parameter superposable S-curves","abstract":"Straight line equation $y=mx$ with slope $m$, when singularly perturbed as $ay^3+y=mx$ with a positive parameter $a$, results in S-shaped curves or S-curves on a real plane. As $a\\rightarrow 0$, we get back $y=mx$ which is a cumulative distribution function of a continuous uniform distribution that describes the occurrence of every event in an interval to be equally probable. As $a\\rightarrow\\infty$, the derivative of $y$ has finite support only at $y=0$ resembling a degenerate distribution. Based on these arguments, in this work, we propose that these S-curves can represent maximum entropy uniform distribution to a zero entropy single value. We also argue that these S-curves are superposable as they are only parametrically nonlinear but fundamentally linear. So far, the superposed forms have been used to capture the patterns of natural systems such as nonlinear dynamics of biological growth and kinetics of enzyme reactions. Here, we attempt to use the S-curve and its superposed form as statistical models. We fit the models on a classical dataset containing flower measurements of iris plants and analyze their usefulness in pattern recognition. Based on these models, we claim that any non-uniform pattern can be represented as a singular perturbation to uniform distribution. However, our parametric estimation procedure have some limitations such as sensitivity to initial conditions depending on the data at hand.","authors":["Vijay Prakash S"],"url":"https://arxiv.org/abs/2504.19488"}
