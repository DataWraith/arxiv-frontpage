{"created":"2025-04-30","title":"HCT-QA: A Benchmark for Question Answering on Human-Centric Tables","abstract":"Tabular data embedded within PDF files, web pages, and other document formats are prevalent across numerous sectors such as government, engineering, science, and business. These human-centric tables (HCTs) possess a unique combination of high business value, intricate layouts, limited operational power at scale, and sometimes serve as the only data source for critical insights. However, their complexity poses significant challenges to traditional data extraction, processing, and querying methods. While current solutions focus on transforming these tables into relational formats for SQL queries, they fall short in handling the diverse and complex layouts of HCTs and hence being amenable to querying. This paper describes HCT-QA, an extensive benchmark of HCTs, natural language queries, and related answers on thousands of tables. Our dataset includes 2,188 real-world HCTs with 9,835 QA pairs and 4,679 synthetic tables with 67.5K QA pairs. While HCTs can be potentially processed by different type of query engines, in this paper, we focus on Large Language Models as potential engines and assess their ability in processing and querying such tables.","authors":["Mohammad S. Ahmad","Zan A. Naeem","Micha\\\"el Aupetit","Ahmed Elmagarmid","Mohamed Eltabakh","Xiasong Ma","Mourad Ouzzani","Chaoyi Ruan"],"url":"https://arxiv.org/abs/2504.20047"}
{"created":"2025-04-30","title":"Flowshop Machine Scheduling: Markov Modeling, Optimal Schedules and Heuristics","abstract":"Flowshop machine scheduling has been of main interest in several applications where the timing of its processes plays a fundamental role in the utilization of system resources. Addressing the optimal sequencing of the jobs when equivalent failures across machines exist is a decision of particular relevance to the general scheduling problem. Such failures allow for unpredictable time consumption and improper utilization of the machines. Therefore, it is of particular relevance to address the problem with new modeling approaches considering the parallel and sequential components of the manufacturing process under equivalent failure in the jobs at each machine. In this paper, we propose a novel Markov chain to model the N/ M/P/F permutation flowshop. We analyze the time cost encountered due to M consecutive machine equivalent failures in processing N jobs. We derive new closed form expressions of the completion time under such setup. We extend the Markov model into its underlying components, providing another new Markov model with processes that dont encounter failures and compare both systems. We provide new insights on job ordering decision rules and new approaches in a set of proposed algorithms that provide novel optimal and heuristic methods that provides optimal or near optimal schedules. We derive closed form expressions that divide per machine CT and per machine processing and waiting times. Further, we provide a novel scheme that proves intimate connections between such time components and the maximum number of rounds per machine that allows optimal utilization of the machines in one CT.","authors":["Samah A. M. Ghanem"],"url":"https://arxiv.org/abs/2504.20048"}
{"created":"2025-04-30","title":"It's the same but not the same: Do LLMs distinguish Spanish varieties?","abstract":"In recent years, large language models (LLMs) have demonstrated a high capacity for understanding and generating text in Spanish. However, with five hundred million native speakers, Spanish is not a homogeneous language but rather one rich in diatopic variations spanning both sides of the Atlantic. For this reason, in this study, we evaluate the ability of nine language models to identify and distinguish the morphosyntactic and lexical peculiarities of seven varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean, Peninsular, Mexican and Central American and Rioplatense) through a multiple-choice test. The results indicate that the Peninsular Spanish variety is the best identified by all models and that, among them, GPT-4o is the only model capable of recognizing the variability of the Spanish language.","authors":["Marina Mayor-Rocher","Cristina Pozo","Nina Melero","Gonzalo Mart\\'inez","Mar\\'ia Grandury","Pedro Reviriego"],"url":"https://arxiv.org/abs/2504.20049"}
{"created":"2025-04-30","title":"Multi-Party Private Set Operations from Predicative Zero-Sharing","abstract":"Typical protocols in the multi-party private set operations (MPSO) setting enable m > 2 parties to perform certain secure computation on the intersection or union of their private sets, realizing a very limited range of MPSO functionalities. Most works in this field focus on just one or two specific functionalities, resulting in a large variety of isolated schemes and a lack of a unified framework in MPSO research. In this work, we present an MPSO framework, which allows m parties, each holding a set, to securely compute any set formulas (arbitrary compositions of a finite number of binary set operations, including intersection, union and difference) on their private sets. Our framework is highly versatile and can be instantiated to accommodate a broad spectrum of MPSO functionalities. To the best of our knowledge, this is the first framework to achieve such a level of flexibility and generality in MPSO, without relying on generic secure multi-party computation (MPC) techniques.","authors":["Minglang Dong","Yu Chen","Cong Zhang","Yujie Bai","Yang Cao"],"url":"https://arxiv.org/abs/2504.20050"}
{"created":"2025-04-30","title":"Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts","abstract":"Multiword expressions, characterised by non-compositional meanings and syntactic irregularities, are an example of nuanced language. These expressions can be used literally or idiomatically, leading to significant changes in meaning. While large language models have demonstrated strong performance across many tasks, their ability to handle such linguistic subtleties remains uncertain. Therefore, this study evaluates how state-of-the-art language models process the ambiguity of potentially idiomatic multiword expressions, particularly in contexts that are less frequent, where models are less likely to rely on memorisation. By evaluating models across in Portuguese and Galician, in addition to English, and using a novel code-switched dataset and a novel task, we find that large language models, despite their strengths, struggle with nuanced language. In particular, we find that the latest models, including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both detection and semantic tasks, with especially poor performance on the novel tasks we introduce, despite its similarity to existing tasks. Overall, our results demonstrate that multiword expressions, especially those which are ambiguous, continue to be a challenge to models.","authors":["Frances Laureano De Leon","Harish Tayyar Madabushi","Mark G. Lee"],"url":"https://arxiv.org/abs/2504.20051"}
{"created":"2025-04-30","title":"Can Geometry Save Central Views for Sports Field Registration?","abstract":"Single-frame sports field registration often serves as the foundation for extracting 3D information from broadcast videos, enabling applications related to sports analytics, refereeing, or fan engagement. As sports fields have rigorous specifications in terms of shape and dimensions of their line, circle and point components, sports field markings are commonly used as calibration targets for this task. However, because of the sparse and uneven distribution of field markings, close-up camera views around central areas of the field often depict only line and circle markings. On these views, sports field registration is challenging for the vast majority of existing methods, as they focus on leveraging line field markings and their intersections. It is indeed a challenge to include circle correspondences in a set of linear equations. In this work, we propose a novel method to derive a set of points and lines from circle correspondences, enabling the exploitation of circle correspondences for both sports field registration and image annotation. In our experiments, we illustrate the benefits of our bottom-up geometric method against top-performing detectors and show that our method successfully complements them, enabling sports field registration in difficult scenarios.","authors":["Floriane Magera","Thomas Hoyoux","Martin Castin","Olivier Barnich","Anthony Cioppa","Marc Van Droogenbroeck"],"url":"https://arxiv.org/abs/2504.20052"}
{"created":"2025-04-30","title":"Marmot: Multi-Agent Reasoning for Multi-Object Self-Correcting in Improving Image-Text Alignment","abstract":"While diffusion models excel at generating high-quality images, they often struggle with accurate counting, attributes, and spatial relationships in complex multi-object scenes. To address these challenges, we propose Marmot, a novel and generalizable framework that employs Multi-Agent Reasoning for Multi-Object Self-Correcting, enhancing image-text alignment and facilitating more coherent multi-object image editing. Our framework adopts a divide-and-conquer strategy that decomposes the self-correction task into three critical dimensions (counting, attributes, and spatial relationships), and further divided into object-level subtasks. We construct a multi-agent editing system featuring a decision-execution-verification mechanism, effectively mitigating inter-object interference and enhancing editing reliability. To resolve the problem of subtask integration, we propose a Pixel-Domain Stitching Smoother that employs mask-guided two-stage latent space optimization. This innovation enables parallel processing of subtask results, thereby enhancing runtime efficiency while eliminating multi-stage distortion accumulation. Extensive experiments demonstrate that Marmot significantly improves accuracy in object counting, attribute assignment, and spatial relationships for image generation tasks.","authors":["Jiayang Sun","Hongbo Wang","Jie Cao","Huaibo Huang","Ran He"],"url":"https://arxiv.org/abs/2504.20054"}
{"created":"2025-04-30","title":"A constraints-based approach to fully interpretable neural networks for detecting learner behaviors","abstract":"The increasing use of complex machine learning models in education has led to concerns about their interpretability, which in turn has spurred interest in developing explainability techniques that are both faithful to the model's inner workings and intelligible to human end-users. In this paper, we describe a novel approach to creating a neural-network-based behavior detection model that is interpretable by design. Our model is fully interpretable, meaning that the parameters we extract for our explanations have a clear interpretation, fully capture the model's learned knowledge about the learner behavior of interest, and can be used to create explanations that are both faithful and intelligible. We achieve this by implementing a series of constraints to the model that both simplify its inference process and bring it closer to a human conception of the task at hand. We train the model to detect gaming-the-system behavior, evaluate its performance on this task, and compare its learned patterns to those identified by human experts. Our results show that the model is successfully able to learn patterns indicative of gaming-the-system behavior while providing evidence for fully interpretable explanations. We discuss the implications of our approach and suggest ways to evaluate explainability using a human-grounded approach.","authors":["Juan D. Pinto","Luc Paquette"],"url":"https://arxiv.org/abs/2504.20055"}
{"created":"2025-04-30","title":"Recommending Clinical Trials for Online Patient Cases using Artificial Intelligence","abstract":"Clinical trials are crucial for assessing new treatments; however, recruitment challenges - such as limited awareness, complex eligibility criteria, and referral barriers - hinder their success. With the growth of online platforms, patients increasingly turn to social media and health communities for support, research, and advocacy, expanding recruitment pools and established enrollment pathways. Recognizing this potential, we utilized TrialGPT, a framework that leverages a large language model (LLM) as its backbone, to match 50 online patient cases (collected from published case reports and a social media website) to clinical trials and evaluate performance against traditional keyword-based searches. Our results show that TrialGPT outperforms traditional methods by 46% in identifying eligible trials, with each patient, on average, being eligible for around 7 trials. Additionally, our outreach efforts to case authors and trial organizers regarding these patient-trial matches yielded highly positive feedback, which we present from both perspectives.","authors":["Joey Chan","Qiao Jin","Nicholas Wan","Charalampos S. Floudas","Elisabetta Xue","Zhiyong Lu"],"url":"https://arxiv.org/abs/2504.20059"}
{"created":"2025-04-30","title":"A novel real-time aeroelastic hybrid simulation system of section model wind tunnel testing based on adaptive extended Kalman filter","abstract":"Elastically-supported section model tests are the most basic experimental technique in wind engineering, where helical springs are commonly employed to simulate the two-degree-of-freedom low-order modal motions of flexible structures. However, the traditional technique has intrinsic limitations in accurately modeling nonlinear structural behaviors and accurate adjustments of nonlinear structural damping. This study proposes a novel Real-Time Aeroelastic Hybrid Simulation system for section model wind tunnel tests by integrating an active control algorithm of adaptive Kalman filter. The proposed system enables the simulation of nonlinear heave-transverse-torsion coupled vibrations of a section model under the action of the oncoming wind. The structural properties, i.g. mass, damping and stiffness, are numerically simulated via an active control system, and the aerodynamic forces are physically modelled via the model-wind interaction in the wind tunnel. To validate the feasibility and accuracy of the proposed RTAHS system, a MATLAB/Simulink-FLUENT/UDF co-simulation framework is developed. Numerical verification results indicate that the proposed algorithm effectively estimates the motion responses in both linear and nonlinear scenarios.","authors":["Wenkai Du","Guangzhong Gao","Suhan Li","Bo Fu","Jiawu Li","Ledong Zhu"],"url":"https://arxiv.org/abs/2504.20063"}
{"created":"2025-04-30","title":"Against Opacity: Explainable AI and Large Language Models for Effective Digital Advertising","abstract":"The opaqueness of modern digital advertising, exemplified by platforms such as Meta Ads, raises concerns regarding their autonomous control over audience targeting, pricing structures, and ad relevancy assessments. Locked in their leading positions by network effects, ``Metas and Googles of the world'' attract countless advertisers who rely on intuition, with billions of dollars lost on ineffective social media ads. The platforms' algorithms use huge amounts of data unavailable to advertisers, and the algorithms themselves are opaque as well. This lack of transparency hinders the advertisers' ability to make informed decisions and necessitates efforts to promote transparency, standardize industry metrics, and strengthen regulatory frameworks. In this work, we propose novel ways to assist marketers in optimizing their advertising strategies via machine learning techniques designed to analyze and evaluate content, in particular, predict the click-through rates (CTR) of novel advertising content. Another important problem is that large volumes of data available in the competitive landscape, e.g., competitors' ads, impede the ability of marketers to derive meaningful insights. This leads to a pressing need for a novel approach that would allow us to summarize and comprehend complex data. Inspired by the success of ChatGPT in bridging the gap between large language models (LLMs) and a broader non-technical audience, we propose a novel system that facilitates marketers in data interpretation, called SODA, that merges LLMs with explainable AI, enabling better human-AI collaboration with an emphasis on the domain of digital marketing and advertising. By combining LLMs and explainability features, in particular modern text-image models, we aim to improve the synergy between human marketers and AI systems.","authors":["Qi Yang","Marlo Ongpin","Sergey Nikolenko","Alfred Huang","Aleksandr Farseev"],"url":"https://arxiv.org/abs/2504.20064"}
{"created":"2025-04-30","title":"A Computational Analysis and Visualization of In-Text Reference Networks Across Philosophical Texts","abstract":"We applied computational methods to analyze references across 2,245 philosophical texts, spanning from approximately 550 BCE to 1940 AD, in order to measure patterns in how philosophical ideas have spread over time. Using natural language processing and network analysis, we mapped over 294,970 references between authors, classifying each reference into subdisciplines of philosophy based on its surrounding context. We then constructed a graph, with authors as nodes and textual references as edges, to empirically validate, visualize, and quantify intellectual lineages as they are understood within philosophical scholarship. For instance, we find that Plato and Aristotle alone account for nearly 10% of all references from authors in our dataset, suggesting that their influence may still be underestimated. As another example, we support the view that St. Thomas Aquinas served as a synthesizer between Aristotelian and Christian philosophy by analyzing the network structures of Aquinas, Aristotle, and Christian theologians. Our results are presented through an interactive visualization tool, allowing users to dynamically explore these networks, alongside a mathematical analysis of the network's structure. Our methodology demonstrates the value of applying network analysis with textual references to study a large collection of historical works.","authors":["Robert Becker","Aron Culotta"],"url":"https://arxiv.org/abs/2504.20065"}
{"created":"2025-04-30","title":"Scalable and Performant Data Loading","abstract":"We present SPDL (Scalable and Performant Data Loading), an open-source, framework-agnostic library designed for efficiently loading array data to GPU. Data loading is often a bottleneck in AI applications, and is challenging to optimize because it requires coordination of network calls, CPU-bound tasks, and GPU device transfer. On top of that, Python's GIL (Global Interpreter Lock) makes it difficult to gain performance improvement from multi-threading. We found that when data preprocessing functions release the GIL entirely, it is possible to execute them concurrently in a thread pool, thereby improving the workflow performance. Our benchmark shows that compared to the PyTorch DataLoader, SPDL can iterate through the ImageNet dataset 74% faster while using 38% less CPU and 50GB less memory. When training ViT-B/16 model, SPDL can send data to the GPU at a speed that does not starve the training. Additionally, when using SPDL on Python 3.13t, without changing any code, the throughput is further by improved by 33%, thanks to the disabled GIL. SPDL can improve the performance of current AI model training, and receives further performance improvements when Free-Threaded Python is adopted in production systems. SPDL is available at https://github.com/facebookresearch/spdl.","authors":["Moto Hira","Christian Puhrsch","Valentin Andrei","Roman Malinovskyy","Gael Le Lan","Abhinandan Krishnan","Joseph Cummings","Miguel Martin","Gokul Gunasekaran","Yuta Inoue","Alex J Turner","Raghuraman Krishnamoorthi"],"url":"https://arxiv.org/abs/2504.20067"}
{"created":"2025-04-30","title":"Tempo: Application-aware LLM Serving with Mixed SLO Requirements","abstract":"The integration of Large Language Models (LLMs) into diverse applications, ranging from interactive chatbots and cloud AIOps to intelligent agents, has introduced a wide spectrum of Service Level Objectives (SLOs) for responsiveness. These workloads include latency-sensitive requests focused on per-token latency in streaming chat, throughput-intensive requests that require rapid full responses to invoke tools, and collective requests with dynamic dependencies arising from self-reflection or agent-based reasoning. This workload diversity, amplified by unpredictable request information such as response lengths and runtime dependencies, makes existing schedulers inadequate even within their design envelopes.","authors":["Wei Zhang","Zhiyu Wu","Yi Mu","Banruo Liu","Myungjin Lee","Fan Lai"],"url":"https://arxiv.org/abs/2504.20068"}
{"created":"2025-04-30","title":"A Simple Review of EEG Foundation Models: Datasets, Advancements and Future Perspectives","abstract":"Electroencephalogram (EEG) signals play a crucial role in understanding brain activity and diagnosing neurological disorders. This review focuses on the recent development of EEG foundation models(EEG-FMs), which have shown great potential in processing and analyzing EEG data. We discuss various EEG-FMs, including their architectures, pre-training strategies, their pre-training and downstream datasets and other details. The review also highlights the challenges and future directions in this field, aiming to provide a comprehensive overview for researchers and practitioners interested in EEG analysis and related EEG-FMs.","authors":["Junhong Lai","Jiyu Wei","Lin Yao","Yueming Wang"],"url":"https://arxiv.org/abs/2504.20069"}
{"created":"2025-04-30","title":"Improving Deep Knowledge Tracing via Gated Architectures and Adaptive Optimization","abstract":"Deep Knowledge Tracing (DKT) models student learning behavior by using Recurrent Neural Networks (RNNs) to predict future performance based on historical interaction data. However, the original implementation relied on standard RNNs in the Lua-based Torch framework, which limited extensibility and reproducibility. In this work, we revisit the DKT model from two perspectives: architectural improvements and optimization efficiency. First, we enhance the model using gated recurrent units, specifically Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRU), which better capture long-term dependencies and help mitigate vanishing gradient issues. Second, we re-implement DKT using the PyTorch framework, enabling a modular and accessible infrastructure compatible with modern deep learning workflows. We also benchmark several optimization algorithms SGD, RMSProp, Adagrad, Adam, and AdamW to evaluate their impact on convergence speed and predictive accuracy in educational modeling tasks. Experiments on the Synthetic-5 and Khan Academy datasets show that GRUs and LSTMs achieve higher accuracy and improved training stability compared to basic RNNs, while adaptive optimizers such as Adam and AdamW consistently outperform SGD in both early-stage learning and final model performance. Our open-source PyTorch implementation provides a reproducible and extensible foundation for future research in neural knowledge tracing and personalized learning systems.","authors":["Altun Shukurlu"],"url":"https://arxiv.org/abs/2504.20070"}
{"created":"2025-04-30","title":"GenGrid: A Generalised Distributed Experimental Environmental Grid for Swarm Robotics","abstract":"GenGrid is a novel comprehensive open-source, distributed platform intended for conducting extensive swarm robotic experiments. The modular platform is designed to run swarm robotics experiments that are compatible with different types of mobile robots ranging from Colias, Kilobot, and E puck. The platform offers programmable control over the experimental setup and its parameters and acts as a tool to collect swarm robot data, including localization, sensory feedback, messaging, and interaction. GenGrid is designed as a modular grid of attachable computing nodes that offers bidirectional communication between the robotic agent and grid nodes and within grids. The paper describes the hardware and software architecture design of the GenGrid system. Further, it discusses some common experimental studies covering multi-robot and swarm robotics to showcase the platform's use. GenGrid of 25 homogeneous cells with identical sensing and communication characteristics with a footprint of 37.5 cm X 37.5 cm, exhibits multiple capabilities with minimal resources. The open-source hardware platform is handy for running swarm experiments, including robot hopping based on multiple gradients, collective transport, shepherding, continuous pheromone deposition, and subsequent evaporation. The low-cost, modular, and open-source platform is significant in the swarm robotics research community, which is currently driven by commercial platforms that allow minimal modifications.","authors":["Pranav Kedia","Madhav Rao"],"url":"https://arxiv.org/abs/2504.20071"}
{"created":"2025-04-30","title":"RAGEN: Understanding Self-Evolution in LLM Agents via Multi-Turn Reinforcement Learning","abstract":"Training large language models (LLMs) as interactive agents presents unique challenges including long-horizon decision making and interacting with stochastic environment feedback. While reinforcement learning (RL) has enabled progress in static tasks, multi-turn agent RL training remains underexplored. We propose StarPO (State-Thinking-Actions-Reward Policy Optimization), a general framework for trajectory-level agent RL, and introduce RAGEN, a modular system for training and evaluating LLM agents. Our study on three stylized environments reveals three core findings. First, our agent RL training shows a recurring mode of Echo Trap where reward variance cliffs and gradient spikes; we address this with StarPO-S, a stabilized variant with trajectory filtering, critic incorporation, and decoupled clipping. Second, we find the shaping of RL rollouts would benefit from diverse initial states, medium interaction granularity and more frequent sampling. Third, we show that without fine-grained, reasoning-aware reward signals, agent reasoning hardly emerge through multi-turn RL and they may show shallow strategies or hallucinated thoughts. Code and environments are available at https://github.com/RAGEN-AI/RAGEN.","authors":["Zihan Wang","Kangrui Wang","Qineng Wang","Pingyue Zhang","Linjie Li","Zhengyuan Yang","Kefan Yu","Minh Nhat Nguyen","Licheng Liu","Eli Gottlieb","Monica Lam","Yiping Lu","Kyunghyun Cho","Jiajun Wu","Li Fei-Fei","Lijuan Wang","Yejin Choi","Manling Li"],"url":"https://arxiv.org/abs/2504.20073"}
{"created":"2025-04-30","title":"EPSILON: Adaptive Fault Mitigation in Approximate Deep Neural Network using Statistical Signatures","abstract":"The increasing adoption of approximate computing in deep neural network accelerators (AxDNNs) promises significant energy efficiency gains. However, permanent faults in AxDNNs can severely degrade their performance compared to their accurate counterparts (AccDNNs). Traditional fault detection and mitigation approaches, while effective for AccDNNs, introduce substantial overhead and latency, making them impractical for energy-constrained real-time deployment. To address this, we introduce EPSILON, a lightweight framework that leverages pre-computed statistical signatures and layer-wise importance metrics for efficient fault detection and mitigation in AxDNNs. Our framework introduces a novel non-parametric pattern-matching algorithm that enables constant-time fault detection without interrupting normal execution while dynamically adapting to different network architectures and fault patterns. EPSILON maintains model accuracy by intelligently adjusting mitigation strategies based on a statistical analysis of weight distribution and layer criticality while preserving the energy benefits of approximate computing. Extensive evaluations across various approximate multipliers, AxDNN architectures, popular datasets (MNIST, CIFAR-10, CIFAR-100, ImageNet-1k), and fault scenarios demonstrate that EPSILON maintains 80.05\\% accuracy while offering 22\\% improvement in inference time and 28\\% improvement in energy efficiency, establishing EPSILON as a practical solution for deploying reliable AxDNNs in safety-critical edge applications.","authors":["Khurram Khalil","Khaza Anuarul Hoque"],"url":"https://arxiv.org/abs/2504.20074"}
{"created":"2025-04-30","title":"The EU AI Act in Development Practice: A Pro-justice Approach","abstract":"With the adoption of the EU AI Act, companies must understand and implement its compliance requirements--an often complex task, especially in areas like risk management and fundamental rights assessments. This paper introduces our High-risk EU AI Act Toolkit(HEAT), which offers a pro-justice, feminist ethics-informed approach to support meaningful compliance. HEAT not only helps teams meet regulatory standards but also translates ethical theory into practice. We show how feminist perspectives on expertise, stakeholder engagement, accessibility, and environmental justice inform HEAT's methods and expand on the Act's baseline. The theories we draw on are not naively utopian. Instead, they inspire non-innocent approaches to interrogating normativity in systems of all kinds - technological and otherwise. By this we mean that pro-justice orientations are cognizant of their involvement in the systems they seek to critique, and offer best practices with how to grapple with the trade-offs inherent in reducing and eradicating harmful behaviour from within. These best practices, as we explain in this paper, are what HEAT both embodies and enables.","authors":["Tomasz Hollanek","Yulu Pi","Dorian Peters","Selen Yakar","Eleanor Drage"],"url":"https://arxiv.org/abs/2504.20075"}
{"created":"2025-04-30","title":"Edge-Based Learning for Improved Classification Under Adversarial Noise","abstract":"Adversarial noise introduces small perturbations in images, misleading deep learning models into misclassification and significantly impacting recognition accuracy. In this study, we analyzed the effects of Fast Gradient Sign Method (FGSM) adversarial noise on image classification and investigated whether training on specific image features can improve robustness. We hypothesize that while adversarial noise perturbs various regions of an image, edges may remain relatively stable and provide essential structural information for classification. To test this, we conducted a series of experiments using brain tumor and COVID datasets. Initially, we trained the models on clean images and then introduced subtle adversarial perturbations, which caused deep learning models to significantly misclassify the images. Retraining on a combination of clean and noisy images led to improved performance. To evaluate the robustness of the edge features, we extracted edges from the original/clean images and trained the models exclusively on edge-based representations. When noise was introduced to the images, the edge-based models demonstrated greater resilience to adversarial attacks compared to those trained on the original or clean images. These results suggest that while adversarial noise is able to exploit complex non-edge regions significantly more than edges, the improvement in the accuracy after retraining is marginally more in the original data as compared to the edges. Thus, leveraging edge-based learning can improve the resilience of deep learning models against adversarial perturbations.","authors":["Manish Kansana","Keyan Alexander Rahimi","Elias Hossain","Iman Dehzangi","Noorbakhsh Amiri Golilarz"],"url":"https://arxiv.org/abs/2504.20077"}
{"created":"2025-04-30","title":"Low-Rank Matrix Approximation for Neural Network Compression","abstract":"Deep Neural Networks (DNNs) are often constrained by their large memories and computational restrictions. In this paper, we introduce a novel adaptive-rank Singular Value Decomposition (ARSVD) that dynamically chooses the rank increase of the fully connected layers below a certain threshold in energy expenditure. Unlike conventional SVD compression methods that apply a fixed rank reduction in all layers, our ARSVD method uses energy distribution to adaptively select rank per layer while retaining accuracy. This is done for each layer in an effort to use as much energy as possible while maintaining the lowest accuracy loss. Such accuracy-adaptive approaches outperform traditional static rank reduction methods by providing an improved balance between compression and model performance. We first train a simple Multi-Layer Perceptron (MLP) on the MNIST, CIFAR-10, and CIFAR-100 dataset and evaluate its performance using accuracy and F1-score. After applying ARSVD, our results demonstrate that the technique can achieve substantial model compression without compromising classification accuracy. These results illustrate the usefulness of ARSVD in computing scenarios where both computational and memory resources are scarce.","authors":["Kalyan Cherukuri","Aarav Lala"],"url":"https://arxiv.org/abs/2504.20078"}
{"created":"2025-04-30","title":"FX-DARTS: Designing Topology-unconstrained Architectures with Differentiable Architecture Search and Entropy-based Super-network Shrinking","abstract":"Strong priors are imposed on the search space of Differentiable Architecture Search (DARTS), such that cells of the same type share the same topological structure and each intermediate node retains two operators from distinct nodes. While these priors reduce optimization difficulties and improve the applicability of searched architectures, they hinder the subsequent development of automated machine learning (Auto-ML) and prevent the optimization algorithm from exploring more powerful neural networks through improved architectural flexibility. This paper aims to reduce these prior constraints by eliminating restrictions on cell topology and modifying the discretization mechanism for super-networks. Specifically, the Flexible DARTS (FX-DARTS) method, which leverages an Entropy-based Super-Network Shrinking (ESS) framework, is presented to address the challenges arising from the elimination of prior constraints. Notably, FX-DARTS enables the derivation of neural architectures without strict prior rules while maintaining the stability in the enlarged search space. Experimental results on image classification benchmarks demonstrate that FX-DARTS is capable of exploring a set of neural architectures with competitive trade-offs between performance and computational complexity within a single search procedure.","authors":["Xuan Rao","Bo Zhao","Derong Liu","Cesare Alippi"],"url":"https://arxiv.org/abs/2504.20079"}
{"created":"2025-04-30","title":"DNAD: Differentiable Neural Architecture Distillation","abstract":"To meet the demand for designing efficient neural networks with appropriate trade-offs between model performance (e.g., classification accuracy) and computational complexity, the differentiable neural architecture distillation (DNAD) algorithm is developed based on two cores, namely search by deleting and search by imitating. Primarily, to derive neural architectures in a space where cells of the same type no longer share the same topology, the super-network progressive shrinking (SNPS) algorithm is developed based on the framework of differentiable architecture search (DARTS), i.e., search by deleting. Unlike conventional DARTS-based approaches which yield neural architectures with simple structures and derive only one architecture during the search procedure, SNPS is able to derive a Pareto-optimal set of architectures with flexible structures by forcing the dynamic super-network shrink from a dense structure to a sparse one progressively. Furthermore, since knowledge distillation (KD) has shown great effectiveness to train a compact network with the assistance of an over-parameterized model, we integrate SNPS with KD to formulate the DNAD algorithm, i.e., search by imitating. By minimizing behavioral differences between the super-network and teacher network, the over-fitting of one-level DARTS is avoided and well-performed neural architectures are derived. Experiments on CIFAR-10 and ImageNet classification tasks demonstrate that both SNPS and DNAD are able to derive a set of architectures which achieve similar or lower error rates with fewer parameters and FLOPs. Particularly, DNAD achieves the top-1 error rate of 23.7% on ImageNet classification with a model of 6.0M parameters and 598M FLOPs, which outperforms most DARTS-based methods.","authors":["Xuan Rao","Bo Zhao","Derong Liu"],"url":"https://arxiv.org/abs/2504.20080"}
{"created":"2025-04-30","title":"Billions at Stake: How Self-Citation Adjusted Metrics Can Transform Equitable Research Funding","abstract":"Citation metrics serve as the cornerstone of scholarly impact evaluation despite their well-documented vulnerability to inflation through self-citation practices. This paper introduces the Self-Citation Adjusted Index (SCAI), a sophisticated metric designed to recalibrate citation counts by accounting for discipline-specific self-citation patterns. Through comprehensive analysis of 5,000 researcher profiles across diverse disciplines, we demonstrate that excessive self-citation inflates traditional metrics by 10-20%, potentially misdirecting billions in research funding. Recent studies confirm that self-citation patterns exhibit significant gender disparities, with men self-citing up to 70% more frequently than women, exacerbating existing inequalities in academic recognition. Our open-source implementation provides comprehensive tools for calculating SCAI and related metrics, offering a more equitable assessment of research impact that reduces the gender citation gap by approximately 8.5%. This work contributes to the paradigm shift toward transparent, nuanced, and equitable research evaluation methodologies in academia, with direct implications for funding allocation decisions that collectively amount to over $100 billion annually in the United States alone.","authors":["Rahul Vishwakarma"],"url":"https://arxiv.org/abs/2504.20081"}
{"created":"2025-04-30","title":"Evolution of AI in Education: Agentic Workflows","abstract":"Artificial intelligence (AI) has transformed various aspects of education, with large language models (LLMs) driving advancements in automated tutoring, assessment, and content generation. However, conventional LLMs are constrained by their reliance on static training data, limited adaptability, and lack of reasoning. To address these limitations and foster more sustainable technological practices, AI agents have emerged as a promising new avenue for educational innovation. In this review, we examine agentic workflows in education according to four major paradigms: reflection, planning, tool use, and multi-agent collaboration. We critically analyze the role of AI agents in education through these key design paradigms, exploring their advantages, applications, and challenges. To illustrate the practical potential of agentic systems, we present a proof-of-concept application: a multi-agent framework for automated essay scoring. Preliminary results suggest this agentic approach may offer improved consistency compared to stand-alone LLMs. Our findings highlight the transformative potential of AI agents in educational settings while underscoring the need for further research into their interpretability, trustworthiness, and sustainable impact on pedagogical impact.","authors":["Firuz Kamalov","David Santandreu Calonge","Linda Smail","Dilshod Azizov","Dimple R. Thadani","Theresa Kwong","Amara Atif"],"url":"https://arxiv.org/abs/2504.20082"}
{"created":"2025-04-30","title":"A model and package for German ColBERT","abstract":"In this work, we introduce a German version for ColBERT, a late interaction multi-dense vector retrieval method, with a focus on RAG applications. We also present the main features of our package for ColBERT models, supporting both retrieval and fine-tuning workflows.","authors":["Thuong Dang","Qiqi Chen"],"url":"https://arxiv.org/abs/2504.20083"}
{"created":"2025-04-30","title":"AI Awareness","abstract":"Recent breakthroughs in artificial intelligence (AI) have brought about increasingly capable systems that demonstrate remarkable abilities in reasoning, language understanding, and problem-solving. These advancements have prompted a renewed examination of AI awareness, not as a philosophical question of consciousness, but as a measurable, functional capacity. In this review, we explore the emerging landscape of AI awareness, which includes meta-cognition (the ability to represent and reason about its own state), self-awareness (recognizing its own identity, knowledge, limitations, inter alia), social awareness (modeling the knowledge, intentions, and behaviors of other agents), and situational awareness (assessing and responding to the context in which it operates).","authors":["Xiaojian Li","Haoyuan Shi","Rongwu Xu","Wei Xu"],"url":"https://arxiv.org/abs/2504.20084"}
{"created":"2025-04-30","title":"Understanding and Mitigating Risks of Generative AI in Financial Services","abstract":"To responsibly develop Generative AI (GenAI) products, it is critical to define the scope of acceptable inputs and outputs. What constitutes a \"safe\" response is an actively debated question. Academic work puts an outsized focus on evaluating models by themselves for general purpose aspects such as toxicity, bias, and fairness, especially in conversational applications being used by a broad audience. In contrast, less focus is put on considering sociotechnical systems in specialized domains. Yet, those specialized systems can be subject to extensive and well-understood legal and regulatory scrutiny. These product-specific considerations need to be set in industry-specific laws, regulations, and corporate governance requirements. In this paper, we aim to highlight AI content safety considerations specific to the financial services domain and outline an associated AI content risk taxonomy. We compare this taxonomy to existing work in this space and discuss implications of risk category violations on various stakeholders. We evaluate how existing open-source technical guardrail solutions cover this taxonomy by assessing them on data collected via red-teaming activities. Our results demonstrate that these guardrails fail to detect most of the content risks we discuss.","authors":["Sebastian Gehrmann","Claire Huang","Xian Teng","Sergei Yurovski","Iyanuoluwa Shode","Chirag S. Patel","Arjun Bhorkar","Naveen Thomas","John Doucette","David Rosenberg","Mark Dredze","David Rabinowitz"],"url":"https://arxiv.org/abs/2504.20086"}
{"created":"2025-04-30","title":"Spark: A System for Scientifically Creative Idea Generation","abstract":"Recently, large language models (LLMs) have shown promising abilities to generate novel research ideas in science, a direction which coincides with many foundational principles in computational creativity (CC). In light of these developments, we present an idea generation system named Spark that couples retrieval-augmented idea generation using LLMs with a reviewer model named Judge trained on 600K scientific reviews from OpenReview. Our work is both a system demonstration and intended to inspire other CC researchers to explore grounding the generation and evaluation of scientific ideas within foundational CC principles. To this end, we release the annotated dataset used to train Judge, inviting other researchers to explore the use of LLMs for idea generation and creative evaluations.","authors":["Aishik Sanyal","Samuel Schapiro","Sumuk Shashidhar","Royce Moon","Lav R. Varshney","Dilek Hakkani-Tur"],"url":"https://arxiv.org/abs/2504.20090"}
{"created":"2025-04-30","title":"VideoMultiAgents: A Multi-Agent Framework for Video Question Answering","abstract":"Video Question Answering (VQA) inherently relies on multimodal reasoning, integrating visual, temporal, and linguistic cues to achieve a deeper understanding of video content. However, many existing methods rely on feeding frame-level captions into a single model, making it difficult to adequately capture temporal and interactive contexts. To address this limitation, we introduce VideoMultiAgents, a framework that integrates specialized agents for vision, scene graph analysis, and text processing. It enhances video understanding leveraging complementary multimodal reasoning from independently operating agents. Our approach is also supplemented with a question-guided caption generation, which produces captions that highlight objects, actions, and temporal transitions directly relevant to a given query, thus improving the answer accuracy. Experimental results demonstrate that our method achieves state-of-the-art performance on Intent-QA (79.0%, +6.2% over previous SOTA), EgoSchema subset (75.4%, +3.4%), and NExT-QA (79.6%, +0.4%).","authors":["Noriyuki Kugo","Xiang Li","Zixin Li","Ashish Gupta","Arpandeep Khatua","Nidhish Jain","Chaitanya Patel","Yuta Kyuragi","Masamoto Tanabiki","Kazuki Kozuka","Ehsan Adeli"],"url":"https://arxiv.org/abs/2504.20091"}
{"created":"2025-04-30","title":"An Integrated Framework for Contextual Personalized LLM-Based Food Recommendation","abstract":"Personalized food recommendation systems (Food-RecSys) critically underperform due to fragmented component understanding and the failure of conventional machine learning with vast, imbalanced food data. While Large Language Models (LLMs) offer promise, current generic Recommendation as Language Processing (RLP) strategies lack the necessary specialization for the food domain's complexity. This thesis tackles these deficiencies by first identifying and analyzing the essential components for effective Food-RecSys. We introduce two key innovations: a multimedia food logging platform for rich contextual data acquisition and the World Food Atlas, enabling unique geolocation-based food analysis previously unavailable. Building on this foundation, we pioneer the Food Recommendation as Language Processing (F-RLP) framework - a novel, integrated approach specifically architected for the food domain. F-RLP leverages LLMs in a tailored manner, overcoming the limitations of generic models and providing a robust infrastructure for effective, contextual, and truly personalized food recommendations.","authors":["Ali Rostami"],"url":"https://arxiv.org/abs/2504.20092"}
{"created":"2025-04-30","title":"Self-Healing Software Systems: Lessons from Nature, Powered by AI","abstract":"As modern software systems grow in complexity and scale, their ability to autonomously detect, diagnose, and recover from failures becomes increasingly vital. Drawing inspiration from biological healing - where the human body detects damage, signals the brain, and activates targeted recovery - this paper explores the concept of self-healing software driven by artificial intelligence. We propose a novel framework that mimics this biological model system observability tools serve as sensory inputs, AI models function as the cognitive core for diagnosis and repair, and healing agents apply targeted code and test modifications. By combining log analysis, static code inspection, and AI-driven generation of patches or test updates, our approach aims to reduce downtime, accelerate debugging, and enhance software resilience. We evaluate the effectiveness of this model through case studies and simulations, comparing it against traditional manual debugging and recovery workflows. This work paves the way toward intelligent, adaptive and self-reliant software systems capable of continuous healing, akin to living organisms.","authors":["Mohammad Baqar","Rajat Khanda","Saba Naqvi"],"url":"https://arxiv.org/abs/2504.20093"}
{"created":"2025-04-30","title":"MATCHA: Can Multi-Agent Collaboration Build a Trustworthy Conversational Recommender?","abstract":"In this paper, we propose a multi-agent collaboration framework called MATCHA for conversational recommendation system, leveraging large language models (LLMs) to enhance personalization and user engagement. Users can request recommendations via free-form text and receive curated lists aligned with their interests, preferences, and constraints. Our system introduces specialized agents for intent analysis, candidate generation, ranking, re-ranking, explainability, and safeguards. These agents collaboratively improve recommendations accuracy, diversity, and safety. On eight metrics, our model achieves superior or comparable performance to the current state-of-the-art. Through comparisons with six baseline models, our approach addresses key challenges in conversational recommendation systems for game recommendations, including: (1) handling complex, user-specific requests, (2) enhancing personalization through multi-agent collaboration, (3) empirical evaluation and deployment, and (4) ensuring safe and trustworthy interactions.","authors":["Zheng Hui","Xiaokai Wei","Yexi Jiang","Kevin Gao","Chen Wang","Frank Ong","Se-eun Yoon","Rachit Pareek","Michelle Gong"],"url":"https://arxiv.org/abs/2504.20094"}
{"created":"2025-04-30","title":"Towards Practical Second-Order Optimizers in Deep Learning: Insights from Fisher Information Analysis","abstract":"First-order optimization methods remain the standard for training deep neural networks (DNNs). Optimizers like Adam incorporate limited curvature information by preconditioning the stochastic gradient with a diagonal matrix. Despite the widespread adoption of first-order methods, second-order optimization algorithms often exhibit superior convergence compared to methods like Adam and SGD. However, their practicality in training DNNs is still limited by a significantly higher per-iteration computational cost compared to first-order methods. In this thesis, we present AdaFisher, a novel adaptive second-order optimizer that leverages a diagonal block-Kronecker approximation of the Fisher information matrix to adaptively precondition gradients. AdaFisher aims to bridge the gap between the improved convergence and generalization of second-order methods and the computational efficiency needed for training DNNs. Despite the traditionally slower speed of second-order optimizers, AdaFisher is effective for tasks such as image classification and language modeling, ex- hibiting remarkable stability and robustness during hyperparameter tuning. We demonstrate that AdaFisher outperforms state-of-the-art optimizers in both accuracy and convergence speed. The code is available from https://github.com/AtlasAnalyticsLab/AdaFisher.","authors":["Damien Martins Gomes"],"url":"https://arxiv.org/abs/2504.20096"}
{"created":"2025-04-30","title":"Long-Distance Field Demonstration of Imaging-Free Drone Identification in Intracity Environments","abstract":"Detecting small objects, such as drones, over long distances presents a significant challenge with broad implications for security, surveillance, environmental monitoring, and autonomous systems. Traditional imaging-based methods rely on high-resolution image acquisition, but are often constrained by range, power consumption, and cost. In contrast, data-driven single-photon-single-pixel light detection and ranging (\\text{D\\textsuperscript{2}SP\\textsuperscript{2}-LiDAR}) provides an imaging-free alternative, directly enabling target identification while reducing system complexity and cost. However, its detection range has been limited to a few hundred meters. Here, we introduce a novel integration of residual neural networks (ResNet) with \\text{D\\textsuperscript{2}SP\\textsuperscript{2}-LiDAR}, incorporating a refined observation model to extend the detection range to 5~\\si{\\kilo\\meter} in an intracity environment while enabling high-accuracy identification of drone poses and types. Experimental results demonstrate that our approach not only outperforms conventional imaging-based recognition systems, but also achieves 94.93\\% pose identification accuracy and 97.99\\% type classification accuracy, even under weak signal conditions with long distances and low signal-to-noise ratios (SNRs). These findings highlight the potential of imaging-free methods for robust long-range detection of small targets in real-world scenarios.","authors":["Junran Guo","Tonglin Mu","Keyuan Li","Jianing Li","Ziyang Luo","Ye Chen","Xiaodong Fan","Jinquan Huang","Minjie Liu","Jinbei Zhang","Ruoyang Qi","Naiting Gu","Shihai Sun"],"url":"https://arxiv.org/abs/2504.20097"}
{"created":"2025-04-30","title":"Decoding Latent Spaces: Assessing the Interpretability of Time Series Foundation Models for Visual Analytics","abstract":"The present study explores the interpretability of latent spaces produced by time series foundation models, focusing on their potential for visual analysis tasks. Specifically, we evaluate the MOMENT family of models, a set of transformer-based, pre-trained architectures for multivariate time series tasks such as: imputation, prediction, classification, and anomaly detection. We evaluate the capacity of these models on five datasets to capture the underlying structures in time series data within their latent space projection and validate whether fine tuning improves the clarity of the resulting embedding spaces. Notable performance improvements in terms of loss reduction were observed after fine tuning. Visual analysis shows limited improvement in the interpretability of the embeddings, requiring further work. Results suggest that, although Time Series Foundation Models such as MOMENT are robust, their latent spaces may require additional methodological refinements to be adequately interpreted, such as alternative projection techniques, loss functions, or data preprocessing strategies. Despite the limitations of MOMENT, foundation models supose a big reduction in execution time and so a great advance for interactive visual analytics.","authors":["Inmaculada Santamaria-Valenzuela","Victor Rodriguez-Fernandez","Javier Huertas-Tato","Jong Hyuk Park","David Camacho"],"url":"https://arxiv.org/abs/2504.20099"}
{"created":"2025-04-30","title":"GenTorrent: Scaling Large Language Model Serving with An Overley Network","abstract":"While significant progress has been made in research and development on open-source and cost-efficient large-language models (LLMs), serving scalability remains a critical challenge, particularly for small organizations and individuals seeking to deploy and test their LLM innovations. Inspired by peer-to-peer networks that leverage decentralized overlay nodes to increase throughput and availability, we propose GenTorrent, an LLM serving overlay that harnesses computing resources from decentralized contributors. We identify four key research problems inherent to enabling such a decentralized infrastructure: 1) overlay network organization; 2) LLM communication privacy; 3) overlay forwarding for resource efficiency; and 4) verification of serving quality. This work presents the first systematic study of these fundamental problems in the context of decentralized LLM serving. Evaluation results from a prototype implemented on a set of decentralized nodes demonstrate that GenTorrent achieves a latency reduction of over 50% compared to the baseline design without overlay forwarding. Furthermore, the security features introduce minimal overhead to serving latency and throughput. We believe this work pioneers a new direction for democratizing and scaling future AI serving capabilities.","authors":["Fei Fang","Yifan Hua","Shengze Wang","Ruilin Zhou","Yi Liu","Chen Qian","Xiaoxue Zhang"],"url":"https://arxiv.org/abs/2504.20101"}
{"created":"2025-04-30","title":"HyboWaveNet: Hyperbolic Graph Neural Networks with Multi-Scale Wavelet Transform for Protein-Protein Interaction Prediction","abstract":"Protein-protein interactions (PPIs) are fundamental for deciphering cellular functions,disease pathways,and drug discovery.Although existing neural networks and machine learning methods have achieved high accuracy in PPI prediction,their black-box nature leads to a lack of causal interpretation of the prediction results and difficulty in capturing hierarchical geometries and multi-scale dynamic interaction patterns among proteins.To address these challenges, we propose HyboWaveNet,a novel deep learning framework that collaborates with hyperbolic graphical neural networks (HGNNs) and multiscale graphical wavelet transform for robust PPI prediction. Mapping protein features to Lorentz space simulates hierarchical topological relationships among biomolecules via a hyperbolic distance metric,enabling node feature representations that better fit biological a priori.HyboWaveNet inherently simulates hierarchical and scale-free biological relationships, while the integration of wavelet transforms enables adaptive extraction of local and global interaction features across different resolutions. Our framework generates node feature representations via a graph neural network under the Lorenz model and generates pairs of positive samples under multiple different views for comparative learning, followed by further feature extraction via multi-scale graph wavelet transforms to predict potential PPIs. Experiments on public datasets show that HyboWaveNet improves over both existing state-of-the-art methods. We also demonstrate through ablation experimental studies that the multi-scale graph wavelet transform module improves the predictive performance and generalization ability of HyboWaveNet. This work links geometric deep learning and signal processing to advance PPI prediction, providing a principled approach for analyzing complex biological systems","authors":["Qingzhi Yu","Shuai Yan","Wenfeng Dai","Xiang Cheng"],"url":"https://arxiv.org/abs/2504.20102"}
{"created":"2025-04-30","title":"An on-production high-resolution longitudinal neonatal fingerprint database in Brazil","abstract":"The neonatal period is critical for survival, requiring accurate and early identification to enable timely interventions such as vaccinations, HIV treatment, and nutrition programs. Biometric solutions offer potential for child protection by helping to prevent baby swaps, locate missing children, and support national identity systems. However, developing effective biometric identification systems for newborns remains a major challenge due to the physiological variability caused by finger growth, weight changes, and skin texture alterations during early development. Current literature has attempted to address these issues by applying scaling factors to emulate growth-induced distortions in minutiae maps, but such approaches fail to capture the complex and non-linear growth patterns of infants. A key barrier to progress in this domain is the lack of comprehensive, longitudinal biometric datasets capturing the evolution of neonatal fingerprints over time. This study addresses this gap by focusing on designing and developing a high-quality biometric database of neonatal fingerprints, acquired at multiple early life stages. The dataset is intended to support the training and evaluation of machine learning models aimed at emulating the effects of growth on biometric features. We hypothesize that such a dataset will enable the development of more robust and accurate Deep Learning-based models, capable of predicting changes in the minutiae map with higher fidelity than conventional scaling-based methods. Ultimately, this effort lays the groundwork for more reliable biometric identification systems tailored to the unique developmental trajectory of newborns.","authors":["Luiz F. P. Southier","Marcelo Filipak","Luiz A. Zanlorensi","Ildefonso Wasilevski","Fabio Favarim","Jefferson T. Oliva","Marcelo Teixeira","Dalcimar Casanova"],"url":"https://arxiv.org/abs/2504.20104"}
{"created":"2025-04-30","title":"Electricity Cost Minimization for Multi-Workflow Allocation in Geo-Distributed Data Centers","abstract":"Worldwide, Geo-distributed Data Centers (GDCs) provide computing and storage services for massive workflow applications, resulting in high electricity costs that vary depending on geographical locations and time. How to reduce electricity costs while satisfying the deadline constraints of workflow applications is important in GDCs, which is determined by the execution time of servers, power, and electricity price. Determining the completion time of workflows with different server frequencies can be challenging, especially in scenarios with heterogeneous computing resources in GDCs. Moreover, the electricity price is also different in geographical locations and may change dynamically. To address these challenges, we develop a geo-distributed system architecture and propose an Electricity Cost aware Multiple Workflows Scheduling algorithm (ECMWS) for servers of GDCs with fixed frequency and power. ECMWS comprises four stages, namely workflow sequencing, deadline partitioning, task sequencing, and resource allocation where two graph embedding models and a policy network are constructed to solve the Markov Decision Process (MDP). After statistically calibrating parameters and algorithm components over a comprehensive set of workflow instances, the proposed algorithms are compared with the state-of-the-art methods over two types of workflow instances. The experimental results demonstrate that our proposed algorithm significantly outperforms other algorithms, achieving an improvement of over 15\\% while maintaining an acceptable computational time. The source codes are available at https://gitee.com/public-artifacts/ecmws-experiments.","authors":["Shuang Wang","He Zhang","Tianxing Wu","Yueyou Zhang","Wei Emma Zhang","Quan Z. Sheng"],"url":"https://arxiv.org/abs/2504.20105"}
{"created":"2025-04-30","title":"Adaptive Helpfulness-Harmlessness Alignment with Preference Vectors","abstract":"Ensuring that large language models (LLMs) are both helpful and harmless is a critical challenge, as overly strict constraints can lead to excessive refusals, while permissive models risk generating harmful content. Existing approaches, such as reinforcement learning from human feedback (RLHF) and direct preference optimization (DPO), attempt to balance these trade-offs but suffer from performance conflicts, limited controllability, and poor extendability. To address these issues, we propose Preference Vector, a novel framework inspired by task arithmetic. Instead of optimizing multiple preferences within a single objective, we train separate models on individual preferences, extract behavior shifts as preference vectors, and dynamically merge them at test time. This modular approach enables fine-grained, user-controllable preference adjustments and facilitates seamless integration of new preferences without retraining. Experiments show that our proposed Preference Vector framework improves helpfulness without excessive conservatism, allows smooth control over preference trade-offs, and supports scalable multi-preference alignment.","authors":["Ren-Wei Liang","Chin-Ting Hsu","Chan-Hung Yu","Saransh Agrawal","Shih-Cheng Huang","Shang-Tse Chen","Kuan-Hao Huang","Shao-Hua Sun"],"url":"https://arxiv.org/abs/2504.20106"}
{"created":"2025-04-30","title":"Swapped Logit Distillation via Bi-level Teacher Alignment","abstract":"Knowledge distillation (KD) compresses the network capacity by transferring knowledge from a large (teacher) network to a smaller one (student). It has been mainstream that the teacher directly transfers knowledge to the student with its original distribution, which can possibly lead to incorrect predictions. In this article, we propose a logit-based distillation via swapped logit processing, namely Swapped Logit Distillation (SLD). SLD is proposed under two assumptions: (1) the wrong prediction occurs when the prediction label confidence is not the maximum; (2) the \"natural\" limit of probability remains uncertain as the best value addition to the target cannot be determined. To address these issues, we propose a swapped logit processing scheme. Through this approach, we find that the swap method can be effectively extended to teacher and student outputs, transforming into two teachers. We further introduce loss scheduling to boost the performance of two teachers' alignment. Extensive experiments on image classification tasks demonstrate that SLD consistently performs best among previous state-of-the-art methods.","authors":["Stephen Ekaputra Limantoro","Jhe-Hao Lin","Chih-Yu Wang","Yi-Lung Tsai","Hong-Han Shuai","Ching-Chun Huang","Wen-Huang Cheng"],"url":"https://arxiv.org/abs/2504.20108"}
{"created":"2025-04-30","title":"Personalized Artificial General Intelligence (AGI) via Neuroscience-Inspired Continuous Learning Systems","abstract":"Artificial Intelligence has made remarkable advancements in recent years, primarily driven by increasingly large deep learning models. However, achieving true Artificial General Intelligence (AGI) demands fundamentally new architectures rather than merely scaling up existing models. Current approaches largely depend on expanding model parameters, which improves task-specific performance but falls short in enabling continuous, adaptable, and generalized learning. Achieving AGI capable of continuous learning and personalization on resource-constrained edge devices is an even bigger challenge.","authors":["Rajeev Gupta","Suhani Gupta","Ronak Parikh","Divya Gupta","Amir Javaheri","Jairaj Singh Shaktawat"],"url":"https://arxiv.org/abs/2504.20109"}
{"created":"2025-04-30","title":"Attention to Detail: Fine-Scale Feature Preservation-Oriented Geometric Pre-training for AI-Driven Surrogate Modeling","abstract":"AI-driven surrogate modeling has become an increasingly effective alternative to physics-based simulations for 3D design, analysis, and manufacturing. These models leverage data-driven methods to predict physical quantities traditionally requiring computationally expensive simulations. However, the scarcity of labeled CAD-to-simulation datasets has driven recent advancements in self-supervised and foundation models, where geometric representation learning is performed offline and later fine-tuned for specific downstream tasks. While these approaches have shown promise, their effectiveness is limited in applications requiring fine-scale geometric detail preservation. This work introduces a self-supervised geometric representation learning method designed to capture fine-scale geometric features from non-parametric 3D models. Unlike traditional end-to-end surrogate models, this approach decouples geometric feature extraction from downstream physics tasks, learning a latent space embedding guided by geometric reconstruction losses. Key elements include the essential use of near-zero level sampling and the innovative batch-adaptive attention-weighted loss function, which enhance the encoding of intricate design features. The proposed method is validated through case studies in structural mechanics, demonstrating strong performance in capturing design features and enabling accurate few-shot physics predictions. Comparisons with traditional parametric surrogate modeling highlight its potential to bridge the gap between geometric and physics-based representations, providing an effective solution for surrogate modeling in data-scarce scenarios.","authors":["Yu-hsuan Chen","Jing Bi","Cyril Ngo Ngoc","Victor Oancea","Jonathan Cagan","Levent Burak Kara"],"url":"https://arxiv.org/abs/2504.20110"}
{"created":"2025-04-30","title":"Forging and Removing Latent-Noise Diffusion Watermarks Using a Single Image","abstract":"Watermarking techniques are vital for protecting intellectual property and preventing fraudulent use of media. Most previous watermarking schemes designed for diffusion models embed a secret key in the initial noise. The resulting pattern is often considered hard to remove and forge into unrelated images. In this paper, we propose a black-box adversarial attack without presuming access to the diffusion model weights. Our attack uses only a single watermarked example and is based on a simple observation: there is a many-to-one mapping between images and initial noises. There are regions in the clean image latent space pertaining to each watermark that get mapped to the same initial noise when inverted. Based on this intuition, we propose an adversarial attack to forge the watermark by introducing perturbations to the images such that we can enter the region of watermarked images. We show that we can also apply a similar approach for watermark removal by learning perturbations to exit this region. We report results on multiple watermarking schemes (Tree-Ring, RingID, WIND, and Gaussian Shading) across two diffusion models (SDv1.4 and SDv2.0). Our results demonstrate the effectiveness of the attack and expose vulnerabilities in the watermarking methods, motivating future research on improving them.","authors":["Anubhav Jain","Yuya Kobayashi","Naoki Murata","Yuhta Takida","Takashi Shibuya","Yuki Mitsufuji","Niv Cohen","Nasir Memon","Julian Togelius"],"url":"https://arxiv.org/abs/2504.20111"}
{"created":"2025-04-30","title":"Supervised Pretraining for Material Property Prediction","abstract":"Accurate prediction of material properties facilitates the discovery of novel materials with tailored functionalities. Deep learning models have recently shown superior accuracy and flexibility in capturing structure-property relationships. However, these models often rely on supervised learning, which requires large, well-annotated datasets an expensive and time-consuming process. Self-supervised learning (SSL) offers a promising alternative by pretraining on large, unlabeled datasets to develop foundation models that can be fine-tuned for material property prediction. In this work, we propose supervised pretraining, where available class information serves as surrogate labels to guide learning, even when downstream tasks involve unrelated material properties. We evaluate this strategy on two state-of-the-art SSL models and introduce a novel framework for supervised pretraining. To further enhance representation learning, we propose a graph-based augmentation technique that injects noise to improve robustness without structurally deforming material graphs. The resulting foundation models are fine-tuned for six challenging material property predictions, achieving significant performance gains over baselines, ranging from 2% to 6.67% improvement in mean absolute error (MAE) and establishing a new benchmark in material property prediction. This study represents the first exploration of supervised pertaining with surrogate labels in material property prediction, advancing methodology and application in the field.","authors":["Chowdhury Mohammad Abid Rahman","Aldo H. Romero","Prashnna K. Gyawali"],"url":"https://arxiv.org/abs/2504.20112"}
{"created":"2025-04-30","title":"Transforming Evidence Synthesis: A Systematic Review of the Evolution of Automated Meta-Analysis in the Age of AI","abstract":"Exponential growth in scientific literature has heightened the demand for efficient evidence-based synthesis, driving the rise of the field of Automated Meta-analysis (AMA) powered by natural language processing and machine learning. This PRISMA systematic review introduces a structured framework for assessing the current state of AMA, based on screening 978 papers from 2006 to 2024, and analyzing 54 studies across diverse domains. Findings reveal a predominant focus on automating data processing (57%), such as extraction and statistical modeling, while only 17% address advanced synthesis stages. Just one study (2%) explored preliminary full-process automation, highlighting a critical gap that limits AMA's capacity for comprehensive synthesis. Despite recent breakthroughs in large language models (LLMs) and advanced AI, their integration into statistical modeling and higher-order synthesis, such as heterogeneity assessment and bias evaluation, remains underdeveloped. This has constrained AMA's potential for fully autonomous meta-analysis. From our dataset spanning medical (67%) and non-medical (33%) applications, we found that AMA has exhibited distinct implementation patterns and varying degrees of effectiveness in actually improving efficiency, scalability, and reproducibility. While automation has enhanced specific meta-analytic tasks, achieving seamless, end-to-end automation remains an open challenge. As AI systems advance in reasoning and contextual understanding, addressing these gaps is now imperative. Future efforts must focus on bridging automation across all meta-analysis stages, refining interpretability, and ensuring methodological robustness to fully realize AMA's potential for scalable, domain-agnostic synthesis.","authors":["Lingbo Li","Anuradha Mathrani","Teo Susnjak"],"url":"https://arxiv.org/abs/2504.20113"}
{"created":"2025-04-30","title":"TreeHop: Generate and Filter Next Query Embeddings Efficiently for Multi-hop Question Answering","abstract":"Retrieval-augmented generation (RAG) systems face significant challenges in multi-hop question answering (MHQA), where complex queries require synthesizing information across multiple document chunks. Existing approaches typically rely on iterative LLM-based query rewriting and routing, resulting in high computational costs due to repeated LLM invocations and multi-stage processes. To address these limitations, we propose TreeHop, an embedding-level framework without the need for LLMs in query refinement. TreeHop dynamically updates query embeddings by fusing semantic information from prior queries and retrieved documents, enabling iterative retrieval through embedding-space operations alone. This method replaces the traditional \"Retrieve-Rewrite-Vectorize-Retrieve\" cycle with a streamlined \"Retrieve-Embed-Retrieve\" loop, significantly reducing computational overhead. Moreover, a rule-based stop criterion is introduced to further prune redundant retrievals, balancing efficiency and recall rate. Experimental results show that TreeHop rivals advanced RAG methods across three open-domain MHQA datasets, achieving comparable performance with only 5\\%-0.4\\% of the model parameter size and reducing the query latency by approximately 99\\% compared to concurrent approaches. This makes TreeHop a faster and more cost-effective solution for deployment in a range of knowledge-intensive applications. For reproducibility purposes, codes and data are available here: https://github.com/allen-li1231/TreeHop.","authors":["Zhonghao Li","Kunpeng Zhang","Jinghuai Ou","Shuliang Liu","Xuming Hu"],"url":"https://arxiv.org/abs/2504.20114"}
{"created":"2025-04-30","title":"AutoP2C: An LLM-Based Agent Framework for Code Repository Generation from Multimodal Content in Academic Papers","abstract":"Machine Learning (ML) research is spread through academic papers featuring rich multimodal content, including text, diagrams, and tabular results. However, translating these multimodal elements into executable code remains a challenging and time-consuming process that requires substantial ML expertise. We introduce ``Paper-to-Code'' (P2C), a novel task that transforms the multimodal content of scientific publications into fully executable code repositories, which extends beyond the existing formulation of code generation that merely converts textual descriptions into isolated code snippets. To automate the P2C process, we propose AutoP2C, a multi-agent framework based on large language models that processes both textual and visual content from research papers to generate complete code repositories. Specifically, AutoP2C contains four stages: (1) repository blueprint extraction from established codebases, (2) multimodal content parsing that integrates information from text, equations, and figures, (3) hierarchical task decomposition for structured code generation, and (4) iterative feedback-driven debugging to ensure functionality and performance. Evaluation on a benchmark of eight research papers demonstrates the effectiveness of AutoP2C, which can successfully generate executable code repositories for all eight papers, while OpenAI-o1 or DeepSeek-R1 can only produce runnable code for one paper. The code is available at https://github.com/shoushouyu/Automated-Paper-to-Code.","authors":["Zijie Lin","Yiqing Shen","Qilin Cai","He Sun","Jinrui Zhou","Mingjun Xiao"],"url":"https://arxiv.org/abs/2504.20115"}
{"created":"2025-04-30","title":"ResearchCodeAgent: An LLM Multi-Agent System for Automated Codification of Research Methodologies","abstract":"In this paper we introduce ResearchCodeAgent, a novel multi-agent system leveraging large language models (LLMs) agents to automate the codification of research methodologies described in machine learning literature. The system bridges the gap between high-level research concepts and their practical implementation, allowing researchers auto-generating code of existing research papers for benchmarking or building on top-of existing methods specified in the literature with availability of partial or complete starter code. ResearchCodeAgent employs a flexible agent architecture with a comprehensive action suite, enabling context-aware interactions with the research environment. The system incorporates a dynamic planning mechanism, utilizing both short and long-term memory to adapt its approach iteratively. We evaluate ResearchCodeAgent on three distinct machine learning tasks with distinct task complexity and representing different parts of the ML pipeline: data augmentation, optimization, and data batching. Our results demonstrate the system's effectiveness and generalizability, with 46.9% of generated code being high-quality and error-free, and 25% showing performance improvements over baseline implementations. Empirical analysis shows an average reduction of 57.9% in coding time compared to manual implementation. We observe higher gains for more complex tasks. ResearchCodeAgent represents a significant step towards automating the research implementation process, potentially accelerating the pace of machine learning research.","authors":["Shubham Gandhi","Dhruv Shah","Manasi Patwardhan","Lovekesh Vig","Gautam Shroff"],"url":"https://arxiv.org/abs/2504.20117"}
{"created":"2025-04-30","title":"OpenTCM: A GraphRAG-Empowered LLM-based System for Traditional Chinese Medicine Knowledge Retrieval and Diagnosis","abstract":"Traditional Chinese Medicine (TCM) represents a rich repository of ancient medical knowledge that continues to play an important role in modern healthcare. Due to the complexity and breadth of the TCM literature, the integration of AI technologies is critical for its modernization and broader accessibility. However, this integration poses considerable challenges, including the interpretation of obscure classical Chinese texts and the modeling of intricate semantic relationships among TCM concepts. In this paper, we develop OpenTCM, an LLM-based system that combines a domain-specific TCM knowledge graph and Graph-based Retrieval-Augmented Generation (GraphRAG). First, we extract more than 3.73 million classical Chinese characters from 68 gynecological books in the Chinese Medical Classics Database, with the help of TCM and gynecology experts. Second, we construct a comprehensive multi-relational knowledge graph comprising more than 48,000 entities and 152,000 interrelationships, using customized prompts and Chinese-oriented LLMs such as DeepSeek and Kimi to ensure high-fidelity semantic understanding. Last, we integrate OpenTCM with this knowledge graph, enabling high-fidelity ingredient knowledge retrieval and diagnostic question-answering without model fine-tuning. Experimental evaluations demonstrate that our prompt design and model selection significantly improve knowledge graph quality, achieving a precision of 98. 55% and an F1 score of 99. 55%. In addition, OpenTCM achieves mean expert scores of 4.5 in ingredient information retrieval and 3.8 in diagnostic question-answering tasks, outperforming state-of-the-art solutions in real-world TCM use cases.","authors":["Jinglin He","Yunqi Guo","Lai Kwan Lam","Waikei Leung","Lixing He","Yuanan Jiang","Chi Chiu Wang","Guoliang Xing","Hongkai Chen"],"url":"https://arxiv.org/abs/2504.20118"}
{"created":"2025-04-30","title":"Can LLMs Be Trusted for Evaluating RAG Systems? A Survey of Methods and Datasets","abstract":"Retrieval-Augmented Generation (RAG) has advanced significantly in recent years. The complexity of RAG systems, which involve multiple components-such as indexing, retrieval, and generation-along with numerous other parameters, poses substantial challenges for systematic evaluation and quality enhancement. Previous research highlights that evaluating RAG systems is essential for documenting advancements, comparing configurations, and identifying effective approaches for domain-specific applications. This study systematically reviews 63 academic articles to provide a comprehensive overview of state-of-the-art RAG evaluation methodologies, focusing on four key areas: datasets, retrievers, indexing and databases, and the generator component. We observe the feasibility of an automated evaluation approach for each component of a RAG system, leveraging an LLM capable of both generating evaluation datasets and conducting evaluations. In addition, we found that further practical research is essential to provide companies with clear guidance on the do's and don'ts of implementing and evaluating RAG systems. By synthesizing evaluation approaches for key RAG components and emphasizing the creation and adaptation of domain-specific datasets for benchmarking, we contribute to the advancement of systematic evaluation methods and the improvement of evaluation rigor for RAG systems. Furthermore, by examining the interplay between automated approaches leveraging LLMs and human judgment, we contribute to the ongoing discourse on balancing automation and human input, clarifying their respective contributions, limitations, and challenges in achieving robust and reliable evaluations.","authors":["Lorenz Brehme","Thomas Str\\\"ohle","Ruth Breu"],"url":"https://arxiv.org/abs/2504.20119"}
{"created":"2025-04-30","title":"Benchmarking Transferability: A Framework for Fair and Robust Evaluation","abstract":"Transferability scores aim to quantify how well a model trained on one domain generalizes to a target domain. Despite numerous methods proposed for measuring transferability, their reliability and practical usefulness remain inconclusive, often due to differing experimental setups, datasets, and assumptions. In this paper, we introduce a comprehensive benchmarking framework designed to systematically evaluate transferability scores across diverse settings. Through extensive experiments, we observe variations in how different metrics perform under various scenarios, suggesting that current evaluation practices may not fully capture each method's strengths and limitations. Our findings underscore the value of standardized assessment protocols, paving the way for more reliable transferability measures and better-informed model selection in cross-domain applications. Additionally, we achieved a 3.5\\% improvement using our proposed metric for the head-training fine-tuning experimental setup. Our code is available in this repository: https://github.com/alizkzm/pert_robust_platform.","authors":["Alireza Kazemi","Helia Rezvani","Mahsa Baktashmotlagh"],"url":"https://arxiv.org/abs/2504.20121"}
{"created":"2025-04-30","title":"Pediatric Asthma Detection with Googles HeAR Model: An AI-Driven Respiratory Sound Classifier","abstract":"Early detection of asthma in children is crucial to prevent long-term respiratory complications and reduce emergency interventions. This work presents an AI-powered diagnostic pipeline that leverages Googles Health Acoustic Representations (HeAR) model to detect early signs of asthma from pediatric respiratory sounds. The SPRSound dataset, the first open-access collection of annotated respiratory sounds in children aged 1 month to 18 years, is used to extract 2-second audio segments labeled as wheeze, crackle, rhonchi, stridor, or normal. Each segment is embedded into a 512-dimensional representation using HeAR, a foundation model pretrained on 300 million health-related audio clips, including 100 million cough sounds. Multiple classifiers, including SVM, Random Forest, and MLP, are trained on these embeddings to distinguish between asthma-indicative and normal sounds. The system achieves over 91\\% accuracy, with strong performance on precision-recall metrics for positive cases. In addition to classification, learned embeddings are visualized using PCA, misclassifications are analyzed through waveform playback, and ROC and confusion matrix insights are provided. This method demonstrates that short, low-resource pediatric recordings, when powered by foundation audio models, can enable fast, noninvasive asthma screening. The approach is especially promising for digital diagnostics in remote or underserved healthcare settings.","authors":["Abul Ehtesham","Saket Kumar","Aditi Singh","Tala Talaei Khoei"],"url":"https://arxiv.org/abs/2504.20124"}
{"created":"2025-04-30","title":"Towards Large Language Models for Lunar Mission Planning and In Situ Resource Utilization","abstract":"A key factor for lunar mission planning is the ability to assess the local availability of raw materials. However, many potentially relevant measurements are scattered across a variety of scientific publications. In this paper we consider the viability of obtaining lunar composition data by leveraging LLMs to rapidly process a corpus of scientific publications. While leveraging LLMs to obtain knowledge from scientific documents is not new, this particular application presents interesting challenges due to the heterogeneity of lunar samples and the nuances involved in their characterization. Accuracy and uncertainty quantification are particularly crucial since many materials properties can be sensitive to small variations in composition. Our findings indicate that off-the-shelf LLMs are generally effective at extracting data from tables commonly found in these documents. However, there remains opportunity to further refine the data we extract in this initial approach; in particular, to capture fine-grained mineralogy information and to improve performance on more subtle/complex pieces of information.","authors":["Michael Pekala","Gregory Canal","Samuel Barham","Milena B. Graziano","Morgan Trexler","Leslie Hamilton","Elizabeth Reilly","Christopher D. Stiles"],"url":"https://arxiv.org/abs/2504.20125"}
{"created":"2025-04-30","title":"Enhancing Cell Counting through MLOps: A Structured Approach for Automated Cell Analysis","abstract":"Machine Learning (ML) models offer significant potential for advancing cell counting applications in neuroscience, medical research, pharmaceutical development, and environmental monitoring. However, implementing these models effectively requires robust operational frameworks. This paper introduces Cell Counting Machine Learning Operations (CC-MLOps), a comprehensive framework that streamlines the integration of ML in cell counting workflows. CC-MLOps encompasses data access and preprocessing, model training, monitoring, explainability features, and sustainability considerations. Through a practical use case, we demonstrate how MLOps principles can enhance model reliability, reduce human error, and enable scalable Cell Counting solutions. This work provides actionable guidance for researchers and laboratory professionals seeking to implement machine learning (ML)- powered cell counting systems.","authors":["Matteo Testi","Luca Clissa","Matteo Ballabio","Salvatore Ricciardi","Federico Baldo","Emanuele Frontoni","Sara Moccia","Gennario Vessio"],"url":"https://arxiv.org/abs/2504.20126"}
{"created":"2025-04-30","title":"LZ Penalty: An information-theoretic repetition penalty for autoregressive language models","abstract":"We introduce the LZ penalty, a penalty specialized for reducing degenerate repetitions in autoregressive language models without loss of capability. The penalty is based on the codelengths in the LZ77 universal lossless compression algorithm. Through the lens of the prediction-compression duality, decoding the LZ penalty has the interpretation of sampling from the residual distribution after removing the information that is highly compressible. We demonstrate the LZ penalty enables state-of-the-art open-source reasoning models to operate with greedy (temperature zero) decoding without loss of capability and without instances of degenerate repetition. Both the industry-standard frequency penalty and repetition penalty are ineffective, incurring degenerate repetition rates of up to 4%.","authors":["Antonio A. Ginart","Naveen Kodali","Jason Lee","Caiming Xiong","Silvio Savarese","John R. Emmons"],"url":"https://arxiv.org/abs/2504.20131"}
{"created":"2025-04-30","title":"Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models","abstract":"Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared.","authors":["Zae Myung Kim","Chanwoo Park","Vipul Raheja","Dongyeop Kang"],"url":"https://arxiv.org/abs/2504.20157"}
{"created":"2025-04-30","title":"Putting Fair Division on the Map","abstract":"The fair division of indivisible goods is not only a subject of theoretical research, but also an important problem in practice, with solutions being offered on several online platforms. Little is known, however, about the characteristics of real-world allocation instances and how they compare to synthetic instances. Using dimensionality reduction, we compute a \\map of allocation instances: a 2-dimensional embedding such that an instance's location on the map is predictive of the instance's origin and other key instance features. Because the axes of this map closely align with the utility matrix's two largest singular values, we define a second, explicit map, which we theoretically characterize.","authors":["Paula B\\\"ohm","Robert Bredereck","Paul G\\\"olz","Andrzej Kaczmarczyk","Stanisa{\\l}aw Szufa"],"url":"https://arxiv.org/abs/2504.20161"}
{"created":"2025-04-30","title":"Type-safe and portable support for packed data","abstract":"When components of a system exchange data, they need to serialise the data so that it can be sent over the network. Then, the recipient has to deserialise the data in order to be able to process it. These steps take time and have an impact on the overall system's performance.","authors":["Arthur Jamet","Michael Vollmer"],"url":"https://arxiv.org/abs/2504.20166"}
{"created":"2025-04-30","title":"MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools","abstract":"Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer's generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at https://github.com/microsoft/mice_for_cats.","authors":["Nishant Subramani","Jason Eisner","Justin Svegliato","Benjamin Van Durme","Yu Su","Sam Thomson"],"url":"https://arxiv.org/abs/2504.20168"}
{"created":"2025-04-30","title":"Causal Identification in Time Series Models","abstract":"In this paper, we analyze the applicability of the Causal Identification algorithm to causal time series graphs with latent confounders. Since these graphs extend over infinitely many time steps, deciding whether causal effects across arbitrary time intervals are identifiable appears to require computation on graph segments of unbounded size. Even for deciding the identifiability of intervention effects on variables that are close in time, no bound is known on how many time steps in the past need to be considered. We give a first bound of this kind that only depends on the number of variables per time step and the maximum time lag of any direct or latent causal effect. More generally, we show that applying the Causal Identification algorithm to a constant-size segment of the time series graph is sufficient to decide identifiability of causal effects, even across unbounded time intervals.","authors":["Erik Jahn","Karthik Karnik","Leonard J. Schulman"],"url":"https://arxiv.org/abs/2504.20172"}
{"created":"2025-04-30","title":"A Novel Multilevel Taxonomical Approach for Describing High-Dimensional Unlabeled Movement Data","abstract":"Movement data is prevalent across various applications and scientific fields, often characterized by its massive scale and complexity. Exploratory Data Analysis (EDA) plays a crucial role in summarizing and describing such data, enabling researchers to generate insights and support scientific hypotheses. Despite its importance, traditional EDA practices face limitations when applied to high-dimensional, unlabeled movement data. The complexity and multi-faceted nature of this type of data require more advanced methods that go beyond the capabilities of current EDA techniques. This study addresses the gap in current EDA practices by proposing a novel approach that leverages movement variable taxonomies and outlier detection. We hypothesize that organizing movement features into a taxonomy, and applying anomaly detection to combinations of taxonomic nodes, can reveal meaningful patterns and lead to more interpretable descriptions of the data. To test this hypothesis, we introduce TUMD, a new method that integrates movement taxonomies with outlier detection to enhance data analysis and interpretation. TUMD was evaluated across four diverse datasets of moving objects using fixed parameter values. Its effectiveness was assessed through two passes: the first pass categorized the majority of movement patterns as Kinematic, Geometric, or Hybrid for all datasets, while the second pass refined these behaviors into more specific categories such as Speed, Acceleration, or Indentation. TUMD met the effectiveness criteria in three datasets, demonstrating its ability to describe and refine movement behaviors. The results confirmed our hypothesis, showing that the combination of movement taxonomies and anomaly detection successfully uncovers meaningful and interpretable patterns within high-dimensional, unlabeled movement data.","authors":["Yashat Tavakoli","Amilcar Soares","Lourdes Pena"],"url":"https://arxiv.org/abs/2504.20174"}
{"created":"2025-04-30","title":"Evaluation of Switching Technologies for Reflective and Transmissive RISs at Sub-THz Frequencies","abstract":"For the upcoming 6G wireless networks, reconfigurable intelligent surfaces are an essential technology, enabling dynamic beamforming and signal manipulation in both reflective and transmissive modes. It is expected to utilize frequency bands in the millimeter-wave and THz, which presents unique opportunities but also significant challenges. The selection of switching technologies that can support high-frequency operation with minimal loss and high efficiency is particularly complex. In this work, we demonstrate the potential of advanced components such as Schottky diodes, memristor switches, liquid metal-based switches, phase change materials, and RF-SOI technology in RIS designs as an alternative to overcome limitations inherent in traditional technologies in D-band (110-170 GHz).","authors":["Sofia I. In\\'acio","Yihan Ma","Qi Luo","Luca Lucci","Awanish Kumar","Jos\\'e Luis Gonzalez Jimenez","Bruno Reig","Alexandre Siligaris","Denis Mercier","Jonas Deuermeier","Asal Kiazadeh","Ver\\'onica Lain-Rubio","Oleg Cojocari","Tung D. Phan","Ping Jack Soh","S\\'ergio Matos","George C. Alexandropoulos","Lu\\'is M. Pessoa","Antonio Clemente"],"url":"https://arxiv.org/abs/2504.20175"}
{"created":"2025-04-30","title":"A Transformer-based Multimodal Fusion Model for Efficient Crowd Counting Using Visual and Wireless Signals","abstract":"Current crowd-counting models often rely on single-modal inputs, such as visual images or wireless signal data, which can result in significant information loss and suboptimal recognition performance. To address these shortcomings, we propose TransFusion, a novel multimodal fusion-based crowd- counting model that integrates Channel State Information (CSI) with image data. By leveraging the powerful capabilities of Transformer networks, TransFusion effectively combines these two distinct data modalities, enabling the capture of comprehen- sive global contextual information that is critical for accurate crowd estimation. However, while transformers are well capable of capturing global features, they potentially fail to identify finer- grained, local details essential for precise crowd counting. To mitigate this, we incorporate Convolutional Neural Networks (CNNs) into the model architecture, enhancing its ability to extract detailed local features that complement the global context provided by the Transformer. Extensive experimental evaluations demonstrate that TransFusion achieves high accuracy with minimal counting errors while maintaining superior efficiency.","authors":["Zhe Cui","Yuli Li","Le-Nam Tran"],"url":"https://arxiv.org/abs/2504.20178"}
{"created":"2025-04-30","title":"Integration Flow Models","abstract":"Ordinary differential equation (ODE) based generative models have emerged as a powerful approach for producing high-quality samples in many applications. However, the ODE-based methods either suffer the discretization error of numerical solvers of ODE, which restricts the quality of samples when only a few NFEs are used, or struggle with training instability. In this paper, we proposed Integration Flow, which directly learns the integral of ODE-based trajectory paths without solving the ODE functions. Moreover, Integration Flow explicitly incorporates the target state $\\mathbf{x}_0$ as the anchor state in guiding the reverse-time dynamics. We have theoretically proven this can contribute to both stability and accuracy. To the best of our knowledge, Integration Flow is the first model with a unified structure to estimate ODE-based generative models and the first to show the exact straightness of 1-Rectified Flow without reflow. Through theoretical analysis and empirical evaluations, we show that Integration Flows achieve improved performance when it is applied to existing ODE-based models, such as diffusion models, Rectified Flows, and PFGM++. Specifically, Integration Flow achieves one-step generation on CIFAR10 with FIDs of 2.86 for the Variance Exploding (VE) diffusion model, 3.36 for rectified flow without reflow, and 2.91 for PFGM++; and on ImageNet with FIDs of 4.09 for VE diffusion model, 4.35 for rectified flow without reflow and 4.15 for PFGM++.","authors":["Jingjing Wang","Dan Zhang","Joshua Luo","Yin Yang","Feng Luo"],"url":"https://arxiv.org/abs/2504.20179"}
{"created":"2025-04-30","title":"Cybersecurity for Autonomous Vehicles","abstract":"The increasing adoption of autonomous vehicles is bringing a major shift in the automotive industry. However, as these vehicles become more connected, cybersecurity threats have emerged as a serious concern. Protecting the security and integrity of autonomous systems is essential to prevent malicious activities that can harm passengers, other road users, and the overall transportation network. This paper focuses on addressing the cybersecurity issues in autonomous vehicles by examining the challenges and risks involved, which are important for building a secure future. Since autonomous vehicles depend on the communication between sensors, artificial intelligence, external infrastructure, and other systems, they are exposed to different types of cyber threats. A cybersecurity breach in an autonomous vehicle can cause serious problems, including a loss of public trust and safety. Therefore, it is very important to develop and apply strong cybersecurity measures to support the growth and acceptance of self-driving cars. This paper discusses major cybersecurity challenges like vulnerabilities in software and hardware, risks from wireless communication, and threats through external interfaces. It also reviews existing solutions such as secure software development, intrusion detection systems, cryptographic protocols, and anomaly detection methods. Additionally, the paper highlights the role of regulatory bodies, industry collaborations, and cybersecurity standards in creating a secure environment for autonomous vehicles. Setting clear rules and best practices is necessary for consistent protection across manufacturers and regions. By analyzing the current cybersecurity landscape and suggesting practical countermeasures, this paper aims to contribute to the safe development and public trust of autonomous vehicle technology.","authors":["Sai varun reddy Bhemavarapu"],"url":"https://arxiv.org/abs/2504.20180"}
{"created":"2025-04-30","title":"BLADE: Benchmark suite for LLM-driven Automated Design and Evolution of iterative optimisation heuristics","abstract":"The application of Large Language Models (LLMs) for Automated Algorithm Discovery (AAD), particularly for optimisation heuristics, is an emerging field of research. This emergence necessitates robust, standardised benchmarking practices to rigorously evaluate the capabilities and limitations of LLM-driven AAD methods and the resulting generated algorithms, especially given the opacity of their design process and known issues with existing benchmarks. To address this need, we introduce BLADE (Benchmark suite for LLM-driven Automated Design and Evolution), a modular and extensible framework specifically designed for benchmarking LLM-driven AAD methods in a continuous black-box optimisation context. BLADE integrates collections of benchmark problems (including MA-BBOB and SBOX-COST among others) with instance generators and textual descriptions aimed at capability-focused testing, such as generalisation, specialisation and information exploitation. It offers flexible experimental setup options, standardised logging for reproducibility and fair comparison, incorporates methods for analysing the AAD process (e.g., Code Evolution Graphs and various visualisation approaches) and facilitates comparison against human-designed baselines through integration with established tools like IOHanalyser and IOHexplainer. BLADE provides an `out-of-the-box' solution to systematically evaluate LLM-driven AAD approaches. The framework is demonstrated through two distinct use cases exploring mutation prompt strategies and function specialisation.","authors":["Niki van Stein","Anna V. Kononova","Haoran Yin","Thomas B\\\"ack"],"url":"https://arxiv.org/abs/2504.20183"}
{"created":"2025-04-30","title":"AI Supply Chains: An Emerging Ecosystem of AI Actors, Products, and Services","abstract":"The widespread adoption of AI in recent years has led to the emergence of AI supply chains: complex networks of AI actors contributing models, datasets, and more to the development of AI products and services. AI supply chains have many implications yet are poorly understood. In this work, we take a first step toward a formal study of AI supply chains and their implications, providing two illustrative case studies indicating that both AI development and regulation are complicated in the presence of supply chains. We begin by presenting a brief historical perspective on AI supply chains, discussing how their rise reflects a longstanding shift towards specialization and outsourcing that signals the healthy growth of the AI industry. We then model AI supply chains as directed graphs and demonstrate the power of this abstraction by connecting examples of AI issues to graph properties. Finally, we examine two case studies in detail, providing theoretical and empirical results in both. In the first, we show that information passing (specifically, of explanations) along the AI supply chains is imperfect, which can result in misunderstandings that have real-world implications. In the second, we show that upstream design choices (e.g., by base model providers) have downstream consequences (e.g., on AI products fine-tuned on the base model). Together, our findings motivate further study of AI supply chains and their increasingly salient social, economic, regulatory, and technical implications.","authors":["Aspen Hopkins","Sarah H. Cen","Andrew Ilyas","Isabella Struckman","Luis Videgaray","Aleksander M\\k{a}dry"],"url":"https://arxiv.org/abs/2504.20185"}
{"created":"2025-04-30","title":"Communication in Agile Software Development - A Mapping Study","abstract":"Software industry is a fast-moving industry and to keep up with this pace the development process also needs to be fast and efficient and Agile software development (ASD) is the answer to this problem. Even though ASD has been in there for over two decades there are still multiple unknown questions tied to ASD that need to be addressed. In this study we are going to address one of the most critical factors of ASD i.e. Communication. We conducted a review of 14 studies and found the areas under ASD communication that the community is interested in as well as research gaps.","authors":["Suddhasvatta Das","Kevin Gary"],"url":"https://arxiv.org/abs/2504.20186"}
{"created":"2025-04-30","title":"AI Recommendation Systems for Lane-Changing Using Adherence-Aware Reinforcement Learning","abstract":"In this paper, we present an adherence-aware reinforcement learning (RL) approach aimed at seeking optimal lane-changing recommendations within a semi-autonomous driving environment to enhance a single vehicle's travel efficiency. The problem is framed within a Markov decision process setting and is addressed through an adherence-aware deep Q network, which takes into account the partial compliance of human drivers with the recommended actions. This approach is evaluated within CARLA's driving environment under realistic scenarios.","authors":["Weihao Sun","Heeseung Bang","Andreas A. Malikopoulos"],"url":"https://arxiv.org/abs/2504.20187"}
{"created":"2025-04-30","title":"Cosmos: A Cost Model for Serverless Workflows in the 3D Compute Continuum","abstract":"Due to the high scalability, infrastructure management, and pay-per-use pricing model, serverless computing has been adopted in a wide range of applications such as real-time data processing, IoT, and AI-related workflows. However, deploying serverless functions across dynamic and heterogeneous environments such as the 3D (Edge-Cloud-Space) Continuum introduces additional complexity. Each layer of the 3D Continuum shows different performance capabilities and costs according to workload characteristics. Cloud services alone often show significant differences in performance and pricing for similar functions, further complicating cost management. Additionally, serverless workflows consist of functions with diverse characteristics, requiring a granular understanding of performance and cost trade-offs across different infrastructure layers to be able to address them individually. In this paper, we present Cosmos, a cost- and a performance-cost-tradeoff model for serverless workflows that identifies key factors that affect cost changes across different workloads and cloud providers. We present a case study analyzing the main drivers that influence the costs of serverless workflows. We demonstrate how to classify the costs of serverless workflows in leading cloud providers AWS and GCP. Our results show that for data-intensive functions, data transfer and state management costs contribute to up to 75% of the costs in AWS and 52% in GCP. For compute-intensive functions such as AI inference, the cost results show that BaaS services are the largest cost driver, reaching up to 83% in AWS and 97% in GCP.","authors":["Cynthia Marcelino","Sebastian Gollhofer-Berger","Thomas Pusztai","Stefan Nastic"],"url":"https://arxiv.org/abs/2504.20189"}
{"created":"2025-04-30","title":"Debugging WebAssembly? Put some Whamm on it!","abstract":"Debugging and monitoring programs are integral to engineering and deploying software. Dynamic analyses monitor applications through source code or IR injection, machine code or bytecode rewriting, and virtual machine or direct hardware support. While these techniques are viable within their respective domains, common tooling across techniques is rare, leading to fragmentation of skills, duplicated efforts, and inconsistent feature support. We address this problem in the WebAssembly ecosystem with Whamm, a declarative instrumentation DSL for WebAssembly that abstracts above the instrumentation strategy, leveraging bytecode rewriting and engine support as available. Whamm solves three problems: 1) tooling fragmentation, 2) prohibitive instrumentation overhead of general-purpose frameworks, and 3) tedium of tailoring low-level high-performance mechanisms. Whamm provides fully-programmable instrumentation with declarative match rules, static and dynamic predication, automatic state reporting, and user library support, while achieving high performance through compiler and engine optimizations. At the back end, Whamm provides instrumentation to a Wasm engine as Wasm code, reusing existing engine optimizations and unlocking new ones, most notably intrinsification, to minimize overhead. In particular, explicitly requesting program state in match rules, rather than reflection, enables the engine to efficiently bundle arguments and even inline compiled probe logic. Whamm streamlines the tooling effort, as its bytecode-rewriting target can run instrumented programs everywhere, lowering fragmentation and advancing the state of the art for engine support. We evaluate Whamm with case studies of non-trivial monitors and show it is expressive, powerful, and efficient.","authors":["Elizabeth Gilbert","Matthew Schneider","Zixi An","Suhas Thalanki","Wavid Bowman","Alexander Bai","Ben L. Titzer","Heather Miller"],"url":"https://arxiv.org/abs/2504.20192"}
{"created":"2025-04-30","title":"ProFi-Net: Prototype-based Feature Attention with Curriculum Augmentation for WiFi-based Gesture Recognition","abstract":"This paper presents ProFi-Net, a novel few-shot learning framework for WiFi-based gesture recognition that overcomes the chal- lenges of limited training data and sparse feature representations. ProFi- Net employs a prototype-based metric learning architecture enhanced with a feature-level attention mechanism, which dynamically refines the Euclidean distance by emphasizing the most discriminative feature di- mensions. Additionally, our approach introduces a curriculum-inspired data augmentation strategy exclusively on the query set. By progressively incorporating Gaussian noise of increasing magnitude, the model is ex- posed to a broader range of challenging variations, thereby improving its generalization and robustness to overfitting. Extensive experiments con- ducted across diverse real-world environments demonstrate that ProFi- Net significantly outperforms conventional prototype networks and other state-of-the-art few-shot learning methods in terms of classification ac- curacy and training efficiency.","authors":["Zhe Cui","Shuxian Zhang","Kangzhi Lou","Le-Nam Tran"],"url":"https://arxiv.org/abs/2504.20193"}
{"created":"2025-04-30","title":"Prompting LLMs for Code Editing: Struggles and Remedies","abstract":"Large Language Models (LLMs) are rapidly transforming software engineering, with coding assistants embedded in an IDE becoming increasingly prevalent. While research has focused on improving the tools and understanding developer perceptions, a critical gap exists in understanding how developers actually use these tools in their daily workflows, and, crucially, where they struggle. This paper addresses part of this gap through a multi-phased investigation of developer interactions with an LLM-powered code editing and transformation feature, Transform Code, in an IDE widely used at Google. First, we analyze telemetry logs of the feature usage, revealing that frequent re-prompting can be an indicator of developer struggles with using Transform Code. Second, we conduct a qualitative analysis of unsatisfactory requests, identifying five key categories of information often missing from developer prompts. Finally, based on these findings, we propose and evaluate a tool, AutoPrompter, for automatically improving prompts by inferring missing information from the surrounding code context, leading to a 27% improvement in edit correctness on our test set.","authors":["Daye Nam","Ahmed Omran","Ambar Murillo","Saksham Thakur","Abner Araujo","Marcel Blistein","Alexander Fr\\\"ommgen","Vincent Hellendoorn","Satish Chandra"],"url":"https://arxiv.org/abs/2504.20196"}
{"created":"2025-04-30","title":"Representation Learning on a Random Lattice","abstract":"Decomposing a deep neural network's learned representations into interpretable features could greatly enhance its safety and reliability. To better understand features, we adopt a geometric perspective, viewing them as a learned coordinate system for mapping an embedded data distribution. We motivate a model of a generic data distribution as a random lattice and analyze its properties using percolation theory. Learned features are categorized into context, component, and surface features. The model is qualitatively consistent with recent findings in mechanistic interpretability and suggests directions for future research.","authors":["Aryeh Brill"],"url":"https://arxiv.org/abs/2504.20197"}
{"created":"2025-04-30","title":"Leveraging Neural Graph Compilers in Machine Learning Research for Edge-Cloud Systems","abstract":"This work presents a comprehensive evaluation of neural network graph compilers across heterogeneous hardware platforms, addressing the critical gap between theoretical optimization techniques and practical deployment scenarios. We demonstrate how vendor-specific optimizations can invalidate relative performance comparisons between architectural archetypes, with performance advantages sometimes completely reversing after compilation. Our systematic analysis reveals that graph compilers exhibit performance patterns highly dependent on both neural architecture and batch sizes. Through fine-grained block-level experimentation, we establish that vendor-specific compilers can leverage repeated patterns in simple architectures, yielding disproportionate throughput gains as model depth increases. We introduce novel metrics to quantify a compiler's ability to mitigate performance friction as batch size increases. Our methodology bridges the gap between academic research and practical deployment by incorporating compiler effects throughout the research process, providing actionable insights for practitioners navigating complex optimization landscapes across heterogeneous hardware environments.","authors":["Alireza Furutanpey","Carmen Walser","Philipp Raith","Pantelis A. Frangoudis","Schahram Dustdar"],"url":"https://arxiv.org/abs/2504.20198"}
{"created":"2025-04-30","title":"Weaving Context Across Images: Improving Vision-Language Models through Focus-Centric Visual Chains","abstract":"Vision-language models (VLMs) achieve remarkable success in single-image tasks. However, real-world scenarios often involve intricate multi-image inputs, leading to a notable performance decline as models struggle to disentangle critical information scattered across complex visual features. In this work, we propose Focus-Centric Visual Chain, a novel paradigm that enhances VLMs'perception, comprehension, and reasoning abilities in multi-image scenarios. To facilitate this paradigm, we propose Focus-Centric Data Synthesis, a scalable bottom-up approach for synthesizing high-quality data with elaborate reasoning paths. Through this approach, We construct VISC-150K, a large-scale dataset with reasoning data in the form of Focus-Centric Visual Chain, specifically designed for multi-image tasks. Experimental results on seven multi-image benchmarks demonstrate that our method achieves average performance gains of 3.16% and 2.24% across two distinct model architectures, without compromising the general vision-language capabilities. our study represents a significant step toward more robust and capable vision-language systems that can handle complex visual scenarios.","authors":["Juntian Zhang","Chuanqi cheng","Yuhan Liu","Wei Liu","Jian Luan","Rui Yan"],"url":"https://arxiv.org/abs/2504.20199"}
{"created":"2025-04-30","title":"A Novel Parameter-Tying Theorem in Multi-Model Adaptive Systems: Systematic Approach for Efficient Model Selection","abstract":"This paper presents a novel theoretical framework for reducing the computational complexity of multi-model adaptive control/estimation systems through systematic transformation to controllable canonical form. While traditional multi-model approaches face exponential growth in computational demands with increasing system dimension, we introduce a parameter-tying theorem that enables significant dimension reduction through careful analysis of system characteristics in canonical form. The approach leverages monotonicity properties and coordinated parameter relationships to establish minimal sets of identification models while preserving system stability and performance. We develop rigorous criteria for verifying plant inclusion within the convex hull of identification models and derive weight transformation relationships that maintain system properties across coordinate transformations. The effectiveness of the framework is demonstrated through application to coupled lateral-roll vehicle dynamics, where the dimension reduction enables real-time implementation while maintaining estimation accuracy. The results show that the proposed transformation approach can achieve comparable performance to conventional methods while requiring substantially fewer identification models, enabling practical deployment in high-dimensional systems.","authors":["Farid Mafi (Department of Mechanical and Mechatronics Engineering","University of Waterloo","Waterloo","Canada)","Ladan Khoshnevisan (Department of Mechanical and Mechatronics Engineering","University of Waterloo","Waterloo","Canada)","Mohammad Pirani (Department of Mechanical Engineering","University of Ottawa","Ottawa","Canada)","Amir Khajepour (Department of Mechanical and Mechatronics Engineering","University of Waterloo","Waterloo","Canada)"],"url":"https://arxiv.org/abs/2504.20202"}
{"created":"2025-04-30","title":"Remote Sensing Imagery for Flood Detection: Exploration of Augmentation Strategies","abstract":"Floods cause serious problems around the world. Responding quickly and effectively requires accurate and timely information about the affected areas. The effective use of Remote Sensing images for accurate flood detection requires specific detection methods. Typically, Deep Neural Networks are employed, which are trained on specific datasets. For the purpose of river flood detection in RGB imagery, we use the BlessemFlood21 dataset. We here explore the use of different augmentation strategies, ranging from basic approaches to more complex techniques, including optical distortion. By identifying effective strategies, we aim to refine the training process of state-of-the-art Deep Learning segmentation networks.","authors":["Vladyslav Polushko","Damjan Hatic","Ronald R\\\"osch","Thomas M\\\"arz","Markus Rauhut","Andreas Weinmann"],"url":"https://arxiv.org/abs/2504.20203"}
{"created":"2025-04-30","title":"Fault Detection and Human Intervention in Vehicle Platooning: A Multi-Model Framework","abstract":"Vehicle platooning has been a promising solution for improving traffic efficiency and throughput. However, a failure in a single vehicle, including communication loss with neighboring vehicles, can significantly disrupt platoon performance and potentially trigger cascading effects. Similar to modern autonomous vehicles, platoon systems require human drivers to take control during failures, leading to scenarios where vehicles are operated by drivers with diverse driving styles. This paper presents a novel multi-model approach for simultaneously identifying signal drop locations and driver attitudes in vehicular platoons using only tail vehicle measurements. The proposed method distinguishes between attentive and distracted driver behaviors by analyzing the propagation patterns of disturbances through the platoon system. Beyond its application in platooning, our methodology for detecting driver behavior using a multi-model approach provides a novel framework for human driver identification. To enhance computational efficiency for real-time applications, we introduce a blending-based identification method utilizing chosen models and weighted interpolation, significantly reducing the number of required models while maintaining detection accuracy. The effectiveness of our approach is validated through high-fidelity CarSim/Simulink environment simulations. Results demonstrate that the proposed method can accurately identify both the location of signal drops and the corresponding driver behavior. This approach minimizes the complexity and cost of fault detection while ensuring accuracy and reliability.","authors":["Farid Mafi (Department of Mechanical and Mechatronics Engineering","University of Waterloo","Waterloo","Canada)","Mohammad Pirani (Department of Mechanical Engineering","University of Ottawa","Ottawa","Canada)"],"url":"https://arxiv.org/abs/2504.20209"}
{"created":"2025-04-30","title":"Can Large Language Models Learn Formal Logic? A Data-Driven Training and Evaluation Framework","abstract":"This paper investigates the logical reasoning capabilities of large language models (LLMs). For a precisely defined yet tractable formulation, we choose the conceptually simple but technically complex task of constructing proofs in Boolean logic. A trained LLM receives as input a set of assumptions and a goal, and produces as output a proof that formally derives the goal from the assumptions. Incorrect proofs are caught by an automated proof checker. A critical obstacle for training is the scarcity of real-world proofs. We propose an efficient, randomized procedure for synthesizing valid proofs and introduce Template Transformation, a data augmentation technique that enhances the model's ability to handle complex logical expressions. The central evaluation question is whether an LLM has indeed learned to reason. We propose tests to measure the reasoning ability of a black-box LLM. By these measures, experiments demonstrate strong reasoning capabilities for assertions with short proofs, which decline with proof complexity. Notably, template transformation improves accuracy even for smaller models, suggesting its effectiveness across model scales.","authors":["Yuan Xia","Akanksha Atrey","Fadoua Khmaissia","Kedar S. Namjoshi"],"url":"https://arxiv.org/abs/2504.20213"}
{"created":"2025-04-30","title":"Exploring AI-powered Digital Innovations from A Transnational Governance Perspective: Implications for Market Acceptance and Digital Accountability Accountability","abstract":"This study explores the application of the Technology Acceptance Model (TAM) to AI-powered digital innovations within a transnational governance framework. By integrating Latourian actor-network theory (ANT), this study examines how institutional motivations, regulatory compliance, and ethical and cultural acceptance drive organisations to develop and adopt AI innovations, enhancing their market acceptance and transnational accountability. We extend the TAM framework by incorporating regulatory, ethical, and socio-technical considerations as key social pressures shaping AI adoption. Recognizing that AI is embedded within complex actor-networks, we argue that accountability is co-constructed among organisations, regulators, and societal actors rather than being confined to individual developers or adopters. To address these challenges, we propose two key solutions: (1) internal resource reconfiguration, where organisations restructure their governance and compliance mechanisms to align with global standards; and (2) reshaping organisational boundaries through actor-network management, fostering engagement with external stakeholders, regulatory bodies, and transnational governance institutions. These approaches allow organisations to enhance AI accountability, foster ethical and regulatory alignment, and improve market acceptance on a global scale.","authors":["Claire Li","David Peter Wallis Freeborn"],"url":"https://arxiv.org/abs/2504.20215"}
{"created":"2025-04-30","title":"A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports","abstract":"Despite the growing adoption of electronic health records, many processes still rely on paper documents, reflecting the heterogeneous real-world conditions in which healthcare is delivered. The manual transcription process is time-consuming and prone to errors when transferring paper-based data to digital formats. To streamline this workflow, this study presents an open-source pipeline that extracts and categorizes checkbox data from scanned documents. Demonstrated on transfusion reaction reports, the design supports adaptation to other checkbox-rich document types. The proposed method integrates checkbox detection, multilingual optical character recognition (OCR) and multilingual vision-language models (VLMs). The pipeline achieves high precision and recall compared against annually compiled gold-standards from 2017 to 2024. The result is a reduction in administrative workload and accurate regulatory reporting. The open-source availability of this pipeline encourages self-hosted parsing of checkbox forms.","authors":["Henning Sch\\\"afer","Cynthia S. Schmidt","Johannes Wutzkowsky","Kamil Lorek","Lea Reinartz","Johannes R\\\"uckert","Christian Temme","Britta B\\\"ockmann","Peter A. Horn","Christoph M. Friedrich"],"url":"https://arxiv.org/abs/2504.20220"}
{"created":"2025-04-30","title":"FreBIS: Frequency-Based Stratification for Neural Implicit Surface Representations","abstract":"Neural implicit surface representation techniques are in high demand for advancing technologies in augmented reality/virtual reality, digital twins, autonomous navigation, and many other fields. With their ability to model object surfaces in a scene as a continuous function, such techniques have made remarkable strides recently, especially over classical 3D surface reconstruction methods, such as those that use voxels or point clouds. However, these methods struggle with scenes that have varied and complex surfaces principally because they model any given scene with a single encoder network that is tasked to capture all of low through high-surface frequency information in the scene simultaneously. In this work, we propose a novel, neural implicit surface representation approach called FreBIS to overcome this challenge. FreBIS works by stratifying the scene based on the frequency of surfaces into multiple frequency levels, with each level (or a group of levels) encoded by a dedicated encoder. Moreover, FreBIS encourages these encoders to capture complementary information by promoting mutual dissimilarity of the encoded features via a novel, redundancy-aware weighting module. Empirical evaluations on the challenging BlendedMVS dataset indicate that replacing the standard encoder in an off-the-shelf neural surface reconstruction method with our frequency-stratified encoders yields significant improvements. These enhancements are evident both in the quality of the reconstructed 3D surfaces and in the fidelity of their renderings from any viewpoint.","authors":["Naoko Sawada","Pedro Miraldo","Suhas Lohit","Tim K. Marks","Moitreya Chatterjee"],"url":"https://arxiv.org/abs/2504.20222"}
{"created":"2025-04-30","title":"Performance Smells in ML and Non-ML Python Projects: A Comparative Study","abstract":"Python is widely adopted across various domains, especially in Machine Learning (ML) and traditional software projects. Despite its versatility, Python is susceptible to performance smells, i.e., suboptimal coding practices that can reduce application efficiency. This study provides a comparative analysis of performance smells between ML and non-ML projects, aiming to assess the occurrence of these inefficiencies while exploring their distribution across stages in the ML pipeline. For that, we conducted an empirical study analyzing 300 Python-based GitHub projects, distributed across ML and non-ML projects, categorizing performance smells based on the RIdiom tool. Our results indicate that ML projects are more susceptible to performance smells likely due to the computational and data-intensive nature of ML workflows. We also observed that performance smells in the ML pipeline predominantly affect the Data Processing stage. However, their presence in the Model Deployment stage indicates that such smells are not limited to the early stages of the pipeline. Our findings offer actionable insights for developers, emphasizing the importance of targeted optimizations for smells prevalent in ML projects. Furthermore, our study underscores the need to tailor performance optimization strategies to the unique characteristics of ML projects, with particular attention to the pipeline stages most affected by performance smells.","authors":["Fran\\c{c}ois Belias","Leuson Da Silva","Foutse Khomh","Cyrine Zid"],"url":"https://arxiv.org/abs/2504.20224"}
{"created":"2025-04-30","title":"A state reduction approach for learning-based model predictive control for train rescheduling","abstract":"This paper proposes a state reduction method for learning-based model predictive control (MPC) for train rescheduling in urban rail transit systems. The state reduction integrates into a control framework where the discrete decision variables are determined by a learning-based classifier and the continuous decision variables are computed by MPC. Herein, the state representation is designed separately for each component of the control framework. While a reduced state is employed for learning, a full state is used in MPC. Simulations on a large-scale train network highlight the effectiveness of the state reduction mechanism in improving the performance and reducing the memory usage.","authors":["Caio Fabio Oliveira da Silva","Xiaoyu Liu","Azita Dabiri","Bart De Schutter"],"url":"https://arxiv.org/abs/2504.20233"}
{"created":"2025-04-30","title":"Improving trajectory continuity in drone-based crowd monitoring using a set of minimal-cost techniques and deep discriminative correlation filters","abstract":"Drone-based crowd monitoring is the key technology for applications in surveillance, public safety, and event management. However, maintaining tracking continuity and consistency remains a significant challenge. Traditional detection-assignment tracking methods struggle with false positives, false negatives, and frequent identity switches, leading to degraded counting accuracy and making in-depth analysis impossible. This paper introduces a point-oriented online tracking algorithm that improves trajectory continuity and counting reliability in drone-based crowd monitoring. Our method builds on the Simple Online and Real-time Tracking (SORT) framework, replacing the original bounding-box assignment with a point-distance metric. The algorithm is enhanced with three cost-effective techniques: camera motion compensation, altitude-aware assignment, and classification-based trajectory validation. Further, Deep Discriminative Correlation Filters (DDCF) that re-use spatial feature maps from localisation algorithms for increased computational efficiency through neural network resource sharing are integrated to refine object tracking by reducing noise and handling missed detections. The proposed method is evaluated on the DroneCrowd and newly shared UP-COUNT-TRACK datasets, demonstrating substantial improvements in tracking metrics, reducing counting errors to 23% and 15%, respectively. The results also indicate a significant reduction of identity switches while maintaining high tracking accuracy, outperforming baseline online trackers and even an offline greedy optimisation method.","authors":["Bartosz Ptak","Marek Kraft"],"url":"https://arxiv.org/abs/2504.20234"}
{"created":"2025-04-30","title":"Can You Mimic Me? Exploring the Use of Android Record & Replay Tools in Debugging","abstract":"Android User Interface (UI) testing is a critical research area due to the ubiquity of apps and the challenges faced by developers. Record and replay (R&amp;R) tools facilitate manual and automated UI testing by recording UI actions to execute test scenarios and replay bugs. These tools typically support (i) regression testing, (ii) non-crashing functional bug reproduction, and (iii) crashing bug reproduction. However, prior work only examines these tools in fragmented settings, lacking a comprehensive evaluation across common use cases. We address this gap by conducting an empirical study on using R&amp;R tools to record and replay non-crashing failures, crashing bugs, and feature-based user scenarios, and explore combining R&amp;R with automated input generation (AIG) tools to replay crashing bugs. Our study involves one industrial and three academic R&amp;R tools, 34 scenarios from 17 apps, 90 non-crashing failures from 42 apps, and 31 crashing bugs from 17 apps. Results show that 17% of scenarios, 38% of non-crashing bugs, and 44% of crashing bugs cannot be reliably recorded and replayed, mainly due to action interval resolution, API incompatibility, and Android tooling limitations. Our findings highlight key future research directions to enhance the practical application of R&amp;R tools.","authors":["Zihe Song","S M Hasan Mansur","Ravishka Rathnasuriya","Yumna Fatima","Wei Yang","Kevin Moran","Wing Lam"],"url":"https://arxiv.org/abs/2504.20237"}
{"created":"2025-04-30","title":"Physics-Informed Diffusion Models for SAR Ship Wake Generation from Text Prompts","abstract":"Detecting ship presence via wake signatures in SAR imagery is attracting considerable research interest, but limited annotated data availability poses significant challenges for supervised learning. Physics-based simulations are commonly used to address this data scarcity, although they are slow and constrain end-to-end learning. In this work, we explore a new direction for more efficient and end-to-end SAR ship wake simulation using a diffusion model trained on data generated by a physics-based simulator. The training dataset is built by pairing images produced by the simulator with text prompts derived from simulation parameters. Experimental result show that the model generates realistic Kelvin wake patterns and achieves significantly faster inference than the physics-based simulator. These results highlight the potential of diffusion models for fast and controllable wake image generation, opening new possibilities for end-to-end downstream tasks in maritime SAR analysis.","authors":["Kamirul Kamirul","Odysseas Pappas","Alin Achim"],"url":"https://arxiv.org/abs/2504.20241"}
{"created":"2025-04-30","title":"A Case Study on the Use of Representativeness Bias as a Defense Against Adversarial Cyber Threats","abstract":"Cyberspace is an ever-evolving battleground involving adversaries seeking to circumvent existing safeguards and defenders aiming to stay one step ahead by predicting and mitigating the next threat. Existing mitigation strategies have focused primarily on solutions that consider software or hardware aspects, often ignoring the human factor. This paper takes a first step towards psychology-informed, active defense strategies, where we target biases that human beings are susceptible to under conditions of uncertainty.","authors":["Briland Hitaj","Grit Denker","Laura Tinnel","Michael McAnally","Bruce DeBruhl","Nathan Bunting","Alex Fafard","Daniel Aaron","Richard D. Roberts","Joshua Lawson","Greg McCain","Dylan Starink"],"url":"https://arxiv.org/abs/2504.20245"}
{"created":"2025-04-30","title":"Tree embedding based mapping system for low-latency mobile applications in multi-access networks","abstract":"Low-latency applications like AR/VR and online gaming need fast, stable connections. New technologies such as V2X, LEO satellites, and 6G bring unique challenges in mobility management. Traditional solutions based on centralized or distributed anchors often fall short in supporting rapid mobility due to inefficient routing, low versatility, and insufficient multi-access support. In this paper, we design a new end-to-end system for tracking multi-connected mobile devices at scale and optimizing performance for latency-sensitive, highly dynamic applications. Our system, based on the locator/ID separation principle, extends to multi-access networks without requiring specialized routers or caching. Using a novel tree embedding-based overlay, we enable fast session setup while allowing endpoints to directly handle mobility between them. Evaluation with real network data shows our solution cuts connection latency to 7.42% inflation over the shortest path, compared to LISP's 359\\% due to cache misses. It also significantly reduces location update overhead and disruption time during mobility.","authors":["Yu Mi","Randeep Bhatia","Fang Hao","An Wang","Steve Benno","Tv Lakshman"],"url":"https://arxiv.org/abs/2504.20246"}
{"created":"2025-04-30","title":"Temporal Neural Operator for Modeling Time-Dependent Physical Phenomena","abstract":"Neural Operators (NOs) are machine learning models designed to solve partial differential equations (PDEs) by learning to map between function spaces. Neural Operators such as the Deep Operator Network (DeepONet) and the Fourier Neural Operator (FNO) have demonstrated excellent generalization properties when mapping between spatial function spaces. However, they struggle in mapping the temporal dynamics of time-dependent PDEs, especially for time steps not explicitly seen during training. This limits their temporal accuracy as they do not leverage these dynamics in the training process. In addition, most NOs tend to be prohibitively costly to train, especially for higher-dimensional PDEs. In this paper, we propose the Temporal Neural Operator (TNO), an efficient neural operator specifically designed for spatio-temporal operator learning for time-dependent PDEs. TNO achieves this by introducing a temporal-branch to the DeepONet framework, leveraging the best architectural design choices from several other NOs, and a combination of training strategies including Markov assumption, teacher forcing, temporal bundling, and the flexibility to condition the output on the current state or past states. Through extensive benchmarking and an ablation study on a diverse set of example problems we demonstrate the TNO long range temporal extrapolation capabilities, robustness to error accumulation, resolution invariance, and flexibility to handle multiple input functions.","authors":["W. Diab","M. Al-Kobaisi"],"url":"https://arxiv.org/abs/2504.20249"}
{"created":"2025-04-30","title":"Financial Data Analysis with Robust Federated Logistic Regression","abstract":"In this study, we focus on the analysis of financial data in a federated setting, wherein data is distributed across multiple clients or locations, and the raw data never leaves the local devices. Our primary focus is not only on the development of efficient learning frameworks (for protecting user data privacy) in the field of federated learning but also on the importance of designing models that are easier to interpret. In addition, we care about the robustness of the framework to outliers. To achieve these goals, we propose a robust federated logistic regression-based framework that strives to strike a balance between these goals. To verify the feasibility of our proposed framework, we carefully evaluate its performance not only on independently identically distributed (IID) data but also on non-IID data, especially in scenarios involving outliers. Extensive numerical results collected from multiple public datasets demonstrate that our proposed method can achieve comparable performance to those of classical centralized algorithms, such as Logistical Regression, Decision Tree, and K-Nearest Neighbors, in both binary and multi-class classification tasks.","authors":["Kun Yang","Nikhil Krishnan","Sanjeev R. Kulkarni"],"url":"https://arxiv.org/abs/2504.20250"}
{"created":"2025-04-30","title":"A Platform for Generating Educational Activities to Teach English as a Second Language","abstract":"We present a platform for the generation of educational activities oriented to teaching English as a foreign language. The different activities --games and language practice exercises-- are strongly based on Natural Language Processing techniques. The platform offers the possibility of playing out-of-the-box games, generated from resources created semi-automatically and then manually curated. It can also generate games or exercises of greater complexity from texts entered by teachers, providing a stage of review and edition of the generated content before use. As a way of expanding the variety of activities in the platform, we are currently experimenting with image and text generation. In order to integrate them and improve the performance of other neural tools already integrated, we are working on migrating the platform to a more powerful server. In this paper we describe the development of our platform and its deployment for end users, discussing the challenges faced and how we overcame them, and also detail our future work plans.","authors":["Aiala Ros\\'a","Santiago G\\'ongora","Juan Pablo Filevich","Ignacio Sastre","Laura Musto","Brian Carpenter","Luis Chiruzzo"],"url":"https://arxiv.org/abs/2504.20251"}
{"created":"2025-04-30","title":"SA2FE: A Secure, Anonymous, Auditable, and Fair Edge Computing Service Offloading Framework","abstract":"The inclusion of pervasive computing devices in a democratized edge computing ecosystem can significantly expand the capability and coverage of near-end computing for large-scale applications. However, offloading user tasks to heterogeneous and decentralized edge devices comes with the dual risk of both endangered user data security and privacy due to the curious base station or malicious edge servers, and unfair offloading and malicious attacks targeting edge servers from other edge servers and/or users. Existing solutions to edge access control and offloading either rely on \"always-on\" cloud servers with reduced edge benefits or fail to protect sensitive user service information. To address these challenges, this paper presents SA2FE, a novel framework for edge access control, offloading and accounting. We design a rerandomizable puzzle primitive and a corresponding scheme to protect sensitive service information from eavesdroppers and ensure fair offloading decisions, while a blind token-based scheme safeguards user privacy, prevents double spending, and ensures usage accountability. The security of SA2FE is proved under the Universal Composability framework, and its performance and scalability are demonstrated with implementation on commodity mobile devices and edge servers.","authors":["Xiaojian Wang","Huayue Gu","Zhouyu Li","Fangtong Zhou","Ruozhou Yu","Dejun Yang","Guoliang Xue"],"url":"https://arxiv.org/abs/2504.20260"}
{"created":"2025-04-30","title":"An Empirical Analysis of Compatibility Issues for Industrial Mobile Games","abstract":"Detecting and fixing compatibility issues is critical for mobile game development. The rapid evolution of mobile operating systems and device fragmentation make it challenging for developers to timely address these issues across diverse models. Undetected compatibility problems can severely impact user experience and cause financial loss to companies and players. However, mobile game testing remains highly challenging, and compatibility issues are largely underexplored by the research community. To bridge this gap, we conduct an empirical study on common compatibility issues in commercial mobile games. We select four active and representative games with well-documented bug reports, totaling over seven million lines of code and over 20,000 commits. We build a comprehensive dataset linking bugs and fixes, enabling investigation into prevalent symptoms, root causes, and fixing strategies. Through extensive manual analysis, we categorize the most common symptoms and root causes, and summarize the typical fixes for each category. Our findings provide practical guidance for developers and offer insights to inspire future research on testing and fixing compatibility issues in mobile games.","authors":["Zihe Song","Yingfeng Chen","Lei Ma","Shangjie Lu","Honglei Lin","Changjie Fan","Wei Yang"],"url":"https://arxiv.org/abs/2504.20261"}
{"created":"2025-04-30","title":"On closure properties of TotP","abstract":"The class TotP consists of functions that count the number of all paths of a nondeterministic polynomial-time Turing machine. In this paper, we give a second definition of the class TotP in terms of certificates and verifiers, and present a few natural closure properties of this class. We also prove that the closure of TotP under left composition with the class FP+ is equivalent to TotP = FP+ and P = PP, and give examples of FP+-functions such that if TotP is closed under composition with them, then it is closed under composition with FP+.","authors":["Yaroslav Ivanashev"],"url":"https://arxiv.org/abs/2504.20262"}
{"created":"2025-04-30","title":"A Virtual Cybersecurity Department for Securing Digital Twins in Water Distribution Systems","abstract":"Digital twins (DTs) help improve real-time monitoring and decision-making in water distribution systems. However, their connectivity makes them easy targets for cyberattacks such as scanning, denial-of-service (DoS), and unauthorized access. Small and medium-sized enterprises (SMEs) that manage these systems often do not have enough budget or staff to build strong cybersecurity teams. To solve this problem, we present a Virtual Cybersecurity Department (VCD), an affordable and automated framework designed for SMEs. The VCD uses open-source tools like Zabbix for real-time monitoring, Suricata for network intrusion detection, Fail2Ban to block repeated login attempts, and simple firewall settings. To improve threat detection, we also add a machine-learning-based IDS trained on the OD-IDS2022 dataset using an improved ensemble model. This model detects cyber threats such as brute-force attacks, remote code execution (RCE), and network flooding, with 92\\% accuracy and fewer false alarms. Our solution gives SMEs a practical and efficient way to secure water systems using low-cost and easy-to-manage tools.","authors":["Mohammadhossein Homaei","Agustin Di Bartolo","Oscar Mogollon-Gutierrez","Fernando Broncano Morgado","Pablo Garcia Rodriguez"],"url":"https://arxiv.org/abs/2504.20266"}
{"created":"2025-04-30","title":"Investigating task-specific prompts and sparse autoencoders for activation monitoring","abstract":"Language models can behave in unexpected and unsafe ways, and so it is valuable to monitor their outputs. Internal activations of language models encode additional information that could be useful for this. The baseline approach for activation monitoring is some variation of linear probing on a particular layer: starting from a labeled dataset, train a logistic regression classifier on that layer's activations. Recent work has proposed several approaches which may improve on naive linear probing, by leveraging additional computation. One class of techniques, which we call \"prompted probing,\" leverages test time computation to improve monitoring by (1) prompting the model with a description of the monitoring task, and (2) applying a learned linear probe to resulting activations. Another class of techniques uses computation at train time: training sparse autoencoders offline to identify an interpretable basis for the activations, and e.g. max-pooling activations across tokens using that basis before applying a linear probe. However, one can also prompt the model with a description of the monitoring task and use its output directly. We develop and test novel refinements of these methods and compare them against each other. We find asking the model zero-shot is a reasonable baseline when inference-time compute is not limited; however, activation probing methods can substantially outperform this baseline given sufficient training data. Specifically, we recommend prompted probing when inference-time compute is available, due to its superior data efficiency and good generalization performance. Alternatively, if inference-time compute is limited, we find SAE-based probing methods outperform raw activation probing.","authors":["Henk Tillman","Dan Mossing"],"url":"https://arxiv.org/abs/2504.20271"}
{"created":"2025-04-30","title":"Smart Water Security with AI and Blockchain-Enhanced Digital Twins","abstract":"Water distribution systems in rural areas face serious challenges such as a lack of real-time monitoring, vulnerability to cyberattacks, and unreliable data handling. This paper presents an integrated framework that combines LoRaWAN-based data acquisition, a machine learning-driven Intrusion Detection System (IDS), and a blockchain-enabled Digital Twin (BC-DT) platform for secure and transparent water management. The IDS filters anomalous or spoofed data using a Long Short-Term Memory (LSTM) Autoencoder and Isolation Forest before validated data is logged via smart contracts on a private Ethereum blockchain using Proof of Authority (PoA) consensus. The verified data feeds into a real-time DT model supporting leak detection, consumption forecasting, and predictive maintenance. Experimental results demonstrate that the system achieves over 80 transactions per second (TPS) with under 2 seconds of latency while remaining cost-effective and scalable for up to 1,000 smart meters. This work demonstrates a practical and secure architecture for decentralized water infrastructure in under-connected rural environments.","authors":["Mohammadhossein Homaei","Victor Gonzalez Morales","Oscar Mogollon Gutierrez","Ruben Molano Gomez","Andres Caro"],"url":"https://arxiv.org/abs/2504.20275"}
{"created":"2025-04-30","title":"Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi","abstract":"This research delved into GPT-4 and Kimi, two Large Language Models (LLMs), for systematic reviews. We evaluated their performance by comparing LLM-generated codes with human-generated codes from a peer-reviewed systematic review on assessment. Our findings suggested that the performance of LLMs fluctuates by data volume and question complexity for systematic reviews.","authors":["Dandan Chen Kaptur","Yue Huang","Xuejun Ryan Ji","Yanhui Guo","Bradley Kaptur"],"url":"https://arxiv.org/abs/2504.20276"}
{"created":"2025-04-30","title":"Generative Diffusion Models for Resource Allocation in Wireless Networks","abstract":"This paper proposes a supervised training algorithm for learning stochastic resource allocation policies with generative diffusion models (GDMs). We formulate the allocation problem as the maximization of an ergodic utility function subject to ergodic Quality of Service (QoS) constraints. Given samples from a stochastic expert policy that yields a near-optimal solution to the problem, we train a GDM policy to imitate the expert and generate new samples from the optimal distribution. We achieve near-optimal performance through sequential execution of the generated samples. To enable generalization to a family of network configurations, we parameterize the backward diffusion process with a graph neural network (GNN) architecture. We present numerical results in a case study of power control in multi-user interference networks.","authors":["Yigit Berkay Uslu","Samar Hadou","Shirin Saeedi Bidokhti","Alejandro Ribeiro"],"url":"https://arxiv.org/abs/2504.20277"}
{"created":"2025-04-30","title":"Deep Physics Prior for First Order Inverse Optimization","abstract":"Inverse design optimization aims to infer system parameters from observed solutions, posing critical challenges across domains such as semiconductor manufacturing, structural engineering, materials science, and fluid dynamics. The lack of explicit mathematical representations in many systems complicates this process and makes the first order optimization impossible. Mainstream approaches, including generative AI and Bayesian optimization, address these challenges but have limitations. Generative AI is computationally expensive, while Bayesian optimization, relying on surrogate models, suffers from scalability, sensitivity to priors, and noise issues, often leading to suboptimal solutions. This paper introduces Deep Physics Prior (DPP), a novel method enabling first-order gradient-based inverse optimization with surrogate machine learning models. By leveraging pretrained auxiliary Neural Operators, DPP enforces prior distribution constraints to ensure robust and meaningful solutions. This approach is particularly effective when prior data and observation distributions are unknown.","authors":["Haoyu Yang","Kamyar Azizzadenesheli","Haoxing Ren"],"url":"https://arxiv.org/abs/2504.20278"}
{"created":"2025-04-30","title":"FedCCL: Federated Clustered Continual Learning Framework for Privacy-focused Energy Forecasting","abstract":"Privacy-preserving distributed model training is crucial for modern machine learning applications, yet existing Federated Learning approaches struggle with heterogeneous data distributions and varying computational capabilities. Traditional solutions either treat all participants uniformly or require costly dynamic clustering during training, leading to reduced efficiency and delayed model specialization. We present FedCCL (Federated Clustered Continual Learning), a framework specifically designed for environments with static organizational characteristics but dynamic client availability. By combining static pre-training clustering with an adapted asynchronous FedAvg algorithm, FedCCL enables new clients to immediately profit from specialized models without prior exposure to their data distribution, while maintaining reduced coordination overhead and resilience to client disconnections. Our approach implements an asynchronous Federated Learning protocol with a three-tier model topology - global, cluster-specific, and local models - that efficiently manages knowledge sharing across heterogeneous participants. Evaluation using photovoltaic installations across central Europe demonstrates that FedCCL's location-based clustering achieves an energy prediction error of 3.93% (+-0.21%), while maintaining data privacy and showing that the framework maintains stability for population-independent deployments, with 0.14 percentage point degradation in performance for new installations. The results demonstrate that FedCCL offers an effective framework for privacy-preserving distributed learning, maintaining high accuracy and adaptability even with dynamic participant populations.","authors":["Michael A. Helcig","Stefan Nastic"],"url":"https://arxiv.org/abs/2504.20282"}
{"created":"2025-04-30","title":"Computation of Capacity-Distortion-Cost Functions for Continuous Memoryless Channels","abstract":"This paper aims at computing the capacity-distortion-cost (CDC) function for continuous memoryless channels, which is defined as the supremum of the mutual information between channel input and output, constrained by an input cost and an expected distortion of estimating channel state. Solving the optimization problem is challenging because the input distribution does not lie in a finite-dimensional Euclidean space and the optimal estimation function has no closed form in general. We propose to adopt the Wasserstein proximal point method and parametric models such as neural networks (NNs) to update the input distribution and estimation function alternately. To implement it in practice, the importance sampling (IS) technique is used to calculate integrals numerically, and the Wasserstein gradient descent is approximated by pushing forward particles. The algorithm is then applied to an integrated sensing and communications (ISAC) system, validating theoretical results at minimum and maximum distortion as well as the random-deterministic trade-off.","authors":["Xinyang Li","Ziyou Tang","Vlad C. Andrei","Ullrich J. M\\\"onich","Fan Liu","Holger Boche"],"url":"https://arxiv.org/abs/2504.20285"}
{"created":"2025-04-30","title":"Image Interpolation with Score-based Riemannian Metrics of Diffusion Models","abstract":"Diffusion models excel in content generation by implicitly learning the data manifold, yet they lack a practical method to leverage this manifold - unlike other deep generative models equipped with latent spaces. This paper introduces a novel framework that treats the data space of pre-trained diffusion models as a Riemannian manifold, with a metric derived from the score function. Experiments with MNIST and Stable Diffusion show that this geometry-aware approach yields image interpolations that are more realistic, less noisy, and more faithful to prompts than existing methods, demonstrating its potential for improved content generation and editing.","authors":["Shinnosuke Saito","Takashi Matsubara"],"url":"https://arxiv.org/abs/2504.20288"}
{"created":"2025-04-30","title":"Radius-Guided Post-Clustering for Shape-Aware, Scalable Refinement of k-Means Results","abstract":"Traditional k-means clustering underperforms on non-convex shapes and requires the number of clusters k to be specified in advance. We propose a simple geometric enhancement: after standard k-means, each cluster center is assigned a radius (the distance to its farthest assigned point), and clusters whose radii overlap are merged. This post-processing step loosens the requirement for exact k: as long as k is overestimated (but not excessively), the method can often reconstruct non-convex shapes through meaningful merges. We also show that this approach supports recursive partitioning: clustering can be performed independently on tiled regions of the feature space, then globally merged, making the method scalable and suitable for distributed systems. Implemented as a lightweight post-processing step atop scikit-learn's k-means, the algorithm performs well on benchmark datasets, achieving high accuracy with minimal additional computation.","authors":["Stefan Kober"],"url":"https://arxiv.org/abs/2504.20293"}
{"created":"2025-04-30","title":"mrCAD: Multimodal Refinement of Computer-aided Designs","abstract":"A key feature of human collaboration is the ability to iteratively refine the concepts we have communicated. In contrast, while generative AI excels at the \\textit{generation} of content, it often struggles to make specific language-guided \\textit{modifications} of its prior outputs. To bridge the gap between how humans and machines perform edits, we present mrCAD, a dataset of multimodal instructions in a communication game. In each game, players created computer aided designs (CADs) and refined them over several rounds to match specific target designs. Only one player, the Designer, could see the target, and they must instruct the other player, the Maker, using text, drawing, or a combination of modalities. mrCAD consists of 6,082 communication games, 15,163 instruction-execution rounds, played between 1,092 pairs of human players. We analyze the dataset and find that generation and refinement instructions differ in their composition of drawing and text. Using the mrCAD task as a benchmark, we find that state-of-the-art VLMs are better at following generation instructions than refinement instructions. These results lay a foundation for analyzing and modeling a multimodal language of refinement that is not represented in previous datasets.","authors":["William P. McCarthy","Saujas Vaduguru","Karl D. D. Willis","Justin Matejka","Judith E. Fan","Daniel Fried","Yewen Pu"],"url":"https://arxiv.org/abs/2504.20294"}
{"created":"2025-04-30","title":"The Dark Side of Digital Twins: Adversarial Attacks on AI-Driven Water Forecasting","abstract":"Digital twins (DTs) are improving water distribution systems by using real-time data, analytics, and prediction models to optimize operations. This paper presents a DT platform designed for a Spanish water supply network, utilizing Long Short-Term Memory (LSTM) networks to predict water consumption. However, machine learning models are vulnerable to adversarial attacks, such as the Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD). These attacks manipulate critical model parameters, injecting subtle distortions that degrade forecasting accuracy. To further exploit these vulnerabilities, we introduce a Learning Automata (LA) and Random LA-based approach that dynamically adjusts perturbations, making adversarial attacks more difficult to detect. Experimental results show that this approach significantly impacts prediction reliability, causing the Mean Absolute Percentage Error (MAPE) to rise from 26% to over 35%. Moreover, adaptive attack strategies amplify this effect, highlighting cybersecurity risks in AI-driven DTs. These findings emphasize the urgent need for robust defenses, including adversarial training, anomaly detection, and secure data pipelines.","authors":["Mohammadhossein Homaei","Victor Gonzalez Morales","Oscar Mogollon-Gutierrez","Andres Caro"],"url":"https://arxiv.org/abs/2504.20295"}
{"created":"2025-04-30","title":"SoK: A Survey of Mixing Techniques and Mixers for Cryptocurrencies","abstract":"Blockchain technologies have overturned the digital finance industry by introducing a decentralized pseudonymous means of monetary transfer. The pseudonymous nature introduced privacy concerns, enabling various deanonymization techniques, which in turn spurred development of stronger anonymity-preserving measures. The purpose of this paper is to create a comprehensive survey of mixing techniques and implementations within the vast ecosystem surrounding anonymization tools and mechanisms available in blockchain cryptocurrencies. First, we begin by reviewing classifications used in the field. Then, we survey various obfuscation techniques, helping to delve into actual implementations and combinations of these techniques. Next, we identify the positive and negative attributes of the approaches and implementations included. Moreover, we examine the implications of anonymization tools for user privacy, including their effectiveness in preserving anonymity and susceptibility to attacks and vulnerabilities. Finally, we discuss the challenges and innovations for extending mixing services into the realm of smart contracts or cross-chain space.","authors":["Juraj Mariani","Ivan Homoliak"],"url":"https://arxiv.org/abs/2504.20296"}
{"created":"2025-04-30","title":"Towards FAIR and federated Data Ecosystems for interdisciplinary Research","abstract":"Scientific data management is at a critical juncture, driven by exponential data growth, increasing cross-domain dependencies, and a severe reproducibility crisis in modern research. Traditional centralized data management approaches are not only struggle with data volume, but also fail to address the fragmentation of research results across domains, hampering scientific reproducibility, and cross-domain collaboration, while raising concerns about data sovereignty and governance. Here we propose a practical framework for FAIR and federated Data Ecosystems that combines decentralized, distributed systems with existing research infrastructure to enable seamless cross-domain collaboration. Based on established patterns from data commons, data meshes, and data spaces, our approach introduces a layered architecture consisting of governance, data, service, and application layers. Our framework preserves domain-specific expertise and control while facilitating data integration through standardized interfaces and semantic enrichment. Key requirements include adaptive metadata management, simplified user interaction, robust security, and transparent data transactions. Our architecture supports both compute-to-data as well as data-to-compute paradigms, implementing a decentralized peer-to-peer network that scales horizontally. By providing both a technical architecture and a governance framework, FAIR and federated Data Ecosystems enables researchers to build on existing work while maintaining control over their data and computing resources, providing a practical path towards an integrated research infrastructure that respects both domain autonomy and interoperability requirements.","authors":["Sebastian Beyvers","Jannis Hochmuth","Lukas Brehm","Maria Hansen","Alexander Goesmann","Frank F\\\"orster"],"url":"https://arxiv.org/abs/2504.20298"}
{"created":"2025-04-30","title":"Deformable Multibody Modeling for Model Predictive Control in Legged Locomotion with Embodied Compliance","abstract":"The paper presents a method to stabilize dynamic gait for a legged robot with embodied compliance. Our approach introduces a unified description for rigid and compliant bodies to approximate their deformation and a formulation for deformable multibody systems. We develop the centroidal composite predictive deformed inertia (CCPDI) tensor of a deformable multibody system and show how to integrate it with the standard-of-practice model predictive controller (MPC). Simulation shows that the resultant control framework can stabilize trot stepping on a quadrupedal robot with both rigid and compliant spines under the same MPC configurations. Compared to standard MPC, the developed CCPDI-enabled MPC distributes the ground reactive forces closer to the heuristics for body balance, and it is thus more likely to stabilize the gaits of the compliant robot. A parametric study shows that our method preserves some level of robustness within a suitable envelope of key parameter values.","authors":["Keran Ye","Konstantinos Karydis"],"url":"https://arxiv.org/abs/2504.20301"}
{"created":"2025-04-30","title":"DeepAndes: A Self-Supervised Vision Foundation Model for Multi-Spectral Remote Sensing Imagery of the Andes","abstract":"By mapping sites at large scales using remotely sensed data, archaeologists can generate unique insights into long-term demographic trends, inter-regional social networks, and past adaptations to climate change. Remote sensing surveys complement field-based approaches, and their reach can be especially great when combined with deep learning and computer vision techniques. However, conventional supervised deep learning methods face challenges in annotating fine-grained archaeological features at scale. While recent vision foundation models have shown remarkable success in learning large-scale remote sensing data with minimal annotations, most off-the-shelf solutions are designed for RGB images rather than multi-spectral satellite imagery, such as the 8-band data used in our study. In this paper, we introduce DeepAndes, a transformer-based vision foundation model trained on three million multi-spectral satellite images, specifically tailored for Andean archaeology. DeepAndes incorporates a customized DINOv2 self-supervised learning algorithm optimized for 8-band multi-spectral imagery, marking the first foundation model designed explicitly for the Andes region. We evaluate its image understanding performance through imbalanced image classification, image instance retrieval, and pixel-level semantic segmentation tasks. Our experiments show that DeepAndes achieves superior F1 scores, mean average precision, and Dice scores in few-shot learning scenarios, significantly outperforming models trained from scratch or pre-trained on smaller datasets. This underscores the effectiveness of large-scale self-supervised pre-training in archaeological remote sensing. Codes will be available on https://github.com/geopacha/DeepAndes.","authors":["Junlin Guo","James R. Zimmer-Dauphinee","Jordan M. Nieusma","Siqi Lu","Quan Liu","Ruining Deng","Can Cui","Jialin Yue","Yizhe Lin","Tianyuan Yao","Juming Xiong","Junchao Zhu","Chongyu Qu","Yuechen Yang","Mitchell Wilkes","Xiao Wang","Parker VanValkenburgh","Steven A. Wernke","Yuankai Huo"],"url":"https://arxiv.org/abs/2504.20303"}
{"created":"2025-04-30","title":"UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions","abstract":"CHILDES is a widely used resource of transcribed child and child-directed speech. This paper introduces UD-English-CHILDES, the first officially released Universal Dependencies (UD) treebank derived from previously dependency-annotated CHILDES data with consistent and unified annotation guidelines. Our corpus harmonizes annotations from 11 children and their caregivers, totaling over 48k sentences. We validate existing gold-standard annotations under the UD v2 framework and provide an additional 1M silver-standard sentences, offering a consistent resource for computational and linguistic research.","authors":["Xiulin Yang","Zhuoxuan Ju","Lanni Bu","Zoey Liu","Nathan Schneider"],"url":"https://arxiv.org/abs/2504.20304"}
{"created":"2025-04-30","title":"Fast LDL factorization for dense and sparse symmetric matrices over an arbitrary field","abstract":"While existing algorithms may be used to solve a linear system over a general field in matrix-multiplication time, the complexity of constructing a symmetric triangular factorization (LDL) has received relatively little formal study. The LDL factorization is a common tool for factorization of symmetric matrices, and, unlike orthogonal counterparts, generalizes to an arbitrary field. We provide algorithms for dense and sparse LDL factorization and for dense LU factorization that aim to minimize complexity for factorization over a general field. For LU of an $m\\times n$ rank $R$ matrix, we obtain an algorithm with complexity $O(mnR^{\\omega-2})$, where $\\omega$ is the matrix multiplication complexity exponent. For LDL of an $n\\times n$ matrix, we give an algorithm with complexity $O(n^\\omega)$ and for a sparse matrix corresponding to a graph with treewidth $\\tau$, we obtain $O(n\\tau^{\\omega-1})$. Our sparse LDL algorithm is based on an adaptation of the null-space method for solving saddle point systems of equations, which may be of independent interest.","authors":["Edgar Solomonik"],"url":"https://arxiv.org/abs/2504.20305"}
{"created":"2025-04-30","title":"Dynamic Contextual Attention Network: Transforming Spatial Representations into Adaptive Insights for Endoscopic Polyp Diagnosis","abstract":"Colorectal polyps are key indicators for early detection of colorectal cancer. However, traditional endoscopic imaging often struggles with accurate polyp localization and lacks comprehensive contextual awareness, which can limit the explainability of diagnoses. To address these issues, we propose the Dynamic Contextual Attention Network (DCAN). This novel approach transforms spatial representations into adaptive contextual insights, using an attention mechanism that enhances focus on critical polyp regions without explicit localization modules. By integrating contextual awareness into the classification process, DCAN improves decision interpretability and overall diagnostic performance. This advancement in imaging could lead to more reliable colorectal cancer detection, enabling better patient outcomes.","authors":["Teja Krishna Cherukuri","Nagur Shareef Shaik","Sribhuvan Reddy Yellu","Jun-Won Chung","Dong Hye Ye"],"url":"https://arxiv.org/abs/2504.20306"}
{"created":"2025-04-30","title":"FigBO: A Generalized Acquisition Function Framework with Look-Ahead Capability for Bayesian Optimization","abstract":"Bayesian optimization is a powerful technique for optimizing expensive-to-evaluate black-box functions, consisting of two main components: a surrogate model and an acquisition function. In recent years, myopic acquisition functions have been widely adopted for their simplicity and effectiveness. However, their lack of look-ahead capability limits their performance. To address this limitation, we propose FigBO, a generalized acquisition function that incorporates the future impact of candidate points on global information gain. FigBO is a plug-and-play method that can integrate seamlessly with most existing myopic acquisition functions. Theoretically, we analyze the regret bound and convergence rate of FigBO when combined with the myopic base acquisition function expected improvement (EI), comparing them to those of standard EI. Empirically, extensive experimental results across diverse tasks demonstrate that FigBO achieves state-of-the-art performance and significantly faster convergence compared to existing methods.","authors":["Hui Chen","Xuhui Fan","Zhangkai Wu","Longbing Cao"],"url":"https://arxiv.org/abs/2504.20307"}
{"created":"2025-04-30","title":"Online Safety for All: Sociocultural Insights from a Systematic Review of Youth Online Safety in the Global South","abstract":"Youth online safety research in HCI has historically centered on perspectives from the Global North, often overlooking the unique particularities and cultural contexts of regions in the Global South. This paper presents a systematic review of 66 youth online safety studies published between 2014 and 2024, specifically focusing on regions in the Global South. Our findings reveal a concentrated research focus in Asian countries and predominance of quantitative methods. We also found limited research on marginalized youth populations and a primary focus on risks related to cyberbullying. Our analysis underscores the critical role of cultural factors in shaping online safety, highlighting the need for educational approaches that integrate social dynamics and awareness. We propose methodological recommendations and a future research agenda that encourages the adoption of situated, culturally sensitive methodologies and youth-centered approaches to researching youth online safety regions in the Global South. This paper advocates for greater inclusivity in youth online safety research, emphasizing the importance of addressing varied sociocultural contexts to better understand and meet the online safety needs of youth in the Global South.","authors":["Ozioma C. Oguine","Oghenemaro Anuyah","Zainab Agha","Iris Melgarez","Adriana Alvarado Garcia","Karla Badillo-Urquiola"],"url":"https://arxiv.org/abs/2504.20308"}
{"created":"2025-04-30","title":"A Cryptographic Perspective on Mitigation vs. Detection in Machine Learning","abstract":"In this paper, we initiate a cryptographically inspired theoretical study of detection versus mitigation of adversarial inputs produced by attackers of Machine Learning algorithms during inference time.","authors":["Greg Gluch","Shafi Goldwasser"],"url":"https://arxiv.org/abs/2504.20310"}
{"created":"2025-04-30","title":"System Identification of Thrust and Torque Characteristics for a Bipedal Robot with Integrated Propulsion","abstract":"Bipedal robots represent a remarkable and sophisticated class of robotics, designed to emulate human form and movement. Their development marks a significant milestone in the field. However, even the most advanced bipedal robots face challenges related to terrain variation, obstacle negotiation, payload management, weight distribution, and recovering from stumbles. These challenges can be mitigated by incorporating thrusters, which enhance stability on uneven terrain, facilitate obstacle avoidance, and improve recovery after stumbling. Harpy is a bipedal robot equipped with six joints and two thrusters, serving as a hardware platform for implementing and testing advanced control algorithms. This thesis focuses on characterizing Harpy's hardware to improve the system's overall robustness, controllability, and predictability. It also examines simulation results for predicting thrust in propeller-based mechanisms, the integration of thrusters into the Harpy platform and associated testing, as well as an exploration of motor torque characterization methods and their application to hardware in relation to closed-loop force-based impedance control.","authors":["Thomas Cahill"],"url":"https://arxiv.org/abs/2504.20313"}
{"created":"2025-04-30","title":"Perturbation-efficient Zeroth-order Optimization for Hardware-friendly On-device Training","abstract":"Zeroth-order (ZO) optimization is an emerging deep neural network (DNN) training paradigm that offers computational simplicity and memory savings. However, this seemingly promising approach faces a significant and long-ignored challenge. ZO requires generating a substantial number of Gaussian random numbers, which poses significant difficulties and even makes it infeasible for hardware platforms, such as FPGAs and ASICs. In this paper, we identify this critical issue, which arises from the mismatch between algorithm and hardware designers. To address this issue, we proposed PeZO, a perturbation-efficient ZO framework. Specifically, we design random number reuse strategies to significantly reduce the demand for random number generation and introduce a hardware-friendly adaptive scaling method to replace the costly Gaussian distribution with a uniform distribution. Our experiments show that PeZO reduces the required LUTs and FFs for random number generation by 48.6\\% and 12.7\\%, and saves at maximum 86\\% power consumption, all without compromising training performance, making ZO optimization feasible for on-device training. To the best of our knowledge, we are the first to explore the potential of on-device ZO optimization, providing valuable insights for future research.","authors":["Qitao Tan","Sung-En Chang","Rui Xia","Huidong Ji","Chence Yang","Ci Zhang","Jun Liu","Zheng Zhan","Zhou Zou","Yanzhi Wang","Jin Lu","Geng Yuan"],"url":"https://arxiv.org/abs/2504.20314"}
{"created":"2025-04-30","title":"Leveraging Action Relational Structures for Integrated Learning and Planning","abstract":"Recent advances in planning have explored using learning methods to help planning. However, little attention has been given to adapting search algorithms to work better with learning systems. In this paper, we introduce partial-space search, a new search space for classical planning that leverages the relational structure of actions given by PDDL action schemas -- a structure overlooked by traditional planning approaches. Partial-space search provides a more granular view of the search space and allows earlier pruning of poor actions compared to state-space search. To guide partial-space search, we introduce action set heuristics that evaluate sets of actions in a state. We describe how to automatically convert existing heuristics into action set heuristics. We also train action set heuristics from scratch using large training datasets from partial-space search. Our new planner, LazyLifted, exploits our better integrated search and learning heuristics and outperforms the state-of-the-art ML-based heuristic on IPC 2023 learning track (LT) benchmarks. We also show the efficiency of LazyLifted on high-branching factor tasks and show that it surpasses LAMA in the combined IPC 2023 LT and high-branching factor benchmarks.","authors":["Ryan Xiao Wang","Felipe Trevizan"],"url":"https://arxiv.org/abs/2504.20318"}
{"created":"2025-04-30","title":"Bayesian Experimental Design for Model Discrepancy Calibration: An Auto-Differentiable Ensemble Kalman Inversion Approach","abstract":"Bayesian experimental design (BED) offers a principled framework for optimizing data acquisition by leveraging probabilistic inference. However, practical implementations of BED are often compromised by model discrepancy, i.e., the mismatch between predictive models and true physical systems, which can potentially lead to biased parameter estimates. While data-driven approaches have been recently explored to characterize the model discrepancy, the resulting high-dimensional parameter space poses severe challenges for both Bayesian updating and design optimization. In this work, we propose a hybrid BED framework enabled by auto-differentiable ensemble Kalman inversion (AD-EKI) that addresses these challenges by providing a computationally efficient, gradient-free alternative to estimate the information gain for high-dimensional network parameters. The AD-EKI allows a differentiable evaluation of the utility function in BED and thus facilitates the use of standard gradient-based methods for design optimization. In the proposed hybrid framework, we iteratively optimize experimental designs, decoupling the inference of low-dimensional physical parameters handled by standard BED methods, from the high-dimensional model discrepancy handled by AD-EKI. The identified optimal designs for the model discrepancy enable us to systematically collect informative data for its calibration. The performance of the proposed method is studied by a classical convection-diffusion BED example, and the hybrid framework enabled by AD-EKI efficiently identifies informative data to calibrate the model discrepancy and robustly infers the unknown physical parameters in the modeled system. Besides addressing the challenges of BED with model discrepancy, AD-EKI also potentially fosters efficient and scalable frameworks in many other areas with bilevel optimization, such as meta-learning and structure optimization.","authors":["Huchen Yang","Xinghao Dong","Jin-Long Wu"],"url":"https://arxiv.org/abs/2504.20319"}
{"created":"2025-04-30","title":"\"I've talked to ChatGPT about my issues last night.\": Examining Mental Health Conversations with Large Language Models through Reddit Analysis","abstract":"We investigate the role of large language models (LLMs) in supporting mental health by analyzing Reddit posts and comments about mental health conversations with ChatGPT. Our findings reveal that users value ChatGPT as a safe, non-judgmental space, often favoring it over human support due to its accessibility, availability, and knowledgeable responses. ChatGPT provides a range of support, including actionable advice, emotional support, and validation, while helping users better understand their mental states. Additionally, we found that ChatGPT offers innovative support for individuals facing mental health challenges, such as assistance in navigating difficult conversations, preparing for therapy sessions, and exploring therapeutic interventions. However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT's overly validating nature, and privacy concerns. We discuss the implications of LLMs as tools for mental health support in both everyday health and clinical therapy settings and suggest strategies to mitigate risks in LLM-powered interactions.","authors":["Kyuha Jung","Gyuho Lee","Yuanhui Huang","Yunan Chen"],"url":"https://arxiv.org/abs/2504.20320"}
{"created":"2025-04-30","title":"Fine Grain Classification: Connecting Meta using Cross-Contrastive pre-training","abstract":"Fine-grained visual classification aims to recognize objects belonging to multiple subordinate categories within a super-category. However, this remains a challenging problem, as appearance information alone is often insufficient to accurately differentiate between fine-grained visual categories. To address this, we propose a novel and unified framework that leverages meta-information to assist fine-grained identification. We tackle the joint learning of visual and meta-information through cross-contrastive pre-training. In the first stage, we employ three encoders for images, text, and meta-information, aligning their projected embeddings to achieve better representations. We then fine-tune the image and meta-information encoders for the classification task. Experiments on the NABirds dataset demonstrate that our framework effectively utilizes meta-information to enhance fine-grained recognition performance. With the addition of meta-information, our framework surpasses the current baseline on NABirds by 7.83%. Furthermore, it achieves an accuracy of 84.44% on the NABirds dataset, outperforming many existing state-of-the-art approaches that utilize meta-information.","authors":["Sumit Mamtani","Yash Thesia"],"url":"https://arxiv.org/abs/2504.20322"}
{"created":"2025-04-30","title":"Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation","abstract":"This report addresses the challenge of limited labeled datasets for developing legal recommender systems, particularly in specialized domains like labor disputes. We propose a new approach leveraging the co-citation of legal articles within cases to establish similarity and enable algorithmic annotation. This method draws a parallel to the concept of case co-citation, utilizing cited precedents as indicators of shared legal issues. To evaluate the labeled results, we employ a system that recommends similar cases based on plaintiffs' accusations, defendants' rebuttals, and points of disputes. The evaluation demonstrates that the recommender, with finetuned text embedding models and a reasonable BiLSTM module can recommend labor cases whose similarity was measured by the co-citation of the legal articles. This research contributes to the development of automated annotation techniques for legal documents, particularly in areas with limited access to comprehensive legal databases.","authors":["Chao-Lin Liu","Po-Hsien Wu","Yi-Ting Yu"],"url":"https://arxiv.org/abs/2504.20323"}
{"created":"2025-04-30","title":"NMPC-based Unified Posture Manipulation and Thrust Vectoring for Agile and Fault-Tolerant Flight of a Morphing Aerial Robot","abstract":"This thesis presents a unified control framework for agile and fault-tolerant flight of the Multi-Modal Mobility Morphobot (M4) in aerial mode. The M4 robot is capable of transitioning between ground and aerial locomotion. The articulated legs enable more dynamic maneuvers than a standard quadrotor platform. A nonlinear model predictive control (NMPC) approach is developed to simultaneously plan posture manipulation and thrust vectoring actions, allowing the robot to execute sharp turns and dynamic flight trajectories. The framework integrates an agile and fault-tolerant control logic that enables precise tracking under aggressive maneuvers while compensating for actuator failures, ensuring continued operation without significant performance degradation. Simulation results validate the effectiveness of the proposed method, demonstrating accurate trajectory tracking and robust recovery from faults, contributing to resilient autonomous flight in complex environments.","authors":["Shashwat Pandya"],"url":"https://arxiv.org/abs/2504.20326"}
{"created":"2025-04-30","title":"AI in Software Engineering: Perceived Roles and Their Impact on Adoption","abstract":"This paper investigates how developers conceptualize AI-powered Development Tools and how these role attributions influence technology acceptance. Through qualitative analysis of 38 interviews and a quantitative survey with 102 participants, we identify two primary Mental Models: AI as an inanimate tool and AI as a human-like teammate. Factor analysis further groups AI roles into Support Roles (e.g., assistant, reference guide) and Expert Roles (e.g., advisor, problem solver). We find that assigning multiple roles to AI correlates positively with Perceived Usefulness and Perceived Ease of Use, indicating that diverse conceptualizations enhance AI adoption. These insights suggest that AI4SE tools should accommodate varying user expectations through adaptive design strategies that align with different Mental Models.","authors":["Ilya Zakharov","Ekaterina Koshchenko","Agnia Sergeyuk"],"url":"https://arxiv.org/abs/2504.20329"}
{"created":"2025-04-30","title":"Method Names in Jupyter Notebooks: An Exploratory Study","abstract":"Method names play an important role in communicating the purpose and behavior of their functionality. Research has shown that high-quality names significantly improve code comprehension and the overall maintainability of software. However, these studies primarily focus on naming practices in traditional software development. There is limited research on naming patterns in Jupyter Notebooks, a popular environment for scientific computing and data analysis. In this exploratory study, we analyze the naming practices found in 691 methods across 384 Jupyter Notebooks, focusing on three key aspects: naming style conventions, grammatical composition, and the use of abbreviations and acronyms. Our findings reveal distinct characteristics of notebook method names, including a preference for conciseness and deviations from traditional naming patterns. We identified 68 unique grammatical patterns, with only 55.57% of methods beginning with a verb. Further analysis revealed that half of the methods with return statements do not start with a verb. We also found that 30.39% of method names contain abbreviations or acronyms, representing mathematical or statistical terms and image processing concepts, among others. We envision our findings contributing to developing specialized tools and techniques for evaluating and recommending high-quality names in scientific code and creating educational resources tailored to the notebook development community.","authors":["Carol Wong","Gunnar Larsen","Rocky Huang","Bonita Sharif","Anthony Peruma"],"url":"https://arxiv.org/abs/2504.20330"}
{"created":"2025-04-30","title":"List Decoding Expander-Based Codes up to Capacity in Near-Linear Time","abstract":"We give a new framework based on graph regularity lemmas, for list decoding and list recovery of codes based on spectral expanders. Using existing algorithms for computing regularity decompositions of sparse graphs in (randomized) near-linear time, and appropriate choices for the constant-sized inner/base codes, we prove the following:","authors":["Shashank Srivastava","Madhur Tulsiani"],"url":"https://arxiv.org/abs/2504.20333"}
{"created":"2025-04-30","title":"VA-CDH: A Variance-Aware Method to Optimize Latency for Caching with Delayed Hits","abstract":"Caches are fundamental to latency-sensitive systems like Content Delivery Networks (CDNs) and Mobile Edge Computing (MEC). However, the delayed hit phenomenon where multiple requests for an object occur during its fetch from the remote server after a miss significantly inflates user-perceived latency. While recent algorithms acknowledge delayed hits by estimating the resulting aggregate delay, they predominantly focus on its mean value. We identify and demonstrate that such approaches are insufficient, as the real aggregate delay frequently exhibits substantial variance in the true production system, leading to suboptimal latency performance when ignored. Thus, we propose VA-CDH, a variance-aware method to optimize latency for caching with delayed hits. It employs a novel ranking function that explicitly incorporates both the empirically estimated mean and standard deviation of aggregate delay, allowing caching decisions to account for its variation. We derive the analytical distribution of aggregate delay under Poisson arrivals as a theoretical contribution, offering more statistical insight beyond the mean value. Through the simulations conducted on synthetic and real-world datasets, we show that VA-CDH reduces the total latency by 1%-6% approximately compared to state-of-the-art algorithms.","authors":["Bowen Jiang","Chaofan Ma","Duo Wang"],"url":"https://arxiv.org/abs/2504.20335"}
{"created":"2025-04-30","title":"DRO: Doppler-Aware Direct Radar Odometry","abstract":"A renaissance in radar-based sensing for mobile robotic applications is underway. Compared to cameras or lidars, millimetre-wave radars have the ability to `see' through thin walls, vegetation, and adversarial weather conditions such as heavy rain, fog, snow, and dust. In this paper, we propose a novel SE(2) odometry approach for spinning frequency-modulated continuous-wave radars. Our method performs scan-to-local-map registration of the incoming radar data in a direct manner using all the radar intensity information without the need for feature or point cloud extraction. The method performs locally continuous trajectory estimation and accounts for both motion and Doppler distortion of the radar scans. If the radar possesses a specific frequency modulation pattern that makes radial Doppler velocities observable, an additional Doppler-based constraint is formulated to improve the velocity estimate and enable odometry in geometrically feature-deprived scenarios (e.g., featureless tunnels). Our method has been validated on over 250km of on-road data sourced from public datasets (Boreas and MulRan) and collected using our automotive platform. With the aid of a gyroscope, it outperforms state-of-the-art methods and achieves an average relative translation error of 0.26% on the Boreas leaderboard. When using data with the appropriate Doppler-enabling frequency modulation pattern, the translation error is reduced to 0.18% in similar environments. We also benchmarked our algorithm using 1.5 hours of data collected with a mobile robot in off-road environments with various levels of structure to demonstrate its versatility. Our real-time implementation is publicly available: https://github.com/utiasASRL/dro.","authors":["Cedric Le Gentil","Leonardo Brizi","Daniil Lisus","Xinyuan Qiao","Giorgio Grisetti","Timothy D. Barfoot"],"url":"https://arxiv.org/abs/2504.20339"}
{"created":"2025-04-30","title":"A Picture is Worth a Thousand Prompts? Efficacy of Iterative Human-Driven Prompt Refinement in Image Regeneration Tasks","abstract":"With AI-generated content becoming ubiquitous across the web, social media, and other digital platforms, it is vital to examine how such content are inspired and generated. The creation of AI-generated images often involves refining the input prompt iteratively to achieve desired visual outcomes. This study focuses on the relatively underexplored concept of image regeneration using AI, in which a human operator attempts to closely recreate a specific target image by iteratively refining their prompt. Image regeneration is distinct from normal image generation, which lacks any predefined visual reference. A separate challenge lies in determining whether existing image similarity metrics (ISMs) can provide reliable, objective feedback in iterative workflows, given that we do not fully understand if subjective human judgments of similarity align with these metrics. Consequently, we must first validate their alignment with human perception before assessing their potential as a feedback mechanism in the iterative prompt refinement process. To address these research gaps, we present a structured user study evaluating how iterative prompt refinement affects the similarity of regenerated images relative to their targets, while also examining whether ISMs capture the same improvements perceived by human observers. Our findings suggest that incremental prompt adjustments substantially improve alignment, verified through both subjective evaluations and quantitative measures, underscoring the broader potential of iterative workflows to enhance generative AI content creation across various application domains.","authors":["Khoi Trinh","Scott Seidenberger","Raveen Wijewickrama","Murtuza Jadliwala","Anindya Maiti"],"url":"https://arxiv.org/abs/2504.20340"}
{"created":"2025-04-30","title":"Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI","abstract":"Reflexion is an AI-powered platform designed to enable structured emotional self-reflection at scale. By integrating real-time emotion detection, layered reflective prompting, and metaphorical storytelling generation, Reflexion empowers users to engage in autonomous emotional exploration beyond basic sentiment categorization. Grounded in theories of expressive writing, cognitive restructuring, self-determination, and critical consciousness development, the system scaffolds a progressive journey from surface-level emotional recognition toward value-aligned action planning. Initial pilot studies with diverse participants demonstrate positive outcomes in emotional articulation, cognitive reframing, and perceived psychological resilience. Reflexion represents a promising direction for scalable, theory-informed affective computing interventions aimed at fostering emotional literacy and psychological growth across educational, therapeutic, and public health contexts.","authors":["Shou-Tzu Han"],"url":"https://arxiv.org/abs/2504.20342"}
{"created":"2025-04-30","title":"MicarVLMoE: A Modern Gated Cross-Aligned Vision-Language Mixture of Experts Model for Medical Image Captioning and Report Generation","abstract":"Medical image reporting (MIR) aims to generate structured clinical descriptions from radiological images. Existing methods struggle with fine-grained feature extraction, multimodal alignment, and generalization across diverse imaging types, often relying on vanilla transformers and focusing primarily on chest X-rays. We propose MicarVLMoE, a vision-language mixture-of-experts model with gated cross-aligned fusion, designed to address these limitations. Our architecture includes: (i) a multiscale vision encoder (MSVE) for capturing anatomical details at varying resolutions, (ii) a multihead dual-branch latent attention (MDLA) module for vision-language alignment through latent bottleneck representations, and (iii) a modulated mixture-of-experts (MoE) decoder for adaptive expert specialization. We extend MIR to CT scans, retinal imaging, MRI scans, and gross pathology images, reporting state-of-the-art results on COVCTR, MMR, PGROSS, and ROCO datasets. Extensive experiments and ablations confirm improved clinical accuracy, cross-modal alignment, and model interpretability. Code is available at https://github.com/AI-14/micar-vl-moe.","authors":["Amaan Izhar","Nurul Japar","Norisma Idris","Ting Dang"],"url":"https://arxiv.org/abs/2504.20343"}
{"created":"2025-04-30","title":"Clustering-Based Evolutionary Federated Multiobjective Optimization and Learning","abstract":"Federated learning enables decentralized model training while preserving data privacy, yet it faces challenges in balancing communication efficiency, model performance, and privacy protection. To address these trade-offs, we formulate FL as a federated multiobjective optimization problem and propose FedMOEAC, a clustering-based evolutionary algorithm that efficiently navigates the Pareto-optimal solution space. Our approach integrates quantization, weight sparsification, and differential privacy to reduce communication overhead while ensuring model robustness and privacy. The clustering mechanism en-hances population diversity, preventing premature convergence and improving optimization efficiency. Experimental results on MNIST and CIFAR-10 demonstrate that FedMOEAC achieves 98.2% accuracy, reduces communication overhead by 45%, and maintains a privacy budget below 1.0, outperforming NSGA-II in convergence speed by 33%. This work provides a scalable and efficient FL framework, ensuring an optimal balance between accuracy, communication efficiency, and privacy in resource-constrained environments.","authors":["Chengui Xiao","Songbai Liu"],"url":"https://arxiv.org/abs/2504.20346"}
{"created":"2025-04-30","title":"CarbonCall: Sustainability-Aware Function Calling for Large Language Models on Edge Devices","abstract":"Large Language Models (LLMs) enable real-time function calling in edge AI systems but introduce significant computational overhead, leading to high power consumption and carbon emissions. Existing methods optimize for performance while neglecting sustainability, making them inefficient for energy-constrained environments. We introduce CarbonCall, a sustainability-aware function-calling framework that integrates dynamic tool selection, carbon-aware execution, and quantized LLM adaptation. CarbonCall adjusts power thresholds based on real-time carbon intensity forecasts and switches between model variants to sustain high tokens-per-second throughput under power constraints. Experiments on an NVIDIA Jetson AGX Orin show that CarbonCall reduces carbon emissions by up to 52%, power consumption by 30%, and execution time by 30%, while maintaining high efficiency.","authors":["Varatheepan Paramanayakam","Andreas Karatzas","Iraklis Anagnostopoulos","Dimitrios Stamoulis"],"url":"https://arxiv.org/abs/2504.20348"}
{"created":"2025-04-30","title":"SoK: Enhancing Privacy-Preserving Software Development from a Developers' Perspective","abstract":"In software development, privacy preservation has become essential with the rise of privacy concerns and regulations such as GDPR and CCPA. While several tools, guidelines, methods, methodologies, and frameworks have been proposed to support developers embedding privacy into software applications, most of them are proofs-of-concept without empirical evaluations, making their practical applicability uncertain. These solutions should be evaluated for different types of scenarios (e.g., industry settings such as rapid software development environments, teams with different privacy knowledge, etc.) to determine what their limitations are in various industry settings and what changes are required to refine current solutions before putting them into industry and developing new developer-supporting approaches. For that, a thorough review of empirically evaluated current solutions will be very effective. However, the existing secondary studies that examine the available developer support provide broad overviews but do not specifically analyze empirically evaluated solutions and their limitations. Therefore, this Systematic Literature Review (SLR) aims to identify and analyze empirically validated solutions that are designed to help developers in privacy-preserving software development. The findings will provide valuable insights for researchers to improve current privacy-preserving solutions and for practitioners looking for effective and validated solutions to embed privacy into software development.","authors":["Tharaka Wijesundara","Nalin Asanka Gamagedara Arachchilage","Matthew Warren"],"url":"https://arxiv.org/abs/2504.20350"}
{"created":"2025-04-30","title":"Local Prompt Optimization","abstract":"In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization space (tokens) leads to insufficient guidance for a better prompt. In this work, we introduce Local Prompt Optimization (LPO) that integrates with any general automatic prompt engineering method. We identify the optimization tokens in a prompt and nudge the LLM to focus only on those tokens in its optimization step. We observe remarkable performance improvements on Math Reasoning (GSM8k and MultiArith) and BIG-bench Hard benchmarks across various automatic prompt engineering methods. Further, we show that LPO converges to the optimal prompt faster than global methods.","authors":["Yash Jain","Vishal Chowdhary"],"url":"https://arxiv.org/abs/2504.20355"}
{"created":"2025-04-30","title":"What Causes Knowledge Loss in Multilingual Language Models?","abstract":"Cross-lingual transfer in natural language processing (NLP) models enhances multilingual performance by leveraging shared linguistic knowledge. However, traditional methods that process all data simultaneously often fail to mimic real-world scenarios, leading to challenges like catastrophic forgetting, where fine-tuning on new tasks degrades performance on previously learned ones. Our study explores this issue in multilingual contexts, focusing on linguistic differences affecting representational learning rather than just model parameters. We experiment with 52 languages using LoRA adapters of varying ranks to evaluate non-shared, partially shared, and fully shared parameters. Our aim is to see if parameter sharing through adapters can mitigate forgetting while preserving prior knowledge. We find that languages using non-Latin scripts are more susceptible to catastrophic forgetting, whereas those written in Latin script facilitate more effective cross-lingual transfer.","authors":["Maria Khelli","Samuel Cahyawijaya","Ayu Purwarianti","Genta Indra Winata"],"url":"https://arxiv.org/abs/2504.20356"}
{"created":"2025-04-30","title":"Automated Unit Test Case Generation: A Systematic Literature Review","abstract":"Software is omnipresent within all factors of society. It is thus important to ensure that software are well tested to mitigate bad user experiences as well as the potential for severe financial and human losses. Software testing is however expensive and absorbs valuable time and resources. As a result, the field of automated software testing has grown of interest to researchers in past decades. In our review of present and past research papers, we have identified an information gap in the areas of improvement for the Genetic Algorithm and Particle Swarm Optimisation. A gap in knowledge in the current challenges that face automated testing has also been identified. We therefore present this systematic literature review in an effort to consolidate existing knowledge in regards to the evolutionary approaches as well as their improvements and resulting limitations. These improvements include hybrid algorithm combinations as well as interoperability with mutation testing and neural networks. We will also explore the main test criterion that are used in these algorithms alongside the challenges currently faced in the field related to readability, mocking and more.","authors":["Jason Wang","Basem Suleiman","Muhammad Johan Alibasa"],"url":"https://arxiv.org/abs/2504.20357"}
{"created":"2025-04-30","title":"PRISM-DP: Spatial Pose-based Observations for Diffusion-Policies via Segmentation, Mesh Generation, and Pose Tracking","abstract":"Diffusion-based visuomotor policies generate robot motions by learning to denoise action-space trajectories conditioned on observations. These observations are commonly streams of RGB images, whose high dimensionality includes substantial task-irrelevant information, requiring large models to extract relevant patterns. In contrast, using more structured observations, such as the spatial poses (positions and orientations) of key objects over time, enables training more compact policies that can recognize relevant patterns with fewer parameters. However, obtaining accurate object poses in open-set, real-world environments remains challenging. For instance, it is impractical to assume that all relevant objects are equipped with markers, and recent learning-based 6D pose estimation and tracking methods often depend on pre-scanned object meshes, requiring manual reconstruction. In this work, we propose PRISM-DP, an approach that leverages segmentation, mesh generation, pose estimation, and pose tracking models to enable compact diffusion policy learning directly from the spatial poses of task-relevant objects. Crucially, because PRISM-DP uses a mesh generation model, it eliminates the need for manual mesh processing or creation, improving scalability and usability in open-set, real-world environments. Experiments across a range of tasks in both simulation and real-world settings show that PRISM-DP outperforms high-dimensional image-based diffusion policies and achieves performance comparable to policies trained with ground-truth state information. We conclude with a discussion of the broader implications and limitations of our approach.","authors":["Xiatao Sun","Yinxing Chen","Daniel Rakita"],"url":"https://arxiv.org/abs/2504.20359"}
{"created":"2025-04-30","title":"TTTFusion: A Test-Time Training-Based Strategy for Multimodal Medical Image Fusion in Surgical Robots","abstract":"With the increasing use of surgical robots in clinical practice, enhancing their ability to process multimodal medical images has become a key research challenge. Although traditional medical image fusion methods have made progress in improving fusion accuracy, they still face significant challenges in real-time performance, fine-grained feature extraction, and edge preservation.In this paper, we introduce TTTFusion, a Test-Time Training (TTT)-based image fusion strategy that dynamically adjusts model parameters during inference to efficiently fuse multimodal medical images. By adapting the model during the test phase, our method optimizes the parameters based on the input image data, leading to improved accuracy and better detail preservation in the fusion results.Experimental results demonstrate that TTTFusion significantly enhances the fusion quality of multimodal images compared to traditional fusion methods, particularly in fine-grained feature extraction and edge preservation. This approach not only improves image fusion accuracy but also offers a novel technical solution for real-time image processing in surgical robots.","authors":["Qinhua Xie","Hao Tang"],"url":"https://arxiv.org/abs/2504.20362"}
{"created":"2025-04-30","title":"Thoughtful, Confused, or Untrustworthy: How Text Presentation Influences Perceptions of AI Writing Tools","abstract":"AI writing tools have been shown to dramatically change the way people write, yet the effects of AI text presentation are not well understood nor always intentionally designed. Although text presentation in existing large language model interfaces is linked to the speed of the underlying model, text presentation speed can impact perceptions of AI systems, potentially influencing whether AI suggestions are accepted or rejected. In this paper, we analyze the effects of varying text generation speed in creative and professional writing scenarios on an online platform (n=297). We find that speed is correlated with perceived humanness and trustworthiness of the AI tool, as well as the perceived quality of the generated text. We discuss its implications on creative and writing processes, along with future steps in the intentional design of AI writing tool interfaces.","authors":["David Zhou","John R. Gallagher","Sarah Sterman"],"url":"https://arxiv.org/abs/2504.20365"}
{"created":"2025-04-30","title":"Theoretical Grid-Forming Extreme of Inverters","abstract":"What are the theoretical and physical limits of a grid-forming inverter? This letter proposes that the extreme grid-forming ability of inverters is limited by their dc-side, ac-side, circuit topology dynamics, but not control. While many papers focus on how to improve grid-forming inverters stability, power sharing, inertia emulation, fault response, few, if any, formally define the fundamental theoretical limits or extremes of grid-forming behavior. It seems that the grid-forming can be improved endlessly. No physical system can support a grid indefinitely without limitations, especially under increasing levels of disturbance or uncertainty. Therefore, this boundary is explicitly shown by a mathematical expression in this letter. Consequently, the results show that relatively low dc-side voltage and high active power injection could damage the grid-forming ability. Poor consideration of dc-side, ac-side, and circuit topology dynamics in real practice will cause jeopardizing oscillation even by the theoretical best grid-forming control strategy.","authors":["Qianxi Tang","Li Peng"],"url":"https://arxiv.org/abs/2504.20367"}
{"created":"2025-04-30","title":"AKIBoards: A Structure-Following Multiagent System for Predicting Acute Kidney Injury","abstract":"Diagnostic reasoning entails a physician's local (mental) model based on an assumed or known shared perspective (global model) to explain patient observations with evidence assigned towards a clinical assessment. But in several (complex) medical situations, multiple experts work together as a team to optimize health evaluation and decision-making by leveraging different perspectives. Such consensus-driven reasoning reflects individual knowledge contributing toward a broader perspective on the patient. In this light, we introduce STRUCture-following for Multiagent Systems (STRUC-MAS), a framework automating the learning of these global models and their incorporation as prior beliefs for agents in multiagent systems (MAS) to follow. We demonstrate proof of concept with a prosocial MAS application for predicting acute kidney injuries (AKIs). In this case, we found that incorporating a global structure enabled multiple agents to achieve better performance (average precision, AP) in predicting AKI 48 hours before onset (structure-following-fine-tuned, SF-FT, AP=0.195; SF-FT-retrieval-augmented generation, SF-FT-RAG, AP=0.194) vs. baseline (non-structure-following-FT, NSF-FT, AP=0.141; NSF-FT-RAG, AP=0.180) for balanced precision-weighted-recall-weighted voting. Markedly, SF-FT agents with higher recall scores reported lower confidence levels in the initial round on true positive and false negative cases. But after explicit interactions, their confidence in their decisions increased (suggesting reinforced belief). In contrast, the SF-FT agent with the lowest recall decreased its confidence in true positive and false negative cases (suggesting a new belief). This approach suggests that learning and leveraging global structures in MAS is necessary prior to achieving competitive classification and diagnostic reasoning performance.","authors":["David Gordon","Panayiotis Petousis","Susanne B. Nicholas","Alex A. T. Bui"],"url":"https://arxiv.org/abs/2504.20368"}
{"created":"2025-04-30","title":"Perception-aware Sampling for Scatterplot Visualizations","abstract":"Visualizing data is often a crucial first step in data analytics workflows, but growing data sizes pose challenges due to computational and visual perception limitations. As a result, data analysts commonly down-sample their data and work with subsets. Deriving representative samples, however, remains a challenge. This paper focuses on scatterplots, a widely-used visualization type, and introduces a novel sampling objective -- perception-awareness -- aiming to improve sample efficacy by targeting humans' perception of a visualization.","authors":["Zafeiria Moumoulidou","Hamza Elhamdadi","Ke Yang","Subrata Mitra","Cindy Xiong Bearfield","Alexandra Meliou"],"url":"https://arxiv.org/abs/2504.20369"}
{"created":"2025-04-30","title":"ABO: Abandon Bayer Filter for Adaptive Edge Offloading in Responsive Augmented Reality","abstract":"Bayer-patterned color filter array (CFA) has been the go-to solution for color image sensors. In augmented reality (AR), although color interpolation (i.e., demosaicing) of pre-demosaic RAW images facilitates a user-friendly rendering, it creates no benefits in offloaded DNN analytics but increases the image channels by 3 times inducing higher transmission overheads. The potential optimization in frame preprocessing of DNN offloading is yet to be investigated. To that end, we propose ABO, an adaptive RAW frame offloading framework that parallelizes demosaicing with DNN computation. Its contributions are three-fold: First, we design a configurable tile-wise RAW image neural codec to compress frame sizes while sustaining downstream DNN accuracy under bandwidth constraints. Second, based on content-aware tiles-in-frame selection and runtime bandwidth estimation, a dynamic transmission controller adaptively calibrates codec configurations to maximize the DNN accuracy. Third, we further optimize the system pipelining to achieve lower end-to-end frame processing latency and higher throughput. Through extensive evaluations on a prototype platform, ABO consistently achieves 40% more frame processing throughput and 30% less end-to-end latency while improving the DNN accuracy by up to 15% than SOTA baselines. It also exhibits improved robustness against dim lighting and motion blur situations.","authors":["Yongxuan Han","Shengzhong Liu","Fan Wu","Guihai Chen"],"url":"https://arxiv.org/abs/2504.20370"}
{"created":"2025-04-30","title":"DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation","abstract":"Currently, Large Language Models (LLMs) have achieved remarkable results in machine translation. However, their performance in multi-domain translation (MDT) is less satisfactory; the meanings of words can vary across different domains, highlighting the significant ambiguity inherent in MDT. Therefore, evaluating the disambiguation ability of LLMs in MDT remains an open problem. To this end, we present an evaluation and analysis of LLMs on disambiguation in multi-domain translation (DMDTEval), our systematic evaluation framework consisting of three critical aspects: (1) we construct a translation test set with multi-domain ambiguous word annotation, (2) we curate a diverse set of disambiguation prompting templates, and (3) we design precise disambiguation metrics, and study the efficacy of various prompting strategies on multiple state-of-the-art LLMs. Our extensive experiments reveal a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the disambiguation of LLMs.","authors":["Zhibo Man","Yuanmeng Chen","Yujie Zhang","Yufeng Chen","Jinan Xu"],"url":"https://arxiv.org/abs/2504.20371"}
{"created":"2025-04-30","title":"Approximately Dominating Sets in Elections","abstract":"Condorcet's paradox is a fundamental result in social choice theory which states that there exist elections in which, no matter which candidate wins, a majority of voters prefer a different candidate. In fact, even if we can select any $k$ winners, there still may exist another candidate that would beat each of the winners in a majority vote. That is, elections may require arbitrarily large dominating sets.","authors":["Moses Charikar","Prasanna Ramakrishnan","Kangning Wang"],"url":"https://arxiv.org/abs/2504.20372"}
{"created":"2025-04-30","title":"Estimation of Tissue Deformation and Interactive Force in Robotic Surgery through Vision-based Learning","abstract":"Goal: A limitation in robotic surgery is the lack of force feedback, due to challenges in suitable sensing techniques. To enhance the perception of the surgeons and precise force rendering, estimation of these forces along with tissue deformation level is presented here. Methods: An experimental test bed is built for studying the interaction, and the forces are estimated from the raw data. Since tissue deformation and stiffness are non-linearly related, they are independently computed for enhanced reliability. A Convolutional Neural Network (CNN) based vision model is deployed, and both classification and regression models are developed. Results: The forces applied on the tissue are estimated, and the tissue is classified based on its deformation. The exact deformation of the tissue is also computed. Conclusions: The surgeons can render precise forces and detect tumors using the proposed method. The rarely discussed efficacy of computing the deformation level is also demonstrated.","authors":["Srikar Annamraju","Yuxi Chen","Jooyoung Lim","Inki Kim"],"url":"https://arxiv.org/abs/2504.20373"}
{"created":"2025-04-30","title":"Generative Learning for Slow Manifolds and Bifurcation Diagrams","abstract":"In dynamical systems characterized by separation of time scales, the approximation of so called ``slow manifolds'', on which the long term dynamics lie, is a useful step for model reduction. Initializing on such slow manifolds is a useful step in modeling, since it circumvents fast transients, and is crucial in multiscale algorithms alternating between fine scale (fast) and coarser scale (slow) simulations. In a similar spirit, when one studies the infinite time dynamics of systems depending on parameters, the system attractors (e.g., its steady states) lie on bifurcation diagrams. Sampling these manifolds gives us representative attractors (here, steady states of ODEs or PDEs) at different parameter values. Algorithms for the systematic construction of these manifolds are required parts of the ``traditional'' numerical nonlinear dynamics toolkit.","authors":["Ellis R. Crabtree","Dimitris G. Giovanis","Nikolaos Evangelou","Juan M. Bello-Rivas","Ioannis G. Kevrekidis"],"url":"https://arxiv.org/abs/2504.20375"}
{"created":"2025-04-30","title":"Inception: Jailbreak the Memory Mechanism of Text-to-Image Generation Systems","abstract":"Currently, the memory mechanism has been widely and successfully exploited in online text-to-image (T2I) generation systems ($e.g.$, DALL$\\cdot$E 3) for alleviating the growing tokenization burden and capturing key information in multi-turn interactions. Despite its practicality, its security analyses have fallen far behind. In this paper, we reveal that this mechanism exacerbates the risk of jailbreak attacks. Different from previous attacks that fuse the unsafe target prompt into one ultimate adversarial prompt, which can be easily detected or may generate non-unsafe images due to under- or over-optimization, we propose Inception, the first multi-turn jailbreak attack against the memory mechanism in real-world text-to-image generation systems. Inception embeds the malice at the inception of the chat session turn by turn, leveraging the mechanism that T2I generation systems retrieve key information in their memory. Specifically, Inception mainly consists of two modules. It first segments the unsafe prompt into chunks, which are subsequently fed to the system in multiple turns, serving as pseudo-gradients for directive optimization. Specifically, we develop a series of segmentation policies that ensure the images generated are semantically consistent with the target prompt. Secondly, after segmentation, to overcome the challenge of the inseparability of minimum unsafe words, we propose recursion, a strategy that makes minimum unsafe words subdivisible. Collectively, segmentation and recursion ensure that all the request prompts are benign but can lead to malicious outcomes. We conduct experiments on the real-world text-to-image generation system ($i.e.$, DALL$\\cdot$E 3) to validate the effectiveness of Inception. The results indicate that Inception surpasses the state-of-the-art by a 14\\% margin in attack success rate.","authors":["Shiqian Zhao","Jiayang Liu","Yiming Li","Runyi Hu","Xiaojun Jia","Wenshu Fan","Xinfeng Li","Jie Zhang","Wei Dong","Tianwei Zhang","Luu Anh Tuan"],"url":"https://arxiv.org/abs/2504.20376"}
{"created":"2025-04-30","title":"Sparse2DGS: Geometry-Prioritized Gaussian Splatting for Surface Reconstruction from Sparse Views","abstract":"We present a Gaussian Splatting method for surface reconstruction using sparse input views. Previous methods relying on dense views struggle with extremely sparse Structure-from-Motion points for initialization. While learning-based Multi-view Stereo (MVS) provides dense 3D points, directly combining it with Gaussian Splatting leads to suboptimal results due to the ill-posed nature of sparse-view geometric optimization. We propose Sparse2DGS, an MVS-initialized Gaussian Splatting pipeline for complete and accurate reconstruction. Our key insight is to incorporate the geometric-prioritized enhancement schemes, allowing for direct and robust geometric learning under ill-posed conditions. Sparse2DGS outperforms existing methods by notable margins while being ${2}\\times$ faster than the NeRF-based fine-tuning approach.","authors":["Jiang Wu","Rui Li","Yu Zhu","Rong Guo","Jinqiu Sun","Yanning Zhang"],"url":"https://arxiv.org/abs/2504.20378"}
{"created":"2025-04-30","title":"GSFeatLoc: Visual Localization Using Feature Correspondence on 3D Gaussian Splatting","abstract":"In this paper, we present a method for localizing a query image with respect to a precomputed 3D Gaussian Splatting (3DGS) scene representation. First, the method uses 3DGS to render a synthetic RGBD image at some initial pose estimate. Second, it establishes 2D-2D correspondences between the query image and this synthetic image. Third, it uses the depth map to lift the 2D-2D correspondences to 2D-3D correspondences and solves a perspective-n-point (PnP) problem to produce a final pose estimate. Results from evaluation across three existing datasets with 38 scenes and over 2,700 test images show that our method significantly reduces both inference time (by over two orders of magnitude, from more than 10 seconds to as fast as 0.1 seconds) and estimation error compared to baseline methods that use photometric loss minimization. Results also show that our method tolerates large errors in the initial pose estimate of up to 55{\\deg} in rotation and 1.1 units in translation (normalized by scene scale), achieving final pose errors of less than 5{\\deg} in rotation and 0.05 units in translation on 90% of images from the Synthetic NeRF and Mip-NeRF360 datasets and on 42% of images from the more challenging Tanks and Temples dataset.","authors":["Jongwon Lee","Timothy Bretl"],"url":"https://arxiv.org/abs/2504.20379"}
{"created":"2025-04-30","title":"LPVIMO-SAM: Tightly-coupled LiDAR/Polarization Vision/Inertial/Magnetometer/Optical Flow Odometry via Smoothing and Mapping","abstract":"We propose a tightly-coupled LiDAR/Polarization Vision/Inertial/Magnetometer/Optical Flow Odometry via Smoothing and Mapping (LPVIMO-SAM) framework, which integrates LiDAR, polarization vision, inertial measurement unit, magnetometer, and optical flow in a tightly-coupled fusion. This framework enables high-precision and highly robust real-time state estimation and map construction in challenging environments, such as LiDAR-degraded, low-texture regions, and feature-scarce areas. The LPVIMO-SAM comprises two subsystems: a Polarized Vision-Inertial System and a LiDAR/Inertial/Magnetometer/Optical Flow System. The polarized vision enhances the robustness of the Visual/Inertial odometry in low-feature and low-texture scenarios by extracting the polarization information of the scene. The magnetometer acquires the heading angle, and the optical flow obtains the speed and height to reduce the accumulated error. A magnetometer heading prior factor, an optical flow speed observation factor, and a height observation factor are designed to eliminate the cumulative errors of the LiDAR/Inertial odometry through factor graph optimization. Meanwhile, the LPVIMO-SAM can maintain stable positioning even when one of the two subsystems fails, further expanding its applicability in LiDAR-degraded, low-texture, and low-feature environments. Code is available on https://github.com/junxiaofanchen/LPVIMO-SAM.","authors":["Derui Shan","Peng Guo","Wenshuo Li","Du Tao"],"url":"https://arxiv.org/abs/2504.20380"}
{"created":"2025-04-30","title":"An Empirical Study on Common Defects in Modern Web Browsers Using Knowledge Embedding in GPT-4o","abstract":"Technology is advancing at an unprecedented pace. With the advent of cutting-edge technologies, keeping up with rapid changes are becoming increasingly challenging. In addition to that, increasing dependencies on the cloud technologies have imposed enormous pressure on modern web browsers leading to adapting new technologies faster and making them more susceptible to defects/bugs. Although, many studies have explored browser bugs, a comparative study among the modern browsers generalizing the bug categories and their nature was still lacking. To fill this gap, we undertook an empirical investigation aimed at gaining insights into the prevalent bugs in Google Chromium and Mozilla Firefox as the representatives of modern web browsers. We used GPT-4.o to identify the defect (bugs) categories and analyze the clusters of the most commonly appeared bugs in the two prominent web browsers. Additionally, we compared our LLM based bug categorization with the traditional NLP based approach using TF-IDF and K-Means clustering. We found that although Google Chromium and Firefox have evolved together since almost around the same time (2006-2008), Firefox suffers from high number of bugs having extremely high defect-prone components compared to Chromium. This exploratory study offers valuable insights on the browser bugs and defect-prone components to the developers, enabling them to craft web browsers and web-applications with enhanced resilience and reduced errors.","authors":["Rahul Singh","Yousuf Sultan","Tajmilur Rahman","Sri Vidya Puttareddygari"],"url":"https://arxiv.org/abs/2504.20381"}
{"created":"2025-04-30","title":"Neural Stereo Video Compression with Hybrid Disparity Compensation","abstract":"Disparity compensation represents the primary strategy in stereo video compression (SVC) for exploiting cross-view redundancy. These mechanisms can be broadly categorized into two types: one that employs explicit horizontal shifting, and another that utilizes an implicit cross-attention mechanism to reduce cross-view disparity redundancy. In this work, we propose a hybrid disparity compensation (HDC) strategy that leverages explicit pixel displacement as a robust prior feature to simplify optimization and perform implicit cross-attention mechanisms for subsequent warping operations, thereby capturing a broader range of disparity information. Specifically, HDC first computes a similarity map by fusing the horizontally shifted cross-view features to capture pixel displacement information. This similarity map is then normalized into an \"explicit pixel-wise attention score\" to perform the cross-attention mechanism, implicitly aligning features from one view to another. Building upon HDC, we introduce a novel end-to-end optimized neural stereo video compression framework, which integrates HDC-based modules into key coding operations, including cross-view feature extraction and reconstruction (HDC-FER) and cross-view entropy modeling (HDC-EM). Extensive experiments on SVC benchmarks, including KITTI 2012, KITTI 2015, and Nagoya, which cover both autonomous driving and general scenes, demonstrate that our framework outperforms both neural and traditional SVC methodologies.","authors":["Shiyin Jiang","Zhenghao Chen","Minghao Han","Xingyu Zhou","Leheng Zhang","Shuhang Gu"],"url":"https://arxiv.org/abs/2504.20383"}
{"created":"2025-04-30","title":"FiLA-Video: Spatio-Temporal Compression for Fine-Grained Long Video Understanding","abstract":"Recent advancements in video understanding within visual large language models (VLLMs) have led to notable progress. However, the complexity of video data and contextual processing limitations still hinder long-video comprehension. A common approach is video feature compression to reduce token input to large language models, yet many methods either fail to prioritize essential features, leading to redundant inter-frame information, or introduce computationally expensive modules.To address these issues, we propose FiLA(Fine-grained Vision Language Model)-Video, a novel framework that leverages a lightweight dynamic-weight multi-frame fusion strategy, which adaptively integrates multiple frames into a single representation while preserving key video information and reducing computational costs. To enhance frame selection for fusion, we introduce a keyframe selection strategy, effectively identifying informative frames from a larger pool for improved summarization. Additionally, we present a simple yet effective long-video training data generation strategy, boosting model performance without extensive manual annotation. Experimental results demonstrate that FiLA-Video achieves superior efficiency and accuracy in long-video comprehension compared to existing methods.","authors":["Yanan Guo","Wenhui Dong","Jun Song","Shiding Zhu","Xuan Zhang","Hanqing Yang","Yingbo Wang","Yang Du","Xianing Chen","Bo Zheng"],"url":"https://arxiv.org/abs/2504.20384"}
{"created":"2025-04-30","title":"Weighted GKAT: Completeness and Complexity","abstract":"We propose Weighted Guarded Kleene Algebra with Tests (wGKAT), an uninterpreted weighted programming language equipped with branching, conditionals, and loops. We provide an operational semantics for wGKAT using a variant of weighted automata and introduce a sound and complete axiomatization. We also provide a polynomial time decision procedure for bisimulation equivalence.","authors":["Spencer Van Koevering","Wojciech R\\'o\\.zowski","Alexandra Silva"],"url":"https://arxiv.org/abs/2504.20385"}
{"created":"2025-04-30","title":"DEER: Deep Runahead for Instruction Prefetching on Modern Mobile Workloads","abstract":"Mobile workloads incur heavy frontend stalls due to increasingly large code footprints as well as long repeat cycles. Existing instruction-prefetching techniques suffer from low coverage, poor timeliness, or high cost. We provide a SW/HW co-designed I-prefetcher; DEER uses profile analysis to extract metadata information that allow the hardware to prefetch the most likely future instruction cachelines, hundreds of instructions earlier. This profile analysis skips over loops and recursions to go deeper into the future, and uses a return-address stack on the hardware side to allow prefetch on the return-path from large call-stacks. The produced metadata table is put in DRAM, pointed to by an in-hardware register; the high depth of the lookahead allows to preload the metadata in time and thus nearly no on-chip metadata storage is needed. Gem5 evaluation on real-world modern mobile workloads shows up to 45% reduction in L2 instruction-miss rate (19.6% on average), resulting in up to 8% speedup (4.7% on average). These gains are up to 4X larger than full-hardware record-and-replay prefetchers, while needing two orders of magnitude smaller on-chip storage.","authors":["Parmida Vahdatniya","Julian Humecki","Henry Kao","Tony Li","Ali Sedaghati","Fang Su","Ruoyu Zhou","Alex Bi","Reza Azimi","Maziar Goudarzi"],"url":"https://arxiv.org/abs/2504.20387"}
{"created":"2025-04-30","title":"CloudQC: A Network-aware Framework for Multi-tenant Distributed Quantum Computing","abstract":"Distributed quantum computing (DQC) that allows a large quantum circuit to be executed simultaneously on multiple quantum processing units (QPUs) becomes a promising approach to increase the scalability of quantum computing. It is natural to envision the near-future DQC platform as a multi-tenant cluster of QPUs, called a Quantum Cloud. However, no existing DQC work has addressed the two key problems of running DQC in a multi-tenant quantum cloud: placing multiple quantum circuits to QPUs and scheduling network resources to complete these jobs. This work is the first attempt to design a circuit placement and resource scheduling framework for a multi-tenant environment. The proposed framework is called CloudQC, which includes two main functional components, circuit placement and network scheduler, with the objectives of optimizing both quantum network cost and quantum computing time. Experimental results with real quantum circuit workloads show that CloudQC significantly reduces the average job completion time compared to existing DQC placement algorithms for both single-circuit and multi-circuit DQC. We envision this work will motivate more future work on network-aware quantum cloud.","authors":["Ruilin Zhou","Yuhang Gan","Yi Liu","Chen Qian"],"url":"https://arxiv.org/abs/2504.20389"}
{"created":"2025-04-30","title":"Manifold Clustering with Schatten p-norm Maximization","abstract":"Manifold clustering, with its exceptional ability to capture complex data structures, holds a pivotal position in cluster analysis. However, existing methods often focus only on finding the optimal combination between K-means and manifold learning, and overlooking the consistency between the data structure and labels. To address this issue, we deeply explore the relationship between K-means and manifold learning, and on this basis, fuse them to develop a new clustering framework. Specifically, the algorithm uses labels to guide the manifold structure and perform clustering on it, which ensures the consistency between the data structure and labels. Furthermore, in order to naturally maintain the class balance in the clustering process, we maximize the Schatten p-norm of labels, and provide a theoretical proof to support this. Additionally, our clustering framework is designed to be flexible and compatible with many types of distance functions, which facilitates efficient processing of nonlinear separable data. The experimental results of several databases confirm the superiority of our proposed model.","authors":["Fangfang Li","Quanxue Gao"],"url":"https://arxiv.org/abs/2504.20390"}
{"created":"2025-04-30","title":"Partial Answer of How Transformers Learn Automata","abstract":"We introduce a novel framework for simulating finite automata using representation-theoretic semidirect products and Fourier modules, achieving more efficient Transformer-based implementations.","authors":["Tiantian (Crystal)","Zhang"],"url":"https://arxiv.org/abs/2504.20395"}
{"created":"2025-04-30","title":"Creating Your Editable 3D Photorealistic Avatar with Tetrahedron-constrained Gaussian Splatting","abstract":"Personalized 3D avatar editing holds significant promise due to its user-friendliness and availability to applications such as AR/VR and virtual try-ons. Previous studies have explored the feasibility of 3D editing, but often struggle to generate visually pleasing results, possibly due to the unstable representation learning under mixed optimization of geometry and texture in complicated reconstructed scenarios. In this paper, we aim to provide an accessible solution for ordinary users to create their editable 3D avatars with precise region localization, geometric adaptability, and photorealistic renderings. To tackle this challenge, we introduce a meticulously designed framework that decouples the editing process into local spatial adaptation and realistic appearance learning, utilizing a hybrid Tetrahedron-constrained Gaussian Splatting (TetGS) as the underlying representation. TetGS combines the controllable explicit structure of tetrahedral grids with the high-precision rendering capabilities of 3D Gaussian Splatting and is optimized in a progressive manner comprising three stages: 3D avatar instantiation from real-world monocular videos to provide accurate priors for TetGS initialization; localized spatial adaptation with explicitly partitioned tetrahedrons to guide the redistribution of Gaussian kernels; and geometry-based appearance generation with a coarse-to-fine activation strategy. Both qualitative and quantitative experiments demonstrate the effectiveness and superiority of our approach in generating photorealistic 3D editable avatars.","authors":["Hanxi Liu","Yifang Men","Zhouhui Lian"],"url":"https://arxiv.org/abs/2504.20403"}
{"created":"2025-04-30","title":"Skill Discovery for Software Scripting Automation via Offline Simulations with LLMs","abstract":"Scripting interfaces enable users to automate tasks and customize software workflows, but creating scripts traditionally requires programming expertise and familiarity with specific APIs, posing barriers for many users. While Large Language Models (LLMs) can generate code from natural language queries, runtime code generation is severely limited due to unverified code, security risks, longer response times, and higher computational costs. To bridge the gap, we propose an offline simulation framework to curate a software-specific skillset, a collection of verified scripts, by exploiting LLMs and publicly available scripting guides. Our framework comprises two components: (1) task creation, using top-down functionality guidance and bottom-up API synergy exploration to generate helpful tasks; and (2) skill generation with trials, refining and validating scripts based on execution feedback. To efficiently navigate the extensive API landscape, we introduce a Graph Neural Network (GNN)-based link prediction model to capture API synergy, enabling the generation of skills involving underutilized APIs and expanding the skillset's diversity. Experiments with Adobe Illustrator demonstrate that our framework significantly improves automation success rates, reduces response time, and saves runtime token costs compared to traditional runtime code generation. This is the first attempt to use software scripting interfaces as a testbed for LLM-based systems, highlighting the advantages of leveraging execution feedback in a controlled environment and offering valuable insights into aligning AI capabilities with user needs in specialized software domains.","authors":["Paiheng Xu","Gang Wu","Xiang Chen","Tong Yu","Chang Xiao","Franck Dernoncourt","Tianyi Zhou","Wei Ai","Viswanathan Swaminathan"],"url":"https://arxiv.org/abs/2504.20406"}
{"created":"2025-04-30","title":"FourierSpecNet: Neural Collision Operator Approximation Inspired by the Fourier Spectral Method for Solving the Boltzmann Equation","abstract":"The Boltzmann equation, a fundamental model in kinetic theory, describes the evolution of particle distribution functions through a nonlinear, high-dimensional collision operator. However, its numerical solution remains computationally demanding, particularly for inelastic collisions and high-dimensional velocity domains. In this work, we propose the Fourier Neural Spectral Network (FourierSpecNet), a hybrid framework that integrates the Fourier spectral method with deep learning to approximate the collision operator in Fourier space efficiently. FourierSpecNet achieves resolution-invariant learning and supports zero-shot super-resolution, enabling accurate predictions at unseen resolutions without retraining. Beyond empirical validation, we establish a consistency result showing that the trained operator converges to the spectral solution as the discretization is refined. We evaluate our method on several benchmark cases, including Maxwellian and hard-sphere molecular models, as well as inelastic collision scenarios. The results demonstrate that FourierSpecNet offers competitive accuracy while significantly reducing computational cost compared to traditional spectral solvers. Our approach provides a robust and scalable alternative for solving the Boltzmann equation across both elastic and inelastic regimes.","authors":["Jae Yong Lee","Gwang Jae Jung","Byung Chan Lim","Hyung Ju Hwang"],"url":"https://arxiv.org/abs/2504.20408"}
{"created":"2025-04-30","title":"GarmentX: Autoregressive Parametric Representations for High-Fidelity 3D Garment Generation","abstract":"This work presents GarmentX, a novel framework for generating diverse, high-fidelity, and wearable 3D garments from a single input image. Traditional garment reconstruction methods directly predict 2D pattern edges and their connectivity, an overly unconstrained approach that often leads to severe self-intersections and physically implausible garment structures. In contrast, GarmentX introduces a structured and editable parametric representation compatible with GarmentCode, ensuring that the decoded sewing patterns always form valid, simulation-ready 3D garments while allowing for intuitive modifications of garment shape and style. To achieve this, we employ a masked autoregressive model that sequentially predicts garment parameters, leveraging autoregressive modeling for structured generation while mitigating inconsistencies in direct pattern prediction. Additionally, we introduce GarmentX dataset, a large-scale dataset of 378,682 garment parameter-image pairs, constructed through an automatic data generation pipeline that synthesizes diverse and high-quality garment images conditioned on parametric garment representations. Through integrating our method with GarmentX dataset, we achieve state-of-the-art performance in geometric fidelity and input image alignment, significantly outperforming prior approaches. We will release GarmentX dataset upon publication.","authors":["Jingfeng Guo","Jinnan Chen","Weikai Chen","Zhenyu Sun","Lanjiong Li","Baozhu Zhao","Lingting Zhu","Xin Wang","Qi Liu"],"url":"https://arxiv.org/abs/2504.20409"}
{"created":"2025-04-30","title":"Terahertz Wireless Data Center: Gaussian Beam or Airy Beam?","abstract":"Terahertz (THz) communication is emerging as a pivotal enabler for 6G and beyond wireless systems owing to its multi-GHz bandwidth. One of its novel applications is in wireless data centers, where it enables ultra-high data rates while enhancing network reconfigurability and scalability. However, due to numerous racks, supporting walls, and densely deployed antennas, the line-of-sight (LoS) path in data centers is often instead of fully obstructed, resulting in quasi-LoS propagation and degradation of spectral efficiency. To address this issue, Airy beam-based hybrid beamforming is investigated in this paper as a promising technique to mitigate quasi-LoS propagation and enhance spectral efficiency in THz wireless data centers. Specifically, a cascaded geometrical and wave-based channel model (CGWCM) is proposed for quasi-LoS scenarios, which accounts for diffraction effects while being more simplified than conventional wave-based model. Then, the characteristics and generation of the Airy beam are analyzed, and beam search methods for quasi-LoS scenarios are proposed, including hierarchical focusing-Airy beam search, and low-complexity beam search. Simulation results validate the effectiveness of the CGWCM and demonstrate the superiority of the Airy beam over Gaussian beams in mitigating blockages, verifying its potential for practical THz wireless communication in data centers.","authors":["Wenqi Zhao","Sergi Abadal","Guochao Song","Jiamo Jiang","Chong Han"],"url":"https://arxiv.org/abs/2504.20410"}
{"created":"2025-04-30","title":"ADiff4TPP: Asynchronous Diffusion Models for Temporal Point Processes","abstract":"This work introduces a novel approach to modeling temporal point processes using diffusion models with an asynchronous noise schedule. At each step of the diffusion process, the noise schedule injects noise of varying scales into different parts of the data. With a careful design of the noise schedules, earlier events are generated faster than later ones, thus providing stronger conditioning for forecasting the more distant future. We derive an objective to effectively train these models for a general family of noise schedules based on conditional flow matching. Our method models the joint distribution of the latent representations of events in a sequence and achieves state-of-the-art results in predicting both the next inter-event time and event type on benchmark datasets. Additionally, it flexibly accommodates varying lengths of observation and prediction windows in different forecasting settings by adjusting the starting and ending points of the generation process. Finally, our method shows superior performance in long-horizon prediction tasks, outperforming existing baseline methods.","authors":["Amartya Mukherjee","Ruizhi Deng","He Zhao","Yuzhen Mao","Leonid Sigal","Frederick Tung"],"url":"https://arxiv.org/abs/2504.20411"}
{"created":"2025-04-30","title":"CrashFixer: A crash resolution agent for the Linux kernel","abstract":"Code large language models (LLMs) have shown impressive capabilities on a multitude of software engineering tasks. In particular, they have demonstrated remarkable utility in the task of code repair. However, common benchmarks used to evaluate the performance of code LLMs are often limited to small-scale settings. In this work, we build upon kGym, which shares a benchmark for system-level Linux kernel bugs and a platform to run experiments on the Linux kernel.","authors":["Alex Mathai","Chenxi Huang","Suwei Ma","Jihwan Kim","Hailie Mitchell","Aleksandr Nogikh","Petros Maniatis","Franjo Ivan\\v{c}i\\'c","Junfeng Yang","Baishakhi Ray"],"url":"https://arxiv.org/abs/2504.20412"}
{"created":"2025-04-30","title":"Enhancing Leakage Attacks on Searchable Symmetric Encryption Using LLM-Based Synthetic Data Generation","abstract":"Searchable Symmetric Encryption (SSE) enables efficient search capabilities over encrypted data, allowing users to maintain privacy while utilizing cloud storage. However, SSE schemes are vulnerable to leakage attacks that exploit access patterns, search frequency, and volume information. Existing studies frequently assume that adversaries possess a substantial fraction of the encrypted dataset to mount effective inference attacks, implying there is a database leakage of such documents, thus, an assumption that may not hold in real-world scenarios. In this work, we investigate the feasibility of enhancing leakage attacks under a more realistic threat model in which adversaries have access to minimal leaked data. We propose a novel approach that leverages large language models (LLMs), specifically GPT-4 variants, to generate synthetic documents that statistically and semantically resemble the real-world dataset of Enron emails. Using the email corpus as a case study, we evaluate the effectiveness of synthetic data generated via random sampling and hierarchical clustering methods on the performance of the SAP (Search Access Pattern) keyword inference attack restricted to token volumes only. Our results demonstrate that, while the choice of LLM has limited effect, increasing dataset size and employing clustering-based generation significantly improve attack accuracy, achieving comparable performance to attacks using larger amounts of real data. We highlight the growing relevance of LLMs in adversarial contexts.","authors":["Joshua Chiu","Partha Protim Paul","Zahin Wahab"],"url":"https://arxiv.org/abs/2504.20414"}
{"created":"2025-04-30","title":"Undecidability of the Emptiness Problem of Deterministic Propositional While Programs with Graph Loop: Hypothesis Elimination Using Loops","abstract":"We show that the emptiness (unsatisfiability) problem is undecidable and $\\mathrm{\\Pi}^{0}_{1}$-complete for deterministic propositional while programs with (graph) loop. To this end, we introduce a hypothesis elimination using loops. Using this, we give reductions from the complement of the periodic domino problem.","authors":["Yoshiki Nakamura"],"url":"https://arxiv.org/abs/2504.20415"}
{"created":"2025-04-30","title":"Plant Disease Detection through Multimodal Large Language Models and Convolutional Neural Networks","abstract":"Automation in agriculture plays a vital role in addressing challenges related to crop monitoring and disease management, particularly through early detection systems. This study investigates the effectiveness of combining multimodal Large Language Models (LLMs), specifically GPT-4o, with Convolutional Neural Networks (CNNs) for automated plant disease classification using leaf imagery. Leveraging the PlantVillage dataset, we systematically evaluate model performance across zero-shot, few-shot, and progressive fine-tuning scenarios. A comparative analysis between GPT-4o and the widely used ResNet-50 model was conducted across three resolutions (100, 150, and 256 pixels) and two plant species (apple and corn). Results indicate that fine-tuned GPT-4o models achieved slightly better performance compared to the performance of ResNet-50, achieving up to 98.12% classification accuracy on apple leaf images, compared to 96.88% achieved by ResNet-50, with improved generalization and near-zero training loss. However, zero-shot performance of GPT-4o was significantly lower, underscoring the need for minimal training. Additional evaluations on cross-resolution and cross-plant generalization revealed the models' adaptability and limitations when applied to new domains. The findings highlight the promise of integrating multimodal LLMs into automated disease detection pipelines, enhancing the scalability and intelligence of precision agriculture systems while reducing the dependence on large, labeled datasets and high-resolution sensor infrastructure. Large Language Models, Vision Language Models, LLMs and CNNs, Disease Detection with Vision Language Models, VLMs","authors":["Konstantinos I. Roumeliotis","Ranjan Sapkota","Manoj Karkee","Nikolaos D. Tselikas","Dimitrios K. Nasiopoulos"],"url":"https://arxiv.org/abs/2504.20419"}
{"created":"2025-04-30","title":"A Geography-Inspired and Self-Adaptive Clustering Algorithm: A Study in Channel Measurement","abstract":"The phenomenon that multi-path components (MPCs) arrive in clusters has been verified by channel measurements, and is widely adopted by cluster-based channel models. As a crucial intermediate processing step, MPC clustering bridges raw data in channel measurement and cluster characteristics for channel modeling. In this paper, a physical-interpretable and self-adaptive MPC clustering algorithm is proposed, which can locate both single-point and wide-spread scatterers without prior knowledge. Inspired by the concept in geography, a novel metaphor that interprets features of MPC attributes in the power-delay-angle profile (PDAP) as topographic concepts is developed. In light of the interpretation, the proposed algorithm disassembles the PDAP by constructing contour lines and identifying characteristic points that indicate the skeleton of MPC clusters, which are fitted by analytical models that associate MPCs with physical scatterer locations. Besides, a new clustering performance index, the power gradient consistency index, is proposed. Calculated as the weighted Spearman correlation coefficient between the power and the distance to the center, the index captures the intrinsic property of MPC clusters that the dominant high-power path is surrounded by lower-power paths. The performance of the proposed algorithm is analyzed and compared with the counterparts of conventional clustering algorithms based on the channel measurement conducted in an outdoor scenario. The proposed algorithm performs better in average Silhouette index and weighted Spearman correlation coefficient, and the average root mean square error (RMSE) of the estimated scatterer location is 0.1 m.","authors":["Yiqin Wang","Chong Han"],"url":"https://arxiv.org/abs/2504.20420"}
{"created":"2025-04-30","title":"Understanding GNNs and Homophily in Dynamic Node Classification","abstract":"Homophily, as a measure, has been critical to increasing our understanding of graph neural networks (GNNs). However, to date this measure has only been analyzed in the context of static graphs. In our work, we explore homophily in dynamic settings. Focusing on graph convolutional networks (GCNs), we demonstrate theoretically that in dynamic settings, current GCN discriminative performance is characterized by the probability that a node's future label is the same as its neighbors' current labels. Based on this insight, we propose dynamic homophily, a new measure of homophily that applies in the dynamic setting. This new measure correlates with GNN discriminative performance and sheds light on how to potentially design more powerful GNNs for dynamic graphs. Leveraging a variety of dynamic node classification datasets, we demonstrate that popular GNNs are not robust to low dynamic homophily. Going forward, our work represents an important step towards understanding homophily and GNN performance in dynamic node classification.","authors":["Michael Ito","Danai Koutra","Jenna Wiens"],"url":"https://arxiv.org/abs/2504.20421"}
{"created":"2025-04-30","title":"Metaheuristic Optimization of Trajectory and Dynamic Time Splitting for UAV Communication Systems","abstract":"The integration of unmanned aerial vehicles (UAVs) into wireless communication systems has emerged as a transformative approach, promising cost-efficient connectivity. This paper addresses the optimization of the dynamic time-splitting ratio and flight trajectory for a communication system linking a ground base station to the UAV equipped with backscatter devices (referred to as UB), and from UB to an end user. Given the inherent non-convexity of the problem, we develop two meta-heuristic-based approaches inspired by genetic algorithm and particle swarm optimization to enhance the total achievable rate while reducing computational complexity. Numerical results demonstrate the effectiveness of these meta-heuristic solutions, showcasing significant improvements in the achievable rate and computation time compared to existing benchmarks.","authors":["Trinh Van Chien","Nguyen Minh Quan","Oh-Soon Shin","Van-Dinh Nguyen"],"url":"https://arxiv.org/abs/2504.20425"}
{"created":"2025-04-30","title":"RV-Syn: Rational and Verifiable Mathematical Reasoning Data Synthesis based on Structured Function Library","abstract":"The advancement of reasoning capabilities in Large Language Models (LLMs) requires substantial amounts of high-quality reasoning data, particularly in mathematics. Existing data synthesis methods, such as data augmentation from annotated training sets or direct question generation based on relevant knowledge points and documents, have expanded datasets but face challenges in mastering the inner logic of the problem during generation and ensuring the verifiability of the solutions. To address these issues, we propose RV-Syn, a novel Rational and Verifiable mathematical Synthesis approach. RV-Syn constructs a structured mathematical operation function library based on initial seed problems and generates computational graphs as solutions by combining Python-formatted functions from this library. These graphs are then back-translated into complex problems. Based on the constructed computation graph, we achieve solution-guided logic-aware problem generation. Furthermore, the executability of the computational graph ensures the verifiability of the solving process. Experimental results show that RV-Syn surpasses existing synthesis methods, including those involving human-generated problems, achieving greater efficient data scaling. This approach provides a scalable framework for generating high-quality reasoning datasets.","authors":["Jiapeng Wang","Jinhao Jiang","Zhiqiang Zhang","Jun Zhou","Wayne Xin Zhao"],"url":"https://arxiv.org/abs/2504.20426"}
{"created":"2025-04-30","title":"Learning Laplacian Positional Encodings for Heterophilous Graphs","abstract":"In this work, we theoretically demonstrate that current graph positional encodings (PEs) are not beneficial and could potentially hurt performance in tasks involving heterophilous graphs, where nodes that are close tend to have different labels. This limitation is critical as many real-world networks exhibit heterophily, and even highly homophilous graphs can contain local regions of strong heterophily. To address this limitation, we propose Learnable Laplacian Positional Encodings (LLPE), a new PE that leverages the full spectrum of the graph Laplacian, enabling them to capture graph structure on both homophilous and heterophilous graphs. Theoretically, we prove LLPE's ability to approximate a general class of graph distances and demonstrate its generalization properties. Empirically, our evaluation on 12 benchmarks demonstrates that LLPE improves accuracy across a variety of GNNs, including graph transformers, by up to 35% and 14% on synthetic and real-world graphs, respectively. Going forward, our work represents a significant step towards developing PEs that effectively capture complex structures in heterophilous graphs.","authors":["Michael Ito","Jiong Zhu","Dexiong Chen","Danai Koutra","Jenna Wiens"],"url":"https://arxiv.org/abs/2504.20430"}
{"created":"2025-04-30","title":"An Algebraic Approach to Asymmetric Delegation and Polymorphic Label Inference (Technical Report)","abstract":"Language-based information flow control (IFC) enables reasoning about and enforcing security policies in decentralized applications. While information flow properties are relatively extensional and compositional, designing expressive systems that enforce such properties remains challenging. In particular, it can be difficult to use IFC labels to model certain security assumptions, such as semi-honest agents.","authors":["Silei Ren","Co\\c{s}ku Acay","Andrew C. Myers"],"url":"https://arxiv.org/abs/2504.20432"}
{"created":"2025-04-30","title":"Fiber to the Room: Key Technologies, Challenges, and Prospects","abstract":"Fiber to the Room (FTTR) is a next-generation access network designed to deliver high bandwidth, low latency, and room-level optical coverage. This paper presents a comprehensive analysis of the FTTR system architecture and protocol stack, focusing on three key technical aspects: centralized scheduling and control, integrated management and maintenance, and green energy-saving mechanisms. A simplified FTTR architecture based on the convergence of the medium access control (MAC) and physical (PHY) layers is introduced to enhance coordination and scheduling efficiency. An extended remote management scheme, based on the optical network unit management and control interface (OMCI), is described to enable unified control across main fiber units (MFUs) and sub-fiber units (SFUs). Furthermore, a service-aware energy-saving framework is discussed for dynamic power optimization. The paper also explores the integration of artificial intelligence (AI) and passive sensing into FTTR systems to support intelligent scheduling, energy management, and environment-aware optimization. These insights provide technical guidance for the scalable deployment and future evolution of FTTR networks.","authors":["Jinhan Cai","Xiaolong Zhang","Xiang Wang","Tianhai Chang","Gangxiang Shen"],"url":"https://arxiv.org/abs/2504.20433"}
{"created":"2025-04-30","title":"ARCS: Agentic Retrieval-Augmented Code Synthesis with Iterative Refinement","abstract":"In supercomputing, efficient and optimized code generation is essential to leverage high-performance systems effectively. We propose Agentic Retrieval-Augmented Code Synthesis (ARCS), an advanced framework for accurate, robust, and efficient code generation, completion, and translation. ARCS integrates Retrieval-Augmented Generation (RAG) with Chain-of-Thought (CoT) reasoning to systematically break down and iteratively refine complex programming tasks. An agent-based RAG mechanism retrieves relevant code snippets, while real-time execution feedback drives the synthesis of candidate solutions. This process is formalized as a state-action search tree optimization, balancing code correctness with editing efficiency. Evaluations on the Geeks4Geeks and HumanEval benchmarks demonstrate that ARCS significantly outperforms traditional prompting methods in translation and generation quality. By enabling scalable and precise code synthesis, ARCS offers transformative potential for automating and optimizing code development in supercomputing applications, enhancing computational resource utilization.","authors":["Manish Bhattarai","Miguel Cordova","Javier Santos","Dan O'Malley"],"url":"https://arxiv.org/abs/2504.20434"}
{"created":"2025-04-30","title":"AI Assisted Cervical Cancer Screening for Cytology Samples in Developing Countries","abstract":"Cervical cancer remains a significant health challenge, with high incidence and mortality rates, particularly in transitioning countries. Conventional Liquid-Based Cytology(LBC) is a labor-intensive process, requires expert pathologists and is highly prone to errors, highlighting the need for more efficient screening methods. This paper introduces an innovative approach that integrates low-cost biological microscopes with our simple and efficient AI algorithms for automated whole-slide analysis. Our system uses a motorized microscope to capture cytology images, which are then processed through an AI pipeline involving image stitching, cell segmentation, and classification. We utilize the lightweight UNet-based model involving human-in-the-loop approach to train our segmentation model with minimal ROIs. CvT-based classification model, trained on the SIPaKMeD dataset, accurately categorizes five cell types. Our framework offers enhanced accuracy and efficiency in cervical cancer screening compared to various state-of-art methods, as demonstrated by different evaluation metrics.","authors":["Love Panta","Suraj Prasai","Karishma Malla Vaidya","Shyam Shrestha","Suresh Manandhar"],"url":"https://arxiv.org/abs/2504.20435"}
{"created":"2025-04-30","title":"Network Attack Traffic Detection With Hybrid Quantum-Enhanced Convolution Neural Network","abstract":"The emerging paradigm of Quantum Machine Learning (QML) combines features of quantum computing and machine learning (ML). QML enables the generation and recognition of statistical data patterns that classical computers and classical ML methods struggle to effectively execute. QML utilizes quantum systems to enhance algorithmic computation speed and real-time data processing capabilities, making it one of the most promising tools in the field of ML. Quantum superposition and entanglement features also hold the promise to potentially expand the potential feature representation capabilities of ML. Therefore, in this study, we explore how quantum computing affects ML and whether it can further improve the detection performance on network traffic detection, especially on unseen attacks which are types of malicious traffic that do not exist in the ML training dataset. Classical ML models often perform poorly in detecting these unseen attacks because they have not been trained on such traffic. Hence, this paper focuses on designing and proposing novel hybrid structures of Quantum Convolutional Neural Network (QCNN) to achieve the detection of malicious traffic. The detection performance, generalization, and robustness of the QML solutions are evaluated and compared with classical ML running on classical computers. The emphasis lies in assessing whether the QML-based malicious traffic detection outperforms classical solutions. Based on experiment results, QCNN models demonstrated superior performance compared to classical ML approaches on unseen attack detection.","authors":["Zihao Wang","Kar Wai Fok","Vrizlynn L. L. Thing"],"url":"https://arxiv.org/abs/2504.20436"}
{"created":"2025-04-30","title":"GaLore 2: Large-Scale LLM Pre-Training by Gradient Low-Rank Projection","abstract":"Large language models (LLMs) have revolutionized natural language understanding and generation but face significant memory bottlenecks during training. GaLore, Gradient Low-Rank Projection, addresses this issue by leveraging the inherent low-rank structure of weight gradients, enabling substantial memory savings without sacrificing performance. Recent works further extend GaLore from various aspects, including low-bit quantization and higher-order tensor structures. However, there are several remaining challenges for GaLore, such as the computational overhead of SVD for subspace updates and the integration with state-of-the-art training parallelization strategies (e.g., FSDP). In this paper, we present GaLore 2, an efficient and scalable GaLore framework that addresses these challenges and incorporates recent advancements. In addition, we demonstrate the scalability of GaLore 2 by pre-training Llama 7B from scratch using up to 500 billion training tokens, highlighting its potential impact on real LLM pre-training scenarios.","authors":["DiJia Su","Andrew Gu","Jane Xu","Yuandong Tian","Jiawei Zhao"],"url":"https://arxiv.org/abs/2504.20437"}
{"created":"2025-04-30","title":"PixelHacker: Image Inpainting with Structural and Semantic Consistency","abstract":"Image inpainting is a fundamental research area between image editing and image generation. Recent state-of-the-art (SOTA) methods have explored novel attention mechanisms, lightweight architectures, and context-aware modeling, demonstrating impressive performance. However, they often struggle with complex structure (e.g., texture, shape, spatial relations) and semantics (e.g., color consistency, object restoration, and logical correctness), leading to artifacts and inappropriate generation. To address this challenge, we design a simple yet effective inpainting paradigm called latent categories guidance, and further propose a diffusion-based model named PixelHacker. Specifically, we first construct a large dataset containing 14 million image-mask pairs by annotating foreground and background (potential 116 and 21 categories, respectively). Then, we encode potential foreground and background representations separately through two fixed-size embeddings, and intermittently inject these features into the denoising process via linear attention. Finally, by pre-training on our dataset and fine-tuning on open-source benchmarks, we obtain PixelHacker. Extensive experiments show that PixelHacker comprehensively outperforms the SOTA on a wide range of datasets (Places2, CelebA-HQ, and FFHQ) and exhibits remarkable consistency in both structure and semantics. Project page at https://hustvl.github.io/projects/PixelHacker.","authors":["Ziyang Xu","Kangsheng Duan","Xiaolei Shen","Zhifeng Ding","Wenyu Liu","Xiaohu Ruan","Xiaoxin Chen","Xinggang Wang"],"url":"https://arxiv.org/abs/2504.20438"}
{"created":"2025-04-30","title":"Multidimensional precipitation index prediction based on CNN-LSTM hybrid framework","abstract":"With the intensification of global climate change, accurate prediction of weather indicators is of great significance in disaster prevention and mitigation, agricultural production, and transportation. Precipitation, as one of the key meteorological indicators, plays a crucial role in water resource management, agricultural production, and urban flood control. This study proposes a multidimensional precipitation index prediction model based on a CNN- LSTM hybrid framework, aiming to improve the accuracy of precipitation forecasts. The dataset is sourced from Pune, Maharashtra, India, covering monthly mean precipitation data from 1972 to 2002. This dataset includes nearly 31 years (1972-2002) of monthly average precipitation, reflecting the long-term fluctuations and seasonal variations of precipitation in the region. By analyzing these time series data, the CNN-LSTM model effectively captures local features and long-term dependencies. Experimental results show that the model achieves a root mean square error (RMSE) of 6.752, which demonstrates a significant advantage over traditional time series prediction methods in terms of prediction accuracy and generalization ability. Furthermore, this study provides new research ideas for precipitation prediction. However, the model requires high computational resources when dealing with large-scale datasets, and its predictive ability for multidimensional precipitation data still needs improvement. Future research could extend the model to support and predict multidimensional precipitation data, thereby promoting the development of more accurate and efficient meteorological prediction technologies.","authors":["Yuchen Wang","Pengfei Jia","Zhitao Shu","Keyan Liu","Abdul Rashid Mohamed Shariff"],"url":"https://arxiv.org/abs/2504.20442"}
{"created":"2025-04-30","title":"On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?","abstract":"We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and Claude. We do this by repurposing the famous experiment Asch (1946) conducted using human subjects. The experiment is simple, given two candidates with equal descriptions which one is preferred if one description has positive adjectives first before negative ones and another description has negative adjectives followed by positive ones. We test this in two experiments. In one experiment, LLMs are given both candidates simultaneously in the same prompt, and in another experiment, LLMs are given both candidates separately. We test all the models with 200 candidate pairs. We found that, in the first experiment, ChatGPT preferred the candidate with positive adjectives listed first, while Gemini preferred both equally often. Claude refused to make a choice. In the second experiment, ChatGPT and Claude were most likely to rank both candidates equally. In the case where they did not give an equal rating, both showed a clear preference to a candidate that had negative adjectives listed first. Gemini was most likely to prefer a candidate with negative adjectives listed first.","authors":["Mika H\\\"am\\\"al\\\"ainen"],"url":"https://arxiv.org/abs/2504.20444"}
{"created":"2025-04-30","title":"Head-Tail-Aware KL Divergence in Knowledge Distillation for Spiking Neural Networks","abstract":"Spiking Neural Networks (SNNs) have emerged as a promising approach for energy-efficient and biologically plausible computation. However, due to limitations in existing training methods and inherent model constraints, SNNs often exhibit a performance gap when compared to Artificial Neural Networks (ANNs). Knowledge distillation (KD) has been explored as a technique to transfer knowledge from ANN teacher models to SNN student models to mitigate this gap. Traditional KD methods typically use Kullback-Leibler (KL) divergence to align output distributions. However, conventional KL-based approaches fail to fully exploit the unique characteristics of SNNs, as they tend to overemphasize high-probability predictions while neglecting low-probability ones, leading to suboptimal generalization. To address this, we propose Head-Tail Aware Kullback-Leibler (HTA-KL) divergence, a novel KD method for SNNs. HTA-KL introduces a cumulative probability-based mask to dynamically distinguish between high- and low-probability regions. It assigns adaptive weights to ensure balanced knowledge transfer, enhancing the overall performance. By integrating forward KL (FKL) and reverse KL (RKL) divergence, our method effectively align both head and tail regions of the distribution. We evaluate our methods on CIFAR-10, CIFAR-100 and Tiny ImageNet datasets. Our method outperforms existing methods on most datasets with fewer timesteps.","authors":["Tianqing Zhang","Zixin Zhu","Kairong Yu","Hongwei Wang"],"url":"https://arxiv.org/abs/2504.20445"}
{"created":"2025-04-30","title":"FT-MoE: Sustainable-learning Mixture of Experts Model for Fault-Tolerant Computing with Multiple Tasks","abstract":"Intelligent fault-tolerant (FT) computing has recently demonstrated significant advantages of predicting and diagnosing faults in advance, enabling reliable service delivery. However, due to heterogeneity of fault knowledge and complex dependence relationships of time series log data, existing deep learning-based FT algorithms further improve detection performance relying on single neural network model with difficulty. To this end, we propose FT-MoE, a sustainable-learning mixture-of-experts model for fault-tolerant computing with multiple tasks, which enables different parameters learning distinct fault knowledge to achieve high-reliability for service system. Firstly, we use decoder-based transformer models to obtain fault prototype vectors of decoupling long-distance dependencies. Followed by, we present a dual mixture of experts networks for high-accurate prediction for both fault detection and classification tasks. Then, we design a two-stage optimization scheme of offline training and online tuning, which allows that in operation FT-MoE can also keep learning to adapt to dynamic service environments. Finally, to verify the effectiveness of FT-MoE, we conduct extensive experiments on the FT benchmark. Experimental results show that FT-MoE achieves superior performance compared to the state-of-the-art methods. Code will be available upon publication.","authors":["Wenjing Xiao","Wenhao Song","Miaojiang Chen","Ruikun Luo","Min Chen"],"url":"https://arxiv.org/abs/2504.20446"}
{"created":"2025-04-30","title":"APG-MOS: Auditory Perception Guided-MOS Predictor for Synthetic Speech","abstract":"Automatic speech quality assessment aims to quantify subjective human perception of speech through computational models to reduce the need for labor-consuming manual evaluations. While models based on deep learning have achieved progress in predicting mean opinion scores (MOS) to assess synthetic speech, the neglect of fundamental auditory perception mechanisms limits consistency with human judgments. To address this issue, we propose an auditory perception guided-MOS prediction model (APG-MOS) that synergistically integrates auditory modeling with semantic analysis to enhance consistency with human judgments. Specifically, we first design a perceptual module, grounded in biological auditory mechanisms, to simulate cochlear functions, which encodes acoustic signals into biologically aligned electrochemical representations. Secondly, we propose a residual vector quantization (RVQ)-based semantic distortion modeling method to quantify the degradation of speech quality at the semantic level. Finally, we design a residual cross-attention architecture, coupled with a progressive learning strategy, to enable multimodal fusion of encoded electrochemical signals and semantic representations. Experiments demonstrate that APG-MOS achieves superior performance on two primary benchmarks. Our code and checkpoint will be available on a public repository upon publication.","authors":["Zhicheng Lian","Lizhi Wang","Hua Huang"],"url":"https://arxiv.org/abs/2504.20447"}
{"created":"2025-04-30","title":"Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs","abstract":"Translating knowledge-intensive and entity-rich text between English and Korean requires transcreation to preserve language-specific and cultural nuances beyond literal, phonetic or word-for-word conversion. We evaluate 13 models (LLMs and MT models) using automatic metrics and human assessment by bilingual annotators. Our findings show LLMs outperform traditional MT systems but struggle with entity translation requiring cultural adaptation. By constructing an error taxonomy, we identify incorrect responses and entity name errors as key issues, with performance varying by entity type and popularity level. This work exposes gaps in automatic evaluation metrics and hope to enable future work in completing culturally-nuanced machine translation.","authors":["Daniel Lee","Harsh Sharma","Jieun Han","Sunny Jeong","Alice Oh","Vered Shwartz"],"url":"https://arxiv.org/abs/2504.20451"}
{"created":"2025-04-30","title":"Enhancing News Recommendation with Hierarchical LLM Prompting","abstract":"Personalized news recommendation systems often struggle to effectively capture the complexity of user preferences, as they rely heavily on shallow representations, such as article titles and abstracts. To address this problem, we introduce a novel method, namely PNR-LLM, for Large Language Models for Personalized News Recommendation. Specifically, PNR-LLM harnesses the generation capabilities of LLMs to enrich news titles and abstracts, and consequently improves recommendation quality. PNR-LLM contains a novel module, News Enrichment via LLMs, which generates deeper semantic information and relevant entities from articles, transforming shallow contents into richer representations. We further propose an attention mechanism to aggregate enriched semantic- and entity-level data, forming unified user and news embeddings that reveal a more accurate user-news match. Extensive experiments on MIND datasets show that PNR-LLM outperforms state-of-the-art baselines. Moreover, the proposed data enrichment module is model-agnostic, and we empirically show that applying our proposed module to multiple existing models can further improve their performance, verifying the advantage of our design.","authors":["Hai-Dang Kieu","Delvin Ce Zhang","Minh Duc Nguyen","Min Xu","Qiang Wu","Dung D. Le"],"url":"https://arxiv.org/abs/2504.20452"}
{"created":"2025-04-30","title":"Reviving Any-Subset Autoregressive Models with Principled Parallel Sampling and Speculative Decoding","abstract":"In arbitrary-order language models, it is an open question how to sample tokens in parallel from the correct joint distribution. With discrete diffusion models, the more tokens they generate in parallel, the less their predicted distributions adhere to the originally learned data distribution, as they rely on a conditional independence assumption that only works with infinitesimally small timesteps. We find that a different class of models, any-subset autoregressive models (AS-ARMs), holds the solution. As implied by the name, AS-ARMs can generate tokens in any order, and in parallel. Moreover, AS-ARMs support parallelized joint probability density estimation, allowing them to correct their own parallel-generated token distributions, via our Any-Subset Speculative Decoding (ASSD) algorithm. ASSD provably enables generation of tokens from the correct joint distribution, with the number of neural network calls upper bounded by the number of tokens predicted. We empirically verify that ASSD speeds up language generation, without sacrificing quality. Furthermore, we provide a mathematically justified scheme for training AS-ARMs for generation, and show that AS-ARMs achieve state-of-the-art performance among sub-200M parameter models on infilling benchmark tasks, and nearly match the performance of models 50X larger on code generation. Our theoretical and empirical results indicate that the once-forgotten AS-ARMs are a promising direction of language modeling.","authors":["Gabe Guo","Stefano Ermon"],"url":"https://arxiv.org/abs/2504.20456"}
{"created":"2025-04-30","title":"Search-Based Interaction For Conversation Recommendation via Generative Reward Model Based Simulated User","abstract":"Conversational recommendation systems (CRSs) use multi-turn interaction to capture user preferences and provide personalized recommendations. A fundamental challenge in CRSs lies in effectively understanding user preferences from conversations. User preferences can be multifaceted and complex, posing significant challenges for accurate recommendations even with access to abundant external knowledge. While interaction with users can clarify their true preferences, frequent user involvement can lead to a degraded user experience.","authors":["Xiaolei Wang","Chunxuan Xia","Junyi Li","Fanzhe Meng","Lei Huang","Jinpeng Wang","Wayne Xin Zhao","Ji-Rong Wen"],"url":"https://arxiv.org/abs/2504.20458"}
{"created":"2025-04-30","title":"SAS-Prompt: Large Language Models as Numerical Optimizers for Robot Self-Improvement","abstract":"We demonstrate the ability of large language models (LLMs) to perform iterative self-improvement of robot policies. An important insight of this paper is that LLMs have a built-in ability to perform (stochastic) numerical optimization and that this property can be leveraged for explainable robot policy search. Based on this insight, we introduce the SAS Prompt (Summarize, Analyze, Synthesize) -- a single prompt that enables iterative learning and adaptation of robot behavior by combining the LLM's ability to retrieve, reason and optimize over previous robot traces in order to synthesize new, unseen behavior. Our approach can be regarded as an early example of a new family of explainable policy search methods that are entirely implemented within an LLM. We evaluate our approach both in simulation and on a real-robot table tennis task. Project website: sites.google.com/asu.edu/sas-llm/","authors":["Heni Ben Amor","Laura Graesser","Atil Iscen","David D'Ambrosio","Saminda Abeyruwan","Alex Bewley","Yifan Zhou","Kamalesh Kalirathinam","Swaroop Mishra","Pannag Sanketi"],"url":"https://arxiv.org/abs/2504.20459"}
{"created":"2025-04-30","title":"Sequence Reconstruction under Channels with Multiple Bursts of Insertions or Deletions","abstract":"The sequence reconstruction problem involves a model where a sequence is transmitted over several identical channels. This model investigates the minimum number of channels required for the unique reconstruction of the transmitted sequence. Levenshtein established that this number exceeds the maximum size of the intersection between the error balls of any two distinct transmitted sequences by one. In this paper, we consider channels subject to multiple bursts of insertions and multiple bursts of deletions, respectively, where each burst has an exact length of value b. We provide a complete solution for the insertion case while partially addressing the deletion case.","authors":["Zhaojun Lan","Yubo Sun","Wenjun Yu","Gennian Ge"],"url":"https://arxiv.org/abs/2504.20460"}
{"created":"2025-04-30","title":"Efficient Graph-Based Approximate Nearest Neighbor Search Achieving: Low Latency Without Throughput Loss","abstract":"The increase in the dimensionality of neural embedding models has enhanced the accuracy of semantic search capabilities but also amplified the computational demands for Approximate Nearest Neighbor Searches (ANNS). This complexity poses significant challenges in online and interactive services, where query latency is a critical performance metric. Traditional graph-based ANNS methods, while effective for managing large datasets, often experience substantial throughput reductions when scaled for intra-query parallelism to minimize latency. This reduction is largely due to inherent inefficiencies in the conventional fork-join parallelism model.","authors":["Jingjia Luo","Mingxing Zhang","Kang Chen","Xia Liao","Yingdi Shan","Jinlei Jiang","Yongwei Wu"],"url":"https://arxiv.org/abs/2504.20461"}
{"created":"2025-04-30","title":"TAMO:Fine-Grained Root Cause Analysis via Tool-Assisted LLM Agent with Multi-Modality Observation Data","abstract":"With the development of distributed systems, microservices and cloud native technologies have become central to modern enterprise software development. Despite bringing significant advantages, these technologies also increase system complexity and operational challenges. Traditional root cause analysis (RCA) struggles to achieve automated fault response, heavily relying on manual intervention. In recent years, large language models (LLMs) have made breakthroughs in contextual inference and domain knowledge integration, providing new solutions for Artificial Intelligence for Operations (AIOps). However, Existing LLM-based approaches face three key challenges: text input constraints, dynamic service dependency hallucinations, and context window limitations. To address these issues, we propose a tool-assisted LLM agent with multi-modality observation data, namely TAMO, for fine-grained RCA. It unifies multi-modal observational data into time-aligned representations to extract consistent features and employs specialized root cause localization and fault classification tools for perceiving the contextual environment. This approach overcomes the limitations of LLM in handling real-time changing service dependencies and raw observational data and guides LLM to generate repair strategies aligned with system contexts by structuring key information into a prompt. Experimental results show that TAMO performs well in root cause analysis when dealing with public datasets characterized by heterogeneity and common fault types, demonstrating its effectiveness.","authors":["Qi Wang","Xiao Zhang","Mingyi Li","Yuan Yuan","Mengbai Xiao","Fuzhen Zhuang","Dongxiao Yu"],"url":"https://arxiv.org/abs/2504.20462"}
{"created":"2025-04-30","title":"A Summary on GUI Agents with Foundation Models Enhanced by Reinforcement Learning","abstract":"Graphical User Interface (GUI) agents, driven by Multi-modal Large Language Models (MLLMs), have emerged as a promising paradigm for enabling intelligent interaction with digital systems. This paper provides a structured summary of recent advances in GUI agents, focusing on architectures enhanced by Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov Decision Processes and discuss typical execution environments and evaluation metrics. We then review the modular architecture of (M)LLM-based GUI agents, covering Perception, Planning, and Acting modules, and trace their evolution through representative works. Furthermore, we categorize GUI agent training methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and RL-based approaches, highlighting the progression from simple prompt engineering to dynamic policy learning via RL. Our summary illustrates how recent innovations in multimodal perception, decision reasoning, and adaptive action generation have significantly improved the generalization and robustness of GUI agents in complex real-world environments. We conclude by identifying key challenges and future directions for building more capable and reliable GUI agents.","authors":["Jiahao Li","Kaer Huang"],"url":"https://arxiv.org/abs/2504.20464"}
{"created":"2025-04-30","title":"LMM4Gen3DHF: Benchmarking and Evaluating Multimodal 3D Human Face Generation with LMMs","abstract":"The rapid advancement in generative artificial intelligence have enabled the creation of 3D human faces (HFs) for applications including media production, virtual reality, security, healthcare, and game development, etc. However, assessing the quality and realism of these AI-generated 3D human faces remains a significant challenge due to the subjective nature of human perception and innate perceptual sensitivity to facial features. To this end, we conduct a comprehensive study on the quality assessment of AI-generated 3D human faces. We first introduce Gen3DHF, a large-scale benchmark comprising 2,000 videos of AI-Generated 3D Human Faces along with 4,000 Mean Opinion Scores (MOS) collected across two dimensions, i.e., quality and authenticity, 2,000 distortion-aware saliency maps and distortion descriptions. Based on Gen3DHF, we propose LMME3DHF, a Large Multimodal Model (LMM)-based metric for Evaluating 3DHF capable of quality and authenticity score prediction, distortion-aware visual question answering, and distortion-aware saliency prediction. Experimental results show that LMME3DHF achieves state-of-the-art performance, surpassing existing methods in both accurately predicting quality scores for AI-generated 3D human faces and effectively identifying distortion-aware salient regions and distortion types, while maintaining strong alignment with human perceptual judgments. Both the Gen3DHF database and the LMME3DHF will be released upon the publication.","authors":["Woo Yi Yang","Jiarui Wang","Sijing Wu","Huiyu Duan","Yuxin Zhu","Liu Yang","Kang Fu","Guangtao Zhai","Xiongkuo Min"],"url":"https://arxiv.org/abs/2504.20466"}
{"created":"2025-04-30","title":"Antidote: A Unified Framework for Mitigating LVLM Hallucinations in Counterfactual Presupposition and Object Perception","abstract":"Large Vision-Language Models (LVLMs) have achieved impressive results across various cross-modal tasks. However, hallucinations, i.e., the models generating counterfactual responses, remain a challenge. Though recent studies have attempted to alleviate object perception hallucinations, they focus on the models' response generation, and overlooking the task question itself. This paper discusses the vulnerability of LVLMs in solving counterfactual presupposition questions (CPQs), where the models are prone to accept the presuppositions of counterfactual objects and produce severe hallucinatory responses. To this end, we introduce \"Antidote\", a unified, synthetic data-driven post-training framework for mitigating both types of hallucination above. It leverages synthetic data to incorporate factual priors into questions to achieve self-correction, and decouple the mitigation process into a preference optimization problem. Furthermore, we construct \"CP-Bench\", a novel benchmark to evaluate LVLMs' ability to correctly handle CPQs and produce factual responses. Applied to the LLaVA series, Antidote can simultaneously enhance performance on CP-Bench by over 50%, POPE by 1.8-3.3%, and CHAIR & SHR by 30-50%, all without relying on external supervision from stronger LVLMs or human feedback and introducing noticeable catastrophic forgetting issues.","authors":["Yuanchen Wu","Lu Zhang","Hang Yao","Junlong Du","Ke Yan","Shouhong Ding","Yunsheng Wu","Xiaoqiang Li"],"url":"https://arxiv.org/abs/2504.20468"}
{"created":"2025-04-30","title":"Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models","abstract":"Understanding how news narratives frame entities is crucial for studying media's impact on societal perceptions of events. In this paper, we evaluate the zero-shot capabilities of large language models (LLMs) in classifying framing roles. Through systematic experimentation, we assess the effects of input context, prompting strategies, and task decomposition. Our findings show that a hierarchical approach of first identifying broad roles and then fine-grained roles, outperforms single-step classification. We also demonstrate that optimal input contexts and prompts vary across task levels, highlighting the need for subtask-specific strategies. We achieve a Main Role Accuracy of 89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our approach. Our findings emphasize the importance of tailored prompt design and input context optimization for improving LLM performance in entity framing.","authors":["Enfa Fane","Mihai Surdeanu","Eduardo Blanco","Steven R. Corman"],"url":"https://arxiv.org/abs/2504.20469"}
{"created":"2025-04-30","title":"The Estimation of Continual Causal Effect for Dataset Shifting Streams","abstract":"Causal effect estimation has been widely used in marketing optimization. The framework of an uplift model followed by a constrained optimization algorithm is popular in practice. To enhance performance in the online environment, the framework needs to be improved to address the complexities caused by temporal dataset shift. This paper focuses on capturing the dataset shift from user behavior and domain distribution changing over time. We propose an Incremental Causal Effect with Proxy Knowledge Distillation (ICE-PKD) framework to tackle this challenge. The ICE-PKD framework includes two components: (i) a multi-treatment uplift network that eliminates confounding bias using counterfactual regression; (ii) an incremental training strategy that adapts to the temporal dataset shift by updating with the latest data and protects generalization via replay-based knowledge distillation. We also revisit the uplift modeling metrics and introduce a novel metric for more precise online evaluation in multiple treatment scenarios. Extensive experiments on both simulated and online datasets show that the proposed framework achieves better performance. The ICE-PKD framework has been deployed in the marketing system of Huaxiaozhu, a ride-hailing platform in China.","authors":["Baining Chen","Yiming Zhang","Yuqiao Han","Ruyue Zhang","Ruihuan Du","Zhishuo Zhou","Zhengdan Zhu","Xun Liu","Jiecheng Guo"],"url":"https://arxiv.org/abs/2504.20471"}
{"created":"2025-04-30","title":"Robustness via Referencing: Defending against Prompt Injection Attacks by Referencing the Executed Instruction","abstract":"Large language models (LLMs) have demonstrated impressive performance and have come to dominate the field of natural language processing (NLP) across various tasks. However, due to their strong instruction-following capabilities and inability to distinguish between instructions and data content, LLMs are vulnerable to prompt injection attacks. These attacks manipulate LLMs into deviating from the original input instructions and executing maliciously injected instructions within data content, such as web documents retrieved from search engines. Existing defense methods, including prompt-engineering and fine-tuning approaches, typically instruct models to follow the original input instructions while suppressing their tendencies to execute injected instructions. However, our experiments reveal that suppressing instruction-following tendencies is challenging. Through analyzing failure cases, we observe that although LLMs tend to respond to any recognized instructions, they are aware of which specific instructions they are executing and can correctly reference them within the original prompt. Motivated by these findings, we propose a novel defense method that leverages, rather than suppresses, the instruction-following abilities of LLMs. Our approach prompts LLMs to generate responses that include both answers and their corresponding instruction references. Based on these references, we filter out answers not associated with the original input instructions. Comprehensive experiments demonstrate that our method outperforms prompt-engineering baselines and achieves performance comparable to fine-tuning methods, reducing the attack success rate (ASR) to 0 percent in some scenarios. Moreover, our approach has minimal impact on overall utility.","authors":["Yulin Chen","Haoran Li","Yuan Sui","Yue Liu","Yufei He","Yangqiu Song","Bryan Hooi"],"url":"https://arxiv.org/abs/2504.20472"}
{"created":"2025-04-30","title":"Combining Quality of Service and System Health Metrics in MAPE-K based ROS Systems through Behavior Trees","abstract":"In recent years, the field of robotics has witnessed a significant shift from operating in structured environments to handling dynamic and unpredictable settings. To tackle these challenges, methodologies from the field of self-adaptive systems enabling these systems to react to unforeseen circumstances during runtime have been applied. The Monitoring-Analysis- Planning-Execution over Knowledge (MAPE-K) feedback loop model is a popular approach, often implemented in a managing subsystem, responsible for monitoring and adapting a managed subsystem. This work explores the implementation of the MAPE- K feedback loop based on Behavior Trees (BTs) within the Robot Operating System 2 (ROS2) framework. By delineating the managed and managing subsystems, our approach enhances the flexibility and adaptability of ROS-based systems, ensuring they not only meet Quality-of-Service (QoS), but also system health metric requirements, namely availability of ROS nodes and communication channels. Our implementation allows for the application of the method to new managed subsystems without needing custom BT nodes as the desired behavior can be configured within a specific rule set. We demonstrate the effectiveness of our method through various experiments on a system showcasing an aerial perception use case. By evaluating different failure cases, we show both an increased perception quality and a higher system availability. Our code is open source","authors":["Andreas Wiedholz","Rafael Paintner","Julian Glei{\\ss}ner","Alwin Hoffmann"],"url":"https://arxiv.org/abs/2504.20477"}
{"created":"2025-04-30","title":"Group Relative Knowledge Distillation: Learning from Teacher's Relational Inductive Bias","abstract":"Knowledge distillation typically transfers knowledge from a teacher model to a student model by minimizing differences between their output distributions. However, existing distillation approaches largely focus on mimicking absolute probabilities and neglect the valuable relational inductive biases embedded in the teacher's relative predictions, leading to exposure bias. In this paper, we propose Group Relative Knowledge Distillation (GRKD), a novel framework that distills teacher knowledge by learning the relative ranking among classes, rather than directly fitting the absolute distribution. Specifically, we introduce a group relative loss that encourages the student model to preserve the pairwise preference orderings provided by the teacher's outputs. Extensive experiments on classification benchmarks demonstrate that GRKD achieves superior generalization compared to existing methods, especially in tasks requiring fine-grained class differentiation. Our method provides a new perspective on exploiting teacher knowledge, focusing on relational structure rather than absolute likelihood.","authors":["Chao Li","Changhua Zhou","Jia Chen"],"url":"https://arxiv.org/abs/2504.20482"}
{"created":"2025-04-30","title":"Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training","abstract":"Large language models (LLMs) exhibit remarkable multilingual capabilities despite English-dominated pre-training, attributed to cross-lingual mechanisms during pre-training. Existing methods for enhancing cross-lingual transfer remain constrained by parallel resources, suffering from limited linguistic and domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT), a simple and scalable approach that enhances cross-lingual transfer by leveraging semantically related bilingual texts via simple next-word prediction. We construct CrossIC-PT samples by interleaving semantic-related bilingual Wikipedia documents into a single context window. To access window size constraints, we implement a systematic segmentation policy to split long bilingual document pairs into chunks while adjusting the sliding window mechanism to preserve contextual coherence. We further extend data availability through a semantic retrieval framework to construct CrossIC-PT samples from web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%, 3.99%, and 1.95%, respectively, with additional improvements after data augmentation.","authors":["Linjuan Wu","Haoran Wei","Huan Lin","Tianhao Li","Baosong Yang","Weiming Lu"],"url":"https://arxiv.org/abs/2504.20484"}
{"created":"2025-04-30","title":"Sleeping Giants - Activating Dormant Java Deserialization Gadget Chains through Stealthy Code Changes","abstract":"Java deserialization gadget chains are a well-researched critical software weakness. The vast majority of known gadget chains rely on gadgets from software dependencies. Furthermore, it has been shown that small code changes in dependencies have enabled these gadget chains. This makes gadget chain detection a purely reactive endeavor. Even if one dependency's deployment pipeline employs gadget chain detection, a gadget chain can still result from gadgets in other dependencies. In this work, we assess how likely small code changes are to enable a gadget chain. These changes could either be accidental or intentional as part of a supply chain attack. Specifically, we show that class serializability is a strongly fluctuating property over a dependency's evolution. Then, we investigate three change patterns by which an attacker could stealthily introduce gadgets into a dependency. We apply these patterns to 533 dependencies and run three state-of-the-art gadget chain detectors both on the original and the modified dependencies. The tools detect that applying the modification patterns can activate/inject gadget chains in 26.08% of the dependencies we selected. Finally, we verify the newly detected chains. As such, we identify dormant gadget chains in 53 dependencies that could be added through minor code modifications. This both shows that Java deserialization gadget chains are a broad liability to software and proves dormant gadget chains as a lucrative supply chain attack vector.","authors":["Bruno Kreyssig","Sabine Houy","Timoth\\'ee Riom","Alexandre Bartel"],"url":"https://arxiv.org/abs/2504.20485"}
{"created":"2025-04-30","title":"Hetu v2: A General and Scalable Deep Learning System with Hierarchical and Heterogeneous Single Program Multiple Data Annotations","abstract":"The Single Program Multiple Data (SPMD) paradigm provides a unified abstraction to annotate various parallel dimensions in distributed deep learning (DL) training. With SPMD, users can write training programs from the viewpoint of a single device, and the system will automatically deduce the tensor sharding and communication patterns. However, with the recent development in large-scale DL models, distributed training exhibits spatial and temporal workload heterogeneity, arising from both device disparities (e.g., mixed hardware, failures) and data variations (e.g., uneven sequence lengths). Such heterogeneity violates SPMD's assumption of uniform workload partitioning, which restricts its ability to express and optimize heterogeneous parallel strategies effectively.","authors":["Haoyang Li","Fangcheng Fu","Hao Ge","Sheng Lin","Xuanyu Wang","Jiawen Niu","Xupeng Miao","Bin Cui"],"url":"https://arxiv.org/abs/2504.20490"}
{"created":"2025-04-30","title":"Separation and Definability in Fragments of Two-Variable First-Order Logic with Counting","abstract":"For fragments L of first-order logic (FO) with counting quantifiers, we consider the definability problem, which asks whether a given L-formula can be equivalently expressed by a formula in some fragment of L without counting, and the more general separation problem asking whether two mutually exclusive L-formulas can be separated in some counting-free fragment of L. We show that separation is undecidable for the two-variable fragment of FO extended with counting quantifiers and for the graded modal logic with inverse, nominals and universal modality. On the other hand, if inverse or nominals are dropped, separation becomes coNExpTime- or 2ExpTime-complete, depending on whether the universal modality is present. In contrast, definability can often be reduced in polynomial time to validity in L. We also consider uniform separation and show that it often behaves similarly to definability.","authors":["Louwe Kuijer","Tony Tan","Frank Wolter","Michael Zakharyaschev"],"url":"https://arxiv.org/abs/2504.20491"}
{"created":"2025-04-30","title":"Triadic Closure-Heterogeneity-Harmony GCN for Link Prediction","abstract":"Link prediction aims to estimate the likelihood of connections between pairs of nodes in complex networks, which is beneficial to many applications from friend recommendation to metabolic network reconstruction. Traditional heuristic-based methodologies in the field of complex networks typically depend on predefined assumptions about node connectivity, limiting their generalizability across diverse networks. While recent graph neural network (GNN) approaches capture global structural features effectively, they often neglect node attributes and intrinsic structural relationships between node pairs. To address this, we propose TriHetGCN, an extension of traditional Graph Convolutional Networks (GCNs) that incorporates explicit topological indicators -- triadic closure and degree heterogeneity. TriHetGCN consists of three modules: topology feature construction, graph structural representation, and connection probability prediction. The topology feature module constructs node features using shortest path distances to anchor nodes, enhancing global structure perception. The graph structural module integrates topological indicators into the GCN framework to model triadic closure and heterogeneity. The connection probability module uses deep learning to predict links. Evaluated on nine real-world datasets, from traditional networks without node attributes to large-scale networks with rich features, TriHetGCN achieves state-of-the-art performance, outperforming mainstream methods. This highlights its strong generalization across diverse network types, offering a promising framework that bridges statistical physics and graph deep learning.","authors":["Ke-ke Shang","Junfan Yi","Michael Small","Yijie Zhou"],"url":"https://arxiv.org/abs/2504.20492"}
{"created":"2025-04-30","title":"Token-Efficient Prompt Injection Attack: Provoking Cessation in LLM Reasoning via Adaptive Token Compression","abstract":"While reasoning large language models (LLMs) demonstrate remarkable performance across various tasks, they also contain notable security vulnerabilities. Recent research has uncovered a \"thinking-stopped\" vulnerability in DeepSeek-R1, where model-generated reasoning tokens can forcibly interrupt the inference process, resulting in empty responses that compromise LLM-integrated applications. However, existing methods triggering this vulnerability require complex mathematical word problems with long prompts--even exceeding 5,000 tokens. To reduce the token cost and formally define this vulnerability, we propose a novel prompt injection attack named \"Reasoning Interruption Attack\", based on adaptive token compression. We demonstrate that simple standalone arithmetic tasks can effectively trigger this vulnerability, and the prompts based on such tasks exhibit simpler logical structures than mathematical word problems. We develop a systematic approach to efficiently collect attack prompts and an adaptive token compression framework that utilizes LLMs to automatically compress these prompts. Experiments show our compression framework significantly reduces prompt length while maintaining effective attack capabilities. We further investigate the attack's performance via output prefix and analyze the underlying causes of the vulnerability, providing valuable insights for improving security in reasoning LLMs.","authors":["Yu Cui","Yujun Cai","Yiwei Wang"],"url":"https://arxiv.org/abs/2504.20493"}
{"created":"2025-04-30","title":"An energy-stable minimal deformation rate scheme for mean curvature flow and surface diffusion","abstract":"We propose a new parametric finite element method, referred to as the BGN-MDR method, for simulating both mean curvature flow and surface diffusion for closed hypersurfaces, as well as open hypersurfaces with moving contact lines in three dimensions. The method is also applicable to closed and open curves with moving contact points in two dimensions. The proposed scheme inherits the energy stability from the BGN scheme proposed by Barrett, Garcke, and N\\\"urnberg in 2008, and offers improved mesh quality similar to the minimal deformation rate (MDR) method proposed by Hu and Li in 2022, especially for small time step sizes where the BGN scheme may become unstable and result in deteriorated meshes.","authors":["Guangwei Gao","Harald Garcke","Buyang Li","Rong Tang"],"url":"https://arxiv.org/abs/2504.20494"}
{"created":"2025-04-30","title":"Large-scale visual SLAM for in-the-wild videos","abstract":"Accurate and robust 3D scene reconstruction from casual, in-the-wild videos can significantly simplify robot deployment to new environments. However, reliable camera pose estimation and scene reconstruction from such unconstrained videos remains an open challenge. Existing visual-only SLAM methods perform well on benchmark datasets but struggle with real-world footage which often exhibits uncontrolled motion including rapid rotations and pure forward movements, textureless regions, and dynamic objects. We analyze the limitations of current methods and introduce a robust pipeline designed to improve 3D reconstruction from casual videos. We build upon recent deep visual odometry methods but increase robustness in several ways. Camera intrinsics are automatically recovered from the first few frames using structure-from-motion. Dynamic objects and less-constrained areas are masked with a predictive model. Additionally, we leverage monocular depth estimates to regularize bundle adjustment, mitigating errors in low-parallax situations. Finally, we integrate place recognition and loop closure to reduce long-term drift and refine both intrinsics and pose estimates through global bundle adjustment. We demonstrate large-scale contiguous 3D models from several online videos in various environments. In contrast, baseline methods typically produce locally inconsistent results at several points, producing separate segments or distorted maps. In lieu of ground-truth pose data, we evaluate map consistency, execution time and visual accuracy of re-rendered NeRF models. Our proposed system establishes a new baseline for visual reconstruction from casual uncontrolled videos found online, demonstrating more consistent reconstructions over longer sequences of in-the-wild videos than previously achieved.","authors":["Shuo Sun","Torsten Sattler","Malcolm Mielle","Achim J. Lilienthal","Martin Magnusson"],"url":"https://arxiv.org/abs/2504.20496"}
{"created":"2025-04-30","title":"Style-Adaptive Detection Transformer for Single-Source Domain Generalized Object Detection","abstract":"Single-source Domain Generalization (SDG) in object detection aims to develop a detector using only data from a source domain that can exhibit strong generalization capability when applied to unseen target domains. Existing methods are built upon CNN-based detectors and primarily improve robustness by employing carefully designed data augmentation strategies integrated with feature alignment techniques. However, data augmentation methods have inherent drawbacks; they are only effective when the augmented sample distribution approximates or covers the unseen scenarios, thus failing to enhance generalization across all unseen domains. Furthermore, while the recent Detection Transformer (DETR) has demonstrated superior generalization capability in domain adaptation tasks due to its efficient global information extraction, its potential in SDG tasks remains unexplored. To this end, we introduce a strong DETR-based detector named the Style-Adaptive Detection Transformer (SA-DETR) for SDG in object detection. Specifically, we present a domain style adapter that projects the style representation of the unseen target domain into the training domain, enabling dynamic style adaptation. Then, we propose an object-aware contrastive learning module to guide the detector in extracting domain-invariant features through contrastive learning. By using object-aware gating masks to constrain feature aggregation in both spatial and semantic dimensions, this module achieves cross-domain contrast of instance-level features, thereby enhancing generalization. Extensive experiments demonstrate the superior performance and generalization capability of SA-DETR across five different weather scenarios. Code is released at https://github.com/h751410234/SA-DETR.","authors":["Jianhong Han","Yupei Wang","Liang Chen"],"url":"https://arxiv.org/abs/2504.20498"}
{"created":"2025-04-30","title":"UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation","abstract":"We present UniDetox, a universally applicable method designed to mitigate toxicity across various large language models (LLMs). Previous detoxification methods are typically model-specific, addressing only individual models or model families, and require careful hyperparameter tuning due to the trade-off between detoxification efficacy and language modeling performance. In contrast, UniDetox provides a detoxification technique that can be universally applied to a wide range of LLMs without the need for separate model-specific tuning. Specifically, we propose a novel and efficient dataset distillation technique for detoxification using contrastive decoding. This approach distills detoxifying representations in the form of synthetic text data, enabling universal detoxification of any LLM through fine-tuning with the distilled text. Our experiments demonstrate that the detoxifying text distilled from GPT-2 can effectively detoxify larger models, including OPT, Falcon, and LLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter tuning for each model, as a single hyperparameter configuration can be seamlessly applied across different models. Additionally, analysis of the detoxifying text reveals a reduction in politically biased content, providing insights into the attributes necessary for effective detoxification of LLMs.","authors":["Huimin Lu","Masaru Isonuma","Junichiro Mori","Ichiro Sakata"],"url":"https://arxiv.org/abs/2504.20500"}
{"created":"2025-04-30","title":"MuRAL: A Multi-Resident Ambient Sensor Dataset Annotated with Natural Language for Activities of Daily Living","abstract":"Recent advances in Large Language Models (LLMs) have shown promising potential for human activity recognition (HAR) using ambient sensors, especially through natural language reasoning and zero-shot learning. However, existing datasets such as CASAS, ARAS, and MARBLE were not originally designed with LLMs in mind and therefore lack the contextual richness, complexity, and annotation granularity required to fully exploit LLM capabilities. In this paper, we introduce MuRAL, the first Multi-Resident Ambient sensor dataset with natural Language, comprising over 21 hours of multi-user sensor data collected from 21 sessions in a smart-home environment. MuRAL is annotated with fine-grained natural language descriptions, resident identities, and high-level activity labels, all situated in dynamic, realistic multi-resident settings. We benchmark MuRAL using state-of-the-art LLMs for three core tasks: subject assignment, action description, and activity classification. Our results demonstrate that while LLMs can provide rich semantic interpretations of ambient data, current models still face challenges in handling multi-user ambiguity and under-specified sensor contexts. We release MuRAL to support future research on LLM-powered, explainable, and socially aware activity understanding in smart environments. For access to the dataset, please reach out to us via the provided contact information. A direct link for dataset retrieval will be made available at this location in due course.","authors":["Xi Chen (M-PSI)","Julien Cumin (M-PSI)","Fano Ramparany (M-PSI)","Dominique Vaufreydaz (M-PSI)"],"url":"https://arxiv.org/abs/2504.20505"}
{"created":"2025-04-30","title":"SPARK Hand: Scooping-Pinching Adaptive Robotic Hand with Kempe Mechanism for Vertical Passive Grasp in Environmental Constraints","abstract":"This paper presents the SPARK finger, an innovative passive adaptive robotic finger capable of executing both parallel pinching and scooping grasps. The SPARK finger incorporates a multi-link mechanism with Kempe linkages to achieve a vertical linear fingertip trajectory. Furthermore, a parallelogram linkage ensures the fingertip maintains a fixed orientation relative to the base, facilitating precise and stable manipulation. By integrating these mechanisms with elastic elements, the design enables effective interaction with surfaces, such as tabletops, to handle challenging objects. The finger employs a passive switching mechanism that facilitates seamless transitions between pinching and scooping modes, adapting automatically to various object shapes and environmental constraints without additional actuators. To demonstrate its versatility, the SPARK Hand, equipped with two SPARK fingers, has been developed. This system exhibits enhanced grasping performance and stability for objects of diverse sizes and shapes, particularly thin and flat objects that are traditionally challenging for conventional grippers. Experimental results validate the effectiveness of the SPARK design, highlighting its potential for robotic manipulation in constrained and dynamic environments.","authors":["Jiaqi Yin","Tianyi Bi","Wenzeng Zhang"],"url":"https://arxiv.org/abs/2504.20506"}
{"created":"2025-04-30","title":"Taxonomic Trace Links: Rethinking Traceability and its Benefits","abstract":"Traceability greatly supports knowledge-intensive tasks, e.g., coverage check and impact analysis. Despite its clear benefits, the \\emph{practical} implementation of traceability poses significant challenges, leading to a reduced focus on the creation and maintenance of trace links. We propose a new approach -- Taxonomic Trace Links (TTL) -- which rethinks traceability and its benefits. With TTL, trace links are created indirectly through a domain-specific taxonomy, a simplified version of a domain model. TTL has the potential to address key traceability challenges, such as the granularity of trace links, the lack of a common data structure among software development artifacts, and unclear responsibility for traceability. We explain how TTL addresses these challenges and perform an initial validation with practitioners. We identified six challenges associated with TTL implementation that need to be addressed. Finally, we propose a research roadmap to further develop and evaluate the technical solution of TTL. TTL appears to be particularly feasible in practice where a domain taxonomy is already established","authors":["Waleed Abdeen","Michael Unterkalmsteiner","Alexandros Chirtoglou","Christoph Paul Schimanski","Heja Goli","Krzysztof Wnuk"],"url":"https://arxiv.org/abs/2504.20507"}
{"created":"2025-04-30","title":"The Panel Complexity of Sortition: Is 12 Angry Men Enough?","abstract":"Sortition is the practice of delegating public decision-making to randomly selected panels. Recently, it has gained momentum worldwide through its use in citizens' assemblies, sparking growing interest within the computer science community. One key appeal of sortition is that random panels tend to be more representative of the population than elected committees or parliaments. Our main conceptual contribution is a novel definition of representative panels, based on the Wasserstein distance from statistical learning theory. Using this definition, we develop a framework for analyzing the panel complexity problem -- determining the required panel size to ensure desirable properties. We focus on three key desiderata: (1) that efficiency at the panel level extends to the whole population, measured by social welfare; (2) that fairness guarantees for the panel translate to fairness for the population, captured by the core; and (3) that the probability of an outlier panel, for which the decision significantly deviates from the optimal one, remains low. We establish near-tight panel complexity guarantees for these desiderata across two fundamental social choice settings: participatory budgeting and facility location.","authors":["Johannes Brustle","Simone Fioravanti","Tomasz Ponitka","Jeremy Vollen"],"url":"https://arxiv.org/abs/2504.20508"}
{"created":"2025-04-30","title":"MambaMoE: Mixture-of-Spectral-Spatial-Experts State Space Model for Hyperspectral Image Classification","abstract":"The Mamba model has recently demonstrated strong potential in hyperspectral image (HSI) classification, owing to its ability to perform context modeling with linear computational complexity. However, existing Mamba-based methods usually neglect the spectral and spatial directional characteristics related to heterogeneous objects in hyperspectral scenes, leading to limited classification performance. To address these issues, we propose MambaMoE, a novel spectral-spatial mixture-of-experts framework, representing the first MoE-based approach in the HSI classification community. Specifically, we design a Mixture of Mamba Expert Block (MoMEB) that leverages sparse expert activation to enable adaptive spectral-spatial modeling. Furthermore, we introduce an uncertainty-guided corrective learning (UGCL) strategy to encourage the model's attention toward complex regions prone to prediction ambiguity. Extensive experiments on multiple public HSI benchmarks demonstrate that MambaMoE achieves state-of-the-art performance in both accuracy and efficiency compared to existing advanced approaches, especially for Mamba-based methods. Code will be released.","authors":["Yichu Xu","Di Wang","Hongzan Jiao","Lefei Zhang","Liangpei Zhang"],"url":"https://arxiv.org/abs/2504.20509"}
{"created":"2025-04-30","title":"SteelBlastQC: Shot-blasted Steel Surface Dataset with Interpretable Detection of Surface Defects","abstract":"Automating the quality control of shot-blasted steel surfaces is crucial for improving manufacturing efficiency and consistency. This study presents a dataset of 1654 labeled RGB images (512x512) of steel surfaces, classified as either \"ready for paint\" or \"needs shot-blasting.\" The dataset captures real-world surface defects, including discoloration, welding lines, scratches and corrosion, making it well-suited for training computer vision models. Additionally, three classification approaches were evaluated: Compact Convolutional Transformers (CCT), Support Vector Machines (SVM) with ResNet-50 feature extraction, and a Convolutional Autoencoder (CAE). The supervised methods (CCT and SVM) achieve 95% classification accuracy on the test set, with CCT leveraging transformer-based attention mechanisms and SVM offering a computationally efficient alternative. The CAE approach, while less effective, establishes a baseline for unsupervised quality control. We present interpretable decision-making by all three neural networks, allowing industry users to visually pinpoint problematic regions and understand the model's rationale. By releasing the dataset and baseline codes, this work aims to support further research in defect detection, advance the development of interpretable computer vision models for quality control, and encourage the adoption of automated inspection systems in industrial applications.","authors":["Irina Ruzavina","Lisa Sophie Theis","Jesse Lemeer","Rutger de Groen","Leo Ebeling","Andrej Hulak","Jouaria Ali","Guangzhi Tang","Rico Mockel"],"url":"https://arxiv.org/abs/2504.20510"}
{"created":"2025-04-30","title":"Challenges of Requirements Communication and Digital Assets Verification in Infrastructure Projects","abstract":"Background: Poor communication of requirements between clients and suppliers contributes to project overruns,in both software and infrastructure projects. Existing literature offers limited insights into the communication challenges at this interface. Aim: Our research aim to explore the processes and associated challenges with requirements activities that include client-supplier interaction and communication. Method: we study requirements validation, communication, and digital asset verification processes through two case studies in the road and railway sectors, involving interviews with ten experts across three companies. Results: We identify 13 challenges, along with their causes and consequences, and suggest solution areas from existing literature. Conclusion: Interestingly, the challenges in infrastructure projects mirror those found in software engineering, highlighting a need for further research to validate potential solutions.","authors":["Waleed Abdeen","Krzysztof Wnuk","Michael Unterkalmsteiner","Alexandros Chirtoglou"],"url":"https://arxiv.org/abs/2504.20511"}
{"created":"2025-04-30","title":"Enhancing Binary Search via Overlapping Partitions","abstract":"This paper considers the task of performing binary search under noisy decisions, focusing on the application of target area localization. In the presence of noise, the classical partitioning approach of binary search is prone to error propagation due to the use of strictly disjoint splits. While existing works on noisy binary search propose techniques such as query repetition or probabilistic updates to mitigate errors, they often lack explicit mechanisms to manage the trade-off between error probability and search complexity, with some providing only asymptotic guarantees. To address this gap, we propose a binary search framework with tunable overlapping partitions, which introduces controlled redundancy into the search process to enhance robustness against noise. We analyze the performance of the proposed algorithm in both discrete and continuous domains for the problem of area localization, quantifying how the overlap parameter impacts the trade-off between search tree depth and error probability. Unlike previous methods, this approach allows for direct control over the balance between reliability and efficiency. Our results emphasize the versatility and effectiveness of the proposed method, providing a principled extension to existing noisy search paradigms and enabling new insights into the interplay between partitioning strategies and measurement reliability.","authors":["Kaan Buyukkalayci","Merve Karakas","Xinlin Li","Christina Fragouli"],"url":"https://arxiv.org/abs/2504.20513"}
{"created":"2025-04-30","title":"Dynamic Attention Analysis for Backdoor Detection in Text-to-Image Diffusion Models","abstract":"Recent studies have revealed that text-to-image diffusion models are vulnerable to backdoor attacks, where attackers implant stealthy textual triggers to manipulate model outputs. Previous backdoor detection methods primarily focus on the static features of backdoor samples. However, a vital property of diffusion models is their inherent dynamism. This study introduces a novel backdoor detection perspective named Dynamic Attention Analysis (DAA), showing that these dynamic characteristics serve as better indicators for backdoor detection. Specifically, by examining the dynamic evolution of cross-attention maps, we observe that backdoor samples exhibit distinct feature evolution patterns at the $<$EOS$>$ token compared to benign samples. To quantify these dynamic anomalies, we first introduce DAA-I, which treats the tokens' attention maps as spatially independent and measures dynamic feature using the Frobenius norm. Furthermore, to better capture the interactions between attention maps and refine the feature, we propose a dynamical system-based approach, referred to as DAA-S. This model formulates the spatial correlations among attention maps using a graph-based state equation and we theoretically analyze the global asymptotic stability of this method. Extensive experiments across five representative backdoor attack scenarios demonstrate that our approach significantly surpasses existing detection methods, achieving an average F1 Score of 79.49% and an AUC of 87.67%. The code is available at https://github.com/Robin-WZQ/DAA.","authors":["Zhongqi Wang","Jie Zhang","Shiguang Shan","Xilin Chen"],"url":"https://arxiv.org/abs/2504.20518"}
{"created":"2025-04-30","title":"Conversations with AI Chatbots Increase Short-Term Vaccine Intentions But Do Not Outperform Standard Public Health Messaging","abstract":"Large language model (LLM) based chatbots show promise in persuasive communication, but existing studies often rely on weak controls or focus on belief change rather than behavioral intentions or outcomes. This pre-registered multi-country (US, Canada, UK) randomized controlled trial involving 930 vaccine-hesitant parents evaluated brief (three-minute) multi-turn conversations with LLM-based chatbots against standard public health messaging approaches for increasing human papillomavirus (HPV) vaccine intentions for their children. Participants were randomly assigned to: (1) a weak control (no message), (2) a strong control reflecting the standard of care (reading official public health materials), or (3 and 4) one of two chatbot conditions. One chatbot was prompted to deliver short, conversational responses, while the other used the model's default output style (longer with bullet points). While chatbot interactions significantly increased self-reported vaccination intent (by 7.1-10.3 points on a 100-point scale) compared to no message, they did not outperform standard public health materials, with the conversational chatbot performing significantly worse. Additionally, while the short-term effects of chatbot interactions faded during a 15-day follow-up, the effects of public health material persisted relative to no message. These findings suggest that while LLMs can effectively shift vaccination intentions in the short-term, their incremental value over existing public health communications is questionable, offering a more tempered view of their persuasive capabilities and highlighting the importance of integrating AI-driven tools alongside, rather than replacing, current public health strategies.","authors":["Neil K. R. Sehgal","Sunny Rai","Manuel Tonneau","Anish K. Agarwal","Joseph Cappella","Melanie Kornides","Lyle Ungar","Alison Buttenheim","Sharath Chandra Guntuku"],"url":"https://arxiv.org/abs/2504.20519"}
{"created":"2025-04-30","title":"PRISM: Projection-based Reward Integration for Scene-Aware Real-to-Sim-to-Real Transfer with Few Demonstrations","abstract":"Learning from few demonstrations to develop policies robust to variations in robot initial positions and object poses is a problem of significant practical interest in robotics. Compared to imitation learning, which often struggles to generalize from limited samples, reinforcement learning (RL) can autonomously explore to obtain robust behaviors. Training RL agents through direct interaction with the real world is often impractical and unsafe, while building simulation environments requires extensive manual effort, such as designing scenes and crafting task-specific reward functions. To address these challenges, we propose an integrated real-to-sim-to-real pipeline that constructs simulation environments based on expert demonstrations by identifying scene objects from images and retrieving their corresponding 3D models from existing libraries. We introduce a projection-based reward model for RL policy training that is supervised by a vision-language model (VLM) using human-guided object projection relationships as prompts, with the policy further fine-tuned using expert demonstrations. In general, our work focuses on the construction of simulation environments and RL-based policy training, ultimately enabling the deployment of reliable robotic control policies in real-world scenarios.","authors":["Haowen Sun","Han Wang","Chengzhong Ma","Shaolong Zhang","Jiawei Ye","Xingyu Chen","Xuguang Lan"],"url":"https://arxiv.org/abs/2504.20520"}
{"created":"2025-04-30","title":"Wavelet-Filtering of Symbolic Music Representations for Folk Tune Segmentation and Classification","abstract":"The aim of this study is to evaluate a machine-learning method in which symbolic representations of folk songs are segmented and classified into tune families with Haar-wavelet filtering. The method is compared with previously proposed Gestalt-based method. Melodies are represented as discrete symbolic pitch-time signals. We apply the continuous wavelet transform (CWT) with the Haar wavelet at specific scales, obtaining filtered versions of melodies emphasizing their information at particular time-scales. We use the filtered signal for representation and segmentation, using the wavelet coefficients' local maxima to indicate local boundaries and classify segments by means of k-nearest neighbours based on standard vector-metrics (Euclidean, cityblock), and compare the results to a Gestalt-based segmentation method and metrics applied directly to the pitch signal. We found that the wavelet based segmentation and wavelet-filtering of the pitch signal lead to better classification accuracy in cross-validated evaluation when the time-scale and other parameters are optimized.","authors":["Gissel Velarde","Tillman Weyde","David Meredith"],"url":"https://arxiv.org/abs/2504.20522"}
{"created":"2025-04-30","title":"Finite element method with Gr\\\"unwald-Letnikov type approximation in time for a constant time delay subdiffusion equation","abstract":"In this work, a subdiffusion equation with constant time delay $\\tau$ is considered. First, the regularity of the solution to the considered problem is investigated, finding that its first-order time derivative exhibits singularity at $t=0^+$ and its second-order time derivative shows singularity at both $t=0^+$ and $\\tau^+$, while the solution can be decomposed into its singular and regular components. Then, we derive a fully discrete finite element scheme to solve the considered problem based on the standard Galerkin finite element method in space and the Gr\\\"unwald-Letnikov type approximation in time. The analysis shows that the developed numerical scheme is stable. In order to discuss the error estimate, a new discrete Gronwall inequality is established. Under the above decomposition of the solution, we obtain a local error estimate in time for the developed numerical scheme. Finally, some numerical tests are provided to support our theoretical analysis.","authors":["Weiping Bu","Xueqin Zhang","Weizhi Liao","Yue Zhao"],"url":"https://arxiv.org/abs/2504.20524"}
{"created":"2025-04-30","title":"Geometry-aware Temporal Aggregation Network for Monocular 3D Lane Detection","abstract":"Monocular 3D lane detection aims to estimate 3D position of lanes from frontal-view (FV) images. However, current monocular 3D lane detection methods suffer from two limitations, including inaccurate geometric information of the predicted 3D lanes and difficulties in maintaining lane integrity. To address these issues, we seek to fully exploit the potential of multiple input frames. First, we aim at enhancing the ability to perceive the geometry of scenes by leveraging temporal geometric consistency. Second, we strive to improve the integrity of lanes by revealing more instance information from temporal sequences. Therefore, we propose a novel Geometry-aware Temporal Aggregation Network (GTA-Net) for monocular 3D lane detection. On one hand, we develop the Temporal Geometry Enhancement Module (TGEM), which exploits geometric consistency across successive frames, facilitating effective geometry perception. On the other hand, we present the Temporal Instance-aware Query Generation (TIQG), which strategically incorporates temporal cues into query generation, thereby enabling the exploration of comprehensive instance information. Experiments demonstrate that our GTA-Net achieves SoTA results, surpassing existing monocular 3D lane detection solutions.","authors":["Huan Zheng","Wencheng Han","Tianyi Yan","Cheng-zhong Xu","Jianbing Shen"],"url":"https://arxiv.org/abs/2504.20525"}
{"created":"2025-04-30","title":"Safe Bottom-Up Flexibility Provision from Distributed Energy Resources","abstract":"Modern renewables-based power systems need to tap on the flexibility of Distributed Energy Resources (DERs) connected to distribution networks. It is important, however, that DER owners/users remain in control of their assets, decisions, and objectives. At the same time, the dynamic landscape of DER-penetrated distribution networks calls for agile, data-driven flexibility management frameworks. In the face of these developments, the Multi-Agent Reinforcement Learning (MARL) paradigm is gaining significant attention, as a distributed and data-driven decision-making policy. This paper addresses the need for bottom-up DER management decisions to account for the distribution network's safety-related constraints. While the related literature on safe MARL typically assumes that network characteristics are available and incorporated into the policy's safety layer, which implies active DSO engagement, this paper ensures that self-organized DER communities are enabled to provide distribution-network-safe flexibility services without relying on the aspirational and problematic requirement of bringing the DSO in the decision-making loop.","authors":["Costas Mylonas","Emmanouel Varvarigos","Georgios Tsaousoglou"],"url":"https://arxiv.org/abs/2504.20529"}
{"created":"2025-04-30","title":"Beyond the Horizon: Decoupling UAVs Multi-View Action Recognition via Partial Order Transfer","abstract":"Action recognition in unmanned aerial vehicles (UAVs) poses unique challenges due to significant view variations along the vertical spatial axis. Unlike traditional ground-based settings, UAVs capture actions from a wide range of altitudes, resulting in considerable appearance discrepancies. We introduce a multi-view formulation tailored to varying UAV altitudes and empirically observe a partial order among views, where recognition accuracy consistently decreases as the altitude increases. This motivates a novel approach that explicitly models the hierarchical structure of UAV views to improve recognition performance across altitudes. To this end, we propose the Partial Order Guided Multi-View Network (POG-MVNet), designed to address drastic view variations by effectively leveraging view-dependent information across different altitude levels. The framework comprises three key components: a View Partition (VP) module, which uses the head-to-body ratio to group views by altitude; an Order-aware Feature Decoupling (OFD) module, which disentangles action-relevant and view-specific features under partial order guidance; and an Action Partial Order Guide (APOG), which leverages the partial order to transfer informative knowledge from easier views to support learning in more challenging ones. We conduct experiments on Drone-Action, MOD20, and UAV datasets, demonstrating that POG-MVNet significantly outperforms competing methods. For example, POG-MVNet achieves a 4.7% improvement on Drone-Action dataset and a 3.5% improvement on UAV dataset compared to state-of-the-art methods ASAT and FAR. The code for POG-MVNet will be made available soon.","authors":["Wenxuan Liu","Xian Zhong","Zhuo Zhou","Siyuan Yang","Chia-Wen Lin","Alex Chichung Kot"],"url":"https://arxiv.org/abs/2504.20530"}
{"created":"2025-04-30","title":"TriniMark: A Robust Generative Speech Watermarking Method for Trinity-Level Attribution","abstract":"The emergence of diffusion models has facilitated the generation of speech with reinforced fidelity and naturalness. While deepfake detection technologies have manifested the ability to identify AI-generated content, their efficacy decreases as generative models become increasingly sophisticated. Furthermore, current research in the field has not adequately addressed the necessity for robust watermarking to safeguard the intellectual property rights associated with synthetic speech and generative models. To remedy this deficiency, we propose a \\textbf{ro}bust generative \\textbf{s}peech wat\\textbf{e}rmarking method (TriniMark) for authenticating the generated content and safeguarding the copyrights by enabling the traceability of the diffusion model. We first design a structure-lightweight watermark encoder that embeds watermarks into the time-domain features of speech and reconstructs the waveform directly. A temporal-aware gated convolutional network is meticulously designed in the watermark decoder for bit-wise watermark recovery. Subsequently, the waveform-guided fine-tuning strategy is proposed for fine-tuning the diffusion model, which leverages the transferability of watermarks and enables the diffusion model to incorporate watermark knowledge effectively. When an attacker trains a surrogate model using the outputs of the target model, the embedded watermark can still be learned by the surrogate model and correctly extracted. Comparative experiments with state-of-the-art methods demonstrate the superior robustness of our method, particularly in countering compound attacks.","authors":["Yue Li","Weizhi Liu","Dongdong Lin"],"url":"https://arxiv.org/abs/2504.20532"}
{"created":"2025-04-30","title":"DeeP-Mod: Deep Dynamic Programming based Environment Modelling using Feature Extraction","abstract":"The DeeP-Mod framework builds an environment model using features from a Deep Dynamic Programming Network (DDPN), trained via a Deep Q-Network (DQN). While Deep Q-Learning is effective in decision-making, state information is lost in deeper DQN layers due to mixed state-action representations. We address this by using Dynamic Programming (DP) to train a DDPN, where Value Iteration ensures the output represents state values, not state-action pairs. Extracting features from the DDPN preserves state information, enabling task and action set independence. We show that a reduced DDPN can be trained using features extracted from the original DDPN trained on an identical problem. This reduced DDPN achieves faster convergence under noise and outperforms the original DDPN. Finally, we introduce the DeeP-Mod framework, which creates an environment model using the evolution of features extracted from a DDPN in response to actions. A second DDPN, which learns directly from this feature model rather than raw states, can learn an effective feature-value representation and thus optimal policy. A key advantage of DeeP-Mod is that an externally defined environment model is not needed at any stage, making DDPN applicable to a wide range of environments.","authors":["Chris Child","Lam Ngo"],"url":"https://arxiv.org/abs/2504.20535"}
{"created":"2025-04-30","title":"Starfish: Rebalancing Multi-Party Off-Chain Payment Channels","abstract":"Blockchain technology has revolutionized the way transactions are executed, but scalability remains a major challenge. Payment Channel Network (PCN), as a Layer-2 scaling solution, has been proposed to address this issue. However, skewed payments can deplete the balance of one party within a channel, restricting the ability of PCNs to transact through a path and subsequently reducing the transaction success rate. To address this issue, the technology of rebalancing has been proposed. However, existing rebalancing strategies in PCNs are limited in their capacity and efficiency. Cycle-based approaches only address rebalancing within groups of nodes that form a cycle network, while non-cycle-based approaches face high complexity of on-chain operations and limitations on rebalancing capacity. In this study, we propose Starfish, a rebalancing approach that captures the star-shaped network structure to provide high rebalancing efficiency and large channel capacity. Starfish requires only $N$-time on-chain operations to connect independent channels and aggregate the total budget of all channels. To demonstrate the correctness and advantages of our method, we provide a formal security proof of the Starfish protocol and conduct comparative experiments with existing rebalancing techniques.","authors":["Minghui Xu","Wenxuan Yu","Guangyong Shang","Guangpeng Qi","Dongliang Duan","Shan Wang","Kun Li","Yue Zhang","Xiuzhen Cheng"],"url":"https://arxiv.org/abs/2504.20536"}
{"created":"2025-04-30","title":"Autoencoder Models for Point Cloud Environmental Synthesis from WiFi Channel State Information: A Preliminary Study","abstract":"This paper introduces a deep learning framework for generating point clouds from WiFi Channel State Information data. We employ a two-stage autoencoder approach: a PointNet autoencoder with convolutional layers for point cloud generation, and a Convolutional Neural Network autoencoder to map CSI data to a matching latent space. By aligning these latent spaces, our method enables accurate environmental point cloud reconstruction from WiFi data. Experimental results validate the effectiveness of our approach, highlighting its potential for wireless sensing and environmental mapping applications.","authors":["Daniele Pannone","Danilo Avola"],"url":"https://arxiv.org/abs/2504.20541"}
{"created":"2025-04-30","title":"Digital Twin-Empowered Cooperative Autonomous Car-sharing Services: Proof-of-Concept","abstract":"This paper presents a digital twin-empowered real-time optimal delivery system specifically validated through a proof-of-concept (PoC) demonstration of a real-world autonomous car-sharing service. This study integrates real-time data from roadside units (RSUs) and connected and autonomous vehicles (CAVs) within a digital twin of a campus environment to address the dynamic challenges of urban traffic. The proposed system leverages the Age of Information (AoI) metric to optimize vehicle routing by maintaining data freshness and dynamically adapting to real-time traffic conditions. Experimental results from the PoC demonstrate a 22% improvement in delivery efficiency compared to conventional shortest-path methods that do not consider information freshness. Furthermore, digital twin-based simulation results demonstrate that this proposed system improves overall delivery efficiency by 12% and effectively reduces the peak average AoI by 23% compared to the conventional method, where each vehicle selects the shortest route without considering information freshness. This study confirms the practical feasibility of cooperative driving systems, highlighting their potential to enhance smart mobility solutions through scalable digital twin deployments in complex urban environments.","authors":["Kazuma Nonomura","Kui Wang","Zongdian Li","Tao Yu","Kei Sakaguchi"],"url":"https://arxiv.org/abs/2504.20542"}
{"created":"2025-04-30","title":"Efficient patient-centric EMR sharing block tree","abstract":"Flexible sharing of electronic medical records (EMRs) is an urgent need in healthcare, as fragmented storage creates EMR management complexity for both practitioners and patients. Blockchain has emerged as a promising solution to address the limitations of centralized EMR systems regarding interoperability, data ownership, and trust concerns. Whilst its healthcare implementation continues to face scalability challenges, particularly in uploading lag time as EMR volumes increase. In this paper, we describe the design of a novel blockchain-based data structure, MedBlockTree, which aims to solve the scalability issue in blockchain-based EMR systems, particularly low block throughput and patient awareness. MedBlockTree leverages a chameleon hash function to generate collision blocks for existing patients and expand a single chain into a growing block tree with $n$ branches that are capable of processing $n$ new blocks in a single consensus round. We also introduce the EnhancedPro consensus algorithm to manage multiple branches and maintain network consistency. Our comprehensive simulation evaluates performance across four dimensions: branch number, worker number, collision rate, and network latency. Comparative analysis against a traditional blockchain-based EMR system demonstrates outstanding throughput improvements across all dimensions, achieving processing speeds $\\nu\\cdot n$ times faster than conventional approaches.","authors":["Xiaohan Hu","Jyoti Sahni","Colin R. Simpson","Normalia Samian","Winston K. G. Seah"],"url":"https://arxiv.org/abs/2504.20544"}
{"created":"2025-04-30","title":"WakeLoc: An Ultra-Low Power, Accurate and Scalable On-Demand RTLS using Wake-Up Radios","abstract":"For future large scale robotic moon missions, the availability of infrastructure-less, cheap and low power real-time locating systems (RTLSs) is critical. Traditional RTLS face significant trade-offs between power consumption and localization latency, often requiring anchors to be connected to the power grid or sacrificing speed for energy efficiency. This paper proposes WakeLoc, an on-demand RTLS based on ultra-wideband (UWB), enabling both low-latency and ultra-low power consumption by leveraging UWB wake-up radios (WuRs). In WakeLoc, tags independently start a localization procedure by sending a wake-up call (WuC) to anchors, before performing the actual localization. Distributed tags equipped with WuRs listen to the WuC and use passive listening of the UWB messages to determine their own position. Experimental measurements demonstrate that the localization accuracy in a 2D setup achieves less than 12.9cm error, both for the active and the passive tag. Additional power simulations based on real-world measurements were performed in a realistic environment, showing that anchors can achieve a power consumption as low as 15.53{\\mu}W while the RTLS performs one on-demand localization per minute for 5 tags, thus operate up to 5.01 years on a single coin cell battery (690mWh).","authors":["Silvano Cortesi","Christian Vogt","Michele Magno"],"url":"https://arxiv.org/abs/2504.20545"}
{"created":"2025-04-30","title":"Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records","abstract":"The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an openly available MIMIC-IV benchmark for electronic health records (EHRs) to address this issue. First, we integrate the MIMIC-IV data within the Hugging Face datasets library to allow an easy share and use of this collection. Second, we investigate the application of templates to convert EHR tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the mortality of patients task show that fine-tuned text-based models are competitive against robust tabular classifiers. In contrast, zero-shot LLMs struggle to leverage EHR representations. This study underlines the potential of text-based approaches in the medical field and highlights areas for further improvement.","authors":["Jesus Lovon (IRIT-IRIS)","Thouria Ben-Haddi (IRIT-IRIS)","Jules Di Scala (IRIT-IRIS)","Jose G. Moreno (IRIT-IRIS)","Lynda Tamine (IRIT-IRIS)"],"url":"https://arxiv.org/abs/2504.20547"}
{"created":"2025-04-30","title":"Identification over Poisson ISI Channels: Feedback and Molecular Applications","abstract":"Molecular communication (MC) enables information transfer via molecules, making it ideal for biomedical applications where traditional methods fall short. In many such scenarios, identifying specific events is more critical than decoding full messages, motivating the use of deterministic identification (DI). This paper investigates DI over discrete-time Poisson channels (DTPCs) with inter-symbol interference (ISI), a realistic setting due to channel memory effects. We improve the known upper bound on DI capacity under power constraints from $\\frac{3}{2} + \\kappa$ to $\\frac{1 + \\kappa}{2}$. Additionally, we present the first results on deterministic identification with feedback (DIF) in this context, providing a constructive lower bound. These findings enhance the theoretical understanding of MC and support more efficient, feedback-driven biomedical systems.","authors":["Yaning Zhao","Pau Colomer","Holger Boche","Christian Deppe"],"url":"https://arxiv.org/abs/2504.20550"}
{"created":"2025-04-30","title":"BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters","abstract":"This project introduces BrAIcht, an AI conversational agent that creates dialogues in the distinctive style of the famous German playwright Bertolt Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7 billion parameters and a modified version of the base Llama2 suitable for German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of other German plays that are stylistically similar to Bertolt Brecht are used to form a more di-erse dataset. Due to the limited memory capacity, a parameterefficient fine-tuning technique called QLoRA is implemented to train the large language model. The results, based on BLEU score and perplexity, show very promising performance of BrAIcht in generating dialogues in the style of Bertolt Brecht.","authors":["Baz Roland (LIASD)","Kristina Malyseva (LIASD)","Anna Pappa (LIASD)","Tristan Cazenave (APA)"],"url":"https://arxiv.org/abs/2504.20552"}
{"created":"2025-04-30","title":"Characterizing the Polynomial-Time Minimizable $\\omega$-Automata","abstract":"A central question in the theory of automata is which classes of automata can be minimized in polynomial time. We close the remaining gaps for deterministic and history-deterministic automata over infinite words by proving that deterministic co-B\\\"uchi automata with transition-based acceptance are NP-hard to minimize, as are history-deterministic B\\\"uchi automata with transition-based acceptance.","authors":["Bader Abu Radi","R\\\"udiger Ehlers"],"url":"https://arxiv.org/abs/2504.20553"}
{"created":"2025-04-30","title":"From regular expressions to deterministic finite automata: $2^{\\frac{n}{2}+\\sqrt{n}(\\log n)^{\\Theta(1)}}$ states are necessary and sufficient","abstract":"It is proved that every regular expression of alphabetic width $n$, that is, with $n$ occurrences of symbols of the alphabet, can be transformed into a deterministic finite automaton (DFA) with $2^{\\frac{n}{2}+(\\frac{\\log_2 e}{2\\sqrt{2}}+o(1))\\sqrt{n\\ln n}}$ states recognizing the same language (the best upper bound up to date is $2^n$). At the same time, it is also shown that this bound is close to optimal, namely, that there exist regular expressions of alphabetic width $n$ over a two-symbol alphabet, such that every DFA for the same language has at least $2^{\\frac{n}{2}+(\\sqrt{2} + o(1))\\sqrt{\\frac{n}{\\ln n}}}$ states (the previously known lower bound is $\\frac{5}{4}2^{\\frac{n}{2}}$). The same bounds are obtained for an intermediate problem of determinizing nondetermistic finite automata (NFA) with each state having all incoming transitions by the same symbol.","authors":["Olga Martynova","Alexander Okhotin"],"url":"https://arxiv.org/abs/2504.20555"}
{"created":"2025-04-30","title":"Mutual Information Minimization for Side-Channel Attack Resistance via Optimal Noise Injection","abstract":"Side-channel attacks (SCAs) pose a serious threat to system security by extracting secret keys through physical leakages such as power consumption, timing variations, and electromagnetic emissions. Among existing countermeasures, artificial noise injection is recognized as one of the most effective techniques. However, its high power consumption poses a major challenge for resource-constrained systems such as Internet of Things (IoT) devices, motivating the development of more efficient protection schemes. In this paper, we model SCAs as a communication channel and aim to suppress information leakage by minimizing the mutual information between the secret information and side-channel observations, subject to a power constraint on the artificial noise. We propose an optimal artificial noise injection method to minimize the mutual information in systems with Gaussian inputs. Specifically, we formulate two convex optimization problems: 1) minimizing the total mutual information, and 2) minimizing the maximum mutual information across observations. Numerical results show that the proposed methods significantly reduce both total and maximum mutual information compared to conventional techniques, confirming their effectiveness for resource-constrained, security-critical systems.","authors":["Jiheon Woo","Daewon Seo","Young-Sik Kim","Namyoon Lee","Yuval Cassuto","Yongjune Kim"],"url":"https://arxiv.org/abs/2504.20556"}
{"created":"2025-04-30","title":"SNR-aware Semantic Image Transmission with Deep Learning-based Channel Estimation in Fading Channels","abstract":"Semantic communications (SCs) play a central role in shaping the future of the sixth generation (6G) wireless systems, which leverage rapid advances in deep learning (DL). In this regard, end-to-end optimized DL-based joint source-channel coding (JSCC) has been adopted to achieve SCs, particularly in image transmission. Utilizing vision transformers in the encoder/decoder design has enabled significant advancements in image semantic extraction, surpassing traditional convolutional neural networks (CNNs). In this paper, we propose a new JSCC paradigm for image transmission, namely Swin semantic image transmission (SwinSIT), based on the Swin transformer. The Swin transformer is employed to construct both the semantic encoder and decoder for efficient image semantic extraction and reconstruction. Inspired by the squeezing-and-excitation (SE) network, we introduce a signal-to-noise-ratio (SNR)-aware module that utilizes SNR feedback to adaptively perform a double-phase enhancement for the encoder-extracted semantic map and its noisy version at the decoder. Additionally, a CNN-based channel estimator and compensator (CEAC) module repurposes an image-denoising CNN to mitigate fading channel effects. To optimize deployment in resource-constrained IoT devices, a joint pruning and quantization scheme compresses the SwinSIT model. Simulations evaluate the SwinSIT performance against conventional benchmarks demonstrating its effectiveness. Moreover, the model's compressed version substantially reduces its size while maintaining favorable PSNR performance.","authors":["Mahmoud M. Salim","Mohamed S. Abdalzaher","Ali H. Muqaibel","Hussein A. Elsayed","Inkyu Lee"],"url":"https://arxiv.org/abs/2504.20557"}
{"created":"2025-04-30","title":"Generate more than one child in your co-evolutionary semi-supervised learning GAN","abstract":"Generative Adversarial Networks (GANs) are very useful methods to address semi-supervised learning (SSL) datasets, thanks to their ability to generate samples similar to real data. This approach, called SSL-GAN has attracted many researchers in the last decade. Evolutionary algorithms have been used to guide the evolution and training of SSL-GANs with great success. In particular, several co-evolutionary approaches have been applied where the two networks of a GAN (the generator and the discriminator) are evolved in separate populations. The co-evolutionary approaches published to date assume some spatial structure of the populations, based on the ideas of cellular evolutionary algorithms. They also create one single individual per generation and follow a generational replacement strategy in the evolution. In this paper, we re-consider those algorithmic design decisions and propose a new co-evolutionary approach, called Co-evolutionary Elitist SSL-GAN (CE-SSLGAN), with panmictic population, elitist replacement, and more than one individual in the offspring. We evaluate the performance of our proposed method using three standard benchmark datasets. The results show that creating more than one offspring per population and using elitism improves the results in comparison with a classical SSL-GAN.","authors":["Francisco Sede\\~no","Jamal Toutouh","Francisco Chicano"],"url":"https://arxiv.org/abs/2504.20560"}
{"created":"2025-04-30","title":"Turing machines deciders, part I","abstract":"The Busy Beaver Challenge (or bbchallenge) aims at collaboratively solving the following conjecture: \"$S(5) = 47{,}176{,}870$\" [Rad\\'o, 1962], [Marxen and Buntrock, 1990], [Aaronson, 2020]. This conjecture says that if a 5-state Turing machine runs for more than 47,176,870 steps without halting, then it will never halt -- starting from the all-0 tape. Proving this conjecture amounts to deciding whether 181,385,789 Turing machines with 5 states halt or not -- starting from the all-0 tape [bbchallenge, 2025]. To do so, we write $\\textit{deciders}$: programs that take as input a Turing machine and output either HALT, NONHALT, or UNKNOWN. Each decider is specialised in recognising a particular type of non-halting behavior.","authors":["The bbchallenge Collaboration","Justin Blanchard","Konrad Deka","Nathan Fenner","Tony Guilfoyle","Iijil","Maja K\\k{a}dzio{\\l}ka","Pavel Kropitz","Shawn Ligocki","Pascal Michel","Mateusz Na\\'sciszewski","Tristan St\\'erin"],"url":"https://arxiv.org/abs/2504.20563"}
{"created":"2025-04-30","title":"Inclusive Training Separation and Implicit Knowledge Interaction for Balanced Online Class-Incremental Learning","abstract":"Online class-incremental learning (OCIL) focuses on gradually learning new classes (called plasticity) from a stream of data in a single-pass, while concurrently preserving knowledge of previously learned classes (called stability). The primary challenge in OCIL lies in maintaining a good balance between the knowledge of old and new classes within the continually updated model. Most existing methods rely on explicit knowledge interaction through experience replay, and often employ exclusive training separation to address bias problems. Nevertheless, it still remains a big challenge to achieve a well-balanced learner, as these methods often exhibit either reduced plasticity or limited stability due to difficulties in continually integrating knowledge in the OCIL setting. In this paper, we propose a novel replay-based method, called Balanced Online Incremental Learning (BOIL), which can achieve both high plasticity and stability, thus ensuring more balanced performance in OCIL. Our BOIL method proposes an inclusive training separation strategy using dual classifiers so that knowledge from both old and new classes can effectively be integrated into the model, while introducing implicit approaches for transferring knowledge across the two classifiers. Extensive experimental evaluations over three widely-used OCIL benchmark datasets demonstrate the superiority of BOIL, showing more balanced yet better performance compared to state-of-the-art replay-based OCIL methods.","authors":["Shunjie Wen","Thomas Heinis","Dong-Wan Choi"],"url":"https://arxiv.org/abs/2504.20566"}
{"created":"2025-04-30","title":"Explanation format does not matter; but explanations do - An Eggsbert study on explaining Bayesian Optimisation tasks","abstract":"Bayesian Optimisation (BO) is a family of methods for finding optimal parameters when the underlying function to be optimised is unknown. BO is used, for example, for hyperparameter tuning in machine learning and as an expert support tool for tuning cyberphysical systems. For settings where humans are involved in the tuning task, methods have been developed to explain BO (Explainable Bayesian Optimization, XBO). However, there is little guidance on how to present XBO results to humans so that they can tune the system effectively and efficiently. In this paper, we investigate how the XBO explanation format affects users' task performance, task load, understanding and trust in XBO. We chose a task that is accessible to a wide range of users. Specifically, we set up an egg cooking scenario with 6 parameters that participants had to adjust to achieve a perfect soft-boiled egg. We compared three different explanation formats: a bar chart, a list of rules and a textual explanation in a between-subjects online study with 213 participants. Our results show that adding any type of explanation increases task success, reduces the number of trials needed to achieve success, and improves comprehension and confidence. While explanations add more information for participants to process, we found no increase in user task load. We also found that the aforementioned results were independent of the explanation format; all formats had a similar effect.This is an interesting finding for practical applications, as it suggests that explanations can be added to BO tuning tasks without the burden of designing or selecting specific explanation formats. In the future, it would be interesting to investigate scenarios of prolonged use of the explanation formats and whether they have different effects on users' mental models of the underlying system.","authors":["Tanmay Chakraborty","Marion Koelle","J\\\"org Schl\\\"otterer","Nadine Schlicker","Christian Wirth","Christin Seifert"],"url":"https://arxiv.org/abs/2504.20567"}
{"created":"2025-04-30","title":"Digital Shielding for Cross-Domain Wi-Fi Signal Adaptation using Relativistic Average Generative Adversarial Network","abstract":"Wi-Fi sensing uses radio-frequency signals from Wi-Fi devices to analyze environments, enabling tasks such as tracking people, detecting intrusions, and recognizing gestures. The rise of this technology is driven by the IEEE 802.11bf standard and growing demand for tools that can ensure privacy and operate through obstacles. However, the performance of Wi-Fi sensing is heavily influenced by environmental conditions, especially when extracting spatial and temporal features from the surrounding scene. A key challenge is achieving robust generalization across domains, ensuring stable performance even when the sensing environment changes significantly. This paper introduces a novel deep learning model for cross-domain adaptation of Wi-Fi signals, inspired by physical signal shielding. The model uses a Relativistic average Generative Adversarial Network (RaGAN) with Bidirectional Long Short-Term Memory (Bi-LSTM) architectures for both the generator and discriminator. To simulate physical shielding, an acrylic box lined with electromagnetic shielding fabric was constructed, mimicking a Faraday cage. Wi-Fi signal spectra were collected from various materials both inside (domain-free) and outside (domain-dependent) the box to train the model. A multi-class Support Vector Machine (SVM) was trained on domain-free spectra and tested on signals denoised by the RaGAN. The system achieved 96% accuracy and demonstrated strong material discrimination capabilities, offering potential for use in security applications to identify concealed objects based on their composition.","authors":["Danilo Avola","Federica Bruni","Gian Luca Foresti","Daniele Pannone","Amedeo Ranaldi"],"url":"https://arxiv.org/abs/2504.20568"}
{"created":"2025-04-30","title":"VIMU: Effective Physics-based Realtime Detection and Recovery against Stealthy Attacks on UAVs","abstract":"Sensor attacks on robotic vehicles have become pervasive and manipulative. Their latest advancements exploit sensor and detector characteristics to bypass detection. Recent security efforts have leveraged the physics-based model to detect or mitigate sensor attacks. However, these approaches are only resilient to a few sensor attacks and still need improvement in detection effectiveness. We present VIMU, an efficient sensor attack detection and resilience system for unmanned aerial vehicles. We propose a detection algorithm, CS-EMA, that leverages low-pass filtering to identify stealthy gyroscope attacks while achieving an overall effective sensor attack detection. We develop a fine-grained nonlinear physical model with precise aerodynamic and propulsion wrench modeling. We also augment the state estimation with a FIFO buffer safeguard to mitigate the impact of high-rate IMU attacks. The proposed physical model and buffer safeguard provide an effective system state recovery toward maintaining flight stability. We implement VIMU on PX4 autopilot. The evaluation results demonstrate the effectiveness of VIMU in detecting and mitigating various realistic sensor attacks, especially stealthy attacks.","authors":["Yunbo Wang","Cong Sun","Qiaosen Liu","Bingnan Su","Zongxu Zhang","Michael Norris","Gang Tan","Jianfeng Ma"],"url":"https://arxiv.org/abs/2504.20569"}
{"created":"2025-04-30","title":"ReCIT: Reconstructing Full Private Data from Gradient in Parameter-Efficient Fine-Tuning of Large Language Models","abstract":"Parameter-efficient fine-tuning (PEFT) has emerged as a practical solution for adapting large language models (LLMs) to custom datasets with significantly reduced computational cost. When carrying out PEFT under collaborative learning scenarios (e.g., federated learning), it is often required to exchange model updates (or gradients) across parties. These gradients, even with limited dimensions, can cause severe breach of data privacy. Recent works have shown that both contextual prefixes and personally identifiable information (PII) can be exposed through gradients. However, \\emph{simultaneously} and \\emph{accurately} recovering both components from the same training instance remains infeasible due to the following challenges: 1) limited number of PEFT parameters; 2) high-dimensional token spaces; and 3) large batch sizes. We propose ReCIT, a novel privacy attack that addresses all challenges, and achieves recovery of \\emph{full} private data from PEFT gradients with high fidelity. Specifically, ReCIT proposes to enhance the memorization capability of the pre-trained model through malicious fine-tuning with Personal Notes; ReCIT also proposes a novel filter-based token extraction technique and a token pairing mechanism, to accurately reconstruct tokens from the training sequences with large batch sizes. Extensive evaluations show that ReCIT consistently outperforms state-of-the-art gradient inversion and memorization-based attacks across different PEFT paradigms. It achieves up to 10$\\times$ higher PII recovery rates and remains effective across varying batch sizes, especially in settings where prefix reconstruction is intractable for conventional approaches. These findings highlight an urgent need to reassess the privacy guarantees of PEFT, especially in decentralized or shared training environments.","authors":["Jin Xie","Ruishi He","Songze Li","Xiaojun Jia","Shouling Ji"],"url":"https://arxiv.org/abs/2504.20570"}
{"created":"2025-04-30","title":"Reinforcement Learning for Reasoning in Large Language Models with One Training Example","abstract":"We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6%, and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7%. This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which includes the aforementioned example. Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples (many of which yield approximately 30% or greater improvement on MATH500 when employed as a single training example). In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-domain generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by adding entropy loss with an appropriate coefficient) in 1-shot RLVR training. As a bonus, we observe that applying entropy loss alone, without any outcome reward, significantly enhances Qwen2.5-Math-1.5B's performance on MATH500 by 27.4%. These findings can inspire future work on RLVR data efficiency and encourage a re-examination of both recent progress and the underlying mechanisms in RLVR. Our code, model, and data are open source at https://github.com/ypwang61/One-Shot-RLVR","authors":["Yiping Wang","Qing Yang","Zhiyuan Zeng","Liliang Ren","Lucas Liu","Baolin Peng","Hao Cheng","Xuehai He","Kuan Wang","Jianfeng Gao","Weizhu Chen","Shuohang Wang","Simon Shaolei Du","Yelong Shen"],"url":"https://arxiv.org/abs/2504.20571"}
{"created":"2025-04-30","title":"Representation Learning Preserving Ignorability and Covariate Matching for Treatment Effects","abstract":"Estimating treatment effects from observational data is challenging due to two main reasons: (a) hidden confounding, and (b) covariate mismatch (control and treatment groups not having identical distributions). Long lines of works exist that address only either of these issues. To address the former, conventional techniques that require detailed knowledge in the form of causal graphs have been proposed. For the latter, covariate matching and importance weighting methods have been used. Recently, there has been progress in combining testable independencies with partial side information for tackling hidden confounding. A common framework to address both hidden confounding and selection bias is missing. We propose neural architectures that aim to learn a representation of pre-treatment covariates that is a valid adjustment and also satisfies covariate matching constraints. We combine two different neural architectures: one based on gradient matching across domains created by subsampling a suitable anchor variable that assumes causal side information, followed by the other, a covariate matching transformation. We prove that approximately invariant representations yield approximate valid adjustment sets which would enable an interval around the true causal effect. In contrast to usual sensitivity analysis, where an unknown nuisance parameter is varied, we have a testable approximation yielding a bound on the effect estimate. We also outperform various baselines with respect to ATE and PEHE errors on causal benchmarks that include IHDP, Jobs, Cattaneo, and an image-based Crowd Management dataset.","authors":["Praharsh Nanavati","Ranjitha Prasad","Karthikeyan Shanmugam"],"url":"https://arxiv.org/abs/2504.20579"}
{"created":"2025-04-30","title":"ClonEval: An Open Voice Cloning Benchmark","abstract":"We present a novel benchmark for voice cloning text-to-speech models. The benchmark consists of an evaluation protocol, an open-source library for assessing the performance of voice cloning models, and an accompanying leaderboard. The paper discusses design considerations and presents a detailed description of the evaluation procedure. The usage of the software library is explained, along with the organization of results on the leaderboard.","authors":["Iwona Christop","Tomasz Kuczy\\'nski","Marek Kubis"],"url":"https://arxiv.org/abs/2504.20581"}
{"created":"2025-04-30","title":"Hydra: Marker-Free RGB-D Hand-Eye Calibration","abstract":"This work presents an RGB-D imaging-based approach to marker-free hand-eye calibration using a novel implementation of the iterative closest point (ICP) algorithm with a robust point-to-plane (PTP) objective formulated on a Lie algebra. Its applicability is demonstrated through comprehensive experiments using three well known serial manipulators and two RGB-D cameras. With only three randomly chosen robot configurations, our approach achieves approximately 90% successful calibrations, demonstrating 2-3x higher convergence rates to the global optimum compared to both marker-based and marker-free baselines. We also report 2 orders of magnitude faster convergence time (0.8 +/- 0.4 s) for 9 robot configurations over other marker-free methods. Our method exhibits significantly improved accuracy (5 mm in task space) over classical approaches (7 mm in task space) whilst being marker-free. The benchmarking dataset and code are open sourced under Apache 2.0 License, and a ROS 2 integration with robot abstraction is provided to facilitate deployment.","authors":["Martin Huber","Huanyu Tian","Christopher E. Mower","Lucas-Raphael M\\\"uller","S\\'ebastien Ourselin","Christos Bergeles","Tom Vercauteren"],"url":"https://arxiv.org/abs/2504.20584"}
{"created":"2025-04-30","title":"Independent Learning in Performative Markov Potential Games","abstract":"Performative Reinforcement Learning (PRL) refers to a scenario in which the deployed policy changes the reward and transition dynamics of the underlying environment. In this work, we study multi-agent PRL by incorporating performative effects into Markov Potential Games (MPGs). We introduce the notion of a performatively stable equilibrium (PSE) and show that it always exists under a reasonable sensitivity assumption. We then provide convergence results for state-of-the-art algorithms used to solve MPGs. Specifically, we show that independent policy gradient ascent (IPGA) and independent natural policy gradient (INPG) converge to an approximate PSE in the best-iterate sense, with an additional term that accounts for the performative effects. Furthermore, we show that INPG asymptotically converges to a PSE in the last-iterate sense. As the performative effects vanish, we recover the convergence rates from prior work. For a special case of our game, we provide finite-time last-iterate convergence results for a repeated retraining approach, in which agents independently optimize a surrogate objective. We conduct extensive experiments to validate our theoretical findings.","authors":["Rilind Sahitaj","Paulius Sasnauskas","Yi\\u{g}it Yal{\\i}n","Debmalya Mandal","Goran Radanovi\\'c"],"url":"https://arxiv.org/abs/2504.20593"}
{"created":"2025-04-30","title":"ReasonIR: Training Retrievers for Reasoning Tasks","abstract":"We present ReasonIR-8B, the first retriever specifically trained for general reasoning tasks. Existing retrievers have shown limited gains on reasoning tasks, in part because existing training datasets focus on short factual queries tied to documents that straightforwardly answer them. We develop a synthetic data generation pipeline that, for each document, our pipeline creates a challenging and relevant query, along with a plausibly related but ultimately unhelpful hard negative. By training on a mixture of our synthetic data and existing public data, ReasonIR-8B achieves a new state-of-the-art of 29.9 nDCG@10 without reranker and 36.9 nDCG@10 with reranker on BRIGHT, a widely-used reasoning-intensive information retrieval (IR) benchmark. When applied to RAG tasks, ReasonIR-8B improves MMLU and GPQA performance by 6.4% and 22.6% respectively, relative to the closed-book baseline, outperforming other retrievers and search engines. In addition, ReasonIR-8B uses test-time compute more effectively: on BRIGHT, its performance consistently increases with longer and more information-rich rewritten queries; it continues to outperform other retrievers when combined with an LLM reranker. Our training recipe is general and can be easily extended to future LLMs; to this end, we open-source our code, data, and model.","authors":["Rulin Shao","Rui Qiao","Varsha Kishore","Niklas Muennighoff","Xi Victoria Lin","Daniela Rus","Bryan Kian Hsiang Low","Sewon Min","Wen-tau Yih","Pang Wei Koh","Luke Zettlemoyer"],"url":"https://arxiv.org/abs/2504.20595"}
{"created":"2025-04-30","title":"Natural Language Processing tools for Pharmaceutical Manufacturing Information Extraction from Patents Natural Language Processing (NLP) tools for Pharmaceutical Manufacturing Information Extraction from Patents","abstract":"Abundant and diverse data on medicines manufacturing and other lifecycle components has been made easily accessible in the last decades. However, a significant proportion of this information is characterised by not being tabulated and usable for machine learning purposes. Thus, natural language processing tools have been used to build databases in domains such as biomedical and chemical to address this limitation. This has allowed the development of artificial intelligence applications, which have improved drug discovery and treatments. In the pharmaceutical manufacturing context, some initiatives and datasets for primary processing can be found, but the manufacturing of drug products is an area which is still lacking, to the best of our knowledge. This works aims to explore and adapt NLP tools used in other domains to extract information on both primary and secondary manufacturing, employing patents as the main source of data. Thus, two independent, but complementary, models were developed comprising a method to select fragments of text that contain manufacturing data, and a named entity recognition system that enables extracting information on operations, materials, and conditions of a process. For the first model, the identification of relevant sections was achieved using an unsupervised approach combining Latent Dirichlet Allocation and k-Means clustering. The performance of this model measured as a Cohen's kappa between model output and manual revision was higher than 90%. NER model consisted of a deep neural network, and an f1-score micro average of 84.2% was obtained which is comparable to other works. Some considerations for these tools to be used in data extraction are discussed throughout this document.","authors":["Diego Alvarado-Maldonado","Blair Johnston","Cameron J. Brown"],"url":"https://arxiv.org/abs/2504.20598"}
{"created":"2025-04-30","title":"PartHOI: Part-based Hand-Object Interaction Transfer via Generalized Cylinders","abstract":"Learning-based methods to understand and model hand-object interactions (HOI) require a large amount of high-quality HOI data. One way to create HOI data is to transfer hand poses from a source object to another based on the objects' geometry. However, current methods for transferring hand poses between objects rely on shape matching, limiting the ability to transfer poses across different categories due to differences in their shapes and sizes. We observe that HOI often involves specific semantic parts of objects, which often have more consistent shapes across categories. In addition, constructing size-invariant correspondences between these parts is important for cross-category transfer. Based on these insights, we introduce a novel method PartHOI for part-based HOI transfer. Using a generalized cylinder representation to parameterize an object parts' geometry, PartHOI establishes a robust geometric correspondence between object parts, and enables the transfer of contact points. Given the transferred points, we optimize a hand pose to fit the target object well. Qualitative and quantitative results demonstrate that our method can generalize HOI transfers well even for cross-category objects, and produce high-fidelity results that are superior to the existing methods.","authors":["Qiaochu Wang","Chufeng Xiao","Manfred Lau","Hongbo Fu"],"url":"https://arxiv.org/abs/2504.20599"}
{"created":"2025-04-30","title":"A citation index bridging Hirsch's h and Egghe's g","abstract":"We propose a citation index $\\nu$ (``nu'') and show that it lies between the classical $h$-index and $g$-index. This idea is then generalized to a monotone parametric family $(\\nu_\\alpha)$ ($\\alpha\\ge 0$), whereby $h=\\nu_0$ and $\\nu=\\nu_1$, while the limiting value $\\nu_\\infty$ is expressed in terms of the maximum citation.","authors":["Ruheyan Nuermaimaiti","Leonid V. Bogachev","Jochen Voss"],"url":"https://arxiv.org/abs/2504.20600"}
{"created":"2025-04-30","title":"Purifying, Labeling, and Utilizing: A High-Quality Pipeline for Small Object Detection","abstract":"Small object detection is a broadly investigated research task and is commonly conceptualized as a \"pipeline-style\" engineering process. In the upstream, images serve as raw materials for processing in the detection pipeline, where pre-trained models are employed to generate initial feature maps. In the midstream, an assigner selects training positive and negative samples. Subsequently, these samples and features are fed into the downstream for classification and regression. Previous small object detection methods often focused on improving isolated stages of the pipeline, thereby neglecting holistic optimization and consequently constraining overall performance gains. To address this issue, we have optimized three key aspects, namely Purifying, Labeling, and Utilizing, in this pipeline, proposing a high-quality Small object detection framework termed PLUSNet. Specifically, PLUSNet comprises three sequential components: the Hierarchical Feature Purifier (HFP) for purifying upstream features, the Multiple Criteria Label Assignment (MCLA) for improving the quality of midstream training samples, and the Frequency Decoupled Head (FDHead) for more effectively exploiting information to accomplish downstream tasks. The proposed PLUS modules are readily integrable into various object detectors, thus enhancing their detection capabilities in multi-scale scenarios. Extensive experiments demonstrate the proposed PLUSNet consistently achieves significant and consistent improvements across multiple datasets for small object detection.","authors":["Siwei Wang","Zhiwei Chen","Liujuan Cao","Rongrong Ji"],"url":"https://arxiv.org/abs/2504.20602"}
{"created":"2025-04-30","title":"TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models","abstract":"Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.","authors":["Mihai Nadas","Laura Diosan","Andrei Piscoran","Andreea Tomescu"],"url":"https://arxiv.org/abs/2504.20605"}
{"created":"2025-04-30","title":"EfficientHuman: Efficient Training and Reconstruction of Moving Human using Articulated 2D Gaussian","abstract":"3D Gaussian Splatting (3DGS) has been recognized as a pioneering technique in scene reconstruction and novel view synthesis. Recent work on reconstructing the 3D human body using 3DGS attempts to leverage prior information on human pose to enhance rendering quality and improve training speed. However, it struggles to effectively fit dynamic surface planes due to multi-view inconsistency and redundant Gaussians. This inconsistency arises because Gaussian ellipsoids cannot accurately represent the surfaces of dynamic objects, which hinders the rapid reconstruction of the dynamic human body. Meanwhile, the prevalence of redundant Gaussians means that the training time of these works is still not ideal for quickly fitting a dynamic human body. To address these, we propose EfficientHuman, a model that quickly accomplishes the dynamic reconstruction of the human body using Articulated 2D Gaussian while ensuring high rendering quality. The key innovation involves encoding Gaussian splats as Articulated 2D Gaussian surfels in canonical space and then transforming them to pose space via Linear Blend Skinning (LBS) to achieve efficient pose transformations. Unlike 3D Gaussians, Articulated 2D Gaussian surfels can quickly conform to the dynamic human body while ensuring view-consistent geometries. Additionally, we introduce a pose calibration module and an LBS optimization module to achieve precise fitting of dynamic human poses, enhancing the model's performance. Extensive experiments on the ZJU-MoCap dataset demonstrate that EfficientHuman achieves rapid 3D dynamic human reconstruction in less than a minute on average, which is 20 seconds faster than the current state-of-the-art method, while also reducing the number of redundant Gaussians.","authors":["Hao Tian","Rui Liu","Wen Shen","Yilong Hu","Zhihao Zheng","Xiaolin Qin"],"url":"https://arxiv.org/abs/2504.20607"}
{"created":"2025-04-30","title":"WenyanGPT: A Large Language Model for Classical Chinese Tasks","abstract":"Classical Chinese, as the core carrier of Chinese culture, plays a crucial role in the inheritance and study of ancient literature. However, existing natural language processing models primarily optimize for Modern Chinese, resulting in inadequate performance on Classical Chinese. This paper presents a comprehensive solution for Classical Chinese language processing. By continuing pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we construct a large language model, WenyanGPT, which is specifically designed for Classical Chinese tasks. Additionally, we develop an evaluation benchmark dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that WenyanGPT significantly outperforms current advanced LLMs in various Classical Chinese tasks. We make the model's training data, instruction fine-tuning data\\footnote, and evaluation benchmark dataset publicly available to promote further research and development in the field of Classical Chinese processing.","authors":["Xinyu Yao","Mengdi Wang","Bo Chen","Xiaobing Zhao"],"url":"https://arxiv.org/abs/2504.20609"}
{"created":"2025-04-30","title":"Information Retrieval in the Age of Generative AI: The RGB Model","abstract":"The advent of Large Language Models (LLMs) and generative AI is fundamentally transforming information retrieval and processing on the Internet, bringing both great potential and significant concerns regarding content authenticity and reliability. This paper presents a novel quantitative approach to shed light on the complex information dynamics arising from the growing use of generative AI tools. Despite their significant impact on the digital ecosystem, these dynamics remain largely uncharted and poorly understood. We propose a stochastic model to characterize the generation, indexing, and dissemination of information in response to new topics. This scenario particularly challenges current LLMs, which often rely on real-time Retrieval-Augmented Generation (RAG) techniques to overcome their static knowledge limitations. Our findings suggest that the rapid pace of generative AI adoption, combined with increasing user reliance, can outpace human verification, escalating the risk of inaccurate information proliferation across digital resources. An in-depth analysis of Stack Exchange data confirms that high-quality answers inevitably require substantial time and human effort to emerge. This underscores the considerable risks associated with generating persuasive text in response to new questions and highlights the critical need for responsible development and deployment of future generative AI tools.","authors":["Michele Garetto","Alessandro Cornacchia","Franco Galante","Emilio Leonardi","Alessandro Nordio","Alberto Tarable"],"url":"https://arxiv.org/abs/2504.20610"}
{"created":"2025-04-30","title":"The Hidden Risks of LLM-Generated Web Application Code: A Security-Centric Evaluation of Code Generation Capabilities in Large Language Models","abstract":"The rapid advancement of Large Language Models (LLMs) has enhanced software development processes, minimizing the time and effort required for coding and enhancing developer productivity. However, despite their potential benefits, code generated by LLMs has been shown to generate insecure code in controlled environments, raising critical concerns about their reliability and security in real-world applications. This paper uses predefined security parameters to evaluate the security compliance of LLM-generated code across multiple models, such as ChatGPT, DeepSeek, Claude, Gemini and Grok. The analysis reveals critical vulnerabilities in authentication mechanisms, session management, input validation and HTTP security headers. Although some models implement security measures to a limited extent, none fully align with industry best practices, highlighting the associated risks in automated software development. Our findings underscore that human expertise is crucial to ensure secure software deployment or review of LLM-generated code. Also, there is a need for robust security assessment frameworks to enhance the reliability of LLM-generated code in real-world applications.","authors":["Swaroop Dora","Deven Lunkad","Naziya Aslam","S. Venkatesan","Sandeep Kumar Shukla"],"url":"https://arxiv.org/abs/2504.20612"}
{"created":"2025-04-30","title":"Multi-Sensor Fusion for Quadruped Robot State Estimation using Invariant Filtering and Smoothing","abstract":"This letter introduces two multi-sensor state estimation frameworks for quadruped robots, built on the Invariant Extended Kalman Filter (InEKF) and Invariant Smoother (IS). The proposed methods, named E-InEKF and E-IS, fuse kinematics, IMU, LiDAR, and GPS data to mitigate position drift, particularly along the z-axis, a common issue in proprioceptive-based approaches. We derived observation models that satisfy group-affine properties to integrate LiDAR odometry and GPS into InEKF and IS. LiDAR odometry is incorporated using Iterative Closest Point (ICP) registration on a parallel thread, preserving the computational efficiency of proprioceptive-based state estimation. We evaluate E-InEKF and E-IS with and without exteroceptive sensors, benchmarking them against LiDAR-based odometry methods in indoor and outdoor experiments using the KAIST HOUND2 robot. Our methods achieve lower Relative Position Errors (RPE) and significantly reduce Absolute Trajectory Error (ATE), with improvements of up to 28% indoors and 40% outdoors compared to LIO-SAM and FAST-LIO2. Additionally, we compare E-InEKF and E-IS in terms of computational efficiency and accuracy.","authors":["Ylenia Nistic\\`o","Hajun Kim","Jo\\~ao Carlos Virgolino Soares","Geoff Fink","Hae-Won Park","Claudio Semini"],"url":"https://arxiv.org/abs/2504.20615"}
{"created":"2025-04-30","title":"Statistical Channel Based Low-Complexity Rotation and Position Optimization for 6D Movable Antennas Enabled Wireless Communication","abstract":"Six-dimensional movable antenna (6DMA) is a promising technology to fully exploit spatial variation in wireless channels by allowing flexible adjustment of three-dimensional (3D) positions and rotations of antennas at the transceiver. In this paper, we investigate the practical low-complexity design of 6DMA-enabled communication systems, including transmission protocol, statistical channel information (SCI) acquisition, and joint position and rotation optimization of 6DMA surfaces based on the SCI of users. Specifically, an orthogonal matching pursuit (OMP)-based algorithm is proposed for the estimation of SCI of users at all possible position-rotation pairs of 6DMA surfaces based on the channel measurements at a small subset of position-rotation pairs. Then, the average sum logarithmic rate of all users is maximized by jointly designing the positions and rotations of 6DMA surfaces based on their SCI acquired. Different from prior works on 6DMA which adopt alternating optimization to design 6DMA positions/rotations with iterations, we propose a new sequential optimization approach that first determines 6DMA rotations and then finds their feasible positions to realize the optimized rotations subject to practical antenna placement constraints. Simulation results show that the proposed sequential optimization significantly reduces the computational complexity of conventional alternating optimization, while achieving comparable communication performance. It is also shown that the proposed SCI-based 6DMA design can effectively enhance the communication throughput of wireless networks over existing fixed (position and rotation) antenna arrays, yet with a practically appealing low-complexity implementation.","authors":["Qijun Jiang","Xiaodan Shao","Rui Zhang"],"url":"https://arxiv.org/abs/2504.20618"}
{"created":"2025-04-30","title":"Breaking the Barrier of Self-Concordant Barriers: Faster Interior Point Methods for M-Matrices","abstract":"We study two fundamental optimization problems: (1) scaling a symmetric positive definite matrix by a positive diagonal matrix so that the resulting matrix has row and column sums equal to 1; and (2) minimizing a quadratic function subject to hard non-negativity constraints. Both problems lend themselves to efficient algorithms based on interior point methods (IPMs). For general instances, standard self-concordance theory places a limit on the iteration complexity of these methods at $\\widetilde{O}\\left(n^{1/2}\\right)$, where $n$ denotes the matrix dimension. We show via an amortized analysis that, when the input matrix is an M-matrix, an IPM with adaptive step sizes solves both problems in only $\\widetilde{O}\\left(n^{1/3}\\right)$ iterations. As a corollary, using fast Laplacian solvers, we obtain an $\\ell_{2}$ flow diffusion algorithm with depth $\\widetilde{O}\\left(n^{1/3}\\right)$ and work $\\widetilde{O}$$\\left(n^{1/3}\\cdot\\text{nnz}\\right)$. This result marks a significant instance in which a standard log-barrier IPM permits provably fewer than $\\Theta\\left(n^{1/2}\\right)$ iterations.","authors":["Adrian Vladu"],"url":"https://arxiv.org/abs/2504.20619"}
{"created":"2025-04-30","title":"PaRT: Enhancing Proactive Social Chatbots with Personalized Real-Time Retrieval","abstract":"Social chatbots have become essential intelligent companions in daily scenarios ranging from emotional support to personal interaction. However, conventional chatbots with passive response mechanisms usually rely on users to initiate or sustain dialogues by bringing up new topics, resulting in diminished engagement and shortened dialogue duration. In this paper, we present PaRT, a novel framework enabling context-aware proactive dialogues for social chatbots through personalized real-time retrieval and generation. Specifically, PaRT first integrates user profiles and dialogue context into a large language model (LLM), which is initially prompted to refine user queries and recognize their underlying intents for the upcoming conversation. Guided by refined intents, the LLM generates personalized dialogue topics, which then serve as targeted queries to retrieve relevant passages from RedNote. Finally, we prompt LLMs with summarized passages to generate knowledge-grounded and engagement-optimized responses. Our approach has been running stably in a real-world production environment for more than 30 days, achieving a 21.77\\% improvement in the average duration of dialogues.","authors":["Zihan Niu","Zheyong Xie","Shaosheng Cao","Chonggang Lu","Zheyu Ye","Tong Xu","Zuozhu Liu","Yan Gao","Jia Chen","Zhe Xu","Yi Wu","Yao Hu"],"url":"https://arxiv.org/abs/2504.20624"}
{"created":"2025-04-30","title":"DiffusionRIR: Room Impulse Response Interpolation using Diffusion Models","abstract":"Room Impulse Responses (RIRs) characterize acoustic environments and are crucial in multiple audio signal processing tasks. High-quality RIR estimates drive applications such as virtual microphones, sound source localization, augmented reality, and data augmentation. However, obtaining RIR measurements with high spatial resolution is resource-intensive, making it impractical for large spaces or when dense sampling is required. This research addresses the challenge of estimating RIRs at unmeasured locations within a room using Denoising Diffusion Probabilistic Models (DDPM). Our method leverages the analogy between RIR matrices and image inpainting, transforming RIR data into a format suitable for diffusion-based reconstruction.","authors":["Sagi Della Torre","Mirco Pezzoli","Fabio Antonacci","Sharon Gannot"],"url":"https://arxiv.org/abs/2504.20625"}
{"created":"2025-04-30","title":"A Novel Cipher for Enhancing MAVLink Security: Design, Security Analysis, and Performance Evaluation Using a Drone Testbed","abstract":"We present MAVShield, a novel lightweight cipher designed to secure communications in Unmanned Aerial Vehicles (UAVs) using the MAVLink protocol, which by default transmits unencrypted messages between UAVs and Ground Control Stations (GCS). While existing studies propose encryption for MAVLink, most remain theoretical or simulation-based. We implement MAVShield alongside AES-CTR, ChaCha20, Speck-CTR, and Rabbit, and evaluate them on a real drone testbed. A comprehensive security analysis using statistical test suites (NIST and Diehard) demonstrates strong resistance of the novel cipher to cryptanalysis. Performance evaluation across key metrics including memory usage, CPU load, and battery power consumption, demonstrates that MAVShield outperforms existing algorithms and offers an efficient, real-world solution for securing MAVLink communications in UAVs.","authors":["Bhavya Dixit","Ananthapadmanabhan A.","Adheeba Thahsin","Saketh Pathak","Gaurav S. Kasbekar","Arnab Maity"],"url":"https://arxiv.org/abs/2504.20626"}
{"created":"2025-04-30","title":"Cognitive maps are generative programs","abstract":"Making sense of the world and acting in it relies on building simplified mental representations that abstract away aspects of reality. This principle of cognitive mapping is universal to agents with limited resources. Living organisms, people, and algorithms all face the problem of forming functional representations of their world under various computing constraints. In this work, we explore the hypothesis that human resource-efficient planning may arise from representing the world as predictably structured. Building on the metaphor of concepts as programs, we propose that cognitive maps can take the form of generative programs that exploit predictability and redundancy, in contrast to directly encoding spatial layouts. We use a behavioral experiment to show that people who navigate in structured spaces rely on modular planning strategies that align with programmatic map representations. We describe a computational model that predicts human behavior in a variety of structured scenarios. This model infers a small distribution over possible programmatic cognitive maps conditioned on human prior knowledge of the world, and uses this distribution to generate resource-efficient plans. Our models leverages a Large Language Model as an embedding of human priors, implicitly learned through training on a vast corpus of human data. Our model demonstrates improved computational efficiency, requires drastically less memory, and outperforms unstructured planning algorithms with cognitive constraints at predicting human behavior, suggesting that human planning strategies rely on programmatic cognitive maps.","authors":["Marta Kryven","Cole Wyeth","Aidan Curtis","Kevin Ellis"],"url":"https://arxiv.org/abs/2504.20628"}
{"created":"2025-04-30","title":"AlignDiT: Multimodal Aligned Diffusion Transformer for Synchronized Speech Generation","abstract":"In this paper, we address the task of multimodal-to-speech generation, which aims to synthesize high-quality speech from multiple input modalities: text, video, and reference audio. This task has gained increasing attention due to its wide range of applications, such as film production, dubbing, and virtual avatars. Despite recent progress, existing methods still suffer from limitations in speech intelligibility, audio-video synchronization, speech naturalness, and voice similarity to the reference speaker. To address these challenges, we propose AlignDiT, a multimodal Aligned Diffusion Transformer that generates accurate, synchronized, and natural-sounding speech from aligned multimodal inputs. Built upon the in-context learning capability of the DiT architecture, AlignDiT explores three effective strategies to align multimodal representations. Furthermore, we introduce a novel multimodal classifier-free guidance mechanism that allows the model to adaptively balance information from each modality during speech synthesis. Extensive experiments demonstrate that AlignDiT significantly outperforms existing methods across multiple benchmarks in terms of quality, synchronization, and speaker similarity. Moreover, AlignDiT exhibits strong generalization capability across various multimodal tasks, such as video-to-speech synthesis and visual forced alignment, consistently achieving state-of-the-art performance. The demo page is available at https://mm.kaist.ac.kr/projects/AlignDiT .","authors":["Jeongsoo Choi","Ji-Hoon Kim","Kim Sung-Bin","Tae-Hyun Oh","Joon Son Chung"],"url":"https://arxiv.org/abs/2504.20629"}
{"created":"2025-04-30","title":"On Stochastic Rounding with Few Random Bits","abstract":"Large-scale numerical computations make increasing use of low-precision (LP) floating point formats and mixed precision arithmetic, which can be enhanced by the technique of stochastic rounding (SR), that is, rounding an intermediate high-precision value up or down randomly as a function of the value's distance to the two rounding candidates. Stochastic rounding requires, in addition to the high-precision input value, a source of random bits. As the provision of high-quality random bits is an additional computational cost, it is of interest to require as few bits as possible while maintaining the desirable properties of SR in a given computation, or computational domain. This paper examines a number of possible implementations of few-bit stochastic rounding (FBSR), and shows how several natural implementations can introduce sometimes significant bias into the rounding process, which are not present in the case of infinite-bit, infinite-precision examinations of these implementations. The paper explores the impact of these biases in machine learning examples, and hence opens another class of configuration parameters of which practitioners should be aware when developing or adopting low-precision floating point. Code is available at http://github.com/graphcore-research/arith25-stochastic-rounding.","authors":["Andrew Fitzgibbon","Stephen Felix"],"url":"https://arxiv.org/abs/2504.20634"}
{"created":"2025-04-30","title":"Bridging the Generalisation Gap: Synthetic Data Generation for Multi-Site Clinical Model Validation","abstract":"Ensuring the generalisability of clinical machine learning (ML) models across diverse healthcare settings remains a significant challenge due to variability in patient demographics, disease prevalence, and institutional practices. Existing model evaluation approaches often rely on real-world datasets, which are limited in availability, embed confounding biases, and lack the flexibility needed for systematic experimentation. Furthermore, while generative models aim for statistical realism, they often lack transparency and explicit control over factors driving distributional shifts. In this work, we propose a novel structured synthetic data framework designed for the controlled benchmarking of model robustness, fairness, and generalisability. Unlike approaches focused solely on mimicking observed data, our framework provides explicit control over the data generating process, including site-specific prevalence variations, hierarchical subgroup effects, and structured feature interactions. This enables targeted investigation into how models respond to specific distributional shifts and potential biases. Through controlled experiments, we demonstrate the framework's ability to isolate the impact of site variations, support fairness-aware audits, and reveal generalisation failures, particularly highlighting how model complexity interacts with site-specific effects. This work contributes a reproducible, interpretable, and configurable tool designed to advance the reliable deployment of ML in clinical settings.","authors":["Bradley Segal","Joshua Fieggen","David Clifton","Lei Clifton"],"url":"https://arxiv.org/abs/2504.20635"}
{"created":"2025-04-30","title":"Protocol Dialects as Formal Patterns: A Composable Theory of Lingos - Technical report","abstract":"Protocol dialects are methods for modifying protocols that provide light-weight security, especially against easy attacks that can lead to more serious ones. A lingo is a dialect's key security component by making attackers unable to \"speak\" the lingo. A lingo's \"talk\" changes all the time, becoming a moving target for attackers. We present several kinds of lingo transformations and compositions to generate stronger lingos from simpler ones, thus making dialects more secure.","authors":["V\\'ictor Garc\\'ia","Santiago Escobar","Catherine Meadows","Jose Meseguer"],"url":"https://arxiv.org/abs/2504.20637"}
{"created":"2025-04-30","title":"Multi-Message Secure Aggregation with Demand Privacy","abstract":"This paper considers a multi-message secure aggregation with privacy problem, in which a server aims to compute $\\sf K_c\\geq 1$ linear combinations of local inputs from $\\sf K$ distributed users. The problem addresses two tasks: (1) security, ensuring that the server can only obtain the desired linear combinations without any else information about the users' inputs, and (2) privacy, preventing users from learning about the server's computation task. In addition, the effect of user dropouts is considered, where at most $\\sf{K-U}$ users can drop out and the identity of these users cannot be predicted in advance. We propose two schemes for $\\sf K_c$ is equal to (1) and $\\sf 2\\leq K_c\\leq U-1$, respectively. For $\\sf K_c$ is equal to (1), we introduce multiplicative encryption of the server's demand using a random variable, where users share coded keys offline and transmit masked models in the first round, followed by aggregated coded keys in the second round for task recovery. For $\\sf{2\\leq K_c \\leq U-1}$, we use robust symmetric private computation to recover linear combinations of keys in the second round. The objective is to minimize the number of symbols sent by each user during the two rounds. Our proposed schemes have achieved the optimal rate region when $ \\sf K_c $ is equal to (1) and the order optimal rate (within 2) when $\\sf{2\\leq K_c \\leq U-1}$.","authors":["Chenyi Sun","Ziting Zhang","Kai Wan","Giuseppe Caire"],"url":"https://arxiv.org/abs/2504.20639"}
{"created":"2025-04-30","title":"Decision-centric fairness: Evaluation and optimization for resource allocation problems","abstract":"Data-driven decision support tools play an increasingly central role in decision-making across various domains. In this work, we focus on binary classification models for predicting positive-outcome scores and deciding on resource allocation, e.g., credit scores for granting loans or churn propensity scores for targeting customers with a retention campaign. Such models may exhibit discriminatory behavior toward specific demographic groups through their predicted scores, potentially leading to unfair resource allocation. We focus on demographic parity as a fairness metric to compare the proportions of instances that are selected based on their positive outcome scores across groups. In this work, we propose a decision-centric fairness methodology that induces fairness only within the decision-making region -- the range of relevant decision thresholds on the score that may be used to decide on resource allocation -- as an alternative to a global fairness approach that seeks to enforce parity across the entire score distribution. By restricting the induction of fairness to the decision-making region, the proposed decision-centric approach avoids imposing overly restrictive constraints on the model, which may unnecessarily degrade the quality of the predicted scores. We empirically compare our approach to a global fairness approach on multiple (semi-synthetic) datasets to identify scenarios in which focusing on fairness where it truly matters, i.e., decision-centric fairness, proves beneficial.","authors":["Simon De Vos","Jente Van Belle","Andres Algaba","Wouter Verbeke","Sam Verboven"],"url":"https://arxiv.org/abs/2504.20642"}
{"created":"2025-04-30","title":"Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations","abstract":"Large Language Models (LLMs) excel at countless tasks, yet struggle with creativity. In this paper, we introduce a novel approach that couples LLMs with structured representations and cognitively inspired manipulations to generate more creative and diverse ideas. Our notion of creativity goes beyond superficial token-level variations; rather, we explicitly recombine structured representations of existing ideas, allowing our algorithm to effectively explore the more abstract landscape of ideas. We demonstrate our approach in the culinary domain with DishCOVER, a model that generates creative recipes. Experiments comparing our model's results to those of GPT-4o show greater diversity. Domain expert evaluations reveal that our outputs, which are mostly coherent and feasible culinary creations, significantly surpass GPT-4o in terms of novelty, thus outperforming it in creative generation. We hope our work inspires further research into structured creativity in AI.","authors":["Moran Mizrahi","Chen Shani","Gabriel Stanovsky","Dan Jurafsky","Dafna Shahaf"],"url":"https://arxiv.org/abs/2504.20643"}
{"created":"2025-04-30","title":"Combatting Dimensional Collapse in LLM Pre-Training Data via Diversified File Selection","abstract":"Selecting high-quality pre-training data for large language models (LLMs) is crucial for enhancing their overall performance under limited computation budget, improving both training and sample efficiency. Recent advancements in file selection primarily rely on using an existing or trained proxy model to assess the similarity of samples to a target domain, such as high quality sources BookCorpus and Wikipedia. However, upon revisiting these methods, the domain-similarity selection criteria demonstrates a diversity dilemma, i.e.dimensional collapse in the feature space, improving performance on the domain-related tasks but causing severe degradation on generic performance. To prevent collapse and enhance diversity, we propose a DiverSified File selection algorithm (DiSF), which selects the most decorrelated text files in the feature space. We approach this with a classical greedy algorithm to achieve more uniform eigenvalues in the feature covariance matrix of the selected texts, analyzing its approximation to the optimal solution under a formulation of $\\gamma$-weakly submodular optimization problem. Empirically, we establish a benchmark and conduct extensive experiments on the TinyLlama architecture with models from 120M to 1.1B parameters. Evaluating across nine tasks from the Harness framework, DiSF demonstrates a significant improvement on overall performance. Specifically, DiSF saves 98.5% of 590M training files in SlimPajama, outperforming the full-data pre-training within a 50B training budget, and achieving about 1.5x training efficiency and 5x data efficiency.","authors":["Ziqing Fan","Siyuan Du","Shengchao Hu","Pingjie Wang","Li Shen","Ya Zhang","Dacheng Tao","Yanfeng Wang"],"url":"https://arxiv.org/abs/2504.20644"}
{"created":"2025-04-30","title":"LDPoly: Latent Diffusion for Polygonal Road Outline Extraction in Large-Scale Topographic Mapping","abstract":"Polygonal road outline extraction from high-resolution aerial images is an important task in large-scale topographic mapping, where roads are represented as vectorized polygons, capturing essential geometric features with minimal vertex redundancy. Despite its importance, no existing method has been explicitly designed for this task. While polygonal building outline extraction has been extensively studied, the unique characteristics of roads, such as branching structures and topological connectivity, pose challenges to these methods. To address this gap, we introduce LDPoly, the first dedicated framework for extracting polygonal road outlines from high-resolution aerial images. Our method leverages a novel Dual-Latent Diffusion Model with a Channel-Embedded Fusion Module, enabling the model to simultaneously generate road masks and vertex heatmaps. A tailored polygonization method is then applied to obtain accurate vectorized road polygons with minimal vertex redundancy. We evaluate LDPoly on a new benchmark dataset, Map2ImLas, which contains detailed polygonal annotations for various topographic objects in several Dutch regions. Our experiments include both in-region and cross-region evaluations, with the latter designed to assess the model's generalization performance on unseen regions. Quantitative and qualitative results demonstrate that LDPoly outperforms state-of-the-art polygon extraction methods across various metrics, including pixel-level coverage, vertex efficiency, polygon regularity, and road connectivity. We also design two new metrics to assess polygon simplicity and boundary smoothness. Moreover, this work represents the first application of diffusion models for extracting precise vectorized object outlines without redundant vertices from remote-sensing imagery, paving the way for future advancements in this field.","authors":["Weiqin Jiao","Hao Cheng","George Vosselman","Claudio Persello"],"url":"https://arxiv.org/abs/2504.20645"}
{"created":"2025-04-30","title":"SpaRE: Enhancing Spatial Reasoning in Vision-Language Models with Synthetic Data","abstract":"Vision-language models (VLMs) work well in tasks ranging from image captioning to visual question answering (VQA), yet they struggle with spatial reasoning, a key skill for understanding our physical world that humans excel at. We find that spatial relations are generally rare in widely used VL datasets, with only a few being well represented, while most form a long tail of underrepresented relations. This gap leaves VLMs ill-equipped to handle diverse spatial relationships. To bridge it, we construct a synthetic VQA dataset focused on spatial reasoning generated from hyper-detailed image descriptions in Localized Narratives, DOCCI, and PixMo-Cap. Our dataset consists of 455k samples containing 3.4 million QA pairs. Trained on this dataset, our Spatial-Reasoning Enhanced (SpaRE) VLMs show strong improvements on spatial reasoning benchmarks, achieving up to a 49% performance gain on the What's Up benchmark, while maintaining strong results on general tasks. Our work narrows the gap between human and VLM spatial reasoning and makes VLMs more capable in real-world tasks such as robotics and navigation.","authors":["Michael Ogezi","Freda Shi"],"url":"https://arxiv.org/abs/2504.20648"}
{"created":"2025-04-30","title":"RuleKit 2: Faster and simpler rule learning","abstract":"Rules offer an invaluable combination of predictive and descriptive capabilities. Our package for rule-based data analysis, RuleKit, has proven its effectiveness in classification, regression, and survival problems. Here we present its second version. New algorithms and optimized implementations of those previously included, significantly improved the computational performance of our suite, reducing the analysis time of some data sets by two orders of magnitude. The usability of RuleKit 2 is provided by two new components: Python package and browser application with a graphical user interface. The former complies with scikit-learn, the most popular data mining library for Python, allowing RuleKit 2 to be straightforwardly integrated into existing data analysis pipelines. RuleKit 2 is available at GitHub under GNU AGPL 3 license (https://github.com/adaa-polsl/RuleKit)","authors":["Adam Gudy\\'s","Cezary Maszczyk","Joanna Badura","Adam Grzelak","Marek Sikora","{\\L}ukasz Wr\\'obel"],"url":"https://arxiv.org/abs/2504.20650"}
{"created":"2025-04-30","title":"ComplexVCoder: An LLM-Driven Framework for Systematic Generation of Complex Verilog Code","abstract":"Recent advances have demonstrated the promising capabilities of large language models (LLMs) in generating register-transfer level (RTL) code, such as Verilog. However, existing LLM-based frameworks still face significant challenges in accurately handling the complexity of real-world RTL designs, particularly those that are large-scale and involve multi-level module instantiations. To address this issue, we present ComplexVCoder, an open-source LLM-driven framework that enhances both the generation quality and efficiency of complex Verilog code. Specifically, we introduce a two-stage generation mechanism, which leverages an intermediate representation to enable a more accurate and structured transition from natural language descriptions to intricate Verilog designs. In addition, we introduce a rule-based alignment method and a domain-specific retrieval-augmented generation (RAG) to further improve the correctness of the synthesized code by incorporating relevant design knowledge during generation. To evaluate our approach, we construct a comprehensive dataset comprising 55 complex Verilog designs derived from real-world implementations. We also release an open-source benchmark suite for systematically assessing the quality of auto-generated RTL code together with the ComplexVCoder framework. Experimental results show that ComplexVCoder outperforms SOTA frameworks such as CodeV and RTLCoder by 14.6% and 22.2%, respectively, in terms of function correctness on complex Verilog benchmarks. Furthermore, ComplexVcoder achieves comparable generation performances in terms of functionality correctness using a lightweight 32B model (Qwen2.5), rivaling larger-scale models such as GPT-3.5 and DeepSeek-V3.","authors":["Jian Zuo","Junzhe Liu","Xianyong Wang","Yicheng Liu","Navya Goli","Tong Xu","Hao Zhang","Umamaheswara Rao Tida","Zhenge Jia","Mengying Zhao"],"url":"https://arxiv.org/abs/2504.20653"}
{"created":"2025-04-30","title":"Federated learning, ethics, and the double black box problem in medical AI","abstract":"Federated learning (FL) is a machine learning approach that allows multiple devices or institutions to collaboratively train a model without sharing their local data with a third-party. FL is considered a promising way to address patient privacy concerns in medical artificial intelligence. The ethical risks of medical FL systems themselves, however, have thus far been underexamined. This paper aims to address this gap. We argue that medical FL presents a new variety of opacity -- federation opacity -- that, in turn, generates a distinctive double black box problem in healthcare AI. We highlight several instances in which the anticipated benefits of medical FL may be exaggerated, and conclude by highlighting key challenges that must be overcome to make FL ethically feasible in medicine.","authors":["Joshua Hatherley","Anders S{\\o}gaard","Angela Ballantyne","Ruben Pauwels"],"url":"https://arxiv.org/abs/2504.20656"}
{"created":"2025-04-30","title":"Image deidentification in the XNAT ecosystem: use cases and solutions","abstract":"XNAT is a server-based data management platform widely used in academia for curating large databases of DICOM images for research projects. We describe in detail a deidentification workflow for DICOM data using facilities in XNAT, together with independent tools in the XNAT \"ecosystem\". We list different contexts in which deidentification might be needed, based on our prior experience. The starting point for participation in the Medical Image De-Identification Benchmark (MIDI-B) challenge was a set of pre-existing local methodologies, which were adapted during the validation phase of the challenge. Our result in the test phase was 97.91\\%, considerably lower than our peers, due largely to an arcane technical incompatibility of our methodology with the challenge's Synapse platform, which prevented us receiving feedback during the validation phase. Post-submission, additional discrepancy reports from the organisers and via the MIDI-B Continuous Benchmarking facility, enabled us to improve this score significantly to 99.61\\%. An entirely rule-based approach was shown to be capable of removing all name-related information in the test corpus, but exhibited failures in dealing fully with address data. Initial experiments using published machine-learning models to remove addresses were partially successful but showed the models to be \"over-aggressive\" on other types of free-text data, leading to a slight overall degradation in performance to 99.54\\%. Future development will therefore focus on improving address-recognition capabilities, but also on better removal of identifiable data burned into the image pixels. Several technical aspects relating to the \"answer key\" are still under discussion with the challenge organisers, but we estimate that our percentage of genuine deidentification failures on the MIDI-B test corpus currently stands at 0.19\\%. (Abridged from original for arXiv submission)","authors":["Alex Michie","Simon J Doran"],"url":"https://arxiv.org/abs/2504.20657"}
{"created":"2025-04-30","title":"TrueFake: A Real World Case Dataset of Last Generation Fake Images also Shared on Social Networks","abstract":"AI-generated synthetic media are increasingly used in real-world scenarios, often with the purpose of spreading misinformation and propaganda through social media platforms, where compression and other processing can degrade fake detection cues. Currently, many forensic tools fail to account for these in-the-wild challenges. In this work, we introduce TrueFake, a large-scale benchmarking dataset of 600,000 images including top notch generative techniques and sharing via three different social networks. This dataset allows for rigorous evaluation of state-of-the-art fake image detectors under very realistic and challenging conditions. Through extensive experimentation, we analyze how social media sharing impacts detection performance, and identify current most effective detection and training strategies. Our findings highlight the need for evaluating forensic models in conditions that mirror real-world use.","authors":["Stefano Dell'Anna (University of Trento","Trento","Italy)","Andrea Montibeller (University of Trento","Trento","Italy)","Giulia Boato (University of Trento","Trento","Italy","Truebees srl","Trento","Italy)"],"url":"https://arxiv.org/abs/2504.20658"}
{"created":"2025-04-30","title":"Quantum-Enhanced Hybrid Reinforcement Learning Framework for Dynamic Path Planning in Autonomous Systems","abstract":"In this paper, a novel quantum classical hybrid framework is proposed that synergizes quantum with Classical Reinforcement Learning. By leveraging the inherent parallelism of quantum computing, the proposed approach generates robust Q tables and specialized turn cost estimations, which are then integrated with a classical Reinforcement Learning pipeline. The Classical Quantum fusion results in rapid convergence of training, reducing the training time significantly and improved adaptability in scenarios featuring static, dynamic, and moving obstacles. Simulator based evaluations demonstrate significant enhancements in path efficiency, trajectory smoothness, and mission success rates, underscoring the potential of framework for real time, autonomous navigation in complex and unpredictable environments. Furthermore, the proposed framework was tested beyond simulations on practical scenarios, including real world map data such as the IIT Delhi campus, reinforcing its potential for real time, autonomous navigation in complex and unpredictable environments.","authors":["Sahil Tomar","Shamshe Alam","Sandeep Kumar","Amit Mathur"],"url":"https://arxiv.org/abs/2504.20660"}
{"created":"2025-04-30","title":"On the Optimal Source Key Size of Secure Gradient Coding","abstract":"With gradient coding, a user node can efficiently aggregate gradients from server nodes processing local datasets, achieving low communication costs and maintaining resilience against straggling servers. This paper considers a secure gradient coding problem, where a user aims to compute the sum of the gradients from $K$ datasets with the assistance of $N$ distributed servers. The user should recover the sum of gradients by receiving transmissions from any $N_r$ servers, and each dataset is assigned to $N - N_r + m$ servers. The security constraint guarantees that even if the user receives transmissions from all servers, it cannot obtain any additional information about the datasets beyond the sum of gradients.","authors":["Yang Zhou","Wenbo Huang","Kai Wan","Robert Caiming Qiu"],"url":"https://arxiv.org/abs/2504.20662"}
{"created":"2025-04-30","title":"SFi-Former: Sparse Flow Induced Attention for Graph Transformer","abstract":"Graph Transformers (GTs) have demonstrated superior performance compared to traditional message-passing graph neural networks in many studies, especially in processing graph data with long-range dependencies. However, GTs tend to suffer from weak inductive bias, overfitting and over-globalizing problems due to the dense attention. In this paper, we introduce SFi-attention, a novel attention mechanism designed to learn sparse pattern by minimizing an energy function based on network flows with l1-norm regularization, to relieve those issues caused by dense attention. Furthermore, SFi-Former is accordingly devised which can leverage the sparse attention pattern of SFi-attention to generate sparse network flows beyond adjacency matrix of graph data. Specifically, SFi-Former aggregates features selectively from other nodes through flexible adaptation of the sparse attention, leading to a more robust model. We validate our SFi-Former on various graph datasets, especially those graph data exhibiting long-range dependencies. Experimental results show that our SFi-Former obtains competitive performance on GNN Benchmark datasets and SOTA performance on LongRange Graph Benchmark (LRGB) datasets. Additionally, our model gives rise to smaller generalization gaps, which indicates that it is less prone to over-fitting. Click here for codes.","authors":["Zhonghao Li","Ji Shi","Xinming Zhang","Miao Zhang","Bo Li"],"url":"https://arxiv.org/abs/2504.20666"}
{"created":"2025-04-30","title":"Explanations Go Linear: Interpretable and Individual Latent Encoding for Post-hoc Explainability","abstract":"Post-hoc explainability is essential for understanding black-box machine learning models. Surrogate-based techniques are widely used for local and global model-agnostic explanations but have significant limitations. Local surrogates capture non-linearities but are computationally expensive and sensitive to parameters, while global surrogates are more efficient but struggle with complex local behaviors. In this paper, we present ILLUME, a flexible and interpretable framework grounded in representation learning, that can be integrated with various surrogate models to provide explanations for any black-box classifier. Specifically, our approach combines a globally trained surrogate with instance-specific linear transformations learned with a meta-encoder to generate both local and global explanations. Through extensive empirical evaluations, we demonstrate the effectiveness of ILLUME in producing feature attributions and decision rules that are not only accurate but also robust and faithful to the black-box, thus providing a unified explanation framework that effectively addresses the limitations of traditional surrogate methods.","authors":["Simone Piaggesi","Riccardo Guidotti","Fosca Giannotti","Dino Pedreschi"],"url":"https://arxiv.org/abs/2504.20667"}
{"created":"2025-04-30","title":"A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages","abstract":"Online disinformation poses a global challenge, placing significant demands on fact-checkers who must verify claims efficiently to prevent the spread of false information. A major issue in this process is the redundant verification of already fact-checked claims, which increases workload and delays responses to newly emerging claims. This research introduces an approach that retrieves previously fact-checked claims, evaluates their relevance to a given input, and provides supplementary information to support fact-checkers. Our method employs large language models (LLMs) to filter irrelevant fact-checks and generate concise summaries and explanations, enabling fact-checkers to faster assess whether a claim has been verified before. In addition, we evaluate our approach through both automatic and human assessments, where humans interact with the developed tool to review its effectiveness. Our results demonstrate that LLMs are able to filter out many irrelevant fact-checks and, therefore, reduce effort and streamline the fact-checking process.","authors":["Ivan Vykopal","Martin Hyben","Robert Moro","Michal Gregor","Jakub Simko"],"url":"https://arxiv.org/abs/2504.20668"}
{"created":"2025-04-30","title":"Advance Fake Video Detection via Vision Transformers","abstract":"Recent advancements in AI-based multimedia generation have enabled the creation of hyper-realistic images and videos, raising concerns about their potential use in spreading misinformation. The widespread accessibility of generative techniques, which allow for the production of fake multimedia from prompts or existing media, along with their continuous refinement, underscores the urgent need for highly accurate and generalizable AI-generated media detection methods, underlined also by new regulations like the European Digital AI Act. In this paper, we draw inspiration from Vision Transformer (ViT)-based fake image detection and extend this idea to video. We propose an {original} %innovative framework that effectively integrates ViT embeddings over time to enhance detection performance. Our method shows promising accuracy, generalization, and few-shot learning capabilities across a new, large and diverse dataset of videos generated using five open source generative techniques from the state-of-the-art, as well as a separate dataset containing videos produced by proprietary generative methods.","authors":["Joy Battocchio (University of Trento","Trento","Italy)","Stefano Dell'Anna (University of Trento","Trento","Italy)","Andrea Montibeller (University of Trento","Trento","Italy)","Giulia Boato (University of Trento","Trento","Italy","Truebees srl","Trento","Italy)"],"url":"https://arxiv.org/abs/2504.20669"}
{"created":"2025-04-30","title":"FBRT-YOLO: Faster and Better for Real-Time Aerial Image Detection","abstract":"Embedded flight devices with visual capabilities have become essential for a wide range of applications. In aerial image detection, while many existing methods have partially addressed the issue of small target detection, challenges remain in optimizing small target detection and balancing detection accuracy with efficiency. These issues are key obstacles to the advancement of real-time aerial image detection. In this paper, we propose a new family of real-time detectors for aerial image detection, named FBRT-YOLO, to address the imbalance between detection accuracy and efficiency. Our method comprises two lightweight modules: Feature Complementary Mapping Module (FCM) and Multi-Kernel Perception Unit(MKP), designed to enhance object perception for small targets in aerial images. FCM focuses on alleviating the problem of information imbalance caused by the loss of small target information in deep networks. It aims to integrate spatial positional information of targets more deeply into the network,better aligning with semantic information in the deeper layers to improve the localization of small targets. We introduce MKP, which leverages convolutions with kernels of different sizes to enhance the relationships between targets of various scales and improve the perception of targets at different scales. Extensive experimental results on three major aerial image datasets, including Visdrone, UAVDT, and AI-TOD,demonstrate that FBRT-YOLO outperforms various real-time detectors in terms of performance and speed.","authors":["Yao Xiao","Tingfa Xu","Yu Xin","Jianan Li"],"url":"https://arxiv.org/abs/2504.20670"}
{"created":"2025-04-30","title":"CoCo-Bench: A Comprehensive Code Benchmark For Multi-task Large Language Model Evaluation","abstract":"Large language models (LLMs) play a crucial role in software engineering, excelling in tasks like code generation and maintenance. However, existing benchmarks are often narrow in scope, focusing on a specific task and lack a comprehensive evaluation framework that reflects real-world applications. To address these gaps, we introduce CoCo-Bench (Comprehensive Code Benchmark), designed to evaluate LLMs across four critical dimensions: code understanding, code generation, code modification, and code review. These dimensions capture essential developer needs, ensuring a more systematic and representative evaluation. CoCo-Bench includes multiple programming languages and varying task difficulties, with rigorous manual review to ensure data quality and accuracy. Empirical results show that CoCo-Bench aligns with existing benchmarks while uncovering significant variations in model performance, effectively highlighting strengths and weaknesses. By offering a holistic and objective evaluation, CoCo-Bench provides valuable insights to guide future research and technological advancements in code-oriented LLMs, establishing a reliable benchmark for the field.","authors":["Wenjing Yin","Tianze Sun","Yijiong Yu","Jiawei Fang","Guangyao Su","Jiancheng Wang","Zekun Wang","Wei Wang","Ran Chen","Ziyun Dai","Shuai Yuan","Menghang Dong","Peng Luo","Dong Cao","Da Lei","Yajun Zhang","Hao Chen","Xiang Ma","Yong Liu","Weifeng Liu","Yuanjian Xu","Ji Pei"],"url":"https://arxiv.org/abs/2504.20673"}
{"created":"2025-04-30","title":"DiffLiB: High-fidelity differentiable modeling of lithium-ion batteries and efficient gradient-based parameter identification","abstract":"The physics-based Doyle-Fuller-Newman (DFN) model, widely adopted for its precise electrochemical modeling, stands out among various simulation models of lithium-ion batteries (LIBs). Although the DFN model is powerful in forward predictive analysis, the inverse identification of its model parameters has remained a long-standing challenge. The numerous unknown parameters associated with the nonlinear, time-dependent, and multi-scale DFN model are extremely difficult to be determined accurately and efficiently, hindering the practical use of such battery simulation models in industrial applications. To tackle this challenge, we introduce DiffLiB, a high-fidelity finite-element-based LIB simulation framework, equipped with advanced differentiable programming techniques so that efficient gradient-based inverse parameter identification is enabled. Customized automatic differentiation rules are defined by identifying the VJP (vector-Jacobian product) structure in the chain rule and implemented using adjoint-based implicit differentiation methods. Four numerical examples, including both 2D and 3D forward predictions and inverse parameter identification, are presented to validate the accuracy and computational efficiency of DiffLiB. Benchmarking against COMSOL demonstrates excellent agreement in forward predictions, with terminal voltage discrepancies maintaining a root-mean-square error (RMSE) below 2 mV across all test conditions. In parameter identification tasks using experimentally measured voltage data, the proposed gradient-based optimization scheme achieves superior computational performance, with 96% fewer forward predictions and 72% less computational time compared with gradient-free approaches. These results demonstrate that DiffLiB is a versatile and powerful computational framework for the development of advanced LIBs.","authors":["Weipeng Xu","Kaiqi Yang","Yuzhi Zhang","Shichao Sun","Sheng Mao","Tianju Xue"],"url":"https://arxiv.org/abs/2504.20674"}
{"created":"2025-04-30","title":"The Limits of AI Explainability: An Algorithmic Information Theory Approach","abstract":"This paper establishes a theoretical foundation for understanding the fundamental limits of AI explainability through algorithmic information theory. We formalize explainability as the approximation of complex models by simpler ones, quantifying both approximation error and explanation complexity using Kolmogorov complexity. Our key theoretical contributions include: (1) a complexity gap theorem proving that any explanation significantly simpler than the original model must differ from it on some inputs; (2) precise bounds showing that explanation complexity grows exponentially with input dimension but polynomially with error tolerance for Lipschitz functions; and (3) a characterization of the gap between local and global explainability, demonstrating that local explanations can be significantly simpler while maintaining accuracy in relevant regions. We further establish a regulatory impossibility theorem proving that no governance framework can simultaneously pursue unrestricted AI capabilities, human-interpretable explanations, and negligible error. These results highlight considerations likely to be relevant to the design, evaluation, and oversight of explainable AI systems.","authors":["Shrisha Rao"],"url":"https://arxiv.org/abs/2504.20676"}
{"created":"2025-04-30","title":"Occlusion-aware Driver Monitoring System using the Driver Monitoring Dataset","abstract":"This paper presents a robust, occlusion-aware driver monitoring system (DMS) utilizing the Driver Monitoring Dataset (DMD). The system performs driver identification, gaze estimation by regions, and face occlusion detection under varying lighting conditions, including challenging low-light scenarios. Aligned with EuroNCAP recommendations, the inclusion of occlusion detection enhances situational awareness and system trustworthiness by indicating when the system's performance may be degraded. The system employs separate algorithms trained on RGB and infrared (IR) images to ensure reliable functioning. We detail the development and integration of these algorithms into a cohesive pipeline, addressing the challenges of working with different sensors and real-car implementation. Evaluation on the DMD and in real-world scenarios demonstrates the effectiveness of the proposed system, highlighting the superior performance of RGB-based models and the pioneering contribution of robust occlusion detection in DMS.","authors":["Paola Natalia Ca\\~nas","Alexander Diez","David Galva\\~n","Marcos Nieto","Igor Rodr\\'iguez"],"url":"https://arxiv.org/abs/2504.20677"}
{"created":"2025-04-30","title":"Non-native Children's Automatic Speech Assessment Challenge (NOCASA)","abstract":"This paper presents the \"Non-native Children's Automatic Speech Assessment\" (NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA challenges participants to develop new systems that can assess single-word pronunciations of young second language (L2) learners as part of a gamified pronunciation training app. To achieve this, several issues must be addressed, most notably the limited nature of available training data and the highly unbalanced distribution among the pronunciation level categories. To expedite the development, we provide a pseudo-anonymized training data (TeflonNorL2), containing 10,334 recordings from 44 speakers attempting to pronounce 205 distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that should be given in the game). In addition to the data, two already trained systems are released as official baselines: an SVM classifier trained on the ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter achieves the best performance on the challenge test set, with an unweighted average recall (UAR) of 36.37%.","authors":["Yaroslav Getman","Tam\\'as Gr\\'osz","Mikko Kurimo","Giampiero Salvi"],"url":"https://arxiv.org/abs/2504.20678"}
{"created":"2025-04-30","title":"Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?","abstract":"Automated detection of semantically equivalent questions in longitudinal social science surveys is crucial for long-term studies informing empirical research in the social, economic, and health sciences. Retrieving equivalent questions faces dual challenges: inconsistent representation of theoretical constructs (i.e. concept/sub-concept) across studies as well as between question and response options, and the evolution of vocabulary and structure in longitudinal text. To address these challenges, our multi-disciplinary collaboration of computer scientists and survey specialists presents a new information retrieval (IR) task of identifying concept (e.g. Housing, Job, etc.) equivalence across question and response options to harmonise longitudinal population studies. This paper investigates multiple unsupervised approaches on a survey dataset spanning 1946-2020, including probabilistic models, linear probing of language models, and pre-trained neural networks specialised for IR. We show that IR-specialised neural models achieve the highest overall performance with other approaches performing comparably. Additionally, the re-ranking of the probabilistic model's results with neural models only introduces modest improvements of 0.07 at most in F1-score. Qualitative post-hoc evaluation by survey specialists shows that models generally have a low sensitivity to questions with high lexical overlap, particularly in cases where sub-concepts are mismatched. Altogether, our analysis serves to further research on harmonising longitudinal studies in social science.","authors":["Wing Yan Li","Zeqiang Wang","Jon Johnson","Suparna De"],"url":"https://arxiv.org/abs/2504.20679"}
{"created":"2025-04-30","title":"Overcoming Quadratic Hardware Scaling for a Fully Connected Digital Oscillatory Neural Network","abstract":"Computing with coupled oscillators or oscillatory neural networks (ONNs) has recently attracted a lot of interest due to their potential for massive parallelism and energy-efficient computing. However, to date, ONNs have primarily been explored either analytically or through analog circuit implementations. This paper shifts the focus to the digital implementation of ONNs, examining various design architectures. We first report on an existing digital ONN design based on a recurrent architecture. The major challenge for scaling such recurrent architectures is the quadratic increase in coupling hardware with the network size. To overcome this challenge, we introduce a novel hybrid architecture that balances serialization and parallelism in the coupling elements that shows near-linear hardware scaling, on the order of about 1.2 with the network size. Furthermore, we evaluate the benefits and costs of these different digital ONN architectures in terms time to solution and resource usage on FPGA emulation. The proposed hybrid architecture allows for a 10.5$\\times$ increase in the number of oscillators while using 5-bits to represent the coupling weights and 4-bits to represent the oscillator phase on a Zynq-7020 FPGA board. The near-linear scaling is a major step towards implementing large scale ONN architectures. To the best of our knowledge, this work presents the largest fully connected digital ONN architecture implemented thus far with a total of 506 fully connected oscillators.","authors":["Bram Haverkort","Aida Todri-Sanial"],"url":"https://arxiv.org/abs/2504.20680"}
{"created":"2025-04-30","title":"Data Encryption Battlefield: A Deep Dive into the Dynamic Confrontations in Ransomware Attacks","abstract":"In the rapidly evolving landscape of cybersecurity threats, ransomware represents a significant challenge. Attackers increasingly employ sophisticated encryption methods, such as entropy reduction through Base64 encoding, and partial or intermittent encryption to evade traditional detection methods. This study explores the dynamic battle between adversaries who continuously refine encryption strategies and defenders developing advanced countermeasures to protect vulnerable data. We investigate the application of online incremental machine learning algorithms designed to predict file encryption activities despite adversaries evolving obfuscation techniques. Our analysis utilizes an extensive dataset of 32.6 GB, comprising 11,928 files across multiple formats, including Microsoft Word documents (doc), PowerPoint presentations (ppt), Excel spreadsheets (xlsx), image formats (jpg, jpeg, png, tif, gif), PDFs (pdf), audio (mp3), and video (mp4) files. These files were encrypted by 75 distinct ransomware families, facilitating a robust empirical evaluation of machine learning classifiers effectiveness against diverse encryption tactics. Results highlight the Hoeffding Tree algorithms superior incremental learning capability, particularly effective in detecting traditional and AES-Base64 encryption methods employed to lower entropy. Conversely, the Random Forest classifier with warm-start functionality excels at identifying intermittent encryption methods, demonstrating the necessity of tailored machine learning solutions to counter sophisticated ransomware strategies.","authors":["Arash Mahboubi","Hamed Aboutorab","Seyit Camtepe","Hang Thanh Bui","Khanh Luong","Keyvan Ansari","Shenlu Wang","Bazara Barry"],"url":"https://arxiv.org/abs/2504.20681"}
{"created":"2025-04-30","title":"OG-HFYOLO :Orientation gradient guidance and heterogeneous feature fusion for deformation table cell instance segmentation","abstract":"Table structure recognition is a key task in document analysis. However, the geometric deformation in deformed tables causes a weak correlation between content information and structure, resulting in downstream tasks not being able to obtain accurate content information. To obtain fine-grained spatial coordinates of cells, we propose the OG-HFYOLO model, which enhances the edge response by Gradient Orientation-aware Extractor, combines a Heterogeneous Kernel Cross Fusion module and a scale-aware loss function to adapt to multi-scale objective features, and introduces mask-driven non-maximal suppression in the post-processing, which replaces the traditional bounding box suppression mechanism. Furthermore, we also propose a data generator, filling the gap in the dataset for fine-grained deformation table cell spatial coordinate localization, and derive a large-scale dataset named Deformation Wired Table (DWTAL). Experiments show that our proposed model demonstrates excellent segmentation accuracy on all mainstream instance segmentation models. The dataset and the source code are open source: https://github.com/justliulong/OGHFYOLO.","authors":["Long Liu","Cihui Yang"],"url":"https://arxiv.org/abs/2504.20682"}
{"created":"2025-04-30","title":"Identifying Uncertainty in Self-Adaptive Robotics with Large Language Models","abstract":"Future self-adaptive robots are expected to operate in highly dynamic environments while effectively managing uncertainties. However, identifying the sources and impacts of uncertainties in such robotic systems and defining appropriate mitigation strategies is challenging due to the inherent complexity of self-adaptive robots and the lack of comprehensive knowledge about the various factors influencing uncertainty. Hence, practitioners often rely on intuition and past experiences from similar systems to address uncertainties. In this article, we evaluate the potential of large language models (LLMs) in enabling a systematic and automated approach to identify uncertainties in self-adaptive robotics throughout the software engineering lifecycle. For this evaluation, we analyzed 10 advanced LLMs with varying capabilities across four industrial-sized robotics case studies, gathering the practitioners' perspectives on the LLM-generated responses related to uncertainties. Results showed that practitioners agreed with 63-88% of the LLM responses and expressed strong interest in the practicality of LLMs for this purpose.","authors":["Hassan Sartaj","Jalil Boudjadar","Mirgita Frasheri","Shaukat Ali","Peter Gorm Larsen"],"url":"https://arxiv.org/abs/2504.20684"}
{"created":"2025-04-30","title":"Efficient Listener: Dyadic Facial Motion Synthesis via Action Diffusion","abstract":"Generating realistic listener facial motions in dyadic conversations remains challenging due to the high-dimensional action space and temporal dependency requirements. Existing approaches usually consider extracting 3D Morphable Model (3DMM) coefficients and modeling in the 3DMM space. However, this makes the computational speed of the 3DMM a bottleneck, making it difficult to achieve real-time interactive responses. To tackle this problem, we propose Facial Action Diffusion (FAD), which introduces the diffusion methods from the field of image generation to achieve efficient facial action generation. We further build the Efficient Listener Network (ELNet) specially designed to accommodate both the visual and audio information of the speaker as input. Considering of FAD and ELNet, the proposed method learns effective listener facial motion representations and leads to improvements of performance over the state-of-the-art methods while reducing 99% computational time.","authors":["Zesheng Wang","Alexandre Bruckert","Patrick Le Callet","Guangtao Zhai"],"url":"https://arxiv.org/abs/2504.20685"}
{"created":"2025-04-30","title":"What's Wrong with Your Synthetic Tabular Data? Using Explainable AI to Evaluate Generative Models","abstract":"Evaluating synthetic tabular data is challenging, since they can differ from the real data in so many ways. There exist numerous metrics of synthetic data quality, ranging from statistical distances to predictive performance, often providing conflicting results. Moreover, they fail to explain or pinpoint the specific weaknesses in the synthetic data. To address this, we apply explainable AI (XAI) techniques to a binary detection classifier trained to distinguish real from synthetic data. While the classifier identifies distributional differences, XAI concepts such as feature importance and feature effects, analyzed through methods like permutation feature importance, partial dependence plots, Shapley values and counterfactual explanations, reveal why synthetic data are distinguishable, highlighting inconsistencies, unrealistic dependencies, or missing patterns. This interpretability increases transparency in synthetic data evaluation and provides deeper insights beyond conventional metrics, helping diagnose and improve synthetic data quality. We apply our approach to two tabular datasets and generative models, showing that it uncovers issues overlooked by standard evaluation techniques.","authors":["Jan Kapar","Niklas Koenen","Martin Jullum"],"url":"https://arxiv.org/abs/2504.20687"}
{"created":"2025-04-30","title":"DICOM Compatible, 3D Multimodality Image Encryption using Hyperchaotic Signal","abstract":"Medical image encryption plays an important role in protecting sensitive health information from cyberattacks and unauthorized access. In this paper, we introduce a secure and robust encryption scheme that is multi-modality compatible and works with MRI, CT, X-Ray and Ultrasound images for different anatomical region of interest. The method utilizes hyperchaotic signals and multi-level diffusion methods. The encryption starts by taking DICOM image as input, then padding to increase the image area. Chaotic signals are produced by a logistic map and are used to carry out pixel random permutation. Then, multi-level diffusion is carried out by 4-bit, 8-bit, radial and adjacent diffusion to provide high randomness and immunity against statistical attacks. In addition, we propose a captcha-based authentication scheme to further improve security. An algorithm generates alphanumeric captcha-based image which is encrypted with the same chaotic and diffusion methods as the medical image. Both encrypted images(DICOM image and captcha image) are then superimposed to create a final encrypted output, essentially integrating dual-layer security. Upon decryption, the superimposed image is again decomposed back to original medical and captcha images, and inverse operations are performed to obtain the original unencrypted data. Experimental results show that the proposed method provides strong protection with no loss in image integrity, thereby reducing unauthorized data breaches to a significant level. The dual-encryption approach not only protects the confidentiality of the medical images but also enhances authentication by incorporating captcha.","authors":["Anandik N Anand","Sishu Shankar Muni","Abhishek Kaushik"],"url":"https://arxiv.org/abs/2504.20689"}
{"created":"2025-04-30","title":"In-Context Edit: Enabling Instructional Image Editing with In-Context Generation in Large Scale Diffusion Transformer","abstract":"Instruction-based image editing enables robust image modification via natural language prompts, yet current methods face a precision-efficiency tradeoff. Fine-tuning methods demand significant computational resources and large datasets, while training-free techniques struggle with instruction comprehension and edit quality. We resolve this dilemma by leveraging large-scale Diffusion Transformer (DiT)' enhanced generation capacity and native contextual awareness. Our solution introduces three contributions: (1) an in-context editing framework for zero-shot instruction compliance using in-context prompting, avoiding structural changes; (2) a LoRA-MoE hybrid tuning strategy that enhances flexibility with efficient adaptation and dynamic expert routing, without extensive retraining; and (3) an early filter inference-time scaling method using vision-language models (VLMs) to select better initial noise early, improving edit quality. Extensive evaluations demonstrate our method's superiority: it outperforms state-of-the-art approaches while requiring only 0.5% training data and 1% trainable parameters compared to conventional baselines. This work establishes a new paradigm that enables high-precision yet efficient instruction-guided editing. Codes and demos can be found in https://river-zhang.github.io/ICEdit-gh-pages/.","authors":["Zechuan Zhang","Ji Xie","Yu Lu","Zongxin Yang","Yi Yang"],"url":"https://arxiv.org/abs/2504.20690"}
{"created":"2025-04-30","title":"A Decentralized Local Flexibility Market for Local Energy Communities to Mitigate Grid Congestion: A Case Study in Sweden","abstract":"This paper proposes a preventive congestion management framework with joint Local Flexibility Capacity Market (LFCM) and Local Energy Markets (LEMs). The framework enables Local Energy Communities (LECs) to optimize their flexibility potential across the LEM, LFCM, and heat markets. The LECs utilize their heat and electricity resources to offer flexibility services to Distribution System Operators (DSOs) for congestion relief. In this framework, energy and flexibility are treated as separate variables, each subject to different pricing scheme. Flexibility prices are market-driven, dynamically reflecting the location and severity of congestion. A case study conducted at Chalmers University of Technology, Sweden, shows that the proposed framework can effectively mitigate congestion by trading the LECs flexibility in the LFCM. The study also highlights up to 40% financial benefits for LECs, promoting the LFCM as a viable solution for congestion management in future decentralized energy systems.","authors":["Maryam Mohiti","Mohammadreza Mazidi","David Steen","Le Anh Tuan"],"url":"https://arxiv.org/abs/2504.20697"}
{"created":"2025-04-30","title":"Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?","abstract":"A frequently observed problem with LLMs is their tendency to generate output that is nonsensical, illogical, or factually incorrect, often referred to broadly as hallucination. Building on the recently proposed HalluciGen task for hallucination detection and generation, we evaluate a suite of open-access LLMs on their ability to detect intrinsic hallucinations in two conditional generation tasks: translation and paraphrasing. We study how model performance varies across tasks and language and we investigate the impact of model size, instruction tuning, and prompt choice. We find that performance varies across models but is consistent across prompts. Finally, we find that NLI models perform comparably well, suggesting that LLM-based detectors are not the only viable option for this specific task.","authors":["Evangelia Gogoulou","Shorouq Zahra","Liane Guillou","Luise D\\\"urlich","Joakim Nivre"],"url":"https://arxiv.org/abs/2504.20699"}
{"created":"2025-04-30","title":"Building Trust in Healthcare with Privacy Techniques: Blockchain in the Cloud","abstract":"This study introduces a cutting-edge architecture developed for the NewbornTime project, which uses advanced AI to analyze video data at birth and during newborn resuscitation, with the aim of improving newborn care. The proposed architecture addresses the crucial issues of patient consent, data security, and investing trust in healthcare by integrating Ethereum blockchain with cloud computing. Our blockchain-based consent application simplifies patient consent's secure and transparent management. We explain the smart contract mechanisms and privacy measures employed, ensuring data protection while permitting controlled data sharing among authorized parties. This work demonstrates the potential of combining blockchain and cloud technologies in healthcare, emphasizing their role in maintaining data integrity, with implications for computer science and healthcare innovation.","authors":["Ferhat Ozgur Catak","Chunming Rong","{\\O}yvind Meinich-Bache","Sara Brunner","Kjersti Engan"],"url":"https://arxiv.org/abs/2504.20700"}
{"created":"2025-04-30","title":"BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification","abstract":"This paper presents our system developed for the SemEval-2025 Task 9: The Food Hazard Detection Challenge. The shared task's objective is to evaluate explainable classification systems for classifying hazards and products in two levels of granularity from food recall incident reports. In this work, we propose text augmentation techniques as a way to improve poor performance on minority classes and compare their effect for each category on various transformer and machine learning models. We explore three word-level data augmentation techniques, namely synonym replacement, random word swapping, and contextual word insertion. The results show that transformer models tend to have a better overall performance. None of the three augmentation techniques consistently improved overall performance for classifying hazards and products. We observed a statistically significant improvement (P < 0.05) in the fine-grained categories when using the BERT model to compare the baseline with each augmented model. Compared to the baseline, the contextual words insertion augmentation improved the accuracy of predictions for the minority hazard classes by 6%. This suggests that targeted augmentation of minority classes can improve the performance of transformer models.","authors":["Foteini Papadopoulou","Osman Mutlu","Neris \\\"Ozen","Bas H. M. van der Velden","Iris Hendrickx","Ali H\\\"urriyeto\\u{g}lu"],"url":"https://arxiv.org/abs/2504.20703"}
{"created":"2025-04-30","title":"Asymptotic Fair Division: Chores Are Easier Than Goods","abstract":"When dividing items among agents, two of the most widely studied fairness notions are envy-freeness and proportionality. We consider a setting where $m$ chores are allocated to $n$ agents and the disutility of each chore for each agent is drawn from a probability distribution. We show that an envy-free allocation exists with high probability provided that $m \\ge 2n$, and moreover, $m$ must be at least $n+\\Theta(n)$ in order for the existence to hold. On the other hand, we prove that a proportional allocation is likely to exist as long as $m = \\omega(1)$, and this threshold is asymptotically tight. Our results reveal a clear contrast with the allocation of goods, where a larger number of items is necessary to ensure existence for both notions.","authors":["Pasin Manurangsi","Warut Suksompong"],"url":"https://arxiv.org/abs/2504.20704"}
{"created":"2025-04-30","title":"Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think","abstract":"Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\\% and 10\\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.","authors":["Hasan Abed Al Kader Hammoud","Hani Itani","Bernard Ghanem"],"url":"https://arxiv.org/abs/2504.20708"}
{"created":"2025-04-30","title":"Neural semi-Lagrangian method for high-dimensional advection-diffusion problems","abstract":"This work is devoted to the numerical approximation of high-dimensional advection-diffusion equations. It is well-known that classical methods, such as the finite volume method, suffer from the curse of dimensionality, and that their time step is constrained by a stability condition. The semi-Lagrangian method is known to overcome the stability issue, while recent time-discrete neural network-based approaches overcome the curse of dimensionality. In this work, we propose a novel neural semi-Lagrangian method that combines these last two approaches. It relies on projecting the initial condition onto a finite-dimensional neural space, and then solving an optimization problem, involving the backwards characteristic equation, at each time step. It is particularly well-suited for implementation on GPUs, as it is fully parallelizable and does not require a mesh. We provide rough error estimates, and present several high-dimensional numerical experiments to assess the performance of our approach, and compare it to other neural methods.","authors":["Emmanuel Franck","Victor Michel-Dansac","Laurent Navoret"],"url":"https://arxiv.org/abs/2504.20715"}
{"created":"2025-04-30","title":"Learning a General Model: Folding Clothing with Topological Dynamics","abstract":"The high degrees of freedom and complex structure of garments present significant challenges for clothing manipulation. In this paper, we propose a general topological dynamics model to fold complex clothing. By utilizing the visible folding structure as the topological skeleton, we design a novel topological graph to represent the clothing state. This topological graph is low-dimensional and applied for complex clothing in various folding states. It indicates the constraints of clothing and enables predictions regarding clothing movement. To extract graphs from self-occlusion, we apply semantic segmentation to analyze the occlusion relationships and decompose the clothing structure. The decomposed structure is then combined with keypoint detection to generate the topological graph. To analyze the behavior of the topological graph, we employ an improved Graph Neural Network (GNN) to learn the general dynamics. The GNN model can predict the deformation of clothing and is employed to calculate the deformation Jacobi matrix for control. Experiments using jackets validate the algorithm's effectiveness to recognize and fold complex clothing with self-occlusion.","authors":["Yiming Liu","Lijun Han","Enlin Gu","Hesheng Wang"],"url":"https://arxiv.org/abs/2504.20720"}
{"created":"2025-04-30","title":"Enhancing Vulnerability Reports with Automated and Augmented Description Summarization","abstract":"Public vulnerability databases, such as the National Vulnerability Database (NVD), document vulnerabilities and facilitate threat information sharing. However, they often suffer from short descriptions and outdated or insufficient information. In this paper, we introduce Zad, a system designed to enrich NVD vulnerability descriptions by leveraging external resources. Zad consists of two pipelines: one collects and filters supplementary data using two encoders to build a detailed dataset, while the other fine-tunes a pre-trained model on this dataset to generate enriched descriptions. By addressing brevity and improving content quality, Zad produces more comprehensive and cohesive vulnerability descriptions. We evaluate Zad using standard summarization metrics and human assessments, demonstrating its effectiveness in enhancing vulnerability information.","authors":["Hattan Althebeiti","Mohammed Alkinoon","Manar Mohaisen","Saeed Salem","DaeHun Nyang","David Mohaisen"],"url":"https://arxiv.org/abs/2504.20726"}
{"created":"2025-04-30","title":"Handling Large-Scale Network Flow Records: A Comparative Study on Lossy Compression","abstract":"Flow records, that summarize the characteristics of traffic flows, represent a practical and powerful way to monitor a network. While they already offer significant compression compared to full packet captures, their sheer volume remains daunting, especially for large Internet Service Providers (ISPs). In this paper, we investigate several lossy compression techniques to further reduce storage requirements while preserving the utility of flow records for key tasks, such as predicting the domain name of contacted servers. Our study evaluates scalar quantization, Principal Component Analysis (PCA), and vector quantization, applied to a real-world dataset from an operational campus network. Results reveal that scalar quantization provides the best tradeoff between compression and accuracy. PCA can preserve predictive accuracy but hampers subsequent entropic compression, and while vector quantization shows promise, it struggles with scalability due to the high-dimensional nature of the data. These findings result in practical strategies for optimizing flow record storage in large-scale monitoring scenarios.","authors":["Gabriele Merlach","Damiano Ravalico","Martino Trevisan","Fabio Palmese","Giovanni Baccichet","Alessandro E. C. Redondi"],"url":"https://arxiv.org/abs/2504.20729"}
{"created":"2025-04-30","title":"Bayesian Inference in Quantum Programs","abstract":"Conditioning is a key feature in probabilistic programming to enable modeling the influence of data (also known as observations) to the probability distribution described by such programs. Determining the posterior distribution is also known as Bayesian inference. This paper equips a quantum while-language with conditioning, defines its denotational and operational semantics over infinite-dimensional Hilbert spaces, and shows their equivalence. We provide sufficient conditions for the existence of weakest (liberal) precondition-transformers and derive inductive characterizations of these transformers. It is shown how w(l)p-transformers can be used to assess the effect of Bayesian inference on (possibly diverging) quantum programs.","authors":["Christina Gehnen","Dominique Unruh","Joost-Pieter Katoen"],"url":"https://arxiv.org/abs/2504.20732"}
{"created":"2025-04-30","title":"Unsupervised Surrogate Anomaly Detection","abstract":"In this paper, we study unsupervised anomaly detection algorithms that learn a neural network representation, i.e. regular patterns of normal data, which anomalies are deviating from. Inspired by a similar concept in engineering, we refer to our methodology as surrogate anomaly detection. We formalize the concept of surrogate anomaly detection into a set of axioms required for optimal surrogate models and propose a new algorithm, named DEAN (Deep Ensemble ANomaly detection), designed to fulfill these criteria. We evaluate DEAN on 121 benchmark datasets, demonstrating its competitive performance against 19 existing methods, as well as the scalability and reliability of our method.","authors":["Simon Kl\\\"uttermann","Tim Katzke","Emmanuel M\\\"uller"],"url":"https://arxiv.org/abs/2504.20733"}
{"created":"2025-04-30","title":"UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities","abstract":"Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.","authors":["Woongyeong Yeo","Kangsan Kim","Soyeong Jeong","Jinheon Baek","Sung Ju Hwang"],"url":"https://arxiv.org/abs/2504.20734"}
{"created":"2025-04-30","title":"Intelligent Task Offloading in VANETs: A Hybrid AI-Driven Approach for Low-Latency and Energy Efficiency","abstract":"Vehicular Ad-hoc Networks (VANETs) are integral to intelligent transportation systems, enabling vehicles to offload computational tasks to nearby roadside units (RSUs) and mobile edge computing (MEC) servers for real-time processing. However, the highly dynamic nature of VANETs introduces challenges, such as unpredictable network conditions, high latency, energy inefficiency, and task failure. This research addresses these issues by proposing a hybrid AI framework that integrates supervised learning, reinforcement learning, and Particle Swarm Optimization (PSO) for intelligent task offloading and resource allocation. The framework leverages supervised models for predicting optimal offloading strategies, reinforcement learning for adaptive decision-making, and PSO for optimizing latency and energy consumption. Extensive simulations demonstrate that the proposed framework achieves significant reductions in latency and energy usage while improving task success rates and network throughput. By offering an efficient, and scalable solution, this framework sets the foundation for enhancing real-time applications in dynamic vehicular environments.","authors":["Tariq Qayyum","Asadullah Tariq","Muhammad Ali","Mohamed Adel Serhani","Zouheir Trabelsi","Maite L\\'opez-S\\'anchez"],"url":"https://arxiv.org/abs/2504.20735"}
{"created":"2025-04-30","title":"A Survey on Event-based Optical Marker Systems","abstract":"The advent of event-based cameras, with their low latency, high dynamic range, and reduced power consumption, marked a significant change in robotic vision and machine perception. In particular, the combination of these neuromorphic sensors with widely-available passive or active optical markers (e.g. AprilTags, arrays of blinking LEDs), has recently opened up a wide field of possibilities. This survey paper provides a comprehensive review on Event-Based Optical Marker Systems (EBOMS). We analyze the basic principles and technologies on which these systems are based, with a special focus on their asynchronous operation and robustness against adverse lighting conditions. We also describe the most relevant applications of EBOMS, including object detection and tracking, pose estimation, and optical communication. The article concludes with a discussion of possible future research directions in this rapidly-emerging and multidisciplinary field.","authors":["Nafiseh Jabbari Tofighi","Maxime Robic","Fabio Morbidi","Pascal Vasseur"],"url":"https://arxiv.org/abs/2504.20736"}
{"created":"2025-04-30","title":"EDD-NSTE: Edge Data Distribution as a Network Steiner Tree Estimation in Edge Computing","abstract":"Edge computing is a distributed computing paradigm that brings computation and data storage closer to the user's geographical location to improve response times and save bandwidth. It also helps to power a variety of applications requiring low latency. These application data hosted on the cloud needs to be transferred to the respective edge servers in a specific area to help provide low-latency app functionalities to the users of that area. Meanwhile, these arbitrary heavy data transactions from the cloud to the edge servers result in high cost and time penalties. Thus, we need an application data distribution strategy that minimizes these penalties within the app vendors' specific latency constraint. In this work, we provide a refined formulation of an optimal approach to solve this Edge Data Distribution (EDD) problem using Integer Programming (IP) technique. Due to the time complexity limitation of the IP approach, we suggest an O(k) approximation algorithm based on network Steiner tree estimation (EDD-NSTE) for estimating solutions to dense, large-scale EDD problems. Integer Programming and EDD-NSTE are evaluated on a standard real-world EUA data set and the result demonstrates that EDD-NSTE significantly outperforms with a performance margin of 86.67% over the other three representative approaches and the state-of-the-art approach.","authors":["Ravi Shankar","Aryabartta Sahu"],"url":"https://arxiv.org/abs/2504.20738"}
{"created":"2025-04-30","title":"Formal and Empirical Study of Metadata-Based Profiling for Resource Management in the Computing Continuum","abstract":"We present and formalize a general approach for profiling workload by leveraging only a priori available static metadata to supply appropriate resource needs. Understanding the requirements and characteristics of a workload's runtime is essential. Profiles are essential for the platform (or infrastructure) provider because they want to ensure that Service Level Agreements and their objectives (SLOs) are fulfilled and, at the same time, avoid allocating too many resources to the workload. When the infrastructure to manage is the computing continuum (i.e., from IoT to Edge to Cloud nodes), there is a big problem of placement and tradeoff or distribution and performance. Still, existing techniques either rely on static predictions or runtime profiling, which are proven to deliver poor performance in runtime environments or require laborious mechanisms to produce fast and reliable evaluations. We want to propose a new approach for it. Our profile combines the information from past execution traces with the related workload metadata, equipping an infrastructure orchestrator with a fast and precise association of newly submitted workloads. We differentiate from previous works because we extract the profile group metadata saliency from the groups generated by grouping similar runtime behavior. We first formalize its functioning and its main components. Subsequently, we implement and empirically analyze our proposed technique on two public data sources: Alibaba cloud machine learning workloads and Google cluster data. Despite relying on partially anonymized or obscured information, the approach provides accurate estimates of workload runtime behavior in real-time.","authors":["Andrea Morichetta","Stefan Nastic","Victor Casamayor Pujol","Schahram Dustdar"],"url":"https://arxiv.org/abs/2504.20740"}
{"created":"2025-04-30","title":"In defence of post-hoc explanations in medical AI","abstract":"Since the early days of the Explainable AI movement, post-hoc explanations have been praised for their potential to improve user understanding, promote trust, and reduce patient safety risks in black box medical AI systems. Recently, however, critics have argued that the benefits of post-hoc explanations are greatly exaggerated since they merely approximate, rather than replicate, the actual reasoning processes that black box systems take to arrive at their outputs. In this article, we aim to defend the value of post-hoc explanations against this recent critique. We argue that even if post-hoc explanations do not replicate the exact reasoning processes of black box systems, they can still improve users' functional understanding of black box systems, increase the accuracy of clinician-AI teams, and assist clinicians in justifying their AI-informed decisions. While post-hoc explanations are not a \"silver bullet\" solution to the black box problem in medical AI, we conclude that they remain a useful strategy for addressing the black box problem in medical AI.","authors":["Joshua Hatherley","Lauritz Munch","Jens Christian Bjerring"],"url":"https://arxiv.org/abs/2504.20741"}
{"created":"2025-04-30","title":"Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers","abstract":"Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.","authors":["Roman Abramov","Felix Steinbauer","Gjergji Kasneci"],"url":"https://arxiv.org/abs/2504.20752"}
{"created":"2025-04-30","title":"DDPS: Discrete Diffusion Posterior Sampling for Paths in Layered Graphs","abstract":"Diffusion models form an important class of generative models today, accounting for much of the state of the art in cutting edge AI research. While numerous extensions beyond image and video generation exist, few of such approaches address the issue of explicit constraints in the samples generated. In this paper, we study the problem of generating paths in a layered graph (a variant of a directed acyclic graph) using discrete diffusion models, while guaranteeing that our generated samples are indeed paths. Our approach utilizes a simple yet effective representation for paths which we call the padded adjacency-list matrix (PALM). In addition, we show how to effectively perform classifier guidance, which helps steer the sampled paths to specific preferred edges without any retraining of the diffusion model. Our preliminary results show that empirically, our method outperforms alternatives which do not explicitly account for path constraints.","authors":["Hao Luan","See-Kiong Ng","Chun Kai Ling"],"url":"https://arxiv.org/abs/2504.20754"}
{"created":"2025-04-30","title":"Graph-Based Fault Diagnosis for Rotating Machinery: Adaptive Segmentation and Structural Feature Integration","abstract":"This paper proposes a novel graph-based framework for robust and interpretable multiclass fault diagnosis in rotating machinery. The method integrates entropy-optimized signal segmentation, time-frequency feature extraction, and graph-theoretic modeling to transform vibration signals into structured representations suitable for classification. Graph metrics, such as average shortest path length, modularity, and spectral gap, are computed and combined with local features to capture global and segment-level fault characteristics. The proposed method achieves high diagnostic accuracy when evaluated on two benchmark datasets, the CWRU bearing dataset (under 0-3 HP loads) and the SU gearbox and bearing datasets (under different speed-load configurations). Classification scores reach up to 99.8% accuracy on Case Western Reserve University (CWRU) and 100% accuracy on the Southeast University datasets using a logistic regression classifier. Furthermore, the model exhibits strong noise resilience, maintaining over 95.4% accuracy at high noise levels (standard deviation = 0.5), and demonstrates excellent cross-domain transferability with up to 99.7% F1-score in load-transfer scenarios. Compared to traditional techniques, this approach requires no deep learning architecture, enabling lower complexity while ensuring interpretability. The results confirm the method's scalability, reliability, and potential for real-time deployment in industrial diagnostics.","authors":["Moirangthem Tiken Singh"],"url":"https://arxiv.org/abs/2504.20756"}
{"created":"2025-04-30","title":"Confidence-based Intent Prediction for Teleoperation in Bimanual Robotic Suturing","abstract":"Robotic-assisted procedures offer enhanced precision, but while fully autonomous systems are limited in task knowledge, difficulties in modeling unstructured environments, and generalisation abilities, fully manual teleoperated systems also face challenges such as delay, stability, and reduced sensory information. To address these, we developed an interactive control strategy that assists the human operator by predicting their motion plan at both high and low levels. At the high level, a surgeme recognition system is employed through a Transformer-based real-time gesture classification model to dynamically adapt to the operator's actions, while at the low level, a Confidence-based Intention Assimilation Controller adjusts robot actions based on user intent and shared control paradigms. The system is built around a robotic suturing task, supported by sensors that capture the kinematics of the robot and task dynamics. Experiments across users with varying skill levels demonstrated the effectiveness of the proposed approach, showing statistically significant improvements in task completion time and user satisfaction compared to traditional teleoperation.","authors":["Zhaoyang Jacopo Hu","Haozheng Xu","Sion Kim","Yanan Li","Ferdinando Rodriguez y Baena","Etienne Burdet"],"url":"https://arxiv.org/abs/2504.20761"}
{"created":"2025-04-30","title":"An Online Cross-layered Defense Strategy with Bandwidth Allocation for Multi-channel Systems under DoS Attacks","abstract":"This paper proposes an online cross-layered defense strategy for multi-channel systems with switched dynamics under DoS attacks. The enabling condition of a channel under attacks is formulated with respect to attack flow and channel bandwidth, then a new networked control system model bridging the gap between system dynamics and network deployment is built. Based on this, the cross-layered defense strategy is proposed. It jointly optimizes the controller gain and bandwidth allocation of channels according to the real-time attack flow and system dynamics, by solving a mixed-integer semidefinite programming online. A smart enumeration algorithm for non-convex bi-level optimization is proposed to analyze the stability under the strategy. Numerical examples are given to illustrate the high resilience from the cross-layered feature.","authors":["Liheng Wan","Panshuo Li","James Lam"],"url":"https://arxiv.org/abs/2504.20762"}
{"created":"2025-04-30","title":"Understanding Large Language Model Supply Chain: Structure, Domain, and Vulnerabilities","abstract":"Large Language Models (LLMs) have revolutionized artificial intelligence (AI), driving breakthroughs in natural language understanding, text generation, and autonomous systems. However, the rapid growth of LLMs presents significant challenges in the security and reliability of the Large Language Model Supply Chain (LLMSC), a complex network of open-source components, libraries, and tools essential for LLM development and deployment. Despite its critical importance, the LLMSC remains underexplored, particularly regarding its structural characteristics, domain composition, and security vulnerabilities. To address this gap, we conduct the first empirical study of the LLMSC, analyzing a curated dataset of open-source packages from PyPI and NPM across 14 functional domains. We construct a directed dependency graph comprising 15,725 nodes, 10,402 edges, and 180 unique vulnerabilities to investigate the structural characteristics of the LLMSC and analyze how security risks propagate through its dependency network. Our findings reveal that the LLMSC exhibits a ``locally dense, globally sparse'' topology, with 79.7% of dependency trees containing fewer than 5 nodes, while a few large trees dominate the ecosystem, accounting for 77.66% of all nodes. The graph is characterized by high-degree hubs, with the top 5 most connected nodes averaging 1,282 dependents each. Security analysis shows that critical vulnerabilities propagate to an average of 142.1 nodes at the second layer of dependency trees and peak at 237.8 affected nodes at the third layer. Notably, cascading risks are concentrated in critical hub nodes such as transformers, which directly or indirectly affect over 1,300 downstream packages. These findings provide quantitative insights into the structural and security dynamics of the LLMSC and emphasize the need for targeted mitigation strategies to enhance ecosystem resilience.","authors":["Yanzhe Hu","Shenao Wang","Tianyuan Nie","Yanjie Zhao","Haoyu Wang"],"url":"https://arxiv.org/abs/2504.20763"}
{"created":"2025-04-30","title":"did:self A registry-less DID method","abstract":"We introduce did:self, a Decentralized Identifier (DID) method that does not depend on any trusted registry for storing the corresponding DID documents. Information for authenticating a did:self subject can be disseminated using any means and without making any security assumption about the delivery method. did:self is lightweight, it allows controlled delegation, it offers increased security and privacy, and it can be used for identifying people, content, as well as IoT devices. Furthermore, DID documents in did:self can be implicit, allowing re-construction of DID documents based on other authentication material, such as JSON Web Tokens and X.509 certificates.","authors":["Nikos Fotiou","George C. Polyzos","Vasilios A. Siris"],"url":"https://arxiv.org/abs/2504.20767"}
{"created":"2025-04-30","title":"LakeVilla: Multi-Table Transactions for Lakehouses","abstract":"Data lakehouses (LHs) are at the core of current cloud analytics stacks by providing elastic, relational compute on data in cloud data lakes across vendors. For relational semantics, they rely on open table formats (OTFs). Unfortunately, they have many missing features inherent to their metadata designs, like no support for multi-table transactions and recovery in case of an abort in concurrent, multi-query workloads. This, in turn, can lead to non-repeatable reads, stale data, and high costs in productive cloud systems. In this work, we introduce LakeVilla, a complementary solution that introduces recovery, multi-query/table transactions, and transaction isolation to state-of-the-art OTFs like Apache Iceberg and Delta Lake tables. We investigate its transactional guarantees and show it has minimal impact on performance (2% YCSB writes, 2.5% TPC-DS reads) and provides concurrency control for multiple readers and writers for arbitrary long transactions in OTFs in a non-invasive way.","authors":["Tobias G\\\"otz","Daniel Ritter","Jana Giceva"],"url":"https://arxiv.org/abs/2504.20768"}
{"created":"2025-04-30","title":"Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption","abstract":"Chain-of-thought prompting has demonstrated great success in facilitating the reasoning abilities of large language models. In this work, we explore how these enhanced reasoning abilities can be exploited to improve the robustness of large language models in tasks that are not necessarily reasoning-focused. In particular, we show how a wide range of large language models exhibit significantly improved robustness against reference corruption using a simple method called chain-of-defensive-thought, where only a few exemplars with structured and defensive reasoning are provided as demonstrations. Empirically, the improvements can be astounding, especially given the simplicity and applicability of the method. For example, in the Natural Questions task, the accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting when 1 out of 10 references provided is corrupted with prompt injection attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting maintains an accuracy of 50%.","authors":["Wenxiao Wang","Parsa Hosseini","Soheil Feizi"],"url":"https://arxiv.org/abs/2504.20769"}
{"created":"2025-04-30","title":"JTreeformer: Graph-Transformer via Latent-Diffusion Model for Molecular Generation","abstract":"The discovery of new molecules based on the original chemical molecule distributions is of great importance in medicine. The graph transformer, with its advantages of high performance and scalability compared to traditional graph networks, has been widely explored in recent research for applications of graph structures. However, current transformer-based graph decoders struggle to effectively utilize graph information, which limits their capacity to leverage only sequences of nodes rather than the complex topological structures of molecule graphs. This paper focuses on building a graph transformer-based framework for molecular generation, which we call \\textbf{JTreeformer} as it transforms graph generation into junction tree generation. It combines GCN parallel with multi-head attention as the encoder. It integrates a directed acyclic GCN into a graph-based Transformer to serve as a decoder, which can iteratively synthesize the entire molecule by leveraging information from the partially constructed molecular structure at each step. In addition, a diffusion model is inserted in the latent space generated by the encoder, to enhance the efficiency and effectiveness of sampling further. The empirical results demonstrate that our novel framework outperforms existing molecule generation methods, thus offering a promising tool to advance drug discovery (https://anonymous.4open.science/r/JTreeformer-C74C).","authors":["Ji Shi","Chengxun Xie","Zhonghao Li","Xinming Zhang","Miao Zhang"],"url":"https://arxiv.org/abs/2504.20770"}
{"created":"2025-04-30","title":"Turing Machine Evaluation for Large Language Model","abstract":"With the rapid development and widespread application of Large Language Models (LLMs), rigorous evaluation has become particularly crucial. This research adopts a novel perspective, focusing on evaluating the core computational reasoning ability of LLMs, defined as the capacity of model to accurately understand rules, and execute logically computing operations. This capability assesses the reliability of LLMs as precise executors, and is critical to advanced tasks such as complex code generation and multi-step problem-solving. We propose an evaluation framework based on Universal Turing Machine (UTM) simulation. This framework requires LLMs to strictly follow instructions and track dynamic states, such as tape content and read/write head position, during multi-step computations. To enable standardized evaluation, we developed TMBench, a benchmark for systematically studying the computational reasoning capabilities of LLMs. TMBench provides several key advantages, including knowledge-agnostic evaluation, adjustable difficulty, foundational coverage through Turing machine encoding, and unlimited capacity for instance generation, ensuring scalability as models continue to evolve. We find that model performance on TMBench correlates strongly with performance on other recognized reasoning benchmarks (Pearson correlation coefficient is 0.73), clearly demonstrating that computational reasoning is a significant dimension for measuring the deep capabilities of LLMs. Code and data are available at https://github.com/HaitaoWuTJU/Turing-Machine-Bench.","authors":["Haitao Wu","Zongbo Han","Huaxi Huang","Changqing Zhang"],"url":"https://arxiv.org/abs/2504.20771"}
{"created":"2025-04-30","title":"On the Effect of Time Preferences on the Price of Anarchy","abstract":"This paper examines the impact of agents' myopic optimization on the efficiency of systems comprised by many selfish agents. In contrast to standard congestion games where agents interact in a one-shot fashion, in our model each agent chooses an infinite sequence of actions and maximizes the total reward stream discounted over time under different ways of computing present values. Our model assumes that actions consume common resources that get congested, and the action choice by an agent affects the completion times of actions chosen by other agents, which in turn affects the time rewards are accrued and their discounted value. This is a mean-field game, where an agent's reward depends on the decisions of the other agents through the resulting action completion times. For this type of game we define stationary equilibria, and analyze their existence and price of anarchy (PoA). Overall, we find that the PoA depends entirely on the type of discounting rather than its specific parameters. For exponential discounting, myopic behaviour leads to extreme inefficiency: the PoA is infinity for any value of the discount parameter. For power law discounting, such inefficiency is greatly reduced and the PoA is 2 whenever stationary equilibria exist. This matches the PoA when there is no discounting and players maximize long-run average rewards. Additionally, we observe that exponential discounting may introduce unstable equilibria in learning algorithms, if action completion times are interdependent. In contrast, under no discounting all equilibria are stable.","authors":["Yunpeng Li","Antonis Dimakis","Costas A. Courcoubetis"],"url":"https://arxiv.org/abs/2504.20774"}
{"created":"2025-04-30","title":"ECOSoundSet: a finely annotated dataset for the automated acoustic identification of Orthoptera and Cicadidae in North, Central and temperate Western Europe","abstract":"Currently available tools for the automated acoustic recognition of European insects in natural soundscapes are limited in scope. Large and ecologically heterogeneous acoustic datasets are currently needed for these algorithms to cross-contextually recognize the subtle and complex acoustic signatures produced by each species, thus making the availability of such datasets a key requisite for their development. Here we present ECOSoundSet (European Cicadidae and Orthoptera Sound dataSet), a dataset containing 10,653 recordings of 200 orthopteran and 24 cicada species (217 and 26 respective taxa when including subspecies) present in North, Central, and temperate Western Europe (Andorra, Belgium, Denmark, mainland France and Corsica, Germany, Ireland, Luxembourg, Monaco, Netherlands, United Kingdom, Switzerland), collected partly through targeted fieldwork in South France and Catalonia and partly through contributions from various European entomologists. The dataset is composed of a combination of coarsely labeled recordings, for which we can only infer the presence, at some point, of their target species (weak labeling), and finely annotated recordings, for which we know the specific time and frequency range of each insect sound present in the recording (strong labeling). We also provide a train/validation/test split of the strongly labeled recordings, with respective approximate proportions of 0.8, 0.1 and 0.1, in order to facilitate their incorporation in the training and evaluation of deep learning algorithms. This dataset could serve as a meaningful complement to recordings already available online for the training of deep learning algorithms for the acoustic classification of orthopterans and cicadas in North, Central, and temperate Western Europe.","authors":["David Funosas","Elodie Massol","Yves Bas","Svenja Schmidt","Dominik Arend","Alexander Gebhard","Luc Barbaro","Sebastian K\\\"onig","Rafael Carbonell Font","David Sannier","Fernand Deroussen","J\\'er\\^ome Sueur","Christian Roesti","Tomi Trilar","Wolfgang Forstmeier","Lucas Roger","Elo\\\"isa Matheu","Piotr Guzik","Julien Barataud","Laurent Pelozuelo","St\\'ephane Puissant","Sandra Mueller","Bj\\\"orn Schuller","Jose M. Montoya","Andreas Triantafyllopoulos","Maxime Cauchoix"],"url":"https://arxiv.org/abs/2504.20776"}
{"created":"2025-04-30","title":"Deterministic Dynamic Maximal Matching in Sublinear Update Time","abstract":"We give a fully dynamic deterministic algorithm for maintaining a maximal matching of an $n$-vertex graph in $\\tilde{O}(n^{8/9})$ amortized update time. This breaks the long-standing $\\Omega(n)$-update-time barrier on dense graphs, achievable by trivially scanning all incident vertices of the updated edge, and affirmatively answers a major open question repeatedly asked in the literature [BGS15, BCHN18, Sol22]. We also present a faster randomized algorithm against an adaptive adversary with $\\tilde{O}(n^{3/4})$ amortized update time.","authors":["Aaron Bernstein","Sayan Bhattacharya","Peter Kiss","Thatchaphol Saranurak"],"url":"https://arxiv.org/abs/2504.20780"}
{"created":"2025-04-30","title":"Using LLMs in Generating Design Rationale for Software Architecture Decisions","abstract":"Design Rationale (DR) for software architecture decisions refers to the reasoning underlying architectural choices, which provides valuable insights into the different phases of the architecting process throughout software development. However, in practice, DR is often inadequately documented due to a lack of motivation and effort from developers. With the recent advancements in Large Language Models (LLMs), their capabilities in text comprehension, reasoning, and generation may enable the generation and recovery of DR for architecture decisions. In this study, we evaluated the performance of LLMs in generating DR for architecture decisions. First, we collected 50 Stack Overflow (SO) posts, 25 GitHub issues, and 25 GitHub discussions related to architecture decisions to construct a dataset of 100 architecture-related problems. Then, we selected five LLMs to generate DR for the architecture decisions with three prompting strategies, including zero-shot, chain of thought (CoT), and LLM-based agents. With the DR provided by human experts as ground truth, the Precision of LLM-generated DR with the three prompting strategies ranges from 0.267 to 0.278, Recall from 0.627 to 0.715, and F1-score from 0.351 to 0.389. Additionally, 64.45% to 69.42% of the arguments of DR not mentioned by human experts are also helpful, 4.12% to 4.87% of the arguments have uncertain correctness, and 1.59% to 3.24% of the arguments are potentially misleading. Based on the results, we further discussed the pros and cons of the three prompting strategies and the strengths and limitations of the DR generated by LLMs.","authors":["Xiyu Zhou","Ruiyin Li","Peng Liang","Beiqi Zhang","Mojtaba Shahin","Zengyang Li","Chen Yang"],"url":"https://arxiv.org/abs/2504.20781"}
{"created":"2025-04-30","title":"Integrating Human Feedback into a Reinforcement Learning-Based Framework for Adaptive User Interfaces","abstract":"Adaptive User Interfaces (AUI) play a crucial role in modern software applications by dynamically adjusting interface elements to accommodate users' diverse and evolving needs. However, existing adaptation strategies often lack real-time responsiveness. Reinforcement Learning (RL) has emerged as a promising approach for addressing complex, sequential adaptation challenges, enabling adaptive systems to learn optimal policies based on previous adaptation experiences. Although RL has been applied to AUIs,integrating RL agents effectively within user interactions remains a challenge.","authors":["Daniel Gaspar-Figueiredo","Marta Fern\\'andez-Diego","Silvia Abrah\\~ao","Emilio Insfran"],"url":"https://arxiv.org/abs/2504.20782"}
{"created":"2025-04-30","title":"Approximate Lifted Model Construction","abstract":"Probabilistic relational models such as parametric factor graphs enable efficient (lifted) inference by exploiting the indistinguishability of objects. In lifted inference, a representative of indistinguishable objects is used for computations. To obtain a relational (i.e., lifted) representation, the Advanced Colour Passing (ACP) algorithm is the state of the art. The ACP algorithm, however, requires underlying distributions, encoded as potential-based factorisations, to exactly match to identify and exploit indistinguishabilities. Hence, ACP is unsuitable for practical applications where potentials learned from data inevitably deviate even if associated objects are indistinguishable. To mitigate this problem, we introduce the $\\varepsilon$-Advanced Colour Passing ($\\varepsilon$-ACP) algorithm, which allows for a deviation of potentials depending on a hyperparameter $\\varepsilon$. $\\varepsilon$-ACP efficiently uncovers and exploits indistinguishabilities that are not exact. We prove that the approximation error induced by $\\varepsilon$-ACP is strictly bounded and our experiments show that the approximation error is close to zero in practice.","authors":["Malte Luttermann","Jan Speller","Marcel Gehrke","Tanya Braun","Ralf M\\\"oller","Mattis Hartwig"],"url":"https://arxiv.org/abs/2504.20784"}
{"created":"2025-04-30","title":"Evaluating Effects of Augmented SELFIES for Molecular Understanding Using QK-LSTM","abstract":"Identifying molecular properties, including side effects, is a critical yet time-consuming step in drug development. Failing to detect these side effects before regulatory submission can result in significant financial losses and production delays, and overlooking them during the regulatory review can lead to catastrophic consequences. This challenge presents an opportunity for innovative machine learning approaches, particularly hybrid quantum-classical models like the Quantum Kernel-Based Long Short-Term Memory (QK-LSTM) network. The QK-LSTM integrates quantum kernel functions into the classical LSTM framework, enabling the capture of complex, non-linear patterns in sequential data. By mapping input data into a high-dimensional quantum feature space, the QK-LSTM model reduces the need for large parameter sets, allowing for model compression without sacrificing accuracy in sequence-based tasks. Recent advancements have been made in the classical domain using augmented variations of the Simplified Molecular Line-Entry System (SMILES). However, to the best of our knowledge, no research has explored the impact of augmented SMILES in the quantum domain, nor the role of augmented Self-Referencing Embedded Strings (SELFIES) in either classical or hybrid quantum-classical settings. This study presents the first analysis of these approaches, providing novel insights into their potential for enhancing molecular property prediction and side effect identification. Results reveal that augmenting SELFIES yields in statistically significant improvements from SMILES by a 5.97% improvement for the classical domain and a 5.91% improvement for the hybrid quantum-classical domain.","authors":["Collin Beaudoin","Swaroop Ghosh"],"url":"https://arxiv.org/abs/2504.20789"}
{"created":"2025-04-30","title":"RecGaze: The First Eye Tracking and User Interaction Dataset for Carousel Interfaces","abstract":"Carousel interfaces are widely used in e-commerce and streaming services, but little research has been devoted to them. Previous studies of interfaces for presenting search and recommendation results have focused on single ranked lists, but it appears their results cannot be extrapolated to carousels due to the added complexity. Eye tracking is a highly informative approach to understanding how users click, yet there are no eye tracking studies concerning carousels. There are very few interaction datasets on recommenders with carousel interfaces and none that contain gaze data.","authors":["Santiago de Leon-Martinez","Jingwei Kang","Robert Moro","Maarten de Rijke","Branislav Kveton","Harrie Oosterhuis","Maria Bielikova"],"url":"https://arxiv.org/abs/2504.20792"}
{"created":"2025-04-30","title":"Q-Fusion: Diffusing Quantum Circuits","abstract":"Quantum computing holds great potential for solving socially relevant and computationally complex problems. Furthermore, quantum machine learning (QML) promises to rapidly improve our current machine learning capabilities. However, current noisy intermediate-scale quantum (NISQ) devices are constrained by limitations in the number of qubits and gate counts, which hinder their full capabilities. Furthermore, the design of quantum algorithms remains a laborious task, requiring significant domain expertise and time. Quantum Architecture Search (QAS) aims to streamline this process by automatically generating novel quantum circuits, reducing the need for manual intervention. In this paper, we propose a diffusion-based algorithm leveraging the LayerDAG framework to generate new quantum circuits. This method contrasts with other approaches that utilize large language models (LLMs), reinforcement learning (RL), variational autoencoders (VAE), and similar techniques. Our results demonstrate that the proposed model consistently generates 100% valid quantum circuit outputs.","authors":["Collin Beaudoin","Swaroop Ghosh"],"url":"https://arxiv.org/abs/2504.20794"}
{"created":"2025-04-30","title":"Effective Index Construction Algorithm for Optimal $(k,\\eta)$-cores Computation","abstract":"Computing $(k,\\eta)$-cores from uncertain graphs is a fundamental problem in uncertain graph analysis. UCF-Index is the state-of-the-art resolution to support $(k,\\eta)$-core queries, allowing the $(k,\\eta)$-core for any combination of $k$ and $\\eta$ to be computed in an optimal time. However, this index constructed by current algorithm is usually incorrect. During decomposition, the key is to obtain the $k$-probabilities of its neighbors when the vertex with minimum $k$-probability is deleted. Current method uses recursive floating-point division to update it, which can lead to serious errors. We propose a correct and efficient index construction algorithm to address this issue. Firstly, we propose tight bounds on the $k$-probabilities of the vertices that need to be updated, and the accurate $k$-probabilities are recalculated in an on-demand manner. Secondly, vertices partitioning and progressive refinement strategy is devised to search the vertex with the minimum $k$-probability, thereby reducing initialization overhead for each $k$ and avoiding unnecessary recalculations. Finally, extensive experiments demonstrate the efficiency and scalability of our approach.","authors":["Shengli Sun","Peng Xu","Guanming Jiang","Philip S. Yu","Yi Li"],"url":"https://arxiv.org/abs/2504.20795"}
{"created":"2025-04-30","title":"Partitioned Memory Storage Inspired Few-Shot Class-Incremental learning","abstract":"Current mainstream deep learning techniques exhibit an over-reliance on extensive training data and a lack of adaptability to the dynamic world, marking a considerable disparity from human intelligence. To bridge this gap, Few-Shot Class-Incremental Learning (FSCIL) has emerged, focusing on continuous learning of new categories with limited samples without forgetting old knowledge. Existing FSCIL studies typically use a single model to learn knowledge across all sessions, inevitably leading to the stability-plasticity dilemma. Unlike machines, humans store varied knowledge in different cerebral cortices. Inspired by this characteristic, our paper aims to develop a method that learns independent models for each session. It can inherently prevent catastrophic forgetting. During the testing stage, our method integrates Uncertainty Quantification (UQ) for model deployment. Our method provides a fresh viewpoint for FSCIL and demonstrates the state-of-the-art performance on CIFAR-100 and mini-ImageNet datasets.","authors":["Renye Zhang","Yimin Yin","Jinghua Zhang"],"url":"https://arxiv.org/abs/2504.20797"}
{"created":"2025-04-30","title":"Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges","abstract":"Recent technical breakthroughs in large language models (LLMs) have enabled them to fluently generate source code. Software developers often leverage both general-purpose and code-specialized LLMs to revise existing code or even generate a whole function from scratch. These capabilities are also beneficial in no-code or low-code contexts, in which one can write programs without a technical background. However, due to their internal design, LLMs are prone to generating hallucinations, which are incorrect, nonsensical, and not justifiable information but difficult to identify its presence. This problem also occurs when generating source code. Once hallucinated code is produced, it is often challenging for users to identify and fix it, especially when such hallucinations can be identified under specific execution paths. As a result, the hallucinated code may remain unnoticed within the codebase. This survey investigates recent studies and techniques relevant to hallucinations generated by CodeLLMs. We categorize the types of hallucinations in the code generated by CodeLLMs, review existing benchmarks and mitigation strategies, and identify open challenges. Based on these findings, this survey outlines further research directions in the detection and removal of hallucinations produced by CodeLLMs.","authors":["Yunseo Lee","John Youngeun Song","Dongsun Kim","Jindae Kim","Mijung Kim","Jaechang Nam"],"url":"https://arxiv.org/abs/2504.20799"}
{"created":"2025-04-30","title":"Adept: Annotation-Denoising Auxiliary Tasks with Discrete Cosine Transform Map and Keypoint for Human-Centric Pretraining","abstract":"Human-centric perception is the core of diverse computer vision tasks and has been a long-standing research focus. However, previous research studied these human-centric tasks individually, whose performance is largely limited to the size of the public task-specific datasets. Recent human-centric methods leverage the additional modalities, e.g., depth, to learn fine-grained semantic information, which limits the benefit of pretraining models due to their sensitivity to camera views and the scarcity of RGB-D data on the Internet. This paper improves the data scalability of human-centric pretraining methods by discarding depth information and exploring semantic information of RGB images in the frequency space by Discrete Cosine Transform (DCT). We further propose new annotation denoising auxiliary tasks with keypoints and DCT maps to enforce the RGB image extractor to learn fine-grained semantic information of human bodies. Our extensive experiments show that when pretrained on large-scale datasets (COCO and AIC datasets) without depth annotation, our model achieves better performance than state-of-the-art methods by +0.5 mAP on COCO, +1.4 PCKh on MPII and -0.51 EPE on Human3.6M for pose estimation, by +4.50 mIoU on Human3.6M for human parsing, by -3.14 MAE on SHA and -0.07 MAE on SHB for crowd counting, by +1.1 F1 score on SHA and +0.8 F1 score on SHA for crowd localization, and by +0.1 mAP on Market1501 and +0.8 mAP on MSMT for person ReID. We also validate the effectiveness of our method on MPII+NTURGBD datasets","authors":["Weizhen He","Yunfeng Yan","Shixiang Tang","Yiheng Deng","Yangyang Zhong","Pengxin Luo","Donglian Qi"],"url":"https://arxiv.org/abs/2504.20800"}
{"created":"2025-04-30","title":"Unlocking User-oriented Pages: Intention-driven Black-box Scanner for Real-world Web Applications","abstract":"Black-box scanners have played a significant role in detecting vulnerabilities for web applications. A key focus in current black-box scanning is increasing test coverage (i.e., accessing more web pages). However, since many web applications are user-oriented, some deep pages can only be accessed through complex user interactions, which are difficult to reach by existing black-box scanners. To fill this gap, a key insight is that web pages contain a wealth of semantic information that can aid in understanding potential user intention. Based on this insight, we propose Hoyen, a black-box scanner that uses the Large Language Model to predict user intention and provide guidance for expanding the scanning scope. Hoyen has been rigorously evaluated on 12 popular open-source web applications and compared with 6 representative tools. The results demonstrate that Hoyen performs a comprehensive exploration of web applications, expanding the attack surface while achieving about 2x than the coverage of other scanners on average, with high request accuracy. Furthermore, Hoyen detected over 90% of its requests towards the core functionality of the application, detecting more vulnerabilities than other scanners, including unique vulnerabilities in well-known web applications. Our data/code is available at https://hoyen.tjunsl.com/","authors":["Weizhe Wang","Yao Zhang","Kaitai Liang","Guangquan Xu","Hongpeng Bai","Qingyang Yan","Xi Zheng","Bin Wu"],"url":"https://arxiv.org/abs/2504.20801"}
{"created":"2025-04-30","title":"SoccerDiffusion: Toward Learning End-to-End Humanoid Robot Soccer from Gameplay Recordings","abstract":"This paper introduces SoccerDiffusion, a transformer-based diffusion model designed to learn end-to-end control policies for humanoid robot soccer directly from real-world gameplay recordings. Using data collected from RoboCup competitions, the model predicts joint command trajectories from multi-modal sensor inputs, including vision, proprioception, and game state. We employ a distillation technique to enable real-time inference on embedded platforms that reduces the multi-step diffusion process to a single step. Our results demonstrate the model's ability to replicate complex motion behaviors such as walking, kicking, and fall recovery both in simulation and on physical robots. Although high-level tactical behavior remains limited, this work provides a robust foundation for subsequent reinforcement learning or preference optimization methods. We release the dataset, pretrained models, and code under: https://bit-bots.github.io/SoccerDiffusion","authors":["Florian Vahl","J\\\"orn Griepenburg","Jan Gutsche","Jasper G\\\"uldenstein","Jianwei Zhang"],"url":"https://arxiv.org/abs/2504.20808"}
{"created":"2025-04-30","title":"Periodic Proprioceptive Stimuli Learning and Internal Model Development for Avian-inspired Flapping-wing Flight State Estimation","abstract":"This paper presents a novel learning-based approach for online state estimation in flapping wing aerial vehicles (FWAVs). Leveraging low-cost Magnetic, Angular Rate, and Gravity (MARG) sensors, the proposed method effectively mitigates the adverse effects of flapping-induced oscillations that challenge conventional estimation techniques. By employing a divide-and-conquer strategy grounded in cycle-averaged aerodynamics, the framework decouples the slow-varying components from the high-frequency oscillatory components, thereby preserving crit- ical transient behaviors while delivering an smooth inter- nal state representation. The complete oscillatory state of FWAV can be reconstructed based on above two components, leading to substantial improvements in accurate state prediction. Experimental validations on an avian-inspired FWAV demonstrate that the estimator enhances accuracy and smoothness, even under complex aerodynamic disturbances. These encouraging results highlight the potential of learning algorithms to overcome issues of flapping-wing induced oscillation dynamics.","authors":["Chen Qian","Jiaxi Xing","Jifu Yan","Mingyu Luo","Shiyu Song","Xuyi Lian","Yongchun Fang","Fei Gao","Tiefeng Li"],"url":"https://arxiv.org/abs/2504.20809"}
{"created":"2025-04-30","title":"A high-order energy-conserving semi-Lagrangian discontinuous Galerkin method for the Vlasov-Ampere system","abstract":"In this paper, we propose a high-order energy-conserving semi-Lagrangian discontinuous Galerkin(ECSLDG) method for the Vlasov-Ampere system. The method employs a semi-Lagrangian discontinuous Galerkin scheme for spatial discretization of the Vlasov equation, achieving high-order accuracy while removing the Courant-Friedrichs-Lewy (CFL) constraint. To ensure energy conservation and eliminate the need to resolve the plasma period, we adopt an energy-conserving time discretization introduced by Liu et al. [J. Comput. Phys., 492 (2023), 112412]. Temporal accuracy is further enhanced through a high-order operator splitting strategy, yielding a method that is high-order accurate in both space and time. The resulting ECSLDG scheme is unconditionally stable and conserves both mass and energy at the fully discrete level, regardless of spatial or temporal resolution. Numerical experiments demonstrate the accuracy, stability, and conservation properties of the proposed method. In particular, the method achieves more accurate enforcement of Gauss's law and improved numerical fidelity over low-order schemes, especially when using a large CFL number.","authors":["Xiaofeng Cai","Qingtao Li","Hongtao Liu","Haibiao Zheng"],"url":"https://arxiv.org/abs/2504.20813"}
{"created":"2025-04-30","title":"Secure Coding with AI, From Creation to Inspection","abstract":"While prior studies have explored security in code generated by ChatGPT and other Large Language Models, they were conducted in controlled experimental settings and did not use code generated or provided from actual developer interactions. This paper not only examines the security of code generated by ChatGPT based on real developer interactions, curated in the DevGPT dataset, but also assesses ChatGPT's capability to find and fix these vulnerabilities. We analysed 1,586 C, C++, and C# code snippets using static scanners, which detected potential issues in 124 files. After manual analysis, we selected 26 files with 32 confirmed vulnerabilities for further investigation.","authors":["Vladislav Belozerov","Peter J Barclay","Ashkan Sami"],"url":"https://arxiv.org/abs/2504.20814"}
{"created":"2025-04-30","title":"The When and How of Target Variable Transformations","abstract":"The machine learning pipeline typically involves the iterative process of (1) collecting the data, (2) preparing the data, (3) learning a model, and (4) evaluating a model. Practitioners recognize the importance of the data preparation phase in terms of its impact on the ability to learn accurate models. In this regard, significant attention is often paid to manipulating the feature set (e.g., selection, transformations, dimensionality reduction). A point that is less well appreciated is that transformations on the target variable can also have a large impact on whether it is possible to learn a suitable model. These transformations may include accounting for subject-specific biases (e.g., in how someone uses a rating scale), contexts (e.g., population size effects), and general trends (e.g., inflation). However, this point has received a much more cursory treatment in the existing literature. The goal of this paper is three-fold. First, we aim to highlight the importance of this problem by showing when transforming the target variable has been useful in practice. Second, we will provide a set of generic ``rules of thumb'' that indicate situations when transforming the target variable may be needed. Third, we will discuss which transformations should be considered in a given situation.","authors":["Loren Nuyts","Jesse Davis"],"url":"https://arxiv.org/abs/2504.20821"}
{"created":"2025-04-30","title":"An approach to melodic segmentation and classification based on filtering with the Haar-wavelet","abstract":"We present a novel method of classification and segmentation of melodies in symbolic representation. The method is based on filtering pitch as a signal over time with the Haar-wavelet, and we evaluate it on two tasks. The filtered signal corresponds to a single-scale signal ws from the continuous Haar wavelet transform. The melodies are first segmented using local maxima or zero-crossings of w_s. The segments of w_s are then classified using the k-nearest neighbour algorithm with Euclidian and city-block distances. The method proves more effective than using unfiltered pitch signals and Gestalt-based segmentation when used to recognize the parent works of segments from Bach's Two-Part Inventions (BWV 772-786). When used to classify 360 Dutch folk tunes into 26 tune families, the performance of the method is comparable to the use of pitch signals, but not as good as that of string-matching methods based on multiple features.","authors":["Gissel Velarde","Tillman Weyde","David Meredith"],"url":"https://arxiv.org/abs/2504.20822"}
{"created":"2025-04-30","title":"Hybrid Quantum Recurrent Neural Network For Remaining Useful Life Prediction","abstract":"Predictive maintenance in aerospace heavily relies on accurate estimation of the remaining useful life of jet engines. In this paper, we introduce a Hybrid Quantum Recurrent Neural Network frame- work, combining Quantum Long Short-Term Memory layers with classical dense layers for Remaining Useful Life forecasting on NASA's Commercial Modular Aero-Propulsion System Simulation dataset. Each Quantum Long Short-Term Memory gate replaces conventional linear transformations with Quantum Depth-Infused circuits, allowing the network to learn high-frequency components more effectively. Experimental results demonstrate that, despite having fewer trainable parameters, the Hybrid Quantum Recurrent Neural Network achieves up to a 5% improvement over a Recurrent Neural Network based on stacked Long Short-Term Memory layers in terms of mean root mean squared error and mean absolute error. Moreover, a thorough comparison of our method with established techniques, including Random Forest, Convolutional Neural Network, and Multilayer Perceptron, demonstrates that our approach, which achieves a Root Mean Squared Error of 15.46, surpasses these baselines by approximately 13.68%, 16.21%, and 7.87%, respectively. Nevertheless, it remains outperformed by certain advanced joint architectures. Our findings highlight the poten- tial of hybrid quantum-classical approaches for robust time-series forecasting under limited data conditions, offering new avenues for enhancing reliability in predictive maintenance tasks.","authors":["Olga Tsurkan","Aleksandra Konstantinova","Aleksandr Sedykh","Dmitrii Zhiganov","Arsenii Senokosov","Daniil Tarpanov","Matvei Anoshin","Leonid Fedichkin"],"url":"https://arxiv.org/abs/2504.20823"}
{"created":"2025-04-30","title":"DP-SMOTE: Integrating Differential Privacy and Oversampling Technique to Preserve Privacy in Smart Homes","abstract":"Smart homes represent intelligent environments where interconnected devices gather information, enhancing users living experiences by ensuring comfort, safety, and efficient energy management. To enhance the quality of life, companies in the smart device industry collect user data, including activities, preferences, and power consumption. However, sharing such data necessitates privacy-preserving practices. This paper introduces a robust method for secure sharing of data to service providers, grounded in differential privacy (DP). This empowers smart home residents to contribute usage statistics while safeguarding their privacy. The approach incorporates the Synthetic Minority Oversampling technique (SMOTe) and seamlessly integrates Gaussian noise to generate synthetic data, enabling data and statistics sharing while preserving individual privacy. The proposed method employs the SMOTe algorithm and applies Gaussian noise to generate data. Subsequently, it employs a k-anonymity function to assess reidentification risk before sharing the data. The simulation outcomes demonstrate that our method delivers strong performance in safeguarding privacy and in accuracy, recall, and f-measure metrics. This approach is particularly effective in smart homes, offering substantial utility in privacy at a reidentification risk of 30%, with Gaussian noise set to 0.3, SMOTe at 500%, and the application of a k-anonymity function with k = 2. Additionally, it shows a high classification accuracy, ranging from 90% to 98%, across various classification techniques.","authors":["Amr Tarek Elsayed","Almohammady Sobhi Alsharkawy","Mohamed Sayed Farag","Shaban Ebrahim Abu Yusuf"],"url":"https://arxiv.org/abs/2504.20827"}
{"created":"2025-04-30","title":"Ascendra: Dynamic Request Prioritization for Efficient LLM Serving","abstract":"The rapid advancement of Large Language Models (LLMs) has driven the need for more efficient serving strategies. In this context, efficiency refers to the proportion of requests that meet their Service Level Objectives (SLOs), particularly for Time To First Token (TTFT) and Time Between Tokens (TBT). However, existing systems often prioritize one metric at the cost of the other. We present Ascendra, an LLM serving system designed to meet both TTFT and TBT SLOs simultaneously. The core insight behind Ascendra is that a request's urgency evolves as it approaches its deadline. To leverage this, Ascendra partitions GPU resources into two types of instances: low-priority and high-priority. Low-priority instances maximize throughput by processing requests out of arrival order, but at the risk of request starvation. To address this, Ascendra employs a performance model to predict requests at risk of missing their SLOs and proactively offloads them to high-priority instances. High-priority instances are optimized for low-latency execution and handle urgent requests nearing their deadlines. This partitioned architecture enables Ascendra to effectively balance high throughput and low latency. Extensive evaluation shows that Ascendra improves system throughput by up to 1.7x compared to vLLM and Sarathi-Serve while meeting both TTFT and TBT SLOs.","authors":["Azam Ikram","Xiang Li","Sameh Elnikety","Saurabh Bagchi"],"url":"https://arxiv.org/abs/2504.20828"}
{"created":"2025-04-30","title":"GaussTrap: Stealthy Poisoning Attacks on 3D Gaussian Splatting for Targeted Scene Confusion","abstract":"As 3D Gaussian Splatting (3DGS) emerges as a breakthrough in scene representation and novel view synthesis, its rapid adoption in safety-critical domains (e.g., autonomous systems, AR/VR) urgently demands scrutiny of potential security vulnerabilities. This paper presents the first systematic study of backdoor threats in 3DGS pipelines. We identify that adversaries may implant backdoor views to induce malicious scene confusion during inference, potentially leading to environmental misperception in autonomous navigation or spatial distortion in immersive environments. To uncover this risk, we propose GuassTrap, a novel poisoning attack method targeting 3DGS models. GuassTrap injects malicious views at specific attack viewpoints while preserving high-quality rendering in non-target views, ensuring minimal detectability and maximizing potential harm. Specifically, the proposed method consists of a three-stage pipeline (attack, stabilization, and normal training) to implant stealthy, viewpoint-consistent poisoned renderings in 3DGS, jointly optimizing attack efficacy and perceptual realism to expose security risks in 3D rendering. Extensive experiments on both synthetic and real-world datasets demonstrate that GuassTrap can effectively embed imperceptible yet harmful backdoor views while maintaining high-quality rendering in normal views, validating its robustness, adaptability, and practical applicability.","authors":["Jiaxin Hong","Sixu Chen","Shuoyang Sun","Hongyao Yu","Hao Fang","Yuqi Tan","Bin Chen","Shuhan Qi","Jiawei Li"],"url":"https://arxiv.org/abs/2504.20829"}
{"created":"2025-04-30","title":"CMT: A Cascade MAR with Topology Predictor for Multimodal Conditional CAD Generation","abstract":"While accurate and user-friendly Computer-Aided Design (CAD) is crucial for industrial design and manufacturing, existing methods still struggle to achieve this due to their over-simplified representations or architectures incapable of supporting multimodal design requirements. In this paper, we attempt to tackle this problem from both methods and datasets aspects. First, we propose a cascade MAR with topology predictor (CMT), the first multimodal framework for CAD generation based on Boundary Representation (B-Rep). Specifically, the cascade MAR can effectively capture the ``edge-counters-surface'' priors that are essential in B-Reps, while the topology predictor directly estimates topology in B-Reps from the compact tokens in MAR. Second, to facilitate large-scale training, we develop a large-scale multimodal CAD dataset, mmABC, which includes over 1.3 million B-Rep models with multimodal annotations, including point clouds, text descriptions, and multi-view images. Extensive experiments show the superior of CMT in both conditional and unconditional CAD generation tasks. For example, we improve Coverage and Valid ratio by +10.68% and +10.3%, respectively, compared to state-of-the-art methods on ABC in unconditional generation. CMT also improves +4.01 Chamfer on image conditioned CAD generation on mmABC. The dataset, code and pretrained network shall be released.","authors":["Jianyu Wu","Yizhou Wang","Xiangyu Yue","Xinzhu Ma","Jingyang Guo","Dongzhan Zhou","Wanli Ouyang","Shixiang Tang"],"url":"https://arxiv.org/abs/2504.20830"}
{"created":"2025-04-30","title":"Reinforcement Learning for LLM Reasoning Under Memory Constraints","abstract":"We explore reinforcement learning (RL) techniques to enhance reasoning within targeted problem spaces in large language models (LLMs) under memory and compute constraints. Our focus is on critic-free methods compatible with LoRA fine-tuning on a single 40GB GPU, a common limitation in academic settings. We introduce S-GRPO, a memory-efficient variant of Group Relative Policy Optimization, and T-SPMO, a token-level prefix matching strategy for fine-grained credit assignment. Despite limited resources, when used to fine-tune Qwen2-1.5B both methods significantly improve SVAMP benchmark accuracy from 46% to above 70% using LoRA training. T-SPMO also excels in multi-digit multiplication tasks, underscoring the potential of RL fine-tuning under hardware constraints. Additionally, we find that our full-token GRPO baseline under LoRA fine-tuning did not improve model performance (compared to base model) on either task, suggesting that our memory-efficient methods may act as a form of regularization that stabilizes training when only a small subset of parameters are updated.","authors":["Alan Lee","Harry Tong"],"url":"https://arxiv.org/abs/2504.20834"}
{"created":"2025-04-30","title":"Enhancing Non-Core Language Instruction-Following in Speech LLMs via Semi-Implicit Cross-Lingual CoT Reasoning","abstract":"Large language models have been extended to the speech domain, leading to the development of speech large language models (SLLMs). While existing SLLMs demonstrate strong performance in speech instruction-following for core languages (e.g., English), they often struggle with non-core languages due to the scarcity of paired speech-text data and limited multilingual semantic reasoning capabilities. To address this, we propose the semi-implicit Cross-lingual Speech Chain-of-Thought (XS-CoT) framework, which integrates speech-to-text translation into the reasoning process of SLLMs. The XS-CoT generates four types of tokens: instruction and response tokens in both core and non-core languages, enabling cross-lingual transfer of reasoning capabilities. To mitigate inference latency in generating target non-core response tokens, we incorporate a semi-implicit CoT scheme into XS-CoT, which progressively compresses the first three types of intermediate reasoning tokens while retaining global reasoning logic during training. By leveraging the robust reasoning capabilities of the core language, XS-CoT improves responses for non-core languages by up to 45\\% in GPT-4 score when compared to direct supervised fine-tuning on two representative SLLMs, Qwen2-Audio and SALMONN. Moreover, the semi-implicit XS-CoT reduces token delay by more than 50\\% with a slight drop in GPT-4 scores. Importantly, XS-CoT requires only a small amount of high-quality training data for non-core languages by leveraging the reasoning capabilities of core languages. To support training, we also develop a data pipeline and open-source speech instruction-following datasets in Japanese, German, and French.","authors":["Hongfei Xue","Yufeng Tang","Hexin Liu","Jun Zhang","Xuelong Geng","Lei Xie"],"url":"https://arxiv.org/abs/2504.20835"}
{"created":"2025-04-30","title":"Non-Linear Modeling and Analysis of Amplifier-Less Potentiostat Architectures","abstract":"In this article, a previously published amplifier-less potentiostat architecture is further examined. Starting with a linearized model, the impact of the most important parameters is studied taking in account the electrodes-solution electrochemical interface. A detailed model is obtained and thoroughly verified, and recommended operating conditions are given for certain limit load conditions. Then, a more complete non-linear model is developed to take in account the measurement uncertainty introduced by the circuit non-linear components. This non-linear model is compared to a time domain description of the circuit and it is verified that it can predict the non-linear behavior with a precision better than 20%. This result enables the circuit designers to compensate for these effects and ultimately reduce the overall measurement uncertainty.","authors":["Andrea Sannino","David-Peter Wiens","Maurits Ortmanns","Jos\\'e I. Artigas","Ar\\'anzazu Ot\\'in"],"url":"https://arxiv.org/abs/2504.20836"}
{"created":"2025-04-30","title":"RadSAM: Segmenting 3D radiological images with a 2D promptable model","abstract":"Medical image segmentation is a crucial and time-consuming task in clinical care, where mask precision is extremely important. The Segment Anything Model (SAM) offers a promising approach, as it provides an interactive interface based on visual prompting and edition to refine an initial segmentation. This model has strong generalization capabilities, does not rely on predefined classes, and adapts to diverse objects; however, it is pre-trained on natural images and lacks the ability to process medical data effectively. In addition, this model is built for 2D images, whereas a whole medical domain is based on 3D images, such as CT and MRI. Recent adaptations of SAM for medical imaging are based on 2D models, thus requiring one prompt per slice to segment 3D objects, making the segmentation process tedious. They also lack important features such as editing. To bridge this gap, we propose RadSAM, a novel method for segmenting 3D objects with a 2D model from a single prompt. In practice, we train a 2D model using noisy masks as initial prompts, in addition to bounding boxes and points. We then use this novel prompt type with an iterative inference pipeline to reconstruct the 3D mask slice-by-slice. We introduce a benchmark to evaluate the model's ability to segment 3D objects in CT images from a single prompt and evaluate the models' out-of-domain transfer and edition capabilities. We demonstrate the effectiveness of our approach against state-of-the-art models on this benchmark using the AMOS abdominal organ segmentation dataset.","authors":["Julien Khlaut","Elodie Ferreres","Daniel Tordjman","H\\'el\\`ene Philippe","Tom Boeken","Pierre Manceron","Corentin Dancette"],"url":"https://arxiv.org/abs/2504.20837"}
{"created":"2025-04-30","title":"Bitcoin, a DAO?","abstract":"This paper investigates whether Bitcoin can be regarded as a decentralized autonomous organization (DAO), what insights it may offer for the broader DAO ecosystem, and how Bitcoin governance can be improved. First, a quantitative literature analysis reveals that Bitcoin is increasingly overlooked in DAO research, even though early works often classified it as a DAO. Next, the paper applies a DAO viability framework - centering on collective intelligence, digital democracy, and adaptation - to examine Bitcoin's organizational and governance mechanisms. Findings suggest that Bitcoin instantitates key DAO principles by enabling open participation, and employing decentralized decision-making through Bitcoin Improvement Proposals (BIPs), miner signaling, and user-activated soft forks. However, this governance carries potential risks, including reduced clarity on who truly 'votes' due to the concentration of economic power among large stakeholders. The paper concludes by highlighting opportunities to refine Bitcoin's deliberation process and reflecting on broader implications for DAO design, such as the absence of a legal entity. In doing so, it underscores Bitcoin's continued relevance as an archetype for decentralized governance, offering important findings for future DAO implementations.","authors":["Mark C. Ballandies","Guangyao Li","Claudio J. Tessone"],"url":"https://arxiv.org/abs/2504.20838"}
{"created":"2025-04-30","title":"Universal language model with the intervention of quantum theory","abstract":"This paper examines language modeling based on the theory of quantum mechanics. It focuses on the introduction of quantum mechanics into the symbol-meaning pairs of language in order to build a representation model of natural language. At the same time, it is realized that word embedding, which is widely used as a basic technique for statistical language modeling, can be explained and improved by the mathematical framework of quantum mechanics. On this basis, this paper continues to try to use quantum statistics and other related theories to study the mathematical representation, natural evolution and statistical properties of natural language. It is also assumed that the source of such quantum properties is the physicality of information. The feasibility of using quantum theory to model natural language is pointed out through the construction of a experimental code. The paper discusses, in terms of applications, the possible help of the theory in constructing generative models that are popular nowadays. A preliminary discussion of future applications of the theory to quantum computers is also presented.","authors":["D. -F. Qin"],"url":"https://arxiv.org/abs/2504.20839"}
{"created":"2025-04-30","title":"Effect of Avatar Head Movement on Communication Behaviour, Experience of Presence and Conversation Success in Triadic Conversations","abstract":"Interactive communication in virtual reality can be used in experimental paradigms to increase the ecological validity of hearing device evaluations. This requires the virtual environment to elicit natural communication behaviour in listeners. This study evaluates the effect of virtual animated characters' head movements on participants' communication behaviour and experience.","authors":["Angelika Kothe","Volker Hohmann","Giso Grimm"],"url":"https://arxiv.org/abs/2504.20844"}
{"created":"2025-04-30","title":"Disjunctive and Conjunctive Normal Form Explanations of Clusters Using Auxiliary Information","abstract":"We consider generating post-hoc explanations of clusters generated from various datasets using auxiliary information which was not used by clustering algorithms. Following terminology used in previous work, we refer to the auxiliary information as tags. Our focus is on two forms of explanations, namely disjunctive form (where the explanation for a cluster consists of a set of tags) and a two-clause conjunctive normal form (CNF) explanation (where the explanation consists of two sets of tags, combined through the AND operator). We use integer linear programming (ILP) as well as heuristic methods to generate these explanations. We experiment with a variety of datasets and discuss the insights obtained from our explanations. We also present experimental results regarding the scalability of our explanation methods.","authors":["Robert F. Downey","S. S. Ravi"],"url":"https://arxiv.org/abs/2504.20846"}
{"created":"2025-04-30","title":"Mitigating the Structural Bias in Graph Adversarial Defenses","abstract":"In recent years, graph neural networks (GNNs) have shown great potential in addressing various graph structure-related downstream tasks. However, recent studies have found that current GNNs are susceptible to malicious adversarial attacks. Given the inevitable presence of adversarial attacks in the real world, a variety of defense methods have been proposed to counter these attacks and enhance the robustness of GNNs. Despite the commendable performance of these defense methods, we have observed that they tend to exhibit a structural bias in terms of their defense capability on nodes with low degree (i.e., tail nodes), which is similar to the structural bias of traditional GNNs on nodes with low degree in the clean graph. Therefore, in this work, we propose a defense strategy by including hetero-homo augmented graph construction, $k$NN augmented graph construction, and multi-view node-wise attention modules to mitigate the structural bias of GNNs against adversarial attacks. Notably, the hetero-homo augmented graph consists of removing heterophilic links (i.e., links connecting nodes with dissimilar features) globally and adding homophilic links (i.e., links connecting nodes with similar features) for nodes with low degree. To further enhance the defense capability, an attention mechanism is adopted to adaptively combine the representations from the above two kinds of graph views. We conduct extensive experiments to demonstrate the defense and debiasing effect of the proposed strategy on benchmark datasets.","authors":["Junyuan Fang","Huimin Liu","Han Yang","Jiajing Wu","Zibin Zheng","Chi K. Tse"],"url":"https://arxiv.org/abs/2504.20848"}
{"created":"2025-04-30","title":"JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry","abstract":"Online platforms are increasingly interested in using Data-to-Text technologies to generate content and help their users. Unfortunately, traditional generative methods often fall into repetitive patterns, resulting in monotonous galleries of texts after only a few iterations. In this paper, we investigate LLM-based data-to-text approaches to automatically generate marketing texts that are of sufficient quality and diverse enough for broad adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in conjunction with fine-tuning, few-shot, and zero-shot approaches to set a baseline for diverse marketing texts. We also introduce a metric JaccDiv to evaluate the diversity of a set of texts. This research extends its relevance beyond the music industry, proving beneficial in various fields where repetitive automated content generation is prevalent.","authors":["Anum Afzal","Alexandre Mercier","Florian Matthes"],"url":"https://arxiv.org/abs/2504.20849"}
{"created":"2025-04-30","title":"Fostering Self-Directed Growth with Generative AI: Toward a New Learning Analytics Framework","abstract":"In an era increasingly shaped by decentralized knowledge ecosystems and pervasive AI technologies, fostering sustainable learner agency has become a critical educational imperative. This study introduces a novel conceptual framework integrating Generative Artificial Intelligence and Learning Analytics to cultivate Self-Directed Growth, a dynamic competency that enables learners to iteratively drive their own developmental pathways across diverse contexts.Building upon critical gaps in current research on Self Directed Learning and AI-mediated education, the proposed Aspire to Potentials for Learners (A2PL) model reconceptualizes the interplay of learner aspirations, complex thinking, and summative self-assessment within GAI supported environments.Methodological implications for future intervention design and learning analytics applications are discussed, positioning Self-Directed Growth as a pivotal axis for developing equitable, adaptive, and sustainable learning systems in the digital era.","authors":["Qianrun Mao"],"url":"https://arxiv.org/abs/2504.20851"}
{"created":"2025-04-30","title":"Towards Easy and Realistic Network Infrastructure Testing for Large-scale Machine Learning","abstract":"This paper lays the foundation for Genie, a testing framework that captures the impact of real hardware network behavior on ML workload performance, without requiring expensive GPUs. Genie uses CPU-initiated traffic over a hardware testbed to emulate GPU to GPU communication, and adapts the ASTRA-sim simulator to model interaction between the network and the ML workload.","authors":["Jinsun Yoo","ChonLam Lao","Lianjie Cao","Bob Lantz","Minlan Yu","Tushar Krishna","Puneet Sharma"],"url":"https://arxiv.org/abs/2504.20854"}
{"created":"2025-04-30","title":"Online General Knapsack with Reservation Costs","abstract":"In the online general knapsack problem, an algorithm is presented with an item $x=(s,v)$ of size $s$ and value $v$ and must irrevocably choose to pack such an item into the knapsack or reject it before the next item appears. The goal is to maximize the total value of the packed items without overflowing the knapsack's capacity.","authors":["Elisabet Burjons","Matthias Gehnen"],"url":"https://arxiv.org/abs/2504.20855"}
{"created":"2025-04-30","title":"X-Cross: Dynamic Integration of Language Models for Cross-Domain Sequential Recommendation","abstract":"As new products are emerging daily, recommendation systems are required to quickly adapt to possible new domains without needing extensive retraining. This work presents ``X-Cross'' -- a novel cross-domain sequential-recommendation model that recommends products in new domains by integrating several domain-specific language models; each model is fine-tuned with low-rank adapters (LoRA). Given a recommendation prompt, operating layer by layer, X-Cross dynamically refines the representation of each source language model by integrating knowledge from all other models. These refined representations are propagated from one layer to the next, leveraging the activations from each domain adapter to ensure domain-specific nuances are preserved while enabling adaptability across domains. Using Amazon datasets for sequential recommendation, X-Cross achieves performance comparable to a model that is fine-tuned with LoRA, while using only 25% of the additional parameters. In cross-domain tasks, such as adapting from Toys domain to Tools, Electronics or Sports, X-Cross demonstrates robust performance, while requiring about 50%-75% less fine-tuning data than LoRA to make fine-tuning effective. Furthermore, X-Cross achieves significant improvement in accuracy over alternative cross-domain baselines. Overall, X-Cross enables scalable and adaptive cross-domain recommendations, reducing computational overhead and providing an efficient solution for data-constrained environments.","authors":["Guy Hadad","Haggai Roitman","Yotam Eshel","Bracha Shapira","Lior Rokach"],"url":"https://arxiv.org/abs/2504.20859"}
{"created":"2025-04-30","title":"FedMVP: Federated Multi-modal Visual Prompt Tuning for Vision-Language Models","abstract":"Textual prompt tuning adapts Vision-Language Models (e.g., CLIP) in federated learning by tuning lightweight input tokens (or prompts) on local client data, while keeping network weights frozen. Post training, only the prompts are shared by the clients with the central server for aggregation. However, textual prompt tuning often struggles with overfitting to known concepts and may be overly reliant on memorized text features, limiting its adaptability to unseen concepts. To address this limitation, we propose Federated Multimodal Visual Prompt Tuning (FedMVP) that conditions the prompts on comprehensive contextual information -- image-conditioned features and textual attribute features of a class -- that is multimodal in nature. At the core of FedMVP is a PromptFormer module that synergistically aligns textual and visual features through cross-attention, enabling richer contexual integration. The dynamically generated multimodal visual prompts are then input to the frozen vision encoder of CLIP, and trained with a combination of CLIP similarity loss and a consistency loss. Extensive evaluation on 20 datasets spanning three generalization settings demonstrates that FedMVP not only preserves performance on in-distribution classes and domains, but also displays higher generalizability to unseen classes and domains when compared to state-of-the-art methods. Codes will be released upon acceptance.","authors":["Mainak Singha","Subhankar Roy","Sarthak Mehrotra","Ankit Jha","Moloud Abdar","Biplab Banerjee","Elisa Ricci"],"url":"https://arxiv.org/abs/2504.20860"}
{"created":"2025-04-30","title":"Simulating Heterogeneity within Elastic and Inelastic Discrete Mechanical Models","abstract":"The study investigates the elastic and fracture behaviors of discrete, elastically homogeneous models of heterogeneous media. The homogeneity is accomplished either by volumetric-deviatoric decomposition of constitutive function or by an auxiliary stress homogenization method. The elastic parameters of the homogenized material models are randomly varied in space to introduce heterogeneity independently of the geometric properties of the discrete model. Several forms of randomization are investigated using statistical properties of nodal stress oscillations in periodic representative volume elements (RVEs). It is found that the stress oscillations present in discrete models built on heterogeneous geometric structures with standard constitutive models cannot be replicated by randomization of the elastically homogeneous discrete system. The marginal distributions as well as dependencies between stress tensor components cannot be adequately matched.","authors":["Jan Raisinger","Qiwei Zhang","John E. Bolander","Jan Eli\\'a\\v{s}"],"url":"https://arxiv.org/abs/2504.20861"}
{"created":"2025-04-30","title":"Tabular Data Adapters: Improving Outlier Detection for Unlabeled Private Data","abstract":"The remarkable success of Deep Learning approaches is often based and demonstrated on large public datasets. However, when applying such approaches to internal, private datasets, one frequently faces challenges arising from structural differences in the datasets, domain shift, and the lack of labels. In this work, we introduce Tabular Data Adapters (TDA), a novel method for generating soft labels for unlabeled tabular data in outlier detection tasks. By identifying statistically similar public datasets and transforming private data (based on a shared autoencoder) into a format compatible with state-of-the-art public models, our approach enables the generation of weak labels. It thereby can help to mitigate the cold start problem of labeling by basing on existing outlier detection models for public datasets. In experiments on 50 tabular datasets across different domains, we demonstrate that our method is able to provide more accurate annotations than baseline approaches while reducing computational time. Our approach offers a scalable, efficient, and cost-effective solution, to bridge the gap between public research models and real-world industrial applications.","authors":["Dayananda Herurkar","J\\\"orn Hees","Vesselin Tzvetkov","Andreas Dengel"],"url":"https://arxiv.org/abs/2504.20862"}
{"created":"2025-04-30","title":"Bayesian Optimization-based Tire Parameter and Uncertainty Estimation for Real-World Data","abstract":"This work presents a methodology to estimate tire parameters and their uncertainty using a Bayesian optimization approach. The literature mainly considers the estimation of tire parameters but lacks an evaluation of the parameter identification quality and the required slip ratios for an adequate model fit. Therefore, we examine the use of Stochastical Variational Inference as a methodology to estimate both - the parameters and their uncertainties. We evaluate the method compared to a state-of-the-art Nelder-Mead algorithm for theoretical and real-world application. The theoretical study considers parameter fitting at different slip ratios to evaluate the required excitation for an adequate fitting of each parameter. The results are compared to a sensitivity analysis for a Pacejka Magic Formula tire model. We show the application of the algorithm on real-world data acquired during the Abu Dhabi Autonomous Racing League and highlight the uncertainties in identifying the curvature and shape parameters due to insufficient excitation. The gathered insights can help assess the acquired data's limitations and instead utilize standardized parameters until higher slip ratios are captured. We show that our proposed method can be used to assess the mean values and the uncertainties of tire model parameters in real-world conditions and derive actions for the tire modeling based on our simulative study.","authors":["Sven Goblirsch","Benedikt Ruhland","Johannes Betz","Markus Lienkamp"],"url":"https://arxiv.org/abs/2504.20863"}
{"created":"2025-04-30","title":"AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection","abstract":"The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators.","authors":["Lorenzo Pellegrini","Davide Cozzolino","Serafino Pandolfini","Davide Maltoni","Matteo Ferrara","Luisa Verdoliva","Marco Prati","Marco Ramilli"],"url":"https://arxiv.org/abs/2504.20865"}
{"created":"2025-04-30","title":"Predicting the Performance of Scientific Workflow Tasks for Cluster Resource Management: An Overview of the State of the Art","abstract":"Scientific workflow management systems support large-scale data analysis on cluster infrastructures. For this, they interact with resource managers which schedule workflow tasks onto cluster nodes. In addition to workflow task descriptions, resource managers rely on task performance estimates such as main memory consumption and runtime to efficiently manage cluster resources. Such performance estimates should be automated, as user-based task performance estimates are error-prone.","authors":["Jonathan Bader","Kathleen West","Soeren Becker","Svetlana Kulagina","Fabian Lehmann","Lauritz Thamsen","Henning Meyerhenke","Odej Kao"],"url":"https://arxiv.org/abs/2504.20867"}
{"created":"2025-04-30","title":"Quantifying the Noise of Structural Perturbations on Graph Adversarial Attacks","abstract":"Graph neural networks have been widely utilized to solve graph-related tasks because of their strong learning power in utilizing the local information of neighbors. However, recent studies on graph adversarial attacks have proven that current graph neural networks are not robust against malicious attacks. Yet much of the existing work has focused on the optimization objective based on attack performance to obtain (near) optimal perturbations, but paid less attention to the strength quantification of each perturbation such as the injection of a particular node/link, which makes the choice of perturbations a black-box model that lacks interpretability. In this work, we propose the concept of noise to quantify the attack strength of each adversarial link. Furthermore, we propose three attack strategies based on the defined noise and classification margins in terms of single and multiple steps optimization. Extensive experiments conducted on benchmark datasets against three representative graph neural networks demonstrate the effectiveness of the proposed attack strategies. Particularly, we also investigate the preferred patterns of effective adversarial perturbations by analyzing the corresponding properties of the selected perturbation nodes.","authors":["Junyuan Fang","Han Yang","Haixian Wen","Jiajing Wu","Zibin Zheng","Chi K. Tse"],"url":"https://arxiv.org/abs/2504.20869"}
{"created":"2025-04-30","title":"FLIM-based Salient Object Detection Networks with Adaptive Decoders","abstract":"Salient Object Detection (SOD) methods can locate objects that stand out in an image, assign higher values to their pixels in a saliency map, and binarize the map outputting a predicted segmentation mask. A recent tendency is to investigate pre-trained lightweight models rather than deep neural networks in SOD tasks, coping with applications under limited computational resources. In this context, we have investigated lightweight networks using a methodology named Feature Learning from Image Markers (FLIM), which assumes that the encoder's kernels can be estimated from marker pixels on discriminative regions of a few representative images. This work proposes flyweight networks, hundreds of times lighter than lightweight models, for SOD by combining a FLIM encoder with an adaptive decoder, whose weights are estimated for each input image by a given heuristic function. Such FLIM networks are trained from three to four representative images only and without backpropagation, making the models suitable for applications under labeled data constraints as well. We study five adaptive decoders; two of them are introduced here. Differently from the previous ones that rely on one neuron per pixel with shared weights, the heuristic functions of the new adaptive decoders estimate the weights of each neuron per pixel. We compare FLIM models with adaptive decoders for two challenging SOD tasks with three lightweight networks from the state-of-the-art, two FLIM networks with decoders trained by backpropagation, and one FLIM network whose labeled markers define the decoder's weights. The experiments demonstrate the advantages of the proposed networks over the baselines, revealing the importance of further investigating such methods in new applications.","authors":["Gilson Junior Soares","Matheus Abrantes Cerqueira","Jancarlo F. Gomes","Laurent Najman","Silvio Jamil F. Guimar\\~aes","Alexandre Xavier Falc\\~ao"],"url":"https://arxiv.org/abs/2504.20872"}
{"created":"2025-04-30","title":"The Leaderboard Illusion","abstract":"Measuring progress is fundamental to the advancement of any scientific field. As benchmarks play an increasingly central role, they also grow more susceptible to distortion. Chatbot Arena has emerged as the go-to leaderboard for ranking the most capable AI systems. Yet, in this work we identify systematic issues that have resulted in a distorted playing field. We find that undisclosed private testing practices benefit a handful of providers who are able to test multiple variants before public release and retract scores if desired. We establish that the ability of these providers to choose the best score leads to biased Arena scores due to selective disclosure of performance results. At an extreme, we identify 27 private LLM variants tested by Meta in the lead-up to the Llama-4 release. We also establish that proprietary closed models are sampled at higher rates (number of battles) and have fewer models removed from the arena than open-weight and open-source alternatives. Both these policies lead to large data access asymmetries over time. Providers like Google and OpenAI have received an estimated 19.2% and 20.4% of all data on the arena, respectively. In contrast, a combined 83 open-weight models have only received an estimated 29.7% of the total data. We show that access to Chatbot Arena data yields substantial benefits; even limited additional data can result in relative performance gains of up to 112% on the arena distribution, based on our conservative estimates. Together, these dynamics result in overfitting to Arena-specific dynamics rather than general model quality. The Arena builds on the substantial efforts of both the organizers and an open community that maintains this valuable evaluation platform. We offer actionable recommendations to reform the Chatbot Arena's evaluation framework and promote fairer, more transparent benchmarking for the field","authors":["Shivalika Singh","Yiyang Nan","Alex Wang","Daniel D'Souza","Sayash Kapoor","Ahmet \\\"Ust\\\"un","Sanmi Koyejo","Yuntian Deng","Shayne Longpre","Noah Smith","Beyza Ermis","Marzieh Fadaee","Sara Hooker"],"url":"https://arxiv.org/abs/2504.20879"}
{"created":"2025-04-30","title":"Guessing Efficiently for Constrained Subspace Approximation","abstract":"In this paper we study constrained subspace approximation problem. Given a set of $n$ points $\\{a_1,\\ldots,a_n\\}$ in $\\mathbb{R}^d$, the goal of the {\\em subspace approximation} problem is to find a $k$ dimensional subspace that best approximates the input points. More precisely, for a given $p\\geq 1$, we aim to minimize the $p$th power of the $\\ell_p$ norm of the error vector $(\\|a_1-\\bm{P}a_1\\|,\\ldots,\\|a_n-\\bm{P}a_n\\|)$, where $\\bm{P}$ denotes the projection matrix onto the subspace and the norms are Euclidean. In \\emph{constrained} subspace approximation (CSA), we additionally have constraints on the projection matrix $\\bm{P}$. In its most general form, we require $\\bm{P}$ to belong to a given subset $\\mathcal{S}$ that is described explicitly or implicitly.","authors":["Aditya Bhaskara","Sepideh Mahabadi","Madhusudhan Reddy Pittu","Ali Vakilian","David P. Woodruff"],"url":"https://arxiv.org/abs/2504.20883"}
{"created":"2025-04-30","title":"Mapping a Movement: Exploring a Proposed Police Training Facility in Atlanta and the Stop Cop City Movement through Online Maps","abstract":"In 2021, the City of Atlanta and Atlanta Police Foundation launched plans to build a large police training facility in the South River Forest in unincorporated DeKalb County, GA. Residents of Atlanta and DeKalb County, environmental activists, police and prison abolitionists, and other activists and concerned individuals formed the movement in opposition to the facility, known as the Stop Cop City / Defend the Atlanta Forest movement. Social media and digital maps became common tools for communicating information about the facility and the movement. Here, we examine online maps about the facility and the opposition movement, originating from grassroots organizations, the City of Atlanta, news media outlets, the Atlanta Police Foundation, and individuals. We gather and examine 32 publicly available maps collected through the Google Search API, Twitter (now X), Instagram and reddit. Using a framework of critical cartography, we conduct a content analysis of these maps to identify the mapping technologies and techniques (data, cartographic elements, styles) used by different stakeholders and roles that maps and mapping technologies can play in social movements. We examine the extent to which these maps provide data to confirm or contradict concerns raised by grassroots organizations and local residents about the facility. We find that stakeholders and mapmakers use geospatial tools in different ways and likely have varied access to mapping technologies. We argue that documenting the use of maps to communicate information about a contentious project can help enumerate community positions and perspectives, and we advocate for accessible mapmaking tools. We conclude by discussing the implications of accessibility of mapping technology and posting maps to social media, and share example map images that extend the geographic information systems (GIS) techniques seen in the retrieved maps.","authors":["Camille Harris","Clio Andris"],"url":"https://arxiv.org/abs/2504.20886"}
{"created":"2025-04-30","title":"Return Capping: Sample-Efficient CVaR Policy Gradient Optimisation","abstract":"When optimising for conditional value at risk (CVaR) using policy gradients (PG), current meth- ods rely on discarding a large proportion of tra- jectories, resulting in poor sample efficiency. We propose a reformulation of the CVaR optimisation problem by capping the total return of trajecto- ries used in training, rather than simply discard- ing them, and show that this is equivalent to the original problem if the cap is set appropriately. We show, with empirical results in an number of environments, that this reformulation of the prob- lem results in consistently improved performance compared to baselines.","authors":["Harry Mead","Clarissa Costen","Bruno Lacerda","Nick Hawes"],"url":"https://arxiv.org/abs/2504.20887"}
{"created":"2025-04-30","title":"New Capacity Bounds for PIR on Graph and Multigraph-Based Replicated Storage","abstract":"In this paper, we study the problem of private information retrieval (PIR) in both graph-based and multigraph-based replication systems, where each file is stored on exactly two servers, and any pair of servers shares at most $r$ files. We derive upper bounds on the PIR capacity for such systems and construct PIR schemes that approach these bounds. For graph-based systems, we determine the exact PIR capacity for path graphs and improve upon existing results for complete bipartite graphs and complete graphs. For multigraph-based systems, we propose a PIR scheme that leverages the symmetry of the underlying graph-based construction, yielding a capacity lower bound for such multigraphs. Furthermore, we establish several general upper and lower bounds on the PIR capacity of multigraphs, which are tight in certain cases.","authors":["Xiangliang Kong","Shreya Meel","Thomas Jacob Maranzatto","Itzhak Tamo","Sennur Ulukus"],"url":"https://arxiv.org/abs/2504.20888"}
{"created":"2025-04-30","title":"Does Feedback Help in Bandits with Arm Erasures?","abstract":"We study a distributed multi-armed bandit (MAB) problem over arm erasure channels, motivated by the increasing adoption of MAB algorithms over communication-constrained networks. In this setup, the learner communicates the chosen arm to play to an agent over an erasure channel with probability $\\epsilon \\in [0,1)$; if an erasure occurs, the agent continues pulling the last successfully received arm; the learner always observes the reward of the arm pulled. In past work, we considered the case where the agent cannot convey feedback to the learner, and thus the learner does not know whether the arm played is the requested or the last successfully received one. In this paper, we instead consider the case where the agent can send feedback to the learner on whether the arm request was received, and thus the learner exactly knows which arm was played. Surprisingly, we prove that erasure feedback does not improve the worst-case regret upper bound order over the previously studied no-feedback setting. In particular, we prove a regret lower bound of $\\Omega(\\sqrt{KT} + K / (1 - \\epsilon))$, where $K$ is the number of arms and $T$ the time horizon, that matches no-feedback upper bounds up to logarithmic factors. We note however that the availability of feedback enables simpler algorithm designs that may achieve better constants (albeit not better order) regret bounds; we design one such algorithm and evaluate its performance numerically.","authors":["Merve Karakas","Osama Hanna","Lin F. Yang","Christina Fragouli"],"url":"https://arxiv.org/abs/2504.20894"}
{"created":"2025-04-30","title":"LELANTE: LEveraging LLM for Automated ANdroid TEsting","abstract":"Given natural language test case description for an Android application, existing testing approaches require developers to manually write scripts using tools such as Appium and Espresso to execute the corresponding test case. This process is labor-intensive and demands significant effort to maintain as UI interfaces evolve throughout development. In this work, we introduce LELANTE, a novel framework that utilizes large language models (LLMs) to automate test case execution without requiring pre-written scripts. LELANTE interprets natural language test case descriptions, iteratively generate action plans, and perform the actions directly on the Android screen using its GUI. LELANTE employs a screen refinement process to enhance LLM interpretability, constructs a structured prompt for LLMs, and implements an action generation mechanism based on chain-of-thought reasoning of LLMs. To further reduce computational cost and enhance scalability, LELANTE utilizes model distillation using a foundational LLM. In experiments across 390 test cases spanning 10 popular Android applications, LELANTE achieved a 73% test execution success rate. Our results demonstrate that LLMs can effectively bridge the gap between natural language test case description and automated execution, making mobile testing more scalable and adaptable.","authors":["Shamit Fatin","Mehbubul Hasan Al-Quvi","Haz Sameen Shahgir","Sukarna Barua","Anindya Iqbal","Sadia Sharmin","Md. Mostofa Akbar","Kallol Kumar Pal","A. Asif Al Rashid"],"url":"https://arxiv.org/abs/2504.20896"}
{"created":"2025-04-30","title":"CBM-RAG: Demonstrating Enhanced Interpretability in Radiology Report Generation with Multi-Agent RAG and Concept Bottleneck Models","abstract":"Advancements in generative Artificial Intelligence (AI) hold great promise for automating radiology workflows, yet challenges in interpretability and reliability hinder clinical adoption. This paper presents an automated radiology report generation framework that combines Concept Bottleneck Models (CBMs) with a Multi-Agent Retrieval-Augmented Generation (RAG) system to bridge AI performance with clinical explainability. CBMs map chest X-ray features to human-understandable clinical concepts, enabling transparent disease classification. Meanwhile, the RAG system integrates multi-agent collaboration and external knowledge to produce contextually rich, evidence-based reports. Our demonstration showcases the system's ability to deliver interpretable predictions, mitigate hallucinations, and generate high-quality, tailored reports with an interactive interface addressing accuracy, trust, and usability challenges. This framework provides a pathway to improving diagnostic consistency and empowering radiologists with actionable insights.","authors":["Hasan Md Tusfiqur Alam","Devansh Srivastav","Abdulrahman Mohamed Selim","Md Abdul Kadir","Md Moktadiurl Hoque Shuvo","Daniel Sonntag"],"url":"https://arxiv.org/abs/2504.20898"}
{"created":"2025-04-30","title":"Evaluating Generative Models for Tabular Data: Novel Metrics and Benchmarking","abstract":"Generative models have revolutionized multiple domains, yet their application to tabular data remains underexplored. Evaluating generative models for tabular data presents unique challenges due to structural complexity, large-scale variability, and mixed data types, making it difficult to intuitively capture intricate patterns. Existing evaluation metrics offer only partial insights, lacking a comprehensive measure of generative performance. To address this limitation, we propose three novel evaluation metrics: FAED, FPCAD, and RFIS. Our extensive experimental analysis, conducted on three standard network intrusion detection datasets, compares these metrics with established evaluation methods such as Fidelity, Utility, TSTR, and TRTS. Our results demonstrate that FAED effectively captures generative modeling issues overlooked by existing metrics. While FPCAD exhibits promising performance, further refinements are necessary to enhance its reliability. Our proposed framework provides a robust and practical approach for assessing generative models in tabular data applications.","authors":["Dayananda Herurkar","Ahmad Ali","Andreas Dengel"],"url":"https://arxiv.org/abs/2504.20900"}
{"created":"2025-04-30","title":"Classifier-to-Bias: Toward Unsupervised Automatic Bias Detection for Visual Classifiers","abstract":"A person downloading a pre-trained model from the web should be aware of its biases. Existing approaches for bias identification rely on datasets containing labels for the task of interest, something that a non-expert may not have access to, or may not have the necessary resources to collect: this greatly limits the number of tasks where model biases can be identified. In this work, we present Classifier-to-Bias (C2B), the first bias discovery framework that works without access to any labeled data: it only relies on a textual description of the classification task to identify biases in the target classification model. This description is fed to a large language model to generate bias proposals and corresponding captions depicting biases together with task-specific target labels. A retrieval model collects images for those captions, which are then used to assess the accuracy of the model w.r.t. the given biases. C2B is training-free, does not require any annotations, has no constraints on the list of biases, and can be applied to any pre-trained model on any classification task. Experiments on two publicly available datasets show that C2B discovers biases beyond those of the original datasets and outperforms a recent state-of-the-art bias detection baseline that relies on task-specific annotations, being a promising first step toward addressing task-agnostic unsupervised bias detection.","authors":["Quentin Guimard","Moreno D'Inc\\`a","Massimiliano Mancini","Elisa Ricci"],"url":"https://arxiv.org/abs/2504.20902"}
{"created":"2025-04-30","title":"Modeling AI-Human Collaboration as a Multi-Agent Adaptation","abstract":"We develop an agent-based simulation to formalize AI-human collaboration as a function of task structure, advancing a generalizable framework for strategic decision-making in organizations. Distinguishing between heuristic-based human adaptation and rule-based AI search, we model interactions across modular (parallel) and sequenced (interdependent) tasks using an NK model. Our results reveal that in modular tasks, AI often substitutes for humans - delivering higher payoffs unless human expertise is very high, and the AI search space is either narrowly focused or extremely broad. In sequenced tasks, interesting complementarities emerge. When an expert human initiates the search and AI subsequently refines it, aggregate performance is maximized. Conversely, when AI leads, excessive heuristic refinement by the human can reduce payoffs. We also show that even \"hallucinatory\" AI - lacking memory or structure - can improve outcomes when augmenting low-capability humans by helping escape local optima. These results yield a robust implication: the effectiveness of AI-human collaboration depends less on context or industry, and more on the underlying task structure. By elevating task decomposition as the central unit of analysis, our model provides a transferable lens for strategic decision-making involving humans and an agentic AI across diverse organizational settings.","authors":["Prothit Sen","Sai Mihir Jakkaraju"],"url":"https://arxiv.org/abs/2504.20903"}
{"created":"2025-04-30","title":"Dual Explanations via Subgraph Matching for Malware Detection","abstract":"Interpretable malware detection is crucial for understanding harmful behaviors and building trust in automated security systems. Traditional explainable methods for Graph Neural Networks (GNNs) often highlight important regions within a graph but fail to associate them with known benign or malicious behavioral patterns. This limitation reduces their utility in security contexts, where alignment with verified prototypes is essential. In this work, we introduce a novel dual prototype-driven explainable framework that interprets GNN-based malware detection decisions. This dual explainable framework integrates a base explainer (a state-of-the-art explainer) with a novel second-level explainer which is designed by subgraph matching technique, called SubMatch explainer. The proposed explainer assigns interpretable scores to nodes based on their association with matched subgraphs, offering a fine-grained distinction between benign and malicious regions. This prototype-guided scoring mechanism enables more interpretable, behavior-aligned explanations. Experimental results demonstrate that our method preserves high detection performance while significantly improving interpretability in malware analysis.","authors":["Hossein Shokouhinejad","Roozbeh Razavi-Far","Griffin Higgins","Ali A. Ghorbani"],"url":"https://arxiv.org/abs/2504.20904"}
{"created":"2025-04-30","title":"GiBy: A Giant-Step Baby-Step Classifier For Anomaly Detection In Industrial Control Systems","abstract":"The continuous monitoring of the interactions between cyber-physical components of any industrial control system (ICS) is required to secure automation of the system controls, and to guarantee plant processes are fail-safe and remain in an acceptably safe state. Safety is achieved by managing actuation (where electric signals are used to trigger physical movement), dependent on corresponding sensor readings; used as ground truth in decision making. Timely detection of anomalies (attacks, faults and unascertained states) in ICSs is crucial for the safe running of a plant, the safety of its personnel, and for the safe provision of any services provided. We propose an anomaly detection method that involves accurate linearization of the non-linear forms arising from sensor-actuator(s) relationships, primarily because solving linear models is easier and well understood. Further, the time complexity of the anomaly detection scenario/problem at hand is lowered using dimensionality reduction of the actuator(s) in relationship with a sensor. We accomplish this by using a well-known water treatment testbed as a use case. Our experiments show millisecond time response to detect anomalies and provide explainability; that are not simultaneously achieved by other state of the art AI/ML models with eXplainable AI (XAI) used for the same purpose. Further, we pin-point the sensor(s) and its actuation state for which anomaly was detected.","authors":["Sarad Venugopalan","Sridhar Adepu"],"url":"https://arxiv.org/abs/2504.20906"}
{"created":"2025-04-30","title":"MANILA: A Low-Code Application to Benchmark Machine Learning Models and Fairness-Enhancing Methods","abstract":"This paper presents MANILA, a web-based low-code application to benchmark machine learning models and fairness-enhancing methods and select the one achieving the best fairness and effectiveness trade-off. It is grounded on an Extended Feature Model that models a general fairness benchmarking workflow as a Software Product Line. The constraints defined among the features guide users in creating experiments that do not lead to execution errors. We describe the architecture and implementation of MANILA and evaluate it in terms of expressiveness and correctness.","authors":["Giordano d'Aloisio"],"url":"https://arxiv.org/abs/2504.20907"}
{"created":"2025-04-30","title":"MOSIC: Model-Agnostic Optimal Subgroup Identification with Multi-Constraint for Improved Reliability","abstract":"Identifying subgroups that benefit from specific treatments using observational data is a critical challenge in personalized medicine. Most existing approaches solely focus on identifying a subgroup with an improved treatment effect. However, practical considerations, such as ensuring a minimum subgroup size for representativeness or achieving sufficient confounder balance for reliability, are also important for making findings clinically meaningful and actionable. While some studies address these constraints individually, none offer a unified approach to handle them simultaneously. To bridge this gap, we propose a model-agnostic framework for optimal subgroup identification under multiple constraints. We reformulate this combinatorial problem as an unconstrained min-max optimization problem with novel modifications and solve it by a gradient descent ascent algorithm. We further prove its convergence to a feasible and locally optimal solution. Our method is stable and highly flexible, supporting various models and techniques for estimating and optimizing treatment effectiveness with observational data. Extensive experiments on both synthetic and real-world datasets demonstrate its effectiveness in identifying subgroups that satisfy multiple constraints, achieving higher treatment effects and better confounder balancing results across different group sizes.","authors":["Wenxin Chen","Weishen Pan","Kyra Gan","Fei Wang"],"url":"https://arxiv.org/abs/2504.20908"}
{"created":"2025-04-30","title":"When Testing AI Tests Us: Safeguarding Mental Health on the Digital Frontlines","abstract":"Red-teaming is a core part of the infrastructure that ensures that AI models do not produce harmful content. Unlike past technologies, the black box nature of generative AI systems necessitates a uniquely interactional mode of testing, one in which individuals on red teams actively interact with the system, leveraging natural language to simulate malicious actors and solicit harmful outputs. This interactional labor done by red teams can result in mental health harms that are uniquely tied to the adversarial engagement strategies necessary to effectively red team. The importance of ensuring that generative AI models do not propagate societal or individual harm is widely recognized -- one less visible foundation of end-to-end AI safety is also the protection of the mental health and wellbeing of those who work to keep model outputs safe. In this paper, we argue that the unmet mental health needs of AI red-teamers is a critical workplace safety concern. Through analyzing the unique mental health impacts associated with the labor done by red teams, we propose potential individual and organizational strategies that could be used to meet these needs, and safeguard the mental health of red-teamers. We develop our proposed strategies through drawing parallels between common red-teaming practices and interactional labor common to other professions (including actors, mental health professionals, conflict photographers, and content moderators), describing how individuals and organizations within these professional spaces safeguard their mental health given similar psychological demands. Drawing on these protective practices, we describe how safeguards could be adapted for the distinct mental health challenges experienced by red teaming organizations as they mitigate emerging technological risks on the new digital frontlines.","authors":["Sachin R. Pendse","Darren Gergle","Rachel Kornfield","Jonah Meyerhoff","David Mohr","Jina Suh","Annie Wescott","Casey Williams","Jessica Schleider"],"url":"https://arxiv.org/abs/2504.20910"}
{"created":"2025-04-30","title":"An Empirical Study on the Capability of LLMs in Decomposing Bug Reports","abstract":"Background: Bug reports are essential to the software development life cycle. They help developers track and resolve issues, but are often difficult to process due to their complexity, which can delay resolution and affect software quality. Aims: This study investigates whether large language models (LLMs) can assist developers in automatically decomposing complex bug reports into smaller, self-contained units, making them easier to understand and address. Method: We conducted an empirical study on 127 resolved privacy-related bug reports collected from Apache Jira. We evaluated ChatGPT and DeepSeek using different prompting strategies. We first tested both LLMs with zero-shot prompts, then applied improved prompts with demonstrations (using few-shot prompting) to measure their abilities in bug decomposition. Results: Our findings show that LLMs are capable of decomposing bug reports, but their overall performance still requires further improvement and strongly depends on the quality of the prompts. With zero-shot prompts, both studied LLMs (ChatGPT and DeepSeek) performed poorly. After prompt tuning, ChatGPT's true decomposition rate increased by 140\\% and DeepSeek's by 163.64\\%. Conclusions: LLMs show potential in helping developers analyze and decompose complex bug reports, but they still need improvement in terms of accuracy and bug understanding.","authors":["Zhiyuan Chen","Vanessa Nava-Camal","Ahmad Suleiman","Yiming Tang","Daqing Hou","Weiyi Shang"],"url":"https://arxiv.org/abs/2504.20911"}
{"created":"2025-04-30","title":"On the Secrecy-Sensing Optimization of RIS-assisted Full-Duplex Integrated Sensing and Communication Network","abstract":"Integrated sensing and communication (ISAC) has recently emerged as a viable technique for establishing sensing and communication using the same resources. Nonetheless, the operation of ISAC networks is often challenged by the absence of a direct link between the sensing node and the targets, and by the risk of disclosing confidential data to malicious targets when using the same signal for both tasks. In this paper, a robust reconfigurable intelligent surface (RIS)-aided scheme for securing a full-duplex (FD) ISAC network is proposed. The considered network consists of uplink and downlink users served in FD through a multi-antenna dual-functional radar communication base station (BS), which employs co-located multi-antenna communication-radar arrays to detect multiple malicious targets while preserving communication secrecy in their presence. Additionally, the BS utilizes an optimized artificial noise (AN) that serves to disrupt the malicious targets' reception and increase the sensing power. By optimally designing the RIS phase shifts, transmit beamforming, AN covariance, and uplink users' transmit power and combining vectors using an alternating optimization-based algorithm, the network's sensing performance is maximized under secrecy and total power constraints. Numerical results present the proposed scheme's efficacy, particularly when a direct link between the BS and the various nodes/targets is absent.","authors":["Elmehdi Illi","Ahmad Bazzi","Marwa Qaraqe","Ali Ghrayeb"],"url":"https://arxiv.org/abs/2504.20912"}
{"created":"2025-04-30","title":"Statistical and Predictive Analysis to Identify Risk Factors and Effects of Post COVID-19 Syndrome","abstract":"Based on recent studies, some COVID-19 symptoms can persist for months after infection, leading to what is termed long COVID. Factors such as vaccination timing, patient characteristics, and symptoms during the acute phase of infection may contribute to the prolonged effects and intensity of long COVID. Each patient, based on their unique combination of factors, develops a specific risk or intensity of long COVID. In this work, we aim to achieve two objectives: (1) conduct a statistical analysis to identify relationships between various factors and long COVID, and (2) perform predictive analysis of long COVID intensity using these factors. We benchmark and interpret various data-driven approaches, including linear models, random forests, gradient boosting, and neural networks, using data from the Lifelines COVID-19 cohort. Our results show that Neural Networks (NN) achieve the best performance in terms of MAPE, with predictions averaging 19\\% error. Additionally, interpretability analysis reveals key factors such as loss of smell, headache, muscle pain, and vaccination timing as significant predictors, while chronic disease and gender are critical risk factors. These insights provide valuable guidance for understanding long COVID and developing targeted interventions.","authors":["Milad Leyli-abadi","Jean-Patrick Brunet","Axel Tahmasebimoradi"],"url":"https://arxiv.org/abs/2504.20915"}
{"created":"2025-04-30","title":"Leveraging Generative AI Through Prompt Engineering and Rigorous Validation to Create Comprehensive Synthetic Datasets for AI Training in Healthcare","abstract":"Access to high-quality medical data is often restricted due to privacy concerns, posing significant challenges for training artificial intelligence (AI) algorithms within Electronic Health Record (EHR) applications. In this study, prompt engineering with the GPT-4 API was employed to generate high-quality synthetic datasets aimed at overcoming this limitation. The generated data encompassed a comprehensive array of patient admission information, including healthcare provider details, hospital departments, wards, bed assignments, patient demographics, emergency contacts, vital signs, immunizations, allergies, medical histories, appointments, hospital visits, laboratory tests, diagnoses, treatment plans, medications, clinical notes, visit logs, discharge summaries, and referrals. To ensure data quality and integrity, advanced validation techniques were implemented utilizing models such as BERT's Next Sentence Prediction for sentence coherence, GPT-2 for overall plausibility, RoBERTa for logical consistency, autoencoders for anomaly detection, and conducted diversity analysis. Synthetic data that met all validation criteria were integrated into a comprehensive PostgreSQL database, serving as the data management system for the EHR application. This approach demonstrates that leveraging generative AI models with rigorous validation can effectively produce high-quality synthetic medical data, facilitating the training of AI algorithms while addressing privacy concerns associated with real patient data.","authors":["Polycarp Nalela"],"url":"https://arxiv.org/abs/2504.20921"}
{"created":"2025-04-30","title":"DYNAMAX: Dynamic computing for Transformers and Mamba based architectures","abstract":"Early exits (EEs) offer a promising approach to reducing computational costs and latency by dynamically terminating inference once a satisfactory prediction confidence on a data sample is achieved. Although many works integrate EEs into encoder-only Transformers, their application to decoder-only architectures and, more importantly, Mamba models, a novel family of state-space architectures in the LLM realm, remains insufficiently explored. This work introduces DYNAMAX, the first framework to exploit the unique properties of Mamba architectures for early exit mechanisms. We not only integrate EEs into Mamba but also repurpose Mamba as an efficient EE classifier for both Mamba-based and transformer-based LLMs, showcasing its versatility. Our experiments employ the Mistral 7B transformer compared to the Codestral 7B Mamba model, using data sets such as TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and consistency. The results highlight the adaptability of Mamba as a powerful EE classifier and its efficiency in balancing computational cost and performance quality across NLP tasks. By leveraging Mamba's inherent design for dynamic processing, we open pathways for scalable and efficient inference in embedded applications and resource-constrained environments. This study underscores the transformative potential of Mamba in redefining dynamic computing paradigms for LLMs.","authors":["Miguel Nogales","Matteo Gambella","Manuel Roveri"],"url":"https://arxiv.org/abs/2504.20922"}
{"created":"2025-04-30","title":"End-to-end Audio Deepfake Detection from RAW Waveforms: a RawNet-Based Approach with Cross-Dataset Evaluation","abstract":"Audio deepfakes represent a growing threat to digital security and trust, leveraging advanced generative models to produce synthetic speech that closely mimics real human voices. Detecting such manipulations is especially challenging under open-world conditions, where spoofing methods encountered during testing may differ from those seen during training. In this work, we propose an end-to-end deep learning framework for audio deepfake detection that operates directly on raw waveforms. Our model, RawNetLite, is a lightweight convolutional-recurrent architecture designed to capture both spectral and temporal features without handcrafted preprocessing. To enhance robustness, we introduce a training strategy that combines data from multiple domains and adopts Focal Loss to emphasize difficult or ambiguous samples. We further demonstrate that incorporating codec-based manipulations and applying waveform-level audio augmentations (e.g., pitch shifting, noise, and time stretching) leads to significant generalization improvements under realistic acoustic conditions. The proposed model achieves over 99.7% F1 and 0.25% EER on in-domain data (FakeOrReal), and up to 83.4% F1 with 16.4% EER on a challenging out-of-distribution test set (AVSpoof2021 + CodecFake). These findings highlight the importance of diverse training data, tailored objective functions and audio augmentations in building resilient and generalizable audio forgery detectors. Code and pretrained models are available at https://iplab.dmi.unict.it/mfs/Deepfakes/PaperRawNet2025/.","authors":["Andrea Di Pierno (IMT School of Advanced Studies","Lucca","Italy","Department of Mathematics and Computer Science","University of Catania","Italy)","Luca Guarnera (Department of Mathematics and Computer Science","University of Catania","Italy)","Dario Allegra (Department of Mathematics and Computer Science","University of Catania","Italy)","Sebastiano Battiato (Department of Mathematics and Computer Science","University of Catania","Italy)"],"url":"https://arxiv.org/abs/2504.20923"}
{"created":"2025-04-30","title":"A Domain-Agnostic Scalable AI Safety Ensuring Framework","abstract":"Ensuring the safety of AI systems has recently emerged as a critical priority for real-world deployment, particularly in physical AI applications. Current approaches to AI safety typically address predefined domain-specific safety conditions, limiting their ability to generalize across contexts.","authors":["Beomjun Kim","Kangyeon Kim","Sunwoo Kim","Heejin Ahn"],"url":"https://arxiv.org/abs/2504.20924"}
{"created":"2025-04-30","title":"Bipartite Randomized Response Mechanism for Local Differential Privacy","abstract":"With the increasing importance of data privacy, Local Differential Privacy (LDP) has recently become a strong measure of privacy for protecting each user's privacy from data analysts without relying on a trusted third party. In many cases, both data providers and data analysts hope to maximize the utility of released data. In this paper, we study the fundamental trade-off formulated as a constrained optimization problem: maximizing data utility subject to the constraint of LDP budgets. In particular, the Generalized Randomized Response (GRR) treats all discrete data equally except for the true data. For this, we introduce an adaptive LDP mechanism called Bipartite Randomized Response (BRR), which solves the above privacy-utility maximization problem from the global standpoint. We prove that for any utility function and any privacy level, solving the maximization problem is equivalent to confirming how many high-utility data to be treated equally as the true data on release probability, the outcome of which gives the optimal randomized response. Further, solving this linear program can be computationally cheap in theory. Several examples of utility functions defined by distance metrics and applications in decision trees and deep learning are presented. The results of various experiments show that our BRR significantly outperforms the state-of-the-art LDP mechanisms of both continuous and distributed types.","authors":["Shun Zhang","Hai Zhu","Zhili Chen","Neal N. Xiong"],"url":"https://arxiv.org/abs/2504.20926"}
{"created":"2025-04-30","title":"Exploiting inter-agent coupling information for efficient reinforcement learning of cooperative LQR","abstract":"Developing scalable and efficient reinforcement learning algorithms for cooperative multi-agent control has received significant attention over the past years. Existing literature has proposed inexact decompositions of local Q-functions based on empirical information structures between the agents. In this paper, we exploit inter-agent coupling information and propose a systematic approach to exactly decompose the local Q-function of each agent. We develop an approximate least square policy iteration algorithm based on the proposed decomposition and identify two architectures to learn the local Q-function for each agent. We establish that the worst-case sample complexity of the decomposition is equal to the centralized case and derive necessary and sufficient graphical conditions on the inter-agent couplings to achieve better sample efficiency. We demonstrate the improved sample efficiency and computational efficiency on numerical examples.","authors":["Shahbaz P Qadri Syed","He Bai"],"url":"https://arxiv.org/abs/2504.20927"}
{"created":"2025-04-30","title":"ChestX-Reasoner: Advancing Radiology Foundation Models with Reasoning through Step-by-Step Verification","abstract":"Recent advances in reasoning-enhanced large language models (LLMs) and multimodal LLMs (MLLMs) have significantly improved performance in complex tasks, yet medical AI models often overlook the structured reasoning processes inherent in clinical practice. In this work, we present ChestX-Reasoner, a radiology diagnosis MLLM designed to leverage process supervision mined directly from clinical reports, reflecting the step-by-step reasoning followed by radiologists. We construct a large dataset by extracting and refining reasoning chains from routine radiology reports. Our two-stage training framework combines supervised fine-tuning and reinforcement learning guided by process rewards to better align model reasoning with clinical standards. We introduce RadRBench-CXR, a comprehensive benchmark featuring 59K visual question answering samples with 301K clinically validated reasoning steps, and propose RadRScore, a metric evaluating reasoning factuality, completeness, and effectiveness. ChestX-Reasoner outperforms existing medical and general-domain MLLMs in both diagnostic accuracy and reasoning ability, achieving 16%, 5.9%, and 18% improvements in reasoning ability compared to the best medical MLLM, the best general MLLM, and its base model, respectively, as well as 3.3%, 24%, and 27% improvements in outcome accuracy. All resources are open-sourced to facilitate further research in medical reasoning MLLMs.","authors":["Ziqing Fan","Cheng Liang","Chaoyi Wu","Ya Zhang","Yanfeng Wang","Weidi Xie"],"url":"https://arxiv.org/abs/2504.20930"}
{"created":"2025-04-30","title":"Improvements of Dark Experience Replay and Reservoir Sampling towards Better Balance between Consolidation and Plasticity","abstract":"Continual learning is the one of the most essential abilities for autonomous agents, which can incrementally learn daily-life skills. For this ultimate goal, a simple but powerful method, dark experience replay (DER), has been proposed recently. DER mitigates catastrophic forgetting, in which the skills acquired in the past are unintentionally forgotten, by stochastically storing the streaming data in a reservoir sampling (RS) buffer and by relearning them or retaining the past outputs for them. However, since DER considers multiple objectives, it will not function properly without appropriate weighting of them. In addition, the ability to retain past outputs inhibits learning if the past outputs are incorrect due to distribution shift or other effects. This is due to a tradeoff between memory consolidation and plasticity. The tradeoff is hidden even in the RS buffer, which gradually stops storing new data for new skills in it as data is continuously passed to it. To alleviate the tradeoff and achieve better balance, this paper proposes improvement strategies to each of DER and RS. Specifically, DER is improved with automatic adaptation of weights, block of replaying erroneous data, and correction of past outputs. RS is also improved with generalization of acceptance probability, stratification of plural buffers, and intentional omission of unnecessary data. These improvements are verified through multiple benchmarks including regression, classification, and reinforcement learning problems. As a result, the proposed methods achieve steady improvements in learning performance by balancing the memory consolidation and plasticity.","authors":["Taisuke Kobayashi"],"url":"https://arxiv.org/abs/2504.20932"}
{"created":"2025-04-30","title":"DejaVuzz: Disclosing Transient Execution Bugs with Dynamic Swappable Memory and Differential Information Flow Tracking assisted Processor Fuzzing","abstract":"Transient execution vulnerabilities have emerged as a critical threat to modern processors. Hardware fuzzing testing techniques have recently shown promising results in discovering transient execution bugs in large-scale out-of-order processor designs. However, their poor microarchitectural control- lability and observability prevent them from effectively and efficiently detecting transient execution vulnerabilities.","authors":["Jinyan Xu","Yangye Zhou","Xingzhi Zhang","Yinshuai Li","Qinhan Tan","Yinqian Zhang","Yajin Zhou","Rui Chang","Wenbo Shen"],"url":"https://arxiv.org/abs/2504.20934"}
{"created":"2025-04-30","title":"Note about the complexity of the acyclic orientation with parity constraint problem","abstract":"Let G = (V, E) be a connected graph, and let T in V be a subset of vertices. An orientation of G is called T -odd if any vertex v in V has odd in-degree if and only if it is in T . Finding a T -odd orientation of G can be solved in polynomial time as shown by Chevalier, Jaeger, Payan and Xuong (1983). Since then, T -odd orientations have continued to attract interest, particularly in the context of global constraints on the orientation. For instance, Frank and Kir\\'aly (2002) investigated k-connected T -odd orientations and raised questions about acyclic T -odd orientations. This problem is now recognized as an Egres problem and is known as the \"Acyclic orientation with parity constraints\" problem. Szegedy ( 2005) proposed a randomized polynomial algorithm to address this problem. An easy consequence of his work provides a polynomial time algorithm for planar graphs whenever |T | = |V | - 1. Nevertheless, it remains unknown whether it exists in general. In this paper we contribute to the understanding of the complexity of this problem by studying a more general one. We prove that finding a T -odd acyclic orientation on graphs having some directed edges is NP-complete.","authors":["Sylvain Gravier","Matthieu Petiteau","Isabelle Sivignon"],"url":"https://arxiv.org/abs/2504.20935"}
{"created":"2025-04-30","title":"M\\`imir: A real-time interactive visualization library for CUDA programs","abstract":"Real-time visualization of computational simulations running over graphics processing units (GPU) is a valuable feature in modern science and technological research, as it allows researchers to visually assess the quality and correctness of their computational models during the simulation. Due to the high throughput involved in GPU-based simulations, classical visualization approaches such as ones based on copying to RAM or storage are not feasible anymore, as they imply large memory transfers between GPU and CPU at each moment, reducing both computational performance and interactivity. Implementing real-time visualizers for GPU simulation codes is a challenging task as it involves dealing with i) low-level integration of graphics APIs (e.g, OpenGL and Vulkan) into the general-purpose GPU code, ii) a careful and efficient handling of memory spaces and iii) finding a balance between rendering and computing as both need the GPU resources. In this work we present M\\`imir, a CUDA/Vulkan interoperability C++ library that allows users to add real-time 2D/3D visualization to CUDA codes with low programming effort. With M\\`imir, researchers can leverage state-of-the-art CUDA/Vulkan interoperability features without needing to invest time in learning the complex low-level technical aspects involved. Internally, M\\`imir streamlines the interoperability mapping between CUDA device memory containing simulation data and Vulkan graphics resources, so that changes on the data are instantly reflected in the visualization. This abstraction scheme allows generating visualizations with minimal alteration over the original source code, needing only to replace the GPU memory allocation lines of the data to be visualized by the API calls provided by M\\`imir among other optional changes.","authors":["Francisco Carter","Nancy Hitschfeld","Crist\\'obal A. Navarro"],"url":"https://arxiv.org/abs/2504.20937"}
{"created":"2025-04-30","title":"Towards Understanding the Nature of Attention with Low-Rank Sparse Decomposition","abstract":"We propose Low-Rank Sparse Attention (Lorsa), a sparse replacement model of Transformer attention layers to disentangle original Multi Head Self Attention (MHSA) into individually comprehensible components. Lorsa is designed to address the challenge of attention superposition to understand attention-mediated interaction between features in different token positions. We show that Lorsa heads find cleaner and finer-grained versions of previously discovered MHSA behaviors like induction heads, successor heads and attention sink behavior (i.e., heavily attending to the first token). Lorsa and Sparse Autoencoder (SAE) are both sparse dictionary learning methods applied to different Transformer components, and lead to consistent findings in many ways. For instance, we discover a comprehensive family of arithmetic-specific Lorsa heads, each corresponding to an atomic operation in Llama-3.1-8B. Automated interpretability analysis indicates that Lorsa achieves parity with SAE in interpretability while Lorsa exhibits superior circuit discovery properties, especially for features computed collectively by multiple MHSA heads. We also conduct extensive experiments on architectural design ablation, Lorsa scaling law and error analysis.","authors":["Zhengfu He","Junxuan Wang","Rui Lin","Xuyang Ge","Wentao Shu","Qiong Tang","Junping Zhang","Xipeng Qiu"],"url":"https://arxiv.org/abs/2504.20938"}
{"created":"2025-04-30","title":"Flexible Semantic-Aware Resource Allocation: Serving More Users Through Similarity Range Constraints","abstract":"Semantic communication (SemCom) aims to enhance the resource efficiency of next-generation networks by transmitting the underlying meaning of messages, focusing on information relevant to the end user. Existing literature on SemCom primarily emphasizes learning the encoder and decoder through end-to-end deep learning frameworks, with the objective of minimizing a task-specific semantic loss function. Beyond its influence on the physical and application layer design, semantic variability across users in multi-user systems enables the design of resource allocation schemes that incorporate user-specific semantic requirements. To this end, \\emph{a semantic-aware resource allocation} scheme is proposed with the objective of maximizing transmission and semantic reliability, ultimately increasing the number of users whose semantic requirements are met. The resulting resource allocation problem is a non-convex mixed-integer nonlinear program (MINLP), which is known to be NP-hard. To make the problem tractable, it is decomposed into a set of sub-problems, each of which is efficiently solved via geometric programming techniques. Finally, simulations demonstrate that the proposed method improves user satisfaction by up to $17.1\\%$ compared to state of the art methods based on quality of experience-aware SemCom methods.","authors":["Nasrin Gholami","Neda Moghim","Behrouz Shahgholi Ghahfarokhi","Pouyan Salavati","Christo Kurisummoottil Thomas","Sachin Shetty","Tahereh Rahmati"],"url":"https://arxiv.org/abs/2504.20939"}
{"created":"2025-04-30","title":"Conformal-DP: Differential Privacy on Riemannian Manifolds via Conformal Transformation","abstract":"Differential Privacy (DP) has been established as a safeguard for private data sharing by adding perturbations to information release. Prior research on DP has extended beyond data in the flat Euclidean space and addressed data on curved manifolds, e.g., diffusion tensor MRI, social networks, or organ shape analysis, by adding perturbations along geodesic distances. However, existing manifold-aware DP methods rely on the assumption that samples are uniformly distributed across the manifold. In reality, data densities vary, leading to a biased noise imbalance across manifold regions, weakening the privacy-utility trade-offs. To address this gap, we propose a novel mechanism: Conformal-DP, utilizing conformal transformations on the Riemannian manifold to equalize local sample density and to redefine geodesic distances accordingly while preserving the intrinsic geometry of the manifold. Our theoretical analysis yields two main results. First, we prove that the conformal factor computed from local kernel-density estimates is explicitly data-density-aware; Second, under the conformal metric, the mechanism satisfies $ \\varepsilon $-differential privacy on any complete Riemannian manifold and admits a closed-form upper bound on the expected geodesic error that depends only on the maximal density ratio, not on global curvatureof the manifold. Our experimental results validate that the mechanism achieves high utility while providing the $ \\varepsilon $-DP guarantee for both homogeneous and especially heterogeneous manifold data.","authors":["Peilin He","Liou Tang","M. Amin Rahimian","James Joshi"],"url":"https://arxiv.org/abs/2504.20941"}
{"created":"2025-04-30","title":"Scenario-based Compositional Verification of Autonomous Systems with Neural Perception","abstract":"Recent advances in deep learning have enabled the development of autonomous systems that use deep neural networks for perception. Formal verification of these systems is challenging due to the size and complexity of the perception DNNs as well as hard-to-quantify, changing environment conditions. To address these challenges, we propose a probabilistic verification framework for autonomous systems based on the following key concepts: (1) Scenario-based Modeling: We decompose the task (e.g., car navigation) into a composition of scenarios, each representing a different environment condition. (2) Probabilistic Abstractions: For each scenario, we build a compact abstraction of perception based on the DNN's performance on an offline dataset that represents the scenario's environment condition. (3) Symbolic Reasoning and Acceleration: The abstractions enable efficient compositional verification of the autonomous system via symbolic reasoning and a novel acceleration proof rule that bounds the error probability of the system under arbitrary variations of environment conditions. We illustrate our approach on two case studies: an experimental autonomous system that guides airplanes on taxiways using high-dimensional perception DNNs and a simulation model of an F1Tenth autonomous car using LiDAR observations.","authors":["Christopher Watson","Rajeev Alur","Divya Gopinath","Ravi Mangal","Corina S. Pasareanu"],"url":"https://arxiv.org/abs/2504.20942"}
{"created":"2025-04-30","title":"Deep Learning Characterizes Depression and Suicidal Ideation from Eye Movements","abstract":"Identifying physiological and behavioral markers for mental health conditions is a longstanding challenge in psychiatry. Depression and suicidal ideation, in particular, lack objective biomarkers, with screening and diagnosis primarily relying on self-reports and clinical interviews. Here, we investigate eye tracking as a potential marker modality for screening purposes. Eye movements are directly modulated by neuronal networks and have been associated with attentional and mood-related patterns; however, their predictive value for depression and suicidality remains unclear. We recorded eye-tracking sequences from 126 young adults as they read and responded to affective sentences, and subsequently developed a deep learning framework to predict their clinical status. The proposed model included separate branches for trials of positive and negative sentiment, and used 2D time-series representations to account for both intra-trial and inter-trial variations. We were able to identify depression and suicidal ideation with an area under the receiver operating curve (AUC) of 0.793 (95% CI: 0.765-0.819) against healthy controls, and suicidality specifically with 0.826 AUC (95% CI: 0.797-0.852). The model also exhibited moderate, yet significant, accuracy in differentiating depressed from suicidal participants, with 0.609 AUC (95% CI 0.571-0.646). Discriminative patterns emerge more strongly when assessing the data relative to response generation than relative to the onset time of the final word of the sentences. The most pronounced effects were observed for negative-sentiment sentences, that are congruent to depressed and suicidal participants. Our findings highlight eye tracking as an objective tool for mental health assessment and underscore the modulatory impact of emotional stimuli on cognitive processes affecting oculomotor control.","authors":["Kleanthis Avramidis","Woojae Jeong","Aditya Kommineni","Sudarsana R. Kadiri","Marcus Ma","Colin McDaniel","Myzelle Hughes","Thomas McGee","Elsi Kaiser","Dani Byrd","Assal Habibi","B. Rael Cahn","Idan A. Blank","Kristina Lerman","Takfarinas Medani","Richard M. Leahy","Shrikanth Narayanan"],"url":"https://arxiv.org/abs/2504.20944"}
{"created":"2025-04-30","title":"Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models","abstract":"As Large Language Models (LLMs) continue to be leveraged for daily tasks, prompt engineering remains an active field of contribution within computational linguistics, particularly in domains requiring specialized knowledge such as arithmetic reasoning. While these LLMs are optimized for a variety of tasks, their exhaustive employment may become computationally or financially cumbersome for small teams. Additionally, complete reliance on proprietary, closed-source models often limits customization and adaptability, posing significant challenges in research and application scalability. Instead, by leveraging open-source models at or below 7 billion parameters, we can optimize our resource usage while still observing remarkable gains over standard prompting approaches. To cultivate this notion, we introduce Trace-of-Thought Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to create observable subproblems using critical problem-solving, specifically designed to enhance arithmetic reasoning capabilities. When applied to open-source models in tandem with GPT-4, we observe that Trace-of-Thought not only allows novel insight into the problem-solving process but also introduces performance gains as large as 125% on language models at or below 7 billion parameters. This approach underscores the potential of open-source initiatives in democratizing AI research and improving the accessibility of high-quality computational linguistics applications.","authors":["Tyler McDonald","Ali Emami"],"url":"https://arxiv.org/abs/2504.20946"}
{"created":"2025-04-30","title":"Opinion-Driven Decision-Making for Multi-Robot Navigation through Narrow Corridors","abstract":"We propose an opinion-driven navigation framework for multi-robot traversal through a narrow corridor. Our approach leverages a multi-agent decision-making model known as the Nonlinear Opinion Dynamics (NOD) to address the narrow corridor passage problem, formulated as a multi-robot navigation game. By integrating the NOD model with a multi-robot path planning algorithm, we demonstrate that the framework effectively reduces the likelihood of deadlocks during corridor traversal. To ensure scalability with an increasing number of robots, we introduce a game reduction technique that enables efficient coordination in larger groups. Extensive simulation studies are conducted to validate the effectiveness of the proposed approach.","authors":["Norah K. Alghamdi","Shinkyu Park"],"url":"https://arxiv.org/abs/2504.20947"}
{"created":"2025-04-30","title":"DS_FusionNet: Dynamic Dual-Stream Fusion with Bidirectional Knowledge Distillation for Plant Disease Recognition","abstract":"Given the severe challenges confronting the global growth security of economic crops, precise identification and prevention of plant diseases has emerged as a critical issue in artificial intelligence-enabled agricultural technology. To address the technical challenges in plant disease recognition, including small-sample learning, leaf occlusion, illumination variations, and high inter-class similarity, this study innovatively proposes a Dynamic Dual-Stream Fusion Network (DS_FusionNet). The network integrates a dual-backbone architecture, deformable dynamic fusion modules, and bidirectional knowledge distillation strategy, significantly enhancing recognition accuracy. Experimental results demonstrate that DS_FusionNet achieves classification accuracies exceeding 90% using only 10% of the PlantDisease and CIFAR-10 datasets, while maintaining 85% accuracy on the complex PlantWild dataset, exhibiting exceptional generalization capabilities. This research not only provides novel technical insights for fine-grained image classification but also establishes a robust foundation for precise identification and management of agricultural diseases.","authors":["Yanghui Song","Chengfu Yang"],"url":"https://arxiv.org/abs/2504.20948"}
{"created":"2025-04-30","title":"Improved Bounds on the Space Complexity of Circuit Evaluation","abstract":"Williams (STOC 2025) recently proved that time-$t$ multitape Turing machines can be simulated using $O(\\sqrt{t \\log t})$ space using the Cook-Mertz (STOC 2024) tree evaluation procedure. As Williams notes, applying this result to fast algorithms for the circuit value problem implies an $O(\\sqrt{s} \\cdot \\mathrm{polylog}\\; s)$ algorithm for evaluating size $s$ circuits.","authors":["Yakov Shalunov"],"url":"https://arxiv.org/abs/2504.20950"}
{"created":"2025-04-30","title":"Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models","abstract":"We propose a theoretical model called \"information gravity\" to describe the text generation process in large language models (LLMs). The model uses physical apparatus from field theory and spacetime geometry to formalize the interaction between user queries and the probability distribution of generated tokens. A query is viewed as an object with \"information mass\" that curves the semantic space of the model, creating gravitational potential wells that \"attract\" tokens during generation. This model offers a mechanism to explain several observed phenomena in LLM behavior, including hallucinations (emerging from low-density semantic voids), sensitivity to query formulation (due to semantic field curvature changes), and the influence of sampling temperature on output diversity.","authors":["Maryna Vyshnyvetska"],"url":"https://arxiv.org/abs/2504.20951"}
{"created":"2025-04-30","title":"The Development of Reflective Practice on a Work-Based Software Engineering Program: A Longitudinal Study","abstract":"This study examines the development of reflective practice among students on a four-year work-based Software Engineering program. Using two established models of reflection - Boud et al.'s Model of Reflective Process and Bain et al.'s 5R Framework for Reflection - we analyse a series of reflective assignments submitted by students over four years. Our longitudinal analysis reveals clear trends in how students' reflective abilities evolve over the course of the program. We find that more sophisticated forms of reflection, such as integration of knowledge, appropriation of skills, and reconstruction of practice, increase markedly in prevalence in later years. The complementary nature of workplace experience and university study is highlighted in students' reflections, demonstrating a key benefit of the work-based learning approach. By the final year, all students demonstrate the ability to reconstruct their experiences to inform future practice. Our findings provide insight into how reflective practice develops in Software Engineering education and suggest potential value in incorporating more structured reflection into traditional degree programs. The study also reveals instances of meta-reflection, where students reflect on the value of reflection itself, indicating a deep engagement with the reflective process. While acknowledging limitations, this work offers a unique longitudinal perspective on the development of reflective practice in work-based Software Engineering education.","authors":["Matthew Barr","Syed Waqar Nabi","Oana Andrei"],"url":"https://arxiv.org/abs/2504.20956"}
{"created":"2025-04-30","title":"Simple Finite-Length Achievability and Converse Bounds for the Deletion Channel and the Insertion Channel","abstract":"We develop upper bounds on code size for independent and identically distributed deletion (insertion) channel for given code length and target frame error probability. The bounds are obtained as a variation of a general converse bound, which, though available for any channel, is inefficient and not easily computable without a good reference distribution over the output alphabet. We obtain a reference output distribution for a general finite-input finite-output channel and provide a simple formula for the converse bound on the capacity employing this distribution. We then evaluate the bound for the deletion channel with a finite block length and show that the resulting upper bound on the code side is tighter than that for a binary erasure channel, which is the only alternative converse bound for this finite-length setting. Also, we provide the similar results for the insertion channel.","authors":["Ruslan Morozov","Tolga Mete Duman"],"url":"https://arxiv.org/abs/2504.20961"}
{"created":"2025-04-30","title":"OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification","abstract":"We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) in generating complete specification code pertaining to operating system kernel verification tasks. The benchmark first defines the specification generation problem into a program synthesis problem within a confined scope of syntax and semantics by providing LLMs with the programming model. The LLMs are required to understand the provided verification assumption and the potential syntax and semantics space to search for, then generate the complete specification for the potentially buggy operating system code implementation under the guidance of the high-level functional description of the operating system. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each is a long context task of about 20k-30k tokens. Our comprehensive evaluation of 12 LLMs exhibits the limited performance of the current LLMs on the specification generation tasks for operating system verification. Significant disparities in their performance on the benchmark highlight differences in their ability to handle long-context code generation tasks. The evaluation toolkit and benchmark are available at https://github.com/lishangyu-hkust/OSVBench.","authors":["Shangyu Li","Juyong Jiang","Tiancheng Zhao","Jiasi Shen"],"url":"https://arxiv.org/abs/2504.20964"}
{"created":"2025-04-30","title":"AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security","abstract":"We introduce AegisLLM, a cooperative multi-agent defense against adversarial attacks and information leakage. In AegisLLM, a structured workflow of autonomous agents - orchestrator, deflector, responder, and evaluator - collaborate to ensure safe and compliant LLM outputs, while self-improving over time through prompt optimization. We show that scaling agentic reasoning system at test-time - both by incorporating additional agent roles and by leveraging automated prompt optimization (such as DSPy)- substantially enhances robustness without compromising model utility. This test-time defense enables real-time adaptability to evolving attacks, without requiring model retraining. Comprehensive evaluations across key threat scenarios, including unlearning and jailbreaking, demonstrate the effectiveness of AegisLLM. On the WMDP unlearning benchmark, AegisLLM achieves near-perfect unlearning with only 20 training examples and fewer than 300 LM calls. For jailbreaking benchmarks, we achieve 51% improvement compared to the base model on StrongReject, with false refusal rates of only 7.9% on PHTest compared to 18-55% for comparable methods. Our results highlight the advantages of adaptive, agentic reasoning over static defenses, establishing AegisLLM as a strong runtime alternative to traditional approaches based on model modifications. Code is available at https://github.com/zikuicai/aegisllm","authors":["Zikui Cai","Shayan Shabihi","Bang An","Zora Che","Brian R. Bartoldson","Bhavya Kailkhura","Tom Goldstein","Furong Huang"],"url":"https://arxiv.org/abs/2504.20965"}
{"created":"2025-04-30","title":"Softpick: No Attention Sink, No Massive Activations with Rectified Softmax","abstract":"We introduce softpick, a rectified, not sum-to-one, drop-in replacement for softmax in transformer attention mechanisms that eliminates attention sink and massive activations. Our experiments with 340M parameter models demonstrate that softpick maintains performance parity with softmax on standard benchmarks while achieving 0% sink rate. The softpick transformer produces hidden states with significantly lower kurtosis (340 vs 33,510) and creates sparse attention maps (46.97% sparsity). Models using softpick consistently outperform softmax when quantized, with particularly pronounced advantages at lower bit precisions. Our analysis and discussion shows how softpick has the potential to open new possibilities for quantization, low-precision training, sparsity optimization, pruning, and interpretability. Our code is available at https://github.com/zaydzuhri/softpick-attention.","authors":["Zayd M. K. Zuhri","Erland Hilman Fuadi","Alham Fikri Aji"],"url":"https://arxiv.org/abs/2504.20966"}
{"created":"2025-04-30","title":"XPG-RL: Reinforcement Learning with Explainable Priority Guidance for Efficiency-Boosted Mechanical Search","abstract":"Mechanical search (MS) in cluttered environments remains a significant challenge for autonomous manipulators, requiring long-horizon planning and robust state estimation under occlusions and partial observability. In this work, we introduce XPG-RL, a reinforcement learning framework that enables agents to efficiently perform MS tasks through explainable, priority-guided decision-making based on raw sensory inputs. XPG-RL integrates a task-driven action prioritization mechanism with a learned context-aware switching strategy that dynamically selects from a discrete set of action primitives such as target grasping, occlusion removal, and viewpoint adjustment. Within this strategy, a policy is optimized to output adaptive threshold values that govern the discrete selection among action primitives. The perception module fuses RGB-D inputs with semantic and geometric features to produce a structured scene representation for downstream decision-making. Extensive experiments in both simulation and real-world settings demonstrate that XPG-RL consistently outperforms baseline methods in task success rates and motion efficiency, achieving up to 4.5$\\times$ higher efficiency in long-horizon tasks. These results underscore the benefits of integrating domain knowledge with learnable decision-making policies for robust and efficient robotic manipulation.","authors":["Yiting Zhang","Shichen Li","Elena Shrestha"],"url":"https://arxiv.org/abs/2504.20969"}
{"created":"2025-04-30","title":"SVD Based Least Squares for X-Ray Pneumonia Classification Using Deep Features","abstract":"Accurate and early diagnosis of pneumonia through X-ray imaging is essential for effective treatment and improved patient outcomes. Recent advancements in machine learning have enabled automated diagnostic tools that assist radiologists in making more reliable and efficient decisions. In this work, we propose a Singular Value Decomposition-based Least Squares (SVD-LS) framework for multi-class pneumonia classification, leveraging powerful feature representations from state-of-the-art self-supervised and transfer learning models. Rather than relying on computationally expensive gradient based fine-tuning, we employ a closed-form, non-iterative classification approach that ensures efficiency without compromising accuracy. Experimental results demonstrate that SVD-LS achieves competitive performance while offering significantly reduced computational costs, making it a viable alternative for real-time medical imaging applications.","authors":["Mete Erdogan","Sebnem Demirtas"],"url":"https://arxiv.org/abs/2504.20970"}
{"created":"2025-04-30","title":"SetKE: Knowledge Editing for Knowledge Elements Overlap","abstract":"Large Language Models (LLMs) excel in tasks such as retrieval and question answering but require updates to incorporate new knowledge and reduce inaccuracies and hallucinations. Traditional updating methods, like fine-tuning and incremental learning, face challenges such as overfitting and high computational costs. Knowledge Editing (KE) provides a promising alternative but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where multiple triplets share common elements, leading to editing conflicts. We identify the prevalence of KEO in existing KE datasets and show its significant impact on current KE methods, causing performance degradation in handling such triplets. To address this, we propose a new formulation, Knowledge Set Editing (KSE), and introduce SetKE, a method that edits sets of triplets simultaneously. Experimental results demonstrate that SetKE outperforms existing methods in KEO scenarios on mainstream LLMs. Additionally, we introduce EditSet, a dataset containing KEO triplets, providing a comprehensive benchmark.","authors":["Yifan Wei","Xiaoyan Yu","Ran Song","Hao Peng","Angsheng Li"],"url":"https://arxiv.org/abs/2504.20972"}
{"created":"2025-04-30","title":"Local energy communities optimization considering cost and greenhouse gases minimization","abstract":"Local Energy Communities (LECs) facilitate consumer involvement in local electricity generation and distribution, offering a significant opportunity for society to participate in the energy transition. This paper presents the optimization of a renewable energy community in Spain, consisting of four office buildings, a collectively owned centralized photovoltaic system, and a Li-ion battery storage system. The case study assesses the performance and feasibility of the proposed solutions. The results indicate a 6% reduction in emissions and a 20% reduction in electricity costs, demonstrating the potential of LECs to enhance energy security by saving costs and emission while mitigating the vulnerability of local areas to power outages and disruptions","authors":["S. Barja-Martinez","M. Aragues-Penalba","A. Sumper","R. Villafafila-Robles"],"url":"https://arxiv.org/abs/2504.20973"}
{"created":"2025-04-30","title":"Equivariant non-linear maps for neural networks on homogeneous spaces","abstract":"This paper presents a novel framework for non-linear equivariant neural network layers on homogeneous spaces. The seminal work of Cohen et al. on equivariant $G$-CNNs on homogeneous spaces characterized the representation theory of such layers in the linear setting, finding that they are given by convolutions with kernels satisfying so-called steerability constraints. Motivated by the empirical success of non-linear layers, such as self-attention or input dependent kernels, we set out to generalize these insights to the non-linear setting. We derive generalized steerability constraints that any such layer needs to satisfy and prove the universality of our construction. The insights gained into the symmetry-constrained functional dependence of equivariant operators on feature maps and group elements informs the design of future equivariant neural network layers. We demonstrate how several common equivariant network architectures - $G$-CNNs, implicit steerable kernel networks, conventional and relative position embedded attention based transformers, and LieTransformers - may be derived from our framework.","authors":["Elias Nyholm","Oscar Carlsson","Maurice Weiler","Daniel Persson"],"url":"https://arxiv.org/abs/2504.20974"}
{"created":"2025-04-30","title":"Real-Time Wayfinding Assistant for Blind and Low-Vision Users","abstract":"Navigating unfamiliar places continues to be one of the most persistent and essential everyday obstacles for those who are blind or have limited vision (BLV). Existing assistive technologies, such as GPS-based navigation systems, AI-powered smart glasses, and sonar-equipped canes, often face limitations in real-time obstacle avoidance, precise localization, and adaptability to dynamic surroundings. To investigate potential solutions, we introduced PathFinder, a novel map-less navigation system that explores different models for understanding 2D images, including Vision Language Models (VLMs), Large Language Models (LLMs), and employs monocular depth estimation for free-path detection. Our approach integrates a Depth-First Search (DFS) algorithm on depth images to determine the longest obstacle-free path, ensuring optimal route selection while maintaining computational efficiency. We conducted comparative evaluations against existing AI-powered navigation methods and performed a usability study with BLV participants. The results demonstrate that PathFinder achieves a favorable balance between accuracy, computational efficiency, and real-time responsiveness. Notably, it reduces mean absolute error (MAE) and improves decision-making speed in outdoor navigation compared to AI-based alternatives. Participant feedback emphasizes the system's usability and effectiveness in outside situations, but also identifies issues in complicated indoor locations and low-light conditions. Usability testing revealed that 73% of participants understood how to use the app in about a minute, and 80% praised its balance of accuracy, quick response, and overall convenience.","authors":["Dabbrata Das","Argho Deb Das","Farhan Sadaf"],"url":"https://arxiv.org/abs/2504.20976"}
{"created":"2025-04-30","title":"Jekyll-and-Hyde Tipping Point in an AI's Behavior","abstract":"Trust in AI is undermined by the fact that there is no science that predicts -- or that can explain to the public -- when an LLM's output (e.g. ChatGPT) is likely to tip mid-response to become wrong, misleading, irrelevant or dangerous. With deaths and trauma already being blamed on LLMs, this uncertainty is even pushing people to treat their 'pet' LLM more politely to 'dissuade' it (or its future Artificial General Intelligence offspring) from suddenly turning on them. Here we address this acute need by deriving from first principles an exact formula for when a Jekyll-and-Hyde tipping point occurs at LLMs' most basic level. Requiring only secondary school mathematics, it shows the cause to be the AI's attention spreading so thin it suddenly snaps. This exact formula provides quantitative predictions for how the tipping-point can be delayed or prevented by changing the prompt and the AI's training. Tailored generalizations will provide policymakers and the public with a firm platform for discussing any of AI's broader uses and risks, e.g. as a personal counselor, medical advisor, decision-maker for when to use force in a conflict situation. It also meets the need for clear and transparent answers to questions like ''should I be polite to my LLM?''","authors":["Neil F. Johnson","Frank Yingjie Huo"],"url":"https://arxiv.org/abs/2504.20980"}
{"created":"2025-04-30","title":"LTLf Adaptive Synthesis for Multi-Tier Goals in Nondeterministic Domains","abstract":"We study a variant of LTLf synthesis that synthesizes adaptive strategies for achieving a multi-tier goal, consisting of multiple increasingly challenging LTLf objectives in nondeterministic planning domains. Adaptive strategies are strategies that at any point of their execution (i) enforce the satisfaction of as many objectives as possible in the multi-tier goal, and (ii) exploit possible cooperation from the environment to satisfy as many as possible of the remaining ones. This happens dynamically: if the environment cooperates (ii) and an objective becomes enforceable (i), then our strategies will enforce it. We provide a game-theoretic technique to compute adaptive strategies that is sound and complete. Notably, our technique is polynomial, in fact quadratic, in the number of objectives. In other words, it handles multi-tier goals with only a minor overhead compared to standard LTLf synthesis.","authors":["Giuseppe De Giacomo","Gianmarco Parretti","Shufang Zhu"],"url":"https://arxiv.org/abs/2504.20983"}
{"created":"2025-04-30","title":"ACE: A Security Architecture for LLM-Integrated App Systems","abstract":"LLM-integrated app systems extend the utility of Large Language Models (LLMs) with third-party apps that are invoked by a system LLM using interleaved planning and execution phases to answer user queries. These systems introduce new attack vectors where malicious apps can cause integrity violation of planning or execution, availability breakdown, or privacy compromise during execution.","authors":["Evan Li","Tushin Mallick","Evan Rose","William Robertson","Alina Oprea","Cristina Nita-Rotaru"],"url":"https://arxiv.org/abs/2504.20984"}
{"created":"2025-04-30","title":"Hubs and Spokes Learning: Efficient and Scalable Collaborative Machine Learning","abstract":"We introduce the Hubs and Spokes Learning (HSL) framework, a novel paradigm for collaborative machine learning that combines the strengths of Federated Learning (FL) and Decentralized Learning (P2PL). HSL employs a two-tier communication structure that avoids the single point of failure inherent in FL and outperforms the state-of-the-art P2PL framework, Epidemic Learning Local (ELL). At equal communication budgets (total edges), HSL achieves higher performance than ELL, while at significantly lower communication budgets, it can match ELL's performance. For instance, with only 400 edges, HSL reaches the same test accuracy that ELL achieves with 1000 edges for 100 peers (spokes) on CIFAR-10, demonstrating its suitability for resource-constrained systems. HSL also achieves stronger consensus among nodes after mixing, resulting in improved performance with fewer training rounds. We substantiate these claims through rigorous theoretical analyses and extensive experimental results, showcasing HSL's practicality for large-scale collaborative learning.","authors":["Atul Sharma","Kavindu Herath","Saurabh Bagchi","Chaoyue Liu","Somali Chaterji"],"url":"https://arxiv.org/abs/2504.20988"}
{"created":"2025-04-30","title":"Quantum Hypothesis Testing Lemma for Deterministic Identification over Quantum Channels","abstract":"In our previous work, we presented the Hypothesis Testing Lemma, a key tool that establishes sufficient conditions for the existence of good deterministic identification (DI) codes for memoryless channels with finite output, but arbitrary input alphabets. In this work, we provide a full quantum analogue of this lemma, which shows that the existence of a DI code in the quantum setting follows from a suitable packing in a modified space of output quantum states. Specifically, we demonstrate that such a code can be constructed using product states derived from this packing. This result enables us to tighten the capacity lower bound for DI over quantum channels beyond the simultaneous decoding approach. In particular, we can now express these bounds solely in terms of the Minkowski dimension of a certain state space, giving us new insights to better understand the nature of the protocol, and the separation between simultaneous and non-simultaneous codes. We extend the discussion with a particular channel example for which we can construct an optimum code.","authors":["Pau Colomer","Christian Deppe","Holger Boche","Andreas Winter"],"url":"https://arxiv.org/abs/2504.20991"}
{"created":"2025-04-30","title":"TesserAct: Learning 4D Embodied World Models","abstract":"This paper presents an effective approach for learning novel 4D embodied world models, which predict the dynamic evolution of 3D scenes over time in response to an embodied agent's actions, providing both spatial and temporal consistency. We propose to learn a 4D world model by training on RGB-DN (RGB, Depth, and Normal) videos. This not only surpasses traditional 2D models by incorporating detailed shape, configuration, and temporal changes into their predictions, but also allows us to effectively learn accurate inverse dynamic models for an embodied agent. Specifically, we first extend existing robotic manipulation video datasets with depth and normal information leveraging off-the-shelf models. Next, we fine-tune a video generation model on this annotated dataset, which jointly predicts RGB-DN (RGB, Depth, and Normal) for each frame. We then present an algorithm to directly convert generated RGB, Depth, and Normal videos into a high-quality 4D scene of the world. Our method ensures temporal and spatial coherence in 4D scene predictions from embodied scenarios, enables novel view synthesis for embodied environments, and facilitates policy learning that significantly outperforms those derived from prior video-based world models.","authors":["Haoyu Zhen","Qiao Sun","Hongxin Zhang","Junyan Li","Siyuan Zhou","Yilun Du","Chuang Gan"],"url":"https://arxiv.org/abs/2504.20995"}
{"created":"2025-04-30","title":"X-Fusion: Introducing New Modality to Frozen Large Language Models","abstract":"We propose X-Fusion, a framework that extends pretrained Large Language Models (LLMs) for multimodal tasks while preserving their language capabilities. X-Fusion employs a dual-tower design with modality-specific weights, keeping the LLM's parameters frozen while integrating vision-specific information for both understanding and generation. Our experiments demonstrate that X-Fusion consistently outperforms alternative architectures on both image-to-text and text-to-image tasks. We find that incorporating understanding-focused data improves generation quality, reducing image data noise enhances overall performance, and feature alignment accelerates convergence for smaller models but has minimal impact on larger ones. Our findings provide valuable insights into building efficient unified multimodal models.","authors":["Sicheng Mo","Thao Nguyen","Xun Huang","Siddharth Srinivasan Iyer","Yijun Li","Yuchen Liu","Abhishek Tandon","Eli Shechtman","Krishna Kumar Singh","Yong Jae Lee","Bolei Zhou","Yuheng Li"],"url":"https://arxiv.org/abs/2504.20996"}
{"created":"2025-04-30","title":"Toward Efficient Exploration by Large Language Model Agents","abstract":"A burgeoning area within reinforcement learning (RL) is the design of sequential decision-making agents centered around large language models (LLMs). While autonomous decision-making agents powered by modern LLMs could facilitate numerous real-world applications, such successes demand agents that are capable of data-efficient RL. One key obstacle to achieving data efficiency in RL is exploration, a challenge that we demonstrate many recent proposals for LLM agent designs struggle to contend with. Meanwhile, classic algorithms from the RL literature known to gracefully address exploration require technical machinery that can be challenging to operationalize in purely natural language settings. In this work, rather than relying on finetuning or in-context learning to coax LLMs into implicitly imitating a RL algorithm, we illustrate how LLMs can be used to explicitly implement an existing RL algorithm (Posterior Sampling for Reinforcement Learning) whose capacity for statistically-efficient exploration is already well-studied. We offer empirical results demonstrating how our LLM-based implementation of a known, data-efficient RL algorithm can be considerably more effective in natural language tasks that demand prudent exploration.","authors":["Dilip Arumugam","Thomas L. Griffiths"],"url":"https://arxiv.org/abs/2504.20997"}
{"created":"2025-04-30","title":"YoChameleon: Personalized Vision and Language Generation","abstract":"Large Multimodal Models (e.g., GPT-4, Gemini, Chameleon) have evolved into powerful tools with millions of users. However, they remain generic models and lack personalized knowledge of specific user concepts. Previous work has explored personalization for text generation, yet it remains unclear how these methods can be adapted to new modalities, such as image generation. In this paper, we introduce Yo'Chameleon, the first attempt to study personalization for large multimodal models. Given 3-5 images of a particular concept, Yo'Chameleon leverages soft-prompt tuning to embed subject-specific information to (i) answer questions about the subject and (ii) recreate pixel-level details to produce images of the subject in new contexts. Yo'Chameleon is trained with (i) a self-prompting optimization mechanism to balance performance across multiple modalities, and (ii) a ``soft-positive\" image generation approach to enhance image quality in a few-shot setting.","authors":["Thao Nguyen","Krishna Kumar Singh","Jing Shi","Trung Bui","Yong Jae Lee","Yuheng Li"],"url":"https://arxiv.org/abs/2504.20998"}
{"created":"2025-04-30","title":"Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities","abstract":"Large Language Models (LLMs) are limited by their parametric knowledge, leading to hallucinations in knowledge-extensive tasks. To address this, Retrieval-Augmented Generation (RAG) incorporates external document chunks to expand LLM knowledge. Furthermore, compressing information from document chunks through extraction or summarization can improve LLM performance. Nonetheless, LLMs still struggle to notice and utilize scattered key information, a problem known as the \"lost-in-the-middle\" syndrome. Therefore, we typically need to restructure the content for LLM to recognize the key information. We propose $\\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that operates in the post-retrieval process of RAG. $\\textit{Refiner}$ leverages a single decoder-only LLM to adaptively extract query-relevant contents verbatim along with the necessary context, and section them based on their interconnectedness, thereby highlights information distinction, and aligns downstream LLMs with the original context effectively. Experiments show that a trained $\\textit{Refiner}$ (with 7B parameters) exhibits significant gain to downstream LLM in improving answer accuracy, and outperforms other state-of-the-art advanced RAG and concurrent compressing approaches in various single-hop and multi-hop QA tasks. Notably, $\\textit{Refiner}$ achieves a 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared to the next best solution. $\\textit{Refiner}$ is a plug-and-play solution that can be seamlessly integrated with RAG systems, facilitating its application across diverse open-source frameworks.","authors":["Zhonghao Li","Xuming Hu","Aiwei Liu","Kening Zheng","Sirui Huang","Hui Xiong"],"url":"https://arxiv.org/abs/2406.11357"}
{"created":"2025-04-30","title":"Probabilistic Shoenfield Machines","abstract":"This article provides the theoretical framework of Probabilistic Shoenfield Machines (PSMs), an extension of the classical Shoenfield Machine that models randomness in the computation process. PSMs are brought in contexts where deterministic computation is insufficient, such as randomized algorithms. By allowing transitions to multiple possible states with certain probabilities, PSMs can solve problems and make decisions based on probabilistic outcomes, hence expanding the variety of possible computations. We provide an overview of PSMs, detailing their formal definitions as well as the computation mechanism and their equivalence with Non-deterministic Shoenfield Machines (NSM).","authors":["Maksymilian Bujok","Adam Mata"],"url":"https://arxiv.org/abs/2407.05777"}
{"created":"2025-04-30","title":"Predictive AI with External Knowledge Infusion for Stocks","abstract":"Fluctuations in stock prices are influenced by a complex interplay of factors that go beyond mere historical data. These factors, themselves influenced by external forces, encompass inter-stock dynamics, broader economic factors, various government policy decisions, outbreaks of wars, etc. Furthermore, all of these factors are dynamic and exhibit changes over time. In this paper, for the first time, we tackle the forecasting problem under external influence by proposing learning mechanisms that not only learn from historical trends but also incorporate external knowledge from temporal knowledge graphs. Since there are no such datasets or temporal knowledge graphs available, we study this problem with stock market data, and we construct comprehensive temporal knowledge graph datasets. In our proposed approach, we model relations on external temporal knowledge graphs as events of a Hawkes process on graphs. With extensive experiments, we show that learned dynamic representations effectively rank stocks based on returns across multiple holding periods, outperforming related baselines on relevant metrics.","authors":["Ambedkar Dukkipati","Kawin Mayilvaghanan","Naveen Kumar Pallekonda","Sai Prakash Hadnoor","Ranga Shaarad Ayyagari"],"url":"https://arxiv.org/abs/2504.20058"}
{"created":"2025-04-30","title":"Research Power Ranking: Adapting the Elo System to Quantify Scientist Evaluation","abstract":"This paper presents an original model for assessing scientific productivity, research power ranking, RPR, which is based on the adaptation of the Elo rating system to the context of scientific activity. Unlike traditional scientometric indicators, RPR accounts for the dynamics, multidimensionality, and relativity of research power. The model comprises three components fundamental, applied, and commercial activity each represented by a separate rating and updated on the basis of probabilistic scientific games analogous to chess matches. The scientific rating of each researcher is calculated as a weighted sum of the components, allowing the model to reflect not only their current position but also their career trajectory, including phase transitions, breakthroughs, and changes in scientific style. Numerical simulations were conducted for both the individual trajectories and population level distributions of the researchers. Phase diagrams were constructed, and a typology of scientific styles was formulated. The results demonstrate that RPR can serve as a universal tool for objective assessment, strategic planning, and visualization of scientific reputation in both academic and applied environments.","authors":["Eldar Knar"],"url":"https://arxiv.org/abs/2504.20061"}
{"created":"2025-04-30","title":"Deep Learning vs. Black-Scholes: Option Pricing Performance on Brazilian Petrobras Stocks","abstract":"This paper explores the use of deep residual networks for pricing European options on Petrobras, one of the world's largest oil and gas producers, and compares its performance with the Black-Scholes (BS) model. Using eight years of historical data from B3 (Brazilian Stock Exchange) collected via web scraping, a deep learning model was trained using a custom built hybrid loss function that incorporates market data and analytical pricing. The data for training and testing were drawn between the period spanning November 2016 to January 2025, using an 80-20 train-test split. The test set consisted of data from the final three months: November, December, and January 2025. The deep residual network model achieved a 64.3\\% reduction in the mean absolute error for the 3-19 BRL (Brazilian Real) range when compared to the Black-Scholes model on the test set. Furthermore, unlike the Black-Scholes solution, which tends to decrease its accuracy for longer periods of time, the deep learning model performed accurately for longer expiration periods. These findings highlight the potential of deep learning in financial modeling, with future work focusing on specialized models for different price ranges.","authors":["Joao Felipe Gueiros","Hemanth Chandravamsi","Steven H. Frankel"],"url":"https://arxiv.org/abs/2504.20088"}
{"created":"2025-04-30","title":"Heterogeneous network drug-target interaction prediction model based on graph wavelet transform and multi-level contrastive learning","abstract":"Drug-target interaction (DTI) prediction is a core task in drug development and precision medicine in the biomedical field. However, traditional machine learning methods generally have the black box problem, which makes it difficult to reveal the deep correlation between the model decision mechanism and the interaction pattern between biological molecules. This study proposes a heterogeneous network drug target interaction prediction framework, integrating graph neural network and multi scale signal processing technology to construct a model with both efficient prediction and multi level interpretability. Its technical breakthroughs are mainly reflected in the following three dimensions:Local global feature collaborative perception module. Based on heterogeneous graph convolutional neural network (HGCN), a multi order neighbor aggregation strategy is designed.Multi scale graph signal decomposition and biological interpretation module. A deep hierarchical node feature transform (GWT) architecture is proposed.Contrastive learning combining multi dimensional perspectives and hierarchical representations. By comparing the learning models, the node representations from the two perspectives of HGCN and GWT are aligned and fused, so that the model can integrate multi dimensional information and improve the prediction robustness. Experimental results show that our framework shows excellent prediction performance on all datasets. This study provides a complete solution for drug target discovery from black box prediction to mechanism decoding, and its methodology has important reference value for modeling complex biomolecular interaction systems.","authors":["Wenfeng Dai","Yanhong Wang","Shuai Yan","Qingzhi Yu","Xiang Cheng"],"url":"https://arxiv.org/abs/2504.20103"}
{"created":"2025-04-30","title":"Multiscale modelling of thermally stressed superelastic polyimide","abstract":"Many thermo-mechanical processes, such as thermal expansion and stress relaxation, originate at the atomistic scale. We develop a sequential multiscale approach to study thermally stressed superelastic polyimide to explore these effects. The continuum-scale smoothed particle hydrodynamics (SPH) model is coupled with atomistic molecular dynamics (MD) through constitutive modelling, where thermo-mechanical properties and equations of state are derived from MD simulations. The results are verified through benchmark problems of heat transfer. Finally, we analyse the insulating capabilities of superelastic polyimide by simulating the thermal response of an aluminium plate. The result shows a considerable reduction in the thermal stress, strain and temperature field development in the aluminium plate when superelastic polyimide is used as an insulator. The present work demonstrates the effectiveness of the multi-scale method in capturing thermo-mechanical interactions in superelastic polyimide.","authors":["Jerome Samuel S","Puneet Kumar Patra","Md Rushdie Ibne Islam"],"url":"https://arxiv.org/abs/2504.20123"}
{"created":"2025-04-30","title":"Learning Hierarchical Interaction for Accurate Molecular Property Prediction","abstract":"Discovering molecules with desirable molecular properties, including ADMET (Absorption, Distribution, Metabolism, Excretion, and Toxicity) profiles, is of great importance in drug discovery. Existing approaches typically employ deep learning models, such as Graph Neural Networks (GNNs) and Transformers, to predict these molecular properties by learning from diverse chemical information. However, these models often fail to efficiently capture and utilize the hierarchical nature of molecular structures, and lack mechanisms for effective interaction among multi-level features. To address these limitations, we propose a Hierarchical Interaction Message Passing Mechanism, which serves as the foundation of our novel model, HimNet. Our method enables interaction-aware representation learning across atomic, motif, and molecular levels via hierarchical attention-guided message passing. This design allows HimNet to effectively balance global and local information, ensuring rich and task-relevant feature extraction for downstream property prediction tasks, such as Blood-Brain Barrier Permeability (BBBP). Extensive experiments on multiple benchmark datasets demonstrate that HimNet achieves the best or near-best performance in most molecular property prediction tasks. Furthermore, our method exhibits promising hierarchical interpretability, aligning well with chemical intuition on representative molecules. We believe that HimNet offers an accurate and efficient solution for molecular activity and ADMET property prediction, contributing significantly to advanced decision-making in the early stages of drug discovery.","authors":["Huiyang Hong","Xinkai Wu","Hongyu Sun","Qi Wang","Yuquan Li"],"url":"https://arxiv.org/abs/2504.20127"}
{"created":"2025-04-30","title":"A Physically Driven Long Short Term Memory Model for Estimating Snow Water Equivalent over the Continental United States","abstract":"Snow is an essential input for various land surface models. Seasonal snow estimates are available as snow water equivalent (SWE) from process-based reanalysis products or locally from in situ measurements. While the reanalysis products are computationally expensive and available at only fixed spatial and temporal resolutions, the in situ measurements are highly localized and sparse. To address these issues and enable the analysis of the effect of a large suite of physical, morphological, and geological conditions on the presence and amount of snow, we build a Long Short-Term Memory (LSTM) network, which is able to estimate the SWE based on time series input of the various physical/meteorological factors as well static spatial/morphological factors. Specifically, this model breaks down the SWE estimation into two separate tasks: (i) a classification task that indicates the presence/absence of snow on a specific day and (ii) a regression task that indicates the height of the SWE on a specific day in the case of snow presence. The model is trained using physical/in situ SWE measurements from the SNOw TELemetry (SNOTEL) snow pillows in the western United States. We will show that trained LSTM models have a classification accuracy of $\\geq 93\\%$ for the presence of snow and a coefficient of correlation of $\\sim 0.9$ concerning their SWE estimates. We will also demonstrate that the models can generalize both spatially and temporally to previously unseen data.","authors":["Arun M. Saranathan","Mahmoud Saeedimoghaddam","Brandon Smith","Deepthi Raghunandan","Grey Nearing","Craig Pelissier"],"url":"https://arxiv.org/abs/2504.20129"}
{"created":"2025-04-30","title":"Network-Aware Scheduling for Remote Gate Execution in Quantum Data Centers","abstract":"Modular quantum computing provides a scalable approach to overcome the limitations of monolithic quantum architectures by interconnecting multiple Quantum Processing Units (QPUs) through a quantum network. In this work, we explore and evaluate two entanglement scheduling strategies-static and dynamic-and analyze their performance in terms of circuit execution delay and network resource utilization under realistic assumptions and practical limitations such as probabilistic entanglement generation, limited communication qubits, photonic switch reconfiguration delays, and topology-induced contention. We show that dynamic scheduling consistently outperforms static scheduling in scenarios with high entanglement parallelism, especially when network resources are scarce. Furthermore, we investigate the impact of communication qubit coherence time, modeled as a cutoff for holding EPR pairs, and demonstrate that aggressive lookahead strategies can degrade performance when coherence times are short, due to premature entanglement discarding and wasted resources. We also identify congestion-free BSM provisioning by profiling peak BSM usage per switch. Our results provide actionable insights for scheduler design and resource provisioning in realistic quantum data centers, bringing system-level considerations closer to practical quantum computing deployment.","authors":["Shahrooz Pouryousef","Reza Nejabati","Don Towsley","Ramana Kompella","Eneet Kaur"],"url":"https://arxiv.org/abs/2504.20176"}
{"created":"2025-04-30","title":"Coreset selection for the Sinkhorn divergence and generic smooth divergences","abstract":"We introduce CO2, an efficient algorithm to produce convexly-weighted coresets with respect to generic smooth divergences. By employing a functional Taylor expansion, we show a local equivalence between sufficiently regular losses and their second order approximations, reducing the coreset selection problem to maximum mean discrepancy minimization. We apply CO2 to the Sinkhorn divergence, providing a novel sampling procedure that requires logarithmically many data points to match the approximation guarantees of random sampling. To show this, we additionally verify several new regularity properties for entropically regularized optimal transport of independent interest. Our approach leads to a new perspective linking coreset selection and kernel quadrature to classical statistical methods such as moment and score matching. We showcase this method with a practical application of subsampling image data, and highlight key directions to explore for improved algorithmic efficiency and theoretical guarantees.","authors":["Alex Kokot","Alex Luedtke"],"url":"https://arxiv.org/abs/2504.20194"}
{"created":"2025-04-30","title":"Testing the Limit of Atmospheric Predictability with a Machine Learning Weather Model","abstract":"Atmospheric predictability research has long held that the limit of skillful deterministic weather forecasts is about 14 days. We challenge this limit using GraphCast, a machine-learning weather model, by optimizing forecast initial conditions using gradient-based techniques for twice-daily forecasts spanning 2020. This approach yields an average error reduction of 86% at 10 days, with skill lasting beyond 30 days. Mean optimal initial-condition perturbations reveal large-scale, spatially coherent corrections to ERA5, primarily reflecting an intensification of the Hadley circulation. Forecasts using GraphCast-optimal initial conditions in the Pangu-Weather model achieve a 21% error reduction, peaking at 4 days, indicating that analysis corrections reflect a combination of both model bias and a reduction in analysis error. These results demonstrate that, given accurate initial conditions, skillful deterministic forecasts are consistently achievable far beyond two weeks, challenging long-standing assumptions about the limits of atmospheric predictability.","authors":["P. Trent Vonich","Gregory J. Hakim"],"url":"https://arxiv.org/abs/2504.20238"}
{"created":"2025-04-30","title":"Optimizing Hard Thresholding for Sparse Model Discovery","abstract":"Many model selection algorithms rely on sparse dictionary learning to provide interpretable and physics-based governing equations. The optimization algorithms typically use a hard thresholding process to enforce sparse activations in the model coefficients by removing library elements from consideration. By introducing an annealing scheme that reactivates a fraction of the removed terms with a cooling schedule, we are able to improve the performance of these sparse learning algorithms. We concentrate on two approaches to the optimization, SINDy, and an alternative using hard thresholding pursuit. We see in both cases that annealing can improve model accuracy. The effectiveness of annealing is demonstrated through comparisons on several nonlinear systems pulled from convective flows, excitable systems, and population dynamics. Finally we apply these algorithms to experimental data for projectile motion.","authors":["Derek W. Jollie","Scott G. McCalla"],"url":"https://arxiv.org/abs/2504.20256"}
{"created":"2025-04-30","title":"Global Optimality Characterizations and Algorithms for Minimizing Quartically-Regularized Third-Order Taylor Polynomials","abstract":"High-order methods for convex and nonconvex optimization, particularly $p$th-order Adaptive Regularization Methods (AR$p$), have attracted significant research interest by naturally incorporating high-order Taylor models into adaptive regularization frameworks, resulting in algorithms with faster global and local convergence rates than first- and second-order methods. This paper establishes global optimality conditions for general, nonconvex cubic polynomials with quartic regularization. These criteria generalise existing results, recovering the optimality results for regularized quadratic polynomials, and can be further simplified in the low-rank and diagonal tensor cases. Under suitable assumptions on the Taylor polynomial, we derive a lower bound for the regularization parameter such that the necessary and sufficient criteria coincide, establishing a connection between this bound and the subproblem's convexification and sum-of-squares (SoS) convexification techniques. Leveraging the optimality characterization, we develop a Diagonal Tensor Method (DTM) for minimizing quartically-regularized cubic Taylor polynomials by iteratively minimizing a sequence of local models that incorporate both diagonal cubic terms and quartic regularization (DTM model). We show that the DTM algorithm is provably convergent, with a global evaluation complexity of $\\mathcal{O}(\\epsilon^{-3/2})$. Furthermore, when special structure is present (such as low rank or diagonal), DTM can exactly solve the given problem (in one iteration). In our numerical experiments, we propose practical DTM variants that exploit local problem information for model construction, which we then show to be competitive with cubic regularization and other subproblem solvers, with superior performance on problems with special structure.","authors":["Wenqi Zhu","Coralia Cartis"],"url":"https://arxiv.org/abs/2504.20259"}
{"created":"2025-04-30","title":"Entropy based lower dimension bounds for finite-time prediction of Dynamic Mode Decomposition algorithms","abstract":"Motivated by Dynamic Mode Decomposition algorithms, we provide lower bounds on the dimension of a finite-dimensional subspace $F \\subseteq \\mathrm{L}^2(\\mathrm{X})$ required for predicting the behavior of dynamical systems over long time horizons. We distinguish between two cases: (i) If $F$ is determined by a finite partition of $X$ we derive a lower bound that depends on the dynamical measure-theoretic entropy of the partition. (ii) We consider general finite-dimensional subspaces $F$ and establish a lower bound for the dimension of $F$ that is contingent on the spectral structure of the Koopman operator of the system, via the approximation entropy of $F$ as studied by Voiculescu. Furthermore, we motivate the use of delay observables to improve the predictive qualities of Dynamic Mode Decomposition algorithms.","authors":["Till Hauser","Julian H\\\"olz"],"url":"https://arxiv.org/abs/2504.20269"}
{"created":"2025-04-30","title":"Quantum Gate Decomposition: A Study of Compilation Time vs. Execution Time Trade-offs","abstract":"Similar to classical programming, high-level quantum programming languages generate code that cannot be executed directly by quantum hardware and must be compiled. However, unlike classical code, quantum programs must be compiled before each execution, making the trade-off between compilation time and execution time particularly significant. In this paper, we address the first step of quantum compilation: multi-qubit gate decomposition. We analyze the trade-offs of state-of-the-art decomposition algorithms by implementing them in the Ket quantum programming platform and collecting numerical performance data. This is the first study to both implement and analyze the current state-of-the-art decomposition methods within a single platform. Based on our findings, we propose two compilation profiles: one optimized for minimizing compilation time and another for minimizing quantum execution time. Our results provide valuable insights for both quantum compiler developers and quantum programmers, helping them make informed decisions about gate decomposition strategies and their impact on overall performance.","authors":["Evandro C. R. Rosa","Jerusa Marchi","Eduardo I. Duzzioni","Rafael de Santiago"],"url":"https://arxiv.org/abs/2504.20291"}
{"created":"2025-04-30","title":"Safe and Optimal N-Spacecraft Swarm Reconfiguration in Non-Keplerian Cislunar Orbits","abstract":"This paper presents a novel fuel-optimal guidance and control methodology for spacecraft swarm reconfiguration in Restricted Multi-Body Problems (RMBPs) with a guarantee of passive safety, maintaining miss distance even under abrupt loss of control authority. A new set of constraints exploits a quasi-periodic structure of RMBPs to guarantee passive safety. Particularly, this can be expressed as simple geometric constraints by solving optimal control in Local Toroidal Coordinates, which is based on a local eigenspace of a quasi-periodic motion around the corresponding periodic orbit. The proposed formulation enables a significant simplification of problem structure, which is highly applicable to large-scale swarm reconfiguration in cislunar orbits. The method is demonstrated in various models of RMBPs (Elliptical Restricted Three-Body Problem and Bi-Circular Restricted Four-Body Problem) and also validated in the full-ephemeris dynamics. By extending and generalizing well-known concepts from the two- to the three- and four-body problems, this paper lays the foundation for the practical control schemes of relative motion in cislunar space.","authors":["Yuji Takubo","Walter Manuel","Ethan Foss","Simone D'Amico"],"url":"https://arxiv.org/abs/2504.20386"}
{"created":"2025-04-30","title":"The Mean of Multi-Object Trajectories","abstract":"This paper introduces the concept of a mean for trajectories and multi-object trajectories--sets or multi-sets of trajectories--along with algorithms for computing them. Specifically, we use the Fr\\'{e}chet mean, and metrics based on the optimal sub-pattern assignment (OSPA) construct, to extend the notion of average from vectors to trajectories and multi-object trajectories. Further, we develop efficient algorithms to compute these means using greedy search and Gibbs sampling. Using distributed multi-object tracking as an application, we demonstrate that the Fr\\'{e}chet mean approach to multi-object trajectory consensus significantly outperforms state-of-the-art distributed multi-object tracking methods.","authors":["Tran Thien Dat Nguyen","Ba Tuong Vo","Ba-Ngu Vo","Hoa Van Nguyen","Changbeom Shim"],"url":"https://arxiv.org/abs/2504.20391"}
{"created":"2025-04-30","title":"Nonlinear Computation with Linear Optics via Source-Position Encoding","abstract":"Optical computing systems provide an alternate hardware model which appears to be aligned with the demands of neural network workloads. However, the challenge of implementing energy efficient nonlinearities in optics -- a key requirement for realizing neural networks -- is a conspicuous missing link. In this work we introduce a novel method to achieve nonlinear computation in fully linear media. Our method can operate at low power and requires only the ability to drive the optical system at a data-dependent spatial position. Leveraging this positional encoding, we formulate a fully automated, topology-optimization-based hardware design framework for extremely specialized optical neural networks, drawing on modern advancements in optimization and machine learning. We evaluate our optical designs on machine learning classification tasks: demonstrating significant improvements over linear methods, and competitive performance when compared to standard artificial neural networks.","authors":["N. Richardson","C. Bosch","R. P. Adams"],"url":"https://arxiv.org/abs/2504.20401"}
{"created":"2025-04-30","title":"SCOPE-MRI: Bankart Lesion Detection as a Case Study in Data Curation and Deep Learning for Challenging Diagnoses","abstract":"While deep learning has shown strong performance in musculoskeletal imaging, existing work has largely focused on pathologies where diagnosis is not a clinical challenge, leaving more difficult problems underexplored, such as detecting Bankart lesions (anterior-inferior glenoid labral tears) on standard MRIs. Diagnosing these lesions is challenging due to their subtle imaging features, often leading to reliance on invasive MRI arthrograms (MRAs). This study introduces ScopeMRI, the first publicly available, expert-annotated dataset for shoulder pathologies, and presents a deep learning (DL) framework for detecting Bankart lesions on both standard MRIs and MRAs. ScopeMRI includes 586 shoulder MRIs (335 standard, 251 MRAs) from 558 patients who underwent arthroscopy. Ground truth labels were derived from intraoperative findings, the gold standard for diagnosis. Separate DL models for MRAs and standard MRIs were trained using a combination of CNNs and transformers. Predictions from sagittal, axial, and coronal views were ensembled to optimize performance. The models were evaluated on a 20% hold-out test set (117 MRIs: 46 MRAs, 71 standard MRIs). The models achieved an AUC of 0.91 and 0.93, sensitivity of 83% and 94%, and specificity of 91% and 86% for standard MRIs and MRAs, respectively. Notably, model performance on non-invasive standard MRIs matched or surpassed radiologists interpreting MRAs. External validation demonstrated initial generalizability across imaging protocols. This study demonstrates that DL models can achieve radiologist-level diagnostic performance on standard MRIs, reducing the need for invasive MRAs. By releasing ScopeMRI and a modular codebase for training and evaluating deep learning models on 3D medical imaging data, we aim to accelerate research in musculoskeletal imaging and support the development of new datasets for clinically challenging diagnostic tasks.","authors":["Sahil Sethi","Sai Reddy","Mansi Sakarvadia","Jordan Serotte","Darlington Nwaudo","Nicholas Maassen","Lewis Shi"],"url":"https://arxiv.org/abs/2504.20405"}
{"created":"2025-04-30","title":"On the structure of (dart, odd hole)-free graphs","abstract":"A hole is a chordless cycle with at least four vertices. A hole is odd if it has an odd number of vertices. A dart is a graph which vertices a, b, c, d, e and edges ab, bc, bd, be, cd, de. Dart-free graphs have been actively studied in the literature. We prove that a (dart, odd hole)-free graph is perfect, or does not contain a stable set on three vertices, or is the join or co-join of two smaller graphs. Using this structure result, we design a polynomial- time algorithm for finding an optimal colouring of (dart, odd hole)-free graphs. A graph G is perfectly divisible if every induced subgraph H of G contains a set X of vertices such that X meets all largest cliques of H, and X induces a perfect graph. The chromatic number of a perfectly divisible graph G is bounded by {\\omega}^2 where {\\omega} denotes the number of vertices in a largest clique of G. We prove that (dart, odd hole)-free graphs are perfectly divisible.","authors":["Ch\\'inh T. Ho\\`ang"],"url":"https://arxiv.org/abs/2504.20422"}
{"created":"2025-04-30","title":"LymphAtlas- A Unified Multimodal Lymphoma Imaging Repository Delivering AI-Enhanced Diagnostic Insight","abstract":"This study integrates PET metabolic information with CT anatomical structures to establish a 3D multimodal segmentation dataset for lymphoma based on whole-body FDG PET/CT examinations, which bridges the gap of the lack of standardised multimodal segmentation datasets in the field of haematological malignancies. We retrospectively collected 483 examination datasets acquired between March 2011 and May 2024, involving 220 patients (106 non-Hodgkin lymphoma, 42 Hodgkin lymphoma); all data underwent ethical review and were rigorously de-identified. Complete 3D structural information was preserved during data acquisition, preprocessing and annotation, and a high-quality dataset was constructed based on the nnUNet format. By systematic technical validation and evaluation of the preprocessing process, annotation quality and automatic segmentation algorithm, the deep learning model trained based on this dataset is verified to achieve accurate segmentation of lymphoma lesions in PET/CT images with high accuracy, good robustness and reproducibility, which proves the applicability and stability of this dataset in accurate segmentation and quantitative analysis. The deep fusion of PET/CT images achieved with this dataset not only significantly improves the accurate portrayal of the morphology, location and metabolic features of tumour lesions, but also provides solid data support for early diagnosis, clinical staging and personalized treatment, and promotes the development of automated image segmentation and precision medicine based on deep learning. The dataset and related resources are available at https://github.com/SuperD0122/LymphAtlas-.","authors":["Jiajun Ding","Beiyao Zhu","Xiaosheng Liu","Lishen Zhang","Zhao Liu"],"url":"https://arxiv.org/abs/2504.20454"}
{"created":"2025-04-30","title":"Full-field surrogate modeling of cardiac function encoding geometric variability","abstract":"Combining physics-based modeling with data-driven methods is critical to enabling the translation of computational methods to clinical use in cardiology. The use of rigorous differential equations combined with machine learning tools allows for model personalization with uncertainty quantification in time frames compatible with clinical practice. However, accurate and efficient surrogate models of cardiac function, built from physics-based numerical simulation, are still mostly geometry-specific and require retraining for different patients and pathological conditions. We propose a novel computational pipeline to embed cardiac anatomies into full-field surrogate models. We generate a dataset of electrophysiology simulations using a complex multi-scale mathematical model coupling partial and ordinary differential equations. We adopt Branched Latent Neural Maps (BLNMs) as an effective scientific machine learning method to encode activation maps extracted from physics-based numerical simulations into a neural network. Leveraging large deformation diffeomorphic metric mappings, we build a biventricular anatomical atlas and parametrize the anatomical variability of a small and challenging cohort of 13 pediatric patients affected by Tetralogy of Fallot. We propose a novel statistical shape modeling based z-score sampling approach to generate a new synthetic cohort of 52 biventricular geometries that are compatible with the original geometrical variability. This synthetic cohort acts as the training set for BLNMs. Our surrogate model demonstrates robustness and great generalization across the complex original patient cohort, achieving an average adimensional mean squared error of 0.0034. The Python implementation of our BLNM model is publicly available under MIT License at https://github.com/StanfordCBCL/BLNM.","authors":["Elena Martinez","Beatrice Moscoloni","Matteo Salvador","Fanwei Kong","Mathias Peirlinck","Alison Lesley Marsden"],"url":"https://arxiv.org/abs/2504.20479"}
{"created":"2025-04-30","title":"SAM-Guided Robust Representation Learning for One-Shot 3D Medical Image Segmentation","abstract":"One-shot medical image segmentation (MIS) is crucial for medical analysis due to the burden of medical experts on manual annotation. The recent emergence of the segment anything model (SAM) has demonstrated remarkable adaptation in MIS but cannot be directly applied to one-shot medical image segmentation (MIS) due to its reliance on labor-intensive user interactions and the high computational cost. To cope with these limitations, we propose a novel SAM-guided robust representation learning framework, named RRL-MedSAM, to adapt SAM to one-shot 3D MIS, which exploits the strong generalization capabilities of the SAM encoder to learn better feature representation. We devise a dual-stage knowledge distillation (DSKD) strategy to distill general knowledge between natural and medical images from the foundation model to train a lightweight encoder, and then adopt a mutual exponential moving average (mutual-EMA) to update the weights of the general lightweight encoder and medical-specific encoder. Specifically, pseudo labels from the registration network are used to perform mutual supervision for such two encoders. Moreover, we introduce an auto-prompting (AP) segmentation decoder which adopts the mask generated from the general lightweight model as a prompt to assist the medical-specific model in boosting the final segmentation performance. Extensive experiments conducted on three public datasets, i.e., OASIS, CT-lung demonstrate that the proposed RRL-MedSAM outperforms state-of-the-art one-shot MIS methods for both segmentation and registration tasks. Especially, our lightweight encoder uses only 3\\% of the parameters compared to the encoder of SAM-Base.","authors":["Jia Wang","Yunan Mei","Jiarui Liu","Xin Fan"],"url":"https://arxiv.org/abs/2504.20501"}
{"created":"2025-04-30","title":"Quality-factor inspired deep neural network solver for solving inverse scattering problems","abstract":"Deep neural networks have been applied to address electromagnetic inverse scattering problems (ISPs) and shown superior imaging performances, which can be affected by the training dataset, the network architecture and the applied loss function. Here, the quality of data samples is cared and valued by the defined quality factor. Based on the quality factor, the composition of the training dataset is optimized. The network architecture is integrated with the residual connections and channel attention mechanism to improve feature extraction. A loss function that incorporates data-fitting error, physical-information constraints and the desired feature of the solution is designed and analyzed to suppress the background artifacts and improve the reconstruction accuracy. Various numerical analysis are performed to demonstrate the superiority of the proposed quality-factor inspired deep neural network (QuaDNN) solver and the imaging performance is finally verified by experimental imaging test.","authors":["Yutong Du","Zicheng Liu","Miao Cao","Zupeng Liang","Yali Zong","Changyou Li"],"url":"https://arxiv.org/abs/2504.20504"}
{"created":"2025-04-30","title":"Randomstrasse101: Open Problems of 2024","abstract":"$\\texttt{Randomstrasse101}$ is a blog dedicated to Open Problems in Mathematics, with a focus on Probability Theory, Computation, Combinatorics, Statistics, and related topics. This manuscript serves as a stable record of the Open Problems posted in 2024, with the goal of easing academic referencing. The blog can currently be accessed at $\\texttt{randomstrasse101.math.ethz.ch}$.","authors":["Afonso S. Bandeira","Anastasia Kireeva","Antoine Maillard","Almut R\\\"odder"],"url":"https://arxiv.org/abs/2504.20539"}
{"created":"2025-04-30","title":"Faster Random Walk-based Capacitance Extraction with Generalized Antithetic Sampling","abstract":"Floating random walk-based capacitance extraction has emerged in recent years as a tried and true approach for extracting parasitic capacitance in very large scale integrated circuits. Being a Monte Carlo method, its performance is dependent on the variance of sampled quantities and variance reduction methods are crucial for the challenges posed by ever denser process technologies and layout-dependent effects. In this work, we present a novel, universal variance reduction method for floating random walk-based capacitance extraction, which is conceptually simple, highly efficient and provably reduces variance in all extractions, especially when layout-dependent effects are present. It is complementary to existing mathematical formulations for variance reduction and its performance gains are experienced on top of theirs. Numerical experiments demonstrate substantial such gains of up to 30% in number of walks necessary and even more in actual extraction times compared to the best previously proposed variance reduction approaches for the floating random-walk.","authors":["Periklis Liaskovitis","Marios Visvardis","Efthymios Efstathiou"],"url":"https://arxiv.org/abs/2504.20586"}
{"created":"2025-04-30","title":"Sobolev norm inconsistency of kernel interpolation","abstract":"We study the consistency of minimum-norm interpolation in reproducing kernel Hilbert spaces corresponding to bounded kernels. Our main result give lower bounds for the generalization error of the kernel interpolation measured in a continuous scale of norms that interpolate between $L^2$ and the hypothesis space. These lower bounds imply that kernel interpolation is always inconsistent, when the smoothness index of the norm is larger than a constant that depends only on the embedding index of the hypothesis space and the decay rate of the eigenvalues.","authors":["Yunfei Yang"],"url":"https://arxiv.org/abs/2504.20617"}
{"created":"2025-04-30","title":"Data Driven Deep Learning for Correcting Global Climate Model Projections of SST and DSL in the Bay of Bengal","abstract":"Climate change alters ocean conditions, notably temperature and sea level. In the Bay of Bengal, these changes influence monsoon precipitation and marine productivity, critical to the Indian economy. In Phase 6 of the Coupled Model Intercomparison Project (CMIP6), Global Climate Models (GCMs) use different shared socioeconomic pathways (SSPs) to obtain future climate projections. However, significant discrepancies are observed between these models and the reanalysis data in the Bay of Bengal for 2015-2024. Specifically, the root mean square error (RMSE) between the climate model output and the Ocean Reanalysis System (ORAS5) is 1.2C for the sea surface temperature (SST) and 1.1 m for the dynamic sea level (DSL). We introduce a new data-driven deep learning model to correct for this bias. The deep neural model for each variable is trained using pairs of climatology-removed monthly climate projections as input and the corresponding month's ORAS5 as output. This model is trained with historical data (1950 to 2014), validated with future projection data from 2015 to 2020, and tested with future projections from 2021 to 2023. Compared to the conventional EquiDistant Cumulative Distribution Function (EDCDF) statistical method for bias correction in climate models, our approach decreases RMSE by 0.15C for SST and 0.3 m for DSL. The trained model subsequently corrects the projections for 2024-2100. A detailed analysis of the monthly, seasonal, and decadal means and variability is performed to underscore the implications of the novel dynamics uncovered in our corrected projections.","authors":["Abhishek Pasula","Deepak N. Subramani"],"url":"https://arxiv.org/abs/2504.20620"}
{"created":"2025-04-30","title":"ISDrama: Immersive Spatial Drama Generation through Multimodal Prompting","abstract":"Multimodal immersive spatial drama generation focuses on creating continuous multi-speaker binaural speech with dramatic prosody based on multimodal prompts, with potential applications in AR, VR, and others. This task requires simultaneous modeling of spatial information and dramatic prosody based on multimodal inputs, with high data collection costs. To the best of our knowledge, our work is the first attempt to address these challenges. We construct MRSDrama, the first multimodal recorded spatial drama dataset, containing binaural drama audios, scripts, videos, geometric poses, and textual prompts. Then, we propose ISDrama, the first immersive spatial drama generation model through multimodal prompting. ISDrama comprises these primary components: 1) Multimodal Pose Encoder, based on contrastive learning, considering the Doppler effect caused by moving speakers to extract unified pose information from multimodal prompts. 2) Immersive Drama Transformer, a flow-based mamba-transformer model that generates high-quality drama, incorporating Drama-MOE to select proper experts for enhanced prosody and pose control. We also design a context-consistent classifier-free guidance strategy to coherently generate complete drama. Experimental results show that ISDrama outperforms baseline models on objective and subjective metrics. The demos and dataset are available at https://aaronz345.github.io/ISDramaDemo.","authors":["Yu Zhang","Wenxiang Guo","Changhao Pan","Zhiyuan Zhu","Tao Jin","Zhou Zhao"],"url":"https://arxiv.org/abs/2504.20630"}
{"created":"2025-04-30","title":"Learning and Generalization with Mixture Data","abstract":"In many, if not most, machine learning applications the training data is naturally heterogeneous (e.g. federated learning, adversarial attacks and domain adaptation in neural net training). Data heterogeneity is identified as one of the major challenges in modern day large-scale learning. A classical way to represent heterogeneous data is via a mixture model. In this paper, we study generalization performance and statistical rates when data is sampled from a mixture distribution. We first characterize the heterogeneity of the mixture in terms of the pairwise total variation distance of the sub-population distributions. Thereafter, as a central theme of this paper, we characterize the range where the mixture may be treated as a single (homogeneous) distribution for learning. In particular, we study the generalization performance under the classical PAC framework and the statistical error rates for parametric (linear regression, mixture of hyperplanes) as well as non-parametric (Lipschitz, convex and H\\\"older-smooth) regression problems. In order to do this, we obtain Rademacher complexity and (local) Gaussian complexity bounds with mixture data, and apply them to get the generalization and convergence rates respectively. We observe that as the (regression) function classes get more complex, the requirement on the pairwise total variation distance gets stringent, which matches our intuition. We also do a finer analysis for the case of mixed linear regression and provide a tight bound on the generalization error in terms of heterogeneity.","authors":["Harsh Vardhan","Avishek Ghosh","Arya Mazumdar"],"url":"https://arxiv.org/abs/2504.20651"}
{"created":"2025-04-30","title":"Warehouse storage and retrieval optimization via clustering, dynamic systems modeling, and GPU-accelerated routing","abstract":"This paper introduces a warehouse optimization procedure aimed at enhancing the efficiency of product storage and retrieval. By representing product locations and order flows within a time-evolving graph structure, we employ unsupervised clustering to define and refine compact order regions, effectively reducing picking distances. We describe the procedure using a dynamic mathematical model formulated using tools from random dynamical systems theory, enabling a principled analysis of the system's behavior over time even under random operational variations. For routing within this framework, we implement a parallelized Bellman-Ford algorithm, utilizing GPU acceleration to evaluate path segments efficiently. To address scalability challenges inherent in large routing graphs, we introduce a segmentation strategy that preserves performance while maintaining tractable memory requirements. Our results demonstrate significant improvements in both operational efficiency and computational feasibility for large-scale warehouse environments.","authors":["Magnus Bengtsson","Jens Wittsten","Jonas Waidringer"],"url":"https://arxiv.org/abs/2504.20655"}
{"created":"2025-04-30","title":"The relative entropy of primes in arithmetic progressions is really small","abstract":"Fix a modulus $q$. One would expect the number of primes in each invertible residue class mod $q$ to be multinomially distributed, i.e. for each $p \\,\\mathrm{mod}\\, q$ to behave like an independent random variable uniform on $(\\mathbb{Z}/q\\mathbb{Z})^\\times$. Using techniques from data science, we discover overwhelming evidence to the contrary: primes are much more uniformly distributed than iid uniform random variables. This phenomenon was previously unknown, and there is no clear theoretical explanation for it.","authors":["Alex Cowan"],"url":"https://arxiv.org/abs/2504.20691"}
{"created":"2025-04-30","title":"Robust Recurrence of Discrete-Time Infinite-Horizon Stochastic Optimal Control with Discounted Cost","abstract":"We analyze the stability of general nonlinear discrete-time stochastic systems controlled by optimal inputs that minimize an infinite-horizon discounted cost. Under a novel stochastic formulation of cost-controllability and detectability assumptions inspired by the related literature on deterministic systems, we prove that uniform semi-global practical recurrence holds for the closed-loop system, where the adjustable parameter is the discount factor. Under additional continuity assumptions, we further prove that this property is robust.","authors":["Robert H. Moldenhauer","Dragan Ne\\v{s}i\\'c","Mathieu Granzotto","Romain Postoyan","Andrew R. Teel"],"url":"https://arxiv.org/abs/2504.20705"}
{"created":"2025-04-30","title":"Periodicity and local complexity of Delone sets","abstract":"We study complexity and periodicity of Delone sets by applying an algebraic approach to multidimensional symbolic dynamics. In this algebraic approach, $\\mathbb{Z}^d$-configurations $c: \\mathbb{Z}^d \\to \\mathcal{A}$ for a finite set $\\mathcal{A} \\subseteq \\mathbb{C}$ and finite $\\mathbb{Z}^d$-patterns are regarded as formal power series and Laurent polynomials, respectively. In this paper we study also functions $c: \\mathbb{R}^d \\to \\mathcal{A}$ where $\\mathcal{A}$ is as above. These functions are called $\\mathbb{R}^d$-configurations. Any Delone set may be regarded as an $\\mathbb{R}^d$-configuration by simply presenting it as its indicator function. Conversely, any $\\mathbb{R}^d$-configuration whose support (that is, the set of cells for which the configuration gets non-zero values) is a Delone set can be seen as a colored Delone set. We generalize the concept of annihilators and periodizers of $\\mathbb{Z}^d$-configurations for $\\mathbb{R}^d$-configurations. We show that if an $\\mathbb{R}^d$-configuration has a non-trivial annihilator, that is, if a linear combination of some finitely many of its translations is the zero function, then it has an annihilator of a particular form. Moreover, we show that $\\mathbb{R}^d$-configurations with integer coefficients that have non-trivial annihilators are sums of finitely many periodic functions $c_1,\\ldots,c_m: \\mathbb{R}^d \\to \\mathbb{Z}$. Also, $\\mathbb{R}^d$-pattern complexity is studied alongside with the classical patch-complexity of Delone sets. We point out the fact that sufficiently low $\\mathbb{R}^d$-pattern complexity of an $\\mathbb{R}^d$-configuration implies the existence of non-trivial annihilators. Moreover, it is shown that if a Meyer set has sufficiently slow patch-complexity growth, then it has a non-trivial annihilator. Finally, a condition for forced periodicity of colored Delone sets of finite local complexity is provided.","authors":["Pyry Herva","Jarkko Kari"],"url":"https://arxiv.org/abs/2504.20709"}
{"created":"2025-04-30","title":"On optimal error rates for strong approximation of SDEs with a H\\\"older continuous drift coefficient","abstract":"In the present article we study strong approximation of solutions of scalar stochastic differential equations (SDEs) with bounded and $\\alpha$-H\\\"older continuous drift coefficient and constant diffusion coefficient at time point $1$. Recently, it was shown in [arXiv:1909.07961v4 (2021)] that for such SDEs the equidistant Euler scheme achieves an $L^p$-error rate of at least $(1+\\alpha)/2$, up to an arbitrary small $\\varepsilon$, for all $p\\geq 1$ and all $\\alpha\\in(0, 1]$ in terms of the number of evaluations of the driving Brownian motion $W$. In this article we prove a matching lower error bound for $\\alpha\\in(0, 1)$. More precisely, we show that for every $\\alpha\\in(0, 1)$, the $L^p$-error rate $(1+\\alpha)/2$ of the Euler scheme in [arXiv:1909.07961v4 (2021)] can not be improved in general by no numerical method based on finitely many evaluations of $W$ at fixed time points. Up to now, this result was known in the literature only for $\\alpha=1$.","authors":["Simon Ellinger","Thomas M\\\"uller-Gronbach","Larisa Yaroslavtseva"],"url":"https://arxiv.org/abs/2504.20728"}
{"created":"2025-04-30","title":"Avoided-crossings, degeneracies and Berry phases in the spectrum of quantum noise through analytic Bloch-Messiah decomposition","abstract":"The Bloch-Messiah decomposition (BMD) is a fundamental tool in quantum optics, enabling the analysis and tailoring of multimode Gaussian states by decomposing linear optical transformations into passive interferometers and single-mode squeezers. Its extension to frequency-dependent matrix-valued functions, recently introduced as the ``analytic Bloch-Messiah decomposition\" (ABMD), provides the most general approach for characterizing the driven-dissipative dynamics of quantum optical systems governed by quadratic Hamiltonians. In this work, we present a detailed study of the ABMD, focusing on the typical behavior of parameter-dependent singular values and of their corresponding singular vectors. In particular, we analyze the hitherto unexplored occurrence of avoided and genuine crossings in the spectrum of quantum noise, the latter being manifested by nontrivial topological Berry phases of the singular vectors. We demonstrate that avoided crossings arise naturally when a single parameter is varied, leading to hypersensitivity of the singular vectors and suggesting the presence of genuine crossings in nearby systems. We highlight the possibility of programming the spectral response of photonic systems through the deliberate design of avoided crossings. As a notable example, we show that such control can be exploited to generate broad, flat-band squeezing spectra -- a desirable feature for enhancing degaussification protocols. This study provides new insights into the structure of multimode quantum correlations and offers a theoretical framework for experimental exploitation of complex quantum optical systems.","authors":["Giuseppe Patera","Alessandro Pugliese"],"url":"https://arxiv.org/abs/2504.20730"}
{"created":"2025-04-30","title":"Preference-centric Bandits: Optimality of Mixtures and Regret-efficient Algorithms","abstract":"The objective of canonical multi-armed bandits is to identify and repeatedly select an arm with the largest reward, often in the form of the expected value of the arm's probability distribution. Such a utilitarian perspective and focus on the probability models' first moments, however, is agnostic to the distributions' tail behavior and their implications for variability and risks in decision-making. This paper introduces a principled framework for shifting from expectation-based evaluation to an alternative reward formulation, termed a preference metric (PM). The PMs can place the desired emphasis on different reward realization and can encode a richer modeling of preferences that incorporate risk aversion, robustness, or other desired attitudes toward uncertainty. A fundamentally distinct observation in such a PM-centric perspective is that designing bandit algorithms will have a significantly different principle: as opposed to the reward-based models in which the optimal sampling policy converges to repeatedly sampling from the single best arm, in the PM-centric framework the optimal policy converges to selecting a mix of arms based on specific mixing weights. Designing such mixture policies departs from the principles for designing bandit algorithms in significant ways, primarily because of uncountable mixture possibilities. The paper formalizes the PM-centric framework and presents two algorithm classes (horizon-dependent and anytime) that learn and track mixtures in a regret-efficient fashion. These algorithms have two distinctions from their canonical counterparts: (i) they involve an estimation routine to form reliable estimates of optimal mixtures, and (ii) they are equipped with tracking mechanisms to navigate arm selection fractions to track the optimal mixtures. These algorithms' regret guarantees are investigated under various algebraic forms of the PMs.","authors":["Meltem Tatl{\\i}","Arpan Mukherjee","Prashanth L. A.","Karthikeyan Shanmugam","Ali Tajer"],"url":"https://arxiv.org/abs/2504.20877"}
{"created":"2025-04-30","title":"Energy-Based Coarse-Graining in Molecular Dynamics: A Flow-Based Framework Without Data","abstract":"Coarse-grained (CG) models offer an effective route to reducing the complexity of molecular simulations, yet conventional approaches depend heavily on long all-atom molecular dynamics (MD) trajectories to adequately sample configurational space. This data-driven dependence limits their accuracy and generalizability, as unvisited configurations remain excluded from the resulting CG model. We introduce a data-free generative framework for coarse-graining that directly targets the all-atom Boltzmann distribution. Our model defines a structured latent space comprising slow collective variables, which are statistically associated with multimodal marginal densities capturing metastable states, and fast variables, which represent the remaining degrees of freedom with simple, unimodal conditional distributions. A potentially learnable, bijective map from the full latent space to the all-atom configuration space enables automatic and accurate reconstruction of molecular structures. The model is trained using an energy-based objective that minimizes the reverse Kullback-Leibler divergence, relying solely on the interatomic potential rather than sampled trajectories. A tempering scheme is used to stabilize training and promote exploration of diverse configurations. Once trained, the model can generate unbiased, one-shot equilibrium all-atom samples. We validate the method on two synthetic systems-a double-well potential and a Gaussian mixture-as well as on the benchmark alanine dipeptide. The model captures all relevant modes of the Boltzmann distribution, accurately reconstructs atomic configurations, and learns physically meaningful coarse-grained representations, all without any simulation data.","authors":["Maximilian Stupp","P. S. Koutsourelakis"],"url":"https://arxiv.org/abs/2504.20940"}
{"created":"2025-04-30","title":"Provably faster randomized and quantum algorithms for k-means clustering via uniform sampling","abstract":"The $k$-means algorithm (Lloyd's algorithm) is a widely used method for clustering unlabeled data. A key bottleneck of the $k$-means algorithm is that each iteration requires time linear in the number of data points, which can be expensive in big data applications. This was improved in recent works proposing quantum and quantum-inspired classical algorithms to approximate the $k$-means algorithm locally, in time depending only logarithmically on the number of data points (along with data dependent parameters) [$q$-means: A quantum algorithm for unsupervised machine learning; Kerenidis, Landman, Luongo, and Prakash, NeurIPS 2019; Do you know what $q$-means?, Doriguello, Luongo, Tang]. In this work, we describe a simple randomized mini-batch $k$-means algorithm and a quantum algorithm inspired by the classical algorithm. We prove worse-case guarantees that significantly improve upon the bounds for previous algorithms. Our improvements are due to a careful use of uniform sampling, which preserves certain symmetries of the $k$-means problem that are not preserved in previous algorithms that use data norm-based sampling.","authors":["Tyler Chen","Archan Ray","Akshay Seshadri","Dylan Herman","Bao Bach","Pranav Deshpande","Abhishek Som","Niraj Kumar","Marco Pistoia"],"url":"https://arxiv.org/abs/2504.20982"}
{"created":"2025-04-30","title":"Average Case Analysis of Leaf-Centric Binary Tree Sources","abstract":"We study the average number of distinct fringe subtrees in random trees generated by leaf-centric binary tree sources as introduced by Zhang, Yang and Kieffer. A leaf-centric binary tree source induces for every $n \\geq 2$ a probability distribution on the set of binary trees with $n$ leaves. We generalize a result by Flajolet, Gourdon, Martinez and Devroye, according to which the average number of distinct fringe subtrees in a random binary search tree of size $n$ is in $\\Theta(n/\\log n)$, as well as a result by Flajolet, Sipala and Steayert, according to which the number of distinct fringe subtrees in a uniformly random binary tree of size $n$ is in $\\Theta(n/\\sqrt{\\log n})$.","authors":["Louisa Seelbach Benkner","Markus Lohrey","Stephan Wagner"],"url":"https://arxiv.org/abs/1804.10396"}
{"created":"2025-04-30","title":"Gaussian Pre-Activations in Neural Networks: Myth or Reality?","abstract":"The study of feature propagation at initialization in neural networks lies at the root of numerous initialization designs. An assumption very commonly made in the field states that the pre-activations are Gaussian. Although this convenient Gaussian hypothesis can be justified when the number of neurons per layer tends to infinity, it is challenged by both theoretical and experimental works for finite-width neural networks. Our major contribution is to construct a family of pairs of activation functions and initialization distributions that ensure that the pre-activations remain Gaussian throughout the network's depth, even in narrow neural networks. In the process, we discover a set of constraints that a neural network should fulfill to ensure Gaussian pre-activations. Additionally, we provide a critical review of the claims of the Edge of Chaos line of works and build an exact Edge of Chaos analysis. We also propose a unified view on pre-activations propagation, encompassing the framework of several well-known initialization procedures. Finally, our work provides a principled framework for answering the much-debated question: is it desirable to initialize the training of a neural network whose pre-activations are ensured to be Gaussian? Our code is available on GitHub: https://github.com/p-wol/gaussian-preact/ .","authors":["Pierre Wolinski","Julyan Arbel"],"url":"https://arxiv.org/abs/2205.12379"}
{"created":"2025-04-30","title":"A multi-language toolkit for the semi-automated checking of research outputs","abstract":"This article presents a free and open source toolkit that supports the semi-automated checking of research outputs (SACRO) for privacy disclosure within secure data environments. SACRO is a framework that applies best-practice principles-based statistical disclosure control (SDC) techniques on-the-fly as researchers conduct their analyses. SACRO is designed to assist human checkers rather than seeking to replace them as with current automated rules-based approaches. The toolkit is composed of a lightweight Python package that sits over well-known analysis tools that produce outputs such as tables, plots, and statistical models. This package adds functionality to (i) automatically identify potentially disclosive outputs against a range of commonly used disclosure tests; (ii) apply optional disclosure mitigation strategies as requested; (iii) report reasons for applying SDC; and (iv) produce simple summary documents trusted research environment staff can use to streamline their workflow and maintain auditable records. This creates an explicit change in the dynamics so that SDC is something done with researchers rather than to them, and enables more efficient communication with checkers. A graphical user interface supports human checkers by displaying the requested output and results of the checks in an immediately accessible format, highlighting identified issues, potential mitigation options, and tracking decisions made. The major analytical programming languages used by researchers (Python, R, and Stata) are supported by providing front-end packages that interface with the core Python back-end. Source code, packages, and documentation are available under MIT license at https://github.com/AI-SDC/ACRO","authors":["Richard J. Preen","Maha Albashir","Simon Davy","Jim Smith"],"url":"https://arxiv.org/abs/2212.02935"}
{"created":"2025-04-30","title":"QMP: Q-switch Mixture of Policies for Multi-Task Behavior Sharing","abstract":"Multi-task reinforcement learning (MTRL) aims to learn several tasks simultaneously for better sample efficiency than learning them separately. Traditional methods achieve this by sharing parameters or relabeled data between tasks. In this work, we introduce a new framework for sharing behavioral policies across tasks, which can be used in addition to existing MTRL methods. The key idea is to improve each task's off-policy data collection by employing behaviors from other task policies. Selectively sharing helpful behaviors acquired in one task to collect training data for another task can lead to higher-quality trajectories, leading to more sample-efficient MTRL. Thus, we introduce a simple and principled framework called Q-switch mixture of policies (QMP) that selectively shares behavior between different task policies by using the task's Q-function to evaluate and select useful shareable behaviors. We theoretically analyze how QMP improves the sample efficiency of the underlying RL algorithm. Our experiments show that QMP's behavioral policy sharing provides complementary gains over many popular MTRL algorithms and outperforms alternative ways to share behaviors in various manipulation, locomotion, and navigation environments. Videos are available at https://qmp-mtrl.github.io.","authors":["Grace Zhang","Ayush Jain","Injune Hwang","Shao-Hua Sun","Joseph J. Lim"],"url":"https://arxiv.org/abs/2302.00671"}
{"created":"2025-04-30","title":"Prophet: Prompting Large Language Models with Complementary Answer Heuristics for Knowledge-based Visual Question Answering","abstract":"Knowledge-based visual question answering (VQA) requires external knowledge beyond the image to answer the question. Early studies retrieve required knowledge from explicit knowledge bases (KBs), which often introduces irrelevant information to the question, hence restricting the performance of their models. Recent works have resorted to using a powerful large language model (LLM) as an implicit knowledge engine to acquire the necessary knowledge for answering. Despite the encouraging results achieved by these methods, we argue that they have not fully activated the capacity of the \\emph{blind} LLM as the provided textual input is insufficient to depict the required visual information to answer the question. In this paper, we present Prophet -- a conceptually simple, flexible, and general framework designed to prompt LLM with answer heuristics for knowledge-based VQA. Specifically, we first train a vanilla VQA model on a specific knowledge-based VQA dataset without external knowledge. After that, we extract two types of complementary answer heuristics from the VQA model: answer candidates and answer-aware examples. The two types of answer heuristics are jointly encoded into a formatted prompt to facilitate the LLM's understanding of both the image and question, thus generating a more accurate answer. By incorporating the state-of-the-art LLM GPT-3, Prophet significantly outperforms existing state-of-the-art methods on four challenging knowledge-based VQA datasets. Prophet is general that can be instantiated with the combinations of different VQA models (i.e., both discriminative and generative ones) and different LLMs (i.e., both commercial and open-source ones). Moreover, Prophet can also be integrated with modern large multimodal models in different stages, which is named Prophet++, to further improve the capabilities on knowledge-based VQA tasks.","authors":["Zhou Yu","Xuecheng Ouyang","Zhenwei Shao","Meng Wang","Jun Yu"],"url":"https://arxiv.org/abs/2303.01903"}
{"created":"2025-04-30","title":"A Nystr\\\"{o}m Method for Scattering by a Two-layered Medium with a Rough Boundary","abstract":"This paper is concerned with problems of scattering of time-harmonic acoustic waves by a two-layered medium with a non-locally perturbed boundary (called a rough boundary in this paper) in two dimensions, where a Dirichlet or impedance boundary condition is imposed on the boundary. The two-layered medium is composed of two unbounded media with different physical properties and the interface between the two media is considered to be a planar surface. We formulate the scattering problems considered as boundary value problems and prove the result of the well-posedness of each boundary value problem by utilizing the integral equation method associated with the two-layered Green function. Moreover, we develop a Nystr\\\"{o}m method for numerically solving the boundary value problems considered, based on the proposed integral equation formulations. We establish the convergence results of the Nystr\\\"{o}m method with the convergence rates depending on the smoothness of the rough boundary. It is worth noting that in establishing the well-posedness of the boundary value problems as well as the convergence results of the Nystr\\\"{o}m method, an essential role is played by the investigation of the asymptotic properties of the two-layered Green function for small and large arguments. Finally, numerical experiments are carried out to show the effectiveness of the Nystr\\\"{o}m method.","authors":["Haiyang Liu","Long Li","Jiansheng Yang","Bo Zhang","Haiwen Zhang"],"url":"https://arxiv.org/abs/2303.02339"}
{"created":"2025-04-30","title":"Practical solutions to the relative pose of three calibrated cameras","abstract":"We study the challenging problem of estimating the relative pose of three calibrated cameras from four point correspondences. We propose novel efficient solutions to this problem that are based on the simple idea of using four correspondences to estimate an approximate geometry of the first two views. We model this geometry either as an affine or a fully perspective geometry estimated using one additional approximate correspondence. We generate such an approximate correspondence using a very simple and efficient strategy, where the new point is the mean point of three corresponding input points. The new solvers are efficient and easy to implement, since they are based on existing efficient minimal solvers, i.e., the 4-point affine fundamental matrix, the well-known 5-point relative pose solver, and the P3P solver. Extensive experiments on real data show that the proposed solvers, when properly coupled with local optimization, achieve state-of-the-art results, with the novel solver based on approximate mean-point correspondences being more robust and accurate than the affine-based solver.","authors":["Charalambos Tzamos","Viktor Kocur","Yaqing Ding","Daniel Barath","Zuzana Berger Haladova","Torsten Sattler","Zuzana Kukelova"],"url":"https://arxiv.org/abs/2303.16078"}
{"created":"2025-04-30","title":"Instruct-ReID: A Multi-purpose Person Re-identification Task with Instructions","abstract":"Human intelligence can retrieve any person according to both visual and language descriptions. However, the current computer vision community studies specific person re-identification (ReID) tasks in different scenarios separately, which limits the applications in the real world. This paper strives to resolve this problem by proposing a new instruct-ReID task that requires the model to retrieve images according to the given image or language instructions. Our instruct-ReID is a more general ReID setting, where existing 6 ReID tasks can be viewed as special cases by designing different instructions. We propose a large-scale OmniReID benchmark and an adaptive triplet loss as a baseline method to facilitate research in this new setting. Experimental results show that the proposed multi-purpose ReID model, trained on our OmniReID benchmark without fine-tuning, can improve +0.5%, +0.6%, +7.7% mAP on Market1501, MSMT17, CUHK03 for traditional ReID, +6.4%, +7.1%, +11.2% mAP on PRCC, VC-Clothes, LTCC for clothes-changing ReID, +11.7% mAP on COCAS+ real2 for clothes template based clothes-changing ReID when using only RGB images, +24.9% mAP on COCAS+ real2 for our newly defined language-instructed ReID, +4.3% on LLCM for visible-infrared ReID, +2.6% on CUHK-PEDES for text-to-image ReID. The datasets, the model, and code will be available at https://github.com/hwz-zju/Instruct-ReID.","authors":["Weizhen He","Yiheng Deng","Shixiang Tang","Qihao Chen","Qingsong Xie","Yizhou Wang","Lei Bai","Feng Zhu","Rui Zhao","Wanli Ouyang","Donglian Qi","Yunfeng Yan"],"url":"https://arxiv.org/abs/2306.07520"}
{"created":"2025-04-30","title":"Deciding Predicate Logical Theories of Real-Valued Functions","abstract":"The notion of a real-valued function is central to mathematics, computer science, and many other scientific fields. Despite this importance, there are hardly any positive results on decision procedures for predicate logical theories that reason about real-valued functions. This paper defines a first-order predicate language for reasoning about multi-dimensional smooth real-valued functions and their derivatives, and demonstrates that - despite the obvious undecidability barriers - certain positive decidability results for such a language are indeed possible.","authors":["Stefan Ratschan"],"url":"https://arxiv.org/abs/2306.16505"}
{"created":"2025-04-30","title":"Settling the Sample Complexity of Online Reinforcement Learning","abstract":"A central issue lying at the heart of online reinforcement learning (RL) is data efficiency. While a number of recent works achieved asymptotically minimal regret in online RL, the optimality of these results is only guaranteed in a ``large-sample'' regime, imposing enormous burn-in cost in order for their algorithms to operate optimally. How to achieve minimax-optimal regret without incurring any burn-in cost has been an open problem in RL theory.","authors":["Zihan Zhang","Yuxin Chen","Jason D. Lee","Simon S. Du"],"url":"https://arxiv.org/abs/2307.13586"}
{"created":"2025-04-30","title":"An Adaptive Algorithm Based on Stochastic Discontinuous Galerkin for Convection Dominated Equations with Random Data","abstract":"In this paper, we propose an adaptive approach, based on mesh refinement or parametric enrichment with polynomial degree adaption, for numerical solution of convection dominated equations with random input data. A parametric system emerged from an application of stochastic Galerkin approach is discretized by using symmetric interior penalty Galerkin (SIPG) method with upwinding for the convection term in the spatial domain. We derive a residual-based error estimator contributed by the error due to the SIPG discretization, the (generalized) polynomial chaos discretization in the stochastic space, and data oscillations. Then, the reliability of the proposed error estimator, an upper bound for the energy error up to a multiplicative constant, is shown. Moreover, to balance the errors stemmed from spatial and stochastic spaces, the truncation error emerged from Karhunen--Lo\\`{e}ve expansion are considered in the numerical simulations. Last, several benchmark examples including a random diffusivity parameter, a random convectivity parameter, random diffusivity/convectivity parameters, and a random (jump) discontinuous diffusivity parameter, are tested to illustrate the performance of the proposed estimator.","authors":["Pelin \\c{C}ilo\\u{g}lu","Hamdullah Y\\\"ucel"],"url":"https://arxiv.org/abs/2308.05500"}
{"created":"2025-04-30","title":"Semantic Consistency for Assuring Reliability of Large Language Models","abstract":"Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold.","authors":["Harsh Raj","Vipul Gupta","Domenic Rosati","Subhabrata Majumdar"],"url":"https://arxiv.org/abs/2308.09138"}
{"created":"2025-04-30","title":"Offline and Online Use of Interval and Set-Based Approaches for Control and State Estimation: A Selection of Methodological Approaches and Their Application","abstract":"Control and state estimation procedures need to be robust against imprecisely known parameters, uncertainty in initial conditions, and external disturbances. Interval methods and other set-based techniques form the basis for the implementation of powerful approaches that can be used to identify parameters of dynamic system models in the presence of the aforementioned types of uncertainty. Moreover, they are applicable to a verified feasibility and stability analysis of controllers and state estimators. In addition to these approaches which are typically used offline for analysis of system models designed with classical floating point procedures, interval and set-based methods have also been developed in recent years, which allow to directly solve the associated design tasks and to implement reliable techniques that are applicable online, i.e., during system operation. The latter approaches include set-based model predictive control, online parameter adaptation techniques for nonlinear variable-structure and backstepping controllers, interval observers, and fault diagnosis techniques. This paper provides an overview of the methodological background and reviews numerous practical applications for which interval and other set-valued approaches have been employed successfully.","authors":["Andreas Rauh","Marit Lahme","Simon Rohou","Luc Jaulin","Thach Ngoc Dinh","Tarek Raissi","Mohamed Fnadi"],"url":"https://arxiv.org/abs/2309.11622"}
{"created":"2025-04-30","title":"LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models","abstract":"Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at https://github.com/eric-ai-lab/llm_coordination.","authors":["Saaket Agashe","Yue Fan","Anthony Reyna","Xin Eric Wang"],"url":"https://arxiv.org/abs/2310.03903"}
{"created":"2025-04-30","title":"Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education","abstract":"In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or D) that is the most likely answer to a question, given the context of the question. Our evaluation of six well-known LLMs, namely BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising results on the MCSB ability of LLMs for Vietnamese. The dataset is available for research purposes only.","authors":["Duc-Vu Nguyen","Quoc-Nam Nguyen"],"url":"https://arxiv.org/abs/2310.12059"}
{"created":"2025-04-30","title":"3DCoMPaT$^{++}$: An improved Large-scale 3D Vision Dataset for Compositional Recognition","abstract":"In this work, we present 3DCoMPaT$^{++}$, a multimodal 2D/3D dataset with 160 million rendered views of more than 10 million stylized 3D shapes carefully annotated at the part-instance level, alongside matching RGB point clouds, 3D textured meshes, depth maps, and segmentation masks. 3DCoMPaT$^{++}$ covers 41 shape categories, 275 fine-grained part categories, and 293 fine-grained material classes that can be compositionally applied to parts of 3D objects. We render a subset of one million stylized shapes from four equally spaced views as well as four randomized views, leading to a total of 160 million renderings. Parts are segmented at the instance level, with coarse-grained and fine-grained semantic levels. We introduce a new task, called Grounded CoMPaT Recognition (GCR), to collectively recognize and ground compositions of materials on parts of 3D objects. Additionally, we report the outcomes of a data challenge organized at CVPR2023, showcasing the winning method's utilization of a modified PointNet$^{++}$ model trained on 6D inputs, and exploring alternative techniques for GCR enhancement. We hope our work will help ease future research on compositional 3D Vision.","authors":["Habib Slim","Xiang Li","Yuchen Li","Mahmoud Ahmed","Mohamed Ayman","Ujjwal Upadhyay","Ahmed Abdelreheem","Arpit Prajapati","Suhail Pothigara","Peter Wonka","Mohamed Elhoseiny"],"url":"https://arxiv.org/abs/2310.18511"}
{"created":"2025-04-30","title":"TOP-Former: A Multi-Agent Transformer Approach for the Team Orienteering Problem","abstract":"Route planning for a fleet of vehicles is an important task in applications such as package delivery, surveillance, or transportation, often integrated within larger Intelligent Transportation Systems (ITS). This problem is commonly formulated as a Vehicle Routing Problem (VRP) known as the Team Orienteering Problem (TOP). Existing solvers for this problem primarily rely on either linear programming, which provides accurate solutions but requires computation times that grow with the size of the problem, or heuristic methods, which typically find suboptimal solutions in a shorter time. In this paper, we introduce TOP-Former, a multi-agent route planning neural network designed to efficiently and accurately solve the Team Orienteering Problem. The proposed algorithm is based on a centralized Transformer neural network capable of learning to encode the scenario (modeled as a graph) and analyze the complete context of all agents to deliver fast, precise, and collaborative solutions. Unlike other neural network-based approaches that adopt a more local perspective, TOP-Former is trained to understand the global situation of the vehicle fleet and generate solutions that maximize long-term expected returns. Extensive experiments demonstrate that the presented system outperforms most state-of-the-art methods in terms of both accuracy and computation speed.","authors":["Daniel Fuertes","Carlos R. del-Blanco","Fernando Jaureguizar","Narciso Garc\\'ia"],"url":"https://arxiv.org/abs/2311.18662"}
{"created":"2025-04-30","title":"GaitGuard: Towards Private Gait in Mixed Reality","abstract":"Augmented/Mixed Reality (AR/MR) technologies usher in a new era of immersive, collective experiences, differentiating them from traditional mobile systems. As these technologies evolve, prioritizing privacy and security is critical. This paper focuses on gait privacy, where gait, the way a person walks, can reveal sensitive information such as age, ethnicity, or disorders. We present GaitGuard, a real-time system that protects gait privacy against video-based gait extraction attacks in MR environments. GaitGuard leverages a multi-threaded framework to efficiently process video frames, incorporating dedicated modules for stream capture, body detection and tracking, and privacy leak mitigation. We compare and combine multiple mitigation techniques, offering guidance to navigate the privacy-utility tradeoff. Through extensive experiments covering 248 settings across mitigation regions, types, and tunable parameters, we assess the impact of these techniques on privacy, video quality, and system performance. GaitGuard reduces the confidence of video-based gait extraction attacks by introducing a substantial distribution shift (Jensen-Shannon Divergence of 0.63, indicating highly altered gait features) and a decrease in identification risks by up to 68%, while maintaining 29 FPS and preserving video clarity. GaitGuard provides a practical real-time solution for privacy-preserving MR applications without affecting the MR user experience based on 20 subjective user surveys.","authors":["Diana Romero","Ruchi Jagdish Patel","Athina Markopoulou","Salma Elmalaki"],"url":"https://arxiv.org/abs/2312.04470"}
{"created":"2025-04-30","title":"Trading off regional and overall energy system design flexibility in the net-zero transition","abstract":"The transition to net-zero emissions in Europe is determined by a patchwork of country-level and EU-wide policy, creating coordination challenges in an interconnected system. We use an optimisation model to map out near-optimal energy system designs for 2050, focussing on the planning flexibility of individual regions while maintaining overall system robustness against different weather years, cost assumptions, and land use limitations. Our results reveal extensive flexibility at a regional level, where only few technologies (solar around the Adriatic and wind on the British Isles and in Germany) cannot be substituted. National policymakers can influence renewable energy export and hydrogen strategies significantly, provided they coordinate this with the remaining European system. However, stronger commitment to solar in Southern Europe and Germany unlocks more design options for Europe overall. These results on regional trade-offs facilitate more meaningful policy discussions which are crucial in the transition to a sustainable energy system.","authors":["Koen van Greevenbroek","Aleksander Grochowicz","Marianne Zeyringer","Fred Espen Benth"],"url":"https://arxiv.org/abs/2312.11264"}
{"created":"2025-04-30","title":"Intelligent Condition Monitoring of Industrial Plants: An Overview of Methodologies and Uncertainty Management Strategies","abstract":"Condition monitoring plays a significant role in the safety and reliability of modern industrial systems. Artificial intelligence (AI) approaches are gaining attention from academia and industry as a growing subject in industrial applications and as a powerful way of identifying faults. This paper provides an overview of intelligent condition monitoring and fault detection and diagnosis methods for industrial plants with a focus on the open-source benchmark Tennessee Eastman Process (TEP). In this survey, the most popular and state-of-the-art deep learning (DL) and machine learning (ML) algorithms for industrial plant condition monitoring, fault detection, and diagnosis are summarized and the advantages and disadvantages of each algorithm are studied. Challenges like imbalanced data, unlabelled samples and how deep learning models can handle them are also covered. Finally, a comparison of the accuracies and specifications of different algorithms utilizing the Tennessee Eastman Process (TEP) is conducted. This research will be beneficial for both researchers who are new to the field and experts, as it covers the literature on condition monitoring and state-of-the-art methods alongside the challenges and possible solutions to them.","authors":["Maryam Ahang","Todd Charter","Oluwaseyi Ogunfowora","Maziyar Khadivi","Mostafa Abbasi","Homayoun Najjaran"],"url":"https://arxiv.org/abs/2401.10266"}
{"created":"2025-04-30","title":"Timeout Asynchronous Session Types: Safe Asynchronous Mixed-Choice For Timed Interactions","abstract":"Mixed-choice has long been barred from models of asynchronous communication since it compromises the decidability of key properties of communicating finite-state machines. Session types inherit this restriction, which precludes them from fully modelling timeouts -- a core property of web and cloud services. To address this deficiency, we present (binary) Timeout Asynchronous Session Types (TOAST) as an extension to (binary) asynchronous timed session types, that permits mixed-choice. TOAST deploys timing constraints to regulate the use of mixed-choice so as to preserve communication safety. We provide a new behavioural semantics for TOAST which guarantees progress in the presence of mixed-choice. Building upon TOAST, we provide a calculus featuring process timers which is capable of modelling timeouts using a receive-after pattern, much like Erlang, and capture the correspondence with TOAST specifications via a type system for which we prove subject reduction.","authors":["Jonah Pears","Laura Bocchi","Maurizio Murgia","Andy King"],"url":"https://arxiv.org/abs/2401.11197"}
{"created":"2025-04-30","title":"On the Approximate Core and Nucleon of Flow Games with Public Arcs","abstract":"We investigate flow games featuring both private arcs owned by individual players and public arcs accessible cost-free to all coalitions. We explore two solution concepts within this framework: the approximate core and the nucleon. The approximate core relaxes core requirements by permitting a bounded relative payoff deviation for every coalition, and the nucleon is a multiplicative analogue of Schmeidler's nucleolus which lexicographically maximizes the vector consisting of relative payoff deviations for every coalition arranged in a non-decreasing order. By leveraging a decomposition property for paths and cycles in a flow network, we derive complete characterizations for the approximate core and demonstrate that the nucleon can be computed in polynomial time.","authors":["Pengfei Liu","Han Xiao","Tianhang Lu","Qizhi Fang"],"url":"https://arxiv.org/abs/2401.13535"}
{"created":"2025-04-30","title":"A randomized algorithm for simultaneously diagonalizing symmetric matrices by congruence","abstract":"A family of symmetric matrices $A_1,\\ldots, A_d$ is SDC (simultaneous diagonalization by congruence, also called non-orthogonal joint diagonalization) if there is an invertible matrix $X$ such that every $X^T A_k X$ is diagonal. In this work, a novel randomized SDC (RSDC) algorithm is proposed that reduces SDC to a generalized eigenvalue problem by considering two (random) linear combinations of the family. We establish exact recovery: RSDC achieves diagonalization with probability $1$ if the family is exactly SDC. Under a mild regularity assumption, robust recovery is also established: Given a family that is $\\epsilon$-close to SDC then RSDC diagonalizes, with high probability, the family up to an error of norm $\\mathcal{O}(\\epsilon)$. Under a positive definiteness assumption, which often holds in applications, stronger results are established, including a bound on the condition number of the transformation matrix. For practical use, we suggest to combine RSDC with an optimization algorithm. The performance of the resulting method is verified for synthetic data, image separation and EEG analysis tasks. It turns out that our newly developed method outperforms existing optimization-based methods in terms of efficiency while achieving a comparable level of accuracy.","authors":["Haoze He","Daniel Kressner"],"url":"https://arxiv.org/abs/2402.16557"}
{"created":"2025-04-30","title":"Input-Output Extension of Underactuated Nonlinear Systems","abstract":"This letter addresses the challenge of enhancing the task-space capabilities of commercial underactuated systems through the integration of auxiliary actuators, while preserving their internal control architecture. We propose a method where additional actuators are combined with a feedback-linearizing outer loop controller, enabling full-pose tracking without modifying the platform's certified low-level controller. The key contribution is a theoretical framework establishing conditions under which legacy high-level commands and new actuator inputs can be cohesively coordinated to achieve decoupled control of all degrees of freedom. These conditions are validated through numerical simulations on a quadrotor retrofitted with two additional propellers. The results demonstrate stable 6D trajectory tracking, highlighting the method's ability to reconcile enhanced dexterity with industrial constraints such as certification and safety.","authors":["Mirko Mizzoni","Amr Afifi","Antonio Franchi"],"url":"https://arxiv.org/abs/2403.03117"}
{"created":"2025-04-30","title":"A Survey on Adversarial Contention Resolution","abstract":"Contention resolution addresses the challenge of coordinating access by multiple processes to a shared resource such as memory, disk storage, or a communication channel. Originally spurred by challenges in database systems and bus networks, contention resolution has endured as an important abstraction for resource sharing, despite decades of technological change. Here, we survey the literature on resolving worst-case contention, where the number of processes and the time at which each process may start seeking access to the resource is dictated by an adversary. We also highlight the evolution of contention resolution, where new concerns -- such as security, quality of service, and energy efficiency -- are motivated by modern systems. These efforts have yielded insights into the limits of randomized and deterministic approaches, as well as the impact of different model assumptions such as global clock synchronization, knowledge of the number of processors, feedback from access attempts, and attacks on the availability of the shared resource.","authors":["Ioana Banicescu","Trisha Chakraborty","Seth Gilbert","Maxwell Young"],"url":"https://arxiv.org/abs/2403.03876"}
{"created":"2025-04-30","title":"Specification Mining for Smart Contracts with Trace Slicing and Predicate Abstraction","abstract":"Smart contracts are computer programs running on blockchains to implement Decentralized Applications. The absence of contract specifications hinders routine tasks, such as contract understanding and testing. In this work, we propose a specification mining approach to infer contract specifications from past transaction histories. Our approach derives high-level behavioral automata of function invocations, accompanied by program invariants statistically inferred from the transaction histories. We implemented our approach as tool SMCON and evaluated it on eleven well-studied Azure benchmark smart contracts and six popular real-world DApp smart contracts. The experiments show that SMCON mines reasonably accurate specifications that can be used to enhance symbolic analysis of smart contracts achieving higher code coverage and up to 56% speedup, and facilitate DApp developers in maintaining high-quality documentation and test suites.","authors":["Ye Liu","Yixuan Liu","Yi Li","Cyrille Artho"],"url":"https://arxiv.org/abs/2403.13279"}
{"created":"2025-04-30","title":"Agentic AI: The Era of Semantic Decoding","abstract":"Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.","authors":["Maxime Peyrard","Martin Josifoski","Robert West"],"url":"https://arxiv.org/abs/2403.14562"}
{"created":"2025-04-30","title":"Twin Auto-Encoder Model for Learning Separable Representation in Cyberattack Detection","abstract":"Representation learning (RL) methods for cyberattack detection face the diversity and sophistication of attack data, leading to the issue of mixed representations of different classes, particularly as the number of classes increases. To address this, the paper proposes a novel deep learning architecture/model called the Twin Auto-Encoder (TAE). TAE first maps the input data into latent space and then deterministically shifts data samples of different classes further apart to create separable data representations, referred to as representation targets. TAE's decoder then projects the input data into these representation targets. After training, TAE's decoder extracts data representations. TAE's representation target serves as a novel dynamic codeword, which refers to the vector that represents a specific class. This vector is updated after each training epoch for every data sample, in contrast to the conventional fixed codeword that does not incorporate information from the input data. We conduct extensive experiments on diverse cybersecurity datasets, including seven IoT botnet datasets, two network IDS datasets, three malware datasets, one cloud DDoS dataset, and ten artificial datasets as the number of classes increases. TAE boosts accuracy and F-score in attack detection by around 2% compared to state-of-the-art models, achieving up to 96.1% average accuracy in IoT attack detection. Additionally, TAE is well-suited for cybersecurity applications and potentially for IoT systems, with a model size of approximately 1 MB and an average running time of around 2.6E-07 seconds for extracting a data sample.","authors":["Phai Vu Dinh","Quang Uy Nguyen","Thai Hoang Dinh","Diep N. Nguyen","Bao Son Pham","Eryk Dutkiewicz"],"url":"https://arxiv.org/abs/2403.15509"}
{"created":"2025-04-30","title":"Predicate Debiasing in Vision-Language Models Integration for Scene Graph Generation Enhancement","abstract":"Scene Graph Generation (SGG) provides basic language representation of visual scenes, requiring models to grasp complex and diverse semantics between objects. This complexity and diversity in SGG leads to underrepresentation, where parts of triplet labels are rare or even unseen during training, resulting in imprecise predictions. To tackle this, we propose integrating the pretrained Vision-language Models to enhance representation. However, due to the gap between pretraining and SGG, direct inference of pretrained VLMs on SGG leads to severe bias, which stems from the imbalanced predicates distribution in the pretraining language set. To alleviate the bias, we introduce a novel LM Estimation to approximate the unattainable predicates distribution. Finally, we ensemble the debiased VLMs with SGG models to enhance the representation, where we design a certainty-aware indicator to score each sample and dynamically adjust the ensemble weights. Our training-free method effectively addresses the predicates bias in pretrained VLMs, enhances SGG's representation, and significantly improve the performance.","authors":["Yuxuan Wang","Xiaoyuan Liu"],"url":"https://arxiv.org/abs/2403.16184"}
{"created":"2025-04-30","title":"Free Sets in Planar Graphs: History and Applications","abstract":"A subset $S$ of vertices in a planar graph $G$ is a free set if, for every set $P$ of $|S|$ points in the plane, there exists a straight-line crossing-free drawing of $G$ in which vertices of $S$ are mapped to distinct points in $P$. In this survey, we review - several equivalent definitions of free sets, - results on the existence of large free sets in planar graphs and subclasses of planar graphs, - and applications of free sets in graph drawing. The survey concludes with a list of open problems in this still very active research area.","authors":["Vida Dujmovi\\'c","Pat Morin"],"url":"https://arxiv.org/abs/2403.17090"}
{"created":"2025-04-30","title":"Common pitfalls to avoid while using multiobjective optimization in machine learning","abstract":"Recently, there has been an increasing interest in the application of multiobjective optimization (MOO) in machine learning (ML). This interest is driven by the numerous real-life situations where multiple objectives must be optimized simultaneously. A key aspect of MOO is the existence of a Pareto set, rather than a single optimal solution, which represents the optimal trade-offs between different objectives. Despite its potential, there is a noticeable lack of satisfactory literature serving as an entry-level guide for ML practitioners aiming to apply MOO effectively. In this paper, our goal is to provide such a resource and highlight pitfalls to avoid. We begin by establishing the groundwork for MOO, focusing on well-known approaches such as the weighted sum (WS) method, alongside more advanced techniques like the multiobjective gradient descent algorithm (MGDA). We critically review existing studies across various ML fields where MOO has been applied and identify challenges that can lead to incorrect interpretations. One of these fields is physics informed neural networks (PINNs), which we use as a guiding example to carefully construct experiments illustrating these pitfalls. By comparing WS and MGDA with one of the most common evolutionary algorithms, NSGA-II, we demonstrate that difficulties can arise regardless of the specific MOO method used. We emphasize the importance of understanding the specific problem, the objective space, and the selected MOO method, while also noting that neglecting factors such as convergence criteria can result in misleading experiments.","authors":["Junaid Akhter","Paul David F\\\"ahrmann","Konstantin Sonntag","Sebastian Peitz","Daniel Schwietert"],"url":"https://arxiv.org/abs/2405.01480"}
{"created":"2025-04-30","title":"Perception in Pixels: Effects of Avatar Representation in Video-Mediated Collaborative Interactions","abstract":"Interactive collaborative video is now a common part of remote work. Despite its prevalence, traditional video conferencing can be challenging, sometimes causing social discomforts that undermine process and outcomes. Avatars on 2D displays offer a promising alternative for enhancing self-representation, bridging the gap between virtual reality (VR) and traditional non-immersive video. However, the use of such avatars in activity-oriented group settings remains underexplored. To address this gap, we conducted a mixed-methods, within-subject study investigating the impacts of avatar-mediated versus traditional video representations on collaboration satisfaction and self-esteem. 32 participants (8 groups of 4 with pre-established relationships) engaged in goal-directed activities, followed by group interviews. Results indicate that avatars significantly enhance self-esteem and collaboration satisfaction, while qualitative insights reveal the dynamic perceptions and experiences of avatars, including benefits, challenges, and factors influencing adoption likelihood. Our study contributes to understanding and implications of avatars as a camera-driven representation in video-mediated collaborative interactions.","authors":["Pitch Sinlapanuntakul","Mark Zachry"],"url":"https://arxiv.org/abs/2405.03844"}
{"created":"2025-04-30","title":"QA-MDT: Quality-aware Masked Diffusion Transformer for Enhanced Music Generation","abstract":"Text-to-music (TTM) generation, which converts textual descriptions into audio, opens up innovative avenues for multimedia creation. Achieving high quality and diversity in this process demands extensive, high-quality data, which are often scarce in available datasets. Most open-source datasets frequently suffer from issues like low-quality waveforms and low text-audio consistency, hindering the advancement of music generation models. To address these challenges, we propose a novel quality-aware training paradigm for generating high-quality, high-musicality music from large-scale, quality-imbalanced datasets. Additionally, by leveraging unique properties in the latent space of musical signals, we adapt and implement a masked diffusion transformer (MDT) model for the TTM task, showcasing its capacity for quality control and enhanced musicality. Furthermore, we introduce a three-stage caption refinement approach to address low-quality captions' issue. Experiments show state-of-the-art (SOTA) performance on benchmark datasets including MusicCaps and the Song-Describer Dataset with both objective and subjective metrics. Demo audio samples are available at https://qa-mdt.github.io/, code and pretrained checkpoints are open-sourced at https://github.com/ivcylc/OpenMusic.","authors":["Chang Li","Ruoyu Wang","Lijuan Liu","Jun Du","Yixuan Sun","Zilu Guo","Zhenrong Zhang","Yuan Jiang","Jianqing Gao","Feng Ma"],"url":"https://arxiv.org/abs/2405.15863"}
{"created":"2025-04-30","title":"Instruct-ReID++: Towards Universal Purpose Instruction-Guided Person Re-identification","abstract":"Human intelligence can retrieve any person according to both visual and language descriptions. However, the current computer vision community studies specific person re-identification (ReID) tasks in different scenarios separately, which limits the applications in the real world. This paper strives to resolve this problem by proposing a novel instruct-ReID task that requires the model to retrieve images according to the given image or language instructions. Instruct-ReID is the first exploration of a general ReID setting, where existing 6 ReID tasks can be viewed as special cases by assigning different instructions. To facilitate research in this new instruct-ReID task, we propose a large-scale OmniReID++ benchmark equipped with diverse data and comprehensive evaluation methods e.g., task specific and task-free evaluation settings. In the task-specific evaluation setting, gallery sets are categorized according to specific ReID tasks. We propose a novel baseline model, IRM, with an adaptive triplet loss to handle various retrieval tasks within a unified framework. For task-free evaluation setting, where target person images are retrieved from task-agnostic gallery sets, we further propose a new method called IRM++ with novel memory bank-assisted learning. Extensive evaluations of IRM and IRM++ on OmniReID++ benchmark demonstrate the superiority of our proposed methods, achieving state-of-the-art performance on 10 test sets. The datasets, the model, and the code will be available at https://github.com/hwz-zju/Instruct-ReID","authors":["Weizhen He","Yiheng Deng","Yunfeng Yan","Feng Zhu","Yizhou Wang","Lei Bai","Qingsong Xie","Donglian Qi","Wanli Ouyang","Shixiang Tang"],"url":"https://arxiv.org/abs/2405.17790"}
{"created":"2025-04-30","title":"Drawing with Distance","abstract":"Drawing (a multiset of) coloured balls from an urn is one of the most basic models in discrete probability theory. Three modes of drawing are commonly distinguished: multinomial (draw-replace), hypergeometric (draw-delete), and Polya (draw-add). These drawing operations are represented as maps from urns to distributions over multisets of draws. The set of urns is a metric space via the Kantorovich distance. The set of distributions over draws is also a metric space, using Kantorovich-over-Kantorovich. It is shown that these three draw operations are all isometries, that is, they exactly preserve the Kantorovich distances. Further, drawing is studied in the limit, both for large urns and for large draws. First it is shown that, as the urn size increases, the Kantorovich distances go to zero between hypergeometric and multinomial draws, and also between P\\'olya and multinomial draws. Second, it is shown that, as the drawsize increases, the Kantorovich distance goes to zero (in probability) between an urn and (normalised) multinomial draws from the urn. These results are known, but here, they are formulated in a novel metric manner as limits of Kantorovich distances. We call these two limit results the law of large urns and the law of large draws.","authors":["Bart Jacobs"],"url":"https://arxiv.org/abs/2405.18182"}
{"created":"2025-04-30","title":"Scalable Surrogate Verification of Image-based Neural Network Control Systems using Composition and Unrolling","abstract":"Verifying safety of neural network control systems that use images as input is a difficult problem because, from a given system state, there is no known way to mathematically model what images are possible in the real-world. We build on recent work that considers a surrogate verification approach, training a conditional generative adversarial network (cGAN) as an image generator in place of the real world. This enables set-based formal analysis of the closed-loop system, providing analysis beyond simulation and testing. While existing work is effective on small examples, excessive overapproximation both within a single control period and across multiple control periods limits its scalability. We propose approaches to overcome these two sources of error. First, we overcome one-step error by composing the system's dynamics along with the cGAN and neural network controller, without losing the dependencies between input states and the control outputs as in the monotonic analysis of the system dynamics. Second, we reduce multi-step error by repeating the single-step composition, essentially unrolling multiple steps of the control loop into a large neural network. We then leverage existing network verification tools to compute accurate reachable sets for multiple steps, avoiding the accumulation of abstraction error at each step. We demonstrate the effectiveness of our approach in terms of both accuracy and scalability using two case studies: an autonomous aircraft taxiing system and an advanced emergency braking system. On the aircraft taxiing system, the converged reachable set is 175% larger using the prior baseline method compared with our proposed approach. On the emergency braking system, with 24x the number of image output variables from the cGAN, the baseline method fails to prove any states are safe, whereas our improvements enable set-based safety analysis.","authors":["Feiyang Cai","Chuchu Fan","Stanley Bak"],"url":"https://arxiv.org/abs/2405.18554"}
{"created":"2025-04-30","title":"Synthesizing Scoring Functions for Rankings Using Symbolic Gradient Descent","abstract":"Given a relation and a ranking of its tuples, but no information about the ranking function, we are interested in synthesizing simple scoring functions that reproduce the ranking. Our system RankHow identifies linear scoring functions that minimize position-based error, while supporting flexible constraints on their weights. It is based on a new formulation as a mixed-integer linear program (MILP). While MILP is NP-hard in general, we show that RankHow is orders of magnitude faster than a tree-based algorithm that guarantees polynomial time complexity (PTIME) in the number of input tuples by reducing the MILP problem to many linear programs (LPs). We hypothesize that this is caused by 2 properties: First, the PTIME algorithm is equivalent to a naive evaluation strategy for the MILP program. Second, MILP solvers rely on advanced heuristics to reason holistically about the entire program, while the PTIME algorithm solves many sub-problems in isolation. To further improve RankHow's scalability, we propose a novel approximation technique called symbolic gradient descent (Sym-GD). It exploits problem structure to more quickly find local minima of the error function. Experiments demonstrate that RankHow can solve realistic problems, finding more accurate linear scoring functions than the state of the art.","authors":["Zixuan Chen","Panagiotis Manolios","Mirek Riedewald"],"url":"https://arxiv.org/abs/2406.11797"}
{"created":"2025-04-30","title":"Seamless Monitoring of Stress Levels Leveraging a Universal Model for Time Sequences","abstract":"Monitoring the stress level in patients with neurodegenerative diseases can help manage symptoms, improve patient's quality of life, and provide insight into disease progression. In the literature, ECG, actigraphy, speech, voice, and facial analysis have proven effective at detecting patients' emotions. On the other hand, these tools are invasive and do not integrate smoothly into the patient's daily life. HRV has also been proven to effectively indicate stress conditions, especially in combination with other signals. However, when HRV is derived from less invasive devices than the ECG, like wristbands and smartwatches, the quality of measurements significantly degrades. This paper presents a methodology for stress detection from a wristband based on a universal model for time series, UniTS, which we finetuned for the task and equipped with explainability features. We cast the problem as anomaly detection rather than classification to favor model adaptation to individual patients and allow the clinician to maintain greater control over the system's predictions. We demonstrate that our proposed model considerably surpasses 12 top-performing methods on three benchmark datasets. Furthermore, unlike other state-of-the-art systems, UniTS enables seamless monitoring, as it shows comparable performance when using signals from invasive or lightweight devices.","authors":["Davide Gabrielli","Bardh Prenkaj","Paola Velardi"],"url":"https://arxiv.org/abs/2407.03821"}
{"created":"2025-04-30","title":"On the Scientific Method: The Role of Hypotheses and Involved Mathematics","abstract":"The paper investigates the role of data, hypotheses and mathematical methods that can be used in the discovery of a law y=fo(u), relating variables u and y of a physical phenomenon, making use of experimental measurements of such variables. Since the exact knowledge of the function fo cannot be expected, the problem of deriving approximate functions giving a small approximation error, measured by some function norm, is discussed. The main contributions of the paper are summarized as follows. At first, it is proven that deriving a reliable approximation, i.e., having a finite error, is not possible using measured data only. Thus, for deriving a reliable approximation, hypotheses on the function fo and on the disturbances corrupting the measurements must be introduced. Second, necessary and sufficient conditions for deriving a reliable approximation are provided. If such conditions are satisfied, suitable accuracy properties of the approximation can be defined, called theoretical properties. Third, it is shown that it is not possible to verify the conditions necessary for deriving a reliable approximation, but it is possible to verify that hypotheses on fo and on the disturbances are falsified by experimental measurements, showing that no function and disturbances satisfying the given hypotheses exist, able to reproduce the measurements (this is called falsification property). The above properties are then discussed for hypotheses belonging to the following classes: Parametric Probabilistic, where fo is assumed to be a function depending on a vector p and the disturbances are assumed to be stochastic variables; Set Membership class, where fo is assumed to be a bounded smooth function and the disturbances are assumed to be bounded variables; Parametric Set Membership class, able to integrate Parametric Probabilistic hypotheses with Set Membership hypotheses.","authors":["Mario Milanese","Carlo Novara","Michele Taragna"],"url":"https://arxiv.org/abs/2407.06225"}
{"created":"2025-04-30","title":"A Self-organizing Interval Type-2 Fuzzy Neural Network for Multi-Step Time Series Prediction","abstract":"Data uncertainty is inherent in many real-world applications and poses significant challenges for accurate time series predictions. The interval type 2 fuzzy neural network (IT2FNN) has shown exceptional performance in uncertainty modelling for single-step prediction tasks. However, extending it for multi-step ahead predictions introduces further issues in uncertainty handling as well as model interpretability and accuracy. To address these issues, this paper proposes a new selforganizing interval type-2 fuzzy neural network with multiple outputs (SOIT2FNN-MO). Differing from the traditional six-layer IT2FNN, a nine-layer network architecture is developed. First, a new co-antecedent layer and a modified consequent layer are devised to improve the interpretability of the fuzzy model for multi-step time series prediction problems. Second, a new link layer is created to improve the accuracy by building temporal connections between multi-step predictions. Third, a new transformation layer is designed to address the problem of the vanishing rule strength caused by high-dimensional inputs. Furthermore, a two-stage, self-organizing learning mechanism is developed to automatically extract fuzzy rules from data and optimize network parameters. Experimental results on chaotic and microgrid prediction problems demonstrate that SOIT2FNN-MO outperforms state-of-the-art methods, by achieving a better accuracy ranging from 1.6% to 30% depending on the level of noises in data. Additionally, the proposed model is more interpretable, offering deeper insights into the prediction process.","authors":["Fulong Yao","Wanqing Zhao","Matthew Forshaw","Yang Song"],"url":"https://arxiv.org/abs/2407.08010"}
{"created":"2025-04-30","title":"Enforcing conservation laws and dissipation inequalities numerically via auxiliary variables","abstract":"We propose a general strategy for enforcing multiple conservation laws and dissipation inequalities in the numerical solution of initial value problems. The key idea is to represent each conservation law or dissipation inequality by means of an associated test function; we introduce auxiliary variables representing the projection of these test functions onto a discrete test set, and modify the equation to use these new variables. We demonstrate these ideas by their application to the Navier-Stokes equations. We generalize to arbitrary order the energy-dissipating and helicity-tracking scheme of Rebholz for the incompressible Navier-Stokes equations, and devise the first time discretization of the compressible equations that conserves mass, momentum, and energy, and provably dissipates entropy.","authors":["Boris D. Andrews","Patrick E. Farrell"],"url":"https://arxiv.org/abs/2407.11904"}
{"created":"2025-04-30","title":"A Practical Analysis of Human Alignment with *PO","abstract":"At the forefront of state-of-the-art human alignment methods are preference optimization methods (*PO). Prior research has often concentrated on identifying the best-performing method, typically involving a grid search over hyperparameters, which can be impractical for general practitioners. In this paper, we examine the robustness of existing state-of-the-art methods to varying hyperparameters in a realistic out-of-distribution (OOD) scenario that mirrors real-world applications of human alignment. Our goal is to empirically find the method that increases the likelihood of achieving better results through the lens of various metrics, such as KL divergence and response length. We also introduce LN-DPO, a simple length-normalized version of DPO that is more stable across hyperparameters, effectively reduces the average response length, and improves performance. Our analysis of state-of-the-art reference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO) methods reveals that they perform similarly at their peak (i.e., best possible scenario). However, we uncover that the pattern of change in performance greatly varies as we move away from the best possible scenario.","authors":["Kian Ahrabian","Xihui Lin","Barun Patra","Vishrav Chaudhary","Alon Benhaim","Jay Pujara","Xia Song"],"url":"https://arxiv.org/abs/2407.15229"}
{"created":"2025-04-30","title":"Hierarchically Disentangled Recurrent Network for Factorizing System Dynamics of Multi-scale Systems: An application on Hydrological Systems","abstract":"We present a framework for modeling multi-scale processes, and study its performance in the context of streamflow forecasting in hydrology. Specifically, we propose a novel hierarchical recurrent neural architecture that factorizes the system dynamics at multiple temporal scales and captures their interactions. This framework consists of an inverse and a forward model. The inverse model is used to empirically resolve the system's temporal modes from data (physical model simulations, observed data, or a combination of them from the past), and these states are then used in the forward model to predict streamflow. Experiments on several catchments from the National Weather Service North Central River Forecast Center show that FHNN outperforms standard baselines, including physics-based models and transformer-based approaches. The model demonstrates particular effectiveness in catchments with low runoff ratios and colder climates. We further validate FHNN on the CAMELS (Catchment Attributes and MEteorology for Large-sample Studies), which is a widely used continental-scale hydrology benchmark dataset, confirming consistent performance improvements for 1-7 day streamflow forecasts across diverse hydrological conditions. Additionally, we show that FHNN can maintain accuracy even with limited training data through effective pre-training strategies and training global models.","authors":["Rahul Ghosh","Arvind Renganathan","Zac McEachran","Kelly Lindsay","Somya Sharma","Michael Steinbach","John Nieber","Christopher Duffy","Vipin Kumar"],"url":"https://arxiv.org/abs/2407.20152"}
{"created":"2025-04-30","title":"Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment","abstract":"A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that language models exhibit a negative bias in the binary decisions of complex reasoning tasks. Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and quantitatively formulate negative bias. Based on NAS, we identify attention heads that attend to negative tokens provided in the instructions as answer candidate of binary decisions, regardless of the question in the prompt, and validate their association with the negative bias. Additionally, we propose the negative attention score alignment (NASA) method, which is a parameter-efficient fine-tuning technique to address the extracted negatively biased attention heads. Experimental results from various domains of reasoning tasks and large model search space demonstrate that NASA significantly reduces the gap between precision and recall caused by negative bias while preserving their generalization abilities.","authors":["Sangwon Yu","Jongyoon Song","Bongkyu Hwang","Hoyoung Kang","Sooah Cho","Junhwa Choi","Seongho Joe","Taehee Lee","Youngjune L. Gwon","Sungroh Yoon"],"url":"https://arxiv.org/abs/2408.00137"}
{"created":"2025-04-30","title":"Efficiency Unleashed: Inference Acceleration for LLM-based Recommender Systems with Speculative Decoding","abstract":"The past few years have witnessed a growing interest in LLM-based recommender systems (RSs), although their industrial deployment remains in a preliminary stage. Most existing deployments leverage LLMs offline as feature enhancers, generating augmented knowledge for downstream tasks. However, in recommendation scenarios with numerous users and items, even offline knowledge generation with LLMs demands significant time and computational resources. This inefficiency arises from the autoregressive nature of LLMs. A promising solution is speculative decoding, a Draft-Then-Verify approach that increases the number of tokens generated per decoding step. In this work, we first identify recommendation knowledge generation as a highly fitting use case for retrieval-based speculative decoding. Then, we discern its two characteristics: (1) the vast number of items and users in RSs leads to retrieval inefficiency, and (2) RSs exhibit high diversity tolerance for LLM-generated text. Building on these insights, we introduce Lossless Acceleration via Speculative Decoding for LLM-based Recommender Systems (LASER), which features a Customized Retrieval Pool to enhance retrieval efficiency and Relaxed Verification to improve the acceptance rate of draft tokens. LASER achieves a 3-5x speedup on public datasets and saves about 67\\% of computational resources during the online A/B test on a large-scale advertising scenario with lossless downstream recommendation performance. Our code is available at https://github.com/YunjiaXi/LASER","authors":["Yunjia Xi","Hangyu Wang","Bo Chen","Jianghao Lin","Menghui Zhu","Weiwen Liu","Ruiming Tang","Zhewei Wei","Weinan Zhang","Yong Yu"],"url":"https://arxiv.org/abs/2408.05676"}
{"created":"2025-04-30","title":"Problem Solving Through Human-AI Preference-Based Cooperation","abstract":"While there is a widespread belief that artificial general intelligence (AGI) -- or even superhuman AI -- is imminent, complex problems in expert domains are far from being solved. We argue that such problems require human-AI cooperation and that the current state of the art in generative AI is unable to play the role of a reliable partner due to a multitude of shortcomings, including difficulty to keep track of a complex solution artifact (e.g., a software program), limited support for versatile human preference expression and lack of adapting to human preference in an interactive setting. To address these challenges, we propose HAICo2, a novel human-AI co-construction framework. We take first steps towards a formalization of HAICo2 and discuss the difficult open research problems that it faces.","authors":["Subhabrata Dutta","Timo Kaufmann","Goran Glava\\v{s}","Ivan Habernal","Kristian Kersting","Frauke Kreuter","Mira Mezini","Iryna Gurevych","Eyke H\\\"ullermeier","Hinrich Schuetze"],"url":"https://arxiv.org/abs/2408.07461"}
{"created":"2025-04-30","title":"Courteous MPC for Autonomous Driving with CBF-inspired Risk Assessment","abstract":"With more autonomous vehicles (AVs) sharing roadways with human-driven vehicles (HVs), ensuring safe and courteous maneuvers that respect HVs' behavior becomes increasingly important. To promote both safety and courtesy in AV's behavior, an extension of Control Barrier Functions (CBFs)-inspired risk evaluation framework is proposed in this paper by considering both noisy observed positions and velocities of surrounding vehicles. The perceived risk by the ego vehicle can be visualized as a risk map that reflects the understanding of the surrounding environment and thus shows the potential for facilitating safe and courteous driving. By incorporating the risk evaluation framework into the Model Predictive Control (MPC) scheme, we propose a Courteous MPC for ego AV to generate courteous behaviors that 1) reduce the overall risk imposed on other vehicles and 2) respect the hard safety constraints and the original objective for efficiency. We demonstrate the performance of the proposed Courteous MPC via theoretical analysis and simulation experiments.","authors":["Yanze Zhang","Yiwei Lyu","Sude E. Demir","Xingyu Zhou","Yupeng Yang","Junmin Wang","Wenhao Luo"],"url":"https://arxiv.org/abs/2408.12822"}
{"created":"2025-04-30","title":"Dual-Interrelated Diffusion Model for Few-Shot Anomaly Image Generation","abstract":"The performance of anomaly inspection in industrial manufacturing is constrained by the scarcity of anomaly data. To overcome this challenge, researchers have started employing anomaly generation approaches to augment the anomaly dataset. However, existing anomaly generation methods suffer from limited diversity in the generated anomalies and struggle to achieve a seamless blending of this anomaly with the original image. Moreover, the generated mask is usually not aligned with the generated anomaly. In this paper, we overcome these challenges from a new perspective, simultaneously generating a pair of the overall image and the corresponding anomaly part. We propose DualAnoDiff, a novel diffusion-based few-shot anomaly image generation model, which can generate diverse and realistic anomaly images by using a dual-interrelated diffusion model, where one of them is employed to generate the whole image while the other one generates the anomaly part. Moreover, we extract background and shape information to mitigate the distortion and blurriness phenomenon in few-shot image generation. Extensive experiments demonstrate the superiority of our proposed model over state-of-the-art methods in terms of diversity, realism and the accuracy of mask. Overall, our approach significantly improves the performance of downstream anomaly inspection tasks, including anomaly detection, anomaly localization, and anomaly classification tasks.","authors":["Ying Jin","Jinlong Peng","Qingdong He","Teng Hu","Jiafu Wu","Hao Chen","Haoxuan Wang","Wenbing Zhu","Mingmin Chi","Jun Liu","Yabiao Wang"],"url":"https://arxiv.org/abs/2408.13509"}
{"created":"2025-04-30","title":"Many-Worlds Inverse Rendering","abstract":"Discontinuous visibility changes remain a major bottleneck when optimizing surfaces within a physically-based inverse renderer. Many previous works have proposed sophisticated algorithms and data structures to sample visibility silhouettes more efficiently.","authors":["Ziyi Zhang","Nicolas Roussel","Wenzel Jakob"],"url":"https://arxiv.org/abs/2408.16005"}
{"created":"2025-04-30","title":"Generic Objects as Pose Probes for Few-shot View Synthesis","abstract":"Radiance fields including NeRFs and 3D Gaussians demonstrate great potential in high-fidelity rendering and scene reconstruction, while they require a substantial number of posed images as inputs. COLMAP is frequently employed for preprocessing to estimate poses, while it necessitates a large number of feature matches to operate effectively, and it struggles with scenes characterized by sparse features, large baselines between images, or a limited number of input images. We aim to tackle few-view NeRF reconstruction using only 3 to 6 unposed scene images. Traditional methods often use calibration boards but they are not common in images. We propose a novel idea of utilizing everyday objects, commonly found in both images and real life, as \"pose probes\". The probe object is automatically segmented by SAM, whose shape is initialized from a cube. We apply a dual-branch volume rendering optimization (object NeRF and scene NeRF) to constrain the pose optimization and jointly refine the geometry. Specifically, object poses of two views are first estimated by PnP matching in an SDF representation, which serves as initial poses. PnP matching, requiring only a few features, is suitable for feature-sparse scenes. Additional views are incrementally incorporated to refine poses from preceding views. In experiments, PoseProbe achieves state-of-the-art performance in both pose estimation and novel view synthesis across multiple datasets. We demonstrate its effectiveness, particularly in few-view and large-baseline scenes where COLMAP struggles. In ablations, using different objects in a scene yields comparable performance. Our project page is available at: \\href{https://zhirui-gao.github.io/PoseProbe.github.io/}{this https URL}","authors":["Zhirui Gao","Renjiao Yi","Chenyang Zhu","Ke Zhuang","Wei Chen","Kai Xu"],"url":"https://arxiv.org/abs/2408.16690"}
{"created":"2025-04-30","title":"Debias Can be Unreliable: Mitigating Bias Issue in Evaluating Debiasing Recommendation","abstract":"Recent work has improved recommendation models remarkably by equipping them with debiasing methods. Due to the unavailability of fully-exposed datasets, most existing approaches resort to randomly-exposed datasets as a proxy for evaluating debiased models, employing traditional evaluation scheme to represent the recommendation performance. However, in this study, we reveal that traditional evaluation scheme is not suitable for randomly-exposed datasets, leading to inconsistency between the Recall performance obtained using randomly-exposed datasets and that obtained using fully-exposed datasets. Such inconsistency indicates the potential unreliability of experiment conclusions on previous debiasing techniques and calls for unbiased Recall evaluation using randomly-exposed datasets. To bridge the gap, we propose the Unbiased Recall Evaluation (URE) scheme, which adjusts the utilization of randomly-exposed datasets to unbiasedly estimate the true Recall performance on fully-exposed datasets. We provide theoretical evidence to demonstrate the rationality of URE and perform extensive experiments on real-world datasets to validate its soundness.","authors":["Chengbing Wang","Wentao Shi","Jizhi Zhang","Wenjie Wang","Hang Pan","Fuli Feng"],"url":"https://arxiv.org/abs/2409.04810"}
{"created":"2025-04-30","title":"AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge","abstract":"Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM's output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently misjudge the degree of conflict and struggle to handle instances that vary in their amount of conflict, with static methods over-adjusting when conflict is absent. We propose a fine-grained, instance-level approach called AdaCAD, which dynamically infers the weight of adjustment based on the degree of conflict, as measured by the Jensen-Shannon divergence between distributions representing contextual and parametric knowledge. Across four LLMs, six question-answering (QA) and three summarization datasets, we demonstrate that ADACAD consistently outperforms other decoding baselines with average QA accuracy gains of 14.21% (absolute) over a static contrastive baseline, and improves the factuality of summaries by 6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt performance when conflict is absent, ADACAD mitigates these losses, making it more applicable to real-world datasets in which some examples have conflict and others do not.","authors":["Han Wang","Archiki Prasad","Elias Stengel-Eskin","Mohit Bansal"],"url":"https://arxiv.org/abs/2409.07394"}
{"created":"2025-04-30","title":"Hook-Based Aerial Payload Grasping from a Moving Platform","abstract":"This paper investigates payload grasping from a moving platform using a hook-equipped aerial manipulator. First, a computationally efficient trajectory optimization based on complementarity constraints is proposed to determine the optimal grasping time. To enable application in complex, dynamically changing environments, the future motion of the payload is predicted using a physics simulator-based model. The success of payload grasping under model uncertainties and external disturbances is formally verified through a robustness analysis method based on integral quadratic constraints. The proposed algorithms are evaluated in a high-fidelity physical simulator, and in real flight experiments using a custom-designed aerial manipulator platform.","authors":["P\\'eter Antal","Tam\\'as P\\'eni","Roland T\\'oth"],"url":"https://arxiv.org/abs/2409.11788"}
{"created":"2025-04-30","title":"Invisible Servoing: a Visual Servoing Approach with Return-Conditioned Latent Diffusion","abstract":"In this paper, we present a novel visual servoing (VS) approach based on latent Denoising Diffusion Probabilistic Models (DDPMs), that explores the application of generative models for vision-based navigation of UAVs (Uncrewed Aerial Vehicles). Opposite to classical VS methods, the proposed approach allows reaching the desired target view, even when the target is initially not visible. This is possible thanks to the learning of a latent representation that the DDPM uses for planning and a dataset of trajectories encompassing target-invisible initial views. A compact representation is learned from raw images using a Cross-Modal Variational Autoencoder. Given the current image, the DDPM generates trajectories in the latent space driving the robotic platform to the desired visual target. The approach has been validated in simulation using two generic multi-rotor UAVs (a quadrotor and a hexarotor). The results show that we can successfully reach the visual target, even if not visible in the initial view.","authors":["Bishoy Gerges","Barbara Bazzana","Nicol\\`o Botteghi","Youssef Aboudorra","Antonio Franchi"],"url":"https://arxiv.org/abs/2409.13337"}
{"created":"2025-04-30","title":"An average case efficient algorithm for solving two variable linear diophantine equations","abstract":"Solving two variable linear Diophantine equations has application in many cryptographic protocols such as RSA and Elliptic curve cryptography. The Extended Euclid's algorithm is the most widely used algorithm to solve these equations. We revisit two algorithms to solve two variable linear Diophantine equations. For one of those, we do a fine-grained analysis of the number of recursive calls and arrive at a periodic function that represents the number of recursive calls. We find the period and use it to derive an accurate closed-form expression for the average number of recursive calls incurred by that algorithm. We find multiple loose upper bounds on the average number of recursive calls in different cases based on whether a solution exists or not. If we know that for a fixed value of $a,b$ and a varying $c$, an equation $ax+by=c$ (where $a>b$) is solvable, then we can find the solution in $O\\left(\\frac{\\log b}{gcd(a,b)}\\right)$ average number of recursion or steps. We computationally evaluate this bound as well as one more upper bound and compare them with the average number of recursive calls in Extended Euclid's algorithm on a number of random $ n$-bit inputs. We observe that the average number of iterations in the analyzed algorithm decreases with an increase in $gcd(a,b)$. We propose an iterative version of the algorithm. We implement this algorithm and find that the average number of iterations by our algorithm is less than that of two existing algorithms.","authors":["Mayank Deora","Pinakpani Pal"],"url":"https://arxiv.org/abs/2409.14052"}
{"created":"2025-04-30","title":"Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination","abstract":"The scientific ideation process often involves blending salient aspects of existing papers to create new ideas, and facet-based ideation is an established framework for idea generation. To see how large language models (LLMs) might assist in this process, we contribute a novel mixed-initiative ideation tool called Scideator. Starting from a user-provided set of scientific papers, Scideator extracts key facets -- purposes, mechanisms, and evaluations -- from these and related papers, allowing users to explore the idea space by interactively recombining facets to synthesize inventive ideas. Scideator also helps users gauge idea originality by searching the literature for overlaps, assessing idea novelty and providing explanations. To support these tasks, Scideator introduces three LLM-powered retrieval-augmented generation (RAG) modules: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty Checker. In a within-subjects user study (N=22) with computer-science researchers comparing Scideator to a strong baseline, our tool provided significantly more creativity support, particularly with respect to exploration, which participants considered the most important factor for idea generation.","authors":["Marissa Radensky","Simra Shahid","Raymond Fok","Pao Siangliulue","Tom Hope","Daniel S. Weld"],"url":"https://arxiv.org/abs/2409.14634"}
{"created":"2025-04-30","title":"Perfectly to a Tee: Understanding User Perceptions of Personalized LLM-Enhanced Narrative Interventions","abstract":"Stories about overcoming personal struggles can effectively illustrate the application of psychological theories in real life, yet they may fail to resonate with individuals' experiences. In this work, we employ large language models (LLMs) to create tailored narratives that acknowledge and address unique challenging thoughts and situations faced by individuals. Our study, involving 346 young adults across two settings, demonstrates that personalized LLM-enhanced stories were perceived to be better than human-written ones in conveying key takeaways, promoting reflection, and reducing belief in negative thoughts. These stories were not only seen as more relatable but also similarly authentic to human-written ones, highlighting the potential of LLMs in helping young adults manage their struggles. The findings of this work provide crucial design considerations for future narrative-based digital mental health interventions, such as the need to maintain relatability without veering into implausibility and refining the wording and tone of AI-enhanced content.","authors":["Ananya Bhattacharjee","Sarah Yi Xu","Pranav Rao","Yuchen Zeng","Jonah Meyerhoff","Syed Ishtiaque Ahmed","David C Mohr","Michael Liut","Alex Mariakakis","Rachel Kornfield","Joseph Jay Williams"],"url":"https://arxiv.org/abs/2409.16732"}
{"created":"2025-04-30","title":"Underwater Camouflaged Object Tracking Meets Vision-Language SAM2","abstract":"Over the past decade, significant progress has been made in visual object tracking, largely due to the availability of large-scale datasets. However, these datasets have primarily focused on open-air scenarios and have largely overlooked underwater animal tracking-especially the complex challenges posed by camouflaged marine animals. To bridge this gap, we take a step forward by proposing the first large-scale multi-modal underwater camouflaged object tracking dataset, namely UW-COT220. Based on the proposed dataset, this work first comprehensively evaluates current advanced visual object tracking methods, including SAM- and SAM2-based trackers, in challenging underwater environments, \\eg, coral reefs. Our findings highlight the improvements of SAM2 over SAM, demonstrating its enhanced ability to handle the complexities of underwater camouflaged objects. Furthermore, we propose a novel vision-language tracking framework called VL-SAM2, based on the video foundation model SAM2. Experimental results demonstrate that our VL-SAM2 achieves state-of-the-art performance on the UW-COT220 dataset. The dataset and codes are available at~\\href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\\color{magenta}{here}}.","authors":["Chunhui Zhang","Li Liu","Guanjie Huang","Zhipeng Zhang","Hao Wen","Xi Zhou","Shiming Ge","Yanfeng Wang"],"url":"https://arxiv.org/abs/2409.16902"}
{"created":"2025-04-30","title":"From Innermost to Full Probabilistic Term Rewriting: Almost-Sure Termination, Complexity, and Modularity","abstract":"There are many evaluation strategies for term rewrite systems, but automatically proving termination or analyzing complexity is usually easiest for innermost rewriting. Several syntactic criteria exist when innermost termination implies full termination or when runtime complexity and innermost runtime complexity coincide. We adapt these criteria to the probabilistic setting, e.g., we show when it suffices to analyze almost-sure termination w.r.t.\\ innermost rewriting in order to prove full almost-sure termination of probabilistic term rewrite systems. These criteria can be applied for both termination and complexity analysis in the probabilistic setting. We implemented and evaluated our new contributions in the tool AProVE. Moreover, we also use our new results on innermost and full probabilistic rewriting to investigate the modularity of probabilistic termination properties.","authors":["Jan-Christoph Kassing","J\\\"urgen Giesl"],"url":"https://arxiv.org/abs/2409.17714"}
{"created":"2025-04-30","title":"Racing Thoughts: Explaining Contextualization Errors in Large Language Models","abstract":"The profound success of transformer-based language models can largely be attributed to their ability to integrate relevant contextual information from an input sequence in order to generate a response or complete a task. However, we know very little about the algorithms that a model employs to implement this capability, nor do we understand their failure modes. For example, given the prompt \"John is going fishing, so he walks over to the bank. Can he make an ATM transaction?\", a model may incorrectly respond \"Yes\" if it has not properly contextualized \"bank\" as a geographical feature, rather than a financial institution. We propose the LLM Race Conditions Hypothesis as an explanation of contextualization errors of this form. This hypothesis identifies dependencies between tokens (e.g., \"bank\" must be properly contextualized before the final token, \"?\", integrates information from \"bank\"), and claims that contextualization errors are a result of violating these dependencies. Using a variety of techniques from mechanistic intepretability, we provide correlational and causal evidence in support of the hypothesis, and suggest inference-time interventions to address it.","authors":["Michael A. Lepori","Michael C. Mozer","Asma Ghandeharioun"],"url":"https://arxiv.org/abs/2410.02102"}
{"created":"2025-04-30","title":"DiSK: Differentially Private Optimizer with Simplified Kalman Filter for Noise Reduction","abstract":"Differential privacy (DP) offers a robust framework for safeguarding individual data privacy. To utilize DP in training modern machine learning models, differentially private optimizers have been widely used in recent years. A popular approach to privatize an optimizer is to clip the individual gradients and add sufficiently large noise to the clipped gradient. This approach led to the development of DP optimizers that have comparable performance with their non-private counterparts in fine-tuning tasks or in tasks with a small number of training parameters. However, a significant performance drop is observed when these optimizers are applied to large-scale training. This degradation stems from the substantial noise injection required to maintain DP, which disrupts the optimizer's dynamics. This paper introduces DiSK, a novel framework designed to significantly enhance the performance of DP optimizers. DiSK employs Kalman filtering, a technique drawn from control and signal processing, to effectively denoise privatized gradients and generate progressively refined gradient estimations. To ensure practicality for large-scale training, we simplify the Kalman filtering process, minimizing its memory and computational demands. We establish theoretical privacy-utility trade-off guarantees for DiSK, and demonstrate provable improvements over standard DP optimizers like DPSGD in terms of iteration complexity upper-bound. Extensive experiments across diverse tasks, including vision tasks such as CIFAR-100 and ImageNet-1k and language fine-tuning tasks such as GLUE, E2E, and DART, validate the effectiveness of DiSK. The results showcase its ability to significantly improve the performance of DP optimizers, surpassing state-of-the-art results under the same privacy constraints on several benchmarks.","authors":["Xinwei Zhang","Zhiqi Bu","Borja Balle","Mingyi Hong","Meisam Razaviyayn","Vahab Mirrokni"],"url":"https://arxiv.org/abs/2410.03883"}
{"created":"2025-04-30","title":"WearableMil: An End-to-End Framework for Military Activity Recognition and Performance Monitoring","abstract":"Musculoskeletal injuries during military training significantly impact readiness, making prevention through activity monitoring crucial. While Human Activity Recognition (HAR) using wearable devices offers promising solutions, it faces challenges in processing continuous data streams and recognizing diverse activities without predefined sessions. This paper introduces an end-to-end framework for preprocessing, analyzing, and recognizing activities from wearable data in military training contexts. Using data from 135 soldiers wearing \\textit{Garmin--55} smartwatches over six months with over 15 million minutes. We develop a hierarchical deep learning approach that achieves 93.8% accuracy in temporal splits and 83.8% in cross-user evaluation. Our framework addresses missing data through physiologically-informed methods, reducing unknown sleep states from 40.38% to 3.66%. We demonstrate that while longer time windows (45-60 minutes) improve basic state classification, they present trade-offs in detecting fine-grained activities. Additionally, we introduce an intuitive visualization system that enables real-time comparison of individual performance against group metrics across multiple physiological indicators. This approach to activity recognition and performance monitoring provides military trainers with actionable insights for optimizing training programs and preventing injuries.","authors":["Barak Gahtan","Shany Funk","Einat Kodesh","Itay Ketko","Tsvi Kuflik","Alex M. Bronstein"],"url":"https://arxiv.org/abs/2410.05452"}
{"created":"2025-04-30","title":"Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context","abstract":"Multi-hop reasoning, which requires multi-step reasoning based on the supporting documents within a given context, remains challenging for large language models (LLMs). LLMs often struggle to filter out irrelevant documents within the context, and their performance is sensitive to the absolute position of supporting documents within that context. In this paper, we identify an additional challenge: LLMs' performance is also sensitive to the order, relative position, in which the supporting documents are presented. We refer to this as the misordered context problem. To address this issue, based on the theoretical approach, we propose a simple yet effective method called context repetition (CoRe), which involves prompting the model by repeatedly presenting the context. This ensures that certain contiguous reasoning segments within supporting documents are presented in the optimal order, effectively guiding the model's reasoning in the appropriate direction. Applying CoRe, we improve the F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known \"lost-in-the-middle\" problem in LLMs and can be effectively combined with retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.","authors":["Sangwon Yu","Ik-hwan Kim","Jongyoon Song","Saehyung Lee","Junsung Park","Sungroh Yoon"],"url":"https://arxiv.org/abs/2410.07103"}
{"created":"2025-04-30","title":"ROMAN: Open-Set Object Map Alignment for Robust View-Invariant Global Localization","abstract":"Global localization is a fundamental capability required for long-term and drift-free robot navigation. However, current methods fail to relocalize when faced with significantly different viewpoints. We present ROMAN (Robust Object Map Alignment Anywhere), a global localization method capable of localizing in challenging and diverse environments by creating and aligning maps of open-set and view-invariant objects. ROMAN formulates and solves a registration problem between object submaps using a unified graph-theoretic global data association approach with a novel incorporation of a gravity direction prior and object shape and semantic similarity. This work's open-set object mapping and information-rich object association algorithm enables global localization, even in instances when maps are created from robots traveling in opposite directions. Through a set of challenging global localization experiments in indoor, urban, and unstructured/forested environments, we demonstrate that ROMAN achieves higher relative pose estimation accuracy than other image-based pose estimation methods or segment-based registration methods. Additionally, we evaluate ROMAN as a loop closure module in large-scale multi-robot SLAM and show a 35% improvement in trajectory estimation error compared to standard SLAM systems using visual features for loop closures. Code and videos can be found at https://acl.mit.edu/roman.","authors":["Mason B. Peterson","Yixuan Jia","Yulun Tian","Annika Thomas","Jonathan P. How"],"url":"https://arxiv.org/abs/2410.08262"}
{"created":"2025-04-30","title":"Regularized Robustly Reliable Learners and Instance Targeted Attacks","abstract":"Instance-targeted data poisoning attacks, where an adversary corrupts a training set to induce errors on specific test points, have raised significant concerns. Balcan et al (2022) proposed an approach to addressing this challenge by defining a notion of robustly-reliable learners that provide per-instance guarantees of correctness under well-defined assumptions, even in the presence of data poisoning attacks. They then give a generic optimal (but computationally inefficient) robustly reliable learner as well as a computationally efficient algorithm for the case of linear separators over log-concave distributions.","authors":["Avrim Blum","Donya Saless"],"url":"https://arxiv.org/abs/2410.10572"}
{"created":"2025-04-30","title":"Rethinking the Role of Infrastructure in Collaborative Perception","abstract":"Collaborative Perception (CP) is a process in which an ego agent receives and fuses sensor information from surrounding vehicles and infrastructure to enhance its perception capability. To evaluate the need for infrastructure equipped with sensors, extensive and quantitative analysis of the role of infrastructure data in CP is crucial, yet remains underexplored. To address this gap, we first quantitatively assess the importance of infrastructure data in existing vehicle-centric CP, where the ego agent is a vehicle. Furthermore, we compare vehicle-centric CP with infra-centric CP, where the ego agent is now the infrastructure, to evaluate the effectiveness of each approach. Our results demonstrate that incorporating infrastructure data improves 3D detection accuracy by up to 10.30%, and infra-centric CP shows enhanced noise robustness and increases accuracy by up to 46.47% compared with vehicle-centric CP.","authors":["Hyunchul Bae","Minhee Kang","Minwoo Song","Heejin Ahn"],"url":"https://arxiv.org/abs/2410.11259"}
{"created":"2025-04-30","title":"High-Resolution Frame Interpolation with Patch-based Cascaded Diffusion","abstract":"Despite the recent progress, existing frame interpolation methods still struggle with processing extremely high resolution input and handling challenging cases such as repetitive textures, thin objects, and large motion. To address these issues, we introduce a patch-based cascaded pixel diffusion model for high resolution frame interpolation, HiFI, that excels in these scenarios while achieving competitive performance on standard benchmarks. Cascades, which generate a series of images from low to high resolution, can help significantly with large or complex motion that require both global context for a coarse solution and detailed context for high resolution output. However, contrary to prior work on cascaded diffusion models which perform diffusion on increasingly large resolutions, we use a single model that always performs diffusion at the same resolution and upsamples by processing patches of the inputs and the prior solution. At inference time, this drastically reduces memory usage and allows a single model, solving both frame interpolation (base model's task) and spatial up-sampling, saving training cost as well. HiFI excels at high-resolution images and complex repeated textures that require global context, achieving comparable or state-of-the-art performance on various benchmarks (Vimeo, Xiph, X-Test, and SEPE-8K). We further introduce a new dataset, LaMoR, that focuses on particularly challenging cases, and HiFI significantly outperforms other baselines. Please visit our project page for video results: https://hifi-diffusion.github.io","authors":["Junhwa Hur","Charles Herrmann","Saurabh Saxena","Janne Kontkanen","Wei-Sheng Lai","Yichang Shih","Michael Rubinstein","David J. Fleet","Deqing Sun"],"url":"https://arxiv.org/abs/2410.11838"}
{"created":"2025-04-30","title":"Pose-Based Sign Language Appearance Transfer","abstract":"We introduce a method for transferring the signer's appearance in sign language skeletal poses while preserving the sign content. Using estimated poses, we transfer the appearance of one signer to another, maintaining natural movements and transitions. This approach improves pose-based rendering and sign stitching while obfuscating identity. Our experiments show that while the method reduces signer identification accuracy, it slightly harms sign recognition performance, highlighting a tradeoff between privacy and utility. Our code is available at https://github.com/sign-language-processing/pose-anonymization.","authors":["Amit Moryossef","Gerard Sant","Zifan Jiang"],"url":"https://arxiv.org/abs/2410.13675"}
{"created":"2025-04-30","title":"UniVST: A Unified Framework for Training-free Localized Video Style Transfer","abstract":"This paper presents UniVST, a unified framework for localized video style transfer based on diffusion models. It operates without the need for training, offering a distinct advantage over existing diffusion methods that transfer style across entire videos. The endeavors of this paper comprise: (1) A point-matching mask propagation strategy that leverages the feature maps from the DDIM inversion. This streamlines the model's architecture by obviating the need for tracking models. (2) A training-free AdaIN-guided localized video stylization mechanism that operates at both the latent and attention levels. This balances content fidelity and style richness, mitigating the loss of localized details commonly associated with direct video stylization. (3) A sliding-window consistent smoothing scheme that harnesses optical flow within the pixel representation and refines predicted noise to update the latent space. This significantly enhances temporal consistency and diminishes artifacts in stylized video. Our proposed UniVST has been validated to be superior to existing methods in quantitative and qualitative metrics. It adeptly addresses the challenges of preserving the primary object's style while ensuring temporal consistency and detail preservation. Our code is available at https://github.com/QuanjianSong/UniVST.","authors":["Quanjian Song","Mingbao Lin","Wengyi Zhan","Shuicheng Yan","Liujuan Cao","Rongrong Ji"],"url":"https://arxiv.org/abs/2410.20084"}
{"created":"2025-04-30","title":"Trustworthiness of Stochastic Gradient Descent in Distributed Learning","abstract":"Distributed learning (DL) uses multiple nodes to accelerate training, enabling efficient optimization of large-scale models. Stochastic Gradient Descent (SGD), a key optimization algorithm, plays a central role in this process. However, communication bottlenecks often limit scalability and efficiency, leading to increasing adoption of compressed SGD techniques to alleviate these challenges. Despite addressing communication overheads, compressed SGD introduces trustworthiness concerns, as gradient exchanges among nodes are vulnerable to attacks like gradient inversion (GradInv) and membership inference attacks (MIA). The trustworthiness of compressed SGD remains unexplored, leaving important questions about its reliability unanswered.","authors":["Hongyang Li","Caesar Wu","Mohammed Chadli","Said Mammar","Pascal Bouvry"],"url":"https://arxiv.org/abs/2410.21491"}
{"created":"2025-04-30","title":"MDCure: A Scalable Pipeline for Multi-Document Instruction-Following","abstract":"Multi-document (MD) processing is crucial for LLMs to handle real-world tasks such as summarization and question-answering across large sets of documents. While LLMs have improved at processing long inputs, MD contexts still present unique difficulties, including management of inter-document dependencies, redundancy, and incoherent structures. To address this challenge, we introduce MDCure, a scalable and effective instruction data generation framework to enhance the MD capabilities of LLMs without the computational cost of pre-training or reliance on human-annotated data. MDCure generates high-quality synthetic MD instruction data over sets of articles via targeted prompts. We also introduce MDCureRM, a cost-effective, MD-specific reward model to score and filter generated data based on their training utility for MD settings. MDCure is compatible with open- and closed-source models in addition to policy optimization methods such as PPO, enabling even small open-source models to surpass proprietary LLMs as strong generators of high-quality MD instruction data without further data filtering. With MDCure, we fine-tune a wide variety of LLMs up to 70B parameters in size from the FlanT5, Qwen2, and LLAMA3.1 model families. Extensive evaluations on a wide range of MD and long-context benchmarks spanning various tasks and domains show MDCure consistently improves performance over pre-trained baselines and base models by up to 75.1%. Our code, datasets, and models are available at https://github.com/yale-nlp/MDCure.","authors":["Gabrielle Kaili-May Liu","Bowen Shi","Avi Caciularu","Idan Szpektor","Arman Cohan"],"url":"https://arxiv.org/abs/2410.23463"}
{"created":"2025-04-30","title":"Constraint Back-translation Improves Complex Instruction Following of Large Language Models","abstract":"Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.","authors":["Yunjia Qi","Hao Peng","Xiaozhi Wang","Bin Xu","Lei Hou","Juanzi Li"],"url":"https://arxiv.org/abs/2410.24175"}
{"created":"2025-04-30","title":"Confidence Aware Learning for Reliable Face Anti-spoofing","abstract":"Current Face Anti-spoofing (FAS) models tend to make overly confident predictions even when encountering unfamiliar scenarios or unknown presentation attacks, which leads to serious potential risks. To solve this problem, we propose a Confidence Aware Face Anti-spoofing (CA-FAS) model, which is aware of its capability boundary, thus achieving reliable liveness detection within this boundary. To enable the CA-FAS to \"know what it doesn't know\", we propose to estimate its confidence during the prediction of each sample. Specifically, we build Gaussian distributions for both the live faces and the known attacks. The prediction confidence for each sample is subsequently assessed using the Mahalanobis distance between the sample and the Gaussians for the \"known data\". We further introduce the Mahalanobis distance-based triplet mining to optimize the parameters of both the model and the constructed Gaussians as a whole. Extensive experiments show that the proposed CA-FAS can effectively recognize samples with low prediction confidence and thus achieve much more reliable performance than other FAS models by filtering out samples that are beyond its reliable range.","authors":["Xingming Long","Jie Zhang","Shiguang Shan"],"url":"https://arxiv.org/abs/2411.01263"}
{"created":"2025-04-30","title":"Mapping Global Floods with 10 Years of Satellite Radar Data","abstract":"Floods cause extensive global damage annually, making effective monitoring essential. While satellite observations have proven invaluable for flood detection and tracking, comprehensive global flood datasets spanning extended time periods remain scarce. In this study, we introduce a novel deep learning flood detection model that leverages the cloud-penetrating capabilities of Sentinel-1 Synthetic Aperture Radar (SAR) satellite imagery, enabling consistent flood extent mapping in through cloud cover and in both day and night conditions. By applying this model to 10 years of SAR data, we create a unique, longitudinal global flood extent dataset with predictions unaffected by cloud coverage, offering comprehensive and consistent insights into historically flood-prone areas over the past decade. We use our model predictions to identify historically flood-prone areas in Ethiopia and demonstrate real-time disaster response capabilities during the May 2024 floods in Kenya. Additionally, our longitudinal analysis reveals potential increasing trends in global flood extent over time, although further validation is required to explore links to climate change. To maximize impact, we provide public access to both our model predictions and a code repository, empowering researchers and practitioners worldwide to advance flood monitoring and enhance disaster response strategies.","authors":["Amit Misra","Kevin White","Simone Fobi Nsutezo","William Straka","Juan Lavista"],"url":"https://arxiv.org/abs/2411.01411"}
{"created":"2025-04-30","title":"Unmasking the Shadows: Pinpoint the Implementations of Anti-Dynamic Analysis Techniques in Malware Using LLM","abstract":"Sandboxes and other dynamic analysis processes are prevalent in malware detection systems nowadays to enhance the capability of detecting 0-day malware. Therefore, techniques of anti-dynamic analysis (TADA) are prevalent in modern malware samples, and sandboxes can suffer from false negatives and analysis failures when analyzing the samples with TADAs. In such cases, human reverse engineers will get involved in conducting dynamic analysis manually (i.e., debugging, patching), which in turn also gets obstructed by TADAs. In this work, we propose a Large Language Model (LLM) based workflow that can pinpoint the location of the TADA implementation in the code, to help reverse engineers place breakpoints used in debugging. Our evaluation shows that we successfully identified the locations of 87.80% known TADA implementations adopted from public repositories. In addition, we successfully pinpoint the locations of TADAs in 4 well-known malware samples that are documented in online malware analysis blogs.","authors":["Haizhou Wang","Nanqing Luo","Xusheng Li","Peng LIu"],"url":"https://arxiv.org/abs/2411.05982"}
{"created":"2025-04-30","title":"A New 8/14 Two-Phase Switched Reluctance Motor","abstract":"Despite their simple and robust structure, low cost, and simple cooling system, switched reluctance motors (SRMs) face the challenge of low mean torque. A possible solution is to change the structure of SRMs. This article introduces an innovative combination of the number of rotor teeth and stator teeth of a two-phase switch reluctance motor (TPSRM) with eight teeth for the stator and fourteen teeth for the rotor. As a result of its unique design, which has a short path for passing the main flux, it requires less magnetomotive force. This leads to less core and copper loss, resulting in increased efficiency. Each tooth of the stator in a phase develops a positive torque during the rotation of the rotor, which increases the torque and consequently increases the mean torque of the proposed TPSRM. A current hysteresis control (CHC) is simulated by 2D FEM for the proposed 8/14 TPSRM and the conventional 8/12 TPSRM under the same mechanical load on the shaft to get a current hysteresis reference of 15A at the nominal speed of 600 rpm. To verify the novelty and advantages of the suggested TPSRM, it is compared with the conventional 8/12 TPSRM in terms of mean and peak torque, torque density, and core and copper losses were compared. Lastly, the proposed 8/14 TPSRM is shown to have better performance than the conventional 8/12 TPSRM.","authors":["Gholamreza Davarpanah","Hossein Shirzad","Jawad Faiz"],"url":"https://arxiv.org/abs/2411.06161"}
{"created":"2025-04-30","title":"Benchmarking LLMs' Judgments with No Gold Standard","abstract":"We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by Large Language Models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.","authors":["Shengwei Xu","Yuxuan Lu","Grant Schoenebeck","Yuqing Kong"],"url":"https://arxiv.org/abs/2411.07127"}
{"created":"2025-04-30","title":"Exploring the loss landscape of regularized neural networks via convex duality","abstract":"We discuss several aspects of the loss landscape of regularized neural networks: the structure of stationary points, connectivity of optimal solutions, path with nonincreasing loss to arbitrary global optimum, and the nonuniqueness of optimal solutions, by casting the problem into an equivalent convex problem and considering its dual. Starting from two-layer neural networks with scalar output, we first characterize the solution set of the convex problem using its dual and further characterize all stationary points. With the characterization, we show that the topology of the global optima goes through a phase transition as the width of the network changes, and construct counterexamples where the problem may have a continuum of optimal solutions. Finally, we show that the solution set characterization and connectivity results can be extended to different architectures, including two-layer vector-valued neural networks and parallel three-layer neural networks.","authors":["Sungyoon Kim","Aaron Mishkin","Mert Pilanci"],"url":"https://arxiv.org/abs/2411.07729"}
{"created":"2025-04-30","title":"Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset","abstract":"In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a thorough investigation of the dataset's content through both manual and automated evaluation; (2) experiments demonstrating the dataset's impact on models' safety; and (3) an analysis of the 100 most influential papers citing this dataset. Through our audit, we showcase how conceptualization failures and quality issues identified in the HH dataset can create additional harms by leading to disparate safety behaviors across demographic groups. Our findings highlight the need for more nuanced, context-sensitive approaches to safety mitigation in LLMs.","authors":["Khaoula Chehbouni","Jonathan Cola\\c{c}o Carr","Yash More","Jackie CK Cheung","Golnoosh Farnadi"],"url":"https://arxiv.org/abs/2411.08243"}
{"created":"2025-04-30","title":"Optimal Oblivious Subspace Embeddings with Near-optimal Sparsity","abstract":"An oblivious subspace embedding is a random $m\\times n$ matrix $\\Pi$ such that, for any $d$-dimensional subspace, with high probability $\\Pi$ preserves the norms of all vectors in that subspace within a $1\\pm\\epsilon$ factor. In this work, we give an oblivious subspace embedding with the optimal dimension $m=\\Theta(d/\\epsilon^2)$ that has a near-optimal sparsity of $\\tilde O(1/\\epsilon)$ non-zero entries per column of $\\Pi$. This is the first result to nearly match the conjecture of Nelson and Nguyen [FOCS 2013] in terms of the best sparsity attainable by an optimal oblivious subspace embedding, improving on a prior bound of $\\tilde O(1/\\epsilon^6)$ non-zeros per column [Chenakkod et al., STOC 2024]. We further extend our approach to the non-oblivious setting, proposing a new family of Leverage Score Sparsified embeddings with Independent Columns, which yield faster runtimes for matrix approximation and regression tasks.","authors":["Shabarish Chenakkod","Micha{\\l} Derezi\\'nski","Xiaoyu Dong"],"url":"https://arxiv.org/abs/2411.08773"}
{"created":"2025-04-30","title":"Detecting Children with Autism Spectrum Disorder based on Script-Centric Behavior Understanding with Emotional Enhancement","abstract":"The early diagnosis of autism spectrum disorder (ASD) is critically dependent on systematic observation and analysis of children's social behaviors. While current methodologies predominantly utilize supervised learning approaches, their clinical adoption faces two principal limitations: insufficient ASD diagnostic samples and inadequate interpretability of the detection outcomes. This paper presents a novel zero-shot ASD detection framework based on script-centric behavioral understanding with emotional enhancement, which is designed to overcome the aforementioned clinical constraints. The proposed pipeline automatically converts audio-visual data into structured behavioral text scripts through computer vision techniques, subsequently capitalizing on the generalization capabilities of large language models (LLMs) for zero-shot/few-shot ASD detection. Three core technical contributions are introduced: (1) A multimodal script transcription module transforming behavioral cues into structured textual representations. (2) An emotion textualization module encoding emotional dynamics as the contextual features to augment behavioral understanding. (3) A domain-specific prompt engineering strategy enables the injection of clinical knowledge into LLMs. Our method achieves an F1-score of 95.24\\% in diagnosing ASD in children with an average age of two years while generating interpretable detection rationales. This work opens up new avenues for leveraging the power of LLMs in analyzing and understanding ASD-related human behavior, thereby enhancing the accuracy of assisted autism diagnosis.","authors":["Wenxing Liu","Yueran Pan","Dong Zhang","Hongzhu Deng","Xiaobing Zou","Ming Li"],"url":"https://arxiv.org/abs/2411.09413"}
{"created":"2025-04-30","title":"A Bayesian Optimization Approach to Machine Translation Reranking","abstract":"Reranking a list of candidates from a machine translation system with an external scoring model and returning the highest-scoring candidate remains a simple and effective method for improving the overall output quality. Translation scoring models continue to grow in size, with the best models being comparable to generation models. Thus, reranking can add substantial computational cost to the translation pipeline. In this work, we pose reranking as a Bayesian optimization (BayesOpt) problem. By strategically selecting candidates to score based on a balance of exploration and exploitation, we show that it is possible to find top-scoring candidates when scoring only a fraction of the candidate list. For instance, our method achieves the same CometKiwi score using only 70 scoring evaluations compared a baseline system using 180. We present a multi-fidelity setting for BayesOpt, where the candidates are first scored with a cheaper but noisier proxy scoring model, which further improves the cost-performance tradeoff when using smaller but well-trained distilled proxy scorers.","authors":["Julius Cheng","Maike Z\\\"ufle","Vil\\'em Zouhar","Andreas Vlachos"],"url":"https://arxiv.org/abs/2411.09694"}
{"created":"2025-04-30","title":"Efficient Depth Estimation for Unstable Stereo Camera Systems on AR Glasses","abstract":"Stereo depth estimation is a fundamental component in augmented reality (AR), which requires low latency for real-time processing. However, preprocessing such as rectification and non-ML computations such as cost volume require significant amount of latency exceeding that of an ML model itself, which hinders the real-time processing required by AR. Therefore, we develop alternative approaches to the rectification and cost volume that consider ML acceleration (GPU and NPUs) in recent hardware. For pre-processing, we eliminate it by introducing homography matrix prediction network with a rectification positional encoding (RPE), which delivers both low latency and robustness to unrectified images. For cost volume, we replace it with a group-pointwise convolution-based operator and approximation of cosine similarity based on layernorm and dot product. Based on our approaches, we develop MultiHeadDepth (replacing cost volume) and HomoDepth (MultiHeadDepth + removing pre-processing) models. MultiHeadDepth provides 11.8-30.3% improvements in accuracy and 22.9-25.2% reduction in latency compared to a state-of-the-art depth estimation model for AR glasses from industry. HomoDepth, which can directly process unrectified images, reduces the end-to-end latency by 44.5%. We also introduce a multi-task learning method to handle misaligned stereo inputs on HomoDepth, which reduces the AbsRel error by 10.0-24.3%. The overall results demonstrate the efficacy of our approaches, which not only reduce the inference latency but also improve the model performance. Our code is available at https://github.com/UCI-ISA-Lab/MultiHeadDepth-HomoDepth","authors":["Yongfan Liu","Hyoukjun Kwon"],"url":"https://arxiv.org/abs/2411.10013"}
{"created":"2025-04-30","title":"On the Verification of Control Flow Attestation Evidence","abstract":"Remote run-time attestation methods, including Control Flow Attestation (CFA) and Data Flow Attestation (DFA), have been proposed to generate precise evidence of execution's control flow path (in CFA) and optionally execution data inputs (in DFA) on a remote and potentially compromised embedded device, hereby referred to as a Prover (Prv). Recent advances in run-time attestation architectures are also able to guarantee that a remote Verifier (Vrf) reliably receives this evidence from Prv, even when Prv's software state is fully compromised. This, in theory, enables secure \"run-time auditing\" in addition to best-effort attestation, i.e., it guarantees that Vrf can examine execution evidence to identify previously unknown compromises as soon as they are exploited, pinpoint their root cause(s), and remediate them. However, prior work has for the most part focused on securely implementing Prv's root of trust (responsible for generating authentic run-time evidence), leaving Vrf 's perspective in this security service unexplored. In this work, we argue that run-time attestation and auditing are only truly useful if Vrf can effectively analyze received evidence. From this premise, we characterize different types of evidence produced by existing run-time attestation/auditing architectures in terms of Vrf 's ability to detect and remediate (previously unknown) vulnerabilities. As a case study for practical uses of run-time evidence by Vrf, we propose SABRE: a Security Analysis and Binary Repair Engine. SABRE showcases how Vrf can systematically leverage run-time evidence to detect control flow attacks, pinpoint corrupted control data and specific instructions used to corrupt them, and leverage this evidence to automatically generate binary patches to buffer overflow and use-after-free vulnerabilities without source code knowledge.","authors":["Adam Caulfield","Norrathep Rattanavipanon","Ivan De Oliveira Nunes"],"url":"https://arxiv.org/abs/2411.10855"}
{"created":"2025-04-30","title":"ForPKG: A Framework for Constructing Forestry Policy Knowledge Graph and Application Analysis","abstract":"A policy knowledge graph can provide decision support for tasks such as project compliance, policy analysis, and intelligent question answering, and can also serve as an external knowledge base to assist the reasoning process of related large language models. Although there have been many related works on knowledge graphs, there is currently a lack of research on the construction methods of policy knowledge graphs. This paper, focusing on the forestry field, designs a complete policy knowledge graph construction framework, including: firstly, proposing a fine-grained forestry policy domain ontology; then, proposing an unsupervised policy information extraction method, and finally, constructing a complete forestry policy knowledge graph. The experimental results show that the proposed ontology has good expressiveness and extensibility, and the policy information extraction method proposed in this paper achieves better results than other unsupervised methods. Furthermore, by analyzing the application of the knowledge graph in the retrieval-augmented-generation task of the large language models, the practical application value of the knowledge graph in the era of large language models is confirmed. The knowledge graph resource will be released on an open-source platform and can serve as the basic knowledge base for forestry policy-related intelligent systems. It can also be used for academic research. In addition, this study can provide reference and guidance for the construction of policy knowledge graphs in other fields. Our data is provided on Github https://github.com/luozhongze/ForPKG.","authors":["Jingyun Sun","Zhongze Luo"],"url":"https://arxiv.org/abs/2411.11090"}
{"created":"2025-04-30","title":"Differentiable GPU-Parallelized Task and Motion Planning","abstract":"Planning long-horizon robot manipulation requires making discrete decisions about which objects to interact with and continuous decisions about how to interact with them. A robot planner must select grasps, placements, and motions that are feasible and safe. This class of problems falls under Task and Motion Planning (TAMP) and poses significant computational challenges in terms of algorithm runtime and solution quality, particularly when the solution space is highly constrained. To address these challenges, we propose a new bilevel TAMP algorithm that leverages GPU parallelism to efficiently explore thousands of candidate continuous solutions simultaneously. Our approach uses GPU parallelism to sample an initial batch of solution seeds for a plan skeleton and to apply differentiable optimization on this batch to satisfy plan constraints and minimize solution cost with respect to soft objectives. We demonstrate that our algorithm can effectively solve highly constrained problems with non-convex constraints in just seconds, substantially outperforming serial TAMP approaches, and validate our approach on multiple real-world robots. Project website and code: https://cutamp.github.io","authors":["William Shen","Caelan Garrett","Nishanth Kumar","Ankit Goyal","Tucker Hermans","Leslie Pack Kaelbling","Tom\\'as Lozano-P\\'erez","Fabio Ramos"],"url":"https://arxiv.org/abs/2411.11833"}
{"created":"2025-04-30","title":"PyAWD: A Library for Generating Large Synthetic Datasets of Acoustic Wave Propagation","abstract":"Seismic data is often sparse and unevenly distributed due to the high costs and logistical challenges associated with deploying physical seismometers, limiting the application of Machine Learning (ML) in earthquake analysis. While simulation methods exist, no tool allows the generation of large datasets containing simulated measurements of the ground motion. To address this gap, we introduce PyAWD, a Python library designed to generate high-resolution synthetic datasets simulating spatio-temporal acoustic wave propagation in both two-dimensional and three-dimensional heterogeneous media. By allowing fine control over parameters such as the wave speed, external forces, spatial and temporal discretization, and media composition, PyAWD enables the creation of ML-scale datasets that capture the complexity of seismic wave behavior. We illustrate the library's potential with an epicenter retrieval task, showcasing its suitability for designing complex, accurate seismic problems that require advanced ML approaches in the absence or lack of dense real-world data. We also show the usefulness of our tool to tackle the problem of data budgeting in the framework of epicenter retrieval.","authors":["Pascal Tribel","Gianluca Bontempi"],"url":"https://arxiv.org/abs/2411.12636"}
{"created":"2025-04-30","title":"EasyHOI: Unleashing the Power of Large Models for Reconstructing Hand-Object Interactions in the Wild","abstract":"Our work aims to reconstruct hand-object interactions from a single-view image, which is a fundamental but ill-posed task. Unlike methods that reconstruct from videos, multi-view images, or predefined 3D templates, single-view reconstruction faces significant challenges due to inherent ambiguities and occlusions. These challenges are further amplified by the diverse nature of hand poses and the vast variety of object shapes and sizes. Our key insight is that current foundational models for segmentation, inpainting, and 3D reconstruction robustly generalize to in-the-wild images, which could provide strong visual and geometric priors for reconstructing hand-object interactions. Specifically, given a single image, we first design a novel pipeline to estimate the underlying hand pose and object shape using off-the-shelf large models. Furthermore, with the initial reconstruction, we employ a prior-guided optimization scheme, which optimizes hand pose to comply with 3D physical constraints and the 2D input image content. We perform experiments across several datasets and show that our method consistently outperforms baselines and faithfully reconstructs a diverse set of hand-object interactions. Here is the link of our project page: https://lym29.github.io/EasyHOI-page/","authors":["Yumeng Liu","Xiaoxiao Long","Zemin Yang","Yuan Liu","Marc Habermann","Christian Theobalt","Yuexin Ma","Wenping Wang"],"url":"https://arxiv.org/abs/2411.14280"}
{"created":"2025-04-30","title":"Omni-IML: Towards Unified Image Manipulation Localization","abstract":"Existing Image Manipulation Localization (IML) methods mostly rely heavily on task-specific designs, making them perform well only on the target IML task, while joint training on multiple IML tasks causes significant performance degradation, hindering real applications.","authors":["Chenfan Qu","Yiwu Zhong","Fengjun Guo","Lianwen Jin"],"url":"https://arxiv.org/abs/2411.14823"}
{"created":"2025-04-30","title":"HI-SLAM2: Geometry-Aware Gaussian SLAM for Fast Monocular Scene Reconstruction","abstract":"We present HI-SLAM2, a geometry-aware Gaussian SLAM system that achieves fast and accurate monocular scene reconstruction using only RGB input. Existing Neural SLAM or 3DGS-based SLAM methods often trade off between rendering quality and geometry accuracy, our research demonstrates that both can be achieved simultaneously with RGB input alone. The key idea of our approach is to enhance the ability for geometry estimation by combining easy-to-obtain monocular priors with learning-based dense SLAM, and then using 3D Gaussian splatting as our core map representation to efficiently model the scene. Upon loop closure, our method ensures on-the-fly global consistency through efficient pose graph bundle adjustment and instant map updates by explicitly deforming the 3D Gaussian units based on anchored keyframe updates. Furthermore, we introduce a grid-based scale alignment strategy to maintain improved scale consistency in prior depths for finer depth details. Through extensive experiments on Replica, ScanNet, and ScanNet++, we demonstrate significant improvements over existing Neural SLAM methods and even surpass RGB-D-based methods in both reconstruction and rendering quality. The project page and source code will be made available at https://hi-slam2.github.io/.","authors":["Wei Zhang","Qing Cheng","David Skuddis","Niclas Zeller","Daniel Cremers","Norbert Haala"],"url":"https://arxiv.org/abs/2411.17982"}
{"created":"2025-04-30","title":"Optimal In-Network Distribution of Learning Functions for a Secure-by-Design Programmable Data Plane of Next-Generation Networks","abstract":"The rise of programmable data plane (PDP) and in-network computing (INC) paradigms paves the way for the development of network devices (switches, network interface cards, etc.) capable of performing advanced processing tasks. This allows running various types of algorithms, including machine learning, within the network itself to support user and network services. In particular, this paper delves into the deployment of in-network learning models with the aim of implementing fully distributed intrusion detection systems (IDS) or intrusion prevention systems (IPS). Specifically, a model is proposed for the optimal distribution of the IDS/IPS workload among data plane devices with the aim of ensuring complete network security without excessively burdening the normal operations of the devices. Furthermore, a meta-heuristic approach is proposed to reduce the long computation time required by the exact solution provided by the mathematical model and its performance is evaluated. The analysis conducted and the results obtained demonstrate the enormous potential of the proposed new approach for the creation of intelligent data planes that act effectively and autonomously as the first line of defense against cyber attacks, with minimal additional workload on the network devices involved.","authors":["Mattia Giovanni Spina","Edoardo Scalzo","Floriano De Rango","Francesca Guerriero","Antonio Iera"],"url":"https://arxiv.org/abs/2411.18384"}
{"created":"2025-04-30","title":"Verified Foundations for Differential Privacy","abstract":"Differential privacy (DP) has become the gold standard for privacy-preserving data analysis, but implementing it correctly has proven challenging. Prior work has focused on verifying DP at a high level, assuming the foundations are correct and a perfect source of randomness is available. However, the underlying theory of differential privacy can be very complex and subtle. Flaws in basic mechanisms and random number generation have been a critical source of vulnerabilities in real-world DP systems.","authors":["Markus de Medeiros","Muhammad Naveed","Tancr\\`ede Lepoint","Temesghen Kahsai","Tristan Ravitch","Stefan Zetzsche","Anjali Joshi","Joseph Tassarotti","Aws Albarghouthi","Jean-Baptiste Tristan"],"url":"https://arxiv.org/abs/2412.01671"}
{"created":"2025-04-30","title":"DP-2Stage: Adapting Language Models as Differentially Private Tabular Data Generators","abstract":"Generating tabular data under differential privacy (DP) protection ensures theoretical privacy guarantees but poses challenges for training machine learning models, primarily due to the need to capture complex structures under noisy supervision signals. Recently, pre-trained Large Language Models (LLMs) -- even those at the scale of GPT-2 -- have demonstrated great potential in synthesizing tabular data. However, their applications under DP constraints remain largely unexplored. In this work, we address this gap by applying DP techniques to the generation of synthetic tabular data. Our findings shows that LLMs face difficulties in generating coherent text when fine-tuned with DP, as privacy budgets are inefficiently allocated to non-private elements like table structures. To overcome this, we propose DP-2Stage, a two-stage fine-tuning framework for differentially private tabular data generation. The first stage involves non-private fine-tuning on a pseudo dataset, followed by DP fine-tuning on a private dataset. Our empirical results show that this approach improves performance across various settings and metrics compared to directly fine-tuned LLMs in DP contexts. We release our code and setup at https://github.com/tejuafonja/DP-2Stage.","authors":["Tejumade Afonja","Hui-Po Wang","Raouf Kerkouche","Mario Fritz"],"url":"https://arxiv.org/abs/2412.02467"}
{"created":"2025-04-30","title":"HANDI: Hand-Centric Text-and-Image Conditioned Video Generation","abstract":"Despite the recent strides in video generation, state-of-the-art methods still struggle with elements of visual detail. One particularly challenging case is the class of videos in which the intricate motion of the hand coupled with a mostly stable and otherwise distracting environment is necessary to convey the execution of some complex action and its effects. To address these challenges, we introduce a new method for video generation that focuses on hand-centric actions. Our diffusion-based method incorporates two distinct innovations. First, we propose an automatic method to generate the motion area -- the region in the video in which the detailed activities occur -- guided by both the visual context and the action text prompt, rather than assuming this region can be provided manually as is now commonplace. Second, we introduce a critical Hand Refinement Loss to guide the diffusion model to focus on smooth and consistent hand poses. We evaluate our method on challenging augmented datasets based on EpicKitchens and Ego4D, demonstrating significant improvements over state-of-the-art methods in terms of action clarity, especially of the hand motion in the target region, across diverse environments and actions. Video results can be found in https://zhicaoisexcited.github.io/project_page","authors":["Yayuan Li","Zhi Cao","Jason J. Corso"],"url":"https://arxiv.org/abs/2412.04189"}
{"created":"2025-04-30","title":"An Overview of Cyber Security Funding for Open Source Software","abstract":"Context: Many open source software (OSS) projects need more human resources for maintenance, improvements, and sometimes even their survival. This need allegedly applies even to vital OSS projects that can be seen as being a part of the world's critical infrastructures. To address this resourcing problem, new funding instruments for OSS projects have been established in recent years. Objectives: The paper examines two such funding bodies for OSS and the projects they have funded. The focus of both funding bodies is on software security and cyber security in general. Methods: The methodology is based on qualitative thematic analysis. Results: Particularly OSS supply chains, network and cryptography libraries, programming languages, and operating systems and their low-level components have been funded and thus seen as critical in terms of cyber security by the two funding bodies. Conclusions: In addition to the qualitative results presented, the paper makes a contribution by connecting the research branches of critical infrastructure and sustainability of OSS projects. A further contribution is made by connecting the topic examined to recent cyber security regulations. Finally, an important argument is raised that neither cyber security nor sustainability alone can entirely explain the rationales behind the funding decisions made by the two bodies.","authors":["Jukka Ruohonen","Gaurav Choudhary","Adam Alami"],"url":"https://arxiv.org/abs/2412.05887"}
{"created":"2025-04-30","title":"SLA Management in Reconfigurable Multi-Agent RAG: A Systems Approach to Question Answering","abstract":"Retrieval Augmented Generation (RAG) enables Large Language Models (LLMs) to generalize to new information by decoupling reasoning capabilities from static knowledge bases. Traditional RAG enhancements have explored vertical scaling-assigning subtasks to specialized modules-and horizontal scaling-replicating tasks across multiple agents-to improve performance. However, real-world applications impose diverse Service Level Agreements (SLAs) and Quality of Service (QoS) requirements, involving trade-offs among objectives such as reducing cost, ensuring answer quality, and adhering to specific operational constraints.","authors":["Michael Iannelli","Sneha Kuchipudi","Vera Dvorak"],"url":"https://arxiv.org/abs/2412.06832"}
{"created":"2025-04-30","title":"Optimizing Personalized Federated Learning through Adaptive Layer-Wise Learning","abstract":"Real-life deployment of federated Learning (FL) often faces non-IID data, which leads to poor accuracy and slow convergence. Personalized FL (pFL) tackles these issues by tailoring local models to individual data sources and using weighted aggregation methods for client-specific learning. However, existing pFL methods often fail to provide each local model with global knowledge on demand while maintaining low computational overhead. Additionally, local models tend to over-personalize their data during the training process, potentially dropping previously acquired global information. We propose FLAYER, a novel layer-wise learning method for pFL that optimizes local model personalization performance. FLAYER considers the different roles and learning abilities of neural network layers of individual local models. It incorporates global information for each local model as needed to initialize the local model cost-effectively. It then dynamically adjusts learning rates for each layer during local training, optimizing the personalized learning process for each local model while preserving global knowledge. Additionally, to enhance global representation in pFL, FLAYER selectively uploads parameters for global aggregation in a layer-wise manner. We evaluate FLAYER on four representative datasets in computer vision and natural language processing domains. Compared to six state-of-the-art pFL methods, FLAYER improves the inference accuracy, on average, by 5.40\\% (up to 14.29\\%).","authors":["Weihang Chen","Cheng Yang","Jie Ren","Zhiqiang Li","Zheng Wang"],"url":"https://arxiv.org/abs/2412.07062"}
{"created":"2025-04-30","title":"Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed GFlowNets","abstract":"While one commonly trains large diffusion models by collecting datasets on target downstream tasks, it is often desired to align and finetune pretrained diffusion models with some reward functions that are either designed by experts or learned from small-scale datasets. Existing post-training methods for reward finetuning of diffusion models typically suffer from lack of diversity in generated samples, lack of prior preservation, and/or slow convergence in finetuning. In response to this challenge, we take inspiration from recent successes in generative flow networks (GFlowNets) and propose a reinforcement learning method for diffusion model finetuning, dubbed Nabla-GFlowNet (abbreviated as $\\nabla$-GFlowNet), that leverages the rich signal in reward gradients for probabilistic diffusion finetuning. We show that our proposed method achieves fast yet diversity- and prior-preserving finetuning of Stable Diffusion, a large-scale text-conditioned image diffusion model, on different realistic reward functions.","authors":["Zhen Liu","Tim Z. Xiao","Weiyang Liu","Yoshua Bengio","Dinghuai Zhang"],"url":"https://arxiv.org/abs/2412.07775"}
{"created":"2025-04-30","title":"A physics-informed transformer neural operator for learning generalized solutions of initial boundary value problems","abstract":"Initial boundary value problems arise commonly in applications with engineering and natural systems governed by nonlinear partial differential equations (PDEs). Operator learning is an emerging field for solving these equations by using a neural network to learn a map between infinite dimensional input and output function spaces. These neural operators are trained using a combination of data (observations or simulations) and PDE-residuals (physics-loss). A major drawback of existing neural approaches is the requirement to retrain with new initial/boundary conditions, and the necessity for a large amount of simulation data for training. We develop a physics-informed transformer neural operator (named PINTO) that efficiently generalizes to unseen initial and boundary conditions, trained in a simulation-free setting using only physics loss. The main innovation lies in our new iterative kernel integral operator units, implemented using cross-attention, to transform the PDE solution's domain points into an initial/boundary condition-aware representation vector, enabling efficient learning of the solution function for new scenarios. The PINTO architecture is applied to simulate the solutions of important equations used in engineering applications: advection, Burgers, and steady and unsteady Navier-Stokes equations (three flow scenarios). For these five test cases, we show that the relative errors during testing under challenging conditions of unseen initial/boundary conditions are only one-fifth to one-third of other leading physics informed operator learning methods. Moreover, our PINTO model is able to accurately solve the advection and Burgers equations at time steps that are not included in the training collocation points. The code is available at https://github.com/quest-lab-iisc/PINTO","authors":["Sumanth Kumar Boya","Deepak Subramani"],"url":"https://arxiv.org/abs/2412.09009"}
{"created":"2025-04-30","title":"Leveraging Memory Retrieval to Enhance LLM-based Generative Recommendation","abstract":"Leveraging Large Language Models (LLMs) to harness user-item interaction histories for item generation has emerged as a promising paradigm in generative recommendation. However, the limited context window of LLMs often restricts them to focusing on recent user interactions only, leading to the neglect of long-term interests involved in the longer histories. To address this challenge, we propose a novel Automatic Memory-Retrieval framework (AutoMR), which is capable of storing long-term interests in the memory and extracting relevant information from it for next-item generation within LLMs. Extensive experimental results on two real-world datasets demonstrate the effectiveness of our proposed AutoMR framework in utilizing long-term interests for generative recommendation.","authors":["Chengbing Wang","Yang Zhang","Fengbin Zhu","Jizhi Zhang","Tianhao Shi","Fuli Feng"],"url":"https://arxiv.org/abs/2412.17593"}
{"created":"2025-04-30","title":"3DEnhancer: Consistent Multi-View Diffusion for 3D Enhancement","abstract":"Despite advances in neural rendering, due to the scarcity of high-quality 3D datasets and the inherent limitations of multi-view diffusion models, view synthesis and 3D model generation are restricted to low resolutions with suboptimal multi-view consistency. In this study, we present a novel 3D enhancement pipeline, dubbed 3DEnhancer, which employs a multi-view latent diffusion model to enhance coarse 3D inputs while preserving multi-view consistency. Our method includes a pose-aware encoder and a diffusion-based denoiser to refine low-quality multi-view images, along with data augmentation and a multi-view attention module with epipolar aggregation to maintain consistent, high-quality 3D outputs across views. Unlike existing video-based approaches, our model supports seamless multi-view enhancement with improved coherence across diverse viewing angles. Extensive evaluations show that 3DEnhancer significantly outperforms existing methods, boosting both multi-view enhancement and per-instance 3D optimization tasks.","authors":["Yihang Luo","Shangchen Zhou","Yushi Lan","Xingang Pan","Chen Change Loy"],"url":"https://arxiv.org/abs/2412.18565"}
{"created":"2025-04-30","title":"Double Spending Analysis of Nakamoto Consensus for Time-Varying Mining Rates with Ruin Theory","abstract":"Theoretical guarantees for double spending probabilities for the Nakamoto consensus under the $k$-deep confirmation rule have been extensively studied for zero/bounded network delays and fixed mining rates. In this paper, we introduce a ruin-theoretical model of double spending for Nakamoto consensus under the $k$-deep confirmation rule when the honest mining rate is allowed to be an arbitrary function of time including the block delivery periods, i.e., time periods during which mined blocks are being delivered to all other participants of the network. Time-varying mining rates are considered to capture the intrinsic characteristics of the peer to peer network delays as well as dynamic participation of miners such as the gap game and switching between different cryptocurrencies. Ruin theory is leveraged to obtain the double spend probabilities and numerical examples are presented to validate the effectiveness of the proposed analytical method.","authors":["Mustafa Doger","Sennur Ulukus","Nail Akar"],"url":"https://arxiv.org/abs/2412.18599"}
{"created":"2025-04-30","title":"AFLNet Five Years Later: On Coverage-Guided Protocol Fuzzing","abstract":"Protocol implementations are stateful which makes them difficult to test: Sending the same test input message twice might yield a different response every time. Our proposal to consider a sequence of messages as a seed for coverage-directed greybox fuzzing, to associate each message with the corresponding protocol state, and to maximize the coverage of both the state space and the code was first published in 2020 in a short tool demonstration paper. AFLNet was the first code- and state-coverage-guided protocol fuzzer; it used the response code as an indicator of the current protocol state. Over the past five years, the tool paper has gathered hundreds of citations, the code repository was forked almost 200 times and has seen over thirty pull requests from practitioners and researchers, and our initial proposal has been improved upon in many significant ways. In this paper, we first provide an extended discussion and a full empirical evaluation of the technical contributions of AFLNet and then reflect on the impact that our approach and our tool had in the past five years, on both the research and the practice of protocol fuzzing.","authors":["Ruijie Meng","Van-Thuan Pham","Marcel B\\\"ohme","Abhik Roychoudhury"],"url":"https://arxiv.org/abs/2412.20324"}
{"created":"2025-04-30","title":"WeAudit: Scaffolding User Auditors and AI Practitioners in Auditing Generative AI","abstract":"There has been growing interest from both practitioners and researchers in engaging end users in AI auditing, to draw upon users' unique knowledge and lived experiences. However, we know little about how to effectively scaffold end users in auditing in ways that can generate actionable insights for AI practitioners. Through formative studies with both users and AI practitioners, we first identified a set of design goals to support user-engaged AI auditing. We then developed WeAudit, a workflow and system that supports end users in auditing AI both individually and collectively. We evaluated WeAudit through a three-week user study with user auditors and interviews with industry Generative AI practitioners. Our findings offer insights into how WeAudit supports users in noticing and reflecting upon potential AI harms and in articulating their findings in ways that industry practitioners can act upon. Based on our observations and feedback from both users and practitioners, we identify several opportunities to better support user engagement in AI auditing processes. We discuss implications for future research to support effective and responsible user engagement in AI auditing and red-teaming.","authors":["Wesley Hanwen Deng","Wang Claire","Howard Ziyu Han","Jason I. Hong","Kenneth Holstein","Motahhare Eslami"],"url":"https://arxiv.org/abs/2501.01397"}
{"created":"2025-04-30","title":"MADGEN: Mass-Spec attends to De Novo Molecular generation","abstract":"The annotation (assigning structural chemical identities) of MS/MS spectra remains a significant challenge due to the enormous molecular diversity in biological samples and the limited scope of reference databases. Currently, the vast majority of spectral measurements remain in the \"dark chemical space\" without structural annotations. To improve annotation, we propose MADGEN (Mass-spec Attends to De Novo Molecular GENeration), a scaffold-based method for de novo molecular structure generation guided by mass spectrometry data. MADGEN operates in two stages: scaffold retrieval and spectra-conditioned molecular generation starting with the scaffold. In the first stage, given an MS/MS spectrum, we formulate scaffold retrieval as a ranking problem and employ contrastive learning to align mass spectra with candidate molecular scaffolds. In the second stage, starting from the retrieved scaffold, we employ the MS/MS spectrum to guide an attention-based generative model to generate the final molecule. Our approach constrains the molecular generation search space, reducing its complexity and improving generation accuracy. We evaluate MADGEN on three datasets (NIST23, CANOPUS, and MassSpecGym) and evaluate MADGEN's performance with a predictive scaffold retriever and with an oracle retriever. We demonstrate the effectiveness of using attention to integrate spectral information throughout the generation process to achieve strong results with the oracle retriever.","authors":["Yinkai Wang","Xiaohui Chen","Liping Liu","Soha Hassoun"],"url":"https://arxiv.org/abs/2501.01950"}
{"created":"2025-04-30","title":"SR-Reward: Taking The Path More Traveled","abstract":"In this paper, we propose a novel method for learning reward functions directly from offline demonstrations. Unlike traditional inverse reinforcement learning (IRL), our approach decouples the reward function from the learner's policy, eliminating the adversarial interaction typically required between the two. This results in a more stable and efficient training process. Our reward function, called \\textit{SR-Reward}, leverages successor representation (SR) to encode a state based on expected future states' visitation under the demonstration policy and transition dynamics. By utilizing the Bellman equation, SR-Reward can be learned concurrently with most reinforcement learning (RL) algorithms without altering the existing training pipeline. We also introduce a negative sampling strategy to mitigate overestimation errors by reducing rewards for out-of-distribution data, thereby enhancing robustness. This strategy inherently introduces a conservative bias into RL algorithms that employ the learned reward. We evaluate our method on the D4RL benchmark, achieving competitive results compared to offline RL algorithms with access to true rewards and imitation learning (IL) techniques like behavioral cloning. Moreover, our ablation studies on data size and quality reveal the advantages and limitations of SR-Reward as a proxy for true rewards.","authors":["Seyed Mahdi B. Azad","Zahra Padar","Gabriel Kalweit","Joschka Boedecker"],"url":"https://arxiv.org/abs/2501.02330"}
{"created":"2025-04-30","title":"Physics-informed deep learning for infectious disease forecasting","abstract":"Accurate forecasting of contagious diseases is critical for public health policymaking and pandemic preparedness. We propose a new infectious disease forecasting model based on physics-informed neural networks (PINNs), an emerging scientific machine learning approach. By embedding a compartmental model into the loss function, our method integrates epidemiological theory with data, helping to prevent model overfitting. We further enhance the model with a sub-network that accounts for covariates such as mobility and cumulative vaccine doses, which influence the transmission rate. Using state-level COVID-19 data from California, we demonstrate that the PINN model accurately predicts cases, deaths, and hospitalizations, aligning well with existing benchmarks. Notably, the PINN model outperforms naive baseline forecasts and several sequence deep learning models, including Recurrent Neural Networks (RNNs), Long Short-Term Memory (LSTM) networks, Gated Recurrent Units (GRUs), and Transformers. It also achieves performance comparable to a sophisticated Gaussian infection state forecasting model that combines compartmental dynamics, a data observation model, and parameter regression. However, the PINN model features a simpler structure and is easier to implement. In summary, we systematically evaluate the PINN model's ability to forecast infectious disease dynamics, demonstrating its potential as an efficient computational tool to strengthen forecasting capabilities.","authors":["Ying Qian","Kui Zhang","\\'Eric Marty","Avranil Basu","Eamon B. O'Dea","Xianqiao Wang","Spencer Fox","Pejman Rohani","John M. Drake","He Li"],"url":"https://arxiv.org/abs/2501.09298"}
{"created":"2025-04-30","title":"Exploring AI-based System Design for Pixel-level Protected Health Information Detection in Medical Images","abstract":"De-identification of medical images is a critical step to ensure privacy during data sharing in research and clinical settings. The initial step in this process involves detecting Protected Health Information (PHI), which can be found in image metadata or imprinted within image pixels. Despite the importance of such systems, there has been limited evaluation of existing AI-based solutions, creating barriers to the development of reliable and robust tools. In this study, we present an AI-based pipeline for PHI detection, comprising three key components: text detection, text extraction, and text analysis. We benchmark three models, YOLOv11, EasyOCR, and GPT-4o, across different setups corresponding to these components, evaluating the performance based on precision, recall, F1 score, and accuracy. All setups demonstrate excellent PHI detection, with all metrics exceeding 0.9. The combination of YOLOv11 for text localization and GPT-4o for extraction and analysis yields the best results. However, this setup incurs higher costs due to GPT-4o's token generation. Conversely, an end-to-end pipeline that relies solely on GPT-4o shows lower performance but highlights the potential of multimodal models for complex tasks. We recommend fine-tuning a dedicated object detection model and utilizing built-in OCR tools to achieve optimal performance and cost-effectiveness. Additionally, leveraging language models such as GPT-4o can facilitate thorough and flexible analysis of text content.","authors":["Tuan Truong","Ivo M. Baltruschat","Mark Klemens","Grit Werner","Matthias Lenga"],"url":"https://arxiv.org/abs/2501.09552"}
{"created":"2025-04-30","title":"Good things come in small packages: Should we build AI clusters with Lite-GPUs?","abstract":"To match the blooming demand of generative AI workloads, GPU designers have so far been trying to pack more and more compute and memory into single complex and expensive packages. However, there is growing uncertainty about the scalability of individual GPUs and thus AI clusters, as state-of-the-art GPUs are already displaying packaging, yield, and cooling limitations. We propose to rethink the design and scaling of AI clusters through efficiently-connected large clusters of Lite-GPUs, GPUs with single, small dies and a fraction of the capabilities of larger GPUs. We think recent advances in co-packaged optics can enable distributing AI workloads onto many Lite-GPUs through high bandwidth and efficient communication. In this paper, we present the key benefits of Lite-GPUs on manufacturing cost, blast radius, yield, and power efficiency; and discuss systems opportunities and challenges around resource, workload, memory, and network management.","authors":["Burcu Canakci","Junyi Liu","Xingbo Wu","Nathana\\\"el Cheriere","Paolo Costa","Sergey Legtchenko","Dushyanth Narayanan","Ant Rowstron"],"url":"https://arxiv.org/abs/2501.10187"}
{"created":"2025-04-30","title":"Synthesising Activity Participations and Scheduling with Deep Generative Machine Learning","abstract":"Using a deep generative machine learning approach, we synthesise human activity participations and scheduling (the choices of what activities to participate in and when). Activity schedules, which represent what people do and when, are a core component of many applied transport, energy, and epidemiology models. Our data-driven approach learns the distributions resulting from human preferences and scheduling logic without the need for complex interacting combinations of sub-models and custom rules, This makes our approach significantly faster and simpler to operate than existing approaches to synthesise or anonymise schedule data. We additionally contribute a novel schedule representation and a comprehensive evaluation framework. We evaluate a range of schedule encoding and deep model architecture combinations. The evaluation shows our approach can rapidly generate large, diverse, novel, and realistic synthetic samples of activity schedules.","authors":["Fred Shone","Tim Hillel"],"url":"https://arxiv.org/abs/2501.10221"}
{"created":"2025-04-30","title":"Robust Joint Message and State Transmission under Arbitrarily Varying Jamming","abstract":"Joint message and state transmission under arbitrarily varying jamming is investigated in this paper. The problem is modeled as the transmission over a channel with random states with a fixed distribution and jamming that varies in an unknown manner. We provide lower bounds of the capacity-distortion function of strictly causal and noncausal observations of the states at the encoder under the average error criterion when the jammer is not aware of the transmitted message, as well as the maximal error criterion when the jammer knows the message. Some capacity-achieving cases are also provided. The proposed coding schemes are deterministic, and no randomness is needed to achieve reliable communication and estimation. It turns out that the performance of the system under the average error can strictly outperform the maximal error case, which is in accordance with normal communication over arbitrarily varying channels.","authors":["Yiqi Chen","Holger Boche"],"url":"https://arxiv.org/abs/2501.10896"}
{"created":"2025-04-30","title":"Open FinLLM Leaderboard: Towards Financial AI Readiness","abstract":"Financial large language models (FinLLMs) with multimodal capabilities are envisioned to revolutionize applications across business, finance, accounting, and auditing. However, real-world adoption requires robust benchmarks of FinLLMs' and FinAgents' performance. Maintaining an open leaderboard is crucial for encouraging innovative adoption and improving model effectiveness. In collaboration with Linux Foundation and Hugging Face, we create an open FinLLM leaderboard, which serves as an open platform for assessing and comparing AI models' performance on a wide spectrum of financial tasks. By demoncratizing access to advances of financial knowledge and intelligence, a chatbot or agent may enhance the analytical capabilities of the general public to a professional level within a few months of usage. This open leaderboard welcomes contributions from academia, open-source community, industry, and stakeholders. In particular, we encourage contributions of new datasets, tasks, and models for continual update. Through fostering a collaborative and open ecosystem, we seek to promote financial AI readiness.","authors":["Shengyuan Colin Lin","Felix Tian","Keyi Wang","Xingjian Zhao","Jimin Huang","Qianqian Xie","Luca Borella","Matt White","Christina Dan Wang","Kairong Xiao","Xiao-Yang Liu Yanglet","Li Deng"],"url":"https://arxiv.org/abs/2501.10963"}
{"created":"2025-04-30","title":"Efficient Frame Extraction: A Novel Approach Through Frame Similarity and Surgical Tool Tracking for Video Segmentation","abstract":"The interest in leveraging Artificial Intelligence (AI) for surgical procedures to automate analysis has witnessed a significant surge in recent years. One of the primary tools for recording surgical procedures and conducting subsequent analyses, such as performance assessment, is through videos. However, these operative videos tend to be notably lengthy compared to other fields, spanning from thirty minutes to several hours, which poses a challenge for AI models to effectively learn from them. Despite this challenge, the foreseeable increase in the volume of such videos in the near future necessitates the development and implementation of innovative techniques to tackle this issue effectively. In this article, we propose a novel technique called Kinematics Adaptive Frame Recognition (KAFR) that can efficiently eliminate redundant frames to reduce dataset size and computation time while retaining useful frames to improve accuracy. Specifically, we compute the similarity between consecutive frames by tracking the movement of surgical tools. Our approach follows these steps: $i)$ Tracking phase: a YOLOv8 model is utilized to detect tools presented in the scene, $ii)$ Similarity phase: Similarities between consecutive frames are computed by estimating variation in the spatial positions and velocities of the tools, $iii$) Classification phase: An X3D CNN is trained to classify segmentation. We evaluate the effectiveness of our approach by analyzing datasets obtained through retrospective reviews of cases at two referral centers. The newly annotated Gastrojejunostomy (GJ) dataset covers procedures performed between 2017 and 2021, while the previously annotated Pancreaticojejunostomy (PJ) dataset spans from 2011 to 2022 at the same centers.","authors":["Huu Phong Nguyen","Shekhar Madhav Khairnar","Sofia Garces Palacios","Amr Al-Abbas","Melissa E. Hogg","Amer H. Zureikat","Patricio M. Polanco","Herbert Zeh III","Ganesh Sankaranarayanan"],"url":"https://arxiv.org/abs/2501.11153"}
{"created":"2025-04-30","title":"An Achievable Scheme for the K-user Linear Computation Broadcast Channel","abstract":"This paper presents a new achievable scheme for the K-user Linear Computation Broadcast Channel (K-LCBC). A K-LCBC comprises data stored on a server and K users, each aiming to retrieve a desired linear function of the data by leveraging their prior locally available side information in the form of another linear function of the data. The proposed scheme is based on a subspace decomposition derived from representable polymatroid spaces. This decomposition enables the server to effectively design multicast messages that simultaneously benefit multiple users and allow users to eliminate interference using their available side information. This work extends existing results for the 3-LCBC by introducing a linear programming framework to optimize multicast opportunities across an arbitrary number of users. The proposed approach can be used to derive achievable scheme for the K-user coded caching problem with linear coded placement and scalar linear function retrieval, which was our original motivation to investigate the K-LCBC.","authors":["Yinbin Ma","Daniela Tuninetti"],"url":"https://arxiv.org/abs/2501.12322"}
{"created":"2025-04-30","title":"Test-time regression: a unifying framework for designing sequence models with associative memory","abstract":"Sequence models lie at the heart of modern deep learning. However, rapid advancements have produced a diversity of seemingly unrelated architectures, such as Transformers and recurrent alternatives. In this paper, we introduce a unifying framework to understand and derive these sequence models, inspired by the empirical importance of associative recall, the capability to retrieve contextually relevant tokens. We formalize associative recall as a two-step process, memorization and retrieval, casting memorization as a regression problem. Layers that combine these two steps perform associative recall via ``test-time regression'' over its input tokens. Prominent layers, including linear attention, state-space models, fast-weight programmers, online learners, and softmax attention, arise as special cases defined by three design choices: the regression weights, the regressor function class, and the test-time optimization algorithm. Our approach clarifies how linear attention fails to capture inter-token correlations and offers a mathematical justification for the empirical effectiveness of query-key normalization in softmax attention. Further, it illuminates unexplored regions within the design space, which we use to derive novel higher-order generalizations of softmax attention. Beyond unification, our work bridges sequence modeling with classic regression methods, a field with extensive literature, paving the way for developing more powerful and theoretically principled architectures.","authors":["Ke Alexander Wang","Jiaxin Shi","Emily B. Fox"],"url":"https://arxiv.org/abs/2501.12352"}
{"created":"2025-04-30","title":"Owls are wise and foxes are unfaithful: Uncovering animal stereotypes in vision-language models","abstract":"Animal stereotypes are deeply embedded in human culture and language. They often shape our perceptions and expectations of various species. Our study investigates how animal stereotypes manifest in vision-language models during the task of image generation. Through targeted prompts, we explore whether DALL-E perpetuates stereotypical representations of animals, such as \"owls as wise,\" \"foxes as unfaithful,\" etc. Our findings reveal significant stereotyped instances where the model consistently generates images aligned with cultural biases. The current work is the first of its kind to examine animal stereotyping in vision-language models systematically and to highlight a critical yet underexplored dimension of bias in AI-generated visual content.","authors":["Tabinda Aman","Mohammad Nadeem","Shahab Saquib Sohail","Mohammad Anas","Erik Cambria"],"url":"https://arxiv.org/abs/2501.12433"}
{"created":"2025-04-30","title":"A quantitative comparison of high-order asymptotic-preserving and asymptotically-accurate IMEX methods for the Euler equations with non-ideal gases","abstract":"We present a quantitative comparison between two different Implicit-Explicit Runge-Kutta (IMEX-RK) approaches for the Euler equations of gas dynamics, specifically tailored for the low Mach limit. In this regime, a classical IMEX-RK approach involves an implicit coupling between the momentum and energy balance so as to avoid the acoustic CFL restriction, while the density can be treated in a fully explicit fashion. This approach leads to a mildly nonlinear equation for the pressure, which can be solved according to a fixed point procedure. An alternative strategy consists of employing a semi-implicit temporal integrator based on IMEX-RK methods (SI-IMEX-RK). The stiff dependence is carefully analyzed, so as to avoid the solution of a nonlinear equation for the pressure also for equations of state (EOS) of non-ideal gases. The spatial discretization is based on a Discontinuous Galerkin (DG) method, which naturally allows high-order accuracy. The asymptotic-preserving (AP) and the asymptotically-accurate (AA) properties of the two approaches are assessed on a number of classical benchmarks for ideal gases and on their extension to non-ideal gases.","authors":["Giuseppe Orlando","Sebastiano Boscarino","Giovanni Russo"],"url":"https://arxiv.org/abs/2501.12733"}
{"created":"2025-04-30","title":"On entropy-constrained Gaussian channel capacity via the moment problem","abstract":"We study the capacity of the power-constrained additive Gaussian channel with an entropy constraint at the input. In particular, we characterize this capacity in the low signal-to-noise ratio regime at small entropy. This follows as a corollary of the following general result on a moment matching problem: We show that for any continuous random variable with finite moments, the largest number of initial moments that can be matched by a discrete random variable of sufficiently small but positive entropy is three.","authors":["Adway Girish","Shlomo Shamai","Emre Telatar"],"url":"https://arxiv.org/abs/2501.13814"}
{"created":"2025-04-30","title":"An Empirical Study on Decision-Making Aspects in Responsible Software Engineering for AI","abstract":"Incorporating responsible practices into software engineering (SE) for AI is essential to ensure ethical principles, societal impact, and accountability remain at the forefront of AI system design and deployment. This study investigates the ethical challenges and complexities inherent in responsible software engineering (RSE) for AI, underscoring the need for practical,scenario-driven operational guidelines. Given the complexity of AI and the relative inexperience of professionals in this rapidly evolving field, continuous learning and market adaptation are crucial. Through qualitative interviews with seven practitioners(conducted until saturation), quantitative surveys of 51 practitioners, and static validation of results with four industry experts in AI, this study explores how personal values, emerging roles, and awareness of AIs societal impact influence responsible decision-making in RSE for AI. A key finding is the gap between the current state of the art and actual practice in RSE for AI, particularly in the failure to operationalize ethical and responsible decision-making within the software engineering life cycle for AI. While ethical issues in RSE for AI largely mirror those found in broader SE process, the study highlights a distinct lack of operational frameworks and resources to guide RSE practices for AI effectively. The results reveal that current ethical guidelines are insufficiently implemented at the operational level, reinforcing the complexity of embedding ethics throughout the software engineering life cycle. The study concludes that interdisciplinary collaboration, H-shaped competencies(Ethical-Technical dual competence), and a strong organizational culture of ethics are critical for fostering RSE practices for AI, with a particular focus on transparency and accountability.","authors":["Lekshmi Murali Rani","Faezeh Mohammadi","Robert Feldt","Richard Berntsson Svensson"],"url":"https://arxiv.org/abs/2501.15691"}
{"created":"2025-04-30","title":"From tools to thieves: Measuring and understanding public perceptions of AI through crowdsourced metaphors","abstract":"How has the public responded to the increasing prevalence of artificial intelligence (AI)-based technologies? We investigate public perceptions of AI by collecting over 12,000 responses over 12 months from a nationally representative U.S. sample. Participants provided open-ended metaphors reflecting their mental models of AI, a methodology that overcomes the limitations of traditional self-reported measures by capturing more nuance. Using a mixed-methods approach combining quantitative clustering and qualitative coding, we identify 20 dominant metaphors shaping public understanding of AI. To analyze these metaphors systematically, we present a scalable framework integrating language modeling (LM)-based techniques to measure key dimensions of public perception: anthropomorphism (attribution of human-like qualities), warmth, and competence. We find that Americans generally view AI as warm and competent, and that over the past year, perceptions of AI's human-likeness and warmth have significantly increased ($+34\\%, r = 0.80, p < 0.01; +41\\%, r = 0.62, p < 0.05$). These implicit perceptions, along with the identified dominant metaphors, strongly predict trust in and willingness to adopt AI ($r^2 = 0.21, 0.18, p < 0.001$). Moreover, we uncover systematic demographic differences in metaphors and implicit perceptions, such as the higher propensity of women, older individuals, and people of color to anthropomorphize AI, which shed light on demographic disparities in trust and adoption. In addition to our dataset and framework for tracking evolving public attitudes, we provide actionable insights on using metaphors for inclusive and responsible AI development.","authors":["Myra Cheng","Angela Y. Lee","Kristina Rapuano","Kate Niederhoffer","Alex Liebscher","Jeffrey Hancock"],"url":"https://arxiv.org/abs/2501.18045"}
{"created":"2025-04-30","title":"Radiance Surfaces: Optimizing Surface Representations with a 5D Radiance Field Loss","abstract":"We present a fast and simple technique to convert images into a radiance surface-based scene representation. Building on existing radiance volume reconstruction algorithms, we introduce a subtle yet impactful modification of the loss function requiring changes to only a few lines of code: instead of integrating the radiance field along rays and supervising the resulting images, we project the training images into the scene to directly supervise the spatio-directional radiance field.","authors":["Ziyi Zhang","Nicolas Roussel","Thomas M\\\"uller","Tizian Zeltner","Merlin Nimier-David","Fabrice Rousselle","Wenzel Jakob"],"url":"https://arxiv.org/abs/2501.18627"}
{"created":"2025-04-30","title":"A Survey on Class-Agnostic Counting: Advancements from Reference-Based to Open-World Text-Guided Approaches","abstract":"Visual object counting has recently shifted towards class-agnostic counting (CAC), which addresses the challenge of counting objects across arbitrary categories -- a crucial capability for flexible and generalizable counting systems. Unlike humans, who effortlessly identify and count objects from diverse categories without prior knowledge, most existing counting methods are restricted to enumerating instances of known classes, requiring extensive labeled datasets for training and struggling in open-vocabulary settings. In contrast, CAC aims to count objects belonging to classes never seen during training, operating in a few-shot setting. In this paper, we present the first comprehensive review of CAC methodologies. We propose a taxonomy to categorize CAC approaches into three paradigms based on how target object classes can be specified: reference-based, reference-less, and open-world text-guided. Reference-based approaches achieve state-of-the-art performance by relying on exemplar-guided mechanisms. Reference-less methods eliminate exemplar dependency by leveraging inherent image patterns. Finally, open-world text-guided methods use vision-language models, enabling object class descriptions via textual prompts, offering a flexible and promising solution. Based on this taxonomy, we provide an overview of the architectures of 29 CAC approaches and report their results on gold-standard benchmarks. We compare their performance and discuss their strengths and limitations. Specifically, we present results on the FSC-147 dataset, setting a leaderboard using gold-standard metrics, and on the CARPK dataset to assess generalization capabilities. Finally, we offer a critical discussion of persistent challenges, such as annotation dependency and generalization, alongside future directions. We believe this survey will be a valuable resource, showcasing CAC advancements and guiding future research.","authors":["Luca Ciampi","Ali Azmoudeh","Elif Ecem Akbaba","Erdi Sar{\\i}ta\\c{s}","Ziya Ata Yaz{\\i}c{\\i}","Haz{\\i}m Kemal Ekenel","Giuseppe Amato","Fabrizio Falchi"],"url":"https://arxiv.org/abs/2501.19184"}
{"created":"2025-04-30","title":"Model non-collapse: Minimax bounds for recursive discrete distribution estimation","abstract":"Learning discrete distributions from i.i.d. samples is a well-understood problem. However, advances in generative machine learning prompt an interesting new, non-i.i.d. setting: after receiving a certain number of samples, an estimated distribution is fixed, and samples from this estimate are drawn and introduced into the sample corpus, undifferentiated from real samples. Subsequent generations of estimators now face contaminated environments, a scenario referred to in the machine learning literature as self-consumption. Empirically, it has been observed that models in fully synthetic self-consuming loops collapse -- their performance deteriorates with each batch of training -- but accumulating data has been shown to prevent complete degeneration. This, in turn, begs the question: What happens when fresh real samples \\textit{are} added at every stage? In this paper, we study the minimax loss of self-consuming discrete distribution estimation in such loops. We show that even when model collapse is consciously averted, the ratios between the minimax losses with and without source information can grow unbounded as the batch size increases. In the data accumulation setting, where all batches of samples are available for estimation, we provide minimax lower bounds and upper bounds that are order-optimal under mild conditions for the expected $\\ell_2^2$ and $\\ell_1$ losses at every stage. We provide conditions for regimes where there is a strict gap in the convergence rates compared to the corresponding oracle-assisted minimax loss where real and synthetic samples are differentiated, and provide examples where this gap is easily observed. We also provide a lower bound on the minimax loss in the data replacement setting, where only the latest batch of samples is available, and use it to find a lower bound for the worst-case loss for bounded estimate trajectories.","authors":["Millen Kanabar","Michael Gastpar"],"url":"https://arxiv.org/abs/2501.19273"}
{"created":"2025-04-30","title":"Mobile Robot Navigation Using Hand-Drawn Maps: A Vision Language Model Approach","abstract":"Hand-drawn maps can be used to convey navigation instructions between humans and robots in a natural and efficient manner. However, these maps can often contain inaccuracies such as scale distortions and missing landmarks which present challenges for mobile robot navigation. This paper introduces a novel Hand-drawn Map Navigation (HAM-Nav) architecture that leverages pre-trained vision language models (VLMs) for robot navigation across diverse environments, hand-drawing styles, and robot embodiments, even in the presence of map inaccuracies. HAM-Nav integrates a unique Selective Visual Association Prompting approach for topological map-based position estimation and navigation planning as well as a Predictive Navigation Plan Parser to infer missing landmarks. Extensive experiments were conducted in photorealistic simulated environments, using both wheeled and legged robots, demonstrating the effectiveness of HAM-Nav in terms of navigation success rates and Success weighted by Path Length. Furthermore, a user study in real-world environments highlighted the practical utility of hand-drawn maps for robot navigation as well as successful navigation outcomes compared against a non-hand-drawn map approach.","authors":["Aaron Hao Tan","Angus Fung","Haitong Wang","Goldie Nejat"],"url":"https://arxiv.org/abs/2502.00114"}
{"created":"2025-04-30","title":"PAC Learning is just Bipartite Matching (Sort of)","abstract":"The main goal of this article is to convince you, the reader, that supervised learning in the Probably Approximately Correct (PAC) model is closely related to -- of all things -- bipartite matching! En-route from PAC learning to bipartite matching, I will overview a particular transductive model of learning, and associated one-inclusion graphs, which can be viewed as a generalization of some of the hat puzzles that are popular in recreational mathematics. Whereas this transductive model is far from new, it has recently seen a resurgence of interest as a tool for tackling deep questions in learning theory. A secondary purpose of this article could be as a (biased) tutorial on the connections between the PAC and transductive models of learning.","authors":["Shaddin Dughmi"],"url":"https://arxiv.org/abs/2502.00607"}
{"created":"2025-04-30","title":"RankFlow: A Multi-Role Collaborative Reranking Workflow Utilizing Large Language Models","abstract":"In an Information Retrieval (IR) system, reranking plays a critical role by sorting candidate passages according to their relevance to a specific query. This process demands a nuanced understanding of the variations among passages linked to the query. In this work, we introduce RankFlow, a multi-role reranking workflow that leverages the capabilities of Large Language Models (LLMs) and role specializations to improve reranking performance. RankFlow enlists LLMs to fulfill four distinct roles: the query Rewriter, the pseudo Answerer, the passage Summarizer, and the Reranker. This orchestrated approach enables RankFlow to: (1) accurately interpret queries, (2) draw upon LLMs' extensive pre-existing knowledge, (3) distill passages into concise versions, and (4) assess passages in a comprehensive manner, resulting in notably better reranking results. Our experimental results reveal that RankFlow outperforms existing leading approaches on widely recognized IR benchmarks, such as TREC-DL, BEIR, and NovelEval. Additionally, we investigate the individual contributions of each role in RankFlow.","authors":["Can Jin","Hongwu Peng","Anxiang Zhang","Nuo Chen","Jiahui Zhao","Xi Xie","Kuangzheng Li","Shuya Feng","Kai Zhong","Caiwen Ding","Dimitris N. Metaxas"],"url":"https://arxiv.org/abs/2502.00709"}
{"created":"2025-04-30","title":"An Inquiry into Datacenter TCO for LLM Inference with FP8","abstract":"As large language models (LLMs) continue to scale, their inference demands present significant challenges, particularly due to the high power consumption of AI accelerators in datacenters. These facilities require specialized cooling and power management systems, substantially increasing the total cost of ownership (TCO) for cloud service providers (CSPs). In this work, we analyze the computational characteristics and constraints of LLM inference from a TCO perspective, focusing on two representative accelerators: the Gaudi 2 and NVIDIA H100. We present a generalizable framework that enables CSPs to compare and select AI accelerators according to diverse operational requirements. Using this model, we analyze the impact of FP8 precision and LLM inference workload characteristics as key factors influencing TCO. We investigate FP8 quantization, which is gaining adoption in LLM training, as a technique to improve inference throughput while maintaining cost efficiency. Furthermore, our analysis of LLM inference workloads reveals that performance on thin GEMMs, which dominate the decode phase, can have a greater impact than theoretical hardware peak performance. By studying the interaction between power consumption, quantization strategies, and hardware architecture, we offer insights that support informed deployment decisions and guide future accelerator designs to improve the TCO of LLM inference.","authors":["Jiwoo Kim","Joonhyung Lee","Gunho Park","Byeongwook Kim","Se Jung Kwon","Dongsoo Lee","Youngjoo Lee"],"url":"https://arxiv.org/abs/2502.01070"}
{"created":"2025-04-30","title":"The Nonlinear Filter Model of Stream Cipher Redivivus","abstract":"The nonlinear filter model is an old and well understood approach to the design of secure stream ciphers. Extensive research over several decades has shown how to attack stream ciphers based on this model and has identified the security properties required of the Boolean function used as the filtering function to resist such attacks. This led to the problem of constructing Boolean functions which provide adequate security \\textit{and} at the same time are efficient to implement. Unfortunately, over the last two decades no good solutions to this problem appeared in the literature. The lack of good solutions has effectively led to nonlinear filter model becoming more or less obsolete. This is a big loss to the cryptographic design toolkit, since the great advantages of the nonlinear filter model are its simplicity, well understood security and the potential to provide low cost solutions for hardware oriented stream ciphers. In this paper, we revive the nonlinear filter model by constructing appropriate Boolean functions which provide required security and are also efficient to implement. We put forward concrete suggestions of stream ciphers which are $\\kappa$-bit secure against known types of attacks for $\\kappa=80,128,160,192,224$ and $256$. For the $80$-bit, $128$-bit, and the $256$-bit security levels, the circuits for the corresponding stream ciphers require about 1743.5, 2771.5, and 5607.5 NAND gates respectively. For the $80$-bit and the $128$-bit security levels, the gate count estimates compare quite well to the famous ciphers Trivium and Grain-128a respectively, while for the $256$-bit security level, we do not know of any other stream cipher design which has such a low gate count.","authors":["Claude Carlet","Palash Sarkar"],"url":"https://arxiv.org/abs/2502.01110"}
{"created":"2025-04-30","title":"Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding","abstract":"Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model's parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. The Code is Available at https://github.com/MingyuJ666/Rope_with_LLM.","authors":["Mingyu Jin","Kai Mei","Wujiang Xu","Mingjie Sun","Ruixiang Tang","Mengnan Du","Zirui Liu","Yongfeng Zhang"],"url":"https://arxiv.org/abs/2502.01563"}
{"created":"2025-04-30","title":"Deep Learning-Based Approach for Identification of Potato Leaf Diseases Using Wrapper Feature Selection and Feature Concatenation","abstract":"The potato is a widely grown crop in many regions of the world. In recent decades, potato farming has gained incredible traction in the world. Potatoes are susceptible to several illnesses that stunt their development. This plant seems to have significant leaf disease. Early Blight and Late Blight are two prevalent leaf diseases that affect potato plants. The early detection of these diseases would be beneficial for enhancing the yield of this crop. The ideal solution is to use image processing to identify and analyze these disorders. Here, we present an autonomous method based on image processing and machine learning to detect late blight disease affecting potato leaves. The proposed method comprises four different phases: (1) Histogram Equalization is used to improve the quality of the input image; (2) feature extraction is performed using a Deep CNN model, then these extracted features are concatenated; (3) feature selection is performed using wrapper-based feature selection; (4) classification is performed using an SVM classifier and its variants. This proposed method achieves the highest accuracy of 99% using SVM by selecting 550 features.","authors":["Muhammad Ahtsam Naeem","Muhammad Asim Saleem","Muhammad Imran Sharif","Shahzad Akber","Sajjad Saleem","Zahid Akhtar","Kamran Siddique"],"url":"https://arxiv.org/abs/2502.03370"}
{"created":"2025-04-30","title":"REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations","abstract":"Existing image editing models struggle to meet real-world demands. Despite excelling in academic benchmarks, they have yet to be widely adopted for real user needs. Datasets that power these models use artificial edits, lacking the scale and ecological validity necessary to address the true diversity of user requests. We introduce REALEDIT, a large-scale image editing dataset with authentic user requests and human-made edits sourced from Reddit. REALEDIT includes a test set of 9300 examples to evaluate models on real user requests. Our results show that existing models fall short on these tasks, highlighting the need for realistic training data. To address this, we introduce 48K training examples and train our REALEDIT model, achieving substantial gains - outperforming competitors by up to 165 Elo points in human judgment and 92 percent relative improvement on the automated VIEScore metric. We deploy our model on Reddit, testing it on new requests, and receive positive feedback. Beyond image editing, we explore REALEDIT's potential in detecting edited images by partnering with a deepfake detection non-profit. Finetuning their model on REALEDIT data improves its F1-score by 14 percentage points, underscoring the dataset's value for broad applications.","authors":["Peter Sushko","Ayana Bharadwaj","Zhi Yang Lim","Vasily Ilin","Ben Caffee","Dongping Chen","Mohammadreza Salehi","Cheng-Yu Hsieh","Ranjay Krishna"],"url":"https://arxiv.org/abs/2502.03629"}
{"created":"2025-04-30","title":"EgoAgent: A Joint Predictive Agent Model in Egocentric Worlds","abstract":"This paper addresses the task of learning an agent model behaving like humans, which can jointly perceive, predict, and act in egocentric worlds. Previous methods usually train separate models for these three abilities, which prevents them from learning from each other. In this paper, we propose a joint predictive agent model, named EgoAgent, that simultaneously learns to represent the world, predict future states, and take reasonable actions within a single transformer. EgoAgent introduces two innovations to learn from the causal and temporally intertwined nature of these abilities: (1) Interleaved sequential modeling of states and actions with the causal attention mechanism, and (2) A joint embedding-action-prediction architecture featuring temporal asymmetric predictor-observer branches. Integrating these designs based on JEPA, EgoAgent unifies these capabilities in a cohesive learning framework. Comprehensive evaluations of EgoAgent on representative tasks such as image classification, egocentric future state prediction, and 3D human motion prediction tasks demonstrate the superiority of our method. The code and trained model will be released for reproducibility.","authors":["Lu Chen","Yizhou Wang","Shixiang Tang","Qianhong Ma","Tong He","Wanli Ouyang","Xiaowei Zhou","Hujun Bao","Sida Peng"],"url":"https://arxiv.org/abs/2502.05857"}
{"created":"2025-04-30","title":"LoCA: Location-Aware Cosine Adaptation for Parameter-Efficient Fine-Tuning","abstract":"Low-rank adaptation (LoRA) has become a prevalent method for adapting pre-trained large language models to downstream tasks. However, the simple low-rank decomposition form may constrain the hypothesis space. To address this limitation, we introduce Location-aware Cosine Adaptation (LoCA), a novel frequency-domain parameter-efficient fine-tuning method based on inverse Discrete Cosine Transform (iDCT) with selective locations of learnable components. We begin with a comprehensive theoretical comparison between frequency-domain and low-rank decompositions for fine-tuning pre-trained large models. Our analysis reveals that frequency-domain decomposition with carefully selected frequency components can surpass the expressivity of traditional low-rank-based methods. Furthermore, we demonstrate that iDCT offers a more efficient implementation compared to inverse Discrete Fourier Transform (iDFT), allowing for better selection and tuning of frequency components while maintaining equivalent expressivity to the optimal iDFT-based adaptation. By employing finite-difference approximation to estimate gradients for discrete locations of learnable coefficients on the DCT spectrum, LoCA dynamically selects the most informative frequency components during training. Experiments on diverse language and vision fine-tuning tasks demonstrate that LoCA offers enhanced parameter efficiency while maintains computational feasibility comparable to low-rank-based methods.","authors":["Zhekai Du","Yinjie Min","Jingjing Li","Ke Lu","Changliang Zou","Liuhua Peng","Tingjin Chu","Mingming Gong"],"url":"https://arxiv.org/abs/2502.06820"}
{"created":"2025-04-30","title":"Learnable Residual-based Latent Denoising in Semantic Communication","abstract":"A latent denoising semantic communication (SemCom) framework is proposed for robust image transmission over noisy channels. By incorporating a learnable latent denoiser into the receiver, the received signals are preprocessed to effectively remove the channel noise and recover the semantic information, thereby enhancing the quality of the decoded images. Specifically, a latent denoising mapping is established by an iterative residual learning approach to improve the denoising efficiency while ensuring stable performance. Moreover, channel signal-to-noise ratio (SNR) is utilized to estimate and predict the latent similarity score (SS) for conditional denoising, where the number of denoising steps is adapted based on the predicted SS sequence, further reducing the communication latency. Finally, simulations demonstrate that the proposed framework can effectively and efficiently remove the channel noise at various levels and reconstruct visual-appealing images.","authors":["Mingkai Xu","Yongpeng Wu","Yuxuan Shi","Xiang-Gen Xia","Wenjun Zhang","Ping Zhang"],"url":"https://arxiv.org/abs/2502.07319"}
{"created":"2025-04-30","title":"Time2Lang: Bridging Time-Series Foundation Models and Large Language Models for Health Sensing Beyond Prompting","abstract":"Large language models (LLMs) show promise for health applications when combined with behavioral sensing data. Traditional approaches convert sensor data into text prompts, but this process is prone to errors, computationally expensive, and requires domain expertise. These challenges are particularly acute when processing extended time series data. While time series foundation models (TFMs) have recently emerged as powerful tools for learning representations from temporal data, bridging TFMs and LLMs remains challenging. Here, we present Time2Lang, a framework that directly maps TFM outputs to LLM representations without intermediate text conversion. Our approach first trains on synthetic data using periodicity prediction as a pretext task, followed by evaluation on mental health classification tasks. We validate Time2Lang on two longitudinal wearable and mobile sensing datasets: daily depression prediction using step count data (17,251 days from 256 participants) and flourishing classification based on conversation duration (46 participants over 10 weeks). Time2Lang maintains near constant inference times regardless of input length, unlike traditional prompting methods. The generated embeddings preserve essential time-series characteristics such as auto-correlation. Our results demonstrate that TFMs and LLMs can be effectively integrated while minimizing information loss and enabling performance transfer across these distinct modeling paradigms. To our knowledge, we are the first to integrate a TFM and an LLM for health, thus establishing a foundation for future research combining general-purpose large models for complex healthcare tasks.","authors":["Arvind Pillai","Dimitris Spathis","Subigya Nepal","Amanda C Collins","Daniel M Mackin","Michael V Heinz","Tess Z Griffin","Nicholas C Jacobson","Andrew Campbell"],"url":"https://arxiv.org/abs/2502.07608"}
{"created":"2025-04-30","title":"Towards Principled Multi-Agent Task Agnostic Exploration","abstract":"In reinforcement learning, we typically refer to task-agnostic exploration when we aim to explore the environment without access to the task specification a priori. In a single-agent setting the problem has been extensively studied and mostly understood. A popular approach cast the task-agnostic objective as maximizing the entropy of the state distribution induced by the agent's policy, from which principles and methods follows. In contrast, little is known about task-agnostic exploration in multi-agent settings, which are ubiquitous in the real world. How should different agents explore in the presence of others? In this paper, we address this question through a generalization to multiple agents of the problem of maximizing the state distribution entropy. First, we investigate alternative formulations, highlighting respective positives and negatives. Then, we present a scalable, decentralized, trust-region policy search algorithm to address the problem in practical settings. Finally, we provide proof of concept experiments to both corroborate the theoretical findings and pave the way for task-agnostic exploration in challenging multi-agent settings.","authors":["Riccardo Zamboni","Mirco Mutti","Marcello Restelli"],"url":"https://arxiv.org/abs/2502.08365"}
{"created":"2025-04-30","title":"Who is Responsible? The Data, Models, Users or Regulations? A Comprehensive Survey on Responsible Generative AI for a Sustainable Future","abstract":"Responsible Artificial Intelligence (RAI) has emerged as a crucial framework for addressing ethical concerns in the development and deployment of Artificial Intelligence (AI) systems. A significant body of literature exists, primarily focusing on either RAI guidelines and principles or the technical aspects of RAI, largely within the realm of traditional AI. However, a notable gap persists in bridging theoretical frameworks with practical implementations in real-world settings, as well as transitioning from RAI to Responsible Generative AI (Gen AI). To bridge this gap, we present this article, which examines the challenges and opportunities in implementing ethical, transparent, and accountable AI systems in the post-ChatGPT era, an era significantly shaped by Gen AI. Our analysis includes governance and technical frameworks, the exploration of explainable AI as the backbone to achieve RAI, key performance indicators in RAI, alignment of Gen AI benchmarks with governance frameworks, reviews of AI-ready test beds, and RAI applications across multiple sectors. Additionally, we discuss challenges in RAI implementation and provide a philosophical perspective on the future of RAI. This comprehensive article aims to offer an overview of RAI, providing valuable insights for researchers, policymakers, users, and industry practitioners to develop and deploy AI systems that benefit individuals and society while minimizing potential risks and societal impacts. A curated list of resources and datasets covered in this survey is available on GitHub {https://github.com/anas-zafar/Responsible-AI}.","authors":["Shaina Raza","Rizwan Qureshi","Anam Zahid","Joseph Fioresi","Ferhat Sadak","Muhammad Saeed","Ranjan Sapkota","Aditya Jain","Anas Zafar","Muneeb Ul Hassan","Aizan Zafar","Hasan Maqbool","Ashmal Vayani","Jia Wu","Maged Shoman"],"url":"https://arxiv.org/abs/2502.08650"}
{"created":"2025-04-30","title":"FuncGenFoil: Airfoil Generation and Editing Model in Function Space","abstract":"Aircraft manufacturing is the jewel in the crown of industry, among which generating high-fidelity airfoil geometries with controllable and editable representations remains a fundamental challenge. While existing deep-learning-based methods rely on predefined parametric function families, e.g., B\\'ezier curves and discrete point-based representations, they suffer from inherent trade-offs between expressiveness and resolution flexibility. To tackle this challenge, we introduce FuncGenFoil, a novel function-space generative model that directly learns functional airfoil geometries. Our method inherits both the advantages of arbitrary resolution sampling and the smoothness of parametric functions, as well as the strong expressiveness of discrete point-based functions. Empirical evaluations on the AFBench dataset demonstrate that FuncGenFoil improves upon state-of-the-art methods in airfoil generation by achieving a relative -74.4 label error reduction and +23.2 diversity increase on the AF-200K dataset. Our results highlight the advantages of function-space modeling for aerodynamic shape optimization, offering a powerful and flexible framework for high-fidelity airfoil design. Our code will be released.","authors":["Jinouwen Zhang","Junjie Ren","Aobo Yang","Yan Lu","Lu Chen","Hairun Xie","Jing Wang","Miao Zhang","Wanli Ouyang","Shixiang Tang"],"url":"https://arxiv.org/abs/2502.10712"}
{"created":"2025-04-30","title":"Nexus Machine: An Active Message Inspired Reconfigurable Architecture for Irregular Workloads","abstract":"Modern reconfigurable architectures are increasingly favored for resource-constrained edge devices as they balance high performance, energy efficiency, and programmability well. However, their proficiency in handling regular compute patterns constrains their effectiveness in executing irregular workloads, such as sparse linear algebra and graph analytics with unpredictable access patterns and control flow. To address this limitation, we introduce the Nexus Machine, a novel reconfigurable architecture consisting of a PE array designed to efficiently handle irregularity by distributing sparse tensors across the fabric and employing active messages that morph instructions based on dynamic control flow. As the inherent irregularity in workloads can lead to high load imbalance among different Processing Elements (PEs), Nexus Machine deploys and executes instructions en-route on idle PEs at run-time. Thus, unlike traditional reconfigurable architectures with only static instructions within each PE, Nexus Machine brings dynamic control to the idle compute units, mitigating load imbalance and enhancing overall performance. Our experiments demonstrate that Nexus Machine achieves 90% better performance compared to state-of-the-art (SOTA) reconfigurable architectures, within the same power budget and area. Nexus Machine also achieves 70% higher fabric utilization, in contrast to SOTA architectures.","authors":["Rohan Juneja","Pranav Dangi","Thilini Kaushalya Bandara","Tulika Mitra","Li-shiuan Peh"],"url":"https://arxiv.org/abs/2502.12380"}
{"created":"2025-04-30","title":"Innamark: A Whitespace Replacement Information-Hiding Method","abstract":"Large language models (LLMs) have gained significant popularity in recent years. Differentiating between a text written by a human and one generated by an LLM has become almost impossible. Information-hiding techniques such as digital watermarking or steganography can help by embedding information inside text in a form that is unlikely to be noticed. However, existing techniques, such as linguistic-based or format-based methods, change the semantics or cannot be applied to pure, unformatted text. In this paper, we introduce a novel method for information hiding called Innamark, which can conceal any byte-encoded sequence within a sufficiently long cover text. This method is implemented as a multi-platform library using the Kotlin programming language, which is accompanied by a command-line tool and a web interface. By substituting conventional whitespace characters with visually similar Unicode whitespace characters, our proposed scheme preserves the semantics of the cover text without changing the number of characters. Furthermore, we propose a specified structure for secret messages that enables configurable compression, encryption, hashing, and error correction. An experimental benchmark comparison on a dataset of 1000000 Wikipedia articles compares ten algorithms. The results demonstrate the robustness of our proposed Innamark method in various applications and the imperceptibility of its watermarks to humans. We discuss the limits to the embedding capacity and robustness of the algorithm and how these could be addressed in future work.","authors":["Malte Hellmeier","Hendrik Norkowski","Ernst-Christoph Schrewe","Haydar Qarawlus","Falk Howar"],"url":"https://arxiv.org/abs/2502.12710"}
{"created":"2025-04-30","title":"An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation","abstract":"Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs' limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub.","authors":["Mohammad Feli","Iman Azimi","Pasi Liljeberg","Amir M. Rahmani"],"url":"https://arxiv.org/abs/2502.12836"}
{"created":"2025-04-30","title":"Debiasing Functions of Private Statistics in Postprocessing","abstract":"Given a differentially private unbiased estimate $\\tilde{q}=q(D) +\\nu$ of a statistic $q(D)$, we wish to obtain unbiased estimates of functions of $q(D)$, such as $1/q(D)$, solely through post-processing of $\\tilde{q}$, with no further access to the confidential dataset $D$. To this end, we adapt the deconvolution method used for unbiased estimation in the statistical literature, deriving unbiased estimators for a broad family of twice-differentiable functions when the privacy-preserving noise $\\nu$ is drawn from the Laplace distribution (Dwork et al., 2006). We further extend this technique to a more general class of functions, deriving approximately optimal estimators that are unbiased for values in a user-specified interval (possibly extending to $\\pm \\infty$). We use these results to derive an unbiased estimator for private means when the size $n$ of the dataset is not publicly known. In a numerical application, we find that a mechanism that uses our estimator to return an unbiased sample size and mean outperforms a mechanism that instead uses the previously known unbiased privacy mechanism for such means (Kamath et al., 2023). We also apply our estimators to develop unbiased transformation mechanisms for per-record differential privacy, a privacy concept in which the privacy guarantee is a public function of a record's value (Seeman et al., 2024). Our mechanisms provide stronger privacy guarantees than those in prior work (Finley et al., 2024) by using Laplace, rather than Gaussian, noise. Finally, using a different approach, we go beyond Laplace noise by deriving unbiased estimators for polynomials under the weak condition that the noise distribution has sufficiently many moments.","authors":["Flavio Calmon","Elbert Du","Cynthia Dwork","Brian Finley","Grigory Franguridi"],"url":"https://arxiv.org/abs/2502.13314"}
{"created":"2025-04-30","title":"Exploiting Epistemic Uncertainty in Cold-Start Recommendation Systems","abstract":"The cold-start problem remains a significant challenge in recommendation systems based on generative models. Current methods primarily focus on enriching embeddings or inputs by gathering more data, often overlooking the effectiveness of how existing training knowledge is utilized. This inefficiency can lead to missed opportunities for improving cold-start recommendations. To address this, we propose the use of epistemic uncertainty, which reflects a lack of certainty about the optimal model, as a tool to measure and enhance the efficiency with which a recommendation system leverages available knowledge. By considering epistemic uncertainty as a reducible component of overall uncertainty, we introduce a new approach to refine model performance. The effectiveness of this approach is validated through extensive offline experiments on publicly available datasets, demonstrating its superior performance and robustness in tackling the cold-start problem.","authors":["Yang Xiang","Li Fan","Chenke Yin","Menglin Kong","Chengtao Ji"],"url":"https://arxiv.org/abs/2502.16256"}
{"created":"2025-04-30","title":"MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation","abstract":"Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.","authors":["Mar\\'ia Andrea Cruz Bland\\'on","Jayasimha Talur","Bruno Charron","Dong Liu","Saab Mansour","Marcello Federico"],"url":"https://arxiv.org/abs/2502.17163"}
{"created":"2025-04-30","title":"Research on Enhancing Cloud Computing Network Security using Artificial Intelligence Algorithms","abstract":"Cloud computing environments are increasingly vulnerable to security threats such as distributed denial-of-service (DDoS) attacks and SQL injection. Traditional security mechanisms, based on rule matching and feature recognition, struggle to adapt to evolving attack strategies. This paper proposes an adaptive security protection framework leveraging deep learning to construct a multi-layered defense architecture. The proposed system is evaluated in a real-world business environment, achieving a detection accuracy of 97.3%, an average response time of 18 ms, and an availability rate of 99.999%. Experimental results demonstrate that the proposed method significantly enhances detection accuracy, response efficiency, and resource utilization, offering a novel and effective approach to cloud computing security.","authors":["Yuqing Wang","Xiao Yang"],"url":"https://arxiv.org/abs/2502.17801"}
{"created":"2025-04-30","title":"Research on Edge Computing and Cloud Collaborative Resource Scheduling Optimization Based on Deep Reinforcement Learning","abstract":"This study addresses the challenge of resource scheduling optimization in edge-cloud collaborative computing using deep reinforcement learning (DRL). The proposed DRL-based approach improves task processing efficiency, reduces overall processing time, enhances resource utilization, and effectively controls task migrations. Experimental results demonstrate the superiority of DRL over traditional scheduling algorithms, particularly in managing complex task allocation, dynamic workloads, and multiple resource constraints. Despite its advantages, further improvements are needed to enhance learning efficiency, reduce training time, and address convergence issues. Future research should focus on increasing the algorithm's fault tolerance to handle more complex and uncertain scheduling scenarios, thereby advancing the intelligence and efficiency of edge-cloud computing systems.","authors":["Yuqing Wang","Xiao Yang"],"url":"https://arxiv.org/abs/2502.18773"}
{"created":"2025-04-30","title":"NoPain: No-box Point Cloud Attack via Optimal Transport Singular Boundary","abstract":"Adversarial attacks exploit the vulnerability of deep models against adversarial samples. Existing point cloud attackers are tailored to specific models, iteratively optimizing perturbations based on gradients in either a white-box or black-box setting. Despite their promising attack performance, they often struggle to produce transferable adversarial samples due to overfitting the specific parameters of surrogate models. To overcome this issue, we shift our focus to the data distribution itself and introduce a novel approach named NoPain, which employs optimal transport (OT) to identify the inherent singular boundaries of the data manifold for cross-network point cloud attacks. Specifically, we first calculate the OT mapping from noise to the target feature space, then identify singular boundaries by locating non-differentiable positions. Finally, we sample along singular boundaries to generate adversarial point clouds. Once the singular boundaries are determined, NoPain can efficiently produce adversarial samples without the need of iterative updates or guidance from the surrogate classifiers. Extensive experiments demonstrate that the proposed end-to-end method outperforms baseline approaches in terms of both transferability and efficiency, while also maintaining notable advantages even against defense strategies. Code and model are available at https://github.com/cognaclee/nopain","authors":["Zezeng Li","Xiaoyu Du","Na Lei","Liming Chen","Weimin Wang"],"url":"https://arxiv.org/abs/2503.00063"}
{"created":"2025-04-30","title":"Few-shot Sim2Real Based on High Fidelity Rendering with Force Feedback Teleoperation","abstract":"Teleoperation offers a promising approach to robotic data collection and human-robot interaction. However, existing teleoperation methods for data collection are still limited by efficiency constraints in time and space, and the pipeline for simulation-based data collection remains unclear. The problem is how to enhance task performance while minimizing reliance on real-world data. To address this challenge, we propose a teleoperation pipeline for collecting robotic manipulation data in simulation and training a few-shot sim-to-real visual-motor policy. Force feedback devices are integrated into the teleoperation system to provide precise end-effector gripping force feedback. Experiments across various manipulation tasks demonstrate that force feedback significantly improves both success rates and execution efficiency, particularly in simulation. Furthermore, experiments with different levels of visual rendering quality reveal that enhanced visual realism in simulation substantially boosts task performance while reducing the need for real-world data.","authors":["Yanwen Zou","Junda Huang","Boyuan Liang","Honghao Guo","Zhengyang Liu","Xin Ma","Jianshu Zhou","Masayoshi Tomizuka"],"url":"https://arxiv.org/abs/2503.01301"}
{"created":"2025-04-30","title":"STAA-SNN: Spatial-Temporal Attention Aggregator for Spiking Neural Networks","abstract":"Spiking Neural Networks (SNNs) have gained significant attention due to their biological plausibility and energy efficiency, making them promising alternatives to Artificial Neural Networks (ANNs). However, the performance gap between SNNs and ANNs remains a substantial challenge hindering the widespread adoption of SNNs. In this paper, we propose a Spatial-Temporal Attention Aggregator SNN (STAA-SNN) framework, which dynamically focuses on and captures both spatial and temporal dependencies. First, we introduce a spike-driven self-attention mechanism specifically designed for SNNs. Additionally, we pioneeringly incorporate position encoding to integrate latent temporal relationships into the incoming features. For spatial-temporal information aggregation, we employ step attention to selectively amplify relevant features at different steps. Finally, we implement a time-step random dropout strategy to avoid local optima. As a result, STAA-SNN effectively captures both spatial and temporal dependencies, enabling the model to analyze complex patterns and make accurate predictions. The framework demonstrates exceptional performance across diverse datasets and exhibits strong generalization capabilities. Notably, STAA-SNN achieves state-of-the-art results on neuromorphic datasets CIFAR10-DVS, with remarkable performances of 97.14%, 82.05% and 70.40% on the static datasets CIFAR-10, CIFAR-100 and ImageNet, respectively. Furthermore, our model exhibits improved performance ranging from 0.33\\% to 2.80\\% with fewer time steps. The code for the model is available on GitHub.","authors":["Tianqing Zhang","Kairong Yu","Xian Zhong","Hongwei Wang","Qi Xu","Qiang Zhang"],"url":"https://arxiv.org/abs/2503.02689"}
{"created":"2025-04-30","title":"Temporal Separation with Entropy Regularization for Knowledge Distillation in Spiking Neural Networks","abstract":"Spiking Neural Networks (SNNs), inspired by the human brain, offer significant computational efficiency through discrete spike-based information transfer. Despite their potential to reduce inference energy consumption, a performance gap persists between SNNs and Artificial Neural Networks (ANNs), primarily due to current training methods and inherent model limitations. While recent research has aimed to enhance SNN learning by employing knowledge distillation (KD) from ANN teacher networks, traditional distillation techniques often overlook the distinctive spatiotemporal properties of SNNs, thus failing to fully leverage their advantages. To overcome these challenge, we propose a novel logit distillation method characterized by temporal separation and entropy regularization. This approach improves existing SNN distillation techniques by performing distillation learning on logits across different time steps, rather than merely on aggregated output features. Furthermore, the integration of entropy regularization stabilizes model optimization and further boosts the performance. Extensive experimental results indicate that our method surpasses prior SNN distillation strategies, whether based on logit distillation, feature distillation, or a combination of both. The code will be available on GitHub.","authors":["Kairong Yu","Chengting Yu","Tianqing Zhang","Xiaochen Zhao","Shu Yang","Hongwei Wang","Qiang Zhang","Qi Xu"],"url":"https://arxiv.org/abs/2503.03144"}
{"created":"2025-04-30","title":"Arc Blanc: a real time ocean simulation framework","abstract":"The oceans cover the vast majority of the Earth. Therefore, their simulation has many scientific, industrial and military interests, including computer graphics domain. By fully exploiting the multi-threading power of GPU and CPU, current state-of-the-art tools can achieve real-time ocean simulation, even if it is sometimes needed to reduce the physical realism for large scenes. Although most of the building blocks for implementing an ocean simulator are described in the literature, a clear explanation of how they interconnect is lacking. Hence, this paper proposes to bring all these components together, detailing all their interactions, in a comprehensive and fully described real-time framework that simulates the free ocean surface and the coupling between solids and fluid. This article also presents several improvements to enhance the physical realism of our model. The two main ones are: calculating the real-time velocity of ocean fluids at any depth; computing the input of the solid to fluid coupling algorithm.","authors":["David Algis (UP","XLIM)","B\\'erenger Bramas (CAMUS","ICube","UNISTRA)","Emmanuelle Darles (UP","XLIM)","Lilian Aveneau (UP","XLIM-ASALI)"],"url":"https://arxiv.org/abs/2503.03326"}
{"created":"2025-04-30","title":"Cloud Computing Energy Consumption Prediction Based on Kernel Extreme Learning Machine Algorithm Improved by Vector Weighted Average Algorithm","abstract":"With the rapid expansion of cloud computing infrastructure, energy consumption has become a critical challenge, driving the need for accurate and efficient prediction models. This study proposes a novel Vector Weighted Average Kernel Extreme Learning Machine (VWAA-KELM) model to enhance energy consumption prediction in cloud computing environments. By integrating a vector weighted average algorithm (VWAA) with kernel extreme learning machine (KELM), the proposed model dynamically adjusts feature weights and optimizes kernel functions, significantly improving prediction accuracy and generalization. Experimental results demonstrate the superior performance of VWAA-KELM: 94.7% of test set prediction errors fall within [0, 50] units, with only three cases exceeding 100 units, indicating strong stability. The model achieves a coefficient of determination (R2) of 0.987 in the training set (RMSE = 28.108, RPD = 8.872) and maintains excellent generalization with R2 = 0.973 in the test set (RMSE = 43.227, RPD = 6.202). Visual analysis confirms that predicted values closely align with actual energy consumption trends, avoiding overfitting while capturing nonlinear dependencies. A key innovation of this study is the introduction of adaptive feature weighting, allowing the model to dynamically assign importance to different input parameters, thereby enhancing high-dimensional data processing. This advancement provides a scalable and efficient approach for optimizing cloud data center energy consumption. Beyond cloud computing, the proposed hybrid framework has broader applications in Internet of Things (IoT) and edge computing, supporting real-time energy management and intelligent resource allocation.","authors":["Yuqing Wang","Xiao Yang"],"url":"https://arxiv.org/abs/2503.04088"}
{"created":"2025-04-30","title":"One-Shot Clustering for Federated Learning","abstract":"Federated Learning (FL) is a widespread and well adopted paradigm of decentralized learning that allows training one model from multiple sources without the need to directly transfer data between participating clients. Since its inception in 2015, it has been divided into numerous sub-fields that deal with application-specific issues, be it data heterogeneity or resource allocation. One such sub-field, Clustered Federated Learning (CFL), is dealing with the problem of clustering the population of clients into separate cohorts to deliver personalized models. Although few remarkable works have been published in this domain, the problem is still largely unexplored, as its basic assumption and settings are slightly different from standard FL. In this work, we present One-Shot Clustered Federated Learning (OCFL), a clustering-agnostic algorithm that can automatically detect the earliest suitable moment for clustering. Our algorithm is based on the computation of cosine similarity between gradients of the clients and a temperature measure that detects when the federated model starts to converge. We empirically evaluate our methodology by testing various one-shot clustering algorithms for over thirty different tasks on three benchmark datasets. Our experiments showcase the good performance of our approach when used to perform CFL in an automated manner without the need to adjust hyperparameters.","authors":["Maciej Krzysztof Zuziak","Roberto Pellungrini","Salvatore Rinzivillo"],"url":"https://arxiv.org/abs/2503.04231"}
{"created":"2025-04-30","title":"The Best of Both Worlds: Integrating Language Models and Diffusion Models for Video Generation","abstract":"Recent advancements in text-to-video (T2V) generation have been driven by two competing paradigms: autoregressive language models and diffusion models. However, each paradigm has intrinsic limitations: language models struggle with visual quality and error accumulation, while diffusion models lack semantic understanding and causal modeling. In this work, we propose LanDiff, a hybrid framework that synergizes the strengths of both paradigms through coarse-to-fine generation. Our architecture introduces three key innovations: (1) a semantic tokenizer that compresses 3D visual features into compact 1D discrete representations through efficient semantic compression, achieving a $\\sim$14,000$\\times$ compression ratio; (2) a language model that generates semantic tokens with high-level semantic relationships; (3) a streaming diffusion model that refines coarse semantics into high-fidelity videos. Experiments show that LanDiff, a 5B model, achieves a score of 85.43 on the VBench T2V benchmark, surpassing the state-of-the-art open-source models Hunyuan Video (13B) and other commercial models such as Sora, Kling, and Hailuo. Furthermore, our model also achieves state-of-the-art performance in long video generation, surpassing other open-source models in this field. Our demo can be viewed at https://landiff.github.io/.","authors":["Aoxiong Yin","Kai Shen","Yichong Leng","Xu Tan","Xinyu Zhou","Juncheng Li","Siliang Tang"],"url":"https://arxiv.org/abs/2503.04606"}
{"created":"2025-04-30","title":"Wanda++: Pruning Large Language Models via Regional Gradients","abstract":"Large Language Models (LLMs) pruning seeks to remove unimportant weights for inference speedup with minimal performance impact. However, existing methods often suffer from performance loss without full-model sparsity-aware fine-tuning. This paper presents Wanda++, a novel pruning framework that outperforms the state-of-the-art methods by utilizing decoder-block-level \\textbf{regional} gradients. Specifically, Wanda++ improves the pruning score with regional gradients for the first time and proposes an efficient regional optimization method to minimize pruning-induced output discrepancies between the dense and sparse decoder output. Notably, Wanda++ improves perplexity by up to 32\\% over Wanda in the language modeling task and generalizes effectively to downstream tasks. Further experiments indicate our proposed method is orthogonal to sparsity-aware fine-tuning, where Wanda++ can be combined with LoRA fine-tuning to achieve a similar perplexity improvement as the Wanda method. The proposed method is lightweight, pruning a 7B LLaMA model in under 10 minutes on a single NVIDIA H100 GPU.","authors":["Yifan Yang","Kai Zhen","Bhavana Ganesh","Aram Galstyan","Goeric Huybrechts","Markus M\\\"uller","Jonas M. K\\\"ubler","Rupak Vignesh Swaminathan","Athanasios Mouchtaris","Sravan Babu Bodapati","Nathan Susanj","Zheng Zhang","Jack FitzGerald","Abhishek Kumar"],"url":"https://arxiv.org/abs/2503.04992"}
{"created":"2025-04-30","title":"Adaptive Neural Unscented Kalman Filter","abstract":"The unscented Kalman filter is an algorithm capable of handling nonlinear scenarios. Uncertainty in process noise covariance may decrease the filter estimation performance or even lead to its divergence. Therefore, it is important to adjust the process noise covariance matrix in real time. In this paper, we developed an adaptive neural unscented Kalman filter to cope with time-varying uncertainties during platform operation. To this end, we devised ProcessNet, a simple yet efficient end-to-end regression network to adaptively estimate the process noise covariance matrix. We focused on the nonlinear inertial sensor and Doppler velocity log fusion problem in the case of autonomous underwater vehicle navigation. Using a real-world recorded dataset from an autonomous underwater vehicle, we demonstrated our filter performance and showed its advantages over other adaptive and non-adaptive nonlinear filters.","authors":["Amit Levy","Itzik Klein"],"url":"https://arxiv.org/abs/2503.05490"}
{"created":"2025-04-30","title":"What's in a Latent? Leveraging Diffusion Latent Space for Domain Generalization","abstract":"Domain Generalization aims to develop models that can generalize to novel and unseen data distributions. In this work, we study how model architectures and pre-training objectives impact feature richness and propose a method to effectively leverage them for domain generalization. Specifically, given a pre-trained feature space, we first discover latent domain structures, referred to as pseudo-domains, that capture domain-specific variations in an unsupervised manner. Next, we augment existing classifiers with these complementary pseudo-domain representations making them more amenable to diverse unseen test domains. We analyze how different pre-training feature spaces differ in the domain-specific variances they capture. Our empirical studies reveal that features from diffusion models excel at separating domains in the absence of explicit domain labels and capture nuanced domain-specific information. On 5 datasets, we show that our very simple framework improves generalization to unseen domains by a maximum test accuracy improvement of over 4% compared to the standard baseline Empirical Risk Minimization (ERM). Crucially, our method outperforms most algorithms that access domain labels during training.","authors":["Xavier Thomas","Deepti Ghadiyaram"],"url":"https://arxiv.org/abs/2503.06698"}
{"created":"2025-04-30","title":"KAP: MLLM-assisted OCR Text Enhancement for Hybrid Retrieval in Chinese Non-Narrative Documents","abstract":"Hybrid Retrieval systems, combining Sparse and Dense Retrieval methods, struggle with Traditional Chinese non-narrative documents due to their complex formatting, rich vocabulary, and the insufficient understanding of Chinese synonyms by common embedding models. Previous approaches inadequately address the dual needs of these systems, focusing mainly on general text quality improvement rather than optimizing for retrieval. We propose Knowledge-Aware Preprocessing (KAP), a framework that transforms noisy OCR outputs into retrieval-optimized text. KAP adopts a two-stage approach: it first extracts text using OCR, then employs Multimodal Large Language Models to refine the output by integrating visual information from the original documents. This design reduces OCR noise, reconstructs structural elements, and formats the text to satisfy the distinct requirements of both sparse and dense retrieval. Empirical results demonstrate that KAP consistently and significantly outperforms conventional preprocessing approaches. Our code is available at https://github.com/JustinHsu1019/KAP.","authors":["Hsin-Ling Hsu","Ping-Sheng Lin","Jing-Di Lin","Jengnan Tzeng"],"url":"https://arxiv.org/abs/2503.08452"}
{"created":"2025-04-30","title":"Training Plug-n-Play Knowledge Modules with Deep Context Distillation","abstract":"Dynamically integrating new or rapidly evolving information after (Large) Language Model pre-training remains challenging, particularly in low-data scenarios or when dealing with private and specialized documents. In-context learning and retrieval-augmented generation (RAG) face limitations, including their high inference costs and their inability to capture global document information. In this paper, we propose a way of modularizing knowledge by training document-level Knowledge Modules (KMs). KMs are lightweight components implemented as parameter-efficient LoRA modules, which are trained to store information about new documents and can be easily plugged into models on demand. We show that next-token prediction performs poorly as the training objective for KMs. We instead propose Deep Context Distillation: we learn KMs parameters such as to simulate hidden states and logits of a teacher that takes the document in context. Our method outperforms standard next-token prediction and pre-instruction training techniques, across two datasets. Finally, we highlight synergies between KMs and RAG.","authors":["Lucas Caccia","Alan Ansell","Edoardo Ponti","Ivan Vuli\\'c","Alessandro Sordoni"],"url":"https://arxiv.org/abs/2503.08727"}
{"created":"2025-04-30","title":"LocAgent: Graph-Guided LLM Agents for Code Localization","abstract":"Code localization--identifying precisely where in a codebase changes need to be made--is a fundamental yet challenging task in software maintenance. Existing approaches struggle to efficiently navigate complex codebases when identifying relevant code sections. The challenge lies in bridging natural language problem descriptions with the appropriate code elements, often requiring reasoning across hierarchical structures and multiple dependencies. We introduce LocAgent, a framework that addresses code localization through graph-based representation. By parsing codebases into directed heterogeneous graphs, LocAgent creates a lightweight representation that captures code structures (files, classes, functions) and their dependencies (imports, invocations, inheritance), enabling LLM agents to effectively search and locate relevant entities through powerful multi-hop reasoning. Experimental results on real-world benchmarks demonstrate that our approach significantly enhances accuracy in code localization. Notably, our method with the fine-tuned Qwen-2.5-Coder-Instruct-32B model achieves comparable results to SOTA proprietary models at greatly reduced cost (approximately 86% reduction), reaching up to 92.7% accuracy on file-level localization while improving downstream GitHub issue resolution success rates by 12% for multiple attempts (Pass@10). Our code is available at https://github.com/gersteinlab/LocAgent.","authors":["Zhaoling Chen","Xiangru Tang","Gangda Deng","Fang Wu","Jialong Wu","Zhiwei Jiang","Viktor Prasanna","Arman Cohan","Xingyao Wang"],"url":"https://arxiv.org/abs/2503.09089"}
{"created":"2025-04-30","title":"WAFFLED: Exploiting Parsing Discrepancies to Bypass Web Application Firewalls","abstract":"Web Application Firewalls (WAFs) have been introduced as essential and popular security gates that inspect incoming HTTP traffic to filter out malicious requests and provide defenses against a diverse array of web-based threats. Evading WAFs can compromise these defenses, potentially harming Internet users. In recent years, parsing discrepancies have plagued many entities in the communication path; however, their potential impact on WAF evasion and request smuggling remains largely unexplored. In this work, we present an innovative approach to bypassing WAFs by uncovering and exploiting parsing discrepancies through advanced fuzzing techniques. By targeting non-malicious components such as headers and segments of the body and using widely used content-types such as application/json, multipart/form-data, and application/xml, we identified and confirmed 1207 bypasses across 5 well-known WAFs, AWS, Azure, Cloud Armor, Cloudflare, and ModSecurity. To validate our findings, we conducted a study in the wild, revealing that more than 90% of websites accepted both application/x-www-form-urlencoded and multipart/form-data interchangeably, highlighting a significant vulnerability and the broad applicability of our bypass techniques. We have reported these vulnerabilities to the affected parties and received acknowledgments from all, as well as bug bounty rewards from some vendors. Further, to mitigate these vulnerabilities, we introduce HTTP-Normalizer, a robust proxy tool designed to rigorously validate HTTP requests against current RFC standards. Our results demonstrate its effectiveness in normalizing or blocking all bypass attempts presented in this work.","authors":["Seyed Ali Akhavani","Bahruz Jabiyev","Ben Kallus","Cem Topcuoglu","Sergey Bratus","Engin Kirda"],"url":"https://arxiv.org/abs/2503.10846"}
{"created":"2025-04-30","title":"Cooperative Deterministic Learning-Based Formation Control for a Group of Nonlinear Mechanical Systems Under Complete Uncertainty","abstract":"In this work we address the formation control problem for a group of nonlinear mechanical systems with complete uncertain dynamics under a virtual leader-following framework. We propose a novel cooperative deterministic learning-based adaptive formation control algorithm. This algorithm is designed by utilizing artificial neural networks to simultaneously achieve formation tracking control and locally-accurate identification/learning of the nonlinear uncertain dynamics of the considered group of mechanical systems. To demonstrate the practicality and verify the effectiveness of the proposed results, numerical simulations have been conducted.","authors":["Maryam Norouzi","Mingxi Zhou","Chengzhi Yuan"],"url":"https://arxiv.org/abs/2503.13688"}
{"created":"2025-04-30","title":"SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation","abstract":"Idiomatic expressions present a unique challenge in NLP, as their meanings are often not directly inferable from their constituent words. Despite recent advancements in Large Language Models (LLMs), idiomaticity remains a significant obstacle to robust semantic representation. We present datasets and tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity Representation), which challenges the community to assess and improve models' ability to interpret idiomatic expressions in multimodal contexts and in multiple languages. Participants competed in two subtasks: ranking images based on their alignment with idiomatic or literal meanings, and predicting the next image in a sequence. The most effective methods achieved human-level performance by leveraging pretrained LLMs and vision-language models in mixture-of-experts settings, with multiple queries used to smooth over the weaknesses in these models' representations of idiomaticity.","authors":["Thomas Pickard","Aline Villavicencio","Maggie Mi","Wei He","Dylan Phelps","Marco Idiart"],"url":"https://arxiv.org/abs/2503.15358"}
{"created":"2025-04-30","title":"CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation","abstract":"Large language models (LLMs) have revolutionized natural language processing (NLP), particularly through Retrieval-Augmented Generation (RAG), which enhances LLM capabilities by integrating external knowledge. However, traditional RAG systems face critical limitations, including disrupted contextual integrity due to text chunking, and over-reliance on semantic similarity for retrieval. To address these issues, we propose CausalRAG, a novel framework that incorporates causal graphs into the retrieval process. By constructing and tracing causal relationships, CausalRAG preserves contextual continuity and improves retrieval precision, leading to more accurate and interpretable responses. We evaluate CausalRAG against regular RAG and graph-based RAG approaches, demonstrating its superiority across several metrics. Our findings suggest that grounding retrieval in causal reasoning provides a promising approach to knowledge-intensive tasks.","authors":["Nengbo Wang","Xiaotian Han","Jagdip Singh","Jing Ma","Vipin Chaudhary"],"url":"https://arxiv.org/abs/2503.19878"}
{"created":"2025-04-30","title":"RocketPPA: Ultra-Fast LLM-Based PPA Estimator at Code-Level Abstraction","abstract":"Large language models have recently transformed hardware design, yet bridging the gap between code synthesis and PPA (power, performance, and area) estimation remains a challenge. In this work, we introduce a novel framework that leverages a 21k dataset of thoroughly cleaned and synthesizable Verilog modules, each annotated with detailed power, delay, and area metrics. By employing chain-of-thought techniques, we automatically debug and curate this dataset to ensure high fidelity in downstream applications. We then fine-tune CodeLlama using LoRA-based parameter-efficient methods, framing the task as a regression problem to accurately predict PPA metrics from Verilog code. Furthermore, we augment our approach with a mixture-of-experts architecture-integrating both LoRA and an additional MLP expert layer-to further refine predictions. Experimental results demonstrate significant improvements: power estimation accuracy is enhanced by 5.9% at a 20% error threshold and by 7.2% at a 10% threshold, delay estimation improves by 5.1% and 3.9%, and area estimation sees gains of 4% and 7.9% for the 20% and 10% thresholds, respectively. Notably, the incorporation of the mixture-of-experts module contributes an additional 3--4% improvement across these tasks. Our results establish a new benchmark for PPA-aware Verilog generation, highlighting the effectiveness of our integrated dataset and modeling strategies for next-generation EDA workflows.","authors":["Armin Abdollahi","Mehdi Kamal","Massoud Pedram"],"url":"https://arxiv.org/abs/2503.21971"}
{"created":"2025-04-30","title":"Probabilistic Uncertain Reward Model: A Natural Generalization of Bradley-Terry Reward Model","abstract":"Reinforcement Learning from Human Feedback (RLHF) has emerged as a critical technique for training large language models. However, reward hacking-a phenomenon where models exploit flaws in the reward model-remains a significant barrier to achieving robust and scalable intelligence through long-term training. Existing studies have proposed the uncertain reward models to address reward hacking, however, they often lack systematic or theoretical foundations, failing to model the uncertainty intrinsically emerging from preference data, and thus cannot sufficiently mitigate reward hacking to sustain prolonged RLHF training and exploration. In this paper, we propose a Probabilistic Uncertain Reward Model (PURM), a natural generalization of the classical Bradley-Terry reward model, that can directly learn the reward distribution emerged from the preference data. We theoretically derived PURM's loss function and the reward distribution uncertainty calculation based on Bhattacharyya Coefficient. To mitigate reward hacking with PURM, we further introduce an uncertainty-aware penalty into Proximal Policy Optimization (PPO), which leverages the learned uncertainty to dynamically balance reward optimization and exploration. We propose a lightweight and easy-to-use implementation of PURM. Experiments demonstrate that PURM effectively models the rewards and uncertainties, and significantly delays the onset of reward hacking while improving final reward performance compared with existing methods.","authors":["Wangtao Sun","Xiang Cheng","Xing Yu","Haotian Xu","Zhao Yang","Shizhu He","Jun Zhao","Kang Liu"],"url":"https://arxiv.org/abs/2503.22480"}
{"created":"2025-04-30","title":"Data-Driven Worker Activity Recognition and Efficiency Estimation in Manual Fruit Harvesting","abstract":"Manual fruit harvesting is common in agriculture, but the amount of time pickers spend on non-productive activities can make it very inefficient. Accurately identifying picking vs. non-picking activity is crucial for estimating picker efficiency and optimising labour management and harvest processes. In this study, a practical system was developed to calculate the efficiency of pickers in commercial strawberry harvesting. Instrumented picking carts were developed to record the harvested fruit weight, geolocation, and cart movement in real time. These carts were deployed during the commercial strawberry harvest season in Santa Maria, CA. The collected data was then used to train a CNN-LSTM-based deep neural network to classify a picker's activity into \"Pick\" and \"NoPick\" classes. Experimental evaluations showed that the CNN-LSTM model showed promising activity recognition performance with an F1 score accuracy of over 0.97. The recognition results were then used to compute picker efficiency and the time required to fill a tray. Analysis of the season-long harvest data showed that the average picker efficiency was 75.07% with an estimation accuracy of 95.22%. Furthermore, the average tray fill time was 6.79 minutes with an estimation accuracy of 96.43%. When integrated into commercial harvesting, the proposed technology can aid growers in monitoring automated worker activity and optimising harvests to reduce non-productive time and enhance overall harvest efficiency.","authors":["Uddhav Bhattarai","Rajkishan Arikapudi","Steven A. Fennimore","Frank N Martin","Stavros G. Vougioukas"],"url":"https://arxiv.org/abs/2503.22809"}
{"created":"2025-04-30","title":"UP-dROM : Uncertainty-Aware and Parametrised dynamic Reduced-Order Model, application to unsteady flows","abstract":"Reduced order models (ROMs) play a critical role in fluid mechanics by providing low-cost predictions, making them an attractive tool for engineering applications. However, for ROMs to be widely applicable, they must not only generalise well across different regimes, but also provide a measure of confidence in their predictions. While recent data-driven approaches have begun to address nonlinear reduction techniques to improve predictions in transient environments, challenges remain in terms of robustness and parametrisation. In this work, we present a nonlinear reduction strategy specifically designed for transient flows that incorporates parametrisation and uncertainty quantification. Our reduction strategy features a variational auto-encoder (VAE) that uses variational inference for confidence measurement. We use a latent space transformer that incorporates recent advances in attention mechanisms to predict dynamical systems. Attention's versatility in learning sequences and capturing their dependence on external parameters enhances generalisation across a wide range of dynamics. Prediction, coupled with confidence, enables more informed decision making and addresses the need for more robust models. In addition, this confidence is used to cost-effectively sample the parameter space, improving model performance a priori across the entire parameter space without requiring evaluation data for the entire domain.","authors":["Isma\\\"el Zighed","Nicolas Thome","Patrick Gallinari","Taraneh Sayadi"],"url":"https://arxiv.org/abs/2503.23236"}
{"created":"2025-04-30","title":"Filtering with Time-frequency Analysis: An Adaptive and Lightweight Model for Sequential Recommender Systems Based on Discrete Wavelet Transform","abstract":"Sequential Recommender Systems (SRS) aim to model sequential behaviors of users to capture their interests which usually evolve over time. Transformer-based SRS have achieved distinguished successes recently. However, studies reveal self-attention mechanism in Transformer-based models is essentially a low-pass filter and ignores high frequency information potentially including meaningful user interest patterns. This motivates us to seek better filtering technologies for SRS, and finally we find Discrete Wavelet Transform (DWT), a famous time-frequency analysis technique from digital signal processing field, can effectively process both low-frequency and high-frequency information. We design an adaptive time-frequency filter with DWT technique, which decomposes user interests into multiple signals with different frequency and time, and can automatically learn weights of these signals. Furthermore, we develop DWTRec, a model for sequential recommendation all based on the adaptive time-frequency filter. Thanks to fast DWT technique, DWTRec has a lower time complexity and space complexity theoretically, and is Proficient in modeling long sequences. Experiments show that our model outperforms state-of-the-art baseline models in datasets with different domains, sparsity levels and average sequence lengths. Especially, our model shows great performance increase in contrast with previous models when the sequence grows longer, which demonstrates another advantage of our model.","authors":["Sheng Lu","Mingxi Ge","Jiuyi Zhang","Wanli Zhu","Guanjin Li","Yuang Li","Linhao Wang","Fangming Gu"],"url":"https://arxiv.org/abs/2503.23436"}
{"created":"2025-04-30","title":"4D mmWave Radar for Sensing Enhancement in Adverse Environments: Advances and Challenges","abstract":"Intelligent transportation systems require accurate and reliable sensing. However, adverse environments, such as rain, snow, and fog, can significantly degrade the performance of LiDAR and cameras. In contrast, 4D mmWave radar not only provides 3D point clouds and velocity measurements but also maintains robustness in challenging conditions. Recently, research on 4D mmWave radar under adverse environments has been growing, but a comprehensive review is still lacking. To bridge this gap, this work reviews the current research on 4D mmWave radar under adverse environments. First, we present an overview of existing 4D mmWave radar datasets encompassing diverse weather and lighting scenarios. Subsequently, we analyze existing learning-based methods leveraging 4D mmWave radar to enhance performance according to different adverse conditions. Finally, the challenges and potential future directions are discussed for advancing 4D mmWave radar applications in harsh environments. To the best of our knowledge, this is the first review specifically concentrating on 4D mmWave radar in adverse environments. The related studies are listed at: https://github.com/XiangyPeng/4D-mmWave-Radar-in-Adverse-Environments.","authors":["Xiangyuan Peng","Miao Tang","Huawei Sun","Kay Bierzynski","Lorenzo Servadei","Robert Wille"],"url":"https://arxiv.org/abs/2503.24091"}
{"created":"2025-04-30","title":"Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users","abstract":"Electronic dictionaries have largely replaced paper dictionaries and become central tools for L2 learners seeking to expand their vocabulary. Users often assume these resources are reliable and rarely question the validity of the definitions provided. The accuracy of major E-dictionaries is seldom scrutinized, and little attention has been paid to how their corpora are constructed. Research on dictionary use, particularly the limitations of electronic dictionaries, remains scarce. This study adopts a combined method of experimentation, user survey, and dictionary critique to examine Youdao, one of the most widely used E-dictionaries in China. The experiment involved a translation task paired with retrospective reflection. Participants were asked to translate sentences containing words that are insufficiently or inaccurately defined in Youdao. Their consultation behavior was recorded to analyze how faulty definitions influenced comprehension. Results show that incomplete or misleading definitions can cause serious misunderstandings. Additionally, students exhibited problematic consultation habits. The study further explores how such flawed definitions originate, highlighting issues in data processing and the integration of AI and machine learning technologies in dictionary construction. The findings suggest a need for better training in dictionary literacy for users, as well as improvements in the underlying AI models used to build E-dictionaries.","authors":["Xi Wang","Fanfei Meng","Shiyang Zhang","Lan Li"],"url":"https://arxiv.org/abs/2504.00799"}
{"created":"2025-04-30","title":"Two-stage deep learning framework for the restoration of incomplete-ring PET images","abstract":"Positron Emission Tomography (PET) is an important molecular imaging tool widely used in medicine. Traditional PET systems rely on complete detector rings for full angular coverage and reliable data collection. However, incomplete-ring PET scanners have emerged due to hardware failures, cost constraints, or specific clinical needs. Standard reconstruction algorithms often suffer from performance degradation with these systems because of reduced data completeness and geometric inconsistencies. We present a two-stage deep-learning framework that, without incorporating any time-of-flight (TOF) information, restores high-quality images from data with about 50% missing coincidences - double the loss levels previously addressed by CNN-based methods. The pipeline operates in two stages: a projection-domain Attention U-Net first predicts the missing sections of the sinogram by leveraging spatial context from neighbouring slices, after which the completed data are reconstructed with OSEM algorithm and passed to a U-Net-diffusion module that removes residual artefacts while reinstating high-frequency detail. Using 206 brain volumes from a public dataset, the result shows that our model successfully preserves most anatomical structures and tracer distribution features with PSNR of 30.92 dB and SSIM of 0.9708. We also achieve higher inference speed, thus providing an effective solution for incomplete-ring PET imaging.","authors":["Yeqi Fang","Rong Zhou"],"url":"https://arxiv.org/abs/2504.00816"}
{"created":"2025-04-30","title":"SOLAQUA: SINTEF Ocean Large Aquaculture Robotics Dataset","abstract":"This paper presents a dataset gathered with an underwater robot in a sea-based aquaculture setting. Data was gathered from an operational fish farm and includes data from sensors such as the Waterlinked A50 DVL, the Nortek Nucleus 1000 DVL, Sonardyne Micro Ranger 2 USBL, Sonoptix Mulitbeam Sonar, mono and stereo cameras, and vehicle sensor data such as power usage, IMU, pressure, temperature, and more. Data acquisition is performed during both manual and autonomous traversal of the net pen structure. The collected vision data is of undamaged nets with some fish and marine growth presence, and it is expected that both the research community and the aquaculture industry will benefit greatly from the utilization of the proposed SOLAQUA dataset.","authors":["Sveinung Johan Ohrem","Bent Haugal{\\o}kken","Eleni Kelasidi"],"url":"https://arxiv.org/abs/2504.01790"}
{"created":"2025-04-30","title":"Is Less Really More? Fake News Detection with Limited Information","abstract":"The threat that online fake news and misinformation pose to democracy, justice, public confidence, and especially to vulnerable populations, has led to a sharp increase in the need for fake news detection and intervention. Whether multi-modal or pure text-based, most fake news detection methods depend on textual analysis of entire articles. However, these fake news detection methods come with certain limitations. For instance, fake news detection methods that rely on full text can be computationally inefficient, demand large amounts of training data to achieve competitive accuracy, and may lack robustness across different datasets. This is because fake news datasets have strong variations in terms of the level and types of information they provide; where some can include large paragraphs of text with images and metadata, others can be a few short sentences. Perhaps if one could only use minimal information to detect fake news, fake news detection methods could become more robust and resilient to the lack of information. We aim to overcome these limitations by detecting fake news using systematically selected, limited information that is both effective and capable of delivering robust, promising performance. We propose a framework called SLIM Systematically-selected Limited Information) for fake news detection. In SLIM, we quantify the amount of information by introducing information-theoretic measures. SLIM leverages limited information to achieve performance in fake news detection comparable to that of state-of-the-art obtained using the full text. Furthermore, by combining various types of limited information, SLIM can perform even better while significantly reducing the quantity of information required for training compared to state-of-the-art language model-based fake news detection techniques.","authors":["Zhaoyang Cao","John Nguyen","Reza Zafarani"],"url":"https://arxiv.org/abs/2504.01922"}
{"created":"2025-04-30","title":"Model Predictive Control with Visibility Graphs for Humanoid Path Planning and Tracking Against Adversarial Opponents","abstract":"In this paper we detail the methods used for obstacle avoidance, path planning, and trajectory tracking that helped us win the adult-sized, autonomous humanoid soccer league in RoboCup 2024. Our team was undefeated for all seated matches and scored 45 goals over 6 games, winning the championship game 6 to 1. During the competition, a major challenge for collision avoidance was the measurement noise coming from bipedal locomotion and a limited field of view (FOV). Furthermore, obstacles would sporadically jump in and out of our planned trajectory. At times our estimator would place our robot inside a hard constraint. Any planner in this competition must also be be computationally efficient enough to re-plan and react in real time. This motivated our approach to trajectory generation and tracking. In many scenarios long-term and short-term planning is needed. To efficiently find a long-term general path that avoids all obstacles we developed DAVG (Dynamic Augmented Visibility Graphs). DAVG focuses on essential path planning by setting certain regions to be active based on obstacles and the desired goal pose. By augmenting the states in the graph, turning angles are considered, which is crucial for a large soccer playing robot as turning may be more costly. A trajectory is formed by linearly interpolating between discrete points generated by DAVG. A modified version of model predictive control (MPC) is used to then track this trajectory called cf-MPC (Collision-Free MPC). This ensures short-term planning. Without having to switch formulations cf-MPC takes into account the robot dynamics and collision free constraints. Without a hard switch the control input can smoothly transition in cases where the noise places our robot inside a constraint boundary. The nonlinear formulation runs at approximately 120 Hz, while the quadratic version achieves around 400 Hz.","authors":["Ruochen Hou","Gabriel I. Fernandez","Mingzhang Zhu","Dennis W. Hong"],"url":"https://arxiv.org/abs/2504.02184"}
{"created":"2025-04-30","title":"A Dense Neighborhood Lemma: Applications of Partial Concept Classes to Domination and Chromatic Number","abstract":"In its Euclidean form, the Dense Neighborhood Lemma (DNL) asserts that if $V$ is a finite set of points of $\\mathbb{R}^N$ such that for each $v \\in V$ the ball $B(v,1)$ intersects $V$ on at least $\\delta |V|$ points, then for every $\\varepsilon >0$, the points of $V$ can be covered with $f(\\delta,\\varepsilon)$ balls $B(v,1+\\varepsilon)$ with $v \\in V$. DNL also applies to other metric spaces and to abstract set systems, where elements are compared pairwise with respect to (near) disjointness. In its strongest form, DNL provides an $\\varepsilon$-clustering with size exponential in $\\varepsilon^{-1}$, which amounts to a Regularity Lemma with 0/1 densities of some trigraph.","authors":["Romain Bourneuf","Pierre Charbit","St\\'ephan Thomass\\'e"],"url":"https://arxiv.org/abs/2504.02992"}
{"created":"2025-04-30","title":"LLM & HPC:Benchmarking DeepSeek's Performance in High-Performance Computing Tasks","abstract":"Large Language Models (LLMs), such as GPT-4 and DeepSeek, have been applied to a wide range of domains in software engineering. However, their potential in the context of High-Performance Computing (HPC) much remains to be explored. This paper evaluates how well DeepSeek, a recent LLM, performs in generating a set of HPC benchmark codes: a conjugate gradient solver, the parallel heat equation, parallel matrix multiplication, DGEMM, and the STREAM triad operation. We analyze DeepSeek's code generation capabilities for traditional HPC languages like Cpp, Fortran, Julia and Python. The evaluation includes testing for code correctness, performance, and scaling across different configurations and matrix sizes. We also provide a detailed comparison between DeepSeek and another widely used tool: GPT-4. Our results demonstrate that while DeepSeek generates functional code for HPC tasks, it lags behind GPT-4, in terms of scalability and execution efficiency of the generated code.","authors":["Noujoud Nader","Patrick Diehl","Steve Brandt","Hartmut Kaiser"],"url":"https://arxiv.org/abs/2504.03665"}
{"created":"2025-04-30","title":"Investigating and Mitigating Stereotype-aware Unfairness in LLM-based Recommendations","abstract":"Large Language Models (LLMs) have demonstrated unprecedented language understanding and reasoning capabilities to capture diverse user preferences and advance personalized recommendations. Despite the growing interest in LLM-based recommendations, unique challenges are brought to the trustworthiness of LLM-based recommender systems (LLM-RS). Compared to unique user/item representations in conventional recommender systems, users and items share the textual representation (e.g., word embeddings) in LLM-based recommendations. Recent studies have revealed that LLMs are likely to inherit stereotypes that are embedded ubiquitously in word embeddings, due to their training on large-scale uncurated datasets. This leads to LLM-RS exhibiting stereotypical linguistic associations between users and items, causing a form of two-sided (i.e., user-to-item) recommendation fairness. However, there remains a lack of studies investigating the unfairness of LLM-RS due to intrinsic stereotypes, which can simultaneously involve user and item groups. To bridge this gap, this study reveals a new variant of fairness between stereotype groups containing both users and items, to quantify discrimination against stereotypes in LLM-RS. Moreover, in this paper, to mitigate stereotype-aware unfairness in textual user and item representations, we propose a novel framework named Mixture-of-Stereotypes (MoS). In particular, an insightful stereotype-wise routing strategy over multiple stereotype-relevant experts is designed, aiming to learn unbiased representations against different stereotypes in LLM-RS. Extensive experiments are conducted to analyze the influence of stereotype-aware fairness in LLM-RS and the effectiveness of our proposed methods, which consistently outperform competitive benchmarks under various fairness settings.","authors":["Zihuai Zhao","Wenqi Fan","Yao Wu","Qing Li"],"url":"https://arxiv.org/abs/2504.04199"}
{"created":"2025-04-30","title":"Deliberate Planning of 3D Bin Packing on Packing Configuration Trees","abstract":"Online 3D Bin Packing Problem (3D-BPP) has widespread applications in industrial automation. Existing methods usually solve the problem with limited resolution of spatial discretization, and/or cannot deal with complex practical constraints well. We propose to enhance the practical applicability of online 3D-BPP via learning on a novel hierarchical representation, packing configuration tree (PCT). PCT is a full-fledged description of the state and action space of bin packing which can support packing policy learning based on deep reinforcement learning (DRL). The size of the packing action space is proportional to the number of leaf nodes, making the DRL model easy to train and well-performing even with continuous solution space. We further discover the potential of PCT as tree-based planners in deliberately solving packing problems of industrial significance, including large-scale packing and different variations of BPP setting. A recursive packing method is proposed to decompose large-scale packing into smaller sub-trees while a spatial ensemble mechanism integrates local solutions into global. For different BPP variations with additional decision variables, such as lookahead, buffering, and offline packing, we propose a unified planning framework enabling out-of-the-box problem solving. Extensive evaluations demonstrate that our method outperforms existing online BPP baselines and is versatile in incorporating various practical constraints. The planning process excels across large-scale problems and diverse problem variations. We develop a real-world packing robot for industrial warehousing, with careful designs accounting for constrained placement and transportation stability. Our packing robot operates reliably and efficiently on unprotected pallets at 10 seconds per box. It achieves averagely 19 boxes per pallet with 57.4% space utilization for relatively large-size boxes.","authors":["Hang Zhao","Juzhan Xu","Kexiong Yu","Ruizhen Hu","Chenyang Zhu","Kai Xu"],"url":"https://arxiv.org/abs/2504.04421"}
{"created":"2025-04-30","title":"Video-Bench: Human-Aligned Video Generation Benchmark","abstract":"Video generation assessment is essential for ensuring that generative models produce visually realistic, high-quality videos while aligning with human expectations. Current video generation benchmarks fall into two main categories: traditional benchmarks, which use metrics and embeddings to evaluate generated video quality across multiple dimensions but often lack alignment with human judgments; and large language model (LLM)-based benchmarks, though capable of human-like reasoning, are constrained by a limited understanding of video quality metrics and cross-modal consistency. To address these challenges and establish a benchmark that better aligns with human preferences, this paper introduces Video-Bench, a comprehensive benchmark featuring a rich prompt suite and extensive evaluation dimensions. This benchmark represents the first attempt to systematically leverage MLLMs across all dimensions relevant to video generation assessment in generative models. By incorporating few-shot scoring and chain-of-query techniques, Video-Bench provides a structured, scalable approach to generated video evaluation. Experiments on advanced models including Sora demonstrate that Video-Bench achieves superior alignment with human preferences across all dimensions. Moreover, in instances where our framework's assessments diverge from human evaluations, it consistently offers more objective and accurate insights, suggesting an even greater potential advantage over traditional human judgment.","authors":["Hui Han","Siyuan Li","Jiaqi Chen","Yiwen Yuan","Yuling Wu","Chak Tou Leong","Hanwen Du","Junchen Fu","Youhua Li","Jie Zhang","Chi Zhang","Li-jia Li","Yongxin Ni"],"url":"https://arxiv.org/abs/2504.04907"}
{"created":"2025-04-30","title":"LLM-based Automated Grading with Human-in-the-Loop","abstract":"The rise of artificial intelligence (AI) technologies, particularly large language models (LLMs), has brought significant advancements to the field of education. Among various applications, automatic short answer grading (ASAG), which focuses on evaluating open-ended textual responses, has seen remarkable progress with the introduction of LLMs. These models not only enhance grading performance compared to traditional ASAG approaches but also move beyond simple comparisons with predefined \"golden\" answers, enabling more sophisticated grading scenarios, such as rubric-based evaluation. However, existing LLM-powered methods still face challenges in achieving human-level grading performance in rubric-based assessments due to their reliance on fully automated approaches. In this work, we explore the potential of LLMs in ASAG tasks by leveraging their interactive capabilities through a human-in-the-loop (HITL) approach. Our proposed framework, GradeHITL, utilizes the generative properties of LLMs to pose questions to human experts, incorporating their insights to refine grading rubrics dynamically. This adaptive process significantly improves grading accuracy, outperforming existing methods and bringing ASAG closer to human-level evaluation.","authors":["Hang Li","Yucheng Chu","Kaiqi Yang","Yasemin Copur-Gencturk","Jiliang Tang"],"url":"https://arxiv.org/abs/2504.05239"}
{"created":"2025-04-30","title":"Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation","abstract":"The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam benchmark to date for the multilingual evaluation of vision-language models. Kaleidoscope is a large-scale, in-language multimodal benchmark designed to evaluate VLMs across diverse languages and visual inputs. Kaleidoscope covers 18 languages and 14 different subjects, amounting to a total of 20,911 multiple-choice questions. Built through an open science collaboration with a diverse group of researchers worldwide, Kaleidoscope ensures linguistic and cultural authenticity. We evaluate top-performing multilingual vision-language models and find that they perform poorly on low-resource languages and in complex multimodal scenarios. Our results highlight the need for progress on culturally inclusive multimodal evaluation frameworks.","authors":["Israfel Salazar","Manuel Fern\\'andez Burda","Shayekh Bin Islam","Arshia Soltani Moakhar","Shivalika Singh","Fabian Farestam","Angelika Romanou","Danylo Boiko","Dipika Khullar","Mike Zhang","Dominik Krzemi\\'nski","Jekaterina Novikova","Lu\\'isa Shimabucoro","Joseph Marvin Imperial","Rishabh Maheshwary","Sharad Duwal","Alfonso Amayuelas","Swati Rajwal","Jebish Purbey","Ahmed Ruby","Nicholas Popovi\\v{c}","Marek Suppa","Azmine Toushik Wasi","Ram Mohan Rao Kadiyala","Olga Tsymboi","Maksim Kostritsya","Bardia Soltani Moakhar","Gabriel da Costa Merlin","Ot\\'avio Ferracioli Coletti","Maral Jabbari Shiviari","MohammadAmin farahani fard","Silvia Fernandez","Mar\\'ia Grandury","Dmitry Abulkhanov","Drishti Sharma","Andre Guarnier De Mitri","Leticia Bossatto Marchezi","Setayesh Heydari","Johan Obando-Ceron","Nazar Kohut","Beyza Ermis","Desmond Elliott","Enzo Ferrante","Sara Hooker","Marzieh Fadaee"],"url":"https://arxiv.org/abs/2504.07072"}
{"created":"2025-04-30","title":"CAShift: Benchmarking Log-Based Cloud Attack Detection under Normality Shift","abstract":"With the rapid advancement of cloud-native computing, securing cloud environments has become an important task. Log-based Anomaly Detection (LAD) is the most representative technique used in different systems for attack detection and safety guarantee, where multiple LAD methods and relevant datasets have been proposed. However, even though some of these datasets are specifically prepared for cloud systems, they only cover limited cloud behaviors and lack information from a whole-system perspective. Another critical issue to consider is normality shift, which implies that the test distribution could differ from the training distribution and highly affect the performance of LAD. Unfortunately, existing works only focus on simple shift types such as chronological changes, while other cloud-specific shift types are ignored. Therefore, a dataset that captures diverse cloud system behaviors and various types of normality shifts is essential.","authors":["Jiongchi Yu","Xiaofei Xie","Qiang Hu","Bowen Zhang","Ziming Zhao","Yun Lin","Lei Ma","Ruitao Feng","Frank Liauw"],"url":"https://arxiv.org/abs/2504.09115"}
{"created":"2025-04-30","title":"Negate or Embrace: On How Misalignment Shapes Multimodal Representation Learning","abstract":"Multimodal representation learning, exemplified by multimodal contrastive learning (MMCL) using image-text pairs, aims to learn powerful representations by aligning cues across modalities. This approach relies on the core assumption that the exemplar image-text pairs constitute two representations of an identical concept. However, recent research has revealed that real-world datasets often exhibit misalignment. There are two distinct viewpoints on how to address this issue: one suggests mitigating the misalignment, and the other leveraging it. We seek here to reconcile these seemingly opposing perspectives, and to provide a practical guide for practitioners. Using latent variable models we thus formalize misalignment by introducing two specific mechanisms: selection bias, where some semantic variables are missing, and perturbation bias, where semantic variables are distorted -- both affecting latent variables shared across modalities. Our theoretical analysis demonstrates that, under mild assumptions, the representations learned by MMCL capture exactly the information related to the subset of the semantic variables invariant to selection and perturbation biases. This provides a unified perspective for understanding misalignment. Based on this, we further offer actionable insights into how misalignment should inform the design of real-world ML systems. We validate our theoretical findings through extensive empirical studies on both synthetic data and real image-text datasets, shedding light on the nuanced impact of misalignment on multimodal representation learning.","authors":["Yichao Cai","Yuhang Liu","Erdun Gao","Tianjiao Jiang","Zhen Zhang","Anton van den Hengel","Javen Qinfeng Shi"],"url":"https://arxiv.org/abs/2504.10143"}
{"created":"2025-04-30","title":"CCSK:Cognitive Convection of Self-Knowledge Based Retrieval Augmentation for Large Language Models","abstract":"The performance of large language models (LLMs) in Q&amp;A task increased substantially through Retrieval-Augmented Generation (RAG) which brings in external knowledge. However, the main difficulty lies in balancing the inherent self-knowledge of LLMs with external information retrieval (IR). The current threshold-based methods apply one-dimensional static mechanisms with single criterion. As a result, their IR decisions might be irrelevant to the LLMs' response under difficult queries. To alleviate this problem, we propose Cognitive Convection of Self-Knowledge (CCSK). Different from traditional methods that maintain single fixed IR activation criteria, CCSK implements a dynamic joint decision process via a Siamese Network module and a Response Quality Model. The Siamese Network calculates the cosine similarity between the current query and the historical queries. The Response Quality Model evaluates the responses of LLMs through LightGBM. The final decision of the CCSK is derived from the outputs of the two modules, as well as text features fused using a multi-head attention mechanism. Extensive experiments on real-world datasets show that CCSK significantly enhances the model's effectiveness in information retrieval.","authors":["Jianling Lu","Mingqi Lv","Tieming Chen"],"url":"https://arxiv.org/abs/2504.10498"}
{"created":"2025-04-30","title":"Activated LoRA: Fine-tuned LLMs for Intrinsics","abstract":"Low-Rank Adaptation (LoRA) has emerged as a highly efficient framework for finetuning the weights of large foundation models, and has become the go-to method for data-driven customization of LLMs. Despite the promise of highly customized behaviors and capabilities, switching between relevant LoRAs in a multiturn setting is highly inefficient, as the key-value (KV) cache of the entire turn history must be recomputed with the LoRA weights before generation can begin. To address this problem, we propose Activated LoRA (aLoRA), which modifies the LoRA framework to only adapt weights for the tokens in the sequence \\emph{after} the aLoRA is invoked. This change crucially allows aLoRA to accept the base model's KV cache of the input string, meaning that aLoRA can be instantly activated whenever needed in a chain without recomputing the cache. This enables building what we call \\emph{intrinsics}, i.e. highly specialized models invoked to perform well-defined operations on portions of an input chain or conversation that otherwise uses the base model by default. We use aLoRA to train a set of intrinsics models, demonstrating competitive accuracy with standard LoRA while achieving significant inference benefits.","authors":["Kristjan Greenewald","Luis Lastras","Thomas Parnell","Vraj Shah","Lucian Popa","Giulio Zizzo","Chulaka Gunasekara","Ambrish Rawat","David Cox"],"url":"https://arxiv.org/abs/2504.12397"}
{"created":"2025-04-30","title":"Imperative MPC: An End-to-End Self-Supervised Learning with Differentiable MPC for UAV Attitude Control","abstract":"Modeling and control of nonlinear dynamics are critical in robotics, especially in scenarios with unpredictable external influences and complex dynamics. Traditional cascaded modular control pipelines often yield suboptimal performance due to conservative assumptions and tedious parameter tuning. Pure data-driven approaches promise robust performance but suffer from low sample efficiency, sim-to-real gaps, and reliance on extensive datasets. Hybrid methods combining learning-based and traditional model-based control in an end-to-end manner offer a promising alternative. This work presents a self-supervised learning framework combining learning-based inertial odometry (IO) module and differentiable model predictive control (d-MPC) for Unmanned Aerial Vehicle (UAV) attitude control. The IO denoises raw IMU measurements and predicts UAV attitudes, which are then optimized by MPC for control actions in a bi-level optimization (BLO) setup, where the inner MPC optimizes control actions and the upper level minimizes discrepancy between real-world and predicted performance. The framework is thus end-to-end and can be trained in a self-supervised manner. This approach combines the strength of learning-based perception with the interpretable model-based control. Results show the effectiveness even under strong wind. It can simultaneously enhance both the MPC parameter learning and IMU prediction performance.","authors":["Haonan He","Yuheng Qiu","Junyi Geng"],"url":"https://arxiv.org/abs/2504.13088"}
{"created":"2025-04-30","title":"Probing and Inducing Combinational Creativity in Vision-Language Models","abstract":"The ability to combine existing concepts into novel ideas stands as a fundamental hallmark of human intelligence. Recent advances in Vision-Language Models (VLMs) like GPT-4V and DALLE-3 have sparked debate about whether their outputs reflect combinational creativity--defined by M. A. Boden (1998) as synthesizing novel ideas through combining existing concepts--or sophisticated pattern matching of training data. Drawing inspiration from cognitive science, we investigate the combinational creativity of VLMs from the lens of concept blending. We propose the Identification-Explanation-Implication (IEI) framework, which decomposes creative processes into three levels: identifying input spaces, extracting shared attributes, and deriving novel semantic implications. To validate this framework, we curate CreativeMashup, a high-quality dataset of 666 artist-generated visual mashups annotated according to the IEI framework. Through extensive experiments, we demonstrate that in comprehension tasks, best VLMs have surpassed average human performance while falling short of expert-level understanding; in generation tasks, incorporating our IEI framework into the generation pipeline significantly enhances the creative quality of VLMs' outputs. Our findings establish both a theoretical foundation for evaluating artificial creativity and practical guidelines for improving creative generation in VLMs.","authors":["Yongqian Peng","Yuxi Ma","Mengmeng Wang","Yuxuan Wang","Yizhou Wang","Chi Zhang","Yixin Zhu","Zilong Zheng"],"url":"https://arxiv.org/abs/2504.13120"}
{"created":"2025-04-30","title":"Perception Encoder: The best visual embeddings are not at the output of the network","abstract":"We introduce Perception Encoder (PE), a state-of-the-art vision encoder for image and video understanding trained via simple vision-language learning. Traditionally, vision encoders have relied on a variety of pretraining objectives, each tailored to specific downstream tasks such as classification, captioning, or localization. Surprisingly, after scaling our carefully tuned image pretraining recipe and refining with our robust video data engine, we find that contrastive vision-language training alone can produce strong, general embeddings for all of these downstream tasks. There is only one caveat: these embeddings are hidden within the intermediate layers of the network. To draw them out, we introduce two alignment methods: language alignment for multimodal language modeling, and spatial alignment for dense prediction. Together, our PE family of models achieves best-in-class results on a wide variety of tasks, including (1) zero-shot image and video classification and retrieval, simultaneously obtaining 86.6 average zero-shot ImageNet robustness and 76.9 zero-shot Kinetics-400 video classification; (2) document, image, and video Q&amp;A, enabling 94.6 DocVQA, 80.9 InfographicVQA, and 82.7 PerceptionTest with an 8B LLM; and (3) spatial tasks such as detection, tracking, and depth estimation, setting a new COCO state-of-the-art of 66.0 box mAP. To foster further research, we release our models, code, and novel dataset of synthetically and human-annotated videos: https://github.com/facebookresearch/perception_models","authors":["Daniel Bolya","Po-Yao Huang","Peize Sun","Jang Hyun Cho","Andrea Madotto","Chen Wei","Tengyu Ma","Jiale Zhi","Jathushan Rajasegaran","Hanoona Rasheed","Junke Wang","Marco Monteiro","Hu Xu","Shiyu Dong","Nikhila Ravi","Daniel Li","Piotr Doll\\'ar","Christoph Feichtenhofer"],"url":"https://arxiv.org/abs/2504.13181"}
{"created":"2025-04-30","title":"Towards Accurate and Interpretable Neuroblastoma Diagnosis via Contrastive Multi-scale Pathological Image Analysis","abstract":"Neuroblastoma, adrenal-derived, is among the most common pediatric solid malignancies, characterized by significant clinical heterogeneity. Timely and accurate pathological diagnosis from hematoxylin and eosin-stained whole slide images is critical for patient prognosis. However, current diagnostic practices primarily rely on subjective manual examination by pathologists, leading to inconsistent accuracy. Existing automated whole slide image classification methods encounter challenges such as poor interpretability, limited feature extraction capabilities, and high computational costs, restricting their practical clinical deployment. To overcome these limitations, we propose CMSwinKAN, a contrastive-learning-based multi-scale feature fusion model tailored for pathological image classification, which enhances the Swin Transformer architecture by integrating a Kernel Activation Network within its multilayer perceptron and classification head modules, significantly improving both interpretability and accuracy. By fusing multi-scale features and leveraging contrastive learning strategies, CMSwinKAN mimics clinicians' comprehensive approach, effectively capturing global and local tissue characteristics. Additionally, we introduce a heuristic soft voting mechanism guided by clinical insights to seamlessly bridge patch-level predictions to whole slide image-level classifications. We validate CMSwinKAN on the PpNTs dataset, which was collaboratively established with our partner hospital and the publicly accessible BreakHis dataset. Results demonstrate that CMSwinKAN performs better than existing state-of-the-art pathology-specific models pre-trained on large datasets. Our source code is available at https://github.com/JSLiam94/CMSwinKAN.","authors":["Zhu Zhu","Shuo Jiang","Jingyuan Zheng","Yawen Li","Yifei Chen","Manli Zhao","Weizhong Gu","Feiwei Qin","Jinhu Wang","Gang Yu"],"url":"https://arxiv.org/abs/2504.13754"}
{"created":"2025-04-30","title":"Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning","abstract":"We introduce Seed1.5-Thinking, capable of reasoning through thinking before responding, resulting in improved performance on a wide range of benchmarks. Seed1.5-Thinking achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on non-reasoning tasks, indicating its broader applicability. Compared to other state-of-the-art reasoning models, Seed1.5-Thinking is a Mixture-of-Experts (MoE) model with a relatively small size, featuring 20B activated and 200B total parameters. As part of our effort to assess generalized reasoning, we develop two internal benchmarks, BeyondAIME and Codeforces, both of which will be publicly released to support future research. Model trial link: https://www.volcengine.com/experience/ark.","authors":["ByteDance Seed",":","Jiaze Chen","Tiantian Fan","Xin Liu","Lingjun Liu","Zhiqi Lin","Mingxuan Wang","Chengyi Wang","Xiangpeng Wei","Wenyuan Xu","Yufeng Yuan","Yu Yue","Lin Yan","Qiying Yu","Xiaochen Zuo","Chi Zhang","Ruofei Zhu","Zhecheng An","Zhihao Bai","Yu Bao","Xingyan Bin","Jiangjie Chen","Feng Chen","Hongmin Chen","Riwei Chen","Liangqiang Chen","Zixin Chen","Jinsong Chen","Siyan Chen","Kaiyuan Chen","Zhi Chen","Jin Chen","Jiecao Chen","Jinxin Chi","Weinan Dai","Ning Dai","Jiahui Dai","Shihan Dou","Yantao Du","Zhengyin Du","Jianhui Duan","Chen Dun","Ting-Han Fan","Jiazhan Feng","Junda Feng","Ziyuan Feng","Yuwei Fu","Wenqi Fu","Hanjie Fu","Hao Ge","Hongyi Guo","Mingji Han","Li Han","Wenhao Hao","Xintong Hao","Qianyu He","Jerry He","Feng He","Wen Heng","Zehua Hong","Qi Hou","Liang Hu","Shengding Hu","Nan Hu","Kai Hua","Qi Huang","Ziyue Huang","Hongzhi Huang","Zihao Huang","Ting Huang","Wenhao Huang","Wei Jia","Bin Jia","Xiaoying Jia","Yuhua Jiang","Haobin Jiang","Ziheng Jiang","Kaihua Jiang","Chengquan Jiang","Jianpeng Jiao","Xiaoran Jin","Xing Jin","Xunhao Lai","Zheng Li","Xiang Li","Liyi Li","Hongkai Li","Zheng Li","Shengxian Wan","Ya Wang","Yunshui Li","Chenggang Li","Niuniu Li","Siyu Li","Xi Li","Xiao Li","Aoyan Li","Yuntao Li","Nianning Liang","Xinnian Liang","Haibin Lin","Weijian Lin","Ye Lin","Zhicheng Liu","Guanlin Liu","Guanlin Liu","Chenxiao Liu","Yan Liu","Gaohong Liu","Juncai Liu","Chundian Liu","Deyi Liu","Kaibo Liu","Siyao Liu","Qi Liu","Yongfei Liu","Kang Liu","Gan Liu","Boyi Liu","Rui Long","Weiqiang Lou","Chenwei Lou","Xiang Luo","Yao Luo","Caiping Lv","Heyang Lv","Bole Ma","Qianli Ma","Hongzhi Ma","Yiyuan Ma","Jin Ma","Wenchang Ma","Tingting Ma","Chen Mao","Qiyang Min","Zhe Nan","Guanghan Ning","Jinxiang Ou","Haojie Pan","Renming Pang","Yanghua Peng","Tao Peng","Lihua Qian","Lihua Qian","Mu Qiao","Meng Qu","Cheng Ren","Hongbin Ren","Yong Shan","Wei Shen","Ke Shen","Kai Shen","Guangming Sheng","Jinlong Shi","Wenlei Shi","Guang Shi","Shuai Shuai Cao","Yuxin Song","Zuquan Song","Jing Su","Yifan Sun","Tao Sun","Zewei Sun","Borui Wan","Zihan Wang","Xiaohui Wang","Xi Wang","Shuguang Wang","Jun Wang","Qinlong Wang","Chenyuan Wang","Shuai Wang","Zihan Wang","Changbao Wang","Jiaqiang Wang","Shihang Wang","Xuwu Wang","Zaiyuan Wang","Yuxuan Wang","Wenqi Wang","Taiqing Wang","Chengzhi Wei","Houmin Wei","Ziyun Wei","Shufa Wei","Zheng Wu","Yonghui Wu","Yangjun Wu","Bohong Wu","Shuang Wu","Jingqiao Wu","Ning Wu","Shuangzhi Wu","Jianmin Wu","Chenguang Xi","Fan Xia","Yuqiao Xian","Liang Xiang","Boren Xiang","Bowen Xiao","Zhen Xiao","Xia Xiao","Yongsheng Xiao","Chao Xin","Shulin Xin","Yuwen Xiong","Jingjing Xu","Ziwen Xu","Chenyin Xu","Jiayi Xu","Yifan Xu","Wei Xu","Yufei Xu","Shikun Xu","Shipeng Yan","Shen Yan","Qingping Yang","Xi Yang","Tianhao Yang","Yuehang Yang","Yuan Yang","Ximing Yang","Zeyu Yang","Guang Yang","Yifan Yang","Xuesong Yao","Bairen Yi","Fan Yin","Jianian Yin","Ziqiang Ying","Xiangyu Yu","Hongli Yu","Song Yu","Menghan Yu","Huan Yu","Siyu Yuan","Jun Yuan","Yutao Zeng","Tianyang Zhan","Zheng Zhang","Yun Zhang","Mofan Zhang","Wang Zhang","Ru Zhang","Zhi Zhang","Tianqi Zhang","Xinyi Zhang","Zhexi Zhang","Sijun Zhang","Wenqiang Zhang","Xiangxiang Zhang","Yongtao Zhang","Yuyu Zhang","Ge Zhang","He Zhang","Yue Zhang","Renjie Zheng","Ningxin Zheng","Zhuolin Zheng","Yaowei Zheng","Chen Zheng","Xiaoyun Zhi","Wanjun Zhong","Cheng Zhong","Zheng Zhong","Baoquan Zhong","Xun Zhou","Na Zhou","Huan Zhou","Hang Zhu","Defa Zhu","Wenjia Zhu","Lei Zuo"],"url":"https://arxiv.org/abs/2504.13914"}
{"created":"2025-04-30","title":"A Collaborative Platform for Soil Organic Carbon Inference Based on Spatiotemporal Remote Sensing Data","abstract":"Soil organic carbon (SOC) is a key indicator of soil health, fertility, and carbon sequestration, making it essential for sustainable land management and climate change mitigation. However, large-scale SOC monitoring remains challenging due to spatial variability, temporal dynamics, and multiple influencing factors. We present WALGREEN, a platform that enhances SOC inference by overcoming limitations of current applications. Leveraging machine learning and diverse soil samples, WALGREEN generates predictive models using historical public and private data. Built on cloud-based technologies, it offers a user-friendly interface for researchers, policymakers, and land managers to access carbon data, analyze trends, and support evidence-based decision-making. Implemented in Python, Java, and JavaScript, WALGREEN integrates Google Earth Engine and Sentinel Copernicus via scripting, OpenLayers, and Thymeleaf in a Model-View-Controller framework. This paper aims to advance soil science, promote sustainable agriculture, and drive critical ecosystem responses to climate change.","authors":["Jose Manuel Aroca-Fernandez","Jose Francisco Diez-Pastor","Pedro Latorre-Carmona","Victor Elvira","Gustau Camps-Valls","Rodrigo Pascual","Cesar Garcia-Osorio"],"url":"https://arxiv.org/abs/2504.13962"}
{"created":"2025-04-30","title":"Mixed-Precision Conjugate Gradient Solvers with RL-Driven Precision Tuning","abstract":"This paper presents a novel reinforcement learning (RL) framework for dynamically optimizing numerical precision in the preconditioned conjugate gradient (CG) method. By modeling precision selection as a Markov Decision Process (MDP), we employ Q-learning to adaptively assign precision levels to key operations, striking an optimal balance between computational efficiency and numerical accuracy, while ensuring stability through double-precision scalar computations and residual computing. In practice, the algorithm is trained on a set of data and subsequently performs inference for precision selection on out-of-sample data, without requiring re-analysis or retraining for new datasets. This enables the method to adapt seamlessly to new problem instances without the computational overhead of recalibration. Our results demonstrate the effectiveness of RL in enhancing solver's performance, marking the first application of RL to mixed-precision numerical methods. The findings highlight the approach's practical advantages, robustness, and scalability, providing valuable insights into its integration with iterative solvers and paving the way for AI-driven advancements in scientific computing.","authors":["Xinye Chen"],"url":"https://arxiv.org/abs/2504.14268"}
{"created":"2025-04-30","title":"DLW-CI: A Dynamic Likelihood-Weighted Cooperative Infotaxis Approach for Multi-Source Search in Urban Environments Using Consumer Drone Networks","abstract":"Consumer-grade drones equipped with low-cost sensors have emerged as a cornerstone of Autonomous Intelligent Systems (AISs) for environmental monitoring and hazardous substance detection in urban environments. However, existing research primarily addresses single-source search problems, overlooking the complexities of real-world urban scenarios where both the location and quantity of hazardous sources remain unknown. To address this issue, we propose the Dynamic Likelihood-Weighted Cooperative Infotaxis (DLW-CI) approach for consumer drone networks. Our approach enhances multi-drone collaboration in AISs by combining infotaxis (a cognitive search strategy) with optimized source term estimation and an innovative cooperative mechanism. Specifically, we introduce a novel source term estimation method that utilizes multiple parallel particle filters, with each filter dedicated to estimating the parameters of a potentially unknown source within the search scene. Furthermore, we develop a cooperative mechanism based on dynamic likelihood weights to prevent multiple drones from simultaneously estimating and searching for the same source, thus optimizing the energy efficiency and search coverage of the consumer AIS. Experimental results demonstrate that the DLW-CI approach significantly outperforms baseline methods regarding success rate, accuracy, and root mean square error, particularly in scenarios with relatively few sources, regardless of the presence of obstacles. Also, the effectiveness of the proposed approach is verified in a diffusion scenario generated by the computational fluid dynamics (CFD) model. Research findings indicate that our approach could improve source estimation accuracy and search efficiency by consumer drone-based AISs, making a valuable contribution to environmental safety monitoring applications within smart city infrastructure.","authors":["Xiaoran Zhang","Yatai Ji","Yong Zhao","Chuan Ai","Bin Chen","Zhengqiu Zhu"],"url":"https://arxiv.org/abs/2504.14330"}
{"created":"2025-04-30","title":"NTIRE 2025 Challenge on Image Super-Resolution ($\\times$4): Methods and Results","abstract":"This paper presents the NTIRE 2025 image super-resolution ($\\times$4) challenge, one of the associated competitions of the 10th NTIRE Workshop at CVPR 2025. The challenge aims to recover high-resolution (HR) images from low-resolution (LR) counterparts generated through bicubic downsampling with a $\\times$4 scaling factor. The objective is to develop effective network designs or solutions that achieve state-of-the-art SR performance. To reflect the dual objectives of image SR research, the challenge includes two sub-tracks: (1) a restoration track, emphasizes pixel-wise accuracy and ranks submissions based on PSNR; (2) a perceptual track, focuses on visual realism and ranks results by a perceptual score. A total of 286 participants registered for the competition, with 25 teams submitting valid entries. This report summarizes the challenge design, datasets, evaluation protocol, the main results, and methods of each team. The challenge serves as a benchmark to advance the state of the art and foster progress in image SR.","authors":["Zheng Chen","Kai Liu","Jue Gong","Jingkai Wang","Lei Sun","Zongwei Wu","Radu Timofte","Yulun Zhang","Xiangyu Kong","Xiaoxuan Yu","Hyunhee Park","Suejin Han","Hakjae Jeon","Dafeng Zhang","Hyung-Ju Chun","Donghun Ryou","Inju Ha","Bohyung Han","Lu Zhao","Yuyi Zhang","Pengyu Yan","Jiawei Hu","Pengwei Liu","Fengjun Guo","Hongyuan Yu","Pufan Xu","Zhijuan Huang","Shuyuan Cui","Peng Guo","Jiahui Liu","Dongkai Zhang","Heng Zhang","Huiyuan Fu","Huadong Ma","Yanhui Guo","Sisi Tian","Xin Liu","Jinwen Liang","Jie Liu","Jie Tang","Gangshan Wu","Zeyu Xiao","Zhuoyuan Li","Yinxiang Zhang","Wenxuan Cai","Vijayalaxmi Ashok Aralikatti","Nikhil Akalwadi","G Gyaneshwar Rao","Chaitra Desai","Ramesh Ashok Tabib","Uma Mudenagudi","Marcos V. Conde","Alejandro Merino","Bruno Longarela","Javier Abad","Weijun Yuan","Zhan Li","Zhanglu Chen","Boyang Yao","Aagam Jain","Milan Kumar Singh","Ankit Kumar","Shubh Kawa","Divyavardhan Singh","Anjali Sarvaiya","Kishor Upla","Raghavendra Ramachandra","Chia-Ming Lee","Yu-Fan Lin","Chih-Chung Hsu","Risheek V Hiremath","Yashaswini Palani","Yuxuan Jiang","Qiang Zhu","Siyue Teng","Fan Zhang","Shuyuan Zhu","Bing Zeng","David Bull","Jingwei Liao","Yuqing Yang","Wenda Shao","Junyi Zhao","Qisheng Xu","Kele Xu","Sunder Ali Khowaja","Ik Hyun Lee","Snehal Singh Tomar","Rajarshi Ray","Klaus Mueller","Sachin Chaudhary","Surya Vashisth","Akshay Dudhane","Praful Hambarde","Satya Naryan Tazi","Prashant Patil","Santosh Kumar Vipparthi","Subrahmanyam Murala","Bilel Benjdira","Anas M. Ali","Wadii Boulila","Zahra Moammeri","Ahmad Mahmoudi-Aznaveh","Ali Karbasi","Hossein Motamednia","Liangyan Li","Guanhua Zhao","Kevin Le","Yimo Ning","Haoxuan Huang","Jun Chen"],"url":"https://arxiv.org/abs/2504.14582"}
{"created":"2025-04-30","title":"Computational Typology","abstract":"Typology is a subfield of linguistics that focuses on the study and classification of languages based on their structural features. Unlike genealogical classification, which examines the historical relationships between languages, typology seeks to understand the diversity of human languages by identifying common properties and patterns, known as universals. In recent years, computational methods have played an increasingly important role in typological research, enabling the analysis of large-scale linguistic data and the testing of hypotheses about language structure and evolution. This article provides an illustration of the benefits of computational statistical modeling in typology.","authors":["Gerhard J\\\"ager"],"url":"https://arxiv.org/abs/2504.15642"}
{"created":"2025-04-30","title":"ForesightNav: Learning Scene Imagination for Efficient Exploration","abstract":"Understanding how humans leverage prior knowledge to navigate unseen environments while making exploratory decisions is essential for developing autonomous robots with similar abilities. In this work, we propose ForesightNav, a novel exploration strategy inspired by human imagination and reasoning. Our approach equips robotic agents with the capability to predict contextual information, such as occupancy and semantic details, for unexplored regions. These predictions enable the robot to efficiently select meaningful long-term navigation goals, significantly enhancing exploration in unseen environments. We validate our imagination-based approach using the Structured3D dataset, demonstrating accurate occupancy prediction and superior performance in anticipating unseen scene geometry. Our experiments show that the imagination module improves exploration efficiency in unseen environments, achieving a 100% completion rate for PointNav and an SPL of 67% for ObjectNav on the Structured3D Validation split. These contributions demonstrate the power of imagination-driven reasoning for autonomous systems to enhance generalizable and efficient exploration.","authors":["Hardik Shah","Jiaxu Xing","Nico Messikommer","Boyang Sun","Marc Pollefeys","Davide Scaramuzza"],"url":"https://arxiv.org/abs/2504.16062"}
{"created":"2025-04-30","title":"Virology Capabilities Test (VCT): A Multimodal Virology Q&A Benchmark","abstract":"We present the Virology Capabilities Test (VCT), a large language model (LLM) benchmark that measures the capability to troubleshoot complex virology laboratory protocols. Constructed from the inputs of dozens of PhD-level expert virologists, VCT consists of $322$ multimodal questions covering fundamental, tacit, and visual knowledge that is essential for practical work in virology laboratories. VCT is difficult: expert virologists with access to the internet score an average of $22.1\\%$ on questions specifically in their sub-areas of expertise. However, the most performant LLM, OpenAI's o3, reaches $43.8\\%$ accuracy, outperforming $94\\%$ of expert virologists even within their sub-areas of specialization. The ability to provide expert-level virology troubleshooting is inherently dual-use: it is useful for beneficial research, but it can also be misused. Therefore, the fact that publicly available models outperform virologists on VCT raises pressing governance considerations. We propose that the capability of LLMs to provide expert-level troubleshooting of dual-use virology work should be integrated into existing frameworks for handling dual-use technologies in the life sciences.","authors":["Jasper G\\\"otting","Pedro Medeiros","Jon G Sanders","Nathaniel Li","Long Phan","Karam Elabd","Lennart Justen","Dan Hendrycks","Seth Donoughe"],"url":"https://arxiv.org/abs/2504.16137"}
{"created":"2025-04-30","title":"Using games and universal trees to characterise the nondeterministic index of tree languages","abstract":"The parity index problem of tree automata asks, given a regular tree language $L$ and a set of priorities $J$, is $L$ $J$-feasible, that is, recognised by a nondeterministic parity automaton with priorities $J$? This is a long-standing open problem, of which only a few sub-cases and variations are known to be decidable. In a significant but technically difficult step, Colcombet and L\\\"oding reduced the problem to the uniform universality of distance-parity automata. In this article, we revisit the index problem using tools from the parity game literature.","authors":["Olivier Idir","Karoliina Lehtinen"],"url":"https://arxiv.org/abs/2504.16819"}
{"created":"2025-04-30","title":"Utilizing Dynamic Time Warping for Pandemic Surveillance: Understanding the Relationship between Google Trends Network Metrics and COVID-19 Incidences","abstract":"The premise of network statistics derived from Google Trends data to foresee COVID-19 disease progression is gaining momentum in infodemiology. This approach was applied in Metro Manila, National Capital Region, Philippines. Through dynamic time warping (DTW), the temporal alignment was quantified between network metrics and COVID-19 case trajectories, and systematically explored 320 parameter configurations including two network metrics (network density and clustering coefficient), two data preprocessing methods (Rescaling Daily Data and MSV), multiple thresholds, two correlation window sizes, and Sakoe-Chiba band constraints. Results from the Kruskal-Wallis tests revealed that five of the six parameters significantly influenced alignment quality, with the disease comparison type (active cases vs. confirmed cases) demonstrating the strongest effect. The optimal configuration, which is using the network density statistic with a Rescaling Daily Data transformation, a threshold of 0.8, a 15-day window, and a 50-day radius constraint, achieved a DTW score of 36.30. This indicated substantial temporal alignment with the COVID-19 confirmed cases data. The discoveries demonstrate that network metrics rooted from online search behavior can serve as complementary indicators for epidemic surveillance in urban locations like Metro Manila. This strategy leverages the Philippines' extensive online usage during the pandemic to provide potentially valuable early signals of disease spread, and offers a supplementary tool for public health monitoring in resource-limited situations.","authors":["Michael T. Lopez II","Cheska Elise Hung","Maria Regina Justina E. Estuar"],"url":"https://arxiv.org/abs/2504.17146"}
{"created":"2025-04-30","title":"TimeSoccer: An End-to-End Multimodal Large Language Model for Soccer Commentary Generation","abstract":"Soccer is a globally popular sporting event, typically characterized by long matches and distinctive highlight moments. Recent advances in Multimodal Large Language Models (MLLMs) offer promising capabilities in temporal grounding and video understanding, soccer commentary generation often requires precise temporal localization and semantically rich descriptions over long-form video. However, existing soccer MLLMs often rely on the temporal a priori for caption generation, so they cannot process the soccer video end-to-end. While some traditional approaches follow a two-step paradigm that is complex and fails to capture the global context to achieve suboptimal performance. To solve the above issues, we present TimeSoccer, the first end-to-end soccer MLLM for Single-anchor Dense Video Captioning (SDVC) in full-match soccer videos. TimeSoccer jointly predicts timestamps and generates captions in a single pass, enabling global context modeling across 45-minute matches. To support long video understanding of soccer matches, we introduce MoFA-Select, a training-free, motion-aware frame compression module that adaptively selects representative frames via a coarse-to-fine strategy, and incorporates complementary training paradigms to strengthen the model's ability to handle long temporal sequences. Extensive experiments demonstrate that our TimeSoccer achieves State-of-The-Art (SoTA) performance on the SDVC task in an end-to-end form, generating high-quality commentary with accurate temporal alignment and strong semantic relevance.","authors":["Ling You","Wenxuan Huang","Xinni Xie","Xiangyi Wei","Bangyan Li","Shaohui Lin","Yang Li","Changbo Wang"],"url":"https://arxiv.org/abs/2504.17365"}
{"created":"2025-04-30","title":"Safe to Stay: Psychological Safety Sustains Participation in Pull-based Open Source Projects","abstract":"Background: Psychological safety refers to the belief that team members can speak up or make mistakes without fear of negative consequences. While it is recognized as important in traditional software teams, its role in open-source software development remains understudied. Open-source contributors often collaborate without formal roles or structures, where interpersonal relationships can significantly influence participation. Code review, a central and collaborative activity in modern software development, offers a valuable context for observing such team interactions. Aims: This study investigates whether team-level psychological safety, inferred from code review activities, is associated with contributors' sustained participation in open-source projects. Method: Using data from 60,684 pull requests across multiple repositories, we developed a psychological safety index based on observable cues such as merge decisions, comment activity, interaction diversity, and mentions. We analyzed the relationship between this index and contributors' short-term (within 1 year) and long-term (over 4--5 years) sustained participation using three logistic regression models. Results: Contributors are more likely to remain active in repositories with higher levels of psychological safety. Psychological safety is positively associated with both short-term and long-term sustained participation. However, prior participation emerges as a stronger predictor of future engagement, reducing the effect of psychological safety when accounted for. Conclusions: This study introduces a scalable, data-driven approach to measuring psychological safety through pull request data and provides new empirical evidence of its relevance in sustaining participation within open-source development.","authors":["Emeralda Sesari","Federica Sarro","Ayushi Rastogi"],"url":"https://arxiv.org/abs/2504.17510"}
{"created":"2025-04-30","title":"High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures","abstract":"This work presents an overview of the technical details behind a high performance reinforcement learning policy deployment with the Spot RL Researcher Development Kit for low level motor access on Boston Dynamics Spot. This represents the first public demonstration of an end to end end reinforcement learning policy deployed on Spot hardware with training code publicly available through Nvidia IsaacLab and deployment code available through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean Discrepancy to quantify the distributional dissimilarity of data collected on hardware and in simulation to measure our sim2real gap. We use these measures as a scoring function for the Covariance Matrix Adaptation Evolution Strategy to optimize simulated parameters that are unknown or difficult to measure from Spot. Our procedure for modeling and training produces high quality reinforcement learning policies capable of multiple gaits, including a flight phase. We deploy policies capable of over 5.2ms locomotion, more than triple Spots default controller maximum speed, robustness to slippery surfaces, disturbance rejection, and overall agility previously unseen on Spot. We detail our method and release our code to support future work on Spot with the low level API.","authors":["AJ Miller","Fangzhou Yu","Michael Brauckmann","Farbod Farshidian"],"url":"https://arxiv.org/abs/2504.17857"}
{"created":"2025-04-30","title":"VIGMA: An Open-Access Framework for Visual Gait and Motion Analytics","abstract":"Gait disorders are commonly observed in older adults, who frequently experience various issues related to walking. Additionally, researchers and clinicians extensively investigate mobility related to gait in typically and atypically developing children, athletes, and individuals with orthopedic and neurological disorders. Effective gait analysis enables the understanding of the causal mechanisms of mobility and balance control of patients, the development of tailored treatment plans to improve mobility, the reduction of fall risk, and the tracking of rehabilitation progress. However, analyzing gait data is a complex task due to the multivariate nature of the data, the large volume of information to be interpreted, and the technical skills required. Existing tools for gait analysis are often limited to specific patient groups (e.g., cerebral palsy), only handle a specific subset of tasks in the entire workflow, and are not openly accessible. To address these shortcomings, we conducted a requirements assessment with gait practitioners (e.g., researchers, clinicians) via surveys and identified key components of the workflow, including (1) data processing and (2) data analysis and visualization. Based on the findings, we designed VIGMA, an open-access visual analytics framework integrated with computational notebooks and a Python library, to meet the identified requirements. Notably, the framework supports analytical capabilities for assessing disease progression and for comparing multiple patient groups. We validated the framework through usage scenarios with experts specializing in gait and mobility rehabilitation. VIGMA is available at https://github.com/komar41/VIGMA.","authors":["Kazi Shahrukh Omar","Shuaijie Wang","Ridhuparan Kungumaraju","Tanvi Bhatt","Fabio Miranda"],"url":"https://arxiv.org/abs/2504.17960"}
{"created":"2025-04-30","title":"Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy","abstract":"To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles (UAVs) localization in urban areas where global positioning system (GPS) signals are unavailable. Vision-based methods offer a viable alternative but face severe bandwidth, memory and processing constraints on lightweight UAVs. Inspired by mammalian spatial cognition, we propose a task-oriented communication framework, where UAVs equipped with multi-camera systems extract compact multi-view features and offload localization tasks to edge servers. We introduce the Orthogonally-constrained Variational Information Bottleneck encoder (O-VIB), which incorporates automatic relevance determination (ARD) to prune non-informative features while enforcing orthogonality to minimize redundancy. This enables efficient and accurate localization with minimal transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows that O-VIB achieves high-precision localization under stringent bandwidth budgets. Code and dataset will be made publicly available: github.com/fangzr/TOC-Edge-Aerial.","authors":["Zhengru Fang","Zhenghao Liu","Jingjing Wang","Senkang Hu","Yu Guo","Yiqin Deng","Yuguang Fang"],"url":"https://arxiv.org/abs/2504.18317"}
{"created":"2025-04-30","title":"HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?","abstract":"High-resolution image (HRI) understanding aims to process images with a large number of pixels, such as pathological images and agricultural aerial images, both of which can exceed 1 million pixels. Vision Large Language Models (VLMs) can allegedly handle HRIs, however, there is a lack of a comprehensive benchmark for VLMs to evaluate HRI understanding. To address this gap, we introduce HRScene, a novel unified benchmark for HRI understanding with rich scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic datasets with resolutions ranging from 1,024 $\\times$ 1,024 to 35,503 $\\times$ 26,627. HRScene is collected and re-annotated by 10 graduate-level annotators, covering 25 scenarios, ranging from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-image. The two diagnostic evaluation datasets are synthesized by combining the target image with the gold answer and distracting images in different orders, assessing how well models utilize regions in HRI. We conduct extensive experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show that current VLMs achieve an average accuracy of around 50% on real-world tasks, revealing significant gaps in HRI understanding. Results on synthetic datasets reveal that VLMs struggle to effectively utilize HRI regions, showing significant Regional Divergence and lost-in-middle, shedding light on future research.","authors":["Yusen Zhang","Wenliang Zheng","Aashrith Madasu","Peng Shi","Ryo Kamoi","Hao Zhou","Zhuoyang Zou","Shu Zhao","Sarkar Snigdha Sarathi Das","Vipul Gupta","Xiaoxin Lu","Nan Zhang","Ranran Haoran Zhang","Avitej Iyer","Renze Lou","Wenpeng Yin","Rui Zhang"],"url":"https://arxiv.org/abs/2504.18406"}
{"created":"2025-04-30","title":"\\$PINN - a Domain Decomposition Method for Bayesian Physics-Informed Neural Networks","abstract":"Physics-Informed Neural Networks (PINNs) are a novel computational approach for solving partial differential equations (PDEs) with noisy and sparse initial and boundary data. Although, efficient quantification of epistemic and aleatoric uncertainties in big multi-scale problems remains challenging. We propose \\$PINN a novel method of computing global uncertainty in PDEs using a Bayesian framework, by combining local Bayesian Physics-Informed Neural Networks (BPINN) with domain decomposition. The solution continuity across subdomains is obtained by imposing the flux continuity across the interface of neighboring subdomains. To demonstrate the effectiveness of \\$PINN, we conduct a series of computational experiments on PDEs in 1D and 2D spatial domains. Although we have adopted conservative PINNs (cPINNs), the method can be seamlessly extended to other domain decomposition techniques. The results infer that the proposed method recovers the global uncertainty by computing the local uncertainty exactly more efficiently as the uncertainty in each subdomain can be computed concurrently. The robustness of \\$PINN is verified by adding uncorrelated random noise to the training data up to 15% and testing for different domain sizes.","authors":["J\\'ulia Vicens Figueres","Juliette Vanderhaeghen","Federica Bragone","Kateryna Morozovska","Khemraj Shukla"],"url":"https://arxiv.org/abs/2504.19013"}
{"created":"2025-04-30","title":"IM-Portrait: Learning 3D-aware Video Diffusion for Photorealistic Talking Heads from Monocular Videos","abstract":"We propose a novel 3D-aware diffusion-based method for generating photorealistic talking head videos directly from a single identity image and explicit control signals (e.g., expressions). Our method generates Multiplane Images (MPIs) that ensure geometric consistency, making them ideal for immersive viewing experiences like binocular videos for VR headsets. Unlike existing methods that often require a separate stage or joint optimization to reconstruct a 3D representation (such as NeRF or 3D Gaussians), our approach directly generates the final output through a single denoising process, eliminating the need for post-processing steps to render novel views efficiently. To effectively learn from monocular videos, we introduce a training mechanism that reconstructs the output MPI randomly in either the target or the reference camera space. This approach enables the model to simultaneously learn sharp image details and underlying 3D information. Extensive experiments demonstrate the effectiveness of our method, which achieves competitive avatar quality and novel-view rendering capabilities, even without explicit 3D reconstruction or high-quality multi-view training data.","authors":["Yuan Li","Ziqian Bai","Feitong Tan","Zhaopeng Cui","Sean Fanello","Yinda Zhang"],"url":"https://arxiv.org/abs/2504.19165"}
{"created":"2025-04-30","title":"Context Selection and Rewriting for Video-based Educational Question Generation","abstract":"Educational question generation (EQG) is a crucial component of intelligent educational systems, significantly aiding self-assessment, active learning, and personalized education. While EQG systems have emerged, existing datasets typically rely on predefined, carefully edited texts, failing to represent real-world classroom content, including lecture speech with a set of complementary slides. To bridge this gap, we collect a dataset of educational questions based on lectures from real-world classrooms. On this realistic dataset, we find that current methods for EQG struggle with accurately generating questions from educational videos, particularly in aligning with specific timestamps and target answers. Common challenges include selecting informative contexts from extensive transcripts and ensuring generated questions meaningfully incorporate the target answer. To address the challenges, we introduce a novel framework utilizing large language models for dynamically selecting and rewriting contexts based on target timestamps and answers. First, our framework selects contexts from both lecture transcripts and video keyframes based on answer relevance and temporal proximity. Then, we integrate the contexts selected from both modalities and rewrite them into answer-containing knowledge statements, to enhance the logical connection between the contexts and the desired answer. This approach significantly improves the quality and relevance of the generated questions. Our dataset and code are released in https://github.com/mengxiayu/COSER.","authors":["Mengxia Yu","Bang Nguyen","Olivia Zino","Meng Jiang"],"url":"https://arxiv.org/abs/2504.19406"}
{"created":"2025-04-30","title":"Linear-Quadratic Mean-Field Reinforcement Learning: Convergence of Policy Gradient Methods","abstract":"We investigate reinforcement learning in the setting of Markov decision processes for a large number of exchangeable agents interacting in a mean field manner. Applications include, for example, the control of a large number of robots communicating through a central unit dispatching the optimal policy computed by maximizing an aggregate reward. An approximate solution is obtained by learning the optimal policy of a generic agent interacting with the statistical distribution of the states and actions of the other agents. We first provide a full analysis this discrete-time mean field control problem. We then rigorously prove the convergence of exact and model-free policy gradient methods in a mean-field linear-quadratic setting and establish bounds on the rates of convergence. We also provide graphical evidence of the convergence based on implementations of our algorithms.","authors":["Ren\\'e Carmona","Mathieu Lauri\\`ere","Zongjun Tan"],"url":"https://arxiv.org/abs/1910.04295"}
{"created":"2025-04-30","title":"Private Private Information","abstract":"Private signals model noisy information about an unknown state. Although these signals are called \"private,\" they may still carry information about each other. Our paper introduces the concept of private private signals, which contain information about the state but not about other signals. To achieve privacy, signal quality may need to be sacrificed. We study the informativeness of private private signals and characterize those that are optimal in the sense that they cannot be made more informative without violating privacy. We discuss implications for privacy in recommendation systems, information design, causal inference, and mechanism design.","authors":["Kevin He","Fedor Sandomirskiy","Omer Tamuz"],"url":"https://arxiv.org/abs/2112.14356"}
{"created":"2025-04-30","title":"Stochastic Variance-Reduced Newton: Accelerating Finite-Sum Minimization with Large Batches","abstract":"Stochastic variance reduction has proven effective at accelerating first-order algorithms for solving convex finite-sum optimization tasks such as empirical risk minimization. Incorporating second-order information has proven helpful in further improving the performance of these first-order methods. Yet, comparatively little is known about the benefits of using variance reduction to accelerate popular stochastic second-order methods such as Subsampled Newton. To address this, we propose Stochastic Variance-Reduced Newton (SVRN), a finite-sum minimization algorithm that provably accelerates existing stochastic Newton methods from $O(\\alpha\\log(1/\\epsilon))$ to $O\\big(\\frac{\\log(1/\\epsilon)}{\\log(n)}\\big)$ passes over the data, i.e., by a factor of $O(\\alpha\\log(n))$, where $n$ is the number of sum components and $\\alpha$ is the approximation factor in the Hessian estimate. Surprisingly, this acceleration gets more significant the larger the data size $n$, which is a unique property of SVRN. Our algorithm retains the key advantages of Newton-type methods, such as easily parallelizable large-batch operations and a simple unit step size. We use SVRN to accelerate Subsampled Newton and Iterative Hessian Sketch algorithms, and show that it compares favorably to popular first-order methods with variance~reduction.","authors":["Micha{\\l} Derezi\\'nski"],"url":"https://arxiv.org/abs/2206.02702"}
{"created":"2025-04-30","title":"The Adaptive $\\tau$-Lasso: Robustness and Oracle Properties","abstract":"This paper introduces a new regularized version of the robust $\\tau$-regression estimator for analyzing high-dimensional datasets subject to gross contamination in the response variables and covariates. The resulting estimator, termed adaptive $\\tau$-Lasso, is robust to outliers and high-leverage points. It also incorporates an adaptive $\\ell_1$-norm penalty term, which enables the selection of relevant variables and reduces the bias associated with large true regression coefficients. More specifically, this adaptive $\\ell_1$-norm penalty term assigns a weight to each regression coefficient. For a fixed number of predictors $p$, we show that the adaptive $\\tau$-Lasso has the oracle property, ensuring both variable-selection consistency and asymptotic normality. Asymptotic normality applies only to the entries of the regression vector corresponding to the true support, assuming knowledge of the true regression vector support. We characterize its robustness by establishing the finite-sample breakdown point and the influence function. We carry out extensive simulations and observe that the class of $\\tau$-Lasso estimators exhibits robustness and reliable performance in both contaminated and uncontaminated data settings. We also validate our theoretical findings on robustness properties through simulations. In the face of outliers and high-leverage points, the adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators achieve the best performance or match the best performances of competing regularized estimators, with minimal or no loss in terms of prediction and variable selection accuracy for almost all scenarios considered in this study. Therefore, the adaptive $\\tau$-Lasso and $\\tau$-Lasso estimators provide attractive tools for a variety of sparse linear regression problems, particularly in high-dimensional settings and when the data is contaminated by outliers and high-leverage points.","authors":["Emadaldin Mozafari-Majd","Visa Koivunen"],"url":"https://arxiv.org/abs/2304.09310"}
{"created":"2025-04-30","title":"When Deep Learning Meets Polyhedral Theory: A Survey","abstract":"In the past decade, deep learning became the prevalent methodology for predictive modeling thanks to the remarkable accuracy of deep neural networks in tasks such as computer vision and natural language processing. Meanwhile, the structure of neural networks converged back to simpler representations based on piecewise constant and piecewise linear functions such as the Rectified Linear Unit (ReLU), which became the most commonly used type of activation function in neural networks. That made certain types of network structure $\\unicode{x2014}$such as the typical fully-connected feedforward neural network$\\unicode{x2014}$ amenable to analysis through polyhedral theory and to the application of methodologies such as Linear Programming (LP) and Mixed-Integer Linear Programming (MILP) for a variety of purposes. In this paper, we survey the main topics emerging from this fast-paced area of work, which bring a fresh perspective to understanding neural networks in more detail as well as to applying linear optimization techniques to train, verify, and reduce the size of such networks.","authors":["Joey Huchette","Gonzalo Mu\\~noz","Thiago Serra","Calvin Tsay"],"url":"https://arxiv.org/abs/2305.00241"}
{"created":"2025-04-30","title":"Phase retrieval with semi-algebraic and ReLU neural network priors","abstract":"The key ingredient to retrieving a signal from its Fourier magnitudes, namely, to solve the phase retrieval problem, is an effective prior on the sought signal. In this paper, we study the phase retrieval problem under the prior that the signal lies in a semi-algebraic set. This is a very general prior as semi-algebraic sets include linear models, sparse models, and ReLU neural network generative models. The latter is the main motivation of this paper, due to the remarkable success of deep generative models in a variety of imaging tasks, including phase retrieval. We prove that almost all signals in R^N can be determined from their Fourier magnitudes, up to a sign, if they lie in a (generic) semi-algebraic set of dimension N/2. The same is true for all signals if the semi-algebraic set is of dimension N/4. We also generalize these results to the problem of signal recovery from the second moment in multi-reference alignment models with multiplicity free representations of compact groups. This general result is then used to derive improved sample complexity bounds for recovering band-limited functions on the sphere from their noisy copies, each acted upon by a random element of SO(3).","authors":["Tamir Bendory","Nadav Dym","Dan Edidin","Arun Suresh"],"url":"https://arxiv.org/abs/2311.08833"}
{"created":"2025-04-30","title":"The Overlap Gap Property limits limit swapping in QAOA","abstract":"The Quantum Approximate Optimization Algorithm (QAOA) is a quantum algorithm designed for Combinatorial Optimization Problem (COP). We show that if a local algorithm is limited in performance at logarithmic depth for a spin glass type COP with an underlying Erd\\\"os--R\\'enyi hypergraph, then a random regular hypergraph exhibits it as well. As such, we re-derived the fact that the average-case value obtained by QAOA for the Max-$q$-XORSAT for even $q\\ge 4$ is bounded away from optimality even when the algorithm runs indefinitely if optimised using the so-called tree parameters due to the presence of the Overlap Gap Property (OGP). While this result was proven before, the proof is rather technical compared to ours. In addition, we show that the earlier result implicitly also implies limitation at logarithmic depth $p \\le \\epsilon \\log n$ providing an improvement over limitation at constant depth. Lastly, the results suggests that even when sub-optimised, the performance of QAOA on spin glass is equal in performance to classical algorithms in solving the mean field spin glass problem providing further evidence that the conjecture of getting the exact solution under limit swapping for the Sherrington--Kirkpatrick model to be true.","authors":["Mark Xin Hong Goh"],"url":"https://arxiv.org/abs/2404.06087"}
{"created":"2025-04-30","title":"Q-Newton: Hybrid Quantum-Classical Scheduling for Accelerating Neural Network Training with Newton's Gradient Descent","abstract":"Optimization techniques in deep learning are predominantly led by first-order gradient methodologies, such as SGD. However, neural network training can greatly benefit from the rapid convergence characteristics of second-order optimization. Newton's GD stands out in this category, by rescaling the gradient using the inverse Hessian. Nevertheless, one of its major bottlenecks is matrix inversion, which is notably time-consuming in $O(N^3)$ time with weak scalability.","authors":["Pingzhi Li","Junyu Liu","Hanrui Wang","Tianlong Chen"],"url":"https://arxiv.org/abs/2405.00252"}
{"created":"2025-04-30","title":"Comparative Study of Quantum Transpilers: Evaluating the Performance of qiskit-braket-provider, qBraid-SDK, and Pytket Extensions","abstract":"In this study, we present a comprehensive evaluation of popular SDK-to-SDK quantum transpilers (that is transpilers that takes a quantum circuit from an initial SDK and output a quantum circuit in another SDK), focusing on critical metrics such as correctness, failure rate, and transpilation time. To ensure unbiased evaluation and accommodate diverse quantum computing scenarios, we developed two dedicated tools: RandomQC, for generating random quantum circuits across various types (pure random, VQE-like, and SDK-specific circuits), and Benchmarq, to streamline the benchmarking process. Using these tools, we benchmarked prominent quantum transpilers as of February 2024. Our results highlight the superior performance of the qiskit-braket-provider, a specialized transpiler from Qiskit to Braket, achieving a remarkably low failure rate of 0.2%. The qBraid-SDK, offering generalized transpilation across multiple SDKs, demonstrated robust but slower performance. The pytket extensions, while fast, faced limitations with complex circuits due to their one-to-one transpilation approach. In particular, the exceptional performance of the qiskit-bracket-provider stems not only from its specialization but also from its architecture, which combines one-to-one transpilation with gate decomposition for unsupported gates, enhancing both speed and capability. This study aims to provide practical guidelines to users of SDK-to-SDK quantum transpilers and guidance to developers for improving the design and development of future tools.","authors":["Mohamed Messaoud Louamri","Nacer Eddine Belaloui","Abdellah Tounsi","Mohamed Taha Rouabah"],"url":"https://arxiv.org/abs/2406.06836"}
{"created":"2025-04-30","title":"NPA Hierarchy for Quantum Isomorphism and Homomorphism Indistinguishability","abstract":"Man\\v{c}inska and Roberson [FOCS'20] showed that two graphs are quantum isomorphic if and only if they are homomorphism indistinguishable over the class of planar graphs. Atserias et al. [JCTB'19] proved that quantum isomorphism is undecidable in general. The NPA hierarchy gives a sequence of semidefinite programming relaxations of quantum isomorphism. Recently, Roberson and Seppelt [ICALP'23] obtained a homomorphism indistinguishability characterization of the feasibility of each level of the Lasserre hierarchy of semidefinite programming relaxations of graph isomorphism. We prove a quantum analogue of this result by showing that each level of the NPA hierarchy of SDP relaxations for quantum isomorphism of graphs is equivalent to homomorphism indistinguishability over an appropriate class of planar graphs. By combining the convergence of the NPA hierarchy with the fact that the union of these graph classes is the set of all planar graphs, we are able to give a new proof of the result of Man\\v{c}inska and Roberson [FOCS'20] that avoids the use of the theory of quantum groups. This homomorphism indistinguishability characterization also allows us to give a randomized polynomial-time algorithm deciding exact feasibility of each fixed level of the NPA hierarchy of SDP relaxations for quantum isomorphism.","authors":["Prem Nigam Kar","David E. Roberson","Tim Seppelt","Peter Zeman"],"url":"https://arxiv.org/abs/2407.10635"}
{"created":"2025-04-30","title":"Predictive maintenance solution for industrial systems -- an unsupervised approach based on log periodic power law","abstract":"A new unsupervised predictive maintenance analysis method based on the renormalization group approach used to discover critical behavior in complex systems has been proposed. The algorithm analyzes univariate time series and detects critical points based on a newly proposed theorem that identifies critical points using a Log Periodic Power Law function fits. Application of a new algorithm for predictive maintenance analysis of industrial data collected from reciprocating compressor systems is presented. Based on the knowledge of the dynamics of the analyzed compressor system, the proposed algorithm predicts valve and piston rod seal failures well in advance.","authors":["Bogdan {\\L}obodzi\\'nski"],"url":"https://arxiv.org/abs/2408.05231"}
{"created":"2025-04-30","title":"On the Robustness of Kernel Goodness-of-Fit Tests","abstract":"Goodness-of-fit testing is often criticized for its lack of practical relevance: since ``all models are wrong'', the null hypothesis that the data conform to our model is ultimately always rejected as sample size grows. Despite this, probabilistic models are still used extensively, raising the more pertinent question of whether the model is \\emph{good enough} for the task at hand. This question can be formalized as a robust goodness-of-fit testing problem by asking whether the data were generated from a distribution that is a mild perturbation of the model. In this paper, we show that existing kernel goodness-of-fit tests are not robust under common notions of robustness including both qualitative and quantitative robustness. We further show that robustification techniques using tilted kernels, while effective in the parameter estimation literature, are not sufficient to ensure both types of robustness in the testing setting. To address this, we propose the first robust kernel goodness-of-fit test, which resolves this open problem by using kernel Stein discrepancy (KSD) balls. This framework encompasses many well-known perturbation models, such as Huber's contamination and density-band models.","authors":["Xing Liu","Fran\\c{c}ois-Xavier Briol"],"url":"https://arxiv.org/abs/2408.05854"}
{"created":"2025-04-30","title":"Characterization of Circular-arc Graphs: II. McConnell Flipping","abstract":"McConnell [FOCS 2001] presented a flipping transformation from circular-arc graphs to interval graphs with certain patterns of representations. Beyond its algorithmic implications, this transformation is instrumental in identifying all minimal graphs that are not circular-arc graphs. We conduct a structural study of this transformation, and for $C_{4}$-free graphs, we achieve a complete characterization of these patterns. This characterization allows us, among other things, to identify all minimal chordal graphs that are not circular-arc graphs in a companion paper.","authors":["Yixin Cao","Tomasz Krawczyk"],"url":"https://arxiv.org/abs/2408.10892"}
{"created":"2025-04-30","title":"A Deep Generative Model for Five-Class Sleep Staging with Arbitrary Sensor Input","abstract":"Gold-standard sleep scoring is based on epoch-based assignment of sleep stages based on a combination of EEG, EOG and EMG signals. However, a polysomnographic recording consists of many other signals that could be used for sleep staging, including cardio-respiratory modalities. Leveraging this signal variety would offer important advantages, for example increasing reliability, resilience to signal loss, and application to long-term non-obtrusive recordings. We developed a deep generative model for automatic sleep staging from a plurality of sensors and any -arbitrary- combination thereof. We trained a score-based diffusion model using a dataset of 1947 expert-labelled overnight recordings with 36 different signals, and achieved zero-shot inference on any sensor set by leveraging a novel Bayesian factorization of the score function across the sensors. On single-channel EEG, the model reaches the performance limit in terms of polysomnography inter-rater agreement (5- class accuracy 85.6%, Cohen's kappa 0.791). Moreover, the method offers full flexibility to use any sensor set, for example finger photoplethysmography, nasal flow and thoracic respiratory movements, (5-class accuracy 79.0%, Cohen's kappa of 0.697), or even derivations very unconventional for sleep staging, such as tibialis and sternocleidomastoid EMG (5-class accuracy 71.0%, kappa 0.575). Additionally, we propose a novel interpretability metric in terms of information gain per sensor and show this is linearly correlated with classification performance. Finally, our model allows for post- hoc addition of entirely new sensor modalities by merely training a score estimator on the novel input instead of having to retrain from scratch on all inputs.","authors":["Hans van Gorp","Merel M. van Gilst","Pedro Fonseca","Fokke B. van Meulen","Johannes P. van Dijk","Sebastiaan Overeem","Ruud J. G. van Sloun"],"url":"https://arxiv.org/abs/2408.15253"}
{"created":"2025-04-30","title":"Anomaly Detection in Time Series of EDFA Pump Currents to Monitor Degeneration Processes using Fuzzy Clustering","abstract":"This article proposes a novel fuzzy clustering based anomaly detection method for pump current time series of EDFA systems. The proposed change detection framework (CDF) strategically combines the advantages of entropy analysis (EA) and principle component analysis (PCA) with fuzzy clustering procedures. In the framework, EA is applied for dynamic selection of features for reduction of the feature space and increase of computational performance. Furthermore, PCA is utilized to extract features from the raw feature space to enable generalization capability of the subsequent fuzzy clustering procedures. Three different fuzzy clustering methods, more precisely the fuzzy clustering algorithm, a probabilistic clustering algorithm and a possibilistic clustering algorithm are evaluated for performance and generalization. Hence, the proposed framework has the innovative feature to detect changes in pump current time series at an early stage for arbitrary points of operation, compared to state-of-the-art predefined alarms in commercially used EDFAs. Moreover, the approach is implemented and tested using experimental data. In addition, the proposed framework enables further approaches of applying decentralized predictive maintenance for optical fiber networks.","authors":["Dominic Schneider","Lutz Rapp","Christoph Ament"],"url":"https://arxiv.org/abs/2408.15268"}
{"created":"2025-04-30","title":"Best Linear Unbiased Estimate from Privatized Contingency Tables","abstract":"In differential privacy (DP) mechanisms, it can be beneficial to release \"redundant\" outputs, in the sense that some quantities can be estimated in multiple ways by combining different combinations of privatized values. Indeed, the DP 2020 Decennial Census products published by the U.S. Census Bureau consist of such redundant noisy counts. When redundancy is present, the DP output can be improved by enforcing self-consistency (i.e., estimators obtained by combining different values result in the same estimate) and we show that the minimum variance processing is a linear projection. However, standard projection algorithms are too computationally expensive in terms of both memory and execution time for applications such as the Decennial Census. We propose the Scalable Efficient Algorithm for Best Linear Unbiased Estimate (SEA BLUE), based on a two step process of aggregation and differencing that 1) enforces self-consistency through a linear and unbiased procedure, 2) is computationally and memory efficient, 3) achieves the minimum variance solution under certain structural assumptions, and 4) is empirically shown to be robust to violations of these structural assumptions. We propose three methods of calculating confidence intervals from our estimates, under various assumptions. Finally, we apply SEA BLUE to two 2010 Census demonstration products, illustrating its scalability and validity.","authors":["Jordan Awan","Adam Edwards","Paul Bartholomew","Andrew Sillers"],"url":"https://arxiv.org/abs/2409.04387"}
{"created":"2025-04-30","title":"Community detection in multi-layer networks by regularized debiased spectral clustering","abstract":"Community detection is a crucial problem in the analysis of multi-layer networks. While regularized spectral clustering methods using the classical regularized Laplacian matrix have shown great potential in handling sparse single-layer networks, to our knowledge, their potential in multi-layer network community detection remains unexplored. To address this gap, in this work, we introduce a new method, called regularized debiased sum of squared adjacency matrices (RDSoS), to detect communities in multi-layer networks. RDSoS is developed based on a novel regularized Laplacian matrix that regularizes the debiased sum of squared adjacency matrices. In contrast, the classical regularized Laplacian matrix typically regularizes the adjacency matrix of a single-layer network. Therefore, at a high level, our regularized Laplacian matrix extends the classical one to multi layer networks. We establish the consistency property of RDSoS under the multi-layer stochastic block model (MLSBM) and further extend RDSoS and its theoretical results to the degree-corrected version of the MLSBM model. Additionally, we introduce a sum of squared adjacency matrices modularity (SoS-modularity) to measure the quality of community partitions in multi-layer networks and estimate the number of communities by maximizing this metric. Our methods offer promising applications for predicting gene functions, improving recommender systems, detecting medical insurance fraud, and facilitating link prediction. Experimental results demonstrate that our methods exhibit insensitivity to the selection of the regularizer, generally outperform state-of-the-art techniques, uncover the assortative property of real networks, and that our SoS-modularity provides a more accurate assessment of community quality compared to the average of the Newman-Girvan modularity across layers.","authors":["Huan Qing"],"url":"https://arxiv.org/abs/2409.07956"}
{"created":"2025-04-30","title":"Higher order definition of causality by optimally conditioned transfer entropy","abstract":"The description of the dynamics of complex systems, in particular the capture of the interaction structure and causal relationships between elements of the system, is one of the central questions of interdisciplinary research. While the characterization of pairwise causal interactions is a relatively ripe field with established theoretical concepts and the current focus is on technical issues of their efficient estimation, it turns out that the standard concepts such as Granger causality or transfer entropy may not faithfully reflect possible synergies or interactions of higher orders, phenomena highly relevant for many real-world complex systems. In this paper, we propose a generalization and refinement of the information-theoretic approach to causal inference, enabling the description of truly multivariate, rather than multiple pairwise, causal interactions, and moving thus from causal networks to causal hypernetworks. In particular, while keeping the ability to control for mediating variables or common causes, in case of purely synergetic interactions such as the exclusive disjunction, it ascribes the causal role to the multivariate causal set but \\emph{not} to individual inputs, distinguishing it thus from the case of e.g. two additive univariate causes. We demonstrate this concept by application to illustrative theoretical examples as well as a biophysically realistic simulation of biological neuronal dynamics recently reported to employ synergetic computations.","authors":["Jakub Ko\\v{r}enek","Pavel Sanda","Jaroslav Hlinka"],"url":"https://arxiv.org/abs/2409.08295"}
{"created":"2025-04-30","title":"Geometry and Duality of Alternating Markov Chains","abstract":"In this note, we realize the half-steps of a general class of Markov chains as alternating projections with respect to the reverse Kullback-Leibler divergence between convex sets of joint probability distributions. Using this characterization, we provide a geometric proof of an information-theoretic duality between the Markov chains defined by the even and odd half-steps of the alternating projection scheme.","authors":["Deven Mithal","Lorenzo Orecchia"],"url":"https://arxiv.org/abs/2410.12721"}
{"created":"2025-04-30","title":"Advancing Physics Data Analysis through Machine Learning and Physics-Informed Neural Networks","abstract":"In an era increasingly focused on green computing and explainable AI, revisiting traditional approaches in theoretical and phenomenological particle physics is paramount. This project evaluates various machine learning (ML) algorithms-including Nearest Neighbors, Decision Trees, Random Forest, AdaBoost, Naive Bayes, Quadratic Discriminant Analysis (QDA), and XGBoost-alongside standard neural networks and a novel Physics-Informed Neural Network (PINN) for physics data analysis. We apply these techniques to a binary classification task that distinguishes the experimental viability of simulated scenarios based on Higgs observables and essential parameters. Through this comprehensive analysis, we aim to showcase the capabilities and computational efficiency of each model in binary classification tasks, thereby contributing to the ongoing discourse on integrating ML and Deep Neural Networks (DNNs) into physics research. In this study, XGBoost emerged as the preferred choice among the evaluated machine learning algorithms for its speed and effectiveness, especially in the initial stages of computation with limited datasets. However, while standard Neural Networks and Physics-Informed Neural Networks (PINNs) demonstrated superior performance in terms of accuracy and adherence to physical laws, they require more computational time. These findings underscore the trade-offs between computational efficiency and model sophistication.","authors":["Vasileios Vatellis"],"url":"https://arxiv.org/abs/2410.14760"}
{"created":"2025-04-30","title":"Foundations of Safe Online Reinforcement Learning in the Linear Quadratic Regulator: Generalized Baselines","abstract":"Many practical applications of online reinforcement learning require the satisfaction of safety constraints while learning about the unknown environment. In this work, we establish theoretical foundations for reinforcement learning with safety constraints by studying the canonical problem of Linear Quadratic Regulator learning with unknown dynamics, but with the additional constraint that the position must stay within a safe region for the entire trajectory with high probability. Our primary contribution is a general framework for studying stronger baselines of nonlinear controllers that are better suited for constrained problems than linear controllers. Due to the difficulty of analyzing non-linear controllers in a constrained problem, we focus on 1-dimensional state- and action- spaces, however we also discuss how we expect the high-level takeaways can generalize to higher dimensions. Using our framework, we show that for \\emph{any} non-linear baseline satisfying natural assumptions, $\\tilde{O}_T(\\sqrt{T})$-regret is possible when the noise distribution has sufficiently large support, and $\\tilde{O}_T(T^{2/3})$-regret is possible for \\emph{any} subgaussian noise distribution. In proving these results, we introduce a new uncertainty estimation bound for nonlinear controls which shows that enforcing safety in the presence of sufficient noise can provide ``free exploration'' that compensates for the added cost of uncertainty in safety-constrained control.","authors":["Benjamin Schiffer","Lucas Janson"],"url":"https://arxiv.org/abs/2410.21081"}
{"created":"2025-04-30","title":"HAVER: Instance-Dependent Error Bounds for Maximum Mean Estimation and Applications to Q-Learning and Monte Carlo Tree Search","abstract":"We study the problem of estimating the \\emph{value} of the largest mean among K distributions via samples from them (rather than estimating \\emph{which} distribution has the largest mean), which arises from various machine learning tasks including Q-learning and Monte Carlo Tree Search (MCTS). While there have been a few proposed algorithms, their performance analyses have been limited to their biases rather than a precise error metric. In this paper, we propose a novel algorithm called HAVER (Head AVERaging) and analyze its mean squared error. Our analysis reveals that HAVER has a compelling performance in two respects. First, HAVER estimates the maximum mean as well as the oracle who knows the identity of the best distribution and reports its sample mean. Second, perhaps surprisingly, HAVER exhibits even better rates than this oracle when there are many distributions near the best one. Both of these improvements are the first of their kind in the literature, and we also prove that the naive algorithm that reports the largest empirical mean does not achieve these bounds. Finally, we confirm our theoretical findings via numerical experiments where we implement HAVER in bandit, Q-learning, and MCTS algorithms. In these experiments, HAVER consistently outperforms the baseline methods, demonstrating its effectiveness across different applications.","authors":["Tuan Ngo Nguyen","Jay Barrett","Kwang-Sung Jun"],"url":"https://arxiv.org/abs/2411.00405"}
{"created":"2025-04-30","title":"Distributionally Robust Optimization","abstract":"Distributionally robust optimization (DRO) studies decision problems under uncertainty where the probability distribution governing the uncertain problem parameters is itself uncertain. A key component of any DRO model is its ambiguity set, that is, a family of probability distributions consistent with any available structural or statistical information. DRO seeks decisions that perform best under the worst distribution in the ambiguity set. This worst case criterion is supported by findings in psychology and neuroscience, which indicate that many decision-makers have a low tolerance for distributional ambiguity. DRO is rooted in statistics, operations research and control theory, and recent research has uncovered its deep connections to regularization techniques and adversarial training in machine learning. This survey presents the key findings of the field in a unified and self-contained manner.","authors":["Daniel Kuhn","Soroosh Shafiee","Wolfram Wiesemann"],"url":"https://arxiv.org/abs/2411.02549"}
{"created":"2025-04-30","title":"MQFL-FHE: Multimodal Quantum Federated Learning Framework with Fully Homomorphic Encryption","abstract":"The integration of fully homomorphic encryption (FHE) in federated learning (FL) has led to significant advances in data privacy. However, during the aggregation phase, it often results in performance degradation of the aggregated model, hindering the development of robust representational generalization. In this work, we propose a novel multimodal quantum federated learning framework that utilizes quantum computing to counteract the performance drop resulting from FHE. For the first time in FL, our framework combines a multimodal quantum mixture of experts (MQMoE) model with FHE, incorporating multimodal datasets for enriched representation and task-specific learning. Our MQMoE framework enhances performance on multimodal datasets and combined genomics and brain MRI scans, especially for underrepresented categories. Our results also demonstrate that the quantum-enhanced approach mitigates the performance degradation associated with FHE and improves classification accuracy across diverse datasets, validating the potential of quantum interventions in enhancing privacy in FL.","authors":["Siddhant Dutta","Nouhaila Innan","Sadok Ben Yahia","Muhammad Shafique","David Esteban Bernal Neira"],"url":"https://arxiv.org/abs/2412.01858"}
{"created":"2025-04-30","title":"Empirical Bayes Estimation for Lasso-Type Regularizers: Analysis of Automatic Relevance Determination","abstract":"This paper focuses on linear regression models with non-conjugate sparsity-inducing regularizers such as lasso and group lasso. Although the empirical Bayes approach enables us to estimate the regularization parameter, little is known on the properties of the estimators. In particular, many aspects regarding the specific conditions under which the mechanism of automatic relevance determination (ARD) occurs remain unexplained. In this paper, we derive the empirical Bayes estimators for the group lasso regularized linear regression models with limited parameters. It is shown that the estimators diverge under a specific condition, giving rise to the ARD mechanism. We also prove that empirical Bayes methods can produce the ARD mechanism in general regularized linear regression models and clarify the conditions under which models such as ridge, lasso, and group lasso can do so.","authors":["Tsukasa Yoshida","Kazuho Watanabe"],"url":"https://arxiv.org/abs/2501.11280"}
{"created":"2025-04-30","title":"Adaptive Progressive Attention Graph Neural Network for EEG Emotion Recognition","abstract":"In recent years, numerous neuroscientific studies demonstrate that specific areas of the brain are connected to human emotional responses, with these regions exhibiting variability across individuals and emotional states. To fully leverage these neural patterns, we propose an Adaptive Progressive Attention Graph Neural Network (APAGNN), which dynamically captures the spatial relationships among brain regions during emotional processing. The APAGNN employs three specialized experts that progressively analyze brain topology. The first expert captures global brain patterns, the second focuses on region-specific features, and the third examines emotion-related channels. This hierarchical approach enables increasingly refined analysis of neural activity. Additionally, a weight generator integrates the outputs of all three experts, balancing their contributions to produce the final predictive label. Extensive experiments conducted on SEED, SEED-IV and MPED datasets indicate that our method enhances EEG emotion recognition performance, achieving superior results compared to baseline methods.","authors":["Tianzhi Feng","Chennan Wu","Yi Niu","Fu Li","Yang Li","Boxun Fu","Zhifu Zhao","Xiaotian Wang"],"url":"https://arxiv.org/abs/2501.14246"}
{"created":"2025-04-30","title":"Two-Point Deterministic Equivalence for Stochastic Gradient Dynamics in Linear Models","abstract":"We derive a novel deterministic equivalence for the two-point function of a random matrix resolvent. Using this result, we give a unified derivation of the performance of a wide variety of high-dimensional linear models trained with stochastic gradient descent. This includes high-dimensional linear regression, kernel regression, and random feature models. Our results include previously known asymptotics as well as novel ones.","authors":["Alexander Atanasov","Blake Bordelon","Jacob A. Zavatone-Veth","Courtney Paquette","Cengiz Pehlevan"],"url":"https://arxiv.org/abs/2502.05074"}
{"created":"2025-04-30","title":"Deciding Local Unitary Equivalence of Graph States in Quasi-Polynomial Time","abstract":"We describe an algorithm with quasi-polynomial runtime $n^{\\log_2(n)+O(1)}$ for deciding local unitary (LU) equivalence of graph states. The algorithm builds on a recent graphical characterisation of LU-equivalence via generalised local complementation. By first transforming the corresponding graphs into a standard form using usual local complementations, LU-equivalence reduces to the existence of a single generalised local complementation that maps one graph to the other. We crucially demonstrate that this reduces to solving a system of quasi-polynomially many linear equations, avoiding an exponential blow-up. As a byproduct, we generalise Bouchet's algorithm for deciding local Clifford (LC) equivalence of graph states by allowing the addition of arbitrary linear constraints. We also improve existing bounds on the size of graph states that are LU- but not LC-equivalent. While the smallest known examples involve 27 qubits, and it is established that no such examples exist for up to 8 qubits, we refine this bound by proving that LU- and LC-equivalence coincide for graph states involving up to 19 qubits.","authors":["Nathan Claudet","Simon Perdrix"],"url":"https://arxiv.org/abs/2502.06566"}
{"created":"2025-04-30","title":"3D ReX: Causal Explanations in 3D Neuroimaging Classification","abstract":"Explainability remains a significant problem for AI models in medical imaging, making it challenging for clinicians to trust AI-driven predictions. We introduce 3D ReX, the first causality-based post-hoc explainability tool for 3D models. 3D ReX uses the theory of actual causality to generate responsibility maps which highlight the regions most crucial to the model's decision. We test 3D ReX on a stroke detection model, providing insight into the spatial distribution of features relevant to stroke.","authors":["Melane Navaratnarajah","Sophie A. Martin","David A. Kelly","Nathan Blake","Hana Chockler"],"url":"https://arxiv.org/abs/2502.12181"}
{"created":"2025-04-30","title":"RGB-Thermal Infrared Fusion for Robust Depth Estimation in Complex Environments","abstract":"Depth estimation in complex real-world scenarios is a challenging task, especially when relying solely on a single modality such as visible light or thermal infrared (THR) imagery. This paper proposes a novel multimodal depth estimation model, RTFusion, which enhances depth estimation accuracy and robustness by integrating the complementary strengths of RGB and THR data. The RGB modality provides rich texture and color information, while the THR modality captures thermal patterns, ensuring stability under adverse lighting conditions such as extreme illumination. The model incorporates a unique fusion mechanism, EGFusion, consisting of the Mutual Complementary Attention (MCA) module for cross-modal feature alignment and the Edge Saliency Enhancement Module (ESEM) to improve edge detail preservation. Comprehensive experiments on the MS2 and ViViD++ datasets demonstrate that the proposed model consistently produces high-quality depth maps across various challenging environments, including nighttime, rainy, and high-glare conditions. The experimental results highlight the potential of the proposed method in applications requiring reliable depth estimation, such as autonomous driving, robotics, and augmented reality.","authors":["Zelin Meng","Takanori Fukao"],"url":"https://arxiv.org/abs/2503.04821"}
{"created":"2025-04-30","title":"Online Conformal Probabilistic Numerics via Adaptive Edge-Cloud Offloading","abstract":"Consider an edge computing setting in which a user submits queries for the solution of a linear system to an edge processor, which is subject to time-varying computing availability. The edge processor applies a probabilistic linear solver (PLS) so as to be able to respond to the user's query within the allotted time and computing budget. Feedback to the user is in the form of a set of plausible solutions. Due to model misspecification, the highest-probability-density (HPD) set obtained via a direct application of PLS does not come with coverage guarantees with respect to the true solution of the linear system. This work introduces a new method to calibrate the HPD sets produced by PLS with the aim of guaranteeing long-term coverage requirements. The proposed method, referred to as online conformal prediction-PLS (OCP-PLS), assumes sporadic feedback from cloud to edge. This enables the online calibration of uncertainty thresholds via online conformal prediction (OCP), an online optimization method previously studied in the context of prediction models. The validity of OCP-PLS is verified via experiments that bring insights into trade-offs between coverage, prediction set size, and cloud usage.","authors":["Qiushuo Hou","Sangwoo Park","Matteo Zecchin","Yunlong Cai","Guanding Yu","Osvaldo Simeone"],"url":"https://arxiv.org/abs/2503.14453"}
{"created":"2025-04-30","title":"3D variational autoencoder for fingerprinting microstructure volume elements","abstract":"Microstructure quantification is an important step towards establishing structure-property relationships in materials. Machine learning-based image processing methods have been shown to outperform conventional image processing techniques and are increasingly applied to microstructure quantification tasks. In this work, we present a 3D variational autoencoder (VAE) for encoding microstructure volume elements (VEs) comprising voxelated crystallographic orientation data. Crystal symmetries in the orientation space are accounted for by mapping to the crystallographic fundamental zone as a preprocessing step, which allows for a continuous loss function to be used and improves the training convergence rate. The VAE is then used to encode a training set of VEs with an equiaxed polycrystalline microstructure with random texture. Accurate reconstructions are achieved with a relative average misorientation error of 9x10-3 on the test dataset, for a continuous latent space with dimension 256. We show that the model generalises well to microstructures with textures, grain sizes and aspect ratios outside the training distribution. Structure-property relationships are explored through using the training set of VEs as initial configurations in various crystal plasticity (CP) simulations. Microstructural fingerprints extracted from the VAE, which parameterise the VEs in a low-dimensional latent space, are stored alongside the volume-averaged stress response, at each strain increment, to uniaxial tensile deformation from CP simulations. This is then used to train a fully connected neural network mapping the input fingerprint to the resulting stress response, which acts as a surrogate model for the CP simulation. The fingerprint-based surrogate model is shown to accurately predict the microstructural dependence in the CP stress response, with a relative mean-squared error of 8.9x10-4 on unseen test data.","authors":["Michael D. White","Michael D. Atkinson","Adam J. Plowman","Pratheek Shanthraj"],"url":"https://arxiv.org/abs/2503.17427"}
{"created":"2025-04-30","title":"Experimental Study on Time Series Analysis of Lower Limb Rehabilitation Exercise Data Driven by Novel Model Architecture and Large Models","abstract":"This study investigates the application of novel model architectures and large-scale foundational models in temporal series analysis of lower limb rehabilitation motion data, aiming to leverage advancements in machine learning and artificial intelligence to empower active rehabilitation guidance strategies for post-stroke patients in limb motor function recovery. Utilizing the SIAT-LLMD dataset of lower limb movement data proposed by the Shenzhen Institute of Advanced Technology, Chinese Academy of Sciences, we systematically elucidate the implementation and analytical outcomes of the innovative xLSTM architecture and the foundational model Lag-Llama in short-term temporal prediction tasks involving joint kinematics and dynamics parameters. The research provides novel insights for AI-enabled medical rehabilitation applications, demonstrating the potential of cutting-edge model architectures and large-scale models in rehabilitation medicine temporal prediction. These findings establish theoretical foundations for future applications of personalized rehabilitation regimens, offering significant implications for the development of customized therapeutic interventions in clinical practice.","authors":["Hengyu Lin"],"url":"https://arxiv.org/abs/2504.03799"}
{"created":"2025-04-30","title":"Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block for Voice Conversion","abstract":"Voice conversion (VC) transforms source speech into a target voice by preserving the content. However, timbre information from the source speaker is inherently embedded in the content representations, causing significant timbre leakage and reducing similarity to the target speaker. To address this, we introduce a residual block to a content extractor. The residual block consists of two weighted branches: 1) universal semantic dictionary based Content Feature Re-expression (CFR) module, supplying timbre-free content representation. 2) skip connection to the original content layer, providing complementary fine-grained information. In the CFR module, each dictionary entry in the universal semantic dictionary represents a phoneme class, computed statistically using speech from multiple speakers, creating a stable, speaker-independent semantic set. We introduce a CFR method to obtain timbre-free content representations by expressing each content frame as a weighted linear combination of dictionary entries using corresponding phoneme posteriors as weights. Extensive experiments across various VC frameworks demonstrate that our approach effectively mitigates timbre leakage and significantly improves similarity to the target speaker.","authors":["Na Li","Chuke Wang","Yu Gu","Zhifeng Li"],"url":"https://arxiv.org/abs/2504.08524"}
{"created":"2025-04-30","title":"A simulation-heuristics dual-process model for intuitive physics","abstract":"The role of mental simulation in human physical reasoning is widely acknowledged, but whether it is employed across scenarios with varying simulation costs and where its boundary lies remains unclear. Using a pouring-marble task, our human study revealed two distinct error patterns when predicting pouring angles, differentiated by simulation time. While mental simulation accurately captured human judgments in simpler scenarios, a linear heuristic model better matched human predictions when simulation time exceeded a certain boundary. Motivated by these observations, we propose a dual-process framework, Simulation-Heuristics Model (SHM), where intuitive physics employs simulation for short-time simulation but switches to heuristics when simulation becomes costly. By integrating computational methods previously viewed as separate into a unified model, SHM quantitatively captures their switching mechanism. The SHM aligns more precisely with human behavior and demonstrates consistent predictive performance across diverse scenarios, advancing our understanding of the adaptive nature of intuitive physical reasoning.","authors":["Shiqian Li","Yuxi Ma","Jiajun Yan","Bo Dai","Yujia Peng","Chi Zhang","Yixin Zhu"],"url":"https://arxiv.org/abs/2504.09546"}
{"created":"2025-04-30","title":"Molecular Learning Dynamics","abstract":"We apply the physics-learning duality to molecular systems by complementing the physical description of interacting particles with a dual learning description, where each particle is modeled as an agent minimizing a loss function. In the traditional physics framework, the equations of motion are derived from the Lagrangian function, while in the learning framework, the same equations emerge from learning dynamics driven by the agent loss function. The loss function depends on scalar quantities that describe invariant properties of all other agents or particles. To demonstrate this approach, we first infer the loss functions of oxygen and hydrogen directly from a dataset generated by the CP2K physics-based simulation of water molecules. We then employ the loss functions to develop a learning-based simulation of water molecules, which achieves comparable accuracy while being significantly more computationally efficient than standard physics-based simulations.","authors":["Yaroslav Gusev","Vitaly Vanchurin"],"url":"https://arxiv.org/abs/2504.10560"}
{"created":"2025-04-30","title":"Deep learning with missing data","abstract":"In the context of multivariate nonparametric regression with missing covariates, we propose Pattern Embedded Neural Networks (PENNs), which can be applied in conjunction with any existing imputation technique. In addition to a neural network trained on the imputed data, PENNs pass the vectors of observation indicators through a second neural network to provide a compact representation. The outputs are then combined in a third neural network to produce final predictions. Our main theoretical result exploits an assumption that the observation patterns can be partitioned into cells on which the Bayes regression function behaves similarly, and belongs to a compositional H\\\"older class. It provides a finite-sample excess risk bound that holds for an arbitrary missingness mechanism, and in combination with a complementary minimax lower bound, demonstrates that our PENN estimator attains in typical cases the minimax rate of convergence as if the cells of the partition were known in advance, up to a poly-logarithmic factor in the sample size. Numerical experiments on simulated, semi-synthetic and real data confirm that the PENN estimator consistently improves, often dramatically, on standard neural networks without pattern embedding. Code to reproduce our experiments, as well as a tutorial on how to apply our method, is publicly available.","authors":["Tianyi Ma","Tengyao Wang","Richard J. Samworth"],"url":"https://arxiv.org/abs/2504.15388"}
