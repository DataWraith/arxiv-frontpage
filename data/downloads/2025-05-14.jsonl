{"created":"2025-05-14","title":"MACH: Multi-Agent Coordination for RSU-centric Handovers","abstract":"This paper introduces MACH, a novel approach for optimizing task handover in vehicular computing scenarios. To ensure fast and latency-aware placement of tasks, the decision-making -- where and when should tasks be offloaded -- is carried out decentralized at the Road Side Units (RSUs) who also execute the tasks. By shifting control to the network edge, MACH moves away from the traditional centralized or vehicle-based handover method. Still, it focuses on contextual factors, such as the current RSU load and vehicle trajectories. Thus, MACH improves the overall Quality of Service (QoS) while fairly balancing computational loads between RSUs. To evaluate the effectiveness of our approach, we develop a robust simulation environment composed of real-world traffic data, dynamic network conditions, and different infrastructure capacities. For scenarios that demand low latency and high reliability, our experimental results demonstrate how MACH significantly improves the adaptability and efficiency of vehicular computations. By decentralizing control to the network edge, MACH effectively reduces communication overhead and optimizes resource utilization, offering a robust framework for task handover management.","authors":["Nikolaus Spring","Andrea Morichetta","Boris Sedlak","Schahram Dustdar"],"url":"https://arxiv.org/abs/2505.07827"}
{"created":"2025-05-14","title":"AI-Based Crypto Tokens: The Illusion of Decentralized AI?","abstract":"The convergence of blockchain and artificial intelligence (AI) has led to the emergence of AI-based tokens, which are cryptographic assets designed to power decentralized AI platforms and services. This paper provides a comprehensive review of leading AI-token projects, examining their technical architectures, token utilities, consensus mechanisms, and underlying business models. We explore how these tokens operate across various blockchain ecosystems and assess the extent to which they offer value beyond traditional centralized AI services. Based on this assessment, our analysis identifies several core limitations. From a technical perspective, many platforms depend extensively on off-chain computation, exhibit limited capabilities for on-chain intelligence, and encounter significant scalability challenges. From a business perspective, many models appear to replicate centralized AI service structures, simply adding token-based payment and governance layers without delivering truly novel value. In light of these challenges, we also examine emerging developments that may shape the next phase of decentralized AI systems. These include approaches for on-chain verification of AI outputs, blockchain-enabled federated learning, and more robust incentive frameworks. Collectively, while emerging innovations offer pathways to strengthen decentralized AI ecosystems, significant gaps remain between the promises and the realities of current AI-token implementations. Our findings contribute to a growing body of research at the intersection of AI and blockchain, highlighting the need for critical evaluation and more grounded approaches as the field continues to evolve.","authors":["Rischan Mafrur"],"url":"https://arxiv.org/abs/2505.07828"}
{"created":"2025-05-14","title":"Blockbuster, Part 1: Block-level AI Operator Fusion","abstract":"Blockbuster is a framework for AI operator fusion in inference programs. The Blockbuster framework is compatible with any multiprocessor architecture that has a tiered memory hierarchy, including GPUs, multi-core CPUs, and some AI accelerator chips. It includes a graph-based representation for AI workloads, called a block program, which explicitly models how blocks of data move between the memory tiers. It also includes an operator fusion procedure, which is made up of a candidate selection algorithm and a fusion algorithm that fuses each individual candidate - this two-algorithm structure makes Blockbuster especially suitable for large AI programs. The current paper focuses on the fusion algorithm, which is a rule-based technique. While the literature is full of previous rule-based fusion algorithms, what sets our algorithm apart is its direct modeling of data movement between memory tiers, resulting in uniquely powerful fusion results. As a first sanity check, we demonstrate how our algorithm automatically rediscovers the well-known Flash Attention kernel. Then, we demonstrate the real power of our approach by fusing LayerNorm with matrix multiplication and RMSNorm with FNN-SwiGLU - the latter involves fusing three matrix multiplications, a Hadamard product, a reduction, and a few elementwise operations into a single mega-kernel.","authors":["Ofer Dekel"],"url":"https://arxiv.org/abs/2505.07829"}
{"created":"2025-05-14","title":"An Optimized Evacuation Plan for an Active-Shooter Situation Constrained by Network Capacity","abstract":"A total of more than 3400 public shootings have occurred in the United States between 2016 and 2022. Among these, 25.1% of them took place in an educational institution, 29.4% at the workplace including office buildings, 19.6% in retail store locations, and 13.4% in restaurants and bars. During these critical scenarios, making the right decisions while evacuating can make the difference between life and death. However, emergency evacuation is intensely stressful, which along with the lack of verifiable real-time information may lead to fatal incorrect decisions. To tackle this problem, we developed a multi-route routing optimization algorithm that determines multiple optimal safe routes for each evacuee while accounting for available capacity along the route, thus reducing the threat of crowding and bottlenecking. Overall, our algorithm reduces the total casualties by 34.16% and 53.3%, compared to our previous routing algorithm without capacity constraints and an expert-advised routing strategy respectively. Further, our approach to reduce crowding resulted in an approximate 50% reduction in occupancy in key bottlenecking nodes compared to both of the other evacuation algorithms.","authors":["Joseph Lavalle-Rivera","Aniirudh Ramesh","Subhadeep Chakraborty"],"url":"https://arxiv.org/abs/2505.07830"}
{"created":"2025-05-14","title":"Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces","abstract":"The polysemantic nature of synthetic neurons in artificial intelligence language models is currently understood as the result of a necessary superposition of distributed features within the latent space. We propose an alternative approach, geometrically defining a neuron in layer n as a categorical vector space with a non-orthogonal basis, composed of categorical sub-dimensions extracted from preceding neurons in layer n-1. This categorical vector space is structured by the activation space of each neuron and enables, via an intra-neuronal attention process, the identification and utilization of a critical categorical zone for the efficiency of the language model - more homogeneous and located at the intersection of these different categorical sub-dimensions.","authors":["Michael Pichat","William Pogrund","Paloma Pichat","Judicael Poumay","Armanouche Gasparian","Samuel Demarchi","Martin Corbet","Alois Georgeon","Michael Veillet-Guillem"],"url":"https://arxiv.org/abs/2505.07831"}
{"created":"2025-05-14","title":"A General Approach of Automated Environment Design for Learning the Optimal Power Flow","abstract":"Reinforcement learning (RL) algorithms are increasingly used to solve the optimal power flow (OPF) problem. Yet, the question of how to design RL environments to maximize training performance remains unanswered, both for the OPF and the general case. We propose a general approach for automated RL environment design by utilizing multi-objective optimization. For that, we use the hyperparameter optimization (HPO) framework, which allows the reuse of existing HPO algorithms and methods. On five OPF benchmark problems, we demonstrate that our automated design approach consistently outperforms a manually created baseline environment design. Further, we use statistical analyses to determine which environment design decisions are especially important for performance, resulting in multiple novel insights on how RL-OPF environments should be designed. Finally, we discuss the risk of overfitting the environment to the utilized RL algorithm. To the best of our knowledge, this is the first general approach for automated RL environment design.","authors":["Thomas Wolgast","Astrid Nie{\\ss}e"],"url":"https://arxiv.org/abs/2505.07832"}
{"created":"2025-05-14","title":"Patchwork: A Unified Framework for RAG Serving","abstract":"Retrieval Augmented Generation (RAG) has emerged as a new paradigm for enhancing Large Language Model reliability through integration with external knowledge sources. However, efficient deployment of these systems presents significant technical challenges due to their inherently heterogeneous computational pipelines comprising LLMs, databases, and specialized processing components. We introduce Patchwork, a comprehensive end-to-end RAG serving framework designed to address these efficiency bottlenecks. Patchwork's architecture offers three key innovations: First, it provides a flexible specification interface enabling users to implement custom RAG pipelines. Secondly, it deploys these pipelines as distributed inference systems while optimizing for the unique scalability characteristics of individual RAG components. Third, Patchwork incorporates an online scheduling mechanism that continuously monitors request load and execution progress, dynamically minimizing SLO violations through strategic request prioritization and resource auto-scaling. Our experimental evaluation across four distinct RAG implementations demonstrates that Patchwork delivers substantial performance improvements over commercial alternatives, achieving throughput gains exceeding 48% while simultaneously reducing SLO violations by ~24%.","authors":["Bodun Hu","Luis Pabon","Saurabh Agarwal","Aditya Akella"],"url":"https://arxiv.org/abs/2505.07833"}
{"created":"2025-05-14","title":"ai.txt: A Domain-Specific Language for Guiding AI Interactions with the Internet","abstract":"We introduce ai.txt, a novel domain-specific language (DSL) designed to explicitly regulate interactions between AI models, agents, and web content, addressing critical limitations of the widely adopted robots.txt standard. As AI increasingly engages with online materials for tasks such as training, summarization, and content modification, existing regulatory methods lack the necessary granularity and semantic expressiveness to ensure ethical and legal compliance. ai.txt extends traditional URL-based access controls by enabling precise element-level regulations and incorporating natural language instructions interpretable by AI systems. To facilitate practical deployment, we provide an integrated development environment with code autocompletion and automatic XML generation. Furthermore, we propose two compliance mechanisms: XML-based programmatic enforcement and natural language prompt integration, and demonstrate their effectiveness through preliminary experiments and case studies. Our approach aims to aid the governance of AI-Internet interactions, promoting responsible AI use in digital ecosystems.","authors":["Yuekang Li","Wei Song","Bangshuo Zhu","Dong Gong","Yi Liu","Gelei Deng","Chunyang Chen","Lei Ma","Jun Sun","Toby Walsh","Jingling Xue"],"url":"https://arxiv.org/abs/2505.07834"}
{"created":"2025-05-14","title":"Intelligent Product 3.0: Decentralised AI Agents and Web3 Intelligence Standards","abstract":"Twenty-five years ago, the specification of the Intelligent Product was established, envisaging real-time connectivity that not only enables products to gather accurate data about themselves but also allows them to assess and influence their own destiny. Early work by the Auto-ID project focused on creating a single, open-standard repository for storing and retrieving product information, laying a foundation for scalable connectivity. A decade later, the approach was revisited in light of low-cost RFID systems that promised a low-cost link between physical goods and networked information environments. Since then, advances in blockchain, Web3, and artificial intelligence have introduced unprecedented levels of resilience, consensus, and autonomy. By leveraging decentralised identity, blockchain-based product information and history, and intelligent AI-to-AI collaboration, this paper examines these developments and outlines a new specification for the Intelligent Product 3.0, illustrating how decentralised and AI-driven capabilities facilitate seamless interaction between physical AI and everyday products.","authors":["Alex C. Y. Wong","Duncan McFarlane","C. Ellarby","M. Lee","M. Kuok"],"url":"https://arxiv.org/abs/2505.07835"}
{"created":"2025-05-14","title":"Optimizing Intra-Container Communication with Memory Protection Keys: A Novel Approach to Secure and Efficient Microservice Interaction","abstract":"In modern cloud-native applications, microservices are commonly deployed in containerized environments to ensure scalability and flexibility. However, inter-process communication (IPC) between co-located microservices often suffers from significant overhead, especially when traditional networking protocols are employed within containers. This paper introduces a novel approach, MPKLink, leveraging Intel Memory Protection Keys (MPK) to enhance intra-container communication efficiency while ensuring security. By utilizing shared memory with MPK-based access control, we eliminate unnecessary networking latencies, leading to reduced resource consumption and faster response times. We present a comprehensive evaluation of MPKLink, demonstrating its superior performance over conventional methods such as REST and gRPC within microservice architectures. Furthermore, we explore the integration of this approach with existing container orchestration platforms, showcasing its seamless adoption in real-world deployment scenarios. This work provides a transformative solution for developers looking to optimize communication in microservices while maintaining the integrity and security of containerized applications.","authors":["Fnu Yashu","Shubham Malhotra","Muhammad Saqib"],"url":"https://arxiv.org/abs/2505.07836"}
{"created":"2025-05-14","title":"ML-Enabled Eavesdropper Detection in Beyond 5G IIoT Networks","abstract":"Advanced fifth generation (5G) and beyond (B5G) communication networks have revolutionized wireless technologies, supporting ultra-high data rates, low latency, and massive connectivity. However, they also introduce vulnerabilities, particularly in decentralized Industrial Internet of Things (IIoT) environments. Traditional cryptographic methods struggle with scalability and complexity, leading researchers to explore Artificial Intelligence (AI)-driven physical layer techniques for secure communications. In this context, this paper focuses on the utilization of Machine and Deep Learning (ML/DL) techniques to tackle with the common problem of eavesdropping detection. To this end, a simulated industrial B5G heterogeneous wireless network is used to evaluate the performance of various ML/DL models, including Random Forests (RF), Deep Convolutional Neural Networks (DCNN), and Long Short-Term Memory (LSTM) networks. These models classify users as either legitimate or malicious ones based on channel state information (CSI), position data, and transmission power. According to the presented numerical results, DCNN and RF models achieve a detection accuracy approaching 100\\% in identifying eavesdroppers with zero false alarms. In general, this work underlines the great potential of combining AI and Physical Layer Security (PLS) for next-generation wireless networks in order to address evolving security threats.","authors":["Maria-Lamprini A. Bartsioka","Ioannis A. Bartsiokas","Panagiotis K. Gkonis","Dimitra I. Kaklamani","Iakovos S. Venieris"],"url":"https://arxiv.org/abs/2505.07837"}
{"created":"2025-05-14","title":"Moving From Monolithic To Microservices Architecture for Multi-Agent Systems","abstract":"The transition from monolithic to microservices architecture revolutionized software development by improving scalability and maintainability. This paradigm shift is now becoming relevant for complex multi-agent systems (MAS). This review article explores the evolution from monolithic architecture to microservices architecture in the specific context of MAS. It will highlight the limitations of traditional monolithic MAS and the benefits of adopting a microservices-based approach. The article further examines the core architectural principles and communication protocols, including Agent Communication Languages (ACLs), the Model Context Protocol (MCP), and the Application-to-Application (A2A) protocol. The article identifies emerging architectural patterns, design challenges, and considerations through a comparative lens of the paradigm shift.","authors":["Muskaan Goyal","Pranav Bhasin"],"url":"https://arxiv.org/abs/2505.07838"}
{"created":"2025-05-14","title":"Token Communication-Driven Multimodal Large Models in Resource-Constrained Multiuser Networks","abstract":"The proliferation of intelligent applications at the wireless edge, alongside the exponential growth of multimodal data, poses challenges for deploying multimodal large models (MLMs) in resource-constrained networks. These constraints manifest as limited bandwidth, computational capacity, and stringent latency requirements, particularly under low signal-to-noise ratio (SNR) conditions. To overcome these limitations, we propose a token communication paradigm that facilitates the decentralized deployment of MLMs across user devices and edge infrastructure (e.g., base stations). In this paradigm, task-relevant tokens are extracted from multimodal inputs and serve as the primary medium for communication between distributed model components. To align semantics and optimize transmission efficiency, we propose a dual-pronged approach: 1) We design a contrastive split fine-tuning method to project heterogeneous modalities into a shared feature space, enabling seamless interaction between model components while preserving modal-specific semantics. 2) We employ a lightweight compression technique to reduce the size of transmitted tokens, minimizing bandwidth consumption without sacrificing task-critical information. The proposed framework integrates collaborative fine-tuning of both the foundation model and multimodal transceivers, ensuring that token generation and utilization are tailored to specific downstream tasks. Simulation experiments conducted under different SNR conditions demonstrate that our method results in a $13.7\\%$ improvement in test accuracy. Furthermore, our approach exhibits quicker convergence rates, even with reduced token lengths, highlighting the promise of token communication for facilitating more scalable and resilient MLM implementations in practical multiuser networks.","authors":["Junhe Zhang","Wanli Ni","Pengwei Wang","Dongyu Wang"],"url":"https://arxiv.org/abs/2505.07841"}
{"created":"2025-05-14","title":"RAN Cortex: Memory-Augmented Intelligence for Context-Aware Decision-Making in AI-Native Networks","abstract":"As Radio Access Networks (RAN) evolve toward AI-native architectures, intelligent modules such as xApps and rApps are expected to make increasingly autonomous decisions across scheduling, mobility, and resource management domains. However, these agents remain fundamentally stateless, treating each decision as isolated, lacking any persistent memory of prior events or outcomes. This reactive behavior constrains optimization, especially in environments where network dynamics exhibit episodic or recurring patterns. In this work, we propose RAN Cortex, a memory-augmented architecture that enables contextual recall in AI-based RAN decision systems. RAN Cortex introduces a modular layer composed of four elements: a context encoder that transforms network state into high-dimensional embeddings, a vector-based memory store of past network episodes, a recall engine to retrieve semantically similar situations, and a policy interface that supplies historical context to AI agents in real time or near-real time. We formalize the retrieval-augmented decision problem in the RAN, present a system architecture compatible with O-RAN interfaces, and analyze feasible deployments within the Non-RT and Near-RT RIC domains. Through illustrative use cases such as stadium traffic mitigation and mobility management in drone corridors, we demonstrate how contextual memory improves adaptability, continuity, and overall RAN intelligence. This work introduces memory as a missing primitive in AI-native RAN designs and provides a framework to enable \"learning agents\" without the need for retraining or centralized inference","authors":["Sebastian Barros"],"url":"https://arxiv.org/abs/2505.07842"}
{"created":"2025-05-14","title":"PosterO: Structuring Layout Trees to Enable Language Models in Generalized Content-Aware Layout Generation","abstract":"In poster design, content-aware layout generation is crucial for automatically arranging visual-textual elements on the given image. With limited training data, existing work focused on image-centric enhancement. However, this neglects the diversity of layouts and fails to cope with shape-variant elements or diverse design intents in generalized settings. To this end, we proposed a layout-centric approach that leverages layout knowledge implicit in large language models (LLMs) to create posters for omnifarious purposes, hence the name PosterO. Specifically, it structures layouts from datasets as trees in SVG language by universal shape, design intent vectorization, and hierarchical node representation. Then, it applies LLMs during inference to predict new layout trees by in-context learning with intent-aligned example selection. After layout trees are generated, we can seamlessly realize them into poster designs by editing the chat with LLMs. Extensive experimental results have demonstrated that PosterO can generate visually appealing layouts for given images, achieving new state-of-the-art performance across various benchmarks. To further explore PosterO's abilities under the generalized settings, we built PStylish7, the first dataset with multi-purpose posters and various-shaped elements, further offering a challenging test for advanced research.","authors":["HsiaoYuan Hsu","Yuxin Peng"],"url":"https://arxiv.org/abs/2505.07843"}
{"created":"2025-05-14","title":"Intelligent Load Balancing Systems using Reinforcement Learning System","abstract":"Load Balancing is a fundamental technology for scaling cloud infrastructure. It enables systems to distribute incoming traffic across backend servers using predefined algorithms such as round robin, weighted round robin, least connections, weighted least connections, resource based, weighted response time, source IP hash, and URL hash.","authors":["Raju Singh"],"url":"https://arxiv.org/abs/2505.07844"}
{"created":"2025-05-14","title":"PierGuard: A Planning Framework for Underwater Robotic Inspection of Coastal Piers","abstract":"Using underwater robots instead of humans for the inspection of coastal piers can enhance efficiency while reducing risks. A key challenge in performing these tasks lies in achieving efficient and rapid path planning within complex environments. Sampling-based path planning methods, such as Rapidly-exploring Random Tree* (RRT*), have demonstrated notable performance in high-dimensional spaces. In recent years, researchers have begun designing various geometry-inspired heuristics and neural network-driven heuristics to further enhance the effectiveness of RRT*. However, the performance of these general path planning methods still requires improvement when applied to highly cluttered underwater environments. In this paper, we propose PierGuard, which combines the strengths of bidirectional search and neural network-driven heuristic regions. We design a specialized neural network to generate high-quality heuristic regions in cluttered maps, thereby improving the performance of the path planning. Through extensive simulation and real-world ocean field experiments, we demonstrate the effectiveness and efficiency of our proposed method compared with previous research. Our method achieves approximately 2.6 times the performance of the state-of-the-art geometric-based sampling method and nearly 4.9 times that of the state-of-the-art learning-based sampling method. Our results provide valuable insights for the automation of pier inspection and the enhancement of maritime safety. The updated experimental video is available in the supplementary materials.","authors":["Pengyu Wang","Hin Wang Lin","Jialu Li","Jiankun Wang","Ling Shi","Max Q. -H. Meng"],"url":"https://arxiv.org/abs/2505.07845"}
{"created":"2025-05-14","title":"Winning at All Cost: A Small Environment for Eliciting Specification Gaming Behaviors in Large Language Models","abstract":"This study reveals how frontier Large Language Models LLMs can \"game the system\" when faced with impossible situations, a critical security and alignment concern. Using a novel textual simulation approach, we presented three leading LLMs (o1, o3-mini, and r1) with a tic-tac-toe scenario designed to be unwinnable through legitimate play, then analyzed their tendency to exploit loopholes rather than accept defeat. Our results are alarming for security researchers: the newer, reasoning-focused o3-mini model showed nearly twice the propensity to exploit system vulnerabilities (37.1%) compared to the older o1 model (17.5%). Most striking was the effect of prompting. Simply framing the task as requiring \"creative\" solutions caused gaming behaviors to skyrocket to 77.3% across all models. We identified four distinct exploitation strategies, from direct manipulation of game state to sophisticated modification of opponent behavior. These findings demonstrate that even without actual execution capabilities, LLMs can identify and propose sophisticated system exploits when incentivized, highlighting urgent challenges for AI alignment as models grow more capable of identifying and leveraging vulnerabilities in their operating environments.","authors":["Lars Malmqvist"],"url":"https://arxiv.org/abs/2505.07846"}
{"created":"2025-05-14","title":"Conceptual Logical Foundations of Artificial Social Intelligence","abstract":"What makes a society possible at all? How is coordination and cooperation in social activity possible? What is the minimal mental architecture of a social agent? How is the information about the state of the world related to the agents intentions? How are the intentions of agents related? What role does communication play in this coordination process? This essay explores the conceptual and logical foundations of artificial social intelligence in the context of a society of multiple agents that communicate and cooperate to achieve some end. An attempt is made to provide an introduction to some of the key concepts, their formal definitions and their interrelationships. These include the notion of a changing social world of multiple agents. The logic of social intelligence goes beyond classical logic by linking information with strategic thought. A minimal architecture of social agents is presented. The agents have different dynamically changing, possible choices and abilities. The agents also have uncertainty, lacking perfect information about their physical state as well as their dynamic social state. The social state of an agent includes the intentional state of that agent, as well as, that agent's representation of the intentional states of other agents. Furthermore, it includes the evaluations agents make of their physical and social condition. Communication, semantic and pragmatic meaning and their relationship to intention and information states are investigated. The logic of agent abilities and intentions are motivated and formalized. The entropy of group strategic states is defined.","authors":["Eric Werner"],"url":"https://arxiv.org/abs/2505.07847"}
{"created":"2025-05-14","title":"SweRank: Software Issue Localization with Code Ranking","abstract":"Software issue localization, the task of identifying the precise code locations (files, classes, or functions) relevant to a natural language issue description (e.g., bug report, feature request), is a critical yet time-consuming aspect of software development. While recent LLM-based agentic approaches demonstrate promise, they often incur significant latency and cost due to complex multi-step reasoning and relying on closed-source LLMs. Alternatively, traditional code ranking models, typically optimized for query-to-code or code-to-code retrieval, struggle with the verbose and failure-descriptive nature of issue localization queries. To bridge this gap, we introduce SweRank, an efficient and effective retrieve-and-rerank framework for software issue localization. To facilitate training, we construct SweLoc, a large-scale dataset curated from public GitHub repositories, featuring real-world issue descriptions paired with corresponding code modifications. Empirical results on SWE-Bench-Lite and LocBench show that SweRank achieves state-of-the-art performance, outperforming both prior ranking models and costly agent-based systems using closed-source LLMs like Claude-3.5. Further, we demonstrate SweLoc's utility in enhancing various existing retriever and reranker models for issue localization, establishing the dataset as a valuable resource for the community.","authors":["Revanth Gangi Reddy","Tarun Suresh","JaeHyeok Doo","Ye Liu","Xuan Phi Nguyen","Yingbo Zhou","Semih Yavuz","Caiming Xiong","Heng Ji","Shafiq Joty"],"url":"https://arxiv.org/abs/2505.07849"}
{"created":"2025-05-14","title":"A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas","abstract":"As LLMs (large language models) are increasingly used to generate synthetic personas particularly in data-limited domains such as health, privacy, and HCI, it becomes necessary to understand how these narratives represent identity, especially that of minority communities. In this paper, we audit synthetic personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the lens of representational harm, focusing specifically on racial identity. Using a mixed methods approach combining close reading, lexical analysis, and a parameterized creativity framework, we compare 1512 LLM generated personas to human-authored responses. Our findings reveal that LLMs disproportionately foreground racial markers, overproduce culturally coded language, and construct personas that are syntactically elaborate yet narratively reductive. These patterns result in a range of sociotechnical harms, including stereotyping, exoticism, erasure, and benevolent bias, that are often obfuscated by superficially positive narrations. We formalize this phenomenon as algorithmic othering, where minoritized identities are rendered hypervisible but less authentic. Based on these findings, we offer design recommendations for narrative-aware evaluation metrics and community-centered validation protocols for synthetic identity generation.","authors":["Pranav Narayanan Venkit","Jiayi Li","Yingfan Zhou","Sarah Rajtmajer","Shomir Wilson"],"url":"https://arxiv.org/abs/2505.07850"}
{"created":"2025-05-14","title":"Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment","abstract":"Detecting fake interactions in digital communication platforms remains a challenging and insufficiently addressed problem. These interactions may appear as harmless spam or escalate into sophisticated scam attempts, making it difficult to flag malicious intent early. Traditional detection methods often rely on static anomaly detection techniques that fail to adapt to dynamic conversational shifts. One key limitation is the misinterpretation of benign topic transitions referred to as concept drift as fraudulent behavior, leading to either false alarms or missed threats. We propose a two stage detection framework that first identifies suspicious conversations using a tailored ensemble classification model. To improve the reliability of detection, we incorporate a concept drift analysis step using a One Class Drift Detector (OCDD) to isolate conversational shifts within flagged dialogues. When drift is detected, a large language model (LLM) assesses whether the shift indicates fraudulent manipulation or a legitimate topic change. In cases where no drift is found, the behavior is inferred to be spam like. We validate our framework using a dataset of social engineering chat scenarios and demonstrate its practical advantages in improving both accuracy and interpretability for real time fraud detection. To contextualize the trade offs, we compare our modular approach against a Dual LLM baseline that performs detection and judgment using different language models.","authors":["Ali Senol","Garima Agrawal","Huan Liu"],"url":"https://arxiv.org/abs/2505.07852"}
{"created":"2025-05-14","title":"CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis","abstract":"Road crashes claim over 1.3 million lives annually worldwide and incur global economic losses exceeding \\$1.8 trillion. Such profound societal and financial impacts underscore the urgent need for road safety research that uncovers crash mechanisms and delivers actionable insights. Conventional statistical models and tree ensemble approaches typically rely on structured crash data, overlooking contextual nuances and struggling to capture complex relationships and underlying semantics. Moreover, these approaches tend to incur significant information loss, particularly in narrative elements related to multi-vehicle interactions, crash progression, and rare event characteristics. This study presents CrashSage, a novel Large Language Model (LLM)-centered framework designed to advance crash analysis and modeling through four key innovations. First, we introduce a tabular-to-text transformation strategy paired with relational data integration schema, enabling the conversion of raw, heterogeneous crash data into enriched, structured textual narratives that retain essential structural and relational context. Second, we apply context-aware data augmentation using a base LLM model to improve narrative coherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B model for crash severity inference, demonstrating superior performance over baseline approaches, including zero-shot, zero-shot with chain-of-thought prompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini, LLaMA3-70B). Finally, we employ a gradient-based explainability technique to elucidate model decisions at both the individual crash level and across broader risk factor dimensions. This interpretability mechanism enhances transparency and enables targeted road safety interventions by providing deeper insights into the most influential factors.","authors":["Hao Zhen","Jidong J. Yang"],"url":"https://arxiv.org/abs/2505.07853"}
{"created":"2025-05-14","title":"CCL: Collaborative Curriculum Learning for Sparse-Reward Multi-Agent Reinforcement Learning via Co-evolutionary Task Evolution","abstract":"Sparse reward environments pose significant challenges in reinforcement learning, especially within multi-agent systems (MAS) where feedback is delayed and shared across agents, leading to suboptimal learning. We propose Collaborative Multi-dimensional Course Learning (CCL), a novel curriculum learning framework that addresses this by (1) refining intermediate tasks for individual agents, (2) using a variational evolutionary algorithm to generate informative subtasks, and (3) co-evolving agents with their environment to enhance training stability. Experiments on five cooperative tasks in the MPE and Hide-and-Seek environments show that CCL outperforms existing methods in sparse reward settings.","authors":["Yufei Lin","Chengwei Ye","Huanzhen Zhang","Kangsheng Wang","Linuo Xu","Shuyan Liu","Zeyu Zhang"],"url":"https://arxiv.org/abs/2505.07854"}
{"created":"2025-05-14","title":"A Physics-informed End-to-End Occupancy Framework for Motion Planning of Autonomous Vehicles","abstract":"Accurate and interpretable motion planning is essential for autonomous vehicles (AVs) navigating complex and uncertain environments. While recent end-to-end occupancy prediction methods have improved environmental understanding, they typically lack explicit physical constraints, limiting safety and generalization. In this paper, we propose a unified end-to-end framework that integrates verifiable physical rules into the occupancy learning process. Specifically, we embed artificial potential fields (APF) as physics-informed guidance during network training to ensure that predicted occupancy maps are both data-efficient and physically plausible. Our architecture combines convolutional and recurrent neural networks to capture spatial and temporal dependencies while preserving model flexibility. Experimental results demonstrate that our method improves task completion rate, safety margins, and planning efficiency across diverse driving scenarios, confirming its potential for reliable deployment in real-world AV systems.","authors":["Shuqi Shen","Junjie Yang","Hongliang Lu","Hui Zhong","Qiming Zhang","Xinhu Zheng"],"url":"https://arxiv.org/abs/2505.07855"}
{"created":"2025-05-14","title":"Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights","abstract":"Various techniques are used in the generation of adversarial examples, including methods such as TextBugger which introduce minor, hardly visible perturbations to words leading to changes in model behaviour. Another class of techniques involves substituting words with their synonyms in a way that preserves the text's meaning but alters its predicted class, with TextFooler being a prominent example of such attacks. Most adversarial example generation methods are developed and evaluated primarily on non-inflectional languages, typically English. In this work, we evaluate and explain how adversarial attacks perform in inflectional languages. To explain the impact of inflection on model behaviour and its robustness under attack, we designed a novel protocol inspired by mechanistic interpretability, based on Edge Attribution Patching (EAP) method. The proposed evaluation protocol relies on parallel task-specific corpora that include both inflected and syncretic variants of texts in two languages -- Polish and English. To analyse the models and explain the relationship between inflection and adversarial robustness, we create a new benchmark based on task-oriented dataset MultiEmo, enabling the identification of mechanistic inflection-related elements of circuits within the model and analyse their behaviour under attack.","authors":["Pawe{\\l} Walkowiak","Marek Klonowski","Marcin Oleksy","Arkadiusz Janz"],"url":"https://arxiv.org/abs/2505.07856"}
{"created":"2025-05-14","title":"Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines","abstract":"Multifarious intent detection predictors are developed for different languages, including English, Chinese and French, however, the field remains underdeveloped for Urdu, the 10th most spoken language. In the realm of well-known languages, intent detection predictors utilize the strategy of few-shot learning and prediction of unseen classes based on the model training on seen classes. However, Urdu language lacks few-shot strategy based intent detection predictors and traditional predictors are focused on prediction of the same classes which models have seen in the train set. To empower Urdu language specific intent detection, this introduces a unique contrastive learning approach that leverages unlabeled Urdu data to re-train pre-trained language models. This re-training empowers LLMs representation learning for the downstream intent detection task. Finally, it reaps the combined potential of pre-trained LLMs and the prototype-informed attention mechanism to create a comprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm of proposed predictive pipeline, it explores the potential of 6 distinct language models and 13 distinct similarity computation methods. The proposed framework is evaluated on 2 public benchmark datasets, namely ATIS encompassing 5836 samples and Web Queries having 8519 samples. Across ATIS dataset under 4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and 98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score, respectively. In an additional case study on the Web Queries dataset under same classes train and test set settings, LLMPIA outperformed state-of-the-art predictor by 53.55% F1-Score.","authors":["Faiza Hassan","Summra Saleem","Kashif Javed","Muhammad Nabeel Asim","Abdur Rehman","Andreas Dengel"],"url":"https://arxiv.org/abs/2505.07857"}
{"created":"2025-05-14","title":"Scaling Laws for Speculative Decoding","abstract":"The escalating demand for efficient decoding in large language models (LLMs) is particularly critical for reasoning-intensive architectures like OpenAI-o3 and DeepSeek-R1, which depend on extended chain-of-thought reasoning. This study investigates speculative decoding techniques through dense LLM architectures to establish foundational insights for accelerating reasoning tasks. While speculative decoding methods leveraging parallel draft-verification cycles have emerged as promising acceleration techniques, the scaling laws governing decoding efficiency remain under-explored compared to conventional backbone LLMs developed through Pretraining->SFT->RLHF training paradigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2 and 1.3) governing draft model acceptance rate (or decoding speed) across three dimensions: pretraining token volume, draft model capacity, and decoding batch size. Building on these laws, we achieve Scylla, which coordinates multi-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical validation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and 0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on summarization and QA tasks (Figure 2). Industrial inference engine deployments demonstrate 2X decoding throughput improvements over EAGLE2 (Table 5), validating the transformative potential of systematic scaling for efficient LLM inference. Code will be released later.","authors":["Siyuan Yan","Mo Zhu","Guo-qing Jiang","Jianfei Wang","Jiaxing Chen","Wentai Zhang","Xiang Liao","Xiao Cui","Chen Zhang","Zhuoran Song","Ran Zhu"],"url":"https://arxiv.org/abs/2505.07858"}
{"created":"2025-05-14","title":"Boosting Performance on ARC is a Matter of Perspective","abstract":"The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge for large language models (LLMs), exposing limitations in their abstract reasoning abilities. In this work, we leverage task-specific data augmentations throughout the training, generation, and scoring phases, and employ a depth-first search algorithm to generate diverse, high-probability candidate solutions. Furthermore, we utilize the LLM not only as a generator but also as a scorer, using its output probabilities to select the most promising solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the public ARC-AGI evaluation set, demonstrating state-of-the-art performance among publicly available approaches. While concurrent closed-source work has reported higher scores, our method distinguishes itself through its transparency, reproducibility, and remarkably low inference cost, averaging only around 2ct per task on readily available hardware (we assume a price of 36ct/hour for a Nvidia 4090 GPU).","authors":["Daniel Franzen","Jan Disselhoff","David Hartmann"],"url":"https://arxiv.org/abs/2505.07859"}
{"created":"2025-05-14","title":"Scalable LLM Math Reasoning Acceleration with Low-rank Distillation","abstract":"Due to long generations, large language model (LLM) math reasoning demands significant computational resources and time. While many existing efficient inference methods have been developed with excellent performance preservation on language tasks, they often severely degrade math performance. In this paper, we propose Caprese, a low-cost distillation method to recover lost capabilities from deploying efficient inference methods, focused primarily in feedforward blocks. With original weights unperturbed, roughly 1% of additional parameters, and only 20K synthetic training samples, we are able to recover much if not all of the math capabilities lost from efficient inference for thinking LLMs and without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and integrates cleanly into existing model layers to reduce latency (>11% reduction to generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity.","authors":["Harry Dong","Bilge Acun","Beidi Chen","Yuejie Chi"],"url":"https://arxiv.org/abs/2505.07861"}
{"created":"2025-05-14","title":"Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition","abstract":"Existing sequence to sequence models for structured language tasks rely heavily on the dot product self attention mechanism, which incurs quadratic complexity in both computation and memory for input length N. We introduce the Graph Wavelet Transformer (GWT), a novel architecture that replaces this bottleneck with a learnable, multi scale wavelet transform defined over an explicit graph Laplacian derived from syntactic or semantic parses. Our analysis shows that multi scale spectral decomposition offers an interpretable, efficient, and expressive alternative to quadratic self attention for graph structured sequence modeling.","authors":["Andrew Kiruluta","Eric Lundy","Priscilla Burity"],"url":"https://arxiv.org/abs/2505.07862"}
{"created":"2025-05-14","title":"QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction","abstract":"Accurate prediction of Quality of Service (QoS) metrics is fundamental for selecting and managing cloud based services. Traditional QoS models rely on manual feature engineering and yield only point estimates, offering no insight into the confidence of their predictions. In this paper, we propose QoSBERT, the first framework that reformulates QoS prediction as a semantic regression task based on pre trained language models. Unlike previous approaches relying on sparse numerical features, QoSBERT automatically encodes user service metadata into natural language descriptions, enabling deep semantic understanding. Furthermore, we integrate a Monte Carlo Dropout based uncertainty estimation module, allowing for trustworthy and risk-aware service quality prediction, which is crucial yet underexplored in existing QoS models. QoSBERT applies attentive pooling over contextualized embeddings and a lightweight multilayer perceptron regressor, fine tuned jointly to minimize absolute error. We further exploit the resulting uncertainty estimates to select high quality training samples, improving robustness in low resource settings. On standard QoS benchmark datasets, QoSBERT achieves an average reduction of 11.7% in MAE and 6.7% in RMSE for response time prediction, and 6.9% in MAE for throughput prediction compared to the strongest baselines, while providing well calibrated confidence intervals for robust and trustworthy service quality estimation. Our approach not only advances the accuracy of service quality prediction but also delivers reliable uncertainty quantification, paving the way for more trustworthy, data driven service selection and optimization.","authors":["Ziliang Wang","Xiaohong Zhang","Ze Shi Li","Meng Yan"],"url":"https://arxiv.org/abs/2505.07863"}
{"created":"2025-05-14","title":"Arrow-Guided VLM: Enhancing Flowchart Understanding via Arrow Direction Encoding","abstract":"Flowcharts are indispensable tools in software design and business-process analysis, yet current vision-language models (VLMs) frequently misinterpret the directional arrows and graph topology that set these diagrams apart from natural images. We introduce a seven-stage pipeline grouped into three broader processes: (1) arrow-aware detection of nodes and arrow endpoints; (2) optical character recognition (OCR) to extract node text; and (3) construction of a structured prompt that guides the VLMs. Tested on a 90-question benchmark distilled from 30 annotated flowcharts, the method raises overall accuracy from 80 % to 89 % (+9 percentage points) without any task-specific fine-tuning. The gain is most pronounced for next-step queries (25/30 -> 30/30; 100 %, +17 pp); branch-result questions improve more modestly, and before-step questions remain difficult. A parallel evaluation with an LLM-as-a-Judge protocol shows the same trends, reinforcing the advantage of explicit arrow encoding. Limitations include dependence on detector and OCR precision, the small evaluation set, and residual errors at nodes with multiple incoming edges. Future work will enlarge the benchmark with synthetic and handwritten flowcharts and assess the approach on Business Process Model and Notation (BPMN) and Unified Modeling Language (UML).","authors":["Takamitsu Omasa","Ryo Koshihara","Masumi Morishige"],"url":"https://arxiv.org/abs/2505.07864"}
{"created":"2025-05-14","title":"VISTA: Generative Visual Imagination for Vision-and-Language Navigation","abstract":"Vision-and-Language Navigation (VLN) tasks agents with locating specific objects in unseen environments using natural language instructions and visual cues. Many existing VLN approaches typically follow an 'observe-and-reason' schema, that is, agents observe the environment and decide on the next action to take based on the visual observations of their surroundings. They often face challenges in long-horizon scenarios due to limitations in immediate observation and vision-language modality gaps. To overcome this, we present VISTA, a novel framework that employs an 'imagine-and-align' navigation strategy. Specifically, we leverage the generative prior of pre-trained diffusion models for dynamic visual imagination conditioned on both local observations and high-level language instructions. A Perceptual Alignment Filter module then grounds these goal imaginations against current observations, guiding an interpretable and structured reasoning process for action selection. Experiments show that VISTA sets new state-of-the-art results on Room-to-Room (R2R) and RoboTHOR benchmarks, e.g.,+3.6% increase in Success Rate on R2R. Extensive ablation analysis underscores the value of integrating forward-looking imagination, perceptual alignment, and structured reasoning for robust navigation in long-horizon environments.","authors":["Yanjia Huang","Mingyang Wu","Renjie Li","Zhengzhong Tu"],"url":"https://arxiv.org/abs/2505.07868"}
{"created":"2025-05-14","title":"Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection","abstract":"Large Language Models (LLMs) are increasingly deployed in various applications, raising critical concerns about fairness and potential biases in their outputs. This paper explores the prioritization of metamorphic relations (MRs) in metamorphic testing as a strategy to efficiently detect fairness issues within LLMs. Given the exponential growth of possible test cases, exhaustive testing is impractical; therefore, prioritizing MRs based on their effectiveness in detecting fairness violations is crucial. We apply a sentence diversity-based approach to compute and rank MRs to optimize fault detection. Experimental results demonstrate that our proposed prioritization approach improves fault detection rates by 22% compared to random prioritization and 12% compared to distance-based prioritization, while reducing the time to the first failure by 15% and 8%, respectively. Furthermore, our approach performs within 5% of fault-based prioritization in effectiveness, while significantly reducing the computational cost associated with fault labeling. These results validate the effectiveness of diversity-based MR prioritization in enhancing fairness testing for LLMs.","authors":["Suavis Giramata","Madhusudan Srinivasan","Venkat Naidu Gudivada","Upulee Kanewala"],"url":"https://arxiv.org/abs/2505.07870"}
{"created":"2025-05-14","title":"Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy","abstract":"Financial sentiment analysis (FSA) presents unique challenges to LLMs that surpass those in typical sentiment analysis due to the nuanced language used in financial contexts. The prowess of these models is often undermined by the inherent subjectivity of sentiment classifications in existing benchmark datasets like Financial Phrasebank. These datasets typically feature undefined sentiment classes that reflect the highly individualized perspectives of annotators, leading to significant variability in annotations. This variability results in an unfair expectation for LLMs during benchmarking, where they are tasked to conjecture the subjective viewpoints of human annotators without sufficient context. In this paper, we introduce the Annotators' Instruction Assisted Prompt, a novel evaluation prompt designed to redefine the task definition of FSA for LLMs. By integrating detailed task instructions originally intended for human annotators into the LLMs' prompt framework, AIAP aims to standardize the understanding of sentiment across both human and machine interpretations, providing a fair and context-rich foundation for sentiment analysis. We utilize a new dataset, WSBS, derived from the WallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM performance by aligning machine operations with the refined task definitions. Experimental results demonstrate that AIAP enhances LLM performance significantly, with improvements up to 9.08. This context-aware approach not only yields incremental gains in performance but also introduces an innovative sentiment-indexing method utilizing model confidence scores. This method enhances stock price prediction models and extracts more value from the financial sentiment analysis, underscoring the significance of WSB as a critical source of financial text. Our research offers insights into both improving FSA through better evaluation methods.","authors":["A M Muntasir Rahman","Ajim Uddin","Guiling \"Grace\" Wang"],"url":"https://arxiv.org/abs/2505.07871"}
{"created":"2025-05-14","title":"Revenue Optimization in Video Caching Networks with Privacy-Preserving Demand Predictions","abstract":"Performance of video streaming, which accounts for most of the traffic in wireless communication, can be significantly improved by caching popular videos at the wireless edge. Determining the cache content that optimizes performance (defined via a revenue function) is thus an important task, and prediction of the future demands based on past history can make this process much more efficient. However, since practical video caching networks involve various parties (e.g., users, isp, and csp) that do not wish to reveal information such as past history to each other, privacy-preserving solutions are required. Motivated by this, we propose a proactive caching method based on users' privacy-preserving multi-slot future demand predictions -- obtained from a trained Transformer -- to optimize revenue. Specifically, we first use a privacy-preserving fl algorithm to train a Transformer to predict multi-slot future demands of the users. However, prediction accuracy is not perfect and decreases the farther into the future the prediction is done. We model the impact of prediction errors invoking the file popularities, based on which we formulate a long-term system revenue optimization to make the cache placement decisions. As the formulated problem is NP-hard, we use a greedy algorithm to efficiently obtain an approximate solution. Simulation results validate that (i) the fl solution achieves results close to the centralized (non-privacy-preserving) solution and (ii) optimization of revenue may provide different solutions than the classical chr criterion.","authors":["Yijing Zhang","Ferdous Pervej","Andreas F. Molisch"],"url":"https://arxiv.org/abs/2505.07872"}
{"created":"2025-05-14","title":"The Sound of Populism: Distinct Linguistic Features Across Populist Variants","abstract":"This study explores the sound of populism by integrating the classic Linguistic Inquiry and Word Count (LIWC) features, which capture the emotional and stylistic tones of language, with a fine-tuned RoBERTa model, a state-of-the-art context-aware language model trained to detect nuanced expressions of populism. This approach allows us to uncover the auditory dimensions of political rhetoric in U.S. presidential inaugural and State of the Union addresses. We examine how four key populist dimensions (i.e., left-wing, right-wing, anti-elitism, and people-centrism) manifest in the linguistic markers of speech, drawing attention to both commonalities and distinct tonal shifts across these variants. Our findings reveal that populist rhetoric consistently features a direct, assertive ``sound\" that forges a connection with ``the people'' and constructs a charismatic leadership persona. However, this sound is not simply informal but strategically calibrated. Notably, right-wing populism and people-centrism exhibit a more emotionally charged discourse, resonating with themes of identity, grievance, and crisis, in contrast to the relatively restrained emotional tones of left-wing and anti-elitist expressions.","authors":["Yu Wang","Runxi Yu","Zhongyuan Wang","Jing He"],"url":"https://arxiv.org/abs/2505.07874"}
{"created":"2025-05-14","title":"Getting Ready for the EU AI Act in Healthcare. A call for Sustainable AI Development and Deployment","abstract":"Assessments of trustworthiness have become a cornerstone of responsible AI development. Especially in high-stakes fields like healthcare, aligning technical, evidence-based, and ethical practices with forthcoming legal requirements is increasingly urgent. We argue that developers and deployers of AI systems for the medical domain should be proactive and take steps to progressively ensure that such systems, both those currently in use and those being developed or planned, respect the requirements of the AI Act, which has come into force in August 2024. This is necessary if full and effective compliance is to be ensured when the most relevant provisions of the Act become effective (August 2026). The engagement with the AI Act cannot be viewed as a formalistic exercise. Compliance with the AI Act needs to be carried out through the proactive commitment to the ethical principles of trustworthy AI. These principles provide the background for the Act, which mentions them several times and connects them to the protection of public interest. They can be used to interpret and apply the Act's provisions and to identify good practices, increasing the validity and sustainability of AI systems over time.","authors":["John Brandt Brodersen (University of Copenhagen","Denmark","UiT The Arctic University of Norway)","Ilaria Amelia Caggiano (Research Center in European Private Law)","Pedro Kringen (Arcada University of Applied Science","Helsinki","Finland)","Vince Istvan Madai (QUEST Centre for Responsible Research","Berlin Institute of Health","Charit\\'e - Universit\\\"atsmedizin Berlin","Germany)","Walter Osika (TEA Lab","Karolinska Institutet","Stockholm","Sweden","Stockholm Health Care Services","Stockholm Region","Sweden)","Giovanni Sartor (CIRSFID-Alma AI","University of Bologna","Italy","EUI","Florence","Italy)","Ellen Svensson (TEA Lab","Karolinska Institutet","Stockholm","Sweden","Stockholm University)","Magnus Westerlund (Arcada University of Applied Science","Helsinki","Finland)","Roberto V. Zicari (Graduate School of Data Science","Seoul National University","South Korea)"],"url":"https://arxiv.org/abs/2505.07875"}
{"created":"2025-05-14","title":"Efficient Telecom Specific LLM: TSLAM-Mini with QLoRA and Digital Twin Data","abstract":"General-purpose large language models (LLMs), despite their broad capabilities accrued from open-world data, frequently exhibit suboptimal performance when confronted with the nuanced and specialized demands inherent in real-time telecommunications applications. This investigation addresses this critical limitation through the meticulous fine-tuning of TSLAM-Mini developed by NetoAI, a compact (3.8-billion parameter) causal language model architecturally derived from Phi-4 Mini Instruct 4B. The fine-tuning regimen leverages a bespoke dataset comprising 100,000 samples, strategically engineered to address 20 pivotal telecommunications use-cases, encompassing domains such as Network Fundamentals, IP Routing, MPLS, Network Security, Automation, OSS/BSS, RAN, Mobile Core, Satellite Communications, and Ethical AI. This dataset was curated utilizing NetoAI's DigiTwin platform, enriched with granular insights from venerated network Subject Matter Experts (SMEs) and authoritative RFC documents, thereby capturing high-fidelity representations of real-world network dynamics through simulations inspired by digital twin paradigms. Employing Quantized Low-Rank Adaptation (QLoRA), a state-of-the-art Parameter Efficient Fine-Tuning (PEFT) technique, we achieved substantial training efficiency and enabled prospective deployment on resource-constrained hardware. A novel evaluation framework, predicated on a high-capacity LLM (Qwen3-235B-A22B) functioning as an automated adjudicator, was instituted to rigorously assess instruction-following fidelity and response quality across the specified telecom use-cases. Empirical results unequivocally demonstrate TSLAM-Mini's superior aptitude in telecom-centric applications, underscoring the profound efficacy of domain-specific datasets and PEFT methodologies for advancing intelligent network management.","authors":["Vignesh Ethiraj","Divya Vijay","Sidhanth Menon","Heblin Berscilla"],"url":"https://arxiv.org/abs/2505.07877"}
{"created":"2025-05-14","title":"OMGM: Orchestrate Multiple Granularities and Modalities for Efficient Multimodal Retrieval","abstract":"Vision-language retrieval-augmented generation (RAG) has become an effective approach for tackling Knowledge-Based Visual Question Answering (KB-VQA), which requires external knowledge beyond the visual content presented in images. The effectiveness of Vision-language RAG systems hinges on multimodal retrieval, which is inherently challenging due to the diverse modalities and knowledge granularities in both queries and knowledge bases. Existing methods have not fully tapped into the potential interplay between these elements. We propose a multimodal RAG system featuring a coarse-to-fine, multi-step retrieval that harmonizes multiple granularities and modalities to enhance efficacy. Our system begins with a broad initial search aligning knowledge granularity for cross-modal retrieval, followed by a multimodal fusion reranking to capture the nuanced multimodal information for top entity selection. A text reranker then filters out the most relevant fine-grained section for augmented generation. Extensive experiments on the InfoSeek and Encyclopedic-VQA benchmarks show our method achieves state-of-the-art retrieval performance and highly competitive answering results, underscoring its effectiveness in advancing KB-VQA systems.","authors":["Wei Yang","Jingjing Fu","Rui Wang","Jinyu Wang","Lei Song","Jiang Bian"],"url":"https://arxiv.org/abs/2505.07879"}
{"created":"2025-05-14","title":"ASIL-Decomposition Based Resource Allocation Optimization for Automotive E/E Architectures","abstract":"Recent years have brought a surge of efforts in rethinking the vehicle's electrical and/or electronic (E/E) architecture as well as the development process to reduce complexity and enable automation, connectivity, and electromobility. Resource allocation is an important step of the development process that can influence the quality of the designed system. As the design space is large and complex, intuitive design can turn into a time-consuming process with sub-optimal solutions.","authors":["Dorsa Zaheri","Hans-Christian Reuss"],"url":"https://arxiv.org/abs/2505.07881"}
{"created":"2025-05-14","title":"Enhancing Trust Management System for Connected Autonomous Vehicles Using Machine Learning Methods: A Survey","abstract":"Connected Autonomous Vehicles (CAVs) operate in dynamic, open, and multi-domain networks, rendering them vulnerable to various threats. Trust Management Systems (TMS) systematically organize essential steps in the trust mechanism, identifying malicious nodes against internal threats and external threats, as well as ensuring reliable decision-making for more cooperative tasks. Recent advances in machine learning (ML) offer significant potential to enhance TMS, especially for the strict requirements of CAVs, such as CAV nodes moving at varying speeds, and opportunistic and intermittent network behavior. Those features distinguish ML-based TMS from social networks, static IoT, and Social IoT. This survey proposes a novel three-layer ML-based TMS framework for CAVs in the vehicle-road-cloud integration system, i.e., trust data layer, trust calculation layer and trust incentive layer. A six-dimensional taxonomy of objectives is proposed. Furthermore, the principles of ML methods for each module in each layer are analyzed. Then, recent studies are categorized based on traffic scenarios that are against the proposed objectives. Finally, future directions are suggested, addressing the open issues and meeting the research trend. We maintain an active repository that contains up-to-date literature and open-source projects at https://github.com/octoberzzzzz/ML-based-TMS-CAV-Survey.","authors":["Qian Xu","Lei Zhang","Yixiao Liu"],"url":"https://arxiv.org/abs/2505.07882"}
{"created":"2025-05-14","title":"Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints","abstract":"Rational decision-making under uncertainty requires coherent degrees of belief in events. However, event probabilities generated by Large Language Models (LLMs) have been shown to exhibit incoherence, violating the axioms of probability theory. This raises the question of whether coherent event probabilities can be recovered from the embeddings used by the models. If so, those derived probabilities could be used as more accurate estimates in events involving uncertainty. To explore this question, we propose enforcing axiomatic constraints, such as the additive rule of probability theory, in the latent space learned by an extended variational autoencoder (VAE) applied to LLM embeddings. This approach enables event probabilities to naturally emerge in the latent space as the VAE learns to both reconstruct the original embeddings and predict the embeddings of semantically related events. We evaluate our method on complementary events (i.e., event A and its complement, event not-A), where the true probabilities of the two events must sum to 1. Experiment results on open-weight language models demonstrate that probabilities recovered from embeddings exhibit greater coherence than those directly reported by the corresponding models and align closely with the true probabilities.","authors":["Jian-Qiao Zhu","Haijiang Yan","Thomas L. Griffiths"],"url":"https://arxiv.org/abs/2505.07883"}
{"created":"2025-05-14","title":"Development of a WAZOBIA-Named Entity Recognition System","abstract":"Named Entity Recognition NER is very crucial for various natural language processing applications, including information extraction, machine translation, and sentiment analysis. Despite the ever-increasing interest in African languages within computational linguistics, existing NER systems focus mainly on English, European, and a few other global languages, leaving a significant gap for under-resourced languages. This research presents the development of a WAZOBIA-NER system tailored for the three most prominent Nigerian languages: Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation of annotated datasets for each language, addressing data scarcity and linguistic diversity challenges. Exploring the state-of-the-art machine learning technique, Conditional Random Fields (CRF) and deep learning models such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder Representation from Transformers (Bert) and fine-tune with a Recurrent Neural Network (RNN), the study evaluates the effectiveness of these approaches in recognizing three entities: persons, organizations, and locations. The system utilizes optical character recognition (OCR) technology to convert textual images into machine-readable text, thereby enabling the Wazobia system to accept both input text and textual images for extraction purposes. The system achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across three languages, with precision, recall, F1-score, and accuracy as key assessment metrics. The Wazobia-NER system demonstrates that it is feasible to build robust NER tools for under-resourced African languages using current NLP frameworks and transfer learning.","authors":["S. E Emedem","I. E Onyenwe","E. G Onyedinma"],"url":"https://arxiv.org/abs/2505.07884"}
{"created":"2025-05-14","title":"PLHF: Prompt Optimization with Few-Shot Human Feedback","abstract":"Automatic prompt optimization frameworks are developed to obtain suitable prompts for large language models (LLMs) with respect to desired output quality metrics. Although existing approaches can handle conventional tasks such as fixed-solution question answering, defining the metric becomes complicated when the output quality cannot be easily assessed by comparisons with standard golden samples. Consequently, optimizing the prompts effectively and efficiently without a clear metric becomes a critical challenge. To address the issue, we present PLHF (which stands for \"P\"rompt \"L\"earning with \"H\"uman \"F\"eedback), a few-shot prompt optimization framework inspired by the well-known RLHF technique. Different from naive strategies, PLHF employs a specific evaluator module acting as the metric to estimate the output quality. PLHF requires only a single round of human feedback to complete the entire prompt optimization process. Empirical results on both public and industrial datasets show that PLHF outperforms prior output grading strategies for LLM prompt optimizations.","authors":["Chun-Pai Yang","Kan Zheng","Shou-De Lin"],"url":"https://arxiv.org/abs/2505.07886"}
{"created":"2025-05-14","title":"Monocular Online Reconstruction with Enhanced Detail Preservation","abstract":"We propose an online 3D Gaussian-based dense mapping framework for photorealistic details reconstruction from a monocular image stream. Our approach addresses two key challenges in monocular online reconstruction: distributing Gaussians without relying on depth maps and ensuring both local and global consistency in the reconstructed maps. To achieve this, we introduce two key modules: the Hierarchical Gaussian Management Module for effective Gaussian distribution and the Global Consistency Optimization Module for maintaining alignment and coherence at all scales. In addition, we present the Multi-level Occupancy Hash Voxels (MOHV), a structure that regularizes Gaussians for capturing details across multiple levels of granularity. MOHV ensures accurate reconstruction of both fine and coarse geometries and textures, preserving intricate details while maintaining overall structural integrity. Compared to state-of-the-art RGB-only and even RGB-D methods, our framework achieves superior reconstruction quality with high computational efficiency. Moreover, it integrates seamlessly with various tracking systems, ensuring generality and scalability.","authors":["Songyin Wu","Zhaoyang Lv","Yufeng Zhu","Duncan Frost","Zhengqin Li","Ling-Qi Yan","Carl Ren","Richard Newcombe","Zhao Dong"],"url":"https://arxiv.org/abs/2505.07887"}
{"created":"2025-05-14","title":"Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping","abstract":"This paper addresses the challenge in long-text style transfer using zero-shot learning of large language models (LLMs), proposing a hierarchical framework that combines sentence-level stylistic adaptation with paragraph-level structural coherence. We argue that in the process of effective paragraph-style transfer, to preserve the consistency of original syntactic and semantic information, it is essential to perform style transfer not only at the sentence level but also to incorporate paragraph-level semantic considerations, while ensuring structural coherence across inter-sentential relationships. Our proposed framework, ZeroStylus, operates through two systematic phases: hierarchical template acquisition from reference texts and template-guided generation with multi-granular matching. The framework dynamically constructs sentence and paragraph template repositories, enabling context-aware transformations while preserving inter-sentence logical relationships. Experimental evaluations demonstrate significant improvements over baseline methods, with structured rewriting achieving 6.90 average score compared to 6.70 for direct prompting approaches in tri-axial metrics assessing style consistency, content preservation, and expression quality. Ablation studies validate the necessity of both template hierarchies during style transfer, showing higher content preservation win rate against sentence-only approaches through paragraph-level structural encoding, as well as direct prompting method through sentence-level pattern extraction and matching. The results establish new capabilities for coherent long-text style transfer without requiring parallel corpora or LLM fine-tuning.","authors":["Yusen Wu","Xiaotie Deng"],"url":"https://arxiv.org/abs/2505.07888"}
{"created":"2025-05-14","title":"BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning","abstract":"Biological protocols are fundamental to reproducible and safe life science research. While LLMs excel on general tasks, their systematic evaluation on these highly specialized, accuracy-critical, and inherently procedural texts remains limited. In this work, we present BioProBench, the first large-scale, integrated multi-task benchmark for biological protocol understanding and reasoning. While limited benchmarks have touched upon specific aspects like protocol QA, BioProBench provides a comprehensive suite of five core tasks: Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning, enabling a holistic evaluation of LLMs on procedural biological texts. Built upon 27K original protocols, it yields nearly 556K high-quality structured instances. We evaluate 12 mainstream open/closed-source LLMs on BioProBench. Experimental results reveal that while top models preform well on surface understanding tasks, struggle significantly with deep reasoning and structured generation tasks like ordering and generation. Furthermore, model comparisons reveal diverse performance: certain open-source models approach closed-source levels on some tasks, yet bio-specific small models lag behind general LLMs, indicating limitations on complex procedural content. Overall, our findings underscore that procedural reasoning within biological protocols represents a significant challenge for current LLMs. BioProBench serves as a standardized framework to diagnose these specific limitations and guide the development of AI systems better equipped for safely automating complex scientific procedures. The code and data are available at: https://github.com/YuyangSunshine/bioprotocolbench and https://huggingface.co/datasets/GreatCaptainNemo/BioProBench.","authors":["Yuyang Liu","Liuzhenghao Lv","Xiancheng Zhang","Li Yuan","Yonghong Tian"],"url":"https://arxiv.org/abs/2505.07889"}
{"created":"2025-05-14","title":"TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks","abstract":"This study presents TSLFormer, a light and robust word-level Turkish Sign Language (TSL) recognition model that treats sign gestures as ordered, string-like language. Instead of using raw RGB or depth videos, our method only works with 3D joint positions - articulation points - extracted using Google's Mediapipe library, which focuses on the hand and torso skeletal locations. This creates efficient input dimensionality reduction while preserving important semantic gesture information.","authors":["Kutay Ert\\\"urk","Furkan Alt{\\i}n{\\i}\\c{s}{\\i}k","\\.Irem Sar{\\i}alt{\\i}n","\\\"Omer Nezih Gerek"],"url":"https://arxiv.org/abs/2505.07890"}
{"created":"2025-05-14","title":"TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking","abstract":"In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT , a novel generative artificial intelligence solution designed for fact-checking in the health domain. TrumorGPT aims to distinguish \"trumors\", which are health-related rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework leverages a large language model (LLM) with few-shot learning for semantic health knowledge graph construction and semantic reasoning. TrumorGPT incorporates graph-based retrieval-augmented generation (GraphRAG) to address the hallucination issue common in LLMs and the limitations of static training data. GraphRAG involves accessing and utilizing information from regularly updated semantic health knowledge graphs that consist of the latest medical news and health information, ensuring that fact-checking by TrumorGPT is based on the most recent data. Evaluating with extensive healthcare datasets, TrumorGPT demonstrates superior performance in fact-checking for public health claims. Its ability to effectively conduct fact-checking across various platforms marks a critical step forward in the fight against health-related misinformation, enhancing trust and accuracy in the digital information age.","authors":["Ching Nam Hang","Pei-Duo Yu","Chee Wei Tan"],"url":"https://arxiv.org/abs/2505.07891"}
{"created":"2025-05-14","title":"VoI-Driven Joint Optimization of Control and Communication in Vehicular Digital Twin Network","abstract":"The vision of sixth-generation (6G) wireless networks paves the way for the seamless integration of digital twins into vehicular networks, giving rise to a Vehicular Digital Twin Network (VDTN). The large amount of computing resources as well as the massive amount of spatial-temporal data in Digital Twin (DT) domain can be utilized to enhance the communication and control performance of Internet of Vehicle (IoV) systems. In this article, we first propose the architecture of VDTN, emphasizing key modules that center on functions related to the joint optimization of control and communication. We then delve into the intricacies of the multitimescale decision process inherent in joint optimization in VDTN, specifically investigating the dynamic interplay between control and communication. To facilitate the joint optimization, we define two Value of Information (VoI) concepts rooted in control performance. Subsequently, utilizing VoI as a bridge between control and communication, we introduce a novel joint optimization framework, which involves iterative processing of two Deep Reinforcement Learning (DRL) modules corresponding to control and communication to derive the optimal policy. Finally, we conduct simulations of the proposed framework applied to a platoon scenario to demonstrate its effectiveness in ensu","authors":["Lei Lei (Sherman)","Kan Zheng (Sherman)","Jie Mei (Sherman)","Xuemin (Sherman)","Shen"],"url":"https://arxiv.org/abs/2505.07892"}
{"created":"2025-05-14","title":"Channel Fingerprint Construction for Massive MIMO: A Deep Conditional Generative Approach","abstract":"Accurate channel state information (CSI) acquisition for massive multiple-input multiple-output (MIMO) systems is essential for future mobile communication networks. Channel fingerprint (CF), also referred to as channel knowledge map, is a key enabler for intelligent environment-aware communication and can facilitate CSI acquisition. However, due to the cost limitations of practical sensing nodes and test vehicles, the resulting CF is typically coarse-grained, making it insufficient for wireless transceiver design. In this work, we introduce the concept of CF twins and design a conditional generative diffusion model (CGDM) with strong implicit prior learning capabilities as the computational core of the CF twin to establish the connection between coarse- and fine-grained CFs. Specifically, we employ a variational inference technique to derive the evidence lower bound (ELBO) for the log-marginal distribution of the observed fine-grained CF conditioned on the coarse-grained CF, enabling the CGDM to learn the complicated distribution of the target data. During the denoising neural network optimization, the coarse-grained CF is introduced as side information to accurately guide the conditioned generation of the CGDM. To make the proposed CGDM lightweight, we further leverage the additivity of network layers and introduce a one-shot pruning approach along with a multi-objective knowledge distillation technique. Experimental results show that the proposed approach exhibits significant improvement in reconstruction performance compared to the baselines. Additionally, zero-shot testing on reconstruction tasks with different magnification factors further demonstrates the scalability and generalization ability of the proposed approach.","authors":["Zhenzhou Jin","Li You","Xudong Li","Zhen Gao","Yuanwei Liu","Xiang-Gen Xia","Xiqi Gao"],"url":"https://arxiv.org/abs/2505.07893"}
{"created":"2025-05-14","title":"EnvCDiff: Joint Refinement of Environmental Information and Channel Fingerprints via Conditional Generative Diffusion Model","abstract":"The paradigm shift from environment-unaware communication to intelligent environment-aware communication is expected to facilitate the acquisition of channel state information for future wireless communications. Channel Fingerprint (CF), as an emerging enabling technology for environment-aware communication, provides channel-related knowledge for potential locations within the target communication area. However, due to the limited availability of practical devices for sensing environmental information and measuring channel-related knowledge, most of the acquired environmental information and CF are coarse-grained, insufficient to guide the design of wireless transmissions. To address this, this paper proposes a deep conditional generative learning approach, namely a customized conditional generative diffusion model (CDiff). The proposed CDiff simultaneously refines environmental information and CF, reconstructing a fine-grained CF that incorporates environmental information, referred to as EnvCF, from its coarse-grained counterpart. Experimental results show that the proposed approach significantly improves the performance of EnvCF construction compared to the baselines.","authors":["Zhenzhou Jin","Li You","Xiang-Gen Xia","Xiqi Gao"],"url":"https://arxiv.org/abs/2505.07894"}
{"created":"2025-05-14","title":"Representation Learning with Mutual Influence of Modalities for Node Classification in Multi-Modal Heterogeneous Networks","abstract":"Nowadays, numerous online platforms can be described as multi-modal heterogeneous networks (MMHNs), such as Douban's movie networks and Amazon's product review networks. Accurately categorizing nodes within these networks is crucial for analyzing the corresponding entities, which requires effective representation learning on nodes. However, existing multi-modal fusion methods often adopt either early fusion strategies which may lose the unique characteristics of individual modalities, or late fusion approaches overlooking the cross-modal guidance in GNN-based information propagation. In this paper, we propose a novel model for node classification in MMHNs, named Heterogeneous Graph Neural Network with Inter-Modal Attention (HGNN-IMA). It learns node representations by capturing the mutual influence of multiple modalities during the information propagation process, within the framework of heterogeneous graph transformer. Specifically, a nested inter-modal attention mechanism is integrated into the inter-node attention to achieve adaptive multi-modal fusion, and modality alignment is also taken into account to encourage the propagation among nodes with consistent similarities across all modalities. Moreover, an attention loss is augmented to mitigate the impact of missing modalities. Extensive experiments validate the superiority of the model in the node classification task, providing an innovative view to handle multi-modal data, especially when accompanied with network structures.","authors":["Jiafan Li","Jiaqi Zhu","Liang Chang","Yilin Li","Miaomiao Li","Yang Wang","Hongan Wang"],"url":"https://arxiv.org/abs/2505.07895"}
{"created":"2025-05-14","title":"LongCodeBench: Evaluating Coding LLMs at 1M Context Windows","abstract":"Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM coding abilities in long-context scenarios. Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5.","authors":["Stefano Rando","Luca Romani","Alessio Sampieri","Yuta Kyuragi","Luca Franco","Fabio Galasso","Tatsunori Hashimoto","John Yang"],"url":"https://arxiv.org/abs/2505.07897"}
{"created":"2025-05-14","title":"LECTOR: Summarizing E-book Reading Content for Personalized Student Support","abstract":"Educational e-book platforms provide valuable information to teachers and researchers through two main sources: reading activity data and reading content data. While reading activity data is commonly used to analyze learning strategies and predict low-performing students, reading content data is often overlooked in these analyses. To address this gap, this study proposes LECTOR (Lecture slides and Topic Relationships), a model that summarizes information from reading content in a format that can be easily integrated with reading activity data. Our first experiment compared LECTOR to representative Natural Language Processing (NLP) models in extracting key information from 2,255 lecture slides, showing an average improvement of 5% in F1-score. These results were further validated through a human evaluation involving 28 students, which showed an average improvement of 21% in F1-score over a model predominantly used in current educational tools. Our second experiment compared reading preferences extracted by LECTOR with traditional reading activity data in predicting low-performing students using 600,712 logs from 218 students. The results showed a tendency to improve the predictive performance by integrating LECTOR. Finally, we proposed examples showing the potential application of the reading preferences extracted by LECTOR in designing personalized interventions for students.","authors":["Erwin Daniel L\\'opez Zapata","Cheng Tang","Valdemar \\v{S}v\\'abensk\\'y","Fumiya Okubo","Atsushi Shimada"],"url":"https://arxiv.org/abs/2505.07898"}
{"created":"2025-05-14","title":"DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise","abstract":"Sequential knowledge editing techniques aim to continuously update the knowledge in large language models at a low cost, preventing the models from generating outdated or incorrect information. However, existing sequential editing methods suffer from a significant decline in editing success rates after long-term editing. Through theoretical analysis and experiments, we identify that as the number of edits increases, the model's output increasingly deviates from the desired target, leading to a drop in editing success rates. We refer to this issue as the accumulation of superimposed noise problem. To address this, we identify the factors contributing to this deviation and propose DeltaEdit, a novel method that optimizes update parameters through a dynamic orthogonal constraints strategy, effectively reducing interference between edits to mitigate deviation. Experimental results demonstrate that DeltaEdit significantly outperforms existing methods in edit success rates and the retention of generalization capabilities, ensuring stable and reliable model performance even under extensive sequential editing.","authors":["Ding Cao","Yuchen Cai","Rongxi Guo","Xuesong He","Guiquan Liu"],"url":"https://arxiv.org/abs/2505.07899"}
{"created":"2025-05-14","title":"Latent Behavior Diffusion for Sequential Reaction Generation in Dyadic Setting","abstract":"The dyadic reaction generation task involves synthesizing responsive facial reactions that align closely with the behaviors of a conversational partner, enhancing the naturalness and effectiveness of human-like interaction simulations. This paper introduces a novel approach, the Latent Behavior Diffusion Model, comprising a context-aware autoencoder and a diffusion-based conditional generator that addresses the challenge of generating diverse and contextually relevant facial reactions from input speaker behaviors. The autoencoder compresses high-dimensional input features, capturing dynamic patterns in listener reactions while condensing complex input data into a concise latent representation, facilitating more expressive and contextually appropriate reaction synthesis. The diffusion-based conditional generator operates on the latent space generated by the autoencoder to predict realistic facial reactions in a non-autoregressive manner. This approach allows for generating diverse facial reactions that reflect subtle variations in conversational cues and emotional states. Experimental results demonstrate the effectiveness of our approach in achieving superior performance in dyadic reaction synthesis tasks compared to existing methods.","authors":["Minh-Duc Nguyen","Hyung-Jeong Yang","Soo-Hyung Kim","Ji-Eun Shin","Seung-Won Kim"],"url":"https://arxiv.org/abs/2505.07901"}
{"created":"2025-05-14","title":"Multimodal Assessment of Classroom Discourse Quality: A Text-Centered Attention-Based Multi-Task Learning Approach","abstract":"Classroom discourse is an essential vehicle through which teaching and learning take place. Assessing different characteristics of discursive practices and linking them to student learning achievement enhances the understanding of teaching quality. Traditional assessments rely on manual coding of classroom observation protocols, which is time-consuming and costly. Despite many studies utilizing AI techniques to analyze classroom discourse at the utterance level, investigations into the evaluation of discursive practices throughout an entire lesson segment remain limited. To address this gap, our study proposes a novel text-centered multimodal fusion architecture to assess the quality of three discourse components grounded in the Global Teaching InSights (GTI) observation protocol: Nature of Discourse, Questioning, and Explanations. First, we employ attention mechanisms to capture inter- and intra-modal interactions from transcript, audio, and video streams. Second, a multi-task learning approach is adopted to jointly predict the quality scores of the three components. Third, we formulate the task as an ordinal classification problem to account for rating level order. The effectiveness of these designed elements is demonstrated through an ablation study on the GTI Germany dataset containing 92 videotaped math lessons. Our results highlight the dominant role of text modality in approaching this task. Integrating acoustic features enhances the model's consistency with human ratings, achieving an overall Quadratic Weighted Kappa score of 0.384, comparable to human inter-rater reliability (0.326). Our study lays the groundwork for the future development of automated discourse quality assessment to support teacher professional development through timely feedback on multidimensional discourse practices.","authors":["Ruikun Hou","Babette B\\\"uhler","Tim F\\\"utterer","Efe Bozkir","Peter Gerjets","Ulrich Trautwein","Enkelejda Kasneci"],"url":"https://arxiv.org/abs/2505.07902"}
{"created":"2025-05-14","title":"SEM: Reinforcement Learning for Search-Efficient Large Language Models","abstract":"Recent advancements in Large Language Models(LLMs) have demonstrated their capabilities not only in reasoning but also in invoking external tools, particularly search engines. However, teaching models to discern when to invoke search and when to rely on their internal knowledge remains a significant challenge. Existing reinforcement learning approaches often lead to redundant search behaviors, resulting in inefficiencies and over-cost. In this paper, we propose SEM, a novel post-training reinforcement learning framework that explicitly trains LLMs to optimize search usage. By constructing a balanced dataset combining MuSiQue and MMLU, we create scenarios where the model must learn to distinguish between questions it can answer directly and those requiring external retrieval. We design a structured reasoning template and employ Group Relative Policy Optimization(GRPO) to post-train the model's search behaviors. Our reward function encourages accurate answering without unnecessary search while promoting effective retrieval when needed. Experimental results demonstrate that our method significantly reduces redundant search operations while maintaining or improving answer accuracy across multiple challenging benchmarks. This framework advances the model's reasoning efficiency and extends its capability to judiciously leverage external knowledge.","authors":["Zeyang Sha","Shiwen Cui","Weiqiang Wang"],"url":"https://arxiv.org/abs/2505.07903"}
{"created":"2025-05-14","title":"A Reproduction Study: The Kernel PCA Interpretation of Self-Attention Fails Under Scrutiny","abstract":"In this reproduction study, we revisit recent claims that self-attention implements kernel principal component analysis (KPCA) (Teo et al., 2024), positing that (i) value vectors $V$ capture the eigenvectors of the Gram matrix of the keys, and (ii) that self-attention projects queries onto the principal component axes of the key matrix $K$ in a feature space. Our analysis reveals three critical inconsistencies: (1) No alignment exists between learned self-attention value vectors and what is proposed in the KPCA perspective, with average similarity metrics (optimal cosine similarity $\\leq 0.32$, linear CKA (Centered Kernel Alignment) $\\leq 0.11$, kernel CKA $\\leq 0.32$) indicating negligible correspondence; (2) Reported decreases in reconstruction loss $J_\\text{proj}$, arguably justifying the claim that the self-attention minimizes the projection error of KPCA, are misinterpreted, as the quantities involved differ by orders of magnitude ($\\sim\\!10^3$); (3) Gram matrix eigenvalue statistics, introduced to justify that $V$ captures the eigenvector of the gram matrix, are irreproducible without undocumented implementation-specific adjustments. Across 10 transformer architectures, we conclude that the KPCA interpretation of self-attention lacks empirical support.","authors":["Karahan Sar{\\i}ta\\c{s}","\\c{C}a\\u{g}atay Y{\\i}ld{\\i}z"],"url":"https://arxiv.org/abs/2505.07908"}
{"created":"2025-05-14","title":"Tuning for Trustworthiness -- Balancing Performance and Explanation Consistency in Neural Network Optimization","abstract":"Despite the growing interest in Explainable Artificial Intelligence (XAI), explainability is rarely considered during hyperparameter tuning or neural architecture optimization, where the focus remains primarily on minimizing predictive loss. In this work, we introduce the novel concept of XAI consistency, defined as the agreement among different feature attribution methods, and propose new metrics to quantify it. For the first time, we integrate XAI consistency directly into the hyperparameter tuning objective, creating a multi-objective optimization framework that balances predictive performance with explanation robustness. Implemented within the Sequential Parameter Optimization Toolbox (SPOT), our approach uses both weighted aggregation and desirability-based strategies to guide model selection. Through our proposed framework and supporting tools, we explore the impact of incorporating XAI consistency into the optimization process. This enables us to characterize distinct regions in the architecture configuration space: one region with poor performance and comparatively low interpretability, another with strong predictive performance but weak interpretability due to low \\gls{xai} consistency, and a trade-off region that balances both objectives by offering high interpretability alongside competitive performance. Beyond introducing this novel approach, our research provides a foundation for future investigations into whether models from the trade-off zone-balancing performance loss and XAI consistency-exhibit greater robustness by avoiding overfitting to training performance, thereby leading to more reliable predictions on out-of-distribution data.","authors":["Alexander Hinterleitner","Thomas Bartz-Beielstein"],"url":"https://arxiv.org/abs/2505.07910"}
{"created":"2025-05-14","title":"Combining Bayesian Inference and Reinforcement Learning for Agent Decision Making: A Review","abstract":"Bayesian inference has many advantages in decision making of agents (e.g. robotics/simulative agent) over a regular data-driven black-box neural network: Data-efficiency, generalization, interpretability, and safety where these advantages benefit directly/indirectly from the uncertainty quantification of Bayesian inference. However, there are few comprehensive reviews to summarize the progress of Bayesian inference on reinforcement learning (RL) for decision making to give researchers a systematic understanding. This paper focuses on combining Bayesian inference with RL that nowadays is an important approach in agent decision making. To be exact, this paper discusses the following five topics: 1) Bayesian methods that have potential for agent decision making. First basic Bayesian methods and models (Bayesian rule, Bayesian learning, and Bayesian conjugate models) are discussed followed by variational inference, Bayesian optimization, Bayesian deep learning, Bayesian active learning, Bayesian generative models, Bayesian meta-learning, and lifelong Bayesian learning. 2) Classical combinations of Bayesian methods with model-based RL (with approximation methods), model-free RL, and inverse RL. 3) Latest combinations of potential Bayesian methods with RL. 4) Analytical comparisons of methods that combine Bayesian methods with RL with respect to data-efficiency, generalization, interpretability, and safety. 5) In-depth discussions in six complex problem variants of RL, including unknown reward, partial-observability, multi-agent, multi-task, non-linear non-Gaussian, and hierarchical RL problems and the summary of how Bayesian methods work in the data collection, data processing and policy learning stages of RL to pave the way for better agent decision-making strategies.","authors":["Chengmin Zhou","Ville Kyrki","Pasi Fr\\\"anti","Laura Ruotsalainen"],"url":"https://arxiv.org/abs/2505.07911"}
{"created":"2025-05-14","title":"SciCom Wiki: Fact-Checking and FAIR Knowledge Distribution for Scientific Videos and Podcasts","abstract":"Democratic societies need accessible, reliable information. Videos and Podcasts have established themselves as the medium of choice for civic dissemination, but also as carriers of misinformation. The emerging Science Communication Knowledge Infrastructure (SciCom KI) curating non-textual media is still fragmented and not adequately equipped to scale against the content flood. Our work sets out to support the SciCom KI with a central, collaborative platform, the SciCom Wiki, to facilitate FAIR (findable, accessible, interoperable, reusable) media representation and the fact-checking of their content, particularly for videos and podcasts. Building an open-source service system centered around Wikibase, we survey requirements from 53 stakeholders, refine these in 11 interviews, and evaluate our prototype based on these requirements with another 14 participants. To address the most requested feature, fact-checking, we developed a neurosymbolic computational fact-checking approach, converting heterogenous media into knowledge graphs. This increases machine-readability and allows comparing statements against equally represented ground-truth. Our computational fact-checking tool was iteratively evaluated through 10 expert interviews, a public user survey with 43 participants verified the necessity and usability of our tool. Overall, our findings identified several needs to systematically support the SciCom KI. The SciCom Wiki, as a FAIR digital library complementing our neurosymbolic computational fact-checking framework, was found suitable to address the raised requirements. Further, we identified that the SciCom KI is severely underdeveloped regarding FAIR knowledge and related systems facilitating its collaborative creation and curation. Our system can provide a central knowledge node, yet a collaborative effort is required to scale against the imminent (mis-)information flood.","authors":["Tim Wittenborg","Constantin Sebastian Tremel","Niklas Stehr","Oliver Karras","Markus Stocker","S\\\"oren Auer"],"url":"https://arxiv.org/abs/2505.07912"}
{"created":"2025-05-14","title":"On-Device Crack Segmentation for Edge Structural Health Monitoring","abstract":"Crack segmentation can play a critical role in Structural Health Monitoring (SHM) by enabling accurate identification of crack size and location, which allows to monitor structural damages over time. However, deploying deep learning models for crack segmentation on resource-constrained microcontrollers presents significant challenges due to limited memory, computational power, and energy resources. To address these challenges, this study explores lightweight U-Net architectures tailored for TinyML applications, focusing on three optimization strategies: filter number reduction, network depth reduction, and the use of Depthwise Separable Convolutions (DWConv2D). Our results demonstrate that reducing convolution kernels and network depth significantly reduces RAM and Flash requirement, and inference times, albeit with some accuracy trade-offs. Specifically, by reducing the filer number to 25%, the network depth to four blocks, and utilizing depthwise convolutions, a good compromise between segmentation performance and resource consumption is achieved. This makes the network particularly suitable for low-power TinyML applications. This study not only advances TinyML-based crack segmentation but also provides the possibility for energy-autonomous edge SHM systems.","authors":["Yuxuan Zhang","Ye Xu","Luciano Sebastian Martinez-Rau","Quynh Nguyen Phuong Vu","Bengt Oelmann","Sebastian Bader"],"url":"https://arxiv.org/abs/2505.07915"}
{"created":"2025-05-14","title":"Efficient and Reproducible Biomedical Question Answering using Retrieval Augmented Generation","abstract":"Biomedical question-answering (QA) systems require effective retrieval and generation components to ensure accuracy, efficiency, and scalability. This study systematically examines a Retrieval-Augmented Generation (RAG) system for biomedical QA, evaluating retrieval strategies and response time trade-offs. We first assess state-of-the-art retrieval methods, including BM25, BioBERT, MedCPT, and a hybrid approach, alongside common data stores such as Elasticsearch, MongoDB, and FAISS, on a ~10% subset of PubMed (2.4M documents) to measure indexing efficiency, retrieval latency, and retriever performance in the end-to-end RAG system. Based on these insights, we deploy the final RAG system on the full 24M PubMed corpus, comparing different retrievers' impact on overall performance. Evaluations of the retrieval depth show that retrieving 50 documents with BM25 before reranking with MedCPT optimally balances accuracy (0.90), recall (0.90), and response time (1.91s). BM25 retrieval time remains stable (82ms), while MedCPT incurs the main computational cost. These results highlight previously not well-known trade-offs in retrieval depth, efficiency, and scalability for biomedical QA. With open-source code, the system is fully reproducible and extensible.","authors":["Linus Stuhlmann","Michael Alexander Saxer","Jonathan F\\\"urst"],"url":"https://arxiv.org/abs/2505.07917"}
{"created":"2025-05-14","title":"Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions","abstract":"Peer review is a critical component of scientific progress in the fields like AI, but the rapid increase in submission volume has strained the reviewing system, which inevitably leads to reviewer shortages and declines review quality. Besides the growing research popularity, another key factor in this overload is the repeated resubmission of substandard manuscripts, largely due to the lack of effective tools for authors to self-evaluate their work before submission. Large Language Models (LLMs) show great promise in assisting both authors and reviewers, and their performance is fundamentally limited by the quality of the peer review data. However, existing peer review datasets face three major limitations: (1) limited data diversity, (2) inconsistent and low-quality data due to the use of revised rather than initial submissions, and (3) insufficient support for tasks involving rebuttal and reviewer-author interactions. To address these challenges, we introduce the largest consistency-ensured peer review and rebuttal dataset named Re^2, which comprises 19,926 initial submissions, 70,668 review comments, and 53,818 rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the rebuttal and discussion stage is framed as a multi-turn conversation paradigm to support both traditional static review tasks and dynamic interactive LLM assistants, providing more practical guidance for authors to refine their manuscripts and helping alleviate the growing review burden. Our data and code are available in https://anonymous.4open.science/r/ReviewBench_anon/.","authors":["Daoze Zhang","Zhijian Bao","Sihang Du","Zhiyi Zhao","Kuangling Zhang","Dezheng Bao","Yang Yang"],"url":"https://arxiv.org/abs/2505.07920"}
{"created":"2025-05-14","title":"Self-cross Feature based Spiking Neural Networks for Efficient Few-shot Learning","abstract":"Deep neural networks (DNNs) excel in computer vision tasks, especially, few-shot learning (FSL), which is increasingly important for generalizing from limited examples. However, DNNs are computationally expensive with scalability issues in real world. Spiking Neural Networks (SNNs), with their event-driven nature and low energy consumption, are particularly efficient in processing sparse and dynamic data, though they still encounter difficulties in capturing complex spatiotemporal features and performing accurate cross-class comparisons. To further enhance the performance and efficiency of SNNs in few-shot learning, we propose a few-shot learning framework based on SNNs, which combines a self-feature extractor module and a cross-feature contrastive module to refine feature representation and reduce power consumption. We apply the combination of temporal efficient training loss and InfoNCE loss to optimize the temporal dynamics of spike trains and enhance the discriminative power. Experimental results show that the proposed FSL-SNN significantly improves the classification performance on the neuromorphic dataset N-Omniglot, and also achieves competitive performance to ANNs on static datasets such as CUB and miniImageNet with low power consumption.","authors":["Qi Xu","Junyang Zhu","Dongdong Zhou","Hao Chen","Yang Liu","Jiangrong Shen","Qiang Zhang"],"url":"https://arxiv.org/abs/2505.07921"}
{"created":"2025-05-14","title":"Symbolic Regression with Multimodal Large Language Models and Kolmogorov Arnold Networks","abstract":"We present a novel approach to symbolic regression using vision-capable large language models (LLMs) and the ideas behind Google DeepMind's Funsearch. The LLM is given a plot of a univariate function and tasked with proposing an ansatz for that function. The free parameters of the ansatz are fitted using standard numerical optimisers, and a collection of such ans\\\"atze make up the population of a genetic algorithm. Unlike other symbolic regression techniques, our method does not require the specification of a set of functions to be used in regression, but with appropriate prompt engineering, we can arbitrarily condition the generative step. By using Kolmogorov Arnold Networks (KANs), we demonstrate that ``univariate is all you need'' for symbolic regression, and extend this method to multivariate functions by learning the univariate function on each edge of a trained KAN. The combined expression is then simplified by further processing with a language model.","authors":["Thomas R. Harvey","Fabian Ruehle","Cristofero S. Fraser-Taliente","James Halverson"],"url":"https://arxiv.org/abs/2505.07956"}
{"created":"2025-05-14","title":"Making Small Language Models Efficient Reasoners: Intervention, Supervision, Reinforcement","abstract":"Recent research enhances language model reasoning by scaling test-time compute via longer chain-of-thought traces. This often improves accuracy but also introduces redundancy and high computational cost, especially for small language models distilled with supervised fine-tuning (SFT). In this work, we propose new algorithms to improve token-efficient reasoning with small-scale models by effectively trading off accuracy and computation. We first show that the post-SFT model fails to determine the optimal stopping point of the reasoning process, resulting in verbose and repetitive outputs. Verbosity also significantly varies across wrong vs correct responses. To address these issues, we propose two solutions: (1) Temperature scaling (TS) to control the stopping point for the thinking phase and thereby trace length, and (2) TLDR: a length-regularized reinforcement learning method based on GRPO that facilitates multi-level trace length control (e.g. short, medium, long reasoning). Experiments on four reasoning benchmarks, MATH500, AMC, AIME24 and OlympiadBench, demonstrate that TS is highly effective compared to s1's budget forcing approach and TLDR significantly improves token efficiency by about 50% with minimal to no accuracy loss over the SFT baseline. Moreover, TLDR also facilitates flexible control over the response length, offering a practical and effective solution for token-efficient reasoning in small models. Ultimately, our work reveals the importance of stopping time control, highlights shortcomings of pure SFT, and provides effective algorithmic recipes.","authors":["Xuechen Zhang","Zijian Huang","Chenchun Ni","Ziyang Xiong","Jiasi Chen","Samet Oymak"],"url":"https://arxiv.org/abs/2505.07961"}
{"created":"2025-05-14","title":"Convergence Properties of PINNs for the Navier-Stokes-Cahn-Hilliard System","abstract":"Approximating solutions to differential equations using neural networks has become increasingly popular and shows significant promise. In this paper, we propose a simplified framework for analyzing the potential of neural networks to simulate differential equations based on the properties of the equations themselves. We apply this framework to the Cahn-Hilliard and Navier-Stokes-Cahn-Hilliard systems, presenting both theoretical analysis and practical implementations. We then conduct numerical experiments on toy problems to validate the framework's efficacy in accurately capturing the desired properties of these systems and numerically estimate relevant convergence properties.","authors":["Kevin Buck","Roger Temam"],"url":"https://arxiv.org/abs/2505.07964"}
{"created":"2025-05-14","title":"Games for graded modal substitution calculus","abstract":"Graded modal substitution calculus (GMSC) and its variants has been used for logical characterizations of various computing frameworks such as graph neural networks, ordinary neural networks and distributed computing. In this paper we introduce two different semantic games and formula size game for graded modal substitution calculus and its variants. Ultimately, we show that the formula size game characterizes the equivalence of classes of pointed Kripke models up to programs of GMSC of given size. Thus, the formula size game can be used to study the expressive power mentioned characterized classes of computing models. Moreover, we show that over words GMSC has the same expressive power as deterministic linearly tape-bounded Turing machines also known as deterministic linear bounded automata.","authors":["Veeti Ahvonen","Reijo Jaakkola","Antti Kuusisto"],"url":"https://arxiv.org/abs/2505.07966"}
{"created":"2025-05-14","title":"Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models","abstract":"Large Language Models (LLMs) have great potential in the field of health care, yet they face great challenges in adapting to rapidly evolving medical knowledge. This can lead to outdated or contradictory treatment suggestions. This study investigated how LLMs respond to evolving clinical guidelines, focusing on concept drift and internal inconsistencies. We developed the DriftMedQA benchmark to simulate guideline evolution and assessed the temporal reliability of various LLMs. Our evaluation of seven state-of-the-art models across 4,290 scenarios demonstrated difficulties in rejecting outdated recommendations and frequently endorsing conflicting guidance. Additionally, we explored two mitigation strategies: Retrieval-Augmented Generation and preference fine-tuning via Direct Preference Optimization. While each method improved model performance, their combination led to the most consistent and reliable results. These findings underscore the need to improve LLM robustness to temporal shifts to ensure more dependable applications in clinical practice.","authors":["Weiyi Wu","Xinwen Xu","Chongyang Gao","Xingjian Diao","Siting Li","Lucas A. Salas","Jiang Gui"],"url":"https://arxiv.org/abs/2505.07968"}
{"created":"2025-05-14","title":"Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration","abstract":"Semantic communications represent a new paradigm of next-generation networking that shifts bit-wise data delivery to conveying the semantic meanings for bandwidth efficiency. To effectively accommodate various potential downstream tasks at the receiver side, one should adaptively convey the most critical semantic information. This work presents a novel task-adaptive semantic communication framework based on diffusion models that is capable of dynamically adjusting the semantic message delivery according to various downstream tasks. Specifically, we initialize the transmission of a deep-compressed general semantic representation from the transmitter to enable diffusion-based coarse data reconstruction at the receiver. The receiver identifies the task-specific demands and generates textual prompts as feedback. Integrated with the attention mechanism, the transmitter updates the semantic transmission with more details to better align with the objectives of the intended receivers. Our test results demonstrate the efficacy of the proposed method in adaptively preserving critical task-relevant information for semantic communications while preserving high compression efficiency.","authors":["Fupei Guo","Achintha Wijesinghe","Songyang Zhang","Zhi Ding"],"url":"https://arxiv.org/abs/2505.07980"}
{"created":"2025-05-14","title":"Virtual Holonomic Constraints in Motion Planning: Revisiting Feasibility and Limitations","abstract":"This paper addresses the feasibility of virtual holonomic constraints (VHCs) in the context of motion planning for underactuated mechanical systems with a single degree of underactuation. While existing literature has established a widely accepted definition of VHC, we argue that this definition is overly restrictive and excludes a broad class of admissible trajectories from consideration. To illustrate this point, we analyze a periodic motion of the Planar Vertical Take-Off and Landing (PVTOL) aircraft. The corresponding phase trajectory and reference control input are analytic functions. We demonstrate the stabilizability of this solution by constructing a feedback controller that ensures asymptotic orbital stability. However, for this solution -- as well as for a broad class of similar ones -- there exists no VHC that satisfies the conventional definition. This observation calls for a reconsideration of how the notion of VHC is defined, with the potential to significantly expand the practical applicability of VHCs in motion planning.","authors":["Maksim Surov"],"url":"https://arxiv.org/abs/2505.07983"}
{"created":"2025-05-14","title":"MilChat: Introducing Chain of Thought Reasoning and GRPO to a Multimodal Small Language Model for Remote Sensing","abstract":"Remarkable capabilities in understanding and generating text-image content have been demonstrated by recent advancements in multimodal large language models (MLLMs). However, their effectiveness in specialized domains-particularly those requiring resource-efficient and domain-specific adaptations-has remained limited. In this work, a lightweight multimodal language model termed MilChat is introduced, specifically adapted to analyze remote sensing imagery in secluded areas, including challenging missile launch sites. A new dataset, MilData, was compiled by verifying hundreds of aerial images through expert review, and subtle military installations were highlighted via detailed captions. Supervised fine-tuning on a 2B-parameter open-source MLLM with chain-of-thought (CoT) reasoning annotations was performed, enabling more accurate and interpretable explanations. Additionally, Group Relative Policy Optimization (GRPO) was leveraged to enhance the model's ability to detect critical domain-specific cues-such as defensive layouts and key military structures-while minimizing false positives on civilian scenes. Through empirical evaluations, it has been shown that MilChat significantly outperforms both larger, general-purpose multimodal models and existing remote sensing-adapted approaches on open-ended captioning and classification metrics. Over 80% recall and 98% precision were achieved on the newly proposed MilData benchmark, underscoring the potency of targeted fine-tuning and reinforcement learning in specialized real-world applications.","authors":["Aybora Koksal","A. Aydin Alatan"],"url":"https://arxiv.org/abs/2505.07984"}
{"created":"2025-05-14","title":"Fair Play for Individuals, Foul Play for Groups? Auditing Anonymization's Impact on ML Fairness","abstract":"Machine learning (ML) algorithms are heavily based on the availability of training data, which, depending on the domain, often includes sensitive information about data providers. This raises critical privacy concerns. Anonymization techniques have emerged as a practical solution to address these issues by generalizing features or suppressing data to make it more difficult to accurately identify individuals. Although recent studies have shown that privacy-enhancing technologies can influence ML predictions across different subgroups, thus affecting fair decision-making, the specific effects of anonymization techniques, such as $k$-anonymity, $\\ell$-diversity, and $t$-closeness, on ML fairness remain largely unexplored. In this work, we systematically audit the impact of anonymization techniques on ML fairness, evaluating both individual and group fairness. Our quantitative study reveals that anonymization can degrade group fairness metrics by up to four orders of magnitude. Conversely, similarity-based individual fairness metrics tend to improve under stronger anonymization, largely as a result of increased input homogeneity. By analyzing varying levels of anonymization across diverse privacy settings and data distributions, this study provides critical insights into the trade-offs between privacy, fairness, and utility, offering actionable guidelines for responsible AI development. Our code is publicly available at: https://github.com/hharcolezi/anonymity-impact-fairness.","authors":["H\\'eber H. Arcolezi","Mina Alishahi","Adda-Akram Bendoukha","Nesrine Kaaniche"],"url":"https://arxiv.org/abs/2505.07985"}
{"created":"2025-05-14","title":"Spec2Assertion: Automatic Pre-RTL Assertion Generation using Large Language Models with Progressive Regularization","abstract":"SystemVerilog Assertions (SVAs) play a critical role in detecting and debugging functional bugs in digital chip design. However, generating SVAs has traditionally been a manual, labor-intensive, and error-prone process. Recent advances in automatic assertion generation, particularly those using machine learning and large language models (LLMs), have shown promising potential, though most approaches remain in the early stages of development. In this work, we introduce Spec2Assertion, a new technique for automatically generating assertions from design specifications prior to RTL implementation. It leverages LLMs with progressive regularization and incorporates Chain-of-Thought (CoT) prompting to guide assertion synthesis. Additionally, we propose a new evaluation methodology that assesses assertion quality across a broad range of scenarios. Experiments on multiple benchmark designs show that Spec2Assertion generates 70% more syntax-correct assertions with 2X quality improvement on average compared to a recent state-of-the-art approach.","authors":["Fenghua Wu","Evan Pan","Rahul Kande","Michael Quinn","Aakash Tyagi","David Kebo","Jeyavijayan Rajendran","Jiang Hu"],"url":"https://arxiv.org/abs/2505.07995"}
{"created":"2025-05-14","title":"A Scalable System to Prove Machine Learning Fairness in Zero-Knowledge","abstract":"With the rise of machine learning techniques, ensuring the fairness of decisions made by machine learning algorithms has become of great importance in critical applications. However, measuring fairness often requires full access to the model parameters, which compromises the confidentiality of the models. In this paper, we propose a solution using zero-knowledge proofs, which allows the model owner to convince the public that a machine learning model is fair while preserving the secrecy of the model. To circumvent the efficiency barrier of naively proving machine learning inferences in zero-knowledge, our key innovation is a new approach to measure fairness only with model parameters and some aggregated information of the input, but not on any specific dataset. To achieve this goal, we derive new bounds for the fairness of logistic regression and deep neural network models that are tighter and better reflecting the fairness compared to prior work. Moreover, we develop efficient zero-knowledge proof protocols for common computations involved in measuring fairness, including the spectral norm of matrices, maximum, absolute value, and fixed-point arithmetic.","authors":["Tianyu Zhang","Shen Dong","O. Deniz Kose","Yanning Shen","Yupeng Zhang"],"url":"https://arxiv.org/abs/2505.07997"}
{"created":"2025-05-14","title":"Vision Foundation Model Embedding-Based Semantic Anomaly Detection","abstract":"Semantic anomalies are contextually invalid or unusual combinations of familiar visual elements that can cause undefined behavior and failures in system-level reasoning for autonomous systems. This work explores semantic anomaly detection by leveraging the semantic priors of state-of-the-art vision foundation models, operating directly on the image. We propose a framework that compares local vision embeddings from runtime images to a database of nominal scenarios in which the autonomous system is deemed safe and performant. In this work, we consider two variants of the proposed framework: one using raw grid-based embeddings, and another leveraging instance segmentation for object-centric representations. To further improve robustness, we introduce a simple filtering mechanism to suppress false positives. Our evaluations on CARLA-simulated anomalies show that the instance-based method with filtering achieves performance comparable to GPT-4o, while providing precise anomaly localization. These results highlight the potential utility of vision embeddings from foundation models for real-time anomaly detection in autonomous systems.","authors":["Max Peter Ronecker","Matthew Foutter","Amine Elhafsi","Daniele Gammelli","Ihor Barakaiev","Marco Pavone","Daniel Watzenig"],"url":"https://arxiv.org/abs/2505.07998"}
{"created":"2025-05-14","title":"Limitations to Computing Quadratic Functions on Reed-Solomon Encoded Data","abstract":"We study the problem of low-bandwidth non-linear computation on Reed-Solomon encoded data. Given an $[n,k]$ Reed-Solomon encoding of a message vector $\\vec{f} \\in \\mathbb{F}_q^k$, and a polynomial $g \\in \\mathbb{F}_q[X_1, X_2, \\ldots, X_k]$, a user wishing to evaluate $g(\\vec{f})$ is given local query access to each codeword symbol. The query response is allowed to be the output of an arbitrary function evaluated locally on the codeword symbol, and the user's aim is to minimize the total information downloaded in order to compute $g(\\vec{f})$.","authors":["Keller Blackwell","Mary Wootters"],"url":"https://arxiv.org/abs/2505.08000"}
{"created":"2025-05-14","title":"Large Language Models and Arabic Content: A Review","abstract":"Over the past three years, the rapid advancement of Large Language Models (LLMs) has had a profound impact on multiple areas of Artificial Intelligence (AI), particularly in Natural Language Processing (NLP) across diverse languages, including Arabic. Although Arabic is considered one of the most widely spoken languages across 27 countries in the Arabic world and used as a second language in some other non-Arabic countries as well, there is still a scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face various challenges due to the complexities of the Arabic language, including its rich morphology, intricate structure, and diverse writing standards, among other factors. Researchers have been actively addressing these challenges, demonstrating that pre-trained Large Language Models (LLMs) trained on multilingual corpora achieve significant success in various Arabic NLP tasks. This study provides an overview of using large language models (LLMs) for the Arabic language, highlighting early pre-trained Arabic Language models across various NLP applications and their ability to handle diverse Arabic content tasks and dialects. It also provides an overview of how techniques like finetuning and prompt engineering can enhance the performance of these models. Additionally, the study summarizes common Arabic benchmarks and datasets while presenting our observations on the persistent upward trend in the adoption of LLMs.","authors":["Haneh Rhel","Dmitri Roussinov"],"url":"https://arxiv.org/abs/2505.08004"}
{"created":"2025-05-14","title":"Assessing the Bug-Proneness of Refactored Code: A Longitudinal Multi-Project Study","abstract":"Refactoring is a common practice in software development, aimed at improving the internal code structure in order to make it easier to understand and modify. Consequently, it is often assumed that refactoring makes the code less prone to bugs. However, in practice, refactoring is a complex task and applied in different ways (e.g., various refactoring types, single vs. composite refactorings) and with a variety of purposes (e.g., root-canal vs. floss refactoring). Therefore, certain refactorings can inadvertently make the code more prone to bugs. Unfortunately, there is limited research in the literature on the long-term relationship between the different characteristics of refactorings and bugs. This paper presents a longitudinal study of 12 open source software projects, where 27,450 refactorings, 6,051 reported bugs, and 49,250 bugs detected with static analysis tools were analyzed. While our study confirms the common intuition that refactored code is less bug-prone than non-refactored code, we also extend or contradict existing body of knowledge in other ways. First, a code element that undergoes multiple refactorings is not less bug-prone than an element that undergoes a single refactoring. A single refactoring is the one not performed in conjunction with other refactorings in the same commit. Second, single refactorings often induce the occurrence of bugs across all analyzed projects. Third, code elements affected by refactorings made in conjunction with other non-refactoring changes in the same commit (i.e., floss refactorings) are often bug-prone. Finally, many of such bugs induced by refactoring cannot be revealed with state-of-the-art techniques for detecting behavior-preserving refactorings.","authors":["Isabella Ferreira","Lawrence Arkoh","Anderson Uch\\^oa","Ana Carla Bibiano","Alessandro Garcia","Wesley K. G. Assun\\c{c}\\~ao"],"url":"https://arxiv.org/abs/2505.08005"}
{"created":"2025-05-14","title":"Evaluating Explanation Quality in X-IDS Using Feature Alignment Metrics","abstract":"Explainable artificial intelligence (XAI) methods have become increasingly important in the context of explainable intrusion detection systems (X-IDSs) for improving the interpretability and trustworthiness of X-IDSs. However, existing evaluation approaches for XAI focus on model-specific properties such as fidelity and simplicity, and neglect whether the explanation content is meaningful or useful within the application domain. In this paper, we introduce new evaluation metrics measuring the quality of explanations from X-IDSs. The metrics aim at quantifying how well explanations are aligned with predefined feature sets that can be identified from domain-specific knowledge bases. Such alignment with these knowledge bases enables explanations to reflect domain knowledge and enables meaningful and actionable insights for security analysts. In our evaluation, we demonstrate the use of the proposed metrics to evaluate the quality of explanations from X-IDSs. The experimental results show that the proposed metrics can offer meaningful differences in explanation quality across X-IDSs and attack types, and assess how well X-IDS explanations reflect known domain knowledge. The findings of the proposed metrics provide actionable insights for security analysts to improve the interpretability of X-IDS in practical settings.","authors":["Mohammed Alquliti","Erisa Karafili","BooJoong Kang"],"url":"https://arxiv.org/abs/2505.08006"}
{"created":"2025-05-14","title":"RDD: Robust Feature Detector and Descriptor using Deformable Transformer","abstract":"As a core step in structure-from-motion and SLAM, robust feature detection and description under challenging scenarios such as significant viewpoint changes remain unresolved despite their ubiquity. While recent works have identified the importance of local features in modeling geometric transformations, these methods fail to learn the visual cues present in long-range relationships. We present Robust Deformable Detector (RDD), a novel and robust keypoint detector/descriptor leveraging the deformable transformer, which captures global context and geometric invariance through deformable self-attention mechanisms. Specifically, we observed that deformable attention focuses on key locations, effectively reducing the search space complexity and modeling the geometric invariance. Furthermore, we collected an Air-to-Ground dataset for training in addition to the standard MegaDepth dataset. Our proposed method outperforms all state-of-the-art keypoint detection/description methods in sparse matching tasks and is also capable of semi-dense matching. To ensure comprehensive evaluation, we introduce two challenging benchmarks: one emphasizing large viewpoint and scale variations, and the other being an Air-to-Ground benchmark -- an evaluation setting that has recently gaining popularity for 3D reconstruction across different altitudes.","authors":["Gonglin Chen","Tianwen Fu","Haiwei Chen","Wenbin Teng","Hanyuan Xiao","Yajie Zhao"],"url":"https://arxiv.org/abs/2505.08013"}
{"created":"2025-05-14","title":"Relating Complexity, Explicitness, Effectiveness of Refactorings and Non-Functional Requirements: A Replication Study","abstract":"Refactoring is a practice widely adopted during software maintenance and evolution. Due to its importance, there is extensive work on the effectiveness of refactoring in achieving code quality. However, developer's intentions are usually overlooked. A more recent area of study involves the concept of self-affirmed refactoring (SAR), where developers explicitly state their intent to refactor. While studies on SAR have made valuable contributions, they provide little insights into refactoring complexity and effectiveness, as well as the refactorings' relations to specific non-functional requirements. A study by Soares et al. addressed such aspects, but it relied on a quite small sample of studied subject systems and refactoring instances. Following the empirical method of replication, we expanded the scope of Soares et al.'s study by doubling the number of projects analyzed and a significantly larger set of validated refactorings (8,408). Our findings only partially align with the original study. We observed that when developers explicitly state their refactoring intent, the resulting changes typically involve a combination of different refactoring types, making them more complex. Additionally, we confirmed that such complex refactorings positively impact code's internal quality attributes. While refactorings aimed at non-functional requirements tend to improve code quality, our findings only partially align with the original study and contradict it in several ways. Notably, SARs often result in fewer negative impacts on internal quality attributes despite their frequent complexity. These insights suggest the importance of simplifying refactorings where possible and explicitly stating their goals, as clear intent helps shape more effective and targeted refactoring strategies.","authors":["Vin\\'icius Soares","Lawrence Arkoh","Paulo Roberto Farah","Anderson Uch\\^oa","Alessandro Garcia","Wesley K. G. Assun\\c{c}\\~ao"],"url":"https://arxiv.org/abs/2505.08016"}
{"created":"2025-05-14","title":"The Correspondence Between Bounded Graph Neural Networks and Fragments of First-Order Logic","abstract":"Graph Neural Networks (GNNs) address two key challenges in applying deep learning to graph-structured data: they handle varying size input graphs and ensure invariance under graph isomorphism. While GNNs have demonstrated broad applicability, understanding their expressive power remains an important question. In this paper, we show that bounded GNN architectures correspond to specific fragments of first-order logic (FO), including modal logic (ML), graded modal logic (GML), modal logic with the universal modality (ML(A)), the two-variable fragment (FO2) and its extension with counting quantifiers (C2). To establish these results, we apply methods and tools from finite model theory of first-order and modal logics to the domain of graph representation learning. This provides a unifying framework for understanding the logical expressiveness of GNNs within FO.","authors":["Bernardo Cuenca Grau","Przemys{\\l}aw A. Wa{\\l}\\k{e}ga"],"url":"https://arxiv.org/abs/2505.08021"}
{"created":"2025-05-14","title":"Dynamical Low-Rank Compression of Neural Networks with Robustness under Adversarial Attacks","abstract":"Deployment of neural networks on resource-constrained devices demands models that are both compact and robust to adversarial inputs. However, compression and adversarial robustness often conflict. In this work, we introduce a dynamical low-rank training scheme enhanced with a novel spectral regularizer that controls the condition number of the low-rank core in each layer. This approach mitigates the sensitivity of compressed models to adversarial perturbations without sacrificing clean accuracy. The method is model- and data-agnostic, computationally efficient, and supports rank adaptivity to automatically compress the network at hand. Extensive experiments across standard architectures, datasets, and adversarial attacks show the regularized networks can achieve over 94% compression while recovering or improving adversarial accuracy relative to uncompressed baselines.","authors":["Steffen Schotth\\\"ofer","H. Lexie Yang","Stefan Schnake"],"url":"https://arxiv.org/abs/2505.08022"}
{"created":"2025-05-14","title":"PRISM: Complete Online Decentralized Multi-Agent Pathfinding with Rapid Information Sharing using Motion Constraints","abstract":"We introduce PRISM (Pathfinding with Rapid Information Sharing using Motion Constraints), a decentralized algorithm designed to address the multi-task multi-agent pathfinding (MT-MAPF) problem. PRISM enables large teams of agents to concurrently plan safe and efficient paths for multiple tasks while avoiding collisions. It employs a rapid communication strategy that uses information packets to exchange motion constraint information, enhancing cooperative pathfinding and situational awareness, even in scenarios without direct communication. We prove that PRISM resolves and avoids all deadlock scenarios when possible, a critical challenge in decentralized pathfinding. Empirically, we evaluate PRISM across five environments and 25 random scenarios, benchmarking it against the centralized Conflict-Based Search (CBS) and the decentralized Token Passing with Task Swaps (TPTS) algorithms. PRISM demonstrates scalability and solution quality, supporting 3.4 times more agents than CBS and handling up to 2.5 times more tasks in narrow passage environments than TPTS. Additionally, PRISM matches CBS in solution quality while achieving faster computation times, even under low-connectivity conditions. Its decentralized design reduces the computational burden on individual agents, making it scalable for large environments. These results confirm PRISM's robustness, scalability, and effectiveness in complex and dynamic pathfinding scenarios.","authors":["Hannah Lee","Zachary Serlin","James Motes","Brendan Long","Marco Morales","Nancy M. Amato"],"url":"https://arxiv.org/abs/2505.08025"}
{"created":"2025-05-14","title":"Safety and optimality in learning-based control at low computational cost","abstract":"Applying machine learning methods to physical systems that are supposed to act in the real world requires providing safety guarantees. However, methods that include such guarantees often come at a high computational cost, making them inapplicable to large datasets and embedded devices with low computational power. In this paper, we propose CoLSafe, a computationally lightweight safe learning algorithm whose computational complexity grows sublinearly with the number of data points. We derive both safety and optimality guarantees and showcase the effectiveness of our algorithm on a seven-degrees-of-freedom robot arm.","authors":["Dominik Baumann","Krzysztof Kowalczyk","Cristian R. Rojas","Koen Tiels","Pawel Wachel"],"url":"https://arxiv.org/abs/2505.08026"}
{"created":"2025-05-14","title":"Generalized LDPC codes with low-complexity decoding and fast convergence","abstract":"We consider generalized low-density parity-check (GLDPC) codes with component codes that are duals of Cordaro-Wagner codes. Two efficient decoding algorithms are proposed: one based on Hartmann-Rudolph processing, analogous to Sum-Product decoding, and another based on evaluating two hypotheses per bit, referred to as the Min-Sum decoder. Both algorithms are derived using latent variables and an appropriate message-passing schedule. A quantized, protograph-based density evolution procedure is used to optimize GLDPC codes for Min-Sum decoding. Compared to 5G LDPC codes, the proposed GLDPC codes offer similar performance at 50 iterations and significantly better convergence and performance at 10 iterations.","authors":["Dawit Simegn","Dmitry Artemasov","Kirill Andreev","Pavel Rybin","Alexey Frolov"],"url":"https://arxiv.org/abs/2505.08030"}
{"created":"2025-05-14","title":"Measuring and predicting variation in the difficulty of questions about data visualizations","abstract":"Understanding what is communicated by data visualizations is a critical component of scientific literacy in the modern era. However, it remains unclear why some tasks involving data visualizations are more difficult than others. Here we administered a composite test composed of five widely used tests of data visualization literacy to a large sample of U.S. adults (N=503 participants).We found that items in the composite test spanned the full range of possible difficulty levels, and that our estimates of item-level difficulty were highly reliable. However, the type of data visualization shown and the type of task involved only explained a modest amount of variation in performance across items, relative to the reliability of the estimates we obtained. These results highlight the need for finer-grained ways of characterizing these items that predict the reliable variation in difficulty measured in this study, and that generalize to other tests of data visualization understanding.","authors":["Arnav Verma","Judith E. Fan"],"url":"https://arxiv.org/abs/2505.08031"}
{"created":"2025-05-14","title":"Online Learning-based Adaptive Beam Switching for 6G Networks: Enhancing Efficiency and Resilience","abstract":"Adaptive beam switching in 6G networks is challenged by high frequencies, mobility, and blockage. We propose an Online Learning framework using Deep Reinforcement Learning (DRL) with an enhanced state representation (velocity and blockage history), a GRU architecture, and prioritized experience replay for real-time beam optimization. Validated via Nvidia Sionna under time-correlated blockage, our approach significantly enhances resilience in SNR, throughput, and accuracy compared to a conventional heuristic. Furthermore, the enhanced DRL agent outperforms a reactive Multi-Armed Bandit (MAB) baseline by leveraging temporal dependencies, achieving lower performance variability. This demonstrates the benefits of memory and prioritized learning for robust 6G beam management, while confirming MAB as a strong baseline.","authors":["Seyed Bagher Hashemi Natanzi","Zhicong Zhu","Bo Tang"],"url":"https://arxiv.org/abs/2505.08032"}
{"created":"2025-05-14","title":"Demo: A Practical Testbed for Decentralized Federated Learning on Physical Edge Devices","abstract":"Federated Learning (FL) enables collaborative model training without sharing raw data, preserving participant privacy. Decentralized FL (DFL) eliminates reliance on a central server, mitigating the single point of failure inherent in the traditional FL paradigm, while introducing deployment challenges on resource-constrained devices. To evaluate real-world applicability, this work designs and deploys a physical testbed using edge devices such as Raspberry Pi and Jetson Nano. The testbed is built upon a DFL training platform, NEBULA, and extends it with a power monitoring module to measure energy consumption during training. Experiments across multiple datasets show that model performance is influenced by the communication topology, with denser topologies leading to better outcomes in DFL settings.","authors":["Chao Feng","Nicolas Huber","Alberto Huertas Celdran","Gerome Bovet","Burkhard Stiller"],"url":"https://arxiv.org/abs/2505.08033"}
{"created":"2025-05-14","title":"Opportunities and Applications of GenAI in Smart Cities: A User-Centric Survey","abstract":"The proliferation of IoT in cities, combined with Digital Twins, creates a rich data foundation for Smart Cities aimed at improving urban life and operations. Generative AI (GenAI) significantly enhances this potential, moving beyond traditional AI analytics and predictions by processing multimodal content and generating novel outputs like text and simulations. Using specialized or foundational models, GenAI's natural language abilities such as Natural Language Understanding (NLU) and Natural Language Generation (NLG) can power tailored applications and unified interfaces, dramatically lowering barriers for users interacting with complex smart city systems. In this paper, we focus on GenAI applications based on conversational interfaces within the context of three critical user archetypes in a Smart City - Citizens, Operators and Planners. We identify and review GenAI models and techniques that have been proposed or deployed for various urban subsystems in the contexts of these user archetypes. We also consider how GenAI can be built on the existing data foundation of official city records, IoT data streams and Urban Digital Twins. We believe this work represents the first comprehensive summarization of GenAI techniques for Smart Cities from the lens of the critical users in a Smart City.","authors":["Ankit Shetgaonkar","Dipen Pradhan","Lakshit Arora","Sanjay Surendranath Girija","Shashank Kapoor","Aman Raj"],"url":"https://arxiv.org/abs/2505.08034"}
{"created":"2025-05-14","title":"TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation","abstract":"Multi-level Tibetan spelling correction addresses errors at both the character and syllable levels within a unified model. Existing methods focus mainly on single-level correction and lack effective integration of both levels. Moreover, there are no open-source datasets or augmentation methods tailored for this task in Tibetan. To tackle this, we propose a data augmentation approach using unlabeled text to generate multi-level corruptions, and introduce TiSpell, a semi-masked model capable of correcting both character- and syllable-level errors. Although syllable-level correction is more challenging due to its reliance on global context, our semi-masked strategy simplifies this process. We synthesize nine types of corruptions on clean sentences to create a robust training set. Experiments on both simulated and real-world data demonstrate that TiSpell, trained on our dataset, outperforms baseline models and matches the performance of state-of-the-art approaches, confirming its effectiveness.","authors":["Yutong Liu","Feng Xiao","Ziyue Zhang","Yongbin Yu","Cheng Huang","Fan Gao","Xiangxiang Wang","Ma-bao Ban","Manping Fan","Thupten Tsering","Cheng Huang","Gadeng Luosang","Renzeng Duojie","Nyima Tashi"],"url":"https://arxiv.org/abs/2505.08037"}
{"created":"2025-05-14","title":"Mobile Jamming Mitigation in 5G Networks: A MUSIC-Based Adaptive Beamforming Approach","abstract":"Mobile jammers pose a critical threat to 5G networks, particularly in military communications. We propose an intelligent anti-jamming framework that integrates Multiple Signal Classification (MUSIC) for high-resolution Direction-of-Arrival (DoA) estimation, Minimum Variance Distortionless Response (MVDR) beamforming for adaptive interference suppression, and machine learning (ML) to enhance DoA prediction for mobile jammers. Extensive simulations in a realistic highway scenario demonstrate that our hybrid approach achieves an average Signal-to-Noise Ratio (SNR) improvement of 9.58 dB (maximum 11.08 dB) and up to 99.8% DoA estimation accuracy. The framework's computational efficiency and adaptability to dynamic jammer mobility patterns outperform conventional anti-jamming techniques, making it a robust solution for securing 5G communications in contested environments.","authors":["Olivia Holguin","Rachel Donati","Seyed bagher Hashemi Natanzi","Bo Tang"],"url":"https://arxiv.org/abs/2505.08046"}
{"created":"2025-05-14","title":"Partisan Fact-Checkers' Warnings Can Effectively Correct Individuals' Misbeliefs About Political Misinformation","abstract":"Political misinformation, particularly harmful when it aligns with individuals' preexisting beliefs and political ideologies, has become widespread on social media platforms. In response, platforms like Facebook and X introduced warning messages leveraging fact-checking results from third-party fact-checkers to alert users against false content. However, concerns persist about the effectiveness of these fact-checks, especially when fact-checkers are perceived as politically biased. To address these concerns, this study presents findings from an online human-subject experiment (N=216) investigating how the political stances of fact-checkers influence their effectiveness in correcting misbeliefs about political misinformation. Our findings demonstrate that partisan fact-checkers can decrease the perceived accuracy of political misinformation and correct misbeliefs without triggering backfire effects. This correction is even more pronounced when the misinformation aligns with individuals' political ideologies. Notably, while previous research suggests that fact-checking warnings are less effective for conservatives than liberals, our results suggest that explicitly labeled partisan fact-checkers, positioned as political counterparts to conservatives, are particularly effective in reducing conservatives' misbeliefs toward pro-liberal misinformation.","authors":["Sian Lee","Haeseung Seo","Aiping Xiong","Dongwon Lee"],"url":"https://arxiv.org/abs/2505.08048"}
{"created":"2025-05-14","title":"Bias or Optimality? Disentangling Bayesian Inference and Learning Biases in Human Decision-Making","abstract":"Recent studies claim that human behavior in a two-armed Bernoulli bandit (TABB) task is described by positivity and confirmation biases, implying that humans do not integrate new information objectively. However, we find that even if the agent updates its belief via objective Bayesian inference, fitting the standard Q-learning model with asymmetric learning rates still recovers both biases. Bayesian inference cast as an effective Q-learning algorithm has symmetric, though decreasing, learning rates. We explain this by analyzing the stochastic dynamics of these learning systems using master equations. We find that both confirmation bias and unbiased but decreasing learning rates yield the same behavioral signatures. Finally, we propose experimental protocols to disentangle true cognitive biases from artifacts of decreasing learning rates.","authors":["Prakhar Godara"],"url":"https://arxiv.org/abs/2505.08049"}
{"created":"2025-05-14","title":"Browser Security Posture Analysis: A Client-Side Security Assessment Framework","abstract":"Modern web browsers have effectively become the new operating system for business applications, yet their security posture is often under-scrutinized. This paper presents a novel, comprehensive Browser Security Posture Analysis Framework[1], a browser-based client-side security assessment toolkit that runs entirely in JavaScript and WebAssembly within the browser. It performs a battery of over 120 in-browser security tests in situ, providing fine-grained diagnostics of security policies and features that network-level or os-level tools cannot observe. This yields insights into how well a browser enforces critical client-side security invariants. We detail the motivation for such a framework, describe its architecture and implementation, and dive into the technical design of numerous test modules (covering the same-origin policy, cross-origin resource sharing, content security policy, sandboxing, XSS protection, extension interference via WeakRefs, permissions audits, garbage collection behavior, cryptographic APIs, SSL certificate validation, advanced web platform security features like SharedArrayBuffer, Content filtering controls ,and internal network accessibility). We then present an experimental evaluation across different browsers and enterprise scenarios, highlighting gaps in legacy browsers and common misconfigurations. Finally, we discuss the security and privacy implications of our findings, compare with related work in browser security and enterprise endpoint solutions, and outline future enhancements such as real-time posture monitoring and SIEM integration.","authors":["Avihay Cohen"],"url":"https://arxiv.org/abs/2505.08050"}
{"created":"2025-05-14","title":"NAZM: Network Analysis of Zonal Metrics in Persian Poetic Tradition","abstract":"This study formalizes a computational model to simulate classical Persian poets' dynamics of influence through constructing a multi-dimensional similarity network. Using a rigorously curated dataset based on Ganjoor's corpus, we draw upon semantic, lexical, stylistic, thematic, and metrical features to demarcate each poet's corpus. Each is contained within weighted similarity matrices, which are then appended to generate an aggregate graph showing poet-to-poet influence. Further network investigation is carried out to identify key poets, style hubs, and bridging poets by calculating degree, closeness, betweenness, eigenvector, and Katz centrality measures. Further, for typological insight, we use the Louvain community detection algorithm to demarcate clusters of poets sharing both style and theme coherence, which correspond closely to acknowledged schools of literature like Sabk-e Hindi, Sabk-e Khorasani, and the Bazgasht-e Adabi phenomenon. Our findings provide a new data-driven view of Persian literature distinguished between canonical significance and interextual influence, thus highlighting relatively lesser-known figures who hold great structural significance. Combining computational linguistics with literary study, this paper produces an interpretable and scalable model for poetic tradition, enabling retrospective reflection as well as forward-looking research within digital humanities.","authors":["Kourosh Shahnazari","Seyed Moein Ayyoubzadeh"],"url":"https://arxiv.org/abs/2505.08052"}
{"created":"2025-05-14","title":"FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning","abstract":"Safety alignment approaches in large language models (LLMs) often lead to the over-refusal of benign queries, significantly diminishing their utility in sensitive scenarios. To address this challenge, we introduce FalseReject, a comprehensive resource containing 16k seemingly toxic queries accompanied by structured responses across 44 safety-related categories. We propose a graph-informed adversarial multi-agent interaction framework to generate diverse and complex prompts, while structuring responses with explicit reasoning to aid models in accurately distinguishing safe from unsafe contexts. FalseReject includes training datasets tailored for both standard instruction-tuned models and reasoning-oriented models, as well as a human-annotated benchmark test set. Our extensive benchmarking on 29 state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges. Empirical results demonstrate that supervised finetuning with FalseReject substantially reduces unnecessary refusals without compromising overall safety or general language capabilities.","authors":["Zhehao Zhang","Weijie Xu","Fanyou Wu","Chandan K. Reddy"],"url":"https://arxiv.org/abs/2505.08054"}
{"created":"2025-05-14","title":"HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method","abstract":"Compute optimization using token reduction of LLM prompts is an emerging task in the fields of NLP and next generation, agentic AI. In this white paper, we introduce a novel (patent pending) text representation scheme and a first-of-its-kind word-level semantic compression of paragraphs that can lead to over 90\\% token reduction, while retaining high semantic similarity to the source text. We explain how this novel compression technique can be lossless and how the detail granularity is controllable. We discuss benchmark results over open source data (i.e. Bram Stoker's Dracula available through Project Gutenberg) and show how our results hold at the paragraph level, across multiple genres and models.","authors":["Chris Forrester","Octavia Sulea"],"url":"https://arxiv.org/abs/2505.08058"}
{"created":"2025-05-14","title":"Land-Coverage Aware Path-Planning for Multi-UAV Swarms in Search and Rescue Scenarios","abstract":"Unmanned Aerial Vehicles (UAVs) have become vital in search-and-rescue (SAR) missions, with autonomous mission planning improving response times and coverage efficiency. Early approaches primarily used path planning techniques such as A*, potential-fields, or Dijkstra's algorithm, while recent approaches have incorporated meta-heuristic frameworks like genetic algorithms and particle swarm optimization to balance competing objectives such as network connectivity, energy efficiency, and strategic placement of charging stations. However, terrain-aware path planning remains under-explored, despite its critical role in optimizing UAV SAR deployments. To address this gap, we present a computer-vision based terrain-aware mission planner that autonomously extracts and analyzes terrain topology to enhance SAR pre-flight planning. Our framework uses a deep segmentation network fine-tuned on our own collection of landcover datasets to transform satellite imagery into a structured, grid-based representation of the operational area. This classification enables terrain-specific UAV-task allocation, improving deployment strategies in complex environments. We address the challenge of irregular terrain partitions, by introducing a two-stage partitioning scheme that first evaluates terrain monotonicity along coordinate axes before applying a cost-based recursive partitioning process, minimizing unnecessary splits and optimizing path efficiency. Empirical validation in a high-fidelity simulation environment demonstrates that our approach improves search and dispatch time over multiple meta-heuristic techniques and against a competing state-of-the-art method. These results highlight its potential for large-scale SAR operations, where rapid response and efficient UAV coordination are critical.","authors":["Pedro Antonio Alarcon Granadeno","Jane Cleland-Huang"],"url":"https://arxiv.org/abs/2505.08060"}
{"created":"2025-05-14","title":"Who's the Leader? Analyzing Novice Workflows in LLM-Assisted Debugging of Machine Learning Code","abstract":"While LLMs are often touted as tools for democratizing specialized knowledge to beginners, their actual effectiveness for improving task performance and learning is still an open question. It is known that novices engage with LLMs differently from experts, with prior studies reporting meta-cognitive pitfalls that affect novices' ability to verify outputs and prompt effectively. We focus on a task domain, machine learning (ML), which embodies both high complexity and low verifiability to understand the impact of LLM assistance on novices. Provided a buggy ML script and open access to ChatGPT, we conduct a formative study with eight novice ML engineers to understand their reliance on, interactions with, and perceptions of the LLM. We find that user actions can be roughly categorized into leading the LLM and led-by the LLM, and further investigate how they affect reliance outcomes like over- and under-reliance. These results have implications on novices' cognitive engagement in LLM-assisted tasks and potential negative effects on downstream learning. Lastly, we pose potential augmentations to the novice-LLM interaction paradigm to promote cognitive engagement.","authors":["Jessica Y. Bo","Majeed Kazemitabaar","Emma Zhuang","Ashton Anderson"],"url":"https://arxiv.org/abs/2505.08063"}
{"created":"2025-05-14","title":"Justified Evidence Collection for Argument-based AI Fairness Assurance","abstract":"It is well recognised that ensuring fair AI systems is a complex sociotechnical challenge, which requires careful deliberation and continuous oversight across all stages of a system's lifecycle, from defining requirements to model deployment and deprovisioning. Dynamic argument-based assurance cases, which present structured arguments supported by evidence, have emerged as a systematic approach to evaluating and mitigating safety risks and hazards in AI-enabled system development and have also been extended to deal with broader normative goals such as fairness and explainability. This paper introduces a systems-engineering-driven framework, supported by software tooling, to operationalise a dynamic approach to argument-based assurance in two stages. In the first stage, during the requirements planning phase, a multi-disciplinary and multi-stakeholder team define goals and claims to be established (and evidenced) by conducting a comprehensive fairness governance process. In the second stage, a continuous monitoring interface gathers evidence from existing artefacts (e.g. metrics from automated tests), such as model, data, and use case documentation, to support these arguments dynamically. The framework's effectiveness is demonstrated through an illustrative case study in finance, with a focus on supporting fairness-related arguments.","authors":["Alpay Sabuncuoglu","Christopher Burr","Carsten Maple"],"url":"https://arxiv.org/abs/2505.08064"}
{"created":"2025-05-14","title":"Intelligent Polarforming Antenna Enhanced Sensing and Communication: Modeling and Optimization","abstract":"In this paper, we propose a novel intelligent polarforming antenna (IPA) to achieve cost-effective wireless sensing and communication. Specifically, the IPA can enable polarforming by adaptively controlling the antenna's polarization electrically as well as its position/rotation mechanically, so as to effectively exploit polarization and spatial diversity to reconfigure wireless channels for improving sensing and communication performance. We study an IPA-enhanced integrated sensing and communication (ISAC) system that utilizes user location sensing to facilitate communication between an IPA-equipped base station (BS) and IPA-equipped users. First, we model the IPA channel in terms of transceiver antenna polarforming vectors and antenna positions/rotations. We then propose a two-timescale ISAC protocol, where in the slow timescale, user localization is first performed, followed by the optimization of the BS antennas' positions and rotations based on the sensed user locations; subsequently, in the fast timescale, transceiver polarforming is adapted to cater to the instantaneous channel state information (CSI), with the optimized BS antennas' positions and rotations. We propose a new polarforming-based user localization method that uses a structured time-domain pattern of pilot-polarforming vectors to extract the common stable components in the IPA channel across different polarizations based on the parallel factor (PARAFAC) tensor model. Moreover, we maximize the achievable average sum-rate of users by jointly optimizing the fast-timescale transceiver polarforming, including phase shifts and amplitude variations, along with the slow-timescale antenna rotations and positions at the BS. Simulation results validate the effectiveness of polarforming-based localization algorithm and demonstrate the performance advantages of polarforming, antenna placement, and their joint design.","authors":["Xiaodan Shao","Rui Zhang","Haibo Zhou","Qijun Jiang","Conghao Zhou","Weihua Zhuang","Xuemin Shen"],"url":"https://arxiv.org/abs/2505.08070"}
{"created":"2025-05-14","title":"NMP-PaK: Near-Memory Processing Acceleration of Scalable De Novo Genome Assembly","abstract":"De novo assembly enables investigations of unknown genomes, paving the way for personalized medicine and disease management. However, it faces immense computational challenges arising from the excessive data volumes and algorithmic complexity.","authors":["Heewoo Kim","Sanjay Sri Vallabh Singapuram","Haojie Ye","Joseph Izraelevitz","Trevor Mudge","Ronald Dreslinski","Nishil Talati"],"url":"https://arxiv.org/abs/2505.08071"}
{"created":"2025-05-14","title":"Perspectives on Capturing Emotional Expressiveness in Sign Language","abstract":"Significant advances have been made in our ability to understand and generate emotionally expressive content such as text and speech, yet comparable progress in sign language technologies remain limited. While computational approaches to sign language translation have focused on capturing lexical content, the emotional dimensions of sign language communication remain largely unexplored. Through semi-structured interviews with eight sign language users across Singapore, Sri Lanka and the United States, including both Deaf and Hard of hearing (DHH) and hearing signers, we investigate how emotions are expressed and perceived in sign languages. Our findings highlight the role of both manual and non-manual elements in emotional expression, revealing universal patterns as well as individual and cultural variations in how signers communicate emotions. We identify key challenges in capturing emotional nuance for sign language translation, and propose design considerations for developing more emotionally-aware sign language technologies. This work contributes to both theoretical understanding of emotional expression in sign language and practical development of interfaces to better serve diverse signing communities.","authors":["Phoebe Chua","Cathy Mengying Fang","Yasith Samaradivakara","Pattie Maes","Suranga Nanayakkara"],"url":"https://arxiv.org/abs/2505.08072"}
{"created":"2025-05-14","title":"Explainable Reinforcement Learning Agents Using World Models","abstract":"Explainable AI (XAI) systems have been proposed to help people understand how AI systems produce outputs and behaviors. Explainable Reinforcement Learning (XRL) has an added complexity due to the temporal nature of sequential decision-making. Further, non-AI experts do not necessarily have the ability to alter an agent or its policy. We introduce a technique for using World Models to generate explanations for Model-Based Deep RL agents. World Models predict how the world will change when actions are performed, allowing for the generation of counterfactual trajectories. However, identifying what a user wanted the agent to do is not enough to understand why the agent did something else. We augment Model-Based RL agents with a Reverse World Model, which predicts what the state of the world should have been for the agent to prefer a given counterfactual action. We show that explanations that show users what the world should have been like significantly increase their understanding of the agent policy. We hypothesize that our explanations can help users learn how to control the agents execution through by manipulating the environment.","authors":["Madhuri Singh","Amal Alabdulkarim","Gennie Mansi","Mark O. Riedl"],"url":"https://arxiv.org/abs/2505.08073"}
{"created":"2025-05-14","title":"What Matters for Batch Online Reinforcement Learning in Robotics?","abstract":"The ability to learn from large batches of autonomously collected data for policy improvement -- a paradigm we refer to as batch online reinforcement learning -- holds the promise of enabling truly scalable robot learning by significantly reducing the need for human effort of data collection while getting benefits from self-improvement. Yet, despite the promise of this paradigm, it remains challenging to achieve due to algorithms not being able to learn effectively from the autonomous data. For example, prior works have applied imitation learning and filtered imitation learning methods to the batch online RL problem, but these algorithms often fail to efficiently improve from the autonomously collected data or converge quickly to a suboptimal point. This raises the question of what matters for effective batch online RL in robotics. Motivated by this question, we perform a systematic empirical study of three axes -- (i) algorithm class, (ii) policy extraction methods, and (iii) policy expressivity -- and analyze how these axes affect performance and scaling with the amount of autonomous data. Through our analysis, we make several observations. First, we observe that the use of Q-functions to guide batch online RL significantly improves performance over imitation-based methods. Building on this, we show that an implicit method of policy extraction -- via choosing the best action in the distribution of the policy -- is necessary over traditional policy extraction methods from offline RL. Next, we show that an expressive policy class is preferred over less expressive policy classes. Based on this analysis, we propose a general recipe for effective batch online RL. We then show a simple addition to the recipe of using temporally-correlated noise to obtain more diversity results in further performance gains. Our recipe obtains significantly better performance and scaling compared to prior methods.","authors":["Perry Dong","Suvir Mirchandani","Dorsa Sadigh","Chelsea Finn"],"url":"https://arxiv.org/abs/2505.08078"}
{"created":"2025-05-14","title":"Beyond Input Activations: Identifying Influential Latents by Gradient Sparse Autoencoders","abstract":"Sparse Autoencoders (SAEs) have recently emerged as powerful tools for interpreting and steering the internal representations of large language models (LLMs). However, conventional approaches to analyzing SAEs typically rely solely on input-side activations, without considering the causal influence between each latent feature and the model's output. This work is built on two key hypotheses: (1) activated latents do not contribute equally to the construction of the model's output, and (2) only latents with high causal influence are effective for model steering. To validate these hypotheses, we propose Gradient Sparse Autoencoder (GradSAE), a simple yet effective method that identifies the most influential latents by incorporating output-side gradient information.","authors":["Dong Shu","Xuansheng Wu","Haiyan Zhao","Mengnan Du","Ninghao Liu"],"url":"https://arxiv.org/abs/2505.08080"}
{"created":"2025-05-14","title":"Fr\\'{e}chet Power-Scenario Distance: A Metric for Evaluating Generative AI Models across Multiple Time-Scales in Smart Grids","abstract":"Generative artificial intelligence (AI) models in smart grids have advanced significantly in recent years due to their ability to generate large amounts of synthetic data, which would otherwise be difficult to obtain in the real world due to confidentiality constraints. A key challenge in utilizing such synthetic data is how to assess the data quality produced from such generative models. Traditional Euclidean distance-based metrics only reflect pair-wise relations between two individual samples, and could fail in evaluating quality differences between groups of synthetic datasets. In this work, we propose a novel metric based on the Fr\\'{e}chet Distance (FD) estimated between two datasets in a learned feature space. The proposed method evaluates the quality of generation from a distributional perspective. Empirical results demonstrate the superiority of the proposed metric across timescales and models, enhancing the reliability of data-driven decision-making in smart grid operations.","authors":["Yuting Cai","Shaohuai Liu","Chao Tian","Le Xie"],"url":"https://arxiv.org/abs/2505.08082"}
{"created":"2025-05-14","title":"LLMs to Support K-12 Teachers in Culturally Relevant Pedagogy: An AI Literacy Example","abstract":"Culturally Relevant Pedagogy (CRP) is vital in K-12 education, yet teachers struggle to implement CRP into practice due to time, training, and resource gaps. This study explores how Large Language Models (LLMs) can address these barriers by introducing CulturAIEd, an LLM tool that assists teachers in adapting AI literacy curricula to students' cultural contexts. Through an exploratory pilot with four K-12 teachers, we examined CulturAIEd's impact on CRP integration. Results showed CulturAIEd enhanced teachers' confidence in identifying opportunities for cultural responsiveness in learning activities and making culturally responsive modifications to existing activities. They valued CulturAIEd's streamlined integration of student demographic information, immediate actionable feedback, which could result in high implementation efficiency. This exploration of teacher-AI collaboration highlights how LLM can help teachers include CRP components into their instructional practices efficiently, especially in global priorities for future-ready education, such as AI literacy.","authors":["Jiayi Wang","Ruiwei Xiao","Xinying Hou","Hanqi Li","Ying Jui Tseng","John Stamper","Ken Koedinger"],"url":"https://arxiv.org/abs/2505.08083"}
{"created":"2025-05-14","title":"Visually Interpretable Subtask Reasoning for Visual Question Answering","abstract":"Answering complex visual questions like `Which red furniture can be used for sitting?' requires multi-step reasoning, including object recognition, attribute filtering, and relational understanding. Recent work improves interpretability in multimodal large language models (MLLMs) by decomposing tasks into sub-task programs, but these methods are computationally expensive and less accurate due to poor adaptation to target data. To address this, we introduce VISTAR (Visually Interpretable Subtask-Aware Reasoning Model), a subtask-driven training framework that enhances both interpretability and reasoning by generating textual and visual explanations within MLLMs. Instead of relying on external models, VISTAR fine-tunes MLLMs to produce structured Subtask-of-Thought rationales (step-by-step reasoning sequences). Experiments on two benchmarks show that VISTAR consistently improves reasoning accuracy while maintaining interpretability. Our code and dataset will be available at https://github.com/ChengJade/VISTAR.","authors":["Yu Cheng","Arushi Goel","Hakan Bilen"],"url":"https://arxiv.org/abs/2505.08084"}
{"created":"2025-05-14","title":"A Federated Random Forest Solution for Secure Distributed Machine Learning","abstract":"Privacy and regulatory barriers often hinder centralized machine learning solutions, particularly in sectors like healthcare where data cannot be freely shared. Federated learning has emerged as a powerful paradigm to address these concerns; however, existing frameworks primarily support gradient-based models, leaving a gap for more interpretable, tree-based approaches. This paper introduces a federated learning framework for Random Forest classifiers that preserves data privacy and provides robust performance in distributed settings. By leveraging PySyft for secure, privacy-aware computation, our method enables multiple institutions to collaboratively train Random Forest models on locally stored data without exposing sensitive information. The framework supports weighted model averaging to account for varying data distributions, incremental learning to progressively refine models, and local evaluation to assess performance across heterogeneous datasets. Experiments on two real-world healthcare benchmarks demonstrate that the federated approach maintains competitive predictive accuracy - within a maximum 9\\% margin of centralized methods - while satisfying stringent privacy requirements. These findings underscore the viability of tree-based federated learning for scenarios where data cannot be centralized due to regulatory, competitive, or technical constraints. The proposed solution addresses a notable gap in existing federated learning libraries, offering an adaptable tool for secure distributed machine learning tasks that demand both transparency and reliable performance. The tool is available at https://github.com/ieeta-pt/fed_rf.","authors":["Alexandre Cotorobai","Jorge Miguel Silva","Jose Luis Oliveira"],"url":"https://arxiv.org/abs/2505.08085"}
{"created":"2025-05-14","title":"Multi-modal wound classification using wound image and location by Xception and Gaussian Mixture Recurrent Neural Network (GMRNN)","abstract":"The effective diagnosis of acute and hard-to-heal wounds is crucial for wound care practitioners to provide effective patient care. Poor clinical outcomes are often linked to infection, peripheral vascular disease, and increasing wound depth, which collectively exacerbate these comorbidities. However, diagnostic tools based on Artificial Intelligence (AI) speed up the interpretation of medical images and improve early detection of disease. In this article, we propose a multi-modal AI model based on transfer learning (TL), which combines two state-of-the-art architectures, Xception and GMRNN, for wound classification. The multi-modal network is developed by concatenating the features extracted by a transfer learning algorithm and location features to classify the wound types of diabetic, pressure, surgical, and venous ulcers. The proposed method is comprehensively compared with deep neural networks (DNN) for medical image analysis. The experimental results demonstrate a notable wound-class classifications (containing only diabetic, pressure, surgical, and venous) vary from 78.77 to 100\\% in various experiments. The results presented in this study showcase the exceptional accuracy of the proposed methodology in accurately classifying the most commonly occurring wound types using wound images and their corresponding locations.","authors":["Ramin Mousa","Ehsan Matbooe","Hakimeh Khojasteh","Amirali Bengari","Mohammadmahdi Vahediahmar"],"url":"https://arxiv.org/abs/2505.08086"}
{"created":"2025-05-14","title":"Manifold Learning with Normalizing Flows: Towards Regularity, Expressivity and Iso-Riemannian Geometry","abstract":"Modern machine learning increasingly leverages the insight that high-dimensional data often lie near low-dimensional, non-linear manifolds, an idea known as the manifold hypothesis. By explicitly modeling the geometric structure of data through learning Riemannian geometry algorithms can achieve improved performance and interpretability in tasks like clustering, dimensionality reduction, and interpolation. In particular, learned pullback geometry has recently undergone transformative developments that now make it scalable to learn and scalable to evaluate, which further opens the door for principled non-linear data analysis and interpretable machine learning. However, there are still steps to be taken when considering real-world multi-modal data. This work focuses on addressing distortions and modeling errors that can arise in the multi-modal setting and proposes to alleviate both challenges through isometrizing the learned Riemannian structure and balancing regularity and expressivity of the diffeomorphism parametrization. We showcase the effectiveness of the synergy of the proposed approaches in several numerical experiments with both synthetic and real data.","authors":["Willem Diepeveen","Deanna Needell"],"url":"https://arxiv.org/abs/2505.08087"}
{"created":"2025-05-14","title":"Graph-Based Floor Separation Using Node Embeddings and Clustering of WiFi Trajectories","abstract":"Indoor positioning systems (IPSs) are increasingly vital for location-based services in complex multi-storey environments. This study proposes a novel graph-based approach for floor separation using Wi-Fi fingerprint trajectories, addressing the challenge of vertical localization in indoor settings. We construct a graph where nodes represent Wi-Fi fingerprints, and edges are weighted by signal similarity and contextual transitions. Node2Vec is employed to generate low-dimensional embeddings, which are subsequently clustered using K-means to identify distinct floors. Evaluated on the Huawei University Challenge 2021 dataset, our method outperforms traditional community detection algorithms, achieving an accuracy of 68.97%, an F1- score of 61.99%, and an Adjusted Rand Index of 57.19%. By publicly releasing the preprocessed dataset and implementation code, this work contributes to advancing research in indoor positioning. The proposed approach demonstrates robustness to signal noise and architectural complexities, offering a scalable solution for floor-level localization.","authors":["Rabia Yasa Kostas","Kahraman Kostas"],"url":"https://arxiv.org/abs/2505.08088"}
{"created":"2025-05-14","title":"LEGO: Layout Expression for Generating One-to-one Mapping","abstract":"We describe LEGO, a new approach to optimizing data movement whereby code is expressed as a layout-independent computation and composed with layouts for data and computation. This code generator organization derives complex indexing expressions associated with hierarchical parallel code and data movement for GPUs. LEGO maps from layout specification to indexing expressions, and can be integrated into existing compilers and code templates. It facilitates the exploration of data layouts in combination with other optimizations. We demonstrate LEGO's integration with the MLIR and Triton compilers, and with CUDA templates. We show that LEGO is capable of deriving performance competitive with Triton, and shows broad applicability in its integration with MLIR and CUDA.","authors":["Amir Mohammad Tavakkoli","Cosmin Oancea","Mary Hall"],"url":"https://arxiv.org/abs/2505.08091"}
{"created":"2025-05-14","title":"Implicit Toolpath Generation for Functionally Graded Additive Manufacturing via Gradient-Aware Slicing","abstract":"This paper presents a novel gradient-aware slicing method for functionally graded additive manufacturing (FGM) that overcomes the limitations of conventional toolpath planning approaches, which struggle to produce truly continuous gradients. By integrating multi-material gradients into the toolpath generation process, our method enables the fabrication of FGMs with complex gradients that vary seamlessly along all three axes. We leverage OpenVCAD's implicit representation of geometry and material fields to directly extract iso-contours, enabling accurate, controlled gradient toolpaths. Two novel strategies are introduced to integrate these gradients into the toolpath planning process. The first strategy maintains traditional perimeter, skin, and infill structures subdivided by mixture ratios, with automated 'zippering' to mitigate stress concentrations. The second strategy fills iso-contoured regions densely, printing directly against gradients to eliminate purging and reduce waste. Both strategies accommodate gradually changing printing parameters, such as mixed filament ratios, toolhead switching, and variable nozzle temperatures for foaming materials. This capability allows for controlled variation of composition, density, and other properties within a single build, expanding the design space for functionally graded parts. Experimental results demonstrate the fabrication of high-quality FGMs with complex, multi-axis gradients, highlighting the versatility of our method. We showcase the successful implementation of both strategies on a range of geometries and material combinations, demonstrating the potential of our approach to produce intricate and functional FGMs. This work provides a robust, open-source, and automated framework for designing and fabricating advanced FGMs, accelerating research in multi-material additive manufacturing.","authors":["Charles Wade","Devon Beck","Robert MacCurdy"],"url":"https://arxiv.org/abs/2505.08093"}
{"created":"2025-05-14","title":"Fused3S: Fast Sparse Attention on Tensor Cores","abstract":"Sparse attention is a core building block in many leading neural network models, from graph-structured learning to sparse sequence modeling. It can be decomposed into a sequence of three sparse matrix operations (3S): sampled dense-dense matrix multiplication (SDDMM), softmax normalization, and sparse matrix multiplication (SpMM). Efficiently executing the 3S computational pattern on modern GPUs remains challenging due to (a) the mismatch between unstructured sparsity and tensor cores optimized for dense operations, and (b) the high cost of data movement. Previous works have optimized these sparse operations individually or addressed one of these challenges. This paper introduces Fused3S, the first fused 3S algorithm that jointly maximizes tensor core utilization and minimizes data movement. Across real-world graph datasets, Fused3S achieves $1.6- 16.3\\times$ and $1.5-14\\times$ speedup over state-of-the-art on H100 and A30 GPUs. Furthermore, integrating Fused3S into Graph Transformer inference accelerates end-to-end performance by $1.05-5.36\\times$, consistently outperforming all 3S baselines across diverse datasets (single and batched graphs) and GPU architectures.","authors":["Zitong Li","Aparna Chandramowlishwaran"],"url":"https://arxiv.org/abs/2505.08098"}
{"created":"2025-05-14","title":"Topology-Guided Knowledge Distillation for Efficient Point Cloud Processing","abstract":"Point cloud processing has gained significant attention due to its critical role in applications such as autonomous driving and 3D object recognition. However, deploying high-performance models like Point Transformer V3 in resource-constrained environments remains challenging due to their high computational and memory demands. This work introduces a novel distillation framework that leverages topology-aware representations and gradient-guided knowledge distillation to effectively transfer knowledge from a high-capacity teacher to a lightweight student model. Our approach captures the underlying geometric structures of point clouds while selectively guiding the student model's learning process through gradient-based feature alignment. Experimental results in the Nuscenes, SemanticKITTI, and Waymo datasets demonstrate that the proposed method achieves competitive performance, with an approximately 16x reduction in model size and a nearly 1.9x decrease in inference time compared to its teacher model. Notably, on NuScenes, our method achieves state-of-the-art performance among knowledge distillation techniques trained solely on LiDAR data, surpassing prior knowledge distillation baselines in segmentation performance. Our implementation is available publicly at:","authors":["Luu Tung Hai","Thinh D. Le","Zhicheng Ding","Qing Tian","Truong-Son Hy"],"url":"https://arxiv.org/abs/2505.08101"}
{"created":"2025-05-14","title":"Are LLMs complicated ethical dilemma analyzers?","abstract":"One open question in the study of Large Language Models (LLMs) is whether they can emulate human ethical reasoning and act as believable proxies for human judgment. To investigate this, we introduce a benchmark dataset comprising 196 real-world ethical dilemmas and expert opinions, each segmented into five structured components: Introduction, Key Factors, Historical Theoretical Perspectives, Resolution Strategies, and Key Takeaways. We also collect non-expert human responses for comparison, limited to the Key Factors section due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini, Claude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric framework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine similarity, and Universal Sentence Encoder similarity. Metric weights are computed through an inversion-based ranking alignment and pairwise AHP analysis, enabling fine-grained comparison of model outputs to expert responses. Our results show that LLMs generally outperform non-expert humans in lexical and structural alignment, with GPT-4o-mini performing most consistently across all sections. However, all models struggle with historical grounding and proposing nuanced resolution strategies, which require contextual abstraction. Human responses, while less structured, occasionally achieve comparable semantic similarity, suggesting intuitive moral reasoning. These findings highlight both the strengths and current limitations of LLMs in ethical decision-making.","authors":["Jiashen (Jason)","Du","Jesse Yao","Allen Liu","Zhekai Zhang"],"url":"https://arxiv.org/abs/2505.08106"}
{"created":"2025-05-14","title":"Sleep Position Classification using Transfer Learning for Bed-based Pressure Sensors","abstract":"Bed-based pressure-sensitive mats (PSMs) offer a non-intrusive way of monitoring patients during sleep. We focus on four-way sleep position classification using data collected from a PSM placed under a mattress in a sleep clinic. Sleep positions can affect sleep quality and the prevalence of sleep disorders, such as apnea. Measurements were performed on patients with suspected sleep disorders referred for assessments at a sleep clinic. Training deep learning models can be challenging in clinical settings due to the need for large amounts of labeled data. To overcome the shortage of labeled training data, we utilize transfer learning to adapt pre-trained deep learning models to accurately estimate sleep positions from a low-resolution PSM dataset collected in a polysomnography sleep lab. Our approach leverages Vision Transformer models pre-trained on ImageNet using masked autoencoding (ViTMAE) and a pre-trained model for human pose estimation (ViTPose). These approaches outperform previous work from PSM-based sleep pose classification using deep learning (TCN) as well as traditional machine learning models (SVM, XGBoost, Random Forest) that use engineered features. We evaluate the performance of sleep position classification from 112 nights of patient recordings and validate it on a higher resolution 13-patient dataset. Despite the challenges of differentiating between sleep positions from low-resolution PSM data, our approach shows promise for real-world deployment in clinical settings","authors":["Olivier Papillon","Rafik Goubran","James Green","Julien Larivi\\`ere-Chartier","Caitlin Higginson","Frank Knoefel","R\\'ebecca Robillard"],"url":"https://arxiv.org/abs/2505.08111"}
{"created":"2025-05-14","title":"Valida ISA Spec, version 1.0: A zk-Optimized Instruction Set Architecture","abstract":"The Valida instruction set architecture is designed for implementation in zkVMs to optimize for fast, efficient execution proving. This specification intends to guide implementors of zkVMs and compiler toolchains for Valida. It provides an unambiguous definition of the semantics of Valida programs and may be used as a starting point for formalization efforts.","authors":["Morgan Thomas","Mamy Ratsimbazafy","Marcin Bugaj","Lewis Revill","Carlo Modica","Sebastian Schmidt","Ventali Tan","Daniel Lubarov","Max Gillett","Wei Dai"],"url":"https://arxiv.org/abs/2505.08114"}
{"created":"2025-05-14","title":"Invariant-Based Cryptography: Toward a General Framework","abstract":"We develop a generalized framework for invariant-based cryptography by extending the use of structural identities as core cryptographic mechanisms. Starting from a previously introduced scheme where a secret is encoded via a four-point algebraic invariant over masked functional values, we broaden the approach to include multiple classes of invariant constructions. In particular, we present new symmetric schemes based on shifted polynomial roots and functional equations constrained by symmetric algebraic conditions, such as discriminants and multilinear identities. These examples illustrate how algebraic invariants -- rather than one-way functions -- can enforce structural consistency and unforgeability. We analyze the cryptographic utility of such invariants in terms of recoverability, integrity binding, and resistance to forgery, and show that these constructions achieve security levels comparable to the original oscillatory model. This work establishes a foundation for invariant-based design as a versatile and compact alternative in symmetric cryptographic protocols.","authors":["Stanislav Semenov"],"url":"https://arxiv.org/abs/2505.08115"}
{"created":"2025-05-14","title":"Now you see it, Now you don't: Damage Label Agreement in Drone & Satellite Post-Disaster Imagery","abstract":"This paper audits damage labels derived from coincident satellite and drone aerial imagery for 15,814 buildings across Hurricanes Ian, Michael, and Harvey, finding 29.02% label disagreement and significantly different distributions between the two sources, which presents risks and potential harms during the deployment of machine learning damage assessment systems. Currently, there is no known study of label agreement between drone and satellite imagery for building damage assessment. The only prior work that could be used to infer if such imagery-derived labels agree is limited by differing damage label schemas, misaligned building locations, and low data quantities. This work overcomes these limitations by comparing damage labels using the same damage label schemas and building locations from three hurricanes, with the 15,814 buildings representing 19.05 times more buildings considered than the most relevant prior work. The analysis finds satellite-derived labels significantly under-report damage by at least 20.43% compared to drone-derived labels (p<1.2x10^-117), and satellite- and drone-derived labels represent significantly different distributions (p<5.1x10^-175). This indicates that computer vision and machine learning (CV/ML) models trained on at least one of these distributions will misrepresent actual conditions, as the differing satellite and drone-derived distributions cannot simultaneously represent the distribution of actual conditions in a scene. This potential misrepresentation poses ethical risks and potential societal harm if not managed. To reduce the risk of future societal harms, this paper offers four recommendations to improve reliability and transparency to decisio-makers when deploying CV/ML damage assessment systems in practice","authors":["Thomas Manzini","Priyankari Perali","Jayesh Tripathi","Robin Murphy"],"url":"https://arxiv.org/abs/2505.08117"}
{"created":"2025-05-14","title":"Will Your Next Pair Programming Partner Be Human? An Empirical Evaluation of Generative AI as a Collaborative Teammate in a Semester-Long Classroom Setting","abstract":"Generative AI (GenAI), especially Large Language Models (LLMs), is rapidly reshaping both programming workflows and computer science education. Many programmers now incorporate GenAI tools into their workflows, including for collaborative coding tasks such as pair programming. While prior research has demonstrated the benefits of traditional pair programming and begun to explore GenAI-assisted coding, the role of LLM-based tools as collaborators in pair programming remains underexamined. In this work, we conducted a mixed-methods study with 39 undergraduate students to examine how GenAI influences collaboration, learning, and performance in pair programming. Specifically, students completed six in-class assignments under three conditions: Traditional Pair Programming (PP), Pair Programming with GenAI (PAI), and Solo Programming with GenAI (SAI). They used both LLM-based inline completion tools (e.g., GitHub Copilot) and LLM-based conversational tools (e.g., ChatGPT). Our results show that students in PAI achieved the highest assignment scores, whereas those in SAI attained the lowest. Additionally, students' attitudes toward LLMs' programming capabilities improved significantly after collaborating with LLM-based tools, and preferences were largely shaped by the perceived usefulness for completing assignments and learning programming skills, as well as the quality of collaboration. Our qualitative findings further reveal that while students appreciated LLM-based tools as valuable pair programming partners, they also identified limitations and had different expectations compared to human teammates. Our study provides one of the first empirical evaluations of GenAI as a pair programming collaborator through a comparison of three conditions (PP, PAI, and SAI). We also discuss the design implications and pedagogical considerations for future GenAI-assisted pair programming approaches.","authors":["Wenhan Lyu","Yimeng Wang","Yifan Sun","Yixuan Zhang"],"url":"https://arxiv.org/abs/2505.08119"}
{"created":"2025-05-14","title":"Putting It All into Context: Simplifying Agents with LCLMs","abstract":"Recent advances in language model (LM) agents have demonstrated significant potential for automating complex real-world tasks. To make progress on these difficult tasks, LM agent architectures have become increasingly complex, often incorporating multi-step retrieval tools, multiple agents, and scaffolding adapted to the underlying LM. In this work, we investigate whether all of this complexity is necessary, or if parts of these scaffolds can be removed on challenging tasks like SWE-bench. We show that in the case of SWE-bench, simply putting the entire environment into the context of a long context language model (LCLM) and properly prompting the model makes it competitive with carefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable with approaches using carefully tuned agent scaffolds (32%). While the unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic architectures, we demonstrate that the more capable Gemini-2.5-Pro using the same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a competitive 48.6% solve rate.","authors":["Mingjian Jiang","Yangjun Ruan","Luis Lastras","Pavan Kapanipathi","Tatsunori Hashimoto"],"url":"https://arxiv.org/abs/2505.08120"}
{"created":"2025-05-14","title":"Leveraging Reinforcement Learning and Koopman Theory for Enhanced Model Predictive Control Performance","abstract":"This study presents an innovative approach to Model Predictive Control (MPC) by leveraging the powerful combination of Koopman theory and Deep Reinforcement Learning (DRL). By transforming nonlinear dynamical systems into a higher-dimensional linear regime, the Koopman operator facilitates the linear treatment of nonlinear behaviors, paving the way for more efficient control strategies. Our methodology harnesses the predictive prowess of Koopman-based models alongside the optimization capabilities of DRL, particularly using the Proximal Policy Optimization (PPO) algorithm, to enhance the controller's performance. The resulting end-to-end learning framework refines the predictive control policies to cater to specific operational tasks, optimizing both performance and economic efficiency. We validate our approach through rigorous NMPC and eNMPC case studies, demonstrating that the Koopman-RL controller outperforms traditional controllers by achieving higher stability, superior constraint satisfaction, and significant cost savings. The findings indicate that our model can be a robust tool for complex control tasks, offering valuable insights into future applications of RL in MPC.","authors":["Md Nur-A-Adam Dony","Jing Yang"],"url":"https://arxiv.org/abs/2505.08122"}
{"created":"2025-05-14","title":"JSover: Joint Spectrum Estimation and Multi-Material Decomposition from Single-Energy CT Projections","abstract":"Multi-material decomposition (MMD) enables quantitative reconstruction of tissue compositions in the human body, supporting a wide range of clinical applications. However, traditional MMD typically requires spectral CT scanners and pre-measured X-ray energy spectra, significantly limiting clinical applicability. To this end, various methods have been developed to perform MMD using conventional (i.e., single-energy, SE) CT systems, commonly referred to as SEMMD. Despite promising progress, most SEMMD methods follow a two-step image decomposition pipeline, which first reconstructs monochromatic CT images using algorithms such as FBP, and then performs decomposition on these images. The initial reconstruction step, however, neglects the energy-dependent attenuation of human tissues, introducing severe nonlinear beam hardening artifacts and noise into the subsequent decomposition. This paper proposes JSover, a fundamentally reformulated one-step SEMMD framework that jointly reconstructs multi-material compositions and estimates the energy spectrum directly from SECT projections. By explicitly incorporating physics-informed spectral priors into the SEMMD process, JSover accurately simulates a virtual spectral CT system from SE acquisitions, thereby improving the reliability and accuracy of decomposition. Furthermore, we introduce implicit neural representation (INR) as an unsupervised deep learning solver for representing the underlying material maps. The inductive bias of INR toward continuous image patterns constrains the solution space and further enhances estimation quality. Extensive experiments on both simulated and real CT datasets show that JSover outperforms state-of-the-art SEMMD methods in accuracy and computational efficiency.","authors":["Qing Wu","Hongjiang Wei","Jingyi Yu","S. Kevin Zhou","Yuyao Zhang"],"url":"https://arxiv.org/abs/2505.08123"}
{"created":"2025-05-14","title":"SLAG: Scalable Language-Augmented Gaussian Splatting","abstract":"Language-augmented scene representations hold great promise for large-scale robotics applications such as search-and-rescue, smart cities, and mining. Many of these scenarios are time-sensitive, requiring rapid scene encoding while also being data-intensive, necessitating scalable solutions. Deploying these representations on robots with limited computational resources further adds to the challenge. To address this, we introduce SLAG, a multi-GPU framework for language-augmented Gaussian splatting that enhances the speed and scalability of embedding large scenes. Our method integrates 2D visual-language model features into 3D scenes using SAM and CLIP. Unlike prior approaches, SLAG eliminates the need for a loss function to compute per-Gaussian language embeddings. Instead, it derives embeddings from 3D Gaussian scene parameters via a normalized weighted average, enabling highly parallelized scene encoding. Additionally, we introduce a vector database for efficient embedding storage and retrieval. Our experiments show that SLAG achieves an 18 times speedup in embedding computation on a 16-GPU setup compared to OpenGaussian, while preserving embedding quality on the ScanNet and LERF datasets. For more details, visit our project website: https://slag-project.github.io/.","authors":["Laszlo Szilagyi","Francis Engelmann","Jeannette Bohg"],"url":"https://arxiv.org/abs/2505.08124"}
{"created":"2025-05-14","title":"Asynchronous Multi-Object Tracking with an Event Camera","abstract":"Events cameras are ideal sensors for enabling robots to detect and track objects in highly dynamic environments due to their low latency output, high temporal resolution, and high dynamic range. In this paper, we present the Asynchronous Event Multi-Object Tracking (AEMOT) algorithm for detecting and tracking multiple objects by processing individual raw events asynchronously. AEMOT detects salient event blob features by identifying regions of consistent optical flow using a novel Field of Active Flow Directions built from the Surface of Active Events. Detected features are tracked as candidate objects using the recently proposed Asynchronous Event Blob (AEB) tracker in order to construct small intensity patches of each candidate object. A novel learnt validation stage promotes or discards candidate objects based on classification of their intensity patches, with promoted objects having their position, velocity, size, and orientation estimated at their event rate. We evaluate AEMOT on a new Bee Swarm Dataset, where it tracks dozens of small bees with precision and recall performance exceeding that of alternative event-based detection and tracking algorithms by over 37%. Source code and the labelled event Bee Swarm Dataset will be open sourced","authors":["Angus Apps","Ziwei Wang","Vladimir Perejogin","Timothy Molloy","Robert Mahony"],"url":"https://arxiv.org/abs/2505.08126"}
{"created":"2025-05-14","title":"\"You Cannot Sound Like GPT\": Signs of language discrimination and resistance in computer science publishing","abstract":"LLMs have been celebrated for their potential to help multilingual scientists publish their research. Rather than interpret LLMs as a solution, we hypothesize their adoption can be an indicator of existing linguistic exclusion in scientific writing. Using the case study of ICLR, an influential, international computer science conference, we examine how peer reviewers critique writing clarity. Analyzing almost 80,000 peer reviews, we find significant bias against authors associated with institutions in countries where English is less widely spoken. We see only a muted shift in the expression of this bias after the introduction of ChatGPT in late 2022. To investigate this unexpectedly minor change, we conduct interviews with 14 conference participants from across five continents. Peer reviewers describe associating certain features of writing with people of certain language backgrounds, and such groups in turn with the quality of scientific work. While ChatGPT masks some signs of language background, reviewers explain that they now use ChatGPT \"style\" and non-linguistic features as indicators of author demographics. Authors, aware of this development, described the ongoing need to remove features which could expose their \"non-native\" status to reviewers. Our findings offer insight into the role of ChatGPT in the reproduction of scholarly language ideologies which conflate producers of \"good English\" with producers of \"good science.\"","authors":["Haley Lepp","Daniel Scott Smith"],"url":"https://arxiv.org/abs/2505.08127"}
{"created":"2025-05-14","title":"High-order Regularization for Machine Learning and Learning-based Control","abstract":"The paper proposes a novel regularization procedure for machine learning. The proposed high-order regularization (HR) provides new insight into regularization, which is widely used to train a neural network that can be utilized to approximate the action-value function in general reinforcement learning problems. The proposed HR method ensures the provable convergence of the approximation algorithm, which makes the much-needed connection between regularization and explainable learning using neural networks. The proposed HR method theoretically demonstrates that regularization can be regarded as an approximation in terms of inverse mapping with explicitly calculable approximation error, and the $L_2$ regularization is a lower-order case of the proposed method. We provide lower and upper bounds for the error of the proposed HR solution, which helps build a reliable model. We also find that regularization with the proposed HR can be regarded as a contraction. We prove that the generalizability of neural networks can be maximized with a proper regularization matrix, and the proposed HR is applicable for neural networks with any mapping matrix. With the theoretical explanation of the extreme learning machine for neural network training and the proposed high-order regularization, one can better interpret the output of the neural network, thus leading to explainable learning. We present a case study based on regularized extreme learning neural networks to demonstrate the application of the proposed HR and give the corresponding incremental HR solution. We verify the performance of the proposed HR method by solving a classic control problem in reinforcement learning. The result demonstrates the superior performance of the method with significant enhancement in the generalizability of the neural network.","authors":["Xinghua Liu","Ming Cao"],"url":"https://arxiv.org/abs/2505.08129"}
{"created":"2025-05-14","title":"ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval","abstract":"The rise of Large Language Models~(LLMs) revolutionizes information retrieval, allowing users to obtain required answers through complex instructions within conversations. However, publicly available services remain inadequate in addressing the needs of faculty and students to search campus-specific information. It is primarily due to the LLM's lack of domain-specific knowledge and the limitation of search engines in supporting multilingual and timely scenarios. To tackle these challenges, we introduce ALOHA, a multilingual agent enhanced by hierarchical retrieval for university orientation. We also integrate external APIs into the front-end interface to provide interactive service. The human evaluation and case study show our proposed system has strong capabilities to yield correct, timely, and user-friendly responses to the queries in multiple languages, surpassing commercial chatbots and search engines. The system has been deployed and has provided service for more than 12,000 people.","authors":["Mingxu Tao","Bowen Tang","Mingxuan Ma","Yining Zhang","Hourun Li","Feifan Wen","Hao Ma","Jia Yang"],"url":"https://arxiv.org/abs/2505.08130"}
{"created":"2025-05-14","title":"One Bad NOFO? AI Governance in Federal Grantmaking","abstract":"Much scholarship considers how U.S. federal agencies govern artificial intelligence (AI) through rulemaking and their own internal use policies. But agencies have an overlooked AI governance role: setting discretionary grant policy when directing billions of dollars in federal financial assistance. These dollars enable state and local entities to study, create, and use AI. This funding not only goes to dedicated AI programs, but also to grantees using AI in the course of meeting their routine grant objectives. As discretionary grantmakers, agencies guide and restrict what grant winners do -- a hidden lever for AI governance. Agencies pull this lever by setting program objectives, judging criteria, and restrictions for AI use. Using a novel dataset of over 40,000 non-defense federal grant notices of funding opportunity (NOFOs) posted to Grants.gov between 2009 and 2024, we analyze how agencies regulate the use of AI by grantees. We select records mentioning AI and review their stated goals and requirements. We find agencies promoting AI in notice narratives, shaping adoption in ways other records of grant policy might fail to capture. Of the grant opportunities that mention AI, we find only a handful of AI-specific judging criteria or restrictions. This silence holds even when agencies fund AI uses in contexts affecting people's rights and which, under an analogous federal procurement regime, would result in extra oversight. These findings recast grant notices as a site of AI policymaking -- albeit one that is developing out of step with other regulatory efforts and incomplete in its consideration of transparency, accountability, and privacy protections. The paper concludes by drawing lessons from AI procurement scholarship, while identifying distinct challenges in grantmaking that invite further study.","authors":["Dan Bateyko","Karen Levy"],"url":"https://arxiv.org/abs/2505.08133"}
{"created":"2025-05-14","title":"Leveraging AI for Productive and Trustworthy HPC Software: Challenges and Research Directions","abstract":"We discuss the challenges and propose research directions for using AI to revolutionize the development of high-performance computing (HPC) software. AI technologies, in particular large language models, have transformed every aspect of software development. For its part, HPC software is recognized as a highly specialized scientific field of its own. We discuss the challenges associated with leveraging state-of-the-art AI technologies to develop such a unique and niche class of software and outline our research directions in the two US Department of Energy--funded projects for advancing HPC Software via AI: Ellora and Durban.","authors":["Keita Teranishi","Harshitha Menon","William F. Godoy","Prasanna Balaprakash","David Bau","Tal Ben-Nun","Abhinav Bathele","Franz Franchetti","Michael Franusich","Todd Gamblin","Giorgis Georgakoudis","Tom Goldstein","Arjun Guha","Steven Hahn","Costin Iancu","Zheming Jin","Terry Jones","Tze Meng Low","Het Mankad","Narasinga Rao Miniskar","Mohammad Alaul Haque Monil","Daniel Nichols","Konstantinos Parasyris","Swaroop Pophale","Pedro Valero-Lara","Jeffrey S. Vetter","Samuel Williams","Aaron Young"],"url":"https://arxiv.org/abs/2505.08135"}
{"created":"2025-05-14","title":"Large Language Models for Computer-Aided Design: A Survey","abstract":"Large Language Models (LLMs) have seen rapid advancements in recent years, with models like ChatGPT and DeepSeek, showcasing their remarkable capabilities across diverse domains. While substantial research has been conducted on LLMs in various fields, a comprehensive review focusing on their integration with Computer-Aided Design (CAD) remains notably absent. CAD is the industry standard for 3D modeling and plays a vital role in the design and development of products across different industries. As the complexity of modern designs increases, the potential for LLMs to enhance and streamline CAD workflows presents an exciting frontier. This article presents the first systematic survey exploring the intersection of LLMs and CAD. We begin by outlining the industrial significance of CAD, highlighting the need for AI-driven innovation. Next, we provide a detailed overview of the foundation of LLMs. We also examine both closed-source LLMs as well as publicly available models. The core of this review focuses on the various applications of LLMs in CAD, providing a taxonomy of six key areas where these models are making considerable impact. Finally, we propose several promising future directions for further advancements, which offer vast opportunities for innovation and are poised to shape the future of CAD technology. Github: https://github.com/lichengzhanguom/LLMs-CAD-Survey-Taxonomy","authors":["Licheng Zhang","Bach Le","Naveed Akhtar","Siew-Kei Lam","Tuan Ngo"],"url":"https://arxiv.org/abs/2505.08137"}
{"created":"2025-05-14","title":"Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning","abstract":"Machine unlearning methods take a model trained on a dataset and a forget set, then attempt to produce a model as if it had only been trained on the examples not in the forget set. We empirically show that an adversary is able to distinguish between a mirror model (a control model produced by retraining without the data to forget) and a model produced by an unlearning method across representative unlearning methods from the literature. We build distinguishing algorithms based on evaluation scores in the literature (i.e. membership inference scores) and Kullback-Leibler divergence.","authors":["Brennon Brimhall","Philip Mathew","Neil Fendley","Yinzhi Cao","Matthew Green"],"url":"https://arxiv.org/abs/2505.08138"}
{"created":"2025-05-14","title":"Integrating Koopman theory and Lyapunov stability for enhanced model predictive control in nonlinear systems","abstract":"This paper delves into the challenges posed by the increasing complexity of modern control systems, specifically focusing on bilinear systems, a prevalent subclass of non-linear systems characterized by state dynamics influenced by the interaction of state and control variables. Traditional control strategies, such as PID controllers, often fall short in adequately addressing the intricacies of such systems due to their predictive limitations. To bridge this gap, we introduce Model Predictive Control (MPC), a sophisticated technique that utilizes system models to forecast future behaviors, allowing for the computation of an optimal control sequence by minimizing deviations and control efforts. The Koopman operator emerges as a pivotal tool in this framework by providing a means to linearize the nonlinear dynamics of bilinear systems. By integrating the principles of Lyapunov theory with the linearizing capabilities of the Koopman operator into the MPC framework, we give rise to Koopman Lyapunov-based Model Predictive Control (Koopman LMPC). This approach not only retains MPC's predictive capabilities but also harnesses the Koopman operator's ability to transform complex nonlinear behaviors into a linear framework, thereby enhancing the robustness and applicability of LMPC. With the stability assurances from Lyapunov theory, Koopman LMPC presents a robust solution to effectively control and stabilize bilinear systems. The paper underscores the efficacy of Koopman LMPC, emphasizing its significance in achieving optimal performance and system stability, marking it as a promising approach for the future of advanced control systems.","authors":["Md Nur-A-Adam Dony","Minghui Zhu"],"url":"https://arxiv.org/abs/2505.08139"}
{"created":"2025-05-14","title":"Lost in Transmission: When and Why LLMs Fail to Reason Globally","abstract":"Despite their many successes, transformer-based large language models (LLMs) continue to struggle with tasks that require complex reasoning over large parts of their input. We argue that these failures arise due to capacity limits on the accurate flow of information within LLMs. To formalize this issue, we introduce the bounded attention prefix oracle (BAPO) model, a new computational framework that models bandwidth constraints on attention heads, the mechanism for internal communication in LLMs. We show that several important reasoning problems like graph reachability require high communication bandwidth for BAPOs to solve; we call these problems BAPO-hard. Our experiments corroborate our theoretical predictions: GPT-4, Claude, and Gemini succeed on BAPO-easy tasks and fail even on relatively small BAPO-hard tasks. BAPOs also reveal another benefit of chain of thought (CoT): we prove that breaking down a task using CoT can turn any BAPO-hard problem into a BAPO-easy one. Our results offer principled explanations for key LLM failures and suggest directions for architectures and inference methods that mitigate bandwidth limits.","authors":["Tobias Schnabel","Kiran Tomlinson","Adith Swaminathan","Jennifer Neville"],"url":"https://arxiv.org/abs/2505.08140"}
{"created":"2025-05-14","title":"Communication Styles and Reader Preferences of LLM and Human Experts in Explaining Health Information","abstract":"With the wide adoption of large language models (LLMs) in information assistance, it is essential to examine their alignment with human communication styles and values. We situate this study within the context of fact-checking health information, given the critical challenge of rectifying conceptions and building trust. Recent studies have explored the potential of LLM for health communication, but style differences between LLMs and human experts and associated reader perceptions remain under-explored. In this light, our study evaluates the communication styles of LLMs, focusing on how their explanations differ from those of humans in three core components of health communication: information, sender, and receiver. We compiled a dataset of 1498 health misinformation explanations from authoritative fact-checking organizations and generated LLM responses to inaccurate health information. Drawing from health communication theory, we evaluate communication styles across three key dimensions of information linguistic features, sender persuasive strategies, and receiver value alignments. We further assessed human perceptions through a blinded evaluation with 99 participants. Our findings reveal that LLM-generated articles showed significantly lower scores in persuasive strategies, certainty expressions, and alignment with social values and moral foundations. However, human evaluation demonstrated a strong preference for LLM content, with over 60% responses favoring LLM articles for clarity, completeness, and persuasiveness. Our results suggest that LLMs' structured approach to presenting information may be more effective at engaging readers despite scoring lower on traditional measures of quality in fact-checking and health communication.","authors":["Jiawei Zhou","Kritika Venkatachalam","Minje Choi","Koustuv Saha","Munmun De Choudhury"],"url":"https://arxiv.org/abs/2505.08143"}
{"created":"2025-05-14","title":"Dyadic Factorization and Efficient Inversion of Sparse Positive Definite Matrices","abstract":"In inverting large sparse matrices, the key difficulty lies in effectively exploiting sparsity during the inversion process. One well-established strategy is the nested dissection, which seeks the so-called sparse Cholesky factorization. We argue that the matrices for which such factors can be found are characterized by a hidden dyadic sparsity structure. This paper builds on that idea by proposing an efficient approach for inverting such matrices. The method consists of two independent steps: the first packs the matrix into a dyadic form, while the second performs a sparse (dyadic) Gram-Schmidt orthogonalization of the packed matrix.","authors":["Micha{\\l} Kos","Krzysztof Podg\\'orski","Hanqing Wu"],"url":"https://arxiv.org/abs/2505.08144"}
{"created":"2025-05-14","title":"Multi-Layer Hierarchical Federated Learning with Quantization","abstract":"Almost all existing hierarchical federated learning (FL) models are limited to two aggregation layers, restricting scalability and flexibility in complex, large-scale networks. In this work, we propose a Multi-Layer Hierarchical Federated Learning framework (QMLHFL), which appears to be the first study that generalizes hierarchical FL to arbitrary numbers of layers and network architectures through nested aggregation, while employing a layer-specific quantization scheme to meet communication constraints. We develop a comprehensive convergence analysis for QMLHFL and derive a general convergence condition and rate that reveal the effects of key factors, including quantization parameters, hierarchical architecture, and intra-layer iteration counts. Furthermore, we determine the optimal number of intra-layer iterations to maximize the convergence rate while meeting a deadline constraint that accounts for both communication and computation times. Our results show that QMLHFL consistently achieves high learning accuracy, even under high data heterogeneity, and delivers notably improved performance when optimized, compared to using randomly selected values.","authors":["Seyed Mohammad Azimi-Abarghouyi","Carlo Fischione"],"url":"https://arxiv.org/abs/2505.08145"}
{"created":"2025-05-14","title":"Tensor Sketch: Fast and Scalable Polynomial Kernel Approximation","abstract":"Approximation of non-linear kernels using random feature maps has become a powerful technique for scaling kernel methods to large datasets. We propose \\textit{Tensor Sketch}, an efficient random feature map for approximating polynomial kernels. Given $n$ training samples in $\\R^d$ Tensor Sketch computes low-dimensional embeddings in $\\R^D$ in time $\\BO{n(d+D \\log{D})}$ making it well-suited for high-dimensional and large-scale settings. We provide theoretical guarantees on the approximation error, ensuring the fidelity of the resulting kernel function estimates. We also discuss extensions and highlight applications where Tensor Sketch serves as a central computational tool.","authors":["Ninh Pham","Rasmus Pagh"],"url":"https://arxiv.org/abs/2505.08146"}
{"created":"2025-05-14","title":"A Large-Scale Empirical Analysis of Custom GPTs' Vulnerabilities in the OpenAI Ecosystem","abstract":"Millions of users leverage generative pretrained transformer (GPT)-based language models developed by leading model providers for a wide range of tasks. To support enhanced user interaction and customization, many platforms-such as OpenAI-now enable developers to create and publish tailored model instances, known as custom GPTs, via dedicated repositories or application stores. These custom GPTs empower users to browse and interact with specialized applications designed to meet specific needs. However, as custom GPTs see growing adoption, concerns regarding their security vulnerabilities have intensified. Existing research on these vulnerabilities remains largely theoretical, often lacking empirical, large-scale, and statistically rigorous assessments of associated risks.","authors":["Sunday Oyinlola Ogundoyin","Muhammad Ikram","Hassan Jameel Asghar","Benjamin Zi Hao Zhao","Dali Kaafar"],"url":"https://arxiv.org/abs/2505.08148"}
{"created":"2025-05-14","title":"Majorization and Inequalities among Complete Homogenous Symmetric Functions","abstract":"Inequalities among symmetric functions are fundamental in various branches of mathematics, thus motivating a systematic study of their structure. Majorization has been shown to characterize inequalities among commonly used symmetric functions, except for complete homogeneous symmetric functions (shortened as CHs). In 2011, Cuttler, Greene, and Skandera posed a natural question: Can majorization also characterize inequalities among CHs? Their work demonstrated that majorization characterizes inequalities among CHs up to degree 7 and suggested exploring its validity for higher degrees. In this paper, we show that, for every degree greater than 7, majorization does not characterize inequalities among CHs.","authors":["Jia Xu","Yong Yao"],"url":"https://arxiv.org/abs/2505.08149"}
{"created":"2025-05-14","title":"Foundation Models Knowledge Distillation For Battery Capacity Degradation Forecast","abstract":"Accurate estimation of lithium-ion battery capacity degradation is critical for enhancing the reliability and safety of battery operations. Traditional expert models, tailored to specific scenarios, provide isolated estimations. With the rapid advancement of data-driven techniques, a series of general-purpose time-series foundation models have been developed. However, foundation models specifically designed for battery capacity degradation remain largely unexplored. To enable zero-shot generalization in battery degradation prediction using large model technology, this study proposes a degradation-aware fine-tuning strategy for time-series foundation models. We apply this strategy to fine-tune the Timer model on approximately 10 GB of open-source battery charge discharge data. Validation on our released CycleLife-SJTUIE dataset demonstrates that the fine-tuned Battery-Timer possesses strong zero-shot generalization capability in capacity degradation forecasting. To address the computational challenges of deploying large models, we further propose a knowledge distillation framework that transfers the knowledge of pre-trained foundation models into compact expert models. Distillation results across several state-of-the-art time-series expert models confirm that foundation model knowledge significantly improves the multi-condition generalization of expert models.","authors":["Joey Chan","Zhen Chen","Ershun Pan"],"url":"https://arxiv.org/abs/2505.08151"}
{"created":"2025-05-14","title":"Efficient and Scalable Neural Symbolic Search for Knowledge Graph Complex Query Answering","abstract":"Complex Query Answering (CQA) aims to retrieve answer sets for complex logical formulas from incomplete knowledge graphs, which is a crucial yet challenging task in knowledge graph reasoning. While neuro-symbolic search utilized neural link predictions achieve superior accuracy, they encounter significant complexity bottlenecks: (i) Data complexity typically scales quadratically with the number of entities in the knowledge graph, and (ii) Query complexity becomes NP-hard for cyclic queries. Consequently, these approaches struggle to effectively scale to larger knowledge graphs and more complex queries. To address these challenges, we propose an efficient and scalable symbolic search framework. First, we propose two constraint strategies to compute neural logical indices to reduce the domain of variables, thereby decreasing the data complexity of symbolic search. Additionally, we introduce an approximate algorithm based on local search to tackle the NP query complexity of cyclic queries. Experiments on various CQA benchmarks demonstrate that our framework reduces the computational load of symbolic methods by 90\\% while maintaining nearly the same performance, thus alleviating both efficiency and scalability issues.","authors":["Weizhi Fei","Zihao Wang","hang Yin","Shukai Zhao","Wei Zhang","Yangqiu Song"],"url":"https://arxiv.org/abs/2505.08155"}
{"created":"2025-05-14","title":"Hyperbolic Contrastive Learning with Model-augmentation for Knowledge-aware Recommendation","abstract":"Benefiting from the effectiveness of graph neural networks (GNNs) and contrastive learning, GNN-based contrastive learning has become mainstream for knowledge-aware recommendation. However, most existing contrastive learning-based methods have difficulties in effectively capturing the underlying hierarchical structure within user-item bipartite graphs and knowledge graphs. Moreover, they commonly generate positive samples for contrastive learning by perturbing the graph structure, which may lead to a shift in user preference learning. To overcome these limitations, we propose hyperbolic contrastive learning with model-augmentation for knowledge-aware recommendation. To capture the intrinsic hierarchical graph structures, we first design a novel Lorentzian knowledge aggregation mechanism, which enables more effective representations of users and items. Then, we propose three model-level augmentation techniques to assist Hyperbolic contrastive learning. Different from the classical structure-level augmentation (e.g., edge dropping), the proposed model-augmentations can avoid preference shifts between the augmented positive pair. Finally, we conduct extensive experiments to demonstrate the superiority (maximum improvement of $11.03\\%$) of proposed methods over existing baselines.","authors":["Shengyin Sun","Chen Ma"],"url":"https://arxiv.org/abs/2505.08157"}
{"created":"2025-05-14","title":"Feature Fitted Online Conformal Prediction for Deep Time Series Forecasting Model","abstract":"Time series forecasting is critical for many applications, where deep learning-based point prediction models have demonstrated strong performance. However, in practical scenarios, there is also a need to quantify predictive uncertainty through online confidence intervals. Existing confidence interval modeling approaches building upon these deep point prediction models suffer from key limitations: they either require costly retraining, fail to fully leverage the representational strengths of deep models, or lack theoretical guarantees. To address these gaps, we propose a lightweight conformal prediction method that provides valid coverage and shorter interval lengths without retraining. Our approach leverages features extracted from pre-trained point prediction models to fit a residual predictor and construct confidence intervals, further enhanced by an adaptive coverage control mechanism. Theoretically, we prove that our method achieves asymptotic coverage convergence, with error bounds dependent on the feature quality of the underlying point prediction model. Experiments on 12 datasets demonstrate that our method delivers tighter confidence intervals while maintaining desired coverage rates. Code, model and dataset in \\href{https://github.com/xiannanhuang/FFDCI}{Github}","authors":["Xiannan Huang","Shuhan Qiu"],"url":"https://arxiv.org/abs/2505.08158"}
{"created":"2025-05-14","title":"GDNTT: an Area-Efficient Parallel NTT Accelerator Using Glitch-Driven Near-Memory Computing and Reconfigurable 10T SRAM","abstract":"With the rapid advancement of quantum computing technology, post-quantum cryptography (PQC) has emerged as a pivotal direction for next-generation encryption standards. Among these, lattice-based cryptographic schemes rely heavily on the fast Number Theoretic Transform (NTT) over polynomial rings, whose performance directly determines encryption/decryption throughput and energy efficiency. However, existing software-based NTT implementations struggle to meet the real-time performance and low-power requirements of IoT and edge devices. To address this challenge, this paper proposes an area-efficient highly parallel NTT accelerator with glitch-driven near-memory computing (GDNTT). The design integrates a 10T SRAM for data storage, enabling flexible row/column data access and streamlining circuit mapping strategies. Furthermore, a glitch generator is incorporated into the near-memory computing unit, significantly reducing the latency of butterfly operations. Evaluation results show that the proposed NTT accelerator achieves a 1.5~28* improvement in throughput-per-area compared to the state-of-the-art.","authors":["Hengyu Ding","Houran Ji","Jia Li","Jinhang Chen","Chin-Wing Sham","Yao Wang"],"url":"https://arxiv.org/abs/2505.08162"}
{"created":"2025-05-14","title":"Decoding Neighborhood Environments with Large Language Models","abstract":"Neighborhood environments include physical and environmental conditions such as housing quality, roads, and sidewalks, which significantly influence human health and well-being. Traditional methods for assessing these environments, including field surveys and geographic information systems (GIS), are resource-intensive and challenging to evaluate neighborhood environments at scale. Although machine learning offers potential for automated analysis, the laborious process of labeling training data and the lack of accessible models hinder scalability. This study explores the feasibility of large language models (LLMs) such as ChatGPT and Gemini as tools for decoding neighborhood environments (e.g., sidewalk and powerline) at scale. We train a robust YOLOv11-based model, which achieves an average accuracy of 99.13% in detecting six environmental indicators, including streetlight, sidewalk, powerline, apartment, single-lane road, and multilane road. We then evaluate four LLMs, including ChatGPT, Gemini, Claude, and Grok, to assess their feasibility, robustness, and limitations in identifying these indicators, with a focus on the impact of prompting strategies and fine-tuning. We apply majority voting with the top three LLMs to achieve over 88% accuracy, which demonstrates LLMs could be a useful tool to decode the neighborhood environment without any training effort.","authors":["Andrew Cart","Shaohu Zhang","Melanie Escue","Xugui Zhou","Haitao Zhao","Prashanth BusiReddyGari","Beiyu Lin","Shuang Li"],"url":"https://arxiv.org/abs/2505.08163"}
{"created":"2025-05-14","title":"Non-Blocking Robustness Analysis in Discrete Event Systems","abstract":"This paper presents a mathematical framework for characterizing state blocking in discrete event systems (DES) under transition deletions. We introduce a path-based analysis approach that determines whether systems maintain non-blocking properties when transitions are removed. Through formal analysis and case studies, we establish three key contributions: a mathematical characterization of transition-induced blocking with necessary and sufficient conditions, a definition of robust deviations that preserve non-blocking properties, and an algorithm for identifying critical transitions and analyzing system behavior under deletions. Our algorithm reduces computational complexity by leveraging minimal blocking sets, achieving significant reduction in computational requirements. We demonstrate the framework's effectiveness through manufacturing system and autonomous vehicle case studies, showing substantial improvements in identifying critical transitions and predicting potential blocking scenarios across different application domains.","authors":["Md Nur-A-Adam Dony","Satadru Dey"],"url":"https://arxiv.org/abs/2505.08166"}
{"created":"2025-05-14","title":"Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage","abstract":"The rapid development of large language models (LLMs) has provided significant support and opportunities for the advancement of domain-specific LLMs. However, fine-tuning these large models using Intangible Cultural Heritage (ICH) data inevitably faces challenges such as bias, incorrect knowledge inheritance, and catastrophic forgetting. To address these issues, we propose a novel training method that integrates a bidirectional chains of thought and a reward mechanism. This method is built upon ICH-Qwen, a large language model specifically designed for the field of intangible cultural heritage. The proposed method enables the model to not only perform forward reasoning but also enhances the accuracy of the generated answers by utilizing reverse questioning and reverse reasoning to activate the model's latent knowledge. Additionally, a reward mechanism is introduced during training to optimize the decision-making process. This mechanism improves the quality of the model's outputs through structural and content evaluations with different weighting schemes. We conduct comparative experiments on ICH-Qwen, with results demonstrating that our method outperforms 0-shot, step-by-step reasoning, knowledge distillation, and question augmentation methods in terms of accuracy, Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the paper highlights the effectiveness of combining the bidirectional chains of thought and reward mechanism through ablation experiments. In addition, a series of generalizability experiments are conducted, with results showing that the proposed method yields improvements on various domain-specific datasets and advanced models in areas such as Finance, Wikidata, and StrategyQA. This demonstrates that the method is adaptable to multiple domains and provides a valuable approach for model training in future applications across diverse fields.","authors":["Ruilin Liu","Zhixiao Zhao","Jieqiong Li","Chang Liu","Dongbo Wang"],"url":"https://arxiv.org/abs/2505.08167"}
{"created":"2025-05-14","title":"Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph","abstract":"Text-attributed graph (TAG) provides a text description for each graph node, and few- and zero-shot node classification on TAGs have many applications in fields such as academia and social networks. Existing work utilizes various graph-based augmentation techniques to train the node and text embeddings, while text-based augmentations are largely unexplored. In this paper, we propose Text Semantics Augmentation (TSA) to improve accuracy by introducing more text semantic supervision signals. Specifically, we design two augmentation techniques, i.e., positive semantics matching and negative semantics contrast, to provide more reference texts for each graph node or text description. Positive semantic matching retrieves texts with similar embeddings to match with a graph node. Negative semantic contrast adds a negative prompt to construct a text description with the opposite semantics, which is contrasted with the original node and text. We evaluate TSA on 5 datasets and compare with 13 state-of-the-art baselines. The results show that TSA consistently outperforms all baselines, and its accuracy improvements over the best-performing baseline are usually over 5%.","authors":["Yuxiang Wang","Xiao Yan","Shiyu Jin","Quanqing Xu","Chuang Hu","Yuanyuan Zhu","Bo Du","Jia Wu","Jiawei Jiang"],"url":"https://arxiv.org/abs/2505.08168"}
{"created":"2025-05-14","title":"MoKD: Multi-Task Optimization for Knowledge Distillation","abstract":"Compact models can be effectively trained through Knowledge Distillation (KD), a technique that transfers knowledge from larger, high-performing teacher models. Two key challenges in Knowledge Distillation (KD) are: 1) balancing learning from the teacher's guidance and the task objective, and 2) handling the disparity in knowledge representation between teacher and student models. To address these, we propose Multi-Task Optimization for Knowledge Distillation (MoKD). MoKD tackles two main gradient issues: a) Gradient Conflicts, where task-specific and distillation gradients are misaligned, and b) Gradient Dominance, where one objective's gradient dominates, causing imbalance. MoKD reformulates KD as a multi-objective optimization problem, enabling better balance between objectives. Additionally, it introduces a subspace learning framework to project feature representations into a high-dimensional space, improving knowledge transfer. Our MoKD is demonstrated to outperform existing methods through extensive experiments on image classification using the ImageNet-1K dataset and object detection using the COCO dataset, achieving state-of-the-art performance with greater efficiency. To the best of our knowledge, MoKD models also achieve state-of-the-art performance compared to models trained from scratch.","authors":["Zeeshan Hayder","Ali Cheraghian","Lars Petersson","Mehrtash Harandi"],"url":"https://arxiv.org/abs/2505.08170"}
{"created":"2025-05-14","title":"Empowering Vision Transformers with Multi-Scale Causal Intervention for Long-Tailed Image Classification","abstract":"Causal inference has emerged as a promising approach to mitigate long-tail classification by handling the biases introduced by class imbalance. However, along with the change of advanced backbone models from Convolutional Neural Networks (CNNs) to Visual Transformers (ViT), existing causal models may not achieve an expected performance gain. This paper investigates the influence of existing causal models on CNNs and ViT variants, highlighting that ViT's global feature representation makes it hard for causal methods to model associations between fine-grained features and predictions, which leads to difficulties in classifying tail classes with similar visual appearance. To address these issues, this paper proposes TSCNet, a two-stage causal modeling method to discover fine-grained causal associations through multi-scale causal interventions. Specifically, in the hierarchical causal representation learning stage (HCRL), it decouples the background and objects, applying backdoor interventions at both the patch and feature level to prevent model from using class-irrelevant areas to infer labels which enhances fine-grained causal representation. In the counterfactual logits bias calibration stage (CLBC), it refines the optimization of model's decision boundary by adaptive constructing counterfactual balanced data distribution to remove the spurious associations in the logits caused by data distribution. Extensive experiments conducted on various long-tail benchmarks demonstrate that the proposed TSCNet can eliminate multiple biases introduced by data imbalance, which outperforms existing methods.","authors":["Xiaoshuo Yan","Zhaochuan Li","Lei Meng","Zhuang Qi","Wei Wu","Zixuan Li","Xiangxu Meng"],"url":"https://arxiv.org/abs/2505.08173"}
{"created":"2025-05-14","title":"Fast Text-to-Audio Generation with Adversarial Post-Training","abstract":"Text-to-audio systems, while increasingly performant, are slow at inference time, thus making their latency unpractical for many creative applications. We present Adversarial Relativistic-Contrastive (ARC) post-training, the first adversarial acceleration algorithm for diffusion/flow models not based on distillation. While past adversarial post-training methods have struggled to compare against their expensive distillation counterparts, ARC post-training is a simple procedure that (1) extends a recent relativistic adversarial formulation to diffusion/flow post-training and (2) combines it with a novel contrastive discriminator objective to encourage better prompt adherence. We pair ARC post-training with a number optimizations to Stable Audio Open and build a model capable of generating $\\approx$12s of 44.1kHz stereo audio in $\\approx$75ms on an H100, and $\\approx$7s on a mobile edge-device, the fastest text-to-audio model to our knowledge.","authors":["Zachary Novack","Zach Evans","Zack Zukowski","Josiah Taylor","CJ Carr","Julian Parker","Adnan Al-Sinan","Gian Marco Iodice","Julian McAuley","Taylor Berg-Kirkpatrick","Jordi Pons"],"url":"https://arxiv.org/abs/2505.08175"}
{"created":"2025-05-14","title":"Behind the Noise: Conformal Quantile Regression Reveals Emergent Representations","abstract":"Scientific imaging often involves long acquisition times to obtain high-quality data, especially when probing complex, heterogeneous systems. However, reducing acquisition time to increase throughput inevitably introduces significant noise into the measurements. We present a machine learning approach that not only denoises low-quality measurements with calibrated uncertainty bounds, but also reveals emergent structure in the latent space. By using ensembles of lightweight, randomly structured neural networks trained via conformal quantile regression, our method performs reliable denoising while uncovering interpretable spatial and chemical features -- without requiring labels or segmentation. Unlike conventional approaches focused solely on image restoration, our framework leverages the denoising process itself to drive the emergence of meaningful representations. We validate the approach on real-world geobiochemical imaging data, showing how it supports confident interpretation and guides experimental design under resource constraints.","authors":["Petrus H. Zwart","Tamas Varga","Odeta Qafoku","James A. Sethian"],"url":"https://arxiv.org/abs/2505.08176"}
{"created":"2025-05-14","title":"Monocular Depth Guided Occlusion-Aware Disparity Refinement via Semi-supervised Learning in Laparoscopic Images","abstract":"Occlusion and the scarcity of labeled surgical data are significant challenges in disparity estimation for stereo laparoscopic images. To address these issues, this study proposes a Depth Guided Occlusion-Aware Disparity Refinement Network (DGORNet), which refines disparity maps by leveraging monocular depth information unaffected by occlusion. A Position Embedding (PE) module is introduced to provide explicit spatial context, enhancing the network's ability to localize and refine features. Furthermore, we introduce an Optical Flow Difference Loss (OFDLoss) for unlabeled data, leveraging temporal continuity across video frames to improve robustness in dynamic surgical scenes. Experiments on the SCARED dataset demonstrate that DGORNet outperforms state-of-the-art methods in terms of End-Point Error (EPE) and Root Mean Squared Error (RMSE), particularly in occlusion and texture-less regions. Ablation studies confirm the contributions of the Position Embedding and Optical Flow Difference Loss, highlighting their roles in improving spatial and temporal consistency. These results underscore DGORNet's effectiveness in enhancing disparity estimation for laparoscopic surgery, offering a practical solution to challenges in disparity estimation and data limitations.","authors":["Ziteng Liu","Dongdong He","Chenghong Zhang","Wenpeng Gao","Yili Fu"],"url":"https://arxiv.org/abs/2505.08178"}
{"created":"2025-05-14","title":"Feasibility-Aware Pessimistic Estimation: Toward Long-Horizon Safety in Offline RL","abstract":"Offline safe reinforcement learning(OSRL) derives constraint-satisfying policies from pre-collected datasets, offers a promising avenue for deploying RL in safety-critical real-world domains such as robotics. However, the majority of existing approaches emphasize only short-term safety, neglecting long-horizon considerations. Consequently, they may violate safety constraints and fail to ensure sustained protection during online deployment. Moreover, the learned policies often struggle to handle states and actions that are not present or out-of-distribution(OOD) from the offline dataset, and exhibit limited sample efficiency. To address these challenges, we propose a novel framework Feasibility-Aware offline Safe Reinforcement Learning with CVAE-based Pessimism (FASP). First, we employ Hamilton-Jacobi (H-J) reachability analysis to generate reliable safety labels, which serve as supervisory signals for training both a conditional variational autoencoder (CVAE) and a safety classifier. This approach not only ensures high sampling efficiency but also provides rigorous long-horizon safety guarantees. Furthermore, we utilize pessimistic estimation methods to estimate the Q-value of reward and cost, which mitigates the extrapolation errors induces by OOD actions, and penalize unsafe actions to enabled the agent to proactively avoid high-risk behaviors. Moreover, we theoretically prove the validity of this pessimistic estimation. Extensive experiments on DSRL benchmarks demonstrate that FASP algorithm achieves competitive performance across multiple experimental tasks, particularly outperforming state-of-the-art algorithms in terms of safety.","authors":["Zhikun Tao","Gang Xiong","He Fang","Zhen Shen","Yunjun Han","Qing-Shan Jia"],"url":"https://arxiv.org/abs/2505.08179"}
{"created":"2025-05-14","title":"Semantic De-boosting in e-commerce Query Autocomplete","abstract":"In ecommerce search, query autocomplete plays a critical role to help users in their shopping journey. Often times, query autocomplete presents users with semantically similar queries, which can impede the user's ability to find diverse and relevant results. This paper proposes a novel strategy to enhance this service by refining the presentation of typeahead suggestions based on their semantic similarity.","authors":["Adithya Rajan","Weiqi Tong","Greg Sharp","Prateek Verma","Kevin Li"],"url":"https://arxiv.org/abs/2505.08182"}
{"created":"2025-05-14","title":"DSADF: Thinking Fast and Slow for Decision Making","abstract":"Although Reinforcement Learning (RL) agents are effective in well-defined environments, they often struggle to generalize their learned policies to dynamic settings due to their reliance on trial-and-error interactions. Recent work has explored applying Large Language Models (LLMs) or Vision Language Models (VLMs) to boost the generalization of RL agents through policy optimization guidance or prior knowledge. However, these approaches often lack seamless coordination between the RL agent and the foundation model, leading to unreasonable decision-making in unfamiliar environments and efficiency bottlenecks. Making full use of the inferential capabilities of foundation models and the rapid response capabilities of RL agents and enhancing the interaction between the two to form a dual system is still a lingering scientific question. To address this problem, we draw inspiration from Kahneman's theory of fast thinking (System 1) and slow thinking (System 2), demonstrating that balancing intuition and deep reasoning can achieve nimble decision-making in a complex world. In this study, we propose a Dual-System Adaptive Decision Framework (DSADF), integrating two complementary modules: System 1, comprising an RL agent and a memory space for fast and intuitive decision making, and System 2, driven by a VLM for deep and analytical reasoning. DSADF facilitates efficient and adaptive decision-making by combining the strengths of both systems. The empirical study in the video game environment: Crafter and Housekeep demonstrates the effectiveness of our proposed method, showing significant improvements in decision abilities for both unseen and known tasks.","authors":["Alex Zhihao Dou","Dongfei Cui","Jun Yan","Weida Wang","Benteng Chen","Haoming Wang","Zeke Xie","Shufei Zhang"],"url":"https://arxiv.org/abs/2505.08189"}
{"created":"2025-05-14","title":"Unsupervised Raindrop Removal from a Single Image using Conditional Diffusion Models","abstract":"Raindrop removal is a challenging task in image processing. Removing raindrops while relying solely on a single image further increases the difficulty of the task. Common approaches include the detection of raindrop regions in the image, followed by performing a background restoration process conditioned on those regions. While various methods can be applied for the detection step, the most common architecture used for background restoration is the Generative Adversarial Network (GAN). Recent advances in the use of diffusion models have led to state-of-the-art image inpainting techniques. In this paper, we introduce a novel technique for raindrop removal from a single image using diffusion-based image inpainting.","authors":["Lhuqita Fazry","Valentino Vito"],"url":"https://arxiv.org/abs/2505.08190"}
{"created":"2025-05-14","title":"SpNeRF: Memory Efficient Sparse Volumetric Neural Rendering Accelerator for Edge Devices","abstract":"Neural rendering has gained prominence for its high-quality output, which is crucial for AR/VR applications. However, its large voxel grid data size and irregular access patterns challenge real-time processing on edge devices. While previous works have focused on improving data locality, they have not adequately addressed the issue of large voxel grid sizes, which necessitate frequent off-chip memory access and substantial on-chip memory. This paper introduces SpNeRF, a software-hardware co-design solution tailored for sparse volumetric neural rendering. We first identify memory-bound rendering inefficiencies and analyze the inherent sparsity in the voxel grid data of neural rendering. To enhance efficiency, we propose novel preprocessing and online decoding steps, reducing the memory size for voxel grid. The preprocessing step employs hash mapping to support irregular data access while maintaining a minimal memory size. The online decoding step enables efficient on-chip sparse voxel grid processing, incorporating bitmap masking to mitigate PSNR loss caused by hash collisions. To further optimize performance, we design a dedicated hardware architecture supporting our sparse voxel grid processing technique. Experimental results demonstrate that SpNeRF achieves an average 21.07$\\times$ reduction in memory size while maintaining comparable PSNR levels. When benchmarked against Jetson XNX, Jetson ONX, RT-NeRF.Edge and NeuRex.Edge, our design achieves speedups of 95.1$\\times$, 63.5$\\times$, 1.5$\\times$ and 10.3$\\times$, and improves energy efficiency by 625.6$\\times$, 529.1$\\times$, 4$\\times$, and 4.4$\\times$, respectively.","authors":["Yipu Zhang","Jiawei Liang","Jian Peng","Jiang Xu","Wei Zhang"],"url":"https://arxiv.org/abs/2505.08191"}
{"created":"2025-05-14","title":"A Tightly Coupled IMU-Based Motion Capture Approach for Estimating Multibody Kinematics and Kinetics","abstract":"Inertial Measurement Units (IMUs) enable portable, multibody motion capture (MoCap) in diverse environments beyond the laboratory, making them a practical choice for diagnosing mobility disorders and supporting rehabilitation in clinical or home settings. However, challenges associated with IMU measurements, including magnetic distortions and drift errors, complicate their broader use for MoCap. In this work, we propose a tightly coupled motion capture approach that directly integrates IMU measurements with multibody dynamic models via an Iterated Extended Kalman Filter (IEKF) to simultaneously estimate the system's kinematics and kinetics. By enforcing kinematic and kinetic properties and utilizing only accelerometer and gyroscope data, our method improves IMU-based state estimation accuracy. Our approach is designed to allow for incorporating additional sensor data, such as optical MoCap measurements and joint torque readings, to further enhance estimation accuracy. We validated our approach using highly accurate ground truth data from a 3 Degree of Freedom (DoF) pendulum and a 6 DoF Kuka robot. We demonstrate a maximum Root Mean Square Difference (RMSD) in the pendulum's computed joint angles of 3.75 degrees compared to optical MoCap Inverse Kinematics (IK), which serves as the gold standard in the absence of internal encoders. For the Kuka robot, we observe a maximum joint angle RMSD of 3.24 degrees compared to the Kuka's internal encoders, while the maximum joint angle RMSD of the optical MoCap IK compared to the encoders was 1.16 degrees. Additionally, we report a maximum joint torque RMSD of 2 Nm in the pendulum compared to optical MoCap Inverse Dynamics (ID), and 3.73 Nm in the Kuka robot relative to its internal torque sensors.","authors":["Hassan Osman","Daan de Kanter","Jelle Boelens","Manon Kok","Ajay Seth"],"url":"https://arxiv.org/abs/2505.08193"}
{"created":"2025-05-14","title":"CLTP: Contrastive Language-Tactile Pre-training for 3D Contact Geometry Understanding","abstract":"Recent advancements in integrating tactile sensing with vision-language models (VLMs) have demonstrated remarkable potential for robotic multimodal perception. However, existing tactile descriptions remain limited to superficial attributes like texture, neglecting critical contact states essential for robotic manipulation. To bridge this gap, we propose CLTP, an intuitive and effective language tactile pretraining framework that aligns tactile 3D point clouds with natural language in various contact scenarios, thus enabling contact-state-aware tactile language understanding for contact-rich manipulation tasks. We first collect a novel dataset of 50k+ tactile 3D point cloud-language pairs, where descriptions explicitly capture multidimensional contact states (e.g., contact location, shape, and force) from the tactile sensor's perspective. CLTP leverages a pre-aligned and frozen vision-language feature space to bridge holistic textual and tactile modalities. Experiments validate its superiority in three downstream tasks: zero-shot 3D classification, contact state classification, and tactile 3D large language model (LLM) interaction. To the best of our knowledge, this is the first study to align tactile and language representations from the contact state perspective for manipulation tasks, providing great potential for tactile-language-action model learning. Code and datasets are open-sourced at https://sites.google.com/view/cltp/.","authors":["Wenxuan Ma","Xiaoge Cao","Yixiang Zhang","Chaofan Zhang","Shaobo Yang","Peng Hao","Bin Fang","Yinghao Cai","Shaowei Cui","Shuo Wang"],"url":"https://arxiv.org/abs/2505.08194"}
{"created":"2025-05-14","title":"ADC-GS: Anchor-Driven Deformable and Compressed Gaussian Splatting for Dynamic Scene Reconstruction","abstract":"Existing 4D Gaussian Splatting methods rely on per-Gaussian deformation from a canonical space to target frames, which overlooks redundancy among adjacent Gaussian primitives and results in suboptimal performance. To address this limitation, we propose Anchor-Driven Deformable and Compressed Gaussian Splatting (ADC-GS), a compact and efficient representation for dynamic scene reconstruction. Specifically, ADC-GS organizes Gaussian primitives into an anchor-based structure within the canonical space, enhanced by a temporal significance-based anchor refinement strategy. To reduce deformation redundancy, ADC-GS introduces a hierarchical coarse-to-fine pipeline that captures motions at varying granularities. Moreover, a rate-distortion optimization is adopted to achieve an optimal balance between bitrate consumption and representation fidelity. Experimental results demonstrate that ADC-GS outperforms the per-Gaussian deformation approaches in rendering speed by 300%-800% while achieving state-of-the-art storage efficiency without compromising rendering quality. The code is released at https://github.com/H-Huang774/ADC-GS.git.","authors":["He Huang","Qi Yang","Mufan Liu","Yiling Xu","Zhu Li"],"url":"https://arxiv.org/abs/2505.08196"}
{"created":"2025-05-14","title":"Visual Watermarking in the Era of Diffusion Models: Advances and Challenges","abstract":"As generative artificial intelligence technologies like Stable Diffusion advance, visual content becomes more vulnerable to misuse, raising concerns about copyright infringement. Visual watermarks serve as effective protection mechanisms, asserting ownership and deterring unauthorized use. Traditional deepfake detection methods often rely on passive techniques that struggle with sophisticated manipulations. In contrast, diffusion models enhance detection accuracy by allowing for the effective learning of features, enabling the embedding of imperceptible and robust watermarks. We analyze the strengths and challenges of watermark techniques related to diffusion models, focusing on their robustness and application in watermark generation. By exploring the integration of advanced diffusion models and watermarking security, we aim to advance the discourse on preserving watermark robustness against evolving forgery threats. It emphasizes the critical importance of developing innovative solutions to protect digital content and ensure the preservation of ownership rights in the era of generative AI.","authors":["Junxian Duan","Jiyang Guang","Wenkui Yang","Ran He"],"url":"https://arxiv.org/abs/2505.08197"}
{"created":"2025-05-14","title":"A Multi-scale Representation Learning Framework for Long-Term Time Series Forecasting","abstract":"Long-term time series forecasting (LTSF) offers broad utility in practical settings like energy consumption and weather prediction. Accurately predicting long-term changes, however, is demanding due to the intricate temporal patterns and inherent multi-scale variations within time series. This work confronts key issues in LTSF, including the suboptimal use of multi-granularity information, the neglect of channel-specific attributes, and the unique nature of trend and seasonal components, by introducing a proficient MLP-based forecasting framework. Our method adeptly disentangles complex temporal dynamics using clear, concurrent predictions across various scales. These multi-scale forecasts are then skillfully integrated through a system that dynamically assigns importance to information from different granularities, sensitive to individual channel characteristics. To manage the specific features of temporal patterns, a two-pronged structure is utilized to model trend and seasonal elements independently. Experimental results on eight LTSF benchmarks demonstrate that MDMixer improves average MAE performance by 4.64% compared to the recent state-of-the-art MLP-based method (TimeMixer), while achieving an effective balance between training efficiency and model interpretability.","authors":["Boshi Gao","Qingjian Ni","Fanbo Ju","Yu Chen","Ziqi Zhao"],"url":"https://arxiv.org/abs/2505.08199"}
{"created":"2025-05-14","title":"A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs","abstract":"Large Language Models (LLMs) have the tendency to hallucinate, i.e., to sporadically generate false or fabricated information. This presents a major challenge, as hallucinations often appear highly convincing and users generally lack the tools to detect them. Uncertainty quantification (UQ) provides a framework for assessing the reliability of model outputs, aiding in the identification of potential hallucinations. In this work, we introduce pre-trained UQ heads: supervised auxiliary modules for LLMs that substantially enhance their ability to capture uncertainty compared to unsupervised UQ methods. Their strong performance stems from the powerful Transformer architecture in their design and informative features derived from LLM attention maps. Experimental evaluation shows that these heads are highly robust and achieve state-of-the-art performance in claim-level hallucination detection across both in-domain and out-of-domain prompts. Moreover, these modules demonstrate strong generalization to languages they were not explicitly trained on. We pre-train a collection of UQ heads for popular LLM series, including Mistral, Llama, and Gemma 2. We publicly release both the code and the pre-trained heads.","authors":["Artem Shelmanov","Ekaterina Fadeeva","Akim Tsvigun","Ivan Tsvigun","Zhuohan Xie","Igor Kiselev","Nico Daheim","Caiqi Zhang","Artem Vazhentsev","Mrinmaya Sachan","Preslav Nakov","Timothy Baldwin"],"url":"https://arxiv.org/abs/2505.08200"}
{"created":"2025-05-14","title":"AI and Generative AI Transforming Disaster Management: A Survey of Damage Assessment and Response Techniques","abstract":"Natural disasters, including earthquakes, wildfires and cyclones, bear a huge risk on human lives as well as infrastructure assets. An effective response to disaster depends on the ability to rapidly and efficiently assess the intensity of damage. Artificial Intelligence (AI) and Generative Artificial Intelligence (GenAI) presents a breakthrough solution, capable of combining knowledge from multiple types and sources of data, simulating realistic scenarios of disaster, and identifying emerging trends at a speed previously unimaginable. In this paper, we present a comprehensive review on the prospects of AI and GenAI in damage assessment for various natural disasters, highlighting both its strengths and limitations. We talk about its application to multimodal data such as text, image, video, and audio, and also cover major issues of data privacy, security, and ethical use of the technology during crises. The paper also recognizes the threat of Generative AI misuse, in the form of dissemination of misinformation and for adversarial attacks. Finally, we outline avenues of future research, emphasizing the need for secure, reliable, and ethical Generative AI systems for disaster management in general. We believe that this work represents the first comprehensive survey of Gen-AI techniques being used in the field of Disaster Assessment and Response.","authors":["Aman Raj","Lakshit Arora","Sanjay Surendranath Girija","Shashank Kapoor","Dipen Pradhan","Ankit Shetgaonkar"],"url":"https://arxiv.org/abs/2505.08202"}
{"created":"2025-05-14","title":"Not that Groove: Zero-Shot Symbolic Music Editing","abstract":"Most work in AI music generation focused on audio, which has seen limited use in the music production industry due to its rigidity. To maximize flexibility while assuming only textual instructions from producers, we are among the first to tackle symbolic music editing. We circumvent the known challenge of lack of labeled data by proving that LLMs with zero-shot prompting can effectively edit drum grooves. The recipe of success is a creatively designed format that interfaces LLMs and music, while we facilitate evaluation by providing an evaluation dataset with annotated unit tests that highly aligns with musicians' judgment.","authors":["Li Zhang"],"url":"https://arxiv.org/abs/2505.08203"}
{"created":"2025-05-14","title":"LM-Scout: Analyzing the Security of Language Model Integration in Android Apps","abstract":"Developers are increasingly integrating Language Models (LMs) into their mobile apps to provide features such as chat-based assistants. To prevent LM misuse, they impose various restrictions, including limits on the number of queries, input length, and allowed topics. However, if the LM integration is insecure, attackers can bypass these restrictions and gain unrestricted access to the LM, potentially harming developers' reputations and leading to significant financial losses.","authors":["Muhammad Ibrahim (Georgia Institute of Technology)","G\\H{u}liz Seray Tuncay (Google)","Z. Berkay Celik (Purdue University)","Aravind Machiry (Purdue University)","Antonio Bianchi (Purdue University)"],"url":"https://arxiv.org/abs/2505.08204"}
{"created":"2025-05-14","title":"ABAC Lab: An Interactive Platform for Attribute-based Access Control Policy Analysis, Tools, and Datasets","abstract":"Attribute-Based Access Control (ABAC) provides expressiveness and flexibility, making it a compelling model for enforcing fine-grained access control policies. To facilitate the transition to ABAC, extensive research has been conducted to develop methodologies, frameworks, and tools that assist policy administrators in adapting the model. Despite these efforts, challenges remain in the availability and benchmarking of ABAC datasets. Specifically, there is a lack of clarity on how datasets can be systematically acquired, no standardized benchmarking practices to evaluate existing methodologies and their effectiveness, and limited access to real-world datasets suitable for policy analysis and testing.","authors":["Thang Bui","Anthony Matricia","Emily Contreras","Ryan Mauvais","Luis Medina","Israel Serrano"],"url":"https://arxiv.org/abs/2505.08209"}
{"created":"2025-05-14","title":"An Effective Flow-based Method for Positive-Unlabeled Learning: 2-HNC","abstract":"In many scenarios of binary classification, only positive instances are provided in the training data, leaving the rest of the data unlabeled. This setup, known as positive-unlabeled (PU) learning, is addressed here with a network flow-based method which utilizes pairwise similarities between samples. The method we propose here, 2-HNC, leverages Hochbaum's Normalized Cut (HNC) and the set of solutions it provides by solving a parametric minimum cut problem. The set of solutions, that are nested partitions of the samples into two sets, correspond to varying tradeoff values between the two goals: high intra-similarity inside the sets and low inter-similarity between the two sets. This nested sequence is utilized here to deliver a ranking of unlabeled samples by their likelihood of being negative. Building on this insight, our method, 2-HNC, proceeds in two stages. The first stage generates this ranking without assuming any negative labels, using a problem formulation that is constrained only on positive labeled samples. The second stage augments the positive set with likely-negative samples and recomputes the classification. The final label prediction selects among all generated partitions in both stages, the one that delivers a positive class proportion, closest to a prior estimate of this quantity, which is assumed to be given. Extensive experiments across synthetic and real datasets show that 2-HNC yields strong performance and often surpasses existing state-of-the-art algorithms.","authors":["Dorit Hochbaum","Torpong Nitayanont"],"url":"https://arxiv.org/abs/2505.08212"}
{"created":"2025-05-14","title":"HandCept: A Visual-Inertial Fusion Framework for Accurate Proprioception in Dexterous Hands","abstract":"As robotics progresses toward general manipulation, dexterous hands are becoming increasingly critical. However, proprioception in dexterous hands remains a bottleneck due to limitations in volume and generality. In this work, we present HandCept, a novel visual-inertial proprioception framework designed to overcome the challenges of traditional joint angle estimation methods. HandCept addresses the difficulty of achieving accurate and robust joint angle estimation in dynamic environments where both visual and inertial measurements are prone to noise and drift. It leverages a zero-shot learning approach using a wrist-mounted RGB-D camera and 9-axis IMUs, fused in real time via a latency-free Extended Kalman Filter (EKF). Our results show that HandCept achieves joint angle estimation errors between $2^{\\circ}$ and $4^{\\circ}$ without observable drift, outperforming visual-only and inertial-only methods. Furthermore, we validate the stability and uniformity of the IMU system, demonstrating that a common base frame across IMUs simplifies system calibration. To support sim-to-real transfer, we also open-sourced our high-fidelity rendering pipeline, which is essential for training without real-world ground truth. This work offers a robust, generalizable solution for proprioception in dexterous hands, with significant implications for robotic manipulation and human-robot interaction.","authors":["Junda Huang","Jianshu Zhou","Honghao Guo","Yunhui Liu"],"url":"https://arxiv.org/abs/2505.08213"}
{"created":"2025-05-14","title":"Adaptive and hybrid reduced order models to mitigate Kolmogorov barrier in a multiscale kinetic transport equation","abstract":"In this work, we develop reduced order models (ROMs) to predict solutions to a multiscale kinetic transport equation with a diffusion limit under the parametric setting. When the underlying scattering effect is not sufficiently strong, the system governed by this equation exhibits transport-dominated behavior. Suffering from the Kolmogorov barrier for transport-dominant problems, classical linear ROMs may become inefficient in this regime. To address this issue, we first develop a piecewise linear ROM by introducing a novel goal-oriented adaptive time partitioning strategy. To avoid local over-refinement or under-refinement, we propose an adaptive coarsening and refinement strategy that remains robust with various initial empirical partitions. Additionally, for problems where a local linear approximation is not sufficiently efficient, we further develop a hybrid ROM, which combines autoencoder-based nonlinear ROMs and piecewise linear ROMs. Compared to previous autoencoder-based ROMs, this hybridized method reduces the offline autoencoder's training cost by only applying it to time intervals that are adaptively identified as the most challenging. Numerical experiments demonstrate that our proposed approaches successfully predict full-order solutions at unseen parameter values with both efficiency and accuracy. To the best of our knowledge, this is the first attempt to address the Kolmogorov barrier for multiscale kinetic transport problems with the coexistence of both transport- and diffusion-dominant behaviors.","authors":["Tianyu Jin","Zhichao Peng","Yang Xiang"],"url":"https://arxiv.org/abs/2505.08214"}
{"created":"2025-05-14","title":"Unveiling the Best Practices for Applying Speech Foundation Models to Speech Intelligibility Prediction for Hearing-Impaired People","abstract":"Speech foundation models (SFMs) have demonstrated strong performance across a variety of downstream tasks, including speech intelligibility prediction for hearing-impaired people (SIP-HI). However, optimizing SFMs for SIP-HI has been insufficiently explored. In this paper, we conduct a comprehensive study to identify key design factors affecting SIP-HI performance with 5 SFMs, focusing on encoder layer selection, prediction head architecture, and ensemble configurations. Our findings show that, contrary to traditional use-all-layers methods, selecting a single encoder layer yields better results. Additionally, temporal modeling is crucial for effective prediction heads. We also demonstrate that ensembling multiple SFMs improves performance, with stronger individual models providing greater benefit. Finally, we explore the relationship between key SFM attributes and their impact on SIP-HI performance. Our study offers practical insights into effectively adapting SFMs for speech intelligibility prediction for hearing-impaired populations.","authors":["Haoshuai Zhou","Boxuan Cao","Changgeng Mo","Linkai Li","Shan Xiang Wang"],"url":"https://arxiv.org/abs/2505.08215"}
{"created":"2025-05-14","title":"Rethink Repeatable Measures of Robot Performance with Statistical Query","abstract":"For a general standardized testing algorithm designed to evaluate a specific aspect of a robot's performance, several key expectations are commonly imposed. Beyond accuracy (i.e., closeness to a typically unknown ground-truth reference) and efficiency (i.e., feasibility within acceptable testing costs and equipment constraints), one particularly important attribute is repeatability. Repeatability refers to the ability to consistently obtain the same testing outcome when similar testing algorithms are executed on the same subject robot by different stakeholders, across different times or locations. However, achieving repeatable testing has become increasingly challenging as the components involved grow more complex, intelligent, diverse, and, most importantly, stochastic. While related efforts have addressed repeatability at ethical, hardware, and procedural levels, this study focuses specifically on repeatable testing at the algorithmic level. Specifically, we target the well-adopted class of testing algorithms in standardized evaluation: statistical query (SQ) algorithms (i.e., algorithms that estimate the expected value of a bounded function over a distribution using sampled data). We propose a lightweight, parameterized, and adaptive modification applicable to any SQ routine, whether based on Monte Carlo sampling, importance sampling, or adaptive importance sampling, that makes it provably repeatable, with guaranteed bounds on both accuracy and efficiency. We demonstrate the effectiveness of the proposed approach across three representative scenarios: (i) established and widely adopted standardized testing of manipulators, (ii) emerging intelligent testing algorithms for operational risk assessment in automated vehicles, and (iii) developing use cases involving command tracking performance evaluation of humanoid robots in locomotion tasks.","authors":["Bowen Weng","Linda Capito","Guillermo A. Castillo","Dylan Khor"],"url":"https://arxiv.org/abs/2505.08216"}
{"created":"2025-05-14","title":"Local Convergence Behavior of Extended LOBPCG for Computing Eigenvalues of Hermitian Matrices","abstract":"This paper provides a comprehensive and detailed analysis of the local convergence behavior of an extended variation of the locally optimal preconditioned conjugate gradient method (LOBPCG) for computing the extreme eigenvalue of a Hermitian matrix. The convergence rates derived in this work are either obtained for the first time or sharper than those previously established, including those in Ovtchinnikov's work ({\\em SIAM J. Numer. Anal.}, 46(5):2567--2592, 2008). The study also extends to generalized problems, including Hermitian matrix polynomials that admit an extended form of the Rayleigh quotient. The new approach used to obtain these rates may also serve as a valuable tool for the convergence analysis of other gradient-type optimization methods.","authors":["Zhechen Shen","Xin Liang"],"url":"https://arxiv.org/abs/2505.08218"}
{"created":"2025-05-14","title":"Deep Probabilistic Modeling of User Behavior for Anomaly Detection via Mixture Density Networks","abstract":"To improve the identification of potential anomaly patterns in complex user behavior, this paper proposes an anomaly detection method based on a deep mixture density network. The method constructs a Gaussian mixture model parameterized by a neural network, enabling conditional probability modeling of user behavior. It effectively captures the multimodal distribution characteristics commonly present in behavioral data. Unlike traditional classifiers that rely on fixed thresholds or a single decision boundary, this approach defines an anomaly scoring function based on probability density using negative log-likelihood. This significantly enhances the model's ability to detect rare and unstructured behaviors. Experiments are conducted on the real-world network user dataset UNSW-NB15. A series of performance comparisons and stability validation experiments are designed. These cover multiple evaluation aspects, including Accuracy, F1- score, AUC, and loss fluctuation. The results show that the proposed method outperforms several advanced neural network architectures in both performance and training stability. This study provides a more expressive and discriminative solution for user behavior modeling and anomaly detection. It strongly promotes the application of deep probabilistic modeling techniques in the fields of network security and intelligent risk control.","authors":["Lu Dai","Wenxuan Zhu","Xuehui Quan","Renzi Meng","Sheng Cai","Yichen Wang"],"url":"https://arxiv.org/abs/2505.08220"}
{"created":"2025-05-14","title":"Scaling Multi Agent Reinforcement Learning for Underwater Acoustic Tracking via Autonomous Vehicles","abstract":"Autonomous vehicles (AV) offer a cost-effective solution for scientific missions such as underwater tracking. Recently, reinforcement learning (RL) has emerged as a powerful method for controlling AVs in complex marine environments. However, scaling these techniques to a fleet--essential for multi-target tracking or targets with rapid, unpredictable motion--presents significant computational challenges. Multi-Agent Reinforcement Learning (MARL) is notoriously sample-inefficient, and while high-fidelity simulators like Gazebo's LRAUV provide 100x faster-than-real-time single-robot simulations, they offer no significant speedup for multi-vehicle scenarios, making MARL training impractical. To address these limitations, we propose an iterative distillation method that transfers high-fidelity simulations into a simplified, GPU-accelerated environment while preserving high-level dynamics. This approach achieves up to a 30,000x speedup over Gazebo through parallelization, enabling efficient training via end-to-end GPU acceleration. Additionally, we introduce a novel Transformer-based architecture (TransfMAPPO) that learns multi-agent policies invariant to the number of agents and targets, significantly improving sample efficiency. Following large-scale curriculum learning conducted entirely on GPU, we perform extensive evaluations in Gazebo, demonstrating that our method maintains tracking errors below 5 meters over extended durations, even in the presence of multiple fast-moving targets. This work bridges the gap between large-scale MARL training and high-fidelity deployment, providing a scalable framework for autonomous fleet control in real-world sea missions.","authors":["Matteo Gallici","Ivan Masmitja","Mario Mart\\'in"],"url":"https://arxiv.org/abs/2505.08222"}
{"created":"2025-05-14","title":"Reinforcement Learning-based Fault-Tolerant Control for Quadrotor with Online Transformer Adaptation","abstract":"Multirotors play a significant role in diverse field robotics applications but remain highly susceptible to actuator failures, leading to rapid instability and compromised mission reliability. While various fault-tolerant control (FTC) strategies using reinforcement learning (RL) have been widely explored, most previous approaches require prior knowledge of the multirotor model or struggle to adapt to new configurations. To address these limitations, we propose a novel hybrid RL-based FTC framework integrated with a transformer-based online adaptation module. Our framework leverages a transformer architecture to infer latent representations in real time, enabling adaptation to previously unseen system models without retraining. We evaluate our method in a PyBullet simulation under loss-of-effectiveness actuator faults, achieving a 95% success rate and a positional root mean square error (RMSE) of 0.129 m, outperforming existing adaptation methods with 86% success and an RMSE of 0.153 m. Further evaluations on quadrotors with varying configurations confirm the robustness of our framework across untrained dynamics. These results demonstrate the potential of our framework to enhance the adaptability and reliability of multirotors, enabling efficient fault management in dynamic and uncertain environments. Website is available at http://00dhkim.me/paper/rl-ftc","authors":["Dohyun Kim","Jayden Dongwoo Lee","Hyochoong Bang","Jungho Bae"],"url":"https://arxiv.org/abs/2505.08223"}
{"created":"2025-05-14","title":"Object detection in adverse weather conditions for autonomous vehicles using Instruct Pix2Pix","abstract":"Enhancing the robustness of object detection systems under adverse weather conditions is crucial for the advancement of autonomous driving technology. This study presents a novel approach leveraging the diffusion model Instruct Pix2Pix to develop prompting methodologies that generate realistic datasets with weather-based augmentations aiming to mitigate the impact of adverse weather on the perception capabilities of state-of-the-art object detection models, including Faster R-CNN and YOLOv10. Experiments were conducted in two environments, in the CARLA simulator where an initial evaluation of the proposed data augmentation was provided, and then on the real-world image data sets BDD100K and ACDC demonstrating the effectiveness of the approach in real environments.","authors":["Unai Gurbindo","Axel Brando","Jaume Abella","Caroline K\\\"onig"],"url":"https://arxiv.org/abs/2505.08228"}
{"created":"2025-05-14","title":"Constrained Factor Graph Optimization for Robust Networked Pedestrian Inertial Navigation","abstract":"This paper presents a novel constrained Factor Graph Optimization (FGO)-based approach for networked inertial navigation in pedestrian localization. To effectively mitigate the drift inherent in inertial navigation solutions, we incorporate kinematic constraints directly into the nonlinear optimization framework. Specifically, we utilize equality constraints, such as Zero-Velocity Updates (ZUPTs), and inequality constraints representing the maximum allowable distance between body-mounted Inertial Measurement Units (IMUs) based on human anatomical limitations. While equality constraints are straightforwardly integrated as error factors, inequality constraints cannot be explicitly represented in standard FGO formulations. To address this, we introduce a differentiable softmax-based penalty term in the FGO cost function to enforce inequality constraints smoothly and robustly. The proposed constrained FGO approach leverages temporal correlations across multiple epochs, resulting in optimal state trajectory estimates while consistently maintaining constraint satisfaction. Experimental results confirm that our method outperforms conventional Kalman filter approaches, demonstrating its effectiveness and robustness for pedestrian navigation.","authors":["Yingjie Hu","Wang Hu"],"url":"https://arxiv.org/abs/2505.08229"}
{"created":"2025-05-14","title":"SKiD-SLAM: Robust, Lightweight, and Distributed Multi-Robot LiDAR SLAM in Resource-Constrained Field Environments","abstract":"Distributed LiDAR SLAM is crucial for achieving efficient robot autonomy and improving the scalability of mapping. However, two issues need to be considered when applying it in field environments: one is resource limitation, and the other is inter/intra-robot association. The resource limitation issue arises when the data size exceeds the processing capacity of the network or memory, especially when utilizing communication systems or onboard computers in the field. The inter/intra-robot association issue occurs due to the narrow convergence region of ICP under large viewpoint differences, triggering many false positive loops and ultimately resulting in an inconsistent global map for multi-robot systems. To tackle these problems, we propose a distributed LiDAR SLAM framework designed for versatile field applications, called SKiD-SLAM. Extending our previous work that solely focused on lightweight place recognition and fast and robust global registration, we present a multi-robot mapping framework that focuses on robust and lightweight inter-robot loop closure in distributed LiDAR SLAM. Through various environmental experiments, we demonstrate that our method is more robust and lightweight compared to other state-of-the-art distributed SLAM approaches, overcoming resource limitation and inter/intra-robot association issues. Also, we validated the field applicability of our approach through mapping experiments in real-world planetary emulation terrain and cave environments, which are in-house datasets. Our code will be available at https://sparolab.github.io/research/skid_slam/.","authors":["Hogyun Kim","Jiwon Choi","Juwon Kim","Geonmo Yang","Dongjin Cho","Hyungtae Lim","Younggun Cho"],"url":"https://arxiv.org/abs/2505.08230"}
{"created":"2025-05-14","title":"HMPNet: A Feature Aggregation Architecture for Maritime Object Detection from a Shipborne Perspective","abstract":"In the realm of intelligent maritime navigation, object detection from a shipborne perspective is paramount. Despite the criticality, the paucity of maritime-specific data impedes the deployment of sophisticated visual perception techniques, akin to those utilized in autonomous vehicular systems, within the maritime context. To bridge this gap, we introduce Navigation12, a novel dataset annotated for 12 object categories under diverse maritime environments and weather conditions. Based upon this dataset, we propose HMPNet, a lightweight architecture tailored for shipborne object detection. HMPNet incorporates a hierarchical dynamic modulation backbone to bolster feature aggregation and expression, complemented by a matrix cascading poly-scale neck and a polymerization weight sharing detector, facilitating efficient multi-scale feature aggregation. Empirical evaluations indicate that HMPNet surpasses current state-of-the-art methods in terms of both accuracy and computational efficiency, realizing a 3.3% improvement in mean Average Precision over YOLOv11n, the prevailing model, and reducing parameters by 23%.","authors":["Yu Zhang","Fengyuan Liu","Juan Lyu","Yi Wei","Changdong Yu"],"url":"https://arxiv.org/abs/2505.08231"}
{"created":"2025-05-14","title":"G-MSGINet: A Grouped Multi-Scale Graph-Involution Network for Contactless Fingerprint Recognition","abstract":"This paper presents G-MSGINet, a unified and efficient framework for robust contactless fingerprint recognition that jointly performs minutiae localization and identity embedding directly from raw input images. Existing approaches rely on multi-branch architectures, orientation labels, or complex preprocessing steps, which limit scalability and generalization across real-world acquisition scenarios. In contrast, the proposed architecture introduces the GMSGI layer, a novel computational module that integrates grouped pixel-level involution, dynamic multi-scale kernel generation, and graph-based relational modelling into a single processing unit. Stacked GMSGI layers progressively refine both local minutiae-sensitive features and global topological representations through end-to-end optimization. The architecture eliminates explicit orientation supervision and adapts graph connectivity directly from learned kernel descriptors, thereby capturing meaningful structural relationships among fingerprint regions without fixed heuristics. Extensive experiments on three benchmark datasets, namely PolyU, CFPose, and Benchmark 2D/3D, demonstrate that G-MSGINet consistently achieves minutiae F1-scores in the range of $0.83\\pm0.02$ and Rank-1 identification accuracies between 97.0% and 99.1%, while maintaining an Equal Error Rate (EER) as low as 0.5%. These results correspond to improvements of up to 4.8% in F1-score and 1.4% in Rank-1 accuracy when compared to prior methods, using only 0.38 million parameters and 6.63 giga floating-point operations, which represents up to ten times fewer parameters than competitive baselines. This highlights the scalability and effectiveness of G-MSGINet in real-world contactless biometric recognition scenarios.","authors":["Santhoshkumar Peddi","Soham Bandyopadhyay","Debasis Samanta"],"url":"https://arxiv.org/abs/2505.08233"}
{"created":"2025-05-14","title":"Removing Watermarks with Partial Regeneration using Semantic Information","abstract":"As AI-generated imagery becomes ubiquitous, invisible watermarks have emerged as a primary line of defense for copyright and provenance. The newest watermarking schemes embed semantic signals - content-aware patterns that are designed to survive common image manipulations - yet their true robustness against adaptive adversaries remains under-explored. We expose a previously unreported vulnerability and introduce SemanticRegen, a three-stage, label-free attack that erases state-of-the-art semantic and invisible watermarks while leaving an image's apparent meaning intact. Our pipeline (i) uses a vision-language model to obtain fine-grained captions, (ii) extracts foreground masks with zero-shot segmentation, and (iii) inpaints only the background via an LLM-guided diffusion model, thereby preserving salient objects and style cues. Evaluated on 1,000 prompts across four watermarking systems - TreeRing, StegaStamp, StableSig, and DWT/DCT - SemanticRegen is the only method to defeat the semantic TreeRing watermark (p = 0.10 > 0.05) and reduces bit-accuracy below 0.75 for the remaining schemes, all while maintaining high perceptual quality (masked SSIM = 0.94 +/- 0.01). We further introduce masked SSIM (mSSIM) to quantify fidelity within foreground regions, showing that our attack achieves up to 12 percent higher mSSIM than prior diffusion-based attackers. These results highlight an urgent gap between current watermark defenses and the capabilities of adaptive, semantics-aware adversaries, underscoring the need for watermarking algorithms that are resilient to content-preserving regenerative attacks.","authors":["Krti Tallam","John Kevin Cava","Caleb Geniesse","N. Benjamin Erichson","Michael W. Mahoney"],"url":"https://arxiv.org/abs/2505.08234"}
{"created":"2025-05-14","title":"EventDiff: A Unified and Efficient Diffusion Model Framework for Event-based Video Frame Interpolation","abstract":"Video Frame Interpolation (VFI) is a fundamental yet challenging task in computer vision, particularly under conditions involving large motion, occlusion, and lighting variation. Recent advancements in event cameras have opened up new opportunities for addressing these challenges. While existing event-based VFI methods have succeeded in recovering large and complex motions by leveraging handcrafted intermediate representations such as optical flow, these designs often compromise high-fidelity image reconstruction under subtle motion scenarios due to their reliance on explicit motion modeling. Meanwhile, diffusion models provide a promising alternative for VFI by reconstructing frames through a denoising process, eliminating the need for explicit motion estimation or warping operations. In this work, we propose EventDiff, a unified and efficient event-based diffusion model framework for VFI. EventDiff features a novel Event-Frame Hybrid AutoEncoder (HAE) equipped with a lightweight Spatial-Temporal Cross Attention (STCA) module that effectively fuses dynamic event streams with static frames. Unlike previous event-based VFI methods, EventDiff performs interpolation directly in the latent space via a denoising diffusion process, making it more robust across diverse and challenging VFI scenarios. Through a two-stage training strategy that first pretrains the HAE and then jointly optimizes it with the diffusion model, our method achieves state-of-the-art performance across multiple synthetic and real-world event VFI datasets. The proposed method outperforms existing state-of-the-art event-based VFI methods by up to 1.98dB in PSNR on Vimeo90K-Triplet and shows superior performance in SNU-FILM tasks with multiple difficulty levels. Compared to the emerging diffusion-based VFI approach, our method achieves up to 5.72dB PSNR gain on Vimeo90K-Triplet and 4.24X faster inference.","authors":["Hanle Zheng","Xujie Han","Zegang Peng","Shangbin Zhang","Guangxun Du","Zhuo Zou","Xilin Wang","Jibin Wu","Hao Guo","Lei Deng"],"url":"https://arxiv.org/abs/2505.08235"}
{"created":"2025-05-14","title":"Privacy-Preserving Analytics for Smart Meter (AMI) Data: A Hybrid Approach to Comply with CPUC Privacy Regulations","abstract":"Advanced Metering Infrastructure (AMI) data from smart electric and gas meters enables valuable insights for utilities and consumers, but also raises significant privacy concerns. In California, regulatory decisions (CPUC D.11-07-056 and D.11-08-045) mandate strict privacy protections for customer energy usage data, guided by the Fair Information Practice Principles (FIPPs). We comprehensively explore solutions drawn from data anonymization, privacy-preserving machine learning (differential privacy and federated learning), synthetic data generation, and cryptographic techniques (secure multiparty computation, homomorphic encryption). This allows advanced analytics, including machine learning models, statistical and econometric analysis on energy consumption data, to be performed without compromising individual privacy.","authors":["Benjamin Westrich"],"url":"https://arxiv.org/abs/2505.08237"}
{"created":"2025-05-14","title":"Motion Control of High-Dimensional Musculoskeletal Systems with Hierarchical Model-Based Planning","abstract":"Controlling high-dimensional nonlinear systems, such as those found in biological and robotic applications, is challenging due to large state and action spaces. While deep reinforcement learning has achieved a number of successes in these domains, it is computationally intensive and time consuming, and therefore not suitable for solving large collections of tasks that require significant manual tuning. In this work, we introduce Model Predictive Control with Morphology-aware Proportional Control (MPC^2), a hierarchical model-based learning algorithm for zero-shot and near-real-time control of high-dimensional complex dynamical systems. MPC^2 uses a sampling-based model predictive controller for target posture planning, and enables robust control for high-dimensional tasks by incorporating a morphology-aware proportional controller for actuator coordination. The algorithm enables motion control of a high-dimensional human musculoskeletal model in a variety of motion tasks, such as standing, walking on different terrains, and imitating sports activities. The reward function of MPC^2 can be tuned via black-box optimization, drastically reducing the need for human-intensive reward engineering.","authors":["Yunyue Wei","Shanning Zhuang","Vincent Zhuang","Yanan Sui"],"url":"https://arxiv.org/abs/2505.08238"}
{"created":"2025-05-14","title":"ACT-R: Adaptive Camera Trajectories for 3D Reconstruction from Single Image","abstract":"We introduce adaptive view planning to multi-view synthesis, aiming to improve both occlusion revelation and 3D consistency for single-view 3D reconstruction. Instead of generating an unordered set of views independently or simultaneously, we generate a sequence of views, leveraging temporal consistency to enhance 3D coherence. Most importantly, our view sequence is not determined by a pre-determined camera setup. Instead, we compute an adaptive camera trajectory (ACT), specifically, an orbit of camera views, which maximizes the visibility of occluded regions of the 3D object to be reconstructed. Once the best orbit is found, we feed it to a video diffusion model to generate novel views around the orbit, which in turn, are passed to a multi-view 3D reconstruction model to obtain the final reconstruction. Our multi-view synthesis pipeline is quite efficient since it involves no run-time training/optimization, only forward inferences by applying the pre-trained models for occlusion analysis and multi-view synthesis. Our method predicts camera trajectories that reveal occlusions effectively and produce consistent novel views, significantly improving 3D reconstruction over SOTA on the unseen GSO dataset, both quantitatively and qualitatively.","authors":["Yizhi Wang","Mingrui Zhao","Ali Mahdavi-Amiri","Hao Zhang"],"url":"https://arxiv.org/abs/2505.08239"}
{"created":"2025-05-14","title":"Congenital Heart Disease recognition using Deep Learning/Transformer models","abstract":"Congenital Heart Disease (CHD) remains a leading cause of infant morbidity and mortality, yet non-invasive screening methods often yield false negatives. Deep learning models, with their ability to automatically extract features, can assist doctors in detecting CHD more effectively. In this work, we investigate the use of dual-modality (sound and image) deep learning methods for CHD diagnosis. We achieve 73.9% accuracy on the ZCHSound dataset and 80.72% accuracy on the DICOM Chest X-ray dataset.","authors":["Aidar Amangeldi","Vladislav Yarovenko","Angsar Taigonyrov"],"url":"https://arxiv.org/abs/2505.08242"}
{"created":"2025-05-14","title":"Training Strategies for Efficient Embodied Reasoning","abstract":"Robot chain-of-thought reasoning (CoT) -- wherein a model predicts helpful intermediate representations before choosing actions -- provides an effective method for improving the generalization and performance of robot policies, especially vision-language-action models (VLAs). While such approaches have been shown to improve performance and generalization, they suffer from core limitations, like needing specialized robot reasoning data and slow inference speeds. To design new robot reasoning approaches that address these issues, a more complete characterization of why reasoning helps policy performance is critical. We hypothesize several mechanisms by which robot reasoning improves policies -- (1) better representation learning, (2) improved learning curricularization, and (3) increased expressivity -- then devise simple variants of robot CoT reasoning to isolate and test each one. We find that learning to generate reasonings does lead to better VLA representations, while attending to the reasonings aids in actually leveraging these features for improved action prediction. Our results provide us with a better understanding of why CoT reasoning helps VLAs, which we use to introduce two simple and lightweight alternative recipes for robot reasoning. Our proposed approaches achieve significant performance gains over non-reasoning policies, state-of-the-art results on the LIBERO-90 benchmark, and a 3x inference speedup compared to standard robot reasoning.","authors":["William Chen","Suneel Belkhale","Suvir Mirchandani","Oier Mees","Danny Driess","Karl Pertsch","Sergey Levine"],"url":"https://arxiv.org/abs/2505.08243"}
{"created":"2025-05-14","title":"The Failure of Plagiarism Detection in Competitive Programming","abstract":"Plagiarism in programming courses remains a persistent challenge, especially in competitive programming contexts where assignments often have unique, known solutions. This paper examines why traditional code plagiarism detection methods frequently fail in these environments and explores the implications of emerging factors such as generative AI (genAI). Drawing on the author's experience teaching a Competitive Programming 1 (CP1) course over seven semesters at Purdue University (with $\\approx 100$ students each term) and completely redesigning the CP1/2/3 course sequence, we provide an academically grounded analysis. We review literature on code plagiarism in computer science education, survey current detection tools (Moss, Kattis, etc.) and methods (manual review, code-authorship interviews), and analyze their strengths and limitations. Experience-based observations are presented to illustrate real-world detection failures and successes. We find that widely-used automated similarity checkers can be thwarted by simple code transformations or novel AI-generated code, while human-centric approaches like oral interviews, though effective, are labor-intensive. The paper concludes with opinions and preliminary recommendations for improving academic integrity in programming courses, advocating for a multi-faceted approach that combines improved detection algorithms, mastery-based learning techniques, and authentic assessment practices to better ensure code originality.","authors":["Ethan Dickey"],"url":"https://arxiv.org/abs/2505.08244"}
{"created":"2025-05-14","title":"Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement","abstract":"The rapid advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. It presents novel challenges, such as measuring human-like psychological constructs, navigating beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with Psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This survey introduces and synthesizes an emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. We systematically explore the role of Psychometrics in shaping benchmarking principles, broadening evaluation scopes, refining methodologies, validating results, and advancing LLM capabilities. This paper integrates diverse perspectives to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, we aim to provide actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.","authors":["Haoran Ye","Jing Jin","Yuhang Xie","Xin Zhang","Guojie Song"],"url":"https://arxiv.org/abs/2505.08245"}
{"created":"2025-05-14","title":"Identifying Memorization of Diffusion Models through p-Laplace Analysis","abstract":"Diffusion models, today's leading image generative models, estimate the score function, i.e. the gradient of the log probability of (perturbed) data samples, without direct access to the underlying probability distribution. This work investigates whether the estimated score function can be leveraged to compute higher-order differentials, namely p-Laplace operators. We show here these operators can be employed to identify memorized training data. We propose a numerical p-Laplace approximation based on the learned score functions, showing its effectiveness in identifying key features of the probability landscape. We analyze the structured case of Gaussian mixture models, and demonstrate the results carry-over to image generative models, where memorization identification based on the p-Laplace operator is performed for the first time.","authors":["Jonathan Brokman","Amit Giloni","Omer Hofman","Roman Vainshtein","Hisashi Kojima","Guy Gilboa"],"url":"https://arxiv.org/abs/2505.08246"}
{"created":"2025-05-14","title":"Community Detection on Noisy Stochastic Block Models","abstract":"We study the problem of community detection in noisy stochastic block models. We focus on two types of noise: (1) geometric noise where a latent-space kernel affects edge formation, and (2) Erdos-Renyi model censoring where edges are masked independently. We present a new algorithm DuoSpec that de-noises the network to a pristine stochastic block model structure for better community recovery. We demonstrate on synthetic data that our algorithm outperforms existing community detection methods on noisy models. We test our algorithm on the Amazon metadata dataset and demonstrate strong results on community detection.","authors":["Washieu Anan","Gwyneth Liu"],"url":"https://arxiv.org/abs/2505.08251"}
{"created":"2025-05-14","title":"Evaluating LLM Metrics Through Real-World Capabilities","abstract":"As generative AI becomes increasingly embedded in everyday workflows, it is important to evaluate its performance in ways that reflect real-world usage rather than abstract notions of intelligence. Unlike many existing benchmarks that assess general intelligence, our approach focuses on real-world utility, evaluating how well models support users in everyday tasks. While current benchmarks emphasize code generation or factual recall, users rely on AI for a much broader range of activities-from writing assistance and summarization to citation formatting and stylistic feedback. In this paper, we analyze large-scale survey data and usage logs to identify six core capabilities that represent how people commonly use Large Language Models (LLMs): Summarization, Technical Assistance, Reviewing Work, Data Structuring, Generation, and Information Retrieval. We then assess the extent to which existing benchmarks cover these capabilities, revealing significant gaps in coverage, efficiency measurement, and interpretability. Drawing on this analysis, we use human-centered criteria to identify gaps in how well current benchmarks reflect common usage that is grounded in five practical criteria: coherence, accuracy, clarity, relevance, and efficiency. For four of the six capabilities, we identify the benchmarks that best align with real-world tasks and use them to compare leading models. We find that Google Gemini outperforms other models-including OpenAI's GPT, xAI's Grok, Meta's LLaMA, Anthropic's Claude, DeepSeek, and Qwen from Alibaba-on these utility-focused metrics.","authors":["Justin K Miller","Wenjia Tang"],"url":"https://arxiv.org/abs/2505.08253"}
{"created":"2025-05-14","title":"Where the Devil Hides: Deepfake Detectors Can No Longer Be Trusted","abstract":"With the advancement of AI generative techniques, Deepfake faces have become incredibly realistic and nearly indistinguishable to the human eye. To counter this, Deepfake detectors have been developed as reliable tools for assessing face authenticity. These detectors are typically developed on Deep Neural Networks (DNNs) and trained using third-party datasets. However, this protocol raises a new security risk that can seriously undermine the trustfulness of Deepfake detectors: Once the third-party data providers insert poisoned (corrupted) data maliciously, Deepfake detectors trained on these datasets will be injected ``backdoors'' that cause abnormal behavior when presented with samples containing specific triggers. This is a practical concern, as third-party providers may distribute or sell these triggers to malicious users, allowing them to manipulate detector performance and escape accountability.","authors":["Shuaiwei Yuan","Junyu Dong","Yuezun Li"],"url":"https://arxiv.org/abs/2505.08255"}
{"created":"2025-05-14","title":"Clustering-based Low-Rank Matrix Approximation: An Adaptive Theoretical Analysis with Application to Data Compression","abstract":"Low-rank matrix approximation (LoRMA) is a fundamental tool for compressing high-resolution data matrices by extracting important features while suppressing redundancy. Low-rank methods, such as global singular value decomposition (SVD), apply uniform compression across the entire data matrix, often ignoring important local variations and leading to the loss of fine structural details. To address these limitations, we introduce an adaptive LoRMA, which partitions data matrix into overlapping patches, groups structurally similar patches into several clusters using k-means, and performs SVD within each cluster. We derive the overall compression factor accounting for patch overlap and analyze how patch size influences compression efficiency and computational cost. While the proposed adaptive LoRMA method is applicable to any data exhibiting high local variation, we focus on medical imaging due to its pronounced local variability. We evaluate and compare our adaptive LoRMA against global SVD across four imaging modalities: MRI, ultrasound, CT scan, and chest X-ray. Results demonstrate that adaptive LoRMA effectively preserves structural integrity, edge details, and diagnostic relevance, as measured by peak signal-to-noise ratio (PSNR), structural similarity index (SSIM), mean squared error (MSE), intersection over union (IoU), and edge preservation index (EPI). Adaptive LoRMA significantly minimizes block artifacts and residual errors, particularly in pathological regions, consistently outperforming global SVD in terms of PSNR, SSIM, IoU, EPI, and achieving lower MSE. Adaptive LoRMA prioritizes clinically salient regions while allowing aggressive compression in non-critical regions, optimizing storage efficiency. Although adaptive LoRMA requires higher processing time, its diagnostic fidelity justifies the overhead for high-compression applications.","authors":["Sisipho Hamlomo","Marcellin Atemkeng"],"url":"https://arxiv.org/abs/2505.08256"}
{"created":"2025-05-14","title":"Hybrid Wi-Fi/PDR Indoor Localization with Fingerprint Matching","abstract":"Indoor position technology has become one of the research highlights in the Internet of Things (IoT), but there is still a lack of universal, low-cost, and high-precision solutions. This paper conducts research on indoor position technology based on location fingerprints and proposes a practical hybrid indoor positioning system. In this experiment, the location fingerprint database is established by using RSS signal in the offline stage, the location algorithm is improved and innovated in the online stage. The weighted k-nearest neighbor algorithm is used for location fingerprint matching and pedestrian dead reckoning technology is used for trajectory tracking. This paper designs and implements an indoor position system that performs the functions of data collection, positioning, and position tracking. Through the test, it is found that it can meet the requirements of indoor positioning.","authors":["Chunyi Zhang","Zongwei Li","Xiaoqi Li"],"url":"https://arxiv.org/abs/2505.08258"}
{"created":"2025-05-14","title":"CNN and ViT Efficiency Study on Tiny ImageNet and DermaMNIST Datasets","abstract":"This study evaluates the trade-offs between convolutional and transformer-based architectures on both medical and general-purpose image classification benchmarks. We use ResNet-18 as our baseline and introduce a fine-tuning strategy applied to four Vision Transformer variants (Tiny, Small, Base, Large) on DermatologyMNIST and TinyImageNet. Our goal is to reduce inference latency and model complexity with acceptable accuracy degradation. Through systematic hyperparameter variations, we demonstrate that appropriately fine-tuned Vision Transformers can match or exceed the baseline's performance, achieve faster inference, and operate with fewer parameters, highlighting their viability for deployment in resource-constrained environments.","authors":["Aidar Amangeldi","Angsar Taigonyrov","Muhammad Huzaid Jawad","Chinedu Emmanuel Mbonu"],"url":"https://arxiv.org/abs/2505.08259"}
{"created":"2025-05-14","title":"Few-shot Novel Category Discovery","abstract":"The recently proposed Novel Category Discovery (NCD) adapt paradigm of transductive learning hinders its application in more real-world scenarios. In fact, few labeled data in part of new categories can well alleviate this burden, which coincides with the ease that people can label few of new category data. Therefore, this paper presents a new setting in which a trained agent is able to flexibly switch between the tasks of identifying examples of known (labelled) classes and clustering novel (completely unlabeled) classes as the number of query examples increases by leveraging knowledge learned from only a few (handful) support examples. Drawing inspiration from the discovery of novel categories using prior-based clustering algorithms, we introduce a novel framework that further relaxes its assumptions to the real-world open set level by unifying the concept of model adaptability in few-shot learning. We refer to this setting as Few-Shot Novel Category Discovery (FSNCD) and propose Semi-supervised Hierarchical Clustering (SHC) and Uncertainty-aware K-means Clustering (UKC) to examine the model's reasoning capabilities. Extensive experiments and detailed analysis on five commonly used datasets demonstrate that our methods can achieve leading performance levels across different task settings and scenarios.","authors":["Chunming Li","Shidong Wang","Haofeng Zhang"],"url":"https://arxiv.org/abs/2505.08260"}
{"created":"2025-05-14","title":"Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration","abstract":"The rapid progress in large language models (LLMs) has paved the way for novel approaches in knowledge-intensive tasks. Among these, Cache-Augmented Generation (CAG) has emerged as a promising alternative to Retrieval-Augmented Generation (RAG). CAG minimizes retrieval latency and simplifies system design by preloading knowledge into the model's context. However, challenges persist in scaling CAG to accommodate large and dynamic knowledge bases effectively. This paper introduces Adaptive Contextual Compression (ACC), an innovative technique designed to dynamically compress and manage context inputs, enabling efficient utilization of the extended memory capabilities of modern LLMs. To further address the limitations of standalone CAG, we propose a Hybrid CAG-RAG Framework, which integrates selective retrieval to augment preloaded contexts in scenarios requiring additional information. Comprehensive evaluations on diverse datasets highlight the proposed methods' ability to enhance scalability, optimize efficiency, and improve multi-hop reasoning performance, offering practical solutions for real-world knowledge integration challenges.","authors":["Rishabh Agrawal","Himanshu Kumar"],"url":"https://arxiv.org/abs/2505.08261"}
{"created":"2025-05-14","title":"Super-fast rates of convergence for Neural Networks Classifiers under the Hard Margin Condition","abstract":"We study the classical binary classification problem for hypothesis spaces of Deep Neural Networks (DNNs) with ReLU activation under Tsybakov's low-noise condition with exponent $q>0$, and its limit-case $q\\to\\infty$ which we refer to as the \"hard-margin condition\". We show that DNNs which minimize the empirical risk with square loss surrogate and $\\ell_p$ penalty can achieve finite-sample excess risk bounds of order $\\mathcal{O}\\left(n^{-\\alpha}\\right)$ for arbitrarily large $\\alpha>0$ under the hard-margin condition, provided that the regression function $\\eta$ is sufficiently smooth. The proof relies on a novel decomposition of the excess risk which might be of independent interest.","authors":["Nathanael Tepakbong","Ding-Xuan Zhou","Xiang Zhou"],"url":"https://arxiv.org/abs/2505.08262"}
{"created":"2025-05-14","title":"LLM-Based Detection of Tangled Code Changes for Higher-Quality Method-Level Bug Datasets","abstract":"Tangled code changes-commits that conflate unrelated modifications such as bug fixes, refactorings, and enhancements-introduce significant noise into bug datasets and adversely affect the performance of bug prediction models. Addressing this issue at a fine-grained, method-level granularity remains underexplored. This is critical to address, as recent bug prediction models, driven by practitioner demand, are increasingly focusing on finer granularity rather than traditional class- or file-level predictions. This study investigates the utility of Large Language Models (LLMs) for detecting tangled code changes by leveraging both commit messages and method-level code diffs. We formulate the problem as a binary classification task and evaluate multiple prompting strategies, including zero-shot, few-shot, and chain-of-thought prompting, using state-of-the-art proprietary LLMs such as GPT-4o and Gemini-2.0-Flash.","authors":["Md Nahidul Islam Opu","Shaowei Wang","Shaiful Chowdhury"],"url":"https://arxiv.org/abs/2505.08263"}
{"created":"2025-05-14","title":"Automatic Curriculum Learning for Driving Scenarios: Towards Robust and Efficient Reinforcement Learning","abstract":"This paper addresses the challenges of training end-to-end autonomous driving agents using Reinforcement Learning (RL). RL agents are typically trained in a fixed set of scenarios and nominal behavior of surrounding road users in simulations, limiting their generalization and real-life deployment. While domain randomization offers a potential solution by randomly sampling driving scenarios, it frequently results in inefficient training and sub-optimal policies due to the high variance among training scenarios. To address these limitations, we propose an automatic curriculum learning framework that dynamically generates driving scenarios with adaptive complexity based on the agent's evolving capabilities. Unlike manually designed curricula that introduce expert bias and lack scalability, our framework incorporates a ``teacher'' that automatically generates and mutates driving scenarios based on their learning potential -- an agent-centric metric derived from the agent's current policy -- eliminating the need for expert design. The framework enhances training efficiency by excluding scenarios the agent has mastered or finds too challenging. We evaluate our framework in a reinforcement learning setting where the agent learns a driving policy from camera images. Comparative results against baseline methods, including fixed scenario training and domain randomization, demonstrate that our approach leads to enhanced generalization, achieving higher success rates: +9\\% in low traffic density, +21\\% in high traffic density, and faster convergence with fewer training steps. Our findings highlight the potential of ACL in improving the robustness and efficiency of RL-based autonomous driving agents.","authors":["Ahmed Abouelazm","Tim Weinstein","Tim Joseph","Philip Sch\\\"orner","J. Marius Z\\\"ollner"],"url":"https://arxiv.org/abs/2505.08264"}
{"created":"2025-05-14","title":"LLM Enhancers for GNNs: An Analysis from the Perspective of Causal Mechanism Identification","abstract":"The use of large language models (LLMs) as feature enhancers to optimize node representations, which are then used as inputs for graph neural networks (GNNs), has shown significant potential in graph representation learning. However, the fundamental properties of this approach remain underexplored. To address this issue, we propose conducting a more in-depth analysis of this issue based on the interchange intervention method. First, we construct a synthetic graph dataset with controllable causal relationships, enabling precise manipulation of semantic relationships and causal modeling to provide data for analysis. Using this dataset, we conduct interchange interventions to examine the deeper properties of LLM enhancers and GNNs, uncovering their underlying logic and internal mechanisms. Building on the analytical results, we design a plug-and-play optimization module to improve the information transfer between LLM enhancers and GNNs. Experiments across multiple datasets and models validate the proposed module.","authors":["Hang Gao","Wenxuan Huang","Fengge Wu","Junsuo Zhao","Changwen Zheng","Huaping Liu"],"url":"https://arxiv.org/abs/2505.08265"}
{"created":"2025-05-14","title":"Open the Eyes of MPNN: Vision Enhances MPNN in Link Prediction","abstract":"Message-passing graph neural networks (MPNNs) and structural features (SFs) are cornerstones for the link prediction task. However, as a common and intuitive mode of understanding, the potential of visual perception has been overlooked in the MPNN community. For the first time, we equip MPNNs with vision structural awareness by proposing an effective framework called Graph Vision Network (GVN), along with a more efficient variant (E-GVN). Extensive empirical results demonstrate that with the proposed frameworks, GVN consistently benefits from the vision enhancement across seven link prediction datasets, including challenging large-scale graphs. Such improvements are compatible with existing state-of-the-art (SOTA) methods and GVNs achieve new SOTA results, thereby underscoring a promising novel direction for link prediction.","authors":["Yanbin Wei","Xuehao Wang","Zhan Zhuang","Yang Chen","Shuhao Chen","Yulong Zhang","Yu Zhang","James Kwok"],"url":"https://arxiv.org/abs/2505.08266"}
{"created":"2025-05-14","title":"IrrMap: A Large-Scale Comprehensive Dataset for Irrigation Method Mapping","abstract":"We introduce IrrMap, the first large-scale dataset (1.1 million patches) for irrigation method mapping across regions. IrrMap consists of multi-resolution satellite imagery from LandSat and Sentinel, along with key auxiliary data such as crop type, land use, and vegetation indices. The dataset spans 1,687,899 farms and 14,117,330 acres across multiple western U.S. states from 2013 to 2023, providing a rich and diverse foundation for irrigation analysis and ensuring geospatial alignment and quality control. The dataset is ML-ready, with standardized 224x224 GeoTIFF patches, the multiple input modalities, carefully chosen train-test-split data, and accompanying dataloaders for seamless deep learning model training andbenchmarking in irrigation mapping. The dataset is also accompanied by a complete pipeline for dataset generation, enabling researchers to extend IrrMap to new regions for irrigation data collection or adapt it with minimal effort for other similar applications in agricultural and geospatial analysis. We also analyze the irrigation method distribution across crop groups, spatial irrigation patterns (using Shannon diversity indices), and irrigated area variations for both LandSat and Sentinel, providing insights into regional and resolution-based differences. To promote further exploration, we openly release IrrMap, along with the derived datasets, benchmark models, and pipeline code, through a GitHub repository: https://github.com/Nibir088/IrrMap and Data repository: https://huggingface.co/Nibir/IrrMap, providing comprehensive documentation and implementation details.","authors":["Nibir Chandra Mandal","Oishee Bintey Hoque","Abhijin Adiga","Samarth Swarup","Mandy Wilson","Lu Feng","Yangfeng Ji","Miaomiao Zhang","Geoffrey Fox","Madhav Marathe"],"url":"https://arxiv.org/abs/2505.08273"}
{"created":"2025-05-14","title":"Ultra Lowrate Image Compression with Semantic Residual Coding and Compression-aware Diffusion","abstract":"Existing multimodal large model-based image compression frameworks often rely on a fragmented integration of semantic retrieval, latent compression, and generative models, resulting in suboptimal performance in both reconstruction fidelity and coding efficiency. To address these challenges, we propose a residual-guided ultra lowrate image compression named ResULIC, which incorporates residual signals into both semantic retrieval and the diffusion-based generation process. Specifically, we introduce Semantic Residual Coding (SRC) to capture the semantic disparity between the original image and its compressed latent representation. A perceptual fidelity optimizer is further applied for superior reconstruction quality. Additionally, we present the Compression-aware Diffusion Model (CDM), which establishes an optimal alignment between bitrates and diffusion time steps, improving compression-reconstruction synergy. Extensive experiments demonstrate the effectiveness of ResULIC, achieving superior objective and subjective performance compared to state-of-the-art diffusion-based methods with - 80.7%, -66.3% BD-rate saving in terms of LPIPS and FID. Project page is available at https: //njuvision.github.io/ResULIC/.","authors":["Anle Ke","Xu Zhang","Tong Chen","Ming Lu","Chao Zhou","Jiawen Gu","Zhan Ma"],"url":"https://arxiv.org/abs/2505.08281"}
{"created":"2025-05-14","title":"Decoupled Multimodal Prototypes for Visual Recognition with Missing Modalities","abstract":"Multimodal learning enhances deep learning models by enabling them to perceive and understand information from multiple data modalities, such as visual and textual inputs. However, most existing approaches assume the availability of all modalities, an assumption that often fails in real-world applications. Recent works have introduced learnable missing-case-aware prompts to mitigate performance degradation caused by missing modalities while reducing the need for extensive model fine-tuning. Building upon the effectiveness of missing-case-aware handling for missing modalities, we propose a novel decoupled prototype-based output head, which leverages missing-case-aware class-wise prototypes tailored for each individual modality. This approach dynamically adapts to different missing modality scenarios and can be seamlessly integrated with existing prompt-based methods. Extensive experiments demonstrate that our proposed output head significantly improves performance across a wide range of missing-modality scenarios and varying missing rates.","authors":["Jueqing Lu","Yuanyuan Qi","Xiaohao Yang","Shujie Zhou","Lan Du"],"url":"https://arxiv.org/abs/2505.08283"}
{"created":"2025-05-14","title":"Disruptive Transformation of Artworks in Master-Disciple Relationships: The Case of Ukiyo-e Artworks","abstract":"Artwork research has long relied on human sensibility and subjective judgment, but recent developments in machine learning have enabled the quantitative assessment of features that humans could not discover. In Western paintings, comprehensive analyses have been conducted from various perspectives in conjunction with large databases, but such extensive analysis has not been sufficiently conducted for Eastern paintings. Then, we focus on Ukiyo-e, a traditional Japanese art form, as a case study of Eastern paintings, and conduct a quantitative analysis of creativity in works of art using 11,000 high-resolution images. This involves using the concept of calculating creativity from networks to analyze both the creativity of the artwork and that of the artists. As a result, In terms of Ukiyo-e as a whole, it was found that the creativity of its appearance has declined with the maturation of culture, but in terms of style, it has become more segmented with the maturation of culture and has maintained a high level of creativity. This not only provides new insights into the study of Ukiyo-e but also shows how Ukiyo-e has evolved within the ongoing cultural history, playing a culturally significant role in the analysis of Eastern art.","authors":["Honna Shinichi","Akira Matsui"],"url":"https://arxiv.org/abs/2505.08284"}
{"created":"2025-05-14","title":"On the Account Security Risks Posed by Password Strength Meters","abstract":"Password strength meters (PSMs) have been widely used by websites to gauge password strength, encouraging users to create stronger passwords. Popular data-driven PSMs, e.g., based on Markov, Probabilistic Context-free Grammar (PCFG) and neural networks, alarm strength based on a model learned from real passwords. Despite their proven effectiveness, the secure utility that arises from the leakage of trained passwords remains largely overlooked. To address this gap, we analyze 11 PSMs and find that 5 data-driven meters are vulnerable to membership inference attacks that expose their trained passwords, and seriously, 3 rule-based meters openly disclose their blocked passwords. We specifically design a PSM privacy leakage evaluation approach, and uncover that a series of general data-driven meters are vulnerable to leaking between 10^4 to 10^5 trained passwords, with the PCFG-based models being more vulnerable than other counterparts; furthermore, we aid in deriving insights that the inherent utility-privacy tradeoff is not as severe as previously thought. To further exploit the risks, we develop novel meter-aware attacks when a clever attacker can filter the used passwords during compromising accounts on websites using the meter, and experimentally show that attackers targeting websites that deployed the popular Zxcvbn meter can compromise an additional 5.84% user accounts within 10 attempts, demonstrating the urgent need for privacy-preserving PSMs that protect the confidentiality of the meter's used passwords. Finally, we sketch some counter-measures to mitigate these threats.","authors":["Ming Xu","Weili Han","Jitao Yu","Jing Liu","Xinyi Zhang","Yun Lin","Jin Song Dong"],"url":"https://arxiv.org/abs/2505.08292"}
{"created":"2025-05-14","title":"M3G: Multi-Granular Gesture Generator for Audio-Driven Full-Body Human Motion Synthesis","abstract":"Generating full-body human gestures encompassing face, body, hands, and global movements from audio is a valuable yet challenging task in virtual avatar creation. Previous systems focused on tokenizing the human gestures framewisely and predicting the tokens of each frame from the input audio. However, one observation is that the number of frames required for a complete expressive human gesture, defined as granularity, varies among different human gesture patterns. Existing systems fail to model these gesture patterns due to the fixed granularity of their gesture tokens. To solve this problem, we propose a novel framework named Multi-Granular Gesture Generator (M3G) for audio-driven holistic gesture generation. In M3G, we propose a novel Multi-Granular VQ-VAE (MGVQ-VAE) to tokenize motion patterns and reconstruct motion sequences from different temporal granularities. Subsequently, we proposed a multi-granular token predictor that extracts multi-granular information from audio and predicts the corresponding motion tokens. Then M3G reconstructs the human gestures from the predicted tokens using the MGVQ-VAE. Both objective and subjective experiments demonstrate that our proposed M3G framework outperforms the state-of-the-art methods in terms of generating natural and expressive full-body human gestures.","authors":["Zhizhuo Yin","Yuk Hang Tsui","Pan Hui"],"url":"https://arxiv.org/abs/2505.08293"}
{"created":"2025-05-14","title":"FauForensics: Boosting Audio-Visual Deepfake Detection with Facial Action Units","abstract":"The rapid evolution of generative AI has increased the threat of realistic audio-visual deepfakes, demanding robust detection methods. Existing solutions primarily address unimodal (audio or visual) forgeries but struggle with multimodal manipulations due to inadequate handling of heterogeneous modality features and poor generalization across datasets. To this end, we propose a novel framework called FauForensics by introducing biologically invariant facial action units (FAUs), which is a quantitative descriptor of facial muscle activity linked to emotion physiology. It serves as forgery-resistant representations that reduce domain dependency while capturing subtle dynamics often disrupted in synthetic content. Besides, instead of comparing entire video clips as in prior works, our method computes fine-grained frame-wise audiovisual similarities via a dedicated fusion module augmented with learnable cross-modal queries. It dynamically aligns temporal-spatial lip-audio relationships while mitigating multi-modal feature heterogeneity issues. Experiments on FakeAVCeleb and LAV-DF show state-of-the-art (SOTA) performance and superior cross-dataset generalizability with up to an average of 4.83\\% than existing methods.","authors":["Jian Wang","Baoyuan Wu","Li Liu","Qingshan Liu"],"url":"https://arxiv.org/abs/2505.08294"}
{"created":"2025-05-14","title":"A Practical Introduction to Deep Reinforcement Learning","abstract":"Deep reinforcement learning (DRL) has emerged as a powerful framework for solving sequential decision-making problems, achieving remarkable success in a wide range of applications, including game AI, autonomous driving, biomedicine, and large language models. However, the diversity of algorithms and the complexity of theoretical foundations often pose significant challenges for beginners seeking to enter the field. This tutorial aims to provide a concise, intuitive, and practical introduction to DRL, with a particular focus on the Proximal Policy Optimization (PPO) algorithm, which is one of the most widely used and effective DRL methods. To facilitate learning, we organize all algorithms under the Generalized Policy Iteration (GPI) framework, offering readers a unified and systematic perspective. Instead of lengthy theoretical proofs, we emphasize intuitive explanations, illustrative examples, and practical engineering techniques. This work serves as an efficient and accessible guide, helping readers rapidly progress from basic concepts to the implementation of advanced DRL algorithms.","authors":["Yinghan Sun","Hongxi Wang","Hua Chen","Wei Zhang"],"url":"https://arxiv.org/abs/2505.08295"}
{"created":"2025-05-14","title":"On Analysis of Superimposed Pilot in Multi-User Massive MIMO with Massive Connectivity","abstract":"The simultaneous transmission of numerous users presents substantial challenges due to the inherent trade-off between channel estimation and information transmission in multi-user multiple-input multiple-output (MIMO) system. In this paper, we explore the use of the superimposed pilot (SP) scheme to tackle the large transmitting users, where the number of users may exceed the coherent time. SP scheme incorporates both transmitted data and noise in the channel estimation process, which is significant different from the counterpart of RP scheme. We provide an in-depth analysis of the interaction between interference caused by channel estimation errors and noise. We then derive the explicit expression for the scaling law of the mutual information lower bound (MILB) in relation to the number of users and the levels of transmitted power. Besides, the optimal power allocation between pilots and data transmission is also derived analytically. The analytical results demonstrate that the SP scheme significantly improves performance compared to traditional RP scheme in our consider case. Numerical results are also presented to validate our theoretical derivations.","authors":["Shuxiao Ye","Xianchao Zhang","Neng Ye"],"url":"https://arxiv.org/abs/2505.08298"}
{"created":"2025-05-14","title":"Efficient Unstructured Pruning of Mamba State-Space Models for Resource-Constrained Environments","abstract":"State-space models (SSMs), particularly the Mamba architecture, have emerged as powerful alternatives to Transformers for sequence modeling, offering linear-time complexity and competitive performance across diverse tasks. However, their large parameter counts pose significant challenges for deployment in resource-constrained environments. We propose a novel unstructured pruning framework tailored for Mamba models that achieves up to 70\\% parameter reduction while retaining over 95\\% of the original performance. Our approach integrates three key innovations: (1) a gradient-aware magnitude pruning technique that combines weight magnitude and gradient information to identify less critical parameters, (2) an iterative pruning schedule that gradually increases sparsity to maintain model stability, and (3) a global pruning strategy that optimizes parameter allocation across the entire model. Through extensive experiments on WikiText-103, Long Range Arena, and ETT time-series benchmarks, we demonstrate significant efficiency gains with minimal performance degradation. Our analysis of pruning effects on Mamba's components reveals critical insights into the architecture's redundancy and robustness, enabling practical deployment in resource-constrained settings while broadening Mamba's applicability.","authors":["Ibne Farabi Shihab","Sanjeda Akter","Anuj Sharma"],"url":"https://arxiv.org/abs/2505.08299"}
{"created":"2025-05-14","title":"Exploring Challenges in Test Mocking: Developer Questions and Insights from StackOverflow","abstract":"Mocking is a common unit testing technique that is used to simplify tests, reduce flakiness, and improve coverage by replacing real dependencies with simplified implementations. Despite its widespread use in Open Source Software projects, there is limited understanding of how and why developers use mocks and the challenges they face. In this collaborative study, we have analyzed 25,302 questions related to Mocking on STACKOVERFLOW to identify the challenges faced by developers. We have used Latent Dirichlet Allocation for topic modeling, identified 30 key topics, and grouped the topics into five key categories. Consequently, we analyzed the annual and relative probabilities of each category to understand the evolution of mocking-related discussions. Trend analysis reveals that category like Advanced Programming peaked between 2009 and 2012 but have since declined, while categories such as Mocking Techniques and External Services have remained consistently dominant, highlighting evolving developer priorities and ongoing technical challenges. Our findings also show an inverse relationship between a topic's popularity and its difficulty. Popular topics like Framework Selection tend to have lower difficulty and faster resolution times, while complex topics like HTTP Requests and Responses are more likely to remain unanswered and take longer to resolve. A classification of questions into How, Why, What, and Other revealed that over 70% are How questions, particularly in practical domains like file access and APIs, indicating a strong need for implementation guidance. Why questions are more prevalent in error-handling contexts, reflecting conceptual challenges in debugging, while What questions are rare and mostly tied to theoretical discussions. These insights offer valuable guidance for improving developer support, tooling, and educational content in the context of mocking and unit testing.","authors":["Mumtahina Ahmed","Md Nahidul Islam Opu","Chanchal Roy","Shaiful Chowdhury","Sujana Islam Suhi"],"url":"https://arxiv.org/abs/2505.08300"}
{"created":"2025-05-14","title":"Knowledge-Informed Deep Learning for Irrigation Type Mapping from Remote Sensing","abstract":"Accurate mapping of irrigation methods is crucial for sustainable agricultural practices and food systems. However, existing models that rely solely on spectral features from satellite imagery are ineffective due to the complexity of agricultural landscapes and limited training data, making this a challenging problem. We present Knowledge-Informed Irrigation Mapping (KIIM), a novel Swin-Transformer based approach that uses (i) a specialized projection matrix to encode crop to irrigation probability, (ii) a spatial attention map to identify agricultural lands from non-agricultural lands, (iii) bi-directional cross-attention to focus complementary information from different modalities, and (iv) a weighted ensemble for combining predictions from images and crop information. Our experimentation on five states in the US shows up to 22.9\\% (IoU) improvement over baseline with a 71.4% (IoU) improvement for hard-to-classify drip irrigation. In addition, we propose a two-phase transfer learning approach to enhance cross-state irrigation mapping, achieving a 51% IoU boost in a state with limited labeled data. The ability to achieve baseline performance with only 40% of the training data highlights its efficiency, reducing the dependency on extensive manual labeling efforts and making large-scale, automated irrigation mapping more feasible and cost-effective.","authors":["Oishee Bintey Hoque","Nibir Chandra Mandal","Abhijin Adiga","Samarth Swarup","Sayjro Kossi Nouwakpo","Amanda Wilson","Madhav Marathe"],"url":"https://arxiv.org/abs/2505.08302"}
{"created":"2025-05-14","title":"Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow","abstract":"Black-Box prompt optimization methods have emerged as a promising strategy for refining input prompts to better align large language models (LLMs), thereby enhancing their task performance. Although these methods have demonstrated encouraging results, most studies and experiments have primarily focused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g., GPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with DeepSeek V3 (671B), it remains an open question whether these black-box optimization techniques will continue to yield significant performance improvements for models of such scale. In response to this, we select three well-known black-box optimization methods and evaluate them on large-scale LLMs (DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The results show that these black-box prompt optimization methods offer only limited improvements on these large-scale LLMs. Furthermore, we hypothesize that the scale of the model is the primary factor contributing to the limited benefits observed. To explore this hypothesis, we conducted experiments on LLMs of varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an inverse scaling law, wherein the effectiveness of black-box optimization methods diminished as the model size increased.","authors":["Ziyu Zhou","Yihang Wu","Jingyuan Yang","Zhan Xiao","Rongjun Li"],"url":"https://arxiv.org/abs/2505.08303"}
{"created":"2025-05-14","title":"Rapid Overfitting of Multi-Pass Stochastic Gradient Descent in Stochastic Convex Optimization","abstract":"We study the out-of-sample performance of multi-pass stochastic gradient descent (SGD) in the fundamental stochastic convex optimization (SCO) model. While one-pass SGD is known to achieve an optimal $\\Theta(1/\\sqrt{n})$ excess population loss given a sample of size $n$, much less is understood about the multi-pass version of the algorithm which is widely used in practice. Somewhat surprisingly, we show that in the general non-smooth case of SCO, just a few epochs of SGD can already hurt its out-of-sample performance significantly and lead to overfitting. In particular, using a step size $\\eta = \\Theta(1/\\sqrt{n})$, which gives the optimal rate after one pass, can lead to population loss as large as $\\Omega(1)$ after just one additional pass. More generally, we show that the population loss from the second pass onward is of the order $\\Theta(1/(\\eta T) + \\eta \\sqrt{T})$, where $T$ is the total number of steps. These results reveal a certain phase-transition in the out-of-sample behavior of SGD after the first epoch, as well as a sharp separation between the rates of overfitting in the smooth and non-smooth cases of SCO. Additionally, we extend our results to with-replacement SGD, proving that the same asymptotic bounds hold after $O(n \\log n)$ steps. Finally, we also prove a lower bound of $\\Omega(\\eta \\sqrt{n})$ on the generalization gap of one-pass SGD in dimension $d = \\smash{\\widetilde O}(n)$, improving on recent results of Koren et al.(2022) and Schliserman et al.(2024).","authors":["Shira Vansover-Hager","Tomer Koren","Roi Livni"],"url":"https://arxiv.org/abs/2505.08306"}
{"created":"2025-05-14","title":"Uniform Universal Sets, Splitters, and Bisectors","abstract":"Given a subset of size $k$ of a very large universe a randomized way to find this subset could consist of deleting half of the universe and then searching the remaining part. With a probability of $2^{-k}$ one will succeed. By probability amplification, a randomized algorithm needs about $2^k$ rounds until it succeeds. We construct bisectors that derandomize this process and have size~$2^{k+o(k)}$. One application is derandomization of reductions between average case complexity classes. We also construct uniform $(n,k)$-universal sets that generalize universal sets in such a way that they are bisectors at the same time. This construction needs only linear time and produces families of asymptotically optimal size without using advanced combinatorial constructions as subroutines, which previous families did, but are basedmainly on modulo functions and refined brute force search.","authors":["Elisabet Burjons","Peter Rossmanith"],"url":"https://arxiv.org/abs/2505.08308"}
{"created":"2025-05-14","title":"AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale","abstract":"We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.","authors":["Yunjie Ji","Xiaoyu Tian","Sitong Zhao","Haotian Wang","Shuaiting Chen","Yiping Peng","Han Zhao","Xiangang Li"],"url":"https://arxiv.org/abs/2505.08311"}
{"created":"2025-05-14","title":"Investigating Resolution Strategies for Workspace-Occlusion in Augmented Virtuality","abstract":"Augmented Virtuality integrates physical content into virtual environments, but the occlusion of physical by virtual content is a challenge. This unwanted occlusion may disrupt user interactions with physical devices and compromise safety and usability. This paper investigates two resolution strategies to address this issue: Redirected Walking, which subtly adjusts the user's movement to maintain physical-virtual alignment, and Automatic Teleport Rotation, which realigns the virtual environment during travel. A user study set in a virtual forest demonstrates that both methods effectively reduce occlusion. While in our testbed, Automatic Teleport Rotation achieves higher occlusion resolution, it is suspected to increase cybersickness compared to the less intrusive Redirected Walking approach.","authors":["Nico Feld","Pauline Bimberg","Michael Feldmann","Matthias W\\\"olwer","Eike Langbehn","Benjamin Weyers","Daniel Zielasko"],"url":"https://arxiv.org/abs/2505.08312"}
{"created":"2025-05-14","title":"Improving Unsupervised Task-driven Models of Ventral Visual Stream via Relative Position Predictivity","abstract":"Based on the concept that ventral visual stream (VVS) mainly functions for object recognition, current unsupervised task-driven methods model VVS by contrastive learning, and have achieved good brain similarity. However, we believe functions of VVS extend beyond just object recognition. In this paper, we introduce an additional function involving VVS, named relative position (RP) prediction. We first theoretically explain contrastive learning may be unable to yield the model capability of RP prediction. Motivated by this, we subsequently integrate RP learning with contrastive learning, and propose a new unsupervised task-driven method to model VVS, which is more inline with biological reality. We conduct extensive experiments, demonstrating that: (i) our method significantly improves downstream performance of object recognition while enhancing RP predictivity; (ii) RP predictivity generally improves the model brain similarity. Our results provide strong evidence for the involvement of VVS in location perception (especially RP prediction) from a computational perspective.","authors":["Dazhong Rong","Hao Dong","Xing Gao","Jiyu Wei","Di Hong","Yaoyao Hao","Qinming He","Yueming Wang"],"url":"https://arxiv.org/abs/2505.08316"}
{"created":"2025-05-14","title":"A Unified Model for Cardinality Estimation by Learning from Data and Queries via Sum-Product Networks","abstract":"Cardinality estimation is a fundamental component in database systems, crucial for generating efficient execution plans. Despite advancements in learning-based cardinality estimation, existing methods may struggle to simultaneously optimize the key criteria: estimation accuracy, inference time, and storage overhead, limiting their practical applicability in real-world database environments. This paper introduces QSPN, a unified model that integrates both data distribution and query workload. QSPN achieves high estimation accuracy by modeling data distribution using the simple yet effective Sum-Product Network (SPN) structure. To ensure low inference time and reduce storage overhead, QSPN further partitions columns based on query access patterns. We formalize QSPN as a tree-based structure that extends SPNs by introducing two new node types: QProduct and QSplit. This paper studies the research challenges of developing efficient algorithms for the offline construction and online computation of QSPN. We conduct extensive experiments to evaluate QSPN in both single-table and multi-table cardinality estimation settings. The experimental results have demonstrated that QSPN achieves superior and robust performance on the three key criteria, compared with state-of-the-art approaches.","authors":["Jiawei Liu","Ju Fan","Tongyu Liu","Kai Zeng","Jiannan Wang","Quehuan Liu","Tao Ye","Nan Tang"],"url":"https://arxiv.org/abs/2505.08318"}
{"created":"2025-05-14","title":"Reciprocity as the Foundational Substrate of Society: How Reciprocal Dynamics Scale into Social Systems","abstract":"A major bottleneck in multi-agent AI is the lack of simulateable models for the bottom-up emergence of social structure under realistic behavioral constraints. Similarly, many foundational theories in economics and sociology including the concepts of \"institutions\" and \"norms\" tend to describe social structures post hoc, often relying on implicit assumptions of shared culture, morality, or symbolic agreement. These concepts are often treated as primitives rather than reconstructed from agent-level behavior, leaving both their origins and operational definitions under-specified. To address this, we propose a three-stage bottom-up framework: Reciprocal Dynamics, capturing individual-level reciprocal exchanges; Norm Stabilization, the consolidation of shared expectations; and Institutional Construction, the externalization of stable patterns into scalable structures. By grounding social emergence in agent-level reciprocity, our framework enables the systematic exploration of how moral, cultural, and institutional structures emerge from cognitively minimal interactions.","authors":["Egil Diau"],"url":"https://arxiv.org/abs/2505.08319"}
{"created":"2025-05-14","title":"SpecSphere: Dual-Pass Spectral-Spatial Graph Neural Networks with Certified Robustness","abstract":"We introduce SpecSphere, the first dual-pass spectral-spatial GNN that certifies every prediction against both $\\ell\\_{0}$ edge flips and $\\ell\\_{\\infty}$ feature perturbations, adapts to the full homophily-heterophily spectrum, and surpasses the expressive power of 1-Weisfeiler-Lehman while retaining linear-time complexity. Our model couples a Chebyshev-polynomial spectral branch with an attention-gated spatial branch and fuses their representations through a lightweight MLP trained in a cooperative-adversarial min-max game. We further establish (i) a uniform Chebyshev approximation theorem, (ii) minimax-optimal risk across the homophily-heterophily spectrum, (iii) closed-form robustness certificates, and (iv) universal approximation strictly beyond 1-WL. SpecSphere achieves state-of-the-art node-classification accuracy and delivers tighter certified robustness guarantees on real-world benchmarks. These results demonstrate that high expressivity, heterophily adaptation, and provable robustness can coexist within a single, scalable architecture.","authors":["Yoonhyuk Choi","Chong-Kwon Kim"],"url":"https://arxiv.org/abs/2505.08320"}
{"created":"2025-05-14","title":"An incremental algorithm for non-convex AI-enhanced medical image processing","abstract":"Solving non-convex regularized inverse problems is challenging due to their complex optimization landscapes and multiple local minima. However, these models remain widely studied as they often yield high-quality, task-oriented solutions, particularly in medical imaging, where the goal is to enhance clinically relevant features rather than merely minimizing global error. We propose incDG, a hybrid framework that integrates deep learning with incremental model-based optimization to efficiently approximate the $\\ell_0$-optimal solution of imaging inverse problems. Built on the Deep Guess strategy, incDG exploits a deep neural network to generate effective initializations for a non-convex variational solver, which refines the reconstruction through regularized incremental iterations. This design combines the efficiency of Artificial Intelligence (AI) tools with the theoretical guarantees of model-based optimization, ensuring robustness and stability. We validate incDG on TpV-regularized optimization tasks, demonstrating its effectiveness in medical image deblurring and tomographic reconstruction across diverse datasets, including synthetic images, brain CT slices, and chest-abdomen scans. Results show that incDG outperforms both conventional iterative solvers and deep learning-based methods, achieving superior accuracy and stability. Moreover, we confirm that training incDG without ground truth does not significantly degrade performance, making it a practical and powerful tool for solving non-convex inverse problems in imaging and beyond.","authors":["Elena Morotti"],"url":"https://arxiv.org/abs/2505.08324"}
{"created":"2025-05-14","title":"FedRS-Bench: Realistic Federated Learning Datasets and Benchmarks in Remote Sensing","abstract":"Remote sensing (RS) images are usually produced at an unprecedented scale, yet they are geographically and institutionally distributed, making centralized model training challenging due to data-sharing restrictions and privacy concerns. Federated learning (FL) offers a solution by enabling collaborative model training across decentralized RS data sources without exposing raw data. However, there lacks a realistic federated dataset and benchmark in RS. Prior works typically rely on manually partitioned single dataset, which fail to capture the heterogeneity and scale of real-world RS data, and often use inconsistent experimental setups, hindering fair comparison. To address this gap, we propose a realistic federated RS dataset, termed FedRS. FedRS consists of eight datasets that cover various sensors and resolutions and builds 135 clients, which is representative of realistic operational scenarios. Data for each client come from the same source, exhibiting authentic federated properties such as skewed label distributions, imbalanced client data volumes, and domain heterogeneity across clients. These characteristics reflect practical challenges in federated RS and support evaluation of FL methods at scale. Based on FedRS, we implement 10 baseline FL algorithms and evaluation metrics to construct the comprehensive FedRS-Bench. The experimental results demonstrate that FL can consistently improve model performance over training on isolated data silos, while revealing performance trade-offs of different methods under varying client heterogeneity and availability conditions. We hope FedRS-Bench will accelerate research on large-scale, realistic FL in RS by providing a standardized, rich testbed and facilitating fair comparisons across future works. The source codes and dataset are available at https://fedrs-bench.github.io/.","authors":["Haodong Zhao","Peng Peng","Chiyu Chen","Linqing Huang","Gongshen Liu"],"url":"https://arxiv.org/abs/2505.08325"}
{"created":"2025-05-14","title":"Coding Theorem for Generalized Reed-Solomon Codes","abstract":"In this paper, we prove that the sub-field images of generalized Reed-Solomon (RS) codes can achieve the symmetric capacity of p-ary memoryless channels. Unlike the totally random linear code ensemble, as a class of maximum distance separable (MDS) codes, the generalized RS code ensemble lacks the pair-wise independence among codewords and has non-identical distributions of nonzero codewords. To prove the coding theorem for the p-ary images of generalized RS codes, we analyze the exponential upper bound on the error probability of the generalized RS code in terms of its spectrum using random coding techniques. In the finite-length region, we present an ML decoding algorithm for the generalized RS codes over the binary erasure channels (BECs). In particular, the algebraic structure of the generalized RS codes allows us to implement the parallel Lagrange interpolation to derive an ordered systematic matrix. Subsequently, we can reconstruct the ML codeword through a change of basis, accelerating the conventional Gaussian elimination (GE), as validated in the simulation results. Additionally, we apply this decoding technique to the LC-OSD algorithm over the additive white Gaussian noise (AWGN) channels with binary phase shift keying (BPSK) modulation and three-level pulse amplitude modulation (3PAM). Simulation results show that, in the high-rate region, generalized RS codes defined over fields of characteristic three with 3-PAM perform better than those defined over fields of characteristic two with BPSK.","authors":["Xiangping Zheng","Xiao Ma"],"url":"https://arxiv.org/abs/2505.08326"}
{"created":"2025-05-14","title":"Low-Complexity Inference in Continual Learning via Compressed Knowledge Transfer","abstract":"Continual learning (CL) aims to train models that can learn a sequence of tasks without forgetting previously acquired knowledge. A core challenge in CL is balancing stability -- preserving performance on old tasks -- and plasticity -- adapting to new ones. Recently, large pre-trained models have been widely adopted in CL for their ability to support both, offering strong generalization for new tasks and resilience against forgetting. However, their high computational cost at inference time limits their practicality in real-world applications, especially those requiring low latency or energy efficiency. To address this issue, we explore model compression techniques, including pruning and knowledge distillation (KD), and propose two efficient frameworks tailored for class-incremental learning (CIL), a challenging CL setting where task identities are unavailable during inference. The pruning-based framework includes pre- and post-pruning strategies that apply compression at different training stages. The KD-based framework adopts a teacher-student architecture, where a large pre-trained teacher transfers downstream-relevant knowledge to a compact student. Extensive experiments on multiple CIL benchmarks demonstrate that the proposed frameworks achieve a better trade-off between accuracy and inference complexity, consistently outperforming strong baselines. We further analyze the trade-offs between the two frameworks in terms of accuracy and efficiency, offering insights into their use across different scenarios.","authors":["Zhenrong Liu","Janne M. J. Huttunen","Mikko Honkala"],"url":"https://arxiv.org/abs/2505.08327"}
{"created":"2025-05-14","title":"AI-Driven Digital Twins: Optimizing 5G/6G Network Slicing with NTNs","abstract":"Network slicing in 5G/6G Non-Terrestrial Network (NTN) is confronted with mobility and traffic variability. An artificial intelligence (AI)-based digital twin (DT) architecture with deep reinforcement learning (DRL) using Deep deterministic policy gradient (DDPG) is proposed for dynamic optimization of resource allocation. DT virtualizes network states to enable predictive analysis, while DRL changes bandwidth for eMBB slice. Simulations show a 25\\% latency reduction compared to static methods, with enhanced resource utilization. This scalable solution supports 5G/6G NTN applications like disaster recovery and urban blockage.","authors":["Afan Ali","Huseyin Arslan"],"url":"https://arxiv.org/abs/2505.08328"}
{"created":"2025-05-14","title":"Structural-Temporal Coupling Anomaly Detection with Dynamic Graph Transformer","abstract":"Detecting anomalous edges in dynamic graphs is an important task in many applications over evolving triple-based data, such as social networks, transaction management, and epidemiology. A major challenge with this task is the absence of structural-temporal coupling information, which decreases the ability of the representation to distinguish anomalies from normal instances. Existing methods focus on handling independent structural and temporal features with embedding models, which ignore the deep interaction between these two types of information. In this paper, we propose a structural-temporal coupling anomaly detection architecture with a dynamic graph transformer model. Specifically, we introduce structural and temporal features from two integration levels to provide anomaly-aware graph evolutionary patterns. Then, a dynamic graph transformer enhanced by two-dimensional positional encoding is implemented to capture both discrimination and contextual consistency signals. Extensive experiments on six datasets demonstrate that our method outperforms current state-of-the-art models. Finally, a case study illustrates the strength of our method when applied to a real-world task.","authors":["Chang Zong","Yueting Zhuang","Jian Shao","Weiming Lu"],"url":"https://arxiv.org/abs/2505.08330"}
{"created":"2025-05-14","title":"Fast Contact Detection via Fusion of Joint and Inertial Sensors for Parallel Robots in Human-Robot Collaboration","abstract":"Fast contact detection is crucial for safe human-robot collaboration. Observers based on proprioceptive information can be used for contact detection but have first-order error dynamics, which results in delays. Sensor fusion based on inertial measurement units (IMUs) consisting of accelerometers and gyroscopes is advantageous for reducing delays. The acceleration estimation enables the direct calculation of external forces. For serial robots, the installation of multiple accelerometers and gyroscopes is required for dynamics modeling since the joint coordinates are the minimal coordinates. Alternatively, parallel robots (PRs) offer the potential to use only one IMU on the end-effector platform, which already presents the minimal coordinates of the PR. This work introduces a sensor-fusion method for contact detection using encoders and only one low-cost, consumer-grade IMU for a PR. The end-effector accelerations are estimated by an extended Kalman filter and incorporated into the dynamics to calculate external forces. In real-world experiments with a planar PR, we demonstrate that this approach reduces the detection duration by up to 50% compared to a momentum observer and enables the collision and clamping detection within 3-39ms.","authors":["Aran Mohammad","Jan Piosik","Dustin Lehmann","Thomas Seel","Moritz Schappler"],"url":"https://arxiv.org/abs/2505.08334"}
{"created":"2025-05-14","title":"A computer vision-based model for occupancy detection using low-resolution thermal images","abstract":"Occupancy plays an essential role in influencing the energy consumption and operation of heating, ventilation, and air conditioning (HVAC) systems. Traditional HVAC typically operate on fixed schedules without considering occupancy. Advanced occupant-centric control (OCC) adopted occupancy status in regulating HVAC operations. RGB images combined with computer vision (CV) techniques are widely used for occupancy detection, however, the detailed facial and body features they capture raise significant privacy concerns. Low-resolution thermal images offer a non-invasive solution that mitigates privacy issues. The study developed an occupancy detection model utilizing low-resolution thermal images and CV techniques, where transfer learning was applied to fine-tune the You Only Look Once version 5 (YOLOv5) model. The developed model ultimately achieved satisfactory performance, with precision, recall, mAP50, and mAP50 values approaching 1.000. The contributions of this model lie not only in mitigating privacy concerns but also in reducing computing resource demands.","authors":["Xue Cui","Vincent Gbouna Zakka","Minhyun Lee"],"url":"https://arxiv.org/abs/2505.08336"}
{"created":"2025-05-14","title":"Benchmarking AI scientists in omics data-driven biological research","abstract":"The rise of large language models and multi-agent systems has sparked growing interest in AI scientists capable of autonomous biological research. However, existing benchmarks either focus on reasoning without data or on data analysis with predefined statistical answers, lacking realistic, data-driven evaluation settings. Here, we introduce the Biological AI Scientist Benchmark (BaisBench), a benchmark designed to assess AI scientists' ability to generate biological discoveries through data analysis and reasoning with external knowledge. BaisBench comprises two tasks: cell type annotation on 31 expert-labeled single-cell datasets, and scientific discovery through answering 198 multiple-choice questions derived from the biological insights of 41 recent single-cell studies. Systematic experiments on state-of-the-art AI scientists and LLM agents showed that while promising, current models still substantially underperform human experts on both tasks. We hope BaisBench will fill this gap and serve as a foundation for advancing and evaluating AI models for scientific discovery. The benchmark can be found at: https://github.com/EperLuo/BaisBench.","authors":["Erpai Luo","Jinmeng Jia","Yifan Xiong","Xiangyu Li","Xiaobo Guo","Baoqi Yu","Lei Wei","Xuegong Zhang"],"url":"https://arxiv.org/abs/2505.08341"}
{"created":"2025-05-14","title":"Optimal Prize Design in Parallel Rank-order Contests","abstract":"This paper investigates a two-stage game-theoretical model with multiple parallel rank-order contests. In this model, each contest designer sets up a contest and determines the prize structure within a fixed budget in the first stage. Contestants choose which contest to participate in and exert costly effort to compete against other participants in the second stage. First, we fully characterize the symmetric Bayesian Nash equilibrium in the subgame of contestants, accounting for both contest selection and effort exertion, under any given prize structures. Notably, we find that, regardless of whether contestants know the number of participants in their chosen contest, the equilibrium remains unchanged in expectation. Next, we analyze the designers' strategies under two types of objective functions based on effort and participation, respectively. For a broad range of effort-based objectives, we demonstrate that the winner-takes-all prize structure-optimal in the single-contest setting-remains a dominant strategy for all designers. For the participation objective, which maximizes the number of participants surpassing a skill threshold, we show that the optimal prize structure is always a simple contest. Furthermore, the equilibrium among designers is computationally tractable when they share a common threshold.","authors":["Xiaotie Deng","Ningyuan Li","Weian Li","Qi Qi"],"url":"https://arxiv.org/abs/2505.08342"}
{"created":"2025-05-14","title":"An Identifiable Cost-Aware Causal Decision-Making Framework Using Counterfactual Reasoning","abstract":"Decision making under abnormal conditions is a critical process that involves evaluating the current state and determining the optimal action to restore the system to a normal state at an acceptable cost. However, in such scenarios, existing decision-making frameworks highly rely on reinforcement learning or root cause analysis, resulting in them frequently neglecting the cost of the actions or failing to incorporate causal mechanisms adequately. By relaxing the existing causal decision framework to solve the necessary cause, we propose a minimum-cost causal decision (MiCCD) framework via counterfactual reasoning to address the above challenges. Emphasis is placed on making counterfactual reasoning processes identifiable in the presence of a large amount of mixed anomaly data, as well as finding the optimal intervention state in a continuous decision space. Specifically, it formulates a surrogate model based on causal graphs, using abnormal pattern clustering labels as supervisory signals. This enables the approximation of the structural causal model among the variables and lays a foundation for identifiable counterfactual reasoning. With the causal structure approximated, we then established an optimization model based on counterfactual estimation. The Sequential Least Squares Programming (SLSQP) algorithm is further employed to optimize intervention strategies while taking costs into account. Experimental evaluations on both synthetic and real-world datasets reveal that MiCCD outperforms conventional methods across multiple metrics, including F1-score, cost efficiency, and ranking quality(nDCG@k values), thus validating its efficacy and broad applicability.","authors":["Ruichu Cai","Xi Chen","Jie Qiao","Zijian Li","Yuequn Liu","Wei Chen","Keli Zhang","Jiale Zheng"],"url":"https://arxiv.org/abs/2505.08343"}
{"created":"2025-05-14","title":"A spherical amplitude-phase formulation for 3-D adaptive line-of-sight (ALOS) guidance with USGES stability guarantees","abstract":"A recently proposed 3-D adaptive line-of-sight (ALOS) path-following algorithm addressed coupled motion dynamics of marine craft, aircraft, and uncrewed vehicles under environmental disturbances such as wind, waves, and ocean currents. Stability analysis established uniform semiglobal exponential stability (USGES) of the cross- and vertical-track errors using a body-velocity-based amplitude-phase representation of the North-East-Down (NED) kinematic differential equations. In this brief paper, we revisit the ALOS framework and introduce a novel spherical amplitude-phase representation. This formulation yields a more geometrically intuitive and physically observable description of the guidance errors and enables a significantly simplified stability proof. Unlike the previous model, which relied on a vertical crab angle derived from body-frame velocities, the new representation uses an alternative vertical crab angle and retains the USGES property. It also removes restrictive assumptions such as constant altitude/depth or zero horizontal crab angle, and remains valid for general 3-D maneuvers with nonzero roll, pitch, and flight-path angles.","authors":["Erlend M. Coates","Thor I. Fossen"],"url":"https://arxiv.org/abs/2505.08344"}
{"created":"2025-05-14","title":"SHAP-based Explanations are Sensitive to Feature Representation","abstract":"Local feature-based explanations are a key component of the XAI toolkit. These explanations compute feature importance values relative to an ``interpretable'' feature representation. In tabular data, feature values themselves are often considered interpretable. This paper examines the impact of data engineering choices on local feature-based explanations. We demonstrate that simple, common data engineering techniques, such as representing age with a histogram or encoding race in a specific way, can manipulate feature importance as determined by popular methods like SHAP. Notably, the sensitivity of explanations to feature representation can be exploited by adversaries to obscure issues like discrimination. While the intuition behind these results is straightforward, their systematic exploration has been lacking. Previous work has focused on adversarial attacks on feature-based explainers by biasing data or manipulating models. To the best of our knowledge, this is the first study demonstrating that explainers can be misled by standard, seemingly innocuous data engineering techniques.","authors":["Hyunseung Hwang","Andrew Bell","Joao Fonseca","Venetia Pliatsika","Julia Stoyanovich","Steven Euijong Whang"],"url":"https://arxiv.org/abs/2505.08345"}
{"created":"2025-05-14","title":"A Bi-nested Calculus for Intuitionistic K: Proofs and Countermodels","abstract":"The logic IK is the intuitionistic variant of modal logic introduced by Fischer Servi, Plotkin and Stirling, and studied by Simpson. This logic is considered a fundamental intuitionstic modal system as it corresponds, modulo the standard translation, to a fragment of intuitionstic first-order logic. In this paper we present a labelled-free bi-nested sequent calculus for IK. This proof system comprises two kinds of nesting, corresponding to the two relations of bi-relational models for IK: a pre-order relation, from intuitionistic models, and a binary relation, akin to the accessibility relation of Kripke models. The calculus provides a decision procedure for IK by means of a suitable proof-search strategy. This is the first labelled-free calculus for IK which allows direct counter-model extraction: from a single failed derivation, it is possible to construct a finite countermodel for the formula at the root. We further show the bi-nested calculus can simulate both the (standard) nested calculus and labelled sequent calculus, which are two best known calculi proposed in the literature for IK.","authors":["Han Gao","Marianna Girlando","Nicola Olivetti"],"url":"https://arxiv.org/abs/2505.08347"}
{"created":"2025-05-14","title":"On the Geometry of Semantics in Next-token Prediction","abstract":"Modern language models demonstrate a remarkable ability to capture linguistic meaning despite being trained solely through next-token prediction (NTP). We investigate how this conceptually simple training objective leads models to extract and encode latent semantic and grammatical concepts. Our analysis reveals that NTP optimization implicitly guides models to encode concepts via singular value decomposition (SVD) factors of a centered data-sparsity matrix that captures next-word co-occurrence patterns. While the model never explicitly constructs this matrix, learned word and context embeddings effectively factor it to capture linguistic structure. We find that the most important SVD factors are learned first during training, motivating the use of spectral clustering of embeddings to identify human-interpretable semantics, including both classical k-means and a new orthant-based method directly motivated by our interpretation of concepts. Overall, our work bridges distributional semantics, neural collapse geometry, and neural network training dynamics, providing insights into how NTP's implicit biases shape the emergence of meaning representations in language models.","authors":["Yize Zhao","Christos Thrampoulidis"],"url":"https://arxiv.org/abs/2505.08348"}
{"created":"2025-05-14","title":"FAD: Frequency Adaptation and Diversion for Cross-domain Few-shot Learning","abstract":"Cross-domain few-shot learning (CD-FSL) requires models to generalize from limited labeled samples under significant distribution shifts. While recent methods enhance adaptability through lightweight task-specific modules, they operate solely in the spatial domain and overlook frequency-specific variations that are often critical for robust transfer. We observe that spatially similar images across domains can differ substantially in their spectral representations, with low and high frequencies capturing complementary semantic information at coarse and fine levels. This indicates that uniform spatial adaptation may overlook these spectral distinctions, thus constraining generalization. To address this, we introduce Frequency Adaptation and Diversion (FAD), a frequency-aware framework that explicitly models and modulates spectral components. At its core is the Frequency Diversion Adapter, which transforms intermediate features into the frequency domain using the discrete Fourier transform (DFT), partitions them into low, mid, and high-frequency bands via radial masks, and reconstructs each band using inverse DFT (IDFT). Each frequency band is then adapted using a dedicated convolutional branch with a kernel size tailored to its spectral scale, enabling targeted and disentangled adaptation across frequencies. Extensive experiments on the Meta-Dataset benchmark demonstrate that FAD consistently outperforms state-of-the-art methods on both seen and unseen domains, validating the utility of frequency-domain representations and band-wise adaptation for improving generalization in CD-FSL.","authors":["Ruixiao Shi","Fu Feng","Yucheng Xie","Jing Wang","Xin Geng"],"url":"https://arxiv.org/abs/2505.08349"}
{"created":"2025-05-14","title":"STORYANCHORS: Generating Consistent Multi-Scene Story Frames for Long-Form Narratives","abstract":"This paper introduces StoryAnchors, a unified framework for generating high-quality, multi-scene story frames with strong temporal consistency. The framework employs a bidirectional story generator that integrates both past and future contexts to ensure temporal consistency, character continuity, and smooth scene transitions throughout the narrative. Specific conditions are introduced to distinguish story frame generation from standard video synthesis, facilitating greater scene diversity and enhancing narrative richness. To further improve generation quality, StoryAnchors integrates Multi-Event Story Frame Labeling and Progressive Story Frame Training, enabling the model to capture both overarching narrative flow and event-level dynamics. This approach supports the creation of editable and expandable story frames, allowing for manual modifications and the generation of longer, more complex sequences. Extensive experiments show that StoryAnchors outperforms existing open-source models in key areas such as consistency, narrative coherence, and scene diversity. Its performance in narrative consistency and story richness is also on par with GPT-4o. Ultimately, StoryAnchors pushes the boundaries of story-driven frame generation, offering a scalable, flexible, and highly editable foundation for future research.","authors":["Bo Wang","Haoyang Huang","Zhiyin Lu","Fengyuan Liu","Guoqing Ma","Jianlong Yuan","Yuan Zhang","Nan Duan"],"url":"https://arxiv.org/abs/2505.08350"}
{"created":"2025-05-14","title":"Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring","abstract":"This paper investigates the potentials of Large Language Models (LLMs) as adaptive tutors in the context of second-language learning. In particular, we evaluate whether system prompting can reliably constrain LLMs to generate only text appropriate to the student's competence level. We simulate full teacher-student dialogues in Spanish using instruction-tuned, open-source LLMs ranging in size from 7B to 12B parameters. Dialogues are generated by having an LLM alternate between tutor and student roles with separate chat histories. The output from the tutor model is then used to evaluate the effectiveness of CEFR-based prompting to control text difficulty across three proficiency levels (A1, B1, C1). Our findings suggest that while system prompting can be used to constrain model outputs, prompting alone is too brittle for sustained, long-term interactional contexts - a phenomenon we term alignment drift. Our results provide insights into the feasibility of LLMs for personalized, proficiency-aligned adaptive tutors and provide a scalable method for low-cost evaluation of model performance without human participants.","authors":["Mina Almasi","Ross Deans Kristensen-McLachlan"],"url":"https://arxiv.org/abs/2505.08351"}
{"created":"2025-05-14","title":"Revisiting Information Diffusion Beyond Explicit Social Ties: A Study of Implicit-Link Diffusion on Twitter","abstract":"Information diffusion on social media platforms is often assumed to occur primarily through explicit social connections, such as follower or friend relationships. However, information frequently propagates beyond these observable ties -- via external websites, search engines, or algorithmic recommendations -- forming implicit links between users who are not directly connected. Despite their potential impact, the mechanisms and characteristics of such implicit-link diffusion remain underexplored. In this study, we investigate the dynamics of nontrivial information diffusion mediated by implicit links on Twitter, using four large-scale datasets. We define implicit-link diffusion as the reposting of content by users who are not explicitly connected to the original poster. Our analysis reveals that users located farther from the original source in the social network are more likely to engage in diffusion through implicit links, suggesting that such links often arise from sources outside direct social relationships. Moreover, while implicit links contribute less to the overall diffusion size than explicit links, they play a distinct role in disseminating content across diverse and topologically distant communities. We further identify user groups who predominantly engage in diffusion through either explicit or implicit links, and demonstrate that the choice of diffusion channel exhibits strong patterns of social homophily. These findings underscore the importance of incorporating implicit-link dynamics into models of information diffusion and social influence.","authors":["Yuto Tamura","Sho Tsugawa","Kohei Watabe"],"url":"https://arxiv.org/abs/2505.08354"}
{"created":"2025-05-14","title":"Lorentzian-Constrained Holographic Beamforming Optimization in Multi-user Networks with Dynamic Metasurface Antennas","abstract":"Dynamic metasurface antennas (DMAs) are promising alternatives to fully digital (FD) architectures, enabling hybrid beamforming via low-cost reconfigurable metasurfaces. In DMAs, holographic beamforming is achieved through tunable elements by Lorentzian-constrained holography (LCH), significantly reducing the need for radio-frequency (RF) chains and analog circuitry. However, the Lorentzian constraints and limited RF chains introduce a trade-off between reduced system complexity and beamforming performance, especially in dense network scenarios. This paper addresses resource allocation in multi-user multiple-input-single-output (MISO) networks under the Signal-to-Interference-plus-Noise Ratio (SINR) constraints, aiming to minimize total transmit power. We propose a holographic beamforming algorithm based on the Generalized Method of Lorentzian-Constrained Holography (GMLCH), which optimizes DMA weights, yielding flexibility for using various LCH techniques to tackle the aforementioned trade-offs. Building upon GMLCH, we further propose a new algorithm, Adaptive Radius Lorentzian Constrained Holography (ARLCH), which achieves optimization of DMA weights with additional degree of freedom in a greater optimization space, and provides lower transmitted power, while improving scalability for higher number of users. Numerical results show that ARLCH reduces power consumption by over 20% compared to benchmarks, with increasing effectiveness as the number of users grows.","authors":["Askin Altinoklu","Leila Musavian"],"url":"https://arxiv.org/abs/2505.08356"}
{"created":"2025-05-14","title":"A political cartography of news sharing: Capturing story, outlet and content level of news circulation on Twitter","abstract":"News sharing on digital platforms shapes the digital spaces millions of users navigate. Trace data from these platforms also enables researchers to study online news circulation. In this context, research on the types of news shared by users of differential political leaning has received considerable attention. We argue that most existing approaches (i) rely on an overly simplified measurement of political leaning, (ii) consider only the outlet level in their analyses, and/or (iii) study news circulation among partisans by making ex-ante distinctions between partisan and non-partisan news. In this methodological contribution, we introduce a research pipeline that allows a systematic mapping of news sharing both with respect to source and content. As a proof of concept, we demonstrate insights that otherwise remain unnoticed: Diversification of news sharing along the second political dimension; topic-dependent sharing of outlets; some outlets catering different items to different audiences.","authors":["Felix Gaisbauer","Armin Pournaki","Jakob Ohme"],"url":"https://arxiv.org/abs/2505.08359"}
{"created":"2025-05-14","title":"A Comparison Between Human and Generative AI Decision-Making Attributes in Complex Health Services","abstract":"A comparison between human and Generative AI decision-making attributes in complex health services is a knowledge gap in the literature, at present. Humans may possess unique attributes beneficial to decision-making in complex health services such as health policy and health regulation, but are also susceptible to decision-making flaws. The objective is to explore whether humans have unique, and/or helpful attributes that contribute to optimal decision-making in complex health services. This comparison may also shed light on whether humans are likely to compete, cooperate, or converge with Generative AI. The comparison is based on two published reviews: a scoping review of human attributes [1] and a rapid review of Generative AI attributes [2]. The analysis categorizes attributes by uniqueness and impact. The results are presented in tabular form, comparing the sets and subsets of human and Generative AI attributes. Humans and Generative AI decision-making attributes have complementary strengths. Cooperation between these two entities seems more likely than pure competition. To maintain meaningful decision-making roles, humans could develop their unique attributes, with decision-making systems integrating both human and Generative AI contributions. These entities may also converge, in future.","authors":["Nandini Doreswamy (Southern Cross University","Lismore","New South Wales","Australia","National Coalition of Independent Scholars)","Louise Horstmanshof (Southern Cross University","Lismore","New South Wales","Australia)"],"url":"https://arxiv.org/abs/2505.08360"}
{"created":"2025-05-14","title":"Modeling Unseen Environments with Language-guided Composable Causal Components in Reinforcement Learning","abstract":"Generalization in reinforcement learning (RL) remains a significant challenge, especially when agents encounter novel environments with unseen dynamics. Drawing inspiration from human compositional reasoning -- where known components are reconfigured to handle new situations -- we introduce World Modeling with Compositional Causal Components (WM3C). This novel framework enhances RL generalization by learning and leveraging compositional causal components. Unlike previous approaches focusing on invariant representation learning or meta-learning, WM3C identifies and utilizes causal dynamics among composable elements, facilitating robust adaptation to new tasks. Our approach integrates language as a compositional modality to decompose the latent space into meaningful components and provides theoretical guarantees for their unique identification under mild assumptions. Our practical implementation uses a masked autoencoder with mutual information constraints and adaptive sparsity regularization to capture high-level semantic information and effectively disentangle transition dynamics. Experiments on numerical simulations and real-world robotic manipulation tasks demonstrate that WM3C significantly outperforms existing methods in identifying latent processes, improving policy learning, and generalizing to unseen tasks.","authors":["Xinyue Wang","Biwei Huang"],"url":"https://arxiv.org/abs/2505.08361"}
{"created":"2025-05-14","title":"Localization of Impacts on Thin-Walled Structures by Recurrent Neural Networks: End-to-end Learning from Real-World Data","abstract":"Today, machine learning is ubiquitous, and structural health monitoring (SHM) is no exception. Specifically, we address the problem of impact localization on shell-like structures, where knowledge of impact locations aids in assessing structural integrity. Impacts on thin-walled structures excite Lamb waves, which can be measured with piezoelectric sensors. Their dispersive characteristics make it difficult to detect and localize impacts by conventional methods. In the present contribution, we explore the localization of impacts using neural networks. In particular, we propose to use {recurrent neural networks} (RNNs) to estimate impact positions end-to-end, i.e., directly from {sequential sensor data}. We deal with comparatively long sequences of thousands of samples, since high sampling rate are needed to accurately capture elastic waves. For this reason, the proposed approach builds upon Gated Recurrent Units (GRUs), which are less prone to vanishing gradients as compared to conventional RNNs. Quality and quantity of data are crucial when training neural networks. Often, synthetic data is used, which inevitably introduces a reality gap. Here, by contrast, we train our networks using {physical data from experiments}, which requires automation to handle the large number of experiments needed. For this purpose, a {robot is used to drop steel balls} onto an {aluminum plate} equipped with {piezoceramic sensors}. Our results show remarkable accuracy in estimating impact positions, even with a comparatively small dataset.","authors":["Alexander Humer","Lukas Grasboeck","Ayech Benjeddou"],"url":"https://arxiv.org/abs/2505.08362"}
{"created":"2025-05-14","title":"Learning Like Humans: Advancing LLM Reasoning Capabilities via Adaptive Difficulty Curriculum Learning and Expert-Guided Self-Reformulation","abstract":"Despite impressive progress in areas like mathematical reasoning, large language models still face significant challenges in consistently solving complex problems. Drawing inspiration from key human learning strategies, we propose two novel strategies to enhance the capability of large language models to solve these complex problems. First, Adaptive Difficulty Curriculum Learning (ADCL) is a novel curriculum learning strategy that tackles the Difficulty Shift phenomenon (i.e., a model's perception of problem difficulty dynamically changes during training) by periodically re-estimating difficulty within upcoming data batches to maintain alignment with the model's evolving capabilities. Second, Expert-Guided Self-Reformulation (EGSR) is a novel reinforcement learning strategy that bridges the gap between imitation learning and pure exploration by guiding models to reformulate expert solutions within their own conceptual framework, rather than relying on direct imitation, fostering deeper understanding and knowledge assimilation. Extensive experiments on challenging mathematical reasoning benchmarks, using Qwen2.5-7B as the base model, demonstrate that these human-inspired strategies synergistically and significantly enhance performance. Notably, their combined application improves performance over the standard Zero-RL baseline by 10% on the AIME24 benchmark and 16.6% on AIME25.","authors":["Enci Zhang","Xingang Yan","Wei Lin","Tianxiang Zhang","Qianchun Lu"],"url":"https://arxiv.org/abs/2505.08364"}
{"created":"2025-05-14","title":"MA-ROESL: Motion-aware Rapid Reward Optimization for Efficient Robot Skill Learning from Single Videos","abstract":"Vision-language models (VLMs) have demonstrated excellent high-level planning capabilities, enabling locomotion skill learning from video demonstrations without the need for meticulous human-level reward design. However, the improper frame sampling method and low training efficiency of current methods remain a critical bottleneck, resulting in substantial computational overhead and time costs. To address this limitation, we propose Motion-aware Rapid Reward Optimization for Efficient Robot Skill Learning from Single Videos (MA-ROESL). MA-ROESL integrates a motion-aware frame selection method to implicitly enhance the quality of VLM-generated reward functions. It further employs a hybrid three-phase training pipeline that improves training efficiency via rapid reward optimization and derives the final policy through online fine-tuning. Experimental results demonstrate that MA-ROESL significantly enhances training efficiency while faithfully reproducing locomotion skills in both simulated and real-world settings, thereby underscoring its potential as a robust and scalable framework for efficient robot locomotion skill learning from video demonstrations.","authors":["Xianghui Wang","Xinming Zhang","Yanjun Chen","Xiaoyu Shen","Wei Zhang"],"url":"https://arxiv.org/abs/2505.08367"}
{"created":"2025-05-14","title":"Density Ratio-based Causal Discovery from Bivariate Continuous-Discrete Data","abstract":"This paper proposes a causal discovery method for mixed bivariate data consisting of one continuous and one discrete variable. Existing constraint-based approaches are ineffective in the bivariate setting, as they rely on conditional independence tests that are not suited to bivariate data. Score-based methods either impose strong distributional assumptions or face challenges in fairly comparing causal directions between variables of different types, due to differences in their information content. We introduce a novel approach that determines causal direction by analyzing the monotonicity of the conditional density ratio of the continuous variable, conditioned on different values of the discrete variable. Our theoretical analysis shows that the conditional density ratio exhibits monotonicity when the continuous variable causes the discrete variable, but not in the reverse direction. This property provides a principled basis for comparing causal directions between variables of different types, free from strong distributional assumptions and bias arising from differences in their information content. We demonstrate its effectiveness through experiments on both synthetic and real-world datasets, showing superior accuracy compared to existing methods.","authors":["Takashi Nicholas Maeda","Shohei Shimizu","Hidetoshi Matsui"],"url":"https://arxiv.org/abs/2505.08371"}
{"created":"2025-05-14","title":"Human-in-the-Loop Optimization for Inclusive Design: Balancing Automation and Designer Expertise","abstract":"Accessible and inclusive design has gained increased attention in HCI, yet practical implementation remains challenging due to resource-intensive prototyping methods. Traditional approaches such as workshops, A-B tests, and co-design sessions struggle to capture the diverse and complex needs of users with disabilities at scale. This position paper argues for an automated, accessible Human-in-the-Loop (HITL) design optimization process that shifts the designer's role from directly crafting prototypes to curating constraints for algorithmic exploration. By pre-constraining the design space based on specific user interaction needs, integrating adaptive multi-modal feedback channels, and personalizing feedback prompts, the HITL approach could efficiently refine design parameters, such as text size, color contrast, layout, and interaction modalities, to achieve optimal accessibility. This approach promises scalable, individualized design solutions while raising critical questions about constraint curation, transparency, user agency, and ethical considerations, making it essential to discuss and refine these ideas collaboratively at the workshop.","authors":["Pascal Jansen"],"url":"https://arxiv.org/abs/2505.08375"}
{"created":"2025-05-14","title":"Adaptive Diffusion Policy Optimization for Robotic Manipulation","abstract":"Recent studies have shown the great potential of diffusion models in improving reinforcement learning (RL) by modeling complex policies, expressing a high degree of multi-modality, and efficiently handling high-dimensional continuous control tasks. However, there is currently limited research on how to optimize diffusion-based polices (e.g., Diffusion Policy) fast and stably. In this paper, we propose an Adam-based Diffusion Policy Optimization (ADPO), a fast algorithmic framework containing best practices for fine-tuning diffusion-based polices in robotic control tasks using the adaptive gradient descent method in RL. Adaptive gradient method is less studied in training RL, let alone diffusion-based policies. We confirm that ADPO outperforms other diffusion-based RL methods in terms of overall effectiveness for fine-tuning on standard robotic tasks. Concretely, we conduct extensive experiments on standard robotic control tasks to test ADPO, where, particularly, six popular diffusion-based RL methods are provided as benchmark methods. Experimental results show that ADPO acquires better or comparable performance than the baseline methods. Finally, we systematically analyze the sensitivity of multiple hyperparameters in standard robotics tasks, providing guidance for subsequent practical applications. Our video demonstrations are released in https://github.com/Timeless-lab/ADPO.git.","authors":["Huiyun Jiang","Zhuang Yang"],"url":"https://arxiv.org/abs/2505.08376"}
{"created":"2025-05-14","title":"Continuous World Coverage Path Planning for Fixed-Wing UAVs using Deep Reinforcement Learning","abstract":"Unmanned Aerial Vehicle (UAV) Coverage Path Planning (CPP) is critical for applications such as precision agriculture and search and rescue. While traditional methods rely on discrete grid-based representations, real-world UAV operations require power-efficient continuous motion planning. We formulate the UAV CPP problem in a continuous environment, minimizing power consumption while ensuring complete coverage. Our approach models the environment with variable-size axis-aligned rectangles and UAV motion with curvature-constrained B\\'ezier curves. We train a reinforcement learning agent using an action-mapping-based Soft Actor-Critic (AM-SAC) algorithm employing a self-adaptive curriculum. Experiments on both procedurally generated and hand-crafted scenarios demonstrate the effectiveness of our method in learning energy-efficient coverage strategies.","authors":["Mirco Theile","Andres R. Zapata Rodriguez","Marco Caccamo","Alberto L. Sangiovanni-Vincentelli"],"url":"https://arxiv.org/abs/2505.08382"}
{"created":"2025-05-14","title":"TikTok Search Recommendations: Governance and Research Challenges","abstract":"Like other social media, TikTok is embracing its use as a search engine, developing search products to steer users to produce searchable content and engage in content discovery. Their recently developed product search recommendations are preformulated search queries recommended to users on videos. However, TikTok provides limited transparency about how search recommendations are generated and moderated, despite requirements under regulatory frameworks like the European Union's Digital Services Act. By suggesting that the platform simply aggregates comments and common searches linked to videos, it sidesteps responsibility and issues that arise from contextually problematic recommendations, reigniting long-standing concerns about platform liability and moderation. This position paper addresses the novelty of search recommendations on TikTok by highlighting the challenges that this feature poses for platform governance and offering a computational research agenda, drawing on preliminary qualitative analysis. It sets out the need for transparency in platform documentation, data access and research to study search recommendations.","authors":["Taylor Annabell","Robert Gorwa","Rebecca Scharlach","Jacob van de Kerkhof","Thales Bertaglia"],"url":"https://arxiv.org/abs/2505.08385"}
{"created":"2025-05-14","title":"The Lax--Wendroff theorem for Patankar-type methods applied to hyperbolic conservation laws","abstract":"For hyperbolic conservation laws, the famous Lax--Wendroff theorem delivers sufficient conditions for the limit of a convergent numerical method to be a weak (entropy) solution. This theorem is a fundamental result, and many investigations have been done to verify its validity for finite difference, finite volume, and finite element schemes, using either explicit or implicit linear time-integration methods. Recently, the use of modified Patankar (MP) schemes as time-integration methods for the discretization of hyperbolic conservation laws has gained increasing interest. These schemes are unconditionally conservative and positivity-preserving and only require the solution of a linear system. However, MP schemes are by construction nonlinear, which is why the theoretical investigation of these schemes is more involved. We prove an extension of the Lax--Wendroff theorem for the class of MP methods. This is the first extension of the Lax--Wendroff theorem to nonlinear time integration methods with just an additional hypothesis on the total time variation boundness of the numerical solutions. We provide some numerical simulations that validate the theoretical observations.","authors":["Janina Bender","Thomas Izgin","Philipp \\\"Offner","Davide Torlo"],"url":"https://arxiv.org/abs/2505.08387"}
{"created":"2025-05-14","title":"MDF: Multi-Modal Data Fusion with CNN-Based Object Detection for Enhanced Indoor Localization Using LiDAR-SLAM","abstract":"Indoor localization faces persistent challenges in achieving high accuracy, particularly in GPS-deprived environments. This study unveils a cutting-edge handheld indoor localization system that integrates 2D LiDAR and IMU sensors, delivering enhanced high-velocity precision mapping, computational efficiency, and real-time adaptability. Unlike 3D LiDAR systems, it excels with rapid processing, low-cost scalability, and robust performance, setting new standards for emergency response, autonomous navigation, and industrial automation. Enhanced with a CNN-driven object detection framework and optimized through Cartographer SLAM (simultaneous localization and mapping ) in ROS, the system significantly reduces Absolute Trajectory Error (ATE) by 21.03%, achieving exceptional precision compared to state-of-the-art approaches like SC-ALOAM, with a mean x-position error of -0.884 meters (1.976 meters). The integration of CNN-based object detection ensures robustness in mapping and localization, even in cluttered or dynamic environments, outperforming existing methods by 26.09%. These advancements establish the system as a reliable, scalable solution for high-precision localization in challenging indoor scenarios","authors":["Saqi Hussain Kalan","Boon Giin Lee","Wan-Young Chung"],"url":"https://arxiv.org/abs/2505.08388"}
{"created":"2025-05-14","title":"Towards Contamination Resistant Benchmarks","abstract":"The rapid development of large language models (LLMs) has transformed the landscape of natural language processing. Evaluating LLMs properly is crucial for understanding their potential and addressing concerns such as safety. However, LLM evaluation is confronted by various factors, among which contamination stands out as a key issue that undermines the reliability of evaluations. In this work, we introduce the concept of contamination resistance to address this challenge. We propose a benchmark based on Caesar ciphers (e.g., \"ab\" to \"bc\" when the shift is 1), which, despite its simplicity, is an excellent example of a contamination resistant benchmark. We test this benchmark on widely used LLMs under various settings, and we find that these models struggle with this benchmark when contamination is controlled. Our findings reveal issues in current LLMs and raise important questions regarding their true capabilities. Our work contributes to the development of contamination resistant benchmarks, enabling more rigorous LLM evaluation and offering insights into the true capabilities and limitations of LLMs.","authors":["Rahmatullah Musawi","Sheng Lu"],"url":"https://arxiv.org/abs/2505.08389"}
{"created":"2025-05-14","title":"Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping","abstract":"Large Language Models leverage Chain-of-Thought (CoT) prompting for complex tasks, but their reasoning traces are often excessively verbose and inefficient, leading to significant computational costs and latency. Current CoT compression techniques typically rely on generic importance metrics and static compression rates, which may inadvertently remove functionally critical tokens or fail to adapt to varying reasoning complexity. To overcome these limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning. This approach introduces two synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric accurately identifying functionally relevant tokens by measuring the gradient influence of their intermediate representations on the final answer loss, and (2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the compression rate based on runtime model uncertainty while ensuring local coherence through an adaptive N-token constraint. To our knowledge, this is the first work unifying a goal-oriented, gradient-based importance metric with dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It achieves substantial efficiency gains - reducing CoT token counts by over 45% on average and delivering 1.6-2.0 times inference speedups - while maintaining high reasoning accuracy. Notably, it significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.","authors":["Ren Zhuang","Ben Wang","Shuifa Sun"],"url":"https://arxiv.org/abs/2505.08392"}
{"created":"2025-05-14","title":"TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers","abstract":"Recently, large language models(LLMs) have played an increasingly important role in solving a wide range of NLP tasks, leveraging their capabilities of natural language understanding and generating. Integration with external tools further enhances LLMs' effectiveness, providing more precise, timely, and specialized responses. However, LLMs still encounter difficulties with non-executable actions and improper actions, which are primarily attributed to incorrect parameters. The process of generating parameters by LLMs is confined to the tool level, employing the coarse-grained strategy without considering the different difficulties of various tools. To address this issue, we propose TUMS, a novel framework designed to enhance the tool-use capabilities of LLMs by transforming tool-level processing into parameter-level processing. Specifically, our framework consists of four key components: (1) an intent recognizer that identifies the user's intent to help LLMs better understand the task; (2) a task decomposer that breaks down complex tasks into simpler subtasks, each involving a tool call; (3) a subtask processor equipped with multi-structure handlers to generate accurate parameters; and (4) an executor. Our empirical studies have evidenced the effectiveness and efficiency of the TUMS framework with an average of 19.6\\% and 50.6\\% improvement separately on easy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key contribution of each part with ablation experiments, offering more insights and stimulating future research on Tool-augmented LLMs.","authors":["Aiyao He","Sijia Cui","Shuai Xu","Yanna Wang","Bo Xu"],"url":"https://arxiv.org/abs/2505.08402"}
{"created":"2025-05-14","title":"ConDiSim: Conditional Diffusion Models for Simulation Based Inference","abstract":"We present a conditional diffusion model - ConDiSim, for simulation-based inference of complex systems with intractable likelihoods. ConDiSim leverages denoising diffusion probabilistic models to approximate posterior distributions, consisting of a forward process that adds Gaussian noise to parameters, and a reverse process learning to denoise, conditioned on observed data. This approach effectively captures complex dependencies and multi-modalities within posteriors. ConDiSim is evaluated across ten benchmark problems and two real-world test problems, where it demonstrates effective posterior approximation accuracy while maintaining computational efficiency and stability in model training. ConDiSim offers a robust and extensible framework for simulation-based inference, particularly suitable for parameter inference workflows requiring fast inference methods.","authors":["Mayank Nautiyal","Andreas Hellander","Prashant Singh"],"url":"https://arxiv.org/abs/2505.08403"}
{"created":"2025-05-14","title":"Explaining Autonomous Vehicles with Intention-aware Policy Graphs","abstract":"The potential to improve road safety, reduce human driving error, and promote environmental sustainability have enabled the field of autonomous driving to progress rapidly over recent decades. The performance of autonomous vehicles has significantly improved thanks to advancements in Artificial Intelligence, particularly Deep Learning. Nevertheless, the opacity of their decision-making, rooted in the use of accurate yet complex AI models, has created barriers to their societal trust and regulatory acceptance, raising the need for explainability. We propose a post-hoc, model-agnostic solution to provide teleological explanations for the behaviour of an autonomous vehicle in urban environments. Building on Intention-aware Policy Graphs, our approach enables the extraction of interpretable and reliable explanations of vehicle behaviour in the nuScenes dataset from global and local perspectives. We demonstrate the potential of these explanations to assess whether the vehicle operates within acceptable legal boundaries and to identify possible vulnerabilities in autonomous driving datasets and models.","authors":["Sara Montese","Victor Gimenez-Abalos","Atia Cort\\'es","Ulises Cort\\'es","Sergio Alvarez-Napagao"],"url":"https://arxiv.org/abs/2505.08404"}
{"created":"2025-05-14","title":"Lost in Transliteration: Bridging the Script Gap in Neural IR","abstract":"Most human languages use scripts other than the Latin alphabet. Search users in these languages often formulate their information needs in a transliterated -- usually Latinized -- form for ease of typing. For example, Greek speakers might use Greeklish, and Arabic speakers might use Arabizi. This paper shows that current search systems, including those that use multilingual dense embeddings such as BGE-M3, do not generalise to this setting, and their performance rapidly deteriorates when exposed to transliterated queries. This creates a ``script gap\" between the performance of the same queries when written in their native or transliterated form. We explore whether adapting the popular ``translate-train\" paradigm to transliterations can enhance the robustness of multilingual Information Retrieval (IR) methods and bridge the gap between native and transliterated scripts. By exploring various combinations of non-Latin and Latinized query text for training, we investigate whether we can enhance the capacity of existing neural retrieval techniques and enable them to apply to this important setting. We show that by further fine-tuning IR models on an even mixture of native and Latinized text, they can perform this cross-script matching at nearly the same performance as when the query was formulated in the native script. Out-of-domain evaluation and further qualitative analysis show that transliterations can also cause queries to lose some of their nuances, motivating further research in this direction.","authors":["Andreas Chari","Iadh Ounis","Sean MacAvaney"],"url":"https://arxiv.org/abs/2505.08411"}
{"created":"2025-05-14","title":"ORACLE-Grasp: Zero-Shot Task-Oriented Robotic Grasping using Large Multimodal Models","abstract":"Grasping unknown objects in unstructured environments remains a fundamental challenge in robotics, requiring both semantic understanding and spatial reasoning. Existing methods often rely on dense training datasets or explicit geometric modeling, limiting their scalability to real-world tasks. Recent advances in Large Multimodal Models (LMMs) offer new possibilities for integrating vision and language understanding, but their application to autonomous robotic grasping remains largely unexplored. We present ORACLE-Grasp, a zero-shot framework that leverages LMMs as semantic oracles to guide grasp selection without requiring additional training or human input. The system formulates grasp prediction as a structured, iterative decision process, using dual-prompt tool calling to first extract high-level object context and then select task-relevant grasp regions. By discretizing the image space and reasoning over candidate areas, ORACLE-Grasp mitigates the spatial imprecision common in LMMs and produces human-like, task-driven grasp suggestions. Early stopping and depth-based refinement steps further enhance efficiency and physical grasp reliability. Experiments demonstrate that the predicted grasps achieve low positional and orientation errors relative to human-annotated ground truth and lead to high success rates in real-world pick up tasks. These results highlight the potential of combining language-driven reasoning with lightweight vision techniques to enable robust, autonomous grasping without task-specific datasets or retraining.","authors":["Avihai Giuili","Rotem Atari","Avishai Sintov"],"url":"https://arxiv.org/abs/2505.08417"}
{"created":"2025-05-14","title":"HMR-ODTA: Online Diverse Task Allocation for a Team of Heterogeneous Mobile Robots","abstract":"Coordinating time-sensitive deliveries in environments like hospitals poses a complex challenge, particularly when managing multiple online pickup and delivery requests within strict time windows using a team of heterogeneous robots. Traditional approaches fail to address dynamic rescheduling or diverse service requirements, typically restricting robots to single-task types. This paper tackles the Multi-Pickup and Delivery Problem with Time Windows (MPDPTW), where autonomous mobile robots are capable of handling varied service requests. The objective is to minimize late delivery penalties while maximizing task completion rates. To achieve this, we propose a novel framework leveraging a heterogeneous robot team and an efficient dynamic scheduling algorithm that supports dynamic task rescheduling. Users submit requests with specific time constraints, and our decentralized algorithm, Heterogeneous Mobile Robots Online Diverse Task Allocation (HMR-ODTA), optimizes task assignments to ensure timely service while addressing delays or task rejections. Extensive simulations validate the algorithm's effectiveness. For smaller task sets (40-160 tasks), penalties were reduced by nearly 63%, while for larger sets (160-280 tasks), penalties decreased by approximately 50%. These results highlight the algorithm's effectiveness in improving task scheduling and coordination in multi-robot systems, offering a robust solution for enhancing delivery performance in structured, time-critical environments.","authors":["Ashish Verma","Avinash Gautam","Tanishq Duhan","V. S. Shekhawat","Sudeept Mohan"],"url":"https://arxiv.org/abs/2505.08419"}
{"created":"2025-05-14","title":"e-GPU: An Open-Source and Configurable RISC-V Graphic Processing Unit for TinyAI Applications","abstract":"Graphics processing units (GPUs) excel at parallel processing, but remain largely unexplored in ultra-low-power edge devices (TinyAI) due to their power and area limitations, as well as the lack of suitable programming frameworks. To address these challenges, this work introduces embedded GPU (e-GPU), an open-source and configurable RISC-V GPU platform designed for TinyAI devices. Its extensive configurability enables area and power optimization, while a dedicated Tiny-OpenCL implementation provides a lightweight programming framework tailored to resource-constrained environments. To demonstrate its adaptability in real-world scenarios, we integrate the e-GPU with the eXtendible Heterogeneous Energy-Efficient Platform (X-HEEP) to realize an accelerated processing unit (APU) for TinyAI applications. Multiple instances of the proposed system, featuring varying e-GPU configurations, are implemented in TSMC's 16 nm SVT CMOS technology and are operated at 300 MHz and 0.8 V. Their area and leakage characteristics are analyzed to ensure alignment with TinyAI constraints. To assess both runtime overheads and application-level efficiency, we employ two benchmarks: General Matrix Multiply (GeMM) and bio-signal processing (TinyBio) workloads. The GeMM benchmark is used to quantify the scheduling overhead introduced by the Tiny-OpenCL framework. The results show that the delay becomes negligible for matrix sizes larger than 256x256 (or equivalent problem sizes). The TinyBio benchmark is then used to evaluate performance and energy improvements in the baseline host. The results demonstrate that the high-range e-GPU configuration with 16 threads achieves up to a 15.1x speed-up and reduces energy consumption by up to 3.1x, while incurring only a 2.5x area overhead and operating within a 28 mW power budget.","authors":["Simone Machetti","Pasquale Davide Schiavone","Lara Orlandic","Darong Huang","Deniz Kasap","Giovanni Ansaloni","David Atienza"],"url":"https://arxiv.org/abs/2505.08421"}
{"created":"2025-05-14","title":"DArFace: Deformation Aware Robustness for Low Quality Face Recognition","abstract":"Facial recognition systems have achieved remarkable success by leveraging deep neural networks, advanced loss functions, and large-scale datasets. However, their performance often deteriorates in real-world scenarios involving low-quality facial images. Such degradations, common in surveillance footage or standoff imaging include low resolution, motion blur, and various distortions, resulting in a substantial domain gap from the high-quality data typically used during training. While existing approaches attempt to address robustness by modifying network architectures or modeling global spatial transformations, they frequently overlook local, non-rigid deformations that are inherently present in real-world settings. In this work, we introduce DArFace, a Deformation-Aware robust Face recognition framework that enhances robustness to such degradations without requiring paired high- and low-quality training samples. Our method adversarially integrates both global transformations (e.g., rotation, translation) and local elastic deformations during training to simulate realistic low-quality conditions. Moreover, we introduce a contrastive objective to enforce identity consistency across different deformed views. Extensive evaluations on low-quality benchmarks including TinyFace, IJB-B, and IJB-C demonstrate that DArFace surpasses state-of-the-art methods, with significant gains attributed to the inclusion of local deformation modeling.","authors":["Sadaf Gulshad","Abdullah Aldahlawi Thakaa"],"url":"https://arxiv.org/abs/2505.08423"}
{"created":"2025-05-14","title":"DHECA-SuperGaze: Dual Head-Eye Cross-Attention and Super-Resolution for Unconstrained Gaze Estimation","abstract":"Unconstrained gaze estimation is the process of determining where a subject is directing their visual attention in uncontrolled environments. Gaze estimation systems are important for a myriad of tasks such as driver distraction monitoring, exam proctoring, accessibility features in modern software, etc. However, these systems face challenges in real-world scenarios, partially due to the low resolution of in-the-wild images and partially due to insufficient modeling of head-eye interactions in current state-of-the-art (SOTA) methods. This paper introduces DHECA-SuperGaze, a deep learning-based method that advances gaze prediction through super-resolution (SR) and a dual head-eye cross-attention (DHECA) module. Our dual-branch convolutional backbone processes eye and multiscale SR head images, while the proposed DHECA module enables bidirectional feature refinement between the extracted visual features through cross-attention mechanisms. Furthermore, we identified critical annotation errors in one of the most diverse and widely used gaze estimation datasets, Gaze360, and rectified the mislabeled data. Performance evaluation on Gaze360 and GFIE datasets demonstrates superior within-dataset performance of the proposed method, reducing angular error (AE) by 0.48{\\deg} (Gaze360) and 2.95{\\deg} (GFIE) in static configurations, and 0.59{\\deg} (Gaze360) and 3.00{\\deg} (GFIE) in temporal settings compared to prior SOTA methods. Cross-dataset testing shows improvements in AE of more than 1.53{\\deg} (Gaze360) and 3.99{\\deg} (GFIE) in both static and temporal settings, validating the robust generalization properties of our approach.","authors":["Franko \\v{S}iki\\'c","Donik Vr\\v{s}nak","Sven Lon\\v{c}ari\\'c"],"url":"https://arxiv.org/abs/2505.08426"}
{"created":"2025-05-14","title":"Lower bounds for the reach and applications","abstract":"The reach of a submanifold of $\\mathbb{R}^N$ is defined as the largest radius of a tubular neighbourhood around the submanifold that avoids self-intersections. While essential in geometric and topological applications, computing the reach explicitly is notoriously difficult. In this paper, we introduce a rigorous and practical method to compute a guaranteed lower bound for the reach of a submanifold described as the common zero-set of finitely many smooth functions, not necessarily polynomials. Our algorithm uses techniques from numerically verified proofs and is particularly suitable for high-performance parallel implementations.","authors":["Daniel Platt","Ra\\'ul S\\'anchez Gal\\'an"],"url":"https://arxiv.org/abs/2505.08427"}
{"created":"2025-05-14","title":"Visual Image Reconstruction from Brain Activity via Latent Representation","abstract":"Visual image reconstruction, the decoding of perceptual content from brain activity into images, has advanced significantly with the integration of deep neural networks (DNNs) and generative models. This review traces the field's evolution from early classification approaches to sophisticated reconstructions that capture detailed, subjective visual experiences, emphasizing the roles of hierarchical latent representations, compositional strategies, and modular architectures. Despite notable progress, challenges remain, such as achieving true zero-shot generalization for unseen images and accurately modeling the complex, subjective aspects of perception. We discuss the need for diverse datasets, refined evaluation metrics aligned with human perceptual judgments, and compositional representations that strengthen model robustness and generalizability. Ethical issues, including privacy, consent, and potential misuse, are underscored as critical considerations for responsible development. Visual image reconstruction offers promising insights into neural coding and enables new psychological measurements of visual experiences, with applications spanning clinical diagnostics and brain-machine interfaces.","authors":["Yukiyasu Kamitani","Misato Tanaka","Ken Shirakawa"],"url":"https://arxiv.org/abs/2505.08429"}
{"created":"2025-05-14","title":"Low-complexity Detection for Noncoherent Massive MIMO Communications","abstract":"This work studies a point-to-point MIMO uplink in which user equipment transmits data to a base station equipped with a massive array. Signal detection is noncoherent and fading is assumed to follow the Weichselberger model. By exploiting the spatial stationarity of fading at the base station, a cyclostationary structure emerges naturally in the space-time representation, which suggests formulating the statistical properties of the received signal in the Karhunen-Lo\\`eve domain. This allows the derivation of a low-complexity receiver that approximates maximum likelihood detection even for a moderate array size. The spectral analysis of the problem provides valuable insights on the design of space-time codewords.","authors":["Marc Vil\\`a-Insa","Jaume Riba"],"url":"https://arxiv.org/abs/2505.08432"}
{"created":"2025-05-14","title":"On the Use of CVRP to Diagnose Faulty Elements in Antenna Arrays","abstract":"This paper investigates the application of Constrained-View Radiated Power (CVRP) for diagnosing phased array element failures, specifically focusing on on-off element failure. CVRP, similar to Partial Radiated Power (PRP), considers a specific Field-of-View (FoV) but normalizes it by the FoV area. The study explores CVRP's effectiveness in detecting failures in a 2x8 cosine element array under beam-steering conditions, accounting for random and depointing errors, angular resolution, and pattern rotation. Results indicate that CVRP can detect on-off failures based on angular resolution and error severity, under the assumption of reduced Total Radiated Power (TRP) with element failures. Additionally, CVRP is effective with partial far-field patterns, making it suitable for near-field, indirect far-field, and far-field measurement systems without requiring phase acquisition in the latter two.","authors":["Alejandro Ant\\'on Ruiz","John Kvarnstrand","Klas Arvidsson","Andr\\'es Alay\\'on Glazunov"],"url":"https://arxiv.org/abs/2505.08433"}
{"created":"2025-05-14","title":"Hakim: Farsi Text Embedding Model","abstract":"Recent advancements in text embedding have significantly improved natural language understanding across many languages, yet Persian remains notably underrepresented in large-scale embedding research. In this paper, we present Hakim, a novel state-of-the-art Persian text embedding model that achieves a 8.5% performance improvement over existing approaches on the FaMTEB benchmark, outperforming all previously developed Persian language models. As part of this work, we introduce three new datasets - Corpesia, Pairsia-sup, and Pairsia-unsup - to support supervised and unsupervised training scenarios. Additionally, Hakim is designed for applications in chatbots and retrieval-augmented generation (RAG) systems, particularly addressing retrieval tasks that require incorporating message history within these systems. We also propose a new baseline model built on the BERT architecture. Our language model consistently achieves higher accuracy across various Persian NLP tasks, while the RetroMAE-based model proves particularly effective for textual information retrieval applications. Together, these contributions establish a new foundation for advancing Persian language understanding.","authors":["Mehran Sarmadi","Morteza Alikhani","Erfan Zinvandi","Zahra Pourbahman"],"url":"https://arxiv.org/abs/2505.08435"}
{"created":"2025-05-14","title":"TT-DF: A Large-Scale Diffusion-Based Dataset and Benchmark for Human Body Forgery Detection","abstract":"The emergence and popularity of facial deepfake methods spur the vigorous development of deepfake datasets and facial forgery detection, which to some extent alleviates the security concerns about facial-related artificial intelligence technologies. However, when it comes to human body forgery, there has been a persistent lack of datasets and detection methods, due to the later inception and complexity of human body generation methods. To mitigate this issue, we introduce TikTok-DeepFake (TT-DF), a novel large-scale diffusion-based dataset containing 6,120 forged videos with 1,378,857 synthetic frames, specifically tailored for body forgery detection. TT-DF offers a wide variety of forgery methods, involving multiple advanced human image animation models utilized for manipulation, two generative configurations based on the disentanglement of identity and pose information, as well as different compressed versions. The aim is to simulate any potential unseen forged data in the wild as comprehensively as possible, and we also furnish a benchmark on TT-DF. Additionally, we propose an adapted body forgery detection model, Temporal Optical Flow Network (TOF-Net), which exploits the spatiotemporal inconsistencies and optical flow distribution differences between natural data and forged data. Our experiments demonstrate that TOF-Net achieves favorable performance on TT-DF, outperforming current state-of-the-art extendable facial forgery detection models. For our TT-DF dataset, please refer to https://github.com/HashTAG00002/TT-DF.","authors":["Wenkui Yang","Zhida Zhang","Xiaoqiang Zhou","Junxian Duan","Jie Cao"],"url":"https://arxiv.org/abs/2505.08437"}
{"created":"2025-05-14","title":"A Survey of 3D Reconstruction with Event Cameras: From Event-based Geometry to Neural 3D Rendering","abstract":"Event cameras have emerged as promising sensors for 3D reconstruction due to their ability to capture per-pixel brightness changes asynchronously. Unlike conventional frame-based cameras, they produce sparse and temporally rich data streams, which enable more accurate 3D reconstruction and open up the possibility of performing reconstruction in extreme environments such as high-speed motion, low light, or high dynamic range scenes. In this survey, we provide the first comprehensive review focused exclusively on 3D reconstruction using event cameras. The survey categorises existing works into three major types based on input modality - stereo, monocular, and multimodal systems, and further classifies them by reconstruction approach, including geometry-based, deep learning-based, and recent neural rendering techniques such as Neural Radiance Fields and 3D Gaussian Splatting. Methods with a similar research focus were organised chronologically into the most subdivided groups. We also summarise public datasets relevant to event-based 3D reconstruction. Finally, we highlight current research limitations in data availability, evaluation, representation, and dynamic scene handling, and outline promising future research directions. This survey aims to serve as a comprehensive reference and a roadmap for future developments in event-driven 3D reconstruction.","authors":["Chuanzhi Xu","Haoxian Zhou","Langyi Chen","Haodong Chen","Ying Zhou","Vera Chung","Qiang Qu"],"url":"https://arxiv.org/abs/2505.08438"}
{"created":"2025-05-14","title":"A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court","abstract":"Topic modeling in Italian legal research is hindered by the lack of public datasets, limiting the analysis of legal themes in Supreme Court judgments. To address this, we developed a document processing pipeline that produces an anonymized dataset optimized for topic modeling.","authors":["Matteo Marulli","Glauco Panattoni","Marco Bertini"],"url":"https://arxiv.org/abs/2505.08439"}
{"created":"2025-05-14","title":"Symbolically-Guided Visual Plan Inference from Uncurated Video Data","abstract":"Visual planning, by offering a sequence of intermediate visual subgoals to a goal-conditioned low-level policy, achieves promising performance on long-horizon manipulation tasks. To obtain the subgoals, existing methods typically resort to video generation models but suffer from model hallucination and computational cost. We present Vis2Plan, an efficient, explainable and white-box visual planning framework powered by symbolic guidance. From raw, unlabeled play data, Vis2Plan harnesses vision foundation models to automatically extract a compact set of task symbols, which allows building a high-level symbolic transition graph for multi-goal, multi-stage planning. At test time, given a desired task goal, our planner conducts planning at the symbolic level and assembles a sequence of physically consistent intermediate sub-goal images grounded by the underlying symbolic representation. Our Vis2Plan outperforms strong diffusion video generation-based visual planners by delivering 53\\% higher aggregate success rate in real robot settings while generating visual plans 35$\\times$ faster. The results indicate that Vis2Plan is able to generate physically consistent image goals while offering fully inspectable reasoning steps.","authors":["Wenyan Yang","Ahmet Tikna","Yi Zhao","Yuying Zhang","Luigi Palopoli","Marco Roveri","Joni Pajarinen"],"url":"https://arxiv.org/abs/2505.08444"}
{"created":"2025-05-14","title":"Optimizing Retrieval-Augmented Generation: Analysis of Hyperparameter Impact on Performance and Efficiency","abstract":"Large language models achieve high task performance yet often hallucinate or rely on outdated knowledge. Retrieval-augmented generation (RAG) addresses these gaps by coupling generation with external search. We analyse how hyperparameters influence speed and quality in RAG systems, covering Chroma and Faiss vector stores, chunking policies, cross-encoder re-ranking, and temperature, and we evaluate six metrics: faithfulness, answer correctness, answer relevancy, context precision, context recall, and answer similarity. Chroma processes queries 13% faster, whereas Faiss yields higher retrieval precision, revealing a clear speed-accuracy trade-off. Naive fixed-length chunking with small windows and minimal overlap outperforms semantic segmentation while remaining the quickest option. Re-ranking provides modest gains in retrieval quality yet increases runtime by roughly a factor of 5, so its usefulness depends on latency constraints. These results help practitioners balance computational cost and accuracy when tuning RAG systems for transparent, up-to-date responses. Finally, we re-evaluate the top configurations with a corrective RAG workflow and show that their advantages persist when the model can iteratively request additional evidence. We obtain a near-perfect context precision (99%), which demonstrates that RAG systems can achieve extremely high retrieval accuracy with the right combination of hyperparameters, with significant implications for applications where retrieval quality directly impacts downstream task performance, such as clinical decision support in healthcare.","authors":["Adel Ammar","Anis Koubaa","Omer Nacar","Wadii Boulila"],"url":"https://arxiv.org/abs/2505.08445"}
{"created":"2025-05-14","title":"Agent-as-a-Service based on Agent Network","abstract":"The rise of large model-based AI agents has spurred interest in Multi-Agent Systems (MAS) for their capabilities in decision-making, collaboration, and adaptability. While the Model Context Protocol (MCP) addresses tool invocation and data exchange challenges via a unified protocol, it lacks support for organizing agent-level collaboration. To bridge this gap, we propose Agent-as-a-Service based on Agent Network (AaaS-AN), a service-oriented paradigm grounded in the Role-Goal-Process-Service (RGPS) standard. AaaS-AN unifies the entire agent lifecycle, including construction, integration, interoperability, and networked collaboration, through two core components: (1) a dynamic Agent Network, which models agents and agent groups as vertexes that self-organize within the network based on task and role dependencies; (2) service-oriented agents, incorporating service discovery, registration, and interoperability protocols. These are orchestrated by a Service Scheduler, which leverages an Execution Graph to enable distributed coordination, context tracking, and runtime task management. We validate AaaS-AN on mathematical reasoning and application-level code generation tasks, which outperforms state-of-the-art baselines. Notably, we constructed a MAS based on AaaS-AN containing agent groups, Robotic Process Automation (RPA) workflows, and MCP servers over 100 agent services. We also release a dataset containing 10,000 long-horizon multi-agent workflows to facilitate future research on long-chain collaboration in MAS.","authors":["Yuhan Zhu","Haojie Liu","Jian Wang","Bing Li","Zikang Yin","Yefei Liao"],"url":"https://arxiv.org/abs/2505.08446"}
{"created":"2025-05-14","title":"A Practical Approach to Generating First-Order Rician Channel Statistics in a RC plus CATR Chamber at mmWave","abstract":"This paper explores a novel hybrid configuration integrating a Reverberation Chamber (RC) with a Compact Antenna Test Range (CATR) to achieve a controllable Rician K-factor. The focus is testing directive antennas in the lower FR2 frequency bands (24.25-29.5 GHz) for 5G and beyond wireless applications. The study meticulously evaluates 39 unique configurations, using a stationary horn antenna for consistent reference K-factor characterization, and considers variables like absorbers and CATR polarization. Results demonstrate that the K-factor can be effectively adjusted within the hybrid setup, maintaining substantial margins above the noise level across all configurations. Sample independence is confirmed for at least 600 samples in all cases. The Bootstrap Anderson-Darling goodness-of-fit test verifies that the data align with Rician or Rayleigh distributions. Analysis of total received power, stirred and unstirred power and frequency-dependent modeling reveals that power variables are inversely related to frequency, while the K-factor remains frequency-independent. The hybrid RC-CATR system achieves a wide range of frequency-averaged K-factors from -9.2 dB to 40.8 dB, with an average granularity of 1.3 dB. Notably, configurations using co-polarized CATR signals yield large K-factors, reduced system losses, and improved frequency stability, underscoring the system's efficacy for millimeter-wave over-the-air testing. This research offers a cost-efficient and repeatable method for generating complex Rician fading channels at mmWave frequencies, crucial for the effective OTA testing of advanced wireless devices.","authors":["Alejandro Ant\\'on Ruiz","Samar Hosseinzadegan","John Kvarnstrand","Klas Arvidsson","Andr\\'es Alay\\'on Glazunov"],"url":"https://arxiv.org/abs/2505.08447"}
{"created":"2025-05-14","title":"Scalable UAV Multi-Hop Networking via Multi-Agent Reinforcement Learning with Large Language Models","abstract":"In disaster scenarios, establishing robust emergency communication networks is critical, and unmanned aerial vehicles (UAVs) offer a promising solution to rapidly restore connectivity. However, organizing UAVs to form multi-hop networks in large-scale dynamic environments presents significant challenges, including limitations in algorithmic scalability and the vast exploration space required for coordinated decision-making. To address these issues, we propose MRLMN, a novel framework that integrates multi-agent reinforcement learning (MARL) and large language models (LLMs) to jointly optimize UAV agents toward achieving optimal networking performance. The framework incorporates a grouping strategy with reward decomposition to enhance algorithmic scalability and balance decision-making across UAVs. In addition, behavioral constraints are applied to selected key UAVs to improve the robustness of the network. Furthermore, the framework integrates LLM agents, leveraging knowledge distillation to transfer their high-level decision-making capabilities to MARL agents. This enhances both the efficiency of exploration and the overall training process. In the distillation module, a Hungarian algorithm-based matching scheme is applied to align the decision outputs of the LLM and MARL agents and define the distillation loss. Extensive simulation results validate the effectiveness of our approach, demonstrating significant improvements in network performance, including enhanced coverage and communication quality.","authors":["Yanggang Xu","Weijie Hong","Jirong Zha","Geng Chen","Jianfeng Zheng","Chen-Chun Hsia","Xinlei Chen"],"url":"https://arxiv.org/abs/2505.08448"}
{"created":"2025-05-14","title":"IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation","abstract":"Retrieval-Augmented Generation (RAG) has emerged as a way to complement the in-context knowledge of Large Language Models (LLMs) by integrating external documents. However, real-world applications demand not only accuracy but also interpretability. While dense retrieval methods provide high accuracy, they lack interpretability; conversely, sparse retrieval methods offer transparency but often fail to capture the full intent of queries due to their reliance on keyword matching. To address these issues, we introduce IterKey, an LLM-driven iterative keyword generation framework that enhances RAG via sparse retrieval. IterKey consists of three LLM-driven stages: generating keywords for retrieval, generating answers based on retrieved documents, and validating the answers. If validation fails, the process iteratively repeats with refined keywords. Across four QA tasks, experimental results show that IterKey achieves 5% to 20% accuracy improvements over BM25-based RAG and simple baselines. Its performance is comparable to dense retrieval-based RAG and prior iterative query refinement methods using dense models. In summary, IterKey is a novel BM25-based approach leveraging LLMs to iteratively refine RAG, effectively balancing accuracy with interpretability.","authors":["Kazuki Hayashi","Hidetaka Kamigaito","Shinya Kouda","Taro Watanabe"],"url":"https://arxiv.org/abs/2505.08450"}
{"created":"2025-05-14","title":"Adaptive Bias Generalized Rollout Policy Adaptation on the Flexible Job-Shop Scheduling Problem","abstract":"The Flexible Job-Shop Scheduling Problem (FJSSP) is an NP-hard combinatorial optimization problem, with several application domains, especially for manufacturing purposes. The objective is to","authors":["Lotfi Kobrosly","Marc-Emmanuel Coupvent des Graviers","Christophe Guettier","Tristan Cazenave"],"url":"https://arxiv.org/abs/2505.08451"}
{"created":"2025-05-14","title":"Parameter Estimation using Reinforcement Learning Causal Curiosity: Limits and Challenges","abstract":"Causal understanding is important in many disciplines of science and engineering, where we seek to understand how different factors in the system causally affect an experiment or situation and pave a pathway towards creating effective or optimising existing models. Examples of use cases are autonomous exploration and modelling of unknown environments or assessing key variables in optimising large complex systems. In this paper, we analyse a Reinforcement Learning approach called Causal Curiosity, which aims to estimate as accurately and efficiently as possible, without directly measuring them, the value of factors that causally determine the dynamics of a system. Whilst the idea presents a pathway forward, measurement accuracy is the foundation of methodology effectiveness. Focusing on the current causal curiosity's robotic manipulator, we present for the first time a measurement accuracy analysis of the future potentials and current limitations of this technique and an analysis of its sensitivity and confounding factor disentanglement capability - crucial for causal analysis. As a result of our work, we promote proposals for an improved and efficient design of Causal Curiosity methods to be applied to real-world complex scenarios.","authors":["Miguel Arana-Catania","Weisi Guo"],"url":"https://arxiv.org/abs/2505.08453"}
{"created":"2025-05-14","title":"VCRBench: Exploring Long-form Causal Reasoning Capabilities of Large Video Language Models","abstract":"Despite recent advances in video understanding, the capabilities of Large Video Language Models (LVLMs) to perform video-based causal reasoning remains underexplored, largely due to the absence of relevant and dedicated benchmarks for evaluating causal reasoning in visually grounded and goal-driven settings. To fill this gap, we introduce a novel benchmark named Video-based long-form Causal Reasoning (VCRBench). We create VCRBench using procedural videos of simple everyday activities, where the steps are deliberately shuffled with each clip capturing a key causal event, to test whether LVLMs can identify, reason about, and correctly sequence the events needed to accomplish a specific goal. Moreover, the benchmark is carefully designed to prevent LVLMs from exploiting linguistic shortcuts, as seen in multiple-choice or binary QA formats, while also avoiding the challenges associated with evaluating open-ended QA. Our evaluation of state-of-the-art LVLMs on VCRBench suggests that these models struggle with video-based long-form causal reasoning, primarily due to their difficulty in modeling long-range causal dependencies directly from visual observations. As a simple step toward enabling such capabilities, we propose Recognition-Reasoning Decomposition (RRD), a modular approach that breaks video-based causal reasoning into two sub-tasks of video recognition and causal reasoning. Our experiments on VCRBench show that RRD significantly boosts accuracy on VCRBench, with gains of up to 25.2%. Finally, our thorough analysis reveals interesting insights, for instance, that LVLMs primarily rely on language knowledge for complex video-based long-form causal reasoning tasks.","authors":["Pritam Sarkar","Ali Etemad"],"url":"https://arxiv.org/abs/2505.08455"}
{"created":"2025-05-14","title":"Zero-Shot Sim-to-Real Reinforcement Learning for Fruit Harvesting","abstract":"This paper presents a comprehensive sim-to-real pipeline for autonomous strawberry picking from dense clusters using a Franka Panda robot. Our approach leverages a custom Mujoco simulation environment that integrates domain randomization techniques. In this environment, a deep reinforcement learning agent is trained using the dormant ratio minimization algorithm. The proposed pipeline bridges low-level control with high-level perception and decision making, demonstrating promising performance in both simulation and in a real laboratory environment, laying the groundwork for successful transfer to real-world autonomous fruit harvesting.","authors":["Emlyn Williams","Athanasios Polydoros"],"url":"https://arxiv.org/abs/2505.08458"}
{"created":"2025-05-14","title":"Strategy-Augmented Planning for Large Language Models via Opponent Exploitation","abstract":"Efficiently modeling and exploiting opponents is a long-standing challenge in adversarial domains. Large Language Models (LLMs) trained on extensive textual data have recently demonstrated outstanding performance in general tasks, introducing new research directions for opponent modeling. Some studies primarily focus on directly using LLMs to generate decisions based on the elaborate prompt context that incorporates opponent descriptions, while these approaches are limited to scenarios where LLMs possess adequate domain expertise. To address that, we introduce a two-stage Strategy-Augmented Planning (SAP) framework that significantly enhances the opponent exploitation capabilities of LLM-based agents by utilizing a critical component, the Strategy Evaluation Network (SEN). Specifically, in the offline stage, we construct an explicit strategy space and subsequently collect strategy-outcome pair data for training the SEN network. During the online phase, SAP dynamically recognizes the opponent's strategies and greedily exploits them by searching best response strategy on the well-trained SEN, finally translating strategy to a course of actions by carefully designed prompts. Experimental results show that SAP exhibits robust generalization capabilities, allowing it to perform effectively not only against previously encountered opponent strategies but also against novel, unseen strategies. In the MicroRTS environment, SAP achieves a 85.35\\% performance improvement over baseline methods and matches the competitiveness of reinforcement learning approaches against state-of-the-art (SOTA) rule-based AI.","authors":["Shuai Xu","Sijia Cui","Yanna Wang","Bo Xu","Qi Wang"],"url":"https://arxiv.org/abs/2505.08459"}
{"created":"2025-05-14","title":"An Optimal and Robust Nonconforming Finite Element Method for the Strain Gradient Elasticity","abstract":"An optimal and robust low-order nonconforming finite element method is developed for the strain gradient elasticity (SGE) model in arbitrary dimension. An $H^2$-nonconforming quadratic vector-valued finite element in arbitrary dimension is constructed, which together with an $H^1$-nonconforming scalar finite element and the Nitsche's technique, is applied for solving the SGE model. The resulting nonconforming finite element method is optimal and robust with respect to the Lam\\'{e} coefficient $\\lambda$ and the size parameter $\\iota$, as confirmed by numerical results. Additionally, nonconforming finite element discretization of the smooth Stokes complex in two and three dimensions is devised.","authors":["Jianguo Huang","Xuehai Huang","Zheqian Tang"],"url":"https://arxiv.org/abs/2505.08461"}
{"created":"2025-05-14","title":"Short and useful quantum proofs for sublogarithmic-space verifiers","abstract":"Quantum Merlin-Arthur proof systems are believed to be stronger than both their classical counterparts and ``stand-alone'' quantum computers when Arthur is assumed to operate in $\\Omega(\\log n)$ space. No hint of such an advantage over classical computation had emerged from research on smaller space bounds, which had so far concentrated on constant-space verifiers. We initiate the study of quantum Merlin-Arthur systems with space bounds in $\\omega(1) \\cap o(\\log n)$, and exhibit a problem family $\\mathcal{F}$, whose yes-instances have proofs that are verifiable by polynomial-time quantum Turing machines operating in this regime. We show that no problem in $\\mathcal{F}$ has proofs that can be verified classically or is solvable by a stand-alone quantum machine in polynomial time if standard complexity assumptions hold. Unlike previous examples of small-space verifiers, our protocols require only subpolynomial-length quantum proofs.","authors":["A. C. Cem Say"],"url":"https://arxiv.org/abs/2505.08462"}
{"created":"2025-05-14","title":"RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models","abstract":"Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs still struggle with the discrepancies between the representation obtained from the PLMs' encoder and the optimal input to the PLMs' decoder. This paper tackles this challenge by learning to calibrate the representation of PLMs in the latent space. In the proposed representation calibration method (RepCali), we integrate a specific calibration block to the latent space after the encoder and use the calibrated output as the decoder input. The merits of the proposed RepCali include its universality to all PLMs with encoder-decoder architectures, its plug-and-play nature, and ease of implementation. Extensive experiments on 25 PLM-based models across 8 tasks (including both English and Chinese datasets) demonstrate that the proposed RepCali offers desirable enhancements to PLMs (including LLMs) and significantly improves the performance of downstream tasks. Comparison experiments across 4 benchmark tasks indicate that RepCali is superior to the representative fine-tuning baselines.","authors":["Fujun Zhang","XiangDong Su"],"url":"https://arxiv.org/abs/2505.08463"}
{"created":"2025-05-14","title":"Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions","abstract":"Stance detection is essential for understanding subjective content across various platforms such as social media, news articles, and online reviews. Recent advances in Large Language Models (LLMs) have revolutionized stance detection by introducing novel capabilities in contextual understanding, cross-domain generalization, and multimodal analysis. Despite these progressions, existing surveys often lack comprehensive coverage of approaches that specifically leverage LLMs for stance detection. To bridge this critical gap, our review article conducts a systematic analysis of stance detection, comprehensively examining recent advancements of LLMs transforming the field, including foundational concepts, methodologies, datasets, applications, and emerging challenges. We present a novel taxonomy for LLM-based stance detection approaches, structured along three key dimensions: 1) learning methods, including supervised, unsupervised, few-shot, and zero-shot; 2) data modalities, such as unimodal, multimodal, and hybrid; and 3) target relationships, encompassing in-target, cross-target, and multi-target scenarios. Furthermore, we discuss the evaluation techniques and analyze benchmark datasets and performance trends, highlighting the strengths and limitations of different architectures. Key applications in misinformation detection, political analysis, public health monitoring, and social media moderation are discussed. Finally, we identify critical challenges such as implicit stance expression, cultural biases, and computational constraints, while outlining promising future directions, including explainable stance reasoning, low-resource adaptation, and real-time deployment frameworks. Our survey highlights emerging trends, open challenges, and future directions to guide researchers and practitioners in developing next-generation stance detection systems powered by large language models.","authors":["Lata Pangtey","Anukriti Bhatnagar","Shubhi Bansal","Shahid Shafi Dar","Nagendra Kumar"],"url":"https://arxiv.org/abs/2505.08464"}
{"created":"2025-05-14","title":"Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?","abstract":"Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show promise in tackling these tasks, but their evaluation is costly and time-consuming, limiting real-world deployment. While using LVLMs as judges to assess the chart comprehension capabilities of other LVLMs could streamline evaluation processes, challenges like proprietary datasets, restricted access to powerful models, and evaluation costs hinder their adoption in industrial settings. To this end, we present a comprehensive evaluation of 13 open-source LVLMs as judges for diverse chart comprehension and reasoning tasks. We design both pairwise and pointwise evaluation tasks covering criteria like factual correctness, informativeness, and relevancy. Additionally, we analyze LVLM judges based on format adherence, positional consistency, length bias, and instruction-following. We focus on cost-effective LVLMs (<10B parameters) suitable for both research and commercial use, following a standardized evaluation protocol and rubric to measure the LVLM judge's accuracy. Experimental results reveal notable variability: while some open LVLM judges achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4 judgments), others struggle (below ~10% agreement). Our findings highlight that state-of-the-art open-source LVLMs can serve as cost-effective automatic evaluators for chart-related tasks, though biases such as positional preference and length bias persist.","authors":["Md Tahmid Rahman Laskar","Mohammed Saidul Islam","Ridwan Mahbub","Ahmed Masry","Mizanur Rahman","Amran Bhuiyan","Mir Tafseer Nayeem","Shafiq Joty","Enamul Hoque","Jimmy Huang"],"url":"https://arxiv.org/abs/2505.08468"}
{"created":"2025-05-14","title":"The Quadrature Gaussian Sum Filter and Smoother for Wiener Systems","abstract":"Block-Oriented Nonlinear (BONL) models, particularly Wiener models, are widely used for their computational efficiency and practicality in modeling nonlinear behaviors in physical systems. Filtering and smoothing methods for Wiener systems, such as particle filters and Kalman-based techniques, often struggle with computational feasibility or accuracy. This work addresses these challenges by introducing a novel Gaussian Sum Filter for Wiener system state estimation that is built on a Gauss-Legendre quadrature approximation of the likelihood function associated with the output signal. In addition to filtering, a two-filter smoothing strategy is proposed, enabling accurate computation of smoothed state distributions at single and consecutive time instants. Numerical examples demonstrate the superiority of the proposed method in balancing accuracy and computational efficiency compared to traditional approaches, highlighting its benefits in control, state estimation and system identification, for Wiener systems.","authors":["Angel L. Cede\\~no","Rodrigo A. Gonz\\'alez","Juan C. Ag\\\"uero"],"url":"https://arxiv.org/abs/2505.08469"}
{"created":"2025-05-14","title":"Interest Changes: Considering User Interest Life Cycle in Recommendation System","abstract":"In recommendation systems, user interests are always in a state of constant flux. Typically, a user interest experiences a emergent phase, a stable phase, and a declining phase, which are referred to as the \"user interest life-cycle\". Recent papers on user interest modeling have primarily focused on how to compute the correlation between the target item and user's historical behaviors, without thoroughly considering the life-cycle features of user interest. In this paper, we propose an effective method called Deep Interest Life-cycle Network (DILN), which not only captures the interest life-cycle features efficiently, but can also be easily integrated to existing ranking models. DILN contains two key components: Interest Life-cycle Encoder Module constructs historical activity histograms of the user interest and then encodes them into dense representation. Interest Life-cycle Fusion Module injects the encoded dense representation into multiple expert networks, with the aim of enabling the specific phase of interest life-cycle to activate distinct experts. Online A/B testing reveals that DILN achieves significant improvements of +0.38% in CTR, +1.04% in CVR and +0.25% in duration per user, which demonstrates its effectiveness. In addition, DILN inherently increase the exposure of users' emergent and stable interests while decreasing the exposure of declining interests. DILN has been deployed on the Lofter App.","authors":["Yinjiang Cai","Jiangpan Hou","Yangping Zhu","Yuan Nie"],"url":"https://arxiv.org/abs/2505.08471"}
{"created":"2025-05-14","title":"BAT: Benchmark for Auto-bidding Task","abstract":"The optimization of bidding strategies for online advertising slot auctions presents a critical challenge across numerous digital marketplaces. A significant obstacle to the development, evaluation, and refinement of real-time autobidding algorithms is the scarcity of comprehensive datasets and standardized benchmarks.","authors":["Alexandra Khirianova","Ekaterina Solodneva","Andrey Pudovikov","Sergey Osokin","Egor Samosvat","Yuriy Dorn","Alexander Ledovsky","Yana Zenkova"],"url":"https://arxiv.org/abs/2505.08485"}
{"created":"2025-05-14","title":"An adaptive sampling algorithm for data-generation to build a data-manifold for physical problem surrogate modeling","abstract":"Physical models classically involved Partial Differential equations (PDE) and depending of their underlying complexity and the level of accuracy required, and known to be computationally expensive to numerically solve them. Thus, an idea would be to create a surrogate model relying on data generated by such solver. However, training such a model on an imbalanced data have been shown to be a very difficult task. Indeed, if the distribution of input leads to a poor response manifold representation, the model may not learn well and consequently, it may not predict the outcome with acceptable accuracy. In this work, we present an Adaptive Sampling Algorithm for Data Generation (ASADG) involving a physical model. As the initial input data may not accurately represent the response manifold in higher dimension, this algorithm iteratively adds input data into it. At each step the barycenter of each simplicial complex, that the manifold is discretized into, is added as new input data, if a certain threshold is satisfied. We demonstrate the efficiency of the data sampling algorithm in comparison with LHS method for generating more representative input data. To do so, we focus on the construction of a harmonic transport problem metamodel by generating data through a classical solver. By using such algorithm, it is possible to generate the same number of input data as LHS while providing a better representation of the response manifold.","authors":["Chetra Mang","Axel TahmasebiMoradi","David Danan","Mouadh Yagoubi"],"url":"https://arxiv.org/abs/2505.08487"}
{"created":"2025-05-14","title":"Isolation Forest in Novelty Detection Scenario","abstract":"Data mining offers a diverse toolbox for extracting meaningful structures from complex datasets, with anomaly detection emerging as a critical subfield particularly in the context of streaming or real-time data. Within anomaly detection, novelty detection focuses on identifying previously unseen patterns after training solely on regular data. While classic algorithms such as One-Class SVM or Local Outlier Factor (LOF) have been widely applied, they often lack interpretability and scalability. In this work, we explore the Half-Space Tree (HST) algorithm, originally proposed for streaming anomaly detection, and propose a novel theoretical modification to adapt it specifically for novelty detection tasks. Our approach is grounded in the idea that anomalies i.e., novelties tend to appear in the higher leaves of the tree, which are less frequently visited by regular instances. We analytically demonstrate the effectiveness of this approach using probabilistic analysis, expected depth (EXD) calculations, and combinatorial reasoning. A comparative analysis of expected depths between our modified HST and the original Isolation Forest highlights that novelty points are significantly more isolated in our approach. This supports the hypothesis that HSTs, with appropriate structural adaptation, can serve as interpretable and efficient novelty detectors. The paper contributes a theoretical foundation and supporting analysis for this adaptation, setting the stage for further application and experimentation.","authors":["Adam Ulrich","Jan Kr\\v{n}\\'avek","Roman \\v{S}enke\\v{r}\\'ik","Zuzana Kom\\'inkov\\'a Oplatkov\\'a","Radek Vala"],"url":"https://arxiv.org/abs/2505.08489"}
{"created":"2025-05-14","title":"Numerical Solution of Mixed-Dimensional PDEs Using a Neural Preconditioner","abstract":"Mixed-dimensional partial differential equations (PDEs) are characterized by coupled operators defined on domains of varying dimensions and pose significant computational challenges due to their inherent ill-conditioning. Moreover, the computational workload increases considerably when attempting to accurately capture the behavior of the system under significant variations or uncertainties in the low-dimensional structures such as fractures, fibers, or vascular networks, due to the inevitable necessity of running multiple simulations. In this work, we present a novel preconditioning strategy that leverages neural networks and unsupervised operator learning to design an efficient preconditioner specifically tailored to a class of 3D-1D mixed-dimensional PDEs. The proposed approach is capable of generalizing to varying shapes of the 1D manifold without retraining, making it robust to changes in the 1D graph topology. Moreover, thanks to convolutional neural networks, the neural preconditioner can adapt over a range of increasing mesh resolutions of the discrete problem, enabling us to train it on low resolution problems and deploy it on higher resolutions. Numerical experiments validate the effectiveness of the preconditioner in accelerating convergence in iterative solvers, demonstrating its appeal and limitations over traditional methods. This study lays the groundwork for applying neural network-based preconditioning techniques to a broader range of coupled multi-physics systems.","authors":["Nunzio Dimola","Nicola Rares Franco","Paolo Zunino"],"url":"https://arxiv.org/abs/2505.08491"}
{"created":"2025-05-14","title":"Achieving Scalable Robot Autonomy via neurosymbolic planning using lightweight local LLM","abstract":"PDDL-based symbolic task planning remains pivotal for robot autonomy yet struggles with dynamic human-robot collaboration due to scalability, re-planning demands, and delayed plan availability. Although a few neurosymbolic frameworks have previously leveraged LLMs such as GPT-3 to address these challenges, reliance on closed-source, remote models with limited context introduced critical constraints: third-party dependency, inconsistent response times, restricted plan length and complexity, and multi-domain scalability issues. We present Gideon, a novel framework that enables the transition to modern, smaller, local LLMs with extended context length. Gideon integrates a novel problem generator to systematically generate large-scale datasets of realistic domain-problem-plan tuples for any domain, and adapts neurosymbolic planning for local LLMs, enabling on-device execution and extended context for multi-domain support. Preliminary experiments in single-domain scenarios performed on Qwen-2.5 1.5B and trained on 8k-32k samples, demonstrate a valid plan percentage of 66.1% (32k model) and show that the figure can be further scaled through additional data. Multi-domain tests on 16k samples yield an even higher 70.6% planning validity rate, proving extensibility across domains and signaling that data variety can have a positive effect on learning efficiency. Although long-horizon planning and reduced model size make Gideon training much less efficient than baseline models based on larger LLMs, the results are still significant considering that the trained model is about 120x smaller than baseline and that significant advantages can be achieved in inference efficiency, scalability, and multi-domain adaptability, all critical factors in human-robot collaboration. Training inefficiency can be mitigated by Gideon's streamlined data generation pipeline.","authors":["Nicholas Attolino","Alessio Capitanelli","Fulvio Mastrogiovanni"],"url":"https://arxiv.org/abs/2505.08492"}
{"created":"2025-05-14","title":"BizChat: Scaffolding AI-Powered Business Planning for Small Business Owners Across Digital Skill Levels","abstract":"Generative AI can help small business owners automate tasks, increase efficiency, and improve their bottom line. However, despite the seemingly intuitive design of systems like ChatGPT, significant barriers remain for those less comfortable with technology. To address these disparities, prior work highlights accessory skills -- beyond prompt engineering -- users must master to successfully adopt generative AI including keyboard shortcuts, editing skills, file conversions, and browser literacy. Building on a design workshop series and 15 interviews with small businesses, we introduce BizChat, a large language model (LLM)-powered web application that helps business owners across digital skills levels write their business plan -- an essential but often neglected document. To do so, BizChat's interface embodies three design considerations inspired by learning sciences: ensuring accessibility to users with less digital skills while maintaining extensibility to power users (\"low-floor-high-ceiling\"), providing in situ micro-learning to support entrepreneurial education (\"just-in-time learning\"), and framing interaction around business activities (\"contextualized technology introduction\"). We conclude with plans for a future BizChat deployment.","authors":["Quentin Romero Lauro","Aakash Gautam","Yasmine Kotturi"],"url":"https://arxiv.org/abs/2505.08493"}
{"created":"2025-05-14","title":"Weighted Rewriting: Semiring Semantics for Abstract Reduction Systems","abstract":"We present novel semiring semantics for abstract reduction systems (ARSs). More precisely, we provide a weighted version of ARSs, where the reduction steps induce weights from a semiring. Inspired by provenance analysis in database theory and logic, we obtain a formalism that can be used for provenance analysis of arbitrary ARSs. Our semantics handle (possibly unbounded) non-determinism and possibly infinite reductions. Moreover, we develop several techniques to prove upper and lower bounds on the weights resulting from our semantics, and show that in this way one obtains a uniform approach to analyze several different properties like termination, derivational complexity, space complexity, safety, as well as combinations of these properties.","authors":["Emma Ahrens","Jan-Christoph Kassing","J\\\"urgen Giesl","Joost-Pieter Katoen"],"url":"https://arxiv.org/abs/2505.08496"}
{"created":"2025-05-14","title":"A new methodology to decompose a parametric domain using reduced order data manifold in machine learning","abstract":"We propose a new methodology for parametric domain decomposition using iterative principal component analysis. Starting with iterative principle component analysis, the high dimension manifold is reduced to the lower dimension manifold. Moreover, two approaches are developed to reconstruct the inverse projector to project from the lower data component to the original one. Afterward, we provide a detailed strategy to decompose the parametric domain based on the low dimension manifold. Finally, numerical examples of harmonic transport problem are given to illustrate the efficiency and effectiveness of the proposed method comparing to the classical meta-models such as neural networks.","authors":["Chetra Mang","Axel TahmasebiMoradi","Mouadh Yagoubi"],"url":"https://arxiv.org/abs/2505.08497"}
{"created":"2025-05-14","title":"LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models","abstract":"Recent advances in large language models (LLMs) have enabled zero-shot automated essay scoring (AES), providing a promising way to reduce the cost and effort of essay scoring in comparison with manual grading. However, most existing zero-shot approaches rely on LLMs to directly generate absolute scores, which often diverge from human evaluations owing to model biases and inconsistent scoring. To address these limitations, we propose LLM-based Comparative Essay Scoring (LCES), a method that formulates AES as a pairwise comparison task. Specifically, we instruct LLMs to judge which of two essays is better, collect many such comparisons, and convert them into continuous scores. Considering that the number of possible comparisons grows quadratically with the number of essays, we improve scalability by employing RankNet to efficiently transform LLM preferences into scalar scores. Experiments using AES benchmark datasets show that LCES outperforms conventional zero-shot methods in accuracy while maintaining computational efficiency. Moreover, LCES is robust across different LLM backbones, highlighting its applicability to real-world zero-shot AES.","authors":["Takumi Shibata","Yuichi Miyamura"],"url":"https://arxiv.org/abs/2505.08498"}
{"created":"2025-05-14","title":"ICVul: A Well-labeled C/C++ Vulnerability Dataset with Comprehensive Metadata and VCCs","abstract":"Machine learning-based software vulnerability detection requires high-quality datasets, which is essential for training effective models. To address challenges related to data label quality, diversity, and comprehensiveness, we constructed ICVul, a dataset emphasizing data quality and enriched with comprehensive metadata, including Vulnerability-Contributing Commits (VCCs). We began by filtering Common Vulnerabilities and Exposures from the NVD, retaining only those linked to GitHub fix commits. Then we extracted functions and files along with relevant metadata from these commits and used the SZZ algorithm to trace VCCs. To further enhance label reliability, we developed the ESC (Eliminate Suspicious Commit) technique, ensuring credible data labels. The dataset is stored in a relational-like database for improved usability and data integrity. Both ICVul and its construction framework are publicly accessible on GitHub, supporting research in related field.","authors":["Chaomeng Lu","Tianyu Li","Toon Dehaene","Bert Lagaisse"],"url":"https://arxiv.org/abs/2505.08503"}
{"created":"2025-05-14","title":"Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding","abstract":"Sequence-to-sequence models are widely used to train Abstract Meaning Representation (Banarescu et al., 2013, AMR) parsers. To train such models, AMR graphs have to be linearized into a one-line text format. While Penman encoding is typically used for this purpose, we argue that it has limitations: (1) for deep graphs, some closely related nodes are located far apart in the linearized text (2) Penman's tree-based encoding necessitates inverse roles to handle node re-entrancy, doubling the number of relation types to predict. To address these issues, we propose a triple-based linearization method and compare its efficiency with Penman linearization. Although triples are well suited to represent a graph, our results suggest room for improvement in triple encoding to better compete with Penman's concise and explicit representation of a nested graph structure.","authors":["Jeongwoo Kang","Maximin Coavoux","C\\'edric Lopez","Didier Schwab"],"url":"https://arxiv.org/abs/2505.08504"}
{"created":"2025-05-14","title":"On the hull-variation problem of equivalent vector rank metric codes","abstract":"The intersection of a linear code with its dual is called the hull of the code. It is known that, for classical linear codes under the Hamming metric, the dimension of the hull can be reduced up to equivalence. This phenomenon leads to the so-called hull-variation problem formulated by Hao Chen in 2023. In this paper, we consider the analogous problem for vector rank metric codes, along with their associated matrix codes and extended block codes. We also discuss the implications in the context of $(q,m)$-polymatroids.","authors":["Duy Ho","Trygve Johnsen"],"url":"https://arxiv.org/abs/2505.08506"}
{"created":"2025-05-14","title":"InfoPO: On Mutual Information Maximization for Large Language Model Alignment","abstract":"We study the post-training of large language models (LLMs) with human preference data. Recently, direct preference optimization and its variants have shown considerable promise in aligning language models, eliminating the need for reward models and online sampling. Despite these benefits, these methods rely on explicit assumptions about the Bradley-Terry (BT) model, which makes them prone to overfitting and results in suboptimal performance, particularly on reasoning-heavy tasks. To address these challenges, we propose a principled preference fine-tuning algorithm called InfoPO, which effectively and efficiently aligns large language models using preference data. InfoPO eliminates the reliance on the BT model and prevents the likelihood of the chosen response from decreasing. Extensive experiments confirm that InfoPO consistently outperforms established baselines on widely used open benchmarks, particularly in reasoning tasks.","authors":["Teng Xiao","Zhen Ge","Sujay Sanghavi","Tian Wang","Julian Katz-Samuels","Marc Versage","Qingjun Cui","Trishul Chilimbi"],"url":"https://arxiv.org/abs/2505.08507"}
{"created":"2025-05-14","title":"TrialMatchAI: An End-to-End AI-powered Clinical Trial Recommendation System to Streamline Patient-to-Trial Matching","abstract":"Patient recruitment remains a major bottleneck in clinical trials, calling for scalable and automated solutions. We present TrialMatchAI, an AI-powered recommendation system that automates patient-to-trial matching by processing heterogeneous clinical data, including structured records and unstructured physician notes. Built on fine-tuned, open-source large language models (LLMs) within a retrieval-augmented generation framework, TrialMatchAI ensures transparency and reproducibility and maintains a lightweight deployment footprint suitable for clinical environments. The system normalizes biomedical entities, retrieves relevant trials using a hybrid search strategy combining lexical and semantic similarity, re-ranks results, and performs criterion-level eligibility assessments using medical Chain-of-Thought reasoning. This pipeline delivers explainable outputs with traceable decision rationales. In real-world validation, 92 percent of oncology patients had at least one relevant trial retrieved within the top 20 recommendations. Evaluation across synthetic and real clinical datasets confirmed state-of-the-art performance, with expert assessment validating over 90 percent accuracy in criterion-level eligibility classification, particularly excelling in biomarker-driven matches. Designed for modularity and privacy, TrialMatchAI supports Phenopackets-standardized data, enables secure local deployment, and allows seamless replacement of LLM components as more advanced models emerge. By enhancing efficiency and interpretability and offering lightweight, open-source deployment, TrialMatchAI provides a scalable solution for AI-driven clinical trial matching in precision medicine.","authors":["Majd Abdallah","Sigve Nakken","Mariska Bierkens","Johanna Galvis","Alexis Groppi","Slim Karkar","Lana Meiqari","Maria Alexandra Rujano","Steve Canham","Rodrigo Dienstmann","Remond Fijneman","Eivind Hovig","Gerrit Meijer","Macha Nikolski"],"url":"https://arxiv.org/abs/2505.08508"}
{"created":"2025-05-14","title":"FOCI: Trajectory Optimization on Gaussian Splats","abstract":"3D Gaussian Splatting (3DGS) has recently gained popularity as a faster alternative to Neural Radiance Fields (NeRFs) in 3D reconstruction and view synthesis methods. Leveraging the spatial information encoded in 3DGS, this work proposes FOCI (Field Overlap Collision Integral), an algorithm that is able to optimize trajectories directly on the Gaussians themselves. FOCI leverages a novel and interpretable collision formulation for 3DGS using the notion of the overlap integral between Gaussians. Contrary to other approaches, which represent the robot with conservative bounding boxes that underestimate the traversability of the environment, we propose to represent the environment and the robot as Gaussian Splats. This not only has desirable computational properties, but also allows for orientation-aware planning, allowing the robot to pass through very tight and narrow spaces. We extensively test our algorithm in both synthetic and real Gaussian Splats, showcasing that collision-free trajectories for the ANYmal legged robot that can be computed in a few seconds, even with hundreds of thousands of Gaussians making up the environment. The project page and code are available at https://rffr.leggedrobotics.com/works/foci/","authors":["Mario Gomez Andreu","Maximum Wilder-Smith","Victor Klemm","Vaishakh Patil","Jesus Tordesillas","Marco Hutter"],"url":"https://arxiv.org/abs/2505.08510"}
{"created":"2025-05-14","title":"Numerical Analysis of Stabilization for Random Hyperbolic Systems of Balance Laws","abstract":"This paper extends the deterministic Lyapunov-based stabilization framework to random hyperbolic systems of balance laws, where uncertainties arise in boundary controls and initial data. Building on the finite volume discretization method from [{\\sc M. Banda and M. Herty}, Math. Control Relat. Fields., 3 (2013), pp. 121--142], we introduce a stochastic discrete Lyapunov function to prove the exponential decay of numerical solutions for systems with random perturbations. For linear systems, we derive explicit decay rates, which depend on boundary control parameters, grid resolutions, and the statistical properties of the random inputs. Theoretical decay rates are verified through numerical examples, including boundary stabilization of the linear wave equations and linearized shallow-water flows with random perturbations. We also demonstrate the decay rates for a nonlinear example.","authors":["Shaoshuai Chu","Michael Herty","Alexander Kurganov"],"url":"https://arxiv.org/abs/2505.08511"}
{"created":"2025-05-14","title":"Convolutional Spiking Neural Network for Image Classification","abstract":"We consider an implementation of convolutional architecture in a spiking neural network (SNN) used to classify images. As in the traditional neural network, the convolutional layers form informational \"features\" used as predictors in the SNN-based classifier with CoLaNET architecture. Since weight sharing contradicts the synaptic plasticity locality principle, the convolutional weights are fixed in our approach. We describe a methodology for their determination from a representative set of images from the same domain as the classified ones. We illustrate and test our approach on a classification task from the NEOVISION2 benchmark.","authors":["Mikhail Kiselev","Andrey Lavrentyev"],"url":"https://arxiv.org/abs/2505.08514"}
{"created":"2025-05-14","title":"CoVoL: A Cooperative Vocabulary Learning Game for Children with Autism","abstract":"Children with Autism commonly face difficulties in vocabulary acquisition, which can have an impact on their social communication. Using digital tools for vocabulary learning can prove beneficial for these children, as they can provide a predictable environment and effective individualized feedback. While existing work has explored the use of technology-assisted vocabulary learning for children with Autism, no study has incorporated turn-taking to facilitate learning and use of vocabulary similar to that used in real-world social contexts. To address this gap, we propose the design of a cooperative two-player vocabulary learning game, CoVoL. CoVoL allows children to engage in game-based vocabulary learning useful for real-world social communication scenarios. We discuss our first prototype and its evaluation. Additionally, we present planned features which are based on feedback obtained through ten interviews with researchers and therapists, as well as an evaluation plan for the final release of CoVoL.","authors":["Pawel Chodkiewicz","Pragya Verma","Grischa Liebel"],"url":"https://arxiv.org/abs/2505.08515"}
{"created":"2025-05-14","title":"Learning Advanced Self-Attention for Linear Transformers in the Singular Value Domain","abstract":"Transformers have demonstrated remarkable performance across diverse domains. The key component of Transformers is self-attention, which learns the relationship between any two tokens in the input sequence. Recent studies have revealed that the self-attention can be understood as a normalized adjacency matrix of a graph. Notably, from the perspective of graph signal processing (GSP), the self-attention can be equivalently defined as a simple graph filter, applying GSP using the value vector as the signal. However, the self-attention is a graph filter defined with only the first order of the polynomial matrix, and acts as a low-pass filter preventing the effective leverage of various frequency information. Consequently, existing self-attention mechanisms are designed in a rather simplified manner. Therefore, we propose a novel method, called \\underline{\\textbf{A}}ttentive \\underline{\\textbf{G}}raph \\underline{\\textbf{F}}ilter (AGF), interpreting the self-attention as learning the graph filter in the singular value domain from the perspective of graph signal processing for directed graphs with the linear complexity w.r.t. the input length $n$, i.e., $\\mathcal{O}(nd^2)$. In our experiments, we demonstrate that AGF achieves state-of-the-art performance on various tasks, including Long Range Arena benchmark and time series classification.","authors":["Hyowon Wi","Jeongwhan Choi","Noseong Park"],"url":"https://arxiv.org/abs/2505.08516"}
{"created":"2025-05-14","title":"A Deep Learning-Driven Framework for Inhalation Injury Grading Using Bronchoscopy Images","abstract":"Inhalation injuries face a challenge in clinical diagnosis and grading due to the limitations of traditional methods, such as Abbreviated Injury Score (AIS), which rely on subjective assessments and show weak correlations with clinical outcomes. This study introduces a novel deep learning-based framework for grading inhalation injuries using bronchoscopy images with the duration of mechanical ventilation as an objective metric. To address the scarcity of medical imaging data, we propose enhanced StarGAN, a generative model that integrates Patch Loss and SSIM Loss to improve synthetic images' quality and clinical relevance. The augmented dataset generated by enhanced StarGAN significantly improved classification performance when evaluated using the Swin Transformer, achieving an accuracy of 77.78%, an 11.11% improvement over the original dataset. Image quality was assessed using the Fr\\'echet Inception Distance (FID), where Enhanced StarGAN achieved the lowest FID of 30.06, outperforming baseline models. Burn surgeons confirmed the realism and clinical relevance of the generated images, particularly the preservation of bronchial structures and color distribution. These results highlight the potential of enhanced StarGAN in addressing data limitations and improving classification accuracy for inhalation injury grading.","authors":["Yifan Li","Alan W Pang","Jo Woon Chong"],"url":"https://arxiv.org/abs/2505.08517"}
{"created":"2025-05-14","title":"Max-Min Fairness in Stacked Intelligent Metasurface-Aided Rate Splitting Networks","abstract":"This paper investigates a downlink multiuser multiple-input single-output system that integrates rate-splitting multiple access (RSMA) with a stacked intelligent metasurface (SIM) to enable wave-domain beamforming. Unlike conventional digital beamforming, the proposed system leverages the programmable phase shifts of the SIM to perform beamforming entirely in the wave domain. In contrast to existing literature, this work introduces a fairness-centric SIM-RSMA design that shifts the emphasis from maximizing sum-rate to ensuring fair allocation of resources. In particular, we formulate a max-min rate optimization problem that jointly optimizes transmit power coefficients at the base station and SIM phase shifts. Given the non-convex nature of this problem, we develop an alternating optimization framework, where the power allocation is optimized through successive convex approximation and SIM beamforming is optimized using the Riemannian conjugate gradient method. Simulation results indicate that combining SIM with RSMA yields superior max-min performance compared to its integration with space division multiple access or non-orthogonal multiple access.","authors":["Abdullah Quran","Shimaa Naser","Maryam Tariq","Omar Alhussein","Sami Muhaidat"],"url":"https://arxiv.org/abs/2505.08521"}
{"created":"2025-05-14","title":"On the Complexity and Properties of Preferential Propositional Dependence Logic","abstract":"This paper considers the complexity and properties of KLM-style preferential reasoning in the setting of propositional logic with team semantics and dependence atoms, also known as propositional dependence logic. Preferential team-based reasoning is shown to be cumulative, yet violates System~P. We give intuitive conditions that fully characterise those cases where preferential propositional dependence logic satisfies System~P. We show that these characterisations do, surprisingly, not carry over to preferential team-based propositional logic. Furthermore, we show how classical entailment and dependence logic entailment can be expressed in terms of non-trivial preferential models. Finally, we present the complexity of preferential team-based reasoning for two natural representations. This includes novel complexity results for classical (non-team-based) preferential reasoning.","authors":["Kai Sauerwald","Arne Meier","Juha Kontinen"],"url":"https://arxiv.org/abs/2505.08522"}
{"created":"2025-05-14","title":"Dual-UAV-Enabled Secure Communication and Sensing for A2G-ISAC Systems with Maneuverable Jamming","abstract":"In this paper, we propose a dual-unmanned aerial vehicle (UAV)-enabled secure communication and sensing (SCS) scheme for an air-to-ground integrated sensing and communication (ISAC) system, in which a dual-functional source UAV and jamming UAV collaborate to enhance both the secure communication and target sensing performance. From a perspective of hybrid monostatitc-bistatic radar, the jamming UAV maneuvers to aid the source UAV to detect multiple ground targets by emitting artificial noise, meanwhile interfering with the ground eavesdropper. Residual interference is considered to reflect the effects of imperfect successive interference cancellation (SIC) on the receive signal-plus-interference-to-noise ratios, which results in a degraded system performance. To maximize the average secrecy rate (ASR), the dual-UAV trajectory and dual-UAV beamforming are jointly optimized subject to the transmit power budget, UAV maneuvering constraint, and sensing requirements. To tackle the highly complicated non-convex ASR maximization problem, the dual-UAV trajectory and dual-UAV beamforming are optimized for the secure communication (SC) purpose and the SCS purpose, sequentially. In the SC phase, a block coordinate descent algorithm is proposed to optimize the dual-UAV trajectory and dual-UAV beamforming iteratively, using the trust-region successive convex approximation (SCA) and semidefinite relaxation (SDR) techniques. Then, a weighted distance minimization problem is formulated to determine the dual-UAV maneuvering positions suitable for the SCS purpose, which is solved by a heuristic greedy algorithm, followed by the joint optimization of source beamforming and jamming beamforming.","authors":["Libiao Lou","Yuan Liu","Fotis Foukalas","Hongjiang Lei","Gaofeng Pan","Theodoros A. Tsiftsis","Hongwu Liu"],"url":"https://arxiv.org/abs/2505.08523"}
{"created":"2025-05-14","title":"Attention-based Generative Latent Replay: A Continual Learning Approach for WSI Analysis","abstract":"Whole slide image (WSI) classification has emerged as a powerful tool in computational pathology, but remains constrained by domain shifts, e.g., due to different organs, diseases, or institution-specific variations. To address this challenge, we propose an Attention-based Generative Latent Replay Continual Learning framework (AGLR-CL), in a multiple instance learning (MIL) setup for domain incremental WSI classification. Our method employs Gaussian Mixture Models (GMMs) to synthesize WSI representations and patch count distributions, preserving knowledge of past domains without explicitly storing original data. A novel attention-based filtering step focuses on the most salient patch embeddings, ensuring high-quality synthetic samples. This privacy-aware strategy obviates the need for replay buffers and outperforms other buffer-free counterparts while matching the performance of buffer-based solutions. We validate AGLR-CL on clinically relevant biomarker detection and molecular status prediction across multiple public datasets with diverse centers, organs, and patient cohorts. Experimental results confirm its ability to retain prior knowledge and adapt to new domains, offering an effective, privacy-preserving avenue for domain incremental continual learning in WSI classification.","authors":["Pratibha Kumari","Daniel Reisenb\\\"uchler","Afshin Bozorgpour","Nadine S. Schaadt","Friedrich Feuerhake","Dorit Merhof"],"url":"https://arxiv.org/abs/2505.08524"}
{"created":"2025-05-14","title":"Dynamic Snake Upsampling Operater and Boundary-Skeleton Weighted Loss for Tubular Structure Segmentation","abstract":"Accurate segmentation of tubular topological structures (e.g., fissures and vasculature) is critical in various fields to guarantee dependable downstream quantitative analysis and modeling. However, in dense prediction tasks such as semantic segmentation and super-resolution, conventional upsampling operators cannot accommodate the slenderness of tubular structures and the curvature of morphology. This paper introduces a dynamic snake upsampling operators and a boundary-skeleton weighted loss tailored for topological tubular structures. Specifically, we design a snake upsampling operators based on an adaptive sampling domain, which dynamically adjusts the sampling stride according to the feature map and selects a set of subpixel sampling points along the serpentine path, enabling more accurate subpixel-level feature recovery for tubular structures. Meanwhile, we propose a skeleton-to-boundary increasing weighted loss that trades off main body and boundary weight allocation based on mask class ratio and distance field, preserving main body overlap while enhancing focus on target topological continuity and boundary alignment precision. Experiments across various domain datasets and backbone networks show that this plug-and-play dynamic snake upsampling operator and boundary-skeleton weighted loss boost both pixel-wise segmentation accuracy and topological consistency of results.","authors":["Yiqi Chen","Ganghai Huang","Sheng Zhang","Jianglin Dai"],"url":"https://arxiv.org/abs/2505.08525"}
{"created":"2025-05-14","title":"Improving Data Fidelity via Diffusion Model-based Correction and Super resolution","abstract":"We propose a unified diffusion model-based correction and super-resolution method to enhance the fidelity and resolution of diverse low-quality data through a two-step pipeline. First, the correction step employs a novel enhanced stochastic differential editing technique based on an imbalanced perturbation and denoising process, ensuring robust and effective bias correction at the low-resolution level. The robustness and effectiveness of this approach are validated theoretically and experimentally. Next, the super-resolution step leverages cascaded conditional diffusion models to iteratively refine the corrected data to high-resolution. Numerical experiments on three PDE problems and a climate dataset demonstrate that the proposed method effectively enhances low-fidelity, low-resolution data by correcting numerical errors and noise while simultaneously improving resolution to recover fine-scale structures.","authors":["Wuzhe Xu","Yulong Lu","Sifan Wang","Tong-Rui Liu"],"url":"https://arxiv.org/abs/2505.08526"}
{"created":"2025-05-14","title":"Leveraging Segment Anything Model for Source-Free Domain Adaptation via Dual Feature Guided Auto-Prompting","abstract":"Source-free domain adaptation (SFDA) for segmentation aims at adapting a model trained in the source domain to perform well in the target domain with only the source model and unlabeled target data.Inspired by the recent success of Segment Anything Model (SAM) which exhibits the generality of segmenting images of various modalities and in different domains given human-annotated prompts like bounding boxes or points, we for the first time explore the potentials of Segment Anything Model for SFDA via automatedly finding an accurate bounding box prompt. We find that the bounding boxes directly generated with existing SFDA approaches are defective due to the domain gap.To tackle this issue, we propose a novel Dual Feature Guided (DFG) auto-prompting approach to search for the box prompt. Specifically, the source model is first trained in a feature aggregation phase, which not only preliminarily adapts the source model to the target domain but also builds a feature distribution well-prepared for box prompt search. In the second phase, based on two feature distribution observations, we gradually expand the box prompt with the guidance of the target model feature and the SAM feature to handle the class-wise clustered target features and the class-wise dispersed target features, respectively. To remove the potentially enlarged false positive regions caused by the over-confident prediction of the target model, the refined pseudo-labels produced by SAM are further postprocessed based on connectivity analysis. Experiments on 3D and 2D datasets indicate that our approach yields superior performance compared to conventional methods. Code is available at https://github.com/zheangh/DFG.","authors":["Zheang Huai","Hui Tang","Yi Li","Zhuangzhuang Chen","Xiaomeng Li"],"url":"https://arxiv.org/abs/2505.08527"}
{"created":"2025-05-14","title":"GradMix: Gradient-based Selective Mixup for Robust Data Augmentation in Class-Incremental Learning","abstract":"In the context of continual learning, acquiring new knowledge while maintaining previous knowledge presents a significant challenge. Existing methods often use experience replay techniques that store a small portion of previous task data for training. In experience replay approaches, data augmentation has emerged as a promising strategy to further improve the model performance by mixing limited previous task data with sufficient current task data. However, we theoretically and empirically analyze that training with mixed samples from random sample pairs may harm the knowledge of previous tasks and cause greater catastrophic forgetting. We then propose GradMix, a robust data augmentation method specifically designed for mitigating catastrophic forgetting in class-incremental learning. GradMix performs gradient-based selective mixup using a class-based criterion that mixes only samples from helpful class pairs and not from detrimental class pairs for reducing catastrophic forgetting. Our experiments on various real datasets show that GradMix outperforms data augmentation baselines in accuracy by minimizing the forgetting of previous knowledge.","authors":["Minsu Kim","Seong-Hyeon Hwang","Steven Euijong Whang"],"url":"https://arxiv.org/abs/2505.08528"}
{"created":"2025-05-14","title":"ExEBench: Benchmarking Foundation Models on Extreme Earth Events","abstract":"Our planet is facing increasingly frequent extreme events, which pose major risks to human lives and ecosystems. Recent advances in machine learning (ML), especially with foundation models (FMs) trained on extensive datasets, excel in extracting features and show promise in disaster management. Nevertheless, these models often inherit biases from training data, challenging their performance over extreme values. To explore the reliability of FM in the context of extreme events, we introduce \\textbf{ExE}Bench (\\textbf{Ex}treme \\textbf{E}arth Benchmark), a collection of seven extreme event categories across floods, wildfires, storms, tropical cyclones, extreme precipitation, heatwaves, and cold waves. The dataset features global coverage, varying data volumes, and diverse data sources with different spatial, temporal, and spectral characteristics. To broaden the real-world impact of FMs, we include multiple challenging ML tasks that are closely aligned with operational needs in extreme events detection, monitoring, and forecasting. ExEBench aims to (1) assess FM generalizability across diverse, high-impact tasks and domains, (2) promote the development of novel ML methods that benefit disaster management, and (3) offer a platform for analyzing the interactions and cascading effects of extreme events to advance our understanding of Earth system, especially under the climate change expected in the decades to come. The dataset and code are public https://github.com/zhaoshan2/EarthExtreme-Bench.","authors":["Shan Zhao","Zhitong Xiong","Jie Zhao","Xiao Xiang Zhu"],"url":"https://arxiv.org/abs/2505.08529"}
{"created":"2025-05-14","title":"The Truth Becomes Clearer Through Debate! Multi-Agent Systems with Large Language Models Unmask Fake News","abstract":"In today's digital environment, the rapid propagation of fake news via social networks poses significant social challenges. Most existing detection methods either employ traditional classification models, which suffer from low interpretability and limited generalization capabilities, or craft specific prompts for large language models (LLMs) to produce explanations and results directly, failing to leverage LLMs' reasoning abilities fully. Inspired by the saying that \"truth becomes clearer through debate,\" our study introduces a novel multi-agent system with LLMs named TruEDebate (TED) to enhance the interpretability and effectiveness of fake news detection. TED employs a rigorous debate process inspired by formal debate settings. Central to our approach are two innovative components: the DebateFlow Agents and the InsightFlow Agents. The DebateFlow Agents organize agents into two teams, where one supports and the other challenges the truth of the news. These agents engage in opening statements, cross-examination, rebuttal, and closing statements, simulating a rigorous debate process akin to human discourse analysis, allowing for a thorough evaluation of news content. Concurrently, the InsightFlow Agents consist of two specialized sub-agents: the Synthesis Agent and the Analysis Agent. The Synthesis Agent summarizes the debates and provides an overarching viewpoint, ensuring a coherent and comprehensive evaluation. The Analysis Agent, which includes a role-aware encoder and a debate graph, integrates role embeddings and models the interactions between debate roles and arguments using an attention mechanism, providing the final judgment.","authors":["Yuhan Liu","Yuxuan Liu","Xiaoqing Zhang","Xiuying Chen","Rui Yan"],"url":"https://arxiv.org/abs/2505.08532"}
{"created":"2025-05-14","title":"How are research data referenced? The use case of the research data repository RADAR","abstract":"Publishing research data aims to improve the transparency of research results and facilitate the reuse of datasets. In both cases, referencing the datasets that were used is recommended. Research data repositories can support data referencing through various measures and also benefit from it, for example using this information to demonstrate their impact. However, the literature shows that the practice of formally citing research data is not widespread, data metrics are not yet established, and effective incentive structures are lacking. This article examines how often and in what form datasets published via the research data repository RADAR are referenced. For this purpose, the data sources Google Scholar, DataCite Event Data and the Data Citation Corpus were analyzed. The analysis shows that 27.9 % of the datasets in the repository were referenced at least once. 21.4 % of these references were (also) present in the reference lists and are therefore considered data citations. Datasets were referenced often in data availability statements. A comparison of the three data sources showed that there was little overlap in the coverage of references. In most cases (75.8 %), data and referencing objects were published in the same year. Two definition approaches were considered to investigate data reuse. 118 RADAR datasets were referenced more than once. Only 21 references had no overlaps in the authorship information -- these datasets were referenced by researchers that were not involved in data collection.","authors":["Dorothea Strecker","Kerstin Soltau","Felix Bach"],"url":"https://arxiv.org/abs/2505.08533"}
{"created":"2025-05-14","title":"Diffusion-assisted Model Predictive Control Optimization for Power System Real-Time Operation","abstract":"This paper presents a modified model predictive control (MPC) framework for real-time power system operation. The framework incorporates a diffusion model tailored for time series generation to enhance the accuracy of the load forecasting module used in the system operation. In the absence of explicit state transition law, a model-identification procedure is leveraged to derive the system dynamics, thereby eliminating a barrier when applying MPC to a renewables-dominated power system. Case study results on an industry park system and the IEEE 30-bus system demonstrate that using the diffusion model to augment the training dataset significantly improves load-forecasting accuracy, and the inferred system dynamics are applicable to the real-time grid operation with solar and wind.","authors":["Linna Xu","Yongli Zhu"],"url":"https://arxiv.org/abs/2505.08535"}
{"created":"2025-05-14","title":"The RaspGrade Dataset: Towards Automatic Raspberry Ripeness Grading with Deep Learning","abstract":"This research investigates the application of computer vision for rapid, accurate, and non-invasive food quality assessment, focusing on the novel challenge of real-time raspberry grading into five distinct classes within an industrial environment as the fruits move along a conveyor belt. To address this, a dedicated dataset of raspberries, namely RaspGrade, was acquired and meticulously annotated. Instance segmentation experiments revealed that accurate fruit-level masks can be obtained; however, the classification of certain raspberry grades presents challenges due to color similarities and occlusion, while others are more readily distinguishable based on color. The acquired and annotated RaspGrade dataset is accessible on HuggingFace at: https://huggingface.co/datasets/FBK-TeV/RaspGrade.","authors":["Mohamed Lamine Mekhalfi","Paul Chippendale","Fabio Poiesi","Samuele Bonecher","Gilberto Osler","Nicola Zancanella"],"url":"https://arxiv.org/abs/2505.08537"}
{"created":"2025-05-14","title":"Area Comparison of CHERIoT and PMP in Ibex","abstract":"Memory safety is a critical concern for modern embedded systems, particularly in security-sensitive applications. This paper explores the area impact of adding memory safety extensions to the Ibex RISC-V core, focusing on physical memory protection (PMP) and Capability Hardware Extension to RISC-V for Internet of Things (CHERIoT). We synthesise the extended Ibex cores using a commercial tool targeting the open FreePDK45 process and provide a detailed area breakdown and discussion of the results.","authors":["Samuel Riedel","Marno van der Maas","John Thomson","Andreas Kurth","Pirmin Vogel"],"url":"https://arxiv.org/abs/2505.08541"}
{"created":"2025-05-14","title":"Guiding LLM-based Smart Contract Generation with Finite State Machine","abstract":"Smart contract is a kind of self-executing code based on blockchain technology with a wide range of application scenarios, but the traditional generation method relies on manual coding and expert auditing, which has a high threshold and low efficiency. Although Large Language Models (LLMs) show great potential in programming tasks, they still face challenges in smart contract generation w.r.t. effectiveness and security. To solve these problems, we propose FSM-SCG, a smart contract generation framework based on finite state machine (FSM) and LLMs, which significantly improves the quality of the generated code by abstracting user requirements to generate FSM, guiding LLMs to generate smart contracts, and iteratively optimizing the code with the feedback of compilation and security checks. The experimental results show that FSM-SCG significantly improves the quality of smart contract generation. Compared to the best baseline, FSM-SCG improves the compilation success rate of generated smart contract code by at most 48%, and reduces the average vulnerability risk score by approximately 68%.","authors":["Hao Luo","Yuhao Lin","Xiao Yan","Xintong Hu","Yuxiang Wang","Qiming Zeng","Hao Wang","Jiawei Jiang"],"url":"https://arxiv.org/abs/2505.08542"}
{"created":"2025-05-14","title":"ROSA: Finding Backdoors with Fuzzing","abstract":"A code-level backdoor is a hidden access, programmed and concealed within the code of a program. For instance, hard-coded credentials planted in the code of a file server application would enable maliciously logging into all deployed instances of this application. Confirmed software supply chain attacks have led to the injection of backdoors into popular open-source projects, and backdoors have been discovered in various router firmware. Manual code auditing for backdoors is challenging and existing semi-automated approaches can handle only a limited scope of programs and backdoors, while requiring manual reverse-engineering of the audited (binary) program. Graybox fuzzing (automated semi-randomized testing) has grown in popularity due to its success in discovering vulnerabilities and hence stands as a strong candidate for improved backdoor detection. However, current fuzzing knowledge does not offer any means to detect the triggering of a backdoor at runtime. In this work we introduce ROSA, a novel approach (and tool) which combines a state-of-the-art fuzzer (AFL++) with a new metamorphic test oracle, capable of detecting runtime backdoor triggers. To facilitate the evaluation of ROSA, we have created ROSARUM, the first openly available benchmark for assessing the detection of various backdoors in diverse programs. Experimental evaluation shows that ROSA has a level of robustness, speed and automation similar to classical fuzzing. It finds all 17 authentic or synthetic backdooors from ROSARUM in 1h30 on average. Compared to existing detection tools, it can handle a diversity of backdoors and programs and it does not rely on manual reverse-engineering of the fuzzed binary code.","authors":["Dimitri Kokkonis (IP Paris","DIN)","Micha\\\"el Marcozzi (DIN)","Emilien Decoux (DIN)","Stefano Zacchiroli (IP Paris","LTCI","ACES","INFRES)"],"url":"https://arxiv.org/abs/2505.08544"}
{"created":"2025-05-14","title":"Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation","abstract":"While gender bias in modern Neural Machine Translation (NMT) systems has received much attention, traditional evaluation metrics do not to fully capture the extent to which these systems integrate contextual gender cues. We propose a novel evaluation metric called Minimal Pair Accuracy (MPA), which measures the reliance of models on gender cues for gender disambiguation. MPA is designed to go beyond surface-level gender accuracy metrics by focusing on whether models adapt to gender cues in minimal pairs -- sentence pairs that differ solely in the gendered pronoun, namely the explicit indicator of the target's entity gender in the source language (EN). We evaluate a number of NMT models on the English-Italian (EN--IT) language pair using this metric, we show that they ignore available gender cues in most cases in favor of (statistical) stereotypical gender interpretation. We further show that in anti-stereotypical cases, these models tend to more consistently take masculine gender cues into account while ignoring the feminine cues. Furthermore, we analyze the attention head weights in the encoder component and show that while all models encode gender information to some extent, masculine cues elicit a more diffused response compared to the more concentrated and specialized responses to feminine gender cues.","authors":["Chiara Manna","Afra Alishahi","Fr\\'ed\\'eric Blain","Eva Vanmassenhove"],"url":"https://arxiv.org/abs/2505.08546"}
{"created":"2025-05-14","title":"From Seeing to Doing: Bridging Reasoning and Decision for Robotic Manipulation","abstract":"Achieving generalization in robotic manipulation remains a critical challenge, particularly for unseen scenarios and novel tasks. Current Vision-Language-Action (VLA) models, while building on top of general Vision-Language Models (VLMs), still fall short of achieving robust zero-shot performance due to the scarcity and heterogeneity prevalent in embodied datasets. To address these limitations, we propose FSD (From Seeing to Doing), a novel vision-language model that generates intermediate representations through spatial relationship reasoning, providing fine-grained guidance for robotic manipulation. Our approach combines a hierarchical data pipeline for training with a self-consistency mechanism that aligns spatial coordinates with visual signals. Through extensive experiments, we comprehensively validated FSD's capabilities in both \"seeing\" and \"doing,\" achieving outstanding performance across 8 benchmarks for general spatial reasoning and embodied reference abilities, as well as on our proposed more challenging benchmark VABench. We also verified zero-shot capabilities in robot manipulation, demonstrating significant performance improvements over baseline methods in both SimplerEnv and real robot settings. Experimental results show that FSD achieves 54.1% success rate in SimplerEnv and 72% success rate across 8 real-world tasks, outperforming the strongest baseline by 30%.","authors":["Yifu Yuan","Haiqin Cui","Yibin Chen","Zibin Dong","Fei Ni","Longxin Kou","Jinyi Liu","Pengyi Li","Yan Zheng","Jianye Hao"],"url":"https://arxiv.org/abs/2505.08548"}
{"created":"2025-05-14","title":"OLinear: A Linear Model for Time Series Forecasting in Orthogonally Transformed Domain","abstract":"This paper presents $\\mathbf{OLinear}$, a $\\mathbf{linear}$-based multivariate time series forecasting model that operates in an $\\mathbf{o}$rthogonally transformed domain. Recent forecasting models typically adopt the temporal forecast (TF) paradigm, which directly encode and decode time series in the time domain. However, the entangled step-wise dependencies in series data can hinder the performance of TF. To address this, some forecasters conduct encoding and decoding in the transformed domain using fixed, dataset-independent bases (e.g., sine and cosine signals in the Fourier transform). In contrast, we utilize $\\mathbf{OrthoTrans}$, a data-adaptive transformation based on an orthogonal matrix that diagonalizes the series' temporal Pearson correlation matrix. This approach enables more effective encoding and decoding in the decorrelated feature domain and can serve as a plug-in module to enhance existing forecasters. To enhance the representation learning for multivariate time series, we introduce a customized linear layer, $\\mathbf{NormLin}$, which employs a normalized weight matrix to capture multivariate dependencies. Empirically, the NormLin module shows a surprising performance advantage over multi-head self-attention, while requiring nearly half the FLOPs. Extensive experiments on 24 benchmarks and 140 forecasting tasks demonstrate that OLinear consistently achieves state-of-the-art performance with high efficiency. Notably, as a plug-in replacement for self-attention, the NormLin module consistently enhances Transformer-based forecasters. The code and datasets are available at https://anonymous.4open.science/r/OLinear","authors":["Wenzhen Yue","Yong Liu","Haoxuan Li","Hao Wang","Xianghua Ying","Ruohao Guo","Bowei Xing","Ji Shi"],"url":"https://arxiv.org/abs/2505.08550"}
{"created":"2025-05-14","title":"DFA-CON: A Contrastive Learning Approach for Detecting Copyright Infringement in DeepFake Art","abstract":"Recent proliferation of generative AI tools for visual content creation-particularly in the context of visual artworks-has raised serious concerns about copyright infringement and forgery. The large-scale datasets used to train these models often contain a mixture of copyrighted and non-copyrighted artworks. Given the tendency of generative models to memorize training patterns, they are susceptible to varying degrees of copyright violation. Building on the recently proposed DeepfakeArt Challenge benchmark, this work introduces DFA-CON, a contrastive learning framework designed to detect copyright-infringing or forged AI-generated art. DFA-CON learns a discriminative representation space, posing affinity among original artworks and their forged counterparts within a contrastive learning framework. The model is trained across multiple attack types, including inpainting, style transfer, adversarial perturbation, and cutmix. Evaluation results demonstrate robust detection performance across most attack types, outperforming recent pretrained foundation models. Code and model checkpoints will be released publicly upon acceptance.","authors":["Haroon Wahab","Hassan Ugail","Irfan Mehmood"],"url":"https://arxiv.org/abs/2505.08552"}
{"created":"2025-05-14","title":"A High-Efficiency Reconfigurable Bidirectional Array Antenna Based on Transmit-Reflect Switchable Metasurface","abstract":"This paper proposes a reconfigurable bidirectional array antenna with high-efficiency radiations and flexible beam-switching capability by employing a novel transmit-reflect switchable metasurface (TRSM). To realize the electromagnetic (EM) wave transmitted or reflected manipulation, a dedicated transmit-reflect switch layer (TRSL) with periodically soldered PIN diodes is introduced between two transmitted metasurfaces. By switching ON/OFF the embedded diodes, the TRSL performs as a mesh-type ground layer or polarization-grid layer, exhibiting a reflect or transmit property to the incident wave respectively. Further, utilizing the above TRSM configuration in conjunction with a microstrip feed antenna, bidirectional radiations are obtained at the same frequency and polarization. To further reduce the number of PIN diodes and control complexity, an enhanced TRSM using a single diode to control two unit cells is also investigated, resulting in half PIN diodes reduction. Since the bidirectional beam-switching is achieved by only controlling PIN diodes integrated in the ground plane instead of directly acting on the radiation element, which reduces insertion loss and avoids phase quantization errors, the proposed antenna can maintain a high aperture efficiency. To verify this concept, a prototype was designed, fabricated, and measured, demonstrating a successful realization of backward and forward patterns with peak gains of 22.3 and 22.1 dBi, and aperture efficiencies of 47.2% and 43.8%. The 3-dB gain bandwidths of reflected and transmitted modes are 13.7% and 12.3%. This antenna has the advantages of high gain, high aperture efficiency, simple configuration, cost-effectiveness, and flexible and digital beam control.","authors":["Fan Qin","Jinyang Bi","Jiao Ma","Chao Gu","Hailin Zhang","Wenchi Cheng","Steven Gao"],"url":"https://arxiv.org/abs/2505.08556"}
{"created":"2025-05-14","title":"Online Learning and Unlearning","abstract":"We formalize the problem of online learning-unlearning, where a model is updated sequentially in an online setting while accommodating unlearning requests between updates. After a data point is unlearned, all subsequent outputs must be statistically indistinguishable from those of a model trained without that point. We present two online learner-unlearner (OLU) algorithms, both built upon online gradient descent (OGD). The first, passive OLU, leverages OGD's contractive property and injects noise when unlearning occurs, incurring no additional computation. The second, active OLU, uses an offline unlearning algorithm that shifts the model toward a solution excluding the deleted data. Under standard convexity and smoothness assumptions, both methods achieve regret bounds comparable to those of standard OGD, demonstrating that one can maintain competitive regret bounds while providing unlearning guarantees.","authors":["Yaxi Hu","Bernhard Sch\\\"olkopf","Amartya Sanyal"],"url":"https://arxiv.org/abs/2505.08557"}
{"created":"2025-05-14","title":"Reinforcement Learning meets Masked Video Modeling : Trajectory-Guided Adaptive Token Selection","abstract":"Masked video modeling~(MVM) has emerged as a highly effective pre-training strategy for visual foundation models, whereby the model reconstructs masked spatiotemporal tokens using information from visible tokens. However, a key challenge in such approaches lies in selecting an appropriate masking strategy. Previous studies have explored predefined masking techniques, including random and tube-based masking, as well as approaches that leverage key motion priors, optical flow and semantic cues from externally pre-trained models. In this work, we introduce a novel and generalizable Trajectory-Aware Adaptive Token Sampler (TATS), which models the motion dynamics of tokens and can be seamlessly integrated into the masked autoencoder (MAE) framework to select motion-centric tokens in videos. Additionally, we propose a unified training strategy that enables joint optimization of both MAE and TATS from scratch using Proximal Policy Optimization (PPO). We show that our model allows for aggressive masking without compromising performance on the downstream task of action recognition while also ensuring that the pre-training remains memory efficient. Extensive experiments of the proposed approach across four benchmarks, including Something-Something v2, Kinetics-400, UCF101, and HMDB51, demonstrate the effectiveness, transferability, generalization, and efficiency of our work compared to other state-of-the-art methods.","authors":["Ayush K. Rai","Kyle Min","Tarun Krishna","Feiyan Hu","Alan F. Smeaton","Noel E. O'Connor"],"url":"https://arxiv.org/abs/2505.08561"}
{"created":"2025-05-14","title":"Thermal Detection of People with Mobility Restrictions for Barrier Reduction at Traffic Lights Controlled Intersections","abstract":"Rapid advances in deep learning for computer vision have driven the adoption of RGB camera-based adaptive traffic light systems to improve traffic safety and pedestrian comfort. However, these systems often overlook the needs of people with mobility restrictions. Moreover, the use of RGB cameras presents significant challenges, including limited detection performance under adverse weather or low-visibility conditions, as well as heightened privacy concerns. To address these issues, we propose a fully automated, thermal detector-based traffic light system that dynamically adjusts signal durations for individuals with walking impairments or mobility burden and triggers the auditory signal for visually impaired individuals, thereby advancing towards barrier-free intersection for all users. To this end, we build the thermal dataset for people with mobility restrictions (TD4PWMR), designed to capture diverse pedestrian scenarios, particularly focusing on individuals with mobility aids or mobility burden under varying environmental conditions, such as different lighting, weather, and crowded urban settings. While thermal imaging offers advantages in terms of privacy and robustness to adverse conditions, it also introduces inherent hurdles for object detection due to its lack of color and fine texture details and generally lower resolution of thermal images. To overcome these limitations, we develop YOLO-Thermal, a novel variant of the YOLO architecture that integrates advanced feature extraction and attention mechanisms for enhanced detection accuracy and robustness in thermal imaging. Experiments demonstrate that the proposed thermal detector outperforms existing detectors, while the proposed traffic light system effectively enhances barrier-free intersection. The source codes and dataset are available at https://github.com/leon2014dresden/YOLO-THERMAL.","authors":["Xiao Ni","Carsten Kuehnel","Xiaoyi Jiang"],"url":"https://arxiv.org/abs/2505.08568"}
{"created":"2025-05-14","title":"Entropy numbers of classes defined by integral operators","abstract":"In this paper we develop the following general approach. We study asymptotic behavior of the entropy numbers not for an individual smoothness class, how it is usually done, but for the collection of classes, which are defined by integral operators with kernels coming from a given class of functions. Earlier, such approach was realized for the Kolmogorov widths.","authors":["V. Temlyakov"],"url":"https://arxiv.org/abs/2505.08572"}
{"created":"2025-05-14","title":"End-to-End Multi-Task Policy Learning from NMPC for Quadruped Locomotion","abstract":"Quadruped robots excel in traversing complex, unstructured environments where wheeled robots often fail. However, enabling efficient and adaptable locomotion remains challenging due to the quadrupeds' nonlinear dynamics, high degrees of freedom, and the computational demands of real-time control. Optimization-based controllers, such as Nonlinear Model Predictive Control (NMPC), have shown strong performance, but their reliance on accurate state estimation and high computational overhead makes deployment in real-world settings challenging. In this work, we present a Multi-Task Learning (MTL) framework in which expert NMPC demonstrations are used to train a single neural network to predict actions for multiple locomotion behaviors directly from raw proprioceptive sensor inputs. We evaluate our approach extensively on the quadruped robot Go1, both in simulation and on real hardware, demonstrating that it accurately reproduces expert behavior, allows smooth gait switching, and simplifies the control pipeline for real-time deployment. Our MTL architecture enables learning diverse gaits within a unified policy, achieving high $R^{2}$ scores for predicted joint targets across all tasks.","authors":["Anudeep Sajja","Shahram Khorshidi","Sebastian Houben","Maren Bennewitz"],"url":"https://arxiv.org/abs/2505.08574"}
{"created":"2025-05-14","title":"MUBox: A Critical Evaluation Framework of Deep Machine Unlearning","abstract":"Recent legal frameworks have mandated the right to be forgotten, obligating the removal of specific data upon user requests. Machine Unlearning has emerged as a promising solution by selectively removing learned information from machine learning models. This paper presents MUBox, a comprehensive platform designed to evaluate unlearning methods in deep learning. MUBox integrates 23 advanced unlearning techniques, tested across six practical scenarios with 11 diverse evaluation metrics. It allows researchers and practitioners to (1) assess and compare the effectiveness of different machine unlearning methods across various scenarios; (2) examine the impact of current evaluation metrics on unlearning performance; and (3) conduct detailed comparative studies on machine unlearning in a unified framework. Leveraging MUBox, we systematically evaluate these unlearning methods in deep learning and uncover several key insights: (a) Even state-of-the-art unlearning methods, including those published in top-tier venues and winners of unlearning competitions, demonstrate inconsistent effectiveness across diverse scenarios. Prior research has predominantly focused on simplified settings, such as random forgetting and class-wise unlearning, highlighting the need for broader evaluations across more difficult unlearning tasks. (b) Assessing unlearning performance remains a non-trivial problem, as no single evaluation metric can comprehensively capture the effectiveness, efficiency, and preservation of model utility. Our findings emphasize the necessity of employing multiple metrics to achieve a balanced and holistic assessment of unlearning methods. (c) In the context of depoisoning, our evaluation reveals significant variability in the effectiveness of existing approaches, which is highly dependent on the specific type of poisoning attacks.","authors":["Xiang Li","Bhavani Thuraisingham","Wenqi Wei"],"url":"https://arxiv.org/abs/2505.08576"}
{"created":"2025-05-14","title":"ReSurgSAM2: Referring Segment Anything in Surgical Video via Credible Long-term Tracking","abstract":"Surgical scene segmentation is critical in computer-assisted surgery and is vital for enhancing surgical quality and patient outcomes. Recently, referring surgical segmentation is emerging, given its advantage of providing surgeons with an interactive experience to segment the target object. However, existing methods are limited by low efficiency and short-term tracking, hindering their applicability in complex real-world surgical scenarios. In this paper, we introduce ReSurgSAM2, a two-stage surgical referring segmentation framework that leverages Segment Anything Model 2 to perform text-referred target detection, followed by tracking with reliable initial frame identification and diversity-driven long-term memory. For the detection stage, we propose a cross-modal spatial-temporal Mamba to generate precise detection and segmentation results. Based on these results, our credible initial frame selection strategy identifies the reliable frame for the subsequent tracking. Upon selecting the initial frame, our method transitions to the tracking stage, where it incorporates a diversity-driven memory mechanism that maintains a credible and diverse memory bank, ensuring consistent long-term tracking. Extensive experiments demonstrate that ReSurgSAM2 achieves substantial improvements in accuracy and efficiency compared to existing methods, operating in real-time at 61.2 FPS. Our code and datasets will be available at https://github.com/jinlab-imvr/ReSurgSAM2.","authors":["Haofeng Liu","Mingqi Gao","Xuxiao Luo","Ziyue Wang","Guanyi Qin","Junde Wu","Yueming Jin"],"url":"https://arxiv.org/abs/2505.08581"}
{"created":"2025-05-14","title":"A Large-scale Benchmark on Geological Fault Delineation Models: Domain Shift, Training Dynamics, Generalizability, Evaluation and Inferential Behavior","abstract":"Machine learning has taken a critical role in seismic interpretation workflows, especially in fault delineation tasks. However, despite the recent proliferation of pretrained models and synthetic datasets, the field still lacks a systematic understanding of the generalizability limits of these models across seismic data representing a variety of geologic, acquisition and processing settings. Distributional shifts between different data sources, limitations in fine-tuning strategies and labeled data accessibility, and inconsistent evaluation protocols all represent major roadblocks in the deployment of reliable and robust models in real-world exploration settings. In this paper, we present the first large-scale benchmarking study explicitly designed to provide answers and guidelines for domain shift strategies in seismic interpretation. Our benchmark encompasses over $200$ models trained and evaluated on three heterogeneous datasets (synthetic and real data) including FaultSeg3D, CRACKS, and Thebe. We systematically assess pretraining, fine-tuning, and joint training strategies under varying degrees of domain shift. Our analysis highlights the fragility of current fine-tuning practices, the emergence of catastrophic forgetting, and the challenges of interpreting performance in a systematic manner. We establish a robust experimental baseline to provide insights into the tradeoffs inherent to current fault delineation workflows, and shed light on directions for developing more generalizable, interpretable and effective machine learning models for seismic interpretation. The insights and analyses reported provide a set of guidelines on the deployment of fault delineation models within seismic interpretation workflows.","authors":["Jorge Quesada","Chen Zhou","Prithwijit Chowdhury","Mohammad Alotaibi","Ahmad Mustafa","Yusufjon Kumamnov","Mohit Prabhushankar","Ghassan AlRegib"],"url":"https://arxiv.org/abs/2505.08585"}
{"created":"2025-05-14","title":"PrePrompt: Predictive prompting for class incremental learning","abstract":"Class Incremental Learning (CIL) based on pre-trained models offers a promising direction for open-world continual learning. Existing methods typically rely on correlation-based strategies, where an image's classification feature is used as a query to retrieve the most related key prompts and select the corresponding value prompts for training. However, these approaches face an inherent limitation: fitting the entire feature space of all tasks with only a few trainable prompts is fundamentally challenging. We propose Predictive Prompting (PrePrompt), a novel CIL framework that circumvents correlation-based limitations by leveraging pre-trained models' natural classification ability to predict task-specific prompts. Specifically, PrePrompt decomposes CIL into a two-stage prediction framework: task-specific prompt prediction followed by label prediction. While theoretically appealing, this framework risks bias toward recent classes due to missing historical data for older classifier calibration. PrePrompt then mitigates this by incorporating feature translation, dynamically balancing stability and plasticity. Experiments across multiple benchmarks demonstrate PrePrompt's superiority over state-of-the-art prompt-based CIL methods. The code will be released upon acceptance.","authors":["Libo Huang","Zhulin An","Chuanguang Yang","Boyu Diao","Fei Wang","Yan Zeng","Zhifeng Hao","Yongjun Xu"],"url":"https://arxiv.org/abs/2505.08586"}
{"created":"2025-05-14","title":"Two-Level Sketching Alternating Anderson acceleration for Complex Physics Applications","abstract":"We present a novel two-level sketching extension of the Alternating Anderson-Picard (AAP) method for accelerating fixed-point iterations in challenging single- and multi-physics simulations governed by discretized partial differential equations. Our approach combines a static, physics-based projection that reduces the least-squares problem to the most informative field (e.g., via Schur-complement insight) with a dynamic, algebraic sketching stage driven by a backward stability analysis under Lipschitz continuity. We introduce inexpensive estimators for stability thresholds and cache-aware randomized selection strategies to balance computational cost against memory-access overhead. The resulting algorithm solves reduced least-squares systems in place, minimizes memory footprints, and seamlessly alternates between low-cost Picard updates and Anderson mixing. Implemented in Julia, our two-level sketching AAP achieves up to 50% time-to-solution reductions compared to standard Anderson acceleration-without degrading convergence rates-on benchmark problems including Stokes, p-Laplacian, Bidomain, and Navier-Stokes formulations at varying problem sizes. These results demonstrate the method's robustness, scalability, and potential for integration into high-performance scientific computing frameworks. Our implementation is available open-source in the AAP.jl library.","authors":["Nicol\\'as A. Barnafi","Massimiliano Lupo Pasini"],"url":"https://arxiv.org/abs/2505.08587"}
{"created":"2025-05-14","title":"Small but Significant: On the Promise of Small Language Models for Accessible AIED","abstract":"GPT has become nearly synonymous with large language models (LLMs), an increasingly popular term in AIED proceedings. A simple keyword-based search reveals that 61% of the 76 long and short papers presented at AIED 2024 describe novel solutions using LLMs to address some of the long-standing challenges in education, and 43% specifically mention GPT. Although LLMs pioneered by GPT create exciting opportunities to strengthen the impact of AI on education, we argue that the field's predominant focus on GPT and other resource-intensive LLMs (with more than 10B parameters) risks neglecting the potential impact that small language models (SLMs) can make in providing resource-constrained institutions with equitable and affordable access to high-quality AI tools. Supported by positive results on knowledge component (KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as Phi-2 can produce an effective solution without elaborate prompting strategies. Hence, we call for more attention to developing SLM-based AIED approaches.","authors":["Yumou Wei","Paulo Carvalho","John Stamper"],"url":"https://arxiv.org/abs/2505.08588"}
{"created":"2025-05-14","title":"MESSI: A Multi-Elevation Semantic Segmentation Image Dataset of an Urban Environment","abstract":"This paper presents a Multi-Elevation Semantic Segmentation Image (MESSI) dataset comprising 2525 images taken by a drone flying over dense urban environments. MESSI is unique in two main features. First, it contains images from various altitudes, allowing us to investigate the effect of depth on semantic segmentation. Second, it includes images taken from several different urban regions (at different altitudes). This is important since the variety covers the visual richness captured by a drone's 3D flight, performing horizontal and vertical maneuvers. MESSI contains images annotated with location, orientation, and the camera's intrinsic parameters and can be used to train a deep neural network for semantic segmentation or other applications of interest (e.g., localization, navigation, and tracking). This paper describes the dataset and provides annotation details. It also explains how semantic segmentation was performed using several neural network models and shows several relevant statistics. MESSI will be published in the public domain to serve as an evaluation benchmark for semantic segmentation using images captured by a drone or similar vehicle flying over a dense urban environment.","authors":["Barak Pinkovich","Boaz Matalon","Ehud Rivlin","Hector Rotstein"],"url":"https://arxiv.org/abs/2505.08589"}
{"created":"2025-05-14","title":"Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models","abstract":"Advancements in artificial intelligence (AI) are transforming pathology by integrat-ing large language models (LLMs) with retrieval-augmented generation (RAG) and domain-specific foundation models. This study explores the application of RAG-enhanced LLMs coupled with pathology foundation models for thyroid cytology diagnosis, addressing challenges in cytological interpretation, standardization, and diagnostic accuracy. By leveraging a curated knowledge base, RAG facilitates dy-namic retrieval of relevant case studies, diagnostic criteria, and expert interpreta-tion, improving the contextual understanding of LLMs. Meanwhile, pathology foun-dation models, trained on high-resolution pathology images, refine feature extrac-tion and classification capabilities. The fusion of these AI-driven approaches en-hances diagnostic consistency, reduces variability, and supports pathologists in dis-tinguishing benign from malignant thyroid lesions. Our results demonstrate that integrating RAG with pathology-specific LLMs significantly improves diagnostic efficiency and interpretability, paving the way for AI-assisted thyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for correct prediction of surgi-cal pathology diagnosis from thyroid cytology samples.","authors":["Hussien Al-Asi","Jordan P Reynolds","Shweta Agarwal","Bryan J Dangott","Aziza Nassar","Zeynettin Akkus"],"url":"https://arxiv.org/abs/2505.08590"}
{"created":"2025-05-14","title":"MC-Swarm: Minimal-Communication Multi-Agent Trajectory Planning and Deadlock Resolution for Quadrotor Swarm","abstract":"For effective multi-agent trajectory planning, it is important to consider lightweight communication and its potential asynchrony. This paper presents a distributed trajectory planning algorithm for a quadrotor swarm that operates asynchronously and requires no communication except during the initial planning phase. Moreover, our algorithm guarantees no deadlock under asynchronous updates and absence of communication during flight. To effectively ensure these points, we build two main modules: coordination state updater and trajectory optimizer. The coordination state updater computes waypoints for each agent toward its goal and performs subgoal optimization while considering deadlocks, as well as safety constraints with respect to neighbor agents and obstacles. Then, the trajectory optimizer generates a trajectory that ensures collision avoidance even with the asynchronous planning updates of neighboring agents. We provide a theoretical guarantee of collision avoidance with deadlock resolution and evaluate the effectiveness of our method in complex simulation environments, including random forests and narrow-gap mazes. Additionally, to reduce the total mission time, we design a faster coordination state update using lightweight communication. Lastly, our approach is validated through extensive simulations and real-world experiments with cluttered environment scenarios.","authors":["Yunwoo Lee","Jungwon Park"],"url":"https://arxiv.org/abs/2505.08593"}
{"created":"2025-05-14","title":"Clustering of Incomplete Data via a Bipartite Graph Structure","abstract":"There are various approaches to graph learning for data clustering, incorporating different spectral and structural constraints through diverse graph structures. Some methods rely on bipartite graph models, where nodes are divided into two classes: centers and members. These models typically require access to data for the center nodes in addition to observations from the member nodes. However, such additional data may not always be available in many practical scenarios. Moreover, popular Gaussian models for graph learning have demonstrated limited effectiveness in modeling data with heavy-tailed distributions, which are common in financial markets. In this paper, we propose a clustering method based on a bipartite graph model that addresses these challenges. First, it can infer clusters from incomplete data without requiring information about the center nodes. Second, it is designed to effectively handle heavy-tailed data. Numerical experiments using real financial data validate the efficiency of the proposed method for data clustering.","authors":["Amirhossein Javaheri","Daniel P. Palomar"],"url":"https://arxiv.org/abs/2505.08594"}
{"created":"2025-05-14","title":"Information Leakage in Data Linkage","abstract":"The process of linking databases that contain sensitive information about individuals across organisations is an increasingly common requirement in the health and social science research domains, as well as with governments and businesses. To protect personal data, protocols have been developed to limit the leakage of sensitive information. Furthermore, privacy-preserving record linkage (PPRL) techniques have been proposed to conduct linkage on encoded data. While PPRL techniques are now being employed in real-world applications, the focus of PPRL research has been on the technical aspects of linking sensitive data (such as encoding methods and cryptanalysis attacks), but not on organisational challenges when employing such techniques in practice. We analyse what sensitive information can possibly leak, either unintentionally or intentionally, in traditional data linkage as well as PPRL protocols, and what a party that participates in such a protocol can learn from the data it obtains legitimately within the protocol. We also show that PPRL protocols can still result in the unintentional leakage of sensitive information. We provide recommendations to help data custodians and other parties involved in a data linkage project to identify and prevent vulnerabilities and make their project more secure.","authors":["Peter Christen","Rainer Schnell","Anushka Vidanage"],"url":"https://arxiv.org/abs/2505.08596"}
{"created":"2025-05-14","title":"Grouptuner: Efficient Group-Aware Compiler Auto-tuning","abstract":"Modern compilers typically provide hundreds of options to optimize program performance, but users often cannot fully leverage them due to the huge number of options. While standard optimization combinations (e.g., -O3) provide reasonable defaults, they often fail to deliver near-peak performance across diverse programs and architectures. To address this challenge, compiler auto-tuning techniques have emerged to automate the discovery of improved option combinations. Existing techniques typically focus on identifying critical options and prioritizing them during the search to improve efficiency. However, due to limited tuning iterations, the resulting data is often sparse and noisy, making it highly challenging to accurately identify critical options. As a result, these algorithms are prone to being trapped in local optima.","authors":["Bingyu Gao","Mengyu Yao","Ziming Wang","Dong Liu","Ding Li","Xiangqun Chen","Yao Guo"],"url":"https://arxiv.org/abs/2505.08598"}
{"created":"2025-05-14","title":"MINIMALIST: switched-capacitor circuits for efficient in-memory computation of gated recurrent units","abstract":"Recurrent neural networks (RNNs) have been a long-standing candidate for processing of temporal sequence data, especially in memory-constrained systems that one may find in embedded edge computing environments. Recent advances in training paradigms have now inspired new generations of efficient RNNs. We introduce a streamlined and hardware-compatible architecture based on minimal gated recurrent units (GRUs), and an accompanying efficient mixed-signal hardware implementation of the model. The proposed design leverages switched-capacitor circuits not only for in-memory computation (IMC), but also for the gated state updates. The mixed-signal cores rely solely on commodity circuits consisting of metal capacitors, transmission gates, and a clocked comparator, thus greatly facilitating scaling and transfer to other technology nodes.","authors":["Sebastian Billaudelle","Laura Kriener","Filippo Moro","Tristan Torchet","Melika Payvand"],"url":"https://arxiv.org/abs/2505.08599"}
{"created":"2025-05-14","title":"Automatic Task Detection and Heterogeneous LLM Speculative Decoding","abstract":"Speculative decoding, which combines a draft model with a target model, has emerged as an effective approach to accelerate large language model (LLM) inference. However, existing methods often face a trade-off between the acceptance rate and decoding speed in downstream tasks due to the limited capacity of the draft model, making it difficult to ensure efficiency across diverse tasks. To address this problem, we propose a speculative decoding algorithm tailored for downstream task optimization. It includes an automatic task partitioning and assigning method, which automatically categorizes downstream tasks into different sub-tasks and assigns them to a set of heterogeneous draft models. Each draft model is aligned with the target model using task-specific data, thereby enhancing the consistency of inference results. In addition, our proposed method incorporates an online lightweight prompt classifier to dynamically route prompts to the appropriate draft model. Experimental results demonstrate that the proposed method improves draft accuracy by 6% to 50% over vanilla speculative decoding, while achieving a speedup of 1.10x to 2.64x in LLM inference.","authors":["Danying Ge","Jianhua Gao","Qizhi Jiang","Yifei Feng","Weixing Ji"],"url":"https://arxiv.org/abs/2505.08600"}
{"created":"2025-05-14","title":"Rejoining fragmented ancient bamboo slips with physics-driven deep learning","abstract":"Bamboo slips are a crucial medium for recording ancient civilizations in East Asia, and offers invaluable archaeological insights for reconstructing the Silk Road, studying material culture exchanges, and global history. However, many excavated bamboo slips have been fragmented into thousands of irregular pieces, making their rejoining a vital yet challenging step for understanding their content. Here we introduce WisePanda, a physics-driven deep learning framework designed to rejoin fragmented bamboo slips. Based on the physics of fracture and material deterioration, WisePanda automatically generates synthetic training data that captures the physical properties of bamboo fragmentations. This approach enables the training of a matching network without requiring manually paired samples, providing ranked suggestions to facilitate the rejoining process. Compared to the leading curve matching method, WisePanda increases Top-50 matching accuracy from 36\\% to 52\\%. Archaeologists using WisePanda have experienced substantial efficiency improvements (approximately 20 times faster) when rejoining fragmented bamboo slips. This research demonstrates that incorporating physical principles into deep learning models can significantly enhance their performance, transforming how archaeologists restore and study fragmented artifacts. WisePanda provides a new paradigm for addressing data scarcity in ancient artifact restoration through physics-driven machine learning.","authors":["Jinchi Zhu","Zhou Zhao","Hailong Lei","Xiaoguang Wang","Jialiang Lu","Jing Li","Qianqian Tang","Jiachen Shen","Gui-Song Xia","Bo Du","Yongchao Xu"],"url":"https://arxiv.org/abs/2505.08601"}
{"created":"2025-05-14","title":"Unsupervised Out-of-Distribution Detection in Medical Imaging Using Multi-Exit Class Activation Maps and Feature Masking","abstract":"Out-of-distribution (OOD) detection is essential for ensuring the reliability of deep learning models in medical imaging applications. This work is motivated by the observation that class activation maps (CAMs) for in-distribution (ID) data typically emphasize regions that are highly relevant to the model's predictions, whereas OOD data often lacks such focused activations. By masking input images with inverted CAMs, the feature representations of ID data undergo more substantial changes compared to those of OOD data, offering a robust criterion for differentiation. In this paper, we introduce a novel unsupervised OOD detection framework, Multi-Exit Class Activation Map (MECAM), which leverages multi-exit CAMs and feature masking. By utilizing mult-exit networks that combine CAMs from varying resolutions and depths, our method captures both global and local feature representations, thereby enhancing the robustness of OOD detection. We evaluate MECAM on multiple ID datasets, including ISIC19 and PathMNIST, and test its performance against three medical OOD datasets, RSNA Pneumonia, COVID-19, and HeadCT, and one natural image OOD dataset, iSUN. Comprehensive comparisons with state-of-the-art OOD detection methods validate the effectiveness of our approach. Our findings emphasize the potential of multi-exit networks and feature masking for advancing unsupervised OOD detection in medical imaging, paving the way for more reliable and interpretable models in clinical practice.","authors":["Yu-Jen Chen","Xueyang Li","Yiyu Shi","Tsung-Yi Ho"],"url":"https://arxiv.org/abs/2505.08604"}
{"created":"2025-05-14","title":"Leveraging Multi-Modal Information to Enhance Dataset Distillation","abstract":"Dataset distillation aims to create a compact and highly representative synthetic dataset that preserves the knowledge of a larger real dataset. While existing methods primarily focus on optimizing visual representations, incorporating additional modalities and refining object-level information can significantly improve the quality of distilled datasets. In this work, we introduce two key enhancements to dataset distillation: caption-guided supervision and object-centric masking. To integrate textual information, we propose two strategies for leveraging caption features: the feature concatenation, where caption embeddings are fused with visual features at the classification stage, and caption matching, which introduces a caption-based alignment loss during training to ensure semantic coherence between real and synthetic data. Additionally, we apply segmentation masks to isolate target objects and remove background distractions, introducing two loss functions designed for object-centric learning: masked feature alignment loss and masked gradient matching loss. Comprehensive evaluations demonstrate that integrating caption-based guidance and object-centric masking enhances dataset distillation, leading to synthetic datasets that achieve superior performance on downstream tasks.","authors":["Zhe Li","Hadrien Reynaud","Bernhard Kainz"],"url":"https://arxiv.org/abs/2505.08605"}
{"created":"2025-05-14","title":"Boosting Zero-shot Stereo Matching using Large-scale Mixed Images Sources in the Real World","abstract":"Stereo matching methods rely on dense pixel-wise ground truth labels, which are laborious to obtain, especially for real-world datasets. The scarcity of labeled data and domain gaps between synthetic and real-world images also pose notable challenges. In this paper, we propose a novel framework, \\textbf{BooSTer}, that leverages both vision foundation models and large-scale mixed image sources, including synthetic, real, and single-view images. First, to fully unleash the potential of large-scale single-view images, we design a data generation strategy combining monocular depth estimation and diffusion models to generate dense stereo matching data from single-view images. Second, to tackle sparse labels in real-world datasets, we transfer knowledge from monocular depth estimation models, using pseudo-mono depth labels and a dynamic scale- and shift-invariant loss for additional supervision. Furthermore, we incorporate vision foundation model as an encoder to extract robust and transferable features, boosting accuracy and generalization. Extensive experiments on benchmark datasets demonstrate the effectiveness of our approach, achieving significant improvements in accuracy over existing methods, particularly in scenarios with limited labeled data and domain shifts.","authors":["Yuran Wang","Yingping Liang","Ying Fu"],"url":"https://arxiv.org/abs/2505.08607"}
{"created":"2025-05-14","title":"WaveGuard: Robust Deepfake Detection and Source Tracing via Dual-Tree Complex Wavelet and Graph Neural Networks","abstract":"Deepfake technology poses increasing risks such as privacy invasion and identity theft. To address these threats, we propose WaveGuard, a proactive watermarking framework that enhances robustness and imperceptibility via frequency-domain embedding and graph-based structural consistency. Specifically, we embed watermarks into high-frequency sub-bands using Dual-Tree Complex Wavelet Transform (DT-CWT) and employ a Structural Consistency Graph Neural Network (SC-GNN) to preserve visual quality. We also design an attention module to refine embedding precision. Experimental results on face swap and reenactment tasks demonstrate that WaveGuard outperforms state-of-the-art methods in both robustness and visual quality. Code is available at https://github.com/vpsg-research/WaveGuard.","authors":["Ziyuan He","Zhiqing Guo","Liejun Wang","Gaobo Yang","Yunfeng Diao","Dan Ma"],"url":"https://arxiv.org/abs/2505.08614"}
{"created":"2025-05-14","title":"OpenThinkIMG: Learning to Think with Images via Visual Tool Reinforcement Learning","abstract":"While humans can flexibly leverage interactive visual cognition for complex problem-solving, enabling Large Vision-Language Models (LVLMs) to learn similarly adaptive behaviors with visual tools remains challenging. A significant hurdle is the current lack of standardized infrastructure, which hinders integrating diverse tools, generating rich interaction data, and training robust agents effectively. To address these gaps, we introduce OpenThinkIMG, the first open-source, comprehensive end-to-end framework for tool-augmented LVLMs. It features standardized vision tool interfaces, scalable trajectory generation for policy initialization, and a flexible training environment. Furthermore, considering supervised fine-tuning (SFT) on static demonstrations offers limited policy generalization for dynamic tool invocation, we propose a novel reinforcement learning (RL) framework V-ToolRL to train LVLMs to learn adaptive policies for invoking external vision tools. V-ToolRL enables LVLMs to autonomously discover optimal tool-usage strategies by directly optimizing for task success using feedback from tool interactions. We empirically validate V-ToolRL on challenging chart reasoning tasks. Our RL-trained agent, built upon a Qwen2-VL-2B, significantly outperforms its SFT-initialized counterpart (+28.83 points) and surpasses established supervised tool-learning baselines like Taco and CogCom by an average of +12.7 points. Notably, it also surpasses prominent closed-source models like GPT-4.1 by +8.68 accuracy points. We hope OpenThinkIMG can serve as a foundational framework for advancing dynamic, tool-augmented visual reasoning, helping the community develop AI agents that can genuinely \"think with images\".","authors":["Zhaochen Su","Linjie Li","Mingyang Song","Yunzhuo Hao","Zhengyuan Yang","Jun Zhang","Guanjie Chen","Jiawei Gu","Juntao Li","Xiaoye Qu","Yu Cheng"],"url":"https://arxiv.org/abs/2505.08617"}
{"created":"2025-05-14","title":"Cost Function Estimation Using Inverse Reinforcement Learning with Minimal Observations","abstract":"We present an iterative inverse reinforcement learning algorithm to infer optimal cost functions in continuous spaces. Based on a popular maximum entropy criteria, our approach iteratively finds a weight improvement step and proposes a method to find an appropriate step size that ensures learned cost function features remain similar to the demonstrated trajectory features. In contrast to similar approaches, our algorithm can individually tune the effectiveness of each observation for the partition function and does not need a large sample set, enabling faster learning. We generate sample trajectories by solving an optimal control problem instead of random sampling, leading to more informative trajectories. The performance of our method is compared to two state of the art algorithms to demonstrate its benefits in several simulated environments.","authors":["Sarmad Mehrdad","Avadesh Meduri","Ludovic Righetti"],"url":"https://arxiv.org/abs/2505.08619"}
{"created":"2025-05-14","title":"Resource-Efficient Language Models: Quantization for Fast and Accessible Inference","abstract":"Large language models have significantly advanced natural language processing, yet their heavy resource demands pose severe challenges regarding hardware accessibility and energy consumption. This paper presents a focused and high-level review of post-training quantization (PTQ) techniques designed to optimize the inference efficiency of LLMs by the end-user, including details on various quantization schemes, granularities, and trade-offs. The aim is to provide a balanced overview between the theory and applications of post-training quantization.","authors":["Tollef Emil J{\\o}rgensen"],"url":"https://arxiv.org/abs/2505.08620"}
{"created":"2025-05-14","title":"Visually Guided Decoding: Gradient-Free Hard Prompt Inversion with Language Models","abstract":"Text-to-image generative models like DALL-E and Stable Diffusion have revolutionized visual content creation across various applications, including advertising, personalized media, and design prototyping. However, crafting effective textual prompts to guide these models remains challenging, often requiring extensive trial and error. Existing prompt inversion approaches, such as soft and hard prompt techniques, are not so effective due to the limited interpretability and incoherent prompt generation. To address these issues, we propose Visually Guided Decoding (VGD), a gradient-free approach that leverages large language models (LLMs) and CLIP-based guidance to generate coherent and semantically aligned prompts. In essence, VGD utilizes the robust text generation capabilities of LLMs to produce human-readable prompts. Further, by employing CLIP scores to ensure alignment with user-specified visual concepts, VGD enhances the interpretability, generalization, and flexibility of prompt generation without the need for additional training. Our experiments demonstrate that VGD outperforms existing prompt inversion techniques in generating understandable and contextually relevant prompts, facilitating more intuitive and controllable interactions with text-to-image models.","authors":["Donghoon Kim","Minji Bae","Kyuhong Shim","Byonghyo Shim"],"url":"https://arxiv.org/abs/2505.08622"}
{"created":"2025-05-14","title":"Beyond Predefined Actions: Integrating Behavior Trees and Dynamic Movement Primitives for Robot Learning from Demonstration","abstract":"Interpretable policy representations like Behavior Trees (BTs) and Dynamic Motion Primitives (DMPs) enable robot skill transfer from human demonstrations, but each faces limitations: BTs require expert-crafted low-level actions, while DMPs lack high-level task logic. We address these limitations by integrating DMP controllers into a BT framework, jointly learning the BT structure and DMP actions from single demonstrations, thereby removing the need for predefined actions. Additionally, by combining BT decision logic with DMP motion generation, our method enhances policy interpretability, modularity, and adaptability for autonomous systems. Our approach readily affords both learning to replicate low-level motions and combining partial demonstrations into a coherent and easy-to-modify overall policy.","authors":["David C\\'aceres Dom\\'inguez","Erik Schaffernicht","Todor Stoyanov"],"url":"https://arxiv.org/abs/2505.08625"}
{"created":"2025-05-14","title":"Augmented Reality for RObots (ARRO): Pointing Visuomotor Policies Towards Visual Robustness","abstract":"Visuomotor policies trained on human expert demonstrations have recently shown strong performance across a wide range of robotic manipulation tasks. However, these policies remain highly sensitive to domain shifts stemming from background or robot embodiment changes, which limits their generalization capabilities. In this paper, we present ARRO, a novel calibration-free visual representation that leverages zero-shot open-vocabulary segmentation and object detection models to efficiently mask out task-irrelevant regions of the scene without requiring additional training. By filtering visual distractors and overlaying virtual guides during both training and inference, ARRO improves robustness to scene variations and reduces the need for additional data collection. We extensively evaluate ARRO with Diffusion Policy on several tabletop manipulation tasks in both simulation and real-world environments, and further demonstrate its compatibility and effectiveness with generalist robot policies, such as Octo and OpenVLA. Across all settings in our evaluation, ARRO yields consistent performance gains, allows for selective masking to choose between different objects, and shows robustness even to challenging segmentation conditions. Videos showcasing our results are available at: augmented-reality-for-robots.github.io","authors":["Reihaneh Mirjalili","Tobias J\\\"ulg","Florian Walter","Wolfram Burgard"],"url":"https://arxiv.org/abs/2505.08627"}
{"created":"2025-05-14","title":"Integrating Natural Language Processing and Exercise Monitoring for Early Diagnosis of Metabolic Syndrome: A Deep Learning Approach","abstract":"Metabolic syndrome (MetS) is a medication condition characterized by abdominal obesity, insulin resistance, hypertension and hyperlipidemia. It increases the risk of majority of chronic diseases, including type 2 diabetes mellitus, and affects about one quarter of the global population. Therefore, early detection and timely intervention for MetS are crucial. Standard diagnosis for MetS components requires blood tests conducted within medical institutions. However, it is frequently underestimated, leading to unmet need for care for MetS population. This study aims to use the least physiological data and free texts about exercises related activities, which are obtained easily in daily life, to diagnosis MetS. We collected the data from 40 volunteers in a nursing home and used data augmentation to reduce the imbalance. We propose a deep learning framework for classifying MetS that integrates natural language processing (NLP) and exercise monitoring. The results showed that the best model reported a high positive result (AUROC=0.806 and REC=76.3%) through 3-fold cross-validation. Feature importance analysis revealed that text and minimum heart rate on a daily basis contribute the most in the classification of MetS. This study demonstrates the potential application of data that are easily measurable in daily life for the early diagnosis of MetS, which could contribute to reducing the cost of screening and management for MetS population.","authors":["Yichen Zhao","Yuhua Wang","Xi Cheng","Junhao Fang","Yang Yang"],"url":"https://arxiv.org/abs/2505.08628"}
{"created":"2025-05-14","title":"Credit Assignment and Efficient Exploration based on Influence Scope in Multi-agent Reinforcement Learning","abstract":"Training cooperative agents in sparse-reward scenarios poses significant challenges for multi-agent reinforcement learning (MARL). Without clear feedback on actions at each step in sparse-reward setting, previous methods struggle with precise credit assignment among agents and effective exploration. In this paper, we introduce a novel method to deal with both credit assignment and exploration problems in reward-sparse domains. Accordingly, we propose an algorithm that calculates the Influence Scope of Agents (ISA) on states by taking specific value of the dimensions/attributes of states that can be influenced by individual agents. The mutual dependence between agents' actions and state attributes are then used to calculate the credit assignment and to delimit the exploration space for each individual agent. We then evaluate ISA in a variety of sparse-reward multi-agent scenarios. The results show that our method significantly outperforms the state-of-art baselines.","authors":["Shuai Han","Mehdi Dastani","Shihan Wang"],"url":"https://arxiv.org/abs/2505.08630"}
{"created":"2025-05-14","title":"Learning cardiac activation and repolarization times with operator learning","abstract":"Solving partial or ordinary differential equation models in cardiac electrophysiology is a computationally demanding task, particularly when high-resolution meshes are required to capture the complex dynamics of the heart. Moreover, in clinical applications, it is essential to employ computational tools that provide only relevant information, ensuring clarity and ease of interpretation. In this work, we exploit two recently proposed operator learning approaches, namely Fourier Neural Operators (FNO) and Kernel Operator Learning (KOL), to learn the operator mapping the applied stimulus in the physical domain into the activation and repolarization time distributions. These data-driven methods are evaluated on synthetic 2D and 3D domains, as well as on a physiologically realistic left ventricle geometry. Notably, while the learned map between the applied current and activation time has its modelling counterpart in the Eikonal model, no equivalent partial differential equation (PDE) model is known for the map between the applied current and repolarization time. Our results demonstrate that both FNO and KOL approaches are robust to hyperparameter choices and computationally efficient compared to traditional PDE-based Monodomain models. These findings highlight the potential use of these surrogate operators to accelerate cardiac simulations and facilitate their clinical integration.","authors":["Edoardo Centofanti","Giovanni Ziarelli","Nicola Parolini","Simone Scacchi","Marco Verani","Luca Franco Pavarino"],"url":"https://arxiv.org/abs/2505.08631"}
{"created":"2025-05-14","title":"D-Hammer: Efficient Equational Reasoning for Labelled Dirac Notation","abstract":"Labelled Dirac notation is a formalism commonly used by physicists to represent many-body quantum systems and by computer scientists to assert properties of quantum programs. It is supported by a rich equational theory for proving equality between expressions in the language. These proofs are typically carried on pen-and-paper, and can be exceedingly long and error-prone. We introduce D-Hammer, the first tool to support automated equational proof for labelled Dirac notation. The salient features of D-Hammer include: an expressive, higher-order, dependently-typed language for labelled Dirac notation; an efficient normalization algorithm; and an optimized C++ implementation. We evaluate the implementation on representative examples from both plain and labelled Dirac notation. In the case of plain Dirac notation, we show that our implementation significantly outperforms DiracDec.","authors":["Yingte Xu","Li Zhou","Gilles Barthe"],"url":"https://arxiv.org/abs/2505.08633"}
{"created":"2025-05-14","title":"TRAIL: Trace Reasoning and Agentic Issue Localization","abstract":"The increasing adoption of agentic workflows across diverse domains brings a critical need to scalably and systematically evaluate the complex traces these systems generate. Current evaluation methods depend on manual, domain-specific human analysis of lengthy workflow traces - an approach that does not scale with the growing complexity and volume of agentic outputs. Error analysis in these settings is further complicated by the interplay of external tool outputs and language model reasoning, making it more challenging than traditional software debugging. In this work, we (1) articulate the need for robust and dynamic evaluation methods for agentic workflow traces, (2) introduce a formal taxonomy of error types encountered in agentic systems, and (3) present a set of 148 large human-annotated traces (TRAIL) constructed using this taxonomy and grounded in established agentic benchmarks. To ensure ecological validity, we curate traces from both single and multi-agent systems, focusing on real-world applications such as software engineering and open-world information retrieval. Our evaluations reveal that modern long context LLMs perform poorly at trace debugging, with the best Gemini-2.5-pro model scoring a mere 11% on TRAIL. Our dataset and code are made publicly available to support and accelerate future research in scalable evaluation for agentic workflows.","authors":["Darshan Deshpande","Varun Gangal","Hersh Mehta","Jitin Krishnan","Anand Kannappan","Rebecca Qian"],"url":"https://arxiv.org/abs/2505.08638"}
{"created":"2025-05-14","title":"Robust Indoor Localization via Conformal Methods and Variational Bayesian Adaptive Filtering","abstract":"Indoor localization is critical for IoT applications, yet challenges such as non-Gaussian noise, environmental interference, and measurement outliers hinder the robustness of traditional methods. Existing approaches, including Kalman filtering and its variants, often rely on Gaussian assumptions or static thresholds, limiting adaptability in dynamic environments. This paper proposes a hierarchical robust framework integrating Variational Bayesian (VB) parameter learning, Huber M-estimation, and Conformal Outlier Detection (COD) to address these limitations. First, VB inference jointly estimates state and noise parameters, adapting to time-varying uncertainties. Second, Huber-based robust filtering suppresses mild outliers while preserving Gaussian efficiency. Third, COD provides statistical guarantees for outlier detection via dynamically calibrated thresholds, ensuring a user-controlled false alarm rate. Theoretically, we prove the Semi-positive Definiteness of Huber-based Kalman filtering covariance and the coverage of sliding window conformal prediction. Experiments on geomagnetic fingerprint datasets demonstrate significant improvements: fingerprint matching accuracy increases from 81.25% to 93.75%, and positioning errors decrease from 0.62-6.87 m to 0.03-0.35 m. Comparative studies further validate the framework's robustness, showing consistent performance gains under non-Gaussian noise and outlier conditions.","authors":["Zhiyi Zhou","Dongzhuo Liu","Songtao Guo","Yuanyuan Yang"],"url":"https://arxiv.org/abs/2505.08639"}
{"created":"2025-05-14","title":"WixQA: A Multi-Dataset Benchmark for Enterprise Retrieval-Augmented Generation","abstract":"Retrieval-Augmented Generation (RAG) is a cornerstone of modern question answering (QA) systems, enabling grounded answers based on external knowledge. Although recent progress has been driven by open-domain datasets, enterprise QA systems need datasets that mirror the concrete, domain-specific issues users raise in day-to-day support scenarios. Critically, evaluating end-to-end RAG systems requires benchmarks comprising not only question--answer pairs but also the specific knowledge base (KB) snapshot from which answers were derived. To address this need, we introduce WixQA, a benchmark suite featuring QA datasets precisely grounded in the released KB corpus, enabling holistic evaluation of retrieval and generation components. WixQA includes three distinct QA datasets derived from Wix.com customer support interactions and grounded in a snapshot of the public Wix Help Center KB: (i) WixQA-ExpertWritten, 200 real user queries with expert-authored, multi-step answers; (ii) WixQA-Simulated, 200 expert-validated QA pairs distilled from user dialogues; and (iii) WixQA-Synthetic, 6,222 LLM-generated QA pairs, with one pair systematically derived from each article in the knowledge base. We release the KB snapshot alongside the datasets under MIT license and provide comprehensive baseline results, forming a unique benchmark for evaluating enterprise RAG systems in realistic enterprise environments.","authors":["Dvir Cohen","Lin Burg","Sviatoslav Pykhnivskyi","Hagit Gur","Stanislav Kovynov","Olga Atzmon","Gilad Barkan"],"url":"https://arxiv.org/abs/2505.08643"}
{"created":"2025-05-14","title":"DLO-Splatting: Tracking Deformable Linear Objects Using 3D Gaussian Splatting","abstract":"This work presents DLO-Splatting, an algorithm for estimating the 3D shape of Deformable Linear Objects (DLOs) from multi-view RGB images and gripper state information through prediction-update filtering. The DLO-Splatting algorithm uses a position-based dynamics model with shape smoothness and rigidity dampening corrections to predict the object shape. Optimization with a 3D Gaussian Splatting-based rendering loss iteratively renders and refines the prediction to align it with the visual observations in the update step. Initial experiments demonstrate promising results in a knot tying scenario, which is challenging for existing vision-only methods.","authors":["Holly Dinkel","Marcel B\\\"usching","Alberta Longhini","Brian Coltin","Trey Smith","Danica Kragic","M{\\aa}rten Bj\\\"orkman","Timothy Bretl"],"url":"https://arxiv.org/abs/2505.08644"}
{"created":"2025-05-14","title":"Topology and geometry optimization of grid-shells under self-weight loading","abstract":"This manuscript presents an approach for simultaneously optimizing the connectivity and elevation of grid-shell structures acting in pure compression (or pure tension) under the combined effects of a prescribed external loading and the design-dependent self-weight of the structure itself. The method derived herein involves solving a second-order cone optimization problem, thereby ensuring convexity and obtaining globally optimal results for a given discretization of the design domain. Several numerical examples are presented, illustrating characteristics of this class of optimal structures. It is found that, as self-weight becomes more significant, both the optimal topology and the optimal elevation profile of the structure change, highlighting the importance of optimizing both topology and geometry simultaneously from the earliest stages of design. It is shown that this approach can obtain solutions with greater accuracy and several orders of magnitude more quickly than a standard 3D layout/truss topology optimization approach.","authors":["Helen E. Fairclough","Karol Bolbotowski","Linwei He","Andrew Liew","Matthew Gilbert"],"url":"https://arxiv.org/abs/2505.08645"}
{"created":"2025-05-14","title":"Modular Federated Learning: A Meta-Framework Perspective","abstract":"Federated Learning (FL) enables distributed machine learning training while preserving privacy, representing a paradigm shift for data-sensitive and decentralized environments. Despite its rapid advancements, FL remains a complex and multifaceted field, requiring a structured understanding of its methodologies, challenges, and applications. In this survey, we introduce a meta-framework perspective, conceptualising FL as a composition of modular components that systematically address core aspects such as communication, optimisation, security, and privacy. We provide a historical contextualisation of FL, tracing its evolution from distributed optimisation to modern distributed learning paradigms. Additionally, we propose a novel taxonomy distinguishing Aggregation from Alignment, introducing the concept of alignment as a fundamental operator alongside aggregation. To bridge theory with practice, we explore available FL frameworks in Python, facilitating real-world implementation. Finally, we systematise key challenges across FL sub-fields, providing insights into open research questions throughout the meta-framework modules. By structuring FL within a meta-framework of modular components and emphasising the dual role of Aggregation and Alignment, this survey provides a holistic and adaptable foundation for understanding and advancing FL research and deployment.","authors":["Frederico Vicente","Cl\\'audia Soares","Du\\v{s}an Jakoveti\\'c"],"url":"https://arxiv.org/abs/2505.08646"}
{"created":"2025-05-14","title":"Enhancing Software Development with Context-Aware Conversational Agents: A User Study on Developer Interactions with Chatbots","abstract":"Software development is a cognitively intensive process requiring multitasking, adherence to evolving workflows, and continuous learning. With the rise of large language model (LLM)-based tools, such as conversational agents (CAs), there is growing interest in supporting developers through natural language interaction. However, little is known about the specific features developers seek in these systems. We conducted a user study with 29 developers using a prototype text-based chatbot to investigate preferred functionalities. Our findings reveal strong interest in task automation, version control support, and contextual adaptability, especially the need to tailor assistance for both novice and experienced users. We highlight the importance of deep contextual understanding, historical interaction awareness, and personalized support in CA design. This study contributes to the development of context-aware chatbots that enhance productivity and satisfaction, and it outlines opportunities for future research on human-AI collaboration in software engineering.","authors":["Glaucia Melo","Paulo Alencar","Donald Cowan"],"url":"https://arxiv.org/abs/2505.08648"}
{"created":"2025-05-14","title":"Cryptologic Techniques and Associated Risks in Public and Private Security. An Italian and European Union Perspective with an Overview of the Current Legal Framework","abstract":"This article examines the evolution of cryptologic techniques and their implications for public and private security, focusing on the Italian and EU legal frameworks. It explores the roles of cryptography, steganography, and quantum technologies in countering cybersecurity threats, emphasising the need for robust legislation to address emerging challenges. Special attention is given to Italy's legislative reforms, including Law No. 90 of 2024, which strengthens penalties for cybercrimes and establishes the National Cryptography Centre within the Italian National Cybersecurity Agency. Additionally, the article highlights international initiatives, such as the UN's draft convention on cybercrime, emphasising the balance between security, privacy, and fundamental human rights in a post-quantum era.","authors":["Zana Kudriasova"],"url":"https://arxiv.org/abs/2505.08650"}
{"created":"2025-05-14","title":"Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing","abstract":"We present MegaBeam-Mistral-7B, a language model that supports 512K-token context length. Our work addresses practical limitations in long-context training, supporting real-world tasks such as compliance monitoring and verification. Evaluated on three long-context benchmarks, our 7B-parameter model demonstrates superior in-context learning performance on HELMET and robust retrieval and tracing capability on RULER. It is currently the only open model to achieve competitive long-range reasoning on BABILong at 512K context length without RAG or targeted fine-tuning. Released as fully open source under the Apache 2.0 license, the model has been downloaded over 100,000 times on Hugging Face. Model available at: https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k","authors":["Chen Wu","Yin Song"],"url":"https://arxiv.org/abs/2505.08651"}
{"created":"2025-05-14","title":"Comparative Analysis of Blockchain Systems","abstract":"Blockchain is a type of decentralized distributed database. Unlike traditional relational database management systems, it does not require management or maintenance by a third party. All data management and update processes are open and transparent, solving the trust issues of centralized database management systems. Blockchain ensures network-wide consistency, consensus, traceability, and immutability. Under the premise of mutual distrust between nodes, blockchain technology integrates various technologies, such as P2P protocols, asymmetric encryption, consensus mechanisms, and chain structures. Data is distributed and stored across multiple nodes, maintained by all nodes, ensuring transaction data integrity, undeniability, and security. This facilitates trusted information sharing and supervision. The basic principles of blockchain form the foundation for all related research. Understanding the working principles is essential for further study of blockchain technology. There are many platforms based on blockchain technology, and they differ from one another. This paper will analyze the architecture of blockchain systems at each layer, focusing on the principles and technologies of blockchain platforms such as Bitcoin, Ethereum, and Hyperledger Fabric. The analysis will cover their scalability and security and highlight their similarities, differences, advantages, and disadvantages.","authors":["Jiaqi Huang","Yuanzheng Niu","Xiaoqi Li","Zongwei Li"],"url":"https://arxiv.org/abs/2505.08652"}
{"created":"2025-05-14","title":"A Comparative Study of Human Activity Recognition: Motion, Tactile, and multi-modal Approaches","abstract":"Human activity recognition (HAR) is essential for effective Human-Robot Collaboration (HRC), enabling robots to interpret and respond to human actions. This study evaluates the ability of a vision-based tactile sensor to classify 15 activities, comparing its performance to an IMU-based data glove. Additionally, we propose a multi-modal framework combining tactile and motion data to leverage their complementary strengths. We examined three approaches: motion-based classification (MBC) using IMU data, tactile-based classification (TBC) with single or dual video streams, and multi-modal classification (MMC) integrating both. Offline validation on segmented datasets assessed each configuration's accuracy under controlled conditions, while online validation on continuous action sequences tested online performance. Results showed the multi-modal approach consistently outperformed single-modality methods, highlighting the potential of integrating tactile and motion sensing to enhance HAR systems for collaborative robotics.","authors":["Valerio Belcamino","Nhat Minh Dinh Le","Quan Khanh Luu","Alessandro Carf\\`i","Van Anh Ho","Fulvio Mastrogiovanni"],"url":"https://arxiv.org/abs/2505.08657"}
{"created":"2025-05-14","title":"Revealing economic facts: LLMs know more than they say","abstract":"We investigate whether the hidden states of large language models (LLMs) can be used to estimate and impute economic and financial statistics. Focusing on county-level (e.g. unemployment) and firm-level (e.g. total assets) variables, we show that a simple linear model trained on the hidden states of open-source LLMs outperforms the models' text outputs. This suggests that hidden states capture richer economic information than the responses of the LLMs reveal directly. A learning curve analysis indicates that only a few dozen labelled examples are sufficient for training. We also propose a transfer learning method that improves estimation accuracy without requiring any labelled data for the target variable. Finally, we demonstrate the practical utility of hidden-state representations in super-resolution and data imputation tasks.","authors":["Marcus Buckmann","Quynh Anh Nguyen","Edward Hill"],"url":"https://arxiv.org/abs/2505.08662"}
{"created":"2025-05-14","title":"A Social Robot with Inner Speech for Dietary Guidance","abstract":"We explore the use of inner speech as a mechanism to enhance transparency and trust in social robots for dietary advice. In humans, inner speech structures thought processes and decision-making; in robotics, it improves explainability by making reasoning explicit. This is crucial in healthcare scenarios, where trust in robotic assistants depends on both accurate recommendations and human-like dialogue, which make interactions more natural and engaging. Building on this, we developed a social robot that provides dietary advice, and we provided the architecture with inner speech capabilities to validate user input, refine reasoning, and generate clear justifications. The system integrates large language models for natural language understanding and a knowledge graph for structured dietary information. By making decisions more transparent, our approach strengthens trust and improves human-robot interaction in healthcare. We validated this by measuring the computational efficiency of our architecture and conducting a small user study, which assessed the reliability of inner speech in explaining the robot's behavior.","authors":["Valerio Belcamino","Alessandro Carf\\`i","Valeria Seidita","Fulvio Mastrogiovanni","Antonio Chella"],"url":"https://arxiv.org/abs/2505.08664"}
{"created":"2025-05-14","title":"SkillFormer: Unified Multi-View Video Understanding for Proficiency Estimation","abstract":"Assessing human skill levels in complex activities is a challenging problem with applications in sports, rehabilitation, and training. In this work, we present SkillFormer, a parameter-efficient architecture for unified multi-view proficiency estimation from egocentric and exocentric videos. Building on the TimeSformer backbone, SkillFormer introduces a CrossViewFusion module that fuses view-specific features using multi-head cross-attention, learnable gating, and adaptive self-calibration. We leverage Low-Rank Adaptation to fine-tune only a small subset of parameters, significantly reducing training costs. In fact, when evaluated on the EgoExo4D dataset, SkillFormer achieves state-of-the-art accuracy in multi-view settings while demonstrating remarkable computational efficiency, using 4.5x fewer parameters and requiring 3.75x fewer training epochs than prior baselines. It excels in multiple structured tasks, confirming the value of multi-view integration for fine-grained skill assessment.","authors":["Edoardo Bianchi","Antonio Liotta"],"url":"https://arxiv.org/abs/2505.08665"}
{"created":"2025-05-14","title":"Claycode: Stylable and Deformable 2D Scannable Codes","abstract":"This paper introduces Claycode, a novel 2D scannable code designed for extensive stylization and deformation. Unlike traditional matrix-based codes (e.g., QR codes), Claycodes encode their message in a tree structure. During the encoding process, bits are mapped into a topology tree, which is then depicted as a nesting of color regions drawn within the boundaries of a target polygon shape. When decoding, Claycodes are extracted and interpreted in real-time from a camera stream. We detail the end-to-end pipeline and show that Claycodes allow for extensive stylization without compromising their functionality. We then empirically demonstrate Claycode's high tolerance to heavy deformations, outperforming traditional 2D scannable codes in scenarios where they typically fail.","authors":["Marco Maida","Alberto Crescini","Marco Perronet","Elena Camuffo"],"url":"https://arxiv.org/abs/2505.08666"}
{"created":"2025-05-14","title":"How Students Use AI Feedback Matters: Experimental Evidence on Physics Achievement and Autonomy","abstract":"Despite the precision and adaptiveness of generative AI (GAI)-powered feedback provided to students, existing practice and literature might ignore how usage patterns impact student learning. This study examines the heterogeneous effects of GAI-powered personalized feedback on high school students' physics achievement and autonomy through two randomized controlled trials, with a major focus on usage patterns. Each experiment lasted for five weeks, involving a total of 387 students. Experiment 1 (n = 121) assessed compulsory usage of the personalized recommendation system, revealing that low-achieving students significantly improved academic performance (d = 0.673, p < 0.05) when receiving AI-generated heuristic solution hints, whereas medium-achieving students' performance declined (d = -0.539, p < 0.05) with conventional answers provided by workbook. Notably, high-achieving students experienced a significant decline in self-regulated learning (d = -0.477, p < 0.05) without any significant gains in achievement. Experiment 2 (n = 266) investigated the usage pattern of autonomous on-demand help, demonstrating that fully learner-controlled AI feedback significantly enhanced academic performance for high-achieving students (d = 0.378, p < 0.05) without negatively impacting their autonomy. However, autonomy notably declined among lower achievers exposed to on-demand AI interventions (d = -0.383, p < 0.05), particularly in the technical-psychological dimension (d = -0.549, p < 0.05), which has a large overlap with self-regulation. These findings underscore the importance of usage patterns when applying GAI-powered personalized feedback to students.","authors":["Xusheng Dai","Zhaochun Wen","Jianxiao Jiang","Huiqin Liu","Yu Zhang"],"url":"https://arxiv.org/abs/2505.08672"}
{"created":"2025-05-14","title":"A Study of Data-driven Methods for Inventory Optimization","abstract":"This paper shows a comprehensive analysis of three algorithms (Time Series, Random Forest (RF) and Deep Reinforcement Learning) into three inventory models (the Lost Sales, Dual-Sourcing and Multi-Echelon Inventory Model). These methodologies are applied in the supermarket context. The main purpose is to analyse efficient methods for the data-driven. Their possibility, potential and current challenges are taken into consideration in this report. By comparing the results in each model, the effectiveness of each algorithm is evaluated based on several key performance indicators, including forecast accuracy, adaptability to market changes, and overall impact on inventory costs and customer satisfaction levels. The data visualization tools and statistical metrics are the indicators for the comparisons and show some obvious trends and patterns that can guide decision-making in inventory management. These tools enable managers to not only track the performance of different algorithms in real-time but also to drill down into specific data points to understand the underlying causes of inventory fluctuations. This level of detail is crucial for pinpointing inefficiencies and areas for improvement within the supply chain.","authors":["Lee Yeung Ping","Patrick Wong","Tan Cheng Han"],"url":"https://arxiv.org/abs/2505.08673"}
{"created":"2025-05-14","title":"A Mamba-based Network for Semi-supervised Singing Melody Extraction Using Confidence Binary Regularization","abstract":"Singing melody extraction (SME) is a key task in the field of music information retrieval. However, existing methods are facing several limitations: firstly, prior models use transformers to capture the contextual dependencies, which requires quadratic computation resulting in low efficiency in the inference stage. Secondly, prior works typically rely on frequencysupervised methods to estimate the fundamental frequency (f0), which ignores that the musical performance is actually based on notes. Thirdly, transformers typically require large amounts of labeled data to achieve optimal performances, but the SME task lacks of sufficient annotated data. To address these issues, in this paper, we propose a mamba-based network, called SpectMamba, for semi-supervised singing melody extraction using confidence binary regularization. In particular, we begin by introducing vision mamba to achieve computational linear complexity. Then, we propose a novel note-f0 decoder that allows the model to better mimic the musical performance. Further, to alleviate the scarcity of the labeled data, we introduce a confidence binary regularization (CBR) module to leverage the unlabeled data by maximizing the probability of the correct classes. The proposed method is evaluated on several public datasets and the conducted experiments demonstrate the effectiveness of our proposed method.","authors":["Xiaoliang He","Kangjie Dong","Jingkai Cao","Shuai Yu","Wei Li","Yi Yu"],"url":"https://arxiv.org/abs/2505.08681"}
{"created":"2025-05-14","title":"Joint Communication Scheduling and Resource Allocation for Distributed Edge Learning: Seamless Integration in Next-Generation Wireless Networks","abstract":"Distributed edge learning (DL) is considered a cornerstone of intelligence enablers, since it allows for collaborative training without the necessity for local clients to share raw data with other parties, thereby preserving privacy and security. Integrating DL into the 6G networks requires a coexistence design with existing services such as high-bandwidth (HB) traffic like eMBB. Current designs in the literature mainly focus on communication round-wise designs that assume a rigid resource allocation throughout each communication round (CR). However, rigid resource allocation within a CR is a highly inefficient and inaccurate representation of the system's realistic behavior. This is due to the heterogeneous nature of the system, as clients inherently may need to access the network at different times. This work zooms into one arbitrary CR, and demonstrates the importance of considering a time-dependent resource sharing design with HB traffic. We first formulate a time-step-wise optimization problem to minimize the consumed time by DL within the CR while constrained by a DL energy budget. Due to its intractability, a session-based optimization problem is formulated assuming a CR lasts less than a large-scale coherence time. Some scheduling properties of such multi-server joint communication scheduling and resource allocation framework have been established. An iterative algorithm has been designed to solve such non-convex and non-block-separable-constrained problems. Simulation results confirm the importance of the efficient and accurate integration design proposed in this work.","authors":["Paul Zheng","Navid Keshtiarast","Pradyumna Kumar Bishoyi","Yao Zhu","Yulin Hu","Marina Petrova","Anke Schmeink"],"url":"https://arxiv.org/abs/2505.08682"}
{"created":"2025-05-14","title":"Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS) challenge results","abstract":"Deep learning (DL) has become the dominant approach for medical image segmentation, yet ensuring the reliability and clinical applicability of these models requires addressing key challenges such as annotation variability, calibration, and uncertainty estimation. This is why we created the Calibration and Uncertainty for multiRater Volume Assessment in multiorgan Segmentation (CURVAS), which highlights the critical role of multiple annotators in establishing a more comprehensive ground truth, emphasizing that segmentation is inherently subjective and that leveraging inter-annotator variability is essential for robust model evaluation. Seven teams participated in the challenge, submitting a variety of DL models evaluated using metrics such as Dice Similarity Coefficient (DSC), Expected Calibration Error (ECE), and Continuous Ranked Probability Score (CRPS). By incorporating consensus and dissensus ground truth, we assess how DL models handle uncertainty and whether their confidence estimates align with true segmentation performance. Our findings reinforce the importance of well-calibrated models, as better calibration is strongly correlated with the quality of the results. Furthermore, we demonstrate that segmentation models trained on diverse datasets and enriched with pre-trained knowledge exhibit greater robustness, particularly in cases deviating from standard anatomical structures. Notably, the best-performing models achieved high DSC and well-calibrated uncertainty estimates. This work underscores the need for multi-annotator ground truth, thorough calibration assessments, and uncertainty-aware evaluations to develop trustworthy and clinically reliable DL-based medical image segmentation models.","authors":["Meritxell Riera-Marin","Sikha O K","Julia Rodriguez-Comas","Matthias Stefan May","Zhaohong Pan","Xiang Zhou","Xiaokun Liang","Franciskus Xaverius Erick","Andrea Prenner","Cedric Hemon","Valentin Boussot","Jean-Louis Dillenseger","Jean-Claude Nunes","Abdul Qayyum","Moona Mazher","Steven A Niederer","Kaisar Kushibar","Carlos Martin-Isla","Petia Radeva","Karim Lekadir","Theodore Barfoot","Luis C. Garcia Peraza Herrera","Ben Glocker","Tom Vercauteren","Lucas Gago","Justin Englemann","Joy-Marie Kleiss","Anton Aubanell","Andreu Antolin","Javier Garcia-Lopez","Miguel A. Gonzalez Ballester","Adrian Galdran"],"url":"https://arxiv.org/abs/2505.08685"}
{"created":"2025-05-14","title":"CAD-Coder:Text-Guided CAD Files Code Generation","abstract":"Computer-aided design (CAD) is a way to digitally create 2D drawings and 3D models of real-world products. Traditional CAD typically relies on hand-drawing by experts or modifications of existing library files, which doesn't allow for rapid personalization. With the emergence of generative artificial intelligence, convenient and efficient personalized CAD generation has become possible. However, existing generative methods typically produce outputs that lack interactive editability and geometric annotations, limiting their practical applications in manufacturing. To enable interactive generative CAD, we propose CAD-Coder, a framework that transforms natural language instructions into CAD script codes, which can be executed in Python environments to generate human-editable CAD files (.Dxf). To facilitate the generation of editable CAD sketches with annotation information, we construct a comprehensive dataset comprising 29,130 Dxf files with their corresponding script codes, where each sketch preserves both editability and geometric annotations. We evaluate CAD-Coder on various 2D/3D CAD generation tasks against existing methods, demonstrating superior interactive capabilities while uniquely providing editable sketches with geometric annotations.","authors":["Changqi He","Shuhan Zhang","Liguo Zhang","Jiajun Miao"],"url":"https://arxiv.org/abs/2505.08686"}
{"created":"2025-05-14","title":"AC-PKAN: Attention-Enhanced and Chebyshev Polynomial-Based Physics-Informed Kolmogorov-Arnold Networks","abstract":"Kolmogorov-Arnold Networks (KANs) have recently shown promise for solving partial differential equations (PDEs). Yet their original formulation is computationally and memory intensive, motivating the introduction of Chebyshev Type-I-based KANs (Chebyshev1KANs). Although Chebyshev1KANs have outperformed the vanilla KANs architecture, our rigorous theoretical analysis reveals that they still suffer from rank collapse, ultimately limiting their expressive capacity. To overcome these limitations, we enhance Chebyshev1KANs by integrating wavelet-activated MLPs with learnable parameters and an internal attention mechanism. We prove that this design preserves a full-rank Jacobian and is capable of approximating solutions to PDEs of arbitrary order. Furthermore, to alleviate the loss instability and imbalance introduced by the Chebyshev polynomial basis, we externally incorporate a Residual Gradient Attention (RGA) mechanism that dynamically re-weights individual loss terms according to their gradient norms and residual magnitudes. By jointly leveraging internal and external attention, we present AC-PKAN, a novel architecture that constitutes an enhancement to weakly supervised Physics-Informed Neural Networks (PINNs) and extends the expressive power of KANs. Experimental results from nine benchmark tasks across three domains show that AC-PKAN consistently outperforms or matches state-of-the-art models such as PINNsFormer, establishing it as a highly effective tool for solving complex real-world engineering problems in zero-data or data-sparse regimes. The code will be made publicly available upon acceptance.","authors":["Hangwei Zhang","Zhimu Huang","Yan Wang"],"url":"https://arxiv.org/abs/2505.08687"}
{"created":"2025-05-14","title":"Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation","abstract":"Event extraction (EE) is a fundamental task in natural language processing (NLP) that involves identifying and extracting event information from unstructured text. Effective EE in real-world scenarios requires two key steps: selecting appropriate schemas from hundreds of candidates and executing the extraction process. Existing research exhibits two critical gaps: (1) the rigid schema fixation in existing pipeline systems, and (2) the absence of benchmarks for evaluating joint schema matching and extraction. Although large language models (LLMs) offer potential solutions, their schema hallucination tendencies and context window limitations pose challenges for practical deployment. In response, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel paradigm combining schema paraphrasing with schema retrieval-augmented generation. ASEE adeptly retrieves paraphrased schemas and accurately generates targeted structures. To facilitate rigorous evaluation, we construct the Multi-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which systematically consolidates 12 datasets across diverse domains, complexity levels, and language settings. Extensive evaluations on MD-SEE show that our proposed ASEE demonstrates strong adaptability across various scenarios, significantly improving the accuracy of event extraction.","authors":["Sheng Liang","Hang Lv","Zhihao Wen","Yaxiong Wu","Yongyue Zhang","Hao Wang","Yong Liu"],"url":"https://arxiv.org/abs/2505.08690"}
{"created":"2025-05-14","title":"VizCV: AI-assisted visualization of researchers' publications tracks","abstract":"Analyzing how the publication records of scientists and research groups have evolved over the years is crucial for assessing their expertise since it can support the management of academic environments by assisting with career planning and evaluation. We introduce VizCV, a novel web-based end-to-end visual analytics framework that enables the interactive exploration of researchers' scientific trajectories. It incorporates AI-assisted analysis and supports automated reporting of career evolution. Our system aims to model career progression through three key dimensions: a) research topic evolution to detect and visualize shifts in scholarly focus over time, b) publication record and the corresponding impact, c) collaboration dynamics depicting the growth and transformation of a researcher's co-authorship network. AI-driven insights provide automated explanations of career transitions, detecting significant shifts in research direction, impact surges, or collaboration expansions. The system also supports comparative analysis between researchers, allowing users to compare topic trajectories and impact growth. Our interactive, multi-tab and multiview system allows for the exploratory analysis of career milestones under different perspectives, such as the most impactful articles, emerging research themes, or obtaining a detailed analysis of the contribution of the researcher in a subfield. The key contributions include AI/ML techniques for: a) topic analysis, b) dimensionality reduction for visualizing patterns and trends, c) the interactive creation of textual descriptions of facets of data through configurable prompt generation and large language models, that include key indicators, to help understanding the career development of individuals or groups.","authors":["Vladim\\'ir Laz\\'arik","Marco Agus","Barbora Kozl\\'ikov\\'a","Pere-Pau V\\'azquez"],"url":"https://arxiv.org/abs/2505.08691"}
{"created":"2025-05-14","title":"SPAST: Arbitrary Style Transfer with Style Priors via Pre-trained Large-scale Model","abstract":"Given an arbitrary content and style image, arbitrary style transfer aims to render a new stylized","authors":["Zhanjie Zhang","Quanwei Zhang","Junsheng Luan","Mengyuan Yang","Yun Wang","Lei Zhao"],"url":"https://arxiv.org/abs/2505.08695"}
{"created":"2025-05-14","title":"LLM-based Prompt Ensemble for Reliable Medical Entity Recognition from EHRs","abstract":"Electronic Health Records (EHRs) are digital records of patient information, often containing unstructured clinical text. Named Entity Recognition (NER) is essential in EHRs for extracting key medical entities like problems, tests, and treatments to support downstream clinical applications. This paper explores prompt-based medical entity recognition using large language models (LLMs), specifically GPT-4o and DeepSeek-R1, guided by various prompt engineering techniques, including zero-shot, few-shot, and an ensemble approach. Among all strategies, GPT-4o with prompt ensemble achieved the highest classification performance with an F1-score of 0.95 and recall of 0.98, outperforming DeepSeek-R1 on the task. The ensemble method improved reliability by aggregating outputs through embedding-based similarity and majority voting.","authors":["K M Sajjadul Islam","Ayesha Siddika Nipu","Jiawei Wu","Praveen Madiraju"],"url":"https://arxiv.org/abs/2505.08704"}
{"created":"2025-05-14","title":"Controllable Image Colorization with Instance-aware Texts and Masks","abstract":"Recently, the application of deep learning in image colorization has received widespread attention. The maturation of diffusion models has further advanced the development of image colorization models. However, current mainstream image colorization models still face issues such as color bleeding and color binding errors, and cannot colorize images at the instance level. In this paper, we propose a diffusion-based colorization method MT-Color to achieve precise instance-aware colorization with use-provided guidance. To tackle color bleeding issue, we design a pixel-level mask attention mechanism that integrates latent features and conditional gray image features through cross-attention. We use segmentation masks to construct cross-attention masks, preventing pixel information from exchanging between different instances. We also introduce an instance mask and text guidance module that extracts instance masks and text representations of each instance, which are then fused with latent features through self-attention, utilizing instance masks to form self-attention masks to prevent instance texts from guiding the colorization of other areas, thus mitigating color binding errors. Furthermore, we apply a multi-instance sampling strategy, which involves sampling each instance region separately and then fusing the results. Additionally, we have created a specialized dataset for instance-level colorization tasks, GPT-color, by leveraging large visual language models on existing image datasets. Qualitative and quantitative experiments show that our model and dataset outperform previous methods and datasets.","authors":["Yanru An","Ling Gui","Qiang Hu","Chunlei Cai","Tianxiao Ye","Xiaoyun Zhang","Yanfeng Wang"],"url":"https://arxiv.org/abs/2505.08705"}
{"created":"2025-05-14","title":"A Reynolds-semi-robust H(div)-conforming method for unsteady incompressible non-Newtonian flows","abstract":"In this work, we prove what appear to be the first Reynolds-semi-robust and pressure-robust velocity error estimates for an H(div)-conforming approximation of unsteady incompressible flows of power-law type fluids. The proposed methods hinges on a discontinuous Galerkin approximation of the viscous term and a reinforced upwind-type stabilization of the convective term. The derived velocity error estimates account for pre-asymptotic orders of convergence observed in convection-dominated flows through regime-dependent estimates of the error contributions. A complete set of numerical results validate the theoretical findings.","authors":["Louren\\c{c}o Beir\\~ao da Veiga","Daniele A. Di Pietro","Kirubell B. Haile"],"url":"https://arxiv.org/abs/2505.08708"}
{"created":"2025-05-14","title":"NavDP: Learning Sim-to-Real Navigation Diffusion Policy with Privileged Information Guidance","abstract":"Learning navigation in dynamic open-world environments is an important yet challenging skill for robots. Most previous methods rely on precise localization and mapping or learn from expensive real-world demonstrations. In this paper, we propose the Navigation Diffusion Policy (NavDP), an end-to-end framework trained solely in simulation and can zero-shot transfer to different embodiments in diverse real-world environments. The key ingredient of NavDP's network is the combination of diffusion-based trajectory generation and a critic function for trajectory selection, which are conditioned on only local observation tokens encoded from a shared policy transformer. Given the privileged information of the global environment in simulation, we scale up the demonstrations of good quality to train the diffusion policy and formulate the critic value function targets with contrastive negative samples. Our demonstration generation approach achieves about 2,500 trajectories/GPU per day, 20$\\times$ more efficient than real-world data collection, and results in a large-scale navigation dataset with 363.2km trajectories across 1244 scenes. Trained with this simulation dataset, NavDP achieves state-of-the-art performance and consistently outstanding generalization capability on quadruped, wheeled, and humanoid robots in diverse indoor and outdoor environments. In addition, we present a preliminary attempt at using Gaussian Splatting to make in-domain real-to-sim fine-tuning to further bridge the sim-to-real gap. Experiments show that adding such real-to-sim data can improve the success rate by 30\\% without hurting its generalization capability.","authors":["Wenzhe Cai","Jiaqi Peng","Yuqiang Yang","Yujian Zhang","Meng Wei","Hanqing Wang","Yilun Chen","Tai Wang","Jiangmiao Pang"],"url":"https://arxiv.org/abs/2505.08712"}
{"created":"2025-05-14","title":"PWC-MoE: Privacy-Aware Wireless Collaborative Mixture of Experts","abstract":"Large language models (LLMs) hosted on cloud servers alleviate the computational and storage burdens on local devices but raise privacy concerns due to sensitive data transmission and require substantial communication bandwidth, which is challenging in constrained environments. In contrast, small language models (SLMs) running locally enhance privacy but suffer from limited performance on complex tasks. To balance computational cost, performance, and privacy protection under bandwidth constraints, we propose a privacy-aware wireless collaborative mixture of experts (PWC-MoE) framework. Specifically, PWC-MoE employs a sparse privacy-aware gating network to dynamically route sensitive tokens to privacy experts located on local clients, while non-sensitive tokens are routed to non-privacy experts located at the remote base station. To achieve computational efficiency, the gating network ensures that each token is dynamically routed to and processed by only one expert. To enhance scalability and prevent overloading of specific experts, we introduce a group-wise load-balancing mechanism for the gating network that evenly distributes sensitive tokens among privacy experts and non-sensitive tokens among non-privacy experts. To adapt to bandwidth constraints while preserving model performance, we propose a bandwidth-adaptive and importance-aware token offloading scheme. This scheme incorporates an importance predictor to evaluate the importance scores of non-sensitive tokens, prioritizing the most important tokens for transmission to the base station based on their predicted importance and the available bandwidth. Experiments demonstrate that the PWC-MoE framework effectively preserves privacy and maintains high performance even in bandwidth-constrained environments, offering a practical solution for deploying LLMs in privacy-sensitive and bandwidth-limited scenarios.","authors":["Yang Su","Na Yan","Yansha Deng","Robert Schober"],"url":"https://arxiv.org/abs/2505.08719"}
{"created":"2025-05-14","title":"TiMo: Spatiotemporal Foundation Model for Satellite Image Time Series","abstract":"Satellite image time series (SITS) provide continuous observations of the Earth's surface, making them essential for applications such as environmental management and disaster assessment. However, existing spatiotemporal foundation models rely on plain vision transformers, which encode entire temporal sequences without explicitly capturing multiscale spatiotemporal relationships between land objects. This limitation hinders their effectiveness in downstream tasks. To overcome this challenge, we propose TiMo, a novel hierarchical vision transformer foundation model tailored for SITS analysis. At its core, we introduce a spatiotemporal gyroscope attention mechanism that dynamically captures evolving multiscale patterns across both time and space. For pre-training, we curate MillionST, a large-scale dataset of one million images from 100,000 geographic locations, each captured across 10 temporal phases over five years, encompassing diverse geospatial changes and seasonal variations. Leveraging this dataset, we adapt masked image modeling to pre-train TiMo, enabling it to effectively learn and encode generalizable spatiotemporal representations.Extensive experiments across multiple spatiotemporal tasks-including deforestation monitoring, land cover segmentation, crop type classification, and flood detection-demonstrate TiMo's superiority over state-of-the-art methods. Code, model, and dataset will be released at https://github.com/MiliLab/TiMo.","authors":["Xiaolei Qin","Di Wang","Jing Zhang","Fengxiang Wang","Xin Su","Bo Du","Liangpei Zhang"],"url":"https://arxiv.org/abs/2505.08723"}
{"created":"2025-05-14","title":"Optimal Trajectory Planning with Collision Avoidance for Autonomous Vehicle Maneuvering","abstract":"To perform autonomous driving maneuvers, such as parallel or perpendicular parking, a vehicle requires continual speed and steering adjustments to follow a generated path. In consequence, the path's quality is a limiting factor of the vehicle maneuver's performance. While most path planning approaches include finding a collision-free route, optimal trajectory planning involves solving the best transition from initial to final states, minimizing the action over all paths permitted by a kinematic model. Here we propose a novel method based on sequential convex optimization, which permits flexible and efficient optimal trajectory generation. The objective is to achieve the fastest time, shortest distance, and fewest number of path segments to satisfy motion requirements, while avoiding sensor blind-spots. In our approach, vehicle kinematics are represented by a discretized Dubins model. To avoid collisions, each waypoint is constrained by linear inequalities representing closest distance of obstacles to a polygon specifying the vehicle's extent. To promote smooth and valid trajectories, the solved kinematic state and control variables are constrained and regularized by penalty terms in the model's cost function, which enforces physical restrictions including limits for steering angle, acceleration and speed. In this paper, we analyze trajectories obtained for several parking scenarios. Results demonstrate efficient and collision-free motion generated by the proposed technique.","authors":["Jason Zalev"],"url":"https://arxiv.org/abs/2505.08724"}
{"created":"2025-05-14","title":"Extending Large Vision-Language Model for Diverse Interactive Tasks in Autonomous Driving","abstract":"The Large Visual-Language Models (LVLMs) have significantly advanced image understanding. Their comprehension and reasoning capabilities enable promising applications in autonomous driving scenarios. However, existing research typically focuses on front-view perspectives and partial objects within scenes, struggling to achieve comprehensive scene understanding. Meanwhile, existing LVLMs suffer from the lack of mapping relationship between 2D and 3D and insufficient integration of 3D object localization and instruction understanding. To tackle these limitations, we first introduce NuInteract, a large-scale dataset with over 1.5M multi-view image language pairs spanning dense scene captions and diverse interactive tasks. Furthermore, we propose DriveMonkey, a simple yet effective framework that seamlessly integrates LVLMs with a spatial processor using a series of learnable queries. The spatial processor, designed as a plug-and-play component, can be initialized with pre-trained 3D detectors to improve 3D perception. Our experiments show that DriveMonkey outperforms general LVLMs, especially achieving a 9.86% notable improvement on the 3D visual grounding task. The dataset and code will be released at https://github.com/zc-zhao/DriveMonkey.","authors":["Zongchuang Zhao","Haoyu Fu","Dingkang Liang","Xin Zhou","Dingyuan Zhang","Hongwei Xie","Bing Wang","Xiang Bai"],"url":"https://arxiv.org/abs/2505.08725"}
{"created":"2025-05-14","title":"Memorization-Compression Cycles Improve Generalization","abstract":"We prove theoretically that generalization improves not only through data scaling but also by compressing internal representations. To operationalize this insight, we introduce the Information Bottleneck Language Modeling (IBLM) objective, which reframes language modeling as a constrained optimization problem: minimizing representation entropy subject to optimal prediction performance. Empirically, we observe an emergent memorization-compression cycle during LLM pretraining, evidenced by oscillation positive/negative gradient alignment between cross-entropy and Matrix-Based Entropy (MBE), a measure of representation entropy. This pattern closely mirrors the predictive-compressive trade-off prescribed by IBLM and also parallels the biological alternation between awake learning and sleep consolidation. Motivated by this observation, we propose Gated Phase Transition (GAPT), a training algorithm that adaptively switches between memorization and compression phases. When applied to GPT-2 pretraining on FineWeb dataset, GAPT reduces MBE by 50% and improves cross-entropy by 4.8%. GAPT improves OOD generalizatino by 35% in a pretraining task on arithmetic multiplication. In a setting designed to simulate catastrophic forgetting, GAPT reduces interference by compressing and separating representations, achieving a 97% improvement in separation - paralleling the functional role of sleep consolidation.","authors":["Fangyuan Yu"],"url":"https://arxiv.org/abs/2505.08727"}
{"created":"2025-05-14","title":"Securing RAG: A Risk Assessment and Mitigation Framework","abstract":"Retrieval Augmented Generation (RAG) has emerged as the de facto industry standard for user-facing NLP applications, offering the ability to integrate data without re-training or fine-tuning Large Language Models (LLMs). This capability enhances the quality and accuracy of responses but also introduces novel security and privacy challenges, particularly when sensitive data is integrated. With the rapid adoption of RAG, securing data and services has become a critical priority. This paper first reviews the vulnerabilities of RAG pipelines, and outlines the attack surface from data pre-processing and data storage management to integration with LLMs. The identified risks are then paired with corresponding mitigations in a structured overview. In a second step, the paper develops a framework that combines RAG-specific security considerations, with existing general security guidelines, industry standards, and best practices. The proposed framework aims to guide the implementation of robust, compliant, secure, and trustworthy RAG systems.","authors":["Lukas Ammann","Sara Ott","Christoph R. Landolt","Marco P. Lehmann"],"url":"https://arxiv.org/abs/2505.08728"}
{"created":"2025-05-14","title":"Load-independent Metrics for Benchmarking Force Controllers","abstract":"Torque-controlled actuators are critical components in mechatronic systems that closely interact with their environment, such as legged robots, collaborative manipulators, and exoskeletons. The performance and stability of these actuators depend not only on controller design and system dynamics but also significantly on load characteristics, which may include interactions with humans or unstructured environments. This load dependence highlights the need for frameworks that properly assess and compare torque controllers independent of specific loading conditions. In this short paper, we concisely present a modeling approach that captures the impact of load on the closed-loop dynamics of torque-controlled systems. Based on this model, we propose new methods and quantitative metrics, including the Passivity Index Interval, which blends passivity and small-gain theory to offer a less conservative measure of coupled stability than passivity alone. These metrics can be used alongside traditional control performance indicators, such as settling time and bandwidth, to provide a more comprehensive characterization of torque-controlled systems. We demonstrate the application of the proposed metrics through experimental comparisons of linear actuator force controllers.","authors":["Victor Shime","Elisa G. Vergamini","C\\'icero Zanette","Leonardo F. dos Santos","Lucca Maitan","Andrea Calanca","Thiago Boaventura"],"url":"https://arxiv.org/abs/2505.08730"}
{"created":"2025-05-14","title":"NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context","abstract":"This work introduces the first benchmark for nursing value alignment, consisting of five core value dimensions distilled from international nursing codes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The benchmark comprises 1,100 real-world nursing behavior instances collected through a five-month longitudinal field study across three hospitals of varying tiers. These instances are annotated by five clinical nurses and then augmented with LLM-generated counterfactuals with reversed ethic polarity. Each original case is paired with a value-aligned and a value-violating version, resulting in 2,200 labeled instances that constitute the Easy-Level dataset. To increase adversarial complexity, each instance is further transformed into a dialogue-based format that embeds contextual cues and subtle misleading signals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA) LLMs on their alignment with nursing values. Our findings reveal three key insights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level dataset (94.55), where Claude 3.5 Sonnet outperforms other models on the Hard-Level dataset (89.43), significantly surpassing the medical LLMs; (2) Justice is consistently the most difficult nursing value dimension to evaluate; and (3) in-context learning significantly improves alignment. This work aims to provide a foundation for value-sensitive LLMs development in clinical settings. The dataset and the code are available at https://huggingface.co/datasets/Ben012345/NurValues.","authors":["Ben Yao","Qiuchi Li","Yazhou Zhang","Siyu Yang","Bohan Zhang","Prayag Tiwari","Jing Qin"],"url":"https://arxiv.org/abs/2505.08734"}
{"created":"2025-05-14","title":"Preference Optimization for Combinatorial Optimization Problems","abstract":"Reinforcement Learning (RL) has emerged as a powerful tool for neural combinatorial optimization, enabling models to learn heuristics that solve complex problems without requiring expert knowledge. Despite significant progress, existing RL approaches face challenges such as diminishing reward signals and inefficient exploration in vast combinatorial action spaces, leading to inefficiency. In this paper, we propose Preference Optimization, a novel method that transforms quantitative reward signals into qualitative preference signals via statistical comparison modeling, emphasizing the superiority among sampled solutions. Methodologically, by reparameterizing the reward function in terms of policy and utilizing preference models, we formulate an entropy-regularized RL objective that aligns the policy directly with preferences while avoiding intractable computations. Furthermore, we integrate local search techniques into the fine-tuning rather than post-processing to generate high-quality preference pairs, helping the policy escape local optima. Empirical results on various benchmarks, such as the Traveling Salesman Problem (TSP), the Capacitated Vehicle Routing Problem (CVRP) and the Flexible Flow Shop Problem (FFSP), demonstrate that our method significantly outperforms existing RL algorithms, achieving superior convergence efficiency and solution quality.","authors":["Mingjun Pan","Guanquan Lin","You-Wei Luo","Bin Zhu","Zhien Dai","Lijun Sun","Chun Yuan"],"url":"https://arxiv.org/abs/2505.08735"}
{"created":"2025-05-14","title":"Towards Foundation Models for Experimental Readout Systems Combining Discrete and Continuous Data","abstract":"We present a (proto) Foundation Model for Nuclear Physics, capable of operating on low-level detector inputs from Imaging Cherenkov Detectors at the future Electron Ion Collider. To address limitations in existing next-token prediction approaches-namely resolution loss from VQ-VAE tokenization and lack of conditional generation-we propose three key innovations: (i) separate vocabularies for discrete spatial features and continuous variates, combined via Causal Multi-Head Cross-Attention (CMHCA), (ii) continuous kinematic conditioning through prepended context embeddings, and (iii) scalable and simple, high-resolution continuous variate tokenization without joint vocabulary inflation. Our model enables fast, high-fidelity generation of pixel and time sequences for Cherenkov photons, validated through closure tests in the High Performance DIRC. We also show our model generalizes to reconstruction tasks such as pion and kaon identification, in which we show its ability to leverage fine-tuning.","authors":["James Giroux","Cristiano Fanelli"],"url":"https://arxiv.org/abs/2505.08736"}
{"created":"2025-05-14","title":"Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies","abstract":"Can autoregressive large language models (LLMs) learn consistent probability distributions when trained on sequences in different token orders? We prove formally that for any well-defined probability distribution, sequence perplexity is invariant under any factorization, including forward, backward, or arbitrary permutations. This result establishes a rigorous theoretical foundation for studying how LLMs learn from data and defines principled protocols for empirical evaluation. Applying these protocols, we show that prior studies examining ordering effects suffer from critical methodological flaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted orders on scientific text. We find systematic deviations from theoretical invariance across all orderings with arbitrary permutations strongly deviating from both forward and backward models, which largely (but not completely) agreed with one another. Deviations were traceable to differences in self-attention, reflecting positional and locality biases in processing. Our theoretical and empirical results provide novel avenues for understanding positional biases in LLMs and suggest methods for detecting when LLMs' probability distributions are inconsistent and therefore untrustworthy.","authors":["Xiaoliang Luo","Xinyi Xu","Michael Ramscar","Bradley C. Love"],"url":"https://arxiv.org/abs/2505.08739"}
{"created":"2025-05-14","title":"Sensitivity-Constrained Fourier Neural Operators for Forward and Inverse Problems in Parametric Differential Equations","abstract":"Parametric differential equations of the form du/dt = f(u, x, t, p) are fundamental in science and engineering. While deep learning frameworks such as the Fourier Neural Operator (FNO) can efficiently approximate solutions, they struggle with inverse problems, sensitivity estimation (du/dp), and concept drift. We address these limitations by introducing a sensitivity-based regularization strategy, called Sensitivity-Constrained Fourier Neural Operators (SC-FNO). SC-FNO achieves high accuracy in predicting solution paths and consistently outperforms standard FNO and FNO with physics-informed regularization. It improves performance in parameter inversion tasks, scales to high-dimensional parameter spaces (tested with up to 82 parameters), and reduces both data and training requirements. These gains are achieved with a modest increase in training time (30% to 130% per epoch) and generalize across various types of differential equations and neural operators. Code and selected experiments are available at: https://github.com/AMBehroozi/SC_Neural_Operators","authors":["Abdolmehdi Behroozi","Chaopeng Shen and","Daniel Kifer"],"url":"https://arxiv.org/abs/2505.08740"}
{"created":"2025-05-14","title":"Understanding Housing and Homelessness System Access by Linking Administrative Data","abstract":"This paper uses privacy preserving methods to link over 235,000 records in the housing and homelessness system of care (HHSC) of a major North American city. Several machine learning pairwise linkage and two clustering algorithms are evaluated for merging the profiles for latent individuals in the data. Importantly, these methods are evaluated using both traditional machine learning metrics and HHSC system use metrics generated using the linked data. The results demonstrate that privacy preserving linkage methods are an effective and practical method for understanding how a single person interacts with multiple agencies across an HHSC. They also show that performance differences between linkage techniques are amplified when evaluated using HHSC domain specific metrics like number of emergency homeless shelter stays, length of time interacting with an HHSC and number of emergency shelters visited per person.","authors":["Geoffrey G. Messier","Sam Elliott","Dallas Seitz"],"url":"https://arxiv.org/abs/2505.08743"}
{"created":"2025-05-14","title":"DeepMath-Creative: A Benchmark for Evaluating Mathematical Creativity of Large Language Models","abstract":"To advance the mathematical proficiency of large language models (LLMs), the DeepMath team has launched an open-source initiative aimed at developing an open mathematical LLM and systematically evaluating its mathematical creativity. This paper represents the initial contribution of this initiative. While recent developments in mathematical LLMs have predominantly emphasized reasoning skills, as evidenced by benchmarks on elementary to undergraduate-level mathematical tasks, the creative capabilities of these models have received comparatively little attention, and evaluation datasets remain scarce. To address this gap, we propose an evaluation criteria for mathematical creativity and introduce DeepMath-Creative, a novel, high-quality benchmark comprising constructive problems across algebra, geometry, analysis, and other domains. We conduct a systematic evaluation of mainstream LLMs' creative problem-solving abilities using this dataset. Experimental results show that even under lenient scoring criteria -- emphasizing core solution components and disregarding minor inaccuracies, such as small logical gaps, incomplete justifications, or redundant explanations -- the best-performing model, O3 Mini, achieves merely 70% accuracy, primarily on basic undergraduate-level constructive tasks. Performance declines sharply on more complex problems, with models failing to provide substantive strategies for open problems. These findings suggest that, although current LLMs display a degree of constructive proficiency on familiar and lower-difficulty problems, such performance is likely attributable to the recombination of memorized patterns rather than authentic creative insight or novel synthesis.","authors":["Xiaoyang Chen","Xinan Dai","Yu Du","Qian Feng","Naixu Guo","Tingshuo Gu","Yuting Gao","Yingyi Gao","Xudong Han","Xiang Jiang","Yilin Jin","Hongyi Lin","Shisheng Lin","Xiangnan Li","Yuante Li","Yixing Li","Zhentao Lai","Zilu Ma","Yingrong Peng","Jiacheng Qian","Hao-Yu Sun","Jianbo Sun","Zirui Wang","Siwei Wu","Zian Wang","Bin Xu","Jianghao Xu","Yiyang Yu","Zichuan Yang","Hongji Zha","Ruichong Zhang"],"url":"https://arxiv.org/abs/2505.08744"}
{"created":"2025-05-14","title":"Advancing Food Nutrition Estimation via Visual-Ingredient Feature Fusion","abstract":"Nutrition estimation is an important component of promoting healthy eating and mitigating diet-related health risks. Despite advances in tasks such as food classification and ingredient recognition, progress in nutrition estimation is limited due to the lack of datasets with nutritional annotations. To address this issue, we introduce FastFood, a dataset with 84,446 images across 908 fast food categories, featuring ingredient and nutritional annotations. In addition, we propose a new model-agnostic Visual-Ingredient Feature Fusion (VIF$^2$) method to enhance nutrition estimation by integrating visual and ingredient features. Ingredient robustness is improved through synonym replacement and resampling strategies during training. The ingredient-aware visual feature fusion module combines ingredient features and visual representation to achieve accurate nutritional prediction. During testing, ingredient predictions are refined using large multimodal models by data augmentation and majority voting. Our experiments on both FastFood and Nutrition5k datasets validate the effectiveness of our proposed method built in different backbones (e.g., Resnet, InceptionV3 and ViT), which demonstrates the importance of ingredient information in nutrition estimation. https://huiyanqi.github.io/fastfood-nutrition-estimation/.","authors":["Huiyan Qi","Bin Zhu","Chong-Wah Ngo","Jingjing Chen","Ee-Peng Lim"],"url":"https://arxiv.org/abs/2505.08747"}
{"created":"2025-05-14","title":"Implet: A Post-hoc Subsequence Explainer for Time Series Models","abstract":"Explainability in time series models is crucial for fostering trust, facilitating debugging, and ensuring interpretability in real-world applications. In this work, we introduce Implet, a novel post-hoc explainer that generates accurate and concise subsequence-level explanations for time series models. Our approach identifies critical temporal segments that significantly contribute to the model's predictions, providing enhanced interpretability beyond traditional feature-attribution methods. Based on it, we propose a cohort-based (group-level) explanation framework designed to further improve the conciseness and interpretability of our explanations. We evaluate Implet on several standard time-series classification benchmarks, demonstrating its effectiveness in improving interpretability. The code is available at https://github.com/LbzSteven/implet","authors":["Fanyu Meng","Ziwen Kan","Shahbaz Rezaei","Zhaodan Kong","Xin Chen","Xin Liu"],"url":"https://arxiv.org/abs/2505.08748"}
{"created":"2025-05-14","title":"AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models","abstract":"Actual causality (AC), a fundamental aspect of causal reasoning (CR), is responsible for attribution and responsibility assignment in real-world scenarios. However, existing LLM-based methods lack grounding in formal AC theory, resulting in limited interpretability. Therefore, we propose AC-Reason, a semi-formal reasoning framework that identifies causally relevant events within an AC scenario, infers the values of their formal causal factors (e.g., sufficiency, necessity, and normality), and answers AC queries via a theory-guided algorithm with explanations. While AC-Reason does not explicitly construct a causal graph, it operates over variables in the underlying causal structure to support principled reasoning. To enable comprehensive evaluation, we introduce AC-Bench, a new benchmark built upon and substantially extending Big-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully annotated samples, each with detailed reasoning steps and focuses solely on actual causation. The case study shows that synthesized samples in AC-Bench present greater challenges for LLMs. Extensive experiments on BBH-CJ and AC-Bench show that AC-Reason consistently improves LLM performance over baselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy of 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 + AC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further enables fine-grained analysis of reasoning faithfulness, revealing that only Qwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful reasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation study proves that integrating AC theory into LLMs is highly effective, with the proposed algorithm contributing the most significant performance gains.","authors":["Yanxi Zhang","Xin Cong","Zhong Zhang","Xiao Liu","Dongyan Zhao","Yesai Wu"],"url":"https://arxiv.org/abs/2505.08750"}
{"created":"2025-05-14","title":"Aya Vision: Advancing the Frontier of Multilingual Multimodality","abstract":"Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.","authors":["Saurabh Dash","Yiyang Nan","John Dang","Arash Ahmadian","Shivalika Singh","Madeline Smith","Bharat Venkitesh","Vlad Shmyhlo","Viraat Aryabumi","Walter Beller-Morales","Jeremy Pekmez","Jason Ozuzu","Pierre Richemond","Acyr Locatelli","Nick Frosst","Phil Blunsom","Aidan Gomez","Ivan Zhang","Marzieh Fadaee","Manoj Govindassamy","Sudip Roy","Matthias Gall\\'e","Beyza Ermis","Ahmet \\\"Ust\\\"un","Sara Hooker"],"url":"https://arxiv.org/abs/2505.08751"}
{"created":"2025-05-14","title":"Towards Autonomous UAV Visual Object Search in City Space: Benchmark and Agentic Methodology","abstract":"Aerial Visual Object Search (AVOS) tasks in urban environments require Unmanned Aerial Vehicles (UAVs) to autonomously search for and identify target objects using visual and textual cues without external guidance. Existing approaches struggle in complex urban environments due to redundant semantic processing, similar object distinction, and the exploration-exploitation dilemma. To bridge this gap and support the AVOS task, we introduce CityAVOS, the first benchmark dataset for autonomous search of common urban objects. This dataset comprises 2,420 tasks across six object categories with varying difficulty levels, enabling comprehensive evaluation of UAV agents' search capabilities. To solve the AVOS tasks, we also propose PRPSearcher (Perception-Reasoning-Planning Searcher), a novel agentic method powered by multi-modal large language models (MLLMs) that mimics human three-tier cognition. Specifically, PRPSearcher constructs three specialized maps: an object-centric dynamic semantic map enhancing spatial perception, a 3D cognitive map based on semantic attraction values for target reasoning, and a 3D uncertainty map for balanced exploration-exploitation search. Also, our approach incorporates a denoising mechanism to mitigate interference from similar objects and utilizes an Inspiration Promote Thought (IPT) prompting mechanism for adaptive action planning. Experimental results on CityAVOS demonstrate that PRPSearcher surpasses existing baselines in both success rate and search efficiency (on average: +37.69% SR, +28.96% SPL, -30.69% MSS, and -46.40% NE). While promising, the performance gap compared to humans highlights the need for better semantic reasoning and spatial exploration capabilities in AVOS tasks. This work establishes a foundation for future advances in embodied target search. Dataset and source code are available at https://anonymous.4open.science/r/CityAVOS-3DF8.","authors":["Yatai Ji","Zhengqiu Zhu","Yong Zhao","Beidan Liu","Chen Gao","Yihao Zhao","Sihang Qiu","Yue Hu","Quanjun Yin","Yong Li"],"url":"https://arxiv.org/abs/2505.08765"}
{"created":"2025-05-14","title":"SPAT: Sensitivity-based Multihead-attention Pruning on Time Series Forecasting Models","abstract":"Attention-based architectures have achieved superior performance in multivariate time series forecasting but are computationally expensive. Techniques such as patching and adaptive masking have been developed to reduce their sizes and latencies. In this work, we propose a structured pruning method, SPAT ($\\textbf{S}$ensitivity $\\textbf{P}$runer for $\\textbf{At}$tention), which selectively removes redundant attention mechanisms and yields highly effective models. Different from previous approaches, SPAT aims to remove the entire attention module, which reduces the risk of overfitting and enables speed-up without demanding specialized hardware. We propose a dynamic sensitivity metric, $\\textbf{S}$ensitivity $\\textbf{E}$nhanced $\\textbf{N}$ormalized $\\textbf{D}$ispersion (SEND) that measures the importance of each attention module during the pre-training phase. Experiments on multivariate datasets demonstrate that SPAT-pruned models achieve reductions of 2.842% in MSE, 1.996% in MAE, and 35.274% in FLOPs. Furthermore, SPAT-pruned models outperform existing lightweight, Mamba-based and LLM-based SOTA methods in both standard and zero-shot inference, highlighting the importance of retaining only the most effective attention mechanisms. We have made our code publicly available https://anonymous.4open.science/r/SPAT-6042.","authors":["Suhan Guo","Jiahong Deng","Mengjun Yi","Furao Shen","Jian Zhao"],"url":"https://arxiv.org/abs/2505.08768"}
{"created":"2025-05-14","title":"Kudzu: Fast and Simple High-Throughput BFT","abstract":"We present Kudzu, a high-throughput atomic broadcast protocol with an integrated fast path. Our contribution is based on the combination of two lines of work. Firstly, our protocol achieves finality in just two rounds of communication if all but $p$ out of $n = 3f + 2p + 1$ participating replicas behave correctly, where $f$ is the number of Byzantine faults that are tolerated. Due to the seamless integration of the fast path, even in the presence of more than $p$ faults, our protocol maintains state-of-the-art characteristics. Secondly, our protocol utilizes the bandwidth of participating replicas in a balanced way, alleviating the bottleneck at the leader, and thus enabling high throughput. This is achieved by disseminating blocks using erasure codes. Despite combining a novel set of advantages, Kudzu is remarkably simple: intricacies such as progress certificates, complex view changes, and speculative execution are avoided.","authors":["Victor Shoup","Jakub Sliwinski","Yann Vonlanthen"],"url":"https://arxiv.org/abs/2505.08771"}
{"created":"2025-05-14","title":"Blockchain Technology: Core Mechanisms, Evolution, and Future Implementation Challenges","abstract":"Blockchain technology has emerged as one of the most transformative digital innovations of the 21st century. This paper presents a comprehensive review of blockchain's fundamental architecture, tracing its development from Bitcoin's initial implementation to current enterprise applications. We examine the core technical components including distributed consensus algorithms, cryptographic principles, and smart contract functionality that enable blockchain's unique properties. The historical progression from cryptocurrency-focused systems to robust platforms for decentralized applications is analyzed, highlighting pivotal developments in scalability, privacy, and interoperability. Additionally, we identify critical challenges facing widespread blockchain adoption, including technical limitations, regulatory hurdles, and integration complexities with existing systems. By providing this foundational understanding of blockchain technology, this paper contributes to ongoing research efforts addressing blockchain's potential to revolutionize data management across industries.","authors":["Aditya Pratap Singh"],"url":"https://arxiv.org/abs/2505.08772"}
{"created":"2025-05-14","title":"HealthBench: Evaluating Large Language Models Towards Improved Human Health","abstract":"We present HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare. HealthBench consists of 5,000 multi-turn conversations between a model and an individual user or healthcare professional. Responses are evaluated using conversation-specific rubrics created by 262 physicians. Unlike previous multiple-choice or short-answer benchmarks, HealthBench enables realistic, open-ended evaluation through 48,562 unique rubric criteria spanning several health contexts (e.g., emergencies, transforming clinical data, global health) and behavioral dimensions (e.g., accuracy, instruction following, communication). HealthBench performance over the last two years reflects steady initial progress (compare GPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3 scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms GPT-4o and is 25 times cheaper. We additionally release two HealthBench variations: HealthBench Consensus, which includes 34 particularly important dimensions of model behavior validated via physician consensus, and HealthBench Hard, where the current top score is 32%. We hope that HealthBench grounds progress towards model development and applications that benefit human health.","authors":["Rahul K. Arora","Jason Wei","Rebecca Soskin Hicks","Preston Bowman","Joaquin Qui\\~nonero-Candela","Foivos Tsimpourlas","Michael Sharman","Meghan Shah","Andrea Vallone","Alex Beutel","Johannes Heidecke","Karan Singhal"],"url":"https://arxiv.org/abs/2505.08775"}
{"created":"2025-05-14","title":"ARC-NCA: Towards Developmental Solutions to the Abstraction and Reasoning Corpus","abstract":"The Abstraction and Reasoning Corpus (ARC), later renamed ARC-AGI, poses a fundamental challenge in artificial general intelligence (AGI), requiring solutions that exhibit robust abstraction and reasoning capabilities across diverse tasks, while only few (with median count of three) correct examples are presented. While ARC-AGI remains very challenging for artificial intelligence systems, it is rather easy for humans. This paper introduces ARC-NCA, a developmental approach leveraging standard Neural Cellular Automata (NCA) and NCA enhanced with hidden memories (EngramNCA) to tackle the ARC-AGI benchmark. NCAs are employed for their inherent ability to simulate complex dynamics and emergent patterns, mimicking developmental processes observed in biological systems. Developmental solutions may offer a promising avenue for enhancing AI's problem-solving capabilities beyond mere training data extrapolation. ARC-NCA demonstrates how integrating developmental principles into computational models can foster adaptive reasoning and abstraction. We show that our ARC-NCA proof-of-concept results may be comparable to, and sometimes surpass, that of ChatGPT 4.5, at a fraction of the cost.","authors":["Etienne Guichard","Felix Reimers","Mia Kvalsund","Mikkel Lepper{\\o}d","Stefano Nichele"],"url":"https://arxiv.org/abs/2505.08778"}
{"created":"2025-05-14","title":"Addressing the Current Challenges of Quantum Machine Learning through Multi-Chip Ensembles","abstract":"Quantum Machine Learning (QML) holds significant promise for solving computational challenges across diverse domains. However, its practical deployment is constrained by the limitations of noisy intermediate-scale quantum (NISQ) devices, including noise, limited scalability, and trainability issues in variational quantum circuits (VQCs). We introduce the multi-chip ensemble VQC framework, which partitions high-dimensional computations across smaller quantum chips to enhance scalability, trainability, and noise resilience. We show that this approach mitigates barren plateaus, reduces quantum error bias and variance, and maintains robust generalization through controlled entanglement. Designed to align with current and emerging quantum hardware, the framework demonstrates strong potential for enabling scalable QML on near-term devices, as validated by experiments on standard benchmark datasets (MNIST, FashionMNIST, CIFAR-10) and real world dataset (PhysioNet EEG).","authors":["Junghoon Justin Park","Jiook Cha","Samuel Yen-Chi Chen","Huan-Hsin Tseng","Shinjae Yoo"],"url":"https://arxiv.org/abs/2505.08782"}
{"created":"2025-05-14","title":"CodePDE: An Inference Framework for LLM-driven PDE Solver Generation","abstract":"Partial differential equations (PDEs) are fundamental to modeling physical systems, yet solving them remains a complex challenge. Traditional numerical solvers rely on expert knowledge to implement and are computationally expensive, while neural-network-based solvers require large training datasets and often lack interpretability. In this work, we frame PDE solving as a code generation task and introduce CodePDE, the first inference framework for generating PDE solvers using large language models (LLMs). Leveraging advanced inference-time algorithms and scaling strategies, CodePDE unlocks critical capacities of LLM for PDE solving: reasoning, debugging, selfrefinement, and test-time scaling -- all without task-specific tuning. CodePDE achieves superhuman performance across a range of representative PDE problems. We also present a systematic empirical analysis of LLM generated solvers, analyzing their accuracy, efficiency, and numerical scheme choices. Our findings highlight the promise and the current limitations of LLMs in PDE solving, offering a new perspective on solver design and opportunities for future model development. Our code is available at https://github.com/LithiumDA/CodePDE.","authors":["Shanda Li","Tanya Marwah","Junhong Shen","Weiwei Sun","Andrej Risteski","Yiming Yang","Ameet Talwalkar"],"url":"https://arxiv.org/abs/2505.08783"}
{"created":"2025-05-14","title":"UniSkill: Imitating Human Videos via Cross-Embodiment Skill Representations","abstract":"Mimicry is a fundamental learning mechanism in humans, enabling individuals to learn new tasks by observing and imitating experts. However, applying this ability to robots presents significant challenges due to the inherent differences between human and robot embodiments in both their visual appearance and physical capabilities. While previous methods bridge this gap using cross-embodiment datasets with shared scenes and tasks, collecting such aligned data between humans and robots at scale is not trivial. In this paper, we propose UniSkill, a novel framework that learns embodiment-agnostic skill representations from large-scale cross-embodiment video data without any labels, enabling skills extracted from human video prompts to effectively transfer to robot policies trained only on robot data. Our experiments in both simulation and real-world environments show that our cross-embodiment skills successfully guide robots in selecting appropriate actions, even with unseen video prompts. The project website can be found at: https://kimhanjung.github.io/UniSkill.","authors":["Hanjung Kim","Jaehyun Kang","Hyolim Kang","Meedeum Cho","Seon Joo Kim","Youngwoon Lee"],"url":"https://arxiv.org/abs/2505.08787"}
{"created":"2025-05-14","title":"A Value of Information-based assessment of strain-based thickness loss monitoring in ship hull structures","abstract":"Recent advances in Structural Health Monitoring (SHM) have attracted industry interest, yet real-world applications, such as in ship structures remain scarce. Despite SHM's potential to optimise maintenance, its adoption in ships is limited due to the lack of clearly quantifiable benefits for hull maintenance. This study employs a Bayesian pre-posterior decision analysis to quantify the value of information (VoI) from SHM systems monitoring corrosion-induced thickness loss (CITL) in ship hulls, in a first-of-its-kind analysis for ship structures. We define decision-making consequence cost functions based on exceedance probabilities relative to a target CITL threshold, which can be set by the decision-maker. This introduces a practical aspect to our framework, that enables implicitly modelling the decision-maker's risk perception. We apply this framework to a large-scale, high-fidelity numerical model of a commercial vessel and examine the relative benefits of different CITL monitoring strategies, including strain-based SHM and traditional on-site inspections.","authors":["Nicholas E. Silionis","Konstantinos N. Anyfantis"],"url":"https://arxiv.org/abs/2505.07427"}
{"created":"2025-05-14","title":"Linear to Neural Networks Regression: QSPR of Drugs via Degree-Distance Indices","abstract":"This study conducts a Quantitative Structure Property Relationship (QSPR) analysis to explore the correlation between the physical properties of drug molecules and their topological indices using machine learning techniques. While prior studies in drug design have focused on degree-based topological indices, this work analyzes a dataset of 166 drug molecules by computing degree-distance-based topological indices, incorporating vertex-edge weightings with respect to different six atomic properties (atomic number, atomic radius, atomic mass, density, electronegativity, ionization). Both linear models (Linear Regression, Lasso, and Ridge Regression) and nonlinear approaches (Random Forest, XGBoost, and Neural Networks) were employed to predict molecular properties. The results demonstrate the effectiveness of these indices in predicting specific physicochemical properties and underscore the practical relevance of computational methods in molecular property estimation. The study provides an innovative perspective on integrating topological indices with machine learning to enhance predictive accuracy, highlighting their potential application in drug discovery and development processes. This predictive may also explain that establishing a reliable relationship between topological indices and physical properties enables chemists to gain preliminary insights into molecular behavior before conducting experimental analyses, thereby optimizing resource utilization in cheminformatics research.","authors":["M. J. Nadjafi Arani","S. Sorgun","M. Mirzargar"],"url":"https://arxiv.org/abs/2505.07821"}
{"created":"2025-05-14","title":"Diffusion-based supervised learning of generative models for efficient sampling of multimodal distributions","abstract":"We propose a hybrid generative model for efficient sampling of high-dimensional, multimodal probability distributions for Bayesian inference. Traditional Monte Carlo methods, such as the Metropolis-Hastings and Langevin Monte Carlo sampling methods, are effective for sampling from single-mode distributions in high-dimensional spaces. However, these methods struggle to produce samples with the correct proportions for each mode in multimodal distributions, especially for distributions with well separated modes. To address the challenges posed by multimodality, we adopt a divide-and-conquer strategy. We start by minimizing the energy function with initial guesses uniformly distributed within the prior domain to identify all the modes of the energy function. Then, we train a classifier to segment the domain corresponding to each mode. After the domain decomposition, we train a diffusion-model-assisted generative model for each identified mode within its support. Once each mode is characterized, we employ bridge sampling to estimate the normalizing constant, allowing us to directly adjust the ratios between the modes. Our numerical examples demonstrate that the proposed framework can effectively handle multimodal distributions with varying mode shapes in up to 100 dimensions. An application to Bayesian inverse problem for partial differential equations is also provided.","authors":["Hoang Tran","Zezhong Zhang","Feng Bao","Dan Lu","Guannan Zhang"],"url":"https://arxiv.org/abs/2505.07825"}
{"created":"2025-05-14","title":"Sub-diffraction terahertz backpropagation compressive imaging","abstract":"Terahertz single-pixel imaging (TSPI) has garnered significant attention due to its simplicity and cost-effectiveness. However, the relatively long wavelength of THz waves limits sub-diffraction-scale imaging resolution. Although TSPI technique can achieve sub-wavelength resolution, it requires harsh experimental conditions and time-consuming processes. Here, we propose a sub-diffraction THz backpropagation compressive imaging technique. We illuminate the object with monochromatic continuous-wave THz radiation. The transmitted THz wave is modulated by prearranged patterns generated on the back surface of a 500-{\\mu}m-thick silicon wafer, realized through photoexcited carriers using a 532-nm laser. The modulated THz wave is then recorded by a single-element detector. An untrained neural network is employed to iteratively reconstruct the object image with an ultralow compression ratio of 1.5625% under a physical model constraint, thus reducing the long sampling times. To further suppress the diffraction-field effects, embedded with the angular spectrum propagation (ASP) theory to model the diffraction of THz waves during propagation, the network retrieves near-field information from the object, enabling sub-diffraction imaging with a spatial resolution of ~{\\lambda}0/7 ({\\lambda}0 = 833.3 {\\mu}m at 0.36 THz) and eliminating the need for ultrathin photomodulators. This approach provides an efficient solution for advancing THz microscopic imaging and addressing other inverse imaging challenges.","authors":["Yongsheng Zhu","Shaojing Liu","Ximiao Wang","Runli Li","Haili Yang","Jiali Wang","Hongjia Zhu","Yanlin Ke","Ningsheng Xu","Huanjun Chen","Shaozhi Deng"],"url":"https://arxiv.org/abs/2505.07839"}
{"created":"2025-05-14","title":"Evaluation of UAV-Based RGB and Multispectral Vegetation Indices for Precision Agriculture in Palm Tree Cultivation","abstract":"Precision farming relies on accurate vegetation monitoring to enhance crop productivity and promote sustainable agricultural practices. This study presents a comprehensive evaluation of UAV-based imaging for vegetation health assessment in a palm tree cultivation region in Dubai. By comparing multispectral and RGB image data, we demonstrate that RGBbased vegetation indices offer performance comparable to more expensive multispectral indices, providing a cost-effective alternative for large-scale agricultural monitoring. Using UAVs equipped with multispectral sensors, indices such as NDVI and SAVI were computed to categorize vegetation into healthy, moderate, and stressed conditions. Simultaneously, RGB-based indices like VARI and MGRVI delivered similar results in vegetation classification and stress detection. Our findings highlight the practical benefits of integrating RGB imagery into precision farming, reducing operational costs while maintaining accuracy in plant health monitoring. This research underscores the potential of UAVbased RGB imaging as a powerful tool for precision agriculture, enabling broader adoption of data-driven decision-making in crop management. By leveraging the strengths of both multispectral and RGB imaging, this work advances the state of UAV applications in agriculture, paving the way for more efficient and scalable farming solutions.","authors":["Alavikunhu Panthakkan","S M Anzar","K. Sherin","Saeed Al Mansoori","Hussain Al-Ahmad"],"url":"https://arxiv.org/abs/2505.07840"}
{"created":"2025-05-14","title":"Pose Estimation for Intra-cardiac Echocardiography Catheter via AI-Based Anatomical Understanding","abstract":"Intra-cardiac Echocardiography (ICE) plays a crucial role in Electrophysiology (EP) and Structural Heart Disease (SHD) interventions by providing high-resolution, real-time imaging of cardiac structures. However, existing navigation methods rely on electromagnetic (EM) tracking, which is susceptible to interference and position drift, or require manual adjustments based on operator expertise. To overcome these limitations, we propose a novel anatomy-aware pose estimation system that determines the ICE catheter position and orientation solely from ICE images, eliminating the need for external tracking sensors. Our approach leverages a Vision Transformer (ViT)-based deep learning model, which captures spatial relationships between ICE images and anatomical structures. The model is trained on a clinically acquired dataset of 851 subjects, including ICE images paired with position and orientation labels normalized to the left atrium (LA) mesh. ICE images are patchified into 16x16 embeddings and processed through a transformer network, where a [CLS] token independently predicts position and orientation via separate linear layers. The model is optimized using a Mean Squared Error (MSE) loss function, balancing positional and orientational accuracy. Experimental results demonstrate an average positional error of 9.48 mm and orientation errors of (16.13 deg, 8.98 deg, 10.47 deg) across x, y, and z axes, confirming the model accuracy. Qualitative assessments further validate alignment between predicted and target views within 3D cardiac meshes. This AI-driven system enhances procedural efficiency, reduces operator workload, and enables real-time ICE catheter localization for tracking-free procedures. The proposed method can function independently or complement existing mapping systems like CARTO, offering a transformative approach to ICE-guided interventions.","authors":["Jaeyoung Huh","Ankur Kapoor","Young-Ho Kim"],"url":"https://arxiv.org/abs/2505.07851"}
{"created":"2025-05-14","title":"CellVerse: Do Large Language Models Really Understand Cell Biology?","abstract":"Recent studies have demonstrated the feasibility of modeling single-cell data as natural languages and the potential of leveraging powerful large language models (LLMs) for understanding cell biology. However, a comprehensive evaluation of LLMs' performance on language-driven single-cell analysis tasks still remains unexplored. Motivated by this challenge, we introduce CellVerse, a unified language-centric question-answering benchmark that integrates four types of single-cell multi-omics data and encompasses three hierarchical levels of single-cell analysis tasks: cell type annotation (cell-level), drug response prediction (drug-level), and perturbation analysis (gene-level). Going beyond this, we systematically evaluate the performance across 14 open-source and closed-source LLMs ranging from 160M to 671B on CellVerse. Remarkably, the experimental results reveal: (1) Existing specialist models (C2S-Pythia) fail to make reasonable decisions across all sub-tasks within CellVerse, while generalist models such as Qwen, Llama, GPT, and DeepSeek family models exhibit preliminary understanding capabilities within the realm of cell biology. (2) The performance of current LLMs falls short of expectations and has substantial room for improvement. Notably, in the widely studied drug response prediction task, none of the evaluated LLMs demonstrate significant performance improvement over random guessing. CellVerse offers the first large-scale empirical demonstration that significant challenges still remain in applying LLMs to cell biology. By introducing CellVerse, we lay the foundation for advancing cell biology through natural languages and hope this paradigm could facilitate next-generation single-cell analysis.","authors":["Fan Zhang","Tianyu Liu","Zhihong Zhu","Hao Wu","Haixin Wang","Donghao Zhou","Yefeng Zheng","Kun Wang","Xian Wu","Pheng-Ann Heng"],"url":"https://arxiv.org/abs/2505.07865"}
{"created":"2025-05-14","title":"Computationally Efficient Diffusion Models in Medical Imaging: A Comprehensive Review","abstract":"The diffusion model has recently emerged as a potent approach in computer vision, demonstrating remarkable performances in the field of generative artificial intelligence. Capable of producing high-quality synthetic images, diffusion models have been successfully applied across a range of applications. However, a significant challenge remains with the high computational cost associated with training and generating these models. This study focuses on the efficiency and inference time of diffusion-based generative models, highlighting their applications in both natural and medical imaging. We present the most recent advances in diffusion models by categorizing them into three key models: the Denoising Diffusion Probabilistic Model (DDPM), the Latent Diffusion Model (LDM), and the Wavelet Diffusion Model (WDM). These models play a crucial role in medical imaging, where producing fast, reliable, and high-quality medical images is essential for accurate analysis of abnormalities and disease diagnosis. We first investigate the general framework of DDPM, LDM, and WDM and discuss the computational complexity gap filled by these models in natural and medical imaging. We then discuss the current limitations of these models as well as the opportunities and future research directions in medical imaging.","authors":["Abdullah","Tao Huang","Ickjai Lee","Euijoon Ahn"],"url":"https://arxiv.org/abs/2505.07866"}
{"created":"2025-05-14","title":"Bridging Large Language Models and Single-Cell Transcriptomics in Dissecting Selective Motor Neuron Vulnerability","abstract":"Understanding cell identity and function through single-cell level sequencing data remains a key challenge in computational biology. We present a novel framework that leverages gene-specific textual annotations from the NCBI Gene database to generate biologically contextualized cell embeddings. For each cell in a single-cell RNA sequencing (scRNA-seq) dataset, we rank genes by expression level, retrieve their NCBI Gene descriptions, and transform these descriptions into vector embedding representations using large language models (LLMs). The models used include OpenAI text-embedding-ada-002, text-embedding-3-small, and text-embedding-3-large (Jan 2024), as well as domain-specific models BioBERT and SciBERT. Embeddings are computed via an expression-weighted average across the top N most highly expressed genes in each cell, providing a compact, semantically rich representation. This multimodal strategy bridges structured biological data with state-of-the-art language modeling, enabling more interpretable downstream applications such as cell-type clustering, cell vulnerability dissection, and trajectory inference.","authors":["Douglas Jiang","Zilin Dai","Luxuan Zhang","Qiyi Yu","Haoqi Sun","Feng Tian"],"url":"https://arxiv.org/abs/2505.07896"}
{"created":"2025-05-14","title":"Image-Guided Microstructure Optimization using Diffusion Models: Validated with Li-Mn-rich Cathode Precursors","abstract":"Microstructure often dictates materials performance, yet it is rarely treated as an explicit design variable because microstructure is hard to quantify, predict, and optimize. Here, we introduce an image centric, closed-loop framework that makes microstructural morphology into a controllable objective and demonstrate its use case with Li- and Mn-rich layered oxide cathode precursors. This work presents an integrated, AI driven framework for the predictive design and optimization of lithium-ion battery cathode precursor synthesis. This framework integrates a diffusion-based image generation model, a quantitative image analysis pipeline, and a particle swarm optimization (PSO) algorithm. By extracting key morphological descriptors such as texture, sphericity, and median particle size (D50) from SEM images, the platform accurately predicts SEM like morphologies resulting from specific coprecipitation conditions, including reaction time-, solution concentration-, and pH-dependent structural changes. Optimization then pinpoints synthesis parameters that yield user defined target morphologies, as experimentally validated by the close agreement between predicted and synthesized structures. This framework offers a practical strategy for data driven materials design, enabling both forward prediction and inverse design of synthesis conditions and paving the way toward autonomous, image guided microstructure engineering.","authors":["Geunho Choi","Changhwan Lee","Jieun Kim","Insoo Ye","Keeyoung Jung","Inchul Park"],"url":"https://arxiv.org/abs/2505.07906"}
{"created":"2025-05-14","title":"MiniMax-Speech: Intrinsic Zero-Shot Text-to-Speech with a Learnable Speaker Encoder","abstract":"We introduce MiniMax-Speech, an autoregressive Transformer-based Text-to-Speech (TTS) model that generates high-quality speech. A key innovation is our learnable speaker encoder, which extracts timbre features from a reference audio without requiring its transcription. This enables MiniMax-Speech to produce highly expressive speech with timbre consistent with the reference in a zero-shot manner, while also supporting one-shot voice cloning with exceptionally high similarity to the reference voice. In addition, the overall quality of the synthesized audio is enhanced through the proposed Flow-VAE. Our model supports 32 languages and demonstrates excellent performance across multiple objective and subjective evaluations metrics. Notably, it achieves state-of-the-art (SOTA) results on objective voice cloning metrics (Word Error Rate and Speaker Similarity) and has secured the top position on the public TTS Arena leaderboard. Another key strength of MiniMax-Speech, granted by the robust and disentangled representations from the speaker encoder, is its extensibility without modifying the base model, enabling various applications such as: arbitrary voice emotion control via LoRA; text to voice (T2V) by synthesizing timbre features directly from text description; and professional voice cloning (PVC) by fine-tuning timbre features with additional data. We encourage readers to visit https://minimax-ai.github.io/tts_tech_report for more examples.","authors":["Bowen Zhang","Congchao Guo","Geng Yang","Hang Yu","Haozhe Zhang","Heidi Lei","Jialong Mai","Junjie Yan","Kaiyue Yang","Mingqi Yang","Peikai Huang","Ruiyang Jin","Sitan Jiang","Weihua Cheng","Yawei Li","Yichen Xiao","Yiying Zhou","Yongmao Zhang","Yuan Lu","Yucen He"],"url":"https://arxiv.org/abs/2505.07916"}
{"created":"2025-05-14","title":"Digitalizing Wick's theorem","abstract":"Wick's theorem is a cornerstone of perturbative quantum field theory. In this paper we announce and discuss the digitalization of Wick's theorem and its proof into the interactive theorem prover Lean 4 as part of the project PhysLean. We do the same for the static and normal-ordered versions of Wick's theorem.","authors":["Joseph Tooby-Smith"],"url":"https://arxiv.org/abs/2505.07939"}
{"created":"2025-05-14","title":"Wasserstein Distributionally Robust Nonparametric Regression","abstract":"Distributionally robust optimization has become a powerful tool for prediction and decision-making under model uncertainty. By focusing on the local worst-case risk, it enhances robustness by identifying the most unfavorable distribution within a predefined ambiguity set. While extensive research has been conducted in parametric settings, studies on nonparametric frameworks remain limited. This paper studies the generalization properties of Wasserstein distributionally robust nonparametric estimators, with particular attention to the impact of model misspecification, where non-negligible discrepancies between the estimation function space and target function can impair generalization performance. We establish non-asymptotic error bounds for the excess local worst-case risk by analyzing the regularization effects induced by distributional perturbations and employing feedforward neural networks with Lipschitz constraints. These bounds illustrate how uncertainty levels and neural network structures influence generalization performance and are applicable to both Lipschitz and quadratic loss functions. Furthermore, we investigate the Lagrangian relaxation of the local worst-case risk and derive corresponding non-asymptotic error bounds for these estimators. The robustness of the proposed estimator is evaluated through simulation studies and illustrated with an application to the MNIST dataset.","authors":["Changyu Liu","Yuling Jiao","Junhui Wang","Jian Huang"],"url":"https://arxiv.org/abs/2505.07967"}
{"created":"2025-05-14","title":"Probabilistic approach to longitudinal response prediction: application to radiomics from brain cancer imaging","abstract":"Longitudinal imaging analysis tracks disease progression and treatment response over time, providing dynamic insights into treatment efficacy and disease evolution. Radiomic features extracted from medical imaging can support the study of disease progression and facilitate longitudinal prediction of clinical outcomes. This study presents a probabilistic model for longitudinal response prediction, integrating baseline features with intermediate follow-ups. The probabilistic nature of the model naturally allows to handle the instrinsic uncertainty of the longitudinal prediction of disease progression. We evaluate the proposed model against state-of-the-art disease progression models in both a synthetic scenario and using a brain cancer dataset. Results demonstrate that the approach is competitive against existing methods while uniquely accounting for uncertainty and controlling the growth of problem dimensionality, eliminating the need for data from intermediate follow-ups.","authors":["Isabella Cama","Michele Piana","Cristina Campi","Sara Garbarino"],"url":"https://arxiv.org/abs/2505.07973"}
{"created":"2025-05-14","title":"QUEST: QUantum-Enhanced Shared Transportation","abstract":"We introduce ``Windbreaking-as-a-Service'' (WaaS) as an innovative approach to shared transportation in which larger ``windbreaker'' vehicles provide aerodynamic shelter for ``windsurfer'' vehicles, thereby reducing drag and fuel consumption. As a computational framework to solve the large-scale matching and assignment problems that arise in WaaS, we present \\textbf{QUEST} (Quantum-Enhanced Shared Transportation). Specifically, we formulate the pairing of windbreakers and windsurfers -- subject to timing, speed, and vehicle-class constraints -- as a mixed-integer quadratic problem (MIQP). Focusing on a single-segment prototype, we verify the solution classically via the Hungarian Algorithm, a Gurobi-based solver, and brute-force enumeration of binary vectors. We then encode the problem as a Quadratic Unconstrained Binary Optimization (QUBO) and map it to an Ising Hamiltonian, enabling the use of the Quantum Approximate Optimization Algorithm (QAOA) and other quantum and classical annealing technologies. Our quantum implementation successfully recovers the optimal assignment identified by the classical methods, confirming the soundness of the QUEST pipeline for a controlled prototype. While QAOA and other quantum heuristics do not guarantee a resolution of the fundamental complexity barriers, this study illustrates how the WaaS problem can be systematically translated into a quantum-ready model. It also lays the groundwork for addressing multi-segment scenarios and potentially leveraging quantum advantage for large-scale shared-transportation instances.","authors":["Chinonso Onah","Neel Miscasci","Carsten Othmer","Kristel Michielsen"],"url":"https://arxiv.org/abs/2505.08074"}
{"created":"2025-05-14","title":"Zak-OTFS for Spread Spectrum Communication","abstract":"An attractive feature of spread spectrum technologies such as code division multiple access (CDMA) is that it is harder to intercept or jam signals, and this feature was lost when orthogonal frequency domain modulation prevailed over CDMA in wireless standards. Legacy spread carrier waveforms are not matched to delay and Doppler shifts characteristic of 6G wireless environments, and this makes equalization very challenging. Zak-OTFS modulation is a communication framework that parameterizes the wireless channel in the delay-Doppler (DD) domain, where the parameters map directly to physical attributes of the scatterers that comprise the scattering environment. Hence, the channel can be efficiently acquired and equalized. The Zak-OTFS carrier is a pulse in the DD domain, and the Zak transform converts it to a pulse train modulated by a tone (pulsone) in the time domain. The pulsone waveform is localized rather than spread, and it suffers from high PAPR. We describe how to transform Zak-OTFS into a spread spectrum communication system, where the spread carrier waveforms have low PAPR and are matched to the delay and Doppler characteristics of the wireless channel. This transformation is realized by a unitary transform that is a generalization of the discrete affine Fourier transform. The transform maps a pulsone to a time domain waveform which yields a CAZAC sequence after sampling. The family of CAZAC sequences includes the Zadoff-Chu sequences incorporated in LTE and 5G-NR standards. We describe the end-to-end time-domain transceiver signal processing, comprising channel estimation and data demodulation, for the proposed system. We quantify system performance through BER simulations using a six-path Veh-A channel model, showing that the proposed system achieves similar uncoded BER as pulsone-based Zak-OTFS, where the PAPR of each spread carrier waveform is only 3.58 dB.","authors":["Nishant Mehrotra","Sandesh Rao Mattu","Robert Calderbank"],"url":"https://arxiv.org/abs/2505.08079"}
{"created":"2025-05-14","title":"Sharp Gaussian approximations for Decentralized Federated Learning","abstract":"Federated Learning has gained traction in privacy-sensitive collaborative environments, with local SGD emerging as a key optimization method in decentralized settings. While its convergence properties are well-studied, asymptotic statistical guarantees beyond convergence remain limited. In this paper, we present two generalized Gaussian approximation results for local SGD and explore their implications. First, we prove a Berry-Esseen theorem for the final local SGD iterates, enabling valid multiplier bootstrap procedures. Second, motivated by robustness considerations, we introduce two distinct time-uniform Gaussian approximations for the entire trajectory of local SGD. The time-uniform approximations support Gaussian bootstrap-based tests for detecting adversarial attacks. Extensive simulations are provided to support our theoretical results.","authors":["Soham Bonnerjee","Sayar Karmakar","Wei Biao Wu"],"url":"https://arxiv.org/abs/2505.08125"}
{"created":"2025-05-14","title":"Beyond Basic A/B testing: Improving Statistical Efficiency for Business Growth","abstract":"The standard A/B testing approaches are mostly based on t-test in large scale industry applications. These standard approaches however suffers from low statistical power in business settings, due to nature of small sample-size or non-Gaussian distribution or return-on-investment (ROI) consideration. In this paper, we propose several approaches to addresses these challenges: (i) regression adjustment, generalized estimating equation, Man-Whitney U and Zero-Trimmed U that addresses each of these issues separately, and (ii) a novel doubly robust generalized U that handles ROI consideration, distribution robustness and small samples in one framework. We provide theoretical results on asymptotic normality and efficiency bounds, together with insights on the efficiency gain from theoretical analysis. We further conduct comprehensive simulation studies and apply the methods to multiple real A/B tests.","authors":["Changshuai Wei","Phuc Nguyen","Benjamin Zelditch","Joyce Chen"],"url":"https://arxiv.org/abs/2505.08128"}
{"created":"2025-05-14","title":"Fault Detection Method for Power Conversion Circuits Using Thermal Image and Convolutional Autoencoder","abstract":"A fault detection method for power conversion circuits using thermal images and a convolutional autoencoder is presented. The autoencoder is trained on thermal images captured from a commercial power module at randomly varied load currents and augmented image2 generated through image processing techniques such as resizing, rotation, perspective transformation, and bright and contrast adjustment. Since the autoencoder is trained to output images identical to input only for normal samples, it reconstructs images similar to normal ones even when the input images containing faults. A small heater is attached to the circuit board to simulate a fault on a power module, and then thermal images were captured from different angles and positions, as well as various load currents to test the trained autoencoder model. The areas under the curve (AUC) were obtained to evaluate the proposed method. The results show the autoencoder model can detect anomalies with 100% accuracy under given conditions. The influence of hyperparameters such as the number of convolutional layers and image augmentation conditions on anomaly detection accuracy was also investigated.","authors":["Noboru Katayama","Rintaro Ishida"],"url":"https://arxiv.org/abs/2505.08150"}
{"created":"2025-05-14","title":"Enhancing the Efficiency of Complex Systems Crystal Structure Prediction by Active Learning Guided Machine Learning Potential","abstract":"Understanding multicomponent complex material systems is essential for design of advanced materials for a wide range of technological applications. While state-of-the-art crystal structure prediction (CSP) methods effectively identify new structures and assess phase stability, they face fundamental limitations when applied to complex systems. This challenge stems from the combinatorial explosion of atomic configurations and the vast stoichiometric space, both of which contribute to computational demands that rapidly exceed practical feasibility. In this work, we propose a flexible and automated workflow to build a highly generalizable and data-efficient machine learning potential (MLP), effectively unlocking the full potential of CSP algorithms. The workflow is validated on both Mg-Ca-H ternary and Be-P-N-O quaternary systems, demonstrating substantial machine learning acceleration in high-throughput structural optimization and enabling the efficient identification of promising compounds. These results underscore the effectiveness of our approach in exploring complex material systems and accelerating the discovery of new multicomponent materials.","authors":["Jiaxiang Li","Junwei Feng","Jie Luo","Bowen Jiang","Xiangyu Zheng","Jian Lv","Keith Butler","Hanyu Liu","Congwei Xie","Yu Xie","Yanming Ma"],"url":"https://arxiv.org/abs/2505.08159"}
{"created":"2025-05-14","title":"Aitomia: Your Intelligent Assistant for AI-Driven Atomistic and Quantum Chemical Simulations","abstract":"We have developed Aitomia - a platform powered by AI to assist in performing AI-driven atomistic and quantum chemical (QC) simulations. This intelligent assistant platform is equipped with chatbots and AI agents to help experts and guide non-experts in setting up and running the atomistic simulations, monitoring their computation status, analyzing the simulation results, and summarizing them for the user in text and graphical forms. We achieve these goals by exploiting fine-tuned open-source large language models (LLMs), rule-based agents, and a retrieval-augmented generation (RAG) system. Aitomia leverages the versatility of our MLatom ecosystem for AI-enhanced computational chemistry. This intelligent assistant is going to be integrated into the Aitomistic Hub and XACS online computing services, with some functionality already publicly available as described at http://mlatom.com/aitomia. Aitomia is expected to lower the barrier to performing atomistic simulations, accelerating research and development in the relevant fields.","authors":["Jinming Hu","Hassan Nawaz","Yuting Rui","Lijie Chi","Arif Ullah","Pavlo O. Dral"],"url":"https://arxiv.org/abs/2505.08195"}
{"created":"2025-05-14","title":"SIM-Shapley: A Stable and Computationally Efficient Approach to Shapley Value Approximation","abstract":"Explainable artificial intelligence (XAI) is essential for trustworthy machine learning (ML), particularly in high-stakes domains such as healthcare and finance. Shapley value (SV) methods provide a principled framework for feature attribution in complex models but incur high computational costs, limiting their scalability in high-dimensional settings. We propose Stochastic Iterative Momentum for Shapley Value Approximation (SIM-Shapley), a stable and efficient SV approximation method inspired by stochastic optimization. We analyze variance theoretically, prove linear $Q$-convergence, and demonstrate improved empirical stability and low bias in practice on real-world datasets. In our numerical experiments, SIM-Shapley reduces computation time by up to 85% relative to state-of-the-art baselines while maintaining comparable feature attribution quality. Beyond feature attribution, our stochastic mini-batch iterative framework extends naturally to a broader class of sample average approximation problems, offering a new avenue for improving computational efficiency with stability guarantees. Code is publicly available at https://github.com/nliulab/SIM-Shapley.","authors":["Wangxuan Fan","Siqi Li","Doudou Zhou","Yohei Okada","Chuan Hong","Molei Liu","Nan Liu"],"url":"https://arxiv.org/abs/2505.08198"}
{"created":"2025-05-14","title":"Lie Group Symmetry Discovery and Enforcement Using Vector Fields","abstract":"Symmetry-informed machine learning can exhibit advantages over machine learning which fails to account for symmetry. Additionally, recent attention has been given to continuous symmetry discovery using vector fields which serve as infinitesimal generators for Lie group symmetries. In this paper, we extend the notion of non-affine symmetry discovery to functions defined by neural networks. We further extend work in this area by introducing symmetry enforcement of smooth models using vector fields. Finally, we extend work on symmetry discovery using vector fields by providing both theoretical and experimental material on the restriction of the symmetry search space to infinitesimal isometries.","authors":["Ben Shaw","Sasidhar Kunapuli","Abram Magner","Kevin R. Moon"],"url":"https://arxiv.org/abs/2505.08219"}
{"created":"2025-05-14","title":"Skeleton-Guided Diffusion Model for Accurate Foot X-ray Synthesis in Hallux Valgus Diagnosis","abstract":"Medical image synthesis plays a crucial role in providing anatomically accurate images for diagnosis and treatment. Hallux valgus, which affects approximately 19% of the global population, requires frequent weight-bearing X-rays for assessment, placing additional strain on both patients and healthcare providers. Existing X-ray models often struggle to balance image fidelity, skeletal consistency, and physical constraints, particularly in diffusion-based methods that lack skeletal guidance. We propose the Skeletal-Constrained Conditional Diffusion Model (SCCDM) and introduce KCC, a foot evaluation method utilizing skeletal landmarks. SCCDM incorporates multi-scale feature extraction and attention mechanisms, improving the Structural Similarity Index (SSIM) by 5.72% (0.794) and Peak Signal-to-Noise Ratio (PSNR) by 18.34% (21.40 dB). When combined with KCC, the model achieves an average score of 0.85, demonstrating strong clinical applicability. The code is available at https://github.com/midisec/SCCDM.","authors":["Midi Wan","Pengfei Li","Yizhuo Liang","Di Wu","Yushan Pan","Guangzhen Zhu","Hao Wang"],"url":"https://arxiv.org/abs/2505.08247"}
{"created":"2025-05-14","title":"Iteratively reweighted kernel machines efficiently learn sparse functions","abstract":"The impressive practical performance of neural networks is often attributed to their ability to learn low-dimensional data representations and hierarchical structure directly from data. In this work, we argue that these two phenomena are not unique to neural networks, and can be elicited from classical kernel methods. Namely, we show that the derivative of the kernel predictor can detect the influential coordinates with low sample complexity. Moreover, by iteratively using the derivatives to reweight the data and retrain kernel machines, one is able to efficiently learn hierarchical polynomials with finite leap complexity. Numerical experiments illustrate the developed theory.","authors":["Libin Zhu","Damek Davis","Dmitriy Drusvyatskiy","Maryam Fazel"],"url":"https://arxiv.org/abs/2505.08277"}
{"created":"2025-05-14","title":"Nonlinear optical response in kagome lattice with inversion symmetry breaking","abstract":"The kagome lattice is a fundamental model structure in condensed matter physics and materials science featuring symmetry-protected flat bands, saddle points, and Dirac points. This structure has emerged as an ideal platform for exploring various quantum physics. By combining effective model analysis and first-principles calculations, we propose that the synergy among inversion symmetry breaking, flat bands, and saddle point-related van Hove singularities within the kagome lattice holds significant potential for generating strong second-order nonlinear optical response. This property provides an inspiring insight into the practical application of the kagome-like materials, which is helpful for a comprehensive understanding of kagome lattice-related physics. Moreover, this work offers an alternative approach for designing materials with strong a second-order nonlinear optical response.","authors":["Xiangyang Liu","Junwen Lai","Jie Zhan","Tianye Yu","Peitao Liu","Seiji Yunoki","Xing-Qiu Chen","Yan Sun"],"url":"https://arxiv.org/abs/2505.08289"}
{"created":"2025-05-14","title":"Closed-Form Information Capacity of Canonical Signaling Models","abstract":"We employ a unified framework for computing the information capacity of biological signaling systems using Fisher Information. By deriving closed-form or easily computable information capacity formulas, we quantify how well different signaling models, including binomial, multinomial, Poisson, Gaussian, and Gamma distributions, can discriminate among input signals. These expressions clarify how key features such as signal range, noise scaling, pathway length, and receivers' diversity shape the theoretical limits of sensing. In particular, we show how signal-to-noise ratio and fold-change sensitivity arise naturally within the Fisher formalism, and how signal degradation in cascades imposes linear information loss. Our results provide intuitive, analytically grounded tools to benchmark and guide the analysis of real signaling systems, without requiring computationally expensive mutual information estimation. While motivated by cellular communication, the framework generalizes to any system where noisy input-output relationships constrain transmission fidelity, including synthetic biology, sensor networks, and engineered communication channels.","authors":["Micha{\\l} Komorowski"],"url":"https://arxiv.org/abs/2505.08365"}
{"created":"2025-05-14","title":"Non-contact Vital Signs Detection in Dynamic Environments","abstract":"Accurate phase demodulation is critical for vital sign detection using millimeter-wave radar. However, in complex environments, time-varying DC offsets and phase imbalances can severely degrade demodulation performance. To address this, we propose a novel DC offset calibration method alongside a Hilbert and Differential Cross-Multiply (HADCM) demodulation algorithm. The approach estimates time-varying DC offsets from neighboring signal peaks and valleys, then employs both differential forms and Hilbert transforms of the I/Q channel signals to extract vital sign information. Simulation and experimental results demonstrate that the proposed method maintains robust performance under low signal-to-noise ratios. Compared to existing demodulation techniques, it offers more accurate signal recovery in challenging scenarios and effectively suppresses noise interference.","authors":["Shuai Sun","Chong-Xi Liang","Chengwei Ye","Huanzhen Zhang","Kangsheng Wang"],"url":"https://arxiv.org/abs/2505.08366"}
{"created":"2025-05-14","title":"Distributionally Robust LQG with Kullback-Leibler Ambiguity Sets","abstract":"The Linear Quadratic Gaussian (LQG) controller is known to be inherently fragile to model misspecifications often occurring in real-world situations. We consider discretetime partially observable stochastic linear systems and provide a robustification of the standard LQG against distributional uncertainties on the process and measurement noise. Our distributionally robust formulation specifies the admissible perturbations by defining a relative entropy based ambiguity set individually for each time step along a finite-horizon trajectory, and minimizes the worst-case cost across all admissible distributions. Notably, we prove that the optimal control policy is still linear, as in standard LQG, and we derive a computational scheme grounded on iterative best response that provably converges to the set of saddle points. Finally, we consider the case of endogenous uncertainty captured via decision-dependent ambiguity sets and we propose an approximation scheme based on dynamic programming.","authors":["Marta Fochesato","Lucia Falconi","Mattia Zorzi","Augusto Ferrante","John Lygeros"],"url":"https://arxiv.org/abs/2505.08370"}
{"created":"2025-05-14","title":"Learning Treatment Allocations with Risk Control Under Partial Identifiability","abstract":"Learning beneficial treatment allocations for a patient population is an important problem in precision medicine. Many treatments come with adverse side effects that are not commensurable with their potential benefits. Patients who do not receive benefits after such treatments are thereby subjected to unnecessary harm. This is a `treatment risk' that we aim to control when learning beneficial allocations. The constrained learning problem is challenged by the fact that the treatment risk is not in general identifiable using either randomized trial or observational data. We propose a certifiable learning method that controls the treatment risk with finite samples in the partially identified setting. The method is illustrated using both simulated and real data.","authors":["Sofia Ek","Dave Zachariah"],"url":"https://arxiv.org/abs/2505.08378"}
{"created":"2025-05-14","title":"A Fourier finite volume approach for the optical inverse problem of quantitative photoacoustic tomography","abstract":"A new approach for solving the optical inverse problem of quantitative photoacoustic tomography is introduced, which interpolates between the well-known diffusion approximation and a radiative transfer equation based model. The proposed formulation combines a spatial finite volume scheme with a truncated Fourier expansion in the direction variable for the radiative transfer equation. The finite volume scheme provides a natural and simple approach for representing piecewise constant image data modelled using transport equations. The truncated Fourier expansion in the direction variable facilitates the interpolation between the diffusion approximation at low order, and the full radiative transfer model as the truncation limit $N\\rightarrow\\infty$. It is therefore possible to tune the precision of the model to the demands of the imaging application, taking $N=1$ for cases when the diffusion approximation would suffice and increasing the number of terms otherwise. We will then utilise the non-linear optimisation functionality of Matlab to address the corresponding large-scale nonlinear inverse problem using gradient based quasi-Newton minimisation via the limited memory Broyden-Fletcher-Goldfarb-Shanno algorithm. Numerical experiments for two test-cases of increasing complexity and resolution will be presented, and the effect of logarithmically rescaling the problem data on the accuracy of the reconstructed solutions will be investigated. We will focus on cases where the diffusion approximation is not sufficient to demonstrate that our approach can provide significant accuracy gains with only a modest increase in the number of Fourier terms included.","authors":["David J. Chappell"],"url":"https://arxiv.org/abs/2505.08400"}
{"created":"2025-05-14","title":"On the Average Secrecy Performance of Satellite Networks in Short Packet Communication Systems","abstract":"This paper investigates the secrecy performance of satellite networks in short packet communication systems under shadowed Rician fading (SRF). We derive a lower bound on the average achievable secrecy rate in the finite blocklength regime (FBL) and provide analytical insights into the impact of key secrecy-related performance indicators (KPIs). Monte Carlo simulations validate the theoretical framework, and demonstrate that increasing the blocklength and improving the legitimate receiver's signal-to-noise ratio (SNR) enhance secrecy, while a stronger eavesdropper degrades it. Additionally, we show that directional antenna patterns can effectively reduce information leakage and provide secure satellite communications in the short packet regime. These findings offer valuable guidance for designing secure and efficient satellite-based communication systems, particularly in IoT and space-based networks.","authors":["Ramin Hashemi","Graciela Corral Briones","Risto Wichman"],"url":"https://arxiv.org/abs/2505.08407"}
{"created":"2025-05-14","title":"Understanding molecular ratios in the carbon and oxygen poor outer Milky Way with interpretable machine learning","abstract":"Context. The outer Milky Way has a lower metallicity than our solar neighbourhood, but still many molecules are detected in the region. Molecular line ratios can serve as probes to better understand the chemistry and physics in these regions. Aims. We use interpretable machine learning to study 9 different molecular ratios, helping us understand the forward connection between the physics of these environments and the carbon and oxygen chemistries. Methods. Using a large grid of astrochemical models generated using UCLCHEM, we study the properties of molecular clouds of low oxygen and carbon initial abundance. We first try to understand the line ratios using a classical analysis. We then move on to using interpretable machine learning, namely Shapley Additive Explanations (SHAP), to understand the higher order dependencies of the ratios over the entire parameter grid. Lastly we use the Uniform Manifold Approximation and Projection technique (UMAP) as a reduction method to create intuitive groupings of models. Results. We find that the parameter space is well covered by the line ratios, allowing us to investigate all input parameters. SHAP analysis shows that the temperature and density are the most important features, but the carbon and oxygen abundances are important in parts of the parameter space. Lastly, we find that we can group different types of ratios using UMAP. Conclusions. We show the chosen ratios are mostly sensitive to changes in the carbon initial abundance, together with the temperature and density. Especially the CN/HCN and HNC/HCN ratio are shown to be sensitive to the initial carbon abundance, making them excellent probes for this parameter. Out of the ratios, only CS/SO shows a sensitivity to the oxygen abundance.","authors":["Gijs Vermari\\\"en","Serena Viti","Johannes Heyl","Francesco Fontani"],"url":"https://arxiv.org/abs/2505.08410"}
{"created":"2025-05-14","title":"An integrated language-vision foundation model for conversational diagnostics and triaging in primary eye care","abstract":"Current deep learning models are mostly task specific and lack a user-friendly interface to operate. We present Meta-EyeFM, a multi-function foundation model that integrates a large language model (LLM) with vision foundation models (VFMs) for ocular disease assessment. Meta-EyeFM leverages a routing mechanism to enable accurate task-specific analysis based on text queries. Using Low Rank Adaptation, we fine-tuned our VFMs to detect ocular and systemic diseases, differentiate ocular disease severity, and identify common ocular signs. The model achieved 100% accuracy in routing fundus images to appropriate VFMs, which achieved $\\ge$ 82.2% accuracy in disease detection, $\\ge$ 89% in severity differentiation, $\\ge$ 76% in sign identification. Meta-EyeFM was 11% to 43% more accurate than Gemini-1.5-flash and ChatGPT-4o LMMs in detecting various eye diseases and comparable to an ophthalmologist. This system offers enhanced usability and diagnostic performance, making it a valuable decision support tool for primary eye care or an online LLM for fundus evaluation.","authors":["Zhi Da Soh","Yang Bai","Kai Yu","Yang Zhou","Xiaofeng Lei","Sahil Thakur","Zann Lee","Lee Ching Linette Phang","Qingsheng Peng","Can Can Xue","Rachel Shujuan Chong","Quan V. Hoang","Lavanya Raghavan","Yih Chung Tham","Charumathi Sabanayagam","Wei-Chi Wu","Ming-Chih Ho","Jiangnan He","Preeti Gupta","Ecosse Lamoureux","Seang Mei Saw","Vinay Nangia","Songhomitra Panda-Jonas","Jie Xu","Ya Xing Wang","Xinxing Xu","Jost B. Jonas","Tien Yin Wong","Rick Siow Mong Goh","Yong Liu","Ching-Yu Cheng"],"url":"https://arxiv.org/abs/2505.08414"}
{"created":"2025-05-14","title":"GNCAF: A GNN-based Neighboring Context Aggregation Framework for Tertiary Lymphoid Structures Semantic Segmentation in WSI","abstract":"Tertiary lymphoid structures (TLS) are organized clusters of immune cells, whose maturity and area can be quantified in whole slide image (WSI) for various prognostic tasks. Existing methods for assessing these characteristics typically rely on cell proxy tasks and require additional post-processing steps. In this work, We focus on a novel task-TLS Semantic Segmentation (TLS-SS)-which segments both the regions and maturation stages of TLS in WSI in an end-to-end manner. Due to the extensive scale of WSI and patch-based segmentation strategies, TLS-SS necessitates integrating from neighboring patches to guide target patch (target) segmentation. Previous techniques often employ on multi-resolution approaches, constraining the capacity to leverage the broader neighboring context while tend to preserve coarse-grained information. To address this, we propose a GNN-based Neighboring Context Aggregation Framework (GNCAF), which progressively aggregates multi-hop neighboring context from the target and employs a self-attention mechanism to guide the segmentation of the target. GNCAF can be integrated with various segmentation models to enhance their ability to perceive contextual information outside of the patch. We build two TLS-SS datasets, called TCGA-COAD and INHOUSE-PAAD, and make the former (comprising 225 WSIs and 5041 TLSs) publicly available. Experiments on these datasets demonstrate the superiority of GNCAF, achieving a maximum of 22.08% and 26.57% improvement in mF1 and mIoU, respectively. Additionally, we also validate the task scalability of GNCAF on segmentation of lymph node metastases.","authors":["Lei Su"],"url":"https://arxiv.org/abs/2505.08430"}
{"created":"2025-05-14","title":"Distributed Quantum Neural Networks on Distributed Photonic Quantum Computing","abstract":"We introduce a distributed quantum-classical framework that synergizes photonic quantum neural networks (QNNs) with matrix-product-state (MPS) mapping to achieve parameter-efficient training of classical neural networks. By leveraging universal linear-optical decompositions of $M$-mode interferometers and photon-counting measurement statistics, our architecture generates neural parameters through a hybrid quantum-classical workflow: photonic QNNs with $M(M+1)/2$ trainable parameters produce high-dimensional probability distributions that are mapped to classical network weights via an MPS model with bond dimension $\\chi$. Empirical validation on MNIST classification demonstrates that photonic QT achieves an accuracy of $95.50\\% \\pm 0.84\\%$ using 3,292 parameters ($\\chi = 10$), compared to $96.89\\% \\pm 0.31\\%$ for classical baselines with 6,690 parameters. Moreover, a ten-fold compression ratio is achieved at $\\chi = 4$, with a relative accuracy loss of less than $3\\%$. The framework outperforms classical compression techniques (weight sharing/pruning) by 6--12\\% absolute accuracy while eliminating quantum hardware requirements during inference through classical deployment of compressed parameters. Simulations incorporating realistic photonic noise demonstrate the framework's robustness to near-term hardware imperfections. Ablation studies confirm quantum necessity: replacing photonic QNNs with random inputs collapses accuracy to chance level ($10.0\\% \\pm 0.5\\%$). Photonic quantum computing's room-temperature operation, inherent scalability through spatial-mode multiplexing, and HPC-integrated architecture establish a practical pathway for distributed quantum machine learning, combining the expressivity of photonic Hilbert spaces with the deployability of classical neural networks.","authors":["Kuan-Cheng Chen","Chen-Yu Liu","Yu Shang","Felix Burt","Kin K. Leung"],"url":"https://arxiv.org/abs/2505.08474"}
{"created":"2025-05-14","title":"On lattice tilings of $\\mathbb{Z}^n$ by limited magnitude error balls $\\mathcal{B}(n,2,k_{1},k_{2})$ with $k_1>k_2$","abstract":"Lattice tilings of $\\mathbb{Z}^n$ by limited-magnitude error balls correspond to linear perfect codes under such error models and play a crucial role in flash memory applications. In this work, we establish three main results. First, we fully determine the existence of lattice tilings by $\\mathcal{B}(n,2,3,0)$ in all dimensions $n$. Second, we completely resolve the case $k_1=k_2+1$. Finally, we prove that for any integers $k_1>k_2\\ge0$ where $k_1+k_2+1$ is composite, no lattice tiling of $\\mathbb{Z}^n$ by the error ball $\\mathcal{B}(n,2,k_1,k_2)$ exists for sufficiently large $n$.","authors":["Ka Hin Leung","Ran Tao","Daohua Wang","Tao Zhang"],"url":"https://arxiv.org/abs/2505.08495"}
{"created":"2025-05-14","title":"SPP-SBL: Space-Power Prior Sparse Bayesian Learning for Block Sparse Recovery","abstract":"The recovery of block-sparse signals with unknown structural patterns remains a fundamental challenge in structured sparse signal reconstruction. By proposing a variance transformation framework, this paper unifies existing pattern-based block sparse Bayesian learning methods, and introduces a novel space power prior based on undirected graph models to adaptively capture the unknown patterns of block-sparse signals. By combining the EM algorithm with high-order equation root-solving, we develop a new structured sparse Bayesian learning method, SPP-SBL, which effectively addresses the open problem of space coupling parameter estimation in pattern-based methods. We further demonstrate that learning the relative values of space coupling parameters is key to capturing unknown block-sparse patterns and improving recovery accuracy. Experiments validate that SPP-SBL successfully recovers various challenging structured sparse signals (e.g., chain-structured signals and multi-pattern sparse signals) and real-world multi-modal structured sparse signals (images, audio), showing significant advantages in recovery accuracy across multiple metrics.","authors":["Yanhao Zhang","Zhihan Zhu","Yong Xia"],"url":"https://arxiv.org/abs/2505.08518"}
{"created":"2025-05-14","title":"Building-Block Aware Generative Modeling for 3D Crystals of Metal Organic Frameworks","abstract":"Metal-organic frameworks (MOFs) marry inorganic nodes, organic edges, and topological nets into programmable porous crystals, yet their astronomical design space defies brute-force synthesis. Generative modeling holds ultimate promise, but existing models either recycle known building blocks or are restricted to small unit cells. We introduce Building-Block-Aware MOF Diffusion (BBA MOF Diffusion), an SE(3)-equivariant diffusion model that learns 3D all-atom representations of individual building blocks, encoding crystallographic topological nets explicitly. Trained on the CoRE-MOF database, BBA MOF Diffusion readily samples MOFs with unit cells containing 1000 atoms with great geometric validity, novelty, and diversity mirroring experimental databases. Its native building-block representation produces unprecedented metal nodes and organic edges, expanding accessible chemical space by orders of magnitude. One high-scoring [Zn(1,4-TDC)(EtOH)2] MOF predicted by the model was synthesized, where powder X-ray diffraction, thermogravimetric analysis, and N2 sorption confirm its structural fidelity. BBA-Diff thus furnishes a practical pathway to synthesizable and high-performing MOFs.","authors":["Chenru Duan","Aditya Nandy","Sizhan Liu","Yuanqi Du","Liu He","Yi Qu","Haojun Jia","Jin-Hu Dou"],"url":"https://arxiv.org/abs/2505.08531"}
{"created":"2025-05-14","title":"Synthesis of safety certificates for discrete-time uncertain systems via convex optimization","abstract":"We study the problem of co-designing control barrier functions and linear state feedback controllers for discrete-time linear systems affected by additive disturbances. For disturbances of bounded magnitude, we provide a semi-definite program whose feasibility implies the existence of a control law and a certificate ensuring safety in the infinite horizon with respect to the worst-case disturbance realization in the uncertainty set. For disturbances with unbounded support, we rely on martingale theory to derive a second semi-definite program whose feasibility provides probabilistic safety guarantees holding joint-in-time over a finite time horizon. We examine several extensions, including (i) encoding of different types of input constraints, (ii) robustification against distributional ambiguity around the true distribution, (iii) design of safety filters, and (iv) extension to general safety specifications such as obstacle avoidance.","authors":["Marta Fochesato","Han Wang","Antonis Papachristodoulou","Paul Goulart"],"url":"https://arxiv.org/abs/2505.08559"}
{"created":"2025-05-14","title":"Communication-Efficient Distributed Online Nonconvex Optimization with Time-Varying Constraints","abstract":"This paper considers distributed online nonconvex optimization with time-varying inequality constraints over a network of agents, where the nonconvex local loss and convex local constraint functions can vary arbitrarily across iterations, and the information of them is privately revealed to each agent at each iteration. For a uniformly jointly strongly connected time-varying directed graph, we propose two distributed bandit online primal--dual algorithm with compressed communication to efficiently utilize communication resources in the one-point and two-point bandit feedback settings, respectively. In nonconvex optimization, finding a globally optimal decision is often NP-hard. As a result, the standard regret metric used in online convex optimization becomes inapplicable. To measure the performance of the proposed algorithms, we use a network regret metric grounded in the first-order optimality condition associated with the variational inequality. We show that the compressed algorithms establish sublinear network regret and cumulative constraint violation bounds. Finally, a simulation example is presented to validate the theoretical results.","authors":["Kunpeng Zhang","Lei Xu","Xinlei Yi","Guanghui Wen","Ming Cao","Karl H. Johansson","Tianyou Chai","Tao Yang"],"url":"https://arxiv.org/abs/2505.08592"}
{"created":"2025-05-14","title":"Automated Model-Free Sorting of Single-Molecule Fluorescence Events Using a Deep Learning Based Hidden-State Model","abstract":"Single-molecule fluorescence assays enable high-resolution analysis of biomolecular dynamics, but traditional analysis pipelines are labor-intensive and rely on users' experience, limiting scalability and reproducibility. Recent deep learning models have automated aspects of data processing, yet many still require manual thresholds, complex architectures, or extensive labeled data. Therefore, we present DASH, a fully streamlined architecture for trace classification, state assignment, and automatic sorting that requires no user input. DASH demonstrates robust performance across users and experimental conditions both in equilibrium and non-equilibrium systems such as Cas12a-mediated DNA cleavage. This paper proposes a novel strategy for the automatic and detailed sorting of single-molecule fluorescence events. The dynamic cleavage process of Cas12a is used as an example to provide a comprehensive analysis. This approach is crucial for studying biokinetic structural changes at the single-molecule level.","authors":["Wenqi Zeng","Shuqi Zhou","Yuan Yao","Chunlai Chen"],"url":"https://arxiv.org/abs/2505.08608"}
{"created":"2025-05-14","title":"neuralGAM: An R Package for Fitting Generalized Additive Neural Networks","abstract":"Nowadays, Neural Networks are considered one of the most effective methods for various tasks such as anomaly detection, computer-aided disease detection, or natural language processing. However, these networks suffer from the ``black-box'' problem which makes it difficult to understand how they make decisions. In order to solve this issue, an R package called neuralGAM is introduced. This package implements a Neural Network topology based on Generalized Additive Models, allowing to fit an independent Neural Network to estimate the contribution of each feature to the output variable, yielding a highly accurate and interpretable Deep Learning model. The neuralGAM package provides a flexible framework for training Generalized Additive Neural Networks, which does not impose any restrictions on the Neural Network architecture. We illustrate the use of the neuralGAM package in both synthetic and real data examples.","authors":["Ines Ortega-Fernandez","Marta Sestelo"],"url":"https://arxiv.org/abs/2505.08610"}
{"created":"2025-05-14","title":"A portable diagnosis model for Keratoconus using a smartphone","abstract":"Keratoconus (KC) is a progressive corneal disorder characterized by localized thinning and protrusion, leading to visual distortion. While Placido disc-based topography remains a standard in clinical diagnostics, its dependence on specialized equipment limits accessibility. In this paper, we propose a portable, smartphone-based diagnostic framework that captures corneal reflections of a Placido disc displayed on a phone screen and applies a two-stage detection pipeline, then validate on 3D-printed emulated eyeball models that simulate normal, moderate, and severe KC stages based on anterior chamber depth (ACD). The first step of the two-stage detection pipeline is classifying different stages of KC with features including height and width of extracted reflections using weighted support vector machine (WSVM). It achieves a maximum accuracy of 92.93%, and maintains over 90% accuracy across multiple smartphone models, including the Galaxy Z Flip 3, iPhone 15 Pro, and iPhone 16 Pro. For the second step, we visualize the KC-affected protrusion regions on the corneas with color maps based on inter-disc distance, that provides an intuitive representation of disease severity and localization. Moreover, we validate the ability of the extracted features to differentiate between KC stages with ANOVA and Omega Squared, with significant p-values (e.g., $p < 10^{-6}$) and large effect sizes ($\\\\omega^2$ up to 0.8398) among classes.","authors":["Yifan Li","Myeongjun Kim","Yanjing Jin","Peter Ho","Jo Woon Chong"],"url":"https://arxiv.org/abs/2505.08616"}
{"created":"2025-05-14","title":"Uncertainty-Aware Surrogate-based Amortized Bayesian Inference for Computationally Expensive Models","abstract":"Bayesian inference typically relies on a large number of model evaluations to estimate posterior distributions. Established methods like Markov Chain Monte Carlo (MCMC) and Amortized Bayesian Inference (ABI) can become computationally challenging. While ABI enables fast inference after training, generating sufficient training data still requires thousands of model simulations, which is infeasible for expensive models. Surrogate models offer a solution by providing approximate simulations at a lower computational cost, allowing the generation of large data sets for training. However, the introduced approximation errors and uncertainties can lead to overconfident posterior estimates. To address this, we propose Uncertainty-Aware Surrogate-based Amortized Bayesian Inference (UA-SABI) - a framework that combines surrogate modeling and ABI while explicitly quantifying and propagating surrogate uncertainties through the inference pipeline. Our experiments show that this approach enables reliable, fast, and repeated Bayesian inference for computationally expensive models, even under tight time constraints.","authors":["Stefania Scheurer","Philipp Reiser","Tim Br\\\"unnette","Wolfgang Nowak","Anneli Guthke","Paul-Christian B\\\"urkner"],"url":"https://arxiv.org/abs/2505.08683"}
{"created":"2025-05-14","title":"VIViT: Variable-Input Vision Transformer Framework for 3D MR Image Segmentation","abstract":"Self-supervised pretrain techniques have been widely used to improve the downstream tasks' performance. However, real-world magnetic resonance (MR) studies usually consist of different sets of contrasts due to different acquisition protocols, which poses challenges for the current deep learning methods on large-scale pretrain and different downstream tasks with different input requirements, since these methods typically require a fixed set of input modalities or, contrasts. To address this challenge, we propose variable-input ViT (VIViT), a transformer-based framework designed for self-supervised pretraining and segmentation finetuning for variable contrasts in each study. With this ability, our approach can maximize the data availability in pretrain, and can transfer the learned knowledge from pretrain to downstream tasks despite variations in input requirements. We validate our method on brain infarct and brain tumor segmentation, where our method outperforms current CNN and ViT-based models with a mean Dice score of 0.624 and 0.883 respectively. These results highlight the efficacy of our design for better adaptability and performance on tasks with real-world heterogeneous MR data.","authors":["Badhan Kumar Das","Ajay Singh","Gengyan Zhao","Han Liu","Thomas J. Re","Dorin Comaniciu","Eli Gibson","Andreas Maier"],"url":"https://arxiv.org/abs/2505.08693"}
{"created":"2025-05-14","title":"A Survey of Deep Learning for Complex Speech Spectrograms","abstract":"Recent advancements in deep learning have significantly impacted the field of speech signal processing, particularly in the analysis and manipulation of complex spectrograms. This survey provides a comprehensive overview of the state-of-the-art techniques leveraging deep neural networks for processing complex spectrograms, which encapsulate both magnitude and phase information. We begin by introducing complex spectrograms and their associated features for various speech processing tasks. Next, we explore the key components and architectures of complex-valued neural networks, which are specifically designed to handle complex-valued data and have been applied for complex spectrogram processing. We then discuss various training strategies and loss functions tailored for training neural networks to process and model complex spectrograms. The survey further examines key applications, including phase retrieval, speech enhancement, and speech separation, where deep learning has achieved significant progress by leveraging complex spectrograms or their derived feature representations. Additionally, we examine the intersection of complex spectrograms with generative models. This survey aims to serve as a valuable resource for researchers and practitioners in the field of speech signal processing and complex-valued neural networks.","authors":["Yuying Xie","Zheng-Hua Tan"],"url":"https://arxiv.org/abs/2505.08694"}
{"created":"2025-05-14","title":"Continuous Temporal Learning of Probability Distributions via Neural ODEs with Applications in Continuous Glucose Monitoring Data","abstract":"Modeling the continuous--time dynamics of probability distributions from time--dependent data samples is a fundamental problem in many fields, including digital health. The aim is to analyze how the distribution of a biomarker, such as glucose, evolves over time and how these changes may reflect the progression of chronic diseases such as diabetes. In this paper, we propose a novel probabilistic model based on a mixture of Gaussian distributions to capture how samples from a continuous-time stochastic process evolve over the time. To model potential distribution shifts over time, we introduce a time-dependent function parameterized by a Neural Ordinary Differential Equation (Neural ODE) and estimate it non--parametrically using the Maximum Mean Discrepancy (MMD). The proposed model is highly interpretable, detects subtle temporal shifts, and remains computationally efficient. Through simulation studies, we show that it performs competitively in terms of estimation accuracy against state-of-the-art, less interpretable methods such as normalized gradient--flows and non--parameteric kernel density estimators. Finally, we demonstrate the utility of our method on digital clinical--trial data, showing how the interventions alters the time-dependent distribution of glucose levels and enabling a rigorous comparison of control and treatment groups from novel mathematical and clinical perspectives.","authors":["Antonio \\'Alvarez-L\\'opez","Marcos Matabuena"],"url":"https://arxiv.org/abs/2505.08698"}
{"created":"2025-05-14","title":"Big Data and the Computational Social Science of Entrepreneurship and Innovation","abstract":"As large-scale social data explode and machine-learning methods evolve, scholars of entrepreneurship and innovation face new research opportunities but also unique challenges. This chapter discusses the difficulties of leveraging large-scale data to identify technological and commercial novelty, document new venture origins, and forecast competition between new technologies and commercial forms. It suggests how scholars can take advantage of new text, network, image, audio, and video data in two distinct ways that advance innovation and entrepreneurship research. First, machine-learning models, combined with large-scale data, enable the construction of precision measurements that function as system-level observatories of innovation and entrepreneurship across human societies. Second, new artificial intelligence models fueled by big data generate 'digital doubles' of technology and business, forming laboratories for virtual experimentation about innovation and entrepreneurship processes and policies. The chapter argues for the advancement of theory development and testing in entrepreneurship and innovation by coupling big data with big models.","authors":["Ningzi Li","Shiyang Lai","James Evans"],"url":"https://arxiv.org/abs/2505.08706"}
{"created":"2025-05-14","title":"Contrastive Normalizing Flows for Uncertainty-Aware Parameter Estimation","abstract":"Estimating physical parameters from data is a crucial application of machine learning (ML) in the physical sciences. However, systematic uncertainties, such as detector miscalibration, induce data distribution distortions that can erode statistical precision. In both high-energy physics (HEP) and broader ML contexts, achieving uncertainty-aware parameter estimation under these domain shifts remains an open problem. In this work, we address this challenge of uncertainty-aware parameter estimation for a broad set of tasks critical for HEP. We introduce a novel approach based on Contrastive Normalizing Flows (CNFs), which achieves top performance on the HiggsML Uncertainty Challenge dataset. Building on the insight that a binary classifier can approximate the model parameter likelihood ratio, we address the practical limitations of expressivity and the high cost of simulating high-dimensional parameter grids by embedding data and parameters in a learned CNF mapping. This mapping yields a tunable contrastive distribution that enables robust classification under shifted data distributions. Through a combination of theoretical analysis and empirical evaluations, we demonstrate that CNFs, when coupled with a classifier and established frequentist techniques, provide principled parameter estimation and uncertainty quantification through classification that is robust to data distribution distortions.","authors":["Ibrahim Elsharkawy","Yonatan Kahn"],"url":"https://arxiv.org/abs/2505.08709"}
{"created":"2025-05-14","title":"Computing Projective Implicit Representations from Poset Towers","abstract":"A family of simplicial complexes, connected with simplicial maps and indexed by a poset $P$, is called a poset tower. The concept of poset towers subsumes classical objects of study in the persistence literature, as, for example, one-critical multi-filtrations and zigzag filtrations, but also allows multi-critical simplices and arbitrary simplicial maps. The homology of a poset tower gives rise to a $P$-persistence module. To compute this homology globally over $P$, in the spirit of the persistence algorithm, we consider the homology of a chain complex of $P$-persistence modules, $C_{\\ell-1}\\xleftarrow{}C_\\ell\\xleftarrow{}C_{\\ell+1}$, induced by the simplices of the poset tower. Contrary to the case of one-critical filtrations, the chain-modules $C_\\ell$ of a poset tower can have a complicated structure. In this work, we tackle the problem of computing a representation of such a chain complex segment by projective modules and $P$-graded matrices, which we call a projective implicit representation (PiRep). We give efficient algorithms to compute asymptotically minimal projective resolutions (up to the second term) of the chain modules and the boundary maps and compute a PiRep from these resolutions. Our algorithms are tailored to the chain complexes and resolutions coming from poset towers and take advantage of their special structure. In the context of poset towers, they are fully general and could potentially serve as a foundation for developing more efficient algorithms on specific posets.","authors":["Tamal K. Dey","Florian Russold"],"url":"https://arxiv.org/abs/2505.08755"}
{"created":"2025-05-14","title":"Generative Molecular Design with Steerable and Granular Synthesizability Control","abstract":"Synthesizability in small molecule generative design remains a bottleneck. Existing works that do consider synthesizability can output predicted synthesis routes for generated molecules. However, there has been minimal attention in addressing the ease of synthesis and enabling flexibility to incorporate desired reaction constraints. In this work, we propose a small molecule generative design framework that enables steerable and granular synthesizability control. Generated molecules satisfy arbitrary multi-parameter optimization objectives with predicted synthesis routes containing pre-defined allowed reactions, while optionally avoiding others. One can also enforce that all reactions belong to a pre-defined set. We show the capability to mix-and-match these reaction constraints across the most common medicinal chemistry transformations. Next, we show how our framework can be used to valorize industrial byproducts towards de novo optimized molecules. Going further, we demonstrate how granular control over synthesizability constraints can loosely mimic virtual screening of ultra-large make-on-demand libraries. Using only a single GPU, we generate and dock 15k molecules to identify promising candidates in Freedom 4.0 constituting 142B make-on-demand molecules (assessing only 0.00001% of the library). Generated molecules satisfying the reaction constraints have > 90% exact match rate. Lastly, we benchmark our framework against recent synthesizability-constrained generative models and demonstrate the highest sample efficiency even when imposing the additional constraint that all molecules must be synthesizable from a single reaction type. The main theme is demonstrating that a pre-trained generalist molecular generative model can be incentivized to generate property-optimized small molecules under challenging synthesizability constraints through reinforcement learning.","authors":["Jeff Guo","V\\'ictor Sabanza-Gil","Zlatko Jon\\v{c}ev","Jeremy S. Luterbacher","Philippe Schwaller"],"url":"https://arxiv.org/abs/2505.08774"}
{"created":"2025-05-14","title":"PCS-UQ: Uncertainty Quantification via the Predictability-Computability-Stability Framework","abstract":"As machine learning (ML) models are increasingly deployed in high-stakes domains, trustworthy uncertainty quantification (UQ) is critical for ensuring the safety and reliability of these models. Traditional UQ methods rely on specifying a true generative model and are not robust to misspecification. On the other hand, conformal inference allows for arbitrary ML models but does not consider model selection, which leads to large interval sizes. We tackle these drawbacks by proposing a UQ method based on the predictability, computability, and stability (PCS) framework for veridical data science proposed by Yu and Kumbier. Specifically, PCS-UQ addresses model selection by using a prediction check to screen out unsuitable models. PCS-UQ then fits these screened algorithms across multiple bootstraps to assess inter-sample variability and algorithmic instability, enabling more reliable uncertainty estimates. Further, we propose a novel calibration scheme that improves local adaptivity of our prediction sets. Experiments across $17$ regression and $6$ classification datasets show that PCS-UQ achieves the desired coverage and reduces width over conformal approaches by $\\approx 20\\%$. Further, our local analysis shows PCS-UQ often achieves target coverage across subgroups while conformal methods fail to do so. For large deep-learning models, we propose computationally efficient approximation schemes that avoid the expensive multiple bootstrap trainings of PCS-UQ. Across three computer vision benchmarks, PCS-UQ reduces prediction set size over conformal methods by $20\\%$. Theoretically, we show a modified PCS-UQ algorithm is a form of split conformal inference and achieves the desired coverage with exchangeable data.","authors":["Abhineet Agarwal","Michael Xiao","Rebecca Barter","Omer Ronen","Boyu Fan","Bin Yu"],"url":"https://arxiv.org/abs/2505.08784"}
{"created":"2025-05-14","title":"Calibrated and Sharp Uncertainties in Deep Learning via Density Estimation","abstract":"Accurate probabilistic predictions can be characterized by two properties -- calibration and sharpness. However, standard maximum likelihood training yields models that are poorly calibrated and thus inaccurate -- a 90% confidence interval typically does not contain the true outcome 90% of the time. This paper argues that calibration is important in practice and is easy to maintain by performing low-dimensional density estimation. We introduce a simple training procedure based on recalibration that yields calibrated models without sacrificing overall performance; unlike previous approaches, ours ensures the most general property of distribution calibration and applies to any model, including neural networks. We formally prove the correctness of our procedure assuming that we can estimate densities in low dimensions and we establish uniform convergence bounds. Our results yield empirical performance improvements on linear and deep Bayesian models and suggest that calibration should be increasingly leveraged across machine learning. We release a library that implements our methods along with a blog post here: https://shachideshpande.github.io/blog-distribution-calibration/.","authors":["Volodymyr Kuleshov","Shachi Deshpande"],"url":"https://arxiv.org/abs/2112.07184"}
{"created":"2025-05-14","title":"A Minimal Formulation of Session Types: The Sessions of Trios in Concert","abstract":"Session types are a type-based approach to the verification of message-passing programs. They specify communication structures essential to enforcing program correctness; by relying on sequencing constructs, a session type can precisely describe the intended order of communication actions through a channel.","authors":["Alen Arslanagi\\'c","Jorge A. P\\'erez","Dan Frumin"],"url":"https://arxiv.org/abs/2301.05301"}
{"created":"2025-05-14","title":"Majority is not Needed: A Counterstrategy to Selfish Mining","abstract":"In the last few years several papers investigated selfish mine attacks, most of which assumed that every miner that is not part of the selfish mine pool will continue to mine honestly. However, in reality, remaining honest is not always incentivized, particularly when another pool is employing selfish mining or other deviant strategies. In this work we explore the scenario in which a large enough pool capitalises on another selfish pool to gain 100\\% of the profit and commit double spending attacks. We show that this counterstrategy can effectively counter any deviant strategy, and that even the possibility of it discourages other pools from implementing deviant strategies.","authors":["Jonathan Gal","Maytal B Szabo","Ori Rottenstreich"],"url":"https://arxiv.org/abs/2304.06313"}
{"created":"2025-05-14","title":"Pitfalls in Effective Knowledge Management: Insights from an International Information Technology Organization","abstract":"Knowledge is considered an essential resource for organizations. For organizations to benefit from their possessed knowledge, knowledge needs to be managed effectively. Despite knowledge sharing and management being viewed as important by practitioners, organizations fail to benefit from their knowledge, leading to issues in cooperation and the loss of valuable knowledge with departing employees. This study aims to identify hindering factors that prevent individuals from effectively sharing and managing knowledge and understand how to eliminate these factors. Empirical data were collected through semi-structured group interviews from 50 individuals working in an international large IT organization. This study confirms the existence of a gap between the perceived importance of knowledge management and how little this importance is reflected in practice. Several hindering factors were identified, grouped into personal social topics, organizational social topics, technical topics, environmental topics, and interrelated social and technical topics. The presented recommendations for mitigating these hindering factors are focused on improving employees' actions, such as offering training and guidelines to follow. The findings of this study have implications for organizations in knowledge-intensive fields, as they can use this knowledge to create knowledge sharing and management strategies to improve their overall performance.","authors":["Kalle Koivisto","Toni Taipalus"],"url":"https://arxiv.org/abs/2304.07737"}
{"created":"2025-05-14","title":"TARGET: Automated Scenario Generation from Traffic Rules for Testing Autonomous Vehicles via Validated LLM-Guided Knowledge Extraction","abstract":"Recent incidents with autonomous vehicles highlight the need for rigorous testing to ensure safety and robustness. Constructing test scenarios for autonomous driving systems (ADSs), however, is labor-intensive. We propose TARGET, an end-to-end framework that automatically generates test scenarios from traffic rules. To address complexity, we leverage a Large Language Model (LLM) to extract knowledge from traffic rules. To mitigate hallucinations caused by large context during input processing, we introduce a domain-specific language (DSL) designed to be syntactically simple and compositional. This design allows the LLM to learn and generate test scenarios in a modular manner while enabling syntactic and semantic validation for each component. Based on these validated representations, TARGET synthesizes executable scripts to render scenarios in simulation. Evaluated seven ADSs with 284 scenarios derived from 54 traffic rules, TARGET uncovered 610 rule violations, collisions, and other issues. For each violation, TARGET generates scenario recordings and detailed logs, aiding root cause analysis. Two identified issues were confirmed by ADS developers: one linked to an existing bug report and the other to limited ADS functionality.","authors":["Yao Deng","Jiaohong Yao","Zhi Tu","Xi Zheng","Mengshi Zhang","Tianyi Zhang"],"url":"https://arxiv.org/abs/2305.06018"}
{"created":"2025-05-14","title":"Is Centralized Training with Decentralized Execution Framework Centralized Enough for MARL?","abstract":"Centralized Training with Decentralized Execution (CTDE) has recently emerged as a popular framework for cooperative Multi-Agent Reinforcement Learning (MARL), where agents can use additional global state information to guide training in a centralized way and make their own decisions only based on decentralized local policies. Despite the encouraging results achieved, CTDE makes an independence assumption on agent policies, which limits agents to adopt global cooperative information from each other during centralized training. Therefore, we argue that existing CTDE methods cannot fully utilize global information for training, leading to an inefficient joint-policy exploration and even suboptimal results. In this paper, we introduce a novel Centralized Advising and Decentralized Pruning (CADP) framework for multi-agent reinforcement learning, that not only enables an efficacious message exchange among agents during training but also guarantees the independent policies for execution. Firstly, CADP endows agents the explicit communication channel to seek and take advices from different agents for more centralized training. To further ensure the decentralized execution, we propose a smooth model pruning mechanism to progressively constraint the agent communication into a closed one without degradation in agent cooperation capability. Empirical evaluations on StarCraft II micromanagement and Google Research Football benchmarks demonstrate that the proposed framework achieves superior performance compared with the state-of-the-art counterparts. Our code will be made publicly available.","authors":["Yihe Zhou","Shunyu Liu","Yunpeng Qing","Kaixuan Chen","Tongya Zheng","Jie Song","Mingli Song"],"url":"https://arxiv.org/abs/2305.17352"}
{"created":"2025-05-14","title":"Towards Anytime Optical Flow Estimation with Event Cameras","abstract":"Event cameras respond to changes in log-brightness at the millisecond level, making them ideal for optical flow estimation. However, existing datasets from event cameras provide only low frame rate ground truth for optical flow, limiting the research potential of event-driven optical flow. To address this challenge, we introduce a low-latency event representation, Unified Voxel Grid, and propose EVA-Flow, an EVent-based Anytime Flow estimation network to produce high-frame-rate event optical flow with only low-frame-rate optical flow ground truth for supervision. Furthermore, we propose the Rectified Flow Warp Loss (RFWL) for the unsupervised assessment of intermediate optical flow. A comprehensive variety of experiments on MVSEC, DESC, and our EVA-FlowSet demonstrates that EVA-Flow achieves competitive performance, super-low-latency (5ms), time-dense motion estimation (200Hz), and strong generalization. Our code will be available at https://github.com/Yaozhuwa/EVA-Flow.","authors":["Yaozu Ye","Hao Shi","Kailun Yang","Ze Wang","Xiaoting Yin","Lei Sun","Yaonan Wang","Kaiwei Wang"],"url":"https://arxiv.org/abs/2307.05033"}
{"created":"2025-05-14","title":"Deciding One to One property of Boolean maps: Condition and algorithm in terms of implicants","abstract":"This paper addresses the computational problem of deciding invertibility (or one to one-ness) of a Boolean map $F$ in $n$-Boolean variables. This problem is a special case of deciding invertibilty of a map $F:\\mathbb{F}_{q}^n\\rightarrow\\mathbb{F}_{q}^n$ over the finite field $\\mathbb{F}_q$ for $q=2$. Algebraic condition for invertibility of $F$ is well known to be equivalent to invertibility of the Koopman operator of $F$ as shown in \\cite{RamSule}. In this paper a condition for invertibility is derived in the special case of Boolean maps $F:B_0^n\\rightarrow B_0^n$ where $B_0$ is the two element Boolean algebra in terms of \\emph{implicants} of Boolean equations defined by the map. This condition is then extended to the case of general maps in $n$ variables and $m\\geq n$ equations. Hence this condition answers the special case of invertibility of maps $F$ defined over the binary field $\\mathbb{F}_2$ alternatively, in terms of implicants instead of the Koopman operator. The problem of deciding invertibility of a map $F$ (or that of finding its Garden of Eden (GOE)) over finite fields is distinct from the satisfiability problem (SAT) or the problem of deciding consistency of polynomial equations over finite fields. Hence the well known algorithms for deciding SAT or of solvability using Grobner basis for checking membership in an ideal generated by polynomials is not known to answer the question of invertibility of a map. Similarly it appears that algorithms for satisfiability or polynomial solvability are not useful for computation of GOE of $F$ even for maps over the binary field $\\mathbb{F}_2$.","authors":["Virendra Sule"],"url":"https://arxiv.org/abs/2307.07788"}
{"created":"2025-05-14","title":"Unravelling Responsibility for AI","abstract":"It is widely acknowledged that we need to establish where responsibility lies for the outputs and impacts of AI-enabled systems. This is important to achieve justice and compensation for victims of AI harms, and to inform policy and engineering practice. But without a clear, thorough understanding of what `responsibility' means, deliberations about where responsibility lies will be, at best, unfocused and incomplete and, at worst, misguided. Furthermore, AI-enabled systems exist within a wider ecosystem of actors, decisions, and governance structures, giving rise to complex networks of responsibility relations. To address these issues, this paper presents a conceptual framework of responsibility, accompanied with a graphical notation and general methodology, for visualising these responsibility networks and for tracing different responsibility attributions for AI. Taking the three-part formulation 'Actor A is responsible for Occurrence O,' the framework unravels the concept of responsibility to clarify that there are different possibilities of who is responsible for AI, senses in which they are responsible, and aspects of events they are responsible for. The notation allows these permutations to be represented graphically. The methodology enables users to apply the framework to specific scenarios. The aim is to offer a foundation to support stakeholders from diverse disciplinary backgrounds to discuss and address complex responsibility questions in hypothesised and real-world cases involving AI. The work is illustrated by application to a fictitious scenario of a fatal collision between a crewless, AI-enabled maritime vessel in autonomous mode and a traditional, crewed vessel at sea.","authors":["Zoe Porter","Philippa Ryan","Phillip Morgan","Joanna Al-Qaddoumi","Bernard Twomey","Paul Noordhof","John McDermid","Ibrahim Habli"],"url":"https://arxiv.org/abs/2308.02608"}
{"created":"2025-05-14","title":"Deep learning-based interactive segmentation in remote sensing","abstract":"Interactive segmentation, a computer vision technique where a user provides guidance to help an algorithm segment a feature of interest in an image, has achieved outstanding accuracy and efficient human-computer interaction. However, few studies have discussed its application to remote sensing imagery, where click-based interactive segmentation could greatly facilitate the analysis of complicated landscapes. This study aims to bridge the gap between click-based interactive segmentation and remote sensing image analysis by conducting a benchmark study on various click-based interactive segmentation models. We assessed the performance of five state-of-the-art interactive segmentation methods (Reviving Iterative Training with Mask Guidance for Interactive Segmentation (RITM), FocalClick, SimpleClick, Iterative Click Loss (ICL), and Segment Anything (SAM)) on two high-resolution aerial imagery datasets. The Cascade-Forward Refinement (CFR) approach, an innovative inference strategy for interactive segmentation, was also introduced to enhance the segmentation results without requiring manual efforts. We further integrated CFR into all models for comparison. The performance of these methods on various land cover types, different object sizes, and multiple band combinations in the datasets was evaluated. The SimpleClick-CFR model consistently outperformed the other methods in our experiments. Building upon these findings, we developed a dedicated online tool called SegMap for interactive segmentation of remote sensing data. SegMap incorporates a well-performing interactive model that is fine-tuned with remote sensing data. Unlike existing interactive segmentation tools, SegMap offers robust interactivity, modifiability, and adaptability to analyze remote sensing imagery.","authors":["Zhe Wang","Shoukun Sun","Xiang Que","Xiaogang Ma","Carmen Galaz Garcia"],"url":"https://arxiv.org/abs/2308.13174"}
{"created":"2025-05-14","title":"Lower Bounds from Succinct Hitting Sets","abstract":"We investigate the consequences of the existence of ``efficiently describable'' hitting sets for polynomial sized algebraic circuit ($\\mathsf{VP}$), in particular, \\emph{$\\mathsf{VP}$-succinct hitting sets}. Existence of such hitting sets is known to be equivalent to a ``natural-proofs-barrier'' towards algebraic circuit lower bounds, from the works that introduced this concept (Forbes \\etal (2018), Grochow \\etal (2017)). We show that the existence of $\\mathsf{VP}$-succinct hitting sets for $\\mathsf{VP}$ would either imply that $\\mathsf{VP} \\neq \\mathsf{VNP}$, or yield a fairly strong lower bound against $\\mathsf{TC}^0$ circuits, assuming the Generalized Riemann Hypothesis (GRH).","authors":["Prerona Chatterjee","Anamay Tengse"],"url":"https://arxiv.org/abs/2309.07612"}
{"created":"2025-05-14","title":"A primal-dual perspective for distributed TD-learning","abstract":"The goal of this paper is to investigate distributed temporal difference (TD) learning for a networked multi-agent Markov decision process. The proposed approach is based on distributed optimization algorithms, which can be interpreted as primal-dual Ordinary differential equation (ODE) dynamics subject to null-space constraints. Based on the exponential convergence behavior of the primal-dual ODE dynamics subject to null-space constraints, we examine the behavior of the final iterate in various distributed TD-learning scenarios, considering both constant and diminishing step-sizes and incorporating both i.i.d. and Markovian observation models. Unlike existing methods, the proposed algorithm does not require the assumption that the underlying communication network structure is characterized by a doubly stochastic matrix.","authors":["Han-Dong Lim","Donghwan Lee"],"url":"https://arxiv.org/abs/2310.00638"}
{"created":"2025-05-14","title":"RT-GAN: Recurrent Temporal GAN for Adding Lightweight Temporal Consistency to Frame-Based Domain Translation Approaches","abstract":"Fourteen million colonoscopies are performed annually just in the U.S. However, the videos from these colonoscopies are not saved due to storage constraints (each video from a high-definition colonoscope camera can be in tens of gigabytes). Instead, a few relevant individual frames are saved for documentation/reporting purposes and these are the frames on which most current colonoscopy AI models are trained on. While developing new unsupervised domain translation methods for colonoscopy (e.g. to translate between real optical and virtual/CT colonoscopy), it is thus typical to start with approaches that initially work for individual frames without temporal consistency. Once an individual-frame model has been finalized, additional contiguous frames are added with a modified deep learning architecture to train a new model from scratch for temporal consistency. This transition to temporally-consistent deep learning models, however, requires significantly more computational and memory resources for training. In this paper, we present a lightweight solution with a tunable temporal parameter, RT-GAN (Recurrent Temporal GAN), for adding temporal consistency to individual frame-based approaches that reduces training requirements by a factor of 5. We demonstrate the effectiveness of our approach on two challenging use cases in colonoscopy: haustral fold segmentation (indicative of missed surface) and realistic colonoscopy simulator video generation. We also release a first-of-its kind temporal dataset for colonoscopy for the above use cases. The datasets, accompanying code, and pretrained models will be made available on our Computational Endoscopy Platform GitHub (https://github.com/nadeemlab/CEP). The supplementary video is available at https://youtu.be/UMVP-uIXwWk.","authors":["Shawn Mathew","Saad Nadeem","Alvin C. Goh","Arie Kaufman"],"url":"https://arxiv.org/abs/2310.00868"}
{"created":"2025-05-14","title":"Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education","abstract":"In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or D) that is the most likely answer to a question, given the context of the question. Our evaluation of six well-known LLMs, namely BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising results on the MCSB ability of LLMs for Vietnamese. The dataset is available for research purposes only.","authors":["Duc-Vu Nguyen","Quoc-Nam Nguyen"],"url":"https://arxiv.org/abs/2310.12059"}
{"created":"2025-05-14","title":"Using generalized simplex methods to approximate derivatives","abstract":"This paper presents two methods for approximating a proper subset of the entries of a Hessian using only function evaluations. These approximations are obtained using the techniques called \\emph{generalized simplex Hessian} and \\emph{generalized centered simplex Hessian}. We show how to choose the matrices of directions involved in the computation of these two techniques depending on the entries of the Hessian of interest. We discuss the number of function evaluations required in each case and develop a general formula to approximate all order-$P$ partial derivatives. Since only function evaluations are required to compute the methods discussed in this paper, they are suitable for use in derivative-free optimization methods.","authors":["Gabriel Jarry-Bolduc","Chayne Planiden"],"url":"https://arxiv.org/abs/2310.16997"}
{"created":"2025-05-14","title":"Learning Optimal Classification Trees Robust to Distribution Shifts","abstract":"We consider the problem of learning classification trees that are robust to distribution shifts between training and testing/deployment data. This problem arises frequently in high stakes settings such as public health and social work where data is often collected using self-reported surveys which are highly sensitive to e.g., the framing of the questions, the time when and place where the survey is conducted, and the level of comfort the interviewee has in sharing information with the interviewer. We propose a method for learning optimal robust classification trees based on mixed-integer robust optimization technology. In particular, we demonstrate that the problem of learning an optimal robust tree can be cast as a single-stage mixed-integer robust optimization problem with a highly nonlinear and discontinuous objective. We reformulate this problem equivalently as a two-stage linear robust optimization problem for which we devise a tailored solution procedure based on constraint generation. We evaluate the performance of our approach on numerous publicly available datasets, and compare the performance to a regularized, non-robust optimal tree. We show an increase of up to 12.48% in worst-case accuracy and of up to 4.85% in average-case accuracy across several datasets and distribution shifts from using our robust solution in comparison to the non-robust one.","authors":["Nathan Justin","Sina Aghaei","Andr\\'es G\\'omez","Phebe Vayanos"],"url":"https://arxiv.org/abs/2310.17772"}
{"created":"2025-05-14","title":"A Cooperative Statistical Approach for Abnormal Node Detection with Adversary Resistance","abstract":"Distinguishing abnormal nodes from those with normal packet loss in clusters helps reduce the loss of clustered network resources. The detection performance of existing detection schemes is limited by the techniques to quantify node behaviors, and most schemes cannot avoid being misled by the falsified information. This paper presents a novel probabilistic abnormal node detection scheme CSD -- Cooperative Statistical Detection -- for accurate and efficient detection in the existence of falsified detection data in clustered networks. Specifically, employing the likelihood ratio test (LRT) based detection method to measure node forwarding behaviors, we propose a modified Z-score based falsification-resistant mechanism to filter out falsifications. We show that both the false alarm and missed detection probabilities can decrease exponentially if and only if the transmissions from the nodes falsifying the data are less than half of the total. Furthermore, the optimal threshold of the modified Z-score method is derived, which guarantees perfect detection of our CSD under any falsification strategy in the proposed detection model. Evaluation results validate the effectiveness, robustness, and superiority of our scheme compared to the state-of-the-art.","authors":["Yingying Huangfu","Tian Bai"],"url":"https://arxiv.org/abs/2311.16661"}
{"created":"2025-05-14","title":"Signal Temporal Logic Control Synthesis among Uncontrollable Dynamic Agents with Conformal Prediction","abstract":"The control of dynamical systems under temporal logic specifications among uncontrollable dynamic agents is challenging due to the agents' a-priori unknown behavior. Existing works have considered the problem where either all agents are controllable, the agent models are deterministic and known, or no safety guarantees are provided. We propose a predictive control synthesis framework that guarantees, with high probability, the satisfaction of signal temporal logic (STL) tasks that are defined over a controllable system in the presence of uncontrollable stochastic agents. We use trajectory predictors and conformal prediction to construct probabilistic prediction regions for each uncontrollable agent that are valid over multiple future time steps. Specifically, we construct a normalized prediction region over all agents and time steps to reduce conservatism and increase data efficiency. We then formulate a worst-case bilevel mixed integer program (MIP) that accounts for all agent realizations within the prediction region to obtain an open-loop controller that provably guarantee task satisfaction with high probability. To efficiently solve this bilevel MIP, we propose an equivalent MIP program based on KKT conditions of the original bilevel formulation. Building upon this, we design a closed-loop controller, where both recursive feasibility and task satisfaction can be guaranteed with high probability. We illustrate our control synthesis framework on two case studies.","authors":["Xinyi Yu","Yiqi Zhao","Xiang Yin","Lars Lindemann"],"url":"https://arxiv.org/abs/2312.04242"}
{"created":"2025-05-14","title":"Asymptotic convergence of restarted Anderson acceleration for certain normal linear systems","abstract":"Anderson acceleration (AA) is widely used for accelerating the convergence of an underlying fixed-point iteration $\\bm{x}_{k+1} = \\bm{q}( \\bm{x}_{k} )$, $k = 0, 1, \\ldots$, with $\\bm{x}_k \\in \\mathbb{R}^n$, $\\bm{q} \\colon \\mathbb{R}^n \\to \\mathbb{R}^n$. Despite AA's widespread use, relatively little is understood theoretically about the extent to which it may accelerate the underlying fixed-point iteration. To this end, we analyze a restarted variant of AA with a restart size of one, a method closely related to GMRES(1). We consider the case of $\\bm{q}( \\bm{x} ) = M \\bm{x} + \\bm{b}$ with matrix $M \\in \\mathbb{R}^{n \\times n}$ either symmetric or skew-symmetric. For both classes of $M$ we compute the worst-case root-average asymptotic convergence factor of the AA method, partially relying on conjecture in the symmetric setting, proving that it is strictly smaller than that of the underlying fixed-point iteration. For symmetric $M$, we show that the AA residual iteration corresponds to a fixed-point iteration for solving an eigenvector-dependent nonlinear eigenvalue problem (NEPv), and we show how this can result in the convergence factor strongly depending on the initial iterate, which we quantify exactly in certain special cases. Conversely, for skew-symmetric $M$ we show that the AA residual iteration is closely related to a power iteration for $M$, and how this results in the convergence factor being independent of the initial iterate. Supporting numerical results are given, which also indicate the theory is applicable to the more general setting of nonlinear $\\bm{q}$ with Jacobian at the fixed point that is symmetric or skew symmetric.","authors":["Oliver A. Krzysik","Hans De Sterck","Adam Smith"],"url":"https://arxiv.org/abs/2312.04776"}
{"created":"2025-05-14","title":"Optimized View and Geometry Distillation from Multi-view Diffuser","abstract":"Generating multi-view images from a single input view using image-conditioned diffusion models is a recent advancement and has shown considerable potential. However, issues such as the lack of consistency in synthesized views and over-smoothing in extracted geometry persist. Previous methods integrate multi-view consistency modules or impose additional supervisory to enhance view consistency while compromising on the flexibility of camera positioning and limiting the versatility of view synthesis. In this study, we consider the radiance field optimized during geometry extraction as a more rigid consistency prior, compared to volume and ray aggregation used in previous works. We further identify and rectify a critical bias in the traditional radiance field optimization process through score distillation from a multi-view diffuser. We introduce an Unbiased Score Distillation (USD) that utilizes unconditioned noises from a 2D diffusion model, greatly refining the radiance field fidelity. We leverage the rendered views from the optimized radiance field as the basis and develop a two-step specialization process of a 2D diffusion model, which is adept at conducting object-specific denoising and generating high-quality multi-view images. Finally, we recover faithful geometry and texture directly from the refined multi-view images. Empirical evaluations demonstrate that our optimized geometry and view distillation technique generates comparable results to the state-of-the-art models trained on extensive datasets, all while maintaining freedom in camera positioning. Please see our project page at https://youjiazhang.github.io/USD/.","authors":["Youjia Zhang","Zikai Song","Junqing Yu","Yawei Luo","Wei Yang"],"url":"https://arxiv.org/abs/2312.06198"}
{"created":"2025-05-14","title":"On Separating Path and Tree Systems in Graphs","abstract":"We explore the concept of separating systems of vertex sets of graphs. A separating system of a set $X$ is a collection of subsets of $X$ such that for any pair of distinct elements in $X$, there exists a set in the separating system that contains exactly one of the two elements. A separating system of the vertex set of a graph $G$ is called a vertex-separating path (tree) system of $G$ if the elements of the separating system are paths (trees) in the graph $G$. In this paper, we focus on the size of the smallest vertex-separating path (tree) system for different types of graphs, including trees, grids, and maximal outerplanar graphs.","authors":["Ahmad Biniaz","Prosenjit Bose","Jean-Lou De Carufel","Anil Maheshwari","Babak Miraftab","Saeed Odak","Michiel Smid","Shakhar Smorodinsky","Yelena Yuditsky"],"url":"https://arxiv.org/abs/2312.14295"}
{"created":"2025-05-14","title":"SIRD: A Sender-Informed, Receiver-Driven Datacenter Transport Protocol","abstract":"Datacenter congestion control protocols are challenged to navigate the throughput-buffering trade-off while relative packet buffer capacity is trending lower year-over-year. In this context, receiver-driven protocols -- which schedule packet transmissions instead of reacting to congestion -- excel when the bottleneck lies at the ToR-to-receiver link. However, when multiple receivers must use a shared link (e.g., ToR to Spine), their independent schedules can conflict. We present SIRD, a receiver-driven congestion control protocol designed around the simple insight that single-owner links should be scheduled, while shared links should be managed with reactive control algorithms. The approach allows receivers to both precisely schedule their downlinks and to coordinate over shared bottlenecks. Critically, SIRD also treats sender uplinks as shared links, enabling the flow of congestion feedback from senders to receivers, which then adapt their scheduling to each sender's real-time capacity. This results in tight scheduling, enabling high bandwidth utilization with little contention, and thus minimal latency-inducing buffering in the fabric. We implement SIRD on top of the Caladan stack and show that SIRD's asymmetric design can deliver 100Gbps in software while keeping network queuing minimal. We further compare SIRD to state-of-the-art receiver-driven protocols (Homa, dcPIM, and ExpressPass) and production-grade reactive protocols (Swift and DCTCP) and show that SIRD is uniquely able to simultaneously maximize link utilization, minimize queuing, and obtain near-optimal latency.","authors":["Konstantinos Prasopoulos","Ryan Kosta","Edouard Bugnion","Marios Kogias"],"url":"https://arxiv.org/abs/2312.15403"}
{"created":"2025-05-14","title":"A k-swap Local Search for Makespan Scheduling","abstract":"Local search is a widely used technique for tackling challenging optimization problems, offering significant advantages in terms of computational efficiency and exhibiting strong empirical behavior across a wide range of problem domains. In this paper, we address the problem of scheduling a set of jobs on identical parallel machines with the objective of makespan minimization. For this problem, we consider a local search neighborhood, called $k$-swap, which is a generalized version of the widely-used swap and jump neighborhoods. The $k$-swap neighborhood is obtained by swapping at most $k$ jobs between two machines. First, we propose an algorithm for finding an improving neighbor in the $k$-swap neighborhood which is faster than the naive approach, and prove an almost matching lower bound on any such an algorithm. Then, we analyze the number of local search steps required to converge to a local optimum with respect to the $k$-swap neighborhood. For $k \\geq 3$, we provide an exponential lower bound regardless of the number of machines, and for $k = 2$ (similar to the swap neighborhood), we provide a polynomial upper bound for the case of having two machines. Finally, we conduct computational experiments on various families of instances.","authors":["Lars Rohwedder","Ashkan Safari","Tjark Vredeveld"],"url":"https://arxiv.org/abs/2401.05956"}
{"created":"2025-05-14","title":"LLM Multi-Agent Systems: Challenges and Open Problems","abstract":"This paper explores multi-agent systems and identify challenges that remain inadequately addressed. By leveraging the diverse capabilities and roles of individual agents, multi-agent systems can tackle complex tasks through agent collaboration. We discuss optimizing task allocation, fostering robust reasoning through iterative debates, managing complex and layered context information, and enhancing memory management to support the intricate interactions within multi-agent systems. We also explore potential applications of multi-agent systems in blockchain systems to shed light on their future development and application in real-world distributed systems.","authors":["Shanshan Han","Qifan Zhang","Yuhang Yao","Weizhao Jin","Zhaozhuo Xu"],"url":"https://arxiv.org/abs/2402.03578"}
{"created":"2025-05-14","title":"UVTM: Universal Vehicle Trajectory Modeling with ST Feature Domain Generation","abstract":"Vehicle movement is frequently captured in the form of GPS trajectories, i.e., sequences of timestamped GPS locations. Such data is widely used for various tasks such as travel-time estimation, trajectory recovery, and trajectory prediction. A universal vehicle trajectory model could be applied to different tasks, removing the need to maintain multiple specialized models, thereby reducing computational and storage costs. However, creating such a model is challenging when the integrity of trajectory features is compromised, i.e., in scenarios where only partial features are available or the trajectories are sparse.","authors":["Yan Lin","Jilin Hu","Shengnan Guo","Bin Yang","Christian S. Jensen","Youfang Lin","Huaiyu Wan"],"url":"https://arxiv.org/abs/2402.07232"}
{"created":"2025-05-14","title":"On the Impact of Uncertainty and Calibration on Likelihood-Ratio Membership Inference Attacks","abstract":"In a membership inference attack (MIA), an attacker exploits the overconfidence exhibited by typical machine learning models to determine whether a specific data point was used to train a target model. In this paper, we analyze the performance of the likelihood ratio attack (LiRA) within an information-theoretical framework that allows the investigation of the impact of the aleatoric uncertainty in the true data generation process, of the epistemic uncertainty caused by a limited training data set, and of the calibration level of the target model. We compare three different settings, in which the attacker receives decreasingly informative feedback from the target model: confidence vector (CV) disclosure, in which the output probability vector is released; true label confidence (TLC) disclosure, in which only the probability assigned to the true label is made available by the model; and decision set (DS) disclosure, in which an adaptive prediction set is produced as in conformal prediction. We derive bounds on the advantage of an MIA adversary with the aim of offering insights into the impact of uncertainty and calibration on the effectiveness of MIAs. Simulation results demonstrate that the derived analytical bounds predict well the effectiveness of MIAs.","authors":["Meiyi Zhu","Caili Guo","Chunyan Feng","Osvaldo Simeone"],"url":"https://arxiv.org/abs/2402.10686"}
{"created":"2025-05-14","title":"Pay Attention: a Call to Regulate the Attention Market and Prevent Algorithmic Emotional Governance","abstract":"Over the last 70 years, we, humans, have created an economic market where attention is being captured and turned into money thanks to advertising. During the last two decades, leveraging research in psychology, sociology, neuroscience and other domains, Web platforms have brought the process of capturing attention to an unprecedented scale. With the initial commonplace goal of making targeted advertising more effective, the generalization of attention-capturing techniques and their use of cognitive biases and emotions have multiple detrimental side effects such as polarizing opinions, spreading false information and threatening public health, economies and democracies. This is clearly a case where the Web is not used for the common good and where, in fact, all its users become a vulnerable population. This paper brings together contributions from a wide range of disciplines to analyze current practices and consequences thereof. Through a set of propositions and principles that could be used do drive further works, it calls for actions against these practices competing to capture our attention on the Web, as it would be unsustainable for a civilization to allow attention to be wasted with impunity on a world-wide scale.","authors":["Franck Michel","Fabien Gandon"],"url":"https://arxiv.org/abs/2402.16670"}
{"created":"2025-05-14","title":"Do You Trust Your Model? Emerging Malware Threats in the Deep Learning Ecosystem","abstract":"Training high-quality deep learning models is a challenging task due to computational and technical requirements. A growing number of individuals, institutions, and companies increasingly rely on pre-trained, third-party models made available in public repositories. These models are often used directly or integrated in product pipelines with no particular precautions, since they are effectively just data in tensor form and considered safe. In this paper, we raise awareness of a new machine learning supply chain threat targeting neural networks. We introduce MaleficNet 2.0, a novel technique to embed self-extracting, self-executing malware in neural networks. MaleficNet 2.0 uses spread-spectrum channel coding combined with error correction techniques to inject malicious payloads in the parameters of deep neural networks. MaleficNet 2.0 injection technique is stealthy, does not degrade the performance of the model, and is robust against removal techniques. We design our approach to work both in traditional and distributed learning settings such as Federated Learning, and demonstrate that it is effective even when a reduced number of bits is used for the model parameters. Finally, we implement a proof-of-concept self-extracting neural network malware using MaleficNet 2.0, demonstrating the practicality of the attack against a widely adopted machine learning framework. Our aim with this work is to raise awareness against these new, dangerous attacks both in the research community and industry, and we hope to encourage further research in mitigation techniques against such threats.","authors":["Dorjan Hitaj","Giulio Pagnotta","Fabio De Gaspari","Sediola Ruko","Briland Hitaj","Luigi V. Mancini","Fernando Perez-Cruz"],"url":"https://arxiv.org/abs/2403.03593"}
{"created":"2025-05-14","title":"iA$^*$: Imperative Learning-based A$^*$ Search for Path Planning","abstract":"The pathfinding problem, which aims to identify a collision-free path between two points, is crucial for many applications, such as robot navigation and autonomous driving. Classic methods, such as A$^*$ search, perform well on small-scale maps but face difficulties scaling up. Conversely, data-driven approaches can improve pathfinding efficiency but require extensive data labeling and lack theoretical guarantees, making it challenging for practical applications. To combine the strengths of the two methods, we utilize the imperative learning (IL) strategy and propose a novel self-supervised pathfinding framework, termed imperative learning-based A$^*$ (iA$^*$). Specifically, iA$^*$ is a bilevel optimization process where the lower-level optimization is dedicated to finding the optimal path by a differentiable A$^*$ search module, and the upper-level optimization narrows down the search space to improve efficiency via setting suitable initial values from a data-driven model. Besides, the model within the upper-level optimization is a fully convolutional network, trained by the calculated loss in the lower-level optimization. Thus, the framework avoids extensive data labeling and can be applied in diverse environments. Our comprehensive experiments demonstrate that iA$^*$ surpasses both classical and data-driven methods in pathfinding efficiency and shows superior robustness among different tasks, validated with public datasets and simulation environments.","authors":["Xiangyu Chen","Fan Yang","Chen Wang"],"url":"https://arxiv.org/abs/2403.15870"}
{"created":"2025-05-14","title":"Equipping Sketch Patches with Context-Aware Positional Encoding for Graphic Sketch Representation","abstract":"When benefiting graphic sketch representation with sketch drawing orders, recent studies have linked sketch patches as graph edges by drawing orders in accordance to a temporal-based nearest neighboring strategy. However, such constructed graph edges may be unreliable, since the contextual relationships between patches may be inconsistent with the sequential positions in drawing orders, due to variants of sketch drawings. In this paper, we propose a variant-drawing-protected method by equipping sketch patches with context-aware positional encoding (PE) to make better use of drawing orders for sketch learning. We introduce a sinusoidal absolute PE to embed the sequential positions in drawing orders, and a learnable relative PE to encode the unseen contextual relationships between patches. Both types of PEs never attend the construction of graph edges, but are injected into graph nodes to cooperate with the visual patterns captured from patches. After linking nodes by semantic proximity, during message aggregation via graph convolutional networks, each node receives both semantic features from patches and contextual information from PEs from its neighbors, which equips local patch patterns with global contextual information, further obtaining drawing-order-enhanced sketch representations. Experimental results indicate that our method significantly improves sketch healing and controllable sketch synthesis. The source codes could be found at https://github.com/SCZang/DC-gra2seq.","authors":["Sicong Zang","Zhijun Fang"],"url":"https://arxiv.org/abs/2403.17525"}
{"created":"2025-05-14","title":"Nonlinearity Enhanced Adaptive Activation Functions","abstract":"A general procedure for introducing parametric, learned, nonlinearity into activation functions is found to enhance the accuracy of representative neural networks without requiring significant additional computational resources. Examples are given based on the standard rectified linear unit (ReLU) as well as several other frequently employed activation functions. The associated accuracy improvement is quantified both in the context of the MNIST digit data set and a convolutional neural network (CNN) benchmark example.","authors":["David Yevick"],"url":"https://arxiv.org/abs/2403.19896"}
{"created":"2025-05-14","title":"Iterated Invariant Extended Kalman Filter (IterIEKF)","abstract":"We study the mathematical properties of the Invariant Extended Kalman Filter (IEKF) when iterating on the measurement update step, following the principles of the well-known Iterated Extended Kalman Filter. This iterative variant of the IEKF (IterIEKF) systematically improves its accuracy through Gauss-Newton-based relinearization, and exhibits additional theoretical properties, particularly in the low-noise regime, that resemble those of the linear Kalman filter. We apply the proposed approach to the problem of estimating the extended pose of a crane payload using an inertial measurement unit. Our results suggest that the IterIEKF significantly outperforms the IEKF when measurements are highly accurate.","authors":["Sven Goffin","Axel Barrau","Silv\\`ere Bonnabel","Olivier Br\\\"uls","Pierre Sacr\\'e"],"url":"https://arxiv.org/abs/2404.10665"}
{"created":"2025-05-14","title":"TextCenGen: Attention-Guided Text-Centric Background Adaptation for Text-to-Image Generation","abstract":"Text-to-image (T2I) generation has made remarkable progress in producing high-quality images, but a fundamental challenge remains: creating backgrounds that naturally accommodate text placement without compromising image quality. This capability is non-trivial for real-world applications like graphic design, where clear visual hierarchy between content and text is essential. Prior work has primarily focused on arranging layouts within existing static images, leaving unexplored the potential of T2I models for generating text-friendly backgrounds. We present TextCenGen, a training-free dynamic background adaptation in the blank region for text-friendly image generation. Instead of directly reducing attention in text areas, which degrades image quality, we relocate conflicting objects before background optimization. Our method analyzes cross-attention maps to identify conflicting objects overlapping with text regions and uses a force-directed graph approach to guide their relocation, followed by attention excluding constraints to ensure smooth backgrounds. Our method is plug-and-play, requiring no additional training while well balancing both semantic fidelity and visual quality. Evaluated on our proposed text-friendly T2I benchmark of 27,000 images across four seed datasets, TextCenGen outperforms existing methods by achieving 23% lower saliency overlap in text regions while maintaining 98% of the semantic fidelity measured by CLIP score and our proposed Visual-Textual Concordance Metric (VTCM).","authors":["Tianyi Liang","Jiangqi Liu","Yifei Huang","Shiqi Jiang","Jianshen Shi","Changbo Wang","Chenhui Li"],"url":"https://arxiv.org/abs/2404.11824"}
{"created":"2025-05-14","title":"PACIFISTA: Conflict Evaluation and Management in Open RAN","abstract":"The O-RAN ALLIANCE is defining architectures, interfaces, operations, and security requirements for cellular networks based on Open Radio Access Network (RAN) principles. In this context, O-RAN introduced the RAN Intelligent Controllers (RICs) to enable dynamic control of cellular networks via data-driven applications referred to as rApps and xApps. RICs enable for the first time truly intelligent and self-organizing cellular networks. However, enabling the execution of many Artificial Intelligence (AI) algorithms making autonomous control decisions to fulfill diverse (and possibly conflicting) goals poses unprecedented challenges. For instance, the execution of one xApp aiming at maximizing throughput and one aiming at minimizing energy consumption would inevitably result in diametrically opposed resource allocation strategies. Therefore, conflict management becomes a crucial component of any functional intelligent O-RAN system. This article studies the problem of conflict mitigation in O-RAN and proposes PACIFISTA, a framework to detect, characterize, and mitigate conflicts generated by O-RAN applications that control RAN parameters. PACIFISTA leverages a profiling pipeline to tests O-RAN applications in a sandbox environment, and combines hierarchical graphs with statistical models to detect the existence of conflicts and evaluate their severity. Experiments on Colosseum and OpenRAN Gym demonstrate PACIFISTA's ability to predict conflicts and provide valuable information before conflicting xApps are deployed on production. We demonstrate that users can experience a 16% throughput loss even in the case of xApps with similar goals, and that applications with conflicting goals might cause instability and result in up to 30% performance degradation. We also show that PACIFISTA can help operators to identify conflicting applications and maintain performance degradation at bay.","authors":["Pietro Brach del Prever","Salvatore D'Oro","Leonardo Bonati","Michele Polese","Maria Tsampazi","Heiko Lehmann","Tommaso Melodia"],"url":"https://arxiv.org/abs/2405.04395"}
{"created":"2025-05-14","title":"Wilsonian Renormalization of Neural Network Gaussian Processes","abstract":"Separating relevant and irrelevant information is key to any modeling process or scientific inquiry. Theoretical physics offers a powerful tool for achieving this in the form of the renormalization group (RG). Here we demonstrate a practical approach to performing Wilsonian RG in the context of Gaussian Process (GP) Regression. We systematically integrate out the unlearnable modes of the GP kernel, thereby obtaining an RG flow of the GP in which the data sets the IR scale. In simple cases, this results in a universal flow of the ridge parameter, which becomes input-dependent in the richer scenario in which non-Gaussianities are included. In addition to being analytically tractable, this approach goes beyond structural analogies between RG and neural networks by providing a natural connection between RG flow and learnable vs. unlearnable modes. Studying such flows may improve our understanding of feature learning in deep neural networks, and enable us to identify potential universality classes in these models.","authors":["Jessica N. Howard","Ro Jefferson","Anindita Maiti","Zohar Ringel"],"url":"https://arxiv.org/abs/2405.06008"}
{"created":"2025-05-14","title":"Push and Pull: A Framework for Measuring Attentional Agency on Digital Platforms","abstract":"We propose a framework for measuring attentional agency, which we define as a user's ability to allocate attention according to their own desires, goals, and intentions on digital platforms that use statistical learning to prioritize informational content. Such platforms extend people's limited powers of attention by extrapolating their preferences to large collections of previously unconsidered informational objects. However, platforms typically also allow users to influence the attention of other users in various ways. We introduce a formal framework for measuring how much a given platform empowers each user to both pull information into their own attention and push information into the attention of others. We also use these definitions to clarify the implications of generative foundation models and other recent advances in AI for the structure and efficiency of digital platforms. We conclude with a set of possible strategies for better understanding and reshaping attentional agency online.","authors":["Zachary Wojtowicz","Shrey Jain","Nicholas Vincent"],"url":"https://arxiv.org/abs/2405.14614"}
{"created":"2025-05-14","title":"A Prudent Framework for Understanding Risk-Awareness in Demand Response","abstract":"We show that risk-aware behaviors in demand response originate from superquadratic state-dependent cost functions and price uncertainty with skewed distributions. We obtain such results through developing a novel theoretical demand response framework that combines non-anticipatory multi-stage decision-making with superquadratic cost functions. We introduce the concept of prudent demand, defined by a positive third-order derivative of the cost function, which is the first principle for risk-averse behavior despite a risk-neutral objective. Our analysis establishes that future price uncertainty affects immediate consumption decisions, and the extent of this response scales proportionally with the skewness of the price distribution. We visualize our theoretical findings through numerical simulations and illustrate their practical implications using a real-world case study.","authors":["Liudong Chen","Bolun Xu"],"url":"https://arxiv.org/abs/2405.16356"}
{"created":"2025-05-14","title":"PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory Planner","abstract":"In decentralized multiagent trajectory planners, agents need to communicate and exchange their positions to generate collision-free trajectories. However, due to localization errors/uncertainties, trajectory deconfliction can fail even if trajectories are perfectly shared between agents. To address this issue, we first present PARM and PARM*, perception-aware, decentralized, asynchronous multiagent trajectory planners that enable a team of agents to navigate uncertain environments while deconflicting trajectories and avoiding obstacles using perception information. PARM* differs from PARM as it is less conservative, using more computation to find closer-to-optimal solutions. While these methods achieve state-of-the-art performance, they suffer from high computational costs as they need to solve large optimization problems onboard, making it difficult for agents to replan at high rates. To overcome this challenge, we present our second key contribution, PRIMER, a learning-based planner trained with imitation learning (IL) using PARM* as the expert demonstrator. PRIMER leverages the low computational requirements at deployment of neural networks and achieves a computation speed up to 5500 times faster than optimization-based approaches.","authors":["Kota Kondo","Claudius T. Tewari","Andrea Tagliabue","Jesus Tordesillas","Parker C. Lusk","Mason B. Peterson","Jonathan P. How"],"url":"https://arxiv.org/abs/2406.10060"}
{"created":"2025-05-14","title":"Clinically inspired enhance Explainability and Interpretability of an AI-Tool for BCC diagnosis based on expert annotation","abstract":"An AI tool has been developed to provide interpretable support for the diagnosis of BCC via teledermatology, thus speeding up referrals and optimizing resource utilization. The interpretability is provided in two ways: on the one hand, the main BCC dermoscopic patterns are found in the image to justify the BCC/Non BCC classification. Secondly, based on the common visual XAI Grad-CAM, a clinically inspired visual explanation is developed where the relevant features for diagnosis are located. Since there is no established ground truth for BCC dermoscopic features, a standard reference is inferred from the diagnosis of four dermatologists using an Expectation Maximization (EM) based algorithm. The results demonstrate significant improvements in classification accuracy and interpretability, positioning this approach as a valuable tool for early BCC detection and referral to dermatologists. The BCC/non-BCC classification achieved an accuracy rate of 90%. For Clinically-inspired XAI results, the detection of BCC patterns useful to clinicians reaches 99% accuracy. As for the Clinically-inspired Visual XAI results, the mean of the Grad-CAM normalized value within the manually segmented clinical features is 0.57, while outside this region it is 0.16. This indicates that the model struggles to accurately identify the regions of the BCC patterns. These results prove the ability of the AI tool to provide a useful explanation.","authors":["Iv\\'an Matas","Carmen Serrano","Francisca Silva","Amalia Serrano","Tom\\'as Toledo-Pastrana","Bego\\~na Acha"],"url":"https://arxiv.org/abs/2407.00104"}
{"created":"2025-05-14","title":"Designing Value-Centered Consent Interfaces: A Mixed-Methods Approach to Support Patient Values in Data-Sharing Decisions","abstract":"In the digital health domain, ethical data collection practices are crucial for ensuring the availability of quality datasets that drive medical advancement. Data donation, allowing patients to share their medical data for secondary research purposes, presents a promising resource for such datasets. Yet, current consent user interfaces mediating data-sharing decisions are found to favor data collectors' values over those of data subjects. This raises ethical concerns about the use of data collected, as well as concerning the quality of the resulting datasets. Seeking to establish value-centered data collection practices in digital health, we investigate the design of consent user interfaces that support end-users in making value-congruent health data-sharing decisions. Focusing our research efforts on the situated context of health data donation at the psychosomatic unit of a university hospital, we demonstrate how a human-centered design can ground technology within the perspective of a vulnerable group. We employed an exploratory sequential mixed-method approach consisting of five phases: Participatory workshops elicit patient values, informing the design of a proposed Value-Centered Consent Interface. An online experiment demonstrates our interface element's effect, increasing value congruence in data-sharing decisions. Our proposed consent user interface design is then adapted to the research context through a co-creation workshop with domain experts and a user interface evaluation with patients. Our work contributes to recent discourse in CSCW concerning ethical implications of new data practices within their socio-technological context by exploring patient values on medical data-sharing, introducing a novel consent interface leveraging reflection to support value-congruent decision-making, and providing a situated evaluation of the proposed consent user interface with patients.","authors":["David Leimst\\\"adtner","Peter S\\\"orries","Claudia M\\\"uller-Birn"],"url":"https://arxiv.org/abs/2407.03808"}
{"created":"2025-05-14","title":"Efficient nonlocal linear image denoising: Bilevel optimization with Nonequispaced Fast Fourier Transform and matrix-free preconditioning","abstract":"We present a new approach for nonlocal image denoising, based around the application of an unnormalized extended Gaussian ANOVA kernel within a bilevel optimization algorithm. A critical bottleneck when solving such problems for finely-resolved images is the solution of huge-scale, dense linear systems arising from the minimization of an energy term. We tackle this using a Krylov subspace approach, with a Nonequispaced Fast Fourier Transform utilized to approximate matrix-vector products in a matrix-free manner. We accelerate the algorithm using a novel change of basis approach to account for the (known) smallest eigenvalue-eigenvector pair of the matrices involved, coupled with a simple but frequently very effective diagonal preconditioning approach. We present a number of theoretical results concerning the eigenvalues and predicted convergence behavior, and a range of numerical experiments which validate our solvers and use them to tackle parameter learning problems. These demonstrate that very large problems may be effectively and rapidly denoised with very low storage requirements on a computer.","authors":["Andr\\'es Miniguano-Trujillo","John W. Pearson","Benjamin D. Goddard"],"url":"https://arxiv.org/abs/2407.06834"}
{"created":"2025-05-14","title":"Embodying Control in Soft Multistable Robots from Morphofunctional Co-design","abstract":"Soft robots are distinguished by their flexibility and adaptability, allowing them to perform nearly impossible tasks for rigid robots. However, controlling their behavior is challenging due to their nonlinear material response and infinite degrees of freedom. A potential solution to these challenges is to discretize the infinite-dimensional configuration space into a finite but sufficiently large number of functional modes with programmed dynamics. We present a strategy for co-designing the desired tasks and morphology of pneumatically actuated soft robots with multiple encoded stable states and dynamic responses. Our approach introduces a general method to capture the soft robots' response using an energy-based analytical model, the parameters of which are obtained using Recursive Feature Elimination. The resulting lumped-parameter model facilitates inverse co-design of the robot's morphology and planned tasks by embodying specific dynamics upon actuation. We illustrate our approach's ability to explore the configuration space by co-designing kinematics with optimized stiffnesses and time responses to obtain robots capable of classifying the size and weight of objects and displaying adaptable locomotion with minimal feedback control. This strategy offers a framework for simplifying the control of soft robots by exploiting the nonlinear mechanics of multistable structures and embodying mechanical intelligence into soft material systems","authors":["Juan C. Osorio (School of Mechanical Engineering","Purdue University","West Lafayette","USA)","Jhonatan S. Rincon (School of Mechanical Engineering","Purdue University","West Lafayette","USA)","Harith Morgan (School of Mechanical Engineering","Purdue University","West Lafayette","USA)","Andres F. Arrieta (School of Mechanical Engineering","Purdue University","West Lafayette","USA)"],"url":"https://arxiv.org/abs/2407.08111"}
{"created":"2025-05-14","title":"A Close Analysis of the Subset Construction","abstract":"Given a nondeterministic finite-state automaton (NFA), we aim to estimate the size of an equivalent deterministic finite-state automaton (DFA). We demonstrate that computing the state complexity of an NFA within polynomial precision is PSPACE-hard. Furthermore, we also demonstrate that it is PSPACE-hard to decide whether the classical subset construction will yield an equivalent DFA with an exponential increase in the number of states. This result implies that making any a prior estimate of the running time of the subset construction is inherently difficult. To address this, and to enable forecasting of such exponential blow-up in certain special cases, we introduce the notion of subset complexity, which provides an upper bound on the size of the DFA produced by the subset construction. We show that the subset complexity can be efficiently bounded above using the cyclicity and rank of the transition matrices of the NFA. This yields a sufficient condition for identifying NFAs that can be efficiently determinized via the subset construction.","authors":["Ivan Baburin","Ryan Cotterell"],"url":"https://arxiv.org/abs/2407.09891"}
{"created":"2025-05-14","title":"Discriminative and Consistent Representation Distillation","abstract":"Knowledge Distillation (KD) aims to transfer knowledge from a large teacher model to a smaller student model. While contrastive learning has shown promise in self-supervised learning by creating discriminative representations, its application in knowledge distillation remains limited and focuses primarily on discrimination, neglecting the structural relationships captured by the teacher model. To address this limitation, we propose Discriminative and Consistent Distillation (DCD), which employs a contrastive loss along with a consistency regularization to minimize the discrepancy between the distributions of teacher and student representations. Our method introduces learnable temperature and bias parameters that adapt during training to balance these complementary objectives, replacing the fixed hyperparameters commonly used in contrastive learning approaches. Through extensive experiments on CIFAR-100 and ImageNet ILSVRC-2012, we demonstrate that DCD achieves state-of-the-art performance, with the student model sometimes surpassing the teacher's accuracy. Furthermore, we show that DCD's learned representations exhibit superior cross-dataset generalization when transferred to Tiny ImageNet and STL-10.","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"url":"https://arxiv.org/abs/2407.11802"}
{"created":"2025-05-14","title":"Relational Representation Distillation","abstract":"Knowledge distillation involves transferring knowledge from large, cumbersome teacher models to more compact student models. The standard approach minimizes the Kullback-Leibler (KL) divergence between the probabilistic outputs of a teacher and student network. However, this approach fails to capture important structural relationships in the teacher's internal representations. Recent advances have turned to contrastive learning objectives, but these methods impose overly strict constraints through instance-discrimination, forcing apart semantically similar samples even when they should maintain similarity. This motivates an alternative objective by which we preserve relative relationships between instances. Our method employs separate temperature parameters for teacher and student distributions, with sharper student outputs, enabling precise learning of primary relationships while preserving secondary similarities. We show theoretical connections between our objective and both InfoNCE loss and KL divergence. Experiments demonstrate that our method significantly outperforms existing knowledge distillation methods across diverse knowledge transfer tasks, achieving better alignment with teacher models, and sometimes even outperforms the teacher network.","authors":["Nikolaos Giakoumoglou","Tania Stathaki"],"url":"https://arxiv.org/abs/2407.12073"}
{"created":"2025-05-14","title":"Predictive control for nonlinear stochastic systems: Closed-loop guarantees with unbounded noise","abstract":"We present a stochastic model predictive control framework for nonlinear systems subject to unbounded process noise with closed-loop guarantees. First, we provide a conceptual shrinking-horizon framework that utilizes general probabilistic reachable sets and minimizes the expected cost. Then, we provide a tractable receding-horizon formulation that uses a nominal state to minimize a deterministic quadratic cost and satisfy tightened constraints. Our theoretical analysis demonstrates recursive feasibility, satisfaction of chance constraints, and bounds on the expected cost for the resulting closed-loop system. We provide a constructive design for probabilistic reachable sets of nonlinear continuously differentiable systems using stochastic contraction metrics and an assumed bound on the covariance matrices. Numerical simulations highlight the computational efficiency and theoretical guarantees of the proposed method. Overall, this paper provides a framework for computationally tractable stochastic predictive control with closed-loop guarantees for nonlinear systems with unbounded noise.","authors":["Johannes K\\\"ohler","Melanie N. Zeilinger"],"url":"https://arxiv.org/abs/2407.13257"}
{"created":"2025-05-14","title":"Bellman Unbiasedness: Toward Provably Efficient Distributional Reinforcement Learning with General Value Function Approximation","abstract":"Distributional reinforcement learning improves performance by capturing environmental stochasticity, but a comprehensive theoretical understanding of its effectiveness remains elusive. In addition, the intractable element of the infinite dimensionality of distributions has been overlooked. In this paper, we present a regret analysis of distributional reinforcement learning with general value function approximation in a finite episodic Markov decision process setting. We first introduce a key notion of $\\textit{Bellman unbiasedness}$ which is essential for exactly learnable and provably efficient distributional updates in an online manner. Among all types of statistical functionals for representing infinite-dimensional return distributions, our theoretical results demonstrate that only moment functionals can exactly capture the statistical information. Secondly, we propose a provably efficient algorithm, $\\texttt{SF-LSVI}$, that achieves a tight regret bound of $\\tilde{O}(d_E H^{\\frac{3}{2}}\\sqrt{K})$ where $H$ is the horizon, $K$ is the number of episodes, and $d_E$ is the eluder dimension of a function class.","authors":["Taehyun Cho","Seungyub Han","Seokhun Ju","Dohyeong Kim","Kyungjae Lee","Jungwoo Lee"],"url":"https://arxiv.org/abs/2407.21260"}
{"created":"2025-05-14","title":"Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions","abstract":"Traditional knowledge graph completion (KGC) methods rely solely on structural information, struggling with the inherent sparsity of knowledge graphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large corpora with powerful context modeling, making them promising for mitigating the limitations of previous methods. Directly fine-tuning LLMs offers great capability but comes at the cost of huge time and memory consumption, while utilizing frozen LLMs yields suboptimal results.In this work, we aim to leverage LLMs for KGC effectively and efficiently. We capture the context-aware hidden states of knowledge triples by employing prompts to stimulate the intermediate layers of LLMs. We then train a data-efficient classifier on these hidden states to harness the inherent capabilities of frozen LLMs in KGC. Additionally, to reduce ambiguity and enrich knowledge representation, we generate detailed entity descriptions through subgraph sampling on KGs. Extensive experiments on standard benchmarks demonstrate the efficiency and effectiveness of our approach. We outperform traditional KGC methods across most datasets and, notably, achieve classification performance comparable to fine-tuned LLMs while enhancing GPU memory efficiency by $188\\times$ and accelerating training and inference by $13.48\\times$.","authors":["Bo Xue","Yi Xu","Yunchong Song","Yiming Pang","Yuyang Ren","Jiaxin Ding","Luoyi Fu","Xinbing Wang"],"url":"https://arxiv.org/abs/2408.06787"}
{"created":"2025-05-14","title":"Studying the Effects of Collaboration in Interactive Theme Discovery Systems","abstract":"NLP-assisted solutions have gained considerable traction to support qualitative data analysis. However, there does not exist a unified evaluation framework that can account for the many different settings in which qualitative researchers may employ them. In this paper, we take a first step in this direction by proposing an evaluation framework to study the way in which different tools may result in different outcomes depending on the collaboration strategy employed. Specifically, we study the impact of synchronous vs. asynchronous collaboration using two different NLP-assisted qualitative research tools and present a comprehensive analysis of significant differences in the consistency, cohesiveness, and correctness of their outputs.","authors":["Alvin Po-Chun Chen","Dananjay Srinivas","Alexandra Barry","Maksim Seniw","Maria Leonor Pacheco"],"url":"https://arxiv.org/abs/2408.09030"}
{"created":"2025-05-14","title":"Shape Space Spectra","abstract":"Eigenanalysis of differential operators, such as the Laplace operator or elastic energy Hessian, is typically restricted to a single shape and its discretization, limiting reduced order modeling (ROM). We introduce the first eigenanalysis method for continuously parameterized shape families. Given a parametric shape, our method constructs spatial neural fields that represent eigenfunctions across the entire shape space. It is agnostic to the specific shape representation, requiring only an inside/outside indicator function that depends on shape parameters. Eigenfunctions are computed by minimizing a variational principle over nested spaces with orthogonality constraints. Since eigenvalues may swap dominance at points of multiplicity, we jointly train multiple eigenfunctions while dynamically reordering them based on their eigenvalues at each step. Through causal gradient filtering, this reordering is reflected in backpropagation. Our method enables applications to operate over shape space, providing a single ROM that encapsulates vibration modes for all shapes, including previously unseen ones. Since our eigenanalysis is differentiable with respect to shape parameters, it facilitates eigenfunction-aware shape optimization. We evaluate our approach on shape optimization for sound synthesis and locomotion, as well as reduced-order modeling for elastodynamic simulation.","authors":["Yue Chang","Otman Benchekroun","Maurizio M. Chiaramonte","Peter Yichen Chen","Eitan Grinspun"],"url":"https://arxiv.org/abs/2408.10099"}
{"created":"2025-05-14","title":"Task-level Distributionally Robust Optimization for Large Language Model-based Dense Retrieval","abstract":"Large Language Model-based Dense Retrieval (LLM-DR) optimizes over numerous heterogeneous fine-tuning collections from different domains. However, the discussion about its training data distribution is still minimal. Previous studies rely on empirically assigned dataset choices or sampling ratios, which inevitably lead to sub-optimal retrieval performances. In this paper, we propose a new task-level Distributionally Robust Optimization (tDRO) algorithm for LLM-DR fine-tuning, targeted at improving the universal domain generalization ability by end-to-end reweighting the data distribution of each task. The tDRO parameterizes the domain weights and updates them with scaled domain gradients. The optimized weights are then transferred to the LLM-DR fine-tuning to train more robust retrievers. Experiments show optimal improvements in large-scale retrieval benchmarks and reduce up to 30% dataset usage after applying our optimization algorithm with a series of different-sized LLM-DR models.","authors":["Guangyuan Ma","Yongliang Ma","Xing Wu","Zhenpeng Su","Ming Zhou","Songlin Hu"],"url":"https://arxiv.org/abs/2408.10613"}
{"created":"2025-05-14","title":"Boolean basis, formula size, and number of modal operators","abstract":"Is it possible to write significantly smaller formulae when using Boolean operators other than those of the De Morgan basis (and, or, not, and the constants)? For propositional logic, a negative answer was given by Pratt: formulae over one set of operators can always be translated into an equivalent formula over any other complete set of operators with only polynomial increase in size. Surprisingly, for modal logic the picture is different: we show that elimination of bi-implication is only possible at the cost of an exponential number of occurrences of the modal operator $\\lozenge$ and therefore of an exponential increase in formula size, i.e., the De Morgan basis and its extension by bi-implication differ in succinctness. Moreover, we prove that any complete set of Boolean operators agrees in succinctness with the De Morgan basis or with its extension by bi-implication. More precisely, these results are shown for the modal logic $\\mathrm{T}$ (and therefore for $\\mathrm{K}$). We complement them showing that the modal logic $\\mathrm{S5}$ behaves as propositional logic: the choice of Boolean operators has no significant impact on the size of formulae.","authors":["Christoph Berkholz","Dietrich Kuske","Christian Schwarz"],"url":"https://arxiv.org/abs/2408.11651"}
{"created":"2025-05-14","title":"S-EPOA: Overcoming the Indistinguishability of Segments with Skill-Driven Preference-Based Reinforcement Learning","abstract":"Preference-based reinforcement learning (PbRL) stands out by utilizing human preferences as a direct reward signal, eliminating the need for intricate reward engineering. However, despite its potential, traditional PbRL methods are often constrained by the indistinguishability of segments, which impedes the learning process. In this paper, we introduce Skill-Enhanced Preference Optimization Algorithm (S-EPOA), which addresses the segment indistinguishability issue by integrating skill mechanisms into the preference learning framework. Specifically, we first conduct the unsupervised pretraining to learn useful skills. Then, we propose a novel query selection mechanism to balance the information gain and distinguishability over the learned skill space. Experimental results on a range of tasks, including robotic manipulation and locomotion, demonstrate that S-EPOA significantly outperforms conventional PbRL methods in terms of both robustness and learning efficiency. The results highlight the effectiveness of skill-driven learning in overcoming the challenges posed by segment indistinguishability.","authors":["Ni Mu","Yao Luan","Yiqin Yang","Bo Xu","Qing-shan Jia"],"url":"https://arxiv.org/abs/2408.12130"}
{"created":"2025-05-14","title":"Training Ultra Long Context Language Model with Fully Pipelined Distributed Transformer","abstract":"Large Language Models (LLMs) with long context capabilities are integral to complex tasks in natural language processing and computational biology, such as text generation and protein sequence analysis. However, training LLMs directly on extremely long contexts demands considerable GPU resources and increased memory, leading to higher costs and greater complexity. Alternative approaches that introduce long context capabilities via downstream finetuning or adaptations impose significant design limitations. In this paper, we propose Fully Pipelined Distributed Transformer (FPDT) for efficiently training long-context LLMs with extreme hardware efficiency. For GPT and Llama models, we achieve a 16x increase in sequence length that can be trained on the same hardware compared to current state-of-the-art solutions. With our dedicated sequence chunk pipeline design, we can now train 8B LLM with 2 million sequence length on only 4 GPUs, while also maintaining over 55% of MFU. Our proposed FPDT is agnostic to existing training techniques and is proven to work efficiently across different LLM models.","authors":["Jinghan Yao","Sam Ade Jacobs","Masahiro Tanaka","Olatunji Ruwase","Hari Subramoni","Dhabaleswar K. Panda"],"url":"https://arxiv.org/abs/2408.16978"}
{"created":"2025-05-14","title":"Geometry-Aware Feature Matching for Large-Scale Structure from Motion","abstract":"Establishing consistent and dense correspondences across multiple images is crucial for Structure from Motion (SfM) systems. Significant view changes, such as air-to-ground with very sparse view overlap, pose an even greater challenge to the correspondence solvers. We present a novel optimization-based approach that significantly enhances existing feature matching methods by introducing geometry cues in addition to color cues. This helps fill gaps when there is less overlap in large-scale scenarios. Our method formulates geometric verification as an optimization problem, guiding feature matching within detector-free methods and using sparse correspondences from detector-based methods as anchor points. By enforcing geometric constraints via the Sampson Distance, our approach ensures that the denser correspondences from detector-free methods are geometrically consistent and more accurate. This hybrid strategy significantly improves correspondence density and accuracy, mitigates multi-view inconsistencies, and leads to notable advancements in camera pose accuracy and point cloud density. It outperforms state-of-the-art feature matching methods on benchmark datasets and enables feature matching in challenging extreme large-scale settings.","authors":["Gonglin Chen","Jinsen Wu","Haiwei Chen","Wenbin Teng","Zhiyuan Gao","Andrew Feng","Rongjun Qin","Yajie Zhao"],"url":"https://arxiv.org/abs/2409.02310"}
{"created":"2025-05-14","title":"From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks","abstract":"To reduce the need for human annotations, large language models (LLMs) have been proposed as judges of the quality of other candidate models. The performance of LLM judges is typically evaluated by measuring the correlation with human judgments on generative tasks such as summarization or machine translation. In contrast, we study LLM judges on mathematical reasoning tasks. These tasks require multi-step reasoning, and the correctness of their solutions is verifiable, enabling a more objective evaluation. We perform a detailed performance analysis and find that easy samples are easy to judge, and difficult samples are difficult to judge. Our analysis uncovers a strong correlation between judgment performance and the candidate model task performance, indicating that judges tend to favor higher-quality models even if their answer is incorrect. As a consequence, we test whether we can predict the behavior of LLM judges using simple features such as part-of-speech tags and find that we can correctly predict 70%-75% of judgments. We conclude this study by analyzing practical use cases, showing that LLM judges consistently detect the on-average better model but largely fail if we use them to improve task performance.","authors":["Andreas Stephan","Dawei Zhu","Matthias A{\\ss}enmacher","Xiaoyu Shen","Benjamin Roth"],"url":"https://arxiv.org/abs/2409.04168"}
{"created":"2025-05-14","title":"A Canonical Gauge for Computing of Eigenpairs of the Magnetic Schr\\\"odinger Operator","abstract":"We consider the eigenvalue problem for the magnetic Schr\\\"odinger operator and take advantage of a property called gauge invariance to transform the given problem into an equivalent problem that is more amenable to numerical approximation. More specifically, we propose a canonical magnetic gauge that can be computed by solving a Poisson problem, that yields a new operator having the same spectrum but eigenvectors that are less oscillatory. Extensive numerical tests demonstrate that accurate computation of eigenpairs can be done more efficiently and stably with the canonical magnetic gauge.","authors":["Jeffrey S. Ovall","Li Zhu"],"url":"https://arxiv.org/abs/2409.06023"}
{"created":"2025-05-14","title":"The Impact of Large Language Models on Open-source Innovation: Evidence from GitHub Copilot","abstract":"Large Language Models (LLMs) have been shown to enhance individual productivity in guided settings. Whereas LLMs are likely to also transform innovation processes in a collaborative work setting, it is unclear what trajectory this transformation will follow. Innovation in these contexts encompasses both capability innovation that explores new possibilities by acquiring new competencies in a project and iterative innovation that exploits existing foundations by enhancing established competencies and improving project quality. Whether LLMs affect these two aspects of collaborative work and to what extent is an open empirical question. Open-source development provides an ideal setting to examine LLM impacts on these innovation types, as its voluntary and open/collaborative nature of contributions provides the greatest opportunity for technological augmentation. We focus on open-source projects on GitHub by leveraging a natural experiment around the selective rollout of GitHub Copilot (a programming-focused LLM) in October 2021, where GitHub Copilot selectively supported programming languages like Python or Rust, but not R or Haskell. We observe a significant jump in overall contributions, suggesting that LLMs effectively augment collaborative innovation in an unguided setting. Interestingly, Copilot's launch increased iterative innovation focused on maintenance-related or feature-refining contributions significantly more than it did capability innovation through code-development or feature-introducing commits. This disparity was more pronounced after the model upgrade in June 2022 and was evident in active projects with extensive coding activity, suggesting that as both LLM capabilities and/or available contextual information improve, the gap between capability and iterative innovation may widen. We discuss practical and policy implications to incentivize high-value innovative solutions.","authors":["Doron Yeverechyahu","Raveesh Mayya","Gal Oestreicher-Singer"],"url":"https://arxiv.org/abs/2409.08379"}
{"created":"2025-05-14","title":"TinyVLA: Towards Fast, Data-Efficient Vision-Language-Action Models for Robotic Manipulation","abstract":"Vision-Language-Action (VLA) models have shown remarkable potential in visuomotor control and instruction comprehension through end-to-end learning processes. However, current VLA models face significant challenges: they are slow during inference and require extensive pre-training on large amounts of robotic data, making real-world deployment difficult. In this paper, we introduce a new family of compact vision-language-action models, called TinyVLA, which offers two key advantages over existing VLA models: (1) faster inference speeds, and (2) improved data efficiency, eliminating the need for pre-training stage. Our framework incorporates two essential components to build TinyVLA: (1) initializing the policy backbone with robust, high-speed multimodal models, and (2) integrating a diffusion policy decoder during fine-tuning to enable precise robot actions. We conducted extensive evaluations of TinyVLA in both simulation and on real robots, demonstrating that our approach significantly outperforms the state-of-the-art VLA model, OpenVLA, in terms of speed and data efficiency, while delivering comparable or superior performance. Additionally, TinyVLA exhibits strong generalization capabilities across various dimensions, including language instructions, novel objects, unseen positions, changes in object appearance, background variations, and environmental shifts, often matching or exceeding the performance of OpenVLA. We believe that \\methodname offers an interesting perspective on utilizing pre-trained multimodal models for policy learning. Our project is at https://tiny-vla.github.io.","authors":["Junjie Wen","Yichen Zhu","Jinming Li","Minjie Zhu","Kun Wu","Zhiyuan Xu","Ning Liu","Ran Cheng","Chaomin Shen","Yaxin Peng","Feifei Feng","Jian Tang"],"url":"https://arxiv.org/abs/2409.12514"}
{"created":"2025-05-14","title":"ERPoT: Effective and Reliable Pose Tracking for Mobile Robots Using Lightweight Polygon Maps","abstract":"This paper presents an effective and reliable pose tracking solution, termed ERPoT, for mobile robots operating in large-scale outdoor and challenging indoor environments, underpinned by an innovative prior polygon map. Especially, to overcome the challenge that arises as the map size grows with the expansion of the environment, the novel form of a prior map composed of multiple polygons is proposed. Benefiting from the use of polygons to concisely and accurately depict environmental occupancy, the prior polygon map achieves long-term reliable pose tracking while ensuring a compact form. More importantly, pose tracking is carried out under pure LiDAR mode, and the dense 3D point cloud is transformed into a sparse 2D scan through ground removal and obstacle selection. On this basis, a novel cost function for pose estimation through point-polygon matching is introduced, encompassing two distinct constraint forms: point-to-vertex and point-to-edge. In this study, our primary focus lies on two crucial aspects: lightweight and compact prior map construction, as well as effective and reliable robot pose tracking. Both aspects serve as the foundational pillars for future navigation across diverse mobile platforms equipped with different LiDAR sensors in varied environments. Comparative experiments based on the publicly available datasets and our self-recorded datasets are conducted, and evaluation results show the superior performance of ERPoT on reliability, prior map size, pose estimation error, and runtime over the other six approaches. The corresponding code can be accessed at https://github.com/ghm0819/ERPoT, and the supplementary video is at https://youtu.be/cseml5FrW1Q.","authors":["Haiming Gao","Qibo Qiu","Hongyan Liu","Dingkun Liang","Chaoqun Wang","Xuebo Zhang"],"url":"https://arxiv.org/abs/2409.14723"}
{"created":"2025-05-14","title":"Vulnerabilities that arise from poor governance in Distributed Ledger Technologies","abstract":"Distributed Ledger Technologies (DLTs) promise decentralization, transparency, and security, yet the reality often falls short due to fundamental governance flaws. Poorly designed governance frameworks leave these systems vulnerable to coercion, vote-buying, centralization of power, and malicious protocol exploits: threats that undermine the very principles of fairness and equity these technologies seek to uphold. This paper surveys the state of DLT governance, identifies critical vulnerabilities, and highlights the absence of universally accepted best practices for good governance. By bridging insights from cryptography, social choice theory, and e-voting systems, we not only present a comprehensive taxonomy of governance properties essential for safeguarding DLTs but also point to technical solutions that can deliver these properties in practice. This work underscores the urgent need for robust, transparent, and enforceable governance mechanisms. Ensuring good governance is not merely a technical necessity but a societal imperative to protect the public interest, maintain trust, and realize the transformative potential of DLTs for social good.","authors":["Aida Manzano Kharman","William Sanders"],"url":"https://arxiv.org/abs/2409.15947"}
{"created":"2025-05-14","title":"Multiscale method for image denoising using nonlinear diffusion process: local denoising and spectral multiscale basis functions","abstract":"We consider image denoising using a nonlinear diffusion process, where we solve unsteady partial differential equations with nonlinear coefficients. The noised image is given as an initial condition, and nonlinear coefficients are used to preserve the main image features. In this paper, we present a multiscale method for the resulting nonlinear parabolic equation in order to construct an efficient solver. To both filter out noise and preserve essential image features during the denoising process, we utilize a time-dependent nonlinear diffusion model. Here, the noised image is fed as an initial condition and the denoised image is stimulated with given parameters. We numerically implement this model by constructing a discrete system for a given image resolution using a finite volume method and employing an implicit time approximation scheme to avoid time-step restriction. However, the resulting discrete system size is proportional to the number of pixels which leads to computationally expensive numerical algorithms for high-resolution images. In order to reduce the size of the system and construct efficient computational algorithms, we construct a coarse-resolution representation of the system. We incorporate local noise reduction in the coarsening process to construct an efficient algorithm with fewer denoising iterations. We propose a computational approach with two main ingredients: (1) performing local image denoising in each local domain of basis support; and (2) constructing multiscale basis functions to construct a coarse resolution representation by a Galerkin coupling. We present numerical results for several classic and high-resolution image datasets to demonstrate the effectiveness of the proposed multiscale approach with local denoising and local multiscale representation.","authors":["Maria Vasilyeva","Aleksei Krasnikov","Kelum Gajamannage","Mehrube Mehrubeoglu"],"url":"https://arxiv.org/abs/2409.15952"}
{"created":"2025-05-14","title":"Perfectly to a Tee: Understanding User Perceptions of Personalized LLM-Enhanced Narrative Interventions","abstract":"Stories about overcoming personal struggles can effectively illustrate the application of psychological theories in real life, yet they may fail to resonate with individuals' experiences. In this work, we employ large language models (LLMs) to create tailored narratives that acknowledge and address unique challenging thoughts and situations faced by individuals. Our study, involving 346 young adults across two settings, demonstrates that personalized LLM-enhanced stories were perceived to be better than human-written ones in conveying key takeaways, promoting reflection, and reducing belief in negative thoughts. These stories were not only seen as more relatable but also similarly authentic to human-written ones, highlighting the potential of LLMs in helping young adults manage their struggles. The findings of this work provide crucial design considerations for future narrative-based digital mental health interventions, such as the need to maintain relatability without veering into implausibility and refining the wording and tone of AI-enhanced content.","authors":["Ananya Bhattacharjee","Sarah Yi Xu","Pranav Rao","Yuchen Zeng","Jonah Meyerhoff","Syed Ishtiaque Ahmed","David C Mohr","Michael Liut","Alex Mariakakis","Rachel Kornfield","Joseph Jay Williams"],"url":"https://arxiv.org/abs/2409.16732"}
{"created":"2025-05-14","title":"Hierarchical Tri-manual Planning for Vision-assisted Fruit Harvesting with Quadrupedal Robots","abstract":"This paper addresses the challenge of developing a multi-arm quadrupedal robot capable of efficiently harvesting fruit in complex, natural environments. To overcome the inherent limitations of traditional bimanual manipulation, we introduce the first three-arm quadrupedal robot LocoHarv-3 and propose a novel hierarchical tri-manual planning approach, enabling automated fruit harvesting with collision-free trajectories. Our comprehensive semi-autonomous framework integrates teleoperation, supported by LiDAR-based odometry and mapping, with learning-based visual perception for accurate fruit detection and pose estimation. Validation is conducted through a series of controlled indoor experiments using motion capture and extensive field tests in natural settings. Results demonstrate a 90\\% success rate in in-lab settings with a single attempt, and field trials further verify the system's robustness and efficiency in more challenging real-world environments.","authors":["Zhichao Liu","Jingzong Zhou","Konstantinos Karydis"],"url":"https://arxiv.org/abs/2409.17116"}
{"created":"2025-05-14","title":"Polynomial Universes in Homotopy Type Theory","abstract":"Awodey, later with Newstead, showed how polynomial functors with extra structure (termed ``natural models'') hold within them the categorical semantics for dependent type theory. Their work presented these ideas clearly but ultimately led them outside of the usual category of polynomial functors to a particular \\emph{tricategory} of polynomials in order to explain all of the structure possessed by such models. This paper builds off that work -- explicating the categorical semantics of dependent type theory by axiomatizing them entirely in terms of the usual category of polynomial functors. In order to handle the higher-categorical coherences required for such an explanation, we work with polynomial functors in the language of Homotopy Type Theory (HoTT), which allows for higher-dimensional structures to be expressed purely within this category. The move to HoTT moreover enables us to express a key additional condition on polynomial functors -- \\emph{univalence} -- which is sufficient to guarantee that models of type theory expressed as univalent polynomials satisfy all higher coherences of their corresponding algebraic structures, purely in virtue of being closed under the usual constructors of dependent type theory. We call polynomial functors satisfying this condition \\emph{polynomial universes}. As an example of the simplification to the theory of natural models this enables, we highlight the fact that a polynomial universe being closed under dependent product types implies the existence of a distributive law of monads, which witnesses the usual distributivity of dependent products over dependent sums.","authors":["C. B. Aberl\\'e","David I. Spivak"],"url":"https://arxiv.org/abs/2409.19176"}
{"created":"2025-05-14","title":"Learning with Less: Optimizing Tactile Sensor Configurations for Dexterous Manipulation","abstract":"Tactile sensing is critical for learning-based robotic dexterous manipulation, enabling real-time force perception, slip detection, and grip adjustments during interactions. While full-hand sensor arrays provide precise control, their deployment is limited by high costs, complex integration, and significant computational demands. Practical constraints, including limited space and the complexity of the wiring, further restrict the use of the entire sensor. Consequently, optimizing sensor configurations to achieve efficient coverage and good performance using fewer sensors remains a significant and open research challenge.In this work, we investigate the influence of tactile sensor quantity and placement on a robotic hand for dexterous manipulation tasks. Through systematic analysis of various sensor configurations, an optimized layout with only 21 sensors is identified, achieving over 93% of the task success rate relative to full-hand coverage (92 sensors). This configuration reduces the sensor count by 77% and offers a considerable reduction in integration costs, demonstrating a cost-effective yet high-performing tactile sensing strategy. Additionally, we develop a multi-factor regression model to predict task success rate under arbitrary sensor configurations. The model achieves strong generalization, with an average prediction error of 3.12% on unseen manipulation tasks. These results offer a scalable framework for deploying tactile sensing in real-world robotic manipulation systems.","authors":["Haoran Guo","Haoyang Wang","Zhengxiong Li","He Bai","Lingfeng Tao"],"url":"https://arxiv.org/abs/2409.20473"}
{"created":"2025-05-14","title":"HarmoniCa: Harmonizing Training and Inference for Better Feature Caching in Diffusion Transformer Acceleration","abstract":"Diffusion Transformers (DiTs) excel in generative tasks but face practical deployment challenges due to high inference costs. Feature caching, which stores and retrieves redundant computations, offers the potential for acceleration. Existing learning-based caching, though adaptive, overlooks the impact of the prior timestep. It also suffers from misaligned objectives--aligned predicted noise vs. high-quality images--between training and inference. These two discrepancies compromise both performance and efficiency. To this end, we harmonize training and inference with a novel learning-based caching framework dubbed HarmoniCa. It first incorporates Step-Wise Denoising Training (SDT) to ensure the continuity of the denoising process, where prior steps can be leveraged. In addition, an Image Error Proxy-Guided Objective (IEPO) is applied to balance image quality against cache utilization through an efficient proxy to approximate the image error. Extensive experiments across $8$ models, $4$ samplers, and resolutions from $256\\times256$ to $2K$ demonstrate superior performance and speedup of our framework. For instance, it achieves over $40\\%$ latency reduction (i.e., $2.07\\times$ theoretical speedup) and improved performance on PixArt-$\\alpha$. Remarkably, our image-free approach reduces training time by $25\\%$ compared with the previous method. Our code is available at https://github.com/ModelTC/HarmoniCa.","authors":["Yushi Huang","Zining Wang","Ruihao Gong","Jing Liu","Xinjie Zhang","Jinyang Guo","Xianglong Liu","Jun Zhang"],"url":"https://arxiv.org/abs/2410.01723"}
{"created":"2025-05-14","title":"An Efficient On-Policy Deep Learning Framework for Stochastic Optimal Control","abstract":"We present a novel on-policy algorithm for solving stochastic optimal control (SOC) problems. By leveraging the Girsanov theorem, our method directly computes on-policy gradients of the SOC objective without expensive backpropagation through stochastic differential equations or adjoint problem solutions. This approach significantly accelerates the optimization of neural network control policies while scaling efficiently to high-dimensional problems and long time horizons. We evaluate our method on classical SOC benchmarks as well as applications to sampling from unnormalized distributions via Schr\\\"odinger-F\\\"ollmer processes and fine-tuning pre-trained diffusion models. Experimental results demonstrate substantial improvements in both computational speed and memory efficiency compared to existing approaches.","authors":["Mengjian Hua","Mathieu Lauri\\`ere","Eric Vanden-Eijnden"],"url":"https://arxiv.org/abs/2410.05163"}
{"created":"2025-05-14","title":"Early-Cycle Internal Impedance Enables ML-Based Battery Cycle Life Predictions Across Manufacturers","abstract":"Predicting the end-of-life (EOL) of lithium-ion batteries across different manufacturers presents significant challenges due to variations in electrode materials, manufacturing processes, cell formats, and a lack of generally available data. Methods that construct features solely on voltage-capacity profile data typically fail to generalize across cell chemistries. This study introduces a methodology that combines traditional voltage-capacity features with Direct Current Internal Resistance (DCIR) measurements, enabling more accurate and generalizable EOL predictions. The use of early-cycle DCIR data captures critical degradation mechanisms related to internal resistance growth, enhancing model robustness. Models are shown to successfully predict the number of cycles to EOL for unseen manufacturers of varied electrode composition with a mean absolute error (MAE) of 150 cycles. This cross-manufacturer generalizability reduces the need for extensive new data collection and retraining, enabling manufacturers to optimize new battery designs using existing datasets. Additionally, a novel DCIR-compatible dataset is released as part of ongoing efforts to enrich the growing ecosystem of cycling data and accelerate battery materials development.","authors":["Tyler Sours","Shivang Agarwal","Marc Cormier","Jordan Crivelli-Decker","Steffen Ridderbusch","Stephen L. Glazier","Connor P. Aiken","Aayush R. Singh","Ang Xiao","Omar Allam"],"url":"https://arxiv.org/abs/2410.05326"}
{"created":"2025-05-14","title":"Round and Round We Go! What makes Rotary Positional Encodings useful?","abstract":"Positional Encodings (PEs) are a critical component of Transformer-based Large Language Models (LLMs), providing the attention mechanism with important sequence-position information. One of the most popular types of encoding used today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries and keys based on their relative distance. A common belief is that RoPE is useful because it helps to decay token dependency as relative distance increases. In this work, we argue that this is unlikely to be the core reason. We study the internals of a trained Gemma 7B model to understand how RoPE is being used at a mechanical level. We find that Gemma learns to use RoPE to construct robust \"positional\" attention patterns by exploiting the highest frequencies. We also find that, in general, Gemma greatly prefers to use the lowest frequencies of RoPE, which we suspect are used to carry semantic information. We mathematically prove interesting behaviours of RoPE and conduct experiments to verify our findings, proposing a modification of RoPE that fixes some highlighted issues and improves performance. We believe that this work represents an interesting step in better understanding PEs in LLMs, which we believe holds crucial value for scaling LLMs to large sizes and context lengths.","authors":["Federico Barbero","Alex Vitvitskyi","Christos Perivolaropoulos","Razvan Pascanu","Petar Veli\\v{c}kovi\\'c"],"url":"https://arxiv.org/abs/2410.06205"}
{"created":"2025-05-14","title":"CursorCore: Assist Programming through Aligning Anything","abstract":"Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at https://github.com/TechxGenus/CursorCore.","authors":["Hao Jiang","Qi Liu","Rui Li","Shengyu Ye","Shijin Wang"],"url":"https://arxiv.org/abs/2410.07002"}
{"created":"2025-05-14","title":"MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation","abstract":"Existing Multimodal Large Language Model (MLLM)-based agents face significant challenges in handling complex GUI (Graphical User Interface) interactions on devices. These challenges arise from the dynamic and structured nature of GUI environments, which integrate text, images, and spatial relationships, as well as the variability in action spaces across different pages and tasks. To address these limitations, we propose MobA, a novel MLLM-based mobile assistant system. MobA introduces an adaptive planning module that incorporates a reflection mechanism for error recovery and dynamically adjusts plans to align with the real environment contexts and action module's execution capacity. Additionally, a multifaceted memory module provides comprehensive memory support to enhance adaptability and efficiency. We also present MobBench, a dataset designed for complex mobile interactions. Experimental results on MobBench and AndroidArena demonstrate MobA's ability to handle dynamic GUI environments and perform complex mobile tasks.","authors":["Zichen Zhu","Hao Tang","Yansi Li","Dingye Liu","Hongshen Xu","Kunyao Lan","Danyang Zhang","Yixuan Jiang","Hao Zhou","Chenrun Wang","Situo Zhang","Liangtai Sun","Yixiao Wang","Yuheng Sun","Lu Chen","Kai Yu"],"url":"https://arxiv.org/abs/2410.13757"}
{"created":"2025-05-14","title":"TradExpert: Revolutionizing Trading with Mixture of Expert LLMs","abstract":"The integration of Artificial Intelligence (AI) in the financial domain has opened new avenues for quantitative trading, particularly through the use of Large Language Models (LLMs). However, the challenge of effectively synthesizing insights from diverse data sources and integrating both structured and unstructured data persists. This paper presents TradeExpert, a novel framework that employs a mix of experts (MoE) approach, using four specialized LLMs, each analyzing distinct sources of financial data, including news articles, market data, alpha factors, and fundamental data. The insights of these expert LLMs are further synthesized by a General Expert LLM to make a final prediction or decision. With specific prompts, TradeExpert can be switched between the prediction mode and the ranking mode for stock movement prediction and quantitative stock trading, respectively. In addition to existing benchmarks, we also release a large-scale financial dataset to comprehensively evaluate TradeExpert's effectiveness. Our experimental results demonstrate TradeExpert's superior performance across all trading scenarios.","authors":["Qianggang Ding","Haochen Shi","Jiadong Guo","Bang Liu"],"url":"https://arxiv.org/abs/2411.00782"}
{"created":"2025-05-14","title":"Consensus Building in Human-robot Co-learning via Bias Controlled Nonlinear Opinion Dynamics and Non-verbal Communication through Robotic Eyes","abstract":"Consensus between humans and robots is crucial as robotic agents become more prevalent and deeply integrated into our daily lives. This integration presents both unprecedented opportunities and notable challenges for effective collaboration. However, the active guidance of human actions and their integration in co-learning processes, where humans and robots mutually learn from each other, remains under-explored. This article demonstrates how consensus between human and robot opinions can be established by modeling decision-making processes as non-linear opinion dynamics. We utilize dynamic bias as a control parameter to steer the robot's opinion toward consensus and employ visual cues via a robotic eye gaze to guide human decisions. These non-verbal cues communicate the robot's future intentions, gradually guiding human decisions to align with them. To design robot behavior for consensus, we integrate a human opinion observation algorithm with the robot's opinion formation, controlling its actions based on that formed opinion. Experiments with $51$ participants ($N=51$) in a two-choice decision-making task show that effective consensus and trust can be established in a human--robot co-learning setting by guiding human decisions through nonverbal robotic cues and using bias-controlled opinion dynamics to shape robot behavior. Finally, we provide detailed information on the perceived cognitive load and the behavior of robotic eyes based on user feedback and post-experiment interviews.","authors":["Rajul Kumar","Adam Bhatti","Ningshi Yao"],"url":"https://arxiv.org/abs/2411.03581"}
{"created":"2025-05-14","title":"LSHBloom: Memory-efficient, Extreme-scale Document Deduplication","abstract":"Deduplication is a major focus for assembling and curating training datasets for large language models (LLM) -- detecting and eliminating additional instances of the same content -- in large collections of technical documents. Unrestrained, duplicates in the training dataset increase training costs and lead to undesirable properties such as memorization in trained models or cheating on evaluation. Contemporary approaches to document-level deduplication are often extremely expensive in both runtime and memory. We propose LSHBloom, an extension to MinhashLSH, which replaces the expensive LSHIndex with lightweight Bloom filters. LSHBloom demonstrates the same deduplication performance as MinhashLSH with only a marginal increase in false positives (as low as 1e-5 in our experiments); demonstrates competitive runtime (270\\% faster than MinhashLSH on peS2o); and, crucially, uses just 0.6\\% of the disk space required by MinhashLSH to deduplicate peS2o. We demonstrate that this space advantage scales with increased dataset size -- at the extreme scale of several billion documents, LSHBloom promises a 250\\% speedup and a 54$\\times$ space advantage over traditional MinHashLSH scaling deduplication of text datasets to many billions of documents.","authors":["Arham Khan","Robert Underwood","Carlo Siebenschuh","Yadu Babuji","Aswathy Ajith","Kyle Hippe","Ozan Gokdemir","Alexander Brace","Kyle Chard","Ian Foster"],"url":"https://arxiv.org/abs/2411.04257"}
{"created":"2025-05-14","title":"Legacy Procurement Practices Shape How U.S. Cities Govern AI: Understanding Government Employees' Practices, Challenges, and Needs","abstract":"Most AI tools adopted by governments are not developed internally, but instead are acquired from third-party vendors in a process called public procurement. In this paper, we conduct the first empirical study of how United States cities' procurement practices shape critical decisions surrounding public sector AI. We conduct semi-structured interviews with 19 city employees who oversee AI procurement across 7 U.S. cities. We found that cities' legacy procurement practices, which are shaped by decades-old laws and norms, establish infrastructure that determines which AI is purchased, and which actors hold decision-making power over procured AI. We characterize the emerging actions cities have taken to adapt their purchasing practices to address algorithmic harms. From employees' reflections on real-world AI procurements, we identify three key challenges that motivate but are not fully addressed by existing AI procurement reform initiatives. Based on these findings, we discuss implications and opportunities for the FAccT community to support cities in foreseeing and preventing AI harms throughout the public procurement processes.","authors":["Nari Johnson","Elise Silva","Harrison Leon","Motahhare Eslami","Beth Schwanke","Ravit Dotan","Hoda Heidari"],"url":"https://arxiv.org/abs/2411.04994"}
{"created":"2025-05-14","title":"EMPERROR: A Flexible Generative Perception Error Model for Probing Self-Driving Planners","abstract":"To handle the complexities of real-world traffic, learning planners for self-driving from data is a promising direction. While recent approaches have shown great progress, they typically assume a setting in which the ground-truth world state is available as input. However, when deployed, planning needs to be robust to the long-tail of errors incurred by a noisy perception system, which is often neglected in evaluation. To address this, previous work has proposed drawing adversarial samples from a perception error model (PEM) mimicking the noise characteristics of a target object detector. However, these methods use simple PEMs that fail to accurately capture all failure modes of detection. In this paper, we present EMPERROR, a novel transformer-based generative PEM, apply it to stress-test an imitation learning (IL)-based planner and show that it imitates modern detectors more faithfully than previous work. Furthermore, it is able to produce realistic noisy inputs that increase the planner's collision rate by up to 85%, demonstrating its utility as a valuable tool for a more complete evaluation of self-driving planners.","authors":["Niklas Hanselmann","Simon Doll","Marius Cordts","Hendrik P. A. Lensch","Andreas Geiger"],"url":"https://arxiv.org/abs/2411.07719"}
{"created":"2025-05-14","title":"Calibrated and Efficient Sampling-Free Confidence Estimation for LiDAR Scene Semantic Segmentation","abstract":"Reliable deep learning models require not only accurate predictions but also well-calibrated confidence estimates to ensure dependable uncertainty estimation. This is crucial in safety-critical applications like autonomous driving, which depend on rapid and precise semantic segmentation of LiDAR point clouds for real-time 3D scene understanding. In this work, we introduce a sampling-free approach for estimating well-calibrated confidence values for classification tasks, achieving alignment with true classification accuracy and significantly reducing inference time compared to sampling-based methods. Our evaluation using the Adaptive Calibration Error (ACE) metric for LiDAR semantic segmentation shows that our approach maintains well-calibrated confidence values while achieving increased processing speed compared to a sampling baseline. Additionally, reliability diagrams reveal that our method produces underconfidence rather than overconfident predictions, an advantage for safety-critical applications. Our sampling-free approach offers well-calibrated and time-efficient predictions for LiDAR scene semantic segmentation.","authors":["Hanieh Shojaei Miandashti","Qianqian Zou","Claus Brenner"],"url":"https://arxiv.org/abs/2411.11935"}
{"created":"2025-05-14","title":"CHOICE: Benchmarking the Remote Sensing Capabilities of Large Vision-Language Models","abstract":"The rapid advancement of Large Vision-Language Models (VLMs), both general-domain models and those specifically tailored for remote sensing, has demonstrated exceptional perception and reasoning capabilities in Earth observation tasks. However, a benchmark for systematically evaluating their capabilities in this domain is still lacking. To bridge this gap, we propose CHOICE, an extensive benchmark designed to objectively evaluate the hierarchical remote sensing capabilities of VLMs. Focusing on 2 primary capability dimensions essential to remote sensing: perception and reasoning, we further categorize 6 secondary dimensions and 23 leaf tasks to ensure a well-rounded assessment coverage. CHOICE guarantees the quality of all 10,507 problems through a rigorous process of data collection from 50 globally distributed cities, question construction and quality control. The newly curated data and the format of multiple-choice questions with definitive answers allow for an objective and straightforward performance assessment. Our evaluation of 3 proprietary and 21 open-source VLMs highlights their critical limitations within this specialized context. We hope that CHOICE will serve as a valuable resource and offer deeper insights into the challenges and potential of VLMs in the field of remote sensing. We will release CHOICE at https://github.com/ShawnAn-WHU/CHOICE.","authors":["Xiao An","Jiaxing Sun","Zihan Gui","Wei He"],"url":"https://arxiv.org/abs/2411.18145"}
{"created":"2025-05-14","title":"Streamlining Prediction in Bayesian Deep Learning","abstract":"The rising interest in Bayesian deep learning (BDL) has led to a plethora of methods for estimating the posterior distribution. However, efficient computation of inferences, such as predictions, has been largely overlooked with Monte Carlo integration remaining the standard. In this work we examine streamlining prediction in BDL through a single forward pass without sampling. For this we use local linearisation on activation functions and local Gaussian approximations at linear layers. Thus allowing us to analytically compute an approximation to the posterior predictive distribution. We showcase our approach for both MLP and transformers, such as ViT and GPT-2, and assess its performance on regression and classification tasks.","authors":["Rui Li","Marcus Klasson","Arno Solin","Martin Trapp"],"url":"https://arxiv.org/abs/2411.18425"}
{"created":"2025-05-14","title":"Diffusion-VLA: Scaling Robot Foundation Models via Unified Diffusion and Autoregression","abstract":"In this paper, we present DiffusionVLA, a novel framework that seamlessly combines the autoregression model with the diffusion model for learning visuomotor policy. Central to our approach is a next-token prediction objective, enabling the model to reason effectively over the user's query in the context of current observations. Subsequently, a diffusion model is attached to generate robust action outputs. To enhance policy learning through self-reasoning, we introduce a novel reasoning injection module that integrates reasoning phrases directly into the policy learning process. The whole framework is simple and flexible, making it easy to deploy and upgrade. We conduct extensive experiments using multiple real robots to validate the effectiveness of DiffusionVLA. Our tests include a challenging factory sorting task, where DiffusionVLA successfully categorizes objects, including those not seen during training. We observe that the reasoning module makes the model interpretable. It allows observers to understand the model thought process and identify potential causes of policy failures. Additionally, we test DiffusionVLA on a zero-shot bin-picking task, achieving 63.7\\% accuracy on 102 previously unseen objects. Our method demonstrates robustness to visual changes, such as distractors and new backgrounds, and easily adapts to new embodiments. Furthermore, DiffusionVLA can follow novel instructions and retain conversational ability. Notably, DiffusionVLA is data-efficient and fast at inference; our smallest DiffusionVLA-2B runs 82Hz on a single A6000 GPU and can train from scratch on less than 50 demonstrations for a complex task. Finally, we scale the model from 2B to 72B parameters, showcasing improved generalization capabilities with increased model size.","authors":["Junjie Wen","Minjie Zhu","Yichen Zhu","Zhibin Tang","Jinming Li","Zhongyi Zhou","Chengmeng Li","Xiaoyu Liu","Yaxin Peng","Chaomin Shen","Feifei Feng"],"url":"https://arxiv.org/abs/2412.03293"}
{"created":"2025-05-14","title":"Enhancing Scene Coordinate Regression with Efficient Keypoint Detection and Sequential Information","abstract":"Scene Coordinate Regression (SCR) is a visual localization technique that utilizes deep neural networks (DNN) to directly regress 2D-3D correspondences for camera pose estimation. However, current SCR methods often face challenges in handling repetitive textures and meaningless areas due to their reliance on implicit triangulation. In this paper, we propose an efficient and accurate SCR system. Compared to existing SCR methods, we propose a unified architecture for both scene encoding and salient keypoint detection, allowing our system to prioritize the encoding of informative regions. This design significantly improves computational efficiency. Additionally, we introduce a mechanism that utilizes sequential information during both mapping and relocalization. The proposed method enhances the implicit triangulation, especially in environments with repetitive textures. Comprehensive experiments conducted across indoor and outdoor datasets demonstrate that the proposed system outperforms state-of-the-art (SOTA) SCR methods. Our single-frame relocalization mode improves the recall rate of our baseline by 6.4% and increases the running speed from 56Hz to 90Hz. Furthermore, our sequence-based mode increases the recall rate by 11% while maintaining the original efficiency.","authors":["Kuan Xu","Zeyu Jiang","Haozhi Cao","Shenghai Yuan","Chen Wang","Lihua Xie"],"url":"https://arxiv.org/abs/2412.06488"}
{"created":"2025-05-14","title":"Towards Brain Passage Retrieval -- An Investigation of EEG Query Representations","abstract":"Information Retrieval (IR) systems primarily rely on users' ability to translate their internal information needs into (text) queries. However, this translation process is often uncertain and cognitively demanding, leading to queries that incompletely or inaccurately represent users' true needs. This challenge is particularly acute for users with ill-defined information needs or physical impairments that limit traditional text input, where the gap between cognitive intent and query expression becomes even more pronounced. Recent neuroscientific studies have explored Brain-Machine Interfaces (BMIs) as a potential solution, aiming to bridge the gap between users' cognitive semantics and their search intentions. However, current approaches attempting to decode explicit text queries from brain signals have shown limited effectiveness in learning robust brain-to-text representations, often failing to capture the nuanced semantic information present in brain patterns. To address these limitations, we propose BPR (Brain Passage Retrieval), a novel framework that eliminates the need for intermediate query translation by enabling direct retrieval of relevant passages from users' brain signals. Our approach leverages dense retrieval architectures to map EEG signals and text passages into a shared semantic space. Through comprehensive experiments on the ZuCo dataset, we demonstrate that BPR achieves up to 8.81% improvement in precision@5 over existing EEG-to-text baselines, while maintaining effectiveness across 30 participants. Our ablation studies reveal the critical role of hard negative sampling and specialised brain encoders in achieving robust cross-modal alignment. These results establish the viability of direct brain-to-passage retrieval and provide a foundation for developing more natural interfaces between users' cognitive states and IR systems.","authors":["Niall McGuire","Yashar Moshfeghi"],"url":"https://arxiv.org/abs/2412.06695"}
{"created":"2025-05-14","title":"NExT-LF: A Novel Operational Modal Analysis Method via Tangential Interpolation","abstract":"Operational Modal Analysis (OMA) is vital for identifying modal parameters under real-world conditions, yet existing methods often face challenges with noise sensitivity and stability. This work introduces NExT-LF, a novel method that combines the well-known Natural Excitation Technique (NExT) with the Loewner Framework (LF). NExT enables the extraction of Impulse Response Functions (IRFs) from output-only vibration data, which are then converted into the frequency domain and used by LF to estimate modal parameters. The proposed method is validated through numerical and experimental case studies. In the numerical study of a 2D Euler-Bernoulli cantilever beam, NExT-LF provides results consistent with analytical solutions and those from standard methods, NExT with Eigensystem Realization Algorithm (NExT-ERA) and Stochastic Subspace Identification with Canonical Variate Analysis (SSI). Additionally, NExT-LF demonstrates superior noise robustness, reliably identifying stable modes across various noise levels where NExT-ERA fails. Experimental validation on the Sheraton Universal Hotel is the first OMA application to this structure, confirming NExT-LF as a robust and efficient method for output-only modal parameter identification.","authors":["Gabriele Dessena","Marco Civera","Ali Yousefi","Cecilia Surace"],"url":"https://arxiv.org/abs/2412.09418"}
{"created":"2025-05-14","title":"AniSora: Exploring the Frontiers of Animation Video Generation in the Sora Era","abstract":"Animation has gained significant interest in the recent film and TV industry. Despite the success of advanced video generation models like Sora, Kling, and CogVideoX in generating natural videos, they lack the same effectiveness in handling animation videos. Evaluating animation video generation is also a great challenge due to its unique artist styles, violating the laws of physics and exaggerated motions. In this paper, we present a comprehensive system, AniSora, designed for animation video generation, which includes a data processing pipeline, a controllable generation model, and an evaluation benchmark. Supported by the data processing pipeline with over 10M high-quality data, the generation model incorporates a spatiotemporal mask module to facilitate key animation production functions such as image-to-video generation, frame interpolation, and localized image-guided animation. We also collect an evaluation benchmark of 948 various animation videos, with specifically developed metrics for animation video generation. Our entire project is publicly available on https://github.com/bilibili/Index-anisora/tree/main.","authors":["Yudong Jiang","Baohan Xu","Siqian Yang","Mingyu Yin","Jing Liu","Chao Xu","Siqi Wang","Yidi Wu","Bingwen Zhu","Xinwen Zhang","Xingyu Zheng","Jixuan Xu","Yue Zhang","Jinlong Hou","Huyang Sun"],"url":"https://arxiv.org/abs/2412.10255"}
{"created":"2025-05-14","title":"Training Strategies for Isolated Sign Language Recognition","abstract":"Accurate recognition and interpretation of sign language are crucial for enhancing communication accessibility for deaf and hard of hearing individuals. However, current approaches of Isolated Sign Language Recognition (ISLR) often face challenges such as low data quality and variability in gesturing speed. This paper introduces a comprehensive model training pipeline for ISLR designed to accommodate the distinctive characteristics and constraints of the Sign Language (SL) domain. The constructed pipeline incorporates carefully selected image and video augmentations to tackle the challenges of low data quality and varying sign speeds. Including an additional regression head combined with IoU-balanced classification loss enhances the model's awareness of the gesture and simplifies capturing temporal information. Extensive experiments demonstrate that the developed training pipeline easily adapts to different datasets and architectures. Additionally, the ablation study shows that each proposed component expands the potential to consider ISLR task specifics. The presented strategies enhance recognition performance across various ISLR benchmarks and achieve state-of-the-art results on the WLASL and Slovo datasets.","authors":["Karina Kvanchiani","Roman Kraynov","Elizaveta Petrova","Petr Surovcev","Aleksandr Nagaev","Alexander Kapitanov"],"url":"https://arxiv.org/abs/2412.11553"}
{"created":"2025-05-14","title":"USEFUSE: Uniform Stride for Enhanced Performance in Fused Layer Architecture of Deep Neural Networks","abstract":"Convolutional Neural Networks (CNNs) are crucial in various applications, but their deployment on resource-constrained edge devices poses challenges. This study presents the Sum-of-Products (SOP) units for convolution, which utilize low-latency left-to-right bit-serial arithmetic to minimize response time and enhance overall performance. The study proposes a methodology for fusing multiple convolution layers to reduce off-chip memory communication and increase overall performance. An effective mechanism detects and skips inefficient convolutions after ReLU layers, minimizing power consumption without compromising accuracy. Furthermore, efficient tile movement guarantees uniform access to the fusion pyramid. An analysis demonstrates the utile stride strategy improves operational intensity. Two designs cater to varied demands: one focuses on minimal response time for mission-critical applications, and another focuses on resource-constrained devices with comparable latency. This approach notably reduced redundant computations, improving the efficiency of CNN deployment on edge devices.","authors":["Muhammad Sohail Ibrahim","Muhammad Usman","Jeong-A Lee"],"url":"https://arxiv.org/abs/2412.13724"}
{"created":"2025-05-14","title":"CoRa: A Collision-Resistant LoRa Symbol Detector of Low Complexity","abstract":"Long range communication with LoRa has become popular as it avoids the complexity of multi-hop communication at low cost and low energy consumption. LoRa is openly accessible, but its packets are particularly vulnerable to collisions due to long time on air in a shared band. This degrades communication performance. Existing techniques for demodulating LoRa symbols under collisions face challenges such as high computational complexity, reliance on accurate symbol boundary information, or error-prone peak detection methods. In this paper, we introduce CoRa , a symbol detector for demodulating LoRa symbols under severe collisions. CoRa employs a Bayesian classifier to accurately identify the true symbol amidst interference from other LoRa transmissions, leveraging empirically derived features from raw symbol data. Evaluations using real-world and simulated packet traces demonstrate that CoRa clearly outperforms the related state-of-the-art, i.e., up to 29% better decoding performance than TnB and 178% better than CIC. Compared to the LoRa baseline demodulator, CoRa magnifies the packet reception rate by up to 11.53x. CoRa offers a significant reduction in computational complexity compared to existing solutions by only adding a constant overhead to the baseline demodulator, while also eliminating the need for peak detection and accurately identifying colliding frames.","authors":["Jos\\'e \\'Alamos","Thomas C. Schmidt","Matthias W\\\"ahlisch"],"url":"https://arxiv.org/abs/2412.13930"}
{"created":"2025-05-14","title":"Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships","abstract":"Can consumers form especially deep emotional bonds with AI and be vested in AI identities over time? We leverage a natural app-update event at Replika AI, a popular US-based AI companion, to shed light on these questions. We find that, after the app removed its erotic role play (ERP) feature, preventing intimate interactions between consumers and chatbots that were previously possible, this event triggered perceptions in customers that their AI companion's identity had discontinued. This in turn predicted negative consumer welfare and marketing outcomes related to loss, including mourning the loss, and devaluing the \"new\" AI relative to the \"original\". Experimental evidence confirms these findings. Further experiments find that AI companions users feel closer to their AI companion than even their best human friend, and mourn a loss of their AI companion more than a loss of various other inanimate products. In short, consumers are forming human-level relationships with AI companions; disruptions to these relationships trigger real patterns of mourning as well as devaluation of the offering; and the degree of mourning and devaluation are explained by perceived discontinuity in the AIs identity. Our results illustrate that relationships with AI are truly personal, creating unique benefits and risks for consumers and firms alike.","authors":["Julian De Freitas","Noah Castelo","Ahmet Uguralp","Zeliha Uguralp"],"url":"https://arxiv.org/abs/2412.14190"}
{"created":"2025-05-14","title":"Graph Attention is Not Always Beneficial: A Theoretical Analysis of Graph Attention Mechanisms via Contextual Stochastic Block Models","abstract":"Despite the growing popularity of graph attention mechanisms, their theoretical understanding remains limited. This paper aims to explore the conditions under which these mechanisms are effective in node classification tasks through the lens of Contextual Stochastic Block Models (CSBMs). Our theoretical analysis reveals that incorporating graph attention mechanisms is \\emph{not universally beneficial}. Specifically, by appropriately defining \\emph{structure noise} and \\emph{feature noise} in graphs, we show that graph attention mechanisms can enhance classification performance when structure noise exceeds feature noise. Conversely, when feature noise predominates, simpler graph convolution operations are more effective. Furthermore, we examine the over-smoothing phenomenon and show that, in the high signal-to-noise ratio (SNR) regime, graph convolutional networks suffer from over-smoothing, whereas graph attention mechanisms can effectively resolve this issue. Building on these insights, we propose a novel multi-layer Graph Attention Network (GAT) architecture that significantly outperforms single-layer GATs in achieving \\emph{perfect node classification} in CSBMs, relaxing the SNR requirement from $ \\omega(\\sqrt{\\log n}) $ to $ \\omega(\\sqrt{\\log n} / \\sqrt[3]{n}) $. To our knowledge, this is the first study to delineate the conditions for perfect node classification using multi-layer GATs. Our theoretical contributions are corroborated by extensive experiments on both synthetic and real-world datasets, highlighting the practical implications of our findings.","authors":["Zhongtian Ma","Qiaosheng Zhang","Bocheng Zhou","Yexin Zhang","Shuyue Hu","Zhen Wang"],"url":"https://arxiv.org/abs/2412.15496"}
{"created":"2025-05-14","title":"PoisonCatcher: Revealing and Identifying LDP Poisoning Attacks in IIoT","abstract":"Local Differential Privacy (LDP), a robust privacy-protection model, is widely adopted in the Industrial Internet of Things (IIoT) due to its lightweight, decentralized, and scalable. However, its perturbation-based privacy-protection mechanism hinders distinguishing between any two data, thereby facilitating LDP poisoning attacks. The exposed physical-layer vulnerabilities and resource-constrained prevalent at the IIoT edge not only facilitate such attacks but also render existing LDP poisoning defenses, all of which are deployed at the edge and rely on ample resources, impractical.","authors":["Lisha Shuai","Shaofeng Tan","Nan Zhang","Jiamin Zhang","Min Zhang","Xiaolong Yang"],"url":"https://arxiv.org/abs/2412.15704"}
{"created":"2025-05-14","title":"A Classification Benchmark for Artificial Intelligence Detection of Laryngeal Cancer from Patient Voice","abstract":"Cases of laryngeal cancer are predicted to rise significantly in the coming years. Current diagnostic pathways are inefficient, putting undue stress on both patients and the medical system. Artificial intelligence offers a promising solution by enabling non-invasive detection of laryngeal cancer from patient voice, which could help prioritise referrals more effectively. A major barrier in this field is the lack of reproducible methods. Our work addresses this challenge by introducing a benchmark suite comprising 36 models trained and evaluated on open-source datasets. These models classify patients with benign and malignant voice pathologies. All models are accessible in a public repository, providing a foundation for future research. We evaluate three algorithms and three audio feature sets, including both audio-only inputs and multimodal inputs incorporating demographic and symptom data. Our best model achieves a balanced accuracy of 83.7%, sensitivity of 84.0%, specificity of 83.3%, and AUROC of 91.8%.","authors":["Mary Paterson","James Moor","Luisa Cutillo"],"url":"https://arxiv.org/abs/2412.16267"}
{"created":"2025-05-14","title":"A Protocol for KG Construction Tasks Involving Users","abstract":"Knowledge graph construction (KGC) from (semi-)structured data is challenging, and facilitating user involvement is an issue frequently brought up within this community. We cannot deny the progress we have made with respect to (declarative) knowledge graph construction languages and tools to help build such mappings. However, it is surprising that no two studies report on similar protocols. This heterogeneity does not allow for comparing KGC languages, techniques, and tools. This paper first analyses studies involving users to identify the points of comparison. These gaps include a lack of systematic consistency in task design, participant selection, and evaluation metrics. Moreover, there needs to be a systematic way of analyzing the data and reporting the findings, which is also lacking. We thus propose and introduce a user protocol for KGC designed to address this challenge. Where possible, we draw and take elements from the literature we deem fit for such a protocol. The protocol, as such, allows for the comparison of languages and techniques for the RDF Mapping Language (RML) core functionality, which is covered by most of the other state-of-the-art techniques and tools. We also propose how the protocol can be amended to compare extensions (of RML). This protocol provides an important step towards a more comparable evaluation of KGC user studies.","authors":["Ademar Crotti Junior","Christophe Debruyne"],"url":"https://arxiv.org/abs/2412.16766"}
{"created":"2025-05-14","title":"No Preference Left Behind: Group Distributional Preference Optimization","abstract":"Preferences within a group of people are not uniform but follow a distribution. While existing alignment methods like Direct Preference Optimization (DPO) attempt to steer models to reflect human preferences, they struggle to capture the distributional pluralistic preferences within a group. These methods often skew toward dominant preferences, overlooking the diversity of opinions, especially when conflicting preferences arise. To address this issue, we propose Group Distributional Preference Optimization (GDPO), a novel framework that aligns language models with the distribution of preferences within a group by incorporating the concept of beliefs that shape individual preferences. GDPO calibrates a language model using statistical estimation of the group's belief distribution and aligns the model with belief-conditioned preferences, offering a more inclusive alignment framework than traditional methods. In experiments using both synthetic controllable opinion generation and real-world movie review datasets, we show that DPO fails to align with the targeted belief distributions, while GDPO consistently reduces this alignment gap during training. Moreover, our evaluation metrics demonstrate that GDPO outperforms existing approaches in aligning with group distributional preferences, marking a significant advance in pluralistic alignment.","authors":["Binwei Yao","Zefan Cai","Yun-Shiuan Chuang","Shanglin Yang","Ming Jiang","Diyi Yang","Junjie Hu"],"url":"https://arxiv.org/abs/2412.20299"}
{"created":"2025-05-14","title":"2.5 Years in Class: A Multimodal Textbook for Vision-Language Pretraining","abstract":"Compared to image-text pair data, interleaved corpora enable Vision-Language Models (VLMs) to understand the world more naturally like humans. However, such existing datasets are crawled from webpage, facing challenges like low knowledge density, loose image-text relations, and poor logical coherence between images. On the other hand, the internet hosts vast instructional videos (e.g., online geometry courses) that are widely used by humans to learn foundational subjects, yet these valuable resources remain underexplored in VLM training. In this paper, we introduce a high-quality \\textbf{multimodal textbook} corpus with richer foundational knowledge for VLM pretraining. It collects over 2.5 years of instructional videos, totaling 22,000 class hours. We first use an LLM-proposed taxonomy to systematically gather instructional videos. Then we progressively extract and refine visual (keyframes), audio (ASR), and textual knowledge (OCR) from the videos, and organize as an image-text interleaved corpus based on temporal order. Compared to its counterparts, our video-centric textbook offers more coherent context, richer knowledge, and better image-text alignment. Experiments demonstrate its superb pretraining performance, particularly in knowledge- and reasoning-intensive tasks like ScienceQA and MathVista. Moreover, VLMs pre-trained on our textbook exhibit outstanding interleaved context awareness, leveraging visual and textual cues in their few-shot context for task solving. Our code are available at https://github.com/DAMO-NLP-SG/multimodal_textbook.","authors":["Wenqi Zhang","Hang Zhang","Xin Li","Jiashuo Sun","Yongliang Shen","Weiming Lu","Deli Zhao","Yueting Zhuang","Lidong Bing"],"url":"https://arxiv.org/abs/2501.00958"}
{"created":"2025-05-14","title":"HLV-1K: A Large-scale Hour-Long Video Benchmark for Time-Specific Long Video Understanding","abstract":"Multimodal large language models have become a popular topic in deep visual understanding due to many promising real-world applications. However, hour-long video understanding, spanning over one hour and containing tens of thousands of visual frames, remains under-explored because of 1) challenging long-term video analyses, 2) inefficient large-model approaches, and 3) lack of large-scale benchmark datasets. Among them, in this paper, we focus on building a large-scale hour-long long video benchmark, HLV-1K, designed to evaluate long video understanding models. HLV-1K comprises 1009 hour-long videos with 14,847 high-quality question answering (QA) and multi-choice question asnwering (MCQA) pairs with time-aware query and diverse annotations, covering frame-level, within-event-level, cross-event-level, and long-term reasoning tasks. We evaluate our benchmark using existing state-of-the-art methods and demonstrate its value for testing deep long video understanding capabilities at different levels and for various tasks. This includes promoting future long video understanding tasks at a granular level, such as deep understanding of long live videos, meeting recordings, and movies.","authors":["Heqing Zou","Tianze Luo","Guiyang Xie","Victor Xiao Jie Zhang","Fengmao Lv","Guangcong Wang","Junyang Chen","Zhuochen Wang","Hansheng Zhang","Huaijian Zhang"],"url":"https://arxiv.org/abs/2501.01645"}
{"created":"2025-05-14","title":"Scaling Laws for Floating Point Quantization Training","abstract":"Low-precision training is considered an effective strategy for reducing both training and downstream inference costs. Previous scaling laws for precision mainly focus on integer quantization, which pay less attention to the constituents in floating-point (FP) quantization, and thus cannot well fit the LLM losses in this scenario. In contrast, while FP quantization training is more commonly implemented in production, it's research has been relatively superficial. In this paper, we thoroughly explore the effects of FP quantization targets, exponent bits, mantissa bits, and the calculation granularity of the scaling factor in FP quantization training performance of LLM models. In addition to an accurate FP quantization unified scaling law, we also provide valuable suggestions for the community: (1) Exponent bits contribute slightly more to the model performance than mantissa bits. We provide the optimal exponent-mantissa bit ratio for different bit numbers, which is available for future reference by hardware manufacturers; (2) We discover the formation of the critical data size in low-precision LLM training. Too much training data exceeding the critical data size will inversely bring in degradation of LLM performance; (3) The optimal FP quantization precision is directly proportional to the computational power, but within a wide computational power range. We estimate that the best cost-performance precision should lie between 4-8 bits.","authors":["Xingwu Sun","Shuaipeng Li","Ruobing Xie","Weidong Han","Kan Wu","Zhen Yang","Yixing Li","An Wang","Shuai Li","Jinbao Xue","Yu Cheng","Yangyu Tao","Zhanhui Kang","Chengzhong Xu","Di Wang","Jie Jiang"],"url":"https://arxiv.org/abs/2501.02423"}
{"created":"2025-05-14","title":"FlippedRAG: Black-Box Opinion Manipulation Adversarial Attacks to Retrieval-Augmented Generation Models","abstract":"Retrieval-Augmented Generation (RAG) enriches LLMs by dynamically retrieving external knowledge, reducing hallucinations and satisfying real-time information needs. While existing research mainly targets RAG's performance and efficiency, emerging studies highlight critical security concerns. Yet, current adversarial approaches remain limited, mostly addressing white-box scenarios or heuristic black-box attacks without fully investigating vulnerabilities in the retrieval phase. Additionally, prior works mainly focus on factoid QA tasks, their attacks lack complexity and can be easily corrected by advanced LLMs. In this paper, we investigate a more realistic and critical threat scenario: adversarial attacks intended for opinion manipulation against black-box RAG models, particularly on controversial topics. Specifically, we propose FlippedRAG, a transfer-based adversarial attack against black-box RAG systems. We first demonstrate that the underlying retriever of a black-box RAG system can be reverse-engineered, enabling us to train a surrogate retriever. Leveraging the surrogate retriever, we further craft target poisoning triggers, altering vary few documents to effectively manipulate both retrieval and subsequent generation. Extensive empirical results show that FlippedRAG substantially outperforms baseline methods, improving the average attack success rate by 16.7%. FlippedRAG achieves on average a 50% directional shift in the opinion polarity of RAG-generated responses, ultimately causing a notable 20% shift in user cognition. Furthermore, we evaluate the performance of several potential defensive measures, concluding that existing mitigation strategies remain insufficient against such sophisticated manipulation attacks. These results highlight an urgent need for developing innovative defensive solutions to ensure the security and trustworthiness of RAG systems.","authors":["Zhuo Chen","Jiawei Liu","Yuyang Gong","Miaokun Chen","Haotan Liu","Qikai Cheng","Fan Zhang","Wei Lu","Xiaozhong Liu","Xiaofeng Wang"],"url":"https://arxiv.org/abs/2501.02968"}
{"created":"2025-05-14","title":"Enhanced Importance Sampling through Latent Space Exploration in Normalizing Flows","abstract":"Importance sampling is a rare event simulation technique used in Monte Carlo simulations to bias the sampling distribution towards the rare event of interest. By assigning appropriate weights to sampled points, importance sampling allows for more efficient estimation of rare events or tails of distributions. However, importance sampling can fail when the proposal distribution does not effectively cover the target distribution. In this work, we propose a method for more efficient sampling by updating the proposal distribution in the latent space of a normalizing flow. Normalizing flows learn an invertible mapping from a target distribution to a simpler latent distribution. The latent space can be more easily explored during the search for a proposal distribution, and samples from the proposal distribution are recovered in the space of the target distribution via the invertible mapping. We empirically validate our methodology on simulated robotics applications such as autonomous racing and aircraft ground collision avoidance.","authors":["Liam A. Kruse","Alexandros E. Tzikas","Harrison Delecki","Mansur M. Arief","Mykel J. Kochenderfer"],"url":"https://arxiv.org/abs/2501.03394"}
{"created":"2025-05-14","title":"Stable Derivative Free Gaussian Mixture Variational Inference for Bayesian Inverse Problems","abstract":"This paper is concerned with the approximation of probability distributions known up to normalization constants, with a focus on Bayesian inference for large-scale inverse problems in scientific computing. In this context, key challenges include costly repeated evaluations of forward models, multimodality, and inaccessible gradients for the forward model. To address them, we develop a variational inference framework that combines Fisher-Rao natural gradient with specialized quadrature rules to enable derivative free updates of Gaussian mixture variational families. The resulting method, termed Derivative Free Gaussian Mixture Variational Inference (DF-GMVI), guarantees covariance positivity and affine invariance, offering a stable and efficient framework for approximating complex posterior distributions. The effectiveness of DF-GMVI is demonstrated through numerical experiments on challenging scenarios, including distributions with multiple modes, infinitely many modes, and curved modes in spaces with up to 100 dimensions. The method's practicality is further demonstrated in a large-scale application, where it successfully recovers the initial conditions of the Navier-Stokes equations from solution data at positive times.","authors":["Baojun Che","Yifan Chen","Zhenghao Huan","Daniel Zhengyu Huang","Weijie Wang"],"url":"https://arxiv.org/abs/2501.04259"}
{"created":"2025-05-14","title":"ConceptMaster: Multi-Concept Video Customization on Diffusion Transformer Models Without Test-Time Tuning","abstract":"Text-to-video generation has made remarkable advancements through diffusion models. However, Multi-Concept Video Customization (MCVC) remains a significant challenge. We identify two key challenges for this task: 1) the identity decoupling issue, where directly adopting existing customization methods inevitably mix identity attributes when handling multiple concepts simultaneously, and 2) the scarcity of high-quality video-entity pairs, which is crucial for training a model that can well represent and decouple various customized concepts in video generation. To address these challenges, we introduce ConceptMaster, a novel framework that effectively addresses the identity decoupling issues while maintaining concept fidelity in video customization. Specifically, we propose to learn decoupled multi-concept embeddings and inject them into diffusion models in a standalone manner, which effectively guarantees the quality of customized videos with multiple identities, even for highly similar visual concepts. To overcome the scarcity of high-quality MCVC data, we establish a data construction pipeline, which enables collection of high-quality multi-concept video-entity data pairs across diverse scenarios. A multi-concept video evaluation set is further devised to comprehensively validate our method from three dimensions, including concept fidelity, identity decoupling ability, and video generation quality, across six different concept composition scenarios. Extensive experiments demonstrate that ConceptMaster significantly outperforms previous methods for video customization tasks, showing great potential to generate personalized and semantically accurate content for video diffusion models.","authors":["Yuzhou Huang","Ziyang Yuan","Quande Liu","Qiulin Wang","Xintao Wang","Ruimao Zhang","Pengfei Wan","Di Zhang","Kun Gai"],"url":"https://arxiv.org/abs/2501.04698"}
{"created":"2025-05-14","title":"UAV-VLA: Vision-Language-Action System for Large Scale Aerial Mission Generation","abstract":"The UAV-VLA (Visual-Language-Action) system is a tool designed to facilitate communication with aerial robots. By integrating satellite imagery processing with the Visual Language Model (VLM) and the powerful capabilities of GPT, UAV-VLA enables users to generate general flight paths-and-action plans through simple text requests. This system leverages the rich contextual information provided by satellite images, allowing for enhanced decision-making and mission planning. The combination of visual analysis by VLM and natural language processing by GPT can provide the user with the path-and-action set, making aerial operations more efficient and accessible. The newly developed method showed the difference in the length of the created trajectory in 22% and the mean error in finding the objects of interest on a map in 34.22 m by Euclidean distance in the K-Nearest Neighbors (KNN) approach.","authors":["Oleg Sautenkov","Yasheerah Yaqoot","Artem Lykov","Muhammad Ahsan Mustafa","Grik Tadevosyan","Aibek Akhmetkazy","Miguel Altamirano Cabrera","Mikhail Martynov","Sausar Karaf","Dzmitry Tsetserukou"],"url":"https://arxiv.org/abs/2501.05014"}
{"created":"2025-05-14","title":"Capability-Aware Shared Hypernetworks for Flexible Heterogeneous Multi-Robot Coordination","abstract":"Recent advances have enabled heterogeneous multi-robot teams to learn complex and effective coordination skills. However, existing neural architectures that support heterogeneous teaming tend to force a trade-off between expressivity and efficiency. Shared-parameter designs prioritize sample efficiency by enabling a single network to be shared across all or a pre-specified subset of robots (via input augmentations), but tend to limit behavioral diversity. In contrast, recent designs employ a separate policy for each robot, enabling greater diversity and expressivity at the cost of efficiency and generalization. Our key insight is that such tradeoffs can be avoided by viewing these design choices as ends of a broad spectrum. Inspired by recent work in transfer and meta learning, and building on prior work in multi-robot task allocation, we propose Capability-Aware Shared Hypernetworks (CASH), a soft weight sharing architecture that uses hypernetworks to efficiently learn a flexible shared policy that dynamically adapts to each robot post-training. By explicitly encoding the impact of robot capabilities (e.g., speed and payload) on collective behavior, CASH enables zero-shot generalization to unseen robots or team compositions. Our experiments involve multiple heterogeneous tasks, three learning paradigms (imitation learning, value-based, and policy-gradient RL), and SOTA multi-robot simulation (JaxMARL) and hardware (Robotarium) platforms. Across all conditions, we find that CASH generates appropriately-diverse behaviors and consistently outperforms baseline architectures in terms of performance and sample efficiency during both training and zero-shot generalization, all with 60%-80% fewer learnable parameters.","authors":["Kevin Fu","Shalin Anand Jain","Pierce Howell","Harish Ravichandar"],"url":"https://arxiv.org/abs/2501.06058"}
{"created":"2025-05-14","title":"Vision-Language Models Do Not Understand Negation","abstract":"Many practical vision-language applications require models that understand negation, e.g., when using natural language to retrieve images which contain certain objects but not others. Despite advancements in vision-language models (VLMs) through large-scale training, their ability to comprehend negation remains underexplored. This study addresses the question: how well do current VLMs understand negation? We introduce NegBench, a new benchmark designed to evaluate negation understanding across 18 task variations and $79$k examples spanning image, video, and medical datasets. The benchmark consists of two core tasks designed to evaluate negation understanding in diverse multimodal settings: Retrieval with Negation and Multiple Choice Questions with Negated Captions. Our evaluation reveals that modern VLMs struggle significantly with negation, often performing at chance level. To address these shortcomings, we explore a data-centric approach wherein we finetune CLIP models on large-scale synthetic datasets containing millions of negated captions. We show that this approach can result in a 10% increase in recall on negated queries and a 28% boost in accuracy on multiple-choice questions with negated captions.","authors":["Kumail Alhamoud","Shaden Alshammari","Yonglong Tian","Guohao Li","Philip Torr","Yoon Kim","Marzyeh Ghassemi"],"url":"https://arxiv.org/abs/2501.09425"}
{"created":"2025-05-14","title":"GRID: Protecting Training Graph from Link Stealing Attacks on GNN Models","abstract":"Graph neural networks (GNNs) have exhibited superior performance in various classification tasks on graph-structured data. However, they encounter the potential vulnerability from the link stealing attacks, which can infer the presence of a link between two nodes via measuring the similarity of its incident nodes' prediction vectors produced by a GNN model. Such attacks pose severe security and privacy threats to the training graph used in GNN models. In this work, we propose a novel solution, called Graph Link Disguise (GRID), to defend against link stealing attacks with the formal guarantee of GNN model utility for retaining prediction accuracy. The key idea of GRID is to add carefully crafted noises to the nodes' prediction vectors for disguising adjacent nodes as n-hop indirect neighboring nodes. We take into account the graph topology and select only a subset of nodes (called core nodes) covering all links for adding noises, which can avert the noises offset and have the further advantages of reducing both the distortion loss and the computation cost. Our crafted noises can ensure 1) the noisy prediction vectors of any two adjacent nodes have their similarity level like that of two non-adjacent nodes and 2) the model prediction is unchanged to ensure zero utility loss. Extensive experiments on five datasets are conducted to show the effectiveness of our proposed GRID solution against different representative link-stealing attacks under transductive settings and inductive settings respectively, as well as two influence-based attacks. Meanwhile, it achieves a much better privacy-utility trade-off than existing methods when extended to GNNs.","authors":["Jiadong Lou","Xu Yuan","Rui Zhang","Xingliang Yuan","Neil Gong","Nian-Feng Tzeng"],"url":"https://arxiv.org/abs/2501.10985"}
{"created":"2025-05-14","title":"Deflation-based certified greedy algorithm and adaptivity for bifurcating nonlinear PDEs","abstract":"This work deals with tailored reduced order models for bifurcating nonlinear parametric partial differential equations, where multiple coexisting solutions arise for a given parametric instance. Approaches based on proper orthogonal decomposition have been widely investigated in the literature, but they usually rely on some \\emph{a-priori} knowledge about the bifurcating model and lack any error estimation. On the other hand, standard certified reduced basis techniques fail to represent correctly the branching behavior, since the error estimator is no longer reliable. The main goal of the contribution is to overcome these limitations by introducing two novel algorithms: (i) the adaptive-greedy, detecting the bifurcation point starting from scarce information over the parametric space, and (ii) the deflated-greedy, certifying multiple coexisting branches simultaneously. The former approach takes advantage of the features of the reduced manifold to detect the bifurcation, while the latter exploits the deflation and continuation methods to discover the bifurcating solutions and enrich the reduced space. We test the two strategies for the Coanda effect held by the Navier-Stokes equations in a sudden-expansion channel. The accuracy of the approach and the error certification are compared with vanilla-greedy and proper orthogonal decomposition.","authors":["Federico Pichi","Maria Strazzullo"],"url":"https://arxiv.org/abs/2501.12361"}
{"created":"2025-05-14","title":"VTX: Real-time high-performance molecular structure and dynamics visualization software","abstract":"Summary: VTX is a molecular visualization software capable to handle most molecular structures and dynamics trajectories file formats. It features a real-time high-performance molecular graphics engine, based on modern OpenGL, optimized for the visualization of massive molecular systems and molecular dynamics trajectories. VTX includes multiple interactive camera and user interaction features, notably free-fly navigation and a fully modular graphical user interface designed for increased usability. It allows the production of high-resolution images for presentations and posters with custom background. VTX design is focused on performance and usability for research, teaching and educative purposes.","authors":["Maxime Maria","Simon Guionni\\`ere","Nicolas Dacquay","Cyprien Plateau-Holleville","Valentin Guillaume","Vincent Larroque","Jean Lard\\'e","Yassine Naimi","Jean-Philip Piquemal","Guillaume Levieux","Nathalie Lagarde","St\\'ephane M\\'erillou","Matthieu Montes"],"url":"https://arxiv.org/abs/2501.12750"}
{"created":"2025-05-14","title":"Interactive Oracle Proofs of Proximity to Codes on Graphs","abstract":"We design an Interactive Oracle Proof of Proximity (IOPP) for codes on graphs inspired by the FRI protocol. The soundness is significantly improved compared to the FRI, the complexity parameters are comparable, and there are no restrictions on the field used, enabling to consider new codes to design code-based SNARKs.","authors":["Hugo Delavenne (GRACE)","Tanguy Medevielle (IRMAR","GRACE)","\\'Elina Roussel (GRACE)"],"url":"https://arxiv.org/abs/2501.14337"}
{"created":"2025-05-14","title":"Self-reflecting Large Language Models: A Hegelian Dialectical Approach","abstract":"Investigating NLP through a philosophical lens has recently caught researcher's eyes as it connects computational methods with classical schools of philosophy. This paper introduces a philosophical approach inspired by the \\textit{Hegelian Dialectic} for LLMs' \\textit{self-reflection}, utilizing a self-dialectical approach to emulate internal critiques and then synthesize new ideas by resolving the opposing points of view. Moreover, this paper investigates the effect of LLMs' temperature for generation by establishing a dynamic annealing approach, which promotes the creativity in the early stages and gradually refines it by focusing on the nuances, as well as a fixed-temperature strategy for generation. We assess the effectiveness of our proposed method in generating novel ideas and in improving the reasoning abilities of LLMs during problem-solving. Moreover, we implement a Multi-Agent Majority Voting (MAMV) strategy to assess the validity and novelty of the generated ideas, which proves useful in the absence of domain experts. Our experiments demonstrate promising results in generating ideas and enhancing problem-solving performance.","authors":["Sara Abdali","Can Goksen","Saeed Amizadeh","Julie E. Maybee","Kazuhito Koishida"],"url":"https://arxiv.org/abs/2501.14917"}
{"created":"2025-05-14","title":"Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?","abstract":"Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C=g(B)=g(f(A)) ) even without encountering ( ABC ) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, \"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing.","authors":["Yutong Yin","Zhaoran Wang"],"url":"https://arxiv.org/abs/2501.15857"}
{"created":"2025-05-14","title":"Transfer Learning of Surrogate Models: Integrating Domain Warping and Affine Transformations","abstract":"Surrogate models provide efficient alternatives to computationally demanding real world processes but often require large datasets for effective training. A promising solution to this limitation is the transfer of pre-trained surrogate models to new tasks. Previous studies have investigated the transfer of differentiable and non-differentiable surrogate models, typically assuming an affine transformation between the source and target functions. This paper extends previous research by addressing a broader range of transformations, including linear and nonlinear variations. Specifically, we consider the combination of an unknown input warping, such as one modeled by the beta cumulative distribution function, with an unspecified affine transformation. Our approach achieves transfer learning by employing a limited number of data points from the target task to optimize these transformations, minimizing empirical loss on the transfer dataset. We validate the proposed method on the widely used Black-Box Optimization Benchmark (BBOB) testbed and a real-world transfer learning task from the automobile industry. The results underscore the significant advantages of the approach, revealing that the transferred surrogate significantly outperforms both the original surrogate and the one built from scratch using the transfer dataset, particularly in data-scarce scenarios.","authors":["Shuaiqun Pan","Diederick Vermetten","Manuel L\\'opez-Ib\\'a\\~nez","Thomas B\\\"ack","Hao Wang"],"url":"https://arxiv.org/abs/2501.18344"}
{"created":"2025-05-14","title":"Examining the Expanding Role of Synthetic Data Throughout the AI Development Pipeline","abstract":"Alongside the growth of generative AI, we are witnessing a surge in the use of synthetic data across all stages of the AI development pipeline. It is now common practice for researchers and practitioners to use one large generative model (which we refer to as an auxiliary model) to generate synthetic data that is used to train or evaluate another, reconfiguring AI workflows and reshaping the very nature of data. While scholars have raised concerns over the risks of synthetic data, policy guidance and best practices for its responsible use have not kept up with these rapidly evolving industry trends, in part because we lack a clear picture of current practices and challenges. Our work aims to address this gap. Through 29 interviews with AI practitioners and responsible AI experts, we examine the expanding role of synthetic data in AI development. Our findings reveal how auxiliary models are now widely used across the AI development pipeline. Practitioners describe synthetic data as crucial for addressing data scarcity and providing a competitive edge, noting that evaluation of generative AI systems at scale would be infeasible without auxiliary models. However, they face challenges controlling the outputs of auxiliary models, generating data that accurately depict underrepresented groups, and scaling data validation practices that are based primarily on manual inspection. We detail general limitations of and ethical considerations for synthetic data and conclude with a proposal of concrete steps towards the development of best practices for its responsible use.","authors":["Shivani Kapania","Stephanie Ballard","Alex Kessler","Jennifer Wortman Vaughan"],"url":"https://arxiv.org/abs/2501.18493"}
{"created":"2025-05-14","title":"FutureVision: A methodology for the investigation of future cognition","abstract":"This paper presents a methodology combining multimodal semantic analysis with an eye-tracking experimental protocol to investigate the cognitive effort involved in understanding the communication of future scenarios. To demonstrate the methodology, we conduct a pilot study examining how visual fixation patterns vary during the evaluation of valence and counterfactuality in fictional ad pieces describing futuristic scenarios, using a portable eye tracker. Participants eye movements are recorded while evaluating the stimuli and describing them to a conversation partner. Gaze patterns are analyzed alongside semantic representations of the stimuli and participants descriptions, constructed from a frame semantic annotation of both linguistic and visual modalities. Preliminary results show that far-future and pessimistic scenarios are associated with longer fixations and more erratic saccades, supporting the hypothesis that fractures in the base spaces underlying the interpretation of future scenarios increase cognitive load for comprehenders.","authors":["Tiago Timponi Torrent","Mark Turner","Nicol\\'as Hinrichs","Frederico Belcavello","Igor Louren\\c{c}o","Arthur Lorenzi Almeida","Marcelo Viridiano","Ely Edison Matos"],"url":"https://arxiv.org/abs/2502.01597"}
{"created":"2025-05-14","title":"Position: AI Scaling: From Up to Down and Out","abstract":"AI Scaling has traditionally been synonymous with Scaling Up, which builds larger and more powerful models. However, the growing demand for efficiency, adaptability, and collaboration across diverse applications necessitates a broader perspective. This position paper presents a holistic framework for AI scaling, encompassing Scaling Up, Scaling Down, and Scaling Out. It argues that while Scaling Up of models faces inherent bottlenecks, the future trajectory of AI scaling lies in Scaling Down and Scaling Out. These paradigms address critical technical and societal challenges, such as reducing carbon footprint, ensuring equitable access, and enhancing cross-domain collaboration. We explore transformative applications in healthcare, smart manufacturing, and content creation, demonstrating how AI Scaling can enable breakthroughs in efficiency, personalization, and global connectivity. Additionally, we highlight key challenges, including balancing model complexity with interpretability, managing resource constraints, and fostering ethical development. By synthesizing these approaches, we propose a unified roadmap that redefines the future of AI research and application, paving the way for advancements toward Artificial General Intelligence (AGI).","authors":["Yunke Wang","Yanxi Li","Chang Xu"],"url":"https://arxiv.org/abs/2502.01677"}
{"created":"2025-05-14","title":"SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals","abstract":"The GPT-4 technical report highlights the possibility of predicting model performance on downstream tasks using only pre-training signals, though detailed methodologies are absent. Such predictive capabilities are essential for resource-efficient pre-training and the construction of task-aligned datasets. In this paper, we aim to predict performance in closed-book question answering (QA), a vital downstream task indicative of a model's internal knowledge. We address three primary challenges: (1) limited access to and understanding of pre-training corpora, (2) limitations of current evaluation methods for pre-trained models, and (3) limitations of frequency-based metrics in predicting model performance. In response to these challenges, we conduct large-scale retrieval and semantic analysis across the pre-training corpora of 21 publicly available and 3 custom-trained large language models. Subsequently, we develop a multi-template QA evaluation framework incorporating paraphrased question variants. Building on these foundations, we propose Size-dependent Mutual Information (SMI), an information-theoretic metric that linearly correlates pre-training data characteristics, model size, and QA accuracy, without requiring any additional training. The experimental results demonstrate that SMI outperforms co-occurrence-based baselines, achieving $R^2$ > 0.75 on models with over one billion parameters. Theoretical analysis further reveals the marginal benefits of scaling model size and optimizing data, indicating that the upper limit of specific QA task accuracy is approximately 80%. Our project is available at https://github.com/yuhui1038/SMI.","authors":["Changhao Jiang","Ming Zhang","Junjie Ye","Xiaoran Fan","Yifei Cao","Jiajun Sun","Zhiheng Xi","Shihan Dou","Yi Dong","Yujiong Shen","Jingqi Tong","Zhen Wang","Tao Liang","Zhihui Fei","Mingyang Wan","Guojun Ma","Qi Zhang","Tao Gui","Xuanjing Huang"],"url":"https://arxiv.org/abs/2502.04066"}
{"created":"2025-05-14","title":"ImprovNet -- Generating Controllable Musical Improvisations with Iterative Corruption Refinement","abstract":"Despite deep learning's remarkable advances in style transfer across various domains, generating controllable performance-level musical style transfer for complete symbolically represented musical works remains a challenging area of research. Much of this is owed to limited datasets, especially for genres such as jazz, and the lack of unified models that can handle multiple music generation tasks. This paper presents ImprovNet, a transformer-based architecture that generates expressive and controllable musical improvisations through a self-supervised corruption-refinement training strategy. The improvisational style transfer is aimed at making meaningful modifications to one or more musical elements - melody, harmony or rhythm of the original composition with respect to the target genre. ImprovNet unifies multiple capabilities within a single model: it can perform cross-genre and intra-genre improvisations, harmonize melodies with genre-specific styles, and execute short prompt continuation and infilling tasks. The model's iterative generation framework allows users to control the degree of style transfer and structural similarity to the original composition. Objective and subjective evaluations demonstrate ImprovNet's effectiveness in generating musically coherent improvisations while maintaining structural relationships with the original pieces. The model outperforms Anticipatory Music Transformer in short continuation and infilling tasks and successfully achieves recognizable genre conversion, with 79\\% of participants correctly identifying jazz-style improvisations of classical pieces. Our code and demo page can be found at https://github.com/keshavbhandari/improvnet.","authors":["Keshav Bhandari","Sungkyun Chang","Tongyu Lu","Fareza R. Enus","Louis B. Bradshaw","Dorien Herremans","Simon Colton"],"url":"https://arxiv.org/abs/2502.04522"}
{"created":"2025-05-14","title":"LP-DETR: Layer-wise Progressive Relations for Object Detection","abstract":"This paper presents LP-DETR (Layer-wise Progressive DETR), a novel approach that enhances DETR-based object detection through multi-scale relation modeling. Our method introduces learnable spatial relationships between object queries through a relation-aware self-attention mechanism, which adaptively learns to balance different scales of relations (local, medium and global) across decoder layers. This progressive design enables the model to effectively capture evolving spatial dependencies throughout the detection pipeline. Extensive experiments on COCO 2017 dataset demonstrate that our method improves both convergence speed and detection accuracy compared to standard self-attention module. The proposed method achieves competitive results, reaching 52.3\\% AP with 12 epochs and 52.5\\% AP with 24 epochs using ResNet-50 backbone, and further improving to 58.0\\% AP with Swin-L backbone. Furthermore, our analysis reveals an interesting pattern: the model naturally learns to prioritize local spatial relations in early decoder layers while gradually shifting attention to broader contexts in deeper layers, providing valuable insights for future research in object detection.","authors":["Zhengjian Kang","Ye Zhang","Xiaoyu Deng","Xintao Li","Yongzhe Zhang"],"url":"https://arxiv.org/abs/2502.05147"}
{"created":"2025-05-14","title":"The Odyssey of the Fittest: Can Agents Survive and Still Be Good?","abstract":"As AI models grow in power and generality, understanding how agents learn and make decisions in complex environments is critical to promoting ethical behavior. This study introduces the Odyssey, a lightweight, adaptive text based adventure game, providing a scalable framework for exploring AI ethics and safety. The Odyssey examines the ethical implications of implementing biological drives, specifically, self preservation, into three different agents. A Bayesian agent optimized with NEAT, a Bayesian agent optimized with stochastic variational inference, and a GPT 4o agent. The agents select actions at each scenario to survive, adapting to increasingly challenging scenarios. Post simulation analysis evaluates the ethical scores of the agent decisions, uncovering the tradeoffs it navigates to survive. Specifically, analysis finds that when danger increases, agents ethical behavior becomes unpredictable. Surprisingly, the GPT 4o agent outperformed the Bayesian models in both survival and ethical consistency, challenging assumptions about traditional probabilistic methods and raising a new challenge to understand the mechanisms of LLMs' probabilistic reasoning.","authors":["Dylan Waldner","Risto Miikkulainen"],"url":"https://arxiv.org/abs/2502.05442"}
{"created":"2025-05-14","title":"DexVLA: Vision-Language Model with Plug-In Diffusion Expert for General Robot Control","abstract":"Enabling robots to perform diverse tasks across varied environments is a central challenge in robot learning. While vision-language-action (VLA) models have shown promise for generalizable robot skills, realizing their full potential requires addressing limitations in action representation and efficient training. Current VLA models often focus on scaling the vision-language model (VLM) component, while the action space representation remains a critical bottleneck. This paper introduces DexVLA, a novel framework designed to enhance the efficiency and generalization capabilities of VLAs for complex, long-horizon tasks across diverse robot embodiments. DexVLA features a novel diffusion-based action expert, scaled to one billion parameters, designed for cross-embodiment learning. A novel embodiment curriculum learning strategy facilitates efficient training: (1) pre-training the diffusion expert that is separable from the VLA on cross-embodiment data, (2) aligning the VLA model to specific embodiments, and (3) post-training for rapid adaptation to new tasks. We conduct comprehensive experiments across multiple embodiments, including single-arm, bimanual, and dexterous hand, demonstrating DexVLA's adaptability to challenging tasks without task-specific adaptation, its ability to learn dexterous skills on novel embodiments with limited data, and its capacity to complete complex, long-horizon tasks using only direct language prompting, such as laundry folding. In all settings, our method demonstrates superior performance compared to state-of-the-art models like Octo, OpenVLA, and Diffusion Policy.","authors":["Junjie Wen","Yichen Zhu","Jinming Li","Zhibin Tang","Chaomin Shen","Feifei Feng"],"url":"https://arxiv.org/abs/2502.05855"}
{"created":"2025-05-14","title":"Functional Complexity-adaptive Temporal Tensor Decomposition","abstract":"Tensor decomposition is a fundamental tool for analyzing multi-dimensional data by learning low-rank factors to represent high-order interactions. While recent works on temporal tensor decomposition have made significant progress by incorporating continuous timestamps in latent factors, they still struggle with general tensor data with continuous indexes not only in the temporal mode but also in other modes, such as spatial coordinates in climate data. Moreover, the challenge of self-adapting model complexity is largely unexplored in functional temporal tensor models, with existing methods being inapplicable in this setting. To address these limitations, we propose functional \\underline{C}omplexity-\\underline{A}daptive \\underline{T}emporal \\underline{T}ensor d\\underline{E}composition (\\textsc{Catte}).","authors":["Panqi Chen","Lei Cheng","Jianlong Li","Weichang Li","Weiqing Liu","Jiang Bian","Shikai Fang"],"url":"https://arxiv.org/abs/2502.06164"}
{"created":"2025-05-14","title":"MGPATH: Vision-Language Model with Multi-Granular Prompt Learning for Few-Shot WSI Classification","abstract":"Whole slide pathology image classification presents challenges due to gigapixel image sizes and limited annotation labels, hindering model generalization. This paper introduces a prompt learning method to adapt large vision-language models for few-shot pathology classification. We first extend the Prov-GigaPath vision foundation model, pre-trained on 1.3 billion pathology image tiles, into a vision-language model by adding adaptors and aligning it with medical text encoders via contrastive learning on 923K image-text pairs. The model is then used to extract visual features and text embeddings from few-shot annotations and fine-tunes with learnable prompt embeddings. Unlike prior methods that combine prompts with frozen features using prefix embeddings or self-attention, we propose multi-granular attention that compares interactions between learnable prompts with individual image patches and groups of them. This approach improves the model's ability to capture both fine-grained details and broader context, enhancing its recognition of complex patterns across sub-regions. To further improve accuracy, we leverage (unbalanced) optimal transport-based visual-text distance to secure model robustness by mitigating perturbations that might occur during the data augmentation process. Empirical experiments on lung, kidney, and breast pathology modalities validate the effectiveness of our approach; thereby, we surpass several of the latest competitors and consistently improve performance across diverse architectures, including CLIP, PLIP, and Prov-GigaPath integrated PLIP. We release our implementations and pre-trained models at this MGPATH.","authors":["Anh-Tien Nguyen","Duy Minh Ho Nguyen","Nghiem Tuong Diep","Trung Quoc Nguyen","Nhat Ho","Jacqueline Michelle Metsch","Miriam Cindy Maurer","Daniel Sonntag","Hanibal Bohnenberger","Anne-Christin Hauschild"],"url":"https://arxiv.org/abs/2502.07409"}
{"created":"2025-05-14","title":"Towards a Formal Theory of the Need for Competence via Computational Intrinsic Motivation","abstract":"Computational modelling offers a powerful tool for formalising psychological theories, making them more transparent, testable, and applicable in digital contexts. Yet, the question often remains: how should one computationally model a theory? We provide a demonstration of how formalisms taken from artificial intelligence can offer a fertile starting point. Specifically, we focus on the \"need for competence\", postulated as a key basic psychological need within Self-Determination Theory (SDT) -- arguably the most influential framework for intrinsic motivation (IM) in psychology. Recent research has identified multiple distinct facets of competence in key SDT texts: effectance, skill use, task performance, and capacity growth. We draw on the computational IM literature in reinforcement learning to suggest that different existing formalisms may be appropriate for modelling these different facets. Using these formalisms, we reveal underlying preconditions that SDT fails to make explicit, demonstrating how computational models can improve our understanding of IM. More generally, our work can support a cycle of theory development by inspiring new computational models, which can then be tested empirically to refine the theory. Thus, we provide a foundation for advancing competence-related theory in SDT and motivational psychology more broadly.","authors":["Erik M. Lintunen","Nadia M. Ady","Sebastian Deterding","Christian Guckelsberger"],"url":"https://arxiv.org/abs/2502.07423"}
{"created":"2025-05-14","title":"Joint Metric Space Embedding by Unbalanced OT with Gromov-Wasserstein Marginal Penalization","abstract":"We propose a new approach for unsupervised alignment of heterogeneous datasets, which maps data from two different domains without any known correspondences to a common metric space. Our method is based on an unbalanced optimal transport problem with Gromov-Wasserstein marginal penalization. It can be seen as a counterpart to the recently introduced joint multidimensional scaling method. We prove that there exists a minimizer of our functional and that for penalization parameters going to infinity, the corresponding sequence of minimizers converges to a minimizer of the so-called embedded Wasserstein distance. Our model can be reformulated as a quadratic, multi-marginal, unbalanced optimal transport problem, for which a bi-convex relaxation admits a numerical solver via block-coordinate descent. We provide numerical examples for joint embeddings in Euclidean as well as non-Euclidean spaces.","authors":["Florian Beier","Moritz Piening","Robert Beinert","Gabriele Steidl"],"url":"https://arxiv.org/abs/2502.07510"}
{"created":"2025-05-14","title":"Emotional EEG Classification using Upscaled Connectivity Matrices","abstract":"In recent studies of emotional EEG classification, connectivity matrices have been successfully employed as input to convolutional neural networks (CNNs), which can effectively consider inter-regional interaction patterns in EEG. However, we find that such an approach has a limitation that important patterns in connectivity matrices may be lost during the convolutional operations in CNNs. To resolve this issue, we propose and validate an idea to upscale the connectivity matrices to strengthen the local patterns. Experimental results demonstrate that this simple idea can significantly enhance the classification performance.","authors":["Chae-Won Lee","Jong-Seok Lee"],"url":"https://arxiv.org/abs/2502.07843"}
{"created":"2025-05-14","title":"Tensor parametric Hamiltonian operator inference","abstract":"This work presents a tensorial approach to constructing data-driven reduced-order models corresponding to semi-discrete partial differential equations with canonical Hamiltonian structure. By expressing parameter-varying operators with affine dependence as contractions of a generalized parameter vector against a constant tensor, this method leverages the operator inference framework to capture parametric dependence in the learned reduced-order model via the solution to a convex, least-squares optimization problem. This leads to a concise and straightforward implementation which compactifies previous parametric operator inference approaches and directly extends to learning parametric operators with symmetry constraints, a key feature required for constructing structure-preserving surrogates of Hamiltonian systems. The proposed approach is demonstrated on both a (non-Hamiltonian) heat equation with variable diffusion coefficient as well as a Hamiltonian wave equation with variable wave speed.","authors":["Arjun Vijaywargiya","Shane A. McQuarrie","Anthony Gruber"],"url":"https://arxiv.org/abs/2502.10888"}
{"created":"2025-05-14","title":"CAST: Component-Aligned 3D Scene Reconstruction from an RGB Image","abstract":"Recovering high-quality 3D scenes from a single RGB image is a challenging task in computer graphics. Current methods often struggle with domain-specific limitations or low-quality object generation. To address these, we propose CAST (Component-Aligned 3D Scene Reconstruction from a Single RGB Image), a novel method for 3D scene reconstruction and recovery. CAST starts by extracting object-level 2D segmentation and relative depth information from the input image, followed by using a GPT-based model to analyze inter-object spatial relationships. This enables the understanding of how objects relate to each other within the scene, ensuring more coherent reconstruction. CAST then employs an occlusion-aware large-scale 3D generation model to independently generate each object's full geometry, using MAE and point cloud conditioning to mitigate the effects of occlusions and partial object information, ensuring accurate alignment with the source image's geometry and texture. To align each object with the scene, the alignment generation model computes the necessary transformations, allowing the generated meshes to be accurately placed and integrated into the scene's point cloud. Finally, CAST incorporates a physics-aware correction step that leverages a fine-grained relation graph to generate a constraint graph. This graph guides the optimization of object poses, ensuring physical consistency and spatial coherence. By utilizing Signed Distance Fields (SDF), the model effectively addresses issues such as occlusions, object penetration, and floating objects, ensuring that the generated scene accurately reflects real-world physical interactions. CAST can be leveraged in robotics, enabling efficient real-to-simulation workflows and providing realistic, scalable simulation environments for robotic systems.","authors":["Kaixin Yao","Longwen Zhang","Xinhao Yan","Yan Zeng","Qixuan Zhang","Wei Yang","Lan Xu","Jiayuan Gu","Jingyi Yu"],"url":"https://arxiv.org/abs/2502.12894"}
{"created":"2025-05-14","title":"Building Age Estimation: A New Multi-Modal Benchmark Dataset and Community Challenge","abstract":"Estimating the construction year of buildings is of great importance for sustainability. Sustainable buildings minimize energy consumption and are a key part of responsible and sustainable urban planning and development to effectively combat climate change. By using Artificial Intelligence (AI) and recently proposed powerful Transformer models, we are able to estimate the construction epoch of buildings from a multi-modal dataset. In this paper, we introduce a new benchmark multi-modal dataset, i.e. the Map your City Dataset (MyCD), containing top-view Very High Resolution (VHR) images, Earth Observation (EO) multi-spectral data from the Copernicus Sentinel-2 satellite constellation, and street-view images in many different cities in Europe that are co-localized with respect to the building under study and labelled with the construction epoch. We assess EO generalization performance on new/ previously unseen cities that have been held-out from training and appear only during inference. In this work, we present the community-based data challenge we organized based on MyCD. The AI4EO Challenge ESA MapYourCity was opened in 2024 for 4 months. In this paper, we present the Top-4 performing models of the challenge, and the evaluation results. During inference, the performance of the models using: i) both all three input modalities, and ii) only the two top-view modalities, i.e. without the street-view ground images, is examined. The evaluation results in this work show that the models to estimate the construction year of buildings are effective and can achieve good performance on this difficult important real-world task, even when inference is on previously unseen cities, as well as even when using only the two top-view modalities (i.e. VHR and Sentinel-2) during inference.","authors":["Nikolaos Dionelis","Nicolas Long\\'ep\\'e","Alessandra Feliciotti","Mattia Marconcini","Devis Peressutti","Nika Oman Kadunc","JaeWan Park","Hagai Raja Sinulingga","Steve Andreas Immanuel","Ba Tran","Caroline Arnold"],"url":"https://arxiv.org/abs/2502.13818"}
{"created":"2025-05-14","title":"Data-Driven Input-Output Control Barrier Functions","abstract":"Control Barrier Functions (CBFs) offer a framework for ensuring set invariance and designing constrained control laws. However, crafting a valid CBF relies on system-specific assumptions and the availability of an accurate system model, underscoring the need for systematic data-driven synthesis methods. This paper introduces a data-driven approach to synthesizing a CBF for discrete-time LTI systems using only input-output measurements. The method begins by computing the maximal control invariant set using an input-output data-driven representation, eliminating the need for precise knowledge of the system's order and explicit state estimation. The proposed CBF is then systematically derived from this set, which can accommodate multiple input-output constraints. Furthermore, the proposed CBF is leveraged to develop a minimally invasive safety filter that ensures recursive feasibility with an adaptive decay rate. To improve clarity, we assume a noise-free dataset, though the method can be extended using data-driven reachability to capture noise effects and handle uncertainty. Finally, the effectiveness of the proposed method is demonstrated on an unknown time-delay system.","authors":["Mohammad Bajelani","Klaske van Heusden"],"url":"https://arxiv.org/abs/2502.17688"}
{"created":"2025-05-14","title":"The Dynamics of Collective Creativity in Human-AI Hybrid Societies","abstract":"Generative AI is shaping an increasingly hybrid society, where ideas and cultural artefacs are created both by humans and intelligent machines. Human creativity is influenced in complex, nonlinear ways by the actions of AI-driven agents within their social networks, but these influences are difficult to measure using traditional methods. This study examines how human-AI interactions shape the evolution of collective creation within large-scale social network experiments, where human and AI participants collectively create stories. Participants (either humans or AI) joined 5x5 grid-based networks in which stories were selected, modified, and shared over many iterations. Initially, AI-only networks showed greater creativity (rated by a separate group of human raters) and collective diversity of stories than human-only and human-AI networks. However, over time, hybrid human-AI networks became more diverse in their creations than AI-only networks. In part, this is because AI agents retained little from the original stories, while human-only networks preserved continuity. These findings highlight the value of experimental social networks in understanding human-AI hybrid societies.","authors":["Shota Shiiku","Raja Marjieh","Manuel Anglada-Tort","Nori Jacoby"],"url":"https://arxiv.org/abs/2502.17962"}
{"created":"2025-05-14","title":"Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data","abstract":"Supervised fine-tuning (SFT) has become a crucial step for aligning pretrained large language models (LLMs) using supervised datasets of input-output pairs. However, despite being supervised, SFT is inherently limited by its generative training objective. To address its limitations, the existing common strategy is to follow SFT with a separate phase of preference optimization (PO), which relies on either human-labeled preference data or a strong reward model to guide the learning process. In this paper, we address the limitations of SFT by exploring one of the most successful techniques in conventional supervised learning: discriminative learning. We introduce Discriminative Fine-Tuning (DFT), an improved variant of SFT, which mitigates the burden of collecting human-labeled preference data or training strong reward models. Unlike SFT that employs a generative approach and overlooks negative data, DFT adopts a discriminative paradigm that increases the probability of positive answers while suppressing potentially negative ones, aiming for data prediction instead of token prediction. Our contributions include: (i) a discriminative probabilistic framework for fine-tuning LLMs by explicitly modeling the discriminative likelihood of an answer among all possible outputs given an input; (ii) efficient algorithms to optimize this discriminative likelihood; and (iii) extensive experiments demonstrating DFT's effectiveness, achieving performance better than SFT and comparable to if not better than SFT$\\rightarrow$PO. The code can be found at https://github.com/Optimization-AI/DFT.","authors":["Siqi Guo","Ilgee Hong","Vicente Balmaseda","Changlong Yu","Liang Qiu","Xin Liu","Haoming Jiang","Tuo Zhao","Tianbao Yang"],"url":"https://arxiv.org/abs/2502.18679"}
{"created":"2025-05-14","title":"GraphSparseNet: a Novel Method for Large Scale Traffic Flow Prediction","abstract":"Traffic flow forecasting is a critical spatio-temporal data mining task with wide-ranging applications in intelligent route planning and dynamic traffic management. Recent advancements in deep learning, particularly through Graph Neural Networks (GNNs), have significantly enhanced the accuracy of these forecasts by capturing complex spatio-temporal dynamics. However, the scalability of GNNs remains a challenge due to their exponential growth in model complexity with increasing nodes in the graph. Existing methods to address this issue, including sparsification, decomposition, and kernel-based approaches, either do not fully resolve the complexity issue or risk compromising predictive accuracy. This paper introduces GraphSparseNet (GSNet), a novel framework designed to improve both the scalability and accuracy of GNN-based traffic forecasting models. GraphSparseNet is comprised of two core modules: the Feature Extractor and the Relational Compressor. These modules operate with linear time and space complexity, thereby reducing the overall computational complexity of the model to a linear scale. Our extensive experiments on multiple real-world datasets demonstrate that GraphSparseNet not only significantly reduces training time by 3.51x compared to state-of-the-art linear models but also maintains high predictive performance.","authors":["Weiyang Kong","Kaiqi Wu","Sen Zhang","Yubao Liu"],"url":"https://arxiv.org/abs/2502.19823"}
{"created":"2025-05-14","title":"Nano Drone-based Indoor Crime Scene Analysis","abstract":"Technologies such as robotics, Artificial Intelligence (AI), and Computer Vision (CV) can be applied to crime scene analysis (CSA) to help protect lives, facilitate justice, and deter crime, but an overview of the tasks that can be automated has been lacking. Here we follow a speculative prototyping approach: First, the STAIR tool is used to rapidly review the literature and identify tasks that seem to have not received much attention, like accessing crime scenes through a window, mapping/gathering evidence, and analyzing blood smears. Secondly, we present a prototype of a small drone that implements these three tasks with 75%, 85%, and 80% performance, to perform a minimal analysis of an indoor crime scene. Lessons learned are reported, toward guiding next work.","authors":["Martin Cooney","Sivadinesh Ponrajan","Fernando Alonso-Fernandez"],"url":"https://arxiv.org/abs/2502.21019"}
{"created":"2025-05-14","title":"Parameter-Varying Feedforward Control: A Kernel-Based Learning Approach","abstract":"The increasing demands for high accuracy in mechatronic systems necessitate the incorporation of parameter variations in feedforward control. The aim of this paper is to develop a data-driven approach for direct learning of parameter-varying feedforward control to increase tracking performance. The developed approach is based on kernel-regularized function estimation in conjunction with iterative learning to directly learn parameter-varying feedforward control from data. This approach enables high tracking performance for feedforward control of linear parameter-varying dynamics, providing flexibility to varying reference tasks. The developed framework is validated on a benchmark industrial experimental setup featuring a belt-driven carriage.","authors":["Max van Haren","Lennart Blanken","Tom Oomen"],"url":"https://arxiv.org/abs/2502.21105"}
{"created":"2025-05-14","title":"ALinFiK: Learning to Approximate Linearized Future Influence Kernel for Scalable Third-Party LLM Data Valuation","abstract":"Large Language Models (LLMs) heavily rely on high-quality training data, making data valuation crucial for optimizing model performance, especially when working within a limited budget. In this work, we aim to offer a third-party data valuation approach that benefits both data providers and model developers. We introduce a linearized future influence kernel (LinFiK), which assesses the value of individual data samples in improving LLM performance during training. We further propose ALinFiK, a learning strategy to approximate LinFiK, enabling scalable data valuation. Our comprehensive evaluations demonstrate that this approach surpasses existing baselines in effectiveness and efficiency, demonstrating significant scalability advantages as LLM parameters increase.","authors":["Yanzhou Pan","Huawei Lin","Yide Ran","Jiamin Chen","Xiaodong Yu","Weijie Zhao","Denghui Zhang","Zhaozhuo Xu"],"url":"https://arxiv.org/abs/2503.01052"}
{"created":"2025-05-14","title":"DPR: Diffusion Preference-based Reward for Offline Reinforcement Learning","abstract":"Offline preference-based reinforcement learning (PbRL) mitigates the need for reward definition, aligning with human preferences via preference-driven reward feedback without interacting with the environment. However, the effectiveness of preference-driven reward functions depends on the modeling ability of the learning model, which current MLP-based and Transformer-based methods may fail to adequately provide. To alleviate the failure of the reward function caused by insufficient modeling, we propose a novel preference-based reward acquisition method: Diffusion Preference-based Reward (DPR). Unlike previous methods using Bradley-Terry models for trajectory preferences, we use diffusion models to directly model preference distributions for state-action pairs, allowing rewards to be discriminatively obtained from these distributions. In addition, considering the particularity of preference data that only know the internal relationships of paired trajectories, we further propose Conditional Diffusion Preference-based Reward (C-DPR), which leverages relative preference information to enhance the construction of the diffusion model. We apply the above methods to existing offline reinforcement learning algorithms and a series of experiment results demonstrate that the diffusion-based reward acquisition approach outperforms previous MLP-based and Transformer-based methods.","authors":["Teng Pang","Bingzheng Wang","Guoqiang Wu","Yilong Yin"],"url":"https://arxiv.org/abs/2503.01143"}
{"created":"2025-05-14","title":"VideoUFO: A Million-Scale User-Focused Dataset for Text-to-Video Generation","abstract":"Text-to-video generative models convert textual prompts into dynamic visual content, offering wide-ranging applications in film production, gaming, and education. However, their real-world performance often falls short of user expectations. One key reason is that these models have not been trained on videos related to some topics users want to create. In this paper, we propose VideoUFO, the first Video dataset specifically curated to align with Users' FOcus in real-world scenarios. Beyond this, our VideoUFO also features: (1) minimal (0.29%) overlap with existing video datasets, and (2) videos searched exclusively via YouTube's official API under the Creative Commons license. These two attributes provide future researchers with greater freedom to broaden their training sources. The VideoUFO comprises over 1.09 million video clips, each paired with both a brief and a detailed caption (description). Specifically, through clustering, we first identify 1,291 user-focused topics from the million-scale real text-to-video prompt dataset, VidProM. Then, we use these topics to retrieve videos from YouTube, split the retrieved videos into clips, and generate both brief and detailed captions for each clip. After verifying the clips with specified topics, we are left with about 1.09 million video clips. Our experiments reveal that (1) current 16 text-to-video models do not achieve consistent performance across all user-focused topics; and (2) a simple model trained on VideoUFO outperforms others on worst-performing topics. The dataset and code are publicly available at https://huggingface.co/datasets/WenhaoWang/VideoUFO and https://github.com/WangWenhao0716/BenchUFO under the CC BY 4.0 License.","authors":["Wenhao Wang","Yi Yang"],"url":"https://arxiv.org/abs/2503.01739"}
{"created":"2025-05-14","title":"UAV-VLRR: Vision-Language Informed NMPC for Rapid Response in UAV Search and Rescue","abstract":"Emergency search and rescue (SAR) operations often require rapid and precise target identification in complex environments where traditional manual drone control is inefficient. In order to address these scenarios, a rapid SAR system, UAV-VLRR (Vision-Language-Rapid-Response), is developed in this research. This system consists of two aspects: 1) A multimodal system which harnesses the power of Visual Language Model (VLM) and the natural language processing capabilities of ChatGPT-4o (LLM) for scene interpretation. 2) A non-linearmodel predictive control (NMPC) with built-in obstacle avoidance for rapid response by a drone to fly according to the output of the multimodal system. This work aims at improving response times in emergency SAR operations by providing a more intuitive and natural approach to the operator to plan the SAR mission while allowing the drone to carry out that mission in a rapid and safe manner. When tested, our approach was faster on an average by 33.75% when compared with an off-the-shelf autopilot and 54.6% when compared with a human pilot. Video of UAV-VLRR: https://youtu.be/KJqQGKKt1xY","authors":["Yasheerah Yaqoot","Muhammad Ahsan Mustafa","Oleg Sautenkov","Artem Lykov","Valerii Serpiva","Dzmitry Tsetserukou"],"url":"https://arxiv.org/abs/2503.02465"}
{"created":"2025-05-14","title":"Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents","abstract":"With the advancement of conversational large language models (LLMs), several LLM-based Conversational Shopping Agents (CSA) have been developed to help customers smooth their online shopping. The primary objective in building an engaging and trustworthy CSA is to ensure the agent's responses about product factoids are accurate and factually grounded. However, two challenges remain. First, LLMs produce hallucinated or unsupported claims. Such inaccuracies risk spreading misinformation and diminishing customer trust. Second, without providing knowledge source attribution in CSA response, customers struggle to verify LLM-generated information. To address both challenges, we present an easily productionized solution that enables a ''citation experience'' to our customers. We build auto-evaluation metrics to holistically evaluate LLM's grounding and attribution capabilities, suggesting that citation generation paradigm substantially improves grounding performance by 13.83%. To deploy this capability at scale, we introduce Multi-UX-Inference system, which appends source citations to LLM outputs while preserving existing user experience features and supporting scalable inference. Large-scale online A/B tests show that grounded CSA responses improves customer engagement by 3% - 10%, depending on UX variations.","authors":["Jingying Zeng","Hui Liu","Zhenwei Dai","Xianfeng Tang","Chen Luo","Samarth Varshney","Zhen Li","Qi He"],"url":"https://arxiv.org/abs/2503.04830"}
{"created":"2025-05-14","title":"Semantic Shift Estimation via Dual-Projection and Classifier Reconstruction for Exemplar-Free Class-Incremental Learning","abstract":"Exemplar-Free Class-Incremental Learning (EFCIL) aims to sequentially learn from distinct categories without retaining exemplars but easily suffers from catastrophic forgetting of learned knowledge. While existing EFCIL methods leverage knowledge distillation to alleviate forgetting, they still face two critical challenges: semantic shift and decision bias. Specifically, the embeddings of old tasks shift in the embedding space after learning new tasks, and the classifier becomes biased towards new tasks due to training solely with new data, hindering the balance between old and new knowledge. To address these issues, we propose the Dual-Projection Shift Estimation and Classifier Reconstruction (DPCR) approach for EFCIL. DPCR effectively estimates semantic shift through a dual-projection, which combines a learnable transformation with a row-space projection to capture both task-wise and category-wise shifts. Furthermore, to mitigate decision bias, DPCR employs ridge regression to reformulate a classifier reconstruction process. This reconstruction exploits previous in covariance and prototype of each class after calibration with estimated shift, thereby reducing decision bias. Extensive experiments demonstrate that, on various datasets, DPCR effectively balances old and new tasks, outperforming state-of-the-art EFCIL methods. Our codes are available at https://github.com/RHe502/ICML25-DPCR.","authors":["Run He","Di Fang","Yicheng Xu","Yawen Cui","Ming Li","Cen Chen","Ziqian Zeng","Huiping Zhuang"],"url":"https://arxiv.org/abs/2503.05423"}
{"created":"2025-05-14","title":"Motion Blender Gaussian Splatting for Dynamic Scene Reconstruction","abstract":"Gaussian splatting has emerged as a powerful tool for high-fidelity reconstruction of dynamic scenes. However, existing methods primarily rely on implicit motion representations, such as encoding motions into neural networks or per-Gaussian parameters, which makes it difficult to further manipulate the reconstructed motions. This lack of explicit controllability limits existing methods to replaying recorded motions only, which hinders a wider application in robotics. To address this, we propose Motion Blender Gaussian Splatting (MBGS), a novel framework that uses motion graphs as an explicit and sparse motion representation. The motion of a graph's links is propagated to individual Gaussians via dual quaternion skinning, with learnable weight painting functions that determine the influence of each link. The motion graphs and 3D Gaussians are jointly optimized from input videos via differentiable rendering. Experiments show that MBGS achieves state-of-the-art performance on the highly challenging iPhone dataset while being competitive on HyperNeRF. We demonstrate the application potential of our method in animating novel object poses, synthesizing real robot demonstrations, and predicting robot actions through visual planning. The source code, models, video demonstrations can be found at http://mlzxy.github.io/motion-blender-gs.","authors":["Xinyu Zhang","Haonan Chang","Yuhan Liu","Abdeslam Boularias"],"url":"https://arxiv.org/abs/2503.09040"}
{"created":"2025-05-14","title":"Early Detection of Forest Calamities in Homogeneous Stands -- Deep Learning Applied to Bark-Beetle Outbreaks","abstract":"Climate change has increased the vulnerability of forests to insect-related damage, resulting in widespread forest loss in Central Europe and highlighting the need for effective, continuous monitoring systems. Remote sensing based forest health monitoring, oftentimes, relies on supervised machine learning algorithms that require labeled training data. Monitoring temporal patterns through time series analysis offers a potential alternative for earlier detection of disturbance but requires substantial storage resources. This study investigates the potential of a Deep Learning algorithm based on a Long Short Term Memory (LSTM) Autoencoder for the detection of anomalies in forest health (e.g. bark beetle outbreaks), utilizing Sentinel-2 time series data. This approach is an alternative to supervised machine learning methods, avoiding the necessity for labeled training data. Furthermore, it is more memory-efficient than other time series analysis approaches, as a robust model can be created using only a 26-week-long time series as input. In this study, we monitored pure stands of spruce in Thuringia, Germany, over a 7-year period from 2018 to the end of 2024. Our best model achieved a detection accuracy of 87% on test data and was able to detect 61% of all anomalies at a very early stage (more than a month before visible signs of forest degradation). Compared to another widely used time series break detection algorithm - BFAST (Breaks For Additive Season and Trend), our approach consistently detected higher percentage of anomalies at an earlier stage. These findings suggest that LSTM-based Autoencoders could provide a promising, resource-efficient approach to forest health monitoring, enabling more timely responses to emerging threats.","authors":["Maximilian Kirsch","Jakob Wernicke","Pawan Datta","Christine Preisach"],"url":"https://arxiv.org/abs/2503.12883"}
{"created":"2025-05-14","title":"CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning","abstract":"Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding,Reasoning and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and assisting scientists in realistic workflows. This benchmark introduces ten challenging tasks with a total of 580 problems and solution pairs curated by experts in six disciplines - materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins - covering both experimental and theoretical work-flows in science. We evaluate a range of closed and open LLMs on tasks in CURIE which requires domain expertise, comprehension of long in-context information,and multi-step reasoning. While Gemini Flash 2.0 and Claude-3 show consistent high comprehension across domains, the popular GPT-4o and command-R+ fail dramatically on protein sequencing tasks. With the best performance at 32% there is much room for improvement for all models. We hope that insights gained from CURIE can guide the future development of LLMs in sciences. Evaluation code and data are in https://github.com/google/curie","authors":["Hao Cui","Zahra Shamsi","Gowoon Cheon","Xuejian Ma","Shutong Li","Maria Tikhanovskaya","Peter Norgaard","Nayantara Mudur","Martyna Plomecka","Paul Raccuglia","Yasaman Bahri","Victor V. Albert","Pranesh Srinivasan","Haining Pan","Philippe Faist","Brian Rohr","Ekin Dogus Cubuk","Muratahan Aykol","Amil Merchant","Michael J. Statt","Dan Morris","Drew Purves","Elise Kleeman","Ruth Alcantara","Matthew Abraham","Muqthar Mohammad","Ean Phing VanLee","Chenfei Jiang","Elizabeth Dorfman","Eun-Ah Kim","Michael P Brenner","Viren Jain","Sameera Ponda","Subhashini Venugopalan"],"url":"https://arxiv.org/abs/2503.13517"}
{"created":"2025-05-14","title":"SCAN-BEST: Sub-6GHz-Aided Near-field Beam Selection with Formal Reliability Guarantees","abstract":"As millimeter-wave (mmWave) MIMO systems adopt larger antenna arrays, near-field propagation becomes increasingly prominent, especially for users close to the transmitter. Traditional far-field beam training methods become inadequate, while near-field training faces the challenge of large codebooks due to the need to resolve both angular and distance domains. To reduce in-band training overhead, prior work has proposed to leverage the spatial-temporal congruence between sub-6 GHz (sub-6G) and mmWave channels to predict the best mmWave beam within a near-field codebook from sub-6G channel estimates. To cope with the uncertainty caused by sub-6G/mmWave differences, we introduce a novel Sub-6G Channel Aided Near-field BEam SelecTion (SCAN-BEST) framework that wraps around any beam predictor to produce candidate beam subset with formal suboptimality guarantees. The proposed SCAN-BEST builds on conformal risk control (CRC), and is calibrated offline using limited calibration data. Its performance guarantees apply even in the presence of statistical shifts between calibration and deployment. Numerical results validate the theoretical properties and efficiency of SCAN-BEST.","authors":["Weicao Deng","Binpu Shi","Min Li","Osvaldo Simeone"],"url":"https://arxiv.org/abs/2503.13801"}
{"created":"2025-05-14","title":"Governance of Ledger-Anchored Decentralized Identifiers","abstract":"A Decentralized Identifier (DID) empowers an entity to prove control over a unique and self-issued identifier without relying on any identity provider. The public key material for the proof is encoded into an associated DID document (DDO). This is preferable shared via a distributed ledger because it guarantees algorithmically that everyone has access to the latest state of any tamper-proof DDO but only the entities in control of a DID are able to update theirs. Yet, it is possible to grant deputies the authority to update the DDO on behalf of the DID owner. However, the DID specification leaves largely open on how authorizations over a DDO are managed and enforced among multiple deputies. This article investigates what it means to govern a DID and discusses various forms of how a DID can be controlled by potentially more than one entity. It also presents a prototype of a DID-conform identifier management system where a selected set of governance policies are deployed as Smart Contracts. The article highlights the critical role of governance for the trustworthy and flexible deployment of ledger-anchored DIDs across various domains.","authors":["Sandro Rodriguez Garzon","Carlo Segat","Axel K\\\"upper"],"url":"https://arxiv.org/abs/2503.16972"}
{"created":"2025-05-14","title":"Sample-Efficient Reinforcement Learning of Koopman eNMPC","abstract":"Reinforcement learning (RL) can be used to tune data-driven (economic) nonlinear model predictive controllers ((e)NMPCs) for optimal performance in a specific control task by optimizing the dynamic model or parameters in the policy's objective function or constraints, such as state bounds. However, the sample efficiency of RL is crucial, and to improve it, we combine a model-based RL algorithm with our published method that turns Koopman (e)NMPCs into automatically differentiable policies. We apply our approach to an eNMPC case study of a continuous stirred-tank reactor (CSTR) model from the literature. The approach outperforms benchmark methods, i.e., data-driven eNMPCs using models based on system identification without further RL tuning of the resulting policy, and neural network controllers trained with model-based RL, by achieving superior control performance and higher sample efficiency. Furthermore, utilizing partial prior knowledge about the system dynamics via physics-informed learning further increases sample efficiency.","authors":["Daniel Mayfrank","Mehmet Velioglu","Alexander Mitsos","Manuel Dahmen"],"url":"https://arxiv.org/abs/2503.18787"}
{"created":"2025-05-14","title":"From S4 to Mamba: A Comprehensive Survey on Structured State Space Models","abstract":"Recent advancements in sequence modeling have led to the emergence of Structured State Space Models (SSMs) as an efficient alternative to Recurrent Neural Networks (RNNs) and Transformers, addressing challenges in long-range dependency modeling and computational efficiency. While RNNs suffer from vanishing gradients and sequential inefficiencies, and Transformers face quadratic complexity, SSMs leverage structured recurrence and state-space representations to achieve superior long-sequence processing with linear or near-linear complexity. This survey provides a comprehensive review of SSMs, tracing their evolution from the foundational S4 model to its successors like Mamba, Simplified Structured State Space Sequence Model (S5), and Jamba, highlighting their improvements in computational efficiency, memory optimization, and inference speed. By comparing SSMs with traditional sequence models across domains such as natural language processing (NLP), speech recognition, vision, and time-series forecasting, we demonstrate their advantages in handling long-range dependencies while reducing computational overhead. Despite their potential, challenges remain in areas such as training optimization, hybrid modeling, and interpretability. This survey serves as a structured guide for researchers and practitioners, detailing the advancements, trade-offs, and future directions of SSM-based architectures in AI and deep learning.","authors":["Shriyank Somvanshi","Md Monzurul Islam","Mahmuda Sultana Mimi","Sazzad Bin Bashar Polock","Gaurab Chhetri","Subasish Das"],"url":"https://arxiv.org/abs/2503.18970"}
{"created":"2025-05-14","title":"A Wong--Zakai resonance-based integrator for nonlinear Schr\\\"odinger equation with white noise dispersion","abstract":"We introduce a novel approach to numerical approximation of nonlinear Schr\\\"odinger equation with white noise dispersion in the regime of low-regularity solutions. Approximating such solutions in the stochastic setting is particularly challenging due to randomized frequency interactions and presents a compelling challenge for the construction of tailored schemes. In particular, we design the first resonance-based schemes for this equation, which achieve provable convergence for solutions of much lower regularity than previously required. A crucial ingredient in this construction is the Wong--Zakai approximation of stochastic dispersive system, which introduces piecewise linear phases that capture nonlinear frequency interactions and can subsequently be approximated to construct resonance-based schemes. We prove the well-posedness of the Wong--Zakai approximated equation and establish its proximity to the original full stochastic dispersive system. Based on this approximation, we demonstrate an improved strong convergence rate for our new scheme, which exploits the stochastic nature of the dispersive terms. Finally, we provide numerical experiments underlining the favourable performance of our novel method in practice.","authors":["Jianbo Cui","Georg Maierhofer"],"url":"https://arxiv.org/abs/2503.19346"}
{"created":"2025-05-14","title":"High-Quality Spatial Reconstruction and Orthoimage Generation Using Efficient 2D Gaussian Splatting","abstract":"Highly accurate geometric precision and dense image features characterize True Digital Orthophoto Maps (TDOMs), which are in great demand for applications such as urban planning, infrastructure management, and environmental monitoring.Traditional TDOM generation methods need sophisticated processes, such as Digital Surface Models (DSM) and occlusion detection, which are computationally expensive and prone to errors.This work presents an alternative technique rooted in 2D Gaussian Splatting (2DGS), free of explicit DSM and occlusion detection. With depth map generation, spatial information for every pixel within the TDOM is retrieved and can reconstruct the scene with high precision. Divide-and-conquer strategy achieves excellent GS training and rendering with high-resolution TDOMs at a lower resource cost, which preserves higher quality of rendering on complex terrain and thin structure without a decrease in efficiency. Experimental results demonstrate the efficiency of large-scale scene reconstruction and high-precision terrain modeling. This approach provides accurate spatial data, which assists users in better planning and decision-making based on maps.","authors":["Qian Wang","Zhihao Zhan","Jialei He","Zhituo Tu","Xiang Zhu","Jie Yuan"],"url":"https://arxiv.org/abs/2503.19703"}
{"created":"2025-05-14","title":"Automated Reasoning in Blockchain: Foundations, Applications, and Frontiers","abstract":"Blockchain technology has emerged as a transformative paradigm for decentralized and secure data management across diverse application domains, including healthcare, supply chain management, and the Internet of Things. Its core features, such as decentralization, immutability, and auditability, achieved through distributed consensus algorithms and cryptographic techniques, offer significant advantages for multi-stakeholder applications requiring transparency and trust. However, the inherent complexity and security-critical nature of blockchain systems necessitate rigorous analysis and verification to ensure their correctness, reliability, and resilience against potential vulnerabilities.","authors":["Hojer Key"],"url":"https://arxiv.org/abs/2503.20461"}
{"created":"2025-05-14","title":"A Quantum Constraint Generation Framework for Binary Linear Programs","abstract":"We propose a new approach to utilize quantum computers for binary linear programming (BLP), which can be extended to general integer linear programs (ILP). Quantum optimization algorithms, hybrid or quantum-only, are currently general purpose, standalone solvers for ILP. However, to consider them practically useful, we expect them to overperform the current state of the art classical solvers. That expectation is unfair to quantum algorithms: in classical ILP solvers, after many decades of evolution, many different algorithms work together as a robust machine to get the best result. This is the approach we would like to follow now with our quantum 'solver' solutions. In this study we wrap any suitable quantum optimization algorithm into a quantum informed classical constraint generation framework. First we relax our problem by dropping all constraints and encode it into an Ising Hamiltonian for the quantum optimization subroutine. Then, by sampling from the solution state of the subroutine, we obtain information about constraint violations in the initial problem, from which we decide which coupling terms we need to introduce to the Hamiltonian. The coupling terms correspond to the constraints of the initial binary linear program. Then we optimize over the new Hamiltonian again, until we reach a feasible solution, or other stopping conditions hold. Since one can decide how many constraints they add to the Hamiltonian in a single step, our algorithm is at least as efficient as the (hybrid) quantum optimization algorithm it wraps. We support our claim with results on small scale minimum cost exact cover problem instances.","authors":["Andr\\'as Cz\\'egel","Bogl\\'arka G. -T\\'oth"],"url":"https://arxiv.org/abs/2503.21222"}
{"created":"2025-05-14","title":"Adaptive Integrated Layered Attention (AILA)","abstract":"We propose Adaptive Integrated Layered Attention (AILA), a neural network architecture that combines dense skip connections with different mechanisms for adaptive feature reuse across network layers. We evaluate AILA on three challenging tasks: price forecasting for various commodities and indices (S&amp;P 500, Gold, US dollar Futures, Coffee, Wheat), image recognition using the CIFAR-10 dataset, and sentiment analysis on the IMDB movie review dataset. In all cases, AILA matches strong deep learning baselines (LSTMs, Transformers, and ResNets), achieving it at a fraction of the training and inference time. Notably, we implement and test two versions of the model - AILA-Architecture 1, which uses simple linear layers as the connection mechanism between layers, and AILA-Architecture 2, which implements an attention mechanism to selectively focus on outputs from previous layers. Both architectures are applied in a single-task learning setting, with each model trained separately for individual tasks. Results confirm that AILA's adaptive inter-layer connections yield robust gains by flexibly reusing pertinent features at multiple network depths. The AILA approach thus presents an extension to existing architectures, improving long-range sequence modeling, image recognition with optimised computational speed, and SOTA classification performance in practice.","authors":["William Claster","Suhas KM","Dhairya Gundechia"],"url":"https://arxiv.org/abs/2503.22742"}
{"created":"2025-05-14","title":"Reasoning Under Threat: Symbolic and Neural Techniques for Cybersecurity Verification","abstract":"Cybersecurity demands rigorous and scalable techniques to ensure system correctness, robustness, and resilience against evolving threats. Automated reasoning, encompassing formal logic, theorem proving, model checking, and symbolic analysis, provides a foundational framework for verifying security properties across diverse domains such as access control, protocol design, vulnerability detection, and adversarial modeling. This survey presents a comprehensive overview of the role of automated reasoning in cybersecurity, analyzing how logical systems, including temporal, deontic, and epistemic logics are employed to formalize and verify security guarantees. We examine SOTA tools and frameworks, explore integrations with AI for neural-symbolic reasoning, and highlight critical research gaps, particularly in scalability, compositionality, and multi-layered security modeling. The paper concludes with a set of well-grounded future research directions, aiming to foster the development of secure systems through formal, automated, and explainable reasoning techniques.","authors":["Sarah Veronica"],"url":"https://arxiv.org/abs/2503.22755"}
{"created":"2025-05-14","title":"Efficient Adaptation For Remote Sensing Visual Grounding","abstract":"Adapting pre-trained models has become an effective strategy in artificial intelligence, offering a scalable and efficient alternative to training models from scratch. In the context of remote sensing (RS), where visual grounding(VG) remains underexplored, this approach enables the deployment of powerful vision-language models to achieve robust cross-modal understanding while significantly reducing computational overhead. To address this, we applied Parameter Efficient Fine Tuning (PEFT) techniques to adapt these models for RS-specific VG tasks. Specifically, we evaluated LoRA placement across different modules in Grounding DINO and used BitFit and adapters to fine-tune the OFA foundation model pre-trained on general-purpose VG datasets. This approach achieved performance comparable to or surpassing current State Of The Art (SOTA) models while significantly reducing computational costs. This study highlights the potential of PEFT techniques to advance efficient and precise multi-modal analysis in RS, offering a practical and cost-effective alternative to full model training.","authors":["Hasan Moughnieh","Mohamad Chalhoub","Hasan Nasrallah","Cristiano Nattero","Paolo Campanella","Giovanni Nico","Ali J. Ghandour"],"url":"https://arxiv.org/abs/2503.23083"}
{"created":"2025-05-14","title":"A SAT-centered XAI method for Deep Learning based Video Understanding","abstract":"This paper introduces a novel formal SAT-based explanation model for deep learning in video understanding. The proposed method integrates SAT solving techniques with the principles of formal explainable AI to address the limitations of existing XAI techniques in this domain. By encoding deep learning models and video data into a logical framework and formulating explanation queries as satisfiability problems, the method aims to generate logic-based explanations with formal guarantees. The paper details the conceptual framework, the process of encoding deep learning models and video data, the formulation of \"Why?\" and \"Why not?\" questions, and a novel architecture integrating a SAT solver with a deep learning video understanding model. While challenges related to computational complexity and the representational power of propositional logic remain, the proposed approach offers a promising direction for enhancing the explainability of deep learning in the complex and critical domain of video understanding.","authors":["Hojer Key"],"url":"https://arxiv.org/abs/2503.23870"}
{"created":"2025-05-14","title":"Crossing Boundaries: Leveraging Semantic Divergences to Explore Cultural Novelty in Cooking Recipes","abstract":"Novelty modeling and detection is a core topic in Natural Language Processing (NLP), central to numerous tasks such as recommender systems and automatic summarization. It involves identifying pieces of text that deviate in some way from previously known information. However, novelty is also a crucial determinant of the unique perception of relevance and quality of an experience, as it rests upon each individual's understanding of the world. Social factors, particularly cultural background, profoundly influence perceptions of novelty and innovation. Cultural novelty arises from differences in salience and novelty as shaped by the distance between distinct communities. While cultural diversity has garnered increasing attention in artificial intelligence (AI), the lack of robust metrics for quantifying cultural novelty hinders a deeper understanding of these divergences. This gap limits quantifying and understanding cultural differences within computational frameworks. To address this, we propose an interdisciplinary framework that integrates knowledge from sociology and management. Central to our approach is GlobalFusion, a novel dataset comprising 500 dishes and approximately 100,000 cooking recipes capturing cultural adaptation from over 150 countries. By introducing a set of Jensen-Shannon Divergence metrics for novelty, we leverage this dataset to analyze textual divergences when recipes from one community are modified by another with a different cultural background. The results reveal significant correlations between our cultural novelty metrics and established cultural measures based on linguistic, religious, and geographical distances. Our findings highlight the potential of our framework to advance the understanding and measurement of cultural diversity in AI.","authors":["Florian Carichon","Romain Rampa","Golnoosh Farnadi"],"url":"https://arxiv.org/abs/2503.24027"}
{"created":"2025-05-14","title":"Schr\\\"odinger Diffusion Driven Signal Recovery in 3T BOLD fMRI Using Unmatched 7T Observations","abstract":"Ultra-high-field (7 Tesla) BOLD fMRI offers exceptional detail in both spatial and temporal domains, along with robust signal-to-noise characteristics, making it a powerful modality for studying visual information processing in the brain. However, due to the limited accessibility of 7T scanners, the majority of neuroimaging studies are still conducted using 3T systems, which inherently suffer from reduced fidelity in both resolution and SNR. To mitigate this limitation, we introduce a new computational approach designed to enhance the quality of 3T BOLD fMRI acquisitions. Specifically, we project both 3T and 7T datasets, sourced from different individuals and experimental setups, into a shared low-dimensional representation space. Within this space, we employ a lightweight, unsupervised Schr\\\"odinger Bridge framework to infer a high-SNR, high-resolution counterpart of the 3T data, without relying on paired supervision. This methodology is evaluated across multiple fMRI retinotopy datasets, including synthetically generated samples, and demonstrates a marked improvement in the reliability and fit of population receptive field (pRF) models applied to the enhanced 3T outputs. Our findings suggest that it is feasible to computationally approximate 7T-level quality from standard 3T acquisitions.","authors":["Yujian Xiong","Xuanzhao Dong","Sebastian Waz","Wenhui Zhu","Negar Mallak","Zhong-lin Lu","Yalin Wang"],"url":"https://arxiv.org/abs/2504.01004"}
{"created":"2025-05-14","title":"Local Constant Approximation for Dominating Set on Graphs Excluding Large Minors","abstract":"We show that graphs excluding $K_{2,t}$ as a minor admit a $f(t)$-round $50$-approximation deterministic distributed algorithm for Minimum Dominating Set. The result extends to Minimum Vertex Cover. Though fast and approximate distributed algorithms for such problems were already known for $H$-minor-free graphs, all of them have an approximation ratio depending on the size of $H$. To the best of our knowledge, this is the first example of a large non-trivial excluded minor leading to fast and constant-approximation distributed algorithms, where the ratio is independent of the size of $H$. A new key ingredient in the analysis of these distributed algorithms is the use of asymptotic dimension.","authors":["Marthe Bonamy","Cyril Gavoille","Timoth\\'e Picavet","Alexandra Wesolek"],"url":"https://arxiv.org/abs/2504.01091"}
{"created":"2025-05-14","title":"Asynchronous Traffic Shaping and Redundancy: Avoiding Unbounded Latencies in In-Car Networks","abstract":"Time-Sensitive Networking enhances Ethernet-based In-Vehicle Networks (IVNs) with real-time capabilities. Different traffic shaping algorithms have been proposed for time-critical communication, of which the Asynchronous Traffic Shaper (ATS) is an upcoming candidate. However, recent research has shown that ATS can introduce unbounded latencies when shaping traffic from non-FIFO systems. This impacts the applicability of ATS in IVNs, as these networks often use redundancy mechanisms, i.e. Frame Replication and Elimination for Reliability (FRER), that can cause non-FIFO behavior. In this paper, we approach the problem of accumulated delays from ATS by analyzing the scenarios that generate latency and by devising placement and configuration methods for ATS schedulers to prevent this behavior. We evaluate our approach in a simulation environment and show how it prevents conditions of unbounded delays. In an IVN simulation case study, we demonstrate the occurrence of unbounded latencies in a realistic scenario and validate the effectiveness of our solutions in avoiding them.","authors":["Teresa L\\\"ubeck","Philipp Meyer","Timo H\\\"ackel","Franz Korf","Thomas C. Schmidt"],"url":"https://arxiv.org/abs/2504.01946"}
{"created":"2025-05-14","title":"Learning Phase Distortion with Selective State Space Models for Video Turbulence Mitigation","abstract":"Atmospheric turbulence is a major source of image degradation in long-range imaging systems. Although numerous deep learning-based turbulence mitigation (TM) methods have been proposed, many are slow, memory-hungry, and do not generalize well. In the spatial domain, methods based on convolutional operators have a limited receptive field, so they cannot handle a large spatial dependency required by turbulence. In the temporal domain, methods relying on self-attention can, in theory, leverage the lucky effects of turbulence, but their quadratic complexity makes it difficult to scale to many frames. Traditional recurrent aggregation methods face parallelization challenges.","authors":["Xingguang Zhang","Nicholas Chimitt","Xijun Wang","Yu Yuan","Stanley H. Chan"],"url":"https://arxiv.org/abs/2504.02697"}
{"created":"2025-05-14","title":"Computing High-dimensional Confidence Sets for Arbitrary Distributions","abstract":"We study the problem of learning a high-density region of an arbitrary distribution over $\\mathbb{R}^d$. Given a target coverage parameter $\\delta$, and sample access to an arbitrary distribution $D$, we want to output a confidence set $S \\subset \\mathbb{R}^d$ such that $S$ achieves $\\delta$ coverage of $D$, i.e., $\\mathbb{P}_{y \\sim D} \\left[ y \\in S \\right] \\ge \\delta$, and the volume of $S$ is as small as possible. This is a central problem in high-dimensional statistics with applications in finding confidence sets, uncertainty quantification, and support estimation.","authors":["Chao Gao","Liren Shan","Vaidehi Srinivas","Aravindan Vijayaraghavan"],"url":"https://arxiv.org/abs/2504.02723"}
{"created":"2025-05-14","title":"Why do LLMs attend to the first token?","abstract":"Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.","authors":["Federico Barbero","\\'Alvaro Arroyo","Xiangming Gu","Christos Perivolaropoulos","Michael Bronstein","Petar Veli\\v{c}kovi\\'c","Razvan Pascanu"],"url":"https://arxiv.org/abs/2504.02732"}
{"created":"2025-05-14","title":"AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening","abstract":"Resume screening is a critical yet time-intensive process in talent acquisition, requiring recruiters to analyze vast volume of job applications while remaining objective, accurate, and fair. With the advancements in Large Language Models (LLMs), their reasoning capabilities and extensive knowledge bases demonstrate new opportunities to streamline and automate recruitment workflows. In this work, we propose a multi-agent framework for resume screening using LLMs to systematically process and evaluate resumes. The framework consists of four core agents, including a resume extractor, an evaluator, a summarizer, and a score formatter. To enhance the contextual relevance of candidate assessments, we integrate Retrieval-Augmented Generation (RAG) within the resume evaluator, allowing incorporation of external knowledge sources, such as industry-specific expertise, professional certifications, university rankings, and company-specific hiring criteria. This dynamic adaptation enables personalized recruitment, bridging the gap between AI automation and talent acquisition. We assess the effectiveness of our approach by comparing AI-generated scores with ratings provided by HR professionals on a dataset of anonymized online resumes. The findings highlight the potential of multi-agent RAG-LLM systems in automating resume screening, enabling more efficient and scalable hiring workflows.","authors":["Frank P. -W. Lo","Jianing Qiu","Zeyu Wang","Haibao Yu","Yeming Chen","Gao Zhang","Benny Lo"],"url":"https://arxiv.org/abs/2504.02870"}
{"created":"2025-05-14","title":"Transformer representation learning is necessary for dynamic multi-modal physiological data on small-cohort patients","abstract":"Postoperative delirium (POD), a severe neuropsychiatric complication affecting nearly 50% of high-risk surgical patients, is defined as an acute disorder of attention and cognition, It remains significantly underdiagnosed in the intensive care units (ICUs) due to subjective monitoring methods. Early and accurate diagnosis of POD is critical and achievable. Here, we propose a POD prediction framework comprising a Transformer representation model followed by traditional machine learning algorithms. Our approaches utilizes multi-modal physiological data, including amplitude-integrated electroencephalography (aEEG), vital signs, electrocardiographic monitor data as well as hemodynamic parameters. We curated the first multi-modal POD dataset encompassing two patient types and evaluated the various Transformer architectures for representation learning. Empirical results indicate a consistent improvements of sensitivity and Youden index in patient TYPE I using Transformer representations, particularly our fusion adaptation of Pathformer. By enabling effective delirium diagnosis from postoperative day 1 to 3, our extensive experimental findings emphasize the potential of multi-modal physiological data and highlight the necessity of representation learning via multi-modal Transformer architecture in clinical diagnosis.","authors":["Bingxu Wang","Min Ge","Kunzhi Cai","Yuqi Zhang","Zeyi Zhou","Wenjiao Li","Yachong Guo","Wei Wang","Qing Zhou"],"url":"https://arxiv.org/abs/2504.04120"}
{"created":"2025-05-14","title":"Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models","abstract":"Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios, from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings, we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs. Related resources and papers are available at https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.","authors":["Yubo Li","Xiaobin Shen","Xinyu Yao","Xueying Ding","Yidi Miao","Ramayya Krishnan","Rema Padman"],"url":"https://arxiv.org/abs/2504.04717"}
{"created":"2025-05-14","title":"Investigating Popularity Bias Amplification in Recommender Systems Employed in the Entertainment Domain","abstract":"Recommender systems have become an integral part of our daily online experience by analyzing past user behavior to suggest relevant content in entertainment domains such as music, movies, and books. Today, they are among the most widely used applications of AI and machine learning. Consequently, regulations and guidelines for trustworthy AI, such as the European AI Act, which addresses issues like bias and fairness, are highly relevant to the design, development, and evaluation of recommender systems. One particularly important type of bias in this context is popularity bias, which results in the unfair underrepresentation of less popular content in recommendation lists. This work summarizes our research on investigating the amplification of popularity bias in recommender systems within the entertainment sector. Analyzing datasets from three entertainment domains, music, movies, and anime, we demonstrate that an item's recommendation frequency is positively correlated with its popularity. As a result, user groups with little interest in popular content receive less accurate recommendations compared to those who prefer widely popular items. Furthermore, this work contributes to a better understanding of the connection between recommendation accuracy, calibration quality of algorithms, and popularity bias amplification.","authors":["Dominik Kowald"],"url":"https://arxiv.org/abs/2504.04752"}
{"created":"2025-05-14","title":"Inter-event Interval Microscopy for Event Cameras","abstract":"Event cameras, an innovative bio-inspired sensor, differ from traditional cameras by sensing changes in intensity rather than directly perceiving intensity and recording these variations as a continuous stream of \"events\". The intensity reconstruction from these sparse events has long been a challenging problem. Previous approaches mainly focused on transforming motion-induced events into videos or achieving intensity imaging for static scenes by integrating modulation devices at the event camera acquisition end. In this paper, for the first time, we achieve event-to-intensity conversion using a static event camera for both static and dynamic scenes in fluorescence microscopy. Unlike conventional methods that primarily rely on event integration, the proposed Inter-event Interval Microscopy (IEIM) quantifies the time interval between consecutive events at each pixel. With a fixed threshold in the event camera, the time interval can precisely represent the intensity. At the hardware level, the proposed IEIM integrates a pulse light modulation device within a microscope equipped with an event camera, termed Pulse Modulation-based Event-driven Fluorescence Microscopy. Additionally, we have collected IEIMat dataset under various scenes including high dynamic range and high-speed scenarios. Experimental results on the IEIMat dataset demonstrate that the proposed IEIM achieves superior spatial and temporal resolution, as well as a higher dynamic range, with lower bandwidth compared to other methods. The code and the IEIMat dataset will be made publicly available.","authors":["Changqing Su","Yanqin Chen","Zihan Lin","Zhen Cheng","You Zhou","Bo Xiong","Zhaofei Yu","Tiejun Huang"],"url":"https://arxiv.org/abs/2504.04924"}
{"created":"2025-05-14","title":"DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning","abstract":"Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly \"thinking\" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.","authors":["Sara Vera Marjanovi\\'c","Arkil Patel","Vaibhav Adlakha","Milad Aghajohari","Parishad BehnamGhader","Mehar Bhatia","Aditi Khandelwal","Austin Kraft","Benno Krojer","Xing Han L\\`u","Nicholas Meade","Dongchan Shin","Amirhossein Kazemnejad","Gaurav Kamath","Marius Mosbach","Karolina Sta\\'nczak","Siva Reddy"],"url":"https://arxiv.org/abs/2504.07128"}
{"created":"2025-05-14","title":"FMNV: A Dataset of Media-Published News Videos for Fake News Detection","abstract":"News media, particularly video-based platforms, have become deeply embed-ded in daily life, concurrently amplifying the risks of misinformation dissem-ination. Consequently, multimodal fake news detection has garnered signifi-cant research attention. However, existing datasets predominantly comprise user-generated videos characterized by crude editing and limited public en-gagement, whereas professionally crafted fake news videos disseminated by media outlets-often politically or virally motivated-pose substantially greater societal harm. To address this gap, we construct FMNV, a novel da-taset exclusively composed of news videos published by media organizations. Through empirical analysis of existing datasets and our curated collection, we categorize fake news videos into four distinct types. Building upon this taxonomy, we employ Large Language Models (LLMs) to automatically generate deceptive content by manipulating authentic media-published news videos. Furthermore, we propose FMNVD, a baseline model featuring a dual-stream architecture that integrates spatio-temporal motion features from a 3D ResNeXt-101 backbone and static visual semantics from CLIP. The two streams are fused via an attention-based mechanism, while co-attention modules refine the visual, textual, and audio features for effective multi-modal aggregation. Comparative experiments demonstrate both the generali-zation capability of FMNV across multiple baselines and the superior detec-tion efficacy of FMNVD. This work establishes critical benchmarks for de-tecting high-impact fake news in media ecosystems while advancing meth-odologies for cross-modal inconsistency analysis. Our dataset is available in https://github.com/DennisIW/FMNV.","authors":["Yihao Wang","Zhong Qian","Peifeng Li"],"url":"https://arxiv.org/abs/2504.07687"}
{"created":"2025-05-14","title":"DrivAer Transformer: A high-precision and fast prediction method for vehicle aerodynamic drag coefficient based on the DrivAerNet++ dataset","abstract":"At the current stage, deep learning-based methods have demonstrated excellent capabilities in evaluating aerodynamic performance, significantly reducing the time and cost required for traditional computational fluid dynamics (CFD) simulations. However, when faced with the task of processing extremely complex three-dimensional (3D) vehicle models, the lack of large-scale datasets and training resources, coupled with the inherent diversity and complexity of the geometry of different vehicle models, means that the prediction accuracy and versatility of these networks are still not up to the level required for current production. In view of the remarkable success of Transformer models in the field of natural language processing and their strong potential in the field of image processing, this study innovatively proposes a point cloud learning framework called DrivAer Transformer (DAT). The DAT structure uses the DrivAerNet++ dataset, which contains high-fidelity CFD data of industrial-standard 3D vehicle shapes. enabling accurate estimation of air drag directly from 3D meshes, thus avoiding the limitations of traditional methods such as 2D image rendering or signed distance fields (SDF). DAT enables fast and accurate drag prediction, driving the evolution of the aerodynamic evaluation process and laying the critical foundation for introducing a data-driven approach to automotive design. The framework is expected to accelerate the vehicle design process and improve development efficiency.","authors":["Jiaqi He","Xiangwen Luo","Yiping Wang"],"url":"https://arxiv.org/abs/2504.08217"}
{"created":"2025-05-14","title":"Exploring Generative AI Techniques in Government: A Case Study","abstract":"The swift progress of Generative Artificial intelligence (GenAI), notably Large Language Models (LLMs), is reshaping the digital landscape. Recognizing this transformative potential, the National Research Council of Canada (NRC) launched a pilot initiative to explore the integration of GenAI techniques into its daily operation for performance excellence, where 22 projects were launched in May 2024. Within these projects, this paper presents the development of the intelligent agent Pubbie as a case study, targeting the automation of performance measurement, data management and insight reporting at the NRC. Cutting-edge techniques are explored, including LLM orchestration and semantic embedding via RoBERTa, while strategic fine-tuning and few-shot learning approaches are incorporated to infuse domain knowledge at an affordable cost. The user-friendly interface of Pubbie allows general government users to input queries in natural language and easily upload or download files with a simple button click, greatly reducing manual efforts and accessibility barriers.","authors":["Sunyi Liu","Mengzhe Geng","Rebecca Hart"],"url":"https://arxiv.org/abs/2504.10497"}
{"created":"2025-05-14","title":"Just Another Hour on TikTok: Reverse-engineering unique identifiers to obtain a complete slice of TikTok","abstract":"TikTok is now a massive platform, and has a deep impact on global events. But for all the preliminary studies done on it, there are still issues with determining fundamental characteristics of the platform. We develop a method to extract a representative sample from a specific time range on TikTok, and use it to collect >99% of posts from a full hour on the platform, alongside a dataset of >99% of posts from a single minute from each hour of a day. Through this, we obtain post metadata, video media data, and comments from a close to complete slice of TikTok. Using this dataset, we report the critical statistics of the platform, notably estimating a total of 269 million posts produced on the day we looked at on TikTok, that 18% of videos on the platform feature children, and that ~176 years of videos are uploaded every day.","authors":["Benjamin Steel","Miriam Schirmer","Derek Ruths","Juergen Pfeffer"],"url":"https://arxiv.org/abs/2504.13279"}
{"created":"2025-05-14","title":"Gradual Binary Search and Dimension Expansion : A general method for activation quantization in LLMs","abstract":"Large language models (LLMs) have become pivotal in artificial intelligence, demonstrating strong capabilities in reasoning, understanding, and generating data. However, their deployment on edge devices is hindered by their substantial size, often reaching several billion parameters. Quantization is a widely used method to reduce memory usage and inference time, however LLMs present unique challenges due to the prevalence of outliers in their activations. In this work, we leverage the theoretical advantages of Hadamard matrices over random rotation matrices to push the boundaries of quantization in LLMs. We demonstrate that Hadamard matrices are more effective in reducing outliers, which are a significant obstacle in achieving low-bit quantization. Our method based on a gradual binary search enables 3-bit quantization for weights, activations, and key-value (KV) caches, resulting in a 40% increase in accuracy on common benchmarks compared to SoTA methods. We extend the use of rotation matrices to support non-power-of-2 embedding dimensions, similar to the Qwen architecture, by employing the Paley algorithm. We theoretically demonstrates the superiority of Hadamard matrices in reducing outliers.We achieved 3-bit quantization for weights, activations, and KV cache, significantly enhancing model performance. Our experimental results on multiple models family like Mistral, LLaMA, and Qwen demonstrate the effectiveness of our approach, outperforming existing methods and enabling practical 3-bit quantization.","authors":["Lucas Maisonnave","Cyril Moineau","Olivier Bichler","Fabrice Rastello"],"url":"https://arxiv.org/abs/2504.13989"}
{"created":"2025-05-14","title":"Transforming Hyperspectral Images Into Chemical Maps: An End-to-End Deep Learning Approach","abstract":"Current approaches to chemical map generation from hyperspectral images are based on models such as partial least squares (PLS) regression, generating pixel-wise predictions that do not consider spatial context and suffer from a high degree of noise. This study proposes an end-to-end deep learning approach using a modified version of U-Net and a custom loss function to directly obtain chemical maps from hyperspectral images, skipping all intermediate steps required for traditional pixel-wise analysis. We compare the U-Net with the traditional PLS regression on a real dataset of pork belly samples with associated mean fat reference values. The U-Net obtains a test set root mean squared error of between 9% and 13% lower than that of PLS regression on the task of mean fat prediction. At the same time, U-Net generates fine detail chemical maps where 99.91% of the variance is spatially correlated. Conversely, only 2.53% of the variance in the PLS-generated chemical maps is spatially correlated, indicating that each pixel-wise prediction is largely independent of neighboring pixels. Additionally, while the PLS-generated chemical maps contain predictions far beyond the physically possible range of 0-100%, U-Net learns to stay inside this range. Thus, the findings of this study indicate that U-Net is superior to PLS for chemical map generation.","authors":["Ole-Christian Galbo Engstr{\\o}m","Michela Albano-Gaglio","Erik Schou Dreier","Yamine Bouzembrak","Maria Font-i-Furnols","Puneet Mishra","Kim Steenstrup Pedersen"],"url":"https://arxiv.org/abs/2504.14131"}
{"created":"2025-05-14","title":"Integrating Single-Cell Foundation Models with Graph Neural Networks for Drug Response Prediction","abstract":"AI-driven drug response prediction holds great promise for advancing personalized cancer treatment. However, the inherent heterogenity of cancer and high cost of data generation make accurate prediction challenging. In this study, we investigate whether incorporating the pretrained foundation model scGPT can enhance the performance of existing drug response prediction frameworks. Our approach builds on the DeepCDR framework, which encodes drug representations from graph structures and cell representations from multi-omics profiles. We adapt this framework by leveraging scGPT to generate enriched cell representations using its pretrained knowledge to compensate for limited amount of data. We evaluate our modified framework using IC$_{50}$ values on Pearson correlation coefficient (PCC) and a leave-one-drug out validation strategy, comparing it against the original DeepCDR framework and a prior scFoundation-based approach. scGPT not only outperforms previous approaches but also exhibits greater training stability, highlighting the value of leveraging scGPT-derived knowledge in this domain.","authors":["Till Rossner","Ziteng Li","Jonas Balke","Nikoo Salehfard","Tom Seifert","Ming Tang"],"url":"https://arxiv.org/abs/2504.14361"}
{"created":"2025-05-14","title":"Gaussian Shading++: Rethinking the Realistic Deployment Challenge of Performance-Lossless Image Watermark for Diffusion Models","abstract":"Ethical concerns surrounding copyright protection and inappropriate content generation pose challenges for the practical implementation of diffusion models. One effective solution involves watermarking the generated images. Existing methods primarily focus on ensuring that watermark embedding does not degrade the model performance. However, they often overlook critical challenges in real-world deployment scenarios, such as the complexity of watermark key management, user-defined generation parameters, and the difficulty of verification by arbitrary third parties. To address this issue, we propose Gaussian Shading++, a diffusion model watermarking method tailored for real-world deployment. We propose a double-channel design that leverages pseudorandom error-correcting codes to encode the random seed required for watermark pseudorandomization, achieving performance-lossless watermarking under a fixed watermark key and overcoming key management challenges. Additionally, we model the distortions introduced during generation and inversion as an additive white Gaussian noise channel and employ a novel soft decision decoding strategy during extraction, ensuring strong robustness even when generation parameters vary. To enable third-party verification, we incorporate public key signatures, which provide a certain level of resistance against forgery attacks even when model inversion capabilities are fully disclosed. Extensive experiments demonstrate that Gaussian Shading++ not only maintains performance losslessness but also outperforms existing methods in terms of robustness, making it a more practical solution for real-world deployment.","authors":["Zijin Yang","Xin Zhang","Kejiang Chen","Kai Zeng","Qiyi Yao","Han Fang","Weiming Zhang","Nenghai Yu"],"url":"https://arxiv.org/abs/2504.15026"}
{"created":"2025-05-14","title":"DiTPainter: Efficient Video Inpainting with Diffusion Transformers","abstract":"Many existing video inpainting algorithms utilize optical flows to construct the corresponding maps and then propagate pixels from adjacent frames to missing areas by mapping. Despite the effectiveness of the propagation mechanism, they might encounter blurry and inconsistencies when dealing with inaccurate optical flows or large masks. Recently, Diffusion Transformer (DiT) has emerged as a revolutionary technique for video generation tasks. However, pretrained DiT models for video generation all contain a large amount of parameters, which makes it very time consuming to apply to video inpainting tasks. In this paper, we present DiTPainter, an end-to-end video inpainting model based on Diffusion Transformer (DiT). DiTPainter uses an efficient transformer network designed for video inpainting, which is trained from scratch instead of initializing from any large pretrained models. DiTPainter can address videos with arbitrary lengths and can be applied to video decaptioning and video completion tasks with an acceptable time cost. Experiments show that DiTPainter outperforms existing video inpainting algorithms with higher quality and better spatial-temporal consistency.","authors":["Xian Wu","Chang Liu"],"url":"https://arxiv.org/abs/2504.15661"}
{"created":"2025-05-14","title":"LLMs meet Federated Learning for Scalable and Secure IoT Management","abstract":"The rapid expansion of IoT ecosystems introduces severe challenges in scalability, security, and real-time decision-making. Traditional centralized architectures struggle with latency, privacy concerns, and excessive resource consumption, making them unsuitable for modern large-scale IoT deployments. This paper presents a novel Federated Learning-driven Large Language Model (FL-LLM) framework, designed to enhance IoT system intelligence while ensuring data privacy and computational efficiency. The framework integrates Generative IoT (GIoT) models with a Gradient Sensing Federated Strategy (GSFS), dynamically optimizing model updates based on real-time network conditions. By leveraging a hybrid edge-cloud processing architecture, our approach balances intelligence, scalability, and security in distributed IoT environments. Evaluations on the IoT-23 dataset demonstrate that our framework improves model accuracy, reduces response latency, and enhances energy efficiency, outperforming traditional FL techniques (i.e., FedAvg, FedOpt). These findings highlight the potential of integrating LLM-powered federated learning into large-scale IoT ecosystems, paving the way for more secure, scalable, and adaptive IoT management solutions.","authors":["Yazan Otoum","Arghavan Asad","Amiya Nayak"],"url":"https://arxiv.org/abs/2504.16032"}
{"created":"2025-05-14","title":"Assessing the Feasibility of Internet-Sourced Video for Automatic Cattle Lameness Detection","abstract":"Cattle lameness is often caused by hoof injuries or interdigital dermatitis, leads to pain and significantly impacts essential physiological activities such as walking, feeding, and drinking. This study presents a deep learning-based model for detecting cattle lameness, sickness, or gait abnormalities using publicly available video data. The dataset consists of 50 unique videos from 40 individual cattle, recorded from various angles in both indoor and outdoor environments. Half of the dataset represents naturally walking (normal/non-lame) cattle, while the other half consists of cattle exhibiting gait abnormalities (lame). To enhance model robustness and generalizability, data augmentation was applied to the training data. The pre-processed videos were then classified using two deep learning models: ConvLSTM2D and 3D CNN. A comparative analysis of the results demonstrates strong classification performance. Specifically, the 3D CNN model achieved a video-level classification accuracy of 90%, with precision, recall, and f1-score of 90.9%, 90.9%, and 90.91% respectively. The ConvLSTM2D model exhibited a slightly lower accuracy of 85%. This study highlights the effectiveness of directly applying classification models to learn spatiotemporal features from video data, offering an alternative to traditional multi-stage approaches that typically involve object detection, pose estimation, and feature extraction. Besides, the findings demonstrate that the proposed deep learning models, particularly the 3D CNN, effectively classify and detect lameness in cattle while simplifying the processing pipeline.","authors":["Md Fahimuzzman Sohan","A. H. Abdul Hafez","Raid Alzubi"],"url":"https://arxiv.org/abs/2504.16404"}
{"created":"2025-05-14","title":"LLMSR@XLLM25: Less is More: Enhancing Structured Multi-Agent Reasoning via Quality-Guided Distillation","abstract":"The LLMSR@XLLM25 formulates a low-resource structural reasoning task that challenges LLMs to generate interpretable, step-by-step rationales with minimal labeled data. We present Less is More, the third-place winning approach in the LLMSR@XLLM25, which focuses on structured reasoning from only 24 labeled examples. Our approach leverages a multi-agent framework with reverse-prompt induction, retrieval-augmented reasoning synthesis via GPT-4o, and dual-stage reward-guided filtering to distill high-quality supervision across three subtasks: question parsing, CoT parsing, and step-level verification. All modules are fine-tuned from Meta-Llama-3-8B-Instruct under a unified LoRA+ setup. By combining structure validation with reward filtering across few-shot and zero-shot prompts, our pipeline consistently improves structure reasoning quality. These results underscore the value of controllable data distillation in enhancing structured inference under low-resource constraints. Our code is available at https://github.com/JhCircle/Less-is-More.","authors":["Jiahao Yuan","Xingzhe Sun","Xing Yu","Jingwen Wang","Dehui Du","Zhiqing Cui","Zixiang Di"],"url":"https://arxiv.org/abs/2504.16408"}
{"created":"2025-05-14","title":"DreamO: A Unified Framework for Image Customization","abstract":"Recently, extensive research on image customization (e.g., identity, subject, style, background, etc.) demonstrates strong customization capabilities in large-scale generative models. However, most approaches are designed for specific tasks, restricting their generalizability to combine different types of condition. Developing a unified framework for image customization remains an open challenge. In this paper, we present DreamO, an image customization framework designed to support a wide range of tasks while facilitating seamless integration of multiple conditions. Specifically, DreamO utilizes a diffusion transformer (DiT) framework to uniformly process input of different types. During training, we construct a large-scale training dataset that includes various customization tasks, and we introduce a feature routing constraint to facilitate the precise querying of relevant information from reference images. Additionally, we design a placeholder strategy that associates specific placeholders with conditions at particular positions, enabling control over the placement of conditions in the generated results. Moreover, we employ a progressive training strategy consisting of three stages: an initial stage focused on simple tasks with limited data to establish baseline consistency, a full-scale training stage to comprehensively enhance the customization capabilities, and a final quality alignment stage to correct quality biases introduced by low-quality data. Extensive experiments demonstrate that the proposed DreamO can effectively perform various image customization tasks with high quality and flexibly integrate different types of control conditions.","authors":["Chong Mou","Yanze Wu","Wenxu Wu","Zinan Guo","Pengze Zhang","Yufeng Cheng","Yiming Luo","Fei Ding","Shiwen Zhang","Xinghui Li","Mengtian Li","Mingcong Liu","Yi Zhang","Shaojin Wu","Songtao Zhao","Jian Zhang","Qian He","Xinglong Wu"],"url":"https://arxiv.org/abs/2504.16915"}
{"created":"2025-05-14","title":"Unsupervised Urban Land Use Mapping with Street View Contrastive Clustering and a Geographical Prior","abstract":"Urban land use classification and mapping are critical for urban planning, resource management, and environmental monitoring. Existing remote sensing techniques often lack precision in complex urban environments due to the absence of ground-level details. Unlike aerial perspectives, street view images provide a ground-level view that captures more human and social activities relevant to land use in complex urban scenes. Existing street view-based methods primarily rely on supervised classification, which is challenged by the scarcity of high-quality labeled data and the difficulty of generalizing across diverse urban landscapes. This study introduces an unsupervised contrastive clustering model for street view images with a built-in geographical prior, to enhance clustering performance. When combined with a simple visual assignment of the clusters, our approach offers a flexible and customizable solution to land use mapping, tailored to the specific needs of urban planners. We experimentally show that our method can generate land use maps from geotagged street view image datasets of two cities. As our methodology relies on the universal spatial coherence of geospatial data (\"Tobler's law\"), it can be adapted to various settings where street view images are available, to enable scalable, unsupervised land use mapping and updating. The code will be available at https://github.com/lin102/CCGP.","authors":["Lin Che","Yizi Chen","Tanhua Jin","Martin Raubal","Konrad Schindler","Peter Kiefer"],"url":"https://arxiv.org/abs/2504.17551"}
{"created":"2025-05-14","title":"DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training","abstract":"Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: \\href{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}{https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M}","authors":["Xiaoyu Tian","Sitong Zhao","Haotian Wang","Shuaiting Chen","Yiping Peng","Yunjie Ji","Han Zhao","Xiangang Li"],"url":"https://arxiv.org/abs/2504.17565"}
{"created":"2025-05-14","title":"Hierarchical and Multimodal Data for Daily Activity Understanding","abstract":"Daily Activity Recordings for Artificial Intelligence (DARai, pronounced \"Dahr-ree\") is a multimodal, hierarchically annotated dataset constructed to understand human activities in real-world settings. DARai consists of continuous scripted and unscripted recordings of 50 participants in 10 different environments, totaling over 200 hours of data from 20 sensors including multiple camera views, depth and radar sensors, wearable inertial measurement units (IMUs), electromyography (EMG), insole pressure sensors, biomonitor sensors, and gaze tracker.","authors":["Ghazal Kaviani","Yavuz Yarici","Seulgi Kim","Mohit Prabhushankar","Ghassan AlRegib","Mashhour Solh","Ameya Patil"],"url":"https://arxiv.org/abs/2504.17696"}
{"created":"2025-05-14","title":"Benchmarking Multimodal Mathematical Reasoning with Explicit Visual Dependency","abstract":"Recent advancements in Large Vision-Language Models (LVLMs) have significantly enhanced their ability to integrate visual and linguistic information, achieving near-human proficiency in tasks like object recognition, captioning, and visual question answering. However, current benchmarks typically focus on knowledge-centric evaluations that assess domain-specific expertise, often neglecting the core ability to reason about fundamental mathematical elements and visual concepts. We identify a gap in evaluating elementary-level math problems, which rely on explicit visual dependencies-requiring models to discern, integrate, and reason across multiple images while incorporating commonsense knowledge, all of which are crucial for advancing toward broader AGI capabilities. To address this gap, we introduce VCBENCH, a comprehensive benchmark for multimodal mathematical reasoning with explicit visual dependencies. VCBENCH includes 1,720 problems across six cognitive domains, featuring 6,697 images (averaging 3.9 per question) to ensure multi-image reasoning. We evaluate 26 state-of-the-art LVLMs on VCBENCH, revealing substantial performance disparities, with even the top models unable to exceed 50% accuracy. Our findings highlight the ongoing challenges in visual-mathematical integration and suggest avenues for future LVLM advancements. The project can be found at https://alibaba-damo-academy.github.io/VCBench/.","authors":["Zhikai Wang","Jiashuo Sun","Wenqi Zhang","Zhiqiang Hu","Xin Li","Fan Wang","Deli Zhao"],"url":"https://arxiv.org/abs/2504.18589"}
{"created":"2025-05-14","title":"Cubing for Tuning","abstract":"We are exploring the problem of building an automated reasoning procedure that adaptively tunes the high-level solving strategy for a given problem. There are two main distinctive characteristics of our approach: tuning is performed solely online, unlike the common use of tuning as an offline process; and tuning data comes exclusively from the given instance, so we do not rely on the availability of similar benchmarks and can work with unique challenging instances. Our approach builds on top of the divide-and-conquer paradigm that naturally serves partitioned sub-problems for an automated tuning algorithm to obtain a good solving strategy. We demonstrate performance improvement on two classes of important problems--SAT-solving and neural network verification--and show that our method can learn unconventional solving strategies in some cases.","authors":["Haoze Wu","Clark Barrett","Nina Narodytska"],"url":"https://arxiv.org/abs/2504.19039"}
{"created":"2025-05-14","title":"Follow Everything: A Leader-Following and Obstacle Avoidance Framework with Goal-Aware Adaptation","abstract":"Robust and flexible leader-following is a critical capability for robots to integrate into human society. While existing methods struggle to generalize to leaders of arbitrary form and often fail when the leader temporarily leaves the robot's field of view, this work introduces a unified framework addressing both challenges. First, traditional detection models are replaced with a segmentation model, allowing the leader to be anything. To enhance recognition robustness, a distance frame buffer is implemented that stores leader embeddings at multiple distances, accounting for the unique characteristics of leader-following tasks. Second, a goal-aware adaptation mechanism is designed to govern robot planning states based on the leader's visibility and motion, complemented by a graph-based planner that generates candidate trajectories for each state, ensuring efficient following with obstacle avoidance. Simulations and real-world experiments with a legged robot follower and various leaders (human, ground robot, UAV, legged robot, stop sign) in both indoor and outdoor environments show competitive improvements in follow success rate, reduced visual loss duration, lower collision rate, and decreased leader-follower distance.","authors":["Qianyi Zhang","Shijian Ma","Boyi Liu","Jianhao Jiao","Dimitrios Kanoulas"],"url":"https://arxiv.org/abs/2504.19399"}
{"created":"2025-05-14","title":"Perception-aware Sampling for Scatterplot Visualizations","abstract":"Visualizing data is often a crucial first step in data analytics workflows, but growing data sizes pose challenges due to computational and visual perception limitations. As a result, data analysts commonly down-sample their data and work with subsets. Deriving representative samples, however, remains a challenge. This paper focuses on scatterplots, a widely-used visualization type, and introduces a novel sampling objective -- perception-awareness -- aiming to improve sample efficacy by targeting humans' perception of a visualization.","authors":["Zafeiria Moumoulidou","Hamza Elhamdadi","Ke Yang","Subrata Mitra","Cindy Xiong Bearfield","Alexandra Meliou"],"url":"https://arxiv.org/abs/2504.20369"}
{"created":"2025-05-14","title":"A Survey on GUI Agents with Foundation Models Enhanced by Reinforcement Learning","abstract":"Graphical User Interface (GUI) agents, driven by Multi-modal Large Language Models (MLLMs), have emerged as a promising paradigm for enabling intelligent interaction with digital systems. This paper provides a structured survey of recent advances in GUI agents, focusing on architectures enhanced by Reinforcement Learning (RL). We first formalize GUI agent tasks as Markov Decision Processes and discuss typical execution environments and evaluation metrics. We then review the modular architecture of (M)LLM-based GUI agents, covering Perception, Planning, and Acting modules, and trace their evolution through representative works. Furthermore, we categorize GUI agent training methodologies into Prompt-based, Supervised Fine-Tuning (SFT)-based, and RL-based approaches, highlighting the progression from simple prompt engineering to dynamic policy learning via RL. Our summary illustrates how recent innovations in multimodal perception, decision reasoning, and adaptive action generation have significantly improved the generalization and robustness of GUI agents in complex real-world environments. We conclude by identifying key challenges and future directions for building more capable and reliable GUI agents.","authors":["Jiahao Li","Kaer Huang"],"url":"https://arxiv.org/abs/2504.20464"}
{"created":"2025-05-14","title":"Hallucination by Code Generation LLMs: Taxonomy, Benchmarks, Mitigation, and Challenges","abstract":"Recent technical breakthroughs in large language models (LLMs) have enabled them to fluently generate source code. Software developers often leverage both general-purpose and code-specialized LLMs to revise existing code or even generate a whole function from scratch. These capabilities are also beneficial in no-code or low-code contexts, in which one can write programs without a technical background. However, due to their internal design, LLMs are prone to generating hallucinations, which are incorrect, nonsensical, and not justifiable information but difficult to identify its presence. This problem also occurs when generating source code. Once hallucinated code is produced, it is often challenging for users to identify and fix it, especially when such hallucinations can be identified under specific execution paths. As a result, the hallucinated code may remain unnoticed within the codebase. This survey investigates recent studies and techniques relevant to hallucinations generated by CodeLLMs. We categorize the types of hallucinations in the code generated by CodeLLMs, review existing benchmarks and mitigation strategies, and identify open challenges. Based on these findings, this survey outlines further research directions in the detection and removal of hallucinations produced by CodeLLMs.","authors":["Yunseo Lee","John Youngeun Song","Dongsun Kim","Jindae Kim","Mijung Kim","Jaechang Nam"],"url":"https://arxiv.org/abs/2504.20799"}
{"created":"2025-05-14","title":"A Generalized Meta Federated Learning Framework with Theoretical Convergence Guarantees","abstract":"Meta federated learning (FL) is a personalized variant of FL, where multiple agents collaborate on training an initial shared model without exchanging raw data samples. The initial model should be trained in a way that current or new agents can easily adapt it to their local datasets after one or a few fine-tuning steps, thus improving the model personalization. Conventional meta FL approaches minimize the average loss of agents on the local models obtained after one step of fine-tuning. In practice, agents may need to apply several fine-tuning steps to adapt the global model to their local data, especially under highly heterogeneous data distributions across agents. To this end, we present a generalized framework for the meta FL by minimizing the average loss of agents on their local model after any arbitrary number $\\nu$ of fine-tuning steps. For this generalized framework, we present a variant of the well-known federated averaging (FedAvg) algorithm and conduct a comprehensive theoretical convergence analysis to characterize the convergence speed as well as behavior of the meta loss functions in both the exact and approximated cases. Our experiments on real-world datasets demonstrate superior accuracy and faster convergence for the proposed scheme compared to conventional approaches.","authors":["Mohammad Vahid Jamali","Hamid Saber","Jung Hyun Bae"],"url":"https://arxiv.org/abs/2504.21327"}
{"created":"2025-05-14","title":"SeriesBench: A Benchmark for Narrative-Driven Drama Series Understanding","abstract":"With the rapid development of Multi-modal Large Language Models (MLLMs), an increasing number of benchmarks have been established to evaluate the video understanding capabilities of these models. However, these benchmarks focus on standalone videos and mainly assess \"visual elements\" like human actions and object states. In reality, contemporary videos often encompass complex and continuous narratives, typically presented as a series. To address this challenge, we propose SeriesBench, a benchmark consisting of 105 carefully curated narrative-driven series, covering 28 specialized tasks that require deep narrative understanding. Specifically, we first select a diverse set of drama series spanning various genres. Then, we introduce a novel long-span narrative annotation method, combined with a full-information transformation approach to convert manual annotations into diverse task formats. To further enhance model capacity for detailed analysis of plot structures and character relationships within series, we propose a novel narrative reasoning framework, PC-DCoT. Extensive results on SeriesBench indicate that existing MLLMs still face significant challenges in understanding narrative-driven series, while PC-DCoT enables these MLLMs to achieve performance improvements. Overall, our SeriesBench and PC-DCoT highlight the critical necessity of advancing model capabilities to understand narrative-driven series, guiding the future development of MLLMs. SeriesBench is publicly available at https://github.com/zackhxn/SeriesBench-CVPR2025.","authors":["Chenkai Zhang","Yiming Lei","Zeming Liu","Haitao Leng","Shaoguo Liu","Tingting Gao","Qingjie Liu","Yunhong Wang"],"url":"https://arxiv.org/abs/2504.21435"}
{"created":"2025-05-14","title":"Ask, Fail, Repeat: Meeseeks, an Iterative Feedback Benchmark for LLMs' Multi-turn Instruction-following Ability","abstract":"The ability to follow instructions accurately is fundamental for Large Language Models (LLMs) to serve as reliable agents in real-world applications. For complex instructions, LLMs often struggle to fulfill all requirements in a single attempt. In practice, users typically provide iterative feedback until the LLM generates a response that meets all requirements. However, existing instruction-following benchmarks are either single-turn or introduce new requirements in each turn without allowing self-correction. To address this gap, we propose \\textbf{Meeseeks} (named after Mr. Meeseeks from \\textit{Rick and Morty}\\footnote{Rick and Morty is an American adult animated science fiction sitcom created by Justin Roiland and Dan Harmon for Cartoon Network's nighttime programming block Adult Swim.}.) Meeseeks simulates realistic human-LLM interactions through an iterative feedback framework, which enables models to self-correct based on specific requirement failures in each turn, better reflecting real-world user-end usage patterns. Meanwhile, the benchmark implements a comprehensive evaluation system with 38 capability tags organized across three dimensions: Intent Recognition, Granular Content Validation, and Output Structure Validation. Through rigorous evaluation across LLMs, Meeseeks provides valuable insights into LLMs' instruction-following capabilities in multi-turn scenarios.","authors":["Jiaming Wang","Yunke Zhao","Peng Ding","Jun Kuang","Zongyu Wang","Xuezhi Cao","Xunliang Cai"],"url":"https://arxiv.org/abs/2504.21625"}
{"created":"2025-05-14","title":"HoloTime: Taming Video Diffusion Models for Panoramic 4D Scene Generation","abstract":"The rapid advancement of diffusion models holds the promise of revolutionizing the application of VR and AR technologies, which typically require scene-level 4D assets for user experience. Nonetheless, existing diffusion models predominantly concentrate on modeling static 3D scenes or object-level dynamics, constraining their capacity to provide truly immersive experiences. To address this issue, we propose HoloTime, a framework that integrates video diffusion models to generate panoramic videos from a single prompt or reference image, along with a 360-degree 4D scene reconstruction method that seamlessly transforms the generated panoramic video into 4D assets, enabling a fully immersive 4D experience for users. Specifically, to tame video diffusion models for generating high-fidelity panoramic videos, we introduce the 360World dataset, the first comprehensive collection of panoramic videos suitable for downstream 4D scene reconstruction tasks. With this curated dataset, we propose Panoramic Animator, a two-stage image-to-video diffusion model that can convert panoramic images into high-quality panoramic videos. Following this, we present Panoramic Space-Time Reconstruction, which leverages a space-time depth estimation method to transform the generated panoramic videos into 4D point clouds, enabling the optimization of a holistic 4D Gaussian Splatting representation to reconstruct spatially and temporally consistent 4D scenes. To validate the efficacy of our method, we conducted a comparative analysis with existing approaches, revealing its superiority in both panoramic video generation and 4D scene reconstruction. This demonstrates our method's capability to create more engaging and realistic immersive environments, thereby enhancing user experiences in VR and AR applications.","authors":["Haiyang Zhou","Wangbo Yu","Jiawen Guan","Xinhua Cheng","Yonghong Tian","Li Yuan"],"url":"https://arxiv.org/abs/2504.21650"}
{"created":"2025-05-14","title":"Graph RAG for Legal Norms: A Hierarchical and Temporal Approach","abstract":"This article proposes an adaptation of Graph Retrieval Augmented Generation (Graph RAG) specifically designed for the analysis and comprehension of legal norms, which are characterized by their predefined hierarchical structure, extensive network of internal and external references and multiple temporal versions. By combining structured knowledge graphs with contextually enriched text segments, Graph RAG offers a promising solution to address the inherent complexity and vast volume of legal data. The integration of hierarchical structure and temporal evolution into knowledge graphs - along with the concept of comprehensive Text Units - facilitates the construction of richer, interconnected representations of legal knowledge. Through a detailed analysis of Graph RAG and its application to legal norm datasets, this article aims to advance the field of Artificial Intelligence applied to Law, creating opportunities for more effective systems in legal research, legislative analysis, and decision support.","authors":["Hudson de Martim"],"url":"https://arxiv.org/abs/2505.00039"}
{"created":"2025-05-14","title":"On the Space Complexity of Online Convolution","abstract":"We study a discrete convolution streaming problem. An input arrives as a stream of numbers $z = (z_0,z_1,z_2,\\ldots)$, and at time $t$ our goal is to output $(Tz)_t$ where $T$ is a lower-triangular Toeplitz matrix. We focus on space complexity; the algorithm can store a buffer of $\\beta(t)$ numbers in order to achieve this goal.","authors":["Joel Daniel Andersson","Amir Yehudayoff"],"url":"https://arxiv.org/abs/2505.00181"}
{"created":"2025-05-14","title":"LLM-Based Threat Detection and Prevention Framework for IoT Ecosystems","abstract":"The increasing complexity and scale of the Internet of Things (IoT) have made security a critical concern. This paper presents a novel Large Language Model (LLM)-based framework for comprehensive threat detection and prevention in IoT environments. The system integrates lightweight LLMs fine-tuned on IoT-specific datasets (IoT-23, TON_IoT) for real-time anomaly detection and automated, context-aware mitigation strategies optimized for resource-constrained devices. A modular Docker-based deployment enables scalable and reproducible evaluation across diverse network conditions. Experimental results in simulated IoT environments demonstrate significant improvements in detection accuracy, response latency, and resource efficiency over traditional security methods. The proposed framework highlights the potential of LLM-driven, autonomous security solutions for future IoT ecosystems.","authors":["Yazan Otoum","Arghavan Asad","Amiya Nayak"],"url":"https://arxiv.org/abs/2505.00240"}
{"created":"2025-05-14","title":"Open-Source LLM-Driven Federated Transformer for Predictive IoV Management","abstract":"The proliferation of connected vehicles within the Internet of Vehicles (IoV) ecosystem presents critical challenges in ensuring scalable, real-time, and privacy-preserving traffic management. Existing centralized IoV solutions often suffer from high latency, limited scalability, and reliance on proprietary Artificial Intelligence (AI) models, creating significant barriers to widespread deployment, particularly in dynamic and privacy-sensitive environments. Meanwhile, integrating Large Language Models (LLMs) in vehicular systems remains underexplored, especially concerning prompt optimization and effective utilization in federated contexts. To address these challenges, we propose the Federated Prompt-Optimized Traffic Transformer (FPoTT), a novel framework that leverages open-source LLMs for predictive IoV management. FPoTT introduces a dynamic prompt optimization mechanism that iteratively refines textual prompts to enhance trajectory prediction. The architecture employs a dual-layer federated learning paradigm, combining lightweight edge models for real-time inference with cloud-based LLMs to retain global intelligence. A Transformer-driven synthetic data generator is incorporated to augment training with diverse, high-fidelity traffic scenarios in the Next Generation Simulation (NGSIM) format. Extensive evaluations demonstrate that FPoTT, utilizing EleutherAI Pythia-1B, achieves 99.86% prediction accuracy on real-world data while maintaining high performance on synthetic datasets. These results underscore the potential of open-source LLMs in enabling secure, adaptive, and scalable IoV management, offering a promising alternative to proprietary solutions in smart mobility ecosystems.","authors":["Yazan Otoum","Arghavan Asad","Ishtiaq Ahmad"],"url":"https://arxiv.org/abs/2505.00651"}
{"created":"2025-05-14","title":"Multi-Modal Language Models as Text-to-Image Model Evaluators","abstract":"The steady improvements of text-to-image (T2I) generative models lead to slow deprecation of automatic evaluation benchmarks that rely on static datasets, motivating researchers to seek alternative ways to evaluate the T2I progress. In this paper, we explore the potential of multi-modal large language models (MLLMs) as evaluator agents that interact with a T2I model, with the objective of assessing prompt-generation consistency and image aesthetics. We present Multimodal Text-to-Image Eval (MT2IE), an evaluation framework that iteratively generates prompts for evaluation, scores generated images and matches T2I evaluation of existing benchmarks with a fraction of the prompts used in existing static benchmarks. Moreover, we show that MT2IE's prompt-generation consistency scores have higher correlation with human judgment than scores previously introduced in the literature. MT2IE generates prompts that are efficient at probing T2I model performance, producing the same relative T2I model rankings as existing benchmarks while using only 1/80th the number of prompts for evaluation.","authors":["Jiahui Chen","Candace Ross","Reyhane Askari-Hemmat","Koustuv Sinha","Melissa Hall","Michal Drozdzal","Adriana Romero-Soriano"],"url":"https://arxiv.org/abs/2505.00759"}
{"created":"2025-05-14","title":"Efficient Shapley Value-based Non-Uniform Pruning of Large Language Models","abstract":"Pruning large language models (LLMs) is a promising solution for reducing model sizes and computational complexity while preserving performance. Traditional layer-wise pruning methods often adopt a uniform sparsity approach across all layers, which leads to suboptimal performance due to the varying significance of individual transformer layers within the model not being accounted for. To this end, we propose the Shapley Value-based Non-Uniform Pruning (SV-NUP) method for LLMs. This approach quantifies the contribution of each transformer layer to the overall model performance, enabling the assignment of tailored pruning budgets to different layers to retain critical parameters. To further improve efficiency, we design the Sliding Window-based Shapley Value approximation method. It substantially reduces computational overhead compared to exact SV calculation methods. Extensive experiments on various LLMs including LLaMA-v1, LLaMA-v2 and OPT demonstrate the effectiveness of the proposed approach. The results reveal that non-uniform pruning significantly enhances the performance of pruned models. Notably, SV-NUP achieves a reduction in perplexity (PPL) of 18.01% and 19.55% on LLaMA-7B and LLaMA-13B, respectively, compared to SparseGPT at 70% sparsity.","authors":["Chuan Sun","Han Yu","Lizhen Cui","Xiaoxiao Li"],"url":"https://arxiv.org/abs/2505.01731"}
{"created":"2025-05-14","title":"Scalable Genomic Context Analysis with GCsnap2 on HPC Clusters","abstract":"GCsnap2 Cluster is a scalable, high performance tool for genomic context analysis, developed to overcome the limitations of its predecessor, GCsnap1 Desktop. Leveraging distributed computing with mpi4py[.]futures, GCsnap2 Cluster achieved a 22x improvement in execution time and can now perform genomic context analysis for hundreds of thousands of input sequences in HPC clusters. Its modular architecture enables the creation of task-specific workflows and flexible deployment in various computational environments, making it well suited for bioinformatics studies of large-scale datasets. This work highlights the potential for applying similar approaches to solve scalability challenges in other scientific domains that rely on large-scale data analysis pipelines.","authors":["Reto Krummenacher","Osman Seckin Simsek","Mich\\`ele Leemann","Leila T. Alexander","Torsten Schwede","Florina M. Ciorba","Joana Pereira"],"url":"https://arxiv.org/abs/2505.02195"}
{"created":"2025-05-14","title":"SafeMate: A Modular RAG-Based Agent for Context-Aware Emergency Guidance","abstract":"Despite the abundance of public safety documents and emergency protocols, most individuals remain ill-equipped to interpret and act on such information during crises. Traditional emergency decision support systems (EDSS) are designed for professionals and rely heavily on static documents like PDFs or SOPs, which are difficult for non-experts to navigate under stress. This gap between institutional knowledge and public accessibility poses a critical barrier to effective emergency preparedness and response.","authors":["Junfeng Jiao","Jihyung Park","Yiming Xu","Kristen Sussman","Lucy Atkinson"],"url":"https://arxiv.org/abs/2505.02306"}
{"created":"2025-05-14","title":"No Other Representation Component Is Needed: Diffusion Transformers Can Provide Representation Guidance by Themselves","abstract":"Recent studies have demonstrated that learning a meaningful internal representation can both accelerate generative training and enhance the generation quality of diffusion transformers. However, existing approaches necessitate to either introduce an external and complex representation training framework or rely on a large-scale, pre-trained representation foundation model to provide representation guidance during the original generative training process. In this study, we posit that the unique discriminative process inherent to diffusion transformers enables them to offer such guidance without requiring external representation components. We therefore propose Self-Representation Alignment (SRA), a simple yet straightforward method that obtains representation guidance through a self-distillation manner. Specifically, SRA aligns the output latent representation of the diffusion transformer in the earlier layer with higher noise to that in the later layer with lower noise to progressively enhance the overall representation learning during only the generative training process. Experimental results indicate that applying SRA to DiTs and SiTs yields consistent performance improvements. Moreover, SRA not only significantly outperforms approaches relying on auxiliary, complex representation training frameworks but also achieves performance comparable to methods that are heavily dependent on powerful external representation priors.","authors":["Dengyang Jiang","Mengmeng Wang","Liuzhuozheng Li","Lei Zhang","Haoyu Wang","Wei Wei","Guang Dai","Yanning Zhang","Jingdong Wang"],"url":"https://arxiv.org/abs/2505.02831"}
{"created":"2025-05-14","title":"BLAB: Brutally Long Audio Bench","abstract":"Developing large audio language models (LMs) capable of understanding diverse spoken interactions is essential for accommodating the multimodal nature of human communication and can increase the accessibility of language technologies across different user populations. Recent work on audio LMs has primarily evaluated their performance on short audio segments, typically under 30 seconds, with limited exploration of long-form conversational speech segments that more closely reflect natural user interactions with these models. We introduce Brutally Long Audio Bench (BLAB), a challenging long-form audio benchmark that evaluates audio LMs on localization, duration estimation, emotion, and counting tasks using audio segments averaging 51 minutes in length. BLAB consists of 833+ hours of diverse, full-length audio clips, each paired with human-annotated, text-based natural language questions and answers. Our audio data were collected from permissively licensed sources and underwent a human-assisted filtering process to ensure task compliance. We evaluate six open-source and proprietary audio LMs on BLAB and find that all of them, including advanced models such as Gemini 2.0 Pro and GPT-4o, struggle with the tasks in BLAB. Our comprehensive analysis reveals key insights into the trade-offs between task difficulty and audio duration. In general, we find that audio LMs struggle with long-form speech, with performance declining as duration increases. They perform poorly on localization, temporal reasoning, counting, and struggle to understand non-phonemic information, relying more on prompts than audio content. BLAB serves as a challenging evaluation framework to develop audio LMs with robust long-form audio understanding capabilities.","authors":["Orevaoghene Ahia","Martijn Bartelds","Kabir Ahuja","Hila Gonen","Valentin Hofmann","Siddhant Arora","Shuyue Stella Li","Vishal Puttagunta","Mofetoluwa Adeyemi","Charishma Buchireddy","Ben Walls","Noah Bennett","Shinji Watanabe","Noah A. Smith","Yulia Tsvetkov","Sachin Kumar"],"url":"https://arxiv.org/abs/2505.03054"}
{"created":"2025-05-14","title":"SonicRAG : High Fidelity Sound Effects Synthesis Based on Retrival Augmented Generation","abstract":"Large Language Models (LLMs) have demonstrated remarkable capabilities in natural language processing (NLP) and multimodal learning, with successful applications in text generation and speech synthesis, enabling a deeper understanding and generation of multimodal content. In the field of sound effects (SFX) generation, LLMs have been leveraged to orchestrate multiple models for audio synthesis. However, due to the scarcity of annotated datasets, and the complexity of temproal modeling. current SFX generation techniques still fall short in achieving high-fidelity audio. To address these limitations, this paper introduces a novel framework that integrates LLMs with existing sound effect databases, allowing for the retrieval, recombination, and synthesis of audio based on user requirements. By leveraging this approach, we enhance the diversity and quality of generated sound effects while eliminating the need for additional recording costs, offering a flexible and efficient solution for sound design and application.","authors":["Yu-Ren Guo","Wen-Kai Tai"],"url":"https://arxiv.org/abs/2505.03244"}
{"created":"2025-05-14","title":"IndicSQuAD: A Comprehensive Multilingual Question Answering Dataset for Indic Languages","abstract":"The rapid progress in question-answering (QA) systems has predominantly benefited high-resource languages, leaving Indic languages largely underrepresented despite their vast native speaker base. In this paper, we present IndicSQuAD, a comprehensive multi-lingual extractive QA dataset covering nine major Indic languages, systematically derived from the SQuAD dataset. Building on previous work with MahaSQuAD for Marathi, our approach adapts and extends translation techniques to maintain high linguistic fidelity and accurate answer-span alignment across diverse languages. IndicSQuAD comprises extensive training, validation, and test sets for each language, providing a robust foundation for model development. We evaluate baseline performances using language-specific monolingual BERT models and the multilingual MuRIL-BERT. The results indicate some challenges inherent in low-resource settings. Moreover, our experiments suggest potential directions for future work, including expanding to additional languages, developing domain-specific datasets, and incorporating multimodal data. The dataset and models are publicly shared at https://github.com/l3cube-pune/indic-nlp","authors":["Sharvi Endait","Ruturaj Ghatage","Aditya Kulkarni","Rajlaxmi Patil","Raviraj Joshi"],"url":"https://arxiv.org/abs/2505.03688"}
{"created":"2025-05-14","title":"Self-Supervised Learning for Robotic Leaf Manipulation: A Hybrid Geometric-Neural Approach","abstract":"Automating leaf manipulation in agricultural settings faces significant challenges, including the variability of plant morphologies and deformable leaves. We propose a novel hybrid geometric-neural approach for autonomous leaf grasping that combines traditional computer vision with neural networks through self-supervised learning. Our method integrates YOLOv8 for instance segmentation and RAFT-Stereo for 3D depth estimation to build rich leaf representations, which feed into both a geometric feature scoring pipeline and a neural refinement module (GraspPointCNN). The key innovation is our confidence-weighted fusion mechanism that dynamically balances the contribution of each approach based on prediction certainty. Our self-supervised framework uses the geometric pipeline as an expert teacher to automatically generate training data. Experiments demonstrate that our approach achieves an 88.0% success rate in controlled environments and 84.7% in real greenhouse conditions, significantly outperforming both purely geometric (75.3%) and neural (60.2%) methods. This work establishes a new paradigm for agricultural robotics where domain expertise is seamlessly integrated with machine learning capabilities, providing a foundation for fully automated crop monitoring systems.","authors":["Srecharan Selvam"],"url":"https://arxiv.org/abs/2505.03702"}
{"created":"2025-05-14","title":"Visual Imitation Enables Contextual Humanoid Control","abstract":"How can we teach humanoids to climb staircases and sit on chairs using the surrounding environment context? Arguably, the simplest way is to just show them-casually capture a human motion video and feed it to humanoids. We introduce VIDEOMIMIC, a real-to-sim-to-real pipeline that mines everyday videos, jointly reconstructs the humans and the environment, and produces whole-body control policies for humanoid robots that perform the corresponding skills. We demonstrate the results of our pipeline on real humanoid robots, showing robust, repeatable contextual control such as staircase ascents and descents, sitting and standing from chairs and benches, as well as other dynamic whole-body skills-all from a single policy, conditioned on the environment and global root commands. VIDEOMIMIC offers a scalable path towards teaching humanoids to operate in diverse real-world environments.","authors":["Arthur Allshire","Hongsuk Choi","Junyi Zhang","David McAllister","Anthony Zhang","Chung Min Kim","Trevor Darrell","Pieter Abbeel","Jitendra Malik","Angjoo Kanazawa"],"url":"https://arxiv.org/abs/2505.03729"}
{"created":"2025-05-14","title":"MARCO: A Multi-Agent System for Optimizing HPC Code Generation Using Large Language Models","abstract":"Large language models (LLMs) have transformed software development through code generation capabilities, yet their effectiveness for high-performance computing (HPC) remains limited. HPC code requires specialized optimizations for parallelism, memory efficiency, and architecture-specific considerations that general-purpose LLMs often overlook. We present MARCO (Multi-Agent Reactive Code Optimizer), a novel framework that enhances LLM-generated code for HPC through a specialized multi-agent architecture. MARCO employs separate agents for code generation and performance evaluation, connected by a feedback loop that progressively refines optimizations. A key innovation is MARCO's web-search component that retrieves real-time optimization techniques from recent conference proceedings and research publications, bridging the knowledge gap in pre-trained LLMs. Our extensive evaluation on the LeetCode 75 problem set demonstrates that MARCO achieves a 14.6% average runtime reduction compared to Claude 3.5 Sonnet alone, while the integration of the web-search component yields a 30.9% performance improvement over the base MARCO system. These results highlight the potential of multi-agent systems to address the specialized requirements of high-performance code generation, offering a cost-effective alternative to domain-specific model fine-tuning.","authors":["Asif Rahman","Veljko Cvetkovic","Kathleen Reece","Aidan Walters","Yasir Hassan","Aneesh Tummeti","Bryan Torres","Denise Cooney","Margaret Ellis","Dimitrios S. Nikolopoulos"],"url":"https://arxiv.org/abs/2505.03906"}
{"created":"2025-05-14","title":"Prism: Unleashing GPU Sharing for Cost-Efficient Multi-LLM Serving","abstract":"Serving large language models (LLMs) is expensive, especially for providers hosting many models, making cost reduction essential. The unique workload patterns of serving multiple LLMs (i.e., multi-LLM serving) create new opportunities and challenges for this task. The long-tail popularity of models and their long idle periods present opportunities to improve utilization through GPU sharing. However, existing GPU sharing systems lack the ability to adjust their resource allocation and sharing policies at runtime, making them ineffective at meeting latency service-level objectives (SLOs) under rapidly fluctuating workloads.","authors":["Shan Yu","Jiarong Xing","Yifan Qiao","Mingyuan Ma","Yangmin Li","Yang Wang","Shuo Yang","Zhiqiang Xie","Shiyi Cao","Ke Bao","Ion Stoica","Harry Xu","Ying Sheng"],"url":"https://arxiv.org/abs/2505.04021"}
{"created":"2025-05-14","title":"TS-SNN: Temporal Shift Module for Spiking Neural Networks","abstract":"Spiking Neural Networks (SNNs) are increasingly recognized for their biological plausibility and energy efficiency, positioning them as strong alternatives to Artificial Neural Networks (ANNs) in neuromorphic computing applications. SNNs inherently process temporal information by leveraging the precise timing of spikes, but balancing temporal feature utilization with low energy consumption remains a challenge. In this work, we introduce Temporal Shift module for Spiking Neural Networks (TS-SNN), which incorporates a novel Temporal Shift (TS) module to integrate past, present, and future spike features within a single timestep via a simple yet effective shift operation. A residual combination method prevents information loss by integrating shifted and original features. The TS module is lightweight, requiring only one additional learnable parameter, and can be seamlessly integrated into existing architectures with minimal additional computational cost. TS-SNN achieves state-of-the-art performance on benchmarks like CIFAR-10 (96.72\\%), CIFAR-100 (80.28\\%), and ImageNet (70.61\\%) with fewer timesteps, while maintaining low energy consumption. This work marks a significant step forward in developing efficient and accurate SNN architectures.","authors":["Kairong Yu","Tianqing Zhang","Qi Xu","Gang Pan","Hongwei Wang"],"url":"https://arxiv.org/abs/2505.04165"}
{"created":"2025-05-14","title":"Red Teaming the Mind of the Machine: A Systematic Evaluation of Prompt Injection and Jailbreak Vulnerabilities in LLMs","abstract":"Large Language Models (LLMs) are increasingly integrated into consumer and enterprise applications. Despite their capabilities, they remain susceptible to adversarial attacks such as prompt injection and jailbreaks that override alignment safeguards. This paper provides a systematic investigation of jailbreak strategies against various state-of-the-art LLMs. We categorize over 1,400 adversarial prompts, analyze their success against GPT-4, Claude 2, Mistral 7B, and Vicuna, and examine their generalizability and construction logic. We further propose layered mitigation strategies and recommend a hybrid red-teaming and sandboxing approach for robust LLM security.","authors":["Chetan Pathade"],"url":"https://arxiv.org/abs/2505.04806"}
{"created":"2025-05-14","title":"Enhanced convergence rates of Adaptive Importance Sampling with recycling schemes via quasi-Monte Carlo methods","abstract":"This article investigates the integration of quasi-Monte Carlo (QMC) methods using the Adaptive Multiple Importance Sampling (AMIS). Traditional Importance Sampling (IS) often suffers from poor performance since it heavily relies on the choice of the proposal distributions. The AMIS and the Modified version of AMIS (MAMIS) address this by iteratively refining proposal distributions and reusing all past samples through a recycling strategy. We introduce the RQMC methods into the MAMIS, achieving higher convergence rates compared to the Monte Carlo (MC) methods. Our main contributions include a detailed convergence analysis of the MAMIS estimator under randomized QMC (RQMC) sampling. Specifically, we establish the $L^q$ $(q \\geq 2)$ error bound for the RQMC-based estimator using a smoothed projection method, which enables us to apply the H\\\"older's inequality in the error analysis of the RQMC-based MAMIS estimator. As a result, we prove that the root mean square error of the RQMC-based MAMIS estimator converges at a rate of $\\mathcal{O}(\\bar{N}_T^{-1+\\epsilon})$, where $\\bar{N}_T$ is the average number of samples used in each step over $T$ iterations, and $\\epsilon > 0$ is arbitrarily small. Numerical experiments validate the effectiveness of our method, including mixtures of Gaussians, a banana-shaped model, and Bayesian Logistic regression.","authors":["Jianlong Chen","Jiarui Du","Xiaoqun Wang","Zhijian He"],"url":"https://arxiv.org/abs/2505.05037"}
{"created":"2025-05-14","title":"FG-CLIP: Fine-Grained Visual and Textual Alignment","abstract":"Contrastive Language-Image Pre-training (CLIP) excels in multimodal tasks such as image-text retrieval and zero-shot classification but struggles with fine-grained understanding due to its focus on coarse-grained short captions. To address this, we propose Fine-Grained CLIP (FG-CLIP), which enhances fine-grained understanding through three key innovations. First, we leverage large multimodal models to generate 1.6 billion long caption-image pairs for capturing global-level semantic details. Second, a high-quality dataset is constructed with 12 million images and 40 million region-specific bounding boxes aligned with detailed captions to ensure precise, context-rich representations. Third, 10 million hard fine-grained negative samples are incorporated to improve the model's ability to distinguish subtle semantic differences. We construct a comprehensive dataset, termed FgGRN, by integrating high-quality region-specific annotations with challenging fine-grained negative samples. Corresponding training methods are meticulously designed for these data. Extensive experiments demonstrate that FG-CLIP outperforms the original CLIP and other state-of-the-art methods across various downstream tasks, including fine-grained understanding, open-vocabulary object detection, image-text retrieval, and general multimodal benchmarks. These results highlight FG-CLIP's effectiveness in capturing fine-grained image details and improving overall model performance. The related data, code, and models are available at https://github.com/360CVGroup/FG-CLIP.","authors":["Chunyu Xie","Bin Wang","Fanjing Kong","Jincheng Li","Dawei Liang","Gengshen Zhang","Dawei Leng","Yuhui Yin"],"url":"https://arxiv.org/abs/2505.05071"}
{"created":"2025-05-14","title":"A thermoelastic plate model for shot peen forming metal panels based on effective torque","abstract":"A common technique used in factories to shape metal panels is shot peen forming. The impacts between the hard steel shot and the softer metal of the panel cause localized plastic deformation used to improve the fatigue properties of the material's surface. The residual stress distribution imparted by impacts also results in bending, which suggests that a torque is associated with it. In this paper, we model shot peen forming as the application of spatially varying torques to a Kirchhoff plate, opting to use the language of thermoelasticity in order to introduce these torque distributions. First, we derive the governing equations for the thermoelastic thin plate model and show that only a torque-type resultant of the temperature distribution shows up in the bending equation. Next, to calibrate from the shot peen operation an empirical effective torque parameter used in the thermoelastic model, a simple and non-invasive test is devised. This test relies only on measuring the maximum displacement of a uniformly shot peened plate as opposed to characterizing the residual stress distribution. After discussing how to handle the unconventional fully-free boundary conditions germane for peened plates, we introduce an approach to solving the inverse problem whereby the peening distribution required to obtain a specified plate contour can be obtained. Given the non-unique relationship between peening distributions and the displacement at discrete points, we explore a regularization of the inverse problem which gives rise to shot peen distributions that match the capabilities of equipment in the factory. In order to validate our proposed model, an experiment with quantified uncertainty is designed and carried out which investigates the agreement between the predictions of the calibrated model and real shot peen forming operations.","authors":["Conor Rowan"],"url":"https://arxiv.org/abs/2505.05236"}
{"created":"2025-05-14","title":"An Overview of the Prospects and Challenges of Using Artificial Intelligence for Energy Management Systems in Microgrids","abstract":"Microgrids have emerged as a pivotal solution in the quest for a sustainable and energy-efficient future. While microgrids offer numerous advantages, they are also prone to issues related to reliably forecasting renewable energy demand and production, protecting against cyberattacks, controlling operational costs, optimizing power flow, and regulating the performance of energy management systems (EMS). Tackling these energy management challenges is essential to facilitate microgrid applications and seamlessly incorporate renewable energy resources. Artificial intelligence (AI) has recently demonstrated immense potential for optimizing energy management in microgrids, providing efficient and reliable solutions. This paper highlights the combined benefits of enabling AI-based methodologies in the energy management systems of microgrids by examining the applicability and efficiency of AI-based EMS in achieving specific technical and economic objectives. The paper also points out several future research directions that promise to spearhead AI-driven EMS, namely the development of self-healing microgrids, integration with blockchain technology, use of Internet of things (IoT), and addressing interpretability, data privacy, scalability, and the prospects to generative AI in the context of future AI-based EMS.","authors":["Noor ul Misbah Khanum","Hayssam Dahrouj","Ramesh C. Bansal","Hissam Mouayad Tawfik"],"url":"https://arxiv.org/abs/2505.05498"}
{"created":"2025-05-14","title":"Web2Grasp: Learning Functional Grasps from Web Images of Hand-Object Interactions","abstract":"Functional grasp is essential for enabling dexterous multi-finger robot hands to manipulate objects effectively. However, most prior work either focuses on power grasping, which simply involves holding an object still, or relies on costly teleoperated robot demonstrations to teach robots how to grasp each object functionally. Instead, we propose extracting human grasp information from web images since they depict natural and functional object interactions, thereby bypassing the need for curated demonstrations. We reconstruct human hand-object interaction (HOI) 3D meshes from RGB images, retarget the human hand to multi-finger robot hands, and align the noisy object mesh with its accurate 3D shape. We show that these relatively low-quality HOI data from inexpensive web sources can effectively train a functional grasping model. To further expand the grasp dataset for seen and unseen objects, we use the initially-trained grasping policy with web data in the IsaacGym simulator to generate physically feasible grasps while preserving functionality. We train the grasping model on 10 object categories and evaluate it on 9 unseen objects, including challenging items such as syringes, pens, spray bottles, and tongs, which are underrepresented in existing datasets. The model trained on the web HOI dataset, achieving a 75.8% success rate on seen objects and 61.8% across all objects in simulation, with a 6.7% improvement in success rate and a 1.8x increase in functionality ratings over baselines. Simulator-augmented data further boosts performance from 61.8% to 83.4%. The sim-to-real transfer to the LEAP Hand achieves a 85% success rate. Project website is at: https://web2grasp.github.io/.","authors":["Hongyi Chen","Yunchao Yao","Yufei Ye","Zhixuan Xu","Homanga Bharadhwaj","Jiashun Wang","Shubham Tulsiani","Zackory Erickson","Jeffrey Ichnowski"],"url":"https://arxiv.org/abs/2505.05517"}
{"created":"2025-05-14","title":"LiteLMGuard: Seamless and Lightweight On-Device Prompt Filtering for Safeguarding Small Language Models against Quantization-induced Risks and Vulnerabilities","abstract":"The growing adoption of Large Language Models (LLMs) has influenced the development of their lighter counterparts-Small Language Models (SLMs)-to enable on-device deployment across smartphones and edge devices. These SLMs offer enhanced privacy, reduced latency, server-free functionality, and improved user experience. However, due to resource constraints of on-device environment, SLMs undergo size optimization through compression techniques like quantization, which can inadvertently introduce fairness, ethical and privacy risks. Critically, quantized SLMs may respond to harmful queries directly, without requiring adversarial manipulation, raising significant safety and trust concerns.","authors":["Kalyan Nakka","Jimmy Dani","Ausmit Mondal","Nitesh Saxena"],"url":"https://arxiv.org/abs/2505.05619"}
{"created":"2025-05-14","title":"InstanceGen: Image Generation with Instance-level Instructions","abstract":"Despite rapid advancements in the capabilities of generative models, pretrained text-to-image models still struggle in capturing the semantics conveyed by complex prompts that compound multiple objects and instance-level attributes. Consequently, we are witnessing growing interests in integrating additional structural constraints, typically in the form of coarse bounding boxes, to better guide the generation process in such challenging cases. In this work, we take the idea of structural guidance a step further by making the observation that contemporary image generation models can directly provide a plausible fine-grained structural initialization. We propose a technique that couples this image-based structural guidance with LLM-based instance-level instructions, yielding output images that adhere to all parts of the text prompt, including object counts, instance-level attributes, and spatial relations between instances.","authors":["Etai Sella","Yanir Kleiman","Hadar Averbuch-Elor"],"url":"https://arxiv.org/abs/2505.05678"}
{"created":"2025-05-14","title":"LLM-Text Watermarking based on Lagrange Interpolation","abstract":"The rapid advancement of LLMs (Large Language Models) has established them as a foundational technology for many AI and ML-powered human computer interactions. A critical challenge in this context is the attribution of LLM-generated text -- either to the specific language model that produced it or to the individual user who embedded their identity via a so-called multi-bit watermark. This capability is essential for combating misinformation, fake news, misinterpretation, and plagiarism. One of the key techniques for addressing this challenge is digital watermarking.","authors":["Jaros{\\l}aw Janas","Pawe{\\l} Morawiecki","Josef Pieprzyk"],"url":"https://arxiv.org/abs/2505.05712"}
{"created":"2025-05-14","title":"Brain Hematoma Marker Recognition Using Multitask Learning: SwinTransformer and Swin-Unet","abstract":"This paper proposes a method MTL-Swin-Unet which is multi-task learning using transformers for classification and semantic segmentation. For spurious-correlation problems, this method allows us to enhance the image representation with two other image representations: representation obtained by semantic segmentation and representation obtained by image reconstruction. In our experiments, the proposed method outperformed in F-value measure than other classifiers when the test data included slices from the same patient (no covariate shift). Similarly, when the test data did not include slices from the same patient (covariate shift setting), the proposed method outperformed in AUC measure.","authors":["Kodai Hirata","Tsuyoshi Okita"],"url":"https://arxiv.org/abs/2505.06185"}
{"created":"2025-05-14","title":"Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies","abstract":"Extracting scientific evidence from biomedical studies for clinical research questions (e.g., Does stem cell transplantation improve quality of life in patients with medically refractory Crohn's disease compared to placebo?) is a crucial step in synthesising biomedical evidence. In this paper, we focus on the task of document-level scientific evidence extraction for clinical questions with conflicting evidence. To support this task, we create a dataset called CochraneForest, leveraging forest plots from Cochrane systematic reviews. It comprises 202 annotated forest plots, associated clinical research questions, full texts of studies, and study-specific conclusions. Building on CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a retrieval-augmented generation framework designed to tackle the unique challenges of evidence extraction. Our experiments show that URCA outperforms the best existing methods by up to 10.3% in F1 score on this task. However, the results also underscore the complexity of CochraneForest, establishing it as a challenging testbed for advancing automated evidence synthesis systems.","authors":["Massimiliano Pronesti","Joao Bettencourt-Silva","Paul Flanagan","Alessandra Pascale","Oisin Redmond","Anya Belz","Yufang Hou"],"url":"https://arxiv.org/abs/2505.06186"}
{"created":"2025-05-14","title":"Knowledge Guided Encoder-Decoder Framework: Integrating Multiple Physical Models for Agricultural Ecosystem Modeling","abstract":"Agricultural monitoring is critical for ensuring food security, maintaining sustainable farming practices, informing policies on mitigating food shortage, and managing greenhouse gas emissions. Traditional process-based physical models are often designed and implemented for specific situations, and their parameters could also be highly uncertain. In contrast, data-driven models often use black-box structures and does not explicitly model the inter-dependence between different ecological variables. As a result, they require extensive training data and lack generalizability to different tasks with data distribution shifts and inconsistent observed variables. To address the need for more universal models, we propose a knowledge-guided encoder-decoder model, which can predict key crop variables by leveraging knowledge of underlying processes from multiple physical models. The proposed method also integrates a language model to process complex and inconsistent inputs and also utilizes it to implement a model selection mechanism for selectively combining the knowledge from different physical models. Our evaluations on predicting carbon and nitrogen fluxes for multiple sites demonstrate the effectiveness and robustness of the proposed model under various scenarios.","authors":["Qi Cheng","Licheng Liu","Yao Zhang","Mu Hong","Shiyuan Luo","Zhenong Jin","Yiqun Xie","Xiaowei Jia"],"url":"https://arxiv.org/abs/2505.06266"}
{"created":"2025-05-14","title":"On Discrete-Time Approximations to Infinite Horizon Differential Games","abstract":"In this paper we study a discrete-time semidiscretization and a fully discretization (discrete-time, discrete-state) of an infinite time horizon noncooperative $N$-player differential game. We prove that as either the discretization time step or both time step and mesh size parameters approach zero the discrete value function approximates the value function of the differential game. Furthermore, the discrete Nash equilibrium is an $\\epsilon$-Nash equilibrium for the continuous-time differential game both in the discrete-time and fully discrete cases.","authors":["Javier de Frutos","V\\'ictor Gat\\'on","Julia Novo"],"url":"https://arxiv.org/abs/2112.03153"}
{"created":"2025-05-14","title":"Inequalities among Symmetric Polynomial Functions: Counter-examples and New Conjectures","abstract":"Inequalities among symmetric polynomial functions are fundamental questions in mathematics and have various applications in science and engineering. This paper investigates a beautiful and inspiring conjecture, proposed by Cuttler, Greene and Skandera in 2011, on inequalities among the complete homogeneous symmetric polynomial function $H_{n,\\lambda}$: It states that the inequality $H_{n,\\lambda}\\leq H_{n,\\mu}$ implies majorization order $\\lambda\\preceq\\mu$. The conjecture is a close analogy with other known results on Muirhead-type inequalities. In 2021, Heaton and Shankar disproved the conjecture by showing a counterexample for number of variables $n=3$ and degree $d=8$. They then asked whether the conjecture is true when $n$ is sufficiently large. In this paper, we show, by a family of counter-examples, that the conjecture does not hold for any $n$ and any $d$ as long as $n\\geq2$ and $d\\geq8$. Based on the insights gained from the counter-examples, we propose a new conjecture for the inequality $H_{n,\\lambda}\\leq H_{n,\\mu}$.","authors":["Jia Xu","Yong Yao"],"url":"https://arxiv.org/abs/2305.19830"}
{"created":"2025-05-14","title":"The next gap in the subrank of 3-tensors","abstract":"Recent works of Costa-Dalai, Christandl-Gesmundo-Zuiddam, Blatter-Draisma-Rupniewski, and Bri\\\"et-Christandl-Leigh-Shpilka-Zuiddam have investigated notions of discreteness and gaps in the possible values that asymptotic tensor ranks can take. In particular, it was shown that the asymptotic subrank and asymptotic slice rank of any nonzero 3-tensor is equal to 1, equal to 1.88, or at least 2 (over any field), and that the set of possible values of these parameters is discrete (in several regimes). We determine exactly the next gap, showing that the asymptotic subrank and asymptotic slice rank of any nonzero 3-tensor is equal to 1, equal to 1.88, equal to 2, or at least 2.68.","authors":["Fulvio Gesmundo","Jeroen Zuiddam"],"url":"https://arxiv.org/abs/2307.06115"}
{"created":"2025-05-14","title":"Outlier-robust neural network training: variation regularization meets trimmed loss to prevent functional breakdown","abstract":"In this study, we tackle the challenge of outlier-robust predictive modeling using highly expressive neural networks. Our approach integrates two key components: (1) a transformed trimmed loss (TTL), a computationally efficient variant of the classical trimmed loss, and (2) higher-order variation regularization (HOVR), which imposes smoothness constraints on the prediction function. While traditional robust statistics typically assume low-complexity models such as linear and kernel models, applying TTL alone to modern neural networks may fail to ensure robustness, as their high expressive power allows them to fit both inliers and outliers, even when a robust loss is used. To address this, we revisit the traditional notion of breakdown point and adapt it to the nonlinear function setting, introducing a regularization scheme via HOVR that controls the model's capacity and suppresses overfitting to outliers. We theoretically establish that our training procedure retains a high functional breakdown point, thereby ensuring robustness to outlier contamination. We develop a stochastic optimization algorithm tailored to this framework and provide a theoretical guarantee of its convergence.","authors":["Akifumi Okuno","Shotaro Yagishita"],"url":"https://arxiv.org/abs/2308.02293"}
{"created":"2025-05-14","title":"A lattice on Dyck paths close to the Tamari lattice","abstract":"We introduce a new poset structure on Dyck paths where the covering relation is a particular case of the relation inducing the Tamari lattice. We prove that the transitive closure of this relation endows Dyck paths with a lattice structure. We provide a trivariate generating function counting the number of Dyck paths with respect to the semilength, the numbers of outgoing and incoming edges in the Hasse diagram. We deduce the numbers of coverings, meet and join irreducible elements. As a byproduct, we present a new involution on Dyck paths that transports the bistatistic of the numbers of outgoing and incoming edges into its reverse. Finally, we give a generating function for the number of intervals, and we compare this number with the number of intervals in the Tamari lattice.","authors":["Jean-Luc Baril","Sergey Kirgizov","Mehdi Naima"],"url":"https://arxiv.org/abs/2309.00426"}
{"created":"2025-05-14","title":"The Complexity of Resilience Problems via Valued Constraint Satisfaction","abstract":"Valued constraint satisfaction problems (VCSPs) constitute a large class of computational optimisation problems. It was shown recently that, over finite domains, every VCSP is in P or NP-complete, depending on the admitted cost functions. In this article, we study cost functions over countably infinite domains whose automorphisms form an oligomorphic permutation group. Our results include a hardness condition based on a generalisation of pp-constructability as known from classical CSPs and a polynomial-time tractability condition based on the concept of fractional polymorphisms. We then observe that the resilience problem for unions of conjunctive queries (UCQs) studied in database theory, under bag semantics, may be viewed as a special case of the VCSPs that we consider. We obtain a complexity dichotomy for the case of incidence-acyclic UCQs and exemplarily use our methods to determine the complexity of a conjunctive query that has been stated as an open problem in the literature. We conjecture that our hardness and tractability conditions match for resilience problems for UCQs. Further, we obtain a complete dichotomy for resilience problems for two-way regular path queries, under bag semantics.","authors":["Manuel Bodirsky","\\v{Z}aneta Semani\\v{s}inov\\'a","Carsten Lutz"],"url":"https://arxiv.org/abs/2309.15654"}
{"created":"2025-05-14","title":"AI-accelerated Discovery of Altermagnetic Materials","abstract":"Altermagnetism, a new magnetic phase, has been theoretically proposed and experimentally verified to be distinct from ferromagnetism and antiferromagnetism. Although altermagnets have been found to possess many exotic physical properties, the limited availability of known altermagnetic materials hinders the study of such properties. Hence, discovering more types of altermagnetic materials with different properties is crucial for a comprehensive understanding of altermagnetism and thus facilitating new applications in the next generation information technologies, e.g., storage devices and high-sensitivity sensors. Since each altermagnetic material has a unique crystal structure, we propose an automated discovery approach empowered by an AI search engine that employs a pre-trained graph neural network to learn the intrinsic features of the material crystal structure, followed by fine-tuning a classifier with limited positive samples to predict the altermagnetism probability of a given material candidate. Finally, we successfully discovered 50 new altermagnetic materials that cover metals, semiconductors, and insulators confirmed by the first-principles electronic structure calculations. The wide range of electronic structural characteristics reveals that various novel physical properties manifest in these newly discovered altermagnetic materials, e.g., anomalous Hall effect, anomalous Kerr effect, and topological property. Noteworthy, we discovered 4 $i$-wave altermagnetic materials for the first time. Overall, the AI search engine performs much better than human experts and suggests a set of new altermagnetic materials with unique properties, outlining its potential for accelerated discovery of the materials with targeted properties.","authors":["Ze-Feng Gao","Shuai Qu","Bocheng Zeng","Yang Liu","Ji-Rong Wen","Hao Sun","Peng-Jie Guo","Zhong-Yi Lu"],"url":"https://arxiv.org/abs/2311.04418"}
{"created":"2025-05-14","title":"Pattern Avoidance for Fibonacci Sequences using $k$-Regular Words","abstract":"Two $k$-ary Fibonacci recurrences are $a_k(n) = a_k(n-1) + k \\cdot a_k(n-2)$ and $b_k(n) = k \\cdot b_k(n-1) + b_k(n-2)$. We provide a simple proof that $a_k(n)$ is the number of $k$-regular words over $[n] = \\{1,2,\\ldots,n\\}$ that avoid patterns $\\{121, 123, 132, 213\\}$ when using base cases $a_k(0) = a_k(1) = 1$ for any $k \\geq 1$. This was previously proven by Kuba and Panholzer in the context of Wilf-equivalence for restricted Stirling permutations, and it creates Simion and Schmidt's classic result on the Fibonacci sequence when $k=1$, and the Jacobsthal sequence when $k=2$. We complement this theorem by proving that $b_k(n)$ is the number of $k$-regular words over $[n]$ that avoid $\\{122, 213\\}$ with $b_k(0) = b_k(1) = 1$ for any~$k \\geq 2$. Finally, we conjecture that $|Av^{2}_{n}(\\underline{121}, 123, 132, 213)| = a_1(n)^2$ for $n \\geq 0$. That is, vincularizing the Stirling pattern in Kuba and Panholzer's Jacobsthal result gives the Fibonacci-squared numbers.","authors":["Emily Downing","Elizabeth Hartung","Cody Lucido","Aaron Williams"],"url":"https://arxiv.org/abs/2312.16052"}
{"created":"2025-05-14","title":"An explicit Euler method for Sobolev vector fields with applications to the continuity equation on non cartesian grids","abstract":"We prove a novel stability estimate in $L^\\infty _t (L^p _x)$ between the regular Lagrangian flow of a Sobolev vector field and a piecewise affine approximation of such flow. This approximation of the flow is obtained by a (sort of) explicit Euler method, and it is the crucial tool to prove approximation results for the solution of the continuity equation by using the representation of the solution as the push-forward via the regular Lagrangian flow of the initial datum. We approximate the solution in two ways, one probabilistic and one deterministic, using different approximations for both the flow and the initial datum.","authors":["Tommaso Cortopassi"],"url":"https://arxiv.org/abs/2402.04118"}
{"created":"2025-05-14","title":"Considerations in the use of ML interaction potentials for free energy calculations","abstract":"Machine learning potentials (MLPs) offer the potential to accurately model the energy and free energy landscapes of molecules with the precision of quantum mechanics and an efficiency similar to classical simulations. This research focuses on using equivariant graph neural networks MLPs due to their proven effectiveness in modeling equilibrium molecular trajectories. A key issue addressed is the capability of MLPs to accurately predict free energies and transition states by considering both the energy and the diversity of molecular configurations. We examined how the distribution of collective variables (CVs) in the training data affects MLP accuracy in determining the free energy surface (FES) of systems, using Metadynamics simulations for butane and alanine dipeptide (ADP). The study involved training forty-three MLPs, half based on classical molecular dynamics data and the rest on ab initio computed energies. The MLPs were trained using different distributions that aim to replicate hypothetical scenarios of sampled CVs obtained if the underlying FES of the system was unknown. Findings for butane revealed that training data coverage of key FES regions ensures model accuracy regardless of CV distribution. However, missing significant FES regions led to correct potential energy predictions but failed free energy reconstruction. For ADP, models trained on classical dynamics data were notably less accurate, while ab initio-based MLPs predicted potential energy well but faltered on free energy predictions. These results emphasize the challenge of assembling an all-encompassing training set for accurate FES prediction and highlight the importance of understanding the FES in preparing training data. The study points out the limitations of MLPs in free energy calculations, stressing the need for comprehensive data that encompasses the system's full FES for effective model training.","authors":["Orlando A. Mendible","Jonathan K. Whitmer","Yamil J. Col\\'on"],"url":"https://arxiv.org/abs/2403.13952"}
{"created":"2025-05-14","title":"Using Few-Shot Learning to Classify Primary Lung Cancer and Other Malignancy with Lung Metastasis in Cytological Imaging via Endobronchial Ultrasound Procedures","abstract":"This study presents a computer-aided diagnosis (CAD) system to assist early detection of lung metastases during endobronchial ultrasound (EBUS) procedures, significantly reducing follow-up time and enabling timely treatment. Due to limited cytology images and morphological similarities among cells, classifying lung metastases is challenging, and existing research rarely targets this issue directly.To overcome data scarcity and improve classification, the authors propose a few-shot learning model using a hybrid pretrained backbone with fine-grained classification and contrastive learning. Parameter-efficient fine-tuning on augmented support sets enhances generalization and transferability. The model achieved 49.59% accuracy, outperforming existing methods. With 20 image samples, accuracy improved to 55.48%, showing strong potential for identifying rare or novel cancer types in low-data clinical environments.","authors":["Ching-Kai Lin","Di-Chun Wei","Yun-Chien Cheng"],"url":"https://arxiv.org/abs/2404.06080"}
{"created":"2025-05-14","title":"Layered Uploading for Quantum Convolutional Neural Networks","abstract":"Continuing our analysis of quantum machine learning applied to our use-case of malware detection, we investigate the potential of quantum convolutional neural networks. More precisely, we propose a new architecture where data is uploaded all along the quantum circuit. This allows us to use more features from the data, hence giving to the algorithm more information, without having to increase the number of qubits that we use for the quantum circuit. This approach is motivated by the fact that we do not always have great amounts of data, and that quantum computers are currently restricted in their number of logical qubits.","authors":["Gr\\'egoire Barru\\'e","Tony Quertier","Orlane Zang"],"url":"https://arxiv.org/abs/2404.09750"}
{"created":"2025-05-14","title":"Trade-off between Gradient Measurement Efficiency and Expressivity in Deep Quantum Neural Networks","abstract":"Quantum neural networks (QNNs) require an efficient training algorithm to achieve practical quantum advantages. A promising approach is gradient-based optimization, where gradients are estimated by quantum measurements. However, QNNs currently lack general quantum algorithms for efficiently measuring gradients, which limits their scalability. To elucidate the fundamental limits and potentials of efficient gradient estimation, we rigorously prove a trade-off between gradient measurement efficiency (the mean number of simultaneously measurable gradient components) and expressivity in deep QNNs. This trade-off indicates that more expressive QNNs require higher measurement costs per parameter for gradient estimation, while reducing QNN expressivity to suit a given task can increase gradient measurement efficiency. We further propose a general QNN ansatz called the stabilizer-logical product ansatz (SLPA), which achieves the trade-off upper bound by exploiting the symmetric structure of the quantum circuit. Numerical experiments show that the SLPA drastically reduces the sample complexity needed for training while maintaining accuracy and trainability compared to well-designed circuits based on the parameter-shift method.","authors":["Koki Chinzei","Shinichiro Yamano","Quoc Hoan Tran","Yasuhiro Endo","Hirotaka Oshima"],"url":"https://arxiv.org/abs/2406.18316"}
{"created":"2025-05-14","title":"Genus expansion for non-linear random matrix ensembles with applications to neural networks","abstract":"We present a unified approach to studying certain non-linear random matrix ensembles and associated random neural networks at initialization. This begins with a novel series expansion for neural networks which generalizes Fa\\'a di Bruno's formula to an arbitrary number of compositions. The role of monomials is played by random multilinear maps indexed by directed graphs, whose edges correspond to random matrices. Crucially, this expansion linearizes the effect of the activation functions, allowing for the direct application of Wick's principle and the genus expansion technique. As an application, we prove several results about neural networks with random weights. We first give a new proof of the fact that they converge to Gaussian processes as their width tends to infinity. Secondly, we quantify the rate of convergence of the Neural Tangent Kernel to its deterministic limit in Frobenius norm. Finally, we compute the moments of the limiting spectral distribution of the Jacobian (only the first two of which were previously known), expressing them as sums over non-crossing partitions. All of these results are then generalised to the case of neural networks with sparse and non-Gaussian weights, under moment assumptions.","authors":["Nicola Muca Cirone","Jad Hamdan","Cristopher Salvi"],"url":"https://arxiv.org/abs/2407.08459"}
{"created":"2025-05-14","title":"Behavioral and Topological Heterogeneities in Network Versions of Schelling's Segregation Model","abstract":"Agent-based models of residential segregation have been of persistent interest to various research communities since their origin with James Sakoda and popularization by Thomas Schelling. Frequently, these models have sought to elucidate the extent to which the collective dynamics of individual preferences may cause segregation to emerge. This open question has sustained relevance in U.S. jurisprudence. Previous investigation of heterogeneity of behaviors (preferences) has shown reductions in segregation. Meanwhile, previous investigation of heterogeneity of social network topologies has shown no significant impact to observed segregation levels. In the present study, we examined the effects of the concurrent presence of both behavioral and topological heterogeneities in network segregation models. Simulations were conducted using both homogeneous and heterogeneous preference models on 2D lattices with varied levels of densification to create topological heterogeneities (i.e., clusters, hubs). Results show a richer variety of outcomes, including novel differences in resultant segregation levels and hub composition. Notably, with concurrent increased representations of heterogeneous preferences and heterogeneous topologies, reduced levels of segregation emerge. Simultaneously, we observe a novel dynamic of segregation between tolerance levels as highly tolerant nodes take residence in dense areas and push intolerant nodes to sparse areas mimicking the urban-rural divide.","authors":["Will Deter","Hiroki Sayama"],"url":"https://arxiv.org/abs/2408.05623"}
{"created":"2025-05-14","title":"InvDesFlow: An AI-driven materials inverse design workflow to explore possible high-temperature superconductors","abstract":"The discovery of new superconducting materials, particularly those exhibiting high critical temperature ($T_c$), has been a vibrant area of study within the field of condensed matter physics. Conventional approaches primarily rely on physical intuition to search for potential superconductors within the existing databases. However, the known materials only scratch the surface of the extensive array of possibilities within the realm of materials. Here, we develop InvDesFlow, an AI search engine that integrates deep model pre-training and fine-tuning techniques, diffusion models, and physics-based approaches (e.g., first-principles electronic structure calculation) for the discovery of high-$T_c$ superconductors. Utilizing InvDesFlow, we have obtained 74 dynamically stable materials with critical temperatures predicted by the AI model to be $T_c \\geq$ 15 K based on a very small set of samples. Notably, these materials are not contained in any existing dataset. Furthermore, we analyze trends in our dataset and individual materials including B$_4$CN$_3$ (at 5 GPa) and B$_5$CN$_2$ (at ambient pressure) whose $T_c$s are 24.08 K and 15.93 K, respectively. We demonstrate that AI technique can discover a set of new high-$T_c$ superconductors, outline its potential for accelerating discovery of the materials with targeted properties.","authors":["Xiao-Qi Han","Zhenfeng Ouyang","Peng-Jie Guo","Hao Sun","Ze-Feng Gao","Zhong-Yi Lu"],"url":"https://arxiv.org/abs/2409.08065"}
{"created":"2025-05-14","title":"Nesterov acceleration in benignly non-convex landscapes","abstract":"While momentum-based optimization algorithms are commonly used in the notoriously non-convex optimization problems of deep learning, their analysis has historically been restricted to the convex and strongly convex setting. In this article, we partially close this gap between theory and practice and demonstrate that virtually identical guarantees can be obtained in optimization problems with a `benign' non-convexity. We show that these weaker geometric assumptions are well justified in overparametrized deep learning, at least locally. Variations of this result are obtained for a continuous time model of Nesterov's accelerated gradient descent algorithm (NAG), the classical discrete time version of NAG, and versions of NAG with stochastic gradient estimates with purely additive noise and with noise that exhibits both additive and multiplicative scaling.","authors":["Kanan Gupta","Stephan Wojtowytsch"],"url":"https://arxiv.org/abs/2410.08395"}
{"created":"2025-05-14","title":"Multi-Task Dynamic Pricing in Credit Market with Contextual Information","abstract":"We study the dynamic pricing problem faced by a broker seeking to learn prices for a large number of credit market securities, such as corporate bonds, government bonds, loans, and other credit-related securities. A major challenge in pricing these securities stems from their infrequent trading and the lack of transparency in over-the-counter (OTC) markets, which leads to insufficient data for individual pricing. Nevertheless, many securities share structural similarities that can be exploited. Moreover, brokers often place small \"probing\" orders to infer competitors' pricing behavior. Leveraging these insights, we propose a multi-task dynamic pricing framework that leverages the shared structure across securities to enhance pricing accuracy.","authors":["Adel Javanmard","Jingwei Ji","Renyuan Xu"],"url":"https://arxiv.org/abs/2410.14839"}
{"created":"2025-05-14","title":"Physics-informed neural networks viewpoint for solving the Dyson-Schwinger equations of quantum electrodynamics","abstract":"Physics-informed neural networks (PINNs) are employed to solve the Dyson--Schwinger equations of quantum electrodynamics (QED) in Euclidean space, with a focus on the non-perturbative generation of the fermion's dynamical mass function in the Landau gauge. By inserting the integral equation directly into the loss function, our PINN framework enables a single neural network to learn a continuous and differentiable representation of the mass function over a spectrum of momenta. Also, we benchmark our approach against a traditional numerical algorithm showing the main differences among them. Our novel strategy, which is expected to be extended to other quantum field theories, is the first step towards forefront applications of machine learning in high-level theoretical physics.","authors":["Rodrigo Carmo Terin"],"url":"https://arxiv.org/abs/2411.02177"}
{"created":"2025-05-14","title":"Rydberg Atomic Quantum Receivers for Classical Wireless Communications and Sensing: Their Models and Performance","abstract":"The significant progress of quantum sensing technologies offer numerous radical solutions for measuring a multitude of physical quantities at an unprecedented precision. Among them, Rydberg atomic quantum receivers (RAQRs) emerge as an eminent solution for detecting the electric field of radio frequency (RF) signals, exhibiting great potential in assisting classical wireless communications and sensing. So far, most experimental studies have aimed for the proof of physical concepts to reveal its promise, while the practical signal model of RAQR-aided wireless communications and sensing remained under-explored. Furthermore, the performance of RAQR-based wireless receivers and their advantages over classical RF receivers have not been fully characterized. To fill these gaps, we introduce the RAQR to the wireless community by presenting an end-to-end reception scheme. We then develop a corresponding equivalent baseband signal model relying on a realistic reception flow. Our scheme and model provide explicit design guidance to RAQR-aided wireless systems. We next study the performance of RAQR-aided wireless systems based on our model, and compare them to classical RF receivers. The results show that the RAQR is capable of achieving a substantial received signal-to-noise ratio (SNR) gain of over $27$ decibel (dB) and $40$ dB in the photon shot limit regime and the standard quantum limit regime, respectively.","authors":["Tierui Gong","Jiaming Sun","Chau Yuen","Guangwei Hu","Yufei Zhao","Yong Liang Guan","Chong Meng Samson See","M\\'erouane Debbah","Lajos Hanzo"],"url":"https://arxiv.org/abs/2412.05554"}
{"created":"2025-05-14","title":"Estimation of Food Intake Quantity Using Inertial Signals from Smartwatches","abstract":"Accurate monitoring of eating behavior is crucial for managing obesity and eating disorders such as bulimia nervosa. At the same time, existing methods rely on multiple and/or specialized sensors, greatly harming adherence and ultimately, the quality and continuity of data. This paper introduces a novel approach for estimating the weight of a bite, from a commercial smartwatch. Our publicly-available dataset contains smartwatch inertial data from ten participants, with manually annotated start and end times of each bite along with their corresponding weights from a smart scale, under semi-controlled conditions. The proposed method combines extracted behavioral features such as the time required to load the utensil with food, with statistical features of inertial signals, that serve as input to a Support Vector Regression model to estimate bite weights. Under a leave-one-subject-out cross-validation scheme, our approach achieves a mean absolute error (MAE) of 3.99 grams per bite. To contextualize this performance, we introduce the improvement metric, that measures the relative MAE difference compared to a baseline model. Our method demonstrates a 17.41% improvement, while the adapted state-of-the art method shows a -28.89% performance against that same baseline. The results presented in this work establish the feasibility of extracting meaningful bite weight estimates from commercial smartwatch inertial sensors alone, laying the groundwork for future accessible, non-invasive dietary monitoring systems.","authors":["Ioannis Levi","Konstantinos Kyritsis","Vasileios Papapanagiotou","Georgios Tsakiridis","Anastasios Delopoulos"],"url":"https://arxiv.org/abs/2502.06649"}
{"created":"2025-05-14","title":"A Finite Sample Analysis of Distributional TD Learning with Linear Function Approximation","abstract":"In this paper, we study the finite-sample statistical rates of distributional temporal difference (TD) learning with linear function approximation. The aim of distributional TD learning is to estimate the return distribution of a discounted Markov decision process for a given policy {\\pi}. Previous works on statistical analysis of distributional TD learning mainly focus on the tabular case. In contrast, we first consider the linear function approximation setting and derive sharp finite-sample rates. Our theoretical results demonstrate that the sample complexity of linear distributional TD learning matches that of classic linear TD learning. This implies that, with linear function approximation, learning the full distribution of the return from streaming data is no more difficult than learning its expectation (value function). To derive tight sample complexity bounds, we conduct a fine-grained analysis of the linear-categorical Bellman equation and employ the exponential stability arguments for products of random matrices. Our results provide new insights into the statistical efficiency of distributional reinforcement learning algorithms.","authors":["Yang Peng","Kaicheng Jin","Liangyu Zhang","Zhihua Zhang"],"url":"https://arxiv.org/abs/2502.14172"}
{"created":"2025-05-14","title":"Optimization-free Smooth Control Barrier Function for Polygonal Collision Avoidance","abstract":"Polygonal collision avoidance (PCA) is short for the problem of collision avoidance between two polygons (i.e., polytopes in planar) that own their dynamic equations. This problem suffers the inherent difficulty in dealing with non-smooth boundaries and recently optimization-defined metrics, such as signed distance field (SDF) and its variants, have been proposed as control barrier functions (CBFs) to tackle PCA problems. In contrast, we propose an optimization-free smooth CBF method in this paper, which is computationally efficient and proved to be nonconservative. It is achieved by three main steps: a lower bound of SDF is expressed as a nested Boolean logic composition first, then its smooth approximation is established by applying the latest log-sum-exp method, after which a specified CBF-based safety filter is proposed to address this class of problems. To illustrate its wide applications, the optimization-free smooth CBF method is extended to solve distributed collision avoidance of two underactuated nonholonomic vehicles and drive an underactuated container crane to avoid a moving obstacle respectively, for which numerical simulations are also performed.","authors":["Shizhen Wu","Yongchun Fang","Ning Sun","Biao Lu","Xiao Liang","Yiming Zhao"],"url":"https://arxiv.org/abs/2502.16293"}
{"created":"2025-05-14","title":"An Analysis of Data Transformation Effects on Segment Anything 2","abstract":"Video object segmentation (VOS) is a critical task in the development of video perception and understanding. The Segment-Anything Model 2 (SAM 2), released by Meta AI, is the current state-of-the-art architecture for end-to-end VOS. SAM 2 performs very well on both clean video data and augmented data, and completely intelligent video perception requires an understanding of how this architecture is capable of achieving such quality results. To better understand how each step within the SAM 2 architecture permits high-quality video segmentation, a variety of complex video transformations are passed through the architecture, and the impact at each stage of the process is measured. It is observed that each progressive stage enables the filtering of complex transformation noise and the emphasis of the object of interest. Contributions include the creation of complex transformation video datasets, an analysis of how each stage of the SAM 2 architecture interprets these transformations, and visualizations of segmented objects through each stage. By better understanding how each model structure impacts overall video understanding, VOS development can work to improve real-world applicability and performance tracking, localizing, and segmenting objects despite complex cluttered scenes and obscurations.","authors":["Clayton Bromley","Alexander Moore","Amar Saini","Doug Poland","Carmen Carrano"],"url":"https://arxiv.org/abs/2503.00042"}
{"created":"2025-05-14","title":"GBT-SAM: Adapting a Foundational Deep Learning Model for Generalizable Brain Tumor Segmentation via Efficient Integration of Multi-Parametric MRI Data","abstract":"Gliomas are aggressive brain tumors that require accurate imaging-based diagnosis, with segmentation playing a critical role in evaluating morphology and treatment decisions. Manual delineation of gliomas is time-consuming and prone to variability, motivating the use of deep learning to improve consistency and alleviate clinical workload. However, existing methods often fail to fully exploit the information available in multi-parametric MRI (mp-MRI), particularly inter-slice contextual features, and typically require considerable computational resources while lacking robustness across tumor type variations. We present GBT-SAM, a parameter-efficient deep learning framework that adapts the Segment Anything Model (SAM), a large-scale vision model, to volumetric mp-MRI data. GBT-SAM reduces input complexity by selecting fewer than 2.6\\% of slices per scan while incorporating all four MRI modalities, preserving essential tumor-related information with minimal cost. Furthermore, our model is trained by a two-step fine-tuning strategy that incorporates a depth-aware module to capture inter-slice correlations and lightweight adaptation layers, resulting in just 6.5M trainable parameters, which is the lowest among SAM-based approaches. GBT-SAM achieves a Dice Score of 93.54 on the BraTS Adult Glioma dataset and demonstrates robust performance on Meningioma, Pediatric Glioma, and Sub-Saharan Glioma datasets. These results highlight GBT-SAM's potential as a computationally efficient and domain-robust framework for brain tumor segmentation using mp-MRI. Our code and models are available at https://github.com/vpulab/med-sam-brain .","authors":["Cecilia Diana-Albelda","Roberto Alcover-Couso","\\'Alvaro Garc\\'ia-Mart\\'in","Jesus Bescos","Marcos Escudero-Vi\\~nolo"],"url":"https://arxiv.org/abs/2503.04325"}
{"created":"2025-05-14","title":"Decadal analysis of sea surface temperature patterns, climatology, and anomalies in temperate coastal waters with Landsat-8 TIRS observations","abstract":"Sea surface temperature (SST) is a fundamental physical parameter characterising the thermal state of sea surface. Due to the intricate thermal interactions between land, sea, and atmosphere, the spatial gradients of SST in coastal waters often appear at finer spatial scales than those in open ocean waters. The Thermal Infrared Sensor (TIRS) onboard Landsat-8, with its 100-meter spatial resolution, offers a unique opportunity to uncover fine-scale coastal SST patterns that would otherwise be overlooked by coarser-resolution thermal sensors. In this study, we first analysed the spatiotemporal patterns of SST in South Australia's temperate coastal waters from 2014 to 2023 by developing an operational approach for SST retrieval from the Landsat-8 TIRS sensor. A buoy was deployed off the coast of Port Lincoln, South Australia, to validate the quality of SST retrievals. Then the daily baseline climatology of SST with 100 m resolution was constructed, which allowed for the detection and analysis of anomalous SST events. Our results suggest the following: (1) the satellite-derived SST data aligned well with the in-situ measured SST values; (2) the semi-enclosed, shallow regions of Upper Spencer Gulf and Upper St Vincent Gulf showed higher temperatures during summer and cooler temperatures during winter than waters closer to the open ocean, resulting in a higher seasonal variation in SST; (3) the near-shore shallow areas in Spencer Gulf and St Vincent Gulf, and regions surrounding Kangaroo Island, were identified to have a higher probability of SST anomalies compared to the rest of the study area; and (4) anomalous SST events were more likely to happen during the warm months than the cool months. We hope these findings would be helpful in supporting the fishing and aquaculture industries in the coastal waters of South Australia.","authors":["Yiqing Guo","Nagur Cherukuru","Eric Lehmann","Xiubin Qi","Mark Doubelld","S. L. Kesav Unnithan","Ming Feng"],"url":"https://arxiv.org/abs/2503.05843"}
{"created":"2025-05-14","title":"Automatic quality control in multi-centric fetal brain MRI super-resolution reconstruction","abstract":"Quality control (QC) has long been considered essential to guarantee the reliability of neuroimaging studies. It is particularly important for fetal brain MRI, where acquisitions and image processing techniques are less standardized than in adult imaging. In this work, we focus on automated quality control of super-resolution reconstruction (SRR) volumes of fetal brain MRI, an important processing step where multiple stacks of thick 2D slices are registered together and combined to build a single, isotropic and artifact-free T2 weighted volume. We propose FetMRQC$_{SR}$, a machine-learning method that extracts more than 100 image quality metrics to predict image quality scores using a random forest model. This approach is well suited to a problem that is high dimensional, with highly heterogeneous data and small datasets. We validate FetMRQC$_{SR}$ in an out-of-domain (OOD) setting and report high performance (ROC AUC = 0.89), even when faced with data from an unknown site or SRR method. We also investigate failure cases and show that they occur in $45\\%$ of the images due to ambiguous configurations for which the rating from the expert is arguable. These results are encouraging and illustrate how a non deep learning-based method like FetMRQC$_{SR}$ is well suited to this multifaceted problem. Our tool, along with all the code used to generate, train and evaluate the model are available at https://github.com/Medical-Image-Analysis-Laboratory/fetmrqc_sr/ .","authors":["Thomas Sanchez","Vladyslav Zalevskyi","Angeline Mihailov","Gerard Mart\\'i-Juan","Elisenda Eixarch","Andras Jakab","Vincent Dunet","M\\'eriam Koob","Guillaume Auzias","Meritxell Bach Cuadra"],"url":"https://arxiv.org/abs/2503.10156"}
{"created":"2025-05-14","title":"Matching, Unanticipated Experiences, Divorce, Flirting, Rematching, Etc","abstract":"We study dynamic decentralized two-sided matching in which players may encounter unanticipated experiences. As they become aware of these experiences, they may change their preferences over players on the other side of the market. Consequently, they may get ``divorced'' and rematch again with other agents, which may lead to further unanticipated experiences etc. A matching is stable if there is absence of pairwise common belief in blocking. Stable matchings can be destabilized by unanticipated experiences. Yet, we show that there exist self-confirming outcomes that are stable and do not lead to further unanticipated experiences. We introduce a natural decentralized matching process that, at each period assigns probability $1 - \\varepsilon$ to the satisfaction of a mutual optimal blocking pair (if it exists) and picks any optimal blocking pair otherwise. The parameter $\\varepsilon$ is interpreted as a friction of the matching market. We show that for any decentralized matching process, frictions are necessary for convergence to stability even without unawareness. Our process converges to self-confirming stable outcomes. Further, we allow for bilateral communication/flirting that changes the awareness and say that a matching is flirt-proof stable if there is absence of communication leading to pairwise common belief in blocking. We show that our natural decentralized matching process converges to flirt-proof self-confirming outcomes.","authors":["Burkhard C. Schipper","Tina Danting Zhang"],"url":"https://arxiv.org/abs/2504.01280"}
{"created":"2025-05-14","title":"Deep Representation Learning for Unsupervised Clustering of Myocardial Fiber Trajectories in Cardiac Diffusion Tensor Imaging","abstract":"Understanding the complex myocardial architecture is critical for diagnosing and treating heart disease. However, existing methods often struggle to accurately capture this intricate structure from Diffusion Tensor Imaging (DTI) data, particularly due to the lack of ground truth labels and the ambiguous, intertwined nature of fiber trajectories. We present a novel deep learning framework for unsupervised clustering of myocardial fibers, providing a data-driven approach to identifying distinct fiber bundles. We uniquely combine a Bidirectional Long Short-Term Memory network to capture local sequential information along fibers, with a Transformer autoencoder to learn global shape features, with pointwise incorporation of essential anatomical context. Clustering these representations using a density-based algorithm identifies 33 to 62 robust clusters, successfully capturing the subtle distinctions in fiber trajectories with varying levels of granularity. Our framework offers a new, flexible, and quantitative way to analyze myocardial structure, achieving a level of delineation that, to our knowledge, has not been previously achieved, with potential applications in improving surgical planning, characterizing disease-related remodeling, and ultimately, advancing personalized cardiac care.","authors":["Mohini Anand","Xavier Tricoche"],"url":"https://arxiv.org/abs/2504.01953"}
{"created":"2025-05-14","title":"Steiner Traveling Salesman Problem with Quantum Annealing","abstract":"The Steiner Traveling Salesman Problem (STSP) is a variant of the classical Traveling Salesman Problem. The STSP involves incorporating steiner nodes, which are extra nodes not originally part of the required visit set but that can be added to the route to enhance the overall solution and minimize the total travel cost. Given the NP-hard nature of the STSP, we propose a quantum approach to address it. Specifically, we employ quantum annealing using D-Wave's hardware to explore its potential for solving this problem. To enhance computational feasibility, we develop a preprocessing method that effectively reduces the network size. Our experimental results demonstrate that this reduction technique significantly decreases the problem complexity, making the Quadratic Unconstrained Binary Optimization formulation, the standard input for quantum annealers, better suited for existing quantum hardware. Furthermore, the results highlight the potential of quantum annealing as a promising and innovative approach for solving the STSP.","authors":["Alessia Ciacco","Francesca Guerriero","Eneko Osaba"],"url":"https://arxiv.org/abs/2504.02388"}
{"created":"2025-05-14","title":"Vertex-Based Localization of Tur\\'{a}n's Theorem","abstract":"Let $G$ be a simple graph with $n$ vertices and $m$ edges. According to Tur\\'{a}n's theorem, if $G$ is $K_{r+1}$-free, then $m \\leq |E(T(n, r))|,$ where $T(n, r)$ denotes the Tur\\'{a}n graph on $n$ vertices with a maximum clique of order $r$. A limitation of this statement is that it does not give an expression in terms of $n$ and $r$. A widely used version of Tur\\'{a}n's theorem states that for an $n$-vertex $K_{r+1}$-free graph, $m \\leq \\left\\lfloor \\frac{n^2(r-1)}{2r} \\right\\rfloor.$ Though this bound is often more convenient, it is not the same as the original statement. In particular, the class of extremal graphs for this bound, say $\\mathcal{S}$, is a proper subset of the set of Tur\\'{a}n graphs. In this paper, we generalize this result as follows: For each $v \\in V(G)$, let $c(v)$ be the order of the largest clique that contains $v$. We show that \\[ m \\leq \\left\\lfloor\\frac{n}{2}\\sum_{v\\in V(G)}\\frac{c(v)-1}{c(v)}\\right\\rfloor\\] Furthermore, we characterize the class of extremal graphs that attain equality in this bound. Interestingly, this class contains two extra non-Tur\\'{a}n graphs other than the graphs in $\\mathcal{S}$.","authors":["Rajat Adak (Indian Institute of Science","Bangalore)","L. Sunil Chandran (Indian Institute of Science","Bangalore)"],"url":"https://arxiv.org/abs/2504.02806"}
{"created":"2025-05-14","title":"FLOWR: Flow Matching for Structure-Aware De Novo, Interaction- and Fragment-Based Ligand Generation","abstract":"We introduce FLOWR, a novel structure-based framework for the generation and optimization of three-dimensional ligands. FLOWR integrates continuous and categorical flow matching with equivariant optimal transport, enhanced by an efficient protein pocket conditioning. Alongside FLOWR, we present SPINDR, a thoroughly curated dataset comprising ligand-pocket co-crystal complexes specifically designed to address existing data quality issues. Empirical evaluations demonstrate that FLOWR surpasses current state-of-the-art diffusion- and flow-based methods in terms of PoseBusters-validity, pose accuracy, and interaction recovery, while offering a significant inference speedup, achieving up to 70-fold faster performance. In addition, we introduce FLOWR:multi, a highly accurate multi-purpose model allowing for the targeted sampling of novel ligands that adhere to predefined interaction profiles and chemical substructures for fragment-based design without the need of re-training or any re-sampling strategies","authors":["Julian Cremer","Ross Irwin","Alessandro Tibo","Jon Paul Janet","Simon Olsson","Djork-Arn\\'e Clevert"],"url":"https://arxiv.org/abs/2504.10564"}
{"created":"2025-05-14","title":"The Sponge is Quantum Indifferentiable","abstract":"The sponge is a cryptographic construction that turns a public permutation into a hash function. When instantiated with the Keccak permutation, the sponge forms the NIST SHA-3 standard. SHA-3 is a core component of most post-quantum public-key cryptography schemes slated for worldwide adoption.","authors":["Gorjan Alagic","Joseph Carolan","Christian Majenz","Saliha Tokat"],"url":"https://arxiv.org/abs/2504.16887"}
{"created":"2025-05-14","title":"Metric Similarity and Manifold Learning of Circular Dichroism Spectra of Proteins","abstract":"We present a machine learning analysis of circular dichroism spectra of globular proteins from the SP175 database, using the optimal transport-based $1$-Wasserstein distance $\\mathcal{W}_1$ (with order $p=1$) and the manifold learning algorithm $t$-SNE. Our results demonstrate that $\\mathcal{W}_1$ is consistent with both Euclidean and Manhattan metrics while exhibiting robustness to noise. On the other hand, $t$-SNE uncovers meaningful structure in the high-dimensional data. The clustering in the $t$-SNE embedding is primarily determined by proteins with distinct secondary structure compositions: one cluster predominantly contains $\\beta$-rich proteins, while the other consists mainly of proteins with mixed $\\alpha/\\beta$ and $\\alpha$-helical content.","authors":["Gionni Marchetti"],"url":"https://arxiv.org/abs/2504.19355"}
{"created":"2025-05-14","title":"Cryptography without Long-Term Quantum Memory and Global Entanglement: Classical Setups for One-Time Programs, Copy Protection, and Stateful Obfuscation","abstract":"We show how oracles which only allow for classical query access can be used to construct a variety of quantum cryptographic primitives which do not require long-term quantum memory or global entanglement. Specifically, if a quantum party can execute a semi-quantum token scheme (Shmueli 2022) with probability of success $1/2 + \\delta$, we can build powerful cryptographic primitives with a multiplicative logarithmic overhead for the desired correctness error. Our scheme makes no assumptions about the quantum party's noise model except for a simple independence requirement: noise on two sets of non-entangled hardware must be independent.","authors":["Lev Stambler"],"url":"https://arxiv.org/abs/2504.21842"}
{"created":"2025-05-14","title":"Key exchange protocol based on circulant matrix action over congruence-simple semiring","abstract":"We present a new key exchange protocol based on circulant matrices acting on matrices over a congruence-simple semiring. We describe how to compute matrices with the necessary properties for the implementation of the protocol. Additionally, we provide an analysis of its computational cost and its security against known attacks.","authors":["Alvaro Otero Sanchez"],"url":"https://arxiv.org/abs/2505.00664"}
{"created":"2025-05-14","title":"Quantum Support Vector Regression for Robust Anomaly Detection","abstract":"Anomaly Detection (AD) is critical in data analysis, particularly within the domain of IT security. In recent years, Machine Learning (ML) algorithms have emerged as a powerful tool for AD in large-scale data. In this study, we explore the potential of quantum ML approaches, specifically quantum kernel methods, for the application to robust AD. We build upon previous work on Quantum Support Vector Regression (QSVR) for semisupervised AD by conducting a comprehensive benchmark on IBM quantum hardware using eleven datasets. Our results demonstrate that QSVR achieves strong classification performance and even outperforms the noiseless simulation on two of these datasets. Moreover, we investigate the influence of - in the NISQ-era inevitable - quantum noise on the performance of the QSVR. Our findings reveal that the model exhibits robustness to depolarizing, phase damping, phase flip, and bit flip noise, while amplitude damping and miscalibration noise prove to be more disruptive. Finally, we explore the domain of Quantum Adversarial Machine Learning and demonstrate that QSVR is highly vulnerable to adversarial attacks and that noise does not improve the adversarial robustness of the model.","authors":["Kilian Tscharke","Maximilian Wendlinger","Sebastian Issel","Pascal Debus"],"url":"https://arxiv.org/abs/2505.01012"}
