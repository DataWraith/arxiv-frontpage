{"created":"2025-04-28","title":"My Precious Crash Data: Barriers and Opportunities in Encouraging Autonomous Driving Companies to Share Safety-Critical Data","abstract":"Safety-critical data, such as crash and near-crash records, are crucial to improving autonomous vehicle (AV) design and development. Sharing such data across AV companies, academic researchers, regulators, and the public can help make all AVs safer. However, AV companies rarely share safety-critical data externally. This paper aims to pinpoint why AV companies are reluctant to share safety-critical data, with an eye on how these barriers can inform new approaches to promote sharing. We interviewed twelve AV company employees who actively work with such data in their day-to-day work. Findings suggest two key, previously unknown barriers to data sharing: (1) Datasets inherently embed salient knowledge that is key to improving AV safety and are resource-intensive. Therefore, data sharing, even within a company, is fraught with politics. (2) Interviewees believed AV safety knowledge is private knowledge that brings competitive edges to their companies, rather than public knowledge for social good. We discuss the implications of these findings for incentivizing and enabling safety-critical AV data sharing, specifically, implications for new approaches to (1) debating and stratifying public and private AV safety knowledge, (2) innovating data tools and data sharing pipelines that enable easier sharing of public AV safety data and knowledge; (3) offsetting costs of curating safety-critical data and incentivizing data sharing.","authors":["Hauke Sandhaus","Angel Hsing-Chi Hwang","Wendy Ju","Qian Yang"],"url":"https://arxiv.org/abs/2504.17792"}
{"created":"2025-04-28","title":"Near-Driven Autonomous Rover Navigation in Complex Environments: Extensions to Urban Search-and-Rescue and Industrial Inspection","abstract":"This paper explores the use of an extended neuroevolutionary approach, based on NeuroEvolution of Augmenting Topologies (NEAT), for autonomous robots in dynamic environments associated with hazardous tasks like firefighting, urban search-and-rescue (USAR), and industrial inspections. Building on previous research, it expands the simulation environment to larger and more complex settings, demonstrating NEAT's adaptability across different applications. By integrating recent advancements in NEAT and reinforcement learning, the study uses modern simulation frameworks for realism and hybrid algorithms for optimization. Experimental results show that NEAT-evolved controllers achieve success rates comparable to state-of-the-art deep reinforcement learning methods, with superior structural adaptability. The agents reached ~80% success in outdoor tests, surpassing baseline models. The paper also highlights the benefits of transfer learning among tasks and evaluates the effectiveness of NEAT in complex 3D navigation. Contributions include evaluating NEAT for diverse autonomous applications and discussing real-world deployment considerations, emphasizing the approach's potential as an alternative or complement to deep reinforcement learning in autonomous navigation tasks.","authors":["Dhadkan Shrestha","Lincoln Bhattarai"],"url":"https://arxiv.org/abs/2504.17794"}
{"created":"2025-04-28","title":"Fuzzy Based Secure Clustering Schemes for Wireless Sensor Networks","abstract":"This dissertation presents three independent novel approaches for distinct scenarios to solve one or more open challenges. The first concern explains the focus on the lifetime of the networks: this dissertation will utilize a fuzzy logic-based clustering protocol with multi-hop transmission for load balancing, energy consumption minimization, and network lifetime prolongation. The protocol forms unequal clusters with cluster head (CH) being selected by fuzzy logic with competition radius. Node distance to the base station, concentration, and residual energy are input variables. The second concern focuses on network stability: we design a type 2 fuzzy logic-based clustering schemes in a multi-hop WSN to reduce energy consumption and improve network scalability. In this clustering scheme, we propose a cluster head (CH) selection strategy where a sensor node is elected as a CH based on type 2 fuzzy logic inputs. To balance the load of CHs we also select their radius size based on the fuzzy logic inputs. Finally, the third concern is focus on the utility of game theory in defensive Wireless Sensor Networks (WSN) from selfish nodes and malicious behavior. Game theory can effectively model WSNs malicious attacks because of their low complexity and scalability. The study, thus, explores different WSN defense strategies from both external attackers and internal nodes acting selfishly or maliciously using the game theory approach. Also, the chapter highlights the general trust model for decision-making using the game theory framework. Besides, the chapter demonstrates the significance of the theory in ensuring WSN security from acute attacks and its role in enhancing trustworthiness in data and cooperation of nodes in various WSN architectures.","authors":["Mohd Adnan"],"url":"https://arxiv.org/abs/2504.17795"}
{"created":"2025-04-28","title":"Structural Resilience Analysis of an Internet Fragment Against Targeted and Random Attacks -- A Case Study Based on iThena Project Data","abstract":"This article presents an analysis of the structural resilience of a fragment of Internet topology against both targeted and random attacks, based on empirical data obtained from the iThena project. Using a processed visualization of the network, a graph representing node connections was generated and subsequently subjected to detailed analysis using centrality metrics and community detection algorithms. Two attack scenarios were carried out: removal of nodes with the highest betweenness centrality and random removal of an equivalent number of nodes. The results indicate that targeted attacks have a significantly more destructive impact on the cohesion and functionality of the network than random disruptions. The article highlights the importance of identifying critical nodes and developing monitoring and protection mechanisms for Internet infrastructure in the context of cybersecurity.","authors":["Lukasz Swierczewski"],"url":"https://arxiv.org/abs/2504.17796"}
{"created":"2025-04-28","title":"Subfunction Structure Matters: A New Perspective on Local Optima Networks","abstract":"Local optima networks (LONs) capture fitness landscape information. They are typically constructed in a black-box manner; information about the problem structure is not utilised. This also applies to the analysis of LONs: knowledge about the problem, such as interaction between variables, is not considered. We challenge this status-quo with an alternative approach: we consider how LON analysis can be improved by incorporating subfunction-based information - this can either be known a-priori or learned during search. To this end, LONs are constructed for several benchmark pseudo-boolean problems using three approaches: firstly, the standard algorithm; a second algorithm which uses deterministic grey-box crossover; and a third algorithm which selects perturbations based on learned information about variable interactions. Metrics related to subfunction changes in a LON are proposed and compared with metrics from previous literature which capture other aspects of a LON. Incorporating problem structure in LON construction and analysing it can bring enriched insight into optimisation dynamics. Such information may be crucial to understanding the difficulty of solving a given problem with state-of-the-art linkage learning optimisers. In light of the results, we suggest incorporation of problem structure as an alternative paradigm in landscape analysis for problems with known or suspected subfunction structure.","authors":["S. L. Thomson","M. W. Przewozniczek"],"url":"https://arxiv.org/abs/2504.17799"}
{"created":"2025-04-28","title":"Evolution of Optimization Algorithms for Global Placement via Large Language Models","abstract":"Optimization algorithms are widely employed to tackle complex problems, but designing them manually is often labor-intensive and requires significant expertise. Global placement is a fundamental step in electronic design automation (EDA). While analytical approaches represent the state-of-the-art (SOTA) in global placement, their core optimization algorithms remain heavily dependent on heuristics and customized components, such as initialization strategies, preconditioning methods, and line search techniques. This paper presents an automated framework that leverages large language models (LLM) to evolve optimization algorithms for global placement. We first generate diverse candidate algorithms using LLM through carefully crafted prompts. Then we introduce an LLM-based genetic flow to evolve selected candidate algorithms. The discovered optimization algorithms exhibit substantial performance improvements across many benchmarks. Specifically, Our design-case-specific discovered algorithms achieve average HPWL improvements of \\textbf{5.05\\%}, \\text{5.29\\%} and \\textbf{8.30\\%} on MMS, ISPD2005 and ISPD2019 benchmarks, and up to \\textbf{17\\%} improvements on individual cases. Additionally, the discovered algorithms demonstrate good generalization ability and are complementary to existing parameter-tuning methods.","authors":["Xufeng Yao","Jiaxi Jiang","Yuxuan Zhao","Peiyu Liao","Yibo Lin","Bei Yu"],"url":"https://arxiv.org/abs/2504.17801"}
{"created":"2025-04-28","title":"Spectral Dictionary Learning for Generative Image Modeling","abstract":"We propose a novel spectral generative model for image synthesis that departs radically from the common variational, adversarial, and diffusion paradigms. In our approach, images, after being flattened into one-dimensional signals, are reconstructed as linear combinations of a set of learned spectral basis functions, where each basis is explicitly parameterized in terms of frequency, phase, and amplitude. The model jointly learns a global spectral dictionary with time-varying modulations and per-image mixing coefficients that quantify the contributions of each spectral component. Subsequently, a simple probabilistic model is fitted to these mixing coefficients, enabling the deterministic generation of new images by sampling from the latent space. This framework leverages deterministic dictionary learning, offering a highly interpretable and physically meaningful representation compared to methods relying on stochastic inference or adversarial training. Moreover, the incorporation of frequency-domain loss functions, computed via the short-time Fourier transform (STFT), ensures that the synthesized images capture both global structure and fine-grained spectral details, such as texture and edge information. Experimental evaluations on the CIFAR-10 benchmark demonstrate that our approach not only achieves competitive performance in terms of reconstruction quality and perceptual fidelity but also offers improved training stability and computational efficiency. This new type of generative model opens up promising avenues for controlled synthesis, as the learned spectral dictionary affords a direct handle on the intrinsic frequency content of the images, thus providing enhanced interpretability and potential for novel applications in image manipulation and analysis.","authors":["Andrew Kiruluta"],"url":"https://arxiv.org/abs/2504.17804"}
{"created":"2025-04-28","title":"Fuzzy Logic -- Based Scheduling System for Part-Time Workforce","abstract":"This paper explores the application of genetic fuzzy systems to efficiently generate schedules for a team of part-time student workers at a university. Given the preferred number of working hours and availability of employees, our model generates feasible solutions considering various factors, such as maximum weekly hours, required number of workers on duty, and the preferred number of working hours. The algorithm is trained and tested with availability data collected from students at the University of Cincinnati. The results demonstrate the algorithm's efficiency in producing schedules that meet operational criteria and its robustness in understaffed conditions.","authors":["Tri Nguyen","Kelly Cohen"],"url":"https://arxiv.org/abs/2504.17805"}
{"created":"2025-04-28","title":"Research on Cloud Platform Network Traffic Monitoring and Anomaly Detection System based on Large Language Models","abstract":"The rapidly evolving cloud platforms and the escalating complexity of network traffic demand proper network traffic monitoring and anomaly detection to ensure network security and performance. This paper introduces a large language model (LLM)-based network traffic monitoring and anomaly detection system. In addition to existing models such as autoencoders and decision trees, we harness the power of large language models for processing sequence data from network traffic, which allows us a better capture of underlying complex patterns, as well as slight fluctuations in the dataset. We show for a given detection task, the need for a hybrid model that incorporates the attention mechanism of the transformer architecture into a supervised learning framework in order to achieve better accuracy. A pre-trained large language model analyzes and predicts the probable network traffic, and an anomaly detection layer that considers temporality and context is added. Moreover, we present a novel transfer learning-based methodology to enhance the model's effectiveness to quickly adapt to unknown network structures and adversarial conditions without requiring extensive labeled datasets. Actual results show that the designed model outperforms traditional methods in detection accuracy and computational efficiency, effectively identify various network anomalies such as zero-day attacks and traffic congestion pattern, and significantly reduce the false positive rate.","authors":["Ze Yang","Yihong Jin","Juntian Liu","Xinhe Xu","Yihan Zhang","Shuyang Ji"],"url":"https://arxiv.org/abs/2504.17807"}
{"created":"2025-04-28","title":"Monero Peer-to-peer Network Topology Analysis","abstract":"Monero, a privacy-focused cryptocurrency, employs a decentralized peer-to-peer (P2P) network that plays a critical role in transaction propagation and consensus formation. While much research has explored Monero's privacy transaction mechanisms, its underlying P2P network architecture has remained relatively underexplored. In this study, building on our recent work on Monero network detection, we further investigate the network topology of Monero's P2P structure, which has evolved following recent protocol updates that enhanced security by obscuring peer information. Using k-core decomposition, we confirm that the Monero network exhibits a core-periphery structure, where a tightly interconnected core of supernodes is crucial for maintaining network cohesion, while peripheral nodes rely on these core nodes for connectivity. This structure explains why targeting central nodes does not easily lead to the rapid disintegration of the network's largest connected component while also providing a deeper understanding of the true architecture of Monero's peer protocol.","authors":["Yu Gao","Yu Zhang","Matija Pi\\v{s}korec","Claudio J. Tessone"],"url":"https://arxiv.org/abs/2504.17809"}
{"created":"2025-04-28","title":"SmallGS: Gaussian Splatting-based Camera Pose Estimation for Small-Baseline Videos","abstract":"Dynamic videos with small baseline motions are ubiquitous in daily life, especially on social media. However, these videos present a challenge to existing pose estimation frameworks due to ambiguous features, drift accumulation, and insufficient triangulation constraints. Gaussian splatting, which maintains an explicit representation for scenes, provides a reliable novel view rasterization when the viewpoint change is small. Inspired by this, we propose SmallGS, a camera pose estimation framework that is specifically designed for small-baseline videos. SmallGS optimizes sequential camera poses using Gaussian splatting, which reconstructs the scene from the first frame in each video segment to provide a stable reference for the rest. The temporal consistency of Gaussian splatting within limited viewpoint differences reduced the requirement of sufficient depth variations in traditional camera pose estimation. We further incorporate pretrained robust visual features, e.g. DINOv2, into Gaussian splatting, where high-dimensional feature map rendering enhances the robustness of camera pose estimation. By freezing the Gaussian splatting and optimizing camera viewpoints based on rasterized features, SmallGS effectively learns camera poses without requiring explicit feature correspondences or strong parallax motion. We verify the effectiveness of SmallGS in small-baseline videos in TUM-Dynamics sequences, which achieves impressive accuracy in camera pose estimation compared to MonST3R and DORID-SLAM for small-baseline videos in dynamic scenes. Our project page is at: https://yuxinyao620.github.io/SmallGS","authors":["Yuxin Yao","Yan Zhang","Zhening Huang","Joan Lasenby"],"url":"https://arxiv.org/abs/2504.17810"}
{"created":"2025-04-28","title":"OmniSage: Large Scale, Multi-Entity Heterogeneous Graph Representation Learning","abstract":"Representation learning, a task of learning latent vectors to represent entities, is a key task in improving search and recommender systems in web applications. Various representation learning methods have been developed, including graph-based approaches for relationships among entities, sequence-based methods for capturing the temporal evolution of user activities, and content-based models for leveraging text and visual content. However, the development of a unifying framework that integrates these diverse techniques to support multiple applications remains a significant challenge. This paper presents OmniSage, a large-scale representation framework that learns universal representations for a variety of applications at Pinterest. OmniSage integrates graph neural networks with content-based models and user sequence models by employing multiple contrastive learning tasks to effectively process graph data, user sequence data, and content signals. To support the training and inference of OmniSage, we developed an efficient infrastructure capable of supporting Pinterest graphs with billions of nodes. The universal representations generated by OmniSage have significantly enhanced user experiences on Pinterest, leading to an approximate 2.5% increase in sitewide repins (saves) across five applications. This paper highlights the impact of unifying representation learning methods, and we will open source the OmniSage code by the time of publication.","authors":["Anirudhan Badrinath","Alex Yang","Kousik Rajesh","Prabhat Agarwal","Jaewon Yang","Haoyu Chen","Jiajing Xu","Charles Rosenberg"],"url":"https://arxiv.org/abs/2504.17811"}
{"created":"2025-04-28","title":"Object Learning and Robust 3D Reconstruction","abstract":"In this thesis we discuss architectural designs and training methods for a neural network to have the ability of dissecting an image into objects of interest without supervision. The main challenge in 2D unsupervised object segmentation is distinguishing between foreground objects of interest and background. FlowCapsules uses motion as a cue for the objects of interest in 2D scenarios. The last part of this thesis focuses on 3D applications where the goal is detecting and removal of the object of interest from the input images. In these tasks, we leverage the geometric consistency of scenes in 3D to detect the inconsistent dynamic objects. Our transient object masks are then used for designing robust optimization kernels to improve 3D modelling in a casual capture setup. One of our goals in this thesis is to show the merits of unsupervised object based approaches in computer vision. Furthermore, we suggest possible directions for defining objects of interest or foreground objects without requiring supervision. Our hope is to motivate and excite the community into further exploring explicit object representations in image understanding tasks.","authors":["Sara Sabour"],"url":"https://arxiv.org/abs/2504.17812"}
{"created":"2025-04-28","title":"CLOC: Contrastive Learning for Ordinal Classification with Multi-Margin N-pair Loss","abstract":"In ordinal classification, misclassifying neighboring ranks is common, yet the consequences of these errors are not the same. For example, misclassifying benign tumor categories is less consequential, compared to an error at the pre-cancerous to cancerous threshold, which could profoundly influence treatment choices. Despite this, existing ordinal classification methods do not account for the varying importance of these margins, treating all neighboring classes as equally significant. To address this limitation, we propose CLOC, a new margin-based contrastive learning method for ordinal classification that learns an ordered representation based on the optimization of multiple margins with a novel multi-margin n-pair loss (MMNP). CLOC enables flexible decision boundaries across key adjacent categories, facilitating smooth transitions between classes and reducing the risk of overfitting to biases present in the training data. We provide empirical discussion regarding the properties of MMNP and show experimental results on five real-world image datasets (Adience, Historical Colour Image Dating, Knee Osteoarthritis, Indian Diabetic Retinopathy Image, and Breast Carcinoma Subtyping) and one synthetic dataset simulating clinical decision bias. Our results demonstrate that CLOC outperforms existing ordinal classification methods and show the interpretability and controllability of CLOC in learning meaningful, ordered representations that align with clinical and practical needs.","authors":["Dileepa Pitawela","Gustavo Carneiro","Hsiang-Ting Chen"],"url":"https://arxiv.org/abs/2504.17813"}
{"created":"2025-04-28","title":"FIM: Frequency-Aware Multi-View Interest Modeling for Local-Life Service Recommendation","abstract":"People's daily lives involve numerous periodic behaviors, such as eating and traveling. Local-life platforms cater to these recurring needs by providing essential services tied to daily routines. Therefore, users' periodic intentions are reflected in their interactions with the platforms. There are two main challenges in modeling users' periodic behaviors in the local-life service recommendation systems: 1) the diverse demands of users exhibit varying periodicities, which are difficult to distinguish as they are mixed in the behavior sequences; 2) the periodic behaviors of users are subject to dynamic changes due to factors such as holidays and promotional events. Existing methods struggle to distinguish the periodicities of diverse demands and overlook the importance of dynamically capturing changes in users' periodic behaviors. To this end, we employ a Frequency-Aware Multi-View Interest Modeling framework (FIM). Specifically, we propose a multi-view search strategy that decomposes users' demands from different perspectives to separate their various periodic intentions. This allows the model to comprehensively extract their periodic features than category-searched-only methods. Moreover, we propose a frequency-domain perception and evolution module. This module uses the Fourier Transform to convert users' temporal behaviors into the frequency domain, enabling the model to dynamically perceive their periodic features. Extensive offline experiments demonstrate that FIM achieves significant improvements on public and industrial datasets, showing its capability to effectively model users' periodic intentions. Furthermore, the model has been deployed on the Kuaishou local-life service platform. Through online A/B experiments, the transaction volume has been significantly improved.","authors":["Guoquan Wang","Qiang Luo","Weisong Hu","Pengfei Yao","Wencong Zeng","Guorui Zhou","Kun Gai"],"url":"https://arxiv.org/abs/2504.17814"}
{"created":"2025-04-28","title":"Visibility-Uncertainty-guided 3D Gaussian Inpainting via Scene Conceptional Learning","abstract":"3D Gaussian Splatting (3DGS) has emerged as a powerful and efficient 3D representation for novel view synthesis. This paper extends 3DGS capabilities to inpainting, where masked objects in a scene are replaced with new contents that blend seamlessly with the surroundings. Unlike 2D image inpainting, 3D Gaussian inpainting (3DGI) is challenging in effectively leveraging complementary visual and semantic cues from multiple input views, as occluded areas in one view may be visible in others. To address this, we propose a method that measures the visibility uncertainties of 3D points across different input views and uses them to guide 3DGI in utilizing complementary visual cues. We also employ uncertainties to learn a semantic concept of scene without the masked object and use a diffusion model to fill masked objects in input images based on the learned concept. Finally, we build a novel 3DGI framework, VISTA, by integrating VISibility-uncerTainty-guided 3DGI with scene conceptuAl learning. VISTA generates high-quality 3DGS models capable of synthesizing artifact-free and naturally inpainted novel views. Furthermore, our approach extends to handling dynamic distractors arising from temporal object changes, enhancing its versatility in diverse scene reconstruction scenarios. We demonstrate the superior performance of our method over state-of-the-art techniques using two challenging datasets: the SPIn-NeRF dataset, featuring 10 diverse static 3D inpainting scenes, and an underwater 3D inpainting dataset derived from UTB180, including fast-moving fish as inpainting targets.","authors":["Mingxuan Cui","Qing Guo","Yuyi Wang","Hongkai Yu","Di Lin","Qin Zou","Ming-Ming Cheng","Xi Li"],"url":"https://arxiv.org/abs/2504.17815"}
{"created":"2025-04-28","title":"Subject-driven Video Generation via Disentangled Identity and Motion","abstract":"We propose to train a subject-driven customized video generation model through decoupling the subject-specific learning from temporal dynamics in zero-shot without additional tuning. A traditional method for video customization that is tuning-free often relies on large, annotated video datasets, which are computationally expensive and require extensive annotation. In contrast to the previous approach, we introduce the use of an image customization dataset directly on training video customization models, factorizing the video customization into two folds: (1) identity injection through image customization dataset and (2) temporal modeling preservation with a small set of unannotated videos through the image-to-video training method. Additionally, we employ random image token dropping with randomized image initialization during image-to-video fine-tuning to mitigate the copy-and-paste issue. To further enhance learning, we introduce stochastic switching during joint optimization of subject-specific and temporal features, mitigating catastrophic forgetting. Our method achieves strong subject consistency and scalability, outperforming existing video customization models in zero-shot settings, demonstrating the effectiveness of our framework.","authors":["Daneul Kim","Jingxu Zhang","Wonjoon Jin","Sunghyun Cho","Qi Dai","Jaesik Park","Chong Luo"],"url":"https://arxiv.org/abs/2504.17816"}
{"created":"2025-04-28","title":"Learning Underwater Active Perception in Simulation","abstract":"When employing underwater vehicles for the autonomous inspection of assets, it is crucial to consider and assess the water conditions. Indeed, they have a significant impact on the visibility, which also affects robotic operations. Turbidity can jeopardise the whole mission as it may prevent correct visual documentation of the inspected structures. Previous works have introduced methods to adapt to turbidity and backscattering, however, they also include manoeuvring and setup constraints. We propose a simple yet efficient approach to enable high-quality image acquisition of assets in a broad range of water conditions. This active perception framework includes a multi-layer perceptron (MLP) trained to predict image quality given a distance to a target and artificial light intensity. We generated a large synthetic dataset including ten water types with different levels of turbidity and backscattering. For this, we modified the modelling software Blender to better account for the underwater light propagation properties. We validated the approach in simulation and showed significant improvements in visual coverage and quality of imagery compared to traditional approaches. The project code is available on our project page at https://roboticimaging.org/Projects/ActiveUW/.","authors":["Alexandre Cardaillac","Donald G. Dansereau"],"url":"https://arxiv.org/abs/2504.17817"}
{"created":"2025-04-28","title":"Fast Multichannel Topology Discovery in Cognitive Radio Networks","abstract":"In Cognitive Radio Networks (CRNs), secondary users (SUs) must efficiently discover each other across multiple communication channels while avoiding interference from primary users (PUs). Traditional multichannel rendezvous algorithms primarily focus on enabling pairs of SUs to find common channels without explicitly considering the underlying network topology. In this paper, we extend the rendezvous framework to explicitly incorporate network topology, introducing the \\emph{multichannel topology discovery problem}. We propose a novel \\emph{pseudo-random sweep algorithm with forward replacement}, designed to minimize correlation between consecutive unsuccessful rendezvous attempts, thereby significantly reducing the expected time-to-discovery (ETTD). Additionally, we introduce a \\emph{threshold-based stick-together strategy} that dynamically synchronizes user hopping sequences based on partially known information, further enhancing discovery efficiency. Extensive simulation results validate our theoretical analysis, demonstrating that the proposed algorithms substantially outperform conventional (sequential) sweep methods.","authors":["Yung-Li Wang","Yiwei Liu","Cheng-Shang Chang"],"url":"https://arxiv.org/abs/2504.17818"}
{"created":"2025-04-28","title":"How fake news can turn against its spreader","abstract":"When different information sources on a given topic are combined, they interact in a nontrivial manner for a rational receiver of these information sources. Suppose that there are two information sources, one is genuine and the other contains disinformation. It is shown that under the conditions that the signal-to-noise ratio of the genuine information source is sufficiently large, and that the noise terms in the two information sources are positively correlated, the effect of disinformation is reversed from its original intent. That is, the effect of disinformation on a receiver of both information sources, who is unaware of the existence of disinformation, is to generate an opposite interpretation. While the condition in which this phenomenon occurs cannot always be ensured, when it is satisfied, the effect provides an effective way of countering the impacts of disinformation.","authors":["Dorje C. Brody","Tomooki Yuasa"],"url":"https://arxiv.org/abs/2504.17820"}
{"created":"2025-04-28","title":"VideoVista-CulturalLingo: 360$^\\circ$ Horizons-Bridging Cultures, Languages, and Domains in Video Comprehension","abstract":"Assessing the video comprehension capabilities of multimodal AI systems can effectively measure their understanding and reasoning abilities. Most video evaluation benchmarks are limited to a single language, typically English, and predominantly feature videos rooted in Western cultural contexts. In this paper, we present VideoVista-CulturalLingo, the first video evaluation benchmark designed to bridge cultural, linguistic, and domain divide in video comprehension. Our work differs from existing benchmarks in the following ways: 1) Cultural diversity, incorporating cultures from China, North America, and Europe; 2) Multi-linguistics, with questions presented in Chinese and English-two of the most widely spoken languages; and 3) Broad domain, featuring videos sourced from hundreds of human-created domains. VideoVista-CulturalLingo contains 1,389 videos and 3,134 QA pairs, and we have evaluated 24 recent open-source or proprietary video large models. From the experiment results, we observe that: 1) Existing models perform worse on Chinese-centric questions than Western-centric ones, particularly those related to Chinese history; 2) Current open-source models still exhibit limitations in temporal understanding, especially in the Event Localization task, achieving a maximum score of only 45.2%; 3) Mainstream models demonstrate strong performance in general scientific questions, while open-source models demonstrate weak performance in mathematics.","authors":["Xinyu Chen","Yunxin Li","Haoyuan Shi","Baotian Hu","Wenhan Luo","Yaowei Wang","Min Zhang"],"url":"https://arxiv.org/abs/2504.17821"}
{"created":"2025-04-28","title":"A multi-scale vision transformer-based multimodal GeoAI model for mapping Arctic permafrost thaw","abstract":"Retrogressive Thaw Slumps (RTS) in Arctic regions are distinct permafrost landforms with significant environmental impacts. Mapping these RTS is crucial because their appearance serves as a clear indication of permafrost thaw. However, their small scale compared to other landform features, vague boundaries, and spatiotemporal variation pose significant challenges for accurate detection. In this paper, we employed a state-of-the-art deep learning model, the Cascade Mask R-CNN with a multi-scale vision transformer-based backbone, to delineate RTS features across the Arctic. Two new strategies were introduced to optimize multimodal learning and enhance the model's predictive performance: (1) a feature-level, residual cross-modality attention fusion strategy, which effectively integrates feature maps from multiple modalities to capture complementary information and improve the model's ability to understand complex patterns and relationships within the data; (2) pre-trained unimodal learning followed by multimodal fine-tuning to alleviate high computing demand while achieving strong model performance. Experimental results demonstrated that our approach outperformed existing models adopting data-level fusion, feature-level convolutional fusion, and various attention fusion strategies, providing valuable insights into the efficient utilization of multimodal data for RTS mapping. This research contributes to our understanding of permafrost landforms and their environmental implications.","authors":["Wenwen Li","Chia-Yu Hsu","Sizhe Wang","Zhining Gu","Yili Yang","Brendan M. Rogers","Anna Liljedahl"],"url":"https://arxiv.org/abs/2504.17822"}
{"created":"2025-04-28","title":"The Cloud Weaving Model for AI development","abstract":"While analysing challenges in pilot projects developing AI with marginalized communities, we found it difficult to express them within commonly used paradigms. We therefore constructed an alternative conceptual framework to ground AI development in the social fabric -- the Cloud Weaving Model -- inspired (amongst others) by indigenous knowledge, motifs from nature, and Eastern traditions. This paper introduces and elaborates on the fundamental elements of the model (clouds, spiders, threads, spiderwebs, and weather) and their interpretation in an AI context. The framework is then applied to comprehend patterns observed in co-creation pilots approaching marginalized communities, highlighting neglected yet relevant dimensions for responsible AI development.","authors":["Darcy Kim","Aida Kalender","Sennay Ghebreab","Giovanni Sileno"],"url":"https://arxiv.org/abs/2504.17823"}
{"created":"2025-04-28","title":"EduBot -- Can LLMs Solve Personalized Learning and Programming Assignments?","abstract":"The prevalence of Large Language Models (LLMs) is revolutionizing the process of writing code. General and code LLMs have shown impressive performance in generating standalone functions and code-completion tasks with one-shot queries. However, the ability to solve comprehensive programming tasks with recursive requests and bug fixes remains questionable. In this paper, we propose EduBot, an intelligent automated assistant system that combines conceptual knowledge teaching, end-to-end code development, personalized programming through recursive prompt-driven methods, and debugging with limited human interventions powered by LLMs. We show that EduBot can solve complicated programming tasks consisting of sub-tasks with increasing difficulties ranging from conceptual to coding questions by recursive automatic prompt-driven systems without finetuning on LLMs themselves. To further evaluate EduBot's performance, we design and conduct a benchmark suite consisting of 20 scenarios in algorithms, machine learning, and real-world problems. The result shows that EduBot can complete most scenarios in less than 20 minutes. Based on the benchmark suites, we perform a comparative study to take different LLMs as the backbone and to verify EduBot's compatibility and robustness across LLMs with varying capabilities. We believe that EduBot is an exploratory approach to explore the potential of pre-trained LLMs in multi-step reasoning and code generation for solving personalized assignments with knowledge learning and code generation.","authors":["Yibin Wang","Jiaxi Xie","Lakshminarayanan Subramanian"],"url":"https://arxiv.org/abs/2504.17824"}
{"created":"2025-04-28","title":"Dual Prompting Image Restoration with Diffusion Transformers","abstract":"Recent state-of-the-art image restoration methods mostly adopt latent diffusion models with U-Net backbones, yet still facing challenges in achieving high-quality restoration due to their limited capabilities. Diffusion transformers (DiTs), like SD3, are emerging as a promising alternative because of their better quality with scalability. In this paper, we introduce DPIR (Dual Prompting Image Restoration), a novel image restoration method that effectivly extracts conditional information of low-quality images from multiple perspectives. Specifically, DPIR consits of two branches: a low-quality image conditioning branch and a dual prompting control branch. The first branch utilizes a lightweight module to incorporate image priors into the DiT with high efficiency. More importantly, we believe that in image restoration, textual description alone cannot fully capture its rich visual characteristics. Therefore, a dual prompting module is designed to provide DiT with additional visual cues, capturing both global context and local appearance. The extracted global-local visual prompts as extra conditional control, alongside textual prompts to form dual prompts, greatly enhance the quality of the restoration. Extensive experimental results demonstrate that DPIR delivers superior image restoration performance.","authors":["Dehong Kong","Fan Li","Zhixin Wang","Jiaqi Xu","Renjing Pei","Wenbo Li","WenQi Ren"],"url":"https://arxiv.org/abs/2504.17825"}
{"created":"2025-04-28","title":"FashionM3: Multimodal, Multitask, and Multiround Fashion Assistant based on Unified Vision-Language Model","abstract":"Fashion styling and personalized recommendations are pivotal in modern retail, contributing substantial economic value in the fashion industry. With the advent of vision-language models (VLM), new opportunities have emerged to enhance retailing through natural language and visual interactions. This work proposes FashionM3, a multimodal, multitask, and multiround fashion assistant, built upon a VLM fine-tuned for fashion-specific tasks. It helps users discover satisfying outfits by offering multiple capabilities including personalized recommendation, alternative suggestion, product image generation, and virtual try-on simulation. Fine-tuned on the novel FashionRec dataset, comprising 331,124 multimodal dialogue samples across basic, personalized, and alternative recommendation tasks, FashionM3 delivers contextually personalized suggestions with iterative refinement through multiround interactions. Quantitative and qualitative evaluations, alongside user studies, demonstrate FashionM3's superior performance in recommendation effectiveness and practical value as a fashion assistant.","authors":["Kaicheng Pang","Xingxing Zou","Waikeung Wong"],"url":"https://arxiv.org/abs/2504.17826"}
{"created":"2025-04-28","title":"Evolution Meets Diffusion: Efficient Neural Architecture Generation","abstract":"Neural Architecture Search (NAS) has gained widespread attention for its transformative potential in deep learning model design. However, the vast and complex search space of NAS leads to significant computational and time costs. Neural Architecture Generation (NAG) addresses this by reframing NAS as a generation problem, enabling the precise generation of optimal architectures for specific tasks. Despite its promise, mainstream methods like diffusion models face limitations in global search capabilities and are still hindered by high computational and time demands. To overcome these challenges, we propose Evolutionary Diffusion-based Neural Architecture Generation (EDNAG), a novel approach that achieves efficient and training-free architecture generation. EDNAG leverages evolutionary algorithms to simulate the denoising process in diffusion models, using fitness to guide the transition from random Gaussian distributions to optimal architecture distributions. This approach combines the strengths of evolutionary strategies and diffusion models, enabling rapid and effective architecture generation. Extensive experiments demonstrate that EDNAG achieves state-of-the-art (SOTA) performance in architecture optimization, with an improvement in accuracy of up to 10.45%. Furthermore, it eliminates the need for time-consuming training and boosts inference speed by an average of 50 times, showcasing its exceptional efficiency and effectiveness.","authors":["Bingye Zhou","Caiyang Yu"],"url":"https://arxiv.org/abs/2504.17827"}
{"created":"2025-04-28","title":"VEU-Bench: Towards Comprehensive Understanding of Video Editing","abstract":"Widely shared videos on the internet are often edited. Recently, although Video Large Language Models (Vid-LLMs) have made great progress in general video understanding tasks, their capabilities in video editing understanding (VEU) tasks remain unexplored. To address this gap, in this paper, we introduce VEU-Bench (Video Editing Understanding Benchmark), a comprehensive benchmark that categorizes video editing components across various dimensions, from intra-frame features like shot size to inter-shot attributes such as cut types and transitions. Unlike previous video editing understanding benchmarks that focus mainly on editing element classification, VEU-Bench encompasses 19 fine-grained tasks across three stages: recognition, reasoning, and judging. To enhance the annotation of VEU automatically, we built an annotation pipeline integrated with an ontology-based knowledge base. Through extensive experiments with 11 state-of-the-art Vid-LLMs, our findings reveal that current Vid-LLMs face significant challenges in VEU tasks, with some performing worse than random choice. To alleviate this issue, we develop Oscars, a VEU expert model fine-tuned on the curated VEU-Bench dataset. It outperforms existing open-source Vid-LLMs on VEU-Bench by over 28.3% in accuracy and achieves performance comparable to commercial models like GPT-4o. We also demonstrate that incorporating VEU data significantly enhances the performance of Vid-LLMs on general video understanding benchmarks, with an average improvement of 8.3% across nine reasoning tasks.","authors":["Bozheng Li","Yongliang Wu","Yi Lu","Jiashuo Yu","Licheng Tang","Jiawang Cao","Wenqing Zhu","Yuyang Sun","Jay Wu","Wenbo Zhu"],"url":"https://arxiv.org/abs/2504.17828"}
{"created":"2025-04-28","title":"Fine-Tuning Adversarially-Robust Transformers for Single-Image Dehazing","abstract":"Single-image dehazing is an important topic in remote sensing applications, enhancing the quality of acquired images and increasing object detection precision. However, the reliability of such structures has not been sufficiently analyzed, which poses them to the risk of imperceptible perturbations that can significantly hinder their performance. In this work, we show that state-of-the-art image-to-image dehazing transformers are susceptible to adversarial noise, with even 1 pixel change being able to decrease the PSNR by as much as 2.8 dB. Next, we propose two lightweight fine-tuning strategies aimed at increasing the robustness of pre-trained transformers. Our methods results in comparable clean performance, while significantly increasing the protection against adversarial data. We further present their applicability in two remote sensing scenarios, showcasing their robust behavior for out-of-distribution data. The source code for adversarial fine-tuning and attack algorithms can be found at github.com/Vladimirescu/RobustDehazing.","authors":["Vlad Vasilescu","Ana Neacsu","Daniela Faur"],"url":"https://arxiv.org/abs/2504.17829"}
{"created":"2025-04-28","title":"The Role of Open-Source LLMs in Shaping the Future of GeoAI","abstract":"Large Language Models (LLMs) are transforming geospatial artificial intelligence (GeoAI), offering new capabilities in data processing, spatial analysis, and decision support. This paper examines the open-source paradigm's pivotal role in this transformation. While proprietary LLMs offer accessibility, they often limit the customization, interoperability, and transparency vital for specialized geospatial tasks. Conversely, open-source alternatives significantly advance Geographic Information Science (GIScience) by fostering greater adaptability, reproducibility, and community-driven innovation. Open frameworks empower researchers to tailor solutions, integrate cutting-edge methodologies (e.g., reinforcement learning, advanced spatial indexing), and align with FAIR principles. However, the growing reliance on any LLM necessitates careful consideration of security vulnerabilities, ethical risks, and robust governance for AI-generated geospatial outputs. Ongoing debates on accessibility, regulation, and misuse underscore the critical need for responsible AI development strategies. This paper argues that GIScience advances best not through a single model type, but by cultivating a diverse, interoperable ecosystem combining open-source foundations for innovation, bespoke geospatial models, and interdisciplinary collaboration. By critically evaluating the opportunities and challenges of open-source LLMs within the broader GeoAI landscape, this work contributes to a nuanced discourse on leveraging AI to effectively advance spatial research, policy, and decision-making in an equitable, sustainable, and scientifically rigorous manner.","authors":["Xiao Huang","Zhengzhong Tu","Xinyue Ye","Michael Goodchild"],"url":"https://arxiv.org/abs/2504.17833"}
{"created":"2025-04-28","title":"Unveiling the Hidden: Movie Genre and User Bias in Spoiler Detection","abstract":"Spoilers in movie reviews are important on platforms like IMDb and Rotten Tomatoes, offering benefits and drawbacks. They can guide some viewers' choices but also affect those who prefer no plot details in advance, making effective spoiler detection essential. Existing spoiler detection methods mainly analyze review text, often overlooking the impact of movie genres and user bias, limiting their effectiveness. To address this, we analyze movie review data, finding genre-specific variations in spoiler rates and identifying that certain users are more likely to post spoilers. Based on these findings, we introduce a new spoiler detection framework called GUSD (The code is available at https://github.com/AI-explorer-123/GUSD) (Genre-aware and User-specific Spoiler Detection), which incorporates genre-specific data and user behavior bias. User bias is calculated through dynamic graph modeling of review history. Additionally, the R2GFormer module combines RetGAT (Retentive Graph Attention Network) for graph information and GenreFormer for genre-specific aggregation. The GMoE (Genre-Aware Mixture of Experts) model further assigns reviews to specialized experts based on genre. Extensive testing on benchmark datasets shows that GUSD achieves state-of-the-art results. This approach advances spoiler detection by addressing genre and user-specific patterns, enhancing user experience on movie review platforms.","authors":["Haokai Zhang","Shengtao Zhang","Zijian Cai","Heng Wang","Ruixuan Zhu","Zinan Zeng","Minnan Luo"],"url":"https://arxiv.org/abs/2504.17834"}
{"created":"2025-04-28","title":"CaRL: Learning Scalable Planning Policies with Simple Rewards","abstract":"We investigate reinforcement learning (RL) for privileged planning in autonomous driving. State-of-the-art approaches for this task are rule-based, but these methods do not scale to the long tail. RL, on the other hand, is scalable and does not suffer from compounding errors like imitation learning. Contemporary RL approaches for driving use complex shaped rewards that sum multiple individual rewards, \\eg~progress, position, or orientation rewards. We show that PPO fails to optimize a popular version of these rewards when the mini-batch size is increased, which limits the scalability of these approaches. Instead, we propose a new reward design based primarily on optimizing a single intuitive reward term: route completion. Infractions are penalized by terminating the episode or multiplicatively reducing route completion. We find that PPO scales well with higher mini-batch sizes when trained with our simple reward, even improving performance. Training with large mini-batch sizes enables efficient scaling via distributed data parallelism. We scale PPO to 300M samples in CARLA and 500M samples in nuPlan with a single 8-GPU node. The resulting model achieves 64 DS on the CARLA longest6 v2 benchmark, outperforming other RL methods with more complex rewards by a large margin. Requiring only minimal adaptations from its use in CARLA, the same method is the best learning-based approach on nuPlan. It scores 91.3 in non-reactive and 90.6 in reactive traffic on the Val14 benchmark while being an order of magnitude faster than prior work.","authors":["Bernhard Jaeger","Daniel Dauner","Jens Bei{\\ss}wenger","Simon Gerstenecker","Kashyap Chitta","Andreas Geiger"],"url":"https://arxiv.org/abs/2504.17838"}
{"created":"2025-04-28","title":"High-Performance Reinforcement Learning on Spot: Optimizing Simulation Parameters with Distributional Measures","abstract":"This work presents an overview of the technical details behind a high performance reinforcement learning policy deployment with the Spot RL Researcher Development Kit for low level motor access on Boston Dynamics Spot. This represents the first public demonstration of an end to end end reinforcement learning policy deployed on Spot hardware with training code publicly available through Nvidia IsaacLab and deployment code available through Boston Dynamics. We utilize Wasserstein Distance and Maximum Mean Discrepancy to quantify the distributional dissimilarity of data collected on hardware and in simulation to measure our sim2real gap. We use these measures as a scoring function for the Covariance Matrix Adaptation Evolution Strategy to optimize simulated parameters that are unknown or difficult to measure from Spot. Our procedure for modeling and training produces high quality reinforcement learning policies capable of multiple gaits, including a flight phase. We deploy policies capable of over 5.2ms locomotion, more than triple Spots default controller maximum speed, robustness to slippery surfaces, disturbance rejection, and overall agility previously unseen on Spot. We detail our method and release our code to support future work on Spot with the low level API.","authors":["A. J Miller","Fangzhou Yu","Michael Brauckmann","Farbod Farshidian"],"url":"https://arxiv.org/abs/2504.17857"}
{"created":"2025-04-28","title":"Efficient iterative techniques for solving tensor problems with the T-product","abstract":"This paper presents iterative methods for solving tensor equations involving the T-product. The proposed approaches apply tensor computations without matrix construction. For each initial tensor, these algorithms solve related problems in a finite number of iterations, with negligible errors. The theoretical analysis is validated by numerical examples that demonstrate the practicality and effectiveness of these algorithms.","authors":["Malihe Nobakht Kooshkghazi","Salman Ahmadi-Asl","Hamidreza Afshin"],"url":"https://arxiv.org/abs/2504.17861"}
{"created":"2025-04-28","title":"Geodetic Set on Graphs of Constant Pathwidth and Feedback Vertex Set Number","abstract":"In the \\textsc{Geodetic Set} problem, the input consists of a graph $G$ and a positive integer $k$. The goal is to determine whether there exists a subset $S$ of vertices of size $k$ such that every vertex in the graph is included in a shortest path between two vertices in $S$. Kellerhals and Koana [IPEC 2020; J. Graph Algorithms Appl 2022] proved that the problem is $\\W[1]$-hard when parameterized by the pathwidth and the feedback vertex set number of the input graph. They posed the question of whether the problem admits an $\\XP$ algorithm when parameterized by the combination of these two parameters. We answer this in negative by proving that the problem remains \\NP-hard on graphs of constant pathwidth and feedback vertex set number.","authors":["Prafullkumar Tale"],"url":"https://arxiv.org/abs/2504.17862"}
{"created":"2025-04-28","title":"Set Phasers to Stun: Beaming Power and Control to Mobile Robots with Laser Light","abstract":"We present Phaser, a flexible system that directs narrow-beam laser light to moving robots for concurrent wireless power delivery and communication. We design a semi-automatic calibration procedure to enable fusion of stereo-vision-based 3D robot tracking with high-power beam steering, and a low-power optical communication scheme that reuses the laser light as a data channel. We fabricate a Phaser prototype using off-the-shelf hardware and evaluate its performance with battery-free autonomous robots. Phaser delivers optical power densities of over 110 mW/cm$^2$ and error-free data to mobile robots at multi-meter ranges, with on-board decoding drawing 0.3 mA (97\\% less current than Bluetooth Low Energy). We demonstrate Phaser fully powering gram-scale battery-free robots to nearly 2x higher speeds than prior work while simultaneously controlling them to navigate around obstacles and along paths. Code, an open-source design guide, and a demonstration video of Phaser is available at https://mobilex.cs.columbia.edu/phaser.","authors":["Charles J. Carver","Hadleigh Schwartz","Toma Itagaki","Zachary Englhardt","Kechen Liu","Megan Graciela Nauli Manik","Chun-Cheng Chang","Vikram Iyer","Brian Plancher","Xia Zhou"],"url":"https://arxiv.org/abs/2504.17865"}
{"created":"2025-04-28","title":"Preserving Distances in Faulty Colored Graphs","abstract":"We study color fault-tolerant (CFT) network design problems: Given an $n$-vertex graph $G$ whose edges are arbitrarily colored (with no ``legality'' restrictions), the goal is to find a sparse subgraph $H$ such that, when any color fault causes all edges of some color $c$ to crash, the surviving subgraph $H-c$ remains ``similar'' to the surviving graph $G-c$. The similarity is problem-dependent, usually pertaining to distance preserving. If each color class has size $\\Delta$ or less, a brute-force approach can disregard the colors and take $H$ to be $\\Delta$-edge fault-tolerant ($\\Delta$-EFT), so that $H-F$ is similar to $G-F$ for every set $F$ of $\\leq \\Delta$ edges. We ask if the colors can be utilized to provide a sparser $H$.","authors":["Merav Parter","Asaf Petruschka"],"url":"https://arxiv.org/abs/2504.17868"}
{"created":"2025-04-28","title":"Flow Matching Ergodic Coverage","abstract":"Ergodic coverage effectively generates exploratory behaviors for embodied agents by aligning the spatial distribution of the agent's trajectory with a target distribution, where the difference between these two distributions is measured by the ergodic metric. However, existing ergodic coverage methods are constrained by the limited set of ergodic metrics available for control synthesis, fundamentally limiting their performance. In this work, we propose an alternative approach to ergodic coverage based on flow matching, a technique widely used in generative inference for efficient and scalable sampling. We formally derive the flow matching problem for ergodic coverage and show that it is equivalent to a linear quadratic regulator problem with a closed-form solution. Our formulation enables alternative ergodic metrics from generative inference that overcome the limitations of existing ones. These metrics were previously infeasible for control synthesis but can now be supported with no computational overhead. Specifically, flow matching with the Stein variational gradient flow enables control synthesis directly over the score function of the target distribution, improving robustness to the unnormalized distributions; on the other hand, flow matching with the Sinkhorn divergence flow enables an optimal transport-based ergodic metric, improving coverage performance on non-smooth distributions with irregular supports. We validate the improved performance and competitive computational efficiency of our method through comprehensive numerical benchmarks and across different nonlinear dynamics. We further demonstrate the practicality of our method through a series of drawing and erasing tasks on a Franka robot.","authors":["Max Muchen Sun","Allison Pinosky","Todd Murphey"],"url":"https://arxiv.org/abs/2504.17872"}
{"created":"2025-04-28","title":"Enabling Deep Visibility into VxWorks-Based Embedded Controllers in Cyber-Physical Systems for Anomaly Detection","abstract":"We propose the DIVER (Defensive Implant for Visibility into Embedded Run-times) framework for real-time deep visibility into embedded control devices in cyber-physical systems (CPSs). DIVER enables run-time detection of anomalies and is targeted at devices running the real-time operating system (RTOS), VxWorks, which precludes traditional methods of implementing dynamic monitors using OS (e.g., Linux, Windows) functions. DIVER has two components. The \"measurer\" implant is embedded into the VxWorks kernel to collect run-time measurements and provide interactive/streaming interfaces over a TCP/IP port. The remote \"listener\" acquires and analyzes the measurements and provides an interactive user interface. DIVER focuses on small embedded devices with stringent resource constraints (e.g., insufficient storage to locally store measurements). We demonstrate efficacy of DIVER on the Motorola ACE Remote Terminal Unit used in CPS including power systems.","authors":["Prashanth Krishnamurthy","Ramesh Karri","Farshad Khorrami"],"url":"https://arxiv.org/abs/2504.17875"}
{"created":"2025-04-28","title":"Crypto-ncRNA: Non-coding RNA (ncRNA) Based Encryption Algorithm","abstract":"In the looming post-quantum era, traditional cryptographic systems are increasingly vulnerable to quantum computing attacks that can compromise their mathematical foundations. To address this critical challenge, we propose crypto-ncRNA-a bio-convergent cryptographic framework that leverages the dynamic folding properties of non-coding RNA (ncRNA) to generate high-entropy, quantum-resistant keys and produce unpredictable ciphertexts. The framework employs a novel, multi-stage process: encoding plaintext into RNA sequences, predicting and manipulating RNA secondary structures using advanced algorithms, and deriving cryptographic keys through the intrinsic physical unclonability of RNA molecules. Experimental evaluations indicate that, although crypto-ncRNA's encryption speed is marginally lower than that of AES, it significantly outperforms RSA in terms of efficiency and scalability while achieving a 100% pass rate on the NIST SP 800-22 randomness tests. These results demonstrate that crypto-ncRNA offers a promising and robust approach for securing digital infrastructures against the evolving threats posed by quantum computing.","authors":["Xu Wang","Yiquan Wang","Tin-yeh Huang"],"url":"https://arxiv.org/abs/2504.17878"}
{"created":"2025-04-28","title":"Autonomous Navigation Of Quadrupeds Using Coverage Path Planning","abstract":"This paper proposes a novel method of coverage path planning for the purpose of scanning an unstructured environment autonomously. The method uses the morphological skeleton of the prior 2D navigation map via SLAM to generate a sequence of points of interest (POIs). This sequence is then ordered to create an optimal path given the robot's current position. To control the high-level operation, a finite state machine is used to switch between two modes: navigating towards a POI using Nav2, and scanning the local surrounding. We validate the method in a leveled indoor obstacle-free non-convex environment on time efficiency and reachability over five trials. The map reader and the path planner can quickly process maps of width and height ranging between [196,225] pixels and [185,231] pixels in 2.52 ms/pixel and 1.7 ms/pixel, respectively, where their computation time increases with 22.0 ns/pixel and 8.17 $\\mu$s/pixel, respectively. The robot managed to reach 86.5\\% of all waypoints over all five runs. The proposed method suffers from drift occurring in the 2D navigation map.","authors":["Alexander James Becoy","Kseniia Khomenko","Luka Peternel","Raj Thilak Rajan"],"url":"https://arxiv.org/abs/2504.17880"}
{"created":"2025-04-28","title":"PowerSensor3: A Fast and Accurate Open Source Power Measurement Tool","abstract":"Power consumption is a major concern in data centers and HPC applications, with GPUs typically accounting for more than half of system power usage. While accurate power measurement tools are crucial for optimizing the energy efficiency of (GPU) applications, both built-in power sensors as well as state-of-the-art power meters often lack the accuracy and temporal granularity needed, or are impractical to use. Released as open hardware, firmware, and software, PowerSensor3 provides a cost-effective solution for evaluating energy efficiency, enabling advancements in sustainable computing. The toolkit consists of a baseboard with a variety of sensor modules accompanied by host libraries with C++ and Python bindings. PowerSensor3 enables real-time power measurements of SoC boards and PCIe cards, including GPUs, FPGAs, NICs, SSDs, and domain-specific AI and ML accelerators. Additionally, it provides significant improvements over previous tools, such as a robust and modular design, current sensors resistant to external interference, simplified calibration, and a sampling rate up to 20 kHz, which is essential to identify GPU behavior at high temporal granularity. This work describes the toolkit design, evaluates its performance characteristics, and shows several use cases (GPUs, NVIDIA Jetson AGX Orin, and SSD), demonstrating PowerSensor3's potential to significantly enhance energy efficiency in modern computing environments.","authors":["Steven van der Vlugt","Leon Oostrum","Gijs Schoonderbeek","Ben van Werkhoven","Bram Veenboer","Krijn Doekemeijer","John W. Romein"],"url":"https://arxiv.org/abs/2504.17883"}
{"created":"2025-04-28","title":"Unsupervised Corpus Poisoning Attacks in Continuous Space for Dense Retrieval","abstract":"This paper concerns corpus poisoning attacks in dense information retrieval, where an adversary attempts to compromise the ranking performance of a search algorithm by injecting a small number of maliciously generated documents into the corpus. Our work addresses two limitations in the current literature. First, attacks that perform adversarial gradient-based word substitution search do so in the discrete lexical space, while retrieval itself happens in the continuous embedding space. We thus propose an optimization method that operates in the embedding space directly. Specifically, we train a perturbation model with the objective of maintaining the geometric distance between the original and adversarial document embeddings, while also maximizing the token-level dissimilarity between the original and adversarial documents. Second, it is common for related work to have a strong assumption that the adversary has prior knowledge about the queries. In this paper, we focus on a more challenging variant of the problem where the adversary assumes no prior knowledge about the query distribution (hence, unsupervised). Our core contribution is an adversarial corpus attack that is fast and effective. We present comprehensive experimental results on both in- and out-of-domain datasets, focusing on two related tasks: a top-1 attack and a corpus poisoning attack. We consider attacks under both a white-box and a black-box setting. Notably, our method can generate successful adversarial examples in under two minutes per target document; four times faster compared to the fastest gradient-based word substitution methods in the literature with the same hardware. Furthermore, our adversarial generation method generates text that is more likely to occur under the distribution of natural text (low perplexity), and is therefore more difficult to detect.","authors":["Yongkang Li","Panagiotis Eustratiadis","Simon Lupart","Evangelos Kanoulas"],"url":"https://arxiv.org/abs/2504.17884"}
{"created":"2025-04-28","title":"Searching in trees with heavy group sets of fixed size","abstract":"We consider the following generalization of the binary search problem: A searcher is required to find a hidden element $x$ in a tree $T$. To do so, they iteratively perform queries to an oracle about a chosen vertex $v$. After each such call, the oracle responds whether the target was found and if not, the searcher receives as a reply the neighbor of $v$ that lays on the shortest path towards $x$. Additionally, each vertex $v$ may have a different query cost $w(v)$. The goal is to find the optimal querying strategy for the searcher which minimizes the worst case query cost required to find $x$. The problem is known to be NP-hard even in restricted classes of trees such as bounded diameter spiders [Cicalese et al. 2016] and no constant factor approximation algorithm is known for the general case. Inspired by recent studies [Dereniowski et al. 2022, Dereniowski et al. 2024], instead of restricted classes of trees, we explore restrictions on the weight function. We introduce the concept of a heavy group set of a vertex $HG(v,w)$. We show that if for every $v\\in T$: $|HG(v,w)|\\leq k$ an $O(\\log\\log n)$-approximation can be found within $2^{O(\\log^2k)}\\cdot\\text{poly}(n)$ time.","authors":["Micha{\\l} Szyfelbein"],"url":"https://arxiv.org/abs/2504.17887"}
{"created":"2025-04-28","title":"Terrain-Aware Kinodynamic Planning with Efficiently Adaptive State Lattices for Mobile Robot Navigation in Off-Road Environments","abstract":"To safely traverse non-flat terrain, robots must account for the influence of terrain shape in their planned motions. Terrain-aware motion planners use an estimate of the vehicle roll and pitch as a function of pose, vehicle suspension, and ground elevation map to weigh the cost of edges in the search space. Encoding such information in a traditional two-dimensional cost map is limiting because it is unable to capture the influence of orientation on the roll and pitch estimates from sloped terrain. The research presented herein addresses this problem by encoding kinodynamic information in the edges of a recombinant motion planning search space based on the Efficiently Adaptive State Lattice (EASL). This approach, which we describe as a Kinodynamic Efficiently Adaptive State Lattice (KEASL), differs from the prior representation in two ways. First, this method uses a novel encoding of velocity and acceleration constraints and vehicle direction at expanded nodes in the motion planning graph. Second, this approach describes additional steps for evaluating the roll, pitch, constraints, and velocities associated with poses along each edge during search in a manner that still enables the graph to remain recombinant. Velocities are computed using an iterative bidirectional method using Eulerian integration that more accurately estimates the duration of edges that are subject to terrain-dependent velocity limits. Real-world experiments on a Clearpath Robotics Warthog Unmanned Ground Vehicle were performed in a non-flat, unstructured environment. Results from 2093 planning queries from these experiments showed that KEASL provided a more efficient route than EASL in 83.72% of cases when EASL plans were adjusted to satisfy terrain-dependent velocity constraints. An analysis of relative runtimes and differences between planned routes is additionally presented.","authors":["Eric R. Damm","Jason M. Gregory","Eli S. Lancaster","Felix A. Sanchez","Daniel M. Sahu","Thomas M. Howard"],"url":"https://arxiv.org/abs/2504.17889"}
{"created":"2025-04-28","title":"Do We Need Transformers to Play FPS Video Games?","abstract":"In this paper, we explore the Transformer based architectures for reinforcement learning in both online and offline settings within the Doom game environment. Our investigation focuses on two primary approaches: Deep Transformer Q- learning Networks (DTQN) for online learning and Decision Transformers (DT) for offline reinforcement learning. DTQN leverages the sequential modelling capabilities of Transformers to enhance Q-learning in partially observable environments,while Decision Transformers repurpose sequence modelling techniques to enable offline agents to learn from past trajectories without direct interaction with the environment. We conclude that while Transformers might have performed well in Atari games, more traditional methods perform better than Transformer based method in both the settings in the VizDoom environment.","authors":["Karmanbir Batth","Krish Sethi","Aly Shariff","Leo Shi","Hetul Patel"],"url":"https://arxiv.org/abs/2504.17891"}
{"created":"2025-04-28","title":"Token Sequence Compression for Efficient Multimodal Computing","abstract":"The exponential growth of Large Multimodal Models (LMMs) has driven advancements in cross-modal reasoning but at significant computational costs. In this work, we focus on visual language models. We highlight the redundancy and inefficiency in current vision encoders, and seek to construct an adaptive compression method for multimodal data. In this work, we characterize a panoply of visual token selection and merging approaches through both benchmarking and qualitative analysis. In particular, we demonstrate that simple cluster-level token aggregation outperforms prior state-of-the-art works in token selection and merging, including merging at the vision encoder level and attention-based approaches. We underline the redundancy in current vision encoders, and shed light on several puzzling trends regarding principles of visual token selection through cross-modal attention visualizations. This work is a first effort towards more effective encoding and processing of high-dimensional data, and paves the way for more scalable and sustainable multimodal systems.","authors":["Yasmine Omri","Parth Shroff","Thierry Tambe"],"url":"https://arxiv.org/abs/2504.17892"}
{"created":"2025-04-28","title":"DCT-Shield: A Robust Frequency Domain Defense against Malicious Image Editing","abstract":"Advancements in diffusion models have enabled effortless image editing via text prompts, raising concerns about image security. Attackers with access to user images can exploit these tools for malicious edits. Recent defenses attempt to protect images by adding a limited noise in the pixel space to disrupt the functioning of diffusion-based editing models. However, the adversarial noise added by previous methods is easily noticeable to the human eye. Moreover, most of these methods are not robust to purification techniques like JPEG compression under a feasible pixel budget. We propose a novel optimization approach that introduces adversarial perturbations directly in the frequency domain by modifying the Discrete Cosine Transform (DCT) coefficients of the input image. By leveraging the JPEG pipeline, our method generates adversarial images that effectively prevent malicious image editing. Extensive experiments across a variety of tasks and datasets demonstrate that our approach introduces fewer visual artifacts while maintaining similar levels of edit protection and robustness to noise purification techniques.","authors":["Aniruddha Bala","Rohit Chowdhury","Rohan Jaiswal","Siddharth Roheda"],"url":"https://arxiv.org/abs/2504.17894"}
{"created":"2025-04-28","title":"POD-ROM methods: error analysis for continuous parametrized approximations","abstract":"This paper studies the numerical approximation of parametric time-dependent partial differential equations (PDEs) by proper orthogonal decomposition reduced order models (POD-ROMs). Although many papers in the literature consider reduced order models for parametric equations, a complete error analysis of the methods is still a challenge. We introduce and analyze in this paper a new POD method based on finite differences (respect to time and respect to the parameters that may be considered). We obtain a priori bounds for the new method valid for any value of time in a given time interval and any value of the parameter in a given parameter interval. Our design of the new POD method allow us to prove pointwise-in-time error estimates as opposed to average error bounds obtained typically in POD methods. Most of the papers concerning POD methods for parametric equations are just based on the snapshots computed at different times and parameter values instead of their difference quotients. We show that the error analysis of the present paper can also cover the error analysis of that case (that we call standard). Some numerical experiments compare our new approach with the standard one and support the error analysis.","authors":["Bosco Garc\\'ia-Arcilla","Alicia Garc\\'ia-Mascaraque","Julia Novo"],"url":"https://arxiv.org/abs/2504.17895"}
{"created":"2025-04-28","title":"A Walk across Europe: Development of a high-resolution walkability index","abstract":"Physical inactivity significantly contributes to obesity and other non-communicable diseases, yet efforts to increase population-wide physical activity levels have met with limited success. The built environment plays a pivotal role in encouraging active behaviors like walking. Walkability indices, which aggregate various environmental features, provide a valuable tool for promoting healthy, walkable environments. However, a standardized, high-resolution walkability index for Europe has been lacking. This study addresses that gap by developing a standardized, high-resolution walkability index for the entire European region. Seven core components were selected to define walkability: walkable street length, intersection density, green spaces, slope, public transport access, land use mix, and 15-minute walking isochrones. These were derived from harmonized, high-resolution datasets such as Sentinel-2, NASA's elevation models, OpenStreetMap, and CORINE Land Cover. A 100 m x 100 m hierarchical grid system and advanced geospatial methods, like network buffers and distance decay, were used at scale to efficiently model real-world density and proximity effects. The resulting index was weighted by population and analyzed at different spatial levels using visual mapping, spatial clustering, and correlation analysis. Findings revealed a distinct urban-to-rural gradient, with high walkability scores concentrated in compact urban centers rich in street connectivity and land use diversity. The index highlighted cities like Barcelona, Berlin, Munich, Paris, and Warsaw as walkability leaders. This standardized, high-resolution walkability index serves as a practical tool for researchers, planners, and policymakers aiming to support active living and public health across diverse European contexts.","authors":["Nishit Patel","Hoang-Ha Nguyen","Jet van de Geest","Alfred Wagtendonk","Mohan JS Raju","Payam Dadvand","Kees de Hoogh","Marta Cirach","Mark Nieuwenhuijsen","Thao Minh Lam","Jeroen Lakerveld"],"url":"https://arxiv.org/abs/2504.17897"}
{"created":"2025-04-28","title":"Multivariate Newton Interpolation in Downward Closed Spaces Reaches the Optimal Geometric Approximation Rates for Bos--Levenberg--Trefethen Functions","abstract":"We extend the univariate Newton interpolation algorithm to arbitrary spatial dimensions and for any choice of downward-closed polynomial space, while preserving its quadratic runtime and linear storage cost. The generalisation supports any choice of the provided notion of non-tensorial unisolvent interpolation nodes, whose number coincides with the dimension of the chosen-downward closed space. Specifically, we prove that by selecting Leja-ordered Chebyshev-Lobatto or Leja nodes, the optimal geometric approximation rates for a class of analytic functions -- termed Bos--Levenberg--Trefethen functions -- are achieved and extend to the derivatives of the interpolants. In particular, choosing Euclidean degree results in downward-closed spaces whose dimension only grows sub-exponentially with spatial dimension, while delivering approximation rates close to, or even matching those of the tensorial maximum-degree case, mitigating the curse of dimensionality. Several numerical experiments demonstrate the performance of the resulting multivariate Newton interpolation compared to state-of-the-art alternatives and validate our theoretical results.","authors":["Michael Hecht","Phil-Alexander Hofmann","Damar Wicaksono","Uwe Hernandez Acosta","Krzysztof Gonciarz","Jannik Kissinger","Vladimir Sivkin","Ivo F. Sbalzarini"],"url":"https://arxiv.org/abs/2504.17899"}
{"created":"2025-04-28","title":"Beyond Task and Motion Planning: Hierarchical Robot Planning with General-Purpose Policies","abstract":"Task and motion planning is a well-established approach for solving long-horizon robot planning problems. However, traditional methods assume that each task-level robot action, or skill, can be reduced to kinematic motion planning. In this work, we address the challenge of planning with both kinematic skills and closed-loop motor controllers that go beyond kinematic considerations. We propose a novel method that integrates these controllers into motion planning using Composable Interaction Primitives (CIPs), enabling the use of diverse, non-composable pre-learned skills in hierarchical robot planning. Toward validating our Task and Skill Planning (TASP) approach, we describe ongoing robot experiments in real-world scenarios designed to demonstrate how CIPs can allow a mobile manipulator robot to effectively combine motion planning with general-purpose skills to accomplish complex tasks.","authors":["Benned Hedegaard","Ziyi Yang","Yichen Wei","Ahmed Jaafar","Stefanie Tellex","George Konidaris","Naman Shah"],"url":"https://arxiv.org/abs/2504.17901"}
{"created":"2025-04-28","title":"CAMU: Context Augmentation for Meme Understanding","abstract":"Social media memes are a challenging domain for hate detection because they intertwine visual and textual cues into culturally nuanced messages. We introduce a novel framework, CAMU, which leverages large vision-language models to generate more descriptive captions, a caption-scoring neural network to emphasise hate-relevant content, and parameter-efficient fine-tuning of CLIP's text encoder for an improved multimodal understanding of memes. Experiments on publicly available hateful meme datasets show that simple projection layer fine-tuning yields modest gains, whereas selectively tuning deeper text encoder layers significantly boosts performance on all evaluation metrics. Moreover, our approach attains high accuracy (0.807) and F1-score (0.806) on the Hateful Memes dataset, at par with the existing SoTA framework while being much more efficient, offering practical advantages in real-world scenarios that rely on fixed decision thresholds. CAMU also achieves the best F1-score of 0.673 on the MultiOFF dataset for offensive meme identification, demonstrating its generalisability. Additional analyses on benign confounders reveal that robust visual grounding and nuanced text representations are crucial for reliable hate and offence detection. We will publicly release CAMU along with the resultant models for further research.","authors":["Girish A. Koushik","Diptesh Kanojia","Helen Treharne","Aditya Joshi"],"url":"https://arxiv.org/abs/2504.17902"}
{"created":"2025-04-28","title":"Biting the CHERI bullet: Blockers, Enablers and Security Implications of CHERI in Defence","abstract":"There is growing interest in securing the hardware foundations software stacks build upon. However, before making any investment decision, software and hardware supply chain stakeholders require evidence from realistic, multiple long-term studies of adoption. We present results from a 12 month evaluation of one such secure hardware solution, CHERI, where 15 teams from industry and academia ported software relevant to Defence to Arm's experimental Morello board. We identified six types of blocker inhibiting adoption: dependencies, a knowledge premium, missing utilities, performance, platform instability, and technical debt. We also identified three types of enabler: tool assistance, improved quality, and trivial code porting. Finally, we identified five types of potential vulnerability that CHERI could, if not appropriately configured, expand a system's attack surface: state leaks, memory leaks, use after free vulnerabilities, unsafe defaults, and tool chain instability. Future work should remove potentially insecure defaults from CHERI tooling, and develop a CHERI body of knowledge to further adoption.","authors":["Shamal Faily"],"url":"https://arxiv.org/abs/2504.17904"}
{"created":"2025-04-28","title":"\"Shifting Access Control Left\" using Asset and Goal Models","abstract":"Access control needs have broad design implications, but access control specifications may be elicited before, during, or after these needs are captured. Because access control knowledge is distributed, we need to make knowledge asymmetries more transparent, and use expertise already available to stakeholders. In this paper, we present a tool-supported technique identifying knowledge asymmetries around access control based on asset and goal models. Using simple and conventional modelling languages that complement different design techniques, we provide boundary objects to make access control transparent, thereby making knowledge about access control concerns more symmetric. We illustrate this technique using a case study example considering the suitability of a reusable software component in a new military air system.","authors":["Shamal Faily"],"url":"https://arxiv.org/abs/2504.17906"}
{"created":"2025-04-28","title":"The use of Multi-domain Electroencephalogram Representations in the building of Models based on Convolutional and Recurrent Neural Networks for Epilepsy Detection","abstract":"Epilepsy, affecting approximately 50 million people globally, is characterized by abnormal brain activity and remains challenging to treat. The diagnosis of epilepsy relies heavily on electroencephalogram (EEG) data, where specialists manually analyze epileptiform patterns across pre-ictal, ictal, post-ictal, and interictal periods. However, the manual analysis of EEG signals is prone to variability between experts, emphasizing the need for automated solutions. Although previous studies have explored preprocessing techniques and machine learning approaches for seizure detection, there is a gap in understanding how the representation of EEG data (time, frequency, or time-frequency domains) impacts the predictive performance of deep learning models. This work addresses this gap by systematically comparing deep neural networks trained on EEG data in these three domains. Through the use of statistical tests, we identify the optimal data representation and model architecture for epileptic seizure detection. The results demonstrate that frequency-domain data achieves detection metrics exceeding 97\\%, providing a robust foundation for more accurate and reliable seizure detection systems.","authors":["Luiz Antonio Nicolau Anghinoni","Gustavo Weber Denardin","Jadson Castro Gertrudes","Dalcimar Casanova","Jefferson Tales Oliva"],"url":"https://arxiv.org/abs/2504.17908"}
{"created":"2025-04-28","title":"STNet: Prediction of Underwater Sound Speed Profiles with An Advanced Semi-Transformer Neural Network","abstract":"Real time acquisition of accurate underwater sound velocity profile (SSP) is crucial for tracking the propagation trajectory of underwater acoustic signals, making it play a key role in ocean communication positioning. SSPs can be directly measured by instruments or inverted leveraging sound field data. Although measurement techniques provide a good accuracy, they are constrained by limited spatial coverage and require substantial time investment. The inversion method based on real-time measurement of acoustic field data improves operational efficiency, but loses the accuracy of SSP estimation and suffers from limited spatial applicability due to its stringent requirements for ocean observation infrastructure. To achieve accurate long-term ocean SSP estimation independent of real-time underwater data measurements, we propose a Semi-Transformer neural network (STNet) specifically designed for simulating sound velocity distribution patterns from the perspective of time series prediction. The proposed network architecture incorporates an optimized self-attention mechanism to effectively capture long-range temporal dependencies within historical sound velocity time-series data, facilitating accurate estimation of current SSPs or prediction of future SSPs. Through architectural optimization of the Transformer framework and integration of a time encoding mechanism, STNet could effectively improve computational efficiency. Comparative experimental results reveal that STNet outperforms state-of-the-art models in predictive accuracy and maintain good computational efficiency, demonstrating its potential for enabling accurate long-term full-depth ocean SSP forecasting.","authors":["Wei Huang","Jiajun Lu","Hao Zhang","Tianhe Xu"],"url":"https://arxiv.org/abs/2504.17912"}
{"created":"2025-04-28","title":"CANet: ChronoAdaptive Network for Enhanced Long-Term Time Series Forecasting under Non-Stationarity","abstract":"Long-term time series forecasting plays a pivotal role in various real-world applications. Despite recent advancements and the success of different architectures, forecasting is often challenging due to non-stationary nature of the real-world data, which frequently exhibit distribution shifts and temporal changes in statistical properties like mean and variance over time. Previous studies suggest that this inherent variability complicates forecasting, limiting the performance of many models by leading to loss of non-stationarity and resulting in over-stationarization (Liu, Wu, Wang and Long, 2022). To address this challenge, we introduce a novel architecture, ChoronoAdaptive Network (CANet), inspired by style-transfer techniques. The core of CANet is the Non-stationary Adaptive Normalization module, seamlessly integrating the Style Blending Gate and Adaptive Instance Normalization (AdaIN) (Huang and Belongie, 2017). The Style Blending Gate preserves and reintegrates non-stationary characteristics, such as mean and standard deviation, by blending internal and external statistics, preventing over-stationarization while maintaining essential temporal dependencies. Coupled with AdaIN, which dynamically adapts the model to statistical changes, this approach enhances predictive accuracy under non-stationary conditions. CANet also employs multi-resolution patching to handle short-term fluctuations and long-term trends, along with Fourier analysis-based adaptive thresholding to reduce noise. A Stacked Kronecker Product Layer further optimizes the model's efficiency while maintaining high performance. Extensive experiments on real-world datasets validate CANet's superiority over state-of-the-art methods, achieving a 42% reduction in MSE and a 22% reduction in MAE. The source code is publicly available at https://github.com/mertsonmezer/CANet.","authors":["Mert Sonmezer","Seyda Ertekin"],"url":"https://arxiv.org/abs/2504.17913"}
{"created":"2025-04-28","title":"Non-distributive Lattices, Stable Matchings, and Linear Optimization","abstract":"We show that all finite lattices, including non-distributive lattices, arise as stable matching lattices under standard assumptions on choice functions. In the process, we introduce new tools to reason on general lattices for optimization purposes: the partial representation of a lattice, which partially extends Birkhoff's representation theorem to non-distributive lattices; the distributive closure of a lattice, which gives such a partial representation; and join constraints, which can be added to the distributive closure to obtain a representation for the original lattice. Then, we use these techniques to show that the minimum cost stable matching problem under the same standard assumptions on choice functions is NP-hard, by establishing a connection with antimatroid theory.","authors":["Christopher En","Yuri Faenza"],"url":"https://arxiv.org/abs/2504.17916"}
{"created":"2025-04-28","title":"PHast -- Perfect Hashing with fast evaluation","abstract":"Perfect hash functions give unique \"names\" to arbitrary keys requiring only a few bits per key. This is an essential building block in applications like static hash tables, databases, or bioinformatics. This paper introduces the PHast approach that has the currently fastest query time with competitive construction time and space consumption. PHast improves bucket-placement which first hashes each key k to a bucket, and then looks for the bucket seed s such that a secondary hash function maps pairs (s,k) in a collision-free way. PHast can use small-range primary hash functions with linear mapping, fixed-width encoding of seeds, and parallel construction. This is achieved using small overlapping slices of allowed values and bumping to handle unsuccessful seed assignment.","authors":["Piotr Beling","Peter Sanders"],"url":"https://arxiv.org/abs/2504.17918"}
{"created":"2025-04-28","title":"Secured Encryption scheme based on the Ree groups","abstract":"An improved design of a cryptosystem based on small Ree groups is proposed. We have changed the encryption algorithm and propose to use a logarithmic signature for the entire Ree group. This approach improves security against sequential key recovery attacks. Hence, the complexity of the key recovery attack will be defined by a brute-force attack over the entire group. In this paper, we have proved that to construct secure cryptosystems with group computations over a small finite field, it is needed to use a 3-parametric small Ree group.","authors":["Gennady Khalimov","Yevgen Kotukh"],"url":"https://arxiv.org/abs/2504.17919"}
{"created":"2025-04-28","title":"Avoiding Leakage Poisoning: Concept Interventions Under Distribution Shifts","abstract":"In this paper, we investigate how concept-based models (CMs) respond to out-of-distribution (OOD) inputs. CMs are interpretable neural architectures that first predict a set of high-level concepts (e.g., stripes, black) and then predict a task label from those concepts. In particular, we study the impact of concept interventions (i.e., operations where a human expert corrects a CM's mispredicted concepts at test time) on CMs' task predictions when inputs are OOD. Our analysis reveals a weakness in current state-of-the-art CMs, which we term leakage poisoning, that prevents them from properly improving their accuracy when intervened on for OOD inputs. To address this, we introduce MixCEM, a new CM that learns to dynamically exploit leaked information missing from its concepts only when this information is in-distribution. Our results across tasks with and without complete sets of concept annotations demonstrate that MixCEMs outperform strong baselines by significantly improving their accuracy for both in-distribution and OOD samples in the presence and absence of concept interventions.","authors":["Mateo Espinosa Zarlenga","Gabriele Dominici","Pietro Barbiero","Zohreh Shams","Mateja Jamnik"],"url":"https://arxiv.org/abs/2504.17921"}
{"created":"2025-04-28","title":"EAQGA: A Quantum-Enhanced Genetic Algorithm with Novel Entanglement-Aware Crossovers","abstract":"Genetic algorithms are highly effective optimization techniques for many computationally challenging problems, including combinatorial optimization tasks like portfolio optimization. Quantum computing has also shown potential in addressing these complex challenges. Combining these approaches, quantum genetic algorithms leverage the principles of superposition and entanglement to enhance the performance of classical genetic algorithms. In this work, we propose a novel quantum genetic algorithm introducing an innovative crossover strategy to generate quantum circuits from a binary solution. We incorporate a heuristic method to encode entanglement patterns from parent solutions into circuits for the next generation. Our algorithm advances quantum genetic algorithms by utilizing a limited number of entanglements, enabling efficient exploration of optimal solutions without significantly increasing circuit depth, making it suitable for near-term applications. We test this approach on a portfolio optimization problem using an IBM 127 qubits Eagle processor (ibm_quebec) and simulators. Compared to state-of-the-art algorithms, our results show that the proposed method improves fitness values by 33.6% over classical genetic algorithm and 37.2% over quantum-inspired genetic algorithm, using the same iteration counts and population sizes with real quantum hardware employing 100 qubits. These findings highlight the potential of current quantum computers to address real-world utility-scale combinatorial optimization problems.","authors":["Mohammad Kashfi Haghighi","Matthieu Fortin-Desch\\^enes","Christophe Pere","Micka\\\"el Camus"],"url":"https://arxiv.org/abs/2504.17923"}
{"created":"2025-04-28","title":"Learning Attentive Neural Processes for Planning with Pushing Actions","abstract":"Our goal is to enable robots to plan sequences of tabletop actions to push a block with unknown physical properties to a desired goal pose on the table. We approach this problem by learning the constituent models of a Partially-Observable Markov Decision Process (POMDP), where the robot can observe the outcome of a push, but the physical properties of the block that govern the dynamics remain unknown. The pushing problem is a difficult POMDP to solve due to the challenge of state estimation. The physical properties have a nonlinear relationship with the outcomes, requiring computationally expensive methods, such as particle filters, to represent beliefs. Leveraging the Attentive Neural Process architecture, we propose to replace the particle filter with a neural network that learns the inference computation over the physical properties given a history of actions. This Neural Process is integrated into planning as the Neural Process Tree with Double Progressive Widening (NPT-DPW). Simulation results indicate that NPT-DPW generates more effective plans faster than traditional particle filter methods, even in complex pushing scenarios.","authors":["Atharv Jain","Seiji Shaw","Nicholas Roy"],"url":"https://arxiv.org/abs/2504.17924"}
{"created":"2025-04-28","title":"Recursive Identification of Structured Systems: An Instrumental-Variable Approach Applied to Mechanical Systems","abstract":"Online system identification algorithms are widely used for monitoring, diagnostics and control by continuously adapting to time-varying dynamics. Typically, these algorithms consider a model structure that lacks parsimony and offers limited physical interpretability. The objective of this paper is to develop a real-time parameter estimation algorithm aimed at identifying time-varying dynamics within an interpretable model structure. An additive model structure is adopted for this purpose, which offers enhanced parsimony and is shown to be particularly suitable for mechanical systems. The proposed approach integrates the recursive simplified refined instrumental variable method with block-coordinate descent to minimize an exponentially-weighted output error cost function. This novel recursive identification method delivers parametric continuous-time additive models and is applicable in both open-loop and closed-loop controlled systems. Its efficacy is shown using numerical simulations and is further validated using experimental data to detect the time-varying resonance dynamics of a flexible beam system. These results demonstrate the effectiveness of the proposed approach for online and interpretable estimation for advanced monitoring and control applications.","authors":["Koen Classens","Rodrigo A. Gonz\\'alez","Tom Oomen"],"url":"https://arxiv.org/abs/2504.17927"}
{"created":"2025-04-28","title":"ApproXAI: Energy-Efficient Hardware Acceleration of Explainable AI using Approximate Computing","abstract":"Explainable artificial intelligence (XAI) enhances AI system transparency by framing interpretability as an optimization problem. However, this approach often necessitates numerous iterations of computationally intensive operations, limiting its applicability in real-time scenarios. While recent research has focused on XAI hardware acceleration on FPGAs and TPU, these methods do not fully address energy efficiency in real-time settings. To address this limitation, we propose XAIedge, a novel framework that leverages approximate computing techniques into XAI algorithms, including integrated gradients, model distillation, and Shapley analysis. XAIedge translates these algorithms into approximate matrix computations and exploits the synergy between convolution, Fourier transform, and approximate computing paradigms. This approach enables efficient hardware acceleration on TPU-based edge devices, facilitating faster real-time outcome interpretations. Our comprehensive evaluation demonstrates that XAIedge achieves a $2\\times$ improvement in energy efficiency compared to existing accurate XAI hardware acceleration techniques while maintaining comparable accuracy. These results highlight the potential of XAIedge to significantly advance the deployment of explainable AI in energy-constrained real-time applications.","authors":["Ayesha Siddique","Khurram Khalil","Khaza Anuarul Hoque"],"url":"https://arxiv.org/abs/2504.17929"}
{"created":"2025-04-28","title":"Optimized Approaches to Malware Detection: A Study of Machine Learning and Deep Learning Techniques","abstract":"Digital systems find it challenging to keep up with cybersecurity threats. The daily emergence of more than 560,000 new malware strains poses significant hazards to the digital ecosystem. The traditional malware detection methods fail to operate properly and yield high false positive rates with low accuracy of the protection system. This study explores the ways in which malware can be detected using these machine learning (ML) and deep learning (DL) approaches to address those shortcomings. This study also includes a systematic comparison of the performance of some of the widely used ML models, such as random forest, multi-layer perceptron (MLP), and deep neural network (DNN), for determining the effectiveness of the domain of modern malware threat systems. We use a considerable-sized database from Kaggle, which has undergone optimized feature selection and preprocessing to improve model performance. Our finding suggests that the DNN model outperformed the other traditional models with the highest training accuracy of 99.92% and an almost perfect AUC score. Furthermore, the feature selection and preprocessing can help improve the capabilities of detection. This research makes an important contribution by analyzing the performance of the model on the performance metrics and providing insight into the effectiveness of the advanced detection techniques to build more robust and more reliable cybersecurity solutions against the growing malware threats.","authors":["Abrar Fahim","Shamik Dey","Md. Nurul Absur","Md Kamrul Siam","Md. Tahmidul Huque","Jafreen Jafor Godhuli"],"url":"https://arxiv.org/abs/2504.17930"}
{"created":"2025-04-28","title":"Toward a Human-Centered Evaluation Framework for Trustworthy LLM-Powered GUI Agents","abstract":"The rise of Large Language Models (LLMs) has revolutionized Graphical User Interface (GUI) automation through LLM-powered GUI agents, yet their ability to process sensitive data with limited human oversight raises significant privacy and security risks. This position paper identifies three key risks of GUI agents and examines how they differ from traditional GUI automation and general autonomous agents. Despite these risks, existing evaluations focus primarily on performance, leaving privacy and security assessments largely unexplored. We review current evaluation metrics for both GUI and general LLM agents and outline five key challenges in integrating human evaluators for GUI agent assessments. To address these gaps, we advocate for a human-centered evaluation framework that incorporates risk assessments, enhances user awareness through in-context consent, and embeds privacy and security considerations into GUI agent design and evaluation.","authors":["Chaoran Chen","Zhiping Zhang","Ibrahim Khalilov","Bingcan Guo","Simret A Gebreegziabher","Yanfang Ye","Ziang Xiao","Yaxing Yao","Tianshi Li","Toby Jia-Jun Li"],"url":"https://arxiv.org/abs/2504.17934"}
{"created":"2025-04-28","title":"Masked strategies for images with small objects","abstract":"The hematology analytics used for detection and classification of small blood components is a significant challenge. In particular, when objects exists as small pixel-sized entities in a large context of similar objects. Deep learning approaches using supervised models with pre-trained weights, such as residual networks and vision transformers have demonstrated success for many applications. Unfortunately, when applied to images outside the domain of learned representations, these methods often result with less than acceptable performance. A strategy to overcome this can be achieved by using self-supervised models, where representations are learned and weights are then applied for downstream applications. Recently, masked autoencoders have proven to be effective to obtain representations that captures global context information. By masking regions of an image and having the model learn to reconstruct both the masked and non-masked regions, weights can be used for various applications. However, if the sizes of the objects in images are less than the size of the mask, the global context information is lost, making it almost impossible to reconstruct the image. In this study, we investigated the effect of mask ratios and patch sizes for blood components using a MAE to obtain learned ViT encoder representations. We then applied the encoder weights to train a U-Net Transformer for semantic segmentation to obtain both local and global contextual information. Our experimental results demonstrates that both smaller mask ratios and patch sizes improve the reconstruction of images using a MAE. We also show the results of semantic segmentation with and without pre-trained weights, where smaller-sized blood components benefited with pre-training. Overall, our proposed method offers an efficient and effective strategy for the segmentation and classification of small objects.","authors":["H. Martin Gillis","Ming Hill","Paul Hollensen","Alan Fine","Thomas Trappenberg"],"url":"https://arxiv.org/abs/2504.17935"}
{"created":"2025-04-28","title":"An Optimal $3$-Fault-Tolerant Connectivity Oracle","abstract":"We present an optimal oracle for answering connectivity queries in undirected graphs in the presence of at most three vertex failures. Specifically, we show that we can process a graph $G$ in $O(n+m)$ time, in order to build a data structure that occupies $O(n)$ space, which can be used in order to answer queries of the form \"given a set $F$ of at most three vertices, and two vertices $x$ and $y$ not in $F$, are $x$ and $y$ connected in $G\\setminus F$?\" in constant time, where $n$ and $m$ denote the number of vertices and edges, respectively, of $G$. The idea is to rely on the DFS-based framework introduced by Kosinas [ESA'23], for handling connectivity queries in the presence of multiple vertex failures. Our technical contribution is to show how to appropriately extend the toolkit of the DFS-based parameters, in order to optimally handle up to three vertex failures. Our approach has the interesting property that it does not rely on a compact representation of vertex cuts, and has the potential to provide optimal solutions for more vertex failures. Furthermore, we show that the DFS-based framework can be easily extended in order to answer vertex-cut queries, and the number of connected components in the presence of multiple vertex failures. In the case of three vertex failures, we can answer such queries in $O(\\log n)$ time.","authors":["Evangelos Kosinas"],"url":"https://arxiv.org/abs/2504.17937"}
{"created":"2025-04-28","title":"Machine Learning-Based Prediction of Quality Shifts on Video Streaming Over 5G","abstract":"The Quality of Experience (QoE) is the users satisfaction while streaming a video session over an over-the-top (OTT) platform like YouTube. QoE of YouTube reflects the smooth streaming session without any buffering and quality shift events. One of the most important factors nowadays affecting QoE of YouTube is frequent shifts from higher to lower resolutions and vice versa. These shifts ensure a smooth streaming session; however, it might get a lower mean opinion score. For instance, dropping from 1080p to 480p during a video can preserve continuity but might reduce the viewers enjoyment. Over time, OTT platforms are looking for alternative ways to boost user experience instead of relying on traditional Quality of Service (QoS) metrics such as bandwidth, latency, and throughput. As a result, we look into the relationship between quality shifting in YouTube streaming sessions and the channel metrics RSRP, RSRQ, and SNR. Our findings state that these channel metrics positively correlate with shifts. Thus, in real-time, OTT can only rely on them to predict video streaming sessions into lower- and higher-resolution categories, thus providing more resources to improve user experience. Using traditional Machine Learning (ML) classifiers, we achieved an accuracy of 77-percent, while using only RSRP, RSRQ, and SNR. In the era of 5G and beyond, where ultra-reliable, low-latency networks promise enhanced streaming capabilities, the proposed methodology can be used to improve OTT services.","authors":["Raza Ul Mustafa","Sesha Dassanayake"],"url":"https://arxiv.org/abs/2504.17938"}
{"created":"2025-04-28","title":"Causality-Driven Neural Network Repair: Challenges and Opportunities","abstract":"Deep Neural Networks (DNNs) often rely on statistical correlations rather than causal reasoning, limiting their robustness and interpretability. While testing methods can identify failures, effective debugging and repair remain challenging. This paper explores causal inference as an approach primarily for DNN repair, leveraging causal debugging, counterfactual analysis, and structural causal models (SCMs) to identify and correct failures. We discuss in what ways these techniques support fairness, adversarial robustness, and backdoor mitigation by providing targeted interventions. Finally, we discuss key challenges, including scalability, generalization, and computational efficiency, and outline future directions for integrating causality-driven interventions to enhance DNN reliability.","authors":["Fatemeh Vares","Brittany Johnson"],"url":"https://arxiv.org/abs/2504.17946"}
{"created":"2025-04-28","title":"Improving the Threshold for Finding Rank-1 Matrices in a Subspace","abstract":"We consider a basic computational task of finding $s$ planted rank-1 $m \\times n$ matrices in a linear subspace $\\mathcal{U} \\subseteq \\mathbb{R}^{m \\times n}$ where $\\dim(\\mathcal{U}) = R \\ge s$. The work of Johnston-Lovitz-Vijayaraghavan (FOCS 2023) gave a polynomial-time algorithm for this task and proved that it succeeds when ${R \\le (1-o(1))mn/4}$, under minimal genericity assumptions on the input. Aiming to precisely characterize the performance of this algorithm, we improve the bound to ${R \\le (1-o(1))mn/2}$ and also prove that the algorithm fails when ${R \\ge (1+o(1))mn/\\sqrt{2}}$. Numerical experiments indicate that the true breaking point is $R = (1+o(1))mn/\\sqrt{2}$. Our work implies new algorithmic results for tensor decomposition, for instance, decomposing order-4 tensors with twice as many components as before.","authors":["Jeshu Dastidar","Tait Weicht","Alexander S. Wein"],"url":"https://arxiv.org/abs/2504.17947"}
{"created":"2025-04-28","title":"Collaborating Action by Action: A Multi-agent LLM Framework for Embodied Reasoning","abstract":"Collaboration is ubiquitous and essential in day-to-day life -- from exchanging ideas, to delegating tasks, to generating plans together. This work studies how LLMs can adaptively collaborate to perform complex embodied reasoning tasks. To this end we introduce MINDcraft, an easily extensible platform built to enable LLM agents to control characters in the open-world game of Minecraft; and MineCollab, a benchmark to test the different dimensions of embodied and collaborative reasoning. An experimental study finds that the primary bottleneck in collaborating effectively for current state-of-the-art agents is efficient natural language communication, with agent performance dropping as much as 15% when they are required to communicate detailed task completion plans. We conclude that existing LLM agents are ill-optimized for multi-agent collaboration, especially in embodied scenarios, and highlight the need to employ methods beyond in-context and imitation learning. Our website can be found here: https://mindcraft-minecollab.github.io/","authors":["Isadora White","Kolby Nottingham","Ayush Maniar","Max Robinson","Hansen Lillemark","Mehul Maheshwari","Lianhui Qin","Prithviraj Ammanabrolu"],"url":"https://arxiv.org/abs/2504.17950"}
{"created":"2025-04-28","title":"Fishing for Phishers: Learning-Based Phishing Detection in Ethereum Transactions","abstract":"Phishing detection on Ethereum has increasingly leveraged advanced machine learning techniques to identify fraudulent transactions. However, limited attention has been given to understanding the effectiveness of feature selection strategies and the role of graph-based models in enhancing detection accuracy. In this paper, we systematically examine these issues by analyzing and contrasting explicit transactional features and implicit graph-based features, both experimentally and analytically. We explore how different feature sets impact the performance of phishing detection models, particularly in the context of Ethereum's transactional network. Additionally, we address key challenges such as class imbalance and dataset composition and their influence on the robustness and precision of detection methods. Our findings demonstrate the advantages and limitations of each feature type, while also providing a clearer understanding of how feature affect model resilience and generalization in adversarial environments.","authors":["Ahod Alghuried","Abdulaziz Alghamdi","Ali Alkinoon","Soohyeon Choi","Manar Mohaisen","David Mohaisen"],"url":"https://arxiv.org/abs/2504.17953"}
{"created":"2025-04-28","title":"iVR-GS: Inverse Volume Rendering for Explorable Visualization via Editable 3D Gaussian Splatting","abstract":"In volume visualization, users can interactively explore the three-dimensional data by specifying color and opacity mappings in the transfer function (TF) or adjusting lighting parameters, facilitating meaningful interpretation of the underlying structure. However, rendering large-scale volumes demands powerful GPUs and high-speed memory access for real-time performance. While existing novel view synthesis (NVS) methods offer faster rendering speeds with lower hardware requirements, the visible parts of a reconstructed scene are fixed and constrained by preset TF settings, significantly limiting user exploration. This paper introduces inverse volume rendering via Gaussian splatting (iVR-GS), an innovative NVS method that reduces the rendering cost while enabling scene editing for interactive volume exploration. Specifically, we compose multiple iVR-GS models associated with basic TFs covering disjoint visible parts to make the entire volumetric scene visible. Each basic model contains a collection of 3D editable Gaussians, where each Gaussian is a 3D spatial point that supports real-time scene rendering and editing. We demonstrate the superior reconstruction quality and composability of iVR-GS against other NVS solutions (Plenoxels, CCNeRF, and base 3DGS) on various volume datasets. The code is available at https://github.com/TouKaienn/iVR-GS.","authors":["Kaiyuan Tang","Siyuan Yao","Chaoli Wang"],"url":"https://arxiv.org/abs/2504.17954"}
{"created":"2025-04-28","title":"Combinatorial Drone Searching","abstract":"We introduce and study the combinatorial drone searching problem, which we describe in terms of search strategies for finding one or more hikers lost in a forest. An aerial drone can issue a probe to send a signal a given distance such that if there is a lost hiker within this distance, then the drone will learn this. But the drone does not learn the direction or distance to the lost hiker. The optimization problem is to minimize the number of probes and/or hiker responses, as well as possibly minimizing the flight distance for the drone. We describe a number of efficient combinatorial drone searching strategies and we analyze each one in terms of the size, $n$, of the search domain. Moreover, we derive strong bounds for the constant factors for the search costs for our algorithms, which in some cases involve computer-assisted proofs. We also show how to extend these strategies to find all lost hikers using a simple, memoryless drone search, traveling a distance that is $\\mathcal{O}(\\log{k})$-competitive with the optimal traveling salesperson (TSP) tour for $k$ lost hikers.","authors":["Ofek Gila (University of California","Irvine)","Michael T. Goodrich (University of California","Irvine)","Zahra Hadizadeh (University of Rochester)","Daniel S. Hirschberg (University of California","Irvine)","Shayan Taherijam (University of California","Irvine)"],"url":"https://arxiv.org/abs/2504.17955"}
{"created":"2025-04-28","title":"Categorical generalization of spectral decomposition","abstract":"In this paper, we give several equivalent characterizations for a category with finite biproducts and the sum operation of arrows, and called categories satisfying these semiadditive $\\mathbf{C}\\mathbf{Mon}$-categories. This allow us to give equivalent structures without directly confirming the existence of biproducts. Moreover, we define a generalized notion of the spectral decomposition in semiadditive $\\mathbf{C}\\mathbf{Mon}$-categories. We also define the notion of a semiadditive $\\mathbf{C}\\mathbf{Mon}$-functor that preserves the spectral decomposition of arrows. Semiadditive $\\mathbf{C}\\mathbf{Mon}$-categories and semiadditive $\\mathbf{C}\\mathbf{Mon}$-functors include many examples.","authors":["Koki Nishizawa","Yusuke Ide","Norihiro Tsumagari"],"url":"https://arxiv.org/abs/2504.17956"}
{"created":"2025-04-28","title":"CIVIL: Causal and Intuitive Visual Imitation Learning","abstract":"Today's robots learn new tasks by imitating human examples. However, this standard approach to visual imitation learning is fundamentally limited: the robot observes what the human does, but not why the human chooses those behaviors. Without understanding the features that factor into the human's decisions, robot learners often misinterpret the data and fail to perform the task when the environment changes. We therefore propose a shift in perspective: instead of asking human teachers just to show what actions the robot should take, we also enable humans to indicate task-relevant features using markers and language prompts. Our proposed algorithm, CIVIL, leverages this augmented data to filter the robot's visual observations and extract a feature representation that causally informs human actions. CIVIL then applies these causal features to train a transformer-based policy that emulates human behaviors without being confused by visual distractors. Our simulations, real-world experiments, and user study demonstrate that robots trained with CIVIL can learn from fewer human demonstrations and perform better than state-of-the-art baselines, especially in previously unseen scenarios. See videos at our project website: https://civil2025.github.io","authors":["Yinlong Dai","Robert Ramirez Sanchez","Ryan Jeronimus","Shahabedin Sagheb","Cara M. Nunez","Heramb Nemlekar","Dylan P. Losey"],"url":"https://arxiv.org/abs/2504.17959"}
{"created":"2025-04-28","title":"VIGMA: An Open-Access Framework for Visual Gait and Motion Analytics","abstract":"Gait disorders are commonly observed in older adults, who frequently experience various issues related to walking. Additionally, researchers and clinicians extensively investigate mobility related to gait in typically and atypically developing children, athletes, and individuals with orthopedic and neurological disorders. Effective gait analysis enables the understanding of the causal mechanisms of mobility and balance control of patients, the development of tailored treatment plans to improve mobility, the reduction of fall risk, and the tracking of rehabilitation progress. However, analyzing gait data is a complex task due to the multivariate nature of the data, the large volume of information to be interpreted, and the technical skills required. Existing tools for gait analysis are often limited to specific patient groups (e.g., cerebral palsy), only handle a specific subset of tasks in the entire workflow, and are not openly accessible. To address these shortcomings, we conducted a requirements assessment with gait practitioners (e.g., researchers, clinicians) via surveys and identified key components of the workflow, including (1) data processing and (2) data analysis and visualization. Based on the findings, we designed VIGMA, an open-access visual analytics framework integrated with computational notebooks and a Python library, to meet the identified requirements. Notably, the framework supports analytical capabilities for assessing disease progression and for comparing multiple patient groups. We validated the framework through usage scenarios with experts specializing in gait and mobility rehabilitation. VIGMA is available at https://github.com/komar41/VIGMA.","authors":["Kazi Shahrukh Omar","Shuaijie Wang","Ridhuparan Kungumaraju","Tanvi Bhatt","Fabio Miranda"],"url":"https://arxiv.org/abs/2504.17960"}
{"created":"2025-04-28","title":"Mathematics of Continual Learning","abstract":"Continual learning is an emerging subject in machine learning that aims to solve multiple tasks presented sequentially to the learner without forgetting previously learned tasks. Recently, many deep learning based approaches have been proposed for continual learning, however the mathematical foundations behind existing continual learning methods remain underdeveloped. On the other hand, adaptive filtering is a classic subject in signal processing with a rich history of mathematically principled methods. However, its role in understanding the foundations of continual learning has been underappreciated. In this tutorial, we review the basic principles behind both continual learning and adaptive filtering, and present a comparative analysis that highlights multiple connections between them. These connections allow us to enhance the mathematical foundations of continual learning based on existing results for adaptive filtering, extend adaptive filtering insights using existing continual learning methods, and discuss a few research directions for continual learning suggested by the historical developments in adaptive filtering.","authors":["Liangzu Peng","Ren\\'e Vidal"],"url":"https://arxiv.org/abs/2504.17963"}
{"created":"2025-04-28","title":"Evaluating Machine Expertise: How Graduate Students Develop Frameworks for Assessing GenAI Content","abstract":"This paper examines how graduate students develop frameworks for evaluating machine-generated expertise in web-based interactions with large language models (LLMs). Through a qualitative study combining surveys, LLM interaction transcripts, and in-depth interviews with 14 graduate students, we identify patterns in how these emerging professionals assess and engage with AI-generated content. Our findings reveal that students construct evaluation frameworks shaped by three main factors: professional identity, verification capabilities, and system navigation experience. Rather than uniformly accepting or rejecting LLM outputs, students protect domains central to their professional identities while delegating others--with managers preserving conceptual work, designers safeguarding creative processes, and programmers maintaining control over core technical expertise. These evaluation frameworks are further influenced by students' ability to verify different types of content and their experience navigating complex systems. This research contributes to web science by highlighting emerging human-genAI interaction patterns and suggesting how platforms might better support users in developing effective frameworks for evaluating machine-generated expertise signals in AI-mediated web environments.","authors":["Celia Chen","Alex Leitch"],"url":"https://arxiv.org/abs/2504.17964"}
{"created":"2025-04-28","title":"Plug-and-Play Physics-informed Learning using Uncertainty Quantified Port-Hamiltonian Models","abstract":"The ability to predict trajectories of surrounding agents and obstacles is a crucial component in many robotic applications. Data-driven approaches are commonly adopted for state prediction in scenarios where the underlying dynamics are unknown. However, the performance, reliability, and uncertainty of data-driven predictors become compromised when encountering out-of-distribution observations relative to the training data. In this paper, we introduce a Plug-and-Play Physics-Informed Machine Learning (PnP-PIML) framework to address this challenge. Our method employs conformal prediction to identify outlier dynamics and, in that case, switches from a nominal predictor to a physics-consistent model, namely distributed Port-Hamiltonian systems (dPHS). We leverage Gaussian processes to model the energy function of the dPHS, enabling not only the learning of system dynamics but also the quantification of predictive uncertainty through its Bayesian nature. In this way, the proposed framework produces reliable physics-informed predictions even for the out-of-distribution scenarios.","authors":["Kaiyuan Tan","Peilun Li","Jun Wang","Thomas Beckers"],"url":"https://arxiv.org/abs/2504.17966"}
{"created":"2025-04-28","title":"LLM Agent Swarm for Hypothesis-Driven Drug Discovery","abstract":"Drug discovery remains a formidable challenge: more than 90 percent of candidate molecules fail in clinical evaluation, and development costs often exceed one billion dollars per approved therapy. Disparate data streams, from genomics and transcriptomics to chemical libraries and clinical records, hinder coherent mechanistic insight and slow progress. Meanwhile, large language models excel at reasoning and tool integration but lack the modular specialization and iterative memory required for regulated, hypothesis-driven workflows. We introduce PharmaSwarm, a unified multi-agent framework that orchestrates specialized LLM \"agents\" to propose, validate, and refine hypotheses for novel drug targets and lead compounds. Each agent accesses dedicated functionality--automated genomic and expression analysis; a curated biomedical knowledge graph; pathway enrichment and network simulation; interpretable binding affinity prediction--while a central Evaluator LLM continuously ranks proposals by biological plausibility, novelty, in silico efficacy, and safety. A shared memory layer captures validated insights and fine-tunes underlying submodels over time, yielding a self-improving system. Deployable on low-code platforms or Kubernetes-based microservices, PharmaSwarm supports literature-driven discovery, omics-guided target identification, and market-informed repurposing. We also describe a rigorous four-tier validation pipeline spanning retrospective benchmarking, independent computational assays, experimental testing, and expert user studies to ensure transparency, reproducibility, and real-world impact. By acting as an AI copilot, PharmaSwarm can accelerate translational research and deliver high-confidence hypotheses more efficiently than traditional pipelines.","authors":["Kevin Song","Andrew Trotter","Jake Y. Chen"],"url":"https://arxiv.org/abs/2504.17967"}
{"created":"2025-04-28","title":"Virtual Roads, Smarter Safety: A Digital Twin Framework for Mixed Autonomous Traffic Safety Analysis","abstract":"This paper presents a digital-twin platform for active safety analysis in mixed traffic environments. The platform is built using a multi-modal data-enabled traffic environment constructed from drone-based aerial LiDAR, OpenStreetMap, and vehicle sensor data (e.g., GPS and inclinometer readings). High-resolution 3D road geometries are generated through AI-powered semantic segmentation and georeferencing of aerial LiDAR data. To simulate real-world driving scenarios, the platform integrates the CAR Learning to Act (CARLA) simulator, Simulation of Urban MObility (SUMO) traffic model, and NVIDIA PhysX vehicle dynamics engine. CARLA provides detailed micro-level sensor and perception data, while SUMO manages macro-level traffic flow. NVIDIA PhysX enables accurate modeling of vehicle behaviors under diverse conditions, accounting for mass distribution, tire friction, and center of mass. This integrated system supports high-fidelity simulations that capture the complex interactions between autonomous and conventional vehicles. Experimental results demonstrate the platform's ability to reproduce realistic vehicle dynamics and traffic scenarios, enhancing the analysis of active safety measures. Overall, the proposed framework advances traffic safety research by enabling in-depth, physics-informed evaluation of vehicle behavior in dynamic and heterogeneous traffic environments.","authors":["Hao Zhang","Ximin Yue","Kexin Tian","Sixu Li","Keshu Wu","Zihao Li","Dominique Lord","Yang Zhou"],"url":"https://arxiv.org/abs/2504.17968"}
{"created":"2025-04-28","title":"Mixed Bernstein-Fourier Approximants for Optimal Trajectory Generation with Periodic Behavior","abstract":"Efficient trajectory generation is critical for autonomous systems, yet current numerical methods often struggle to handle periodic behaviors effectively, especially when equidistant time nodes are required. This paper introduces a novel mixed Bernstein-Fourier approximation framework tailored explicitly for optimal motion planning. Our proposed methodology leverages the uniform convergence properties of Bernstein polynomials for nonperiodic behaviors while effectively capturing periodic dynamics through Fourier series. Theoretical results are established, including uniform convergence proofs for approximations of functions, derivatives, and integrals, as well as detailed error bound analyses. We further introduce a regulated least squares approach for determining approximation coefficients, enhancing numerical stability and practical applicability. Within an optimal control context, we establish feasibility and consistency of approximated solutions to their continuous counterparts. We also extend the covector mapping theorem, providing theoretical guarantees for approximating dual variables crucial in verifying the necessary optimality conditions from Pontryagin's Maximum Principle. Comprehensive numerical examples illustrate the method's superior performance, demonstrating substantial improvements in computational efficiency and precision in scenarios with complex periodic constraints and dynamics. Our mixed Bernstein-Fourier methodology thus presents a robust, theoretically grounded, and computationally efficient approach for advanced optimal trajectory planning in autonomous systems.","authors":["Liraz Mudrik","Sean Kragelund","Isaac Kaminer"],"url":"https://arxiv.org/abs/2504.17969"}
{"created":"2025-04-28","title":"Cluster-Aware Attacks on Graph Watermarks","abstract":"Data from domains such as social networks, healthcare, finance, and cybersecurity can be represented as graph-structured information. Given the sensitive nature of this data and their frequent distribution among collaborators, ensuring secure and attributable sharing is essential. Graph watermarking enables attribution by embedding user-specific signatures into graph-structured data. While prior work has addressed random perturbation attacks, the threat posed by adversaries leveraging structural properties through community detection remains unexplored. In this work, we introduce a cluster-aware threat model in which adversaries apply community-guided modifications to evade detection. We propose two novel attack strategies and evaluate them on real-world social network graphs. Our results show that cluster-aware attacks can reduce attribution accuracy by up to 80% more than random baselines under equivalent perturbation budgets on sparse graphs. To mitigate this threat, we propose a lightweight embedding enhancement that distributes watermark nodes across graph communities. This approach improves attribution accuracy by up to 60% under attack on dense graphs, without increasing runtime or structural distortion. Our findings underscore the importance of cluster-topological awareness in both watermarking design and adversarial modeling.","authors":["Alexander Nemecek","Emre Yilmaz","Erman Ayday"],"url":"https://arxiv.org/abs/2504.17971"}
{"created":"2025-04-28","title":"Industrial-Scale Neural Network Clone Detection with Disk-Based Similarity Search","abstract":"Code clones are similar code fragments that often arise from copy-and-paste programming. Neural networks can classify pairs of code fragments as clone/not-clone with high accuracy. However, finding clones in industrial-scale code needs a more scalable approach than pairwise comparison. We extend existing neural network-based clone detection schemes to handle codebases that far exceed available memory, using indexing and search methods for external storage such as disks and solid-state drives. We generate a high-dimensional vector embedding for each code fragment using a transformer-based neural network. We then find similar embeddings using efficient multidimensional nearest neighbor search algorithms on external storage to find similar embeddings without pairwise comparison. We identify specific problems with industrial-scale code bases, such as large sets of almost identical code fragments that interact poorly with $k$-nearest neighbour search algorithms, and provide an effective solution. We demonstrate that our disk-based clone search approach achieves similar clone detection accuracy as an equivalent in-memory technique. Using a solid-state drive as external storage, our approach is around 2$\\times$ slower than the in-memory approach for a problem size that can fit within memory. We further demonstrate that our approach can scale to over a billion lines of code, providing valuable insights into the trade-offs between indexing speed, query performance, and storage efficiency for industrial-scale code clone detection.","authors":["Gul Aftab Ahmed","Muslim Chochlov","Abdul Razzaq","James Vincent Patten","Yuanhua Han","Guoxian Lu","Jim Buckley","David Gregg"],"url":"https://arxiv.org/abs/2504.17972"}
{"created":"2025-04-28","title":"Toward Low-Latency Services over PON using OCDMA Private Networks","abstract":"An low-latency service scheme is proposed over Passive Optical Network (PON). The Optical Code Division Multiplexing Access (OCDMA) technique is used to define multiple private networks serving as Virtual GE-PON that mimic the service-based VLAN (S-VLAN) in the optical domain.","authors":["Steevy J. Cordette"],"url":"https://arxiv.org/abs/2504.17973"}
{"created":"2025-04-28","title":"Optimism, Expectation, or Sarcasm? Multi-Class Hope Speech Detection in Spanish and English","abstract":"Hope is a complex and underexplored emotional state that plays a significant role in education, mental health, and social interaction. Unlike basic emotions, hope manifests in nuanced forms ranging from grounded optimism to exaggerated wishfulness or sarcasm, making it difficult for Natural Language Processing systems to detect accurately. This study introduces PolyHope V2, a multilingual, fine-grained hope speech dataset comprising over 30,000 annotated tweets in English and Spanish. This resource distinguishes between four hope subtypes Generalized, Realistic, Unrealistic, and Sarcastic and enhances existing datasets by explicitly labeling sarcastic instances. We benchmark multiple pretrained transformer models and compare them with large language models (LLMs) such as GPT 4 and Llama 3 under zero-shot and few-shot regimes. Our findings show that fine-tuned transformers outperform prompt-based LLMs, especially in distinguishing nuanced hope categories and sarcasm. Through qualitative analysis and confusion matrices, we highlight systematic challenges in separating closely related hope subtypes. The dataset and results provide a robust foundation for future emotion recognition tasks that demand greater semantic and contextual sensitivity across languages.","authors":["Sabur Butt","Fazlourrahman Balouchzahi","Ahmad Imam Amjad","Maaz Amjad","Hector G. Ceballos","Salud Maria Jimenez-Zafra"],"url":"https://arxiv.org/abs/2504.17974"}
{"created":"2025-04-28","title":"From Bugs to Benchmarks: A Comprehensive Survey of Software Defect Datasets","abstract":"Software defect datasets, which are collections of software bugs and their associated information, are essential resources for researchers and practitioners in software engineering and beyond. Such datasets facilitate empirical research and enable standardized benchmarking for a wide range of techniques, including fault detection, fault localization, test generation, test prioritization, automated program repair, and emerging areas like agentic AI-based software development. Over the years, numerous software defect datasets with diverse characteristics have been developed, providing rich resources for the community, yet making it increasingly difficult to navigate the landscape. To address this challenge, this article provides a comprehensive survey of 132 software defect datasets. The survey discusses the scope of existing datasets, e.g., regarding the application domain of the buggy software, the types of defects, and the programming languages used. We also examine the construction of these datasets, including the data sources and construction methods employed. Furthermore, we assess the availability and usability of the datasets, validating their availability and examining how defects are presented. To better understand the practical uses of these datasets, we analyze the publications that cite them, revealing that the primary use cases are evaluations of new techniques and empirical research. Based on our comprehensive review of the existing datasets, this paper suggests potential opportunities for future research, including addressing underrepresented kinds of defects, enhancing availability and usability through better dataset organization, and developing more efficient strategies for dataset construction and maintenance.","authors":["Hao-Nan Zhu","Robert M. Furth","Michael Pradel","Cindy Rubio-Gonz\\'alez"],"url":"https://arxiv.org/abs/2504.17977"}
{"created":"2025-04-28","title":"Fuzzy-RRT for Obstacle Avoidance in a 2-DOF Semi-Autonomous Surgical Robotic Arm","abstract":"AI-driven semi-autonomous robotic surgery is essential for addressing the medical challenges of long-duration interplanetary missions, where limited crew sizes and communication delays restrict traditional surgical approaches. Current robotic surgery systems require full surgeon control, demanding extensive expertise and limiting feasibility in space. We propose a novel adaptation of the Fuzzy Rapidly-exploring Random Tree algorithm for obstacle avoidance and collaborative control in a two-degree-of-freedom robotic arm modeled on the Miniaturized Robotic-Assisted surgical system. It was found that the Fuzzy Rapidly-exploring Random Tree algorithm resulted in an 743 percent improvement to path search time and 43 percent improvement to path cost.","authors":["Kaaustaaub Shankar","Wilhelm Louw","Bharadwaj Dogga","Nick Ernest","Tim Arnett","Kelly Cohen"],"url":"https://arxiv.org/abs/2504.17979"}
{"created":"2025-04-28","title":"A Journey of Modern OS Construction From boot to DOOM","abstract":"VOS is a first-of-its-kind instructional OS that: (1) Runs on commodity, portable hardware. (2) Showcases modern features, including per-app address spaces, threading, commodity filesystems, USB, DMA, multicore, self-hosted debugging, and a window manager. (3) Supports rich applications such as 2D/3D games, music and video players, and a blockchain miner. Unlike traditional instructional systems, VOS emphasizes strong motivation for building systems-supporting engaging, media-rich apps that go beyond basic terminal programs. To achieve this, we design VOS to strike a careful balance between essential OS complexity and overall simplicity. Our method, which we call inverse engineering, breaks down a full-featured OS into a set of incremental, self-contained prototypes. Each prototype introduces a minimal set of OS mechanisms, driven by the needs of specific apps. The construction process (i.e., forward engineering) then progressively enables these apps by bringing up one mechanism at a time. VOS makes it accessible for a wider audience to experience building a software system that is self-contained and usable in everyday scenarios.","authors":["Wonkyo Choe","Rongxiang Wang","Afsara Benazir","Felix Xiaozhu Lin"],"url":"https://arxiv.org/abs/2504.17984"}
{"created":"2025-04-28","title":"From Mapping to Composing: A Two-Stage Framework for Zero-shot Composed Image Retrieval","abstract":"Composed Image Retrieval (CIR) is a challenging multimodal task that retrieves a target image based on a reference image and accompanying modification text. Due to the high cost of annotating CIR triplet datasets, zero-shot (ZS) CIR has gained traction as a promising alternative. Existing studies mainly focus on projection-based methods, which map an image to a single pseudo-word token. However, these methods face three critical challenges: (1) insufficient pseudo-word token representation capacity, (2) discrepancies between training and inference phases, and (3) reliance on large-scale synthetic data. To address these issues, we propose a two-stage framework where the training is accomplished from mapping to composing. In the first stage, we enhance image-to-pseudo-word token learning by introducing a visual semantic injection module and a soft text alignment objective, enabling the token to capture richer and fine-grained image information. In the second stage, we optimize the text encoder using a small amount of synthetic triplet data, enabling it to effectively extract compositional semantics by combining pseudo-word tokens with modification text for accurate target image retrieval. The strong visual-to-pseudo mapping established in the first stage provides a solid foundation for the second stage, making our approach compatible with both high- and low-quality synthetic data, and capable of achieving significant performance gains with only a small amount of synthetic data. Extensive experiments were conducted on three public datasets, achieving superior performance compared to existing approaches.","authors":["Yabing Wang","Zhuotao Tian","Qingpei Guo","Zheng Qin","Sanping Zhou","Ming Yang","Le Wang"],"url":"https://arxiv.org/abs/2504.17990"}
{"created":"2025-04-28","title":"RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation","abstract":"Recent image-goal navigation (ImageNav) methods learn a perception-action policy by separately capturing semantic features of the goal and egocentric images, then passing them to a policy network. However, challenges remain: (1) Semantic features often fail to provide accurate directional information, leading to superfluous actions, and (2) performance drops significantly when viewpoint inconsistencies arise between training and application. To address these challenges, we propose RSRNav, a simple yet effective method that reasons spatial relationships between the goal and current observations as navigation guidance. Specifically, we model the spatial relationship by constructing correlations between the goal and current observations, which are then passed to the policy network for action prediction. These correlations are progressively refined using fine-grained cross-correlation and direction-aware correlation for more precise navigation. Extensive evaluation of RSRNav on three benchmark datasets demonstrates superior navigation performance, particularly in the \"user-matched goal\" setting, highlighting its potential for real-world applications.","authors":["Zheng Qin","Le Wang","Yabing Wang","Sanping Zhou","Gang Hua","Wei Tang"],"url":"https://arxiv.org/abs/2504.17991"}
{"created":"2025-04-28","title":"Improving LLM Personas via Rationalization with Psychological Scaffolds","abstract":"Language models prompted with a user description or persona can predict a user's preferences and opinions, but existing approaches to building personas -- based solely on a user's demographic attributes and/or prior judgments -- fail to capture the underlying reasoning behind said user judgments. We introduce PB&amp;J (Psychology of Behavior and Judgments), a framework that improves LLM personas by incorporating rationales of why a user might make specific judgments. These rationales are LLM-generated, and aim to reason about a user's behavior on the basis of their experiences, personality traits or beliefs. This is done using psychological scaffolds -- structured frameworks grounded in theories such as the Big 5 Personality Traits and Primal World Beliefs -- that help provide structure to the generated rationales. Experiments on public opinion and movie preference prediction tasks demonstrate that LLM personas augmented with PB&amp;J rationales consistently outperform methods using only a user's demographics and/or judgments. Additionally, LLM personas constructed using scaffolds describing user beliefs perform competitively with those using human-written rationales.","authors":["Brihi Joshi","Xiang Ren","Swabha Swayamdipta","Rik Koncel-Kedziorski","Tim Paek"],"url":"https://arxiv.org/abs/2504.17993"}
{"created":"2025-04-28","title":"Back to Fundamentals: Low-Level Visual Features Guided Progressive Token Pruning","abstract":"Vision Transformers (ViTs) excel in semantic segmentation but demand significant computation, posing challenges for deployment on resource-constrained devices. Existing token pruning methods often overlook fundamental visual data characteristics. This study introduces 'LVTP', a progressive token pruning framework guided by multi-scale Tsallis entropy and low-level visual features with twice clustering. It integrates high-level semantics and basic visual attributes for precise segmentation. A novel dynamic scoring mechanism using multi-scale Tsallis entropy weighting overcomes limitations of traditional single-parameter entropy. The framework also incorporates low-level feature analysis to preserve critical edge information while optimizing computational cost. As a plug-and-play module, it requires no architectural changes or additional training. Evaluations across multiple datasets show 20%-45% computational reductions with negligible performance loss, outperforming existing methods in balancing cost and accuracy, especially in complex edge regions.","authors":["Yuanbing Ouyang","Yizhuo Liang","Qingpeng Li","Xinfei Guo","Yiming Luo","Di Wu","Hao Wang","Yushan Pan"],"url":"https://arxiv.org/abs/2504.17996"}
{"created":"2025-04-28","title":"Chatperone: An LLM-Based Negotiable Scaffolding System for Mediating Adolescent Mobile Interactions","abstract":"Adolescents' uncontrolled exposure to digital content can negatively impact their development. Traditional regulatory methods, such as time limits or app restrictions, often take a rigid approach, ignoring adolescents' decision-making abilities. Another issue is the lack of content and services tailored for adolescents. To address this, we propose Chatperone, a concept of a system that provides adaptive scaffolding to support adolescents. Chatperone fosters healthy mobile interactions through three key modules: Perception, Negotiation, and Moderation. This paper outlines these modules' functionalities and discusses considerations for real-world implementation.","authors":["Suwon Yoon","Seungwon Yang","Jeongwon Choi","Wonjeong Park","Inseok Hwang"],"url":"https://arxiv.org/abs/2504.17997"}
{"created":"2025-04-28","title":"Streaming, Fast and Slow: Cognitive Load-Aware Streaming for Efficient LLM Serving","abstract":"Generative conversational interfaces powered by large language models (LLMs) typically stream output token-by-token at a rate determined by computational budget, often neglecting actual human reading speeds and the cognitive load associated with the content. This mismatch frequently leads to inefficient use of computational resources. For example, in cloud-based services, streaming content faster than users can read appears unnecessary, resulting in wasted computational resources and potential delays for other users, particularly during peak usage periods. To address this issue, we propose an adaptive streaming method that dynamically adjusts the pacing of LLM streaming output in real-time based on inferred cognitive load. Our approach estimates the cognitive load associated with streaming content and strategically slows down the stream during complex or information-rich segments, thereby freeing computational resources for other users. Our statistical analysis of computational savings, combined with crowdsourced user studies, provides insights into the trade-offs between service efficiency and user satisfaction, demonstrating that our method can significantly reduce computational consumption up to 16.8\\%. This context-aware computational resource management strategy presents a practical framework for enhancing system efficiency in cloud-based conversational AI interfaces without compromising user experience.","authors":["Chang Xiao","Brenda Yang"],"url":"https://arxiv.org/abs/2504.17999"}
{"created":"2025-04-28","title":"From Cluster to Desktop: A Cache-Accelerated INR framework for Interactive Visualization of Tera-Scale Data","abstract":"Machine learning has enabled the use of implicit neural representations (INRs) to efficiently compress and reconstruct massive scientific datasets. However, despite advances in fast INR rendering algorithms, INR-based rendering remains computationally expensive, as computing data values from an INR is significantly slower than reading them from GPU memory. This bottleneck currently restricts interactive INR visualization to professional workstations. To address this challenge, we introduce an INR rendering framework accelerated by a scalable, multi-resolution GPU cache capable of efficiently representing tera-scale datasets. By minimizing redundant data queries and prioritizing novel volume regions, our method reduces the number of INR computations per frame, achieving an average 5x speedup over the state-of-the-art INR rendering method while still maintaining high visualization quality. Coupled with existing hardware-accelerated INR compressors, our framework enables scientists to generate and compress massive datasets in situ on high-performance computing platforms and then interactively explore them on consumer-grade hardware post hoc.","authors":["Daniel Zavorotny","Qi Wu","David Bauer","Kwan-Liu Ma"],"url":"https://arxiv.org/abs/2504.18001"}
{"created":"2025-04-28","title":"Self-Balancing, Memory Efficient, Dynamic Metric Space Data Maintenance, for Rapid Multi-Kernel Estimation","abstract":"We present a dynamic self-balancing octree data structure that enables efficient neighborhood maintenance in evolving metric spaces, a key challenge in modern machine learning systems. Many learning and generative models operate as dynamical systems whose representations evolve during training, requiring fast, adaptive spatial organization. Our two-parameter octree supports logarithmic-time updates and queries, eliminating the need for costly full rebuilds as data distributions shift. We demonstrate its effectiveness in four areas: (1) accelerating Stein variational gradient descent by supporting more particles with lower overhead; (2) enabling real-time, incremental KNN classification with logarithmic complexity; (3) facilitating efficient, dynamic indexing and retrieval for retrieval-augmented generation; and (4) improving sample efficiency by jointly optimizing input and latent spaces. Across all applications, our approach yields exponential speedups while preserving accuracy, particularly in high-dimensional spaces where maintaining adaptive spatial structure is critical.","authors":["Aditya S Ellendula","Chandrajit Bajaj"],"url":"https://arxiv.org/abs/2504.18003"}
{"created":"2025-04-28","title":"Differential Privacy-Driven Framework for Enhancing Heart Disease Prediction","abstract":"With the rapid digitalization of healthcare systems, there has been a substantial increase in the generation and sharing of private health data. Safeguarding patient information is essential for maintaining consumer trust and ensuring compliance with legal data protection regulations. Machine learning is critical in healthcare, supporting personalized treatment, early disease detection, predictive analytics, image interpretation, drug discovery, efficient operations, and patient monitoring. It enhances decision-making, accelerates research, reduces errors, and improves patient outcomes. In this paper, we utilize machine learning methodologies, including differential privacy and federated learning, to develop privacy-preserving models that enable healthcare stakeholders to extract insights without compromising individual privacy. Differential privacy introduces noise to data to guarantee statistical privacy, while federated learning enables collaborative model training across decentralized datasets. We explore applying these technologies to Heart Disease Data, demonstrating how they preserve privacy while delivering valuable insights and comprehensive analysis. Our results show that using a federated learning model with differential privacy achieved a test accuracy of 85%, ensuring patient data remained secure and private throughout the process.","authors":["Yazan Otoum","Amiya Nayak"],"url":"https://arxiv.org/abs/2504.18007"}
{"created":"2025-04-28","title":"TGDT: A Temporal Graph-based Digital Twin for Urban Traffic Corridors","abstract":"Urban congestion at signalized intersections leads to significant delays, economic losses, and increased emissions. Existing deep learning models often lack spatial generalizability, rely on complex architectures, and struggle with real-time deployment. To address these limitations, we propose the Temporal Graph-based Digital Twin (TGDT), a scalable framework that integrates Temporal Convolutional Networks and Attentional Graph Neural Networks for dynamic, direction-aware traffic modeling and assessment at urban corridors. TGDT estimates key Measures of Effectiveness (MOEs) for traffic flow optimization at both the intersection level (e.g., queue length, waiting time) and the corridor level (e.g., traffic volume, travel time). Its modular architecture and sequential optimization scheme enable easy extension to any number of intersections and MOEs. The model outperforms state-of-the-art baselines by accurately producing high-dimensional, concurrent multi-output estimates. It also demonstrates high robustness and accuracy across diverse traffic conditions, including extreme scenarios, while relying on only a minimal set of traffic features. Fully parallelized, TGDT can simulate over a thousand scenarios within a matter of seconds, offering a cost-effective, interpretable, and real-time solution for traffic signal optimization.","authors":["Nooshin Yousefzadeh","Rahul Sengupta","Sanjay Ranka"],"url":"https://arxiv.org/abs/2504.18008"}
{"created":"2025-04-28","title":"Sky-Drive: A Distributed Multi-Agent Simulation Platform for Socially-Aware and Human-AI Collaborative Future Transportation","abstract":"Recent advances in autonomous system simulation platforms have significantly enhanced the safe and scalable testing of driving policies. However, existing simulators do not yet fully meet the needs of future transportation research, particularly in modeling socially-aware driving agents and enabling effective human-AI collaboration. This paper introduces Sky-Drive, a novel distributed multi-agent simulation platform that addresses these limitations through four key innovations: (a) a distributed architecture for synchronized simulation across multiple terminals; (b) a multi-modal human-in-the-loop framework integrating diverse sensors to collect rich behavioral data; (c) a human-AI collaboration mechanism supporting continuous and adaptive knowledge exchange; and (d) a digital twin (DT) framework for constructing high-fidelity virtual replicas of real-world transportation environments. Sky-Drive supports diverse applications such as autonomous vehicle (AV)-vulnerable road user (VRU) interaction modeling, human-in-the-loop training, socially-aware reinforcement learning, personalized driving policy, and customized scenario generation. Future extensions will incorporate foundation models for context-aware decision support and hardware-in-the-loop (HIL) testing for real-world validation. By bridging scenario generation, data collection, algorithm training, and hardware integration, Sky-Drive has the potential to become a foundational platform for the next generation of socially-aware and human-centered autonomous transportation research. The demo video and code are available at:https://sky-lab-uw.github.io/Sky-Drive-website/","authors":["Zilin Huang","Zihao Sheng","Zhengyang Wan","Yansong Qu","Yuhao Luo","Boyue Wang","Pei Li","Yen-Jung Chen","Jiancong Chen","Keke Long","Jiayi Meng","Yue Leng","Sikai Chen"],"url":"https://arxiv.org/abs/2504.18010"}
{"created":"2025-04-28","title":"Memory Reviving, Continuing Learning and Beyond: Evaluation of Pre-trained Encoders and Decoders for Multimodal Machine Translation","abstract":"Multimodal Machine Translation (MMT) aims to improve translation quality by leveraging auxiliary modalities such as images alongside textual input. While recent advances in large-scale pre-trained language and vision models have significantly benefited unimodal natural language processing tasks, their effectiveness and role in MMT remain underexplored. In this work, we conduct a systematic study on the impact of pre-trained encoders and decoders in multimodal translation models. Specifically, we analyze how different training strategies, from training from scratch to using pre-trained and partially frozen components, affect translation performance under a unified MMT framework. Experiments are carried out on the Multi30K and CoMMuTE dataset across English-German and English-French translation tasks. Our results reveal that pre-training plays a crucial yet asymmetrical role in multimodal settings: pre-trained decoders consistently yield more fluent and accurate outputs, while pre-trained encoders show varied effects depending on the quality of visual-text alignment. Furthermore, we provide insights into the interplay between modality fusion and pre-trained components, offering guidance for future architecture design in multimodal translation systems.","authors":["Zhuang Yu","Shiliang Sun","Jing Zhao","Tengfei Song","Hao Yang"],"url":"https://arxiv.org/abs/2504.18012"}
{"created":"2025-04-28","title":"Diffusion-Driven Universal Model Inversion Attack for Face Recognition","abstract":"Facial recognition technology poses significant privacy risks, as it relies on biometric data that is inherently sensitive and immutable if compromised. To mitigate these concerns, face recognition systems convert raw images into embeddings, traditionally considered privacy-preserving. However, model inversion attacks pose a significant privacy threat by reconstructing these private facial images, making them a crucial tool for evaluating the privacy risks of face recognition systems. Existing methods usually require training individual generators for each target model, a computationally expensive process. In this paper, we propose DiffUMI, a training-free diffusion-driven universal model inversion attack for face recognition systems. DiffUMI is the first approach to apply a diffusion model for unconditional image generation in model inversion. Unlike other methods, DiffUMI is universal, eliminating the need for training target-specific generators. It operates within a fixed framework and pretrained diffusion model while seamlessly adapting to diverse target identities and models. DiffUMI breaches privacy-preserving face recognition systems with state-of-the-art success, demonstrating that an unconditional diffusion model, coupled with optimized adversarial search, enables efficient and high-fidelity facial reconstruction. Additionally, we introduce a novel application of out-of-domain detection (OODD), marking the first use of model inversion to distinguish non-face inputs from face inputs based solely on embeddings.","authors":["Hanrui Wang","Shuo Wang","Chun-Shien Lu","Isao Echizen"],"url":"https://arxiv.org/abs/2504.18015"}
{"created":"2025-04-28","title":"Federated Client-tailored Adapter for Medical Image Segmentation","abstract":"Medical image segmentation in X-ray images is beneficial for computer-aided diagnosis and lesion localization. Existing methods mainly fall into a centralized learning paradigm, which is inapplicable in the practical medical scenario that only has access to distributed data islands. Federated Learning has the potential to offer a distributed solution but struggles with heavy training instability due to client-wise domain heterogeneity (including distribution diversity and class imbalance). In this paper, we propose a novel Federated Client-tailored Adapter (FCA) framework for medical image segmentation, which achieves stable and client-tailored adaptive segmentation without sharing sensitive local data. Specifically, the federated adapter stirs universal knowledge in off-the-shelf medical foundation models to stabilize the federated training process. In addition, we develop two client-tailored federated updating strategies that adaptively decompose the adapter into common and individual components, then globally and independently update the parameter groups associated with common client-invariant and individual client-specific units, respectively. They further stabilize the heterogeneous federated learning process and realize optimal client-tailored instead of sub-optimal global-compromised segmentation models. Extensive experiments on three large-scale datasets demonstrate the effectiveness and superiority of the proposed FCA framework for federated medical segmentation.","authors":["Guyue Hu","Siyuan Song","Yukun Kang","Zhu Yin","Gangming Zhao","Chenglong Li","Jin Tang"],"url":"https://arxiv.org/abs/2504.18020"}
{"created":"2025-04-28","title":"Iterative Joint Detection of Kalman Filter and Channel Decoder for Sensor-to-Controller Link in Wireless Networked Control Systems","abstract":"In this letter, we propose an iterative joint detection algorithm of Kalman filter (KF) and channel decoder for the sensor-to-controller link of wireless networked control systems, which utilizes the prior information of control system to improve the control and communication performance. In the algorithm, we first use the KF to estimate the probability density of the control system outputs and calculate the prior probability of received signals to assist decoding. Then, the possible outputs of the control system are traversed to update the prior probability in order to implement iterative detection. The simulation results show that the prior information can reduce the block error rate performance of communications to improve the root mean square error performance of controls.","authors":["Jinnan Piao","Dong Li","Yiming Sun","Zhibo Li","Ming Yang","Xueting Yu"],"url":"https://arxiv.org/abs/2504.18022"}
{"created":"2025-04-28","title":"SMARTFinRAG: Interactive Modularized Financial RAG Benchmark","abstract":"Financial sectors are rapidly adopting language model technologies, yet evaluating specialized RAG systems in this domain remains challenging. This paper introduces SMARTFinRAG, addressing three critical gaps in financial RAG assessment: (1) a fully modular architecture where components can be dynamically interchanged during runtime; (2) a document-centric evaluation paradigm generating domain-specific QA pairs from newly ingested financial documents; and (3) an intuitive interface bridging research-implementation divides. Our evaluation quantifies both retrieval efficacy and response quality, revealing significant performance variations across configurations. The platform's open-source architecture supports transparent, reproducible research while addressing practical deployment challenges faced by financial institutions implementing RAG systems.","authors":["Yiwei Zha"],"url":"https://arxiv.org/abs/2504.18024"}
{"created":"2025-04-28","title":"ShapeSpeak: Body Shape-Aware Textual Alignment for Visible-Infrared Person Re-Identification","abstract":"Visible-Infrared Person Re-identification (VIReID) aims to match visible and infrared pedestrian images, but the modality differences and the complexity of identity features make it challenging. Existing methods rely solely on identity label supervision, which makes it difficult to fully extract high-level semantic information. Recently, vision-language pre-trained models have been introduced to VIReID, enhancing semantic information modeling by generating textual descriptions. However, such methods do not explicitly model body shape features, which are crucial for cross-modal matching. To address this, we propose an effective Body Shape-aware Textual Alignment (BSaTa) framework that explicitly models and utilizes body shape information to improve VIReID performance. Specifically, we design a Body Shape Textual Alignment (BSTA) module that extracts body shape information using a human parsing model and converts it into structured text representations via CLIP. We also design a Text-Visual Consistency Regularizer (TVCR) to ensure alignment between body shape textual representations and visual body shape features. Furthermore, we introduce a Shape-aware Representation Learning (SRL) mechanism that combines Multi-text Supervision and Distribution Consistency Constraints to guide the visual encoder to learn modality-invariant and discriminative identity features, thus enhancing modality invariance. Experimental results demonstrate that our method achieves superior performance on the SYSU-MM01 and RegDB datasets, validating its effectiveness.","authors":["Shuanglin Yan","Neng Dong","Shuang Li","Rui Yan","Hao Tang","Jing Qin"],"url":"https://arxiv.org/abs/2504.18025"}
{"created":"2025-04-28","title":"Addressing Concept Mislabeling in Concept Bottleneck Models Through Preference Optimization","abstract":"Concept Bottleneck Models (CBMs) propose to enhance the trustworthiness of AI systems by constraining their decisions on a set of human understandable concepts. However, CBMs typically assume that datasets contains accurate concept labels an assumption often violated in practice, which we show can significantly degrade performance (by 25% in some cases). To address this, we introduce the Concept Preference Optimization (CPO) objective, a new loss function based on Direct Preference Optimization, which effectively mitigates the negative impact of concept mislabeling on CBM performance. We provide an analysis on some key properties of the CPO objective showing it directly optimizes for the concept's posterior distribution, and contrast it against Binary Cross Entropy (BCE) where we show CPO is inherently less sensitive to concept noise. We empirically confirm our analysis finding that CPO consistently outperforms BCE in three real world datasets with and without added label noise.","authors":["Emiliano Penaloza","Tianyue H. Zhan","Laurent Charlin","Mateo Espinosa Zarlenga"],"url":"https://arxiv.org/abs/2504.18026"}
{"created":"2025-04-28","title":"A Large Vision-Language Model based Environment Perception System for Visually Impaired People","abstract":"It is a challenging task for visually impaired people to perceive their surrounding environment due to the complexity of the natural scenes. Their personal and social activities are thus highly limited. This paper introduces a Large Vision-Language Model(LVLM) based environment perception system which helps them to better understand the surrounding environment, by capturing the current scene they face with a wearable device, and then letting them retrieve the analysis results through the device. The visually impaired people could acquire a global description of the scene by long pressing the screen to activate the LVLM output, retrieve the categories of the objects in the scene resulting from a segmentation model by tapping or swiping the screen, and get a detailed description of the objects they are interested in by double-tapping the screen. To help visually impaired people more accurately perceive the world, this paper proposes incorporating the segmentation result of the RGB image as external knowledge into the input of LVLM to reduce the LVLM's hallucination. Technical experiments on POPE, MME and LLaVA-QA90 show that the system could provide a more accurate description of the scene compared to Qwen-VL-Chat, exploratory experiments show that the system helps visually impaired people to perceive the surrounding environment effectively.","authors":["Zezhou Chen","Zhaoxiang Liu","Kai Wang","Kohou Wang","Shiguo Lian"],"url":"https://arxiv.org/abs/2504.18027"}
{"created":"2025-04-28","title":"Integrating Explainable AI for Energy Efficient Open Radio Access Networks","abstract":"The Open Radio Access Network (Open RAN) is an emerging idea -- transforming the traditional Radio Access Networks (RAN) that are monolithic and inflexible into more flexible and innovative. By leveraging open standard interfaces, data collection across all RAN layers becomes feasible, paving the way for the development of energy-efficient Open RAN architectures through Artificial Intelligence / Machine Learning (AI/ML). However, the inherent complexity and black-box nature of AI/ML models used for energy consumption prediction pose challenges in interpreting their underlying factors and relationships. This work presents an integration of eXplainable AI (XAI) to understand the key RAN parameters that contribute to energy consumption. Furthermore, the paper delves into the analysis of RAN parameters -- \\emph{airtime}, \\emph{goodput}, \\emph{throughput}, \\emph{buffer status report}, \\emph{number of resource blocks}, and many others -- identified by XAI techniques, highlighting their significance in energy consumption.","authors":["L. Malakalapalli","V. Gudepu","B. Chirumamilla","S. J. Yadhunandan","K. Kondepu"],"url":"https://arxiv.org/abs/2504.18029"}
{"created":"2025-04-28","title":"Joint Resource Estimation and Trajectory Optimization for eVTOL-involved CR network: A Monte Carlo Tree Search-based Approach","abstract":"Electric Vertical Take-Off and Landing (eVTOL) aircraft, pivotal to Advanced Air Mobility (AAM), are emerging as a transformative transportation paradigm with the potential to redefine urban and regional mobility. While these systems offer unprecedented efficiency in transporting people and goods, they rely heavily on computation capability, safety-critical operations such as real-time navigation, environmental sensing, and trajectory tracking--necessitating robust offboard computational support. A widely adopted solution involves offloading these tasks to terrestrial base stations (BSs) along the flight path. However, air-to-ground connectivity is often constrained by spectrum conflicts with terrestrial users, which poses a significant challenge to maintaining reliable task execution. Cognitive radio (CR) techniques offer promising capabilities for dynamic spectrum access, making them a natural fit for addressing this issue. Existing studies often overlook the time-varying nature of BS resources, such as spectrum availability and CPU cycles, which leads to inaccurate trajectory planning, suboptimal offloading success rates, excessive energy consumption, and operational delays. To address these challenges, we propose a trajectory optimization framework for eVTOL swarms that maximizes task offloading success probability while minimizing both energy consumption and resource competition (e.g., spectrum and CPU cycles) with primary terrestrial users. The proposed algorithm integrates a Multi-Armed Bandit (MAB) model to dynamically estimate BS resource availability and a Monte Carlo Tree Search (MCTS) algorithm to determine optimal offloading decisions, selecting both the BSs and access time windows that align with energy and temporal constraints.","authors":["Kai Xiong","Chenxin Yang","Yujie Qin","Chau Yuen"],"url":"https://arxiv.org/abs/2504.18031"}
{"created":"2025-04-28","title":"Enhancing Privacy-Utility Trade-offs to Mitigate Memorization in Diffusion Models","abstract":"Text-to-image diffusion models have demonstrated remarkable capabilities in creating images highly aligned with user prompts, yet their proclivity for memorizing training set images has sparked concerns about the originality of the generated images and privacy issues, potentially leading to legal complications for both model owners and users, particularly when the memorized images contain proprietary content. Although methods to mitigate these issues have been suggested, enhancing privacy often results in a significant decrease in the utility of the outputs, as indicated by text-alignment scores. To bridge the research gap, we introduce a novel method, PRSS, which refines the classifier-free guidance approach in diffusion models by integrating prompt re-anchoring (PR) to improve privacy and incorporating semantic prompt search (SS) to enhance utility. Extensive experiments across various privacy levels demonstrate that our approach consistently improves the privacy-utility trade-off, establishing a new state-of-the-art.","authors":["Chen Chen","Daochang Liu","Mubarak Shah","Chang Xu"],"url":"https://arxiv.org/abs/2504.18032"}
{"created":"2025-04-28","title":"Real-time inversion of two-dimensional Fresnel experimental database using orthogonality sampling method with single and multiple sources: the case of transverse electric polarized waves","abstract":"This paper concerns an application of the orthogonality sampling method (OSM) for a real-time identification of small objects from two-dimensional Fresnel experimental dataset in transverse electric polarization. First, we apply the OSM with a single source by designing an indicator function based on the asymptotic expansion formula for the scattered field in the presence of small objects. We demonstrate that the indicator function can be expressed by an infinite series of Bessel functions of integer order of the first kind, the range of the signal receiver, and the location of the emitter. Based on this, we then investigate the applicability and limitations of the designed OSM. Specifically, we find that the imaging performance is strongly dependent on the source and the applied frequency. We then apply the OSM with multiple sources to improve imaging performance. Based on the identified structure of the OSM with a single source, we design an indicator function with multiple sources and demonstrate that it can be expressed by an infinite series of the Bessel function of integer order of the first kind, and we explain that objects can be identified uniquely using the designed OSM. Numerical simulation results obtained with the Fresnel experimental dataset demonstrate the advantages and disadvantages of the OSM with a single source and confirm that the designed OSM with multiple sources improves imaging performance.","authors":["Junyong Eom","Sangwoo Kang","Minyeob Lee","Won-Kwang Park"],"url":"https://arxiv.org/abs/2504.18033"}
{"created":"2025-04-28","title":"Direct sampling method to retrieve small objects from two-dimensional limited-aperture scattered field data","abstract":"In this study, we investigated the application of the direct sampling method (DSM) to identify small dielectric objects in a limited-aperture inverse scattering problem. Unlike previous studies, we consider the bistatic measurement configuration corresponding to the transmitter location and design indicator functions for both a single source and multiple sources, and we convert the unknown measurement data to a fixed nonzero constant. To explain the applicability and limitation of object detection, we demonstrate that the indicator functions can be expressed by an infinite series of Bessel functions, the material properties of the objects, the bistatic angle, and the converted constant. Based on the theoretical results, we explain how the imaging performance of the DSM is influenced by the bistatic angle and the converted constant. In addition, the results of our analyses demonstrate that a smaller bistatic angle enhances the imaging accuracy and that optimal selection of the converted constant is crucial to realize reliable object detection. The results of the numerical simulations obtained using a two-dimensional Fresnel dataset validated the theoretical findings and illustrate the effectiveness and limitations of the designed indicator functions for small objects.","authors":["Won-Kwang Park"],"url":"https://arxiv.org/abs/2504.18036"}
{"created":"2025-04-28","title":"Complexity and Approximation Algorithms for Fixed Charge Transportation Problems","abstract":"The Fixed Charge Transportation (FCT) problem models transportation scenarios where we need to send a commodity from $n$ sources to $m$ sinks, and the cost of sending a commodity from a source to a sink consists of a linear component and a fixed component. Despite extensive research on exponential time exact algorithms and heuristic algorithms for FCT and its variants, their approximability and computational complexity are not well understood.","authors":["Yong Chen","Shi Li","Zihao Liang"],"url":"https://arxiv.org/abs/2504.18037"}
{"created":"2025-04-28","title":"Optimal Secure Coded Distributed Computation over all Fields","abstract":"We construct optimal secure coded distributed schemes that extend the known optimal constructions over fields of characteristic 0 to all fields. A serendipitous result is that we can encode \\emph{all} functions over finite fields with a recovery threshold proportional to the complexity (tensor rank or multiplicative); this is due to the well-known result that all functions over a finite field can be represented as multivariate polynomials (or symmetric tensors). We get that a tensor of order $\\ell$ (or a multivariate polynomial of degree $\\ell$) can be computed in the faulty network of $N$ nodes setting within a factor of $\\ell$ and an additive term depending on the genus of a code with $N$ rational points and distance covering the number of faulty servers; in particular, we present a coding scheme for general matrix multiplication of two $m \\times m $ matrices with a recovery threshold of $2 m^{\\omega } -1+g$ where $\\omega $ is the exponent of matrix multiplication which is optimal for coding schemes using AG codes. Moreover, we give sufficient conditions for which the Hadamard-Shur product of general linear codes gives a similar recovery threshold, which we call \\textit{log-additive codes}. Finally, we show that evaluation codes with a \\textit{curve degree} function (first defined in [Ben-Sasson et al. (STOC '13)]) that have well-behaved zero sets are log-additive.","authors":["Pedro Soto"],"url":"https://arxiv.org/abs/2504.18038"}
{"created":"2025-04-28","title":"MultiMind: Enhancing Werewolf Agents with Multimodal Reasoning and Theory of Mind","abstract":"Large Language Model (LLM) agents have demonstrated impressive capabilities in social deduction games (SDGs) like Werewolf, where strategic reasoning and social deception are essential. However, current approaches remain limited to textual information, ignoring crucial multimodal cues such as facial expressions and tone of voice that humans naturally use to communicate. Moreover, existing SDG agents primarily focus on inferring other players' identities without modeling how others perceive themselves or fellow players. To address these limitations, we use One Night Ultimate Werewolf (ONUW) as a testbed and present MultiMind, the first framework integrating multimodal information into SDG agents. MultiMind processes facial expressions and vocal tones alongside verbal content, while employing a Theory of Mind (ToM) model to represent each player's suspicion levels toward others. By combining this ToM model with Monte Carlo Tree Search (MCTS), our agent identifies communication strategies that minimize suspicion directed at itself. Through comprehensive evaluation in both agent-versus-agent simulations and studies with human players, we demonstrate MultiMind's superior performance in gameplay. Our work presents a significant advancement toward LLM agents capable of human-like social reasoning across multimodal domains.","authors":["Zheng Zhang","Nuoqian Xiao","Qi Chai","Deheng Ye","Hao Wang"],"url":"https://arxiv.org/abs/2504.18039"}
{"created":"2025-04-28","title":"Cabbage: A Differential Growth Framework for Open Surfaces","abstract":"We propose Cabbage, a differential growth framework to model buckling behavior in 3D open surfaces found in nature-like the curling of flower petals. Cabbage creates high-quality triangular meshes free of self-intersection. Cabbage-Shell is driven by edge subdivision which differentially increases discretization resolution. Shell forces expands the surface, generating buckling over time. Feature-aware smoothing and remeshing ensures mesh quality. Corrective collision effectively prevents self-collision even in tight spaces. We additionally provide Cabbage-Collision, and approximate alternative, followed by CAD-ready surface generation. Cabbage is the first open-source effort with this calibre and robustness, outperforming SOTA methods in its morphological expressiveness, mesh quality, and stably generates large, complex patterns over hundreds of simulation steps. It is a source not only of computational modeling, digital fabrication, education, but also high-quality, annotated data for geometry processing and shape analysis.","authors":["Xiaoyi Liu","Hao Tang"],"url":"https://arxiv.org/abs/2504.18040"}
{"created":"2025-04-28","title":"RAG LLMs are Not Safer: A Safety Analysis of Retrieval-Augmented Generation for Large Language Models","abstract":"Efforts to ensure the safety of large language models (LLMs) include safety fine-tuning, evaluation, and red teaming. However, despite the widespread use of the Retrieval-Augmented Generation (RAG) framework, AI safety work focuses on standard LLMs, which means we know little about how RAG use cases change a model's safety profile. We conduct a detailed comparative analysis of RAG and non-RAG frameworks with eleven LLMs. We find that RAG can make models less safe and change their safety profile. We explore the causes of this change and find that even combinations of safe models with safe documents can cause unsafe generations. In addition, we evaluate some existing red teaming methods for RAG settings and show that they are less effective than when used for non-RAG settings. Our work highlights the need for safety research and red-teaming methods specifically tailored for RAG LLMs.","authors":["Bang An","Shiyue Zhang","Mark Dredze"],"url":"https://arxiv.org/abs/2504.18041"}
{"created":"2025-04-28","title":"AI Ethics and Social Norms: Exploring ChatGPT's Capabilities From What to How","abstract":"Using LLMs in healthcare, Computer-Supported Cooperative Work, and Social Computing requires the examination of ethical and social norms to ensure safe incorporation into human life. We conducted a mixed-method study, including an online survey with 111 participants and an interview study with 38 experts, to investigate the AI ethics and social norms in ChatGPT as everyday life tools. This study aims to evaluate whether ChatGPT in an empirical context operates following ethics and social norms, which is critical for understanding actions in industrial and academic research and achieving machine ethics. The findings of this study provide initial insights into six important aspects of AI ethics, including bias, trustworthiness, security, toxicology, social norms, and ethical data. Significant obstacles related to transparency and bias in unsupervised data collection methods are identified as ChatGPT's ethical concerns.","authors":["Omid Veisi","Sasan Bahrami","Roman Englert","Claudia M\\\"uller"],"url":"https://arxiv.org/abs/2504.18044"}
{"created":"2025-04-28","title":"DMS-Net:Dual-Modal Multi-Scale Siamese Network for Binocular Fundus Image Classification","abstract":"Ophthalmic diseases pose a significant global health challenge, yet traditional diagnosis methods and existing single-eye deep learning approaches often fail to account for binocular pathological correlations. To address this, we propose DMS-Net, a dual-modal multi-scale Siamese network for binocular fundus image classification. Our framework leverages weight-shared Siamese ResNet-152 backbones to extract deep semantic features from paired fundus images. To tackle challenges such as lesion boundary ambiguity and scattered pathological distributions, we introduce a Multi-Scale Context-Aware Module (MSCAM) that integrates adaptive pooling and attention mechanisms for multi-resolution feature aggregation. Additionally, a Dual-Modal Feature Fusion (DMFF) module enhances cross-modal interaction through spatial-semantic recalibration and bidirectional attention, effectively combining global context and local edge features. Evaluated on the ODIR-5K dataset, DMS-Net achieves state-of-the-art performance with 80.5% accuracy, 86.1% recall, and 83.8% Cohen's kappa, demonstrating superior capability in detecting symmetric pathologies and advancing clinical decision-making for ocular diseases.","authors":["Guohao Huo","Zibo Lin","Zitong Wang","Ruiting Dai","Hao Tang"],"url":"https://arxiv.org/abs/2504.18046"}
{"created":"2025-04-28","title":"Spatiotemporal Analysis of Parallelized Computing at the Extreme Edge","abstract":"Extreme Edge Computing (EEC) pushes computing even closer to end users than traditional Multi-access Edge Computing (MEC), harnessing the idle resources of Extreme Edge Devices (EEDs) to enable low-latency, distributed processing. However, EEC faces key challenges, including spatial randomness in device distribution, limited EED computational power necessitating parallel task execution, vulnerability to failure, and temporal randomness due to variability in wireless communication and execution times. These challenges highlight the need for a rigorous analytical framework to evaluate EEC performance. We present the first spatiotemporal mathematical model for EEC over large-scale millimeter-wave networks. Utilizing stochastic geometry and an Absorbing Continuous-Time Markov Chain (ACTMC), the framework captures the complex interaction between communication and computation performance, including their temporal overlap during parallel execution. We evaluate two key metrics: average task response delay and task completion probability. Together, they provide a holistic view of latency and reliability. The analysis considers fundamental offloading strategies, including randomized and location-aware schemes, while accounting for EED failures. Results show that there exists an optimal task segmentation that minimizes delay. Under limited EED availability, we investigate a bias-based EEC and MEC collaboration that offloads excess demand to MEC resources, effectively reducing congestion and improving system responsiveness.","authors":["Yasser Nabil","Mahmoud Abdelhadi","Sameh Sorour","Hesham ElSawy","Sara A. Elsayed","Hossam S. Hassanein"],"url":"https://arxiv.org/abs/2504.18047"}
{"created":"2025-04-28","title":"Modes of Sequence Models and Learning Coefficients","abstract":"We develop a geometric account of sequence modelling that links patterns in the data to measurable properties of the loss landscape in transformer networks. First, we cast conditional sequence distributions into a Hilbert-space framework and apply tensor decompositions to identify their principal modes. Truncating the small-amplitude modes yields an effective data distribution that preserves dominant structure while discarding statistical detail. Second, we show theoretically that Local Learning Coefficient (LLC) estimates are insensitive to modes below a data-dependent threshold. Consequently, the LLC calculated in practice characterises the geometry of the effective rather than the true distribution. This insight clarifies why reliable LLC estimates can be obtained even when a network parameter is not a strict minimiser of the population loss, and it highlights how the inverse temperature in SGLD acts as a resolution dial on the landscape structure.","authors":["Zhongtian Chen","Daniel Murfet"],"url":"https://arxiv.org/abs/2504.18048"}
{"created":"2025-04-28","title":"A BERT-Style Self-Supervised Learning CNN for Disease Identification from Retinal Images","abstract":"In the field of medical imaging, the advent of deep learning, especially the application of convolutional neural networks (CNNs) has revolutionized the analysis and interpretation of medical images. Nevertheless, deep learning methods usually rely on large amounts of labeled data. In medical imaging research, the acquisition of high-quality labels is both expensive and difficult. The introduction of Vision Transformers (ViT) and self-supervised learning provides a pre-training strategy that utilizes abundant unlabeled data, effectively alleviating the label acquisition challenge while broadening the breadth of data utilization. However, ViT's high computational density and substantial demand for computing power, coupled with the lack of localization characteristics of its operations on image patches, limit its efficiency and applicability in many application scenarios. In this study, we employ nn-MobileNet, a lightweight CNN framework, to implement a BERT-style self-supervised learning approach. We pre-train the network on the unlabeled retinal fundus images from the UK Biobank to improve downstream application performance. We validate the results of the pre-trained model on Alzheimer's disease (AD), Parkinson's disease (PD), and various retinal diseases identification. The results show that our approach can significantly improve performance in the downstream tasks. In summary, this study combines the benefits of CNNs with the capabilities of advanced self-supervised learning in handling large-scale unlabeled data, demonstrating the potential of CNNs in the presence of label scarcity.","authors":["Xin Li","Wenhui Zhu","Peijie Qiu","Oana M. Dumitrascu","Amal Youssef","Yalin Wang"],"url":"https://arxiv.org/abs/2504.18049"}
{"created":"2025-04-28","title":"Validating Network Protocol Parsers with Traceable RFC Document Interpretation","abstract":"Validating the correctness of network protocol implementations is highly challenging due to the oracle and traceability problems. The former determines when a protocol implementation can be considered buggy, especially when the bugs do not cause any observable symptoms. The latter allows developers to understand how an implementation violates the protocol specification, thereby facilitating bug fixes. Unlike existing works that rarely take both problems into account, this work considers both and provides an effective solution using recent advances in large language models (LLMs). Our key observation is that network protocols are often released with structured specification documents, a.k.a. RFC documents, which can be systematically translated to formal protocol message specifications via LLMs. Such specifications, which may contain errors due to the hallucination of LLMs, are used as a quasi-oracle to validate protocol parsers, while the validation results in return gradually refine the oracle. Since the oracle is derived from the document, any bugs we find in a protocol implementation can be traced back to the document, thus addressing the traceability problem. We have extensively evaluated our approach using nine network protocols and their implementations written in C, Python, and Go. The results show that our approach outperforms the state-of-the-art and has detected 69 bugs, with 36 confirmed. The project also demonstrates the potential for fully automating software validation based on natural language specifications, a process previously considered predominantly manual due to the need to understand specification documents and derive expected outputs for test inputs.","authors":["Mingwei Zheng","Danning Xie","Qingkai Shi","Chengpeng Wang","Xiangyu Zhang"],"url":"https://arxiv.org/abs/2504.18050"}
{"created":"2025-04-28","title":"DREAM: Disentangling Risks to Enhance Safety Alignment in Multimodal Large Language Models","abstract":"Multimodal Large Language Models (MLLMs) pose unique safety challenges due to their integration of visual and textual data, thereby introducing new dimensions of potential attacks and complex risk combinations. In this paper, we begin with a detailed analysis aimed at disentangling risks through step-by-step reasoning within multimodal inputs. We find that systematic multimodal risk disentanglement substantially enhances the risk awareness of MLLMs. Via leveraging the strong discriminative abilities of multimodal risk disentanglement, we further introduce \\textbf{DREAM} (\\textit{\\textbf{D}isentangling \\textbf{R}isks to \\textbf{E}nhance Safety \\textbf{A}lignment in \\textbf{M}LLMs}), a novel approach that enhances safety alignment in MLLMs through supervised fine-tuning and iterative Reinforcement Learning from AI Feedback (RLAIF). Experimental results show that DREAM significantly boosts safety during both inference and training phases without compromising performance on normal tasks (namely oversafety), achieving a 16.17\\% improvement in the SIUO safe\\&amp;effective score compared to GPT-4V. The data and code are available at https://github.com/Kizna1ver/DREAM.","authors":["Jianyu Liu","Hangyu Guo","Ranjie Duan","Xingyuan Bu","Yancheng He","Shilong Li","Hui Huang","Jiaheng Liu","Yucheng Wang","Chenchen Jing","Xingwei Qu","Xiao Zhang","Yingshui Tan","Yanan Wu","Jihao Gu","Yangguang Li","Jianke Zhu"],"url":"https://arxiv.org/abs/2504.18053"}
{"created":"2025-04-28","title":"A locking free multiscale method for linear elasticity in stress-displacement formulation with high contrast coefficients","abstract":"Achieving strongly symmetric stress approximations for linear elasticity problems in high-contrast media poses a significant computational challenge. Conventional methods often struggle with prohibitively high computational costs due to excessive degrees of freedom, limiting their practical applicability. To overcome this challenge, we introduce an efficient multiscale model reduction method and a computationally inexpensive coarse-grid simulation technique for linear elasticity equations in highly heterogeneous, high-contrast media. We first utilize a stable stress-displacement mixed finite element method to discretize the linear elasticity problem and then present the construction of multiscale basis functions for the displacement and the stress. The mixed formulation offers several advantages such as direct stress computation without post-processing, local momentum conservation (ensuring physical consistency), and robustness against locking effects, even for nearly incompressible materials. Theoretical analysis confirms that our method is inf-sup stable and locking-free, with first-order convergence relative to the coarse mesh size. Notably, the convergence remains independent of contrast ratios as enlarging oversampling regions. Numerical experiments validate the method's effectiveness, demonstrating its superior performance even under extreme contrast conditions.","authors":["Eric T. Chung","Changqing Ye","Xiang Zhong"],"url":"https://arxiv.org/abs/2504.18054"}
{"created":"2025-04-28","title":"Why Does My Transaction Fail? A First Look at Failed Transactions on the Solana Blockchain","abstract":"Solana is an emerging blockchain platform, recognized for its high throughput and low transaction costs, positioning it as a preferred infrastructure for Decentralized Finance (DeFi), Non-Fungible Tokens (NFTs), and other Web 3.0 applications. In the Solana ecosystem, transaction initiators submit various instructions to interact with a diverse range of Solana smart contracts, among which are decentralized exchanges (DEXs) that utilize automated market makers (AMMs), allowing users to trade cryptocurrencies directly on the blockchain without the need for intermediaries. Despite the high throughput and low transaction costs of Solana, the advantages have exposed Solana to bot spamming for financial exploitation, resulting in the prevalence of failed transactions and network congestion.","authors":["Xiaoye Zheng (Zhejiang University)","Zhiyuan Wan (Zhejiang University","Hangzhou High-Tech Zone)","David Lo (Singapore Management University)","Difan Xie (Hangzhou High-Tech Zone)","Xiaohu Yang (Zhejiang University)"],"url":"https://arxiv.org/abs/2504.18055"}
{"created":"2025-04-28","title":"Range-based 6-DoF Monte Carlo SLAM with Gradient-guided Particle Filter on GPU","abstract":"This paper presents range-based 6-DoF Monte Carlo SLAM with a gradient-guided particle update strategy. While non-parametric state estimation methods, such as particle filters, are robust in situations with high ambiguity, they are known to be unsuitable for high-dimensional problems due to the curse of dimensionality. To address this issue, we propose a particle update strategy that improves the sampling efficiency by using the gradient information of the likelihood function to guide particles toward its mode. Additionally, we introduce a keyframe-based map representation that represents the global map as a set of past frames (i.e., keyframes) to mitigate memory consumption. The keyframe poses for each particle are corrected using a simple loop closure method to maintain trajectory consistency. The combination of gradient information and keyframe-based map representation significantly enhances sampling efficiency and reduces memory usage compared to traditional RBPF approaches. To process a large number of particles (e.g., 100,000 particles) in real-time, the proposed framework is designed to fully exploit GPU parallel processing. Experimental results demonstrate that the proposed method exhibits extreme robustness to state ambiguity and can even deal with kidnapping situations, such as when the sensor moves to different floors via an elevator, with minimal heuristics.","authors":["Takumi Nakao","Kenji Koide","Aoki Takanose","Shuji Oishi","Masashi Yokozuka","Hisashi Date"],"url":"https://arxiv.org/abs/2504.18056"}
{"created":"2025-04-28","title":"Opportunistic Collaborative Planning with Large Vision Model Guided Control and Joint Query-Service Optimization","abstract":"Navigating autonomous vehicles in open scenarios is a challenge due to the difficulties in handling unseen objects. Existing solutions either rely on small models that struggle with generalization or large models that are resource-intensive. While collaboration between the two offers a promising solution, the key challenge is deciding when and how to engage the large model. To address this issue, this paper proposes opportunistic collaborative planning (OCP), which seamlessly integrates efficient local models with powerful cloud models through two key innovations. First, we propose large vision model guided model predictive control (LVM-MPC), which leverages the cloud for LVM perception and decision making. The cloud output serves as a global guidance for a local MPC, thereby forming a closed-loop perception-to-control system. Second, to determine the best timing for large model query and service, we propose collaboration timing optimization (CTO), including object detection confidence thresholding (ODCT) and cloud forward simulation (CFS), to decide when to seek cloud assistance and when to offer cloud service. Extensive experiments show that the proposed OCP outperforms existing methods in terms of both navigation time and success rate.","authors":["Jiayi Chen","Shuai Wang","Guoliang Li","Wei Xu","Guangxu Zhu","Derrick Wing Kwan Ng","Chengzhong Xu"],"url":"https://arxiv.org/abs/2504.18057"}
{"created":"2025-04-28","title":"Exploring Personality-Aware Interactions in Salesperson Dialogue Agents","abstract":"The integration of dialogue agents into the sales domain requires a deep understanding of how these systems interact with users possessing diverse personas. This study explores the influence of user personas, defined using the Myers-Briggs Type Indicator (MBTI), on the interaction quality and performance of sales-oriented dialogue agents. Through large-scale testing and analysis, we assess the pre-trained agent's effectiveness, adaptability, and personalization capabilities across a wide range of MBTI-defined user types. Our findings reveal significant patterns in interaction dynamics, task completion rates, and dialogue naturalness, underscoring the future potential for dialogue agents to refine their strategies to better align with varying personality traits. This work not only provides actionable insights for building more adaptive and user-centric conversational systems in the sales domain but also contributes broadly to the field by releasing persona-defined user simulators. These simulators, unconstrained by domain, offer valuable tools for future research and demonstrate the potential for scaling personalized dialogue systems across diverse applications.","authors":["Sijia Cheng","Wen-Yu Chang","Yun-Nung Chen"],"url":"https://arxiv.org/abs/2504.18058"}
{"created":"2025-04-28","title":"POET: Prompt Offset Tuning for Continual Human Action Adaptation","abstract":"As extended reality (XR) is redefining how users interact with computing devices, research in human action recognition is gaining prominence. Typically, models deployed on immersive computing devices are static and limited to their default set of classes. The goal of our research is to provide users and developers with the capability to personalize their experience by adding new action classes to their device models continually. Importantly, a user should be able to add new classes in a low-shot and efficient manner, while this process should not require storing or replaying any of user's sensitive training data. We formalize this problem as privacy-aware few-shot continual action recognition. Towards this end, we propose POET: Prompt-Offset Tuning. While existing prompt tuning approaches have shown great promise for continual learning of image, text, and video modalities; they demand access to extensively pretrained transformers. Breaking away from this assumption, POET demonstrates the efficacy of prompt tuning a significantly lightweight backbone, pretrained exclusively on the base class data. We propose a novel spatio-temporal learnable prompt offset tuning approach, and are the first to apply such prompt tuning to Graph Neural Networks. We contribute two new benchmarks for our new problem setting in human action recognition: (i) NTU RGB+D dataset for activity recognition, and (ii) SHREC-2017 dataset for hand gesture recognition. We find that POET consistently outperforms comprehensive benchmarks. Source code at https://github.com/humansensinglab/POET-continual-action-recognition.","authors":["Prachi Garg","Joseph K J","Vineeth N Balasubramanian","Necati Cihan Camgoz","Chengde Wan","Kenrick Kin","Weiguang Si","Shugao Ma","Fernando De La Torre"],"url":"https://arxiv.org/abs/2504.18059"}
{"created":"2025-04-28","title":"LLM-Guided Open RAN: Empowering Hierarchical RAN Intelligent Control","abstract":"Recent advancements in large language models (LLMs) have led to a significant interest in deploying LLMempowered algorithms for wireless communication networks. Meanwhile, open radio access network (O-RAN) techniques offer unprecedented flexibility, with the non-real-time (non-RT) radio access network (RAN) intelligent controller (RIC) (non-RT RIC) and near-real-time (near-RT) RIC (near-RT RIC) components enabling intelligent resource management across different time scales. In this paper, we propose the LLM empowered hierarchical RIC (LLM-hRIC) framework to improve the collaboration between RICs. This framework integrates LLMs with reinforcement learning (RL) for efficient network resource management. In this framework, LLMs-empowered non-RT RICs provide strategic guidance and high-level policies based on environmental context. Concurrently, RL-empowered near-RT RICs perform low-latency tasks based on strategic guidance and local near-RT observation. We evaluate the LLM-hRIC framework in an integrated access and backhaul (IAB) network setting. Simulation results demonstrate that the proposed framework achieves superior performance. Finally, we discuss the key future challenges in applying LLMs to O-RAN.","authors":["Lingyan Bao","Sinwoong Yun","Jemin Lee","Tony Q. S. Quek"],"url":"https://arxiv.org/abs/2504.18062"}
{"created":"2025-04-28","title":"AllTact Fin Ray: A Compliant Robot Gripper with Omni-Directional Tactile Sensing","abstract":"Tactile sensing plays a crucial role in robot grasping and manipulation by providing essential contact information between the robot and the environment. In this paper, we present AllTact Fin Ray, a novel compliant gripper design with omni-directional and local tactile sensing capabilities. The finger body is unibody-casted using transparent elastic silicone, and a camera positioned at the base of the finger captures the deformation of the whole body and the contact face. Due to the global deformation of the adaptive structure, existing vision-based tactile sensing approaches that assume constant illumination are no longer applicable. To address this, we propose a novel sensing method where the global deformation is first reconstructed from the image using edge features and spatial constraints. Then, detailed contact geometry is computed from the brightness difference against a dynamically retrieved reference image. Extensive experiments validate the effectiveness of our proposed gripper design and sensing method in contact detection, force estimation, object grasping, and precise manipulation.","authors":["Siwei Liang","Yixuan Guan","Jing Xu","Hongyu Qian","Xiangjun Zhang","Dan Wu","Wenbo Ding","Rui Chen"],"url":"https://arxiv.org/abs/2504.18064"}
{"created":"2025-04-28","title":"S3MOT: Monocular 3D Object Tracking with Selective State Space Model","abstract":"Accurate and reliable multi-object tracking (MOT) in 3D space is essential for advancing robotics and computer vision applications. However, it remains a significant challenge in monocular setups due to the difficulty of mining 3D spatiotemporal associations from 2D video streams. In this work, we present three innovative techniques to enhance the fusion and exploitation of heterogeneous cues for monocular 3D MOT: (1) we introduce the Hungarian State Space Model (HSSM), a novel data association mechanism that compresses contextual tracking cues across multiple paths, enabling efficient and comprehensive assignment decisions with linear complexity. HSSM features a global receptive field and dynamic weights, in contrast to traditional linear assignment algorithms that rely on hand-crafted association costs. (2) We propose Fully Convolutional One-stage Embedding (FCOE), which eliminates ROI pooling by directly using dense feature maps for contrastive learning, thus improving object re-identification accuracy under challenging conditions such as varying viewpoints and lighting. (3) We enhance 6-DoF pose estimation through VeloSSM, an encoder-decoder architecture that models temporal dependencies in velocity to capture motion dynamics, overcoming the limitations of frame-based 3D inference. Experiments on the KITTI public test benchmark demonstrate the effectiveness of our method, achieving a new state-of-the-art performance of 76.86~HOTA at 31~FPS. Our approach outperforms the previous best by significant margins of +2.63~HOTA and +3.62~AssA, showcasing its robustness and efficiency for monocular 3D MOT tasks. The code and models are available at https://github.com/bytepioneerX/s3mot.","authors":["Zhuohao Yan","Shaoquan Feng","Xingxing Li","Yuxuan Zhou","Chunxi Xia","Shengyu Li"],"url":"https://arxiv.org/abs/2504.18068"}
{"created":"2025-04-28","title":"PropRAG: Guiding Retrieval with Beam Search over Proposition Paths","abstract":"Retrieval Augmented Generation (RAG) has become the standard non-parametric approach for equipping Large Language Models (LLMs) with up-to-date knowledge and mitigating catastrophic forgetting common in continual learning. However, standard RAG, relying on independent passage retrieval, fails to capture the interconnected nature of human memory crucial for complex reasoning (associativity) and contextual understanding (sense-making). While structured RAG methods like HippoRAG utilize knowledge graphs (KGs) built from triples, the inherent context loss limits fidelity. We introduce PropRAG, a framework leveraging contextually rich propositions and a novel beam search algorithm over proposition paths to explicitly discover multi-step reasoning chains. Crucially, PropRAG's online retrieval process operates entirely without invoking generative LLMs, relying instead on efficient graph traversal and pre-computed embeddings. This avoids online LLM inference costs and potential inconsistencies during evidence gathering. LLMs are used effectively offline for high-quality proposition extraction and post-retrieval for answer generation. PropRAG achieves state-of-the-art zero-shot Recall@5 results on PopQA (55.3%), 2Wiki (93.7%), HotpotQA (97.0%), and MuSiQue (77.3%), alongside top F1 scores (e.g., 52.4% on MuSiQue). By improving evidence retrieval through richer representation and explicit, LLM-free online path finding, PropRAG advances non-parametric continual learning.","authors":["Jingjin Wang"],"url":"https://arxiv.org/abs/2504.18070"}
{"created":"2025-04-28","title":"A Model Zoo on Phase Transitions in Neural Networks","abstract":"Using the weights of trained Neural Network (NN) models as data modality has recently gained traction as a research field - dubbed Weight Space Learning (WSL). Multiple recent works propose WSL methods to analyze models, evaluate methods, or synthesize weights. Weight space learning methods require populations of trained models as datasets for development and evaluation. However, existing collections of models - called `model zoos' - are unstructured or follow a rudimentary definition of diversity. In parallel, work rooted in statistical physics has identified phases and phase transitions in NN models. Models are homogeneous within the same phase but qualitatively differ from one phase to another. We combine the idea of `model zoos' with phase information to create a controlled notion of diversity in populations. We introduce 12 large-scale zoos that systematically cover known phases and vary over model architecture, size, and datasets. These datasets cover different modalities, such as computer vision, natural language processing, and scientific ML. For every model, we compute loss landscape metrics and validate full coverage of the phases. With this dataset, we provide the community with a resource with a wide range of potential applications for WSL and beyond. Evidence suggests the loss landscape phase plays a role in applications such as model training, analysis, or sparsification. We demonstrate this in an exploratory study of the downstream methods like transfer learning or model weights averaging.","authors":["Konstantin Sch\\\"urholt","L\\'eo Meynent","Yefan Zhou","Haiquan Lu","Yaoqing Yang","Damian Borth"],"url":"https://arxiv.org/abs/2504.18072"}
{"created":"2025-04-28","title":"Fictitious Play in Extensive-Form Games of Imperfect Information","abstract":"We study the long-term behavior of the fictitious play process in repeated extensive-form games of imperfect information with perfect recall. Each player maintains incorrect beliefs that the moves at all information sets, except the one at which the player is about to make a move, are made according to fixed random strategies, independently across all information sets. Accordingly, each player makes his moves at any of his information sets to maximize his expected payoff assuming that, at any other information set, the moves are made according to the empirical frequencies of the past moves. We extend the well-known Monderer-Shapley result [1] on the convergence of the empirical frequencies to the set of Nash equilibria to a certain class of extensive-form games with identical interests. We then strengthen this result by the use of inertia and fading memory, and prove the convergence of the realized play-paths to an essentially pure Nash equilibrium in all extensive-form games of imperfect information with identical interests.","authors":["Jason Castiglione","G\\\"urdal Arslan"],"url":"https://arxiv.org/abs/2504.18075"}
{"created":"2025-04-28","title":"Privacy-Preserving Personalized Federated Learning for Distributed Photovoltaic Disaggregation under Statistical Heterogeneity","abstract":"The rapid expansion of distributed photovoltaic (PV) installations worldwide, many being behind-the-meter systems, has significantly challenged energy management and grid operations, as unobservable PV generation further complicates the supply-demand balance. Therefore, estimating this generation from net load, known as PV disaggregation, is critical. Given privacy concerns and the need for large training datasets, federated learning becomes a promising approach, but statistical heterogeneity, arising from geographical and behavioral variations among prosumers, poses new challenges to PV disaggregation. To overcome these challenges, a privacy-preserving distributed PV disaggregation framework is proposed using Personalized Federated Learning (PFL). The proposed method employs a two-level framework that combines local and global modeling. At the local level, a transformer-based PV disaggregation model is designed to generate solar irradiance embeddings for representing local PV conditions. A novel adaptive local aggregation mechanism is adopted to mitigate the impact of statistical heterogeneity on the local model, extracting a portion of global information that benefits the local model. At the global level, a central server aggregates information uploaded from multiple data centers, preserving privacy while enabling cross-center knowledge sharing. Experiments on real-world data demonstrate the effectiveness of this proposed framework, showing improved accuracy and robustness compared to benchmark methods.","authors":["Xiaolu Chen","Chenghao Huang","Yanru Zhang","Hao Wang"],"url":"https://arxiv.org/abs/2504.18078"}
{"created":"2025-04-28","title":"Stabilizing Reasoning in Medical LLMs with Continued Pretraining and Reasoning Preference Optimization","abstract":"Large Language Models (LLMs) show potential in medicine, yet clinical adoption is hindered by concerns over factual accuracy, language-specific limitations (e.g., Japanese), and critically, their reliability when required to generate reasoning explanations -- a prerequisite for trust. This paper introduces Preferred-MedLLM-Qwen-72B, a 72B-parameter model optimized for the Japanese medical domain to achieve both high accuracy and stable reasoning. We employ a two-stage fine-tuning process on the Qwen2.5-72B base model: first, Continued Pretraining (CPT) on a comprehensive Japanese medical corpus instills deep domain knowledge. Second, Reasoning Preference Optimization (RPO), a preference-based method, enhances the generation of reliable reasoning pathways while preserving high answer accuracy. Evaluations on the Japanese Medical Licensing Exam benchmark (IgakuQA) show Preferred-MedLLM-Qwen-72B achieves state-of-the-art performance (0.868 accuracy), surpassing strong proprietary models like GPT-4o (0.866). Crucially, unlike baseline or CPT-only models which exhibit significant accuracy degradation (up to 11.5\\% and 3.8\\% respectively on IgakuQA) when prompted for explanations, our model maintains its high accuracy (0.868) under such conditions. This highlights RPO's effectiveness in stabilizing reasoning generation. This work underscores the importance of optimizing for reliable explanations alongside accuracy. We release the Preferred-MedLLM-Qwen-72B model weights to foster research into trustworthy LLMs for specialized, high-stakes applications.","authors":["Wataru Kawakami","Keita Suzuki","Junichiro Iwasawa"],"url":"https://arxiv.org/abs/2504.18080"}
{"created":"2025-04-28","title":"Hype and Adoption of Generative Artificial Intelligence Applications","abstract":"New technologies create opportunities while displacing others. They enhance life by supporting entertainment, education, and social connectivity but also replace humans in productivity and analytical tasks. Adapting to these shifts requires technical adjustments and social readiness. For digital transformation to succeed, organizations and their workforce must be psychologically prepared. We are entering the era of Generative AI with tools like ChatGPT, Bing AI, and Microsoft Office Copilot. Understanding public sentiment toward these innovations is crucial for refining technology acceptance models and informing market strategies. Using the Gartner Hype Cycle and Kubler-Ross Change Curve, this study suggests that generative AI adoption is a dual-stage process. It follows the phases of technology trigger, peak of expectations, trough of disillusionment, slope of enlightenment, and plateau of productivity, while also reflecting emotional stages like shock, denial, and integration. The study used sentiment and emotion analysis on a large dataset of tweets about generative AI, translating them into scores to track user responses over time. Unlike prior research, which offered a snapshot of sentiment, this study captures the dynamic evolution of attitudes, linking empirical evidence with theoretical frameworks. It shifts the focus from information seekers to content creators. With the release of generative AI tools, there is a significant gap in understanding societal reception and adaptation. Policymakers face uncertainty about guiding markets for these changes. This research validates the applicability of the Gartner Hype Cycle and Kubler-Ross Change Curve to generative AI. It provides insights for businesses in integrating these tools and crafting policies to enhance readiness and resilience.","authors":["Vinh Truong (RMIT University)"],"url":"https://arxiv.org/abs/2504.18081"}
{"created":"2025-04-28","title":"Efficient GNN Training Through Structure-Aware Randomized Mini-Batching","abstract":"Graph Neural Networks (GNNs) enable learning on realworld graphs and mini-batch training has emerged as the de facto standard for training GNNs because it can scale to very large graphs and improve convergence. Current mini-batch construction policies largely ignore efficiency considerations of GNN training. Specifically, existing mini-batching techniques employ randomization schemes to improve accuracy and convergence. However, these randomization schemes are often agnostic to the structural properties of the graph (for eg. community structure), resulting in highly irregular memory access patterns during GNN training that make suboptimal use of on-chip GPU caches. On the other hand, while deterministic mini-batching based solely on graph structure delivers fast runtime performance, the lack of randomness compromises both the final model accuracy and training convergence speed. In this paper, we present Community-structure-aware Randomized Mini-batching (COMM-RAND), a novel methodology that bridges the gap between the above extremes. COMM-RAND allows practitioners to explore the space between pure randomness and pure graph structural awareness during mini-batch construction, leading to significantly more efficient GNN training with similar accuracy. We evaluated COMM-RAND across four popular graph learning benchmarks. COMM-RAND cuts down GNN training time by up to 2.76x (1.8x on average) while achieving an accuracy that is within 1.79% points (0.42% on average) compared to popular random mini-batching approaches.","authors":["Vignesh Balaji","Christos Kozyrakis","Gal Chechik","Haggai Maron"],"url":"https://arxiv.org/abs/2504.18082"}
{"created":"2025-04-28","title":"Automating Function-Level TARA for Automotive Full-Lifecycle Security","abstract":"As modern vehicles evolve into intelligent and connected systems, their growing complexity introduces significant cybersecurity risks. Threat Analysis and Risk Assessment (TARA) has therefore become essential for managing these risks under mandatory regulations. However, existing TARA automation methods rely on static threat libraries, limiting their utility in the detailed, function-level analyses demanded by industry. This paper introduces DefenseWeaver, the first system that automates function-level TARA using component-specific details and large language models (LLMs). DefenseWeaver dynamically generates attack trees and risk evaluations from system configurations described in an extended OpenXSAM++ format, then employs a multi-agent framework to coordinate specialized LLM roles for more robust analysis. To further adapt to evolving threats and diverse standards, DefenseWeaver incorporates Low-Rank Adaptation (LoRA) fine-tuning and Retrieval-Augmented Generation (RAG) with expert-curated TARA reports. We validated DefenseWeaver through deployment in four automotive security projects, where it identified 11 critical attack paths, verified through penetration testing, and subsequently reported and remediated by the relevant automakers and suppliers. Additionally, DefenseWeaver demonstrated cross-domain adaptability, successfully applying to unmanned aerial vehicles (UAVs) and marine navigation systems. In comparison to human experts, DefenseWeaver outperformed manual attack tree generation across six assessment scenarios. Integrated into commercial cybersecurity platforms such as UAES and Xiaomi, DefenseWeaver has generated over 8,200 attack trees. These results highlight its ability to significantly reduce processing time, and its scalability and transformative impact on cybersecurity across industries.","authors":["Yuqiao Yang","Yongzhao Zhang","Wenhao Liu","Jun Li","Pengtao Shi","DingYu Zhong","Jie Yang","Ting Chen","Sheng Cao","Yuntao Ren","Yongyue Wu","Xiaosong Zhang"],"url":"https://arxiv.org/abs/2504.18083"}
{"created":"2025-04-28","title":"RL-Driven Data Generation for Robust Vision-Based Dexterous Grasping","abstract":"This work presents reinforcement learning (RL)-driven data augmentation to improve the generalization of vision-action (VA) models for dexterous grasping. While real-to-sim-to-real frameworks, where a few real demonstrations seed large-scale simulated data, have proven effective for VA models, applying them to dexterous settings remains challenging: obtaining stable multi-finger contacts is nontrivial across diverse object shapes. To address this, we leverage RL to generate contact-rich grasping data across varied geometries. In line with the real-to-sim-to-real paradigm, the grasp skill is formulated as a parameterized and tunable reference trajectory refined by a residual policy learned via RL. This modular design enables trajectory-level control that is both consistent with real demonstrations and adaptable to diverse object geometries. A vision-conditioned policy trained on simulation-augmented data demonstrates strong generalization to unseen objects, highlighting the potential of our approach to alleviate the data bottleneck in training VA models.","authors":["Atsushi Kanehira","Naoki Wake","Kazuhiro Sasabuchi","Jun Takamatsu","Katsushi Ikeuchi"],"url":"https://arxiv.org/abs/2504.18084"}
{"created":"2025-04-28","title":"Random-Set Large Language Models","abstract":"Large Language Models (LLMs) are known to produce very high-quality tests and responses to our queries. But how much can we trust this generated text? In this paper, we study the problem of uncertainty quantification in LLMs. We propose a novel Random-Set Large Language Model (RSLLM) approach which predicts finite random sets (belief functions) over the token space, rather than probability vectors as in classical LLMs. In order to allow so efficiently, we also present a methodology based on hierarchical clustering to extract and use a budget of \"focal\" subsets of tokens upon which the belief prediction is defined, rather than using all possible collections of tokens, making the method scalable yet effective. RS-LLMs encode the epistemic uncertainty induced in their generation process by the size and diversity of its training set via the size of the credal sets associated with the predicted belief functions. The proposed approach is evaluated on CoQA and OBQA datasets using Llama2-7b, Mistral-7b and Phi-2 models and is shown to outperform the standard model in both datasets in terms of correctness of answer while also showing potential in estimating the second level uncertainty in its predictions and providing the capability to detect when its hallucinating.","authors":["Muhammad Mubashar","Shireen Kudukkil Manchingal","Fabio Cuzzolin"],"url":"https://arxiv.org/abs/2504.18085"}
{"created":"2025-04-28","title":"Disentangle Identity, Cooperate Emotion: Correlation-Aware Emotional Talking Portrait Generation","abstract":"Recent advances in Talking Head Generation (THG) have achieved impressive lip synchronization and visual quality through diffusion models; yet existing methods struggle to generate emotionally expressive portraits while preserving speaker identity. We identify three critical limitations in current emotional talking head generation: insufficient utilization of audio's inherent emotional cues, identity leakage in emotion representations, and isolated learning of emotion correlations. To address these challenges, we propose a novel framework dubbed as DICE-Talk, following the idea of disentangling identity with emotion, and then cooperating emotions with similar characteristics. First, we develop a disentangled emotion embedder that jointly models audio-visual emotional cues through cross-modal attention, representing emotions as identity-agnostic Gaussian distributions. Second, we introduce a correlation-enhanced emotion conditioning module with learnable Emotion Banks that explicitly capture inter-emotion relationships through vector quantization and attention-based feature aggregation. Third, we design an emotion discrimination objective that enforces affective consistency during the diffusion process through latent-space classification. Extensive experiments on MEAD and HDTF datasets demonstrate our method's superiority, outperforming state-of-the-art approaches in emotion accuracy while maintaining competitive lip-sync performance. Qualitative results and user studies further confirm our method's ability to generate identity-preserving portraits with rich, correlated emotional expressions that naturally adapt to unseen identities.","authors":["Weipeng Tan","Chuming Lin","Chengming Xu","FeiFan Xu","Xiaobin Hu","Xiaozhong Ji","Junwei Zhu","Chengjie Wang","Yanwei Fu"],"url":"https://arxiv.org/abs/2504.18087"}
{"created":"2025-04-28","title":"Reliable and Efficient Inverse Analysis using Physics-Informed Neural Networks with Distance Functions and Adaptive Weight Tuning","abstract":"Physics-informed neural networks have attracted significant attention in scientific machine learning for their capability to solve forward and inverse problems governed by partial differential equations. However, the accuracy of PINN solutions is often limited by the treatment of boundary conditions. Conventional penalty-based methods, which incorporate boundary conditions as penalty terms in the loss function, cannot guarantee exact satisfaction of the given boundary conditions and are highly sensitive to the choice of penalty parameters. This paper demonstrates that distance functions, specifically R-functions, can be leveraged to enforce boundary conditions, overcoming these limitations. R-functions provide normalized distance fields, enabling accurate representation of boundary geometries, including non-convex domains, and facilitating various types of boundary conditions. We extend this distance function-based boundary condition imposition method to inverse problems using PINNs and introduce an adaptive weight tuning technique to ensure reliable and efficient inverse analysis. We demonstrate the efficacy of the method through several numerical experiments. Numerical results show that the proposed method solves inverse problems more accurately and efficiently than penalty-based methods, even in the presence of complex non-convex geometries. This approach offers a reliable and efficient framework for inverse analysis using PINNs, with potential applications across a wide range of engineering problems.","authors":["Shota Deguchi","Mitsuteru Asai"],"url":"https://arxiv.org/abs/2504.18091"}
{"created":"2025-04-28","title":"Subject-independent Classification of Meditative State from the Resting State using EEG","abstract":"While it is beneficial to objectively determine whether a subject is meditating, most research in the literature reports good results only in a subject-dependent manner. This study aims to distinguish the modified state of consciousness experienced during Rajyoga meditation from the resting state of the brain in a subject-independent manner using EEG data. Three architectures have been proposed and evaluated: The CSP-LDA Architecture utilizes common spatial pattern (CSP) for feature extraction and linear discriminant analysis (LDA) for classification. The CSP-LDA-LSTM Architecture employs CSP for feature extraction, LDA for dimensionality reduction, and long short-term memory (LSTM) networks for classification, modeling the binary classification problem as a sequence learning problem. The SVD-NN Architecture uses singular value decomposition (SVD) to select the most relevant components of the EEG signals and a shallow neural network (NN) for classification. The CSP-LDA-LSTM architecture gives the best performance with 98.2% accuracy for intra-subject classification. The SVD-NN architecture provides significant performance with 96.4\\% accuracy for inter-subject classification. This is comparable to the best-reported accuracies in the literature for intra-subject classification. Both architectures are capable of capturing subject-invariant EEG features for effectively classifying the meditative state from the resting state. The high intra-subject and inter-subject classification accuracies indicate these systems' robustness and their ability to generalize across different subjects.","authors":["Jerrin Thomas Panachakel","Pradeep Kumar G.","Suryaa Seran","Kanishka Sharma","Ramakrishnan Angarai Ganesan"],"url":"https://arxiv.org/abs/2504.18095"}
{"created":"2025-04-28","title":"Combating the Bucket Effect:Multi-Knowledge Alignment for Medication Recommendation","abstract":"Medication recommendation is crucial in healthcare, offering effective treatments based on patient's electronic health records (EHR). Previous studies show that integrating more medication-related knowledge improves medication representation accuracy. However, not all medications encompass multiple types of knowledge data simultaneously. For instance, some medications provide only textual descriptions without structured data. This imbalance in data availability limits the performance of existing models, a challenge we term the \"bucket effect\" in medication recommendation. Our data analysis uncovers the severity of the \"bucket effect\" in medication recommendation. To fill this gap, we introduce a cross-modal medication encoder capable of seamlessly aligning data from different modalities and propose a medication recommendation framework to integrate Multiple types of Knowledge, named MKMed. Specifically, we first pre-train a cross-modal encoder with contrastive learning on five knowledge modalities, aligning them into a unified space. Then, we combine the multi-knowledge medication representations with patient records for recommendations. Extensive experiments on the MIMIC-III and MIMIC-IV datasets demonstrate that MKMed mitigates the \"bucket effect\" in data, and significantly outperforms state-of-the-art baselines in recommendation accuracy and safety.","authors":["Xiang Li","Haixu Ma","Guanyong Wu","Shi Mu","Chen Li","Shunpan Liang"],"url":"https://arxiv.org/abs/2504.18096"}
{"created":"2025-04-28","title":"Tracking Articulatory Dynamics in Speech with a Fixed-Weight BiLSTM-CNN Architecture","abstract":"Speech production is a complex sequential process which involve the coordination of various articulatory features. Among them tongue being a highly versatile active articulator responsible for shaping airflow to produce targeted speech sounds that are intellectual, clear, and distinct. This paper presents a novel approach for predicting tongue and lip articulatory features involved in a given speech acoustics using a stacked Bidirectional Long Short-Term Memory (BiLSTM) architecture, combined with a one-dimensional Convolutional Neural Network (CNN) for post-processing with fixed weights initialization. The proposed network is trained with two datasets consisting of simultaneously recorded speech and Electromagnetic Articulography (EMA) datasets, each introducing variations in terms of geographical origin, linguistic characteristics, phonetic diversity, and recording equipment. The performance of the model is assessed in Speaker Dependent (SD), Speaker Independent (SI), corpus dependent (CD) and cross corpus (CC) modes. Experimental results indicate that the proposed model with fixed weights approach outperformed the adaptive weights initialization with in relatively minimal number of training epochs. These findings contribute to the development of robust and efficient models for articulatory feature prediction, paving the way for advancements in speech production research and applications.","authors":["Leena G Pillai","D. Muhammad Noorul Mubarak","Elizabeth Sherly"],"url":"https://arxiv.org/abs/2504.18099"}
{"created":"2025-04-28","title":"Application and Optimization of Large Models Based on Prompt Tuning for Fact-Check-Worthiness Estimation","abstract":"In response to the growing problem of misinformation in the context of globalization and informatization, this paper proposes a classification method for fact-check-worthiness estimation based on prompt tuning. We construct a model for fact-check-worthiness estimation at the methodological level using prompt tuning. By applying designed prompt templates to large language models, we establish in-context learning and leverage prompt tuning technology to improve the accuracy of determining whether claims have fact-check-worthiness, particularly when dealing with limited or unlabeled data. Through extensive experiments on public datasets, we demonstrate that the proposed method surpasses or matches multiple baseline methods in the classification task of fact-check-worthiness estimation assessment, including classical pre-trained models such as BERT, as well as recent popular large models like GPT-3.5 and GPT-4. Experiments show that the prompt tuning-based method proposed in this study exhibits certain advantages in evaluation metrics such as F1 score and accuracy, thereby effectively validating its effectiveness and advancement in the task of fact-check-worthiness estimation.","authors":["Yinglong Yu","Hao Shen","Zhengyi Lyu","Qi He"],"url":"https://arxiv.org/abs/2504.18104"}
{"created":"2025-04-28","title":"Temperature Estimation in Induction Motors using Machine Learning","abstract":"The number of electrified powertrains is ever increasing today towards a more sustainable future; thus, it is essential that unwanted failures are prevented, and a reliable operation is secured. Monitoring the internal temperatures of motors and keeping them under their thresholds is an important first step. Conventional modeling methods require expert knowledge and complicated mathematical approaches. With all the data a modern electric drive collects nowadays during the system operation, it is feasible to apply data-driven approaches for estimating thermal behaviors. In this paper, multiple machine-learning methods are investigated on their capability to approximate the temperatures of the stator winding and bearing in induction motors. The explored algorithms vary from linear to neural networks. For this reason, experimental lab data have been captured from a powertrain under predetermined operating conditions. For each approach, a hyperparameter search is then performed to find the optimal configuration. All the models are evaluated by various metrics, and it has been found that neural networks perform satisfactorily even under transient conditions.","authors":["Dinan Li","Panagiotis Kakosimos"],"url":"https://arxiv.org/abs/2504.18105"}
{"created":"2025-04-28","title":"Comparative Study on the Discourse Meaning of Chinese and English Media in the Paris Olympics Based on LDA Topic Modeling Technology and LLM Prompt Engineering","abstract":"This study analyzes Chinese and English media reports on the Paris Olympics using topic modeling, Large Language Model (LLM) prompt engineering, and corpus phraseology methods to explore similarities and differences in discourse construction and attitudinal meanings. Common topics include the opening ceremony, athlete performance, and sponsorship brands. Chinese media focus on specific sports, sports spirit, doping controversies, and new technologies, while English media focus on female athletes, medal wins, and eligibility controversies. Chinese reports show more frequent prepositional co-occurrences and positive semantic prosody in describing the opening ceremony and sports spirit. English reports exhibit positive semantic prosody when covering female athletes but negative prosody in predicting opening ceremony reactions and discussing women's boxing controversies.","authors":["Yinglong Yu","Zhaopu Yao","Fang Yuan"],"url":"https://arxiv.org/abs/2504.18106"}
{"created":"2025-04-28","title":"Study on Real-Time Road Surface Reconstruction Using Stereo Vision","abstract":"Road surface reconstruction plays a crucial role in autonomous driving, providing essential information for safe and smooth navigation. This paper enhances the RoadBEV [1] framework for real-time inference on edge devices by optimizing both efficiency and accuracy. To achieve this, we proposed to apply Isomorphic Global Structured Pruning to the stereo feature extraction backbone, reducing network complexity while maintaining performance. Additionally, the head network is redesigned with an optimized hourglass structure, dynamic attention heads, reduced feature channels, mixed precision inference, and efficient probability volume computation. Our approach improves inference speed while achieving lower reconstruction error, making it well-suited for real-time road surface reconstruction in autonomous driving.","authors":["Deepak Ghimire","Byoungjun Kim","Donghoon Kim","SungHwan Jeong"],"url":"https://arxiv.org/abs/2504.18112"}
{"created":"2025-04-28","title":"Learning from Less: SINDy Surrogates in RL","abstract":"This paper introduces an approach for developing surrogate environments in reinforcement learning (RL) using the Sparse Identification of Nonlinear Dynamics (SINDy) algorithm. We demonstrate the effectiveness of our approach through extensive experiments in OpenAI Gym environments, particularly Mountain Car and Lunar Lander. Our results show that SINDy-based surrogate models can accurately capture the underlying dynamics of these environments while reducing computational costs by 20-35%. With only 75 interactions for Mountain Car and 1000 for Lunar Lander, we achieve state-wise correlations exceeding 0.997, with mean squared errors as low as 3.11e-06 for Mountain Car velocity and 1.42e-06 for LunarLander position. RL agents trained in these surrogate environments require fewer total steps (65,075 vs. 100,000 for Mountain Car and 801,000 vs. 1,000,000 for Lunar Lander) while achieving comparable performance to those trained in the original environments, exhibiting similar convergence patterns and final performance metrics. This work contributes to the field of model-based RL by providing an efficient method for generating accurate, interpretable surrogate environments.","authors":["Aniket Dixit","Muhammad Ibrahim Khan","Faizan Ahmed","James Brusey"],"url":"https://arxiv.org/abs/2504.18113"}
{"created":"2025-04-28","title":"Evaluating Evaluation Metrics -- The Mirage of Hallucination Detection","abstract":"Hallucinations pose a significant obstacle to the reliability and widespread adoption of language models, yet their accurate measurement remains a persistent challenge. While many task- and domain-specific metrics have been proposed to assess faithfulness and factuality concerns, the robustness and generalization of these metrics are still untested. In this paper, we conduct a large-scale empirical evaluation of 6 diverse sets of hallucination detection metrics across 4 datasets, 37 language models from 5 families, and 5 decoding methods. Our extensive investigation reveals concerning gaps in current hallucination evaluation: metrics often fail to align with human judgments, take an overtly myopic view of the problem, and show inconsistent gains with parameter scaling. Encouragingly, LLM-based evaluation, particularly with GPT-4, yields the best overall results, and mode-seeking decoding methods seem to reduce hallucinations, especially in knowledge-grounded settings. These findings underscore the need for more robust metrics to understand and quantify hallucinations, and better strategies to mitigate them.","authors":["Atharva Kulkarni","Yuan Zhang","Joel Ruben Antony Moniz","Xiou Ge","Bo-Hsiang Tseng","Dhivya Piraviperumal","Swabha Swayamdipta","Hong Yu"],"url":"https://arxiv.org/abs/2504.18114"}
{"created":"2025-04-28","title":"Think, Prune, Train, Improve: Scaling Reasoning without Scaling Models","abstract":"Large language models (LLMs) have demonstrated strong capabilities in programming and mathematical reasoning tasks, but are constrained by limited high-quality training data. Synthetic data can be leveraged to enhance fine-tuning outcomes, but several factors influence this process, including model size, synthetic data volume, pruning strategy, and number of fine-tuning rounds. We explore these axes and investigate which conditions enable model self-improvement. We introduce the Think, Prune, Train process, a scalable framework that iteratively fine-tunes models on their own reasoning traces, using ground-truth pruning to ensure high-quality training data. This approach yields improved performance: on GSM8K, Gemma2-2B achieves a Pass@1 of 57.6% (from 41.9%), Gemma2-9B reaches 82%, matching LLaMA-3.1-70B, and LLaMA-3.1-70B attains 91%, even surpassing GPT-4o, demonstrating the effectiveness of self-generated reasoning and systematic data selection for improving LLM capabilities.","authors":["Caia Costello","Simon Guo","Anna Goldie","Azalia Mirhoseini"],"url":"https://arxiv.org/abs/2504.18116"}
{"created":"2025-04-28","title":"Salient Region-Guided Spacecraft Image Arbitrary-Scale Super-Resolution Network","abstract":"Spacecraft image super-resolution seeks to enhance low-resolution spacecraft images into high-resolution ones. Although existing arbitrary-scale super-resolution methods perform well on general images, they tend to overlook the difference in features between the spacecraft core region and the large black space background, introducing irrelevant noise. In this paper, we propose a salient region-guided spacecraft image arbitrary-scale super-resolution network (SGSASR), which uses features from the spacecraft core salient regions to guide latent modulation and achieve arbitrary-scale super-resolution. Specifically, we design a spacecraft core region recognition block (SCRRB) that identifies the core salient regions in spacecraft images using a pre-trained saliency detection model. Furthermore, we present an adaptive-weighted feature fusion enhancement mechanism (AFFEM) to selectively aggregate the spacecraft core region features with general image features by dynamic weight parameter to enhance the response of the core salient regions. Experimental results demonstrate that the proposed SGSASR outperforms state-of-the-art approaches.","authors":["Jingfan Yang","Hu Gao","Ying Zhang","Depeng Dang"],"url":"https://arxiv.org/abs/2504.18127"}
{"created":"2025-04-28","title":"Temporal Entailment Pretraining for Clinical Language Models over EHR Data","abstract":"Clinical language models have achieved strong performance on downstream tasks by pretraining on domain specific corpora such as discharge summaries and medical notes. However, most approaches treat the electronic health record as a static document, neglecting the temporally-evolving and causally entwined nature of patient trajectories. In this paper, we introduce a novel temporal entailment pretraining objective for language models in the clinical domain. Our method formulates EHR segments as temporally ordered sentence pairs and trains the model to determine whether a later state is entailed by, contradictory to, or neutral with respect to an earlier state. Through this temporally structured pretraining task, models learn to perform latent clinical reasoning over time, improving their ability to generalize across forecasting and diagnosis tasks. We pretrain on a large corpus derived from MIMIC IV and demonstrate state of the art results on temporal clinical QA, early warning prediction, and disease progression modeling.","authors":["Tatsunori Tanaka","Fi Zheng","Kai Sato","Zhifeng Li","Yuanyun Zhang","Shi Li"],"url":"https://arxiv.org/abs/2504.18128"}
{"created":"2025-04-28","title":"Score-Based Deterministic Density Sampling","abstract":"We propose and analyze a deterministic sampling framework using Score-Based Transport Modeling (SBTM) for sampling an unnormalized target density $\\pi$. While diffusion generative modeling relies on pre-training the score function $\\nabla \\log f_t$ using samples from $\\pi$, SBTM addresses the more general and challenging setting where only $\\nabla \\log\\pi$ is known. SBTM approximates the Wasserstein gradient flow on KL$(f_t\\|\\pi)$ by learning the time-varying score $\\nabla \\log f_t$ on the fly using score matching. The learned score gives immediate access to relative Fisher information, providing a built-in convergence diagnostic. The deterministic trajectories are smooth, interpretable, and free of Brownian-motion noise, while having the same distribution as ULA. We prove that SBTM dissipates relative entropy at the same rate as the exact gradient flow, provided sufficient training. We further extend our framework to annealed dynamics, to handle non log-concave targets. Numerical experiments validate our theoretical findings: SBTM converges at the optimal rate, has smooth trajectories, and is easily integrated with annealed dynamics. We compare to the baselines of ULA and annealed ULA.","authors":["Vasily Ilin","Bamdad Hosseini","Jingwei Hu"],"url":"https://arxiv.org/abs/2504.18130"}
{"created":"2025-04-28","title":"SoK: Timeline based event reconstruction for digital forensics: Terminology, methodology, and current challenges","abstract":"Event reconstruction is a technique that examiners can use to attempt to infer past activities by analyzing digital artifacts. Despite its significance, the field suffers from fragmented research, with studies often focusing narrowly on aspects like timeline creation or tampering detection. This paper addresses the lack of a unified perspective by proposing a comprehensive framework for timeline-based event reconstruction, adapted from traditional forensic science models. We begin by harmonizing existing terminology and presenting a cohesive diagram that clarifies the relationships between key elements of the reconstruction process. Through a comprehensive literature survey, we classify and organize the main challenges, extending the discussion beyond common issues like data volume. Lastly, we highlight recent advancements and propose directions for future research, including specific research gaps. By providing a structured approach, key findings, and a clearer understanding of the underlying challenges, this work aims to strengthen the foundation of digital forensics.","authors":["Frank Breitinger","Hudan Studiawan","Chris Hargreaves"],"url":"https://arxiv.org/abs/2504.18131"}
{"created":"2025-04-28","title":"Tree Boosting Methods for Balanced andImbalanced Classification and their Robustness Over Time in Risk Assessment","abstract":"Most real-world classification problems deal with imbalanced datasets, posing a challenge for Artificial Intelligence (AI), i.e., machine learning algorithms, because the minority class, which is of extreme interest, often proves difficult to be detected. This paper empirically evaluates tree boosting methods' performance given different dataset sizes and class distributions, from perfectly balanced to highly imbalanced. For tabular data, tree-based methods such as XGBoost, stand out in several benchmarks due to detection performance and speed. Therefore, XGBoost and Imbalance-XGBoost are evaluated. After introducing the motivation to address risk assessment with machine learning, the paper reviews evaluation metrics for detection systems or binary classifiers. It proposes a method for data preparation followed by tree boosting methods including hyper-parameter optimization. The method is evaluated on private datasets of 1 thousand (K), 10K and 100K samples on distributions with 50, 45, 25, and 5 percent positive samples. As expected, the developed method increases its recognition performance as more data is given for training and the F1 score decreases as the data distribution becomes more imbalanced, but it is still significantly superior to the baseline of precision-recall determined by the ratio of positives divided by positives and negatives. Sampling to balance the training set does not provide consistent improvement and deteriorates detection. In contrast, classifier hyper-parameter optimization improves recognition, but should be applied carefully depending on data volume and distribution. Finally, the developed method is robust to data variation over time up to some point. Retraining can be used when performance starts deteriorating.","authors":["Gissel Velarde","Michael Weichert","Anuj Deshmunkh","Sanjay Deshmane","Anindya Sudhir","Khushboo Sharma","Vaibhav Joshi"],"url":"https://arxiv.org/abs/2504.18133"}
{"created":"2025-04-28","title":"MASF-YOLO: An Improved YOLOv11 Network for Small Object Detection on Drone View","abstract":"With the rapid advancement of Unmanned Aerial Vehicle (UAV) and computer vision technologies, object detection from UAV perspectives has emerged as a prominent research area. However, challenges for detection brought by the extremely small proportion of target pixels, significant scale variations of objects, and complex background information in UAV images have greatly limited the practical applications of UAV. To address these challenges, we propose a novel object detection network Multi-scale Context Aggregation and Scale-adaptive Fusion YOLO (MASF-YOLO), which is developed based on YOLOv11. Firstly, to tackle the difficulty of detecting small objects in UAV images, we design a Multi-scale Feature Aggregation Module (MFAM), which significantly improves the detection accuracy of small objects through parallel multi-scale convolutions and feature fusion. Secondly, to mitigate the interference of background noise, we propose an Improved Efficient Multi-scale Attention Module (IEMA), which enhances the focus on target regions through feature grouping, parallel sub-networks, and cross-spatial learning. Thirdly, we introduce a Dimension-Aware Selective Integration Module (DASI), which further enhances multi-scale feature fusion capabilities by adaptively weighting and fusing low-dimensional features and high-dimensional features. Finally, we conducted extensive performance evaluations of our proposed method on the VisDrone2019 dataset. Compared to YOLOv11-s, MASFYOLO-s achieves improvements of 4.6% in mAP@0.5 and 3.5% in mAP@0.5:0.95 on the VisDrone2019 validation set. Remarkably, MASF-YOLO-s outperforms YOLOv11-m while requiring only approximately 60% of its parameters and 65% of its computational cost. Furthermore, comparative experiments with state-of-the-art detectors confirm that MASF-YOLO-s maintains a clear competitive advantage in both detection accuracy and model efficiency.","authors":["Liugang Lu","Dabin He","Congxiang Liu","Zhixiang Deng"],"url":"https://arxiv.org/abs/2504.18136"}
{"created":"2025-04-28","title":"Revisiting Algorithmic Audits of TikTok: Poor Reproducibility and Short-term Validity of Findings","abstract":"Social media platforms are constantly shifting towards algorithmically curated content based on implicit or explicit user feedback. Regulators, as well as researchers, are calling for systematic social media algorithmic audits as this shift leads to enclosing users in filter bubbles and leading them to more problematic content. An important aspect of such audits is the reproducibility and generalisability of their findings, as it allows to draw verifiable conclusions and audit potential changes in algorithms over time. In this work, we study the reproducibility of the existing sockpuppeting audits of TikTok recommender systems, and the generalizability of their findings. In our efforts to reproduce the previous works, we find multiple challenges stemming from social media platform changes and content evolution, but also the research works themselves. These drawbacks limit the audit reproducibility and require an extensive effort altogether with inevitable adjustments to the auditing methodology. Our experiments also reveal that these one-shot audit findings often hold only in the short term, implying that the reproducibility and generalizability of the audits heavily depend on the methodological choices and the state of algorithms and content on the platform. This highlights the importance of reproducible audits that allow us to determine how the situation changes in time.","authors":["Matej Mosnar","Adam Skurla","Branislav Pecher","Matus Tibensky","Jan Jakubcik","Adrian Bindas","Peter Sakalik","Ivan Srba"],"url":"https://arxiv.org/abs/2504.18140"}
{"created":"2025-04-28","title":"EDU-NER-2025: Named Entity Recognition in Urdu Educational Texts using XLM-RoBERTa with X (formerly Twitter)","abstract":"Named Entity Recognition (NER) plays a pivotal role in various Natural Language Processing (NLP) tasks by identifying and classifying named entities (NEs) from unstructured data into predefined categories such as person, organization, location, date, and time. While extensive research exists for high-resource languages and general domains, NER in Urdu particularly within domain-specific contexts like education remains significantly underexplored. This is Due to lack of annotated datasets for educational content which limits the ability of existing models to accurately identify entities such as academic roles, course names, and institutional terms, underscoring the urgent need for targeted resources in this domain. To the best of our knowledge, no dataset exists in the domain of the Urdu language for this purpose. To achieve this objective this study makes three key contributions. Firstly, we created a manually annotated dataset in the education domain, named EDU-NER-2025, which contains 13 unique most important entities related to education domain. Second, we describe our annotation process and guidelines in detail and discuss the challenges of labelling EDU-NER-2025 dataset. Third, we addressed and analyzed key linguistic challenges, such as morphological complexity and ambiguity, which are prevalent in formal Urdu texts.","authors":["Fida Ullah","Muhammad Ahmad","Muhammad Tayyab Zamir","Muhammad Arif","Grigori sidorov","Edgardo Manuel Felipe River\\'on","Alexander Gelbukh"],"url":"https://arxiv.org/abs/2504.18142"}
{"created":"2025-04-28","title":"Tutte's theorem as an educational formalization project","abstract":"In this work, we present two results: The first result is the formalization of Tutte's theorem in Lean, a key theorem concerning matchings in graph theory. As this formalization is ready to be integrated in Lean's mathlib, it provides a valuable step in the path towards formalizing research-level mathematics in this area. The second result is a framework for doing educational formalization projects. This framework provides a structure to learn to formalize mathematics with minimal teacher input. This framework applies to both traditional academic settings and independent community-driven environments. We demonstrate the framework's use by connecting it to the process of formalizing Tutte's theorem.","authors":["Pim Otte"],"url":"https://arxiv.org/abs/2504.18146"}
{"created":"2025-04-28","title":"NoEsis: Differentially Private Knowledge Transfer in Modular LLM Adaptation","abstract":"Large Language Models (LLM) are typically trained on vast amounts of data from various sources. Even when designed modularly (e.g., Mixture-of-Experts), LLMs can leak privacy on their sources. Conversely, training such models in isolation arguably prohibits generalization. To this end, we propose a framework, NoEsis, which builds upon the desired properties of modularity, privacy, and knowledge transfer. NoEsis integrates differential privacy with a hybrid two-staged parameter-efficient fine-tuning that combines domain-specific low-rank adapters, acting as experts, with common prompt tokens, acting as a knowledge-sharing backbone. Results from our evaluation on CodeXGLUE showcase that NoEsis can achieve provable privacy guarantees with tangible knowledge transfer across domains, and empirically show protection against Membership Inference Attacks. Finally, on code completion tasks, NoEsis bridges at least 77% of the accuracy gap between the non-shared and the non-private baseline.","authors":["Rob Romijnders","Stefanos Laskaridis","Ali Shahin Shamsabadi","Hamed Haddadi"],"url":"https://arxiv.org/abs/2504.18147"}
{"created":"2025-04-28","title":"A Generative Graph Contrastive Learning Model with Global Signal","abstract":"Graph contrastive learning (GCL) has garnered significant attention recently since it learns complex structural information from graphs through self-supervised learning manner. However, prevalent GCL models may suffer from performance degradation due to inappropriate contrastive signals. Concretely, they commonly generate augmented views based on random perturbation, which leads to biased essential structures due to the introduction of noise. In addition, they assign equal weight to both hard and easy sample pairs, thereby ignoring the difference in importance of the sample pairs. To address these issues, this study proposes a novel Contrastive Signal Generative Framework for Accurate Graph Learning (CSG2L) with the following two-fold ideas: a) building a singular value decomposition (SVD)-directed augmented module (SVD-aug) to obtain the global interactions as well as avoiding the random noise perturbation; b) designing a local-global dependency learning module (LGDL) with an adaptive reweighting strategy which can differentiate the effects of hard and easy sample pairs. Extensive experiments on benchmark datasets demonstrate that the proposed CSG2L outperforms the state-of-art baselines. Moreover, CSG2L is compatible with a variety of GNNs.","authors":["Xiaofan Wei","Binyan Zhang"],"url":"https://arxiv.org/abs/2504.18148"}
{"created":"2025-04-28","title":"Toward Automated Test Generation for Dockerfiles Based on Analysis of Docker Image Layers","abstract":"Docker has gained attention as a lightweight container-based virtualization platform. The process for building a Docker image is defined in a text file called a Dockerfile. A Dockerfile can be considered as a kind of source code that contains instructions on how to build a Docker image. Its behavior should be verified through testing, as is done for source code in a general programming language. For source code in languages such as Java, search-based test generation techniques have been proposed. However, existing automated test generation techniques cannot be applied to Dockerfiles. Since a Dockerfile does not contain branches, the coverage metric, typically used as an objective function in existing methods, becomes meaningless. In this study, we propose an automated test generation method for Dockerfiles based on processing results rather than processing steps. The proposed method determines which files should be tested and generates the corresponding tests based on an analysis of Dockerfile instructions and Docker image layers. The experimental results show that the proposed method can reproduce over 80% of the tests created by developers.","authors":["Yuki Goto","Shinsuke Matsumoto","Shinji Kusumoto"],"url":"https://arxiv.org/abs/2504.18150"}
{"created":"2025-04-28","title":"Leveraging Decoder Architectures for Learned Sparse Retrieval","abstract":"Learned Sparse Retrieval (LSR) has traditionally focused on small-scale encoder-only transformer architectures. With the advent of large-scale pre-trained language models, their capability to generate sparse representations for retrieval tasks across different transformer-based architectures, including encoder-only, decoder-only, and encoder-decoder models, remains largely unexplored. This study investigates the effectiveness of LSR across these architectures, exploring various sparse representation heads and model scales. Our results highlight the limitations of using large language models to create effective sparse representations in zero-shot settings, identifying challenges such as inappropriate term expansions and reduced performance due to the lack of expansion. We find that the encoder-decoder architecture with multi-tokens decoding approach achieves the best performance among the three backbones. While the decoder-only model performs worse than the encoder-only model, it demonstrates the potential to outperform when scaled to a high number of parameters.","authors":["Jingfen Qiao","Thong Nguyen","Evangelos Kanoulas","Andrew Yates"],"url":"https://arxiv.org/abs/2504.18151"}
{"created":"2025-04-28","title":"ActionArt: Advancing Multimodal Large Models for Fine-Grained Human-Centric Video Understanding","abstract":"Fine-grained understanding of human actions and poses in videos is essential for human-centric AI applications. In this work, we introduce ActionArt, a fine-grained video-caption dataset designed to advance research in human-centric multimodal understanding. Our dataset comprises thousands of videos capturing a broad spectrum of human actions, human-object interactions, and diverse scenarios, each accompanied by detailed annotations that meticulously label every limb movement. We develop eight sub-tasks to evaluate the fine-grained understanding capabilities of existing large multimodal models across different dimensions. Experimental results indicate that, while current large multimodal models perform commendably on various tasks, they often fall short in achieving fine-grained understanding. We attribute this limitation to the scarcity of meticulously annotated data, which is both costly and difficult to scale manually. Since manual annotations are costly and hard to scale, we propose proxy tasks to enhance the model perception ability in both spatial and temporal dimensions. These proxy tasks are carefully crafted to be driven by data automatically generated from existing MLLMs, thereby reducing the reliance on costly manual labels. Experimental results show that the proposed proxy tasks significantly narrow the gap toward the performance achieved with manually annotated fine-grained data.","authors":["Yi-Xing Peng","Qize Yang","Yu-Ming Tang","Shenghao Fu","Kun-Yu Lin","Xihan Wei","Wei-Shi Zheng"],"url":"https://arxiv.org/abs/2504.18152"}
{"created":"2025-04-28","title":"Multiple Target Tracking Using a UAV Swarm in Maritime Environments","abstract":"Nowadays, unmanned aerial vehicles (UAVs) are increasingly utilized in search and rescue missions, a trend driven by technological advancements, including enhancements in automation, avionics, and the reduced cost of electronics. In this work, we introduce a collaborative model predictive control (MPC) framework aimed at addressing the joint problem of guidance and state estimation for tracking multiple castaway targets with a fleet of autonomous UAV agents. We assume that each UAV agent is equipped with a camera sensor, which has a limited sensing range and is utilized for receiving noisy observations from multiple moving castaways adrift in maritime conditions. We derive a nonlinear mixed integer programming (NMIP) -based controller that facilitates the guidance of the UAVs by generating non-myopic trajectories within a receding planning horizon. These trajectories are designed to minimize the tracking error across multiple targets by directing the UAV fleet to locations expected to yield targets measurements, thereby minimizing the uncertainty of the estimated target states. Extensive simulation experiments validate the effectiveness of our proposed method in tracking multiple castaways in maritime environments.","authors":["Andreas Anastasiou","Savvas Papaioannou","Panayiotis Kolios","Christos G. Panayiotou"],"url":"https://arxiv.org/abs/2504.18153"}
{"created":"2025-04-28","title":"EcoServe: Enabling Cost-effective LLM Serving with Proactive Intra- and Inter-Instance Orchestration","abstract":"Existing LLM serving strategies can be categorized based on whether prefill and decode phases are disaggregated: non-disaggregated (NoDG) or fully disaggregated (FuDG). However, the NoDG strategy leads to strong prefill-decode interference and the FuDG strategy highly relies on high-performance interconnects, making them less cost-effective.","authors":["Jiangsu Du","Hongbin Zhang","Taosheng Wei","Zhenyi Zheng","Kaiyi Wu","Zhiguang Chen","Yutong Lu"],"url":"https://arxiv.org/abs/2504.18154"}
{"created":"2025-04-28","title":"Hierarchical Cell-Free Massive MIMO: A Simplified Design for Uniform Service Quality","abstract":"In traditional cellular networks, users at the cell edge often suffer from poor quality of service (QoS) due to large distance-dependent path loss and severe inter-cell interference. While cell-free (CF) massive multi-input multi-out (MIMO) mitigates this issue by distributing access points (APs) to ensure uniform QoS, the deployment of numerous distributed APs and a fronthaul network incurs high infrastructure costs. To balance performance and cost efficiency, this article proposes a simplified design called hierarchical cell-free (HCF) massive MIMO. The key idea is to reduce the number of APs, thus minimizing the scale of the fronthaul network. The antennas from the decommissioned APs are aggregated at a central base station (cBS), which also serves as the coordinator for distributed APs. We derive closed-form expressions for uplink and downlink spectral efficiency (SE) for HCF, CF, and cellular massive MIMO under pilot contamination and correlated fading channels, considering the use of multi-antenna APs. Numerical results confirm that the hierarchical architecture achieves $95\\%$-likely per-user SE comparable to CF, enhancing cell-edge user rates in cellular systems by over 100 times, while significantly reducing the complexity and cost of the fronthaul network in CF. We develop max-min fairness algorithms for joint power control of the cBS and APs in the downlink, and the users in the uplink. These algorithms not only boost fairness and system capacity but also dramatically lower transmission power, e.g., achieving over $70\\%$ savings in uplink, particularly beneficial for battery-powered mobile devices.","authors":["Wei Jiang","Hans Dieter Schotten"],"url":"https://arxiv.org/abs/2504.18155"}
{"created":"2025-04-28","title":"E-InMeMo: Enhanced Prompting for Visual In-Context Learning","abstract":"Large-scale models trained on extensive datasets have become the standard due to their strong generalizability across diverse tasks. In-context learning (ICL), widely used in natural language processing, leverages these models by providing task-specific prompts without modifying their parameters. This paradigm is increasingly being adapted for computer vision, where models receive an input-output image pair, known as an in-context pair, alongside a query image to illustrate the desired output. However, the success of visual ICL largely hinges on the quality of these prompts. To address this, we propose Enhanced Instruct Me More (E-InMeMo), a novel approach that incorporates learnable perturbations into in-context pairs to optimize prompting. Through extensive experiments on standard vision tasks, E-InMeMo demonstrates superior performance over existing state-of-the-art methods. Notably, it improves mIoU scores by 7.99 for foreground segmentation and by 17.04 for single object detection when compared to the baseline without learnable prompts. These results highlight E-InMeMo as a lightweight yet effective strategy for enhancing visual ICL. Code is publicly available at: https://github.com/Jackieam/E-InMeMo","authors":["Jiahao Zhang","Bowen Wang","Hong Liu","Liangzhi Li","Yuta Nakashima","Hajime Nagahara"],"url":"https://arxiv.org/abs/2504.18158"}
{"created":"2025-04-28","title":"Offline Learning of Controllable Diverse Behaviors","abstract":"Imitation Learning (IL) techniques aim to replicate human behaviors in specific tasks. While IL has gained prominence due to its effectiveness and efficiency, traditional methods often focus on datasets collected from experts to produce a single efficient policy. Recently, extensions have been proposed to handle datasets of diverse behaviors by mainly focusing on learning transition-level diverse policies or on performing entropy maximization at the trajectory level. While these methods may lead to diverse behaviors, they may not be sufficient to reproduce the actual diversity of demonstrations or to allow controlled trajectory generation. To overcome these drawbacks, we propose a different method based on two key features: a) Temporal Consistency that ensures consistent behaviors across entire episodes and not just at the transition level as well as b) Controllability obtained by constructing a latent space of behaviors that allows users to selectively activate specific behaviors based on their requirements. We compare our approach to state-of-the-art methods over a diverse set of tasks and environments. Project page: https://mathieu-petitbois.github.io/projects/swr/","authors":["Mathieu Petitbois","R\\'emy Portelas","Sylvain Lamprier","Ludovic Denoyer"],"url":"https://arxiv.org/abs/2504.18160"}
{"created":"2025-04-28","title":"Fully Dynamic Algorithms for Transitive Reduction","abstract":"Given a directed graph $G$, a transitive reduction $G^t$ of $G$ (first studied by Aho, Garey, Ullman [SICOMP `72]) is a minimal subgraph of $G$ that preserves the reachability relation between every two vertices in $G$.","authors":["Gramoz Goranci","Adam Karczmarz","Ali Momeni","Nikos Parotsidis"],"url":"https://arxiv.org/abs/2504.18161"}
{"created":"2025-04-28","title":"Recent advances in data-driven methods for degradation modelling across applications","abstract":"Understanding degradation is crucial for ensuring the longevity and performance of materials, systems, and organisms. To illustrate the similarities across applications, this article provides a review of data-based method in materials science, engineering, and medicine. The methods analyzed in this paper include regression analysis, factor analysis, cluster analysis, Markov Chain Monte Carlo, Bayesian statistics, hidden Markov models, nonparametric Bayesian modeling of time series, supervised learning, and deep learning. The review provides an overview of degradation models, referencing books and methods, and includes detailed tables highlighting the applications and insights offered in medicine, power engineering, and material science. It also discusses the classification of methods, emphasizing statistical inference, dynamic prediction, machine learning, and hybrid modeling techniques. Overall, this review enhances understanding of degradation modelling across diverse domains.","authors":["Anna Jarosz","Marta Zagorowska","Jerzy Baranowski"],"url":"https://arxiv.org/abs/2504.18164"}
{"created":"2025-04-28","title":"PerfCam: Digital Twinning for Production Lines Using 3D Gaussian Splatting and Vision Models","abstract":"We introduce PerfCam, an open source Proof-of-Concept (PoC) digital twinning framework that combines camera and sensory data with 3D Gaussian Splatting and computer vision models for digital twinning, object tracking, and Key Performance Indicators (KPIs) extraction in industrial production lines. By utilizing 3D reconstruction and Convolutional Neural Networks (CNNs), PerfCam offers a semi-automated approach to object tracking and spatial mapping, enabling digital twins that capture real-time KPIs such as availability, performance, Overall Equipment Effectiveness (OEE), and rate of conveyor belts in the production line. We validate the effectiveness of PerfCam through a practical deployment within realistic test production lines in the pharmaceutical industry and contribute an openly published dataset to support further research and development in the field. The results demonstrate PerfCam's ability to deliver actionable insights through its precise digital twin capabilities, underscoring its value as an effective tool for developing usable digital twins in smart manufacturing environments and extracting operational analytics.","authors":["Michel Gokan Khan","Renan Guarese","Fabian Johnson","Xi Vincent Wang","Anders Bergman","Benjamin Edvinsson","Mario Romero","J\\'er\\'emy Vachier","Jan Kronqvist"],"url":"https://arxiv.org/abs/2504.18165"}
{"created":"2025-04-28","title":"Revolutionizing Symbiotic Radio: Exploiting Tradeoffs in Hybrid Active-Passive Communications","abstract":"Symbiotic radio (SR), a novel energy- and spectrum-sharing paradigm of backscatter communications (BC), has been deemed a promising solution for ambient Internet of Things (A-IoT), enabling ultra-low power consumption and massive connectivity. However, A-IoT nodes utilizing BC suffer from low transmission rates, which may limit the applications of SR in A-IoT scenarios with data transmission requirements. To address this issue, in this article, we introduce hybrid active-passive communications (HAPC) into SR by exploiting tradeoffs between transmission rate and power consumption. We first present an overview of novel BC paradigms including ambient BC and SR. Then, a novel HAPC-enabled SR is proposed to enhance the transmission rate of A-IoT nodes. Furthermore, within this paradigm, we investigate the resource allocation scheme and present preliminary research results. Simulation results show that the transmission rate of A-IoT nodes in the proposed HAPC-enabled SR surpasses that in traditional SR. Finally, we discuss open issues related to HAPC-enabled SR.","authors":["Rui Xu","Yinghui Ye","Haijian Sun","Liqin Shi","Guangyue Lu"],"url":"https://arxiv.org/abs/2504.18168"}
{"created":"2025-04-28","title":"Reimagining Assistive Walkers: An Exploration of Challenges and Preferences in Older Adults","abstract":"The well-being of older adults relies significantly on maintaining balance and mobility. As physical ability declines, older adults often accept the need for assistive devices. However, existing walkers frequently fail to consider user preferences, leading to perceptions of imposition and reduced acceptance. This research explores the challenges faced by older adults, caregivers, and healthcare professionals when using walkers, assesses their perceptions, and identifies their needs and preferences. A holistic approach was employed, using tailored perception questionnaires for older adults (24 participants), caregivers (30 participants), and healthcare professionals (27 participants), all of whom completed the survey. Over 50% of caregivers and healthcare professionals displayed good knowledge, positive attitudes, and effective practices regarding walkers. However, over 30% of participants perceived current designs as fall risks, citing the need for significant upper body strength, potentially affecting safety and movement. More than 50% highlighted the importance of incorporating fall detection, ergonomic designs, noise reduction, and walker ramps to better meet user needs and preferences.","authors":["Victory A. Aruona","Sergio D. Sierra M.","Nigel Harris","Marcela Munera","Carlos A. Cifuentes"],"url":"https://arxiv.org/abs/2504.18169"}
{"created":"2025-04-28","title":"On the approximation of the von Neumann equation in the semi-classical limit. Part II : numerical analysis","abstract":"This paper is devoted to the numerical analysis of the Hermite spectral method proposed in [14], which provides, in the semi-classical limit, an asymptotic preserving approximation of the von Neumann equation. More precisely, it relies on the use of so-called Weyl's variables to effectively address the stiffness associated to the equation. Then by employing a truncated Hermite expansion of the density operator, we successfully manage this stiffness and provide error estimates by leveraging the propagation of regularity in the exact solution.","authors":["Fran\\c{c}ois Golse (X)","Francis Filbet (IMT)"],"url":"https://arxiv.org/abs/2504.18177"}
{"created":"2025-04-28","title":"Smallest Intersecting and Enclosing Balls","abstract":"We study the smallest intersecting and enclosing ball problems in Euclidean spaces for input objects that are compact and convex. They link and unify many problems in computational geometry and machine learning. We show that both problems can be modeled as zero-sum games, and propose an approximation algorithm for the former. Specifically, the algorithm produces the first results in high-dimensional spaces for various input objects such as convex polytopes, balls, ellipsoids, etc.","authors":["Jiaqi Zheng","Tiow-Seng Tan"],"url":"https://arxiv.org/abs/2504.18178"}
{"created":"2025-04-28","title":"Label-independent hyperparameter-free self-supervised single-view deep subspace clustering","abstract":"Deep subspace clustering (DSC) algorithms face several challenges that hinder their widespread adoption across variois application domains. First, clustering quality is typically assessed using only the encoder's output layer, disregarding valuable information present in the intermediate layers. Second, most DSC approaches treat representation learning and subspace clustering as independent tasks, limiting their effectiveness. Third, they assume the availability of a held-out dataset for hyperparameter tuning, which is often impractical in real-world scenarios. Fourth, learning termination is commonly based on clustering error monitoring, requiring external labels. Finally, their performance often depends on post-processing techniques that rely on labeled data. To address this limitations, we introduce a novel single-view DSC approach that: (i) minimizes a layer-wise self expression loss using a joint representation matrix; (ii) optimizes a subspace-structured norm to enhance clustering quality; (iii) employs a multi-stage sequential learning framework, consisting of pre-training and fine-tuning, enabling the use of multiple regularization terms without hyperparameter tuning; (iv) incorporates a relative error-based self-stopping mechanism to terminate training without labels; and (v) retains a fixed number of leading coefficients in the learned representation matrix based on prior knowledge. We evaluate the proposed method on six datasets representing faces, digits, and objects. The results show that our method outperforms most linear SC algorithms with careffulyl tuned hyperparameters while maintaining competitive performance with the best performing linear appoaches.","authors":["Lovro Sindicic","Ivica Kopriva"],"url":"https://arxiv.org/abs/2504.18179"}
{"created":"2025-04-28","title":"Aligning Language Models for Icelandic Legal Text Summarization","abstract":"The integration of language models in the legal domain holds considerable promise for streamlining processes and improving efficiency in managing extensive workloads. However, the specialized terminology, nuanced language, and formal style of legal texts can present substantial challenges. This study examines whether preference-based training techniques, specifically Reinforcement Learning from Human Feedback and Direct Preference Optimization, can enhance models' performance in generating Icelandic legal summaries that align with domain-specific language standards and user preferences. We compare models fine-tuned with preference training to those using conventional supervised learning. Results indicate that preference training improves the legal accuracy of generated summaries over standard fine-tuning but does not significantly enhance the overall quality of Icelandic language usage. Discrepancies between automated metrics and human evaluations further underscore the importance of qualitative assessment in developing language models for the legal domain.","authors":["{\\TH}\\'orir Hrafn Har{\\dh}arson","Hrafn Loftsson","Stef\\'an \\'Olafsson"],"url":"https://arxiv.org/abs/2504.18180"}
{"created":"2025-04-28","title":"Unveiling 3D Ocean Biogeochemical Provinces: A Machine Learning Approach for Systematic Clustering and Validation","abstract":"Defining ocean regions and water masses helps to understand marine processes and can serve downstream-tasks such as defining marine protected areas. However, such definitions are often a result of subjective decisions potentially producing misleading, unreproducible results. Here, the aim was to objectively define regions of the North Atlantic. For this, a data-driven, systematic machine learning approach was applied to generate and validate ocean clusters employing external, internal and relative validation techniques. About 300 million measured salinity, temperature, and oxygen, nitrate, phosphate and silicate concentration values served as input for various clustering methods (KMeans, agglomerative Ward, and Density-Based Spatial Clustering of Applications with Noise (DBSCAN)). Uniform Manifold Approximation and Projection (UMAP) emphasised (dis-)similarities in the data while reducing dimensionality. Based on a systematic validation of the considered clustering methods and their hyperparameters, the results showed that UMAP-DBSCAN best represented the data. To address stochastic variability, 100 UMAP-DBSCAN clustering runs were conducted and aggregated using Native Emergent Manifold Interrogation (NEMI), producing a final set of 321 clusters. Reproducibility was evaluated by calculating the ensemble overlap (88.81 +- 1.8%) and the mean grid cell-wise uncertainty estimated by NEMI (15.49 +- 20%). The presented clustering results agreed very well with common water mass definitions. This study revealed a more detailed regionalization compared to previous concepts such as the Longhurst provinces. The applied method is objective, efficient and reproducible and will support future research focusing on biogeochemical differences and changes in oceanic regions.","authors":["Yvonne Jenniges","Maike Sonnewald","Sebastian Maneth","Are Olsen","Boris P. Koch"],"url":"https://arxiv.org/abs/2504.18181"}
{"created":"2025-04-28","title":"What Happened in This Pipeline? Diffing Build Logs with CiDiff","abstract":"Continuous integration (CI) is widely used by developers to ensure the quality and reliability of their software projects. However, diagnosing a CI regression is a tedious process that involves the manual analysis of lengthy build logs. In this paper, we explore how textual differencing can support the debugging of CI regressions. As off-the-shelf diff algorithms produce suboptimal results, in this work we introduce a new diff algorithm specifically tailored to build logs called CiDiff. We evaluate CiDiff against several baselines on a novel dataset of 17 906 CI regressions, performing an accuracy study, a quantitative study and a user-study. Notably, our algorithm reduces the number of lines to inspect by about 60 % in the median case, with reasonable overhead compared to the state-of-practice LCS-diff. Finally, our algorithm is preferred by the majority of participants in 70 % of the regression cases, whereas LCS-diff is preferred in only 5 % of the cases.","authors":["Nicolas Hubner (LaBRI)","Jean-R\\'emy Falleri (LaBRI)","Raluca Uricaru (LaBRI","CNRS","Bordeaux INP","UB)","Thomas Degueule (LaBRI)","Thomas Durieux (SPIRALS)"],"url":"https://arxiv.org/abs/2504.18182"}
{"created":"2025-04-28","title":"An Open-Source and Reproducible Implementation of LSTM and GRU Networks for Time Series Forecasting","abstract":"This paper introduces an open-source and reproducible implementation of Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) Networks for time series forecasting. We evaluated LSTM and GRU networks because of their performance reported in related work. We describe our method and its results on two datasets. The first dataset is the S&amp;P BSE BANKEX, composed of stock time series (closing prices) of ten financial institutions. The second dataset, called Activities, comprises ten synthetic time series resembling weekly activities with five days of high activity and two days of low activity. We report Root Mean Squared Error (RMSE) between actual and predicted values, as well as Directional Accuracy (DA). We show that a single time series from a dataset can be used to adequately train the networks if the sequences in the dataset contain patterns that repeat, even with certain variation, and are properly processed. For 1-step ahead and 20-step ahead forecasts, LSTM and GRU networks significantly outperform a baseline on the Activities dataset. The baseline simply repeats the last available value. On the stock market dataset, the networks perform just like the baseline, possibly due to the nature of these series. We release the datasets used as well as the implementation with all experiments performed to enable future comparisons and to make our research reproducible.","authors":["Gissel Velarde","Pedro Branez","Alejandro Bueno","Rodrigo Heredia","Mateo Lopez-Ledezma"],"url":"https://arxiv.org/abs/2504.18185"}
{"created":"2025-04-28","title":"Sampling-Based Grasp and Collision Prediction for Assisted Teleoperation","abstract":"Shared autonomy allows for combining the global planning capabilities of a human operator with the strengths of a robot such as repeatability and accurate control. In a real-time teleoperation setting, one possibility for shared autonomy is to let the human operator decide for the rough movement and to let the robot do fine adjustments, e.g., when the view of the operator is occluded. We present a learning-based concept for shared autonomy that aims at supporting the human operator in a real-time teleoperation setting. At every step, our system tracks the target pose set by the human operator as accurately as possible while at the same time satisfying a set of constraints which influence the robot's behavior. An important characteristic is that the constraints can be dynamically activated and deactivated which allows the system to provide task-specific assistance. Since the system must generate robot commands in real-time, solving an optimization problem in every iteration is not feasible. Instead, we sample potential target configurations and use Neural Networks for predicting the constraint costs for each configuration. By evaluating each configuration in parallel, our system is able to select the target configuration which satisfies the constraints and has the minimum distance to the operator's target pose with minimal delay. We evaluate the framework with a pick and place task on a bi-manual setup with two Franka Emika Panda robot arms with Robotiq grippers.","authors":["Simon Manschitz","Berk Gueler","Wei Ma","Dirk Ruiken"],"url":"https://arxiv.org/abs/2504.18186"}
{"created":"2025-04-28","title":"ClassComet: Exploring and Designing AI-generated Danmaku in Educational Videos to Enhance Online Learning","abstract":"Danmaku, users' live comments synchronized with, and overlaying on videos, has recently shown potential in promoting online video-based learning. However, user-generated danmaku can be scarce-especially in newer or less viewed videos and its quality is unpredictable, limiting its educational impact. This paper explores how large multimodal models (LMM) can be leveraged to automatically generate effective, high-quality danmaku. We first conducted a formative study to identify the desirable characteristics of content- and emotion-related danmaku in educational videos. Based on the obtained insights, we developed ClassComet, an educational video platform with novel LMM-driven techniques for generating relevant types of danmaku to enhance video-based learning. Through user studies, we examined the quality of generated danmaku and their influence on learning experiences. The results indicate that our generated danmaku is comparable to human-created ones, and videos with both content- and emotion-related danmaku showed significant improvement in viewers' engagement and learning outcome.","authors":["Zipeng Ji","Pengcheng An","Jian Zhao"],"url":"https://arxiv.org/abs/2504.18189"}
{"created":"2025-04-28","title":"What is the Added Value of UDA in the VFM Era?","abstract":"Unsupervised Domain Adaptation (UDA) can improve a perception model's generalization to an unlabeled target domain starting from a labeled source domain. UDA using Vision Foundation Models (VFMs) with synthetic source data can achieve generalization performance comparable to fully-supervised learning with real target data. However, because VFMs have strong generalization from their pre-training, more straightforward, source-only fine-tuning can also perform well on the target. As data scenarios used in academic research are not necessarily representative for real-world applications, it is currently unclear (a) how UDA behaves with more representative and diverse data and (b) if source-only fine-tuning of VFMs can perform equally well in these scenarios. Our research aims to close these gaps and, similar to previous studies, we focus on semantic segmentation as a representative perception task. We assess UDA for synth-to-real and real-to-real use cases with different source and target data combinations. We also investigate the effect of using a small amount of labeled target data in UDA. We clarify that while these scenarios are more realistic, they are not necessarily more challenging. Our results show that, when using stronger synthetic source data, UDA's improvement over source-only fine-tuning of VFMs reduces from +8 mIoU to +2 mIoU, and when using more diverse real source data, UDA has no added value. However, UDA generalization is always higher in all synthetic data scenarios than source-only fine-tuning and, when including only 1/16 of Cityscapes labels, synthetic UDA obtains the same state-of-the-art segmentation quality of 85 mIoU as a fully-supervised model using all labels. Considering the mixed results, we discuss how UDA can best support robust autonomous driving at scale.","authors":["Brun\\'o B. Englert","Tommie Kerssies","Gijs Dubbelman"],"url":"https://arxiv.org/abs/2504.18190"}
{"created":"2025-04-28","title":"MVVM Revisited: Exploring Design Variants of the Model-View-ViewModel Pattern","abstract":"Many enterprise software systems provide complex Graphical User Interfaces (GUIs) that need robust architectural patterns for well-structured software design. However, popular GUI architectural patterns like Model-View-ViewModel (MVVM) often lack detailed implementation guidance, leading GUI developers to inappropriately use the pattern without a comprehensive overview of design variants and often-mentioned trade-offs. Therefore, this paper presents an extensive review of MVVM design aspects and trade-offs, extending beyond the standard MVVM definition. We conducted a multivocal literature review (MLR), including white and gray literature, to cover essential knowledge from blogs, published papers, and other unpublished formats like books. Using the standard MVVM definition as a baseline, our study identifies (1) 76 additional design constructs grouped into 29 design aspects and (2) 16 additional benefits and 15 additional drawbacks. These insights can guide enterprise application developers in implementing practical MVVM solutions and enable informed design decisions.","authors":["Mario Fuksa","Sandro Speth","Steffen Becker"],"url":"https://arxiv.org/abs/2504.18191"}
{"created":"2025-04-28","title":"Implementation Analysis of Collaborative Robot Digital Twins in Physics Engines","abstract":"This paper presents a Digital Twin (DT) of a 6G communications system testbed that integrates two robotic manipulators with a high-precision optical infrared tracking system in Unreal Engine 5. Practical details of the setup and implementation insights provide valuable guidance for users aiming to replicate such systems, an endeavor that is crucial to advancing DT applications within the scientific community. Key topics discussed include video streaming, integration within the Robot Operating System 2 (ROS 2), and bidirectional communication. The insights provided are intended to support the development and deployment of DTs in robotics and automation research.","authors":["Christian K\\\"onig","Jan Petershans","Jan Herbst","Matthias R\\\"ub","Dennis Krummacker","Eric Mittag","Hand D. Schooten"],"url":"https://arxiv.org/abs/2504.18200"}
{"created":"2025-04-28","title":"Multi-Grained Compositional Visual Clue Learning for Image Intent Recognition","abstract":"In an era where social media platforms abound, individuals frequently share images that offer insights into their intents and interests, impacting individual life quality and societal stability. Traditional computer vision tasks, such as object detection and semantic segmentation, focus on concrete visual representations, while intent recognition relies more on implicit visual clues. This poses challenges due to the wide variation and subjectivity of such clues, compounded by the problem of intra-class variety in conveying abstract concepts, e.g. \"enjoy life\". Existing methods seek to solve the problem by manually designing representative features or building prototypes for each class from global features. However, these methods still struggle to deal with the large visual diversity of each intent category. In this paper, we introduce a novel approach named Multi-grained Compositional visual Clue Learning (MCCL) to address these challenges for image intent recognition. Our method leverages the systematic compositionality of human cognition by breaking down intent recognition into visual clue composition and integrating multi-grained features. We adopt class-specific prototypes to alleviate data imbalance. We treat intent recognition as a multi-label classification problem, using a graph convolutional network to infuse prior knowledge through label embedding correlations. Demonstrated by a state-of-the-art performance on the Intentonomy and MDID datasets, our approach advances the accuracy of existing methods while also possessing good interpretability. Our work provides an attempt for future explorations in understanding complex and miscellaneous forms of human expression.","authors":["Yin Tang","Jiankai Li","Hongyu Yang","Xuan Dong","Lifeng Fan","Weixin Li"],"url":"https://arxiv.org/abs/2504.18201"}
{"created":"2025-04-28","title":"LiDAR-Guided Monocular 3D Object Detection for Long-Range Railway Monitoring","abstract":"Railway systems, particularly in Germany, require high levels of automation to address legacy infrastructure challenges and increase train traffic safely. A key component of automation is robust long-range perception, essential for early hazard detection, such as obstacles at level crossings or pedestrians on tracks. Unlike automotive systems with braking distances of ~70 meters, trains require perception ranges exceeding 1 km. This paper presents an deep-learning-based approach for long-range 3D object detection tailored for autonomous trains. The method relies solely on monocular images, inspired by the Faraway-Frustum approach, and incorporates LiDAR data during training to improve depth estimation. The proposed pipeline consists of four key modules: (1) a modified YOLOv9 for 2.5D object detection, (2) a depth estimation network, and (3-4) dedicated short- and long-range 3D detection heads. Evaluations on the OSDaR23 dataset demonstrate the effectiveness of the approach in detecting objects up to 250 meters. Results highlight its potential for railway automation and outline areas for future improvement.","authors":["Raul David Dominguez Sanchez","Xavier Diaz Ortiz","Xingcheng Zhou","Max Peter Ronecker","Michael Karner","Daniel Watzenig","Alois Knoll"],"url":"https://arxiv.org/abs/2504.18203"}
{"created":"2025-04-28","title":"Optimizing Multi-Round Enhanced Training in Diffusion Models for Improved Preference Understanding","abstract":"Generative AI has significantly changed industries by enabling text-driven image generation, yet challenges remain in achieving high-resolution outputs that align with fine-grained user preferences. Consequently, multi-round interactions are necessary to ensure the generated images meet expectations. Previous methods enhanced prompts via reward feedback but did not optimize over a multi-round dialogue dataset. In this work, we present a Visual Co-Adaptation (VCA) framework incorporating human-in-the-loop feedback, leveraging a well-trained reward model aligned with human preferences. Using a diverse multi-turn dialogue dataset, our framework applies multiple reward functions, such as diversity, consistency, and preference feedback, while fine-tuning the diffusion model through LoRA, thus optimizing image generation based on user input. We also construct multi-round dialogue datasets of prompts and image pairs aligned with user intent. Experiments demonstrate that our method outperforms state-of-the-art baselines, significantly improving image consistency and alignment with user intent. Our approach consistently surpasses competing models in user satisfaction, especially in multi-turn dialogue scenarios.","authors":["Kun Li","Jianhui Wang","Yangfan He","Xinyuan Song","Ruoyu Wang","Hongyang He","Wenxin Zhang","Jiaqi Chen","Keqin Li","Sida Li","Miao Zhang","Tianyu Shi","Xueqian Wang"],"url":"https://arxiv.org/abs/2504.18204"}
{"created":"2025-04-28","title":"A Machine Learning Approach For Bitcoin Forecasting","abstract":"Bitcoin is one of the cryptocurrencies that is gaining more popularity in recent years. Previous studies have shown that closing price alone is not enough to forecast stock market series. We introduce a new set of time series and demonstrate that a subset is necessary to improve directional accuracy based on a machine learning ensemble. In our experiments, we study which time series and machine learning algorithms deliver the best results. We found that the most relevant time series that contribute to improving directional accuracy are Open, High and Low, with the largest contribution of Low in combination with an ensemble of Gated Recurrent Unit network and a baseline forecast. The relevance of other Bitcoin-related features that are not price-related is negligible. The proposed method delivers similar performance to the state-of-the-art when observing directional accuracy.","authors":["Stefano Sossi-Rojas","Gissel Velarde","Damian Zieba"],"url":"https://arxiv.org/abs/2504.18206"}
{"created":"2025-04-28","title":"Gradient Descent as a Shrinkage Operator for Spectral Bias","abstract":"We generalize the connection between activation function and spline regression/smoothing and characterize how this choice may influence spectral bias within a 1D shallow network. We then demonstrate how gradient descent (GD) can be reinterpreted as a shrinkage operator that masks the singular values of a neural network's Jacobian. Viewed this way, GD implicitly selects the number of frequency components to retain, thereby controlling the spectral bias. An explicit relationship is proposed between the choice of GD hyperparameters (learning rate & number of iterations) and bandwidth (the number of active components). GD regularization is shown to be effective only with monotonic activation functions. Finally, we highlight the utility of non-monotonic activation functions (sinc, Gaussian) as iteration-efficient surrogates for spectral bias.","authors":["Simon Lucey"],"url":"https://arxiv.org/abs/2504.18207"}
{"created":"2025-04-28","title":"Ultra-fast feature learning for the training of two-layer neural networks in the two-timescale regime","abstract":"We study the convergence of gradient methods for the training of mean-field single hidden layer neural networks with square loss. Observing this is a separable non-linear least-square problem which is linear w.r.t. the outer layer's weights, we consider a Variable Projection (VarPro) or two-timescale learning algorithm, thereby eliminating the linear variables and reducing the learning problem to the training of the feature distribution. Whereas most convergence rates or the training of neural networks rely on a neural tangent kernel analysis where features are fixed, we show such a strategy enables provable convergence rates for the sampling of a teacher feature distribution. Precisely, in the limit where the regularization strength vanishes, we show that the dynamic of the feature distribution corresponds to a weighted ultra-fast diffusion equation. Relying on recent results on the asymptotic behavior of such PDEs, we obtain guarantees for the convergence of the trained feature distribution towards the teacher feature distribution in a teacher-student setup.","authors":["Rapha\\\"el Barboni (\\'ENS-PSL)","Gabriel Peyr\\'e (CNRS","\\'ENS-PSL)","Fran\\c{c}ois-Xavier Vialard (LIGM)"],"url":"https://arxiv.org/abs/2504.18208"}
{"created":"2025-04-28","title":"A hybridizable discontinuous Galerkin method with transmission variables for time-harmonic acoustic problems in heterogeneous media","abstract":"We consider the finite element solution of time-harmonic wave propagation problems in heterogeneous media with hybridizable discontinuous Galerkin (HDG) methods. In the case of homogeneous media, it has been observed that the iterative solution of the linear system can be accelerated by hybridizing with transmission variables instead of numerical traces, as performed in standard approaches. In this work, we extend the HDG method with transmission variables, which is called the CHDG method, to the heterogeneous case with piecewise constant physical coefficients. In particular, we consider formulations with standard upwind and general symmetric fluxes. The CHDG hybridized system can be written as a fixed-point problem, which can be solved with stationary iterative schemes for a class of symmetric fluxes. The standard HDG and CHDG methods are systematically studied with the different numerical fluxes by considering a series of 2D numerical benchmarks. The convergence of standard iterative schemes is always faster with the extended CHDG method than with the standard HDG methods, with upwind and scalar symmetric fluxes.","authors":["Simone Pescuma","Gw\\'ena\\\"el Gabard","Th\\'eophile Chaumont-Frelet","Axel Modave"],"url":"https://arxiv.org/abs/2504.18209"}
{"created":"2025-04-28","title":"Dynamic Memory Management on GPUs with SYCL","abstract":"Dynamic memory allocation is not traditionally available in kernels running on GPUs. This work aims to build on Ouroboros, an efficient dynamic memory management library for CUDA applications, by porting the code to SYCL, a cross-platform accelerator API. Since SYCL can be compiled to a CUDA backend, it is possible to compare the performance of the SYCL implementation with that of the original CUDA implementation, as well as test it on non-CUDA platforms such as Intel's Xe graphics.","authors":["Russell K. Standish"],"url":"https://arxiv.org/abs/2504.18211"}
{"created":"2025-04-28","title":"A Data-Centric Approach to 3D Semantic Segmentation of Railway Scenes","abstract":"LiDAR-based semantic segmentation is critical for autonomous trains, requiring accurate predictions across varying distances. This paper introduces two targeted data augmentation methods designed to improve segmentation performance on the railway-specific OSDaR23 dataset. The person instance pasting method enhances segmentation of pedestrians at distant ranges by injecting realistic variations into the dataset. The track sparsification method redistributes point density in LiDAR scans, improving track segmentation at far distances with minimal impact on close-range accuracy. Both methods are evaluated using a state-of-the-art 3D semantic segmentation network, demonstrating significant improvements in distant-range performance while maintaining robustness in close-range predictions. We establish the first 3D semantic segmentation benchmark for OSDaR23, demonstrating the potential of data-centric approaches to address railway-specific challenges in autonomous train perception.","authors":["Nicolas M\\\"unger","Max Peter Ronecker","Xavier Diaz","Michael Karner","Daniel Watzenig","Jan Skaloud"],"url":"https://arxiv.org/abs/2504.18213"}
{"created":"2025-04-28","title":"A Composable Game-Theoretic Framework for Blockchains","abstract":"Blockchains rely on economic incentives to ensure secure and decentralised operation, making incentive compatibility a core design concern. However, protocols are rarely deployed in isolation. Applications interact with the underlying consensus and network layers, and multiple protocols may run concurrently on the same chain. These interactions give rise to complex incentive dynamics that traditional, isolated analyses often fail to capture.","authors":["Zeta Avarikioti","Georg Fuchsbauer","Pim Keer","Matteo Maffei","Fabian Regen"],"url":"https://arxiv.org/abs/2504.18214"}
{"created":"2025-04-28","title":"Unify3D: An Augmented Holistic End-to-end Monocular 3D Human Reconstruction via Anatomy Shaping and Twins Negotiating","abstract":"Monocular 3D clothed human reconstruction aims to create a complete 3D avatar from a single image. To tackle the human geometry lacking in one RGB image, current methods typically resort to a preceding model for an explicit geometric representation. For the reconstruction itself, focus is on modeling both it and the input image. This routine is constrained by the preceding model, and overlooks the integrity of the reconstruction task. To address this, this paper introduces a novel paradigm that treats human reconstruction as a holistic process, utilizing an end-to-end network for direct prediction from 2D image to 3D avatar, eliminating any explicit intermediate geometry display. Based on this, we further propose a novel reconstruction framework consisting of two core components: the Anatomy Shaping Extraction module, which captures implicit shape features taking into account the specialty of human anatomy, and the Twins Negotiating Reconstruction U-Net, which enhances reconstruction through feature interaction between two U-Nets of different modalities. Moreover, we propose a Comic Data Augmentation strategy and construct 15k+ 3D human scans to bolster model performance in more complex case input. Extensive experiments on two test sets and many in-the-wild cases show the superiority of our method over SOTA methods. Our demos can be found in : https://e2e3dgsrecon.github.io/e2e3dgsrecon/.","authors":["Nanjie Yao","Gangjian Zhang","Wenhao Shen","Jian Shu","Hao Wang"],"url":"https://arxiv.org/abs/2504.18215"}
{"created":"2025-04-28","title":"Solving Partial Dominating Set and Related Problems Using Twin-Width","abstract":"Partial vertex cover and partial dominating set are two well-investigated optimization problems. While they are $\\rm W[1]$-hard on general graphs, they have been shown to be fixed-parameter tractable on many sparse graph classes, including nowhere-dense classes. In this paper, we demonstrate that these problems are also fixed-parameter tractable with respect to the twin-width of a graph. Indeed, we establish a more general result: every graph property that can be expressed by a logical formula of the form $\\phi\\equiv\\exists x_1\\ldots \\exists x_k \\#y\\,\\psi(x_1,\\ldots,x_k,y)\\ge t$, where $\\psi$ is a quantifier-free formula, $t$ is an arbitrary number, and $\\#y$ is a counting quantifier, can be evaluated in time $f(d,k)n$, where $n$ is the number of vertices and $d$ is the width of a contraction sequence that is part of the input. Notably, this includes problems such as connected partial dominating set and independent partial dominating set.","authors":["Jakub Balab\\'an","Daniel Mock","Peter Rossmanith"],"url":"https://arxiv.org/abs/2504.18218"}
{"created":"2025-04-28","title":"Optimising ChatGPT for creativity in literary translation: A case study from English into Dutch, Chinese, Catalan and Spanish","abstract":"This study examines the variability of Chat-GPT machine translation (MT) outputs across six different configurations in four languages,with a focus on creativity in a literary text. We evaluate GPT translations in different text granularity levels, temperature settings and prompting strategies with a Creativity Score formula. We found that prompting ChatGPT with a minimal instruction yields the best creative translations, with \"Translate the following text into [TG] creatively\" at the temperature of 1.0 outperforming other configurations and DeepL in Spanish, Dutch, and Chinese. Nonetheless, ChatGPT consistently underperforms compared to human translation (HT).","authors":["Shuxiang Du","Ana Guerberof Arenas","Antonio Toral","Kyo Gerrits","Josep Marco Borillo"],"url":"https://arxiv.org/abs/2504.18221"}
{"created":"2025-04-28","title":"Automated Work Records for Precision Agriculture Management: A Low-Cost GNSS IoT Solution for Paddy Fields in Central Japan","abstract":"Agricultural field operations are generally tracked as work records (WR), incorporating data points such as; work type, machine type, timestamped trajectories and field information. WR data which is automatically recorded by modern machinery equipped with Information and Communication Technologies (ICT) can enable efficient farm management decision making. Globally, farmers often rely on aged or legacy farming machinery and manual data recording, which introduces significant labor costs and increases the risk of inaccurate data input. To address this challenge, a field study in Central Japan was conducted to showcase automated data collection by retrofitting legacy farming machinery with low-cost Internet of Things (IoT) devices. For single-purpose vehicles (SPV), which only carry out single work types such as planting, LTE (Long Term Evolution) and Global Navigation Satellite System (GNSS) units were installed to record trajectory data. For multi-purpose vehicles (MPV), such as tractors which perform multiple work types, the configuration settings of these vehicles had to include implements and attachments data. To obtain this data, industry standard LTE-GNSS Bluetooth gateways were fitted onto MPV and low-cost BLE (Bluetooth Low Energy) beacons were attached to implements. After installation, over a seven-month field preparation and planting period 1,623 WR, including 421 WR for SPV and 1,120 WR for MVP, were automatically obtained. For MPV, the WR included detailed configuration settings enabling detection of the specific work types. These findings demonstrate the potential of low cost IoT GNSS devices for precision agriculture strategies to support management decisions in farming operations.","authors":["M. Grosse","K. Honda","C. Spech","J. C. Pineda"],"url":"https://arxiv.org/abs/2504.18222"}
{"created":"2025-04-28","title":"Even Small Reasoners Should Quote Their Sources: Introducing the Pleias-RAG Model Family","abstract":"We introduce a new generation of small reasoning models for RAG, search, and source summarization. Pleias-RAG-350m and Pleias-RAG-1B are mid-trained on a large synthetic dataset emulating the retrieval of a wide variety of multilingual open sources from the Common Corpus. They provide native support for citation and grounding with literal quotes and reintegrate multiple features associated with RAG workflows, such as query routing, query reformulation, and source reranking. Pleias-RAG-350m and Pleias-RAG-1B outperform SLMs below 4 billion parameters on standardized RAG benchmarks (HotPotQA, 2wiki) and are competitive with popular larger models, including Qwen-2.5-7B, Llama-3.1-8B, and Gemma-3-4B. They are the only SLMs to date maintaining consistent RAG performance across leading European languages and ensuring systematic reference grounding for statements. Due to their size and ease of deployment on constrained infrastructure and higher factuality by design, the models unlock a range of new use cases for generative AI.","authors":["Pierre-Carl Langlais","Pavel Chizhov","Mattia Nee","Carlos Rosas Hinostroza","Matthieu Delsart","Ir\\`ene Girard","Othman Hicheur","Anastasia Stasenko","Ivan P. Yamshchikov"],"url":"https://arxiv.org/abs/2504.18225"}
{"created":"2025-04-28","title":"Games, mobile processes, and functions","abstract":"We establish a tight connection between two models of the $\\lambda$-calculus, namely Milner's encoding into the $\\pi$-calculus (precisely, the Internal $\\pi$-calculus), and operational game semantics (OGS). We first investigate the operational correspondence between the behaviours of the encoding provided by $\\pi$ and OGS. We do so for various LTSs: the standard LTS for $\\pi$ and a new `concurrent' LTS for OGS; an `output-prioritised' LTS for $\\pi$ and the standard alternating LTS for OGS. We then show that the equivalences induced on $\\lambda$-terms by all these LTSs (for $\\pi$ and OGS) coincide. We also prove that when equivalence is based on complete traces, the `concurrent' and `alternating' variants of OGS also coincide with the `well-bracketed' variant.","authors":["Guilhem Jaber","Davide Sangiorgi"],"url":"https://arxiv.org/abs/2504.18227"}
{"created":"2025-04-28","title":"Learning to fuse: dynamic integration of multi-source data for accurate battery lifespan prediction","abstract":"Accurate prediction of lithium-ion battery lifespan is vital for ensuring operational reliability and reducing maintenance costs in applications like electric vehicles and smart grids. This study presents a hybrid learning framework for precise battery lifespan prediction, integrating dynamic multi-source data fusion with a stacked ensemble (SE) modeling approach. By leveraging heterogeneous datasets from the National Aeronautics and Space Administration (NASA), Center for Advanced Life Cycle Engineering (CALCE), MIT-Stanford-Toyota Research Institute (TRC), and nickel cobalt aluminum (NCA) chemistries, an entropy-based dynamic weighting mechanism mitigates variability across heterogeneous datasets. The SE model combines Ridge regression, long short-term memory (LSTM) networks, and eXtreme Gradient Boosting (XGBoost), effectively capturing temporal dependencies and nonlinear degradation patterns. It achieves a mean absolute error (MAE) of 0.0058, root mean square error (RMSE) of 0.0092, and coefficient of determination (R2) of 0.9839, outperforming established baseline models with a 46.2% improvement in R2 and an 83.2% reduction in RMSE. Shapley additive explanations (SHAP) analysis identifies differential discharge capacity (Qdlin) and temperature of measurement (Temp_m) as critical aging indicators. This scalable, interpretable framework enhances battery health management, supporting optimized maintenance and safety across diverse energy storage systems, thereby contributing to improved battery health management in energy storage systems.","authors":["He Shanxuan","Lin Zuhong","Yu Bolun","Gao Xu","Long Biao","Yao Jingjing"],"url":"https://arxiv.org/abs/2504.18230"}
{"created":"2025-04-28","title":"Time and Frequency Domain-based Anomaly Detection in Smart Meter Data for Distribution Network Studies","abstract":"The widespread integration of new technologies in low-voltage distribution networks on the consumer side creates the need for distribution system operators to perform advanced real-time calculations to estimate network conditions. In recent years, data-driven models based on machine learning and big data analysis have emerged for calculation purposes, leveraging the information available in large datasets obtained from smart meters and other advanced measurement infrastructure. However, existing data-driven algorithms do not take into account the quality of data collected from smart meters. They lack built-in anomaly detection mechanisms and fail to differentiate anomalies based on whether the value or context of anomalous data instances deviates from the norm. This paper focuses on methods for detecting and mitigating the impact of anomalies on the consumption of active and reactive power datasets. It proposes an anomaly detection framework based on the Isolation Forest machine learning algorithm and Fast Fourier Transform filtering that works in both the time and frequency domain and is unaffected by point anomalies or contextual anomalies of the power consumption data. The importance of integrating anomaly detection methods is demonstrated in the analysis important for distribution networks with a high share of smart meters.","authors":["Petar Labura","Tomislav Antic","Tomislav Capuder"],"url":"https://arxiv.org/abs/2504.18231"}
{"created":"2025-04-28","title":"Dense Geometry Supervision for Underwater Depth Estimation","abstract":"The field of monocular depth estimation is continually evolving with the advent of numerous innovative models and extensions. However, research on monocular depth estimation methods specifically for underwater scenes remains limited, compounded by a scarcity of relevant data and methodological support. This paper proposes a novel approach to address the existing challenges in current monocular depth estimation methods for underwater environments. We construct an economically efficient dataset suitable for underwater scenarios by employing multi-view depth estimation to generate supervisory signals and corresponding enhanced underwater images. we introduces a texture-depth fusion module, designed according to the underwater optical imaging principles, which aims to effectively exploit and integrate depth information from texture cues. Experimental results on the FLSea dataset demonstrate that our approach significantly improves the accuracy and adaptability of models in underwater settings. This work offers a cost-effective solution for monocular underwater depth estimation and holds considerable promise for practical applications.","authors":["Wenxiang Gua","Lin Qia"],"url":"https://arxiv.org/abs/2504.18233"}
{"created":"2025-04-28","title":"BiasBench: A reproducible benchmark for tuning the biases of event cameras","abstract":"Event-based cameras are bio-inspired sensors that detect light changes asynchronously for each pixel. They are increasingly used in fields like computer vision and robotics because of several advantages over traditional frame-based cameras, such as high temporal resolution, low latency, and high dynamic range. As with any camera, the output's quality depends on how well the camera's settings, called biases for event-based cameras, are configured. While frame-based cameras have advanced automatic configuration algorithms, there are very few such tools for tuning these biases. A systematic testing framework would require observing the same scene with different biases, which is tricky since event cameras only generate events when there is movement. Event simulators exist, but since biases heavily depend on the electrical circuit and the pixel design, available simulators are not well suited for bias tuning. To allow reproducibility, we present BiasBench, a novel event dataset containing multiple scenes with settings sampled in a grid-like pattern. We present three different scenes, each with a quality metric of the downstream application. Additionally, we present a novel, RL-based method to facilitate online bias adjustments.","authors":["Andreas Ziegler","David Joseph","Thomas Gossard","Emil Moldovan","Andreas Zell"],"url":"https://arxiv.org/abs/2504.18235"}
{"created":"2025-04-28","title":"\"Two Means to an End Goal\": Connecting Explainability and Contestability in the Regulation of Public Sector AI","abstract":"Explainability and its emerging counterpart contestability have become important normative and design principles for the trustworthy use of AI as they enable users and subjects to understand and challenge AI decisions. However, the regulation of AI systems spans technical, legal, and organizational dimensions, producing a multiplicity in meaning that complicates the implementation of explainability and contestability. Resolving this conceptual ambiguity requires specifying and comparing the meaning of both principles across regulation dimensions, disciplines, and actors. This process, here defined as translation, is essential to provide guidance on the principles' realization. We present the findings of a semi-structured interview study with 14 interdisciplinary AI regulation experts. We report on the experts' understanding of the intersection between explainability and contestability in public AI regulation, their advice for a decision subject and a public agency in a welfare allocation AI use case, and their perspectives on the connections and gaps within the research landscape. We provide differentiations between descriptive and normative explainability, judicial and non-judicial channels of contestation, and individual and collective contestation action. We further outline three translation processes in the alignment of top-down and bottom-up regulation, the assignment of responsibility for interpreting regulations, and the establishment of interdisciplinary collaboration. Our contributions include an empirically grounded conceptualization of the intersection between explainability and contestability and recommendations on implementing these principles in public institutions. We believe our contributions can inform policy-making and regulation of these core principles and enable more effective and equitable design, development, and deployment of trustworthy public AI systems.","authors":["Timoth\\'ee Schmude","Mireia Yurrita","Kars Alfrink","Thomas Le Goff","Tiphaine Viard"],"url":"https://arxiv.org/abs/2504.18236"}
{"created":"2025-04-28","title":"SecCityVR: Visualization and Collaborative Exploration of Software Vulnerabilities in Virtual Reality","abstract":"Security vulnerabilities in software systems represent significant risks as potential entry points for malicious attacks. Traditional dashboards that display the results of static analysis security testing often use 2D or 3D visualizations, which tend to lack the spatial details required to effectively reveal issues such as the propagation of vulnerabilities across the codebase or the appearance of concurrent vulnerabilities. Additionally, most reporting solutions only treat the analysis results as an artifact that can be reviewed or edited asynchronously by developers, limiting real-time, collaborative exploration. To the best of our knowledge, no VR-based approach exists for the visualization and interactive exploration of software security vulnerabilities. Addressing these challenges, the virtual reality (VR) environment SecCityVR was developed as a proof-of-concept implementation that employs the code city metaphor within VR to visualize software security vulnerabilities as colored building floors inside the surrounding virtual city. By integrating the application's call graph, vulnerabilities are contextualized within related software components. SecCityVR supports multi-user collaboration and interactive exploration. It provides explanations and mitigations for detected issues. A user study comparing SecCityVR with the traditional dashboard find-sec-bugs showed the VR approach provided a favorable experience, with higher usability, lower temporal demand, and significantly lower frustration despite having longer task completion times. This paper and its results contribute to the fields of collaborative and secure software engineering, as well as software visualization. It provides a new application of VR code cities to visualize security vulnerabilities, as well as a novel environment for security audits using collaborative and immersive technologies.","authors":["Dennis W\\\"uppelman","Enes Yigitbas"],"url":"https://arxiv.org/abs/2504.18238"}
{"created":"2025-04-28","title":"Tree Rewriting Calculi for Strictly Positive Logics","abstract":"We study strictly positive logics in the language $\\mathscr{L}^+$, which constructs formulas from $\\top$, propositional variables, conjunction, and diamond modalities. We begin with the base system $\\bf K^+$, the strictly positive fragment of polymodal $\\bf K$, and examine its extensions obtained by adding axioms such as monotonicity, transitivity, and the hierarchy-sensitive interaction axiom $(\\sf J)$, which governs the interplay between modalities of different strengths. The strongest of these systems is the Reflection Calculus ($\\bf RC$), which corresponds to the strictly positive fragment of polymodal $\\bf GLP$.","authors":["Sof\\'ia Santiago-Fern\\'andez","David Fern\\'andez-Duque","Joost J. Joosten"],"url":"https://arxiv.org/abs/2504.18240"}
{"created":"2025-04-28","title":"Switch-Based Multi-Part Neural Network","abstract":"This paper introduces decentralized and modular neural network framework designed to enhance the scalability, interpretability, and performance of artificial intelligence (AI) systems. At the heart of this framework is a dynamic switch mechanism that governs the selective activation and training of individual neurons based on input characteristics, allowing neurons to specialize in distinct segments of the data domain. This approach enables neurons to learn from disjoint subsets of data, mimicking biological brain function by promoting task specialization and improving the interpretability of neural network behavior. Furthermore, the paper explores the application of federated learning and decentralized training for real-world AI deployments, particularly in edge computing and distributed environments. By simulating localized training on non-overlapping data subsets, we demonstrate how modular networks can be efficiently trained and evaluated. The proposed framework also addresses scalability, enabling AI systems to handle large datasets and distributed processing while preserving model transparency and interpretability. Finally, we discuss the potential of this approach in advancing the design of scalable, privacy-preserving, and efficient AI systems for diverse applications.","authors":["Surajit Majumder","Paritosh Ranjan","Prodip Roy","Bhuban Padhan"],"url":"https://arxiv.org/abs/2504.18241"}
{"created":"2025-04-28","title":"Demand Private Coded Caching: Small Cache Size","abstract":"We investigate the demand private coded caching problem, which is an $(N,K)$ coded caching problem with $N$ files, $K$ users, each equipped with a cache of size $M$, and an additional privacy constraint on user demands, i.e., each user can not gain any information about the demands of other users. We focus on scenarios where the size of users' caches is small, aiming to further characterize the fundamental limits of this problem. We first present a new virtual-user-based achievable scheme for arbitrary number of users and files, and two MDS-code-based achievable schemes for the case $N \\le K$. With a newly derived converse bound for the case $N \\le K$, these proposed schemes lead to the optimal memory-rate tradeoff of the demand private coded caching problem for $M \\in \\big[0, \\frac{N}{(K+1)(N-1)} \\big] $ where $N \\le K \\le 2N-2$, and the optimal memory-rate tradeoff for $M \\in \\big[0, \\frac{1}{K+1} \\big] $ where $ K > 2N-2$. Moreover, for the case of 2 files and arbitrary number of users, by deriving another new converse bound, the optimal memory-rate tradeoff is characterized for $M\\in \\big[0,\\frac{2}{K}\\big] \\cup \\big[\\frac{2(K-1)}{K+1},2\\big]$. Finally, we provide the optimal memory-rate tradeoff of the demand private coded caching problem for 2 files and 3 users.","authors":["Qinyi Lu","Nan Liu","Wei Kang","Chunguo Li"],"url":"https://arxiv.org/abs/2504.18242"}
{"created":"2025-04-28","title":"DualRAG: A Dual-Process Approach to Integrate Reasoning and Retrieval for Multi-Hop Question Answering","abstract":"Multi-Hop Question Answering (MHQA) tasks permeate real-world applications, posing challenges in orchestrating multi-step reasoning across diverse knowledge domains. While existing approaches have been improved with iterative retrieval, they still struggle to identify and organize dynamic knowledge. To address this, we propose DualRAG, a synergistic dual-process framework that seamlessly integrates reasoning and retrieval. DualRAG operates through two tightly coupled processes: Reasoning-augmented Querying (RaQ) and progressive Knowledge Aggregation (pKA). They work in concert: as RaQ navigates the reasoning path and generates targeted queries, pKA ensures that newly acquired knowledge is systematically integrated to support coherent reasoning. This creates a virtuous cycle of knowledge enrichment and reasoning refinement. Through targeted fine-tuning, DualRAG preserves its sophisticated reasoning and retrieval capabilities even in smaller-scale models, demonstrating its versatility and core advantages across different scales. Extensive experiments demonstrate that this dual-process approach substantially improves answer accuracy and coherence, approaching, and in some cases surpassing, the performance achieved with oracle knowledge access. These results establish DualRAG as a robust and efficient solution for complex multi-hop reasoning tasks.","authors":["Rong Cheng","Jinyi Liu","YAN ZHENG","Fei Ni","Jiazhen Du","Hangyu Mao","Fuzheng Zhang","Bo Wang","Jianye HAO"],"url":"https://arxiv.org/abs/2504.18243"}
{"created":"2025-04-28","title":"Efficient Single-Pass Training for Multi-Turn Reasoning","abstract":"Training Large Language Models ( LLMs) to generate explicit reasoning before they produce an answer has been shown to improve their performance across various tasks such as mathematics and coding. However, fine-tuning LLMs on multi-turn reasoning datasets presents a unique challenge: LLMs must generate reasoning tokens that are excluded from subsequent inputs to the LLM. This discrepancy prevents us from processing an entire conversation in a single forward pass-an optimization readily available when we fine-tune on a multi-turn non-reasoning dataset. This paper proposes a novel approach that overcomes this limitation through response token duplication and a custom attention mask that enforces appropriate visibility constraints. Our approach significantly reduces the training time and allows efficient fine-tuning on multi-turn reasoning datasets.","authors":["Ritesh Goru","Shanay Mehta","Prateek Jain"],"url":"https://arxiv.org/abs/2504.18246"}
{"created":"2025-04-28","title":"Efficient Matching of Some Fundamental Regular Expressions with Backreferences","abstract":"Regular expression matching is of practical importance due to its widespread use in real-world applications. In practical use, regular expressions are often used with real-world extensions. Accordingly, the matching problem of regular expressions with real-world extensions has been actively studied in recent years, yielding steady progress. However, backreference, a popular extension supported by most modern programming languages such as Java, Python, JavaScript and others in their standard libraries for string processing, is an exception to this positive trend. In fact, it is known that the matching problem of regular expressions with backreferences (rewbs) is theoretically hard and the existence of an asymptotically fast matching algorithm for arbitrary rewbs seems unlikely. Even among currently known partial solutions, the balance between efficiency and generality remains unsatisfactory. To bridge this gap, we present an efficient matching algorithm for rewbs of the form $e_0 (e)_1 e_1 \\backslash 1 e_2$ where $e_0, e, e_1, e_2$ are pure regular expressions, which are fundamental and frequently used in practical applications. It runs in quadratic time with respect to the input string length, substantially improving the best-known cubic time complexity for these rewbs. Our algorithm combines ideas from both stringology and automata theory in a novel way. We leverage two techniques from automata theory, injection and summarization, to simultaneously examine matches whose backreferenced substrings are either a fixed right-maximal repeat or its extendable prefixes, which are concepts from stringology. By further utilizing a subtle property of extendable prefixes, our algorithm correctly decides the matching problem while achieving the quadratic-time complexity.","authors":["Taisei Nogami","Tachio Terauchi"],"url":"https://arxiv.org/abs/2504.18247"}
{"created":"2025-04-28","title":"A finite volume Simo-Reissner beam method for moored floating body dynamics","abstract":"This paper presents a novel finite volume mooring line model based on the geometrically exact Simo-Reissner beam model for analysing the interaction between a floating rigid body and its mooring lines. The coupled numerical model is implemented entirely within a finite volume-based discretisation framework using a popular computational fluid dynamics C++ toolbox, OpenFOAM. Unlike existing methods for modelling mooring lines, which rely on lumped mass models or finite element-based approaches, this work simulates the mooring cables using non-linear beam models implemented in a finite volume framework to account for bending, tensile, and torsional loading. This advancement makes the current work particularly valuable for simulating extreme sea conditions. The coupled model developed in this study has been validated and verified using experimental and numerical data for a floating box moored with four catenary mooring lines under regular wave conditions featuring different wave heights and periods. The results demonstrate strong agreement with both experimental and numerical data, highlighting the model's accuracy in capturing mooring dynamics and floating body motion.","authors":["Amirhossein Taran","Seevani Bali","Zeljko Tukovic","Vikram Pakrashi","Philip Cardiff"],"url":"https://arxiv.org/abs/2504.18248"}
{"created":"2025-04-28","title":"Event-Based Eye Tracking. 2025 Event-based Vision Workshop","abstract":"This survey serves as a review for the 2025 Event-Based Eye Tracking Challenge organized as part of the 2025 CVPR event-based vision workshop. This challenge focuses on the task of predicting the pupil center by processing event camera recorded eye movement. We review and summarize the innovative methods from teams rank the top in the challenge to advance future event-based eye tracking research. In each method, accuracy, model size, and number of operations are reported. In this survey, we also discuss event-based eye tracking from the perspective of hardware design.","authors":["Qinyu Chen","Chang Gao","Min Liu","Daniele Perrone","Yan Ru Pei","Zuowen Wang","Zhuo Zou","Shihang Tan","Tao Han","Guorui Lu","Zhen Xu","Junyuan Ding","Ziteng Wang","Zongwei Wu","Han Han","Yuliang Wu","Jinze Chen","Wei Zhai","Yang Cao","Zheng-jun Zha","Nuwan Bandara","Thivya Kandappu","Archan Misra","Xiaopeng Lin","Hongxiang Huang","Hongwei Ren","Bojun Cheng","Hoang M. Truong","Vinh-Thuan Ly","Huy G. Tran","Thuan-Phat Nguyen","Tram T. Doan"],"url":"https://arxiv.org/abs/2504.18249"}
{"created":"2025-04-28","title":"Depth-Constrained ASV Navigation with Deep RL and Limited Sensing","abstract":"Autonomous Surface Vehicles (ASVs) play a crucial role in maritime operations, yet their navigation in shallow-water environments remains challenging due to dynamic disturbances and depth constraints. Traditional navigation strategies struggle with limited sensor information, making safe and efficient operation difficult. In this paper, we propose a reinforcement learning (RL) framework for ASV navigation under depth constraints, where the vehicle must reach a target while avoiding unsafe areas with only a single depth measurement per timestep from a downward-facing Single Beam Echosounder (SBES). To enhance environmental awareness, we integrate Gaussian Process (GP) regression into the RL framework, enabling the agent to progressively estimate a bathymetric depth map from sparse sonar readings. This approach improves decision-making by providing a richer representation of the environment. Furthermore, we demonstrate effective sim-to-real transfer, ensuring that trained policies generalize well to real-world aquatic conditions. Experimental results validate our method's capability to improve ASV navigation performance while maintaining safety in challenging shallow-water environments.","authors":["Amirhossein Zhalehmehrabi","Daniele Meli","Francesco Dal Santo","Francesco Trotti","Alessandro Farinelli"],"url":"https://arxiv.org/abs/2504.18253"}
{"created":"2025-04-28","title":"SSL4Eco: A Global Seasonal Dataset for Geospatial Foundation Models in Ecology","abstract":"With the exacerbation of the biodiversity and climate crises, macroecological pursuits such as global biodiversity mapping become more urgent. Remote sensing offers a wealth of Earth observation data for ecological studies, but the scarcity of labeled datasets remains a major challenge. Recently, self-supervised learning has enabled learning representations from unlabeled data, triggering the development of pretrained geospatial models with generalizable features. However, these models are often trained on datasets biased toward areas of high human activity, leaving entire ecological regions underrepresented. Additionally, while some datasets attempt to address seasonality through multi-date imagery, they typically follow calendar seasons rather than local phenological cycles. To better capture vegetation seasonality at a global scale, we propose a simple phenology-informed sampling strategy and introduce corresponding SSL4Eco, a multi-date Sentinel-2 dataset, on which we train an existing model with a season-contrastive objective. We compare representations learned from SSL4Eco against other datasets on diverse ecological downstream tasks and demonstrate that our straightforward sampling method consistently improves representation quality, highlighting the importance of dataset construction. The model pretrained on SSL4Eco reaches state of the art performance on 7 out of 8 downstream tasks spanning (multi-label) classification and regression. We release our code, data, and model weights to support macroecological and computer vision research at https://github.com/PlekhanovaElena/ssl4eco.","authors":["Elena Plekhanova","Damien Robert","Johannes Dollinger","Emilia Arens","Philipp Brun","Jan Dirk Wegner","Niklaus Zimmermann"],"url":"https://arxiv.org/abs/2504.18256"}
{"created":"2025-04-28","title":"MAGI: Multi-Agent Guided Interview for Psychiatric Assessment","abstract":"Automating structured clinical interviews could revolutionize mental healthcare accessibility, yet existing large language models (LLMs) approaches fail to align with psychiatric diagnostic protocols. We present MAGI, the first framework that transforms the gold-standard Mini International Neuropsychiatric Interview (MINI) into automatic computational workflows through coordinated multi-agent collaboration. MAGI dynamically navigates clinical logic via four specialized agents: 1) an interview tree guided navigation agent adhering to the MINI's branching structure, 2) an adaptive question agent blending diagnostic probing, explaining, and empathy, 3) a judgment agent validating whether the response from participants meet the node, and 4) a diagnosis Agent generating Psychometric Chain-of- Thought (PsyCoT) traces that explicitly map symptoms to clinical criteria. Experimental results on 1,002 real-world participants covering depression, generalized anxiety, social anxiety and suicide shows that MAGI advances LLM- assisted mental health assessment by combining clinical rigor, conversational adaptability, and explainable reasoning.","authors":["Guanqun Bi","Zhuang Chen","Zhoufu Liu","Hongkai Wang","Xiyao Xiao","Yuqiang Xie","Wen Zhang","Yongkang Huang","Yuxuan Chen","Libiao Peng","Yi Feng","Minlie Huang"],"url":"https://arxiv.org/abs/2504.18260"}
{"created":"2025-04-28","title":"Local Statistical Parity for the Estimation of Fair Decision Trees","abstract":"Given the high computational complexity of decision tree estimation, classical methods construct a tree by adding one node at a time in a recursive way. To facilitate promoting fairness, we propose a fairness criterion local to the tree nodes. We prove how it is related to the Statistical Parity criterion, popular in the Algorithmic Fairness literature, and show how to incorporate it into standard recursive tree estimation algorithms.","authors":["Andrea Quintanilla","Johan Van Horebeek"],"url":"https://arxiv.org/abs/2504.18262"}
{"created":"2025-04-28","title":"Neural operators struggle to learn complex PDEs in pedestrian mobility: Hughes model case study","abstract":"This paper investigates the limitations of neural operators in learning solutions for a Hughes model, a first-order hyperbolic conservation law system for crowd dynamics. The model couples a Fokker-Planck equation representing pedestrian density with a Hamilton-Jacobi-type (eikonal) equation. This Hughes model belongs to the class of nonlinear hyperbolic systems that often exhibit complex solution structures, including shocks and discontinuities. In this study, we assess the performance of three state-of-the-art neural operators (Fourier Neural Operator, Wavelet Neural Operator, and Multiwavelet Neural Operator) in various challenging scenarios. Specifically, we consider (1) discontinuous and Gaussian initial conditions and (2) diverse boundary conditions, while also examining the impact of different numerical schemes.","authors":["Prajwal Chauhan","Salah Eddine Choutri","Mohamed Ghattassi","Nader Masmoudi","Saif Eddin Jabari"],"url":"https://arxiv.org/abs/2504.18267"}
{"created":"2025-04-28","title":"TextTIGER: Text-based Intelligent Generation with Entity Prompt Refinement for Text-to-Image Generation","abstract":"Generating images from prompts containing specific entities requires models to retain as much entity-specific knowledge as possible. However, fully memorizing such knowledge is impractical due to the vast number of entities and their continuous emergence. To address this, we propose Text-based Intelligent Generation with Entity prompt Refinement (TextTIGER), which augments knowledge on entities included in the prompts and then summarizes the augmented descriptions using Large Language Models (LLMs) to mitigate performance degradation from longer inputs. To evaluate our method, we introduce WiT-Cub (WiT with Captions and Uncomplicated Background-explanations), a dataset comprising captions, images, and an entity list. Experiments on four image generation models and five LLMs show that TextTIGER improves image generation performance in standard metrics (IS, FID, and CLIPScore) compared to caption-only prompts. Additionally, multiple annotators' evaluation confirms that the summarized descriptions are more informative, validating LLMs' ability to generate concise yet rich descriptions. These findings demonstrate that refining prompts with augmented and summarized entity-related descriptions enhances image generation capabilities. The code and dataset will be available upon acceptance.","authors":["Shintaro Ozaki","Kazuki Hayashi","Yusuke Sakai","Jingun Kwon","Hidetaka Kamigaito","Katsuhiko Hayashi","Manabu Okumura","Taro Watanabe"],"url":"https://arxiv.org/abs/2504.18269"}
{"created":"2025-04-28","title":"LEAM: A Prompt-only Large Language Model-enabled Antenna Modeling Method","abstract":"Antenna modeling is a time-consuming and complex process, decreasing the speed of antenna analysis and design. In this paper, a large language model (LLM)- enabled antenna modeling method, called LEAM, is presented to address this challenge. LEAM enables automatic antenna model generation based on language descriptions via prompt input, images, descriptions from academic papers, patents, and technical reports (either one or multiple). The effectiveness of LEAM is demonstrated by three examples: a Vivaldi antenna generated from a complete user description, a slotted patch antenna generated from an incomplete user description and the operating frequency, and a monopole slotted antenna generated from images and descriptions scanned from the literature. For all the examples, correct antenna models are generated in a few minutes. The code can be accessed via https://github.com/TaoWu974/LEAM.","authors":["Tao Wu","Kexue Fu","Qiang Hua","Xinxin Liu","Muhammad Ali Imran","Bo Liu"],"url":"https://arxiv.org/abs/2504.18271"}
{"created":"2025-04-28","title":"Efficient Learning on Large Graphs using a Densifying Regularity Lemma","abstract":"Learning on large graphs presents significant challenges, with traditional Message Passing Neural Networks suffering from computational and memory costs scaling linearly with the number of edges. We introduce the Intersecting Block Graph (IBG), a low-rank factorization of large directed graphs based on combinations of intersecting bipartite components, each consisting of a pair of communities, for source and target nodes. By giving less weight to non-edges, we show how to efficiently approximate any graph, sparse or dense, by a dense IBG. Specifically, we prove a constructive version of the weak regularity lemma, showing that for any chosen accuracy, every graph, regardless of its size or sparsity, can be approximated by a dense IBG whose rank depends only on the accuracy. This dependence of the rank solely on the accuracy, and not on the sparsity level, is in contrast to previous forms of the weak regularity lemma. We present a graph neural network architecture operating on the IBG representation of the graph and demonstrating competitive performance on node classification, spatio-temporal graph analysis, and knowledge graph completion, while having memory and computational complexity linear in the number of nodes rather than edges.","authors":["Jonathan Kouchly","Ben Finkelshtein","Michael Bronstein","Ron Levie"],"url":"https://arxiv.org/abs/2504.18273"}
{"created":"2025-04-28","title":"Studying Small Language Models with Susceptibilities","abstract":"We develop a linear response framework for interpretability that treats a neural network as a Bayesian statistical mechanical system. A small, controlled perturbation of the data distribution, for example shifting the Pile toward GitHub or legal text, induces a first-order change in the posterior expectation of an observable localized on a chosen component of the network. The resulting susceptibility can be estimated efficiently with local SGLD samples and factorizes into signed, per-token contributions that serve as attribution scores. Building a set of perturbations (probes) yields a response matrix whose low-rank structure separates functional modules such as multigram and induction heads in a 3M-parameter transformer. Susceptibilities link local learning coefficients from singular learning theory with linear-response theory, and quantify how local loss landscape geometry deforms under shifts in the data distribution.","authors":["Garrett Baker","George Wang","Jesse Hoogland","Daniel Murfet"],"url":"https://arxiv.org/abs/2504.18274"}
{"created":"2025-04-28","title":"Multiplicative Rewards in Markovian Models","abstract":"This paper studies the expected value of multiplicative rewards, where rewards obtained in each step are multiplied (instead of the usual addition), in Markov chains (MCs) and Markov decision processes (MDPs). One of the key differences to additive rewards is that the expected value may diverge to infinity not only due to recurrent, but also due to transient states. For MCs, computing the value is shown to be possible in polynomial time given an oracle for the comparison of succinctly represented integers (CSRI), which is only known to be solvable in polynomial time subject to number-theoretic conjectures. Interestingly, distinguishing whether the value is infinite or 0 is at least as hard as CSRI, while determining if it is one of these two can be done in polynomial time. In MDPs, the optimal value can be computed in polynomial space. Further refined complexity results and results on the complexity of optimal schedulers are presented. The techniques developed for MDPs additionally allow to solve the multiplicative variant of the stochastic shortest path problem. Finally, for MCs and MDPs where an absorbing state is reached almost surely, all considered problems are solvable in polynomial time.","authors":["Christel Baier","Krishnendu Chatterjee","Tobias Meggendorfer","Jakob Piribauer"],"url":"https://arxiv.org/abs/2504.18277"}
{"created":"2025-04-28","title":"A comprehensive review of classifier probability calibration metrics","abstract":"Probabilities or confidence values produced by artificial intelligence (AI) and machine learning (ML) models often do not reflect their true accuracy, with some models being under or over confident in their predictions. For example, if a model is 80% sure of an outcome, is it correct 80% of the time? Probability calibration metrics measure the discrepancy between confidence and accuracy, providing an independent assessment of model calibration performance that complements traditional accuracy metrics. Understanding calibration is important when the outputs of multiple systems are combined, for assurance in safety or business-critical contexts, and for building user trust in models. This paper provides a comprehensive review of probability calibration metrics for classifier and object detection models, organising them according to a number of different categorisations to highlight their relationships. We identify 82 major metrics, which can be grouped into four classifier families (point-based, bin-based, kernel or curve-based, and cumulative) and an object detection family. For each metric, we provide equations where available, facilitating implementation and comparison by future researchers.","authors":["Richard Oliver Lane"],"url":"https://arxiv.org/abs/2504.18278"}
{"created":"2025-04-28","title":"Clustering of return words in languages of interval exchanges","abstract":"A word over an ordered alphabet is said to be clustering if identical letters appear adjacently in its Burrows-Wheeler transform. Such words are strictly related to (discrete) interval exchange transformations. We use an extended version of the well-known Rauzy induction to show that every return word in the language generated by a regular interval exchange transformation is clustering, partially answering a question of Lapointe (2021).","authors":["Francesco Dolce","Christian B. Hughes"],"url":"https://arxiv.org/abs/2504.18280"}
{"created":"2025-04-28","title":"Seeing Soundscapes: Audio-Visual Generation and Separation from Soundscapes Using Audio-Visual Separator","abstract":"Recent audio-visual generative models have made substantial progress in generating images from audio. However, existing approaches focus on generating images from single-class audio and fail to generate images from mixed audio. To address this, we propose an Audio-Visual Generation and Separation model (AV-GAS) for generating images from soundscapes (mixed audio containing multiple classes). Our contribution is threefold: First, we propose a new challenge in the audio-visual generation task, which is to generate an image given a multi-class audio input, and we propose a method that solves this task using an audio-visual separator. Second, we introduce a new audio-visual separation task, which involves generating separate images for each class present in a mixed audio input. Lastly, we propose new evaluation metrics for the audio-visual generation task: Class Representation Score (CRS) and a modified R@K. Our model is trained and evaluated on the VGGSound dataset. We show that our method outperforms the state-of-the-art, achieving 7% higher CRS and 4% higher R@2* in generating plausible images with mixed audio.","authors":["Minjae Kang","Martim Brand\\~ao"],"url":"https://arxiv.org/abs/2504.18283"}
{"created":"2025-04-28","title":"Design and Evaluation of a UGV-Based Robotic Platform for Precision Soil Moisture Remote Sensing","abstract":"This extended abstract presents the design and evaluation of AgriOne, an automated unmanned ground vehicle (UGV) platform for high precision sensing of soil moisture in large agricultural fields. The developed robotic system is equipped with a volumetric water content (VWC) sensor mounted on a robotic manipulator and utilizes a surface-aware data collection framework to ensure accurate measurements in heterogeneous terrains. The framework identifies and removes invalid data points where the sensor fails to penetrate the soil, ensuring data reliability. Multiple field experiments were conducted to validate the platform's performance, while the obtained results demonstrate the efficacy of the AgriOne robot in real-time data acquisition, reducing the need for permanent sensors and labor-intensive methods.","authors":["Ilektra Tsimpidi","Ilias Tevetzidis","Vidya Sumathy","George Nikolakopoulos"],"url":"https://arxiv.org/abs/2504.18284"}
{"created":"2025-04-28","title":"Enhancing Long-Term Re-Identification Robustness Using Synthetic Data: A Comparative Analysis","abstract":"This contribution explores the impact of synthetic training data usage and the prediction of material wear and aging in the context of re-identification. Different experimental setups and gallery set expanding strategies are tested, analyzing their impact on performance over time for aging re-identification subjects. Using a continuously updating gallery, we were able to increase our mean Rank-1 accuracy by 24%, as material aging was taken into account step by step. In addition, using models trained with 10% artificial training data, Rank-1 accuracy could be increased by up to 13%, in comparison to a model trained on only real-world data, significantly boosting generalized performance on hold-out data. Finally, this work introduces a novel, open-source re-identification dataset, pallet-block-2696. This dataset contains 2,696 images of Euro pallets, taken over a period of 4 months. During this time, natural aging processes occurred and some of the pallets were damaged during their usage. These wear and tear processes significantly changed the appearance of the pallets, providing a dataset that can be used to generate synthetically aged pallets or other wooden materials.","authors":["Christian Pionzewski","Rebecca Rademacher","J\\'er\\^ome Rutinowski","Antonia Ponikarov","Stephan Matzke","Tim Chilla","Pia Schreynemackers","Alice Kirchheim"],"url":"https://arxiv.org/abs/2504.18286"}
{"created":"2025-04-28","title":"Secret Sharing in the Rank Metric","abstract":"The connection between secret sharing and matroid theory is well established. In this paper, we generalize the concepts of secret sharing and matroid ports to $q$-polymatroids. Specifically, we introduce the notion of an access structure on a vector space, and consider properties related to duality, minors, and the relationship to $q$-polymatroids. Finally, we show how rank-metric codes give rise to secret sharing schemes within this framework.","authors":["Johan Vester Dinesen","Eimear Byrne","Ragnar Freij-Hollanti","Camilla Hollanti"],"url":"https://arxiv.org/abs/2504.18294"}
{"created":"2025-04-28","title":"Deep Reinforcement Learning Based Navigation with Macro Actions and Topological Maps","abstract":"This paper addresses the challenge of navigation in large, visually complex environments with sparse rewards. We propose a method that uses object-oriented macro actions grounded in a topological map, allowing a simple Deep Q-Network (DQN) to learn effective navigation policies. The agent builds a map by detecting objects from RGBD input and selecting discrete macro actions that correspond to navigating to these objects. This abstraction drastically reduces the complexity of the underlying reinforcement learning problem and enables generalization to unseen environments. We evaluate our approach in a photorealistic 3D simulation and show that it significantly outperforms a random baseline under both immediate and terminal reward conditions. Our results demonstrate that topological structure and macro-level abstraction can enable sample-efficient learning even from pixel data.","authors":["Simon Hakenes","Tobias Glasmachers"],"url":"https://arxiv.org/abs/2504.18300"}
{"created":"2025-04-28","title":"Treewidth Parameterized by Feedback Vertex Number","abstract":"We provide the first algorithm for computing an optimal tree decomposition for a given graph $G$ that runs in single exponential time in the feedback vertex number of $G$, that is, in time $2^{O(\\text{fvn}(G))}\\cdot n^{O(1)}$, where $\\text{fvn}(G)$ is the feedback vertex number of $G$ and $n$ is the number of vertices of $G$. On a classification level, this improves the previously known results by Chapelle et al. [Discrete Applied Mathematics '17] and Fomin et al. [Algorithmica '18], who independently showed that an optimal tree decomposition can be computed in single exponential time in the vertex cover number of $G$.","authors":["Hendrik Molter","Meirav Zehavi","Amit Zivan"],"url":"https://arxiv.org/abs/2504.18302"}
{"created":"2025-04-28","title":"SSA-UNet: Advanced Precipitation Nowcasting via Channel Shuffling","abstract":"Weather forecasting is essential for facilitating diverse socio-economic activity and environmental conservation initiatives. Deep learning techniques are increasingly being explored as complementary approaches to Numerical Weather Prediction (NWP) models, offering potential benefits such as reduced complexity and enhanced adaptability in specific applications. This work presents a novel design, Small Shuffled Attention UNet (SSA-UNet), which enhances SmaAt-UNet's architecture by including a shuffle channeling mechanism to optimize performance and diminish complexity. To assess its efficacy, this architecture and its reduced variant are examined and trained on two datasets: a Dutch precipitation dataset from 2016 to 2019, and a French cloud cover dataset containing radar images from 2017 to 2018. Three output configurations of the proposed architecture are evaluated, yielding outputs of 1, 6, and 12 precipitation maps, respectively. To better understand how this model operates and produces its predictions, a gradient-based approach called Grad-CAM is used to analyze the outputs generated. The analysis of heatmaps generated by Grad-CAM facilitated the identification of regions within the input maps that the model considers most informative for generating its predictions. The implementation of SSA-UNet can be found on our Github\\footnote{\\href{https://github.com/MarcoTurzi/SSA-UNet}{https://github.com/MarcoTurzi/SSA-UNet}}","authors":["Marco Turzi","Siamak Mehrkanoon"],"url":"https://arxiv.org/abs/2504.18309"}
{"created":"2025-04-28","title":"Towards Adaptive Software Agents for Debugging","abstract":"Using multiple agents was found to improve the debugging capabilities of Large Language Models. However, increasing the number of LLM-agents has several drawbacks such as increasing the running costs and rising the risk for the agents to lose focus. In this work, we propose an adaptive agentic design, where the number of agents and their roles are determined dynamically based on the characteristics of the task to be achieved. In this design, the agents roles are not predefined, but are generated after analyzing the problem to be solved. Our initial evaluation shows that, with the adaptive design, the number of agents that are generated depends on the complexity of the buggy code. In fact, for simple code with mere syntax issues, the problem was usually fixed using one agent only. However, for more complex problems, we noticed the creation of a higher number of agents. Regarding the effectiveness of the fix, we noticed an average improvement of 11% compared to the one-shot prompting. Given these promising results, we outline future research directions to improve our design for adaptive software agents that can autonomously plan and conduct their software goals.","authors":["Yacine Majdoub","Eya Ben Charrada","Haifa Touati"],"url":"https://arxiv.org/abs/2504.18316"}
{"created":"2025-04-28","title":"Task-Oriented Communications for Visual Navigation with Edge-Aerial Collaboration in Low Altitude Economy","abstract":"To support the Low Altitude Economy (LAE), precise unmanned aerial vehicles (UAVs) localization in urban areas where global positioning system (GPS) signals are unavailable. Vision-based methods offer a viable alternative but face severe bandwidth, memory and processing constraints on lightweight UAVs. Inspired by mammalian spatial cognition, we propose a task-oriented communication framework, where UAVs equipped with multi-camera systems extract compact multi-view features and offload localization tasks to edge servers. We introduce the Orthogonally-constrained Variational Information Bottleneck encoder (O-VIB), which incorporates automatic relevance determination (ARD) to prune non-informative features while enforcing orthogonality to minimize redundancy. This enables efficient and accurate localization with minimal transmission cost. Extensive evaluation on a dedicated LAE UAV dataset shows that O-VIB achieves high-precision localization under stringent bandwidth budgets. Code and dataset will be made publicly available: github.com/fangzr/TOC-Edge-Aerial.","authors":["Zhengru Fang","Zhenghao Liu","Jingjing Wang","Senkang Hu","Yu Guo","Yiqin Deng","Yuguang Fang"],"url":"https://arxiv.org/abs/2504.18317"}
{"created":"2025-04-28","title":"STP4D: Spatio-Temporal-Prompt Consistent Modeling for Text-to-4D Gaussian Splatting","abstract":"Text-to-4D generation is rapidly developing and widely applied in various scenarios. However, existing methods often fail to incorporate adequate spatio-temporal modeling and prompt alignment within a unified framework, resulting in temporal inconsistencies, geometric distortions, or low-quality 4D content that deviates from the provided texts. Therefore, we propose STP4D, a novel approach that aims to integrate comprehensive spatio-temporal-prompt consistency modeling for high-quality text-to-4D generation. Specifically, STP4D employs three carefully designed modules: Time-varying Prompt Embedding, Geometric Information Enhancement, and Temporal Extension Deformation, which collaborate to accomplish this goal. Furthermore, STP4D is among the first methods to exploit the Diffusion model to generate 4D Gaussians, combining the fine-grained modeling capabilities and the real-time rendering process of 4DGS with the rapid inference speed of the Diffusion model. Extensive experiments demonstrate that STP4D excels in generating high-fidelity 4D content with exceptional efficiency (approximately 4.6s per asset), surpassing existing methods in both quality and speed.","authors":["Yunze Deng","Haijun Xiong","Bin Feng","Xinggang Wang","Wenyu Liu"],"url":"https://arxiv.org/abs/2504.18318"}
{"created":"2025-04-28","title":"Stable localized orthogonal decomposition in Raviart-Thomas spaces","abstract":"This work proposes a computational multiscale method for the mixed formulation of a second-order linear elliptic equation subject to a homogeneous Neumann boundary condition, based on a stable localized orthogonal decomposition (LOD) in Raviart-Thomas finite element spaces. In the spirit of numerical homogenization, the construction provides low-dimensional coarse approximation spaces that incorporate fine-scale information from the heterogeneous coefficients by solving local patch problems on a fine mesh. The resulting numerical scheme is accompanied by a rigorous error analysis, and it is applicable beyond periodicity and scale-separation in spatial dimensions two and three. In particular, this novel realization circumvents the presence of pollution terms observed in a previous LOD construction for elliptic problems in mixed formulation. Finally, various numerical experiments are provided that demonstrate the performance of the method.","authors":["Patrick Henning","Hao Li","Timo Sprekeler"],"url":"https://arxiv.org/abs/2504.18322"}
{"created":"2025-04-28","title":"Outlier-aware Tensor Robust Principal Component Analysis with Self-guided Data Augmentation","abstract":"Tensor Robust Principal Component Analysis (TRPCA) is a fundamental technique for decomposing multi-dimensional data into a low-rank tensor and an outlier tensor, yet existing methods relying on sparse outlier assumptions often fail under structured corruptions. In this paper, we propose a self-guided data augmentation approach that employs adaptive weighting to suppress outlier influence, reformulating the original TRPCA problem into a standard Tensor Principal Component Analysis (TPCA) problem. The proposed model involves an optimization-driven weighting scheme that dynamically identifies and downweights outlier contributions during tensor augmentation. We develop an efficient proximal block coordinate descent algorithm with closed-form updates to solve the resulting optimization problem, ensuring computational efficiency. Theoretical convergence is guaranteed through a framework combining block coordinate descent with majorization-minimization principles. Numerical experiments on synthetic and real-world datasets, including face recovery, background subtraction, and hyperspectral denoising, demonstrate that our method effectively handles various corruption patterns. The results show the improvements in both accuracy and computational efficiency compared to state-of-the-art methods.","authors":["Yangyang Xu","Kexin Li","Li Yang","You-Wei Wen"],"url":"https://arxiv.org/abs/2504.18323"}
{"created":"2025-04-28","title":"Depth3DLane: Monocular 3D Lane Detection via Depth Prior Distillation","abstract":"Monocular 3D lane detection is challenging due to the difficulty in capturing depth information from single-camera images. A common strategy involves transforming front-view (FV) images into bird's-eye-view (BEV) space through inverse perspective mapping (IPM), facilitating lane detection using BEV features. However, IPM's flat-ground assumption and loss of contextual information lead to inaccuracies in reconstructing 3D information, especially height. In this paper, we introduce a BEV-based framework to address these limitations and improve 3D lane detection accuracy. Our approach incorporates a Hierarchical Depth-Aware Head that provides multi-scale depth features, mitigating the flat-ground assumption by enhancing spatial awareness across varying depths. Additionally, we leverage Depth Prior Distillation to transfer semantic depth knowledge from a teacher model, capturing richer structural and contextual information for complex lane structures. To further refine lane continuity and ensure smooth lane reconstruction, we introduce a Conditional Random Field module that enforces spatial coherence in lane predictions. Extensive experiments validate that our method achieves state-of-the-art performance in terms of z-axis error and outperforms other methods in the field in overall performance. The code is released at: https://anonymous.4open.science/r/Depth3DLane-DCDD.","authors":["Dongxin Lyu","Han Huang","Cheng Tan","Zimu Li"],"url":"https://arxiv.org/abs/2504.18325"}
{"created":"2025-04-28","title":"Exhaled Breath Analysis Through the Lens of Molecular Communication: A Survey","abstract":"Molecular Communication (MC) has long been envisioned to enable an Internet of Bio-Nano Things (IoBNT) with medical applications, where nanomachines within the human body conduct monitoring, diagnosis, and therapy at micro- and nanoscale levels. MC involves information transfer via molecules and is supported by well-established theoretical models. However, practically achieving reliable, energy-efficient, and bio-compatible communication at these scales still remains a challenge. Air-Based Molecular Communication (ABMC) is a type of MC that operates over larger, meter-scale distances and extends even outside the human body. Therefore, devices and techniques to realize ABMC are readily accessible, and associated use cases can be very promising in the near future. Exhaled breath analysis has previously been proposed. It provides a non-invasive approach for health monitoring, leveraging existing commercial sensor technologies and reducing deployment barriers. The breath contains a diverse range of molecules and particles that serve as biomarkers linked to various physiological and pathological conditions. The plethora of proven methods, models, and optimization approaches in MC enable macroscale breath analysis, treating human as the transmitter, the breath as the information carrier, and macroscale sensors as the receiver. Using ABMC to interface with the inherent dynamic networks of cells, tissues, and organs could create a novel Internet of Bio Things (IoBT), a preliminary macroscale stage of the IoBNT. This survey extensively reviews exhaled breath modeling and analysis through the lens of MC, offering insights into theoretical frameworks and practical implementations from ABMC, bringing the IoBT a step closer to real-world use.","authors":["Sunasheer Bhattacharjee","Dadi Bi","Pit Hofmann","Alexander Wietfeld","Sophie Becke","Michael Lommel","Pengjie Zhou","Ruifeng Zheng","Ulrich Kertzscher","Yansha Deng","Wolfgang Kellerer","Frank H. P. Fitzek","Falko Dressler"],"url":"https://arxiv.org/abs/2504.18326"}
{"created":"2025-04-28","title":"AI Safety Assurance for Automated Vehicles: A Survey on Research, Standardization, Regulation","abstract":"Assuring safety of artificial intelligence (AI) applied to safety-critical systems is of paramount importance. Especially since research in the field of automated driving shows that AI is able to outperform classical approaches, to handle higher complexities, and to reach new levels of autonomy. At the same time, the safety assurance required for the use of AI in such safety-critical systems is still not in place. Due to the dynamic and far-reaching nature of the technology, research on safeguarding AI is being conducted in parallel to AI standardization and regulation. The parallel progress necessitates simultaneous consideration in order to carry out targeted research and development of AI systems in the context of automated driving. Therefore, in contrast to existing surveys that focus primarily on research aspects, this paper considers research, standardization and regulation in a concise way. Accordingly, the survey takes into account the interdependencies arising from the triplet of research, standardization and regulation in a forward-looking perspective and anticipates and discusses open questions and possible future directions. In this way, the survey ultimately serves to provide researchers and safety experts with a compact, holistic perspective that discusses the current status, emerging trends, and possible future developments.","authors":["Lars Ullrich","Michael Buchholz","Klaus Dietmayer","Knut Graichen"],"url":"https://arxiv.org/abs/2504.18328"}
{"created":"2025-04-28","title":"PHEATPRUNER: Interpretable Data-centric Feature Selection for Multivariate Time Series Classification through Persistent Homology","abstract":"Balancing performance and interpretability in multivariate time series classification is a significant challenge due to data complexity and high dimensionality. This paper introduces PHeatPruner, a method integrating persistent homology and sheaf theory to address these challenges. Persistent homology facilitates the pruning of up to 45% of the applied variables while maintaining or enhancing the accuracy of models such as Random Forest, CatBoost, XGBoost, and LightGBM, all without depending on posterior probabilities or supervised optimization algorithms. Concurrently, sheaf theory contributes explanatory vectors that provide deeper insights into the data's structural nuances. The approach was validated using the UEA Archive and a mastitis detection dataset for dairy cows. The results demonstrate that PHeatPruner effectively preserves model accuracy. Furthermore, our results highlight PHeatPruner's key features, i.e. simplifying complex data and offering actionable insights without increasing processing time or complexity. This method bridges the gap between complexity reduction and interpretability, suggesting promising applications in various fields.","authors":["Anh-Duy Pham","Olivier Basole Kashongwe","Martin Atzmueller","Tim R\\\"omer"],"url":"https://arxiv.org/abs/2504.18329"}
{"created":"2025-04-28","title":"Neural Incremental Input-to-State Stable Control Lyapunov Functions for Unknown Continuous-time Systems","abstract":"This work primarily focuses on synthesizing a controller that guarantees an unknown continuous-time system to be incrementally input-to-state stable ($\\delta$-ISS). In this context, the notion of $\\delta$-ISS control Lyapunov function ($\\delta$-ISS-CLF) for the continuous-time system is introduced. Combined with the controller, the $\\delta$-ISS-CLF guarantees that the system is incrementally stable. As the paper deals with unknown dynamical systems, the controller as well as the $\\delta$-ISS-CLF are parametrized using neural networks. The data set used to train the neural networks is generated from the state space of the system by proper sampling. Now, to give a formal guarantee that the controller makes the system incrementally stable, we develop a validity condition by having some Lipschitz continuity assumptions and incorporate the condition into the training framework to ensure a provable correctness guarantee at the end of the training process. Finally, we demonstrate the effectiveness of the proposed approach through several case studies: a scalar system with a non-affine, non-polynomial structure, a one-link manipulator system, a nonlinear Moore-Greitzer model of a jet engine, and a rotating rigid spacecraft model.","authors":["Ahan Basu","Bhabani Shankar Dey","Pushpak Jagtap"],"url":"https://arxiv.org/abs/2504.18330"}
{"created":"2025-04-28","title":"Unifying Direct and Indirect Learning for Safe Control of Linear Systems","abstract":"This paper aims to learn safe controllers for uncertain discrete-time linear systems under disturbances while achieving the following two crucial goals: 1) integration of different sources of information (i.e., prior information in terms of physical knowledge and posterior information in terms of streaming data), and 2) unifying direct learning with indirect learning. These goals are achieved by representing a parametrized data-driven constrained matrix zonotope form of closed-loop systems that is conformant to prior knowledge. To this end, we first leverage collected data to characterize closed-loop systems by a matrix zonotope and then show that the explainability of these closed-loop systems by prior knowledge can be formalized by adding an equality conformity constraint, which refines the matrix zonotope obtained by data to a constrained matrix zonotope. The prior knowledge is further refined by conforming it to the set of models obtained from a novel zonotope-based system identifier. The source of data used for zonotope-based system identification can be different than the one used for closed-loop representation, allowing to perform transfer learning and online adaptation to new data. The parametrized closed-loop set of systems is then leveraged to directly learn a controller that robustly imposes safety on the closed-loop system. We consider both polytope and zonotope safe sets and provide set inclusion conditions using linear programming to impose safety through {\\lambda}-contractivity. For polytope safe sets, a primal-dual optimization is developed to formalize a linear programming optimization that certifies the set inclusion. For zonotope safe sets, the constrained zonotope set of all next states is formed, and set inclusion is achieved by ensuring the inclusion of this constrained zonotope in a {\\lambda}-scaled level set of the safe set.","authors":["Amir Modares","Niyousha Ghiasi","Bahare Kiumarsi","Hamidreza Modares"],"url":"https://arxiv.org/abs/2504.18331"}
{"created":"2025-04-28","title":"SSD-Poser: Avatar Pose Estimation with State Space Duality from Sparse Observations","abstract":"The growing applications of AR/VR increase the demand for real-time full-body pose estimation from Head-Mounted Displays (HMDs). Although HMDs provide joint signals from the head and hands, reconstructing a full-body pose remains challenging due to the unconstrained lower body. Recent advancements often rely on conventional neural networks and generative models to improve performance in this task, such as Transformers and diffusion models. However, these approaches struggle to strike a balance between achieving precise pose reconstruction and maintaining fast inference speed. To overcome these challenges, a lightweight and efficient model, SSD-Poser, is designed for robust full-body motion estimation from sparse observations. SSD-Poser incorporates a well-designed hybrid encoder, State Space Attention Encoders, to adapt the state space duality to complex motion poses and enable real-time realistic pose reconstruction. Moreover, a Frequency-Aware Decoder is introduced to mitigate jitter caused by variable-frequency motion signals, remarkably enhancing the motion smoothness. Comprehensive experiments on the AMASS dataset demonstrate that SSD-Poser achieves exceptional accuracy and computational efficiency, showing outstanding inference efficiency compared to state-of-the-art methods.","authors":["Shuting Zhao","Linxin Bai","Liangjing Shao","Ye Zhang","Xinrong Chen"],"url":"https://arxiv.org/abs/2504.18332"}
{"created":"2025-04-28","title":"Adversarial Attacks on LLM-as-a-Judge Systems: Insights from Prompt Injections","abstract":"LLM as judge systems used to assess text quality code correctness and argument strength are vulnerable to prompt injection attacks. We introduce a framework that separates content author attacks from system prompt attacks and evaluate five models Gemma 3.27B Gemma 3.4B Llama 3.2 3B GPT 4 and Claude 3 Opus on four tasks with various defenses using fifty prompts per condition. Attacks achieved up to seventy three point eight percent success smaller models proved more vulnerable and transferability ranged from fifty point five to sixty two point six percent. Our results contrast with Universal Prompt Injection and AdvPrompter We recommend multi model committees and comparative scoring and release all code and datasets","authors":["Narek Maloyan","Dmitry Namiot"],"url":"https://arxiv.org/abs/2504.18333"}
{"created":"2025-04-28","title":"Rack-Aware Minimum Storage Partially Cooperative Regenerating Codes with Small Sub-Packetization","abstract":"In the rack-aware model, there are $\\bar{n}$ racks each of which has $u$ nodes with the same storage capacity. Assume that there are $h$ failed nodes uniformly distributed in $\\bar{h}$ host racks ( defined as racks containing failed nodes), each rack containing $h/\\bar{h}$ failed nodes where $h$ is divisible by $\\bar{h}$. Then together with its internal helper nodes, each host rack downloads recovery data from $\\bar{d}$ helper racks and repairs its failed nodes. The repair bandwidth is defined as the total inter-rack data transfer required for failures recovery, as the intra-rack communication does not contribute to this cost. The full cooperative repair model requires that each host rack must exchange the data with all the other $\\bar{h}$ host racks during the cooperative repair phase. However, in the partial cooperative repair model, each host rack only needs to exchange data with $\\bar{h}-\\delta\\ (1\\leq\\delta\\leq\\bar{h}-1)$ other host racks, during the cooperative repair phase. In this paper, we focus on the rack-aware minimum storage partially cooperative regenerating (MSPCR) codes for repairing the $h$ node failures. We first derive the lower bound on the repair bandwidth for rack-aware MSPCR codes using extremal combinatorics, and then construct two classes of optimal repair schemes for rack-aware MSPCR codes with small sub-packetization level. In particular, when $\\delta=1$, our second codes reduce to rack-aware minimum-storage cooperative regenerating (MSCR) codes, while achieving an $(\\bar{h}+1)$-fold reduction in sub-packetization level compared to known rack-aware MSCR codes.","authors":["Hengming Zhao","Dianhua Wu","Minquan Cheng"],"url":"https://arxiv.org/abs/2504.18335"}
{"created":"2025-04-28","title":"Computing Distances on Graph Associahedra is Fixed-parameter Tractable","abstract":"An elimination tree of a connected graph $G$ is a rooted tree on the vertices of $G$ obtained by choosing a root $v$ and recursing on the connected components of $G-v$ to obtain the subtrees of $v$. The graph associahedron of $G$ is a polytope whose vertices correspond to elimination trees of $G$ and whose edges correspond to tree rotations, a natural operation between elimination trees. These objects generalize associahedra, which correspond to the case where $G$ is a path. Ito et al. [ICALP 2023] recently proved that the problem of computing distances on graph associahedra is NP-hard. In this paper we prove that the problem, for a general graph $G$, is fixed-parameter tractable parameterized by the distance $k$. Prior to our work, only the case where $G$ is a path was known to be fixed-parameter tractable. To prove our result, we use a novel approach based on a marking scheme that restricts the search to a set of vertices whose size is bounded by a (large) function of $k$.","authors":["Lu\\'is Felipe I. Cunha","Ignasi Sau","U\\'everton S. Souza","Mario Valencia-Pabon"],"url":"https://arxiv.org/abs/2504.18338"}
{"created":"2025-04-28","title":"Optimal Control of Sensor-Induced Illusions on Robotic Agents","abstract":"This paper presents a novel problem of creating and regulating localization and navigation illusions considering two agents: a receiver and a producer. A receiver is moving on a plane localizing itself using the intensity of signals from three known towers observed at its position. Based on this position estimate, it follows a simple policy to reach its goal. The key idea is that a producer alters the signal intensities to alter the position estimate of the receiver while ensuring it reaches a different destination with the belief that it reached its goal. We provide a precise mathematical formulation of this problem and show that it allows standard techniques from control theory to be applied to generate localization and navigation illusions that result in a desired receiver behavior.","authors":["Lorenzo Medici","Steven M. LaValle","Basak Sakcak"],"url":"https://arxiv.org/abs/2504.18339"}
{"created":"2025-04-28","title":"NRevisit: A Cognitive Behavioral Metric for Code Understandability Assessment","abstract":"Measuring code understandability is both highly relevant and exceptionally challenging. This paper proposes a dynamic code understandability assessment method, which estimates a personalized code understandability score from the perspective of the specific programmer handling the code. The method consists of dynamically dividing the code unit under development or review in code regions (invisible to the programmer) and using the number of revisits (NRevisit) to each region as the primary feature for estimating the code understandability score. This approach removes the uncertainty related to the concept of a \"typical programmer\" assumed by static software code complexity metrics and can be easily implemented using a simple, low-cost, and non-intrusive desktop eye tracker or even a standard computer camera. This metric was evaluated using cognitive load measured through electroencephalography (EEG) in a controlled experiment with 35 programmers. Results show a very high correlation ranging from rs = 0.9067 to rs = 0.9860 (with p nearly 0) between the scores obtained with different alternatives of NRevisit and the ground truth represented by the EEG measurements of programmers' cognitive load, demonstrating the effectiveness of our approach in reflecting the cognitive effort required for code comprehension. The paper also discusses possible practical applications of NRevisit, including its use in the context of AI-generated code, which is already widely used today.","authors":["Gao Hao","Haytham Hijazi","J\\'ulio Medeiros","Jo\\~ao Dur\\~aes","Chan Tong Lam","Paulo de Carvalho","Henrique Madeira"],"url":"https://arxiv.org/abs/2504.18345"}
{"created":"2025-04-28","title":"Comparing Uncertainty Measurement and Mitigation Methods for Large Language Models: A Systematic Review","abstract":"Large Language Models (LLMs) have been transformative across many domains. However, hallucination -- confidently outputting incorrect information -- remains one of the leading challenges for LLMs. This raises the question of how to accurately assess and quantify the uncertainty of LLMs. Extensive literature on traditional models has explored Uncertainty Quantification (UQ) to measure uncertainty and employed calibration techniques to address the misalignment between uncertainty and accuracy. While some of these methods have been adapted for LLMs, the literature lacks an in-depth analysis of their effectiveness and does not offer a comprehensive benchmark to enable insightful comparison among existing solutions. In this work, we fill this gap via a systematic survey of representative prior works on UQ and calibration for LLMs and introduce a rigorous benchmark. Using two widely used reliability datasets, we empirically evaluate six related methods, which justify the significant findings of our review. Finally, we provide outlooks for key future directions and outline open challenges. To the best of our knowledge, this survey is the first dedicated study to review the calibration methods and relevant metrics for LLMs.","authors":["Toghrul Abbasli","Kentaroh Toyoda","Yuan Wang","Leon Witt","Muhammad Asif Ali","Yukai Miao","Dan Li","Qingsong Wei"],"url":"https://arxiv.org/abs/2504.18346"}
{"created":"2025-04-28","title":"TSCL:Multi-party loss Balancing scheme for deep learning Image steganography based on Curriculum learning","abstract":"For deep learning-based image steganography frameworks, in order to ensure the invisibility and recoverability of the information embedding, the loss function usually contains several losses such as embedding loss, recovery loss and steganalysis loss. In previous research works, fixed loss weights are usually chosen for training optimization, and this setting is not linked to the importance of the steganography task itself and the training process. In this paper, we propose a Two-stage Curriculum Learning loss scheduler (TSCL) for balancing multinomial losses in deep learning image steganography algorithms. TSCL consists of two phases: a priori curriculum control and loss dynamics control. The first phase firstly focuses the model on learning the information embedding of the original image by controlling the loss weights in the multi-party adversarial training; secondly, it makes the model shift its learning focus to improving the decoding accuracy; and finally, it makes the model learn to generate a steganographic image that is resistant to steganalysis. In the second stage, the learning speed of each training task is evaluated by calculating the loss drop of the before and after iteration rounds to balance the learning of each task. Experimental results on three large public datasets, ALASKA2, VOC2012 and ImageNet, show that the proposed TSCL strategy improves the quality of steganography, decoding accuracy and security.","authors":["Fengchun Liu. Tong Zhang","Chunying Zhang"],"url":"https://arxiv.org/abs/2504.18348"}
{"created":"2025-04-28","title":"Revisiting Data Auditing in Large Vision-Language Models","abstract":"With the surge of large language models (LLMs), Large Vision-Language Models (VLMs)--which integrate vision encoders with LLMs for accurate visual grounding--have shown great potential in tasks like generalist agents and robotic control. However, VLMs are typically trained on massive web-scraped images, raising concerns over copyright infringement and privacy violations, and making data auditing increasingly urgent. Membership inference (MI), which determines whether a sample was used in training, has emerged as a key auditing technique, with promising results on open-source VLMs like LLaVA (AUC > 80%). In this work, we revisit these advances and uncover a critical issue: current MI benchmarks suffer from distribution shifts between member and non-member images, introducing shortcut cues that inflate MI performance. We further analyze the nature of these shifts and propose a principled metric based on optimal transport to quantify the distribution discrepancy. To evaluate MI in realistic settings, we construct new benchmarks with i.i.d. member and non-member images. Existing MI methods fail under these unbiased conditions, performing only marginally better than chance. Further, we explore the theoretical upper bound of MI by probing the Bayes Optimality within the VLM's embedding space and find the irreducible error rate remains high. Despite this pessimistic outlook, we analyze why MI for VLMs is particularly challenging and identify three practical scenarios--fine-tuning, access to ground-truth texts, and set-based inference--where auditing becomes feasible. Our study presents a systematic view of the limits and opportunities of MI for VLMs, providing guidance for future efforts in trustworthy data auditing.","authors":["Hongyu Zhu","Sichu Liang","Wenwen Wang","Boheng Li","Tongxin Yuan","Fangqi Li","ShiLin Wang","Zhuosheng Zhang"],"url":"https://arxiv.org/abs/2504.18349"}
{"created":"2025-04-28","title":"A Linear Time Algorithm for the Maximum Overlap of Two Convex Polygons Under Translation","abstract":"Given two convex polygons $P$ and $Q$ with $n$ and $m$ edges, the maximum overlap problem is to find a translation of $P$ that maximizes the area of its intersection with $Q$. We give the first randomized algorithm for this problem with linear running time. Our result improves the previous two-and-a-half-decades-old algorithm by de Berg, Cheong, Devillers, van Kreveld, and Teillaud (1998), which ran in $O((n+m)\\log(n+m))$ time, as well as multiple recent algorithms given for special cases of the problem.","authors":["Timothy M. Chan","Isaac M. Hair"],"url":"https://arxiv.org/abs/2504.18352"}
{"created":"2025-04-28","title":"Testing Individual Fairness in Graph Neural Networks","abstract":"The biases in artificial intelligence (AI) models can lead to automated decision-making processes that discriminate against groups and/or individuals based on sensitive properties such as gender and race. While there are many studies on diagnosing and mitigating biases in various AI models, there is little research on individual fairness in Graph Neural Networks (GNNs). Unlike traditional models, which treat data features independently and overlook their inter-relationships, GNNs are designed to capture graph-based structure where nodes are interconnected. This relational approach enables GNNs to model complex dependencies, but it also means that biases can propagate through these connections, complicating the detection and mitigation of individual fairness violations. This PhD project aims to develop a testing framework to assess and ensure individual fairness in GNNs. It first systematically reviews the literature on individual fairness, categorizing existing approaches to define, measure, test, and mitigate model biases, creating a taxonomy of individual fairness. Next, the project will develop a framework for testing and ensuring fairness in GNNs by adapting and extending current fairness testing and mitigation techniques. The framework will be evaluated through industrial case studies, focusing on graph-based large language models.","authors":["Roya Nasiri"],"url":"https://arxiv.org/abs/2504.18353"}
{"created":"2025-04-28","title":"Interpretable Affordance Detection on 3D Point Clouds with Probabilistic Prototypes","abstract":"Robotic agents need to understand how to interact with objects in their environment, both autonomously and during human-robot interactions. Affordance detection on 3D point clouds, which identifies object regions that allow specific interactions, has traditionally relied on deep learning models like PointNet++, DGCNN, or PointTransformerV3. However, these models operate as black boxes, offering no insight into their decision-making processes. Prototypical Learning methods, such as ProtoPNet, provide an interpretable alternative to black-box models by employing a \"this looks like that\" case-based reasoning approach. However, they have been primarily applied to image-based tasks. In this work, we apply prototypical learning to models for affordance detection on 3D point clouds. Experiments on the 3D-AffordanceNet benchmark dataset show that prototypical models achieve competitive performance with state-of-the-art black-box models and offer inherent interpretability. This makes prototypical models a promising candidate for human-robot interaction scenarios that require increased trust and safety.","authors":["Maximilian Xiling Li","Korbinian Rudolf","Nils Blank","Rudolf Lioutikov"],"url":"https://arxiv.org/abs/2504.18355"}
{"created":"2025-04-28","title":"Numerical method for the inverse scattering by random periodic structures","abstract":"Due to manufacturing defects or wear and tear, industrial components may have uncertainties. In order to evaluate the performance of machined components, it is crucial to quantify the uncertainty of the scattering surface. This brings up an important class of inverse scattering problems for random interface reconstruction. In this paper, we present an efficient numerical algorithm for the inverse scattering problem of acoustic-elastic interaction with random periodic interfaces. The proposed algorithm combines the Monte Carlo technique and the continuation method with respect to the wavenumber, which can accurately reconstruct the key statistics of random periodic interfaces from the measured data of the acoustic scattered field. In the implementation of our algorithm, a key two-step strategy is employed: Firstly, the elastic displacement field below the interface is determined by Tikhonov regularization based on the dynamic interface condition; Secondly, the profile function is iteratively updated and optimised using the Landweber method according to the kinematic interface condition. Such a algorithm does not require a priori information about the stochastic structures and performs well for both stationary Gaussian and non-Gaussian stochastic processes. Numerical experiments demonstrate the reliability and effectiveness of our proposed method.","authors":["Yi Wang","Lei Lin","Junliang Lv"],"url":"https://arxiv.org/abs/2504.18356"}
{"created":"2025-04-28","title":"Convergence analysis of Lie and Strang splitting for operator-valued differential Riccati equations","abstract":"Differential Riccati equations (DREs) are semilinear matrix- or operator-valued differential equations with quadratic non-linearities. They arise in many different areas, and are particularly important in optimal control of linear quadratic regulators, where they provide the optimal feedback control laws. In the context of control of partial differential equations, these Riccati equations are operator-valued. To approximate their solutions, both spatial and temporal discretizations are needed. While the former have been well analyzed in the literature, there are very few rigorous convergence analyses of time stepping methods applied to DREs, particularly in the infinite-dimensional, operator-valued setting. In view of this, we analyze two numerical time-stepping schemes, the Lie and Strang splitting methods, in such a setting. The analysis relies on the assumption that the uncontrolled system evolves via an operator that generates an analytic semigroup, and that either the initial condition is sufficiently smooth, or the nonlinearity in the DRE is sufficiently smoothing. These assumptions are mild, in the sense that they are not enough to even guarantee continuity in operator-norm of the exact solution to the DRE. However, they imply certain regularity in a pointwise sense, which can be leveraged to prove convergence in operator-norm with the classical orders. The results are illustrated by four numerical experiments, where convergence with the expected order is correlated with the relevant assumptions being fulfilled. The experiments also demonstrate that matrix-valued DREs which arise as spatial discretizations of operator-valued DREs behave similarly, unless the discretization is coarse.","authors":["Eskil Hansen","Tony Stillfjord","Teodor {\\AA}berg"],"url":"https://arxiv.org/abs/2504.18358"}
{"created":"2025-04-28","title":"On the Generalization of Kitaev Codes as Generalized Bicycle Codes","abstract":"Surface codes have historically been the dominant choice for quantum error correction due to their superior error threshold performance. However, recently, a new class of Generalized Bicycle (GB) codes, constructed from binary circulant matrices with three non-zero elements per row, achieved comparable performance with fewer physical qubits and higher encoding efficiency.","authors":["Fran\\c{c}ois Arnault","Philippe Gaborit","Nicolas Saussay"],"url":"https://arxiv.org/abs/2504.18360"}
{"created":"2025-04-28","title":"COCO-Inpaint: A Benchmark for Image Inpainting Detection and Manipulation Localization","abstract":"Recent advancements in image manipulation have achieved unprecedented progress in generating photorealistic content, but also simultaneously eliminating barriers to arbitrary manipulation and editing, raising concerns about multimedia authenticity and cybersecurity. However, existing Image Manipulation Detection and Localization (IMDL) methodologies predominantly focus on splicing or copy-move forgeries, lacking dedicated benchmarks for inpainting-based manipulations. To bridge this gap, we present COCOInpaint, a comprehensive benchmark specifically designed for inpainting detection, with three key contributions: 1) High-quality inpainting samples generated by six state-of-the-art inpainting models, 2) Diverse generation scenarios enabled by four mask generation strategies with optional text guidance, and 3) Large-scale coverage with 258,266 inpainted images with rich semantic diversity. Our benchmark is constructed to emphasize intrinsic inconsistencies between inpainted and authentic regions, rather than superficial semantic artifacts such as object shapes. We establish a rigorous evaluation protocol using three standard metrics to assess existing IMDL approaches. The dataset will be made publicly available to facilitate future research in this area.","authors":["Haozhen Yan","Yan Hong","Jiahui Zhan","Yikun Ji","Jun Lan","Huijia Zhu","Weiqiang Wang","Jianfu Zhang"],"url":"https://arxiv.org/abs/2504.18361"}
{"created":"2025-04-28","title":"Achievable Rates and Error Probability Bounds of Frequency-based Channels of Unlimited Input Resolution","abstract":"We consider a molecular channel, in which messages are encoded to the frequency of objects in a pool, and whose output during reading time is a noisy version of the input frequencies, as obtained by sampling with replacement from the pool. Motivated by recent DNA storage techniques, we focus on the regime in which the input resolution is unlimited. We propose two error probability bounds for this channel; the first bound is based on random coding analysis of the error probability of the maximum likelihood decoder and the second bound is derived by code expurgation techniques. We deduce an achievable bound on the capacity of this channel, and compare it to both the achievable bounds under limited input resolution, as well as to a converse bound.","authors":["Ran Tamir","Nir Weinberger"],"url":"https://arxiv.org/abs/2504.18364"}
{"created":"2025-04-28","title":"On constrained intersection representations of graphs and digraphs","abstract":"We study the problem of determining optimal directed intersection representations of DAGs in a model introduced by Kostochka, Liu, Machado, and Milenkovic [ISIT2019]: vertices are assigned color sets so that there is an arc from a vertex $u$ to a vertex $v$ if and only if their color sets have nonempty intersection and $v$ gets assigned strictly more colors than $u$, and the goal is to minimize the total number of colors. We show that the problem is polynomially solvable in the class of triangle-free and Hamiltonian DAGs and also disclose the relationship of this problem with several other models of intersection representations of graphs and digraphs.","authors":["Ferdinando Cicalese","Cl\\'ement Dallard","Martin Milani\\v{c}"],"url":"https://arxiv.org/abs/2504.18365"}
{"created":"2025-04-28","title":"Renewable-Colocated Green Hydrogen Production: Optimal Scheduling and Profitability","abstract":"We study the optimal green hydrogen production and energy market participation of a renewable-colocated hydrogen producer (RCHP) that utilizes onsite renewable generation for both hydrogen production and grid services. Under deterministic and stochastic profit-maximization frameworks, we analyze RCHP's multiple market participation models and derive closed-form optimal scheduling policies that dynamically allocate renewable energy to hydrogen production and electricity export to the wholesale market. Analytical characterizations of the RCHP's operating profit and the optimal sizing of renewable and electrolyzer capacities are obtained. We use real-time renewable production and electricity price data from three independent system operators to assess the impacts of hydrogen market prices, renewable generation, and electricity prices on RCHP's profitability.","authors":["Siying Li","Lang Tong","Timothy Mount","Kanchan Upadhyay","Harris Eisenhardt","Pradip Kumar"],"url":"https://arxiv.org/abs/2504.18368"}
{"created":"2025-04-28","title":"ThreMoLIA: Threat Modeling of Large Language Model-Integrated Applications","abstract":"Large Language Models (LLMs) are currently being integrated into industrial software applications to help users perform more complex tasks in less time. However, these LLM-Integrated Applications (LIA) expand the attack surface and introduce new kinds of threats. Threat modeling is commonly used to identify these threats and suggest mitigations. However, it is a time-consuming practice that requires the involvement of a security practitioner. Our goals are to 1) provide a method for performing threat modeling for LIAs early in their lifecycle, (2) develop a threat modeling tool that integrates existing threat models, and (3) ensure high-quality threat modeling. To achieve the goals, we work in collaboration with our industry partner. Our proposed way of performing threat modeling will benefit industry by requiring fewer security experts' participation and reducing the time spent on this activity. Our proposed tool combines LLMs and Retrieval Augmented Generation (RAG) and uses sources such as existing threat models and application architecture repositories to continuously create and update threat models. We propose to evaluate the tool offline -- i.e., using benchmarking -- and online with practitioners in the field. We conducted an early evaluation using ChatGPT on a simple LIA and obtained results that encouraged us to proceed with our research efforts.","authors":["Felix Viktor Jedrzejewski","Davide Fucci","Oleksandr Adamov"],"url":"https://arxiv.org/abs/2504.18369"}
{"created":"2025-04-28","title":"Explainable AI for UAV Mobility Management: A Deep Q-Network Approach for Handover Minimization","abstract":"The integration of unmanned aerial vehicles (UAVs) into cellular networks presents significant mobility management challenges, primarily due to frequent handovers caused by probabilistic line-of-sight conditions with multiple ground base stations (BSs). To tackle these challenges, reinforcement learning (RL)-based methods, particularly deep Q-networks (DQN), have been employed to optimize handover decisions dynamically. However, a major drawback of these learning-based approaches is their black-box nature, which limits interpretability in the decision-making process. This paper introduces an explainable AI (XAI) framework that incorporates Shapley Additive Explanations (SHAP) to provide deeper insights into how various state parameters influence handover decisions in a DQN-based mobility management system. By quantifying the impact of key features such as reference signal received power (RSRP), reference signal received quality (RSRQ), buffer status, and UAV position, our approach enhances the interpretability and reliability of RL-based handover solutions. To validate and compare our framework, we utilize real-world network performance data collected from UAV flight trials. Simulation results show that our method provides intuitive explanations for policy decisions, effectively bridging the gap between AI-driven models and human decision-makers.","authors":["Irshad A. Meer","Bruno H\\\"ormann","Mustafa Ozger","Fabien Geyer","Alberto Viseras","Dominic Schupke","Cicek Cavdar"],"url":"https://arxiv.org/abs/2504.18371"}
{"created":"2025-04-28","title":"Auto-SLURP: A Benchmark Dataset for Evaluating Multi-Agent Frameworks in Smart Personal Assistant","abstract":"In recent years, multi-agent frameworks powered by large language models (LLMs) have advanced rapidly. Despite this progress, there is still a notable absence of benchmark datasets specifically tailored to evaluate their performance. To bridge this gap, we introduce Auto-SLURP, a benchmark dataset aimed at evaluating LLM-based multi-agent frameworks in the context of intelligent personal assistants. Auto-SLURP extends the original SLURP dataset -- initially developed for natural language understanding tasks -- by relabeling the data and integrating simulated servers and external services. This enhancement enables a comprehensive end-to-end evaluation pipeline, covering language understanding, task execution, and response generation. Our experiments demonstrate that Auto-SLURP presents a significant challenge for current state-of-the-art frameworks, highlighting that truly reliable and intelligent multi-agent personal assistants remain a work in progress. The dataset and related code are available at https://github.com/lorashen/Auto-SLURP/.","authors":["Lei Shen","Xiaoyu Shen"],"url":"https://arxiv.org/abs/2504.18373"}
{"created":"2025-04-28","title":"Bandit on the Hunt: Dynamic Crawling for Cyber Threat Intelligence","abstract":"Public information contains valuable Cyber Threat Intelligence (CTI) that is used to prevent future attacks. While standards exist for sharing this information, much appears in non-standardized news articles or blogs. Monitoring online sources for threats is time-consuming and source selection is uncertain. Current research focuses on extracting Indicators of Compromise from known sources, rarely addressing new source identification. This paper proposes a CTI-focused crawler using multi-armed bandit (MAB) and various crawling strategies. It employs SBERT to identify relevant documents while dynamically adapting its crawling path. Our system ThreatCrawl achieves a harvest rate exceeding 25% and expands its seed by over 300% while maintaining topical focus. Additionally, the crawler identifies previously unknown but highly relevant overview pages, datasets, and domains.","authors":["Philipp Kuehn","Dilara Nadermahmoodi","Markus Bayer","Christian Reuter"],"url":"https://arxiv.org/abs/2504.18375"}
{"created":"2025-04-28","title":"Pushing the boundary on Natural Language Inference","abstract":"Natural Language Inference (NLI) is a central task in natural language understanding with applications in fact-checking, question answering, and information retrieval. Despite its importance, current NLI systems heavily rely on supervised learning with datasets that often contain annotation artifacts and biases, limiting generalization and real-world applicability. In this work, we apply a reinforcement learning-based approach using Group Relative Policy Optimization (GRPO) for Chain-of-Thought (CoT) learning in NLI, eliminating the need for labeled rationales and enabling this type of training on more challenging datasets such as ANLI. We fine-tune 7B, 14B, and 32B language models using parameter-efficient techniques (LoRA and QLoRA), demonstrating strong performance across standard and adversarial NLI benchmarks. Our 32B AWQ-quantized model surpasses state-of-the-art results on 7 out of 11 adversarial sets$\\unicode{x2013}$or on all of them considering our replication$\\unicode{x2013}$within a 22GB memory footprint, showing that robust reasoning can be retained under aggressive quantization. This work provides a scalable and practical framework for building robust NLI systems without sacrificing inference quality.","authors":["Pablo Miralles-Gonz\\'alez","Javier Huertas-Tato","Alejandro Mart\\'in","David Camacho"],"url":"https://arxiv.org/abs/2504.18376"}
{"created":"2025-04-28","title":"Spatial Reasoner: A 3D Inference Pipeline for XR Applications","abstract":"Modern extended reality XR systems provide rich analysis of image data and fusion of sensor input and demand AR/VR applications that can reason about 3D scenes in a semantic manner. We present a spatial reasoning framework that bridges geometric facts with symbolic predicates and relations to handle key tasks such as determining how 3D objects are arranged among each other ('on', 'behind', 'near', etc.). Its foundation relies on oriented 3D bounding box representations, enhanced by a comprehensive set of spatial predicates, ranging from topology and connectivity to directionality and orientation, expressed in a formalism related to natural language. The derived predicates form a spatial knowledge graph and, in combination with a pipeline-based inference model, enable spatial queries and dynamic rule evaluation. Implementations for client- and server-side processing demonstrate the framework's capability to efficiently translate geometric data into actionable knowledge, ensuring scalable and technology-independent spatial reasoning in complex 3D environments. The Spatial Reasoner framework is fostering the creation of spatial ontologies, and seamlessly integrates with and therefore enriches machine learning, natural language processing, and rule systems in XR applications.","authors":["Steven H\\\"asler","Philipp Ackermann"],"url":"https://arxiv.org/abs/2504.18380"}
{"created":"2025-04-28","title":"Bridge the Domains: Large Language Models Enhanced Cross-domain Sequential Recommendation","abstract":"Cross-domain Sequential Recommendation (CDSR) aims to extract the preference from the user's historical interactions across various domains. Despite some progress in CDSR, two problems set the barrier for further advancements, i.e., overlap dilemma and transition complexity. The former means existing CDSR methods severely rely on users who own interactions on all domains to learn cross-domain item relationships, compromising the practicability. The latter refers to the difficulties in learning the complex transition patterns from the mixed behavior sequences. With powerful representation and reasoning abilities, Large Language Models (LLMs) are promising to address these two problems by bridging the items and capturing the user's preferences from a semantic view. Therefore, we propose an LLMs Enhanced Cross-domain Sequential Recommendation model (LLM4CDSR). To obtain the semantic item relationships, we first propose an LLM-based unified representation module to represent items. Then, a trainable adapter with contrastive regularization is designed to adapt the CDSR task. Besides, a hierarchical LLMs profiling module is designed to summarize user cross-domain preferences. Finally, these two modules are integrated into the proposed tri-thread framework to derive recommendations. We have conducted extensive experiments on three public cross-domain datasets, validating the effectiveness of LLM4CDSR. We have released the code online.","authors":["Qidong Liu","Xiangyu Zhao","Yejing Wang","Zijian Zhang","Howard Zhong","Chong Chen","Xiang Li","Wei Huang","Feng Tian"],"url":"https://arxiv.org/abs/2504.18383"}
{"created":"2025-04-28","title":"Model Evaluation in the Dark: Robust Classifier Metrics with Missing Labels","abstract":"Missing data in supervised learning is well-studied, but the specific issue of missing labels during model evaluation has been overlooked. Ignoring samples with missing values, a common solution, can introduce bias, especially when data is Missing Not At Random (MNAR). We propose a multiple imputation technique for evaluating classifiers using metrics such as precision, recall, and ROC-AUC. This method not only offers point estimates but also a predictive distribution for these quantities when labels are missing. We empirically show that the predictive distribution's location and shape are generally correct, even in the MNAR regime. Moreover, we establish that this distribution is approximately Gaussian and provide finite-sample convergence bounds. Additionally, a robustness proof is presented, confirming the validity of the approximation under a realistic error model.","authors":["Danial Dervovic","Michael Cashmore"],"url":"https://arxiv.org/abs/2504.18385"}
{"created":"2025-04-28","title":"A UD Treebank for Bohairic Coptic","abstract":"Despite recent advances in digital resources for other Coptic dialects, especially Sahidic, Bohairic Coptic, the main Coptic dialect for pre-Mamluk, late Byzantine Egypt, and the contemporary language of the Coptic Church, remains critically under-resourced. This paper presents and evaluates the first syntactically annotated corpus of Bohairic Coptic, sampling data from a range of works, including Biblical text, saints' lives and Christian ascetic writing. We also explore some of the main differences we observe compared to the existing UD treebank of Sahidic Coptic, the classical dialect of the language, and conduct joint and cross-dialect parsing experiments, revealing the unique nature of Bohairic as a related, but distinct variety from the more often studied Sahidic.","authors":["Amir Zeldes","Nina Speransky","Nicholas Wagner","Caroline T. Schroeder"],"url":"https://arxiv.org/abs/2504.18386"}
{"created":"2025-04-28","title":"Fast Autoregressive Models for Continuous Latent Generation","abstract":"Autoregressive models have demonstrated remarkable success in sequential data generation, particularly in NLP, but their extension to continuous-domain image generation presents significant challenges. Recent work, the masked autoregressive model (MAR), bypasses quantization by modeling per-token distributions in continuous spaces using a diffusion head but suffers from slow inference due to the high computational cost of the iterative denoising process. To address this, we propose the Fast AutoRegressive model (FAR), a novel framework that replaces MAR's diffusion head with a lightweight shortcut head, enabling efficient few-step sampling while preserving autoregressive principles. Additionally, FAR seamlessly integrates with causal Transformers, extending them from discrete to continuous token generation without requiring architectural modifications. Experiments demonstrate that FAR achieves $2.3\\times$ faster inference than MAR while maintaining competitive FID and IS scores. This work establishes the first efficient autoregressive paradigm for high-fidelity continuous-space image generation, bridging the critical gap between quality and scalability in visual autoregressive modeling.","authors":["Tiankai Hang","Jianmin Bao","Fangyun Wei","Dong Chen"],"url":"https://arxiv.org/abs/2504.18391"}
{"created":"2025-04-28","title":"Machine Learning and Statistical Insights into Hospital Stay Durations: The Italian EHR Case","abstract":"Length of hospital stay is a critical metric for assessing healthcare quality and optimizing hospital resource management. This study aims to identify factors influencing LoS within the Italian healthcare context, using a dataset of hospitalization records from over 60 healthcare facilities in the Piedmont region, spanning from 2020 to 2023. We explored a variety of features, including patient characteristics, comorbidities, admission details, and hospital-specific factors. Significant correlations were found between LoS and features such as age group, comorbidity score, admission type, and the month of admission. Machine learning models, specifically CatBoost and Random Forest, were used to predict LoS. The highest R2 score, 0.49, was achieved with CatBoost, demonstrating good predictive performance.","authors":["Marina Andric","Mauro Dragoni"],"url":"https://arxiv.org/abs/2504.18393"}
{"created":"2025-04-28","title":"Maximum Coverage in Turnstile Streams with Applications to Fingerprinting Measures","abstract":"In the maximum coverage problem we are given $d$ subsets from a universe $[n]$, and the goal is to output $k$ subsets such that their union covers the largest possible number of distinct items. We present the first algorithm for maximum coverage in the turnstile streaming model, where updates which insert or delete an item from a subset come one-by-one. Notably our algorithm only uses $poly\\log n$ update time. We also present turnstile streaming algorithms for targeted and general fingerprinting for risk management where the goal is to determine which features pose the greatest re-identification risk in a dataset. As part of our work, we give a result of independent interest: an algorithm to estimate the complement of the $p^{\\text{th}}$ frequency moment of a vector for $p \\geq 2$. Empirical evaluation confirms the practicality of our fingerprinting algorithms demonstrating a speedup of up to $210$x over prior work.","authors":["Alina Ene","Alessandro Epasto","Vahab Mirrokni","Hoai-An Nguyen","Huy L. Nguyen","David P. Woodruff","Peilin Zhong"],"url":"https://arxiv.org/abs/2504.18394"}
{"created":"2025-04-28","title":"Three Types of Calibration with Properties and their Semantic and Formal Relationships","abstract":"Fueled by discussions around \"trustworthiness\" and algorithmic fairness, calibration of predictive systems has regained scholars attention. The vanilla definition and understanding of calibration is, simply put, on all days on which the rain probability has been predicted to be p, the actual frequency of rain days was p. However, the increased attention has led to an immense variety of new notions of \"calibration.\" Some of the notions are incomparable, serve different purposes, or imply each other. In this work, we provide two accounts which motivate calibration: self-realization of forecasted properties and precise estimation of incurred losses of the decision makers relying on forecasts. We substantiate the former via the reflection principle and the latter by actuarial fairness. For both accounts we formulate prototypical definitions via properties $\\Gamma$ of outcome distributions, e.g., the mean or median. The prototypical definition for self-realization, which we call $\\Gamma$-calibration, is equivalent to a certain type of swap regret under certain conditions. These implications are strongly connected to the omniprediction learning paradigm. The prototypical definition for precise loss estimation is a modification of decision calibration adopted from Zhao et al. [73]. For binary outcome sets both prototypical definitions coincide under appropriate choices of reference properties. For higher-dimensional outcome sets, both prototypical definitions can be subsumed by a natural extension of the binary definition, called distribution calibration with respect to a property. We conclude by commenting on the role of groupings in both accounts of calibration often used to obtain multicalibration. In sum, this work provides a semantic map of calibration in order to navigate a fragmented terrain of notions and definitions.","authors":["Rabanus Derr","Jessie Finocchiaro","Robert C. Williamson"],"url":"https://arxiv.org/abs/2504.18395"}
{"created":"2025-04-28","title":"Energy Security and Resilience: Reviewing Concepts and Advancing Planning Perspectives for Transforming Integrated Energy Systems","abstract":"Recent events, including the pandemic, geopolitical conflicts, supply chain disruptions, and climate change impacts, have exposed the critical need to ensure energy security and resilience in energy systems. We review existing definitions and interrelations between energy security and resilience, conceptualising these terms in the context of energy system transformations. We introduce a classification of disturbances into shock events and slow burn processes to highlight key challenges to energy system resilience. Examples illustrate their distinct impacts on technical, economic, and environmental system performance over time. We compile relevant recourse options across resilience capacity levels and system planning horizons to address these challenges, emphasising actionable strategies for an increasingly integrated energy system. Finally, we propose policy recommendations to integrate shock events and slow burn processes into future energy system planning, enabling forward-looking decision-making and system design to analyse and mitigate potential disruptions.","authors":["Richard Schmitz","Franziska Flachsbarth","Leonie Sara Plaga","Martin Braun","Philipp H\\\"artel"],"url":"https://arxiv.org/abs/2504.18396"}
{"created":"2025-04-28","title":"Unsupervised Visual Chain-of-Thought Reasoning via Preference Optimization","abstract":"Chain-of-thought (CoT) reasoning greatly improves the interpretability and problem-solving abilities of multimodal large language models (MLLMs). However, existing approaches are focused on text CoT, limiting their ability to leverage visual cues. Visual CoT remains underexplored, and the only work is based on supervised fine-tuning (SFT) that relies on extensive labeled bounding-box data and is hard to generalize to unseen cases. In this paper, we introduce Unsupervised Visual CoT (UV-CoT), a novel framework for image-level CoT reasoning via preference optimization. UV-CoT performs preference comparisons between model-generated bounding boxes (one is preferred and the other is dis-preferred), eliminating the need for bounding-box annotations. We get such preference data by introducing an automatic data generation pipeline. Given an image, our target MLLM (e.g., LLaVA-1.5-7B) generates seed bounding boxes using a template prompt and then answers the question using each bounded region as input. An evaluator MLLM (e.g., OmniLLM-12B) ranks the responses, and these rankings serve as supervision to train the target MLLM with UV-CoT by minimizing negative log-likelihood losses. By emulating human perception--identifying key regions and reasoning based on them--UV-CoT can improve visual comprehension, particularly in spatial reasoning tasks where textual descriptions alone fall short. Our experiments on six datasets demonstrate the superiority of UV-CoT, compared to the state-of-the-art textual and visual CoT methods. Our zero-shot testing on four unseen datasets shows the strong generalization of UV-CoT. The code is available in https://github.com/kesenzhao/UV-CoT.","authors":["Kesen Zhao","Beier Zhu","Qianru Sun","Hanwang Zhang"],"url":"https://arxiv.org/abs/2504.18397"}
{"created":"2025-04-28","title":"Optimal Control for Network of Coupled Oscillators","abstract":"This paper presents a nonlinear control framework for steering networks of coupled oscillators toward desired phase-locked configurations. Inspired by brain dynamics, where structured phase differences support cognitive functions, the focus is on achieving synchronization patterns beyond global coherence. The Kuramoto model, expressed in phase-difference coordinates, is used to describe the system dynamics. The control problem is formulated within the State-Dependent Riccati Equation (SDRE) framework, enabling the design of feedback laws through state-dependent factorisation. The unconstrained control formulation serves as a principled starting point for developing more general approaches that incorporate coupling constraints and actuation limits. Numerical simulations demonstrate that the proposed approach achieves robust phase-locking in both heterogeneous and large-scale oscillator networks, highlighting its potential applications in neuroscience, robotics, and distributed systems.","authors":["Adnan Tahirovic"],"url":"https://arxiv.org/abs/2504.18399"}
{"created":"2025-04-28","title":"Paradigm shift on Coding Productivity Using GenAI","abstract":"Generative AI (GenAI) applications are transforming software engineering by enabling automated code co-creation. However, empirical evidence on GenAI's productivity effects in industrial settings remains limited. This paper investigates the adoption of GenAI coding assistants (e.g., Codeium, Amazon Q) within telecommunications and FinTech domains. Through surveys and interviews with industrial domain-experts, we identify primary productivity-influencing factors, including task complexity, coding skills, domain knowledge, and GenAI integration. Our findings indicate that GenAI tools enhance productivity in routine coding tasks (e.g., refactoring and Javadoc generation) but face challenges in complex, domain-specific activities due to limited context-awareness of codebases and insufficient support for customized design rules. We highlight new paradigms for coding transfer, emphasizing iterative prompt refinement, immersive development environment, and automated code evaluation as essential for effective GenAI usage.","authors":["Liang Yu"],"url":"https://arxiv.org/abs/2504.18404"}
{"created":"2025-04-28","title":"HRScene: How Far Are VLMs from Effective High-Resolution Image Understanding?","abstract":"High-resolution image (HRI) understanding aims to process images with a large number of pixels, such as pathological images and agricultural aerial images, both of which can exceed 1 million pixels. Vision Large Language Models (VLMs) can allegedly handle HRIs, however, there is a lack of a comprehensive benchmark for VLMs to evaluate HRI understanding. To address this gap, we introduce HRScene, a novel unified benchmark for HRI understanding with rich scenes. HRScene incorporates 25 real-world datasets and 2 synthetic diagnostic datasets with resolutions ranging from 1,024 $\\times$ 1,024 to 35,503 $\\times$ 26,627. HRScene is collected and re-annotated by 10 graduate-level annotators, covering 25 scenarios, ranging from microscopic to radiology images, street views, long-range pictures, and telescope images. It includes HRIs of real-world objects, scanned documents, and composite multi-image. The two diagnostic evaluation datasets are synthesized by combining the target image with the gold answer and distracting images in different orders, assessing how well models utilize regions in HRI. We conduct extensive experiments involving 28 VLMs, including Gemini 2.0 Flash and GPT-4o. Experiments on HRScene show that current VLMs achieve an average accuracy of around 50% on real-world tasks, revealing significant gaps in HRI understanding. Results on synthetic datasets reveal that VLMs struggle to effectively utilize HRI regions, showing significant Regional Divergence and lost-in-middle, shedding light on future research.","authors":["Yusen Zhang","Wenliang Zheng","Aashrith Madasu","Peng Shi","Ryo Kamoi","Hao Zhou","Zhuoyang Zou","Shu Zhao","Sarkar Snigdha Sarathi Das","Vipul Gupta","Xiaoxin Lu","Nan Zhang","Ranran Haoran Zhang","Avitej Iyer","Renze Lou","Wenpeng Yin","Rui Zhang"],"url":"https://arxiv.org/abs/2504.18406"}
{"created":"2025-04-28","title":"Are We on the Same Page? Examining Developer Perception Alignment in Open Source Code Reviews","abstract":"Code reviews are a critical aspect of open-source software (OSS) development, ensuring quality and fostering collaboration. This study examines perceptions, challenges, and biases in OSS code review processes, focusing on the perspectives of Contributors and Maintainers. Through surveys (n=289), interviews (n=23), and repository analysis (n=81), we identify key areas of alignment and disparity. While both groups share common objectives, differences emerge in priorities, e.g, with Maintainers emphasizing alignment with project goals while Contributors overestimated the value of novelty. Bias, particularly familiarity bias, disproportionately affects underrepresented groups, discouraging participation and limiting community growth. Misinterpretation of approach differences as bias further complicates reviews. Our findings underscore the need for improved documentation, better tools, and automated solutions to address delays and enhance inclusivity. This work provides actionable strategies to promote fairness and sustain the long-term innovation of OSS ecosystems.","authors":["Yoseph Berhanu Alebachew","Minhyuk Ko","Chris Brown"],"url":"https://arxiv.org/abs/2504.18407"}
{"created":"2025-04-28","title":"Can Code Outlove Blood? A LLM-based VR Experience to Prompt Reflection on Parental Verbal Abuse","abstract":"Parental verbal abuse leaves lasting emotional impacts, yet current therapeutic approaches often lack immersive self-reflection opportunities. To address this, we developed a VR experience powered by LLMs to foster reflection on parental verbal abuse. Participants with relevant experiences engage in a dual-phase VR experience: first assuming the role of a verbally abusive parent, interacting with an LLM portraying a child, then observing the LLM reframing abusive dialogue into warm, supportive expressions as a nurturing parent. A qualitative study with 12 participants showed that the experience encourages reflection on their past experiences and fosters supportive emotions. However, these effects vary with participants' personal histories, emphasizing the need for greater personalization in AI-driven emotional support. This study explores the use of LLMs in immersive environment to promote emotional reflection, offering insights into the design of AI-driven emotional support systems.","authors":["Jiaying Fu","Jialin Gu","Tianyue Gong","Tiange Zhou"],"url":"https://arxiv.org/abs/2504.18410"}
{"created":"2025-04-28","title":"Heavy-Tailed Privacy: The Symmetric alpha-Stable Privacy Mechanism","abstract":"With the rapid growth of digital platforms, there is increasing apprehension about how personal data is collected, stored, and used by various entities. These concerns arise from the increasing frequency of data breaches, cyber-attacks, and misuse of personal information for targeted advertising and surveillance. To address these matters, Differential Privacy (DP) has emerged as a prominent tool for quantifying a digital system's level of protection. The Gaussian mechanism is commonly used because the Gaussian density is closed under convolution, and is a common method utilized when aggregating datasets. However, the Gaussian mechanism only satisfies an approximate form of Differential Privacy. In this work, we present and analyze of the Symmetric alpha-Stable (SaS) mechanism. We prove that the mechanism achieves pure differential privacy while remaining closed under convolution. Additionally, we study the nuanced relationship between the level of privacy achieved and the parameters of the density. Lastly, we compare the expected error introduced to dataset queries by the Gaussian and SaS mechanisms. From our analysis, we believe the SaS Mechanism is an appealing choice for privacy-focused applications.","authors":["Christopher C. Zawacki","Eyad H. Abed"],"url":"https://arxiv.org/abs/2504.18411"}
{"created":"2025-04-28","title":"Expressing stigma and inappropriate responses prevents LLMs from safely replacing mental health providers","abstract":"Should a large language model (LLM) be used as a therapist? In this paper, we investigate the use of LLMs to *replace* mental health providers, a use case promoted in the tech startup and research space. We conduct a mapping review of therapy guides used by major medical institutions to identify crucial aspects of therapeutic relationships, such as the importance of a therapeutic alliance between therapist and client. We then assess the ability of LLMs to reproduce and adhere to these aspects of therapeutic relationships by conducting several experiments investigating the responses of current LLMs, such as `gpt-4o`. Contrary to best practices in the medical community, LLMs 1) express stigma toward those with mental health conditions and 2) respond inappropriately to certain common (and critical) conditions in naturalistic therapy settings -- e.g., LLMs encourage clients' delusional thinking, likely due to their sycophancy. This occurs even with larger and newer LLMs, indicating that current safety practices may not address these gaps. Furthermore, we note foundational and practical barriers to the adoption of LLMs as therapists, such as that a therapeutic alliance requires human characteristics (e.g., identity and stakes). For these reasons, we conclude that LLMs should not replace therapists, and we discuss alternative roles for LLMs in clinical therapy.","authors":["Jared Moore","Declan Grabb","William Agnew","Kevin Klyman","Stevie Chancellor","Desmond C. Ong","Nick Haber"],"url":"https://arxiv.org/abs/2504.18412"}
{"created":"2025-04-28","title":"An Empirical Study of Evaluating Long-form Question Answering","abstract":"\\Ac{LFQA} aims to generate lengthy answers to complex questions. This scenario presents great flexibility as well as significant challenges for evaluation. Most evaluations rely on deterministic metrics that depend on string or n-gram matching, while the reliability of large language model-based evaluations for long-form answers remains relatively unexplored. We address this gap by conducting an in-depth study of long-form answer evaluation with the following research questions: (i) To what extent do existing automatic evaluation metrics serve as a substitute for human evaluations? (ii) What are the limitations of existing evaluation metrics compared to human evaluations? (iii) How can the effectiveness and robustness of existing evaluation methods be improved? We collect 5,236 factoid and non-factoid long-form answers generated by different large language models and conduct a human evaluation on 2,079 of them, focusing on correctness and informativeness. Subsequently, we investigated the performance of automatic evaluation metrics by evaluating these answers, analyzing the consistency between these metrics and human evaluations. We find that the style, length of the answers, and the category of questions can bias the automatic evaluation metrics. However, fine-grained evaluation helps mitigate this issue on some metrics. Our findings have important implications for the use of large language models for evaluating long-form question answering. All code and datasets are available at https://github.com/bugtig6351/lfqa_evaluation.","authors":["Ning Xian","Yixing Fan","Ruqing Zhang","Maarten de Rijke","Jiafeng Guo"],"url":"https://arxiv.org/abs/2504.18413"}
{"created":"2025-04-28","title":"Online learning to accelerate nonlinear PDE solvers: applied to multiphase porous media flow","abstract":"We propose a novel type of nonlinear solver acceleration for systems of nonlinear partial differential equations (PDEs) that is based on online/adaptive learning. It is applied in the context of multiphase flow in porous media. The proposed method rely on four pillars: (i) dimensionless numbers as input parameters for the machine learning model, (ii) simplified numerical model (two-dimensional) for the offline training, (iii) dynamic control of a nonlinear solver tuning parameter (numerical relaxation), (iv) and online learning for real-time improvement of the machine learning model. This strategy decreases the number of nonlinear iterations by dynamically modifying a single global parameter, the relaxation factor, and by adaptively learning the attributes of each numerical model on-the-run. Furthermore, this work performs a sensitivity study in the dimensionless parameters (machine learning features), assess the efficacy of various machine learning models, demonstrate a decrease in nonlinear iterations using our method in more intricate, realistic three-dimensional models, and fully couple a machine learning model into an open-source multiphase flow simulator achieving up to 85\\% reduction in computational time.","authors":["Vinicius L S Silva","Pablo Salinas","Claire E Heaney","Matthew Jackson","Christopher C Pain"],"url":"https://arxiv.org/abs/2504.18414"}
{"created":"2025-04-28","title":"BitNet v2: Native 4-bit Activations with Hadamard Transformation for 1-bit LLMs","abstract":"Efficient deployment of 1-bit Large Language Models (LLMs) is hindered by activation outliers, which complicate quantization to low bit-widths. We introduce BitNet v2, a novel framework enabling native 4-bit activation quantization for 1-bit LLMs. To tackle outliers in attention and feed-forward network activations, we propose H-BitLinear, a module applying an online Hadamard transformation prior to activation quantization. This transformation smooths sharp activation distributions into more Gaussian-like forms, suitable for low-bit representation. Experiments show BitNet v2 trained from scratch with 8-bit activations matches BitNet b1.58 performance. Crucially, BitNet v2 achieves minimal performance degradation when trained with native 4-bit activations, significantly reducing memory footprint and computational cost for batched inference.","authors":["Hongyu Wang","Shuming Ma","Furu Wei"],"url":"https://arxiv.org/abs/2504.18415"}
{"created":"2025-04-28","title":"A Multimodal Hybrid Late-Cascade Fusion Network for Enhanced 3D Object Detection","abstract":"We present a new way to detect 3D objects from multimodal inputs, leveraging both LiDAR and RGB cameras in a hybrid late-cascade scheme, that combines an RGB detection network and a 3D LiDAR detector. We exploit late fusion principles to reduce LiDAR False Positives, matching LiDAR detections with RGB ones by projecting the LiDAR bounding boxes on the image. We rely on cascade fusion principles to recover LiDAR False Negatives leveraging epipolar constraints and frustums generated by RGB detections of separate views. Our solution can be plugged on top of any underlying single-modal detectors, enabling a flexible training process that can take advantage of pre-trained LiDAR and RGB detectors, or train the two branches separately. We evaluate our results on the KITTI object detection benchmark, showing significant performance improvements, especially for the detection of Pedestrians and Cyclists.","authors":["Carlo Sgaravatti","Roberto Basla","Riccardo Pieroni","Matteo Corno","Sergio M. Savaresi","Luca Magri","Giacomo Boracchi"],"url":"https://arxiv.org/abs/2504.18419"}
{"created":"2025-04-28","title":"Enhancing System Self-Awareness and Trust of AI: A Case Study in Trajectory Prediction and Planning","abstract":"In the trajectory planning of automated driving, data-driven statistical artificial intelligence (AI) methods are increasingly established for predicting the emergent behavior of other road users. While these methods achieve exceptional performance in defined datasets, they usually rely on the independent and identically distributed (i.i.d.) assumption and thus tend to be vulnerable to distribution shifts that occur in the real world. In addition, these methods lack explainability due to their black box nature, which poses further challenges in terms of the approval process and social trustworthiness. Therefore, in order to use the capabilities of data-driven statistical AI methods in a reliable and trustworthy manner, the concept of TrustMHE is introduced and investigated in this paper. TrustMHE represents a complementary approach, independent of the underlying AI systems, that combines AI-driven out-of-distribution detection with control-driven moving horizon estimation (MHE) to enable not only detection and monitoring, but also intervention. The effectiveness of the proposed TrustMHE is evaluated and proven in three simulation scenarios.","authors":["Lars Ullrich","Zurab Mujirishvili","Knut Graichen"],"url":"https://arxiv.org/abs/2504.18421"}
{"created":"2025-04-28","title":"Automated Consistency Analysis for Legal Contracts","abstract":"Business contracts, particularly sale and purchase agreements, often contain a large number of clauses and are correspondingly long and complex. In practice, it is therefore a great challenge to keep track of their legal context and to identify and avoid inconsistencies in such contracts. Against this background, we describe a method and tool called ContractCheck which allows for the consistency analysis of legal contracts, in particular Share Purchase Agreements (SPAs). In order to identify the concepts that are relevant for an analysis we define an ontology for SPAs. The analysis is, then, based on an encoding of the preconditions for the execution of the clauses of an SPA, as well as on a set of proposed consistency constraints formalized using decidable fragments of First-Order Logic (FOL). Based on the ontology for SPAs, textual SPAs are first encoded in a structured natural language format that we refer to as ``blocks''. ContractCheck interprets these blocks and constraints and translates them into assertions formulated in FOL. It then invokes a Satisfiability Modulo Theory (SMT) solver in order to check the executability of a considered contract, either by providing a satisfying model, or by proving the existence of conflicting clauses that prevent the contract from being executed. We illustrate the application of ContractCheck to concrete SPAs, including one example of an SPA of realistic size and complexity, and conclude by suggesting directions for future research.","authors":["Alan Khoja","Martin K\\\"olbl","Stefan Leue","R\\\"udiger Wilhelmi"],"url":"https://arxiv.org/abs/2504.18422"}
{"created":"2025-04-28","title":"LLMpatronous: Harnessing the Power of LLMs For Vulnerability Detection","abstract":"Despite the transformative impact of Artificial Intelligence (AI) across various sectors, cyber security continues to rely on traditional static and dynamic analysis tools, hampered by high false positive rates and superficial code comprehension. While generative AI offers promising automation capabilities for software development, leveraging Large Language Models (LLMs) for vulnerability detection presents unique challenges. This paper explores the potential and limitations of LLMs in identifying vulnerabilities, acknowledging inherent weaknesses such as hallucinations, limited context length, and knowledge cut-offs. Previous attempts employing machine learning models for vulnerability detection have proven ineffective due to limited real-world applicability, feature engineering challenges, lack of contextual understanding, and the complexities of training models to keep pace with the evolving threat landscape. Therefore, we propose a robust AI-driven approach focused on mitigating these limitations and ensuring the quality and reliability of LLM based vulnerability detection. Through innovative methodologies combining Retrieval-Augmented Generation (RAG) and Mixtureof-Agents (MoA), this research seeks to leverage the strengths of LLMs while addressing their weaknesses, ultimately paving the way for dependable and efficient AI-powered solutions in securing the ever-evolving software landscape.","authors":["Rajesh Yarra"],"url":"https://arxiv.org/abs/2504.18423"}
{"created":"2025-04-28","title":"LaRI: Layered Ray Intersections for Single-view 3D Geometric Reasoning","abstract":"We present layered ray intersections (LaRI), a new method for unseen geometry reasoning from a single image. Unlike conventional depth estimation that is limited to the visible surface, LaRI models multiple surfaces intersected by the camera rays using layered point maps. Benefiting from the compact and layered representation, LaRI enables complete, efficient, and view-aligned geometric reasoning to unify object- and scene-level tasks. We further propose to predict the ray stopping index, which identifies valid intersecting pixels and layers from LaRI's output. We build a complete training data generation pipeline for synthetic and real-world data, including 3D objects and scenes, with necessary data cleaning steps and coordination between rendering engines. As a generic method, LaRI's performance is validated in two scenarios: It yields comparable object-level results to the recent large generative model using 4% of its training data and 17% of its parameters. Meanwhile, it achieves scene-level occluded geometry reasoning in only one feed-forward.","authors":["Rui Li","Biao Zhang","Zhenyu Li","Federico Tombari","Peter Wonka"],"url":"https://arxiv.org/abs/2504.18424"}
{"created":"2025-04-28","title":"Subexponential and Parameterized Mixing Times of Glauber Dynamics on Independent Sets","abstract":"Given a graph $G$, the hard-core model defines a probability distribution over its independent sets, assigning to each set of size $k$ a probability of $\\frac{\\lambda^k}{Z}$, where $\\lambda>0$ is a parameter known as the fugacity and $Z$ is a normalization constant. The Glauber dynamics is a simple Markov chain that converges to this distribution and enables efficient sampling. Its mixing time--the number of steps needed to approach the stationary distribution--has been widely studied across various graph classes, with most previous work emphasizing the dichotomy between polynomial and exponential mixing times, with a particular focus on sparse classes of graphs.","authors":["Malory Marin"],"url":"https://arxiv.org/abs/2504.18427"}
{"created":"2025-04-28","title":"PolyMath: Evaluating Mathematical Reasoning in Multilingual Contexts","abstract":"In this paper, we introduce PolyMath, a multilingual mathematical reasoning benchmark covering 18 languages and 4 easy-to-hard difficulty levels. Our benchmark ensures difficulty comprehensiveness, language diversity, and high-quality translation, making it a highly discriminative multilingual mathematical benchmark in the era of reasoning LLMs. We conduct a comprehensive evaluation for advanced LLMs and find that even Deepseek-R1-671B and Qwen-QwQ-32B, achieve only 43.4 and 41.8 benchmark scores, with less than 30% accuracy under the highest level. From a language perspective, our benchmark reveals several key challenges of LLMs in multilingual reasoning: (1) Reasoning performance varies widely across languages for current LLMs; (2) Input-output language consistency is low in reasoning LLMs and may be correlated with performance; (3) The thinking length differs significantly by language for current LLMs. Additionally, we demonstrate that controlling the output language in the instructions has the potential to affect reasoning performance, especially for some low-resource languages, suggesting a promising direction for improving multilingual capabilities in LLMs.","authors":["Yiming Wang","Pei Zhang","Jialong Tang","Haoran Wei","Baosong Yang","Rui Wang","Chenshu Sun","Feitong Sun","Jiran Zhang","Junxuan Wu","Qiqian Cang","Yichang Zhang","Fei Huang","Junyang Lin","Fei Huang","Jingren Zhou"],"url":"https://arxiv.org/abs/2504.18428"}
{"created":"2025-04-28","title":"Efficiency, Expressivity, and Extensibility in a Close-to-Metal NPU Programming Interface","abstract":"Accelerators such as neural processing units (NPUs) deliver an enticing balance of performance and efficiency compared to general purpose compute architectures. However, effectively leveraging accelerator capabilities is not always simple: low-level programming toolkits may require substantial developer effort while high-level programming toolkits may abstract critical optimization features.","authors":["Erika Hunhoff","Joseph Melber","Kristof Denolf","Andra Bisca","Samuel Bayliss","Stephen Neuendorffer","Jeff Fifield","Jack Lo","Pranathi Vasireddy","Phil James-Roxby","Eric Keller"],"url":"https://arxiv.org/abs/2504.18430"}
{"created":"2025-04-28","title":"FlexiNS: A SmartNIC-Centric, Line-Rate and Flexible Network Stack","abstract":"As the gap between network and CPU speeds rapidly increases, the CPU-centric network stack proves inadequate due to excessive CPU and memory overhead. While hardware-offloaded network stacks alleviate these issues, they suffer from limited flexibility in both control and data planes. Offloading network stack to off-path SmartNIC seems promising to provide high flexibility; however, throughput remains constrained by inherent SmartNIC architectural limitations.","authors":["Xuzheng Chen","Jie Zhang","Baolin Zhu","Xueying Zhu","Zhongqing Chen","Shu Ma","Lingjun Zhu","Chao Shi","Yin Zhang","Zeke Wang"],"url":"https://arxiv.org/abs/2504.18432"}
{"created":"2025-04-28","title":"An Axiomatic Assessment of Entropy- and Variance-based Uncertainty Quantification in Regression","abstract":"Uncertainty quantification (UQ) is crucial in machine learning, yet most (axiomatic) studies of uncertainty measures focus on classification, leaving a gap in regression settings with limited formal justification and evaluations. In this work, we introduce a set of axioms to rigorously assess measures of aleatoric, epistemic, and total uncertainty in supervised regression. By utilizing a predictive exponential family, we can generalize commonly used approaches for uncertainty representation and corresponding uncertainty measures. More specifically, we analyze the widely used entropy- and variance-based measures regarding limitations and challenges. Our findings provide a principled foundation for UQ in regression, offering theoretical insights and practical guidelines for reliable uncertainty assessment.","authors":["Christopher B\\\"ulte","Yusuf Sale","Timo L\\\"ohr","Paul Hofman","Gitta Kutyniok","Eyke H\\\"ullermeier"],"url":"https://arxiv.org/abs/2504.18433"}
{"created":"2025-04-28","title":"Constructing Hamiltonian Decompositions of Complete $k$-Uniform Hypergraphs","abstract":"Motivated by the wide-ranging applications of Hamiltonian decompositions in distributed computing, coded caching, routing, resource allocation, load balancing, and fault tolerance, our work presents a comprehensive design for Hamiltonian decompositions of complete $k$-uniform hypergraphs $K_n^k$. Building upon the resolution of the long-standing conjecture of the existence of Hamiltonian decompositions of complete hypergraphs, a problem that was resolved using existence-based methods, our contribution goes beyond the previous explicit designs, which were confined to the specific cases of $k=2$ and $k=3$, by providing explicit designs for all $k$ and $n$ prime, allowing for a broad applicability of Hamiltonian decompositions in various settings.","authors":["Javad Maheri","Petros Elia"],"url":"https://arxiv.org/abs/2504.18434"}
{"created":"2025-04-28","title":"Enhancing Pre-Trained Model-Based Class-Incremental Learning through Neural Collapse","abstract":"Class-Incremental Learning (CIL) is a critical capability for real-world applications, enabling learning systems to adapt to new tasks while retaining knowledge from previous ones. Recent advancements in pre-trained models (PTMs) have significantly advanced the field of CIL, demonstrating superior performance over traditional methods. However, understanding how features evolve and are distributed across incremental tasks remains an open challenge. In this paper, we propose a novel approach to modeling feature evolution in PTM-based CIL through the lens of neural collapse (NC), a striking phenomenon observed in the final phase of training, which leads to a well-separated, equiangular feature space. We explore the connection between NC and CIL effectiveness, showing that aligning feature distributions with the NC geometry enhances the ability to capture the dynamic behavior of continual learning. Based on this insight, we introduce Neural Collapse-inspired Pre-Trained Model-based CIL (NCPTM-CIL), a method that dynamically adjusts the feature space to conform to the elegant NC structure, thereby enhancing the continual learning process. Extensive experiments demonstrate that NCPTM-CIL outperforms state-of-the-art methods across four benchmark datasets. Notably, when initialized with ViT-B/16-IN1K, NCPTM-CIL surpasses the runner-up method by 6.73% on VTAB, 1.25% on CIFAR-100, and 2.5% on OmniBenchmark.","authors":["Kun He","Zijian Song","Shuoxi Zhang","John E. Hopcroft"],"url":"https://arxiv.org/abs/2504.18437"}
{"created":"2025-04-28","title":"The Autonomous Software Stack of the FRED-003C: The Development That Led to Full-Scale Autonomous Racing","abstract":"Scientific development often takes place in the context of research projects carried out by dedicated students during their time at university. In the field of self-driving software research, the Formula Student Driverless competitions are an excellent platform to promote research and attract young engineers. This article presents the software stack developed by BME Formula Racing Team, that formed the foundation of the development that ultimately led us to full-scale autonomous racing. The experience we gained here contributes greatly to our successful participation in the Abu Dhabi Autonomous Racing League. We therefore think it is important to share the system we used, providing a valuable starting point for other ambitious students. We provide a detailed description of the software pipeline we used, including a brief description of the hardware-software architecture. Furthermore, we introduce the methods that we developed for the modules that implement perception; localisation and mapping, planning, and control tasks.","authors":["Zal\\'an Demeter","Levente Pusk\\'as","Bal\\'azs Kov\\'acs","\\'Ad\\'am Matkovics","Martin N\\'adas","Bal\\'azs Tuba","Zsolt Farkas","\\'Armin Bog\\'ar-N\\'emeth","Gergely B\\'ari"],"url":"https://arxiv.org/abs/2504.18439"}
{"created":"2025-04-28","title":"Expectation-based Analysis of Higher-Order Quantum Programs","abstract":"The paper extends the expectation transformer based analysis of higher-order probabilistic programs to the quantum higher-order setting. The quantum language we are considering can be seen as an extension of PCF, featuring unbounded recursion. The language admits classical and quantum data, as well as a tick operator to account for costs. Our quantum expectation transformer translates such programs into a functional, non-quantum language, enriched with a type and operations over so called cost-structures. By specializing the cost-structure, this methodology makes it possible to study several expectation based properties of quantum programs, such as average case cost, probabilities of events or expected values, in terms of the translated non-quantum programs, this way enabling classical reasoning techniques. As a show-case, we adapt a refinement type system, capable of reasoning on upper-bounds.","authors":["Martin Avanzini","Alejandro D\\'iaz-Caro","Emmanuel Hainry","Romain P\\'echoux"],"url":"https://arxiv.org/abs/2504.18441"}
{"created":"2025-04-28","title":"Pseudo-Boolean Proof Logging for Optimal Classical Planning","abstract":"We introduce lower-bound certificates for classical planning tasks, which can be used to prove the unsolvability of a task or the optimality of a plan in a way that can be verified by an independent third party. We describe a general framework for generating lower-bound certificates based on pseudo-Boolean constraints, which is agnostic to the planning algorithm used.","authors":["Simon Dold","Malte Helmert","Jakob Nordstr\\\"om","Gabriele R\\\"oger","Tanja Schindler"],"url":"https://arxiv.org/abs/2504.18443"}
{"created":"2025-04-28","title":"Boosting-Enabled Robust System Identification of Partially Observed LTI Systems Under Heavy-Tailed Noise","abstract":"We consider the problem of system identification of partially observed linear time-invariant (LTI) systems. Given input-output data, we provide non-asymptotic guarantees for identifying the system parameters under general heavy-tailed noise processes. Unlike previous works that assume Gaussian or sub-Gaussian noise, we consider significantly broader noise distributions that are required to admit only up to the second moment. For this setting, we leverage tools from robust statistics to propose a novel system identification algorithm that exploits the idea of boosting. Despite the much weaker noise assumptions, we show that our proposed algorithm achieves sample complexity bounds that nearly match those derived under sub-Gaussian noise. In particular, we establish that our bounds retain a logarithmic dependence on the prescribed failure probability. Interestingly, we show that such bounds can be achieved by requiring just a finite fourth moment on the excitatory input process.","authors":["Vinay Kanakeri","Aritra Mitra"],"url":"https://arxiv.org/abs/2504.18444"}
{"created":"2025-04-28","title":"Iterative Event-based Motion Segmentation by Variational Contrast Maximization","abstract":"Event cameras provide rich signals that are suitable for motion estimation since they respond to changes in the scene. As any visual changes in the scene produce event data, it is paramount to classify the data into different motions (i.e., motion segmentation), which is useful for various tasks such as object detection and visual servoing. We propose an iterative motion segmentation method, by classifying events into background (e.g., dominant motion hypothesis) and foreground (independent motion residuals), thus extending the Contrast Maximization framework. Experimental results demonstrate that the proposed method successfully classifies event clusters both for public and self-recorded datasets, producing sharp, motion-compensated edge-like images. The proposed method achieves state-of-the-art accuracy on moving object detection benchmarks with an improvement of over 30%, and demonstrates its possibility of applying to more complex and noisy real-world scenes. We hope this work broadens the sensitivity of Contrast Maximization with respect to both motion parameters and input events, thus contributing to theoretical advancements in event-based motion segmentation estimation. https://github.com/aoki-media-lab/event_based_segmentation_vcmax","authors":["Ryo Yamaki","Shintaro Shiba","Guillermo Gallego","Yoshimitsu Aoki"],"url":"https://arxiv.org/abs/2504.18447"}
{"created":"2025-04-28","title":"NoiseController: Towards Consistent Multi-view Video Generation via Noise Decomposition and Collaboration","abstract":"High-quality video generation is crucial for many fields, including the film industry and autonomous driving. However, generating videos with spatiotemporal consistencies remains challenging. Current methods typically utilize attention mechanisms or modify noise to achieve consistent videos, neglecting global spatiotemporal information that could help ensure spatial and temporal consistency during video generation. In this paper, we propose the NoiseController, consisting of Multi-Level Noise Decomposition, Multi-Frame Noise Collaboration, and Joint Denoising, to enhance spatiotemporal consistencies in video generation. In multi-level noise decomposition, we first decompose initial noises into scene-level foreground/background noises, capturing distinct motion properties to model multi-view foreground/background variations. Furthermore, each scene-level noise is further decomposed into individual-level shared and residual components. The shared noise preserves consistency, while the residual component maintains diversity. In multi-frame noise collaboration, we introduce an inter-view spatiotemporal collaboration matrix and an intra-view impact collaboration matrix , which captures mutual cross-view effects and historical cross-frame impacts to enhance video quality. The joint denoising contains two parallel denoising U-Nets to remove each scene-level noise, mutually enhancing video generation. We evaluate our NoiseController on public datasets focusing on video generation and downstream tasks, demonstrating its state-of-the-art performance.","authors":["Haotian Dong","Xin Wang","Di Lin","Yipeng Wu","Qin Chen","Ruonan Liu","Kairui Yang","Ping Li","Qing Guo"],"url":"https://arxiv.org/abs/2504.18448"}
{"created":"2025-04-28","title":"Automatic Bias Detection in Source Code Review","abstract":"Bias is an inherent threat to human decision-making, including in decisions made during software development. Extensive research has demonstrated the presence of biases at various stages of the software development life-cycle. Notably, code reviews are highly susceptible to prejudice-induced biases, and individuals are often unaware of these biases as they occur. Developing methods to automatically detect these biases is crucial for addressing the associated challenges. Recent advancements in visual data analytics have shown promising results in detecting potential biases by analyzing user interaction patterns. In this project, we propose a controlled experiment to extend this approach to detect potentially biased outcomes in code reviews by observing how reviewers interact with the code. We employ the \"spotlight model of attention\", a cognitive framework where a reviewer's gaze is tracked to determine their focus areas on the review screen. This focus, identified through gaze tracking, serves as an indicator of the reviewer's areas of interest or concern. We plan to analyze the sequence of gaze focus using advanced sequence modeling techniques, including Markov Models, Recurrent Neural Networks (RNNs), and Conditional Random Fields (CRF). These techniques will help us identify patterns that may suggest biased interactions. We anticipate that the ability to automatically detect potentially biased interactions in code reviews will significantly reduce unnecessary push-backs, enhance operational efficiency, and foster greater diversity and inclusion in software development. This approach not only helps in identifying biases but also in creating a more equitable development environment by mitigating these biases effectively","authors":["Yoseph Berhanu Alebachew","Chris Brown"],"url":"https://arxiv.org/abs/2504.18449"}
{"created":"2025-04-28","title":"Enhancing Strawberry Yield Forecasting with Backcasted IoT Sensor Data and Machine Learning","abstract":"Due to rapid population growth globally, digitally-enabled agricultural sectors are crucial for sustainable food production and making informed decisions about resource management for farmers and various stakeholders. The deployment of Internet of Things (IoT) technologies that collect real-time observations of various environmental (e.g., temperature, humidity, etc.) and operational factors (e.g., irrigation) influencing production is often seen as a critical step to enable additional novel downstream tasks, such as AI-based yield forecasting. However, since AI models require large amounts of data, this creates practical challenges in a real-world dynamic farm setting where IoT observations would need to be collected over a number of seasons. In this study, we deployed IoT sensors in strawberry production polytunnels for two growing seasons to collect environmental data, including water usage, external and internal temperature, external and internal humidity, soil moisture, soil temperature, and photosynthetically active radiation. The sensor observations were combined with manually provided yield records spanning a period of four seasons. To bridge the gap of missing IoT observations for two additional seasons, we propose an AI-based backcasting approach to generate synthetic sensor observations using historical weather data from a nearby weather station and the existing polytunnel observations. We built an AI-based yield forecasting model to evaluate our approach using the combination of real and synthetic observations. Our results demonstrated that incorporating synthetic data improved yield forecasting accuracy, with models incorporating synthetic data outperforming those trained only on historical yield, weather records, and real sensor data.","authors":["Tewodros Alemu Ayall","Andy Li","Matthew Beddows","Milan Markovic","Georgios Leontidis"],"url":"https://arxiv.org/abs/2504.18451"}
{"created":"2025-04-28","title":"Reason Like a Radiologist: Chain-of-Thought and Reinforcement Learning for Verifiable Report Generation","abstract":"Radiology report generation is critical for efficiency but current models lack the structured reasoning of experts, hindering clinical trust and explainability by failing to link visual findings to precise anatomical locations. This paper introduces BoxMed-RL, a groundbreaking unified training framework for generating spatially verifiable and explainable radiology reports. Built on a large vision-language model, BoxMed-RL revolutionizes report generation through two integrated phases: (1) In the Pretraining Phase, we refine the model via medical concept learning, using Chain-of-Thought supervision to internalize the radiologist-like workflow, followed by spatially verifiable reinforcement, which applies reinforcement learning to align medical findings with bounding boxes. (2) In the Downstream Adapter Phase, we freeze the pretrained weights and train a downstream adapter to ensure fluent and clinically credible reports. This framework precisely mimics radiologists' workflow, compelling the model to connect high-level medical concepts with definitive anatomical evidence. Extensive experiments on public datasets demonstrate that BoxMed-RL achieves an average 7% improvement in both METEOR and ROUGE-L metrics compared to state-of-the-art methods. An average 5% improvement in large language model-based metrics further underscores BoxMed-RL's robustness in generating high-quality radiology reports.","authors":["Peiyuan Jing","Kinhei Lee","Zhenxuan Zhang","Huichi Zhou","Zhengqing Yuan","Zhifan Gao","Lei Zhu","Giorgos Papanastasiou","Yingying Fang","Guang Yang"],"url":"https://arxiv.org/abs/2504.18453"}
{"created":"2025-04-28","title":"Pseudo-Asynchronous Local SGD: Robust and Efficient Data-Parallel Training","abstract":"Following AI scaling trends, frontier models continue to grow in size and continue to be trained on larger datasets. Training these models requires huge investments in exascale computational resources, which has in turn driven development of distributed deep learning methods. Data parallelism is an essential approach to speed up training, but it requires frequent global communication between workers, which can bottleneck training at the largest scales. In this work, we propose a method called Pseudo-Asynchronous Local SGD (PALSGD) to improve the efficiency of data-parallel training. PALSGD is an extension of Local SGD (Stich, 2018) and DiLoCo (Douillard et al., 2023), designed to further reduce communication frequency by introducing a pseudo-synchronization mechanism. PALSGD allows the use of longer synchronization intervals compared to standard Local SGD. Despite the reduced communication frequency, the pseudo-synchronization approach ensures that model consistency is maintained, leading to performance results comparable to those achieved with more frequent synchronization. Furthermore, we provide a theoretical analysis of PALSGD, establishing its convergence and deriving its convergence rate. This analysis offers insights into the algorithm's behavior and performance guarantees. We evaluated PALSGD on image classification and language modeling tasks. Our results show that PALSGD achieves better performance in less time compared to existing methods like Distributed Data Parallel (DDP), and DiLoCo. Notably, PALSGD trains 18.4% faster than DDP on ImageNet-1K with ResNet-50, 24.4% faster than DDP on TinyStories with GPT-Neo125M, and 21.1% faster than DDP on TinyStories with GPT-Neo-8M.","authors":["Hiroki Naganuma","Xinzhi Zhang","Man-Chung Yue","Ioannis Mitliagkas","Philipp A. Witte","Russell J. Hewett","Yin Tat Lee"],"url":"https://arxiv.org/abs/2504.18454"}
{"created":"2025-04-28","title":"Improved Dwell-times for Switched Nonlinear Systems using Memory Regression Extension","abstract":"This paper presents a switched systems approach for extending the dwell-time of an autonomous agent during GPS-denied operation by leveraging memory regressor extension (MRE) techniques. To maintain accurate trajectory tracking despite unknown dynamics and environmental disturbances, the agent periodically acquires access to GPS, allowing it to correct accumulated state estimation errors. The motivation for this work arises from the limitations of existing switched system approaches, where increasing estimation errors during GPS-denied intervals and overly conservative dwell-time conditions restrict the operational efficiency of the agent. By leveraging MRE techniques during GPS-available intervals, the developed method refines the estimates of unknown system parameters, thereby enabling longer and more reliable operation in GPS-denied environments. A Lyapunov-based switched-system stability analysis establishes that improved parameter estimates obtained through concurrent learning allow extended operation in GPS-denied intervals without compromising closed-loop system stability. Simulation results validate the theoretical findings, demonstrating dwell-time extensions and enhanced trajectory tracking performance.","authors":["Muzaffar Qureshi","Tochukwu Elijah Ogri","Humberto Ramos","Wanjiku A. Makumi","Zachary I. Bell","Rushikesh Kamalapurkar"],"url":"https://arxiv.org/abs/2504.18457"}
{"created":"2025-04-28","title":"Fast-Slow Thinking for Large Vision-Language Model Reasoning","abstract":"Recent advances in large vision-language models (LVLMs) have revealed an \\textit{overthinking} phenomenon, where models generate verbose reasoning across all tasks regardless of questions. To address this issue, we present \\textbf{FAST}, a novel \\textbf{Fa}st-\\textbf{S}low \\textbf{T}hinking framework that dynamically adapts reasoning depth based on question characteristics. Through empirical analysis, we establish the feasibility of fast-slow thinking in LVLMs by investigating how response length and data distribution affect performance. We develop FAST-GRPO with three components: model-based metrics for question characterization, an adaptive thinking reward mechanism, and difficulty-aware KL regularization. Experiments across seven reasoning benchmarks demonstrate that FAST achieves state-of-the-art accuracy with over 10\\% relative improvement compared to the base model, while reducing token usage by 32.7-67.3\\% compared to previous slow-thinking approaches, effectively balancing reasoning length and accuracy.","authors":["Wenyi Xiao","Leilei Gan","Weilong Dai","Wanggui He","Ziwei Huang","Haoyuan Li","Fangxun Shu","Zhelun Yu","Peng Zhang","Hao Jiang","Fei Wu"],"url":"https://arxiv.org/abs/2504.18458"}
{"created":"2025-04-28","title":"Probabilistic Shaping in MIMO: Going Beyond 1.53dB AWGN Gain With the Non-Linear Demapper","abstract":"Constellation shaping is a well-established method to improve upon a regular quadrature amplitude modulation (QAM). It is known that the gain achieved by any shaping method for an additive white Gaussian noise (AWGN) channel is upper-bounded by 1.53dB. However, the situation becomes less clear in the multiple-input and multiple-output (MIMO) setting.","authors":["Kirill Ivanov","Wei Yang","Jing Jiang"],"url":"https://arxiv.org/abs/2504.18459"}
{"created":"2025-04-28","title":"Discovering Governing Equations of Geomagnetic Storm Dynamics with Symbolic Regression","abstract":"Geomagnetic storms are large-scale disturbances of the Earth's magnetosphere driven by solar wind interactions, posing significant risks to space-based and ground-based infrastructure. The Disturbance Storm Time (Dst) index quantifies geomagnetic storm intensity by measuring global magnetic field variations. This study applies symbolic regression to derive data-driven equations describing the temporal evolution of the Dst index. We use historical data from the NASA OMNIweb database, including solar wind density, bulk velocity, convective electric field, dynamic pressure, and magnetic pressure. The PySR framework, an evolutionary algorithm-based symbolic regression library, is used to identify mathematical expressions linking dDst/dt to key solar wind. The resulting models include a hierarchy of complexity levels and enable a comparison with well-established empirical models such as the Burton-McPherron-Russell and O'Brien-McPherron models. The best-performing symbolic regression models demonstrate superior accuracy in most cases, particularly during moderate geomagnetic storms, while maintaining physical interpretability. Performance evaluation on historical storm events includes the 2003 Halloween Storm, the 2015 St. Patrick's Day Storm, and a 2017 moderate storm. The results provide interpretable, closed-form expressions that capture nonlinear dependencies and thresholding effects in Dst evolution.","authors":["Stefano Markidis","Jonah Ekelund","Luca Pennati","Andong Hu","Ivy Peng"],"url":"https://arxiv.org/abs/2504.18461"}
{"created":"2025-04-28","title":"A Taylor Series Approach to Correction of Input Errors in Gaussian Process Regression","abstract":"Gaussian Processes (GPs) are widely recognized as powerful non-parametric models for regression and classification. Traditional GP frameworks predominantly operate under the assumption that the inputs are either accurately known or subject to zero-mean noise. However, several real-world applications such as mobile sensors have imperfect localization, leading to inputs with biased errors. These biases can typically be estimated through measurements collected over time using, for example, Kalman filters. To avoid recomputation of the entire GP model when better estimates of the inputs used in the training data become available, we introduce a technique for updating a trained GP model to incorporate updated estimates of the inputs. By leveraging the differentiability of the mean and covariance functions derived from the squared exponential kernel, a second-order correction algorithm is developed to update the trained GP models. Precomputed Jacobians and Hessians of kernels enable real-time refinement of the mean and covariance predictions. The efficacy of the developed approach is demonstrated using two simulation studies, with error analyses revealing improvements in both predictive accuracy and uncertainty quantification.","authors":["Muzaffar Qureshi","Tochukwu Elijah Ogri","Zachary I. Bell","Wanjiku A. Makumi","Rushikesh Kamalapurkar"],"url":"https://arxiv.org/abs/2504.18463"}
{"created":"2025-04-28","title":"Generalized Chebyshev Acceleration","abstract":"We use generalized Chebyshev polynomials, associated with the root system $A_2$, to provide a new semi-iterative method for accelerating simple iterative methods for solving linear systems. We apply this semi-iterative method to the Jacobi method, and give an example. There are certain restrictions but the resulting acceleration is rather high.","authors":["Nurg\\\"ul G\\\"okg\\\"oz"],"url":"https://arxiv.org/abs/2504.18465"}
{"created":"2025-04-28","title":"Voltage Stability and Control of Electrical Distribution Systems with High Penetration of Power Electronic Converters","abstract":"Power systems are currently undergoing a rapid paradigm change in their operation. Centralised energy production is being replaced by a number of Distributed Generation (DG) units that are placed at different locations and voltage levels in power networks. These distributed units are mostly based on renewable energy technologies, like wind turbines and photovoltaic cells and are commonly interfaced to the grid via power electronic converters. These sources reduce energy system dependency on conventional generation units based on fossil fuels. At the same time, this shift introduces technical challenges for the safe and reliable operation of electricity network since DG sources do not inherently provide the grid regulation services of conventional, centralised generation units. Moreover, the increased penetration of renewable energy sources and their converter-based interfaces is creating voltage deviation and voltage stability issues in distribution networks. These issues range from overvoltages during hours of peak renewable generation, reverse power flows and sudden voltage drops due to the variable nature of renewable energy production. All of the above jeopardise the reliable operation of the distribution networks that were not originally designed to accommodate for these effects. The objective of this thesis is to propose novel techniques for the accurate assessment of the DG impact on voltage stability in distribution net works and investigate how the control capabilities of converter-based interfaces of DG units can be harnessed to improve stability margins and overall system robustness and performance.","authors":["Dionysios Moutevelis"],"url":"https://arxiv.org/abs/2504.18466"}
{"created":"2025-04-28","title":"RGS-DR: Reflective Gaussian Surfels with Deferred Rendering for Shiny Objects","abstract":"We introduce RGS-DR, a novel inverse rendering method for reconstructing and rendering glossy and reflective objects with support for flexible relighting and scene editing. Unlike existing methods (e.g., NeRF and 3D Gaussian Splatting), which struggle with view-dependent effects, RGS-DR utilizes a 2D Gaussian surfel representation to accurately estimate geometry and surface normals, an essential property for high-quality inverse rendering. Our approach explicitly models geometric and material properties through learnable primitives rasterized into a deferred shading pipeline, effectively reducing rendering artifacts and preserving sharp reflections. By employing a multi-level cube mipmap, RGS-DR accurately approximates environment lighting integrals, facilitating high-quality reconstruction and relighting. A residual pass with spherical-mipmap-based directional encoding further refines the appearance modeling. Experiments demonstrate that RGS-DR achieves high-quality reconstruction and rendering quality for shiny objects, often outperforming reconstruction-exclusive state-of-the-art methods incapable of relighting.","authors":["Georgios Kouros","Minye Wu","Tinne Tuytelaars"],"url":"https://arxiv.org/abs/2504.18468"}
{"created":"2025-04-28","title":"A Novel Taxonomy and Classification Scheme for Code Smell Interactions","abstract":"Code smells are indicators of potential design flaws in source code and do not appear alone but in combination with other smells, creating complex interactions. While existing literature classifies these smell interactions into collocated, coupled, and inter-smell relations, however, to the best of our knowledge, no research has used the existing knowledge of code smells and (or) their relationships with other code smells in the detection of code smells. This gap highlights the need for deeper investigation into how code smells interact with each other and assist in their detection. This would improve the overall comprehension of code smells and how they interact more effectively. This study presents a novel taxonomy and a proposed classification scheme for the possible code smell interactions considering a specific programming language as a domain. This paper has dealt with one scenario called Inter smell detection within the domain. The experiments have been carried out using several popular machine learning (ML) models. Results primarily show the presence of code smell interactions namely Inter-smell Detection within domain. These results are compatible with the available facts in the literature suggesting a promising direction for future research in code smell detection.","authors":["Ruchin Gupta","Sandeep Kumar Singh"],"url":"https://arxiv.org/abs/2504.18469"}
{"created":"2025-04-28","title":"Action Flow Matching for Continual Robot Learning","abstract":"Continual learning in robotics seeks systems that can constantly adapt to changing environments and tasks, mirroring human adaptability. A key challenge is refining dynamics models, essential for planning and control, while addressing issues such as safe adaptation, catastrophic forgetting, outlier management, data efficiency, and balancing exploration with exploitation -- all within task and onboard resource constraints. Towards this goal, we introduce a generative framework leveraging flow matching for online robot dynamics model alignment. Rather than executing actions based on a misaligned model, our approach refines planned actions to better match with those the robot would take if its model was well aligned. We find that by transforming the actions themselves rather than exploring with a misaligned model -- as is traditionally done -- the robot collects informative data more efficiently, thereby accelerating learning. Moreover, we validate that the method can handle an evolving and possibly imperfect model while reducing, if desired, the dependency on replay buffers or legacy model snapshots. We validate our approach using two platforms: an unmanned ground vehicle and a quadrotor. The results highlight the method's adaptability and efficiency, with a record 34.2\\% higher task success rate, demonstrating its potential towards enabling continual robot learning. Code: https://github.com/AlejandroMllo/action_flow_matching.","authors":["Alejandro Murillo-Gonzalez","Lantao Liu"],"url":"https://arxiv.org/abs/2504.18471"}
{"created":"2025-04-28","title":"Generative Induction of Dialogue Task Schemas with Streaming Refinement and Simulated Interactions","abstract":"In task-oriented dialogue (TOD) systems, Slot Schema Induction (SSI) is essential for automatically identifying key information slots from dialogue data without manual intervention. This paper presents a novel state-of-the-art (SoTA) approach that formulates SSI as a text generation task, where a language model incrementally constructs and refines a slot schema over a stream of dialogue data. To develop this approach, we present a fully automatic LLM-based TOD simulation method that creates data with high-quality state labels for novel task domains. Furthermore, we identify issues in SSI evaluation due to data leakage and poor metric alignment with human judgment. We resolve these by creating new evaluation data using our simulation method with human guidance and correction, as well as designing improved evaluation metrics. These contributions establish a foundation for future SSI research and advance the SoTA in dialogue understanding and system development.","authors":["James D. Finch","Yasasvi Josyula","Jinho D. Choi"],"url":"https://arxiv.org/abs/2504.18474"}
{"created":"2025-04-28","title":"Boundaried Kernelization","abstract":"The notion of a (polynomial) kernelization from parameterized complexity is a well-studied model for efficient preprocessing for hard computational problems. By now, it is quite well understood which parameterized problems do or (conditionally) do not admit a polynomial kernelization. Unfortunately, polynomial kernelizations seem to require strong restrictions on the global structure of inputs.","authors":["Leonid Antipov","Stefan Kratsch"],"url":"https://arxiv.org/abs/2504.18476"}
{"created":"2025-04-28","title":"Instrumentation for Better Demonstrations: A Case Study","abstract":"Learning from demonstrations is a powerful paradigm for robot manipulation, but its effectiveness hinges on both the quantity and quality of the collected data. In this work, we present a case study of how instrumentation, i.e. integration of sensors, can improve the quality of demonstrations and automate data collection. We instrument a squeeze bottle with a pressure sensor to learn a liquid dispensing task, enabling automated data collection via a PI controller. Transformer-based policies trained on automated demonstrations outperform those trained on human data in 78% of cases. Our findings indicate that instrumentation not only facilitates scalable data collection but also leads to better-performing policies, highlighting its potential in the pursuit of generalist robotic agents.","authors":["Remko Proesmans","Thomas Lips","Francis wyffels"],"url":"https://arxiv.org/abs/2504.18481"}
{"created":"2025-04-28","title":"Investigating Co-Constructive Behavior of Large Language Models in Explanation Dialogues","abstract":"The ability to generate explanations that are understood by explainees is the quintessence of explainable artificial intelligence. Since understanding depends on the explainee's background and needs, recent research has focused on co-constructive explanation dialogues, where the explainer continuously monitors the explainee's understanding and adapts explanations dynamically. We investigate the ability of large language models (LLMs) to engage as explainers in co-constructive explanation dialogues. In particular, we present a user study in which explainees interact with LLMs, of which some have been instructed to explain a predefined topic co-constructively. We evaluate the explainees' understanding before and after the dialogue, as well as their perception of the LLMs' co-constructive behavior. Our results indicate that current LLMs show some co-constructive behaviors, such as asking verification questions, that foster the explainees' engagement and can improve understanding of a topic. However, their ability to effectively monitor the current understanding and scaffold the explanations accordingly remains limited.","authors":["Leandra Fichtel","Maximilian Splieth\\\"over","Eyke H\\\"ullermeier","Patricia Jimenez","Nils Klowait","Stefan Kopp","Axel-Cyrille Ngonga Ngomo","Amelie Robrecht","Ingrid Scharlau","Lutz Terfloth","Anna-Lisa Vollmer","Henning Wachsmuth"],"url":"https://arxiv.org/abs/2504.18483"}
{"created":"2025-04-28","title":"Tight Lower Bound for Multicolor Discrepancy","abstract":"We prove the following asymptotically tight lower bound for $k$-color discrepancy: For any $k \\geq 2$, there exists a hypergraph with $n$ vertices such that its $k$-color discrepancy is at least $\\Omega(\\sqrt{n})$. This improves on the previously known lower bound of $\\Omega(\\sqrt{n/\\log k})$ due to Caragiannis et al. (arXiv:2502.10516). As an application, we show that our result implies improved lower bounds for group fair division.","authors":["Pasin Manurangsi","Raghu Meka"],"url":"https://arxiv.org/abs/2504.18489"}
{"created":"2025-04-28","title":"An Improved ResNet50 Model for Predicting Pavement Condition Index (PCI) Directly from Pavement Images","abstract":"Accurately predicting the Pavement Condition Index (PCI), a measure of roadway conditions, from pavement images is crucial for infrastructure maintenance. This study proposes an enhanced version of the Residual Network (ResNet50) architecture, integrated with a Convolutional Block Attention Module (CBAM), to predict PCI directly from pavement images without additional annotations. By incorporating CBAM, the model autonomously prioritizes critical features within the images, improving prediction accuracy. Compared to the original baseline ResNet50 and DenseNet161 architectures, the enhanced ResNet50-CBAM model achieved a significantly lower mean absolute percentage error (MAPE) of 58.16%, compared to the baseline models that achieved 70.76% and 65.48% respectively. These results highlight the potential of using attention mechanisms to refine feature extraction, ultimately enabling more accurate and efficient assessments of pavement conditions. This study emphasizes the importance of targeted feature refinement in advancing automated pavement analysis through attention mechanisms.","authors":["Andrews Danyo","Anthony Dontoh","Armstrong Aboah"],"url":"https://arxiv.org/abs/2504.18490"}
{"created":"2025-04-28","title":"Facets, Taxonomies, and Syntheses: Navigating Structured Representations in LLM-Assisted Literature Review","abstract":"Comprehensive literature review requires synthesizing vast amounts of research -- a labor intensive and cognitively demanding process. Most prior work focuses either on helping researchers deeply understand a few papers (e.g., for triaging or reading), or retrieving from and visualizing a vast corpus. Deep analysis and synthesis of large paper collections (e.g., to produce a survey paper) is largely conducted manually with little support. We present DimInd, an interactive system that scaffolds literature review across large paper collections through LLM-generated structured representations. DimInd scaffolds literature understanding with multiple levels of compression, from papers, to faceted literature comparison tables with information extracted from individual papers, to taxonomies of concepts, to narrative syntheses. Users are guided through these successive information transformations while maintaining provenance to source text. In an evaluation with 23 researchers, DimInd supported participants in extracting information and conceptually organizing papers with less effort compared to a ChatGPT-assisted baseline workflow.","authors":["Raymond Fok","Joseph Chee Chang","Marissa Radensky","Pao Siangliulue","Jonathan Bragg","Amy X. Zhang","Daniel S. Weld"],"url":"https://arxiv.org/abs/2504.18496"}
{"created":"2025-04-28","title":"DeSIA: Attribute Inference Attacks Against Limited Fixed Aggregate Statistics","abstract":"Empirical inference attacks are a popular approach for evaluating the privacy risk of data release mechanisms in practice. While an active attack literature exists to evaluate machine learning models or synthetic data release, we currently lack comparable methods for fixed aggregate statistics, in particular when only a limited number of statistics are released. We here propose an inference attack framework against fixed aggregate statistics and an attribute inference attack called DeSIA. We instantiate DeSIA against the U.S. Census PPMF dataset and show it to strongly outperform reconstruction-based attacks. In particular, we show DeSIA to be highly effective at identifying vulnerable users, achieving a true positive rate of 0.14 at a false positive rate of $10^{-3}$. We then show DeSIA to perform well against users whose attributes cannot be verified and when varying the number of aggregate statistics and level of noise addition. We also perform an extensive ablation study of DeSIA and show how DeSIA can be successfully adapted to the membership inference task. Overall, our results show that aggregation alone is not sufficient to protect privacy, even when a relatively small number of aggregates are being released, and emphasize the need for formal privacy mechanisms and testing before aggregate statistics are released.","authors":["Yifeng Mao","Bozhidar Stevanoski","Yves-Alexandre de Montjoye"],"url":"https://arxiv.org/abs/2504.18497"}
{"created":"2025-04-28","title":"Boxi: Design Decisions in the Context of Algorithmic Performance for Robotics","abstract":"Achieving robust autonomy in mobile robots operating in complex and unstructured environments requires a multimodal sensor suite capable of capturing diverse and complementary information. However, designing such a sensor suite involves multiple critical design decisions, such as sensor selection, component placement, thermal and power limitations, compute requirements, networking, synchronization, and calibration. While the importance of these key aspects is widely recognized, they are often overlooked in academia or retained as proprietary knowledge within large corporations. To improve this situation, we present Boxi, a tightly integrated sensor payload that enables robust autonomy of robots in the wild. This paper discusses the impact of payload design decisions made to optimize algorithmic performance for downstream tasks, specifically focusing on state estimation and mapping. Boxi is equipped with a variety of sensors: two LiDARs, 10 RGB cameras including high-dynamic range, global shutter, and rolling shutter models, an RGB-D camera, 7 inertial measurement units (IMUs) of varying precision, and a dual antenna RTK GNSS system. Our analysis shows that time synchronization, calibration, and sensor modality have a crucial impact on the state estimation performance. We frame this analysis in the context of cost considerations and environment-specific challenges. We also present a mobile sensor suite `cookbook` to serve as a comprehensive guideline, highlighting generalizable key design considerations and lessons learned during the development of Boxi. Finally, we demonstrate the versatility of Boxi being used in a variety of applications in real-world scenarios, contributing to robust autonomy. More details and code: https://github.com/leggedrobotics/grand_tour_box","authors":["Jonas Frey","Turcan Tuna","Lanke Frank Tarimo Fu","Cedric Weibel","Katharine Patterson","Benjamin Krummenacher","Matthias M\\\"uller","Julian Nubert","Maurice Fallon","Cesar Cadena","Marco Hutter"],"url":"https://arxiv.org/abs/2504.18500"}
{"created":"2025-04-28","title":"Online Distributed Queue Length Estimation","abstract":"Queue length monitoring is a commonly arising problem in numerous applications such as queue management systems, scheduling, and traffic monitoring. Motivated by such applications, we formulate a queue monitoring problem, where there is a FIFO queue with arbitrary arrivals and departures, and a server needs to monitor the length of a queue by using decentralized pings from packets in the queue. Packets can send pings informing the server about the number of packets ahead of them in the queue. Via novel online policies and lower bounds, we tightly characterize the trade-off between the number of pings sent and the accuracy of the server's real time estimates. Our work studies the trade-off under various arrival and departure processes, including constant-rate, Poisson, and adversarial processes.","authors":["Aditya Bhaskara","Sreenivas Gollapudi","Sungjin Im","Kostas Kollias","Kamesh Munagala"],"url":"https://arxiv.org/abs/2504.18503"}
{"created":"2025-04-28","title":"Information Freshness in Dynamic Gossip Networks","abstract":"We consider a source that shares updates with a network of $n$ gossiping nodes. The network's topology switches between two arbitrary topologies, with switching governed by a two-state continuous time Markov chain (CTMC) process. Information freshness is well-understood for static networks. This work evaluates the impact of time-varying connections on information freshness. In order to quantify the freshness of information, we use the version age of information metric. If the two networks have static long-term average version ages of $f_1(n)$ and $f_2(n)$ with $f_1(n) \\ll f_2(n)$, then the version age of the varying-topologies network is related to $f_1(n)$, $f_2(n)$, and the transition rates in the CTMC. If the transition rates in the CTMC are faster than $f_1(n)$, the average version age of the varying-topologies network is $f_1(n)$. Further, we observe that the behavior of a vanishingly small fraction of nodes can severely impact the long-term average version age of a network in a negative way. This motivates the definition of a typical set of nodes in the network. We evaluate the impact of fast and slow CTMC transition rates on the typical set of nodes.","authors":["Arunabh Srivastava","Thomas Jacob Maranzatto","Sennur Ulukus"],"url":"https://arxiv.org/abs/2504.18504"}
{"created":"2025-04-28","title":"Action-Minimization Meets Generative Modeling: Efficient Transition Path Sampling with the Onsager-Machlup Functional","abstract":"Transition path sampling (TPS), which involves finding probable paths connecting two points on an energy landscape, remains a challenge due to the complexity of real-world atomistic systems. Current machine learning approaches use expensive, task-specific, and data-free training procedures, limiting their ability to benefit from recent advances in atomistic machine learning, such as high-quality datasets and large-scale pre-trained models. In this work, we address TPS by interpreting candidate paths as trajectories sampled from stochastic dynamics induced by the learned score function of pre-trained generative models, specifically denoising diffusion and flow matching. Under these dynamics, finding high-likelihood transition paths becomes equivalent to minimizing the Onsager-Machlup (OM) action functional. This enables us to repurpose pre-trained generative models for TPS in a zero-shot manner, in contrast with bespoke, task-specific TPS models trained in previous work. We demonstrate our approach on varied molecular systems, obtaining diverse, physically realistic transition pathways and generalizing beyond the pre-trained model's original training dataset. Our method can be easily incorporated into new generative models, making it practically relevant as models continue to scale and improve with increased data availability.","authors":["Sanjeev Raja","Martin \\v{S}\\'ipka","Michael Psenka","Tobias Kreiman","Michal Pavelka","Aditi S. Krishnapriyan"],"url":"https://arxiv.org/abs/2504.18506"}
{"created":"2025-04-28","title":"Eval3D: Interpretable and Fine-grained Evaluation for 3D Generation","abstract":"Despite the unprecedented progress in the field of 3D generation, current systems still often fail to produce high-quality 3D assets that are visually appealing and geometrically and semantically consistent across multiple viewpoints. To effectively assess the quality of the generated 3D data, there is a need for a reliable 3D evaluation tool. Unfortunately, existing 3D evaluation metrics often overlook the geometric quality of generated assets or merely rely on black-box multimodal large language models for coarse assessment. In this paper, we introduce Eval3D, a fine-grained, interpretable evaluation tool that can faithfully evaluate the quality of generated 3D assets based on various distinct yet complementary criteria. Our key observation is that many desired properties of 3D generation, such as semantic and geometric consistency, can be effectively captured by measuring the consistency among various foundation models and tools. We thus leverage a diverse set of models and tools as probes to evaluate the inconsistency of generated 3D assets across different aspects. Compared to prior work, Eval3D provides pixel-wise measurement, enables accurate 3D spatial feedback, and aligns more closely with human judgments. We comprehensively evaluate existing 3D generation models using Eval3D and highlight the limitations and challenges of current models.","authors":["Shivam Duggal","Yushi Hu","Oscar Michel","Aniruddha Kembhavi","William T. Freeman","Noah A. Smith","Ranjay Krishna","Antonio Torralba","Ali Farhadi","Wei-Chiu Ma"],"url":"https://arxiv.org/abs/2504.18509"}
{"created":"2025-04-28","title":"Examining the Impact of Optical Aberrations to Image Classification and Object Detection Models","abstract":"Deep neural networks (DNNs) have proven to be successful in various computer vision applications such that models even infer in safety-critical situations. Therefore, vision models have to behave in a robust way to disturbances such as noise or blur. While seminal benchmarks exist to evaluate model robustness to diverse corruptions, blur is often approximated in an overly simplistic way to model defocus, while ignoring the different blur kernel shapes that result from optical systems. To study model robustness against realistic optical blur effects, this paper proposes two datasets of blur corruptions, which we denote OpticsBench and LensCorruptions. OpticsBench examines primary aberrations such as coma, defocus, and astigmatism, i.e. aberrations that can be represented by varying a single parameter of Zernike polynomials. To go beyond the principled but synthetic setting of primary aberrations, LensCorruptions samples linear combinations in the vector space spanned by Zernike polynomials, corresponding to 100 real lenses. Evaluations for image classification and object detection on ImageNet and MSCOCO show that for a variety of different pre-trained models, the performance on OpticsBench and LensCorruptions varies significantly, indicating the need to consider realistic image corruptions to evaluate a model's robustness against blur.","authors":["Patrick M\\\"uller","Alexander Braun","Margret Keuper"],"url":"https://arxiv.org/abs/2504.18510"}
{"created":"2025-04-28","title":"Co-Change Graph Entropy: A New Process Metric for Defect Prediction","abstract":"Process metrics, valued for their language independence and ease of collection, have been shown to outperform product metrics in defect prediction. Among these, change entropy (Hassan, 2009) is widely used at the file level and has proven highly effective. Additionally, past research suggests that co-change patterns provide valuable insights into software quality. Building on these findings, we introduce Co-Change Graph Entropy, a novel metric that models co-changes as a graph to quantify co-change scattering. Experiments on eight Apache projects reveal a significant correlation between co-change entropy and defect counts at the file level, with a Pearson correlation coefficient of up to 0.54. In filelevel defect classification, replacing change entropy with co-change entropy improves AUROC in 72.5% of cases and MCC in 62.5% across 40 experimental settings (five machine learning classifiers and eight projects), though these improvements are not statistically significant. However, when co-change entropy is combined with change entropy, AUROC improves in 82.5% of cases and MCC in 65%, with statistically significant gains confirmed via the Friedman test followed by the post-hoc Nemenyi test. These results indicate that co-change entropy complements change entropy, significantly enhancing defect classification performance and underscoring its practical importance in defect prediction.","authors":["Ethari Hrishikesh","Amit Kumar","Meher Bhardwaj","Sonali Agarwal"],"url":"https://arxiv.org/abs/2504.18511"}
{"created":"2025-04-28","title":"PODNO: Proper Orthogonal Decomposition Neural Operators","abstract":"In this paper, we introduce Proper Orthogonal Decomposition Neural Operators (PODNO) for solving partial differential equations (PDEs) dominated by high-frequency components. Building on the structure of Fourier Neural Operators (FNO), PODNO replaces the Fourier transform with (inverse) orthonormal transforms derived from the Proper Orthogonal Decomposition (POD) method to construct the integral kernel. Due to the optimality of POD basis, the PODNO has potential to outperform FNO in both accuracy and computational efficiency for high-frequency problems. From analysis point of view, we established the universality of a generalization of PODNO, termed as Generalized Spectral Operator (GSO). In addition, we evaluate PODNO's performance numerically on dispersive equations such as the Nonlinear Schrodinger (NLS) equation and the Kadomtsev-Petviashvili (KP) equation.","authors":["Zilan Cheng","Zhongjian Wang","Li-Lian Wang","Mejdi Azaiez"],"url":"https://arxiv.org/abs/2504.18513"}
{"created":"2025-04-28","title":"Intelligent Attacks and Defense Methods in Federated Learning-enabled Energy-Efficient Wireless Networks","abstract":"Federated learning (FL) is a promising technique for learning-based functions in wireless networks, thanks to its distributed implementation capability. On the other hand, distributed learning may increase the risk of exposure to malicious attacks where attacks on a local model may spread to other models by parameter exchange. Meanwhile, such attacks can be hard to detect due to the dynamic wireless environment, especially considering local models can be heterogeneous with non-independent and identically distributed (non-IID) data. Therefore, it is critical to evaluate the effect of malicious attacks and develop advanced defense techniques for FL-enabled wireless networks. In this work, we introduce a federated deep reinforcement learning-based cell sleep control scenario that enhances the energy efficiency of the network. We propose multiple intelligent attacks targeting the learning-based approach and we propose defense methods to mitigate such attacks. In particular, we have designed two attack models, generative adversarial network (GAN)-enhanced model poisoning attack and regularization-based model poisoning attack. As a counteraction, we have proposed two defense schemes, autoencoder-based defense, and knowledge distillation (KD)-enabled defense. The autoencoder-based defense method leverages an autoencoder to identify the malicious participants and only aggregate the parameters of benign local models during the global aggregation, while KD-based defense protects the model from attacks by controlling the knowledge transferred between the global model and local models.","authors":["Han Zhang","Hao Zhou","Medhat Elsayed","Majid Bavand","Raimundas Gaigalas","Yigit Ozcan","Melike Erol-Kantarci"],"url":"https://arxiv.org/abs/2504.18519"}
{"created":"2025-04-28","title":"E-VLC: A Real-World Dataset for Event-based Visible Light Communication And Localization","abstract":"Optical communication using modulated LEDs (e.g., visible light communication) is an emerging application for event cameras, thanks to their high spatio-temporal resolutions. Event cameras can be used simply to decode the LED signals and also to localize the camera relative to the LED marker positions. However, there is no public dataset to benchmark the decoding and localization in various real-world settings. We present, to the best of our knowledge, the first public dataset that consists of an event camera, a frame camera, and ground-truth poses that are precisely synchronized with hardware triggers. It provides various camera motions with various sensitivities in different scene brightness settings, both indoor and outdoor. Furthermore, we propose a novel method of localization that leverages the Contrast Maximization framework for motion estimation and compensation. The detailed analysis and experimental results demonstrate the advantages of LED-based localization with events over the conventional AR-marker--based one with frames, as well as the efficacy of the proposed method in localization. We hope that the proposed dataset serves as a future benchmark for both motion-related classical computer vision tasks and LED marker decoding tasks simultaneously, paving the way to broadening applications of event cameras on mobile devices. https://woven-visionai.github.io/evlc-dataset","authors":["Shintaro Shiba","Quan Kong","Norimasa Kobori"],"url":"https://arxiv.org/abs/2504.18521"}
{"created":"2025-04-28","title":"Augmenting Perceptual Super-Resolution via Image Quality Predictors","abstract":"Super-resolution (SR), a classical inverse problem in computer vision, is inherently ill-posed, inducing a distribution of plausible solutions for every input. However, the desired result is not simply the expectation of this distribution, which is the blurry image obtained by minimizing pixelwise error, but rather the sample with the highest image quality. A variety of techniques, from perceptual metrics to adversarial losses, are employed to this end. In this work, we explore an alternative: utilizing powerful non-reference image quality assessment (NR-IQA) models in the SR context. We begin with a comprehensive analysis of NR-IQA metrics on human-derived SR data, identifying both the accuracy (human alignment) and complementarity of different metrics. Then, we explore two methods of applying NR-IQA models to SR learning: (i) altering data sampling, by building on an existing multi-ground-truth SR framework, and (ii) directly optimizing a differentiable quality score. Our results demonstrate a more human-centric perception-distortion tradeoff, focusing less on non-perceptual pixel-wise distortion, instead improving the balance between perceptual fidelity and human-tuned NR-IQA measures.","authors":["Fengjia Zhang","Samrudhdhi B. Rangrej","Tristan Aumentado-Armstrong","Afsaneh Fazly","Alex Levinshtein"],"url":"https://arxiv.org/abs/2504.18524"}
{"created":"2025-04-28","title":"Robust semi-implicit multilevel SDC methods for conservation laws","abstract":"Semi-implicit multilevel spectral deferred correction (SI-MLSDC) methods provide a promising approach for high-order time integration for nonlinear evolution equations including conservation laws. However, existing methods lack robustness and often do not achieve the expected advantage over single-level SDC. This work adopts the novel SI time integrators from [44] for enhanced stability and extends the single-level SI-SDC method with a multilevel approach to increase computational efficiency. The favourable properties of the resulting SI-MLSDC method are shown by linear temporal stability analysis for a convection-diffusion problem. The robustness and efficiency of the fully discrete method involving a high-order discontinuous Galerkin SEM discretization are demonstrated through numerical experiments for the convection-diffusion, Burgers, Euler and Navier-Stokes equations. The method is shown to yield substantial reductions in fine-grid iterations compared to single-level SI-SDC across a broad range of test cases. Finally, current limitations of the SI-MLSDC framework are identified and discussed, providing guidance for future improvements.","authors":["Erik Pfister","J\\\"org Stiller"],"url":"https://arxiv.org/abs/2504.18526"}
{"created":"2025-04-28","title":"Practical Type-Based Taint Checking and Inference (Extended Version)","abstract":"Many important security properties can be formulated in terms of flows of tainted data, and improved taint analysis tools to prevent such flows are of critical need. Most existing taint analyses use whole-program static analysis, leading to scalability challenges. Type-based checking is a promising alternative, as it enables modular and incremental checking for fast performance. However, type-based approaches have not been widely adopted in practice, due to challenges with false positives and annotating existing codebases. In this paper, we present a new approach to type-based checking of taint properties that addresses these challenges, based on two key techniques. First, we present a new type-based tainting checker with significantly reduced false positives, via more practical handling of third-party libraries and other language constructs. Second, we present a novel technique to automatically infer tainting type qualifiers for existing code. Our technique supports inference of generic type argument annotations, crucial for tainting properties. We implemented our techniques in a tool TaintTyper and evaluated it on real-world benchmarks. TaintTyper exceeds the recall of a state-of-the-art whole-program taint analyzer, with comparable precision, and 2.93X-22.9X faster checking time. Further, TaintTyper infers annotations comparable to those written by hand, suitable for insertion into source code. TaintTyper is a promising new approach to efficient and practical taint checking.","authors":["Nima Karimipour","Kanak Das","Manu Sridharan","Behnaz Hassanshahi"],"url":"https://arxiv.org/abs/2504.18529"}
{"created":"2025-04-28","title":"Scaling Laws For Scalable Oversight","abstract":"Scalable oversight, the process by which weaker AI systems supervise stronger ones, has been proposed as a key strategy to control future superintelligent systems. However, it is still unclear how scalable oversight itself scales. To address this gap, we propose a framework that quantifies the probability of successful oversight as a function of the capabilities of the overseer and the system being overseen. Specifically, our framework models oversight as a game between capability-mismatched players; the players have oversight-specific and deception-specific Elo scores that are a piecewise-linear function of their general intelligence, with two plateaus corresponding to task incompetence and task saturation. We validate our framework with a modified version of the game Nim and then apply it to four oversight games: \"Mafia\", \"Debate\", \"Backdoor Code\" and \"Wargames\". For each game, we find scaling laws that approximate how domain performance depends on general AI system capability (using Chatbot Arena Elo as a proxy for general capability). We then build on our findings in a theoretical study of Nested Scalable Oversight (NSO), a process in which trusted models oversee untrusted stronger models, which then become the trusted models in the next step. We identify conditions under which NSO succeeds and derive numerically (and in some cases analytically) the optimal number of oversight levels to maximize the probability of oversight success. In our numerical examples, the NSO success rate is below 52% when overseeing systems that are 400 Elo points stronger than the baseline overseer, and it declines further for overseeing even stronger systems.","authors":["Joshua Engels","David D. Baek","Subhash Kantamneni","Max Tegmark"],"url":"https://arxiv.org/abs/2504.18530"}
{"created":"2025-04-28","title":"TRACE Back from the Future: A Probabilistic Reasoning Approach to Controllable Language Generation","abstract":"As large language models (LMs) advance, there is an increasing need to control their outputs to align with human values (e.g., detoxification) or desired attributes (e.g., personalization, topic). However, autoregressive models focus on next-token predictions and struggle with global properties that require looking ahead. Existing solutions either tune or post-train LMs for each new attribute - expensive and inflexible - or approximate the Expected Attribute Probability (EAP) of future sequences by sampling or training, which is slow and unreliable for rare attributes. We introduce TRACE (Tractable Probabilistic Reasoning for Adaptable Controllable gEneration), a novel framework that efficiently computes EAP and adapts to new attributes through tractable probabilistic reasoning and lightweight control. TRACE distills a Hidden Markov Model (HMM) from an LM and pairs it with a small classifier to estimate attribute probabilities, enabling exact EAP computation over the HMM's predicted futures. This EAP is then used to reweigh the LM's next-token probabilities for globally compliant continuations. Empirically, TRACE achieves state-of-the-art results in detoxification with only 10% decoding overhead, adapts to 76 low-resource personalized LLMs within seconds, and seamlessly extends to composite attributes.","authors":["Gwen Yidou Weng","Benjie Wang","Guy Van den Broeck"],"url":"https://arxiv.org/abs/2504.18535"}
{"created":"2025-04-28","title":"Adapting Probabilistic Risk Assessment for AI","abstract":"Modern general-purpose artificial intelligence (AI) systems present an urgent risk management challenge, as their rapidly evolving capabilities and potential for catastrophic harm outpace our ability to reliably assess their risks. Current methods often rely on selective testing and undocumented assumptions about risk priorities, frequently failing to make a serious attempt at assessing the set of pathways through which Al systems pose direct or indirect risks to society and the biosphere. This paper introduces the probabilistic risk assessment (PRA) for AI framework, adapting established PRA techniques from high-reliability industries (e.g., nuclear power, aerospace) for the new challenges of advanced AI. The framework guides assessors in identifying potential risks, estimating likelihood and severity, and explicitly documenting evidence, underlying assumptions, and analyses at appropriate granularities. The framework's implementation tool synthesizes the results into a risk report card with aggregated risk estimates from all assessed risks. This systematic approach integrates three advances: (1) Aspect-oriented hazard analysis provides systematic hazard coverage guided by a first-principles taxonomy of AI system aspects (e.g. capabilities, domain knowledge, affordances); (2) Risk pathway modeling analyzes causal chains from system aspects to societal impacts using bidirectional analysis and incorporating prospective techniques; and (3) Uncertainty management employs scenario decomposition, reference scales, and explicit tracing protocols to structure credible projections with novelty or limited data. Additionally, the framework harmonizes diverse assessment methods by integrating evidence into comparable, quantified absolute risk estimates for critical decisions. We have implemented this as a workbook tool for AI developers, evaluators, and regulators, available on the project website.","authors":["Anna Katariina Wisakanto","Joe Rogero","Avyay M. Casheekar","Richard Mallah"],"url":"https://arxiv.org/abs/2504.18536"}
{"created":"2025-04-28","title":"Generalization Capability for Imitation Learning","abstract":"Imitation learning holds the promise of equipping robots with versatile skills by learning from expert demonstrations. However, policies trained on finite datasets often struggle to generalize beyond the training distribution. In this work, we present a unified perspective on the generalization capability of imitation learning, grounded in both information theorey and data distribution property. We first show that the generalization gap can be upper bounded by (i) the conditional information bottleneck on intermediate representations and (ii) the mutual information between the model parameters and the training dataset. This characterization provides theoretical guidance for designing effective training strategies in imitation learning, particularly in determining whether to freeze, fine-tune, or train large pretrained encoders (e.g., vision-language models or vision foundation models) from scratch to achieve better generalization. Furthermore, we demonstrate that high conditional entropy from input to output induces a flatter likelihood landscape, thereby reducing the upper bound on the generalization gap. In addition, it shortens the stochastic gradient descent (SGD) escape time from sharp local minima, which may increase the likelihood of reaching global optima under fixed optimization budgets. These insights explain why imitation learning often exhibits limited generalization and underscore the importance of not only scaling the diversity of input data but also enriching the variability of output labels conditioned on the same input.","authors":["Yixiao Wang"],"url":"https://arxiv.org/abs/2504.18538"}
{"created":"2025-04-28","title":"Persistence of Backdoor-based Watermarks for Neural Networks: A Comprehensive Evaluation","abstract":"Deep Neural Networks (DNNs) have gained considerable traction in recent years due to the unparalleled results they gathered. However, the cost behind training such sophisticated models is resource intensive, resulting in many to consider DNNs to be intellectual property (IP) to model owners. In this era of cloud computing, high-performance DNNs are often deployed all over the internet so that people can access them publicly. As such, DNN watermarking schemes, especially backdoor-based watermarks, have been actively developed in recent years to preserve proprietary rights. Nonetheless, there lies much uncertainty on the robustness of existing backdoor watermark schemes, towards both adversarial attacks and unintended means such as fine-tuning neural network models. One reason for this is that no complete guarantee of robustness can be assured in the context of backdoor-based watermark. In this paper, we extensively evaluate the persistence of recent backdoor-based watermarks within neural networks in the scenario of fine-tuning, we propose/develop a novel data-driven idea to restore watermark after fine-tuning without exposing the trigger set. Our empirical results show that by solely introducing training data after fine-tuning, the watermark can be restored if model parameters do not shift dramatically during fine-tuning. Depending on the types of trigger samples used, trigger accuracy can be reinstated to up to 100%. Our study further explores how the restoration process works using loss landscape visualization, as well as the idea of introducing training data in fine-tuning stage to alleviate watermark vanishing.","authors":["Anh Tu Ngo","Chuan Song Heng","Nandish Chattopadhyay","Anupam Chattopadhyay"],"url":"https://arxiv.org/abs/2501.02704"}
{"created":"2025-04-28","title":"A Deep Bayesian Convolutional Spiking Neural Network-based CAD system with Uncertainty Quantification for Medical Images Classification","abstract":"The Computer_Aided Diagnosis (CAD) systems facilitate accurate diagnosis of diseases. The development of CADs by leveraging third generation neural network, namely, Spiking Neural Network (SNN), is essential to utilize of the benefits of SNNs, such as their event_driven processing, parallelism, low power consumption, and the ability to process sparse temporal_spatial information. However, Deep SNN as a deep learning model faces challenges with unreliability. To deal with unreliability challenges due to inability to quantify the uncertainty of the predictions, we proposed a deep Bayesian Convolutional Spiking Neural Network based_CADs with uncertainty_aware module. In this study, the Monte Carlo Dropout method as Bayesian approximation is used as an uncertainty quantification method. This method was applied to several medical image classification tasks. Our experimental results demonstrate that our proposed model is accurate and reliable and will be a proper alternative to conventional deep learning for medical image classification.","authors":["Mohaddeseh Chegini","Ali Mahloojifar"],"url":"https://arxiv.org/abs/2504.17819"}
{"created":"2025-04-28","title":"Learning Enhanced Ensemble Filters","abstract":"The filtering distribution in hidden Markov models evolves according to the law of a mean-field model in state--observation space. The ensemble Kalman filter (EnKF) approximates this mean-field model with an ensemble of interacting particles, employing a Gaussian ansatz for the joint distribution of the state and observation at each observation time. These methods are robust, but the Gaussian ansatz limits accuracy. This shortcoming is addressed by approximating the mean-field evolution using a novel form of neural operator taking probability distributions as input: a Measure Neural Mapping (MNM). A MNM is used to design a novel approach to filtering, the MNM-enhanced ensemble filter (MNMEF), which is defined in both the mean-fieldlimit and for interacting ensemble particle approximations. The ensemble approach uses empirical measures as input to the MNM and is implemented using the set transformer, which is invariant to ensemble permutation and allows for different ensemble sizes. The derivation of methods from a mean-field formulation allows a single parameterization of the algorithm to be deployed at different ensemble sizes. In practice fine-tuning of a small number of parameters, for specific ensemble sizes, further enhances the accuracy of the scheme. The promise of the approach is demonstrated by its superior root-mean-square-error performance relative to leading methods in filtering the Lorenz 96 and Kuramoto-Sivashinsky models.","authors":["Eviatar Bach","Ricardo Baptista","Edoardo Calvello","Bohan Chen","Andrew Stuart"],"url":"https://arxiv.org/abs/2504.17836"}
{"created":"2025-04-28","title":"SOFARI-R: High-Dimensional Manifold-Based Inference for Latent Responses","abstract":"Data reduction with uncertainty quantification plays a key role in various multi-task learning applications, where large numbers of responses and features are present. To this end, a general framework of high-dimensional manifold-based SOFAR inference (SOFARI) was introduced recently in Zheng, Zhou, Fan and Lv (2024) for interpretable multi-task learning inference focusing on the left factor vectors and singular values exploiting the latent singular value decomposition (SVD) structure. Yet, designing a valid inference procedure on the latent right factor vectors is not straightforward from that of the left ones and can be even more challenging due to asymmetry of left and right singular vectors in the response matrix. To tackle these issues, in this paper we suggest a new method of high-dimensional manifold-based SOFAR inference for latent responses (SOFARI-R), where two variants of SOFARI-R are introduced. The first variant deals with strongly orthogonal factors by coupling left singular vectors with the design matrix and then appropriately rescaling them to generate new Stiefel manifolds. The second variant handles the more general weakly orthogonal factors by employing the hard-thresholded SOFARI estimates and delicately incorporating approximation errors into the distribution. Both variants produce bias-corrected estimators for the latent right factor vectors that enjoy asymptotically normal distributions with justified asymptotic variance estimates. We demonstrate the effectiveness of the newly suggested method using extensive simulation studies and an economic application.","authors":["Zemin Zheng","Xin Zhou","Jinchi Lv"],"url":"https://arxiv.org/abs/2504.17874"}
{"created":"2025-04-28","title":"Quaternion Domain Super MDS for 3D Localization","abstract":"We propose a novel low-complexity three-dimensional (3D) localization algorithm for wireless sensor networks, termed quaternion-domain super multidimensional scaling (QD-SMDS). This algorithm reformulates the conventional SMDS, which was originally developed in the real domain, into the quaternion domain. By representing 3D coordinates as quaternions, the method enables the construction of a rank-1 Gram edge kernel (GEK) matrix that integrates both relative distance and angular (phase) information between nodes, maximizing the noise reduction effect achieved through low-rank truncation via singular value decomposition (SVD). The simulation results indicate that the proposed method demonstrates a notable enhancement in localization accuracy relative to the conventional SMDS algorithm, particularly in scenarios characterized by substantial measurement errors.","authors":["Keigo Masuoka","Takumi Takahashi","Giuseppe Thadeu Freitas de Abreu","Hideki Ochiai"],"url":"https://arxiv.org/abs/2504.17890"}
{"created":"2025-04-28","title":"Material Identification Via RFID For Smart Shopping","abstract":"Cashierless stores rely on computer vision and RFID tags to associate shoppers with items, but concealed items placed in backpacks, pockets, or bags create challenges for theft prevention. We introduce a system that turns existing RFID tagged items into material sensors by exploiting how different containers attenuate and scatter RF signals. Using RSSI and phase angle, we trained a neural network to classify seven common containers. In a simulated retail environment, the model achieves 89% accuracy with one second samples and 74% accuracy from single reads. Incorporating distance measurements, our system achieves 82% accuracy across 0.3-2m tag to reader separations. When deployed at aisle or doorway choke points, the system can flag suspicious events in real time, prompting camera screening or staff intervention. By combining material identification with computer vision tracking, our system provides proactive loss prevention for cashierless retail while utilizing existing infrastructure.","authors":["David Wang","Derek Goh","Jiale Zhang"],"url":"https://arxiv.org/abs/2504.17898"}
{"created":"2025-04-28","title":"Model Error Covariance Estimation for Weak Constraint Data Assimilation","abstract":"State estimates from weak constraint 4D-Var data assimilation can vary significantly depending on the data and model error covariances. As a result, the accuracy of these estimates heavily depends on the correct specification of both model and observational data error covariances. In this work, we assume that the data error is known and and focus on estimating the model error covariance by framing weak constraint 4D-Var as a regularized inverse problem, where the inverse model error covariance serves as the regularization matrix. We consider both isotropic and non-isotropic forms of the model error covariance. Using the representer method, we reduce the 4D-Var problem from state space to data space, enabling the efficient application of regularization parameter selection techniques. The Representer method also provides an analytic expression for the optimal state estimate, allowing us to derive matrix expressions for the three regularization parameter selection methods i.e. the L-curve, generalized cross-validation (GCV), and the Chi-square method. We validate our approach by assimilating simulated data into a 1D transport equation modeling wildfire smoke transport under various observational noise and forward model perturbations. In these experiments the goal is to identify the model error covariances that accurately capture the influence of observational data versus model predictions on assimilated state estimates. The regularization parameter selection methods successfully estimate hyperparameters for both isotropic and non-isotropic model error covariances, that reflect whether the first guess model predictions are more or less reliable than the observational data. The results further indicate that isotropic variances are sufficient when the first guess is more accurate than the data whereas non-isotropic covariances are preferred when the observational data is more reliable.","authors":["Sandra R. Babyale","Jodi Mead","Donna Calhoun","Patricia O. Azike"],"url":"https://arxiv.org/abs/2504.17900"}
{"created":"2025-04-28","title":"A computational model of infant sensorimotor exploration in the mobile paradigm","abstract":"We present a computational model of the mechanisms that may determine infants' behavior in the \"mobile paradigm\". This paradigm has been used in developmental psychology to explore how infants learn the sensory effects of their actions. In this paradigm, a mobile (an articulated and movable object hanging above an infant's crib) is connected to one of the infant's limbs, prompting the infant to preferentially move that \"connected\" limb. This ability to detect a \"sensorimotor contingency\" is considered to be a foundational cognitive ability in development. To understand how infants learn sensorimotor contingencies, we built a model that attempts to replicate infant behavior. Our model incorporates a neural network, action-outcome prediction, exploration, motor noise, preferred activity level, and biologically-inspired motor control. We find that simulations with our model replicate the classic findings in the literature showing preferential movement of the connected limb. An interesting observation is that the model sometimes exhibits a burst of movement after the mobile is disconnected, casting light on a similar occasional finding in infants. In addition to these general findings, the simulations also replicate data from two recent more detailed studies using a connection with the mobile that was either gradual or all-or-none. A series of ablation studies further shows that the inclusion of mechanisms of action-outcome prediction, exploration, motor noise, and biologically-inspired motor control was essential for the model to correctly replicate infant behavior. This suggests that these components are also involved in infants' sensorimotor learning.","authors":["Josua Spisak","Sergiu Tcaci Popescu","Stefan Wermter","Matej Hoffmann","J. Kevin O'Regan"],"url":"https://arxiv.org/abs/2504.17939"}
{"created":"2025-04-28","title":"Predicting Dairy Calf Body Weight from Depth Images Using Deep Learning (YOLOv8) and Threshold Segmentation with Cross-Validation and Longitudinal Analysis","abstract":"Monitoring calf body weight (BW) before weaning is essential for assessing growth, feed efficiency, health, and weaning readiness. However, labor, time, and facility constraints limit BW collection. Additionally, Holstein calf coat patterns complicate image-based BW estimation, and few studies have explored non-contact measurements taken at early time points for predicting later BW. The objectives of this study were to (1) develop deep learning-based segmentation models for extracting calf body metrics, (2) compare deep learning segmentation with threshold-based methods, and (3) evaluate BW prediction using single-time-point cross-validation with linear regression (LR) and extreme gradient boosting (XGBoost) and multiple-time-point cross-validation with LR, XGBoost, and a linear mixed model (LMM). Depth images from Holstein (n = 63) and Jersey (n = 5) pre-weaning calves were collected, with 20 Holstein calves being weighed manually. Results showed that You Only Look Once version 8 (YOLOv8) deep learning segmentation (intersection over union = 0.98) outperformed threshold-based methods (0.89). In single-time-point cross-validation, XGBoost achieved the best BW prediction (R^2 = 0.91, mean absolute percentage error (MAPE) = 4.37%), while LMM provided the most accurate longitudinal BW prediction (R^2 = 0.99, MAPE = 2.39%). These findings highlight the potential of deep learning for automated BW prediction, enhancing farm management.","authors":["Mingsi Liao","Gota Morota","Ye Bi","Rebecca R. Cockrum"],"url":"https://arxiv.org/abs/2504.17943"}
{"created":"2025-04-28","title":"Spectral Bias Correction in PINNs for Myocardial Image Registration of Pathological Data","abstract":"Accurate myocardial image registration is essential for cardiac strain analysis and disease diagnosis. However, spectral bias in neural networks impedes modeling high-frequency deformations, producing inaccurate, biomechanically implausible results, particularly in pathological data. This paper addresses spectral bias in physics-informed neural networks (PINNs) by integrating Fourier Feature mappings and introducing modulation strategies into a PINN framework. Experiments on two distinct datasets demonstrate that the proposed methods enhance the PINN's ability to capture complex, high-frequency deformations in cardiomyopathies, achieving superior registration accuracy while maintaining biomechanical plausibility - thus providing a foundation for scalable cardiac image registration and generalization across multiple patients and pathologies.","authors":["Bastien C. Baluyot","Marta Varela","Chen Qin"],"url":"https://arxiv.org/abs/2504.17945"}
{"created":"2025-04-28","title":"Efficient Tree Generation for Globally Optimal Decisions under Probabilistic Outcomes","abstract":"Many real-world problems require making sequences of decisions where the outcomes of each decision are probabilistic and uncertain, and the availability of different actions is constrained by the outcomes of previous actions. There is a need to generate policies that are adaptive to uncertainty, globally optimal, and yet scalable as the state space grows. In this paper, we propose the generation of optimal decision trees, which dictate which actions should be implemented in different outcome scenarios, while maximizing the expected reward of the strategy. Using a combination of dynamic programming and mixed-integer linear optimization, the proposed methods scale to problems with large but finite state spaces, using problem-specific information to prune away large subsets of the state space that do not yield progress towards rewards. We demonstrate that the presented approach is able to find the globally optimal decision tree in linear time with respect to the number states explored.","authors":["Berk Ozturk","She'ifa Punla-Green","Les Servi"],"url":"https://arxiv.org/abs/2504.17983"}
{"created":"2025-04-28","title":"Assessing the Utility of Audio Foundation Models for Heart and Respiratory Sound Analysis","abstract":"Pre-trained deep learning models, known as foundation models, have become essential building blocks in machine learning domains such as natural language processing and image domains. This trend has extended to respiratory and heart sound models, which have demonstrated effectiveness as off-the-shelf feature extractors. However, their evaluation benchmarking has been limited, resulting in incompatibility with state-of-the-art (SOTA) performance, thus hindering proof of their effectiveness. This study investigates the practical effectiveness of off-the-shelf audio foundation models by comparing their performance across four respiratory and heart sound tasks with SOTA fine-tuning results. Experiments show that models struggled on two tasks with noisy data but achieved SOTA performance on the other tasks with clean data. Moreover, general-purpose audio models outperformed a respiratory sound model, highlighting their broader applicability. With gained insights and the released code, we contribute to future research on developing and leveraging foundation models for respiratory and heart sounds.","authors":["Daisuke Niizumi","Daiki Takeuchi","Masahiro Yasuda","Binh Thien Nguyen","Yasunori Ohishi","Noboru Harada"],"url":"https://arxiv.org/abs/2504.18004"}
{"created":"2025-04-28","title":"Non-identifiability distinguishes Neural Networks among Parametric Models","abstract":"One of the enduring problems surrounding neural networks is to identify the factors that differentiate them from traditional statistical models. We prove a pair of results which distinguish feedforward neural networks among parametric models at the population level, for regression tasks. Firstly, we prove that for any pair of random variables $(X,Y)$, neural networks always learn a nontrivial relationship between $X$ and $Y$, if one exists. Secondly, we prove that for reasonable smooth parametric models, under local and global identifiability conditions, there exists a nontrivial $(X,Y)$ pair for which the parametric model learns the constant predictor $\\mathbb{E}[Y]$. Together, our results suggest that a lack of identifiability distinguishes neural networks among the class of smooth parametric models.","authors":["Sourav Chatterjee","Timothy Sudijono"],"url":"https://arxiv.org/abs/2504.18017"}
{"created":"2025-04-28","title":"Physics-Driven Neural Compensation For Electrical Impedance Tomography","abstract":"Electrical Impedance Tomography (EIT) provides a non-invasive, portable imaging modality with significant potential in medical and industrial applications. Despite its advantages, EIT encounters two primary challenges: the ill-posed nature of its inverse problem and the spatially variable, location-dependent sensitivity distribution. Traditional model-based methods mitigate ill-posedness through regularization but overlook sensitivity variability, while supervised deep learning approaches require extensive training data and lack generalization. Recent developments in neural fields have introduced implicit regularization techniques for image reconstruction, but these methods typically neglect the physical principles underlying EIT, thus limiting their effectiveness. In this study, we propose PhyNC (Physics-driven Neural Compensation), an unsupervised deep learning framework that incorporates the physical principles of EIT. PhyNC addresses both the ill-posed inverse problem and the sensitivity distribution by dynamically allocating neural representational capacity to regions with lower sensitivity, ensuring accurate and balanced conductivity reconstructions. Extensive evaluations on both simulated and experimental data demonstrate that PhyNC outperforms existing methods in terms of detail preservation and artifact resistance, particularly in low-sensitivity regions. Our approach enhances the robustness of EIT reconstructions and provides a flexible framework that can be adapted to other imaging modalities with similar challenges.","authors":["Chuyu Wang","Huiting Deng","Dong Liu"],"url":"https://arxiv.org/abs/2504.18067"}
{"created":"2025-04-28","title":"Efficient witnessing and testing of magic in mixed quantum states","abstract":"Nonstabilizerness or `magic' is a crucial resource for quantum computers which can be distilled from noisy quantum states. However, determining the magic of mixed quantum has been a notoriously difficult task. Here, we provide efficient witnesses of magic based on the stabilizer R\\'enyi entropy which robustly indicate the presence of magic and quantitatively estimate magic monotones. We also design efficient property testing algorithms to reliably distinguish states with high and low magic, assuming the entropy is bounded. We apply our methods to certify the number of noisy T-gates under a wide class of noise models. Additionally, using the IonQ quantum computer, we experimentally verify the magic of noisy random quantum circuits. Surprisingly, we find that magic is highly robust, persisting even under exponentially strong noise. Our witnesses can also be efficiently computed for matrix product states, revealing that subsystems of many-body quantum states can contain extensive magic despite entanglement. Finally, our work also has direct implications for cryptography and pseudomagic: To mimic high magic states with as little magic as possible, one requires an extensive amount of entropy. This implies that entropy is a necessary resource to hide magic from eavesdroppers. Our work uncovers powerful tools to verify and study the complexity of noisy quantum systems.","authors":["Tobias Haug","Poetri Sonya Tarabunga"],"url":"https://arxiv.org/abs/2504.18098"}
{"created":"2025-04-28","title":"Bayesian Quantum Orthogonal Neural Networks for Anomaly Detection","abstract":"Identification of defects or anomalies in 3D objects is a crucial task to ensure correct functionality. In this work, we combine Bayesian learning with recent developments in quantum and quantum-inspired machine learning, specifically orthogonal neural networks, to tackle this anomaly detection problem for an industrially relevant use case. Bayesian learning enables uncertainty quantification of predictions, while orthogonality in weight matrices enables smooth training. We develop orthogonal (quantum) versions of 3D convolutional neural networks and show that these models can successfully detect anomalies in 3D objects. To test the feasibility of incorporating quantum computers into a quantum-enhanced anomaly detection pipeline, we perform hardware experiments with our models on IBM's 127-qubit Brisbane device, testing the effect of noise and limited measurement shots.","authors":["Natansh Mathur","Brian Coyle","Nishant Jain","Snehal Raj","Akshat Tandon","Jasper Simon Krauser","Rainer Stoessel"],"url":"https://arxiv.org/abs/2504.18103"}
{"created":"2025-04-28","title":"Lecture Notes on Normalizing Flows for Lattice Quantum Field Theories","abstract":"Numerical simulations of quantum field theories on lattices serve as a fundamental tool for studying the non-perturbative regime of the theories, where analytic tools often fall short. Challenges arise when one takes the continuum limit or as the system approaches a critical point, especially in the presence of non-trivial topological structures in the theory. Rapid recent advances in machine learning provide a promising avenue for progress in this area. These lecture notes aim to give a brief account of lattice field theories, normalizing flows, and how the latter can be applied to study the former. The notes are based on the lectures given by the first author in various recent research schools.","authors":["Miranda C. N. Cheng","Niki Stratikopoulou"],"url":"https://arxiv.org/abs/2504.18126"}
{"created":"2025-04-28","title":"Kalman-Langevin dynamics : exponential convergence, particle approximation and numerical approximation","abstract":"Langevin dynamics has found a large number of applications in sampling, optimization and estimation. Preconditioning the gradient in the dynamics with the covariance - an idea that originated in literature related to solving estimation and inverse problems using Kalman techniques - results in a mean-field (McKean-Vlasov) SDE. We demonstrate exponential convergence of the time marginal law of the mean-field SDE to the Gibbs measure with non-Gaussian potentials. This extends previous results, obtained in the Gaussian setting, to a broader class of potential functions. We also establish uniform in time bounds on all moments and convergence in $p$-Wasserstein distance. Furthermore, we show convergence of a weak particle approximation, that avoids computing the square root of the empirical covariance matrix, to the mean-field limit. Finally, we prove that an explicit numerical scheme for approximating the particle dynamics converges, uniformly in number of particles, to its continuous-time limit, addressing non-global Lipschitzness in the measure.","authors":["Axel Ringh","Akash Sharma"],"url":"https://arxiv.org/abs/2504.18139"}
{"created":"2025-04-28","title":"DOSE : Drum One-Shot Extraction from Music Mixture","abstract":"Drum one-shot samples are crucial for music production, particularly in sound design and electronic music. This paper introduces Drum One-Shot Extraction, a task in which the goal is to extract drum one-shots that are present in the music mixture. To facilitate this, we propose the Random Mixture One-shot Dataset (RMOD), comprising large-scale, randomly arranged music mixtures paired with corresponding drum one-shot samples. Our proposed model, Drum One- Shot Extractor (DOSE), leverages neural audio codec language models for end-to-end extraction, bypassing traditional source separation steps. Additionally, we introduce a novel onset loss, designed to encourage accurate prediction of the initial transient of drum one-shots, which is essential for capturing timbral characteristics. We compare this approach against a source separation-based extraction method as a baseline. The results, evaluated using Frechet Audio Distance (FAD) and Multi-Scale Spectral loss (MSS), demonstrate that DOSE, enhanced with onset loss, outperforms the baseline, providing more accurate and higher-quality drum one-shots from music mixtures. The code, model checkpoint, and audio examples are available at https://github.com/HSUNEH/DOSE","authors":["Suntae Hwang","Seonghyeon Kang","Kyungsu Kim","Semin Ahn","Kyogu Lee"],"url":"https://arxiv.org/abs/2504.18157"}
{"created":"2025-04-28","title":"Learning Operators by Regularized Stochastic Gradient Descent with Operator-valued Kernels","abstract":"This paper investigates regularized stochastic gradient descent (SGD) algorithms for estimating nonlinear operators from a Polish space to a separable Hilbert space. We assume that the regression operator lies in a vector-valued reproducing kernel Hilbert space induced by an operator-valued kernel. Two significant settings are considered: an online setting with polynomially decaying step sizes and regularization parameters, and a finite-horizon setting with constant step sizes and regularization parameters. We introduce regularity conditions on the structure and smoothness of the target operator and the input random variables. Under these conditions, we provide a dimension-free convergence analysis for the prediction and estimation errors, deriving both expectation and high-probability error bounds. Our analysis demonstrates that these convergence rates are nearly optimal. Furthermore, we present a new technique for deriving bounds with high probability for general SGD schemes, which also ensures almost-sure convergence. Finally, we discuss potential extensions to more general operator-valued kernels and the encoder-decoder framework.","authors":["Jia-Qi Yang","Lei Shi"],"url":"https://arxiv.org/abs/2504.18184"}
{"created":"2025-04-28","title":"Quantum Lifting for Invertible Permutations and Ideal Ciphers","abstract":"In this work, we derive the first lifting theorems for establishing security in the quantum random permutation and ideal cipher models. These theorems relate the success probability of an arbitrary quantum adversary to that of a classical algorithm making only a small number of classical queries.","authors":["Alexandru Cojocaru","Minki Hhan","Qipeng Liu","Takashi Yamakawa","Aaram Yun"],"url":"https://arxiv.org/abs/2504.18188"}
{"created":"2025-04-28","title":"Post-Transfer Learning Statistical Inference in High-Dimensional Regression","abstract":"Transfer learning (TL) for high-dimensional regression (HDR) is an important problem in machine learning, particularly when dealing with limited sample size in the target task. However, there currently lacks a method to quantify the statistical significance of the relationship between features and the response in TL-HDR settings. In this paper, we introduce a novel statistical inference framework for assessing the reliability of feature selection in TL-HDR, called PTL-SI (Post-TL Statistical Inference). The core contribution of PTL-SI is its ability to provide valid $p$-values to features selected in TL-HDR, thereby rigorously controlling the false positive rate (FPR) at desired significance level $\\alpha$ (e.g., 0.05). Furthermore, we enhance statistical power by incorporating a strategic divide-and-conquer approach into our framework. We demonstrate the validity and effectiveness of the proposed PTL-SI through extensive experiments on both synthetic and real-world high-dimensional datasets, confirming its theoretical properties and utility in testing the reliability of feature selection in TL scenarios.","authors":["Nguyen Vu Khai Tam","Cao Huyen My","Vo Nguyen Le Duy"],"url":"https://arxiv.org/abs/2504.18212"}
{"created":"2025-04-28","title":"Towards a deep learning approach for classifying treatment response in glioblastomas","abstract":"Glioblastomas are the most aggressive type of glioma, having a 5-year survival rate of 6.9%. Treatment typically involves surgery, followed by radiotherapy and chemotherapy, and frequent magnetic resonance imaging (MRI) scans to monitor disease progression. To assess treatment response, radiologists use the Response Assessment in Neuro-Oncology (RANO) criteria to categorize the tumor into one of four labels based on imaging and clinical features: complete response, partial response, stable disease, and progressive disease. This assessment is very complex and time-consuming. Since deep learning (DL) has been widely used to tackle classification problems, this work aimed to implement the first DL pipeline for the classification of RANO criteria based on two consecutive MRI acquisitions. The models were trained and tested on the open dataset LUMIERE. Five approaches were tested: 1) subtraction of input images, 2) different combinations of modalities, 3) different model architectures, 4) different pretraining tasks, and 5) adding clinical data. The pipeline that achieved the best performance used a Densenet264 considering only T1-weighted, T2-weighted, and Fluid Attenuated Inversion Recovery (FLAIR) images as input without any pretraining. A median Balanced Accuracy of 50.96% was achieved. Additionally, explainability methods were applied. Using Saliency Maps, the tumor region was often successfully highlighted. In contrast, Grad-CAM typically failed to highlight the tumor region, with some exceptions observed in the Complete Response and Progressive Disease classes, where it effectively identified the tumor region. These results set a benchmark for future studies on glioblastoma treatment response assessment based on the RANO criteria while emphasizing the heterogeneity of factors that might play a role when assessing the tumor's response to treatment.","authors":["Ana Matoso","Catarina Passarinho","Marta P. Loureiro","Jos\\'e Maria Moreira","Patr\\'icia Figueiredo","Rita G. Nunes"],"url":"https://arxiv.org/abs/2504.18268"}
{"created":"2025-04-28","title":"Artificial Intelligence health advice accuracy varies across languages and contexts","abstract":"Using basic health statements authorized by UK and EU registers and 9,100 journalist-vetted public-health assertions on topics such as abortion, COVID-19 and politics from sources ranging from peer-reviewed journals and government advisories to social media and news across the political spectrum, we benchmark six leading large language models from in 21 languages, finding that, despite high accuracy on English-centric textbook claims, performance falls in multiple non-European languages and fluctuates by topic and source, highlighting the urgency of comprehensive multilingual, domain-aware validation before deploying AI in global health communication.","authors":["Prashant Garg","Thiemo Fetzer"],"url":"https://arxiv.org/abs/2504.18310"}
{"created":"2025-04-28","title":"Advanced Channel Decomposition Techniques in OTFS: A GSVD Approach for Multi-User Downlink","abstract":"In this paper, we propose a multi-user downlink system for two users based on the orthogonal time frequency space (OTFS) modulation scheme. The design leverages the generalized singular value decomposition (GSVD) of the channels between the base station and the two users, applying precoding and detection matrices based on the right and left singular vectors, respectively. We derive the analytical expressions for three scenarios and present the corresponding simulation results. These results demonstrate that, in terms of bit error rate (BER), the proposed system outperforms the conventional multi-user OTFS system in two scenarios when using minimum mean square error (MMSE) equalizers or precoder, both for perfect channel state information and for a scenario with channel estimation errors. In the third scenario, the design is equivalent to zero-forcing (ZF) precoding at the transmitter.","authors":["Omid Abbassi Aghd","Oussama Ben Haj Belkacem","Dou Hu","Jo\\~ao Guerreiro","Nuno Souto","Michal Szczachor","Rui Dinis"],"url":"https://arxiv.org/abs/2504.18315"}
{"created":"2025-04-28","title":"NUDF: Neural Unsigned Distance Fields for high resolution 3D medical image segmentation","abstract":"Medical image segmentation is often considered as the task of labelling each pixel or voxel as being inside or outside a given anatomy. Processing the images at their original size and resolution often result in insuperable memory requirements, but downsampling the images leads to a loss of important details. Instead of aiming to represent a smooth and continuous surface in a binary voxel-grid, we propose to learn a Neural Unsigned Distance Field (NUDF) directly from the image. The small memory requirements of NUDF allow for high resolution processing, while the continuous nature of the distance field allows us to create high resolution 3D mesh models of shapes of any topology (i.e. open surfaces). We evaluate our method on the task of left atrial appendage (LAA) segmentation from Computed Tomography (CT) images. The LAA is a complex and highly variable shape, being thus difficult to represent with traditional segmentation methods using discrete labelmaps. With our proposed method, we are able to predict 3D mesh models that capture the details of the LAA and achieve accuracy in the order of the voxel spacing in the CT images.","authors":["Kristine S{\\o}rensen","Oscar Camara","Ole de Backer","Klaus Kofoed","Rasmus Paulsen"],"url":"https://arxiv.org/abs/2504.18344"}
{"created":"2025-04-28","title":"Predicting sampling advantage of stochastic Ising Machines for Quantum Simulations","abstract":"Stochastic Ising machines, sIMs, are highly promising accelerators for optimization and sampling of computational problems that can be formulated as an Ising model. Here we investigate the computational advantage of sIM for simulations of quantum magnets with neural-network quantum states (NQS), in which the quantum many-body wave function is mapped onto an Ising model. We study the sampling performance of sIM for NQS by comparing sampling on a software-emulated sIM with standard Metropolis-Hastings sampling for NQS. We quantify the sampling efficiency by the number of steps required to reach iso-accurate stochastic estimation of the variational energy and show that this is entirely determined by the autocorrelation time of the sampling. This enables predications of sampling advantage without direct deployment on hardware. For the quantum Heisenberg models studied and experimental results on the runtime of sIMs, we project a possible speed-up of 100 to 10000, suggesting great opportunities for studying complex quantum systems at larger scales.","authors":["Rutger J. L. F. Berns","Davi R. Rodrigues","Giovanni Finocchio","Johan H. Mentink"],"url":"https://arxiv.org/abs/2504.18359"}
{"created":"2025-04-28","title":"Enhanced Sampling, Public Dataset and Generative Model for Drug-Protein Dissociation Dynamics","abstract":"Drug-protein binding and dissociation dynamics are fundamental to understanding molecular interactions in biological systems. While many tools for drug-protein interaction studies have emerged, especially artificial intelligence (AI)-based generative models, predictive tools on binding/dissociation kinetics and dynamics are still limited. We propose a novel research paradigm that combines molecular dynamics (MD) simulations, enhanced sampling, and AI generative models to address this issue. We propose an enhanced sampling strategy to efficiently implement the drug-protein dissociation process in MD simulations and estimate the free energy surface (FES). We constructed a program pipeline of MD simulations based on this sampling strategy, thus generating a dataset including 26,612 drug-protein dissociation trajectories containing about 13 million frames. We named this dissociation dynamics dataset DD-13M and used it to train a deep equivariant generative model UnbindingFlow, which can generate collision-free dissociation trajectories. The DD-13M database and UnbindingFlow model represent a significant advancement in computational structural biology, and we anticipate its broad applicability in machine learning studies of drug-protein interactions. Our ongoing efforts focus on expanding this methodology to encompass a broader spectrum of drug-protein complexes and exploring novel applications in pathway prediction.","authors":["Maodong Li","Jiying Zhang","Bin Feng","Wenqi Zeng","Dechin Chen","Zhijun Pan","Yu Li","Zijing Liu","Yi Isaac Yang"],"url":"https://arxiv.org/abs/2504.18367"}
{"created":"2025-04-28","title":"Partition Map-Based Fast Block Partitioning for VVC Inter Coding","abstract":"Among the new techniques of Versatile Video Coding (VVC), the quadtree with nested multi-type tree (QT+MTT) block structure yields significant coding gains by providing more flexible block partitioning patterns. However, the recursive partition search in the VVC encoder increases the encoder complexity substantially. To address this issue, we propose a partition map-based algorithm to pursue fast block partitioning in inter coding. Based on our previous work on partition map-based methods for intra coding, we analyze the characteristics of VVC inter coding, and thus improve the partition map by incorporating an MTT mask for early termination. Next, we develop a neural network that uses both spatial and temporal features to predict the partition map. It consists of several special designs including stacked top-down and bottom-up processing, quantization parameter modulation layers, and partitioning-adaptive warping. Furthermore, we present a dual-threshold decision scheme to achieve a fine-grained trade-off between complexity reduction and rate-distortion (RD) performance loss. The experimental results demonstrate that the proposed method achieves an average 51.30% encoding time saving with a 2.12% Bjontegaard Delta Bit Rate (BDBR) under the random access configuration.","authors":["Xinmin Feng","Zhuoyuan Li","Li Li","Dong Liu","Feng Wu"],"url":"https://arxiv.org/abs/2504.18398"}
{"created":"2025-04-28","title":"A Multimodal Deep Learning Approach for White Matter Shape Prediction in Diffusion MRI Tractography","abstract":"Shape measures have emerged as promising descriptors of white matter tractography, offering complementary insights into anatomical variability and associations with cognitive and clinical phenotypes. However, conventional methods for computing shape measures are computationally expensive and time-consuming for large-scale datasets due to reliance on voxel-based representations. We propose Tract2Shape, a novel multimodal deep learning framework that leverages geometric (point cloud) and scalar (tabular) features to predict ten white matter tractography shape measures. To enhance model efficiency, we utilize a dimensionality reduction algorithm for the model to predict five primary shape components. The model is trained and evaluated on two independently acquired datasets, the HCP-YA dataset, and the PPMI dataset. We evaluate the performance of Tract2Shape by training and testing it on the HCP-YA dataset and comparing the results with state-of-the-art models. To further assess its robustness and generalization ability, we also test Tract2Shape on the unseen PPMI dataset. Tract2Shape outperforms SOTA deep learning models across all ten shape measures, achieving the highest average Pearson's r and the lowest nMSE on the HCP-YA dataset. The ablation study shows that both multimodal input and PCA contribute to performance gains. On the unseen testing PPMI dataset, Tract2Shape maintains a high Pearson's r and low nMSE, demonstrating strong generalizability in cross-dataset evaluation. Tract2Shape enables fast, accurate, and generalizable prediction of white matter shape measures from tractography data, supporting scalable analysis across datasets. This framework lays a promising foundation for future large-scale white matter shape analysis.","authors":["Yui Lo","Yuqian Chen","Dongnan Liu","Leo Zekelman","Jarrett Rushmore","Yogesh Rathi","Nikos Makris","Alexandra J. Golby","Fan Zhang","Weidong Cai","Lauren J. O'Donnell"],"url":"https://arxiv.org/abs/2504.18400"}
{"created":"2025-04-28","title":"HepatoGEN: Generating Hepatobiliary Phase MRI with Perceptual and Adversarial Models","abstract":"Dynamic contrast-enhanced magnetic resonance imaging (DCE-MRI) plays a crucial role in the detection and characterization of focal liver lesions, with the hepatobiliary phase (HBP) providing essential diagnostic information. However, acquiring HBP images requires prolonged scan times, which may compromise patient comfort and scanner throughput. In this study, we propose a deep learning based approach for synthesizing HBP images from earlier contrast phases (precontrast and transitional) and compare three generative models: a perceptual U-Net, a perceptual GAN (pGAN), and a denoising diffusion probabilistic model (DDPM). We curated a multi-site DCE-MRI dataset from diverse clinical settings and introduced a contrast evolution score (CES) to assess training data quality, enhancing model performance. Quantitative evaluation using pixel-wise and perceptual metrics, combined with qualitative assessment through blinded radiologist reviews, showed that pGAN achieved the best quantitative performance but introduced heterogeneous contrast in out-of-distribution cases. In contrast, the U-Net produced consistent liver enhancement with fewer artifacts, while DDPM underperformed due to limited preservation of fine structural details. These findings demonstrate the feasibility of synthetic HBP image generation as a means to reduce scan time without compromising diagnostic utility, highlighting the clinical potential of deep learning for dynamic contrast enhancement in liver MRI. A project demo is available at: https://jhooge.github.io/hepatogen","authors":["Jens Hooge","Gerard Sanroma-Guell","Faidra Stavropoulou","Alexander Ullmann","Gesine Knobloch","Mark Klemens","Carola Schmidt","Sabine Weckbach","Andreas Bolz"],"url":"https://arxiv.org/abs/2504.18405"}
{"created":"2025-04-28","title":"Kimi-Audio Technical Report","abstract":"We present Kimi-Audio, an open-source audio foundation model that excels in audio understanding, generation, and conversation. We detail the practices in building Kimi-Audio, including model architecture, data curation, training recipe, inference deployment, and evaluation. Specifically, we leverage a 12.5Hz audio tokenizer, design a novel LLM-based architecture with continuous features as input and discrete tokens as output, and develop a chunk-wise streaming detokenizer based on flow matching. We curate a pre-training dataset that consists of more than 13 million hours of audio data covering a wide range of modalities including speech, sound, and music, and build a pipeline to construct high-quality and diverse post-training data. Initialized from a pre-trained LLM, Kimi-Audio is continual pre-trained on both audio and text data with several carefully designed tasks, and then fine-tuned to support a diverse of audio-related tasks. Extensive evaluation shows that Kimi-Audio achieves state-of-the-art performance on a range of audio benchmarks including speech recognition, audio understanding, audio question answering, and speech conversation. We release the codes, model checkpoints, as well as the evaluation toolkits in https://github.com/MoonshotAI/Kimi-Audio.","authors":["KimiTeam","Ding Ding","Zeqian Ju","Yichong Leng","Songxiang Liu","Tong Liu","Zeyu Shang","Kai Shen","Wei Song","Xu Tan","Heyi Tang","Zhengtao Wang","Chu Wei","Yifei Xin","Xinran Xu","Jianwei Yu","Yutao Zhang","Xinyu Zhou","Y. Charles","Jun Chen","Yanru Chen","Yulun Du","Weiran He","Zhenxing Hu","Guokun Lai","Qingcheng Li","Yangyang Liu","Weidong Sun","Jianzhou Wang","Yuzhi Wang","Yuefeng Wu","Yuxin Wu","Dongchao Yang","Hao Yang","Ying Yang","Zhilin Yang","Aoxiong Yin","Ruibin Yuan","Yutong Zhang","Zaida Zhou"],"url":"https://arxiv.org/abs/2504.18425"}
{"created":"2025-04-28","title":"Nearly isotropic segmentation for medial temporal lobe subregions in multi-modality MRI","abstract":"Morphometry of medial temporal lobe (MTL) subregions in brain MRI is sensitive biomarker to Alzheimers Disease and other related conditions. While T2-weighted (T2w) MRI with high in-plane resolution is widely used to segment hippocampal subfields due to its higher contrast in hippocampus, its lower out-of-plane resolution reduces the accuracy of subregion thickness measurements. To address this issue, we developed a nearly isotropic segmentation pipeline that incorporates image and label upsampling and high-resolution segmentation in T2w MRI. First, a high-resolution atlas was created based on an existing anisotropic atlas derived from 29 individuals. Both T1-weighted and T2w images in the atlas were upsampled from their original resolution to a nearly isotropic resolution 0.4x0.4x0.52mm3 using a non-local means approach. Manual segmentations within the atlas were also upsampled to match this resolution using a UNet-based neural network, which was trained on a cohort consisting of both high-resolution ex vivo and low-resolution anisotropic in vivo MRI with manual segmentations. Second, a multi-modality deep learning-based segmentation model was trained within this nearly isotropic atlas. Finally, experiments showed the nearly isotropic subregion segmentation improved the accuracy of cortical thickness as an imaging biomarker for neurodegeneration in T2w MRI.","authors":["Yue Li","Pulkit Khandelwal","Long Xie","Laura E. M. Wisse","Nidhi Mundada","Christopher A. Brown","Emily McGrew","Amanda Denning","Sandhitsu R. Das","David A. Wolk","Paul A. Yushkevich"],"url":"https://arxiv.org/abs/2504.18442"}
{"created":"2025-04-28","title":"Generalization Guarantees for Multi-View Representation Learning and Application to Regularization via Gaussian Product Mixture Prior","abstract":"We study the problem of distributed multi-view representation learning. In this problem, $K$ agents observe each one distinct, possibly statistically correlated, view and independently extracts from it a suitable representation in a manner that a decoder that gets all $K$ representations estimates correctly the hidden label. In the absence of any explicit coordination between the agents, a central question is: what should each agent extract from its view that is necessary and sufficient for a correct estimation at the decoder? In this paper, we investigate this question from a generalization error perspective. First, we establish several generalization bounds in terms of the relative entropy between the distribution of the representations extracted from training and \"test\" datasets and a data-dependent symmetric prior, i.e., the Minimum Description Length (MDL) of the latent variables for all views and training and test datasets. Then, we use the obtained bounds to devise a regularizer; and investigate in depth the question of the selection of a suitable prior. In particular, we show and conduct experiments that illustrate that our data-dependent Gaussian mixture priors with judiciously chosen weights lead to good performance. For single-view settings (i.e., $K=1$), our experimental results are shown to outperform existing prior art Variational Information Bottleneck (VIB) and Category-Dependent VIB (CDVIB) approaches. Interestingly, we show that a weighted attention mechanism emerges naturally in this setting. Finally, for the multi-view setting, we show that the selection of the joint prior as a Gaussians product mixture induces a Gaussian mixture marginal prior for each marginal view and implicitly encourages the agents to extract and output redundant features, a finding which is somewhat counter-intuitive.","authors":["Milad Sefidgaran","Abdellatif Zaidi","Piotr Krasnowski"],"url":"https://arxiv.org/abs/2504.18455"}
{"created":"2025-04-28","title":"Filtering of second order generalized stochastic processes corrupted by additive noise","abstract":"We treat the optimal linear filtering problem for a sum of two second order uncorrelated generalized stochastic processes. This is an operator equation involving covariance operators. We study both the wide-sense stationary case and the non-stationary case. In the former case the equation simplifies into a convolution equation. The solution is the Radon--Nikodym derivative between non-negative tempered Radon measures, for signal and signal plus noise respectively, in the frequency domain. In the non-stationary case we work with pseudodifferential operators with symbols in Sj\\\"ostrand modulation spaces which admits the use of its spectral invariance properties.","authors":["Patrik Wahlberg"],"url":"https://arxiv.org/abs/2504.18456"}
{"created":"2025-04-28","title":"Enhancing Visual Interpretability and Explainability in Functional Survival Trees and Forests","abstract":"Functional survival models are key tools for analyzing time-to-event data with complex predictors, such as functional or high-dimensional inputs. Despite their predictive strength, these models often lack interpretability, which limits their value in practical decision-making and risk analysis. This study investigates two key survival models: the Functional Survival Tree (FST) and the Functional Random Survival Forest (FRSF). It introduces novel methods and tools to enhance the interpretability of FST models and improve the explainability of FRSF ensembles. Using both real and simulated datasets, the results demonstrate that the proposed approaches yield efficient, easy-to-understand decision trees that accurately capture the underlying decision-making processes of the model ensemble.","authors":["Giuseppe Loffredo","Elvira Romano","Fabrizio MAturo"],"url":"https://arxiv.org/abs/2504.18498"}
{"created":"2025-04-28","title":"Music Tempo Estimation on Solo Instrumental Performance","abstract":"Recently, automatic music transcription has made it possible to convert musical audio into accurate MIDI. However, the resulting MIDI lacks music notations such as tempo, which hinders its conversion into sheet music. In this paper, we investigate state-of-the-art tempo estimation techniques and evaluate their performance on solo instrumental music. These include temporal convolutional network (TCN) and recurrent neural network (RNN) models that are pretrained on massive of mixed vocals and instrumental music, as well as TCN models trained specifically with solo instrumental performances. Through evaluations on drum, guitar, and classical piano datasets, our TCN models with the new training scheme achieved the best performance. Our newly trained TCN model increases the Acc1 metric by 38.6% for guitar tempo estimation, compared to the pretrained TCN model with an Acc1 of 61.1%. Although our trained TCN model is twice as accurate as the pretrained TCN model in estimating classical piano tempo, its Acc1 is only 50.9%. To improve the performance of deep learning models, we investigate their combinations with various post-processing methods. These post-processing techniques effectively enhance the performance of deep learning models when they struggle to estimate the tempo of specific instruments.","authors":["Zhanhong He","Roberto Togneri","Xiangyu Zhang"],"url":"https://arxiv.org/abs/2504.18502"}
{"created":"2025-04-28","title":"Lower bounds on collective additive spanners","abstract":"In this paper we present various lower bound results on collective tree spanners and on spanners of bounded treewidth. A graph $G$ is said to admit a system of $\\mu$ collective additive tree $c$-spanners if there is a system $\\cal{T}$$(G)$ of at most $\\mu$ spanning trees of $G$ such that for any two vertices $u,v$ of $G$ a tree $T\\in \\cal{T}$$(G)$ exists such that the distance in $T$ between $u$ and $v$ is at most $c$ plus their distance in $G$. A graph $G$ is said to admit an additive $k$-treewidth $c$-spanner if there is a spanning subgraph $H$ of $G$ with treewidth $k$ such that for any pair of vertices $u$ and $v$ their distance in $H$ is at most $c$ plus their distance in $G$. Among other results, we show that:","authors":["Derek G. Corneil","Feodor F. Dragan","Ekkehard K\\\"ohler","Yang Xiang"],"url":"https://arxiv.org/abs/2504.18508"}
{"created":"2025-04-28","title":"RSFR: A Coarse-to-Fine Reconstruction Framework for Diffusion Tensor Cardiac MRI with Semantic-Aware Refinement","abstract":"Cardiac diffusion tensor imaging (DTI) offers unique insights into cardiomyocyte arrangements, bridging the gap between microscopic and macroscopic cardiac function. However, its clinical utility is limited by technical challenges, including a low signal-to-noise ratio, aliasing artefacts, and the need for accurate quantitative fidelity. To address these limitations, we introduce RSFR (Reconstruction, Segmentation, Fusion & Refinement), a novel framework for cardiac diffusion-weighted image reconstruction. RSFR employs a coarse-to-fine strategy, leveraging zero-shot semantic priors via the Segment Anything Model and a robust Vision Mamba-based reconstruction backbone. Our framework integrates semantic features effectively to mitigate artefacts and enhance fidelity, achieving state-of-the-art reconstruction quality and accurate DT parameter estimation under high undersampling rates. Extensive experiments and ablation studies demonstrate the superior performance of RSFR compared to existing methods, highlighting its robustness, scalability, and potential for clinical translation in quantitative cardiac DTI.","authors":["Jiahao Huang","Fanwen Wang","Pedro F. Ferreira","Haosen Zhang","Yinzhe Wu","Zhifan Gao","Lei Zhu","Angelica I. Aviles-Rivero","Carola-Bibiane Schonlieb","Andrew D. Scott","Zohya Khalique","Maria Dwornik","Ramyah Rajakulasingam","Ranil De Silva","Dudley J. Pennell","Guang Yang","Sonia Nielles-Vallespin"],"url":"https://arxiv.org/abs/2504.18520"}
{"created":"2025-04-28","title":"Representation Learning for Distributional Perturbation Extrapolation","abstract":"We consider the problem of modelling the effects of unseen perturbations such as gene knockdowns or drug combinations on low-level measurements such as RNA sequencing data. Specifically, given data collected under some perturbations, we aim to predict the distribution of measurements for new perturbations. To address this challenging extrapolation task, we posit that perturbations act additively in a suitable, unknown embedding space. More precisely, we formulate the generative process underlying the observed data as a latent variable model, in which perturbations amount to mean shifts in latent space and can be combined additively. Unlike previous work, we prove that, given sufficiently diverse training perturbations, the representation and perturbation effects are identifiable up to affine transformation, and use this to characterize the class of unseen perturbations for which we obtain extrapolation guarantees. To estimate the model from data, we propose a new method, the perturbation distribution autoencoder (PDAE), which is trained by maximising the distributional similarity between true and predicted perturbation distributions. The trained model can then be used to predict previously unseen perturbation distributions. Empirical evidence suggests that PDAE compares favourably to existing methods and baselines at predicting the effects of unseen perturbations.","authors":["Julius von K\\\"ugelgen","Jakob Ketterer","Xinwei Shen","Nicolai Meinshausen","Jonas Peters"],"url":"https://arxiv.org/abs/2504.18522"}
{"created":"2025-04-28","title":"Multiple-Instance, Cascaded Classification for Keyword Spotting in Narrow-Band Audio","abstract":"We propose using cascaded classifiers for a keyword spotting (KWS) task on narrow-band (NB), 8kHz audio acquired in non-IID environments -- a more challenging task than most state-of-the-art KWS systems face. We present a model that incorporates Deep Neural Networks (DNNs), cascading, multiple-feature representations, and multiple-instance learning. The cascaded classifiers handle the task's class imbalance and reduce power consumption on computationally-constrained devices via early termination. The KWS system achieves a false negative rate of 6% at an hourly false positive rate of 0.75","authors":["Ahmad AbdulKader","Kareem Nassar","Mohamed El-Geish","Daniel Galvez","Chetan Patil"],"url":"https://arxiv.org/abs/1711.08058"}
{"created":"2025-04-28","title":"Deep Optimal Transport for Domain Adaptation on SPD Manifolds","abstract":"Recent progress in geometric deep learning has drawn increasing attention from the machine learning community toward domain adaptation on symmetric positive definite (SPD) manifolds, especially for neuroimaging data that often suffer from distribution shifts across sessions. These data, typically represented as covariance matrices of brain signals, inherently lie on SPD manifolds due to their symmetry and positive definiteness. However, conventional domain adaptation methods often overlook this geometric structure when applied directly to covariance matrices, which can result in suboptimal performance. To address this issue, we introduce a new geometric deep learning framework that combines optimal transport theory with the geometry of SPD manifolds. Our approach aligns data distributions while respecting the manifold structure, effectively reducing both marginal and conditional discrepancies. We validate our method on three cross-session brain computer interface datasets, KU, BNCI2014001, and BNCI2015001, where it consistently outperforms baseline approaches while maintaining the intrinsic geometry of the data. We also provide quantitative results and visualizations to better illustrate the behavior of the learned embeddings.","authors":["Ce Ju","Cuntai Guan"],"url":"https://arxiv.org/abs/2201.05745"}
{"created":"2025-04-28","title":"Witnessed Symmetric Choice and Interpretations in Fixed-Point Logic with Counting","abstract":"At the core of the quest for a logic for PTime is a mismatch between algorithms making arbitrary choices and isomorphism-invariant logics. One approach to overcome this problem is witnessed symmetric choice. It allows for choices from definable orbits which are certified by definable witnessing automorphisms.","authors":["Moritz Lichter"],"url":"https://arxiv.org/abs/2210.07869"}
{"created":"2025-04-28","title":"CR-LSO: Convex Neural Architecture Optimization in the Latent Space of Graph Variational Autoencoder with Input Convex Neural Networks","abstract":"In neural architecture search (NAS) methods based on latent space optimization (LSO), a deep generative model is trained to embed discrete neural architectures into a continuous latent space. In this case, different optimization algorithms that operate in the continuous space can be implemented to search neural architectures. However, the optimization of latent variables is challenging for gradient-based LSO since the mapping from the latent space to the architecture performance is generally non-convex. To tackle this problem, this paper develops a convexity regularized latent space optimization (CR-LSO) method, which aims to regularize the learning process of latent space in order to obtain a convex architecture performance mapping. Specifically, CR-LSO trains a graph variational autoencoder (G-VAE) to learn the continuous representations of discrete architectures. Simultaneously, the learning process of latent space is regularized by the guaranteed convexity of input convex neural networks (ICNNs). In this way, the G-VAE is forced to learn a convex mapping from the architecture representation to the architecture performance. Hereafter, the CR-LSO approximates the performance mapping using the ICNN and leverages the estimated gradient to optimize neural architecture representations. Experimental results on three popular NAS benchmarks show that CR-LSO achieves competitive evaluation results in terms of both computational complexity and architecture performance.","authors":["Xuan Rao","Bo Zhao","Derong Liu"],"url":"https://arxiv.org/abs/2211.05950"}
{"created":"2025-04-28","title":"Quick Updates for the Perturbed Static Output Feedback Control Problem in Linear Systems","abstract":"This paper introduces a method for efficiently updating a nominal stabilizing static output feedback (SOF) controller in perturbed linear systems. As operating points and state-space matrices change in dynamic systems, accommodating updates to the SOF controller are necessary. Traditional methods address such changes by re-solving for the updated SOF gain, which is often \\textit{(i)} computationally expensive due to the NP-hard nature of the problem or \\textit{(ii)} infeasible due the limitations of its semidefinite programming relaxations. To overcome this, we leverage the concept of \\textit{minimum destabilizing real perturbation} to formulate a norm minimization problem that yields fast, reliable controller updates. This approach accommodates a variety of known perturbations, including abrupt changes, model inaccuracies, and equilibrium-dependent linearizations. We also introduce geometric metrics to quantify the proximity to instability and rigorously define stability-guaranteed regions. Extensive numerical simulations validate the efficiency and robustness of the proposed method. We demonstrate the results on the SOF control of mutlti-machine power networks with changing operating points, and demonstrate that the computed quick updates produce comparable solutions to the SOF ones, while requiring orders of magnitude less time.","authors":["MirSaleh Bahavarnia","Ahmad F. Taha"],"url":"https://arxiv.org/abs/2307.16178"}
{"created":"2025-04-28","title":"Implicit Bonded Discrete Element Method with Manifold Optimization","abstract":"This paper proposes a novel approach that combines variational integration with the bonded discrete element method (BDEM) to achieve faster and more accurate fracture simulations. The approach leverages the efficiency of implicit integration and the accuracy of BDEM in modeling fracture phenomena. We introduce a variational integrator and a manifold optimization approach utilizing a nullspace operator to speed up the solving of quaternion-constrained systems. Additionally, the paper presents an element packing and surface reconstruction method specifically designed for bonded discrete element methods. Results from the experiments prove that the proposed method offers 2.8 to 12 times faster state-of-the-art methods.","authors":["Jia-Ming Lu","Geng-Chen Cao","Chen-Feng Li","Shi-Min Hu"],"url":"https://arxiv.org/abs/2308.10459"}
{"created":"2025-04-28","title":"Forensics and security issues in the Internet of Things","abstract":"Given the exponential expansion of the internet, the possibilities of security attacks and cybercrimes have increased accordingly. However, poorly implemented security mechanisms in the Internet of Things (IoT) devices make them susceptible to cyberattacks, which can directly affect users. IoT forensics is thus needed to investigate and mitigate such attacks. While many works have examined IoT applications and challenges, only a few have focused on both the forensic and security issues in IoT. Therefore, this paper reviews forensic and security issues associated with IoT in different fields. Prospects and challenges in IoT research and development are also highlighted. As the literature demonstrates, most IoT devices are vulnerable to attacks due to a lack of standardized security measures. Unauthorized users could get access, compromise data, and even benefit from control of critical infrastructure. To fulfill the security-conscious needs of consumers, IoT can be used to develop a smart home system by designing the security-conscious needs of consumers; IoT can be used to create a smart home system by designing an IoT can be used to develop a smart home system by designing a FLIP-based system that is highly scalable and adaptable. A blockchain-based authentication mechanism with a multi-chain structure can provide additional security protection between different trust domains. Deep learning can be utilized to develop a network forensics framework with a high-performing system for detecting and tracking cyberattack incidents. Moreover, researchers should consider limiting the amount of data created and delivered when using big data to develop IoT-based smart systems. The findings of this review will stimulate academics to seek potential solutions for the identified issues, thereby advancing the IoT field.","authors":["Shams Forruque Ahmed","Shanjana Shuravi","Afsana Bhuyian","Shaila Afrin","Aanushka Mehjabin","Sweety Angela Kuldeep","Md. Sakib Bin Alam","Amir H. Gandomi"],"url":"https://arxiv.org/abs/2309.02707"}
{"created":"2025-04-28","title":"Virtuoso: High Resource Utilization and {\\mu}s-scale Performance Isolation in a Shared Virtual Machine TCP Network Stack","abstract":"Virtualization improves resource efficiency and ensures security and performance isolation for cloud applications. Today, operators use a layered architecture with separate network stack instances in each VM and container connected to a virtual switch. Decoupling through layering reduces complexity, but induces performance and resource overheads at odds with increasing demands for network bandwidth, connection scalability, and low latency.","authors":["Matheus Stolet","Liam Arzola","Simon Peter","Antoine Kaufmann"],"url":"https://arxiv.org/abs/2309.14016"}
{"created":"2025-04-28","title":"Treating Motion as Option with Output Selection for Unsupervised Video Object Segmentation","abstract":"Unsupervised video object segmentation aims to detect the most salient object in a video without any external guidance regarding the object. Salient objects often exhibit distinctive movements compared to the background, and recent methods leverage this by combining motion cues from optical flow maps with appearance cues from RGB images. However, because optical flow maps are often closely correlated with segmentation masks, networks can become overly dependent on motion cues during training, leading to vulnerability when faced with confusing motion cues and resulting in unstable predictions. To address this challenge, we propose a novel motion-as-option network that treats motion cues as an optional component rather than a necessity. During training, we randomly input RGB images into the motion encoder instead of optical flow maps, which implicitly reduces the network's reliance on motion cues. This design ensures that the motion encoder is capable of processing both RGB images and optical flow maps, leading to two distinct predictions depending on the type of input provided. To make the most of this flexibility, we introduce an adaptive output selection algorithm that determines the optimal prediction during testing.","authors":["Suhwan Cho","Minhyeok Lee","Jungho Lee","MyeongAh Cho","Seungwook Park","Jaeyeob Kim","Hyunsung Jang","Sangyoun Lee"],"url":"https://arxiv.org/abs/2309.14786"}
{"created":"2025-04-28","title":"Randomly sparsified Richardson iteration: A dimension-independent sparse linear solver","abstract":"Recently, a class of algorithms combining classical fixed point iterations with repeated random sparsification of approximate solution vectors has been successfully applied to eigenproblems with matrices as large as $10^{108} \\times 10^{108}$. So far, a complete mathematical explanation for their success has proven elusive.","authors":["Jonathan Weare","Robert J. Webber"],"url":"https://arxiv.org/abs/2309.17270"}
{"created":"2025-04-28","title":"Adversarial Attacks to Latent Representations of Distributed Neural Networks in Split Computing","abstract":"Distributed deep neural networks (DNNs) have been shown to reduce the computational burden of mobile devices and decrease the end-to-end inference latency in edge computing scenarios. While distributed DNNs have been studied, to the best of our knowledge, the resilience of distributed DNNs to adversarial action remains an open problem. In this paper, we fill the existing research gap by rigorously analyzing the robustness of distributed DNNs against adversarial action. We cast this problem in the context of information theory and rigorously proved that (i) the compressed latent dimension improves the robustness but also affect task-oriented performance; and (ii) the deeper splitting point enhances the robustness but also increases the computational burden. These two trade-offs provide a novel perspective to design robust distributed DNN. To test our theoretical findings, we perform extensive experimental analysis by considering 6 different DNN architectures, 6 different approaches for distributed DNN and 10 different adversarial attacks using the ImageNet-1K dataset.","authors":["Milin Zhang","Mohammad Abdi","Jonathan Ashdown","Francesco Restuccia"],"url":"https://arxiv.org/abs/2309.17401"}
{"created":"2025-04-28","title":"Finding 709 Defects in 258 Projects: An Experience Report on Applying CodeQL to Open-Source Embedded Software (Experience Paper) -- Extended Report","abstract":"In this experience paper, we report on a large-scale empirical study of Static Application Security Testing (SAST) in Open-Source Embedded Software (EMBOSS) repositories. We collected a corpus of 258 of the most popular EMBOSS projects, and then measured their use of SAST tools via program analysis and a survey (N=25) of their developers. Advanced SAST tools are rarely used -- only 3% of projects go beyond trivial compiler analyses. Developers cited the perception of ineffectiveness and false positives as reasons for limited adoption. Motivated by this deficit, we applied the state-of-the-art (SOTA) CodeQL SAST tool and measured its ease of use and actual effectiveness. Across the 258 projects, CodeQL reported 709 true defects with a false positive rate of 34%. There were 535 (75%) likely security vulnerabilities, including in major projects maintained by Microsoft, Amazon, and the Apache Foundation. EMBOSS engineers have confirmed 376 (53%) of these defects, mainly by accepting our pull requests. Two CVEs were issued. Based on these results, we proposed pull requests to include our workflows as part of EMBOSS Continuous Integration (CI) pipelines, 37 (71% of active repositories) of these are already merged. In summary, we urge EMBOSS engineers to adopt the current generation of SAST tools, which offer low false positive rates and are effective at finding security-relevant defects.","authors":["Mingjie Shen","Akul Abhilash Pillai","Brian A. Yuan","James C. Davis","Aravind Machiry"],"url":"https://arxiv.org/abs/2310.00205"}
{"created":"2025-04-28","title":"Normal Forms for Elements of ${}^*$-Continuous Kleene Algebras Representing the Context-Free Languages","abstract":"Within the tensor product $K \\mathop{\\otimes_{\\cal R}} C_2'$ of any ${}^*$-continuous Kleene algebra $K$ with the polycyclic ${}^*$-continuous Kleene algebra $C_2'$ over two bracket pairs there is a copy of the fixed-point closure of $K$: the centralizer of $C_2'$ in $K \\mathop{\\otimes_{\\cal R}} C_2'$. Using an automata-theoretic representation of elements of $K\\mathop{\\otimes_{\\cal R}} C_2'$ \\`a la Kleene, with the aid of normal form theorems that restrict the occurrences of brackets on paths through the automata, we develop a foundation for a calculus of context-free expressions without variable binders. We also give some results on the bra-ket ${}^*$-continuous Kleene algebra $C_2$, motivate the ``completeness equation'' that distinguishes $C_2$ from $C_2'$, and show that $C_2'$ already validates a relativized form of this equation.","authors":["Mark Hopkins","Hans Lei{\\ss}"],"url":"https://arxiv.org/abs/2310.17295"}
{"created":"2025-04-28","title":"Fast Approximation Algorithms for Piercing Boxes by Points","abstract":"$\\newcommand{\\popt}{{\\mathcal{p}}} \\newcommand{\\Re}{\\mathbb{R}}\\newcommand{\\N}{{\\mathcal{N}}} \\newcommand{\\BX}{\\mathcal{B}} \\newcommand{\\bb}{\\mathsf{b}} \\newcommand{\\eps}{\\varepsilon} \\newcommand{\\polylog}{\\mathrm{polylog}} $ Let $\\mathcal{B}=\\{\\mathsf{b}_1, \\ldots ,\\mathsf{b}_n\\}$ be a set of $n$ axis-aligned boxes in $\\Re^d$ where $d\\geq2$ is a constant. The \\emph{piercing problem} is to compute a smallest set of points $\\N \\subset \\Re^d$ that hits every box in $\\mathcal{B}$, i.e., $\\N\\cap \\mathsf{b}_i\\neq \\emptyset$, for $i=1,\\ldots, n$. Let $\\popt=\\popt(\\mathcal{B})$, the \\emph{piercing number} be the minimum size of a piercing set of $\\mathcal{B}$. We present a randomized $O(d^2\\log\\log \\popt)$-approximation algorithm with expected running time $O(n^{d/2}\\polylog n)$. Next, we present a faster $O(n^{\\log d+1})$-time algorithm but with a slightly inferior approximation factor of $O(2^{4d}\\log\\log\\popt)$. The running time of both algorithms can be improved to near-linear using a sampling-based technique, if $\\popt = O(n^{1/d})$.","authors":["Pankaj K. Agarwal","Sariel Har-Peled","Rahul Raychaudhury","Stavros Sintos"],"url":"https://arxiv.org/abs/2311.02050"}
{"created":"2025-04-28","title":"StoryGPT-V: Large Language Models as Consistent Story Visualizers","abstract":"Recent generative models have demonstrated impressive capabilities in generating realistic and visually pleasing images grounded on textual prompts. Nevertheless, a significant challenge remains in applying these models for the more intricate task of story visualization. Since it requires resolving pronouns (he, she, they) in the frame descriptions, i.e., anaphora resolution, and ensuring consistent characters and background synthesis across frames. Yet, the emerging Large Language Model (LLM) showcases robust reasoning abilities to navigate through ambiguous references and process extensive sequences. Therefore, we introduce \\emph{StoryGPT-V}, which leverages the merits of the latent diffusion (LDM) and LLM to produce images with consistent and high-quality characters grounded on given story descriptions. First, we train a character-aware LDM, which takes character-augmented semantic embedding as input and includes the supervision of the cross-attention map using character segmentation masks, aiming to enhance character generation accuracy and faithfulness. In the second stage, we enable an alignment between the output of LLM and the character-augmented embedding residing in the input space of the first-stage model. This harnesses the reasoning ability of LLM to address ambiguous references and the comprehension capability to memorize the context. We conduct comprehensive experiments on two visual story visualization benchmarks. Our model reports superior quantitative results and consistently generates accurate characters of remarkable quality with low memory consumption. Our code is publicly available at: \\href{https://xiaoqian-shen.github.io/StoryGPT-V}{https://xiaoqian-shen.github.io/StoryGPT-V}.","authors":["Xiaoqian Shen","Mohamed Elhoseiny"],"url":"https://arxiv.org/abs/2312.02252"}
{"created":"2025-04-28","title":"All for One, and One for All: UrbanSyn Dataset, the third Musketeer of Synthetic Driving Scenes","abstract":"We introduce UrbanSyn, a photorealistic dataset acquired through semi-procedurally generated synthetic urban driving scenarios. Developed using high-quality geometry and materials, UrbanSyn provides pixel-level ground truth, including depth, semantic segmentation, and instance segmentation with object bounding boxes and occlusion degree. It complements GTAV and Synscapes datasets to form what we coin as the 'Three Musketeers'. We demonstrate the value of the Three Musketeers in unsupervised domain adaptation for image semantic segmentation. Results on real-world datasets, Cityscapes, Mapillary Vistas, and BDD100K, establish new benchmarks, largely attributed to UrbanSyn. We make UrbanSyn openly and freely accessible (www.urbansyn.org).","authors":["Jose L. G\\'omez","Manuel Silva","Antonio Seoane","Agn\\`es Borr\\'as","Mario Noriega","Germ\\'an Ros","Jose A. Iglesias-Guitian","Antonio M. L\\'opez"],"url":"https://arxiv.org/abs/2312.12176"}
{"created":"2025-04-28","title":"Tensor Networks for Explainable Machine Learning in Cybersecurity","abstract":"In this paper we show how tensor networks help in developing explainability of machine learning algorithms. Specifically, we develop an unsupervised clustering algorithm based on Matrix Product States (MPS) and apply it in the context of a real use-case of adversary-generated threat intelligence. Our investigation proves that MPS rival traditional deep learning models such as autoencoders and GANs in terms of performance, while providing much richer model interpretability. Our approach naturally facilitates the extraction of feature-wise probabilities, Von Neumann Entropy, and mutual information, offering a compelling narrative for classification of anomalies and fostering an unprecedented level of transparency and interpretability, something fundamental to understand the rationale behind artificial intelligence decisions.","authors":["Borja Aizpurua","Samuel Palmer","Roman Orus"],"url":"https://arxiv.org/abs/2401.00867"}
{"created":"2025-04-28","title":"Sampled-Data Primal-Dual Gradient Dynamics in Model Predictive Control","abstract":"Model Predictive Control (MPC) is a versatile approach capable of accommodating diverse control requirements that holds significant promise for a broad spectrum of industrial applications. Noteworthy challenges associated with MPC include the substantial computational burden, which is sometimes considered excessive even for linear systems. Recently, a rapid computation method that guides the input toward convergence with the optimal control problem solution by employing primal-dual gradient (PDG) dynamics as a controller has been proposed for linear MPCs. However, stability has been ensured under the assumption that the controller is a continuous-time system, leading to potential instability when the controller undergoes discretization and is implemented as a sampled-data system. In this paper, we propose a discrete-time dynamical controller, incorporating specific modifications to the PDG approach, and present stability conditions relevant to the resulting sampled-data system. Additionally, we introduce an extension designed to enhance control performance, that was traded off in the original. Numerical examples substantiate that our proposed method, which can be executed in only 1 $\\mu$s in a standard laptop, not only ensures stability with considering sampled-data implementation but also effectively enhances control performance.","authors":["Ryuta Moriyasu","Sho Kawaguchi","Kenji Kashima"],"url":"https://arxiv.org/abs/2401.05100"}
{"created":"2025-04-28","title":"Line zonotopes: A tool for state estimation and fault diagnosis of unbounded and descriptor systems","abstract":"This paper proposes new methods for set-based state estimation and active fault diagnosis (AFD) of linear descriptor systems (LDS). Unlike intervals, ellipsoids, and zonotopes, constrained zonotopes (CZs) can directly incorporate linear static constraints on state variables - typical of descriptor systems - into their mathematical representation, leading to less conservative enclosures. However, for LDS that are unstable or not fully observable, a bounded representation cannot ensure a valid enclosure of the states over time. To address this limitation, we introduce line zonotopes, a new representation for unbounded sets that retains key properties of CZs, including polynomial time complexity reduction methods, while enabling the description of strips, hyperplanes, and the entire n-dimensional Euclidean space. This extension not only generalizes the use of CZs to unbounded settings but can also enhance set-based estimation and AFD in both stable and unstable scenarios. Additionally, we extend the AFD method for LDS from Rego et al. (2020) to operate over reachable tubes rather than solely on the reachable set at the final time of the considered horizon. This reduces conservatism in input separation and enables more accurate fault diagnosis based on the entire output sequence. The advantages of the proposed methods over existing CZ-based approaches are demonstrated through numerical examples.","authors":["Brenner S. Rego","Davide M. Raimondo","Guilherme V. Raffo"],"url":"https://arxiv.org/abs/2401.10239"}
{"created":"2025-04-28","title":"SafEDMD: A Koopman-based data-driven controller design framework for nonlinear dynamical systems","abstract":"The Koopman operator serves as the theoretical backbone for machine learning of dynamical control systems, where the operator is heuristically approximated by extended dynamic mode decomposition (EDMD). In this paper, we propose SafEDMD, a novel stability- and certificate-oriented EDMD-based controller design framework. Our approach leverages a reliable surrogate model generated in a data-driven fashion in order to provide closed-loop guarantees. In particular, we establish a controller design based on semi-definite programming with guaranteed stabilization of the underlying nonlinear system. As central ingredient, we derive proportional error bounds that vanish at the origin and are tailored to control tasks. We illustrate the developed method by means of several benchmark examples and highlight the advantages over state-of-the-art methods.","authors":["Robin Str\\\"asser","Manuel Schaller","Karl Worthmann","Julian Berberich","Frank Allg\\\"ower"],"url":"https://arxiv.org/abs/2402.03145"}
{"created":"2025-04-28","title":"A Bias-Variance Decomposition for Ensembles over Multiple Synthetic Datasets","abstract":"Recent studies have highlighted the benefits of generating multiple synthetic datasets for supervised learning, from increased accuracy to more effective model selection and uncertainty estimation. These benefits have clear empirical support, but the theoretical understanding of them is currently very light. We seek to increase the theoretical understanding by deriving bias-variance decompositions for several settings of using multiple synthetic datasets, including differentially private synthetic data. Our theory yields a simple rule of thumb to select the appropriate number of synthetic datasets in the case of mean-squared error and Brier score. We investigate how our theory works in practice with several real datasets, downstream predictors and error metrics. As our theory predicts, multiple synthetic datasets often improve accuracy, while a single large synthetic dataset gives at best minimal improvement, showing that our insights are practically relevant.","authors":["Ossi R\\\"ais\\\"a","Antti Honkela"],"url":"https://arxiv.org/abs/2402.03985"}
{"created":"2025-04-28","title":"Mochi: Collision Detection for Spherical Particles using GPU Ray Tracing","abstract":"Efficient Discrete Collision Detection (DCD) uses indexing structures for acceleration, and developing these structures demands meticulous programmer efforts to achieve performance.","authors":["Durga Keerthi Mandarapu","Isaac Fuksman","Artem Pelenitsyn","Gilbert Bernstein","Milind Kulkarni"],"url":"https://arxiv.org/abs/2402.14801"}
{"created":"2025-04-28","title":"Adaptive Boundary Control of the Kuramoto-Sivashinsky Equation Under Intermittent Sensing","abstract":"We study in this paper boundary stabilization, in the L2 sense, of the perturbed Kuramoto-Sivashinsky (KS) equation subject to intermittent sensing. We assume that we measure the state on a given spatial subdomain during certain time intervals, while we measure the state on the remaining spatial subdomain during the remaining time intervals. We assign a feedback law at the boundary of the spatial domain and force to zero the value of the state at the junction of the two subdomains. Throughout the study, the equation's destabilizing coefficient is assumed to be unknown and possibly space dependent but bounded. As a result, adaptive boundary controllers are designed under different assumptions on the perturbation. In particular, we guarantee input-to-state stability (ISS) when an upperbound on the perturbation's size is known. Otherwise, only global uniform ultimate boundedness (GUUB) is guaranteed. In contrast, when the state is measured at every spatial point all the time (full state measurement), convergence to an arbitrarily-small neighborhood of the origin is guaranteed, even if the perturbation's maximal size is unknown. Numerical simulations are performed to illustrate our results.","authors":["Mohamed Camil Belhadjoudja","Mohamed Maghenem","Emmanuel Witrant","Christophe Prieur"],"url":"https://arxiv.org/abs/2403.18055"}
{"created":"2025-04-28","title":"FoC: Figure out the Cryptographic Functions in Stripped Binaries with LLMs","abstract":"Analyzing the behavior of cryptographic functions in stripped binaries is a challenging but essential task. Cryptographic algorithms exhibit greater logical complexity compared to typical code, yet their analysis is unavoidable in areas such as virus analysis and legacy code inspection. Existing methods often rely on data or structural pattern matching, leading to suboptimal generalizability and suffering from manual work. In this paper, we propose a novel framework called FoC to Figure out the Cryptographic functions in stripped binaries. In FoC, we first build a binary large language model (FoC-BinLLM) to summarize the semantics of cryptographic functions in natural language. The prediction of FoC-BinLLM is insensitive to minor changes, such as vulnerability patches. To mitigate it, we further build a binary code similarity model (FoC-Sim) upon the FoC-BinLLM to create change-sensitive representations and use it to retrieve similar implementations of unknown cryptographic functions in a database. In addition, we construct a cryptographic binary dataset for evaluation and to facilitate further research in this domain. And an automated method is devised to create semantic labels for extensive binary functions. Evaluation results demonstrate that FoC-BinLLM outperforms ChatGPT by 14.61% on the ROUGE-L score. FoC-Sim outperforms the previous best methods with a 52% higher Recall@1. Furthermore, our method also shows practical ability in virus analysis and 1-day vulnerability detection.","authors":["Xiuwei Shang","Guoqiang Chen","Shaoyin Cheng","Shikai Guo","Yanming Zhang","Weiming Zhang","Nenghai Yu"],"url":"https://arxiv.org/abs/2403.18403"}
{"created":"2025-04-28","title":"Fast Orthogonal Matching Pursuit through Successive Regression","abstract":"Orthogonal Matching Pursuit (OMP) has been a powerful method in sparse signal recovery and approximation. However, OMP suffers computational issues when the signal has a large number of non-zeros. This paper advances OMP and its extension called generalized OMP (gOMP) by offering fast algorithms for the orthogonal projection of the input signal at each iteration. The proposed modifications directly reduce the computational complexity of OMP and gOMP. Experiment results verified the improvement in computation time. This paper also provides sufficient conditions for exact signal recovery. For general signals with additive noise, the approximation error is at the same order as OMP (gOMP), but is obtained within much less time.","authors":["Huiyuan Yu","Jia He","Maggie Cheng"],"url":"https://arxiv.org/abs/2404.00146"}
{"created":"2025-04-28","title":"PRobELM: Plausibility Ranking Evaluation for Language Models","abstract":"This paper introduces PRobELM (Plausibility Ranking Evaluation for Language Models), a benchmark designed to assess language models' ability to discern more plausible from less plausible scenarios through their parametric knowledge. While benchmarks such as TruthfulQA emphasise factual accuracy or truthfulness, and others such as COPA explore plausible scenarios without explicitly incorporating world knowledge, PRobELM seeks to bridge this gap by evaluating models' capabilities to prioritise plausible scenarios that leverage world knowledge over less plausible alternatives. This design allows us to assess the potential of language models for downstream use cases such as literature-based discovery where the focus is on identifying information that is likely but not yet known. Our benchmark is constructed from a dataset curated from Wikidata edit histories, tailored to align the temporal bounds of the training data for the evaluated models. PRobELM facilitates the evaluation of language models across multiple prompting types, including statement, text completion, and question-answering. Experiments with 10 models of various sizes and architectures on the relationship between model scales, training recency, and plausibility performance, reveal that factual accuracy does not directly correlate with plausibility performance and that up-to-date training data enhances plausibility assessment across different model architectures.","authors":["Zhangdie Yuan","Eric Chamoun","Rami Aly","Chenxi Whitehouse","Andreas Vlachos"],"url":"https://arxiv.org/abs/2404.03818"}
{"created":"2025-04-28","title":"PreGSU-A Generalized Traffic Scene Understanding Model for Autonomous Driving based on Pre-trained Graph Attention Network","abstract":"Scene understanding, defined as learning, extraction, and representation of interactions among traffic elements, is one of the critical challenges toward high-level autonomous driving (AD). Current scene understanding methods mainly focus on one concrete single task, such as trajectory prediction and risk level evaluation. Although they perform well on specific metrics, the generalization ability is insufficient to adapt to the real traffic complexity and downstream demand diversity. In this study, we propose PreGSU, a generalized pre-trained scene understanding model based on graph attention network to learn the universal interaction and reasoning of traffic scenes to support various downstream tasks. After the feature engineering and sub-graph module, all elements are embedded as nodes to form a dynamic weighted graph. Then, four graph attention layers are applied to learn the relationships among agents and lanes. In the pre-train phase, the understanding model is trained on two self-supervised tasks: Virtual Interaction Force (VIF) modeling and Masked Road Modeling (MRM). Based on the artificial potential field theory, VIF modeling enables PreGSU to capture the agent-to-agent interactions while MRM extracts agent-to-road connections. In the fine-tuning process, the pre-trained parameters are loaded to derive detailed understanding outputs. We conduct validation experiments on three datasets and two downstream tasks, i.e., trajectory prediction in urban scenario and intention recognition in highway scenario, to verify the model's generalization and understanding capabilities. Results show that compared with single-task-driven baselines, PreGSU achieves competitive performance on all datasets and downstream tasks, indicating its potential to be generalized to various scenes and targets. Ablation study shows the effectiveness of pre-train task design.","authors":["Yuning Wang","Zhiyuan Liu","Haotian Lin","Junkai Jiang","Shaobing Xu","Jianqiang Wang"],"url":"https://arxiv.org/abs/2404.10263"}
{"created":"2025-04-28","title":"Accelerating Particle-in-Cell Monte Carlo Simulations with MPI, OpenMP/OpenACC and Asynchronous Multi-GPU Programming","abstract":"As fusion energy devices advance, plasma simulations are crucial for reactor design. Our work extends BIT1 hybrid parallelization by integrating MPI with OpenMP and OpenACC, focusing on asynchronous multi-GPU programming. Results show significant performance gains: 16 MPI ranks plus OpenMP threads reduced runtime by 53% on a petascale EuroHPC supercomputer, while OpenACC multicore achieved a 58% reduction. At 64 MPI ranks, OpenACC outperformed OpenMP, improving the particle mover function by 24%. On MareNostrum 5, OpenACC async(n) delivered strong performance, but OpenMP asynchronous multi-GPU approach proved more effective at extreme scaling, maintaining efficiency up to 400 GPUs. Speedup and parallel efficiency (PE) studies revealed OpenMP asynchronous multi-GPU achieving 8.77x speedup (54.81% PE), surpassing OpenACC (8.14x speedup, 50.87% PE). While PE declined at high node counts due to communication overhead, asynchronous execution mitigated scalability bottlenecks. OpenMP nowait and depend clauses improved GPU performance via efficient data transfer and task management. Using NVIDIA Nsight tools, we confirmed BIT1 efficiency for large-scale plasma simulations. OpenMP asynchronous multi-GPU implementation delivered exceptional performance in portability, high throughput, and GPU utilization, positioning BIT1 for exascale supercomputing and advancing fusion energy research. MareNostrum 5 brings us closer to achieving exascale performance.","authors":["Jeremy J. Williams","Felix Liu","Jordy Trilaksono","David Tskhakaya","Stefan Costea","Leon Kos","Ales Podolnik","Jakub Hromadka","Pratibha Hegde","Marta Garcia-Gasulla","Valentin Seitz","Frank Jenko","Erwin Laure","Stefano Markidis"],"url":"https://arxiv.org/abs/2404.10270"}
{"created":"2025-04-28","title":"A Dual Perspective of Reinforcement Learning for Imposing Policy Constraints","abstract":"Model-free reinforcement learning methods lack an inherent mechanism to impose behavioural constraints on the trained policies. Although certain extensions exist, they remain limited to specific types of constraints, such as value constraints with additional reward signals or visitation density constraints. In this work we unify these existing techniques and bridge the gap with classical optimization and control theory, using a generic primal-dual framework for value-based and actor-critic reinforcement learning methods. The obtained dual formulations turn out to be especially useful for imposing additional constraints on the learned policy, as an intrinsic relationship between such dual constraints (or regularization terms) and reward modifications in the primal is revealed. Furthermore, using this framework, we are able to introduce some novel types of constraints, allowing to impose bounds on the policy's action density or on costs associated with transitions between consecutive states and actions. From the adjusted primal-dual optimization problems, a practical algorithm is derived that supports various combinations of policy constraints that are automatically handled throughout training using trainable reward modifications. The proposed $\\texttt{DualCRL}$ method is examined in more detail and evaluated under different (combinations of) constraints on two interpretable environments. The results highlight the efficacy of the method, which ultimately provides the designer of such systems with a versatile toolbox of possible policy constraints.","authors":["Bram De Cooman","Johan Suykens"],"url":"https://arxiv.org/abs/2404.16468"}
{"created":"2025-04-28","title":"On the Loewner framework, the Kolmogorov superposition theorem, and the curse of dimensionality","abstract":"The Loewner framework is an interpolatory approach for the approximation of linear and nonlinear systems. The purpose here is to extend this framework to linear parametric systems with an arbitrary number n of parameters. To achieve this, a new generalized multivariate rational function realization is proposed. Then, we introduce the n-dimensional multivariate Loewner matrices and show that they can be computed by solving a set of coupled Sylvester equations. The null space of these Loewner matrices allows the construction of the multivariate barycentric rational function. The principal result of this work is to show how the null space of the n-dimensional Loewner matrix can be computed using a sequence of 1-dimensional Loewner matrices, leading to a drastic reduction of the computational burden. Equally importantly, this burden is alleviated by avoiding the explicit construction of large-scale n-dimensional Loewner matrices of size $N \\times N$. Instead, the proposed methodology achieves decoupling of variables, leading to (i) a complexity reduction from $O(N^3)$ to below $O(N^{1.5})$ when $n > 5$ and (ii) to memory storage bounded by the largest variable dimension rather than their product, thus taming the curse of dimensionality and making the solution scalable to very large data sets. This decoupling of the variables leads to a result similar to the Kolmogorov superposition theorem for rational functions. Thus, making use of barycentric representations, every multivariate rational function can be computed using the composition and superposition of single-variable functions. Finally, we suggest two algorithms (one direct and one iterative) to construct, directly from data, multivariate (or parametric) realizations ensuring (approximate) interpolation. Numerical examples highlight the effectiveness and scalability of the method.","authors":["Athanasios C. Antoulas","Ion Victor Gosea","Charles Poussot-Vassal"],"url":"https://arxiv.org/abs/2405.00495"}
{"created":"2025-04-28","title":"Decoding complexity: how machine learning is redefining scientific discovery","abstract":"As modern scientific instruments generate vast amounts of data and the volume of information in the scientific literature continues to grow, machine learning (ML) has become an essential tool for organising, analysing, and interpreting these complex datasets. This paper explores the transformative role of ML in accelerating breakthroughs across a range of scientific disciplines. By presenting key examples -- such as brain mapping and exoplanet detection -- we demonstrate how ML is reshaping scientific research. We also explore different scenarios where different levels of knowledge of the underlying phenomenon are available, identifying strategies to overcome limitations and unlock the full potential of ML. Despite its advances, the growing reliance on ML poses challenges for research applications and rigorous validation of discoveries. We argue that even with these challenges, ML is poised to disrupt traditional methodologies and advance the boundaries of knowledge by enabling researchers to tackle increasingly complex problems. Thus, the scientific community can move beyond the necessary traditional oversimplifications to embrace the full complexity of natural systems, ultimately paving the way for interdisciplinary breakthroughs and innovative solutions to humanity's most pressing challenges.","authors":["Ricardo Vinuesa","Paola Cinnella","Jean Rabault","Hossein Azizpour","Stefan Bauer","Bingni W. Brunton","Arne Elofsson","Elias Jarlebring","Hedvig Kjellstrom","Stefano Markidis","David Marlevi","Javier Garcia-Martinez","Steven L. Brunton"],"url":"https://arxiv.org/abs/2405.04161"}
{"created":"2025-04-28","title":"Rapid modelling of reactive transport in porous media using machine learning: limitations and solutions","abstract":"Reactive transport in porous media plays a pivotal role in subsurface reservoir processes, influencing fluid properties and geochemical characteristics. However, coupling fluid flow and transport with geochemical reactions is computationally intensive, requiring geochemical calculations at each grid cell and each time step within a discretized simulation domain. Although recent advancements have integrated machine learning techniques as surrogates for geochemical simulations, ensuring computational efficiency and accuracy remains a challenge. This work investigates machine learning models as replacements for a geochemical module in a simulation of reactive transport in porous media. As a proof of concept, we test this approach on a well-documented cation exchange problem. While the surrogate models excel in isolated predictions, they fall short in rollout predictions over successive time steps. By introducing modifications, including physics-based constraints and tailored dataset generation strategies, we show that machine learning surrogates can achieve accurate rollout predictions. Our findings emphasize that even for a simple sorption equilibrium reaction (cation exchange problem), machine learning surrogates alone fail in predicting over successive time-steps. Incorporating simple physics-based modifications enables us to overcome this limitation. A detailed analysis of the limitations and potential mitigation strategies is presented in this work.","authors":["Vinicius L S Silva","Geraldine Regnier","Pablo Salinas","Claire E Heaney","Matthew D Jackson","Christopher C Pain"],"url":"https://arxiv.org/abs/2405.14548"}
{"created":"2025-04-28","title":"M4U: Evaluating Multilingual Understanding and Reasoning for Large Multimodal Models","abstract":"Multilingual capability is an essential aspect for large multimodal models, since they are usually deployed across various countries and languages. However, most existing benchmarks for multilingual multimodal reasoning struggle to differentiate between models of varying performance; even language models without visual capabilities can easily achieve high scores. This leaves a comprehensive evaluation of leading multilingual multimodal models largely unexplored. In this work, we introduce M4U, a novel and challenging benchmark for assessing the capability of multi-discipline multilingual multimodal understanding and reasoning. M4U contains 10k samples covering 64 disciplines across 16 subfields in Science, Engineering, and Healthcare in six languages. Using M4U, we conduct extensive evaluations of leading Large Multimodal Models (LMMs) and Large Language Models (LLMs) with external tools. The evaluation results demonstrate that the state-of-the-art model, GPT-4o, achieves only 47.6% average accuracy on M4U. Additionally, we observe that the leading LMMs exhibit significant language preferences. Our in-depth analysis indicates that leading LMMs, including GPT-4o, struggle to perform reasoning using multilingual information present in both visual and textual context. Specifically, they suffer performance degradation when prompted with cross-lingual multimodal questions. Our code and dataset is public available.","authors":["Hongyu Wang","Jiayu Xu","Senwei Xie","Ruiping Wang","Jialin Li","Zhaojie Xie","Bin Zhang","Chuyan Xiong","Xilin Chen"],"url":"https://arxiv.org/abs/2405.15638"}
{"created":"2025-04-28","title":"CoCoGesture: Toward Coherent Co-speech 3D Gesture Generation in the Wild","abstract":"Deriving co-speech 3D gestures has seen tremendous progress in virtual avatar animation. Yet, the existing methods often produce stiff and unreasonable gestures with unseen human speech inputs due to the limited 3D speech-gesture data. In this paper, we propose CoCoGesture, a novel framework enabling vivid and diverse gesture synthesis from unseen human speech prompts. Our key insight is built upon the custom-designed pretrain-fintune training paradigm. At the pretraining stage, we aim to formulate a large generalizable gesture diffusion model by learning the abundant postures manifold. Therefore, to alleviate the scarcity of 3D data, we first construct a large-scale co-speech 3D gesture dataset containing more than 40M meshed posture instances across 4.3K speakers, dubbed GES-X. Then, we scale up the large unconditional diffusion model to 1B parameters and pre-train it to be our gesture experts. At the finetune stage, we present the audio ControlNet that incorporates the human voice as condition prompts to guide the gesture generation. Here, we construct the audio ControlNet through a trainable copy of our pre-trained diffusion model. Moreover, we design a novel Mixture-of-Gesture-Experts (MoGE) block to adaptively fuse the audio embedding from the human speech and the gesture features from the pre-trained gesture experts with a routing mechanism. Such an effective manner ensures audio embedding is temporal coordinated with motion features while preserving the vivid and diverse gesture generation. Extensive experiments demonstrate that our proposed CoCoGesture outperforms the state-of-the-art methods on the zero-shot speech-to-gesture generation. The dataset will be publicly available at: https://mattie-e.github.io/GES-X/","authors":["Xingqun Qi","Hengyuan Zhang","Yatian Wang","Jiahao Pan","Chen Liu","Peng Li","Xiaowei Chi","Mengfei Li","Wei Xue","Shanghang Zhang","Wenhan Luo","Qifeng Liu","Yike Guo"],"url":"https://arxiv.org/abs/2405.16874"}
{"created":"2025-04-28","title":"Nearest Neighbor Speculative Decoding for LLM Generation and Attribution","abstract":"Large language models (LLMs) often hallucinate and lack the ability to provide attribution for their generations. Semi-parametric LMs, such as kNN-LM, approach these limitations by refining the output of an LM for a given prompt using its nearest neighbor matches in a non-parametric data store. However, these models often exhibit slow inference speeds and produce non-fluent texts. In this paper, we introduce Nearest Neighbor Speculative Decoding (NEST), a novel semi-parametric language modeling approach that is capable of incorporating real-world text spans of arbitrary length into the LM generations and providing attribution to their sources. NEST performs token-level retrieval at each inference step to compute a semi-parametric mixture distribution and identify promising span continuations in a corpus. It then uses an approximate speculative decoding procedure that accepts a prefix of the retrieved span or generates a new token. NEST significantly enhances the generation quality and attribution rate of the base LM across a variety of knowledge-intensive tasks, surpassing the conventional kNN-LM method and performing competitively with in-context retrieval augmentation. In addition, NEST substantially improves the generation speed, achieving a 1.8x speedup in inference time when applied to Llama-2-Chat 70B. Code will be released at https://github.com/facebookresearch/NEST/tree/main.","authors":["Minghan Li","Xilun Chen","Ari Holtzman","Beidi Chen","Jimmy Lin","Wen-tau Yih","Xi Victoria Lin"],"url":"https://arxiv.org/abs/2405.19325"}
{"created":"2025-04-28","title":"RLeXplore: Accelerating Research in Intrinsically-Motivated Reinforcement Learning","abstract":"Extrinsic rewards can effectively guide reinforcement learning (RL) agents in specific tasks. However, extrinsic rewards frequently fall short in complex environments due to the significant human effort needed for their design and annotation. This limitation underscores the necessity for intrinsic rewards, which offer auxiliary and dense signals and can enable agents to learn in an unsupervised manner. Although various intrinsic reward formulations have been proposed, their implementation and optimization details are insufficiently explored and lack standardization, thereby hindering research progress. To address this gap, we introduce RLeXplore, a unified, highly modularized, and plug-and-play framework offering reliable implementations of eight state-of-the-art intrinsic reward methods. Furthermore, we conduct an in-depth study that identifies critical implementation details and establishes well-justified standard practices in intrinsically-motivated RL. Our documentation, examples, and source code are available at https://github.com/RLE-Foundation/RLeXplore.","authors":["Mingqi Yuan","Roger Creus Castanyer","Bo Li","Xin Jin","Wenjun Zeng","Glen Berseth"],"url":"https://arxiv.org/abs/2405.19548"}
{"created":"2025-04-28","title":"Neural Combinatorial Optimization Algorithms for Solving Vehicle Routing Problems: A Comprehensive Survey with Perspectives","abstract":"Although several surveys on Neural Combinatorial Optimization (NCO) solvers specifically designed to solve Vehicle Routing Problems (VRPs) have been conducted, they did not cover the state-of-the-art (SOTA) NCO solvers emerged recently. More importantly, to establish a comprehensive and up-to-date taxonomy of NCO solvers, we systematically review relevant publications and preprints, categorizing them into four distinct types, namely Learning to Construct, Learning to Improve, Learning to Predict-Once, and Learning to Predict-Multiplicity solvers. Subsequently, we present the inadequacies of the SOTA solvers, including poor generalization, incapability to solve large-scale VRPs, inability to address most types of VRP variants simultaneously, and difficulty in comparing these NCO solvers with the conventional Operations Research algorithms. Simultaneously, we discuss on-going efforts, identify open inadequacies, as well as propose promising and viable directions to overcome these inadequacies. Notably, existing efforts focus on only one or two of these inadequacies, with none attempting to address all of them concurrently. In addition, we compare the performance of representative NCO solvers from the Reinforcement, Supervised, and Unsupervised Learning paradigms across VRPs of varying scales. Finally, following the proposed taxonomy, we provide an accompanying web page as a live repository for NCO solvers. Through this survey and the live repository, we aim to foster further advancements in the NCO community.","authors":["Xuan Wu","Di Wang","Lijie Wen","Yubin Xiao","Chunguo Wu","Yuesong Wu","Chaoyu Yu","Douglas L. Maskell","You Zhou"],"url":"https://arxiv.org/abs/2406.00415"}
{"created":"2025-04-28","title":"Can Kernel Methods Explain How the Data Affects Neural Collapse?","abstract":"A vast amount of literature has recently focused on the \"Neural Collapse\" (NC) phenomenon, which emerges when training neural network (NN) classifiers beyond the zero training error point. The core component of NC is the decrease in the within-class variability of the network's deepest features, dubbed as NC1. The theoretical works that study NC are typically based on simplified unconstrained features models (UFMs) that mask any effect of the data on the extent of collapse. To address this limitation of UFMs, this paper explores the possibility of analyzing NC1 using kernels associated with shallow NNs. We begin by formulating an NC1 metric as a function of the kernel. Then, we specialize it to the NN Gaussian Process kernel (NNGP) and the Neural Tangent Kernel (NTK), associated with wide networks at initialization and during gradient-based training with a small learning rate, respectively. As a key result, we show that the NTK does not represent more collapsed features than the NNGP for Gaussian data of arbitrary dimensions. This showcases the limitations of data-independent kernels such as NTK in approximating the NC behavior of NNs. As an alternative to NTK, we then empirically explore a recently proposed data-aware Gaussian Process kernel, which generalizes NNGP to model feature learning. We show that this kernel yields lower NC1 than NNGP but may not follow the trends of the shallow NN. Our study demonstrates that adaptivity to data may allow kernel-based analysis of NC, though further advancements in this area are still needed. A nice byproduct of our study is showing both theoretically and empirically that the choice of nonlinear activation function affects NC1 (with ERF yielding lower values than ReLU). The code is available at: https://github.com/kvignesh1420/shallow_nc1","authors":["Vignesh Kothapalli","Tom Tirer"],"url":"https://arxiv.org/abs/2406.02105"}
{"created":"2025-04-28","title":"MAP: Low-compute Model Merging with Amortized Pareto Fronts via Quadratic Approximation","abstract":"Model merging has emerged as an effective approach to combine multiple single-task models into a multitask model. This process typically involves computing a weighted average of the model parameters without any additional training. Existing model-merging methods focus on enhancing average task accuracy. However, interference and conflicts between the objectives of different tasks can lead to trade-offs during the merging process. In real-world applications, a set of solutions with various trade-offs can be more informative, helping practitioners make decisions based on diverse preferences. In this paper, we introduce a novel and low-compute algorithm, Model Merging with Amortized Pareto Front (MAP). MAP efficiently identifies a Pareto set of scaling coefficients for merging multiple models, reflecting the trade-offs involved. It amortizes the substantial computational cost of evaluations needed to estimate the Pareto front by using quadratic approximation surrogate models derived from a pre-selected set of scaling coefficients. Experimental results on vision and natural language processing tasks demonstrate that MAP can accurately identify the Pareto front, providing practitioners with flexible solutions to balance competing task objectives. We also introduce Bayesian MAP for scenarios with a relatively low number of tasks and Nested MAP for situations with a high number of tasks, further reducing the computational cost of evaluation.","authors":["Lu Li","Tianyu Zhang","Zhiqi Bu","Suyuchen Wang","Huan He","Jie Fu","Yonghui Wu","Jiang Bian","Yong Chen","Yoshua Bengio"],"url":"https://arxiv.org/abs/2406.07529"}
{"created":"2025-04-28","title":"PRIMER: Perception-Aware Robust Learning-based Multiagent Trajectory Planner","abstract":"In decentralized multiagent trajectory planners, agents need to communicate and exchange their positions to generate collision-free trajectories. However, due to localization errors/uncertainties, trajectory deconfliction can fail even if trajectories are perfectly shared between agents. To address this issue, we first present PARM and PARM*, perception-aware, decentralized, asynchronous multiagent trajectory planners that enable a team of agents to navigate uncertain environments while deconflicting trajectories and avoiding obstacles using perception information. PARM* differs from PARM as it is less conservative, using more computation to find closer-to-optimal solutions. While these methods achieve state-of-the-art performance, they suffer from high computational costs as they need to solve large optimization problems onboard, making it difficult for agents to replan at high rates. To overcome this challenge, we present our second key contribution, PRIMER, a learning-based planner trained with imitation learning (IL) using PARM* as the expert demonstrator. PRIMER leverages the low computational requirements at deployment of neural networks and achieves a computation speed up to 5500 times faster than optimization-based approaches.","authors":["Kota Kondo","Claudius T. Tewari","Andrea Tagliabue","Jesus Tordesillas","Parker C. Lusk","Mason B. Peterson","Jonathan P. How"],"url":"https://arxiv.org/abs/2406.10060"}
{"created":"2025-04-28","title":"Papers-to-Posts: Supporting Detailed Long-Document Summarization with an Interactive LLM-Powered Source Outline","abstract":"Compressing long and technical documents (e.g., >10 pages) into shorter-form articles (e.g., <2 pages) is critical for communicating information to different audiences, for example, blog posts of scientific research paper or legal briefs of dense court proceedings. While large language models (LLMs) are powerful tools for condensing large amounts of text, current interfaces to these models lack support for understanding and controlling what content is included in a detailed summarizing article. Such capability is especially important for detail- and technical-oriented domains, in which tactical selection and coherent synthesis of key details is critical for effective communication to the target audience. For this, we present interactive reverse source outlines, a novel mechanism for controllable long-form summarization featuring outline bullet points with automatic point selections that the user can iteratively adjust to obtain an article with the desired content coverage. We implement this mechanism in Papers-to-Posts, a new LLM-powered system for authoring research-paper blog posts. Through a within-subjects lab study (n=20) and a between-subjects deployment study (n=37 blog posts, 26 participants), we compare Papers-to-Posts to a strong baseline tool that provides an LLM-generated draft and access to free-form prompting. Under time constraints, Papers-to-Posts significantly increases writer satisfaction with blog post quality, particularly with respect to content coverage. Furthermore, quantitative results showed an increase in editing power (change in text for an amount of time or writing actions) while using Papers-to-Posts, and qualitative results showed that participants found incorporating key research-paper insights in their blog posts easier while using Papers-to-Posts.","authors":["Marissa Radensky","Daniel S. Weld","Joseph Chee Chang","Pao Siangliulue","Jonathan Bragg"],"url":"https://arxiv.org/abs/2406.10370"}
{"created":"2025-04-28","title":"AMR-RE: Abstract Meaning Representations for Retrieval-Based In-Context Learning in Relation Extraction","abstract":"Existing in-context learning (ICL) methods for relation extraction (RE) often prioritize language similarity over structural similarity, which can lead to overlooking entity relationships. To address this, we propose an AMR-enhanced retrieval-based ICL method for RE. Our model retrieves in-context examples based on semantic structure similarity between task inputs and training samples. Evaluations on four standard English RE datasets show that our model outperforms baselines in the unsupervised setting across all datasets. In the supervised setting, it achieves state-of-the-art results on three datasets and competitive results on the fourth.","authors":["Peitao Han","Lis Kanashiro Pereira","Fei Cheng","Wan Jou She","Eiji Aramaki"],"url":"https://arxiv.org/abs/2406.10432"}
{"created":"2025-04-28","title":"Multilingual Large Language Models and Curse of Multilinguality","abstract":"Multilingual Large Language Models (LLMs) have gained large popularity among Natural Language Processing (NLP) researchers and practitioners. These models, trained on huge datasets, show proficiency across various languages and demonstrate effectiveness in numerous downstream tasks. This paper navigates the landscape of multilingual LLMs, providing an introductory overview of their technical aspects. It explains underlying architectures, objective functions, pre-training data sources, and tokenization methods. This work explores the unique features of different model types: encoder-only (mBERT, XLM-R), decoder-only (XGLM, PALM, BLOOM, GPT-3), and encoder-decoder models (mT5, mBART). Additionally, it addresses one of the significant limitations of multilingual LLMs - the curse of multilinguality - and discusses current attempts to overcome it.","authors":["Daniil Gurgurov","Tanja B\\\"aumel","Tatiana Anikina"],"url":"https://arxiv.org/abs/2406.10602"}
{"created":"2025-04-28","title":"Trading Devil: Robust backdoor attack via Stochastic investment models and Bayesian approach","abstract":"With the growing use of voice-activated systems and speech recognition technologies, the danger of backdoor attacks on audio data has grown significantly. This research looks at a specific type of attack, known as a Stochastic investment-based backdoor attack (MarketBack), in which adversaries strategically manipulate the stylistic properties of audio to fool speech recognition systems. The security and integrity of machine learning models are seriously threatened by backdoor attacks, in order to maintain the reliability of audio applications and systems, the identification of such attacks becomes crucial in the context of audio data. Experimental results demonstrated that MarketBack is feasible to achieve an average attack success rate close to 100% in seven victim models when poisoning less than 1% of the training data.","authors":["Orson Mengara"],"url":"https://arxiv.org/abs/2406.10719"}
{"created":"2025-04-28","title":"Automatically Generating UI Code from Screenshot: A Divide-and-Conquer-Based Approach","abstract":"Websites are critical in today's digital world, with over 1.11 billion currently active and approximately 252,000 new sites launched daily. Converting website layout design into functional UI code is a time-consuming yet indispensable step of website development. Manual methods of converting visual designs into functional code present significant challenges, especially for non-experts. To explore automatic design-to-code solutions, we first conduct a motivating study on GPT-4o and identify three types of issues in generating UI code: element omission, element distortion, and element misarrangement. We further reveal that a focus on smaller visual segments can help multimodal large language models (MLLMs) mitigate these failures in the generation process.","authors":["Yuxuan Wan","Chaozheng Wang","Yi Dong","Wenxuan Wang","Shuqing Li","Yintong Huo","Michael R. Lyu"],"url":"https://arxiv.org/abs/2406.16386"}
{"created":"2025-04-28","title":"Towards Synchronous Memorizability and Generalizability with Site-Modulated Diffusion Replay for Cross-Site Continual Segmentation","abstract":"The ability to learn sequentially from different data sites is crucial for a deep network in solving practical medical image diagnosis problems due to privacy restrictions and storage limitations. However, adapting on incoming site leads to catastrophic forgetting on past sites and decreases generalizablity on unseen sites. Existing Continual Learning (CL) and Domain Generalization (DG) methods have been proposed to solve these two challenges respectively, but none of them can address both simultaneously. Recognizing this limitation, this paper proposes a novel training paradigm, learning towards Synchronous Memorizability and Generalizability (SMG-Learning). To achieve this, we create the orientational gradient alignment to ensure memorizability on previous sites, and arbitrary gradient alignment to enhance generalizability on unseen sites. This approach is named as Parallel Gradient Alignment (PGA). Furthermore, we approximate the PGA as dual meta-objectives using the first-order Taylor expansion to reduce computational cost of aligning gradients. Considering that performing gradient alignments, especially for previous sites, is not feasible due to the privacy constraints, we design a Site-Modulated Diffusion (SMD) model to generate images with site-specific learnable prompts, replaying images have similar data distributions as previous sites. We evaluate our method on two medical image segmentation tasks, where data from different sites arrive sequentially. Experimental results show that our method efficiently enhances both memorizability and generalizablity better than other state-of-the-art methods, delivering satisfactory performance across all sites. Our code will be available at: https://github.com/dyxu-cuhkcse/SMG-Learning.","authors":["Dunyuan Xu","Xi Wang","Jingyang Zhang","Pheng-Ann Heng"],"url":"https://arxiv.org/abs/2406.18037"}
{"created":"2025-04-28","title":"MCNC: Manifold-Constrained Reparameterization for Neural Compression","abstract":"The outstanding performance of large foundational models across diverse tasks, from computer vision to speech and natural language processing, has significantly increased their demand. However, storing and transmitting these models poses significant challenges due to their massive size (e.g., 750GB for Llama 3.1 405B). Recent literature has focused on compressing the original weights or reducing the number of parameters required for fine-tuning these models. These compression methods generally constrain the parameter space, for example, through low-rank reparametrization (e.g., LoRA), pruning, or quantization (e.g., QLoRA) during or after the model training. In this paper, we present a novel model compression method, which we term Manifold-Constrained Neural Compression (MCNC). This method constrains the parameter space to low-dimensional pre-defined and frozen nonlinear manifolds, which effectively cover this space. Given the prevalence of good solutions in over-parameterized deep neural networks, we show that by constraining the parameter space to our proposed manifold, we can identify high-quality solutions while achieving unprecedented compression rates across a wide variety of tasks and architectures. Through extensive experiments in computer vision and natural language processing tasks, we demonstrate that our method significantly outperforms state-of-the-art baselines in terms of compression, accuracy, and/or model reconstruction time. Our code is publicly available at https://github.com/mint-vu/MCNC.","authors":["Chayne Thrash","Ali Abbasi","Reed Andreas","Parsa Nooralinejad","Soroush Abbasi Koohpayegani","Hamed Pirsiavash","Soheil Kolouri"],"url":"https://arxiv.org/abs/2406.19301"}
{"created":"2025-04-28","title":"GOFA: A Generative One-For-All Model for Joint Graph Language Modeling","abstract":"Foundation models, such as Large Language Models (LLMs) or Large Vision Models (LVMs), have emerged as one of the most powerful tools in the respective fields. However, unlike text and image data, graph data do not have a definitive structure, posing great challenges to developing a Graph Foundation Model (GFM). For example, current attempts at designing general graph models either transform graph data into a language format for LLM-based prediction or still train a GNN model with LLM as an assistant. The former can handle unlimited tasks, while the latter captures graph structure much better -- yet, no existing work can achieve both simultaneously. In this paper, we identify three key desirable properties of a GFM: self-supervised pretraining, fluidity in tasks, and graph awareness. To account for these properties, we extend the conventional language modeling to the graph domain and propose a novel generative graph language model GOFA to solve the problem. The model interleaves randomly initialized GNN layers into a frozen pre-trained LLM so that the semantic and structural modeling abilities are organically combined. GOFA is pre-trained on newly proposed graph-level next-word prediction, question-answering, and structural tasks to obtain the above GFM properties. The pre-trained model is further fine-tuned on downstream tasks to obtain task-solving ability. The fine-tuned model is evaluated on various downstream tasks, demonstrating a strong ability to solve structural and contextual problems in zero-shot scenarios. The code is available at https://github.com/JiaruiFeng/GOFA.","authors":["Lecheng Kong","Jiarui Feng","Hao Liu","Chengsong Huang","Jiaxin Huang","Yixin Chen","Muhan Zhang"],"url":"https://arxiv.org/abs/2407.09709"}
{"created":"2025-04-28","title":"Leave-One-Out Analysis for Nonconvex Robust Matrix Completion with General Thresholding Functions","abstract":"We study the problem of robust matrix completion (RMC), where the partially observed entries of an underlying low-rank matrix is corrupted by sparse noise. Existing analysis of the non-convex methods for this problem either requires the explicit but empirically redundant regularization in the algorithm or requires sample splitting in the analysis. In this paper, we consider a simple yet efficient nonconvex method which alternates between a projected gradient step for the low-rank part and a thresholding step for the sparse noise part. Inspired by leave-one out analysis for low rank matrix completion, it is established that the method can achieve linear convergence for a general class of thresholding functions, including for example soft-thresholding and SCAD. To the best of our knowledge, this is the first leave-one-out analysis on a nonconvex method for RMC. Additionally, when applying our result to low rank matrix completion, it improves the sampling complexity of existing result for the singular value projection method.","authors":["Tianming Wang","Ke Wei"],"url":"https://arxiv.org/abs/2407.19446"}
{"created":"2025-04-28","title":"AutoFL: A Tool for Automatic Multi-granular Labelling of Software Repositories","abstract":"Software comprehension, especially of new code bases, is time consuming for developers, especially in large projects with multiple functionalities spanning various domains. One strategy to reduce this effort involves annotating files with meaningful labels that describe the functionalities contained. However, prior research has so far focused on classifying the whole project using README files as a proxy, resulting in little information gained for the developers.","authors":["Cezar Sas","Andrea Capiluppi"],"url":"https://arxiv.org/abs/2408.02557"}
{"created":"2025-04-28","title":"The Evolution of Information Seeking in Software Development: Understanding the Role and Impact of AI Assistants","abstract":"About 32% of a software practitioners' day involves seeking and using information to support task completion. Although the information needs of software practitioners have been studied extensively, the impact of AI-assisted tools on their needs and information-seeking behaviors remains largely unexplored. To addresses this gap, we conducted a mixed-method study to understand AI-assisted information seeking behavior of practitioners and its impact on their perceived productivity and skill development. We found that developers are increasingly using AI tools to support their information seeking, citing increased efficiency as a key benefit. Our findings also amplify caveats that come with effectively using AI tools for information seeking, especially for learning and skill development, such as the importance of foundational developer knowledge that can guide and inform the information provided by AI tools. Our efforts have implications for the effective integration of AI tools into developer workflows as information retrieval systems and learning aids.","authors":["Ebtesam Al Haque","Chris Brown","Thomas D. LaToza","Brittany Johnson"],"url":"https://arxiv.org/abs/2408.04032"}
{"created":"2025-04-28","title":"Activation degree thresholds and expressiveness of polynomial neural networks","abstract":"We study the expressive power of deep polynomial neural networks through the geometry of their neurovariety. We introduce the notion of the activation degree threshold of a network architecture to express when the dimension of the neurovariety achieves its theoretical maximum. We prove the existence of the activation degree threshold for all polynomial neural networks without width-one bottlenecks and demonstrate a universal upper bound that is quadratic in the width of largest size. In doing so, we prove the high activation degree conjecture of Kileel, Trager, and Bruna. Certain structured architectures have exceptional activation degree thresholds, making them especially expressive in the sense of their neurovariety dimension. In this direction, we prove that polynomial neural networks with equi-width architectures are maximally expressive by showing their activation degree threshold is one.","authors":["Bella Finkel","Jose Israel Rodriguez","Chenxi Wu","Thomas Yahl"],"url":"https://arxiv.org/abs/2408.04569"}
{"created":"2025-04-28","title":"Review-driven Personalized Preference Reasoning with Large Language Models for Recommendation","abstract":"Recent advancements in Large Language Models (LLMs) have demonstrated exceptional performance across a wide range of tasks, generating significant interest in their application to recommendation systems. However, existing methods have not fully capitalized on the potential of LLMs, often constrained by limited input information or failing to fully utilize their advanced reasoning capabilities. To address these limitations, we introduce EXP3RT, a novel LLM-based recommender designed to leverage rich preference information contained in user and item reviews. EXP3RT is basically fine-tuned through distillation from a teacher LLM to perform three key tasks in order: EXP3RT first extracts and encapsulates essential subjective preferences from raw reviews, aggregates and summarizes them according to specific criteria to create user and item profiles. It then generates detailed step-by-step reasoning followed by predicted rating, i.e., reasoning-enhanced rating prediction, by considering both subjective and objective information from user/item profiles and item descriptions. This personalized preference reasoning from EXP3RT enhances rating prediction accuracy and also provides faithful and reasonable explanations for recommendation. Extensive experiments show that EXP3RT outperforms existing methods on both rating prediction and candidate item reranking for top-k recommendation, while significantly enhancing the explainability of recommendation systems.","authors":["Jieyong Kim","Hyunseo Kim","Hyunjin Cho","SeongKu Kang","Buru Chang","Jinyoung Yeo","Dongha Lee"],"url":"https://arxiv.org/abs/2408.06276"}
{"created":"2025-04-28","title":"Towards Robust and Parameter-Efficient Knowledge Unlearning for LLMs","abstract":"Large Language Models (LLMs) have demonstrated strong reasoning and memorization capabilities via pretraining on massive textual corpora. However, this poses risk of privacy and copyright violations, highlighting the need for efficient machine unlearning methods that remove sensitive data without retraining from scratch. While Gradient Ascent (GA) is commonly used to unlearn by reducing the likelihood of generating unwanted content, it leads to unstable optimization and catastrophic forgetting of retrained knowledge. We find that combining GA with low-rank adaptation results in poor trade-offs between computational cost and generative performance. To address these challenges, we propose Low-rank Knowledge Unlearning (LoKU), a novel framework that enables robust and efficient unlearning for LLMs. First, we introduce Inverted Hinge Loss, which suppresses unwanted tokens while maintaining fluency by boosting the probability of the next most likely token. Second, we develop a data-adaptive initialization for LoRA adapters via low-rank approximation weighted with relative Fisher information, thereby focusing updates on parameters critical for removing targeted knowledge. Experiments on the Training Data Extraction Challenge dataset using GPT-Neo models as well as on the TOFU benchmark with Phi-1.5B and Llama2-7B models demonstrate that our approach effectively removes sensitive information while maintaining reasoning and generative capabilities with minimal impact. Our implementation can be found in https://github.com/csm9493/efficient-llm-unlearning.","authors":["Sungmin Cha","Sungjun Cho","Dasol Hwang","Moontae Lee"],"url":"https://arxiv.org/abs/2408.06621"}
{"created":"2025-04-28","title":"Multi-view Hand Reconstruction with a Point-Embedded Transformer","abstract":"This work introduces a novel and generalizable multi-view Hand Mesh Reconstruction (HMR) model, named POEM, designed for practical use in real-world hand motion capture scenarios. The advances of the POEM model consist of two main aspects. First, concerning the modeling of the problem, we propose embedding a static basis point within the multi-view stereo space. A point represents a natural form of 3D information and serves as an ideal medium for fusing features across different views, given its varied projections across these views. Consequently, our method harnesses a simple yet effective idea: a complex 3D hand mesh can be represented by a set of 3D basis points that 1) are embedded in the multi-view stereo, 2) carry features from the multi-view images, and 3) encompass the hand in it. The second advance lies in the training strategy. We utilize a combination of five large-scale multi-view datasets and employ randomization in the number, order, and poses of the cameras. By processing such a vast amount of data and a diverse array of camera configurations, our model demonstrates notable generalizability in the real-world applications. As a result, POEM presents a highly practical, plug-and-play solution that enables user-friendly, cost-effective multi-view motion capture for both left and right hands. The model and source codes are available at https://github.com/JubSteven/POEM-v2.","authors":["Lixin Yang","Licheng Zhong","Pengxiang Zhu","Xinyu Zhan","Junxiao Kong","Jian Xu","Cewu Lu"],"url":"https://arxiv.org/abs/2408.10581"}
{"created":"2025-04-28","title":"Understanding Depth and Height Perception in Large Visual-Language Models","abstract":"Geometric understanding - including depth and height perception - is fundamental to intelligence and crucial for navigating our environment. Despite the impressive capabilities of large Vision Language Models (VLMs), it remains unclear how well they possess the geometric understanding required for practical applications in visual perception. In this work, we focus on evaluating the geometric understanding of these models, specifically targeting their ability to perceive the depth and height of objects in an image. To address this, we introduce GeoMeter, a suite of benchmark datasets - encompassing 2D and 3D scenarios - to rigorously evaluate these aspects. By benchmarking 18 state-of-the-art VLMs, we found that although they excel in perceiving basic geometric properties like shape and size, they consistently struggle with depth and height perception. Our analysis reveal that these challenges stem from shortcomings in their depth and height reasoning capabilities and inherent biases. This study aims to pave the way for developing VLMs with enhanced geometric understanding by emphasizing depth and height perception as critical components necessary for real-world applications.","authors":["Shehreen Azad","Yash Jain","Rishit Garg","Yogesh S Rawat","Vibhav Vineet"],"url":"https://arxiv.org/abs/2408.11748"}
{"created":"2025-04-28","title":"Self-Supervised Representation Learning for Geospatial Objects: A Survey","abstract":"The proliferation of various data sources in urban and territorial environments has significantly facilitated the development of geospatial artificial intelligence (GeoAI) across a wide range of geospatial applications. However, geospatial data, which is inherently linked to geospatial objects, often exhibits data heterogeneity that necessitates specialized fusion and representation strategies while simultaneously being inherently sparse in labels for downstream tasks. Consequently, there is a growing demand for techniques that can effectively leverage geospatial data without heavy reliance on task-specific labels and model designs. This need aligns with the principles of self-supervised learning (SSL), which has garnered increasing attention for its ability to learn effective and generalizable representations directly from data without extensive labeled supervision. This paper presents a comprehensive and up-to-date survey of SSL techniques specifically applied to or developed for geospatial objects in three primary vector geometric types: Point, Polyline, and Polygon. We systematically categorize various SSL techniques into predictive and contrastive methods, and analyze their adaptation to different data types for representation learning across various downstream tasks. Furthermore, we examine the emerging trends in SSL for geospatial objects, particularly the gradual advancements towards geospatial foundation models. Finally, we discuss key challenges in current research and outline promising directions for future investigation. By offering a structured analysis of existing studies, this paper aims to inspire continued progress in integrating SSL with geospatial objects, and the development of geospatial foundation models in a longer term.","authors":["Yile Chen","Weiming Huang","Kaiqi Zhao","Yue Jiang","Gao Cong"],"url":"https://arxiv.org/abs/2408.12133"}
{"created":"2025-04-28","title":"Predictability of Performance in Communication Networks Under Markovian Dynamics","abstract":"With the emergence of time-critical applications in modern communication networks, there is a growing demand for proactive network adaptation and quality of service (QoS) prediction. However, a fundamental question remains largely unexplored: how can we quantify and achieve more predictable communication systems in terms of performance? To address this gap, this paper introduces a theoretical framework for defining and analyzing predictability in communication systems, with a focus on the impact of observations for performance forecasting. We establish a mathematical definition of predictability based on the total variation distance between forecast and marginal performance distributions. A system is deemed unpredictable when the forecast distribution, providing the most comprehensive characterization of future states using all accessible information, is indistinguishable from the marginal distribution, which depicts the system's behavior without any observational input. This framework is applied to multi-hop systems under Markovian conditions, with a detailed analysis of Geo/Geo/1 queuing models in both single-hop and multi-hop scenarios. We derive exact and approximate expressions for predictability in these systems, as well as upper bounds based on spectral analysis of the underlying Markov chains. Our results have implications for the design of efficient monitoring and prediction mechanisms in future communication networks aiming to provide deterministic services.","authors":["Samie Mostafavi","Simon Egger","Gy\\\"orgy D\\'an","James Gross"],"url":"https://arxiv.org/abs/2408.13196"}
{"created":"2025-04-28","title":"FungiTastic: A multi-modal dataset and benchmark for image categorization","abstract":"We introduce a new, challenging benchmark and a dataset, FungiTastic, based on fungal records continuously collected over a twenty-year span. The dataset is labelled and curated by experts and consists of about 350k multimodal observations of 6k fine-grained categories (species). The fungi observations include photographs and additional data, e.g., meteorological and climatic data, satellite images, and body part segmentation masks. FungiTastic is one of the few benchmarks that include a test set with DNA-sequenced ground truth of unprecedented label reliability. The benchmark is designed to support (i) standard closed-set classification, (ii) open-set classification, (iii) multi-modal classification, (iv) few-shot learning, (v) domain shift, and many more. We provide tailored baselines for many use cases, a multitude of ready-to-use pre-trained models on https://huggingface.co/collections/BVRA/fungitastic-66a227ce0520be533dc6403b, and a framework for model training. The documentation and the baselines are available at https://github.com/BohemianVRA/FungiTastic/ and https://www.kaggle.com/datasets/picekl/fungitastic.","authors":["Lukas Picek","Klara Janouskova","Vojtech Cermak","Jiri Matas"],"url":"https://arxiv.org/abs/2408.13632"}
{"created":"2025-04-28","title":"Efficient fine-tuning of 37-level GraphCast with the Canadian global deterministic analysis","abstract":"This work describes a process for efficiently fine-tuning the GraphCast data-driven forecast model to simulate another analysis system, here the Global Deterministic Prediction System (GDPS) of Environment and Climate Change Canada (ECCC). Using two years of training data (July 2019 -- December 2021) and 37 GPU-days of computation to tune the 37-level, quarter-degree version of GraphCast, the resulting model significantly outperforms both the unmodified GraphCast and operational forecast, showing significant forecast skill in the troposphere over lead times from 1 to 10 days. This fine-tuning is accomplished through abbreviating DeepMind's original training curriculum for GraphCast, relying on a shorter single-step forecast stage to accomplish the bulk of the adaptation work and consolidating the autoregressive stages into separate 12hr, 1d, 2d, and 3d stages with larger learning rates. Additionally, training over 3d forecasts is split into two sub-steps to conserve host memory while maintaining a strong correlation with training over the full period.","authors":["Christopher Subich"],"url":"https://arxiv.org/abs/2408.14587"}
{"created":"2025-04-28","title":"Using Large Language Models to Create AI Personas for Replication, Generalization and Prediction of Media Effects: An Empirical Test of 133 Published Experimental Research Findings","abstract":"This report analyzes the potential for large language models (LLMs) to expedite accurate replication and generalization of published research about message effects in marketing. LLM-powered participants (personas) were tested by replicating 133 experimental findings from 14 papers containing 45 recent studies published in the Journal of Marketing. For each study, the measures, stimuli, and sampling specifications were used to generate prompts for LLMs to act as unique personas. The AI personas, 19,447 in total across all of the studies, generated complete datasets and statistical analyses were then compared with the original human study results. The LLM replications successfully reproduced 76% of the original main effects (84 out of 111), demonstrating strong potential for AI-assisted replication. The overall replication rate including interaction effects was 68% (90 out of 133). Furthermore, a test of how human results generalized to different participant samples, media stimuli, and measures showed that replication results can change when tests go beyond the parameters of the original human studies. Implications are discussed for the replication and generalizability crises in social science, the acceleration of theory building in media and marketing psychology, and the practical advantages of rapid message testing for consumer products. Limitations of AI replications are addressed with respect to complex interaction effects, biases in AI models, and establishing benchmarks for AI metrics in marketing research.","authors":["Leo Yeykelis","Kaavya Pichai","James J. Cummings","Byron Reeves"],"url":"https://arxiv.org/abs/2408.16073"}
{"created":"2025-04-28","title":"Bidirectional Decoding: Improving Action Chunking via Guided Test-Time Sampling","abstract":"Predicting and executing a sequence of actions without intermediate replanning, known as action chunking, is increasingly used in robot learning from human demonstrations. Yet, its effects on the learned policy remain inconsistent: some studies find it crucial for achieving strong results, while others observe decreased performance. In this paper, we first dissect how action chunking impacts the divergence between a learner and a demonstrator. We find that action chunking allows the learner to better capture the temporal dependencies in demonstrations but at the cost of reduced reactivity to unexpected states. To address this tradeoff, we propose Bidirectional Decoding (BID), a test-time inference algorithm that bridges action chunking with closed-loop adaptation. At each timestep, BID samples multiple candidate predictions and searches for the optimal one based on two criteria: (i) backward coherence, which favors samples that align with previous decisions; (ii) forward contrast, which seeks samples of high likelihood for future plans. By coupling decisions within and across action chunks, BID promotes both long-term consistency and short-term reactivity. Experimental results show that our method boosts the performance of two state-of-the-art generative policies across seven simulation benchmarks and two real-world tasks. Code and videos are available at https://bid-robot.github.io.","authors":["Yuejiang Liu","Jubayer Ibn Hamid","Annie Xie","Yoonho Lee","Maximilian Du","Chelsea Finn"],"url":"https://arxiv.org/abs/2408.17355"}
{"created":"2025-04-28","title":"On the role of the signature transform in nonlinear systems and data-driven control","abstract":"Classic control techniques typically rely on a model of the system's response to external inputs, which is difficult to obtain from first principles especially if the unknown dynamics are nonlinear. In this paper, we address this issue by presenting an approach based on the so-called signature transform, a tool that is still largely unexplored in data-driven control. We first show that the signature provides rigorous and practically effective features to represent and predict system trajectories. Furthermore, we propose a novel use of this tool on an output-matching problem, paving the way for signature-based, data-driven predictive control.","authors":["Anna Scampicchio","Melanie N. Zeilinger"],"url":"https://arxiv.org/abs/2409.05685"}
{"created":"2025-04-28","title":"Your Weak LLM is Secretly a Strong Teacher for Alignment","abstract":"The burgeoning capabilities of large language models (LLMs) have underscored the need for alignment to ensure these models act in accordance with human values and intentions. Existing alignment frameworks present constraints either in the form of expensive human effort or high computational costs. This paper explores a promising middle ground, where we employ a weak LLM that is significantly less resource-intensive than top-tier models, yet offers more automation than purely human feedback. We present a systematic study to evaluate and understand weak LLM's ability to generate feedback for alignment. Our empirical findings demonstrate that weak LLMs can provide feedback that rivals or even exceeds that of fully human-annotated data. Our study indicates a minimized impact of model size on feedback efficacy, shedding light on a scalable and sustainable alignment strategy. To deepen our understanding of alignment under weak LLM feedback, we conduct a series of qualitative and quantitative analyses, offering novel insights into the quality discrepancies between human feedback vs. weak LLM feedback.","authors":["Leitian Tao","Yixuan Li"],"url":"https://arxiv.org/abs/2409.08813"}
{"created":"2025-04-28","title":"Evolving Distributions Under Local Motion","abstract":"Geometric data sets arising in modern applications are often very large and change dynamically over time. A popular framework for dealing with such data sets is the evolving data framework, where a discrete structure continuously varies over time due to the unseen actions of an evolver, which makes small changes to the data. An algorithm probes the current state through an oracle, and the objective is to maintain a hypothesis of the data set's current state that is close to its actual state at all times. In this paper, we apply this framework to maintaining a set of $n$ point objects in motion in $d$-dimensional Euclidean space. To model the uncertainty in the object locations, both the ground truth and hypothesis are based on spatial probability distributions, and the distance between them is measured by the Kullback-Leibler divergence (relative entropy). We introduce a simple and intuitive motion model where with each time step, the distance that any object can move is a fraction of the distance to its nearest neighbor. We present an algorithm that, in steady state, guarantees a distance of $O(n)$ between the true and hypothesized placements. We also show that for any algorithm in this model, there is an evolver that can generate a distance of $\\Omega(n)$, implying that our algorithm is asymptotically optimal.","authors":["Aditya Acharya","David M. Mount"],"url":"https://arxiv.org/abs/2409.11779"}
{"created":"2025-04-28","title":"Real-Time-Feasible Collision-Free Motion Planning For Ellipsoidal Objects","abstract":"Online planning of collision-free trajectories is a fundamental task for robotics and self-driving car applications. This paper revisits collision avoidance between ellipsoidal objects using differentiable constraints. Two ellipsoids do not overlap if and only if the endpoint of the vector between the center points of the ellipsoids does not lie in the interior of the Minkowski sum of the ellipsoids. This condition is formulated using a parametric over-approximation of the Minkowski sum, which can be made tight in any given direction. The resulting collision avoidance constraint is included in an optimal control problem (OCP) and evaluated in comparison to the separating-hyperplane approach. Not only do we observe that the Minkowski-sum formulation is computationally more efficient in our experiments, but also that using pre-determined over-approximation parameters based on warm-start trajectories leads to a very limited increase in suboptimality. This gives rise to a novel real-time scheme for collision-free motion planning with model predictive control (MPC). Both the real-time feasibility and the effectiveness of the constraint formulation are demonstrated in challenging real-world experiments.","authors":["Yunfan Gao","Florian Messerer","Niels van Duijkeren","Boris Houska","Moritz Diehl"],"url":"https://arxiv.org/abs/2409.12007"}
{"created":"2025-04-28","title":"MeTHanol: Modularized Thinking Language Models with Intermediate Layer Thinking, Decoding and Bootstrapping Reasoning","abstract":"Large Language Model can reasonably understand and generate human expressions but may lack of thorough thinking and reasoning mechanisms. Recently there have been several studies which enhance the thinking ability of language models but most of them are not data-driven or training-based. In this paper, we are motivated by the cognitive mechanism in the natural world, and design a novel model architecture called TaS which allows it to first consider the thoughts and then express the response based upon the query. We design several pipelines to annotate or generate the thought contents from prompt-response samples, then add language heads in a middle layer which behaves as the thinking layer. We train the language model by the thoughts-augmented data and successfully let the thinking layer automatically generate reasonable thoughts and finally output more reasonable responses. Both qualitative examples and quantitative results validate the effectiveness and performance of TaS. Our code is available at https://anonymous.4open.science/r/TadE.","authors":["Ningyuan Xi","Xiaoyu Wang","Yetao Wu","Teng Chen","Qingqing Gu","Yue Zhao","Jinxian Qu","Zhonglin Jiang","Yong Chen","Luo Ji"],"url":"https://arxiv.org/abs/2409.12059"}
{"created":"2025-04-28","title":"Prompts Are Programs Too! Understanding How Developers Build Software Containing Prompts","abstract":"Generative pre-trained models power intelligent software features used by millions of users controlled by developer-written natural language prompts. Despite the impact of prompt-powered software, little is known about its development process and its relationship to programming. In this work, we argue that some prompts are programs and that the development of prompts is a distinct phenomenon in programming known as \"prompt programming\". We develop an understanding of prompt programming using Straussian grounded theory through interviews with 20 developers engaged in prompt development across a variety of contexts, models, domains, and prompt structures. We contribute 15 observations to form a preliminary understanding of current prompt programming practices. For example, rather than building mental models of code, prompt programmers develop mental models of the foundation model (FM)'s behavior on the prompt by interacting with the FM. While prior research shows that experts have well-formed mental models, we find that prompt programmers who have developed dozens of prompts still struggle to develop reliable mental models. Our observations show that prompt programming differs from traditional software development, motivating the creation of prompt programming tools and providing implications for software engineering stakeholders.","authors":["Jenny T. Liang","Melissa Lin","Nikitha Rao","Brad A. Myers"],"url":"https://arxiv.org/abs/2409.12447"}
{"created":"2025-04-28","title":"Misty: UI Prototyping Through Interactive Conceptual Blending","abstract":"UI prototyping often involves iterating and blending elements from examples such as screenshots and sketches, but current tools offer limited support for incorporating these examples. Inspired by the cognitive process of conceptual blending, we introduce a novel UI workflow that allows developers to rapidly incorporate diverse aspects from design examples into work-in-progress UIs. We prototyped this workflow as Misty. Through an exploratory first-use study with 14 frontend developers, we assessed Misty's effectiveness and gathered feedback on this workflow. Our findings suggest that Misty's conceptual blending workflow helps developers kickstart creative explorations, flexibly specify intent in different stages of prototyping, and inspires developers through serendipitous UI blends. Misty demonstrates the potential for tools that blur the boundaries between developers and designers.","authors":["Yuwen Lu","Alan Leung","Amanda Swearngin","Jeffrey Nichols","Titus Barik"],"url":"https://arxiv.org/abs/2409.13900"}
{"created":"2025-04-28","title":"p and hp Spectral Element Methods for Elliptic Boundary Layer Problems","abstract":"In this article, we propose p and hp least-squares spectral element methods for one-dimensional elliptic boundary layer problems. Stability estimates are derived and we design numerical schemes based on minimizing the residuals in the sense of least-squares in appropriate Sobolev norms. We prove parameter robust uniform error estimates i.e. error in the approximation is independent of the boundary layer parameter. For the p-version we prove a robust uniform convergence rate of O(sqrt(log W)/W), where W denotes the polynomial order used in approximation and for the hp-version the convergence rate is shown to be O(e^(-W/logW)). Numerical results are presented for a number of model elliptic boundary layer problems confirming the theoretical estimates and uniform convergence results for the p and hp versions.","authors":["Akhlaq Husain","Aliya Kazmi","Subhashree Mohapatra","Mohammad Sajid","Ziya Uddin"],"url":"https://arxiv.org/abs/2409.14426"}
{"created":"2025-04-28","title":"Whole-body End-Effector Pose Tracking","abstract":"Combining manipulation with the mobility of legged robots is essential for a wide range of robotic applications. However, integrating an arm with a mobile base significantly increases the system's complexity, making precise end-effector control challenging. Existing model-based approaches are often constrained by their modeling assumptions, leading to limited robustness. Meanwhile, recent Reinforcement Learning (RL) implementations restrict the arm's workspace to be in front of the robot or track only the position to obtain decent tracking accuracy. In this work, we address these limitations by introducing a whole-body RL formulation for end-effector pose tracking in a large workspace on rough, unstructured terrains. Our proposed method involves a terrain-aware sampling strategy for the robot's initial configuration and end-effector pose commands, as well as a game-based curriculum to extend the robot's operating range. We validate our approach on the ANYmal quadrupedal robot with a six DoF robotic arm. Through our experiments, we show that the learned controller achieves precise command tracking over a large workspace and adapts across varying terrains such as stairs and slopes. On deployment, it achieves a pose-tracking error of 2.64 cm and 3.64 degrees, outperforming existing competitive baselines.","authors":["Tifanny Portela","Andrei Cramariuc","Mayank Mittal","Marco Hutter"],"url":"https://arxiv.org/abs/2409.16048"}
{"created":"2025-04-28","title":"Let's Make a Splan: Risk-Aware Trajectory Optimization in a Normalized Gaussian Splat","abstract":"Neural Radiance Fields and Gaussian Splatting have recently transformed computer vision by enabling photo-realistic representations of complex scenes. However, they have seen limited application in real-world robotics tasks such as trajectory optimization. This is due to the difficulty in reasoning about collisions in radiance models and the computational complexity associated with operating in dense models. This paper addresses these challenges by proposing SPLANNING, a risk-aware trajectory optimizer operating in a Gaussian Splatting model. This paper first derives a method to rigorously upper-bound the probability of collision between a robot and a radiance field. Then, this paper introduces a normalized reformulation of Gaussian Splatting that enables efficient computation of this collision bound. Finally, this paper presents a method to optimize trajectories that avoid collisions in a Gaussian Splat. Experiments show that SPLANNING outperforms state-of-the-art methods in generating collision-free trajectories in cluttered environments. The proposed system is also tested on a real-world robot manipulator. A project page is available at https://roahmlab.github.io/splanning.","authors":["Jonathan Michaux","Seth Isaacson","Challen Enninful Adu","Adam Li","Rahul Kashyap Swayampakula","Parker Ewen","Sean Rice","Katherine A. Skinner","Ram Vasudevan"],"url":"https://arxiv.org/abs/2409.16915"}
{"created":"2025-04-28","title":"Comodule Representations of Second-Order Functionals","abstract":"We develop and investigate a general theory of representations of second-order functionals, based on a notion of a right comodule for a monad on the category of containers. We show how the notion of comodule representability naturally subsumes classic representations of continuous functionals with well-founded trees. We find other kinds of representations by varying the monad, the comodule, and in some cases the underlying category of containers. Examples include uniformly continuous or finitely supported functionals, functionals querying their arguments precisely once, or at most once, functionals interacting with an ambient environment through computational effects, as well as functionals trivially representing themselves. Many of these rely on our construction of a monad on containers from a monad on shapes and a weak Mendler-style monad algebra on the universe for positions. We show that comodule representability on the category of propositional containers, which have positions valued in a universe of propositions, is closely related to instance reducibility in constructive mathematics, and through it to Weihrauch reducibility in computability theory.","authors":["Danel Ahman","Andrej Bauer"],"url":"https://arxiv.org/abs/2409.17664"}
{"created":"2025-04-28","title":"Embodied Visuomotor Representation","abstract":"Imagine sitting at your desk, looking at various objects on it. While you do not know their exact distances from your eye in meters, you can reach out and touch them. Instead of an externally defined unit, your sense of distance is inherently tied to your action's effect on your embodiment. In contrast, conventional robotics relies on precise calibration to external units with which separate vision and control processes communicate. This necessitates highly engineered and expensive systems that cannot be easily reconfigured.","authors":["Levi Burner","Cornelia Ferm\\\"uller","Yiannis Aloimonos"],"url":"https://arxiv.org/abs/2410.00287"}
{"created":"2025-04-28","title":"GaussianBlock: Building Part-Aware Compositional and Editable 3D Scene by Primitives and Gaussians","abstract":"Recently, with the development of Neural Radiance Fields and Gaussian Splatting, 3D reconstruction techniques have achieved remarkably high fidelity. However, the latent representations learnt by these methods are highly entangled and lack interpretability. In this paper, we propose a novel part-aware compositional reconstruction method, called GaussianBlock, that enables semantically coherent and disentangled representations, allowing for precise and physical editing akin to building blocks, while simultaneously maintaining high fidelity. Our GaussianBlock introduces a hybrid representation that leverages the advantages of both primitives, known for their flexible actionability and editability, and 3D Gaussians, which excel in reconstruction quality. Specifically, we achieve semantically coherent primitives through a novel attention-guided centering loss derived from 2D semantic priors, complemented by a dynamic splitting and fusion strategy. Furthermore, we utilize 3D Gaussians that hybridize with primitives to refine structural details and enhance fidelity. Additionally, a binding inheritance strategy is employed to strengthen and maintain the connection between the two. Our reconstructed scenes are evidenced to be disentangled, compositional, and compact across diverse benchmarks, enabling seamless, direct and precise editing while maintaining high quality.","authors":["Shuyi Jiang","Qihao Zhao","Hossein Rahmani","De Wen Soh","Jun Liu","Na Zhao"],"url":"https://arxiv.org/abs/2410.01535"}
{"created":"2025-04-28","title":"FaithEval: Can Your Language Model Stay Faithful to Context, Even If \"The Moon is Made of Marshmallows\"","abstract":"Ensuring faithfulness to context in large language models (LLMs) and retrieval-augmented generation (RAG) systems is crucial for reliable deployment in real-world applications, as incorrect or unsupported information can erode user trust. Despite advancements on standard benchmarks, faithfulness hallucination-where models generate responses misaligned with the provided context-remains a significant challenge. In this work, we introduce FaithEval, a novel and comprehensive benchmark tailored to evaluate the faithfulness of LLMs in contextual scenarios across three diverse tasks: unanswerable, inconsistent, and counterfactual contexts. These tasks simulate real-world challenges where retrieval mechanisms may surface incomplete, contradictory, or fabricated information. FaithEval comprises 4.9K high-quality problems in total, validated through a rigorous four-stage context construction and validation framework, employing both LLM-based auto-evaluation and human validation. Our extensive study across a wide range of open-source and proprietary models reveals that even state-of-the-art models often struggle to remain faithful to the given context, and that larger models do not necessarily exhibit improved faithfulness.Project is available at: https://github.com/SalesforceAIResearch/FaithEval.","authors":["Yifei Ming","Senthil Purushwalkam","Shrey Pandit","Zixuan Ke","Xuan-Phi Nguyen","Caiming Xiong","Shafiq Joty"],"url":"https://arxiv.org/abs/2410.03727"}
{"created":"2025-04-28","title":"Beyond Fixed Topologies: Unregistered Training and Comprehensive Evaluation Metrics for 3D Talking Heads","abstract":"Generating speech-driven 3D talking heads presents numerous challenges; among those is dealing with varying mesh topologies where no point-wise correspondence exists across all meshes the model can animate. While simplifying the problem, it limits applicability as unseen meshes must adhere to the training topology. This work presents a framework capable of animating 3D faces in arbitrary topologies, including real scanned data. Our approach relies on a model leveraging heat diffusion to predict features robust to the mesh topology. We explore two training settings: a registered one, in which meshes in a training sequences share a fixed topology but any mesh can be animated at test time, and an fully unregistered one, which allows effective training with varying mesh structures. Additionally, we highlight the limitations of current evaluation metrics and propose new metrics for better lip-syncing evaluation between speech and facial movements. Our extensive evaluation shows our approach performs favorably compared to fixed topology techniques, setting a new benchmark by offering a versatile and high-fidelity solution for 3D talking head generation where the topology constraint is dropped.","authors":["Federico Nocentini","Thomas Besnier","Claudio Ferrari","Sylvain Arguillere","Mohamed Daoudi","Stefano Berretti"],"url":"https://arxiv.org/abs/2410.11041"}
{"created":"2025-04-28","title":"P-time Algorithms for Typical #EO Problems","abstract":"In this article, we study the computational complexity of counting weighted Eulerian orientations, denoted as \\#\\textsf{EO}. This problem is considered a pivotal scenario in the complexity classification for \\textsf{Holant}, a counting framework of great significance. Our results consist of three parts. First, we prove a complexity dichotomy theorem for \\#\\textsf{EO} defined by a set of binary and quaternary signatures, which generalizes the previous dichotomy for the six-vertex model. Second, we prove a dichotomy for \\#\\textsf{EO} defined by a set of so-called pure signatures, which possess the closure property under gadget construction. Finally, we present a polynomial-time algorithm for \\#\\textsf{EO} defined by specific rebalancing signatures, which extends the algorithm for pure signatures to a broader range of problems, including \\#\\textsf{EO} defined by non-pure signatures such as $f_{40}$. We also construct a signature $f_{56}$ that is not rebalancing, and whether $\\#\\textsf{EO}(f_{56})$ is computable in polynomial time remains open.","authors":["Boning Meng","Juqiu Wang","Mingji Xia"],"url":"https://arxiv.org/abs/2410.11557"}
{"created":"2025-04-28","title":"MIND: Math Informed syNthetic Dialogues for Pretraining LLMs","abstract":"The utility of synthetic data to enhance pretraining data quality and hence to improve downstream task accuracy has been widely explored in recent large language models (LLMs). Yet, these approaches fall inadequate in complex, multi-hop and mathematical reasoning tasks as the synthetic data typically fails to add complementary knowledge to the existing raw corpus. In this work, we propose a novel large-scale and diverse Math Informed syNthetic Dialogue (MIND) generation method that improves the mathematical reasoning ability of LLMs. Specifically, using MIND, we generate synthetic conversations based on OpenWebMath (OWM), resulting in a new math corpus, MIND-OWM. Our experiments with different conversational settings reveal that incorporating knowledge gaps between dialog participants is essential for generating high-quality math data. We further identify an effective way to format and integrate synthetic and raw data during pretraining to maximize the gain in mathematical reasoning, emphasizing the need to restructure raw data rather than use it as-is. Compared to pretraining just on raw data, a model pretrained on MIND-OWM shows significant boost in mathematical reasoning (GSM8K: +13.42%, MATH: +2.30%), including superior performance in specialized knowledge (MMLU: +4.55%, MMLU-STEM: +4.28%) and general purpose reasoning tasks (GENERAL REASONING: +2.51%).","authors":["Syeda Nahida Akter","Shrimai Prabhumoye","John Kamalu","Sanjeev Satheesh","Eric Nyberg","Mostofa Patwary","Mohammad Shoeybi","Bryan Catanzaro"],"url":"https://arxiv.org/abs/2410.12881"}
{"created":"2025-04-28","title":"Improving Consistency in Diffusion Models for Image Super-Resolution","abstract":"Recent methods exploit the powerful text-to-image (T2I) diffusion models for real-world image super-resolution (Real-ISR) and achieve impressive results compared to previous models. However, we observe two kinds of inconsistencies in diffusion-based methods which hinder existing models from fully exploiting diffusion priors. The first is the semantic inconsistency arising from diffusion guidance. T2I generation focuses on semantic-level consistency with text prompts, while Real-ISR emphasizes pixel-level reconstruction from low-quality (LQ) images, necessitating more detailed semantic guidance from LQ inputs. The second is the training-inference inconsistency stemming from the DDPM, which improperly assumes high-quality (HQ) latent corrupted by Gaussian noise as denoising inputs for each timestep. To address these issues, we introduce ConsisSR to handle both semantic and training-inference consistencies. On the one hand, to address the semantic inconsistency, we proposed a Hybrid Prompt Adapter (HPA). Instead of text prompts with coarse-grained classification information, we leverage the more powerful CLIP image embeddings to explore additional color and texture guidance. On the other hand, we introduce Time-Aware Latent Augmentation (TALA) to bridge the training-inference inconsistency. Based on the probability function p(t), we accordingly enhance the SDSR training strategy. With LQ latent with Gaussian noise as inputs, our TALA not only focuses on diffusion noise but also refine the LQ latent towards the HQ counterpart. Our method demonstrates state-of-the-art performance among existing diffusion models. The code will be made publicly available.","authors":["Junhao Gu","Peng-Tao Jiang","Hao Zhang","Mi Zhou","Jinwei Chen","Wenming Yang","Bo Li"],"url":"https://arxiv.org/abs/2410.13807"}
{"created":"2025-04-28","title":"Co-Designing with Algorithms: Unpacking the Complex Role of GenAI in Interactive System Design Education","abstract":"Generative Artificial Intelligence (GenAI) is transforming Human-Computer Interaction (HCI) education and technology design, yet its impact remains poorly understood. This study explores how graduate students in an applied HCI course used GenAI tools during interactive device design. Despite no encouragement, all groups integrated GenAI into their workflows. Through 12 post-class group interviews, we identified how GenAI co-design behaviors present both benefits, such as enhanced creativity and faster design iterations, and risks, including shallow learning and reflection. Benefits were most evident during the execution phases, while the discovery and reflection phases showed limited gains. A taxonomy of usage patterns revealed that students' outcomes depended more on how they used GenAI than the specific tasks performed. These findings highlight the need for HCI education to adapt to GenAI's role and offer recommendations for curricula to better prepare future designers for effective creative co-design.","authors":["Hauke Sandhaus","Quiquan Gu","Maria Teresa Parreira","Wendy Ju"],"url":"https://arxiv.org/abs/2410.14048"}
{"created":"2025-04-28","title":"Improved Contact Graph Routing in Delay Tolerant Networks with Capacity and Buffer Constraints","abstract":"Satellite communications present challenging characteristics. Continuous end-to-end connectivity may not be available due to the large distances between satellites. Moreover, resources such as link capacity and buffer memory may be limited. Routing in satellite networks is therefore both complex and crucial to avoid packet losses and long delays. The Delay Tolerant Network (DTN) paradigm has emerged as an efficient solution for managing these challenging networks. Contact Graph Routing (CGR), a deterministic routing algorithm, is one of the most popular DTN algorithms. CGR is compatible with the ``store, carry, and forward\" principle, whereby a node receives a message and stores it in its buffer until a transmission opportunity becomes available. However, CGR relies on simplified models to incorporate potential constraints in the route search. For instance, the linear volume assumption is often used to consider capacity constraints. Moreover, capacity management and buffer management are mostly performed during the forwarding phase, once an issue has occurred. In this paper, we propose taking measures before or during the route search in order to find routes that respect both contact-capacity and node-buffer limits. We introduce the contact splitting and edge pruning operations to effectively account for the routing constraints. This ensures that CGR outputs the optimal solution among the subset of valid solutions. The proposed approach can also be used to book resources to be used in case of issues during the forwarding phase.","authors":["Tania Alhajj","Vincent Corlay"],"url":"https://arxiv.org/abs/2410.15546"}
{"created":"2025-04-28","title":"Revealing The Secret Power: How Algorithms Can Influence Content Visibility on Social Media","abstract":"In recent years, the opaque design and the limited public understanding of social networks' recommendation algorithms have raised concerns about potential manipulation of information exposure. While reducing content visibility, aka shadow banning, may help limit harmful content, it can also be used to suppress dissenting voices. This prompts the need for greater transparency and a better understanding of this practice.","authors":["Alessandro Galeazzi","Pujan Paudel","Mauro Conti","Emiliano De Cristofaro","Gianluca Stringhini"],"url":"https://arxiv.org/abs/2410.17390"}
{"created":"2025-04-28","title":"Projection-based Reduced Order Modelling for Unsteady Parametrized Optimal Control Problems in 3D Cardiovascular Flows","abstract":"This paper presents a projection-based reduced order modelling (ROM) framework for unsteady parametrized optimal control problems (OCP$_{(\\mu)}$s) arising from cardiovascular (CV) applications. In real-life scenarios, accurately defining outflow boundary conditions in patient-specific models poses significant challenges due to complex vascular morphologies, physiological conditions, and high computational demands. These challenges make it difficult to compute realistic and reliable CV hemodynamics by incorporating clinical data such as 4D magnetic resonance imaging. To address these challenges, we focus on controlling the outflow boundary conditions to optimize CV flow dynamics and minimize the discrepancy between target and computed flow velocity profiles. The fluid flow is governed by unsteady Navier--Stokes equations with physical parametric dependence, i.e. the Reynolds number. Numerical solutions of OCP$_{(\\mu)}$s require substantial computational resources, highlighting the need for robust and efficient ROMs to perform real-time and many-query simulations. Here, we aim at investigating the performance of a projection-based reduction technique that relies on the offline-online paradigm, enabling significant computational cost savings. The Galerkin finite element method is used to compute the high-fidelity solutions in the offline phase. We implemented a nested-proper orthogonal decomposition (nested-POD) for fast simulation of OCP$_{(\\mu)}$s that encompasses two stages: temporal compression for reducing dimensionality in time, followed by parametric-space compression on the precomputed POD modes. We tested the efficacy of the methodology on vascular models, namely an idealized bifurcation geometry and a patient-specific coronary artery bypass graft, incorporating stress control at the outflow boundary, observing consistent speed-up with respect to high-fidelity strategies.","authors":["Surabhi Rathore","Pasquale Claudio Africa","Francesco Ballarin","Federico Pichi","Michele Girfoglio","Gianluigi Rozza"],"url":"https://arxiv.org/abs/2410.20828"}
{"created":"2025-04-28","title":"Exploring Local Memorization in Diffusion Models via Bright Ending Attention","abstract":"Text-to-image diffusion models have achieved unprecedented proficiency in generating realistic images. However, their inherent tendency to memorize and replicate training data during inference raises significant concerns, including potential copyright infringement. In response, various methods have been proposed to evaluate, detect, and mitigate memorization. Our analysis reveals that existing approaches significantly underperform in handling local memorization, where only specific image regions are memorized, compared to global memorization, where the entire image is replicated. Also, they cannot locate the local memorization regions, making it hard to investigate locally. To address these, we identify a novel \"bright ending\" (BE) anomaly in diffusion models prone to memorizing training images. BE refers to a distinct cross-attention pattern observed in text-to-image diffusion models, where memorized image patches exhibit significantly greater attention to the final text token during the last inference step than non-memorized patches. This pattern highlights regions where the generated image replicates training data and enables efficient localization of memorized regions. Equipped with this, we propose a simple yet effective method to integrate BE into existing frameworks, significantly improving their performance by narrowing the performance gap caused by local memorization. Our results not only validate the successful execution of the new localization task but also establish new state-of-the-art performance across all existing tasks, underscoring the significance of the BE phenomenon.","authors":["Chen Chen","Daochang Liu","Mubarak Shah","Chang Xu"],"url":"https://arxiv.org/abs/2410.21665"}
{"created":"2025-04-28","title":"Investigating Memorization in Video Diffusion Models","abstract":"Diffusion models, widely used for image and video generation, face a significant limitation: the risk of memorizing and reproducing training data during inference, potentially generating unauthorized copyrighted content. While prior research has focused on image diffusion models (IDMs), video diffusion models (VDMs) remain underexplored. To address this gap, we first formally define the two types of memorization in VDMs (content memorization and motion memorization) in a practical way that focuses on privacy preservation and applies to all generation types. We then introduce new metrics specifically designed to separately assess content and motion memorization in VDMs. Additionally, we curate a dataset of text prompts that are most prone to triggering memorization when used as conditioning in VDMs. By leveraging these prompts, we generate diverse videos from various open-source VDMs, successfully extracting numerous training videos from each tested model. Through the application of our proposed metrics, we systematically analyze memorization across various pretrained VDMs, including text-conditional and unconditional models, on a variety of datasets. Our comprehensive study reveals that memorization is widespread across all tested VDMs, indicating that VDMs can also memorize image training data in addition to video datasets. Finally, we propose efficient and effective detection strategies for both content and motion memorization, offering a foundational approach for improving privacy in VDMs.","authors":["Chen Chen","Enhuai Liu","Daochang Liu","Mubarak Shah","Chang Xu"],"url":"https://arxiv.org/abs/2410.21669"}
{"created":"2025-04-28","title":"Contrastive Learning and Adversarial Disentanglement for Task-Oriented Semantic Communications","abstract":"Task-oriented semantic communication systems have emerged as a promising approach to achieving efficient and intelligent data transmission, where only information relevant to a specific task is communicated. However, existing methods struggle to fully disentangle task-relevant and task-irrelevant information, leading to privacy concerns and subpar performance. To address this, we propose an information-bottleneck method, named CLAD (contrastive learning and adversarial disentanglement). CLAD utilizes contrastive learning to effectively capture task-relevant features while employing adversarial disentanglement to discard task-irrelevant information. Additionally, due to the lack of reliable and reproducible methods to gain insight into the informativeness and minimality of the encoded feature vectors, we introduce a new technique to compute the information retention index (IRI), a comparative metric used as a proxy for the mutual information between the encoded features and the input, reflecting the minimality of the encoded features. The IRI quantifies the minimality and informativeness of the encoded feature vectors across different task-oriented communication techniques. Our extensive experiments demonstrate that CLAD outperforms state-of-the-art baselines in terms of semantic extraction, task performance, privacy preservation, and IRI. CLAD achieves a predictive performance improvement of around 2.5-3%, along with a 77-90% reduction in IRI and a 57-76% decrease in adversarial attribute inference attack accuracy.","authors":["Omar Erak","Omar Alhussein","Wen Tong"],"url":"https://arxiv.org/abs/2410.22784"}
{"created":"2025-04-28","title":"Generative AI-Powered Plugin for Robust Federated Learning in Heterogeneous IoT Networks","abstract":"Federated learning enables edge devices to collaboratively train a global model while maintaining data privacy by keeping data localized. However, the Non-IID nature of data distribution across devices often hinders model convergence and reduces performance. In this paper, we propose a novel plugin for federated optimization techniques that approximates Non-IID data distributions to IID through generative AI-enhanced data augmentation and balanced sampling strategy. Key idea is to synthesize additional data for underrepresented classes on each edge device, leveraging generative AI to create a more balanced dataset across the FL network. Additionally, a balanced sampling approach at the central server selectively includes only the most IID-like devices, accelerating convergence while maximizing the global model's performance. Experimental results validate that our approach significantly improves convergence speed and robustness against data imbalance, establishing a flexible, privacy-preserving FL plugin that is applicable even in data-scarce environments.","authors":["Youngjoon Lee","Jinu Gong","Joonhyuk Kang"],"url":"https://arxiv.org/abs/2410.23824"}
{"created":"2025-04-28","title":"Leveraging Label Semantics and Meta-Label Refinement for Multi-Label Question Classification","abstract":"Accurate annotation of educational resources is crucial for effective personalized learning and resource recommendation in online education. However, fine-grained knowledge labels often overlap or share similarities, making it difficult for existing multi-label classification methods to differentiate them. The label distribution imbalance due to sparsity of human annotations further intensifies these challenges. To address these issues, this paper introduces RR2QC, a novel Retrieval Reranking method to multi-label Question Classification by leveraging label semantics and meta-label refinement. First, RR2QC improves the pre-training strategy by utilizing semantic relationships within and across label groups. Second, it introduces a class center learning task to align questions with label semantics during downstream training. Finally, this method decomposes labels into meta-labels and uses a meta-label classifier to rerank the retrieved label sequences. In doing so, RR2QC enhances the understanding and prediction capability of long-tail labels by learning from meta-labels that frequently appear in other labels. Additionally, a mathematical LLM is used to generate solutions for questions, extracting latent information to further refine the model's insights. Experimental results show that RR2QC outperforms existing methods in Precision@K and F1 scores across multiple educational datasets, demonstrating its effectiveness for online education applications. The code and datasets are available at https://github.com/78Erii/RR2QC.","authors":["Shi Dong","Xiaobei Niu","Rui Zhong","Zhifeng Wang","Mingzhang Zuo"],"url":"https://arxiv.org/abs/2411.01841"}
{"created":"2025-04-28","title":"A Picture is Worth A Thousand Numbers: Enabling LLMs Reason about Time Series via Visualization","abstract":"Large language models (LLMs), with demonstrated reasoning abilities across multiple domains, are largely underexplored for time-series reasoning (TsR), which is ubiquitous in the real world. In this work, we propose TimerBed, the first comprehensive testbed for evaluating LLMs' TsR performance. Specifically, TimerBed includes stratified reasoning patterns with real-world tasks, comprehensive combinations of LLMs and reasoning strategies, and various supervised models as comparison anchors. We perform extensive experiments with TimerBed, test multiple current beliefs, and verify the initial failures of LLMs in TsR, evidenced by the ineffectiveness of zero shot (ZST) and performance degradation of few shot in-context learning (ICL). Further, we identify one possible root cause: the numerical modeling of data. To address this, we propose a prompt-based solution VL-Time, using visualization-modeled data and language-guided reasoning. Experimental results demonstrate that Vl-Time enables multimodal LLMs to be non-trivial ZST and powerful ICL reasoners for time series, achieving about 140% average performance improvement and 99% average token costs reduction.","authors":["Haoxin Liu","Chenghao Liu","B. Aditya Prakash"],"url":"https://arxiv.org/abs/2411.06018"}
{"created":"2025-04-28","title":"Electrically-driven phase transition actuators to power soft robot designs","abstract":"In the quest for electrically-driven soft actuators, the focus has shifted away from liquid-gas phase transition, commonly associated with reduced strain rates and actuation delays, in favour of electrostatic and other electrothermal actuation methods. This prevented the technology from capitalizing on its unique characteristics, particularly: low voltage operation, controllability, scalability, and ease of integration into robots. Here, we introduce a liquid-gas phase transition electric soft actuator that uses water as the working fluid and is powered by a coil-type flexible heating element. It achieves strain rates of over 16%/s and pressurization rates of 100 kPa/s. Blocked forces exceeding 50 N were achieved while operating at voltages up to 24 V. We propose a method for selecting working fluids which allows for application-specific optimization, together with a nonlinear control approach that reduces both parasitic vibrations and control lag. We demonstrate the integration of this technology in soft robotic systems, including a cable-driven biomimetic hand and a quadruped robot powered by liquid-gas phase transition.","authors":["Diogo Fonseca","Pedro Neto"],"url":"https://arxiv.org/abs/2411.06963"}
{"created":"2025-04-28","title":"Knowledge-Augmented Multimodal Clinical Rationale Generation for Disease Diagnosis with Small Language Models","abstract":"Interpretation is critical for disease diagnosis, but existing models struggle to balance predictive accuracy with human-understandable rationales. While large language models (LLMs) offer strong reasoning abilities, their clinical use is limited by high computational costs and restricted multimodal reasoning ability. Small language models (SLMs) are efficient but lack advanced reasoning for integrating multimodal medical data. In addition, both LLMs and SLMs lack of domain knowledge for trustworthy reasoning. Therefore, we propose ClinRaGen, enhancing SLMs by leveraging LLM-derived reasoning ability via rationale distillation and domain knowledge injection for trustworthy multimodal rationale generation. Key innovations include a sequential rationale distillation framework that equips SLMs with LLM-comparable mutlimodal reasoning abilities, and a knowledge-augmented attention mechanism that jointly unifies multimodal representation from time series and textual data in a same encoding space, enabling it naturally interpreted by SLMs while incorporating domain knowledge for reliable rationale generation. Experiments on real-world medical datasets show that ClinRaGen achieves state-of-the-art performance in disease diagnosis and rationale generation, demonstrating the effectiveness of combining LLM-driven reasoning with knowledge augmentation for improved interpretability.","authors":["Shuai Niu","Jing Ma","Hongzhan Lin","Liang Bai","Zhihua Wang","Yida Xu","Yunya Song","Xian Yang"],"url":"https://arxiv.org/abs/2411.07611"}
{"created":"2025-04-28","title":"Instant Policy: In-Context Imitation Learning via Graph Diffusion","abstract":"Following the impressive capabilities of in-context learning with large transformers, In-Context Imitation Learning (ICIL) is a promising opportunity for robotics. We introduce Instant Policy, which learns new tasks instantly (without further training) from just one or two demonstrations, achieving ICIL through two key components. First, we introduce inductive biases through a graph representation and model ICIL as a graph generation problem with a learned diffusion process, enabling structured reasoning over demonstrations, observations, and actions. Second, we show that such a model can be trained using pseudo-demonstrations - arbitrary trajectories generated in simulation - as a virtually infinite pool of training data. Simulated and real experiments show that Instant Policy enables rapid learning of various everyday robot tasks. We also show how it can serve as a foundation for cross-embodiment and zero-shot transfer to language-defined tasks. Code and videos are available at https://www.robot-learning.uk/instant-policy.","authors":["Vitalis Vosylius","Edward Johns"],"url":"https://arxiv.org/abs/2411.12633"}
{"created":"2025-04-28","title":"CLIC: Contrastive Learning Framework for Unsupervised Image Complexity Representation","abstract":"As a fundamental visual attribute, image complexity significantly influences both human perception and the performance of computer vision models. However, accurately assessing and quantifying image complexity remains a challenging task. (1) Traditional metrics such as information entropy and compression ratio often yield coarse and unreliable estimates. (2) Data-driven methods require expensive manual annotations and are inevitably affected by human subjective biases. To address these issues, we propose CLIC, an unsupervised framework based on Contrastive Learning for learning Image Complexity representations. CLIC learns complexity-aware features from unlabeled data, thereby eliminating the need for costly labeling. Specifically, we design a novel positive and negative sample selection strategy to enhance the discrimination of complexity features. Additionally, we introduce a complexity-aware loss function guided by image priors to further constrain the learning process. Extensive experiments validate the effectiveness of CLIC in capturing image complexity. When fine-tuned with a small number of labeled samples from IC9600, CLIC achieves performance competitive with supervised methods. Moreover, applying CLIC to downstream tasks consistently improves performance. Notably, both the pretraining and application processes of CLIC are free from subjective bias.","authors":["Shipeng Liu","Liang Zhao","Dengfeng Chen"],"url":"https://arxiv.org/abs/2411.12792"}
{"created":"2025-04-28","title":"Importance-Based Token Merging for Efficient Image and Video Generation","abstract":"Token merging can effectively accelerate various vision systems by processing groups of similar tokens only once and sharing the results across them. However, existing token grouping methods are often ad hoc and random, disregarding the actual content of the samples. We show that preserving high-information tokens during merging - those essential for semantic fidelity and structural details - significantly improves sample quality, producing finer details and more coherent, realistic generations. Despite being simple and intuitive, this approach remains underexplored.","authors":["Haoyu Wu","Jingyi Xu","Hieu Le","Dimitris Samaras"],"url":"https://arxiv.org/abs/2411.16720"}
{"created":"2025-04-28","title":"Semantic Edge Computing and Semantic Communications in 6G Networks: A Unifying Survey and Research Challenges","abstract":"Semantic Edge Computing (SEC) and Semantic Communications (SemComs) have been proposed as viable approaches to achieve real-time edge-enabled intelligence in sixth-generation (6G) wireless networks. On one hand, SemCom leverages the strength of Deep Neural Networks (DNNs) to encode and communicate the semantic information only, while making it robust to channel distortions by compensating for wireless effects. Ultimately, this leads to an improvement in the communication efficiency. On the other hand, SEC has leveraged distributed DNNs to divide the computation of a DNN across different devices based on their computational and networking constraints. Although significant progress has been made in both fields, the literature lacks a systematic view to connect both fields. In this work, we fulfill the current gap by unifying the SEC and SemCom fields. We summarize the research problems in these two fields and provide a comprehensive review of the state of the art with a focus on their technical strengths and challenges.","authors":["Milin Zhang","Mohammad Abdi","Venkat R. Dasari","Francesco Restuccia"],"url":"https://arxiv.org/abs/2411.18199"}
{"created":"2025-04-28","title":"Dense Dynamics-Aware Reward Synthesis: Integrating Prior Experience with Demonstrations","abstract":"Many continuous control problems can be formulated as sparse-reward reinforcement learning (RL) tasks. In principle, online RL methods can automatically explore the state space to solve each new task. However, discovering sequences of actions that lead to a non-zero reward becomes exponentially more difficult as the task horizon increases. Manually shaping rewards can accelerate learning for a fixed task, but it is an arduous process that must be repeated for each new environment. We introduce a systematic reward-shaping framework that distills the information contained in 1) a task-agnostic prior data set and 2) a small number of task-specific expert demonstrations, and then uses these priors to synthesize dense dynamics-aware rewards for the given task. This supervision substantially accelerates learning in our experiments, and we provide analysis demonstrating how the approach can effectively guide online learning agents to faraway goals.","authors":["Cevahir Koprulu","Po-han Li","Tianyu Qiu","Ruihan Zhao","Tyler Westenbroek","David Fridovich-Keil","Sandeep Chinchali","Ufuk Topcu"],"url":"https://arxiv.org/abs/2412.01114"}
{"created":"2025-04-28","title":"An Adaptive Grasping Force Tracking Strategy for Nonlinear and Time-Varying Object Behaviors","abstract":"Accurate grasp force control is one of the key skills for ensuring successful and damage-free robotic grasping of objects. Although existing methods have conducted in-depth research on slip detection and grasping force planning, they often overlook the issue of adaptive tracking of the actual force to the target force when handling objects with different material properties. The optimal parameters of a force tracking controller are significantly influenced by the object's stiffness, and many adaptive force tracking algorithms rely on stiffness estimation. However, real-world objects often exhibit viscous, plastic, or other more complex nonlinear time-varying behaviors, and existing studies provide insufficient support for these materials in terms of stiffness definition and estimation. To address this, this paper introduces the concept of generalized stiffness, extending the definition of stiffness to nonlinear time-varying grasp system models, and proposes an online generalized stiffness estimator based on Long Short-Term Memory (LSTM) networks. Based on generalized stiffness, this paper proposes an adaptive parameter adjustment strategy using a PI controller as an example, enabling dynamic force tracking for objects with varying characteristics. Experimental results demonstrate that the proposed method achieves high precision and short probing time, while showing better adaptability to non-ideal objects compared to existing methods. The method effectively solves the problem of grasp force tracking in unknown, nonlinear, and time-varying grasp systems, demonstrating the generalization capability of our neural network and enhancing the robotic grasping ability in unstructured environments.","authors":["Ziyang Cheng","Xiangyu Tian","Ruomin Sui","Tiemin Li","Yao Jiang"],"url":"https://arxiv.org/abs/2412.02335"}
{"created":"2025-04-28","title":"The Moral Mind(s) of Large Language Models","abstract":"As large language models (LLMs) increasingly participate in tasks with ethical and societal stakes, a critical question arises: do they exhibit an emergent \"moral mind\" - a consistent structure of moral preferences guiding their decisions - and to what extent is this structure shared across models? To investigate this, we applied tools from revealed preference theory to nearly 40 leading LLMs, presenting each with many structured moral dilemmas spanning five foundational dimensions of ethical reasoning. Using a probabilistic rationality test, we found that at least one model from each major provider exhibited behavior consistent with approximately stable moral preferences, acting as if guided by an underlying utility function. We then estimated these utility functions and found that most models cluster around neutral moral stances. To further characterize heterogeneity, we employed a non-parametric permutation approach, constructing a probabilistic similarity network based on revealed preference patterns. The results reveal a shared core in LLMs' moral reasoning, but also meaningful variation: some models show flexible reasoning across perspectives, while others adhere to more rigid ethical profiles. These findings provide a new empirical lens for evaluating moral consistency in LLMs and offer a framework for benchmarking ethical alignment across AI systems.","authors":["Avner Seror"],"url":"https://arxiv.org/abs/2412.04476"}
{"created":"2025-04-28","title":"A Parametric Approach to Adversarial Augmentation for Cross-Domain Iris Presentation Attack Detection","abstract":"Iris-based biometric systems are vulnerable to presentation attacks (PAs), where adversaries present physical artifacts (e.g., printed iris images, textured contact lenses) to defeat the system. This has led to the development of various presentation attack detection (PAD) algorithms, which typically perform well in intra-domain settings. However, they often struggle to generalize effectively in cross-domain scenarios, where training and testing employ different sensors, PA instruments, and datasets. In this work, we use adversarial training samples of both bonafide irides and PAs to improve the cross-domain performance of a PAD classifier. The novelty of our approach lies in leveraging transformation parameters from classical data augmentation schemes (e.g., translation, rotation) to generate adversarial samples. We achieve this through a convolutional autoencoder, ADV-GEN, that inputs original training samples along with a set of geometric and photometric transformations. The transformation parameters act as regularization variables, guiding ADV-GEN to generate adversarial samples in a constrained search space. Experiments conducted on the LivDet-Iris 2017 database, comprising four datasets, and the LivDet-Iris 2020 dataset, demonstrate the efficacy of our proposed method. The code is available at https://github.com/iPRoBe-lab/ADV-GEN-IrisPAD.","authors":["Debasmita Pal","Redwan Sony","Arun Ross"],"url":"https://arxiv.org/abs/2412.07199"}
{"created":"2025-04-28","title":"{\\alpha}-RACER: Real-Time Algorithm for Game-Theoretic Motion Planning and Control in Autonomous Racing using Near-Potential Function","abstract":"Autonomous racing extends beyond the challenge of controlling a racecar at its physical limits. Professional racers employ strategic maneuvers to outwit other competing opponents to secure victory. While modern control algorithms can achieve human-level performance by computing offline racing lines for single-car scenarios, research on real-time algorithms for multi-car autonomous racing is limited. To bridge this gap, we develop game-theoretic modeling framework that incorporates the competitive aspect of autonomous racing like overtaking and blocking through a novel policy parametrization, while operating the car at its limit. Furthermore, we propose an algorithmic approach to compute the (approximate) Nash equilibrium strategy, which represents the optimal approach in the presence of competing agents. Specifically, we introduce an algorithm inspired by recently introduced framework of dynamic near-potential function, enabling real-time computation of the Nash equilibrium. Our approach comprises two phases: offline and online. During the offline phase, we use simulated racing data to learn a near-potential function that approximates utility changes for agents. This function facilitates the online computation of approximate Nash equilibria by maximizing its value. We evaluate our method in a head-to-head 3-car racing scenario, demonstrating superior performance compared to several existing baselines.","authors":["Dvij Kalaria","Chinmay Maheshwari","Shankar Sastry"],"url":"https://arxiv.org/abs/2412.08855"}
{"created":"2025-04-28","title":"EmoDubber: Towards High Quality and Emotion Controllable Movie Dubbing","abstract":"Given a piece of text, a video clip, and a reference audio, the movie dubbing task aims to generate speech that aligns with the video while cloning the desired voice. The existing methods have two primary deficiencies: (1) They struggle to simultaneously hold audio-visual sync and achieve clear pronunciation; (2) They lack the capacity to express user-defined emotions. To address these problems, we propose EmoDubber, an emotion-controllable dubbing architecture that allows users to specify emotion type and emotional intensity while satisfying high-quality lip sync and pronunciation. Specifically, we first design Lip-related Prosody Aligning (LPA), which focuses on learning the inherent consistency between lip motion and prosody variation by duration level contrastive learning to incorporate reasonable alignment. Then, we design Pronunciation Enhancing (PE) strategy to fuse the video-level phoneme sequences by efficient conformer to improve speech intelligibility. Next, the speaker identity adapting module aims to decode acoustics prior and inject the speaker style embedding. After that, the proposed Flow-based User Emotion Controlling (FUEC) is used to synthesize waveform by flow matching prediction network conditioned on acoustics prior. In this process, the FUEC determines the gradient direction and guidance scale based on the user's emotion instructions by the positive and negative guidance mechanism, which focuses on amplifying the desired emotion while suppressing others. Extensive experimental results on three benchmark datasets demonstrate favorable performance compared to several state-of-the-art methods.","authors":["Gaoxiang Cong","Jiadong Pan","Liang Li","Yuankai Qi","Yuxin Peng","Anton van den Hengel","Jian Yang","Qingming Huang"],"url":"https://arxiv.org/abs/2412.08988"}
{"created":"2025-04-28","title":"A Survey on Web Application Testing: A Decade of Evolution","abstract":"As one of the most popular software applications, a web application is a program, accessible through the web, to dynamically generate content based on user interactions or contextual data, for example, online shopping platforms, social networking sites, and financial services. Web applications operate in diverse environments and leverage web technologies such as HTML, CSS, JavaScript, and Ajax, often incorporating features like asynchronous operations to enhance user experience. Due to the increasing user and popularity of web applications, approaches to their quality have become increasingly important. Web Application Testing (WAT) plays a vital role in ensuring web applications' functionality, security, and reliability. Given the speed with which web technologies are evolving, WAT is especially important. Over the last decade, various WAT approaches have been developed. The diversity of approaches reflects the many aspects of web applications, such as dynamic content, asynchronous operations, and diverse user environments. This paper provides a comprehensive overview of the main achievements during the past decade: It examines the main steps involved in WAT, including test-case generation and execution, and evaluation and assessment. The currently available tools for WAT are also examined. The paper also discusses some open research challenges and potential future WAT work.","authors":["Tao Li","Rubing Huang","Chenhui Cui","Dave Towey","Lei Ma","Yuan-Fang Li","Wen Xia"],"url":"https://arxiv.org/abs/2412.10476"}
{"created":"2025-04-28","title":"ElChat: Adapting Chat Language Models Using Only Target Unlabeled Language Data","abstract":"Vocabulary expansion (VE) is the de-facto approach to language adaptation of large language models (LLMs) by adding new tokens and continuing pre-training on target data. While this is effective for base models trained on unlabeled data, it poses challenges for chat models trained to follow instructions through labeled conversation data. Directly adapting the latter with VE on target unlabeled data may result in forgetting chat abilities. While ideal, target chat data is often unavailable or costly to create for low-resource languages, and machine-translated alternatives are not always effective. To address this issue, previous work proposed using a base and chat model from the same family. This method first adapts the base LLM with VE on target unlabeled data and then converts it to a chat model by adding a chat vector (CV) derived from the weight difference between the source base and chat models. We propose ElChat, a new language adaptation method for chat LLMs that adapts a chat model directly on target unlabeled data, without a base model. It elicits chat abilities by injecting information from the source chat model. ElChat offers more robust and competitive target language and safety performance while achieving superior English, chat, and instruction-following abilities compared to CV.","authors":["Atsuki Yamaguchi","Terufumi Morishita","Aline Villavicencio","Nikolaos Aletras"],"url":"https://arxiv.org/abs/2412.11704"}
{"created":"2025-04-28","title":"Optimizing ML Concurrent Computation and Communication with GPU DMA Engines","abstract":"Concurrent computation and communication (C3) is a pervasive paradigm in ML and other domains, making its performance optimization crucial. In this paper, we carefully characterize C3 in ML on GPUs, which are most widely deployed for ML training and inference. We observe that while C3 leads to performance uplifts, the uplifts are far lower than ideal speedups (serial computation and communication versus maximum of computation or communication; all times from isolated executions). That is, C3 on average achieves only 21% of ideal speedup. This is so, due to known challenges of compute and memory interference between concurrent GPU kernels (that is, sharing of GPU's compute units, caches and HBM).","authors":["Anirudha Agrawal","Shaizeen Aga","Suchita Pati","Mahzabeen Islam"],"url":"https://arxiv.org/abs/2412.14335"}
{"created":"2025-04-28","title":"Optimizing Queries with Many-to-Many Joins","abstract":"As database query processing techniques are being used to handle diverse workloads, a key emerging challenge is how to efficiently handle multi-way join queries containing multiple many-to-many joins. While uncommon in traditional enterprise settings that have been the focus of much of the query optimization work to date, such queries are seen frequently in other contexts such as graph workloads. This has led to much work on developing join algorithms for handling cyclic queries, on compressed (factorized) representations for more efficient storage of intermediate results, and on use of semi-joins or predicate transfer to avoid generating large redundant intermediate results. In this paper, we address a core query optimization problem in this context. Specifically, we introduce an improved cost model that more accurately captures the cost of a query plan in such scenarios, and we present several optimization algorithms for query optimization that incorporate these new cost functions. We present an extensive experimental evaluation, that compares the factorized representation approach with a full semi-join reduction approach as well as to an approach that uses bitvectors to eliminate tuples early through sideways information passing. We also present new analyses of robustness of these techniques to the choice of the join order, potentially eliminating the need for more complex query optimization and selectivity estimation techniques.","authors":["Hasara Kalumin","Amol Deshpande"],"url":"https://arxiv.org/abs/2412.16323"}
{"created":"2025-04-28","title":"Structure Learning in Gaussian Graphical Models from Glauber Dynamics","abstract":"Gaussian graphical model selection is an important paradigm with numerous applications, including biological network modeling, financial network modeling, and social network analysis. Traditional approaches assume access to independent and identically distributed (i.i.d) samples, which is often impractical in real-world scenarios. In this paper, we address Gaussian graphical model selection under observations from a more realistic dependent stochastic process known as Glauber dynamics. Glauber dynamics, also called the Gibbs sampler, is a Markov chain that sequentially updates the variables of the underlying model based on the statistics of the remaining model. Such models, aside from frequently being employed to generate samples from complex multivariate distributions, naturally arise in various settings, such as opinion consensus in social networks and clearing/stock-price dynamics in financial networks.","authors":["Vignesh Tirukkonda","Anirudh Rayas","Gautam Dasarathy"],"url":"https://arxiv.org/abs/2412.18594"}
{"created":"2025-04-28","title":"Adaptive Heuristics for Scheduling DNN Inferencing on Edge and Cloud for Personalized UAV Fleets","abstract":"Drone fleets with onboard cameras coupled with computer vision and DNN inferencing models can support diverse applications. One such novel domain is for one or more buddy drones to assist Visually Impaired People (VIPs) lead an active lifestyle. Video inferencing tasks from such drones can help both navigate the drone and provide situation awareness to the VIP, and hence have strict execution deadlines. We propose a deadline-driven heuristic, DEMS-A, to schedule diverse DNN tasks generated continuously to perform inferencing over video segments generated by multiple drones linked to an edge, with the option to execute on the cloud. We use strategies like task dropping, work stealing and migration, and dynamic adaptation to cloud variability, to guarantee a Quality of Service (QoS), i.e. maximize the utility and the number of tasks completed. We also introduce an additional Quality of Experience (QoE) metric useful to the assistive drone domain, which values the frequency of success for task types to ensure the responsiveness and reliability of the VIP application. We extend our DEMS solution to GEMS to solve this. We evaluate these strategies, using (i) an emulated setup of a fleet of over 80 drones supporting over 25 VIPs, with real DNN models executing on pre-recorded drone video streams, using Jetson Nano edges and AWS Lambda cloud functions, and (ii) a real-world setup of a Tello drone and a Jetson Orin Nano edge generating drone commands to follow a VIP in real-time. Our strategies present a task completion rate of up to 88%, up to 2.7x higher QoS utility compared to the baselines, a further 16% higher QoS utility while adapting to network variability, and up to 75% higher QoE utility. Our practical validation exhibits task completion of up to 87% for GEMS and 33% higher total utility of GEMS compared to edge-only.","authors":["Suman Raj","Radhika Mittal","Harshil Gupta","Yogesh Simmhan"],"url":"https://arxiv.org/abs/2412.20860"}
{"created":"2025-04-28","title":"VisTabNet: Adapting Vision Transformers for Tabular Data","abstract":"Although deep learning models have had great success in natural language processing and computer vision, we do not observe comparable improvements in the case of tabular data, which is still the most common data type used in biological, industrial and financial applications. In particular, it is challenging to transfer large-scale pre-trained models to downstream tasks defined on small tabular datasets. To address this, we propose VisTabNet -- a cross-modal transfer learning method, which allows for adapting Vision Transformer (ViT) with pre-trained weights to process tabular data. By projecting tabular inputs to patch embeddings acceptable by ViT, we can directly apply a pre-trained Transformer Encoder to tabular inputs. This approach eliminates the conceptual cost of designing a suitable architecture for processing tabular data, while reducing the computational cost of training the model from scratch. Experimental results on multiple small tabular datasets (less than 1k samples) demonstrate VisTabNet's superiority, outperforming both traditional ensemble methods and recent deep learning models. The proposed method goes beyond conventional transfer learning practice and shows that pre-trained image models can be transferred to solve tabular problems, extending the boundaries of transfer learning. We share our example implementation as a GitHub repository available at https://github.com/wwydmanski/VisTabNet.","authors":["Witold Wydma\\'nski","Ulvi Movsum-zada","Jacek Tabor","Marek \\'Smieja"],"url":"https://arxiv.org/abs/2501.00057"}
{"created":"2025-04-28","title":"Symmetries-enhanced Multi-Agent Reinforcement Learning","abstract":"Multi-agent reinforcement learning has emerged as a powerful framework for enabling agents to learn complex, coordinated behaviors but faces persistent challenges regarding its generalization, scalability and sample efficiency. Recent advancements have sought to alleviate those issues by embedding intrinsic symmetries of the systems in the policy. Yet, most dynamical systems exhibit little to no symmetries to exploit. This paper presents a novel framework for embedding extrinsic symmetries in multi-agent system dynamics that enables the use of symmetry-enhanced methods to address systems with insufficient intrinsic symmetries, expanding the scope of equivariant learning to a wide variety of MARL problems. Central to our framework is the Group Equivariant Graphormer, a group-modular architecture specifically designed for distributed swarming tasks. Extensive experiments on a swarm of symmetry-breaking quadrotors validate the effectiveness of our approach, showcasing its potential for improved generalization and zero-shot scalability. Our method achieves significant reductions in collision rates and enhances task success rates across a diverse range of scenarios and varying swarm sizes.","authors":["Nikolaos Bousias","Stefanos Pertigkiozoglou","Kostas Daniilidis","George Pappas"],"url":"https://arxiv.org/abs/2501.01136"}
{"created":"2025-04-28","title":"Decomposing and Fusing Intra- and Inter-Sensor Spatio-Temporal Signal for Multi-Sensor Wearable Human Activity Recognition","abstract":"Wearable Human Activity Recognition (WHAR) is a prominent research area within ubiquitous computing. Multi-sensor synchronous measurement has proven to be more effective for WHAR than using a single sensor. However, existing WHAR methods use shared convolutional kernels for indiscriminate temporal feature extraction across each sensor variable, which fails to effectively capture spatio-temporal relationships of intra-sensor and inter-sensor variables. We propose the DecomposeWHAR model consisting of a decomposition phase and a fusion phase to better model the relationships between modality variables. The decomposition creates high-dimensional representations of each intra-sensor variable through the improved Depth Separable Convolution to capture local temporal features while preserving their unique characteristics. The fusion phase begins by capturing relationships between intra-sensor variables and fusing their features at both the channel and variable levels. Long-range temporal dependencies are modeled using the State Space Model (SSM), and later cross-sensor interactions are dynamically captured through a self-attention mechanism, highlighting inter-sensor spatial correlations. Our model demonstrates superior performance on three widely used WHAR datasets, significantly outperforming state-of-the-art models while maintaining acceptable computational efficiency.","authors":["Haoyu Xie","Haoxuan Li","Chunyuan Zheng","Haonan Yuan","Guorui Liao","Jun Liao","Li Liu"],"url":"https://arxiv.org/abs/2501.10917"}
{"created":"2025-04-28","title":"Bitcoin: A Non-Continuous Time System","abstract":"In this paper, we explore the concept of time within Bitcoin's blockchain, which operates as a non-continuous time system. We focus on three core aspects that contribute to Bitcoin's time discontinuity: the random and distributed block generation process, the occurrence of forks and rollbacks that disrupt the linear progression of the blockchain, and the nature of transactions within this system, which are subject to potential reordering or invalidation. These elements combine to create a time structure in Bitcoin that is fundamentally different from the continuous, linear time systems typically seen in traditional computing and physics.","authors":["Bin Chen"],"url":"https://arxiv.org/abs/2501.11091"}
{"created":"2025-04-28","title":"The GenUI Study: Exploring the Design of Generative UI Tools to Support UX Practitioners and Beyond","abstract":"AI can now generate high-fidelity UI mock-up screens from a high-level textual description, promising to support UX practitioners' work. However, it remains unclear how UX practitioners would adopt such Generative UI (GenUI) models in a way that is integral and beneficial to their work. To answer this question, we conducted a formative study with 37 UX-related professionals that consisted of four roles: UX designers, UX researchers, software engineers, and product managers. Using a state-of-the-art GenUI tool, each participant went through a week-long, individual mini-project exercise with role-specific tasks, keeping a daily journal of their usage and experiences with GenUI, followed by a semi-structured interview. We report findings on participants' workflow using the GenUI tool, how GenUI can support all and each specific roles, and existing gaps between GenUI and users' needs and expectations, which lead to design implications to inform future work on GenUI development.","authors":["Xiang 'Anthony' Chen","Tiffany Knearem","Yang Li"],"url":"https://arxiv.org/abs/2501.13145"}
{"created":"2025-04-28","title":"Dual-Branch HNSW Approach with Skip Bridges and LID-Driven Optimization","abstract":"The Hierarchical Navigable Small World (HNSW) algorithm is widely used for approximate nearest neighbor (ANN) search, leveraging the principles of navigable small-world graphs. However, it faces some limitations. The first is the local optima problem, which arises from the algorithm's greedy search strategy, selecting neighbors based solely on proximity at each step. This often leads to cluster disconnections. The second limitation is that HNSW frequently fails to achieve logarithmic complexity, particularly in high-dimensional datasets, due to the exhaustive traversal through each layer. To address these limitations, we propose a novel algorithm that mitigates local optima and cluster disconnections while enhancing the construction speed, maintaining inference speed. The first component is a dual-branch HNSW structure with LID-based insertion mechanisms, enabling traversal from multiple directions. This improves outlier node capture, enhances cluster connectivity, accelerates construction speed and reduces the risk of local minima. The second component incorporates a bridge-building technique that bypasses redundant intermediate layers, maintaining inference and making up the additional computational overhead introduced by the dual-branch structure. Experiments on various benchmarks and datasets showed that our algorithm outperforms the original HNSW in both accuracy and speed. We evaluated six datasets across Computer Vision (CV), and Natural Language Processing (NLP), showing recall improvements of 18\\% in NLP, and up to 30\\% in CV tasks while reducing the construction time by up to 20\\% and maintaining the inference speed. We did not observe any trade-offs in our algorithm. Ablation studies revealed that LID-based insertion had the greatest impact on performance, followed by the dual-branch structure and bridge-building components.","authors":["Hy Nguyen","Nguyen Hung Nguyen","Nguyen Linh Bao Nguyen","Srikanth Thudumu","Hung Du","Rajesh Vasa","Kon Mouzakis"],"url":"https://arxiv.org/abs/2501.13992"}
{"created":"2025-04-28","title":"Local Control Networks (LCNs): Optimizing Flexibility in Neural Network Data Pattern Capture","abstract":"The widespread use of Multi-layer perceptrons (MLPs) often relies on a fixed activation function (e.g., ReLU, Sigmoid, Tanh) for all nodes within the hidden layers. While effective in many scenarios, this uniformity may limit the networks ability to capture complex data patterns. We argue that employing the same activation function at every node is suboptimal and propose leveraging different activation functions at each node to increase flexibility and adaptability. To achieve this, we introduce Local Control Networks (LCNs), which leverage B-spline functions to enable distinct activation curves at each node. Our mathematical analysis demonstrates the properties and benefits of LCNs over conventional MLPs. In addition, we demonstrate that more complex architectures, such as Kolmogorov-Arnold Networks (KANs), are unnecessary in certain scenarios, and LCNs can be a more efficient alternative. Empirical experiments on various benchmarks and datasets validate our theoretical findings. In computer vision tasks, LCNs achieve marginal improvements over MLPs and outperform KANs by approximately 5\\%, while also being more computationally efficient than KANs. In basic machine learning tasks, LCNs show a 1\\% improvement over MLPs and a 0.6\\% improvement over KANs. For symbolic formula representation tasks, LCNs perform on par with KANs, with both architectures outperforming MLPs. Our findings suggest that diverse activations at the node level can lead to improved performance and efficiency.","authors":["Hy Nguyen","Duy Khoa Pham","Srikanth Thudumu","Hung Du","Rajesh Vasa","Kon Mouzakis"],"url":"https://arxiv.org/abs/2501.14000"}
{"created":"2025-04-28","title":"Can We Govern the Agent-to-Agent Economy?","abstract":"Current approaches to AI governance often fall short in anticipating a future where AI agents manage critical tasks, such as financial operations, administrative functions, and beyond. While cryptocurrencies could serve as the foundation for monetizing value exchange in a collaboration and delegation dynamic among AI agents, a critical question remains: how can humans ensure meaningful oversight and control as a future economy of AI agents scales and evolves? In this philosophical exploration, we highlight emerging concepts in the industry to inform research and development efforts in anticipation of a future decentralized agentic economy.","authors":["Tomer Jordi Chaffer"],"url":"https://arxiv.org/abs/2501.16606"}
{"created":"2025-04-28","title":"Zero Estimation Cost Strategy for Witsenhausen Counterexample with Causal Encoder","abstract":"We propose a zero estimation cost (ZEC) scheme for causal-encoding noncausal-decoding vector-valued Witsenhausen counterexample based on the coordination coding result. In contrast to source coding, our goal is to communicate a controlled system state. The introduced ZEC scheme is a joint control-communication approach that transforms the system state into a sequence that can be efficiently communicated using block coding. Numerical results show that our approach significantly reduces the power required for achieving zero-estimation-cost state reconstruction at the decoder. In the second part, we introduce a more general non-zero estimation cost (Non-ZEC) scheme. We observe numerically that the Non-ZEC scheme operates as a time-sharing mechanism between the two-point strategy and the ZEC scheme. Overall, by leveraging block-coding gain, our proposed methods substantially improve the power-estimation trade-off for Witsenhausen counterexample.","authors":["Mengyuan Zhao","Tobias J. Oechtering","Ma\\\"el Le Treust"],"url":"https://arxiv.org/abs/2501.18308"}
{"created":"2025-04-28","title":"RankFlow: A Multi-Role Collaborative Reranking Workflow Utilizing Large Language Models","abstract":"In an Information Retrieval (IR) system, reranking plays a critical role by sorting candidate passages according to their relevance to a specific query. This process demands a nuanced understanding of the variations among passages linked to the query. In this work, we introduce RankFlow, a multi-role reranking workflow that leverages the capabilities of Large Language Models (LLMs) and role specializations to improve reranking performance. RankFlow enlists LLMs to fulfill four distinct roles: the query Rewriter, the pseudo Answerer, the passage Summarizer, and the Reranker. This orchestrated approach enables RankFlow to: (1) accurately interpret queries, (2) draw upon LLMs' extensive pre-existing knowledge, (3) distill passages into concise versions, and (4) assess passages in a comprehensive manner, resulting in notably better reranking results. Our experimental results reveal that RankFlow outperforms existing leading approaches on widely recognized IR benchmarks, such as TREC-DL, BEIR, and NovelEval. Additionally, we investigate the individual contributions of each role in RankFlow. Code is available at https://github.com/jincan333/RankFlow","authors":["Can Jin","Hongwu Peng","Anxiang Zhang","Nuo Chen","Jiahui Zhao","Xi Xie","Kuangzheng Li","Shuya Feng","Kai Zhong","Caiwen Ding","Dimitris N. Metaxas"],"url":"https://arxiv.org/abs/2502.00709"}
{"created":"2025-04-28","title":"UniGraph2: Learning a Unified Embedding Space to Bind Multimodal Graphs","abstract":"Existing foundation models, such as CLIP, aim to learn a unified embedding space for multimodal data, enabling a wide range of downstream web-based applications like search, recommendation, and content classification. However, these models often overlook the inherent graph structures in multimodal datasets, where entities and their relationships are crucial. Multimodal graphs (MMGs) represent such graphs where each node is associated with features from different modalities, while the edges capture the relationships between these entities. On the other hand, existing graph foundation models primarily focus on text-attributed graphs (TAGs) and are not designed to handle the complexities of MMGs. To address these limitations, we propose UniGraph2, a novel cross-domain graph foundation model that enables general representation learning on MMGs, providing a unified embedding space. UniGraph2 employs modality-specific encoders alongside a graph neural network (GNN) to learn a unified low-dimensional embedding space that captures both the multimodal information and the underlying graph structure. We propose a new cross-domain multi-graph pre-training algorithm at scale to ensure effective transfer learning across diverse graph domains and modalities. Additionally, we adopt a Mixture of Experts (MoE) component to align features from different domains and modalities, ensuring coherent and robust embeddings that unify the information across modalities. Extensive experiments on a variety of multimodal graph tasks demonstrate that UniGraph2 significantly outperforms state-of-the-art models in tasks such as representation learning, transfer learning, and multimodal generative tasks, offering a scalable and flexible solution for learning on MMGs.","authors":["Yufei He","Yuan Sui","Xiaoxin He","Yue Liu","Yifei Sun","Bryan Hooi"],"url":"https://arxiv.org/abs/2502.00806"}
{"created":"2025-04-28","title":"Factual Knowledge in Language Models: Robustness and Anomalies under Simple Temporal Context Variations","abstract":"This paper explores the robustness of language models (LMs) to variations in the temporal context within factual knowledge. It examines whether LMs can correctly associate a temporal context with a past fact valid over a defined period, by asking them to differentiate correct from incorrect contexts. The accuracy of LMs is analyzed along two dimensions: the distance of the incorrect context from the validity period and the granularity of the context. To this end, a dataset called TimeStress is introduced, enabling the evaluation of 18 diverse LMs. Results reveal that the best LM achieves perfect accuracy for only 6% of the studied facts, with critical errors that humans would not make. This work highlights the limitations of current LMs in temporal representation. We provide all data and code for further research.","authors":["Hichem Ammar Khodja","Fr\\'ed\\'eric B\\'echet","Quentin Brabant","Alexis Nasr","Gw\\'enol\\'e Lecorv\\'e"],"url":"https://arxiv.org/abs/2502.01220"}
{"created":"2025-04-28","title":"A Temporal Convolutional Network-Based Approach and a Benchmark Dataset for Colonoscopy Video Temporal Segmentation","abstract":"Following recent advancements in computer-aided detection and diagnosis systems for colonoscopy, the automated reporting of colonoscopy procedures is set to further revolutionize clinical practice. A crucial yet underexplored aspect in the development of these systems is the creation of computer vision models capable of autonomously segmenting full-procedure colonoscopy videos into anatomical sections and procedural phases. In this work, we aim to create the first open-access dataset for this task and propose a state-of-the-art approach, benchmarked against competitive models. We annotated the publicly available REAL-Colon dataset, consisting of 2.7 million frames from 60 complete colonoscopy videos, with frame-level labels for anatomical locations and colonoscopy phases across nine categories. We then present ColonTCN, a learning-based architecture that employs custom temporal convolutional blocks designed to efficiently capture long temporal dependencies for the temporal segmentation of colonoscopy videos. We also propose a dual k-fold cross-validation evaluation protocol for this benchmark, which includes model assessment on unseen, multi-center data.ColonTCN achieves state-of-the-art performance in classification accuracy while maintaining a low parameter count when evaluated using the two proposed k-fold cross-validation settings, outperforming competitive models. We report ablation studies to provide insights into the challenges of this task and highlight the benefits of the custom temporal convolutional blocks, which enhance learning and improve model efficiency. We believe that the proposed open-access benchmark and the ColonTCN approach represent a significant advancement in the temporal segmentation of colonoscopy procedures, fostering further open-access research to address this clinical need.","authors":["Carlo Biffi","Giorgio Roffo","Pietro Salvagnini","Andrea Cherubini"],"url":"https://arxiv.org/abs/2502.03430"}
{"created":"2025-04-28","title":"DCFormer: Efficient 3D Vision-Language Modeling with Decomposed Convolutions","abstract":"Vision-language models (VLMs) have been widely applied to 2D medical image analysis due to their ability to align visual and textual representations. However, extending VLMs to 3D imaging remains computationally challenging. Existing 3D VLMs often rely on Vision Transformers (ViTs), which are computationally expensive due to the quadratic complexity of self-attention, or on 3D convolutions, which require large numbers of parameters and FLOPs as kernel size increases. We introduce DCFormer, an efficient 3D image encoder that factorizes 3D convolutions into three parallel 1D convolutions along the depth, height, and width dimensions. This design preserves spatial information while significantly reducing computational cost. Integrated into a CLIP-based vision-language framework, DCFormer is trained and evaluated on CT-RATE, a dataset of 50,188 paired 3D chest CT volumes and radiology reports. In zero-shot and fine-tuned detection of 18 pathologies, as well as in image-text retrieval tasks, DCFormer consistently outperforms state-of-the-art 3D vision encoders, including CT-ViT, ViT, ConvNeXt, PoolFormer, and TransUNet. These results highlight DCFormer's potential for scalable, clinically deployable 3D medical VLMs. Our code is available at: https://github.com/mirthAI/DCFormer.","authors":["Gorkem Can Ates","Yu Xin","Kuang Gong","Wei Shao"],"url":"https://arxiv.org/abs/2502.05091"}
{"created":"2025-04-28","title":"Light Edge Fault Tolerant Graph Spanners","abstract":"There has recently been significant interest in fault tolerant spanners, which are spanners that still maintain their stretch guarantees after some nodes or edges fail. This work has culminated in an almost complete understanding of the three-way tradeoff between stretch, sparsity, and number of faults tolerated. However, despite some progress in metric settings, there have been no results to date on the tradeoff in general graphs between stretch, lightness, and number of faults tolerated.","authors":["Greg Bodwin","Michael Dinitz","Ama Koranteng","Lily Wang"],"url":"https://arxiv.org/abs/2502.10890"}
{"created":"2025-04-28","title":"EPO: Explicit Policy Optimization for Strategic Reasoning in LLMs via Reinforcement Learning","abstract":"Large Language Models (LLMs) have shown impressive reasoning capabilities in well-defined problems with clear solutions, such as mathematics and coding. However, they still struggle with complex real-world scenarios like business negotiations, which require strategic reasoning-an ability to navigate dynamic environments and align long-term goals amidst uncertainty. Existing methods for strategic reasoning face challenges in adaptability, scalability, and transferring strategies to new contexts. To address these issues, we propose explicit policy optimization (EPO) for strategic reasoning, featuring an LLM that provides strategies in open-ended action space and can be plugged into arbitrary LLM agents to motivate goal-directed behavior. To improve adaptability and policy transferability, we train the strategic reasoning model via multi-turn reinforcement learning (RL) using process rewards and iterative self-play, without supervised fine-tuning (SFT) as a preliminary step. Experiments across social and physical domains demonstrate EPO's ability of long-term goal alignment through enhanced strategic reasoning, achieving state-of-the-art performance on social dialogue and web navigation tasks. Our findings reveal various collaborative reasoning mechanisms emergent in EPO and its effectiveness in generating novel strategies, underscoring its potential for strategic reasoning in real-world applications. Code and data are available at https://github.com/AlibabaResearch/DAMO-ConvAI/tree/main/EPO.","authors":["Xiaoqian Liu","Ke Wang","Yongbin Li","Yuchuan Wu","Wentao Ma","Aobo Kong","Fei Huang","Jianbin Jiao","Junge Zhang"],"url":"https://arxiv.org/abs/2502.12486"}
{"created":"2025-04-28","title":"L4P: Low-Level 4D Vision Perception Unified","abstract":"The spatio-temporal relationship between the pixels of a video carries critical information for low-level 4D perception tasks. A single model that reasons about it should be able to solve several such tasks well. Yet, most state-of-the-art methods rely on architectures specialized for the task at hand. We present L4P, a feedforward, general-purpose architecture that solves low-level 4D perception tasks in a unified framework. L4P leverages a pre-trained ViT-based video encoder and combines it with per-task heads that are lightweight and therefore do not require extensive training. Despite its general and feedforward formulation, our method matches or surpasses the performance of existing specialized methods on both dense tasks, such as depth or optical flow estimation, and sparse tasks, such as 2D/3D tracking. Moreover, it solves all tasks at once in a time comparable to that of single-task methods.","authors":["Abhishek Badki","Hang Su","Bowen Wen","Orazio Gallo"],"url":"https://arxiv.org/abs/2502.13078"}
{"created":"2025-04-28","title":"Machine-generated text detection prevents language model collapse","abstract":"As Large Language Models (LLMs) become increasingly prevalent, their generated outputs are proliferating across the web, risking a future where machine-generated content dilutes human-authored text. Since online data is the primary resource for LLM pre-training, subsequent models could be trained on an unknown portion of synthetic samples. This will lead to model collapse, a degenerative process whereby LLMs reinforce their own errors, and ultimately yield a declining performance. In this study, we investigate the impact of decoding strategy on model collapse, analysing the characteristics of text at each model generation, the similarity to human references, and the resulting model performance. Using the decoding strategies that lead to the most significant degradation, we evaluate model collapse in more realistic scenarios where the origin of the data (human or synthetic) is unknown. We train a machine-generated text detector and propose an importance sampling approach to alleviate model collapse. Our method is validated on two LLM variants (GPT-2 and SmolLM2) on the open-ended text generation task. We demonstrate that it can not only prevent model collapse but also improve performance when sufficient human-authored samples are present. We release our code at https://github.com/GeorgeDrayson/model_collapse.","authors":["George Drayson","Emine Yilmaz","Vasileios Lampos"],"url":"https://arxiv.org/abs/2502.15654"}
{"created":"2025-04-28","title":"From System 1 to System 2: A Survey of Reasoning Large Language Models","abstract":"Achieving human-level intelligence requires refining the transition from the fast, intuitive System 1 to the slower, more deliberate System 2 reasoning. While System 1 excels in quick, heuristic decisions, System 2 relies on logical reasoning for more accurate judgments and reduced biases. Foundational Large Language Models (LLMs) excel at fast decision-making but lack the depth for complex reasoning, as they have not yet fully embraced the step-by-step analysis characteristic of true System 2 thinking. Recently, reasoning LLMs like OpenAI's o1/o3 and DeepSeek's R1 have demonstrated expert-level performance in fields such as mathematics and coding, closely mimicking the deliberate reasoning of System 2 and showcasing human-like cognitive abilities. This survey begins with a brief overview of the progress in foundational LLMs and the early development of System 2 technologies, exploring how their combination has paved the way for reasoning LLMs. Next, we discuss how to construct reasoning LLMs, analyzing their features, the core methods enabling advanced reasoning, and the evolution of various reasoning LLMs. Additionally, we provide an overview of reasoning benchmarks, offering an in-depth comparison of the performance of representative reasoning LLMs. Finally, we explore promising directions for advancing reasoning LLMs and maintain a real-time \\href{https://github.com/zzli2022/Awesome-Slow-Reason-System}{GitHub Repository} to track the latest developments. We hope this survey will serve as a valuable resource to inspire innovation and drive progress in this rapidly evolving field.","authors":["Zhong-Zhi Li","Duzhen Zhang","Ming-Liang Zhang","Jiaxin Zhang","Zengyan Liu","Yuxuan Yao","Haotian Xu","Junhao Zheng","Pei-Jie Wang","Xiuyi Chen","Yingying Zhang","Fei Yin","Jiahua Dong","Zhiwei Li","Bao-Long Bi","Ling-Rui Mei","Junfeng Fang","Zhijiang Guo","Le Song","Cheng-Lin Liu"],"url":"https://arxiv.org/abs/2502.17419"}
{"created":"2025-04-28","title":"FACTR: Force-Attending Curriculum Training for Contact-Rich Policy Learning","abstract":"Many contact-rich tasks humans perform, such as box pickup or rolling dough, rely on force feedback for reliable execution. However, this force information, which is readily available in most robot arms, is not commonly used in teleoperation and policy learning. Consequently, robot behavior is often limited to quasi-static kinematic tasks that do not require intricate force-feedback. In this paper, we first present a low-cost, intuitive, bilateral teleoperation setup that relays external forces of the follower arm back to the teacher arm, facilitating data collection for complex, contact-rich tasks. We then introduce FACTR, a policy learning method that employs a curriculum which corrupts the visual input with decreasing intensity throughout training. The curriculum prevents our transformer-based policy from over-fitting to the visual input and guides the policy to properly attend to the force modality. We demonstrate that by fully utilizing the force information, our method significantly improves generalization to unseen objects by 43\\% compared to baseline approaches without a curriculum. Video results, codebases, and instructions at https://jasonjzliu.com/factr/","authors":["Jason Jingzhou Liu","Yulong Li","Kenneth Shaw","Tony Tao","Ruslan Salakhutdinov","Deepak Pathak"],"url":"https://arxiv.org/abs/2502.17432"}
{"created":"2025-04-28","title":"EMT: A Visual Multi-Task Benchmark Dataset for Autonomous Driving in the Arab Gulf Region","abstract":"This paper introduces the Emirates Multi-Task (EMT) dataset, designed to support multi-task benchmarking within a unified framework. It comprises over 30,000 frames from a dash-camera perspective and 570,000 annotated bounding boxes, covering approximately 150 kilometers of driving routes that reflect the distinctive road topology, congestion patterns, and driving behavior of Gulf region traffic. The dataset supports three primary tasks: tracking, trajectory forecasting, and intention prediction. Each benchmark is accompanied by corresponding evaluations: (1) multi-agent tracking experiments addressing multi-class scenarios and occlusion handling; (2) trajectory forecasting evaluation using deep sequential and interaction-aware models; and (3) intention prediction experiments based on observed trajectories. The dataset is publicly available at https://avlab.io/emt-dataset, with pre-processing scripts and evaluation models at https://github.com/AV-Lab/emt-dataset.","authors":["Nadya Abdel Madjid","Murad Mebrahtu","Abdelmoamen Nasser","Bilal Hassan","Naoufel Werghi","Jorge Dias","Majid Khonji"],"url":"https://arxiv.org/abs/2502.19260"}
{"created":"2025-04-28","title":"Repurposing the scientific literature with vision-language models","abstract":"Leading vision-language models (VLMs) are trained on general Internet content, overlooking scientific journals' rich, domain-specific knowledge. Training on specialty-specific literature could yield high-performance, task-specific tools, enabling generative AI to match generalist models in specialty publishing, educational, and clinical tasks. We created NeuroPubs, a multimodal dataset of 23,000 Neurosurgery Publications articles (134M words, 78K image-caption pairs). Using NeuroPubs, VLMs generated publication-ready graphical abstracts (70% of 100 abstracts) and board-style questions indistinguishable from human-written ones (54% of 89,587 questions). We used these questions to train CNS-Obsidian, a 34B-parameter VLM. In a blinded, randomized controlled trial, our model demonstrated non-inferiority to then state-of-the-art GPT-4o in neurosurgical differential diagnosis (clinical utility, 40.62% upvotes vs. 57.89%, p=0.1150; accuracy, 59.38% vs. 65.79%, p=0.3797). Our pilot study demonstrates how training generative AI models on specialty-specific journal content - without large-scale internet data - results in high-performance academic and clinical tools, enabling domain-tailored AI across diverse fields.","authors":["Anton Alyakin","Jaden Stryker","Daniel Alexander Alber","Karl L. Sangwon","Jin Vivian Lee","Brandon Duderstadt","Akshay Save","David Kurland","Spencer Frome","Shrutika Singh","Jeff Zhang","Eunice Yang","Ki Yun Park","Cordelia Orillac","Aly A. Valliani","Sean Neifert","Albert Liu","Aneek Patel","Christopher Livia","Darryl Lau","Ilya Laufer","Peter A. Rozman","Eveline Teresa Hidalgo","Howard Riina","Rui Feng","Todd Hollon","Yindalon Aphinyanaphongs","John G. Golfinos","Laura Snyder","Eric Leuthardt","Douglas Kondziolka","Eric Karl Oermann"],"url":"https://arxiv.org/abs/2502.19546"}
{"created":"2025-04-28","title":"HALO: Hardware-aware quantization with low critical-path-delay weights for LLM acceleration","abstract":"Quantization is critical for efficiently deploying large language models (LLMs). Yet conventional methods remain hardware-agnostic, limited to bit-width constraints, and do not account for intrinsic circuit characteristics such as the timing behaviors and energy profiles of Multiply-Accumulate (MAC) units. This disconnect from circuit-level behavior limits the ability to exploit available timing margins and energy-saving opportunities, reducing the overall efficiency of deployment on modern accelerators.","authors":["Rohan Juneja","Shivam Aggarwal","Safeen Huda","Tulika Mitra","Li-Shiuan Peh"],"url":"https://arxiv.org/abs/2502.19662"}
{"created":"2025-04-28","title":"Unified Video Action Model","abstract":"A unified video and action model holds significant promise for robotics, where videos provide rich scene information for action prediction, and actions provide dynamics information for video prediction. However, effectively combining video generation and action prediction remains challenging, and current video generation-based methods struggle to match the performance of direct policy learning in action accuracy and inference speed. To bridge this gap, we introduce the Unified Video Action model (UVA), which jointly optimizes video and action predictions to achieve both high accuracy and efficient action inference. The key lies in learning a joint video-action latent representation and decoupling video-action decoding. The joint latent representation bridges the visual and action domains, effectively modeling the relationship between video and action sequences. Meanwhile, the decoupled decoding, powered by two lightweight diffusion heads, enables high-speed action inference by bypassing video generation during inference. Such a unified framework further enables versatile functionality through masked input training. By selectively masking actions or videos, a single model can tackle diverse tasks beyond policy learning, such as forward and inverse dynamics modeling and video generation. Via an extensive set of experiments, we demonstrate that UVA can serve as a general-purpose solution for a wide range of robotics tasks, such as policy learning, forward/inverse dynamics and video observation prediction, without compromising performance compared to methods tailored for specific applications. Results are best viewed on https://unified-video-action-model.github.io/.","authors":["Shuang Li","Yihuai Gao","Dorsa Sadigh","Shuran Song"],"url":"https://arxiv.org/abs/2503.00200"}
{"created":"2025-04-28","title":"Learning Actionable World Models for Industrial Process Control","abstract":"To go from (passive) process monitoring to active process control, an effective AI system must learn about the behavior of the complex system from very limited training data, forming an ad-hoc digital twin with respect to process inputs and outputs that captures the consequences of actions on the process's world. We propose a novel methodology based on learning world models that disentangles process parameters in the learned latent representation, allowing for fine-grained control. Representation learning is driven by the latent factors influencing the processes through contrastive learning within a joint embedding predictive architecture. This makes changes in representations predictable from changes in inputs and vice versa, facilitating interpretability of key factors responsible for process variations, paving the way for effective control actions to keep the process within operational bounds. The effectiveness of our method is validated on the example of plastic injection molding, demonstrating practical relevance in proposing specific control actions for a notoriously unstable process.","authors":["Peng Yan","Ahmed Abdulkadir","Gerrit A. Schatte","Giulia Aguzzi","Joonsu Gha","Nikola Pascher","Matthias Rosenthal","Yunlong Gao","Benjamin F. Grewe","Thilo Stadelmann"],"url":"https://arxiv.org/abs/2503.01411"}
{"created":"2025-04-28","title":"Revisiting Locally Differentially Private Protocols: Towards Better Trade-offs in Privacy, Utility, and Attack Resistance","abstract":"Local Differential Privacy (LDP) offers strong privacy protection, especially in settings in which the server collecting the data is untrusted. However, designing LDP mechanisms that achieve an optimal trade-off between privacy, utility and robustness to adversarial inference attacks remains challenging. In this work, we introduce a general multi-objective optimization framework for refining LDP protocols, enabling the joint optimization of privacy and utility under various adversarial settings. While our framework is flexible to accommodate multiple privacy and security attacks as well as utility metrics, in this paper, we specifically optimize for Attacker Success Rate (ASR) under \\emph{data reconstruction attack} as a concrete measure of privacy leakage and Mean Squared Error (MSE) as a measure of utility. More precisely, we systematically revisit these trade-offs by analyzing eight state-of-the-art LDP protocols and proposing refined counterparts that leverage tailored optimization techniques. Experimental results demonstrate that our proposed adaptive mechanisms consistently outperform their non-adaptive counterparts, achieving substantial reductions in ASR while preserving utility, and pushing closer to the ASR-MSE Pareto frontier. By bridging the gap between theoretical guarantees and real-world vulnerabilities, our framework enables modular and context-aware deployment of LDP mechanisms with tunable privacy-utility trade-offs.","authors":["H\\'eber H. Arcolezi","S\\'ebastien Gambs"],"url":"https://arxiv.org/abs/2503.01482"}
{"created":"2025-04-28","title":"Reinforcement Learning-based Threat Assessment","abstract":"In some game scenarios, due to the uncertainty of the number of enemy units and the priority of various attributes, the evaluation of the threat level of enemy units as well as the screening has been a challenging research topic, and the core difficulty lies in how to reasonably set the priority of different attributes in order to achieve quantitative evaluation of the threat. In this paper, we innovatively transform the problem of threat assessment into a reinforcement learning problem, and through systematic reinforcement learning training, we successfully construct an efficient neural network evaluator. The evaluator can not only comprehensively integrate the multidimensional attribute features of the enemy, but also effectively combine our state information, thus realizing a more accurate and scientific threat assessment.","authors":["Wuzhou Sun","Siyi Li","Qingxiang Zou","Zixing Liao"],"url":"https://arxiv.org/abs/2503.02612"}
{"created":"2025-04-28","title":"Rethinking Reuse in Dependency Supply Chains: Initial Analysis of NPM packages at the End of the Chain","abstract":"The success of modern software development can be largely attributed to the concept of code reuse, such as the ability to reuse existing functionality via third-party package dependencies, evident within massive package networks like NPM, PyPI and Maven. For a long time, the dominant philosophy has been to `reuse as much as possible, without thought for what is being depended upon', resulting in the formation of large dependency supply chains that spread throughout entire software ecosystems. Such heavy reliance on third-party packages has eventually brought forward resilience and maintenance concerns, such as security attacks and outdated dependencies. In this vision paper, we investigate packages that challenge the typical concepts of reuse--that is, packages with no dependencies themselves that bear the responsibility of being at the end of the dependency supply chain. We find that these end-of-chain packages vary in characteristics and not just packages that can be easily replaced: an active, well-maintained package at the end of the chain; a \"classical\" package that has remained unchanged for 11 years; a trivial package nested deep in the dependency chain; a package that may appear trivial; and a package that bundled up and absorbed its dependencies. The vision of this paper is to advocate for a shift in software development practices toward minimizing reliance on third-party packages, particularly those at the end of dependency supply chains. We argue that these end-of-chain packages offer unique insights, as they play a key role in the ecosystem.","authors":["Brittany Anne Reid","Raula Gaikovina Kula"],"url":"https://arxiv.org/abs/2503.02804"}
{"created":"2025-04-28","title":"Deep Cut-informed Graph Embedding and Clustering","abstract":"Graph clustering aims to divide the graph into different clusters. The recently emerging deep graph clustering approaches are largely built on graph neural networks (GNN). However, GNN is designed for general graph encoding and there is a common issue of representation collapse in existing GNN-based deep graph clustering algorithms. We attribute two main reasons for such issues: (i) the inductive bias of GNN models: GNNs tend to generate similar representations for proximal nodes. Since graphs often contain a non-negligible amount of inter-cluster links, the bias results in error message passing and leads to biased clustering; (ii) the clustering guided loss function: most traditional approaches strive to make all samples closer to pre-learned cluster centers, which causes a degenerate solution assigning all data points to a single label thus making all samples similar and less discriminative. To address these challenges, we investigate graph clustering from a graph cut perspective and propose an innovative and non-GNN-based Deep Cut-informed Graph embedding and Clustering framework, namely DCGC. This framework includes two modules: (i) cut-informed graph encoding; (ii) self-supervised graph clustering via optimal transport. For the encoding module, we derive a cut-informed graph embedding objective to fuse graph structure and attributes by minimizing their joint normalized cut. For the clustering module, we utilize the optimal transport theory to obtain the clustering assignments, which can balance the guidance of \"proximity to the pre-learned cluster center\". With the above two tailored designs, DCGC is more suitable for the graph clustering task, which can effectively alleviate the problem of representation collapse and achieve better performance. We conduct extensive experiments to demonstrate that our method is simple but effective compared with benchmarks.","authors":["Zhiyuan Ning","Zaitian Wang","Ran Zhang","Ping Xu","Kunpeng Liu","Pengyang Wang","Wei Ju","Pengfei Wang","Yuanchun Zhou","Erik Cambria","Chong Chen"],"url":"https://arxiv.org/abs/2503.06635"}
{"created":"2025-04-28","title":"Multi-Robot System for Cooperative Exploration in Unknown Environments: A Survey","abstract":"With the advancement of multi-robot technology, cooperative exploration tasks have garnered increasing attention. This paper presents a comprehensive review of multi-robot cooperative exploration systems. First, we review the evolution of robotic exploration and introduce a modular research framework tailored for multi-robot cooperative exploration. Based on this framework, we systematically categorize and summarize key system components. As a foundational module for multi-robot exploration, the localization and mapping module is primarily introduced by focusing on global and relative pose estimation, as well as multi-robot map merging techniques. The cooperative motion module is further divided into learning-based approaches and multi-stage planning, with the latter encompassing target generation, task allocation, and motion planning strategies. Given the communication constraints of real-world environments, we also analyze the communication module, emphasizing how robots exchange information within local communication ranges and under limited transmission capabilities. Finally, we discuss the challenges and future research directions for multi-robot cooperative exploration in light of real-world trends. This review aims to serve as a valuable reference for researchers and practitioners in the field.","authors":["Chuqi Wang","Chao Yu","Xin Xu","Yuman Gao","Xinyi Yang","Wenhao Tang","Shu'ang Yu","Yinuo Chen","Feng Gao","ZhuoZhu Jian","Xinlei Chen","Fei Gao","Boyu Zhou","Yu Wang"],"url":"https://arxiv.org/abs/2503.07278"}
{"created":"2025-04-28","title":"Analyzing the Usage of Donation Platforms for PyPI Libraries","abstract":"Software systems rely heavily on open source software (OSS) libraries, which offer benefits but also pose risks. When vulnerabilities arise, the OSS community may struggle to address them due to inactivity or lack of resources. Research highlights the link between OSS maintenance and financial support. To sustain the OSS ecosystem, maintainers should register on donation platforms and link these profiles on their project pages, enabling financial support from users and industry stakeholders. However, a detailed study on donation platform usage in OSS is missing. This study analyzes the adoption of donation platforms in the PyPI ecosystem. For each PyPI library, we retrieve assigned URLs, dependencies, and, when available, owner type and GitHub donation links. Using PageRank, we analyze different subsets of libraries from both a library and dependency chain perspective. Our findings reveal that donation platform links are often omitted from PyPI project pages and instead listed on GitHub repositories. GitHub Sponsors is the dominant platform, though many PyPI-listed links are outdated, emphasizing the need for automated link verification. Adoption rates vary significantly across libraries and dependency chains: while individual PyPI libraries show low adoption, those used as dependencies have much higher usage. This suggests that many dependencies actively seek financial support, benefiting developers relying on PyPI libraries.","authors":["Alexandros Tsakpinis","Alexander Pretschner"],"url":"https://arxiv.org/abs/2503.08263"}
{"created":"2025-04-28","title":"Can We Detect Failures Without Failure Data? Uncertainty-Aware Runtime Failure Detection for Imitation Learning Policies","abstract":"Recent years have witnessed impressive robotic manipulation systems driven by advances in imitation learning and generative modeling, such as diffusion- and flow-based approaches. As robot policy performance increases, so does the complexity and time horizon of achievable tasks, inducing unexpected and diverse failure modes that are difficult to predict a priori. To enable trustworthy policy deployment in safety-critical human environments, reliable runtime failure detection becomes important during policy inference. However, most existing failure detection approaches rely on prior knowledge of failure modes and require failure data during training, which imposes a significant challenge in practicality and scalability. In response to these limitations, we present FAIL-Detect, a modular two-stage approach for failure detection in imitation learning-based robotic manipulation. To accurately identify failures from successful training data alone, we frame the problem as sequential out-of-distribution (OOD) detection. We first distill policy inputs and outputs into scalar signals that correlate with policy failures and capture epistemic uncertainty. FAIL-Detect then employs conformal prediction (CP) as a versatile framework for uncertainty quantification with statistical guarantees. Empirically, we thoroughly investigate both learned and post-hoc scalar signal candidates on diverse robotic manipulation tasks. Our experiments show learned signals to be mostly consistently effective, particularly when using our novel flow-based density estimator. Furthermore, our method detects failures more accurately and faster than state-of-the-art (SOTA) failure detection baselines. These results highlight the potential of FAIL-Detect to enhance the safety and reliability of imitation learning-based robotic systems as they progress toward real-world deployment.","authors":["Chen Xu","Tony Khuong Nguyen","Emma Dixon","Christopher Rodriguez","Patrick Miller","Robert Lee","Paarth Shah","Rares Ambrus","Haruki Nishimura","Masha Itkina"],"url":"https://arxiv.org/abs/2503.08558"}
{"created":"2025-04-28","title":"The LZ78 Source","abstract":"We study a family of processes generated according to sequential probability assignments induced by the LZ78 universal compressor. We characterize entropic and distributional properties such as their entropy and relative entropy rates, finite-state compressibility and log loss of their realizations, and the empirical distributions that they induce. Though not quite stationary, these sources are \"almost stationary and ergodic;\" similar to stationary and ergodic processes, they satisfy a Shannon-McMillan-Breiman-type property: the normalized log probability of their realizations converges almost surely to their entropy rate. Further, they are locally \"almost i.i.d.\" in the sense that the finite-dimensional empirical distributions of their realizations converge almost surely to a deterministic i.i.d. law. However, unlike stationary ergodic sources, the finite-state compressibility of their realizations is almost surely strictly larger than their entropy rate by a \"Jensen gap.\" We present simulations demonstrating the theoretical results. Among their potential uses, these sources allow to gauge the performance of sequential probability models on non-Markovian non-stationary data.","authors":["Naomi Sagan","Amir Dembo","Tsachy Weissman"],"url":"https://arxiv.org/abs/2503.10574"}
{"created":"2025-04-28","title":"The Road to Hybrid Quantum Programs: Characterizing the Evolution from Classical to Hybrid Quantum Software","abstract":"Quantum computing exhibits the unique capability to natively and efficiently encode various natural phenomena, promising theoretical speedups of several orders of magnitude. However, not all computational tasks can be efficiently executed on quantum machines, giving rise to hybrid systems, where some portions of an application run on classical machines, while others utilize quantum resources. Efforts to identify quantum candidate code fragments that can meaningfully execute on quantum machines primarily rely on static code analysis. Yet, the state-of-the-art in static code analysis for quantum candidates remains in its infancy, with limited applicability to specific frameworks and languages, and a lack of generalizability. Existing methods often involve a trial-and-error approach, relying on the intuition and expertise of computer scientists, resulting in varying identification durations ranging from minutes to days for a single application.","authors":["Vincenzo De Maio","Ivona Brandic","Ewa Deelman","J\\\"urgen Cito"],"url":"https://arxiv.org/abs/2503.11450"}
{"created":"2025-04-28","title":"Set-based and Dynamical Feedback-augmented Hands-off Control","abstract":"A novel set-theoretical approach to hands-off control is proposed, which focuses on spatial arguments for command limitation, rather than temporal ones. By employing dynamical feedback alongside invariant set-based constraints, actuation is employed only to drive the system's state inside a \"hands-off region\" of its state-space, where the plant may freely evolve in open-loop configuration. A computationally-efficient procedure with strong theoretical guarantees is devised, and its effectiveness is showcased via an intuitive practical example.","authors":["Andrei Speril\\u{a}","Sorin Olaru","St\\'ephane Drobot"],"url":"https://arxiv.org/abs/2503.11795"}
{"created":"2025-04-28","title":"Effective and Efficient Cross-City Traffic Knowledge Transfer: A Privacy-Preserving Perspective","abstract":"Traffic prediction targets forecasting future traffic conditions using historical traffic data, serving a critical role in urban computing and transportation management. To mitigate the scarcity of traffic data while maintaining data privacy, numerous Federated Traffic Knowledge Transfer (FTT) approaches have been developed, which use transfer learning and federated learning to transfer traffic knowledge from data-rich cities to data-scarce cities, enhancing traffic prediction capabilities for the latter. However, current FTT approaches face challenges such as privacy leakage, cross-city data distribution discrepancies, low data quality, and inefficient knowledge transfer, limiting their privacy protection, effectiveness, robustness, and efficiency in real-world applications.","authors":["Zhihao Zeng","Ziquan Fang","Yuting Huang","Lu Chen","Yunjun Gao"],"url":"https://arxiv.org/abs/2503.11963"}
{"created":"2025-04-28","title":"Eval-PPO: Building an Efficient Threat Evaluator Using Proximal Policy Optimization","abstract":"In various game scenarios, selecting a fixed number of targets from multiple enemy units is an extremely challenging task. This difficulty stems from the complex relationship between the threat levels of enemy units and their feature characteristics, which complicates the design of rule-based evaluators. Moreover, traditional supervised learning methods face the challenge of lacking explicit labels during training when applied to this threat evaluation problem. In this study, we redefine the threat evaluation problem as a reinforcement learning task and introduce an efficient evaluator training algorithm, Eval-PPO, based on the Proximal Policy Optimization (PPO) algorithm. Eval-PPO integrates multidimensional enemy features and the state information of friendly units through systematic training, thereby achieving precise threat assessment. Compared with rule-based methods, Eval-PPO demonstrates a significant improvement in average success rate, with an increase of 17.84%.","authors":["Wuzhou Sun","Siyi Li","Qingxiang Zou","Zixing Liao"],"url":"https://arxiv.org/abs/2503.12098"}
{"created":"2025-04-28","title":"Med-R1: Reinforcement Learning for Generalizable Medical Reasoning in Vision-Language Models","abstract":"Vision-language models (VLMs) have achieved impressive progress in natural image reasoning, yet their potential in medical imaging remains underexplored. Medical vision-language tasks demand precise understanding and clinically coherent answers, which are difficult to achieve due to the complexity of medical data and the scarcity of high-quality expert annotations. These challenges limit the effectiveness of conventional supervised fine-tuning (SFT) and Chain-of-Thought (CoT) strategies that work well in general domains. To address these challenges, we propose Med-R1, a reinforcement learning (RL)-enhanced vision-language model designed to improve generalization and reliability in medical reasoning. Built on the DeepSeek strategy, Med-R1 adopts Group Relative Policy Optimization (GRPO) to encourage reward-guided learning beyond static annotations. We comprehensively evaluate Med-R1 across eight distinct medical imaging modalities. Med-R1 achieves a 29.94% improvement in average accuracy over its base model Qwen2-VL-2B, and even outperforms Qwen2-VL-72B-a model with 36x more parameters. To assess cross-task generalization, we further evaluate Med-R1 on five question types. Med-R1 outperforms Qwen2-VL-2B by 32.06% in question-type generalization, also surpassing Qwen2-VL-72B. We further explore the thinking process in Med-R1, a crucial component for the success of Deepseek-R1. Our results show that omitting intermediate rationales (No-Thinking-Med-R1) not only improves in-domain and cross-domain generalization with less training, but also challenges the assumption that more reasoning always helps. These findings suggest that in medical VQA, it is not reasoning itself, but its quality and domain alignment, that determine effectiveness. Together, these results highlight that RL improves medical reasoning and generalization, enabling efficient and reliable VLMs for real-world deployment.","authors":["Yuxiang Lai","Jike Zhong","Ming Li","Shitian Zhao","Xiaofeng Yang"],"url":"https://arxiv.org/abs/2503.13939"}
{"created":"2025-04-28","title":"Attribution Score Alignment in Explainable Data Management","abstract":"Different attribution-scores have been proposed to quantify the relevance of database tuples for a query answer from a database. Among them, we find Causal Responsibility, the Shapley Value, the Banzhaf Power-Index, and the Causal Effect. They have been analyzed in isolation, mainly in terms of computational properties. In this work, we start an investigation into the alignment of these scores on the basis of the queries at hand; that is, on whether they induce compatible rankings of tuples. We are able to identify vast classes of queries for which some pairs of scores are always aligned, and others for which they are not. It turns out that the presence of exogenous tuples makes a crucial difference in this regard.","authors":["Felipe Azua","Leopoldo Bertossi"],"url":"https://arxiv.org/abs/2503.14469"}
{"created":"2025-04-28","title":"Application of linear regression and quasi-Newton methods to the deep reinforcement learning in continuous action cases","abstract":"The linear regression (LR) method offers the advantage that optimal parameters can be calculated relatively easily, although its representation capability is limited than that of the deep learning technique. To improve deep reinforcement learning, the Least Squares Deep Q Network (LS-DQN) method was proposed by Levine et al., which combines Deep Q Network (DQN) with LR method. However, the LS-DQN method assumes that the actions are discrete. In this study, we propose the Double Least Squares Deep Deterministic Policy Gradient (DLS-DDPG) method to address this limitation. This method combines the LR method with the Deep Deterministic Policy Gradient (DDPG) technique, one of the representative deep reinforcement learning algorithms for continuous action cases. For the LR update of the critic network, DLS-DDPG uses an algorithm similar to the Fitted Q iteration, the method which LS-DQN adopted. In addition, we calculated the optimal action using the quasi-Newton method and used it as both the agent's action and the training data for the LR update of the actor network. Numerical experiments conducted in MuJoCo environments showed that the proposed method improved performance at least in some tasks, although there are difficulties such as the inability to make the regularization terms small.","authors":["Hisato Komatsu"],"url":"https://arxiv.org/abs/2503.14976"}
{"created":"2025-04-28","title":"Revisiting DRAM Read Disturbance: Identifying Inconsistencies Between Experimental Characterization and Device-Level Studies","abstract":"Modern DRAM is vulnerable to read disturbance (e.g., RowHammer and RowPress) that significantly undermines the robust operation of the system. Repeatedly opening and closing a DRAM row (RowHammer) or keeping a DRAM row open for a long period of time (RowPress) induces bitflips in nearby unaccessed DRAM rows. Prior works on DRAM read disturbance either 1) perform experimental characterization using commercial-off-the-shelf (COTS) DRAM chips to demonstrate the high-level characteristics of the read disturbance bitflips, or 2) perform device-level simulations to understand the low-level error mechanisms of the read disturbance bitflips.","authors":["Haocong Luo","\\.Ismail Emir Y\\\"uksel","Ataberk Olgun","A. Giray Ya\\u{g}l{\\i}k\\c{c}{\\i}","Onur Mutlu"],"url":"https://arxiv.org/abs/2503.16749"}
{"created":"2025-04-28","title":"GranQ: Granular Zero-Shot Quantization with Unified Layer-Channel Awareness","abstract":"Zero-shot quantization (ZSQ) enables neural network compression without training data, which is crucial in restricted data access environments. However, existing ZSQ methods suffer from significant activation loss in low-bit environments owing to their coarse-grained scaling strategy. To address this issue, we propose GranQ, a novel ZSQ approach that leverages layer-channel awareness to minimize the quantization error. Unlike conventional layer- or channel-wise quantization, GranQ dynamically adjusts quantization granularity by considering both layer- and channel-level activation distributions. This enables fine-grained quantization while minimizing activation distortion. Additionally, we introduce vectorized activation quantization, which enables efficient parallel computation and reduces computational overhead while preserving accuracy. GranQ achieves superior performance compared with those of state-of-the-art ZSQ methods that employ quantization-aware training. With these findings, we anticipate that GranQ will inspire novel research directions beyond conventional ZSQ approaches focused on data generation and model training.","authors":["Inpyo Hong","Youngwan Jo","Hyojeong Lee","Sunghyun Ahn","Sanghyun Park"],"url":"https://arxiv.org/abs/2503.18339"}
{"created":"2025-04-28","title":"Neuroplasticity in Artificial Intelligence -- An Overview and Inspirations on Drop In & Out Learning","abstract":"Artificial Intelligence (AI) has achieved new levels of performance and spread in public usage with the rise of deep neural networks (DNNs). Initially inspired by human neurons and their connections, NNs have become the foundation of AI models for many advanced architectures. However, some of the most integral processes in the human brain, particularly neurogenesis and neuroplasticity in addition to the more spread neuroapoptosis have largely been ignored in DNN architecture design. Instead, contemporary AI development predominantly focuses on constructing advanced frameworks, such as large language models, which retain a static structure of neural connections during training and inference. In this light, we explore how neurogenesis, neuroapoptosis, and neuroplasticity can inspire future AI advances. Specifically, we examine analogous activities in artificial NNs, introducing the concepts of ``dropin'' for neurogenesis and revisiting ``dropout'' and structural pruning for neuroapoptosis. We additionally suggest neuroplasticity combining the two for future large NNs in ``life-long learning'' settings following the biological inspiration. We conclude by advocating for greater research efforts in this interdisciplinary domain and identifying promising directions for future exploration.","authors":["Yupei Li","Manuel Milling","Bj\\\"orn W. Schuller"],"url":"https://arxiv.org/abs/2503.21419"}
{"created":"2025-04-28","title":"Physics-Informed Neural Network-Based Control for Grid-Forming Converter's Stability Under Overload Conditions","abstract":"Grid-forming converters (GFCs) are pivotal in maintaining frequency and voltage stability in modern distribution systems. However, a critical challenge arises when these converters encounter sudden power demands that exceed their rated capacity. Although GFCs are designed to manage DC source saturation and limit excessive AC currents, their ability to ensure sufficient power delivery under such constraints remains a significant concern. Existing studies often overlook this limitation, potentially compromising system stability during high-demand scenarios. This paper proposes a control strategy based on a physics-informed neural network (PINN) to improve GFC performance under overloaded conditions, effectively preventing switch failures and mitigating DC source saturation. The proposed approach outperforms conventional methods by maintaining stable voltage and frequency, even under significant load increases where traditional droop control alone proves inadequate. The post-disturbance operating point of GFCs remains unchanged using PINN-based control. Peak voltage deviation observed during transient reduced to 24.14\\%. Furthermore, the proposed method improves the rate of change of frequency (ROCOF) and rate of change of voltage (ROCOV), keeping both within acceptable limits. This significantly strengthens system resilience, particularly in inertia-less power networks.","authors":["Abhay Kumar","Dushyant Sharma","Mayukha Pal"],"url":"https://arxiv.org/abs/2503.21529"}
{"created":"2025-04-28","title":"Coverage-Guaranteed Speech Emotion Recognition via Calibrated Uncertainty-Adaptive Prediction Sets","abstract":"Road rage, driven by emotional outbursts, endangers road and public safety. Speech Emotion Recognition (SER) can detect early negative emotions to reduce accidents, but traditional methods (e.g., HMMs, LSTMs) using 1D speech signals face overfitting and miscalibration issues. This paper proposes a risk management framework ensuring statistically rigorous correctness coverage for test data. We separate a calibration set, design a binary loss function to check if ground-truth labels are in prediction sets, calibrated by data-driven threshold $\\lambda$. A joint loss function on the calibration set adjusts $\\lambda$ according to user-specified risk level $\\alpha$, bounding the test loss expectation by $\\alpha$. Evaluations on 6 models across 2 datasets show our framework strictly maintains average correctness coverage $\\geq 1-\\alpha$ and controls marginal error rates under various calibration-test splits (e.g., 0.1). Additionally, a small-batch online calibration framework based on local exchangeability is proposed for complex scenarios with data domain offset or non-IID batches. By constructing a non-negative test martingale, it ensures prediction set coverage in dynamic environments, validated via cross-dataset experiments.","authors":["Zijun Jia"],"url":"https://arxiv.org/abs/2503.22712"}
{"created":"2025-04-28","title":"SpINR: Neural Volumetric Reconstruction for FMCW Radars","abstract":"In this paper, we introduce SpINR, a novel framework for volumetric reconstruction using Frequency-Modulated Continuous-Wave (FMCW) radar data. Traditional radar imaging techniques, such as backprojection, often assume ideal signal models and require dense aperture sampling, leading to limitations in resolution and generalization. To address these challenges, SpINR integrates a fully differentiable forward model that operates natively in the frequency domain with implicit neural representations (INRs). This integration leverages the linear relationship between beat frequency and scatterer distance inherent in FMCW radar systems, facilitating more efficient and accurate learning of scene geometry. Additionally, by computing outputs for only the relevant frequency bins, our forward model achieves greater computational efficiency compared to time-domain approaches that process the entire signal before transformation. Through extensive experiments, we demonstrate that SpINR significantly outperforms classical backprojection methods and existing learning-based approaches, achieving higher resolution and more accurate reconstructions of complex scenes. This work represents the first application of neural volumetic reconstruction in the radar domain, offering a promising direction for future research in radar-based imaging and perception systems.","authors":["Harshvardhan Takawale","Nirupam Roy"],"url":"https://arxiv.org/abs/2503.23313"}
{"created":"2025-04-28","title":"Control Center Framework for Teleoperation Support of Automated Vehicles on Public Roads","abstract":"Implementing a teleoperation system with its various actors and interactions is challenging and requires an overview of the necessary functions. This work collects all tasks that arise in a control center for an automated vehicle fleet from literature and assigns them to the two roles Remote Operator and Fleet Manager. Focusing on the driving-related tasks of the remote operator, a process is derived that contains the sequence of tasks, associated vehicle states, and transitions between the states. The resulting state diagram shows all remote operator actions available to effectively resolve automated vehicle disengagements. Thus, the state diagram can be applied to existing legislation or modified based on prohibitions of specific interactions. The developed control center framework and included state diagram should serve as a basis for implementing and testing remote support for automated vehicles to be validated on public roads.","authors":["Maria-Magdalena Wolf","Niklas Krauss","Arwed Schmidt","Frank Diermeyer"],"url":"https://arxiv.org/abs/2503.24249"}
{"created":"2025-04-28","title":"FlowMotion: Target-Predictive Conditional Flow Matching for Jitter-Reduced Text-Driven Human Motion Generation","abstract":"Achieving high-fidelity and temporally smooth 3D human motion generation remains a challenge, particularly within resource-constrained environments. We introduce FlowMotion, a novel method leveraging Conditional Flow Matching (CFM). FlowMotion incorporates a training objective within CFM that focuses on more accurately predicting target motion in 3D human motion generation, resulting in enhanced generation fidelity and temporal smoothness while maintaining the fast synthesis times characteristic of flow-matching-based methods. FlowMotion achieves state-of-the-art jitter performance, achieving the best jitter in the KIT dataset and the second-best jitter in the HumanML3D dataset, and a competitive FID value in both datasets. This combination provides robust and natural motion sequences, offering a promising equilibrium between generation quality and temporal naturalness.","authors":["Manolo Canales Cuba","Vin\\'icius do Carmo Mel\\'icio","Jo\\~ao Paulo Gois"],"url":"https://arxiv.org/abs/2504.01338"}
{"created":"2025-04-28","title":"LRAGE: Legal Retrieval Augmented Generation Evaluation Tool","abstract":"Recently, building retrieval-augmented generation (RAG) systems to enhance the capability of large language models (LLMs) has become a common practice. Especially in the legal domain, previous judicial decisions play a significant role under the doctrine of stare decisis which emphasizes the importance of making decisions based on (retrieved) prior documents. However, the overall performance of RAG system depends on many components: (1) retrieval corpora, (2) retrieval algorithms, (3) rerankers, (4) LLM backbones, and (5) evaluation metrics. Here we propose LRAGE, an open-source tool for holistic evaluation of RAG systems focusing on the legal domain. LRAGE provides GUI and CLI interfaces to facilitate seamless experiments and investigate how changes in the aforementioned five components affect the overall accuracy. We validated LRAGE using multilingual legal benches including Korean (KBL), English (LegalBench), and Chinese (LawBench) by demonstrating how the overall accuracy changes when varying the five components mentioned above. The source code is available at https://github.com/hoorangyee/LRAGE.","authors":["Minhu Park","Hongseok Oh","Eunkyung Choi","Wonseok Hwang"],"url":"https://arxiv.org/abs/2504.01840"}
{"created":"2025-04-28","title":"Generative Evaluation of Complex Reasoning in Large Language Models","abstract":"With powerful large language models (LLMs) demonstrating superhuman reasoning capabilities, a critical question arises: Do LLMs genuinely reason, or do they merely recall answers from their extensive, web-scraped training datasets? Publicly released benchmarks inevitably become contaminated once incorporated into subsequent LLM training sets, undermining their reliability as faithful assessments. To address this, we introduce KUMO, a generative evaluation framework designed specifically for assessing reasoning in LLMs. KUMO synergistically combines LLMs with symbolic engines to dynamically produce diverse, multi-turn reasoning tasks that are partially observable and adjustable in difficulty. Through an automated pipeline, KUMO continuously generates novel tasks across open-ended domains, compelling models to demonstrate genuine generalization rather than memorization. We evaluated 23 state-of-the-art LLMs on 5,000 tasks across 100 domains created by KUMO, benchmarking their reasoning abilities against university students. Our findings reveal that many LLMs have outperformed university-level performance on easy reasoning tasks, and reasoning-scaled LLMs reach university-level performance on complex reasoning challenges. Moreover, LLM performance on KUMO tasks correlates strongly with results on newly released real-world reasoning benchmarks, underscoring KUMO's value as a robust, enduring assessment tool for genuine LLM reasoning capabilities.","authors":["Haowei Lin","Xiangyu Wang","Ruilin Yan","Baizhou Huang","Haotian Ye","Jianhua Zhu","Zihao Wang","James Zou","Jianzhu Ma","Yitao Liang"],"url":"https://arxiv.org/abs/2504.02810"}
{"created":"2025-04-28","title":"Scaling Open-Vocabulary Action Detection","abstract":"In this work, we focus on scaling open-vocabulary action detection. Existing approaches for action detection are predominantly limited to closed-set scenarios and rely on complex, parameter-heavy architectures. Extending these models to the open-vocabulary setting poses two key challenges: (1) the lack of large-scale datasets with many action classes for robust training, and (2) parameter-heavy adaptations to a pretrained vision-language contrastive model to convert it for detection, risking overfitting the additional non-pretrained parameters to base action classes. Firstly, we introduce an encoder-only multimodal model for video action detection, reducing the reliance on parameter-heavy additions for video action detection. Secondly, we introduce a simple weakly supervised training strategy to exploit an existing closed-set action detection dataset for pretraining. Finally, we depart from the ill-posed base-to-novel benchmark used by prior works in open-vocabulary action detection and devise a new benchmark to evaluate on existing closed-set action detection datasets without ever using them for training, showing novel results to serve as baselines for future work. Our code is available at: https://siatheindochinese.github.io/sia_act_page/","authors":["Zhen Hao Sia","Yogesh Singh Rawat"],"url":"https://arxiv.org/abs/2504.03096"}
{"created":"2025-04-28","title":"Extending Cox Proportional Hazards Model with Symbolic Non-Linear Log-Risk Functions for Survival Analysis","abstract":"The Cox proportional hazards (CPH) model has been widely applied in survival analysis to estimate relative risks across different subjects given multiple covariates. Traditional CPH models rely on a linear combination of covariates weighted with coefficients as the log-risk function, which imposes a strong and restrictive assumption, limiting generalization. Recent deep learning methods enable non-linear log-risk functions. However, they often lack interpretability due to the end-to-end training mechanisms. The implementation of Kolmogorov-Arnold Networks (KAN) offers new possibilities for extending the CPH model with fully transparent and symbolic non-linear log-risk functions. In this paper, we introduce Generalized Cox Proportional Hazards (GCPH) model, a novel method for survival analysis that leverages KAN to enable a non-linear mapping from covariates to survival outcomes in a fully symbolic manner. GCPH maintains the interpretability of traditional CPH models while allowing for the estimation of non-linear log-risk functions. Experiments conducted on both synthetic data and various public benchmarks demonstrate that GCPH achieves competitive performance in terms of prediction accuracy and exhibits superior interpretability compared to current state-of-the-art methods.","authors":["Jiaxiang Cheng","Guoqiang Hu"],"url":"https://arxiv.org/abs/2504.04353"}
{"created":"2025-04-28","title":"Lightweight and Direct Document Relevance Optimization for Generative Information Retrieval","abstract":"Generative information retrieval (GenIR) is a promising neural retrieval paradigm that formulates document retrieval as a document identifier (docid) generation task, allowing for end-to-end optimization toward a unified global retrieval objective. However, existing GenIR models suffer from token-level misalignment, where models trained to predict the next token often fail to capture document-level relevance effectively. While reinforcement learning-based methods, such as reinforcement learning from relevance feedback (RLRF), aim to address this misalignment through reward modeling, they introduce significant complexity, requiring the optimization of an auxiliary reward function followed by reinforcement fine-tuning, which is computationally expensive and often unstable. To address these challenges, we propose direct document relevance optimization (DDRO), which aligns token-level docid generation with document-level relevance estimation through direct optimization via pairwise ranking, eliminating the need for explicit reward modeling and reinforcement learning. Experimental results on benchmark datasets, including MS MARCO document and Natural Questions, show that DDRO outperforms reinforcement learning-based methods, achieving a 7.4% improvement in MRR@10 for MS MARCO and a 19.9% improvement for Natural Questions. These findings highlight DDRO's potential to enhance retrieval effectiveness with a simplified optimization approach. By framing alignment as a direct optimization problem, DDRO simplifies the ranking optimization pipeline of GenIR models while offering a viable alternative to reinforcement learning-based methods.","authors":["Kidist Amde Mekonnen","Yubao Tang","Maarten de Rijke"],"url":"https://arxiv.org/abs/2504.05181"}
{"created":"2025-04-28","title":"Three-Factor Learning in Spiking Neural Networks: An Overview of Methods and Trends from a Machine Learning Perspective","abstract":"Three-factor learning rules in Spiking Neural Networks (SNNs) have emerged as a crucial extension to traditional Hebbian learning and Spike-Timing-Dependent Plasticity (STDP), incorporating neuromodulatory signals to improve adaptation and learning efficiency. These mechanisms enhance biological plausibility and facilitate improved credit assignment in artificial neural systems. This paper takes a view on this topic from a machine learning perspective, providing an overview of recent advances in three-factor learning, discusses theoretical foundations, algorithmic implementations, and their relevance to reinforcement learning and neuromorphic computing. In addition, we explore interdisciplinary approaches, scalability challenges, and potential applications in robotics, cognitive modeling, and AI systems. Finally, we highlight key research gaps and propose future directions for bridging the gap between neuroscience and artificial intelligence.","authors":["Szymon Mazurek","Jakub Caputa","Jan K. Argasi\\'nski","Maciej Wielgosz"],"url":"https://arxiv.org/abs/2504.05341"}
{"created":"2025-04-28","title":"AMAD: AutoMasked Attention for Unsupervised Multivariate Time Series Anomaly Detection","abstract":"Unsupervised multivariate time series anomaly detection (UMTSAD) plays a critical role in various domains, including finance, networks, and sensor systems. In recent years, due to the outstanding performance of deep learning in general sequential tasks, many models have been specialized for deep UMTSAD tasks and have achieved impressive results, particularly those based on the Transformer and self-attention mechanisms. However, the sequence anomaly association assumptions underlying these models are often limited to specific predefined patterns and scenarios, such as concentrated or peak anomaly patterns. These limitations hinder their ability to generalize to diverse anomaly situations, especially where the lack of labels poses significant challenges. To address these issues, we propose AMAD, which integrates \\textbf{A}uto\\textbf{M}asked Attention for UMTS\\textbf{AD} scenarios. AMAD introduces a novel structure based on the AutoMask mechanism and an attention mixup module, forming a simple yet generalized anomaly association representation framework. This framework is further enhanced by a Max-Min training strategy and a Local-Global contrastive learning approach. By combining multi-scale feature extraction with automatic relative association modeling, AMAD provides a robust and adaptable solution to UMTSAD challenges. Extensive experimental results demonstrate that the proposed model achieving competitive performance results compared to SOTA benchmarks across a variety of datasets.","authors":["Tiange Huang","Yongjun Li"],"url":"https://arxiv.org/abs/2504.06643"}
{"created":"2025-04-28","title":"Coreset Strikes Back: Improved Parameterized Approximation Schemes for (Constrained) k-Median/Means","abstract":"Algorithmic scatter dimension is a notion of metric spaces introduced recently by Abbasi et al. (FOCS 2023), which unifies many well-known metric spaces, including continuous Euclidean space, bounded doubling space, planar and bounded treewidth metrics. Recently, Bourneuf and Pilipczuk (SODA 2025) showed that metrics induced by graphs from any fixed proper minor closed graph class have bounded scatter dimension. Abbasi et al. presented a unified approach to obtain EPASes (i.e., $(1+\\epsilon)$-approximations running in time FPT in $k$ and $\\epsilon$) for $k$-Clustering in metrics of bounded scatter dimension. However, a seemingly inherent limitation of their approach was that it could only handle clustering objectives where each point was assigned to the closest chosen center. They explicitly asked, if there exist EPASes for constrained $k$-Clustering in metrics of bounded scatter dimension.","authors":["Sujoy Bhore","Ameet Gadekar","Tanmay Inamdar"],"url":"https://arxiv.org/abs/2504.06980"}
{"created":"2025-04-28","title":"Deep Learning-based Intrusion Detection Systems: A Survey","abstract":"Intrusion Detection Systems (IDS) have long been a hot topic in the cybersecurity community. In recent years, with the introduction of deep learning (DL) techniques, IDS have made great progress due to their increasing generalizability. The rationale behind this is that by learning the underlying patterns of known system behaviors, IDS detection can be generalized to intrusions that exploit zero-day vulnerabilities. In this survey, we refer to this type of IDS as DL-based IDS (DL-IDS). From the perspective of DL, this survey systematically reviews all the stages of DL-IDS, including data collection, log storage, log parsing, graph summarization, attack detection, and attack investigation. To accommodate current researchers, a section describing the publicly available benchmark datasets is included. This survey further discusses current challenges and potential future research directions, aiming to help researchers understand the basic ideas and visions of DL-IDS research, as well as to motivate their research interests.","authors":["Zhiwei Xu","Yujuan Wu","Shiheng Wang","Jiabao Gao","Tian Qiu","Ziqi Wang","Hai Wan","Xibin Zhao"],"url":"https://arxiv.org/abs/2504.07839"}
{"created":"2025-04-28","title":"Can Reasoning LLMs Enhance Clinical Document Classification?","abstract":"Clinical document classification is essential for converting unstructured medical texts into standardised ICD-10 diagnoses, yet it faces challenges due to complex medical language, privacy constraints, and limited annotated datasets. Large Language Models (LLMs) offer promising improvements in accuracy and efficiency for this task. This study evaluates the performance and consistency of eight LLMs; four reasoning (Qwen QWQ, Deepseek Reasoner, GPT o3 Mini, Gemini 2.0 Flash Thinking) and four non-reasoning (Llama 3.3, GPT 4o Mini, Gemini 2.0 Flash, Deepseek Chat); in classifying clinical discharge summaries using the MIMIC-IV dataset. Using cTAKES to structure clinical narratives, models were assessed across three experimental runs, with majority voting determining final predictions. Results showed that reasoning models outperformed non-reasoning models in accuracy (71% vs 68%) and F1 score (67% vs 60%), with Gemini 2.0 Flash Thinking achieving the highest accuracy (75%) and F1 score (76%). However, non-reasoning models demonstrated greater stability (91% vs 84% consistency). Performance varied across ICD-10 codes, with reasoning models excelling in complex cases but struggling with abstract categories. Findings indicate a trade-off between accuracy and consistency, suggesting that a hybrid approach could optimise clinical coding. Future research should explore multi-label classification, domain-specific fine-tuning, and ensemble methods to enhance model reliability in real-world applications.","authors":["Akram Mustafa","Usman Naseem","Mostafa Rahimi Azghadi"],"url":"https://arxiv.org/abs/2504.08040"}
{"created":"2025-04-28","title":"Spatial Audio Processing with Large Language Model on Wearable Devices","abstract":"Integrating spatial context into large language models (LLMs) has the potential to revolutionize human-computer interaction, particularly in wearable devices. In this work, we present a novel system architecture that incorporates spatial speech understanding into LLMs, enabling contextually aware and adaptive applications for wearable technologies. Our approach leverages microstructure-based spatial sensing to extract precise Direction of Arrival (DoA) information using a monaural microphone. To address the lack of existing dataset for microstructure-assisted speech recordings, we synthetically create a dataset called OmniTalk by using the LibriSpeech dataset. This spatial information is fused with linguistic embeddings from OpenAI's Whisper model, allowing each modality to learn complementary contextual representations. The fused embeddings are aligned with the input space of LLaMA-3.2 3B model and fine-tuned with lightweight adaptation technique LoRA to optimize for on-device processing. SING supports spatially-aware automatic speech recognition (ASR), achieving a mean error of $25.72^\\circ$-a substantial improvement compared to the 88.52$^\\circ$ median error in existing work-with a word error rate (WER) of 5.3. SING also supports soundscaping, for example, inference how many people were talking and their directions, with up to 5 people and a median DoA error of 16$^\\circ$. Our system demonstrates superior performance in spatial speech understanding while addressing the challenges of power efficiency, privacy, and hardware constraints, paving the way for advanced applications in augmented reality, accessibility, and immersive experiences.","authors":["Ayushi Mishra","Yang Bai","Priyadarshan Narayanasamy","Nakul Garg","Nirupam Roy"],"url":"https://arxiv.org/abs/2504.08907"}
{"created":"2025-04-28","title":"Rethinking Few-Shot Image Fusion: Granular Ball Priors Enable General-Purpose Deep Fusion","abstract":"In image fusion tasks, the absence of real fused images as priors presents a fundamental challenge. Most deep learning-based fusion methods rely on large-scale paired datasets to extract global weighting features from raw images, thereby generating fused outputs that approximate real fused images. In contrast to previous studies, this paper explores few-shot training of neural networks under the condition of having prior knowledge. We propose a novel fusion framework named GBFF, and a Granular Ball Significant Extraction algorithm specifically designed for the few-shot prior setting. All pixel pairs involved in the fusion process are initially modeled as a Coarse-Grained Granular Ball. At the local level, Fine-Grained Granular Balls are used to slide through the brightness space to extract Non-Salient Pixel Pairs, and perform splitting operations to obtain Salient Pixel Pairs. Pixel-wise weights are then computed to generate a pseudo-supervised image. At the global level, pixel pairs with significant contributions to the fusion process are categorized into the Positive Region, while those whose contributions cannot be accurately determined are assigned to the Boundary Region. The Granular Ball performs modality-aware adaptation based on the proportion of the positive region, thereby adjusting the neural network's loss function and enabling it to complement the information of the boundary region. Extensive experiments demonstrate the effectiveness of both the proposed algorithm and the underlying theory. Compared with state-of-the-art (SOTA) methods, our approach shows strong competitiveness in terms of both fusion time and image expressiveness. Our code is publicly available at:","authors":["Minjie Deng","Yan Wei","Hao Zhai","An Wu","Yuncan Ouyang","Qianyao Peng"],"url":"https://arxiv.org/abs/2504.08937"}
{"created":"2025-04-28","title":"CAShift: Benchmarking Log-Based Cloud Attack Detection under Normality Shift","abstract":"With the rapid advancement of cloud-native computing, securing cloud environments has become an important task. Log-based Anomaly Detection (LAD) is the most representative technique used in different systems for attack detection and safety guarantee, where multiple LAD methods and relevant datasets have been proposed. However, even though some of these datasets are specifically prepared for cloud systems, they only cover limited cloud behaviors and lack information from a whole-system perspective. Another critical issue to consider is normality shift, which implies that the test distribution could differ from the training distribution and highly affect the performance of LAD. Unfortunately, existing works only focus on simple shift types such as chronological changes, while other cloud-specific shift types are ignored. Therefore, a dataset that captures diverse cloud system behaviors and various types of normality shifts is essential.","authors":["Jiongchi Yu","Xiaofei Xie","Qiang Hu","Bowen Zhang","Ziming Zhao","Yun Lin","Lei Ma","Ruitao Feng","Frank Liauw"],"url":"https://arxiv.org/abs/2504.09115"}
{"created":"2025-04-28","title":"MSCoT: Structured Chain-of-Thought Generation for Multiple Programming Languages","abstract":"With the rapid development of code intelligence, the application of multiple programming languages is becoming increasingly widespread. However, most existing code generation models mainly focus on a single or a few programming languages, resulting in unsatisfactory performance in a multilingual environment. Chain-of-Thought (CoT) reasoning can significantly improve the performance of the model without the need for retraining or fine-tuning the code generation model by reasonably decomposing complex code generation tasks into multiple subtasks and gradually deriving solutions for each subtask. Nevertheless, the existing CoT generation methods mainly concentrate on Python code, and the performance on other programming languages remains unclear. To fill this gap, we first constructed a CoT generation dataset for 12 programming languages through multi-agent technology. On this basis, we proposed a CoT generation method MSCoT applicable to multiple programming languages. By introducing CoT into the code generation large model, the performance of the code generation large model in a multilingual environment can be improved. Through large-scale empirical research, we compared the generalization abilities of MSCoT and the existing CoT generation methods on multiple programming languages and proved the effectiveness of MSCoT for multiple programming languages. In addition, we also designed a human study to prove the quality of the CoT generated by MSCoT. Finally, we opensourced the model and dataset of MSCoT to promote the research on CoT generation for multiple programming languages.","authors":["Naizhu Jin","Zhong Li","Tian Zhang","Qingkai Zeng"],"url":"https://arxiv.org/abs/2504.10178"}
{"created":"2025-04-28","title":"MSCRS: Multi-modal Semantic Graph Prompt Learning Framework for Conversational Recommender Systems","abstract":"Conversational Recommender Systems (CRSs) aim to provide personalized recommendations by interacting with users through conversations. Most existing studies of CRS focus on extracting user preferences from conversational contexts. However, due to the short and sparse nature of conversational contexts, it is difficult to fully capture user preferences by conversational contexts only. We argue that multi-modal semantic information can enrich user preference expressions from diverse dimensions (e.g., a user preference for a certain movie may stem from its magnificent visual effects and compelling storyline). In this paper, we propose a multi-modal semantic graph prompt learning framework for CRS, named MSCRS. First, we extract textual and image features of items mentioned in the conversational contexts. Second, we capture higher-order semantic associations within different semantic modalities (collaborative, textual, and image) by constructing modality-specific graph structures. Finally, we propose an innovative integration of multi-modal semantic graphs with prompt learning, harnessing the power of large language models to comprehensively explore high-dimensional semantic relationships. Experimental results demonstrate that our proposed method significantly improves accuracy in item recommendation, as well as generates more natural and contextually relevant content in response generation.","authors":["Yibiao Wei","Jie Zou","Weikang Guo","Guoqing Wang","Xing Xu","Yang Yang"],"url":"https://arxiv.org/abs/2504.10921"}
{"created":"2025-04-28","title":"BitNet b1.58 2B4T Technical Report","abstract":"We introduce BitNet b1.58 2B4T, the first open-source, native 1-bit Large Language Model (LLM) at the 2-billion parameter scale. Trained on a corpus of 4 trillion tokens, the model has been rigorously evaluated across benchmarks covering language understanding, mathematical reasoning, coding proficiency, and conversational ability. Our results demonstrate that BitNet b1.58 2B4T achieves performance on par with leading open-weight, full-precision LLMs of similar size, while offering significant advantages in computational efficiency, including substantially reduced memory footprint, energy consumption, and decoding latency. To facilitate further research and adoption, the model weights are released via Hugging Face along with open-source inference implementations for both GPU and CPU architectures.","authors":["Shuming Ma","Hongyu Wang","Shaohan Huang","Xingxing Zhang","Ying Hu","Ting Song","Yan Xia","Furu Wei"],"url":"https://arxiv.org/abs/2504.12285"}
{"created":"2025-04-28","title":"Deriving Equivalent Symbol-Based Decision Models from Feedforward Neural Networks","abstract":"Artificial intelligence (AI) has emerged as a transformative force across industries, driven by advances in deep learning and natural language processing, and fueled by large-scale data and computing resources. Despite its rapid adoption, the opacity of AI systems poses significant challenges to trust and acceptance.","authors":["Sebastian Seidel","Uwe M. Borghoff"],"url":"https://arxiv.org/abs/2504.12446"}
{"created":"2025-04-28","title":"Antidistillation Sampling","abstract":"Frontier models that generate extended reasoning traces inadvertently produce rich token sequences that can facilitate model distillation. Recognizing this vulnerability, model owners may seek sampling strategies that limit the effectiveness of distillation without compromising model performance. Antidistillation sampling provides exactly this capability. By strategically modifying a model's next-token probability distribution, antidistillation sampling poisons reasoning traces, rendering them significantly less effective for distillation while preserving the model's practical utility. For further details, see https://antidistillation.com.","authors":["Yash Savani","Asher Trockman","Zhili Feng","Avi Schwarzschild","Alexander Robey","Marc Finzi","J. Zico Kolter"],"url":"https://arxiv.org/abs/2504.13146"}
{"created":"2025-04-28","title":"Equi-Euler GraphNet: An Equivariant, Temporal-Dynamics Informed Graph Neural Network for Dual Force and Trajectory Prediction in Multi-Body Systems","abstract":"Accurate real-time modeling of multi-body dynamical systems is essential for enabling digital twin applications across industries. While many data-driven approaches aim to learn system dynamics, jointly predicting internal loads and system trajectories remains a key challenge. This dual prediction is especially important for fault detection and predictive maintenance, where internal loads-such as contact forces-act as early indicators of faults, reflecting wear or misalignment before affecting motion. These forces also serve as inputs to degradation models (e.g., crack growth), enabling damage prediction and remaining useful life estimation. We propose Equi-Euler GraphNet, a physics-informed graph neural network (GNN) that simultaneously predicts internal forces and global trajectories in multi-body systems. In this mesh-free framework, nodes represent system components and edges encode interactions. Equi-Euler GraphNet introduces two inductive biases: (1) an equivariant message-passing scheme, interpreting edge messages as interaction forces consistent under Euclidean transformations; and (2) a temporal-aware iterative node update mechanism, based on Euler integration, to capture influence of distant interactions over time. Tailored for cylindrical roller bearings, it decouples ring dynamics from constrained motion of rolling elements. Trained on high-fidelity multiphysics simulations, Equi-Euler GraphNet generalizes beyond the training distribution, accurately predicting loads and trajectories under unseen speeds, loads, and configurations. It outperforms state-of-the-art GNNs focused on trajectory prediction, delivering stable rollouts over thousands of time steps with minimal error accumulation. Achieving up to a 200x speedup over conventional solvers while maintaining comparable accuracy, it serves as an efficient reduced-order model for digital twins, design, and maintenance.","authors":["Vinay Sharma","R\\'emi Tanguy Oddon","Pietro Tesini","Jens Ravesloot","Cees Taal","Olga Fink"],"url":"https://arxiv.org/abs/2504.13768"}
{"created":"2025-04-28","title":"Learning by gaming, coding and making with EDUMING: A new approach to utilising atypical digital games for learning","abstract":"Papert's constructionism makes it clear that learning is particularly effective when learners create tangible artifacts and share and discuss them in social contexts. Technological progress in recent decades has created numerous opportunities for learners to not only passively consume media, but to actively shape it through construction. This article uses the EDUMING concept to present a new method to simplify the development of digital learning games and thus support their integration into learning situations. A key difference between the concept and established ideas such as game-based learning, gamification, serious games, etc. is that games are not closed and are consumed passively, but can also be actively developed by users individually by modifying the source code with the help of an IDE. As part of an empirical study, the usability of the game \"Professor Chip's Learning Quest\" (PCLQ) is recorded, as well as previous experience with digital learning games and the acceptance and motivation to use new technologies. The purpose of this article is to test the PCLQ digital learning game, developed according to the EDUMING concept, as part of an exploratory study regarding its usability, acceptance and suitability for use in schools. The study is intended as a first empirical approach to practical testing of the concept.","authors":["Stefan Pietrusky"],"url":"https://arxiv.org/abs/2504.13878"}
{"created":"2025-04-28","title":"Using customized GPT to develop prompting proficiency in architectural AI-generated images","abstract":"This research investigates the use of customized GPT models to enhance prompting proficiency among architecture students when generating AI-driven images. Prompt engineering is increasingly essential in architectural education due to the widespread adoption of generative AI tools. This study utilized a mixed-methods experimental design involving architecture students divided into three distinct groups: a control group receiving no structured support, a second group provided with structured prompting guides, and a third group supported by both structured guides and interactive AI personas. Students engaged in reverse engineering tasks, first guessing provided image prompts and then generating their own prompts, aiming to boost critical thinking and prompting skills. Variables examined included time spent prompting, word count, prompt similarity, and concreteness. Quantitative analysis involved correlation assessments between these variables and a one-way ANOVA to evaluate differences across groups. While several correlations showed meaningful relationships, not all were statistically significant. ANOVA results indicated statistically significant improvements in word count, similarity, and concreteness, especially in the group supported by AI personas and structured prompting guides. Qualitative feedback complemented these findings, revealing enhanced confidence and critical thinking skills in students. These results suggest tailored GPT interactions substantially improve students' ability to communicate architectural concepts clearly and effectively.","authors":["Juan David Salazar Rodriguez","Sam Conrad Joyce","Julfendi"],"url":"https://arxiv.org/abs/2504.13948"}
{"created":"2025-04-28","title":"Transforming Hyperspectral Images Into Chemical Maps: An End-to-End Deep Learning Approach","abstract":"Current approaches to chemical map generation from hyperspectral images are based on models such as partial least squares (PLS) regression, generating pixel-wise predictions that do not consider spatial context and suffer from a high degree of noise. This study proposes an end-to-end deep learning approach using a modified version of U-Net and a custom loss function to directly obtain chemical maps from hyperspectral images, skipping all intermediate steps required for traditional pixel-wise analysis. We compare the U-Net with the traditional PLS regression on a real dataset of pork belly samples with associated mean fat reference values. The U-Net obtains a test set root mean squared error of between 9% and 13% lower than that of PLS regression on the task of mean fat prediction. At the same time, U-Net generates fine detail chemical maps where 99.91% of the variance is spatially correlated. Conversely, only 2.53% of the variance in the PLS-generated chemical maps is spatially correlated, indicating that each pixel-wise prediction is largely independent of neighboring pixels. Additionally, while the PLS-generated chemical maps contain predictions far beyond the physically possible range of 0-100%, U-Net learns to stay inside this range. Thus, the findings of this study indicate that U-Net is superior to PLS for chemical map generation.","authors":["Ole-Christian Galbo Engstr{\\o}m","Michela Albano-Gaglio","Erik Schou Dreier","Yamine Bouzembrak","Maria Font-i-Furnols","Puneet Mishra","Kim Steenstrup Pedersen"],"url":"https://arxiv.org/abs/2504.14131"}
{"created":"2025-04-28","title":"Pets: General Pattern Assisted Architecture For Time Series Analysis","abstract":"Time series analysis has found widespread applications in areas such as weather forecasting, anomaly detection, and healthcare. However, real-world sequential data often exhibit a superimposed state of various fluctuation patterns, including hourly, daily, and monthly frequencies. Traditional decomposition techniques struggle to effectively disentangle these multiple fluctuation patterns from the seasonal components, making time series analysis challenging. Surpassing the existing multi-period decoupling paradigms, this paper introduces a novel perspective based on energy distribution within the temporal-spectrum space. By adaptively quantifying observed sequences into continuous frequency band intervals, the proposed approach reconstructs fluctuation patterns across diverse periods without relying on domain-specific prior knowledge. Building upon this innovative strategy, we propose Pets, an enhanced architecture that is adaptable to arbitrary model structures. Pets integrates a Fluctuation Pattern Assisted (FPA) module and a Context-Guided Mixture of Predictors (MoP). The FPA module facilitates information fusion among diverse fluctuation patterns by capturing their dependencies and progressively modeling these patterns as latent representations at each layer. Meanwhile, the MoP module leverages these compound pattern representations to guide and regulate the reconstruction of distinct fluctuations hierarchically. Pets achieves state-of-the-art performance across various tasks, including forecasting, imputation, anomaly detection, and classification, while demonstrating strong generalization and robustness.","authors":["Xiangkai Ma","Xiaobin Hong","Wenzhong Li","Sanglu Lu"],"url":"https://arxiv.org/abs/2504.14209"}
{"created":"2025-04-28","title":"Learning and Generating Diverse Residential Load Patterns Using GAN with Weakly-Supervised Training and Weight Selection","abstract":"The scarcity of high-quality residential load data can pose obstacles for decarbonizing the residential sector as well as effective grid planning and operation. The above challenges have motivated research into generating synthetic load data, but existing methods faced limitations in terms of scalability, diversity, and similarity. This paper proposes a Generative Adversarial Network-based Synthetic Residential Load Pattern (RLP-GAN) generation model, a novel weakly-supervised GAN framework, leveraging an over-complete autoencoder to capture dependencies within complex and diverse load patterns and learn household-level data distribution at scale. We incorporate a model weight selection method to address the mode collapse problem and generate load patterns with high diversity. We develop a holistic evaluation method to validate the effectiveness of RLP-GAN using real-world data of 417 households. The results demonstrate that RLP-GAN outperforms state-of-the-art models in capturing temporal dependencies and generating load patterns with higher similarity to real data. Furthermore, we have publicly released the RLP-GAN generated synthetic dataset, which comprises one million synthetic residential load pattern profiles.","authors":["Xinyu Liang","Hao Wang"],"url":"https://arxiv.org/abs/2504.14300"}
{"created":"2025-04-28","title":"Manipulating Multimodal Agents via Cross-Modal Prompt Injection","abstract":"The emergence of multimodal large language models has redefined the agent paradigm by integrating language and vision modalities with external data sources, enabling agents to better interpret human instructions and execute increasingly complex tasks. However, in this work, we identify a critical yet previously overlooked security vulnerability in multimodal agents: cross-modal prompt injection attacks. To exploit this vulnerability, we propose CrossInject, a novel attack framework in which attackers embed adversarial perturbations across multiple modalities to align with target malicious content, allowing external instructions to hijack the agent's decision-making process and execute unauthorized tasks. Our approach consists of two key components. First, we introduce Visual Latent Alignment, where we optimize adversarial features to the malicious instructions in the visual embedding space based on a text-to-image generative model, ensuring that adversarial images subtly encode cues for malicious task execution. Subsequently, we present Textual Guidance Enhancement, where a large language model is leveraged to infer the black-box defensive system prompt through adversarial meta prompting and generate an malicious textual command that steers the agent's output toward better compliance with attackers' requests. Extensive experiments demonstrate that our method outperforms existing injection attacks, achieving at least a +26.4% increase in attack success rates across diverse tasks. Furthermore, we validate our attack's effectiveness in real-world multimodal autonomous agents, highlighting its potential implications for safety-critical applications.","authors":["Le Wang","Zonghao Ying","Tianyuan Zhang","Siyuan Liang","Shengshan Hu","Mingchuan Zhang","Aishan Liu","Xianglong Liu"],"url":"https://arxiv.org/abs/2504.14348"}
{"created":"2025-04-28","title":"DreamID: High-Fidelity and Fast diffusion-based Face Swapping via Triplet ID Group Learning","abstract":"In this paper, we introduce DreamID, a diffusion-based face swapping model that achieves high levels of ID similarity, attribute preservation, image fidelity, and fast inference speed. Unlike the typical face swapping training process, which often relies on implicit supervision and struggles to achieve satisfactory results. DreamID establishes explicit supervision for face swapping by constructing Triplet ID Group data, significantly enhancing identity similarity and attribute preservation. The iterative nature of diffusion models poses challenges for utilizing efficient image-space loss functions, as performing time-consuming multi-step sampling to obtain the generated image during training is impractical. To address this issue, we leverage the accelerated diffusion model SD Turbo, reducing the inference steps to a single iteration, enabling efficient pixel-level end-to-end training with explicit Triplet ID Group supervision. Additionally, we propose an improved diffusion-based model architecture comprising SwapNet, FaceNet, and ID Adapter. This robust architecture fully unlocks the power of the Triplet ID Group explicit supervision. Finally, to further extend our method, we explicitly modify the Triplet ID Group data during training to fine-tune and preserve specific attributes, such as glasses and face shape. Extensive experiments demonstrate that DreamID outperforms state-of-the-art methods in terms of identity similarity, pose and expression preservation, and image fidelity. Overall, DreamID achieves high-quality face swapping results at 512*512 resolution in just 0.6 seconds and performs exceptionally well in challenging scenarios such as complex lighting, large angles, and occlusions.","authors":["Fulong Ye","Miao Hua","Pengze Zhang","Xinghui Li","Qichao Sun","Songtao Zhao","Qian He","Xinglong Wu"],"url":"https://arxiv.org/abs/2504.14509"}
{"created":"2025-04-28","title":"ReasoningV: Efficient Verilog Code Generation with Adaptive Hybrid Reasoning Model","abstract":"Large Language Models (LLMs) have advanced Verilog code generation significantly, yet face challenges in data quality, reasoning capabilities, and computational efficiency. This paper presents ReasoningV, a novel model employing a hybrid reasoning strategy that integrates trained intrinsic capabilities with dynamic inference adaptation for Verilog code generation. Our framework introduces three complementary innovations: (1) ReasoningV-5K, a high-quality dataset of 5,000 functionally verified instances with reasoning paths created through multi-dimensional filtering of PyraNet samples; (2) a two-stage training approach combining parameter-efficient fine-tuning for foundational knowledge with full-parameter optimization for enhanced reasoning; and (3) an adaptive reasoning mechanism that dynamically adjusts reasoning depth based on problem complexity, reducing token consumption by up to 75\\% while preserving performance. Experimental results demonstrate ReasoningV's effectiveness with a pass@1 accuracy of 57.8\\% on VerilogEval-human, achieving performance competitive with leading commercial models like Gemini-2.0-flash (59.5\\%) and exceeding the previous best open-source model by 10.4 percentage points. ReasoningV offers a more reliable and accessible pathway for advancing AI-driven hardware design automation, with our model, data, and code available at https://github.com/BUAA-CLab/ReasoningV.","authors":["Haiyan Qin","Zhiwei Xie","Jingjing Li","Liangchen Li","Xiaotong Feng","Junzhan Liu","Wang Kang"],"url":"https://arxiv.org/abs/2504.14560"}
{"created":"2025-04-28","title":"Generative Auto-Bidding with Value-Guided Explorations","abstract":"Auto-bidding, with its strong capability to optimize bidding decisions within dynamic and competitive online environments, has become a pivotal strategy for advertising platforms. Existing approaches typically employ rule-based strategies or Reinforcement Learning (RL) techniques. However, rule-based strategies lack the flexibility to adapt to time-varying market conditions, and RL-based methods struggle to capture essential historical dependencies and observations within Markov Decision Process (MDP) frameworks. Furthermore, these approaches often face challenges in ensuring strategy adaptability across diverse advertising objectives. Additionally, as offline training methods are increasingly adopted to facilitate the deployment and maintenance of stable online strategies, the issues of documented behavioral patterns and behavioral collapse resulting from training on fixed offline datasets become increasingly significant. To address these limitations, this paper introduces a novel offline Generative Auto-bidding framework with Value-Guided Explorations (GAVE). GAVE accommodates various advertising objectives through a score-based Return-To-Go (RTG) module. Moreover, GAVE integrates an action exploration mechanism with an RTG-based evaluation method to explore novel actions while ensuring stability-preserving updates. A learnable value function is also designed to guide the direction of action exploration and mitigate Out-of-Distribution (OOD) problems. Experimental results on two offline datasets and real-world deployments demonstrate that GAVE outperforms state-of-the-art baselines in both offline evaluations and online A/B tests. By applying the core methods of this framework, we proudly secured first place in the NeurIPS 2024 competition, 'AIGB Track: Learning Auto-Bidding Agents with Generative Models'.","authors":["Jingtong Gao","Yewen Li","Shuai Mao","Peng Jiang","Nan Jiang","Yejing Wang","Qingpeng Cai","Fei Pan","Peng Jiang","Kun Gai","Bo An","Xiangyu Zhao"],"url":"https://arxiv.org/abs/2504.14587"}
{"created":"2025-04-28","title":"UFO2: The Desktop AgentOS","abstract":"Recent Computer-Using Agents (CUAs), powered by multimodal large language models (LLMs), offer a promising direction for automating complex desktop workflows through natural language. However, most existing CUAs remain conceptual prototypes, hindered by shallow OS integration, fragile screenshot-based interaction, and disruptive execution.","authors":["Chaoyun Zhang","He Huang","Chiming Ni","Jian Mu","Si Qin","Shilin He","Lu Wang","Fangkai Yang","Pu Zhao","Chao Du","Liqun Li","Yu Kang","Zhao Jiang","Suzhen Zheng","Rujia Wang","Jiaxu Qian","Minghua Ma","Jian-Guang Lou","Qingwei Lin","Saravan Rajmohan","Dongmei Zhang"],"url":"https://arxiv.org/abs/2504.14603"}
{"created":"2025-04-28","title":"Towards Optimal Circuit Generation: Multi-Agent Collaboration Meets Collective Intelligence","abstract":"Large language models (LLMs) have transformed code generation, yet their application in hardware design produces gate counts 38\\%--1075\\% higher than human designs. We present CircuitMind, a multi-agent framework that achieves human-competitive efficiency through three key innovations: syntax locking (constraining generation to basic logic gates), retrieval-augmented generation (enabling knowledge-driven design), and dual-reward optimization (balancing correctness with efficiency). To evaluate our approach, we introduce TC-Bench, the first gate-level benchmark harnessing collective intelligence from the TuringComplete ecosystem -- a competitive circuit design platform with hundreds of thousands of players. Experiments show CircuitMind enables 55.6\\% of model implementations to match or exceed top-tier human experts in composite efficiency metrics. Most remarkably, our framework elevates the 14B Phi-4 model to outperform both GPT-4o mini and Gemini 2.0 Flash, achieving efficiency comparable to the top 25\\% of human experts without requiring specialized training. These innovations establish a new paradigm for hardware optimization where collaborative AI systems leverage collective human expertise to achieve optimal circuit designs. Our model, data, and code are open-source at https://github.com/BUAA-CLab/CircuitMind.","authors":["Haiyan Qin","Jiahao Feng","Xiaotong Feng","Wei W. Xing","Wang Kang"],"url":"https://arxiv.org/abs/2504.14625"}
{"created":"2025-04-28","title":"A Case Study Exploring the Current Landscape of Synthetic Medical Record Generation with Commercial LLMs","abstract":"Synthetic Electronic Health Records (EHRs) offer a valuable opportunity to create privacy preserving and harmonized structured data, supporting numerous applications in healthcare. Key benefits of synthetic data include precise control over the data schema, improved fairness and representation of patient populations, and the ability to share datasets without concerns about compromising real individuals privacy. Consequently, the AI community has increasingly turned to Large Language Models (LLMs) to generate synthetic data across various domains. However, a significant challenge in healthcare is ensuring that synthetic health records reliably generalize across different hospitals, a long standing issue in the field. In this work, we evaluate the current state of commercial LLMs for generating synthetic data and investigate multiple aspects of the generation process to identify areas where these models excel and where they fall short. Our main finding from this work is that while LLMs can reliably generate synthetic health records for smaller subsets of features, they struggle to preserve realistic distributions and correlations as the dimensionality of the data increases, ultimately limiting their ability to generalize across diverse hospital settings.","authors":["Yihan Lin","Zhirong Bella Yu","Simon Lee"],"url":"https://arxiv.org/abs/2504.14657"}
{"created":"2025-04-28","title":"Rank Bounds and PIT for $\\Sigma^3 \\Pi \\Sigma \\Pi^d$ circuits via a non-linear Edelstein-Kelly theorem","abstract":"We prove a non-linear Edelstein-Kelly theorem for polynomials of constant degree, fully settling a stronger form of Conjecture 30 in Gupta (2014), and generalizing the main result of Peleg and Shpilka (STOC 2021) from quadratic polynomials to polynomials of any constant degree.","authors":["Abhibhav Garg","Rafael Oliveira","Akash Kumar Sengupta"],"url":"https://arxiv.org/abs/2504.14729"}
{"created":"2025-04-28","title":"NetCloak: Dynamic Topology Expansion for Secure and Scalable Configuration Sharing","abstract":"As modern networks continue to grow in both scale and complexity, sharing real-world device configurations poses significant privacy risks, especially when adversaries can infer organizational size or resource distribution from topology data. We present NetCloak, a configuration anonymization framework that adaptively injects synthetic routers and hosts into the network graph to obfuscate true scale, while preserving end-to-end forwarding behavior. NetCloak core techniques include: (1) a graph-embedding expansion algorithm that integrates the original topology into a larger reference graph, ensuring added nodes blend seamlessly with real ones; (2) a k-degree mapping anonymity scheme that selectively adds minimal links to guarantee each original node degree is indistinguishable among at least k peers; (3) a mimicry-driven configuration generator that derives command templates from existing devices, preserving command ordering, naming conventions, and routing policies; and (4) a layered repair process combining SMT-based intra-AS route synthesis with iterative inter-AS filter insertion to restore protocol-correct routing under OSPF and BGP. Extensive experiments on real and emulated campus and data-center topologies demonstrate that NetCloak effectively conceals network size, improving topological rationality by over 70% and configuration fidelity by nearly 30% compared to baseline methods, while reducing route-repair overhead by more than 50% under randomized link costs. NetCloak thus enables safe, privacy-preserving configuration sharing at scale.","authors":["Qianye Wang","Yuejie Wang","Yongting Chen","Guyue Liu"],"url":"https://arxiv.org/abs/2504.14959"}
{"created":"2025-04-28","title":"Deterministic Depth-4 PIT and Normalization","abstract":"In this paper, we initiate the study of deterministic PIT for $\\Sigma^{[k]}\\Pi\\Sigma\\Pi^{[\\delta]}$ circuits over fields of any characteristic, where $k$ and $\\delta$ are bounded. Our main result is a deterministic polynomial-time black-box PIT algorithm for $\\Sigma^{[3]}\\Pi\\Sigma\\Pi^{[\\delta]}$ circuits, under the additional condition that one of the summands at the top $\\Sigma$ gate is squarefree.","authors":["Zeyu Guo","Siki Wang"],"url":"https://arxiv.org/abs/2504.15143"}
{"created":"2025-04-28","title":"Soft-Output from Covered Space Decoding of Product Codes","abstract":"In this work, we propose a new soft-in soft-out decoder called soft-output from covered space (SOCS) decoder. It estimates the a posteriori reliability based on the space explored by a list decoder, i.e., the set of vectors for which the list decoder knows whether they are codewords. This approach enables a more accurate calculation of the a posteriori reliability and results in gains of up to 0.25$\\,$dB for turbo product decoding with SOCS compared to Chase-Pyndiah decoding.","authors":["Tim Janz","Simon Oberm\\\"uller","Andreas Zunker","Stephan ten Brink"],"url":"https://arxiv.org/abs/2504.15204"}
{"created":"2025-04-28","title":"Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model","abstract":"Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data.","authors":["Junshu Pan","Wei Shen","Shulin Huang","Qiji Zhou","Yue Zhang"],"url":"https://arxiv.org/abs/2504.15843"}
{"created":"2025-04-28","title":"Contrasting Deadlock-Free Session Processes (Extended Version)","abstract":"Deadlock freedom is a crucial property for message-passing programs. Over the years, several different type systems for concurrent processes that ensure deadlock freedom have been proposed; this diversity raises the question of how they compare. We address this question, considering two type systems not covered in prior work: Kokke etal's HCP, a type system based on a linear logic with hypersequents, and Padovani's priority-based type system for asynchronous processes, dubbed P. Their distinctive features make formal comparisons relevant and challenging. Our findings are two-fold: (1) the hypersequent setting does not drastically change the class of deadlock-free processes induced by linear logic, and (2) we relate the classes of deadlock-free processes induced by HCP and P. We prove that our results hold under both synchronous and asynchronous communication. Our results provide new insights into the essential mechanisms involved in statically avoiding deadlocks in concurrency.","authors":["Juan C. Jaramillo","Jorge A. P\\'erez"],"url":"https://arxiv.org/abs/2504.15845"}
{"created":"2025-04-28","title":"ScaleGNN: Towards Scalable Graph Neural Networks via Adaptive High-order Neighboring Feature Fusion","abstract":"Graph Neural Networks (GNNs) have demonstrated strong performance across various graph-based tasks by effectively capturing relational information between nodes. These models rely on iterative message passing to propagate node features, enabling nodes to aggregate information from their neighbors. Recent research has significantly improved the message-passing mechanism, enhancing GNN scalability on large-scale graphs. However, GNNs still face two main challenges: over-smoothing, where excessive message passing results in indistinguishable node representations, especially in deep networks incorporating high-order neighbors; and scalability issues, as traditional architectures suffer from high model complexity and increased inference time due to redundant information aggregation. This paper proposes a novel framework for large-scale graphs named ScaleGNN that simultaneously addresses both challenges by adaptively fusing multi-level graph features. We first construct neighbor matrices for each order, learning their relative information through trainable weights through an adaptive high-order feature fusion module. This allows the model to selectively emphasize informative high-order neighbors while reducing unnecessary computational costs. Additionally, we introduce a High-order redundant feature masking mechanism based on a Local Contribution Score (LCS), which enables the model to retain only the most relevant neighbors at each order, preventing redundant information propagation. Furthermore, low-order enhanced feature aggregation adaptively integrates low-order and high-order features based on task relevance, ensuring effective capture of both local and global structural information without excessive complexity. Extensive experiments on real-world datasets demonstrate that our approach consistently outperforms state-of-the-art GNN models in both accuracy and computational efficiency.","authors":["Xiang Li","Haobing Liu","Jianpeng Qi","Yuan Cao","Guoqing Chao","Yanwei Yu"],"url":"https://arxiv.org/abs/2504.15920"}
{"created":"2025-04-28","title":"CAPO: Cost-Aware Prompt Optimization","abstract":"Large language models (LLMs) have revolutionized natural language processing by solving a wide range of tasks simply guided by a prompt. Yet their performance is highly sensitive to prompt formulation. While automated prompt optimization addresses this challenge by finding optimal prompts, current methods require a substantial number of LLM calls and input tokens, making prompt optimization expensive. We introduce CAPO (Cost-Aware Prompt Optimization), an algorithm that enhances prompt optimization efficiency by integrating AutoML techniques. CAPO is an evolutionary approach with LLMs as operators, incorporating racing to save evaluations and multi-objective optimization to balance performance with prompt length. It jointly optimizes instructions and few-shot examples while leveraging task descriptions for improved robustness. Our extensive experiments across diverse datasets and LLMs demonstrate that CAPO outperforms state-of-the-art discrete prompt optimization methods in 11/15 cases with improvements up to 21%p. Our algorithm achieves better performances already with smaller budgets, saves evaluations through racing, and decreases average prompt length via a length penalty, making it both cost-efficient and cost-aware. Even without few-shot examples, CAPO outperforms its competitors and generally remains robust to initial prompts. CAPO represents an important step toward making prompt optimization more powerful and accessible by improving cost-efficiency.","authors":["Tom Zehle","Moritz Schlager","Timo Hei{\\ss}","Matthias Feurer"],"url":"https://arxiv.org/abs/2504.16005"}
{"created":"2025-04-28","title":"Physics-Informed Inference Time Scaling via Simulation-Calibrated Scientific Machine Learning","abstract":"High-dimensional partial differential equations (PDEs) pose significant computational challenges across fields ranging from quantum chemistry to economics and finance. Although scientific machine learning (SciML) techniques offer approximate solutions, they often suffer from bias and neglect crucial physical insights. Inspired by inference-time scaling strategies in language models, we propose Simulation-Calibrated Scientific Machine Learning (SCaSML), a physics-informed framework that dynamically refines and debiases the SCiML predictions during inference by enforcing the physical laws. SCaSML leverages derived new physical laws that quantifies systematic errors and employs Monte Carlo solvers based on the Feynman-Kac and Elworthy-Bismut-Li formulas to dynamically correct the prediction. Both numerical and theoretical analysis confirms enhanced convergence rates via compute-optimal inference methods. Our numerical experiments demonstrate that SCaSML reduces errors by 20-50% compared to the base surrogate model, establishing it as the first algorithm to refine approximated solutions to high-dimensional PDE during inference. Code of SCaSML is available at https://github.com/Francis-Fan-create/SCaSML.","authors":["Zexi Fan","Yan Sun","Shihao Yang","Yiping Lu"],"url":"https://arxiv.org/abs/2504.16172"}
{"created":"2025-04-28","title":"TeLLMe: An Energy-Efficient Ternary LLM Accelerator for Prefilling and Decoding on Edge FPGAs","abstract":"Deploying large language models (LLMs) on edge platforms is challenged by their high computational and memory demands. Although recent low-bit quantization methods (e.g., BitNet, DeepSeek) compress weights to as little as 1.58 bits with minimal accuracy loss, edge deployment is still constrained by limited on-chip resources, power budgets, and the often-neglected latency of the prefill phase. We present TeLLMe, the first ternary LLM accelerator for low-power FPGAs (e.g., AMD KV260) that fully supports both prefill and autoregressive decoding using 1.58-bit weights and 8-bit activations. Our contributions include: (1) a table-lookup matrix engine for ternary matmul that merges grouped activations with online precomputation to minimize resource use; (2) a fused, bandwidth-efficient attention module featuring a reversed reordering scheme to accelerate prefill; and (3) a tightly integrated normalization and quantization--dequantization unit optimized for ultra-low-bit inference. Under a 7W power budget, TeLLMe delivers up to 9 tokens/s throughput over 1,024-token contexts and prefill latencies of 0.55--1.15 s for 64--128 token prompts, marking a significant energy-efficiency advance and establishing a new edge FPGA benchmark for generative AI.","authors":["Ye Qiao","Zhiheng Chen","Yifan Zhang","Yian Wang","Sitao Huang"],"url":"https://arxiv.org/abs/2504.16266"}
{"created":"2025-04-28","title":"Boosting KNNClassifier Performance with Opposition-Based Data Transformation","abstract":"In this paper, we introduce a novel data transformation framework based on Opposition-Based Learning (OBL) to boost the performance of traditional classification algorithms. Originally developed to accelerate convergence in optimization tasks, OBL is leveraged here to generate synthetic opposite samples that enrich the training data and improve decision boundary formation. We explore three OBL variants Global OBL, Class-Wise OBL, and Localized Class-Wise OBL and integrate them with K-Nearest Neighbors (KNN). Extensive experiments conducted on 26 heterogeneous and high-dimensional datasets demonstrate that OBL-enhanced classifiers consistently outperform the basic KNN. These findings underscore the potential of OBL as a lightweight yet powerful data transformation strategy for enhancing classification performance, especially in complex or sparse learning environments.","authors":["Abdesslem Layeb"],"url":"https://arxiv.org/abs/2504.16268"}
{"created":"2025-04-28","title":"COBRA: Algorithm-Architecture Co-optimized Binary Transformer Accelerator for Edge Inference","abstract":"Transformer-based models have demonstrated superior performance in various fields, including natural language processing and computer vision. However, their enormous model size and high demands in computation, memory, and communication limit their deployment to edge platforms for local, secure inference. Binary transformers offer a compact, low-complexity solution for edge deployment with reduced bandwidth needs and acceptable accuracy. However, existing binary transformers perform inefficiently on current hardware due to the lack of binary specific optimizations. To address this, we introduce COBRA, an algorithm-architecture co-optimized binary Transformer accelerator for edge computing. COBRA features a real 1-bit binary multiplication unit, enabling matrix operations with -1, 0, and +1 values, surpassing ternary methods. With further hardware-friendly optimizations in the attention block, COBRA achieves up to 3,894.7 GOPS throughput and 448.7 GOPS/Watt energy efficiency on edge FPGAs, delivering a 311x energy efficiency improvement over GPUs and a 3.5x throughput improvement over the state-of-the-art binary accelerator, with only negligible inference accuracy degradation.","authors":["Ye Qiao","Zhiheng Chen","Yian Wang","Yifan Zhang","Yunzhe Deng","Sitao Huang"],"url":"https://arxiv.org/abs/2504.16269"}
{"created":"2025-04-28","title":"Fully Scalable MPC Algorithms for Euclidean k-Center","abstract":"The $k$-center problem is a fundamental optimization problem with numerous applications in machine learning, data analysis, data mining, and communication networks. The $k$-center problem has been extensively studied in the classical sequential setting for several decades, and more recently there have been some efforts in understanding the problem in parallel computing, on the Massively Parallel Computation (MPC) model. For now, we have a good understanding of $k$-center in the case where each local MPC machine has sufficient local memory to store some representatives from each cluster, that is, when one has $\\Omega(k)$ local memory per machine. While this setting covers the case of small values of $k$, for a large number of clusters these algorithms require undesirably large local memory, making them poorly scalable. The case of large $k$ has been considered only recently for the fully scalable low-local-memory MPC model for the Euclidean instances of the $k$-center problem. However, the earlier works have been considering only the constant dimensional Euclidean space, required a super-constant number of rounds, and produced only $k(1+o(1))$ centers whose cost is a super-constant approximation of $k$-center.","authors":["Artur Czumaj","Guichen Gao","Mohsen Ghaffari","Shaofeng H. -C. Jiang"],"url":"https://arxiv.org/abs/2504.16382"}
{"created":"2025-04-28","title":"Think Hierarchically, Act Dynamically: Hierarchical Multi-modal Fusion and Reasoning for Vision-and-Language Navigation","abstract":"Vision-and-Language Navigation (VLN) aims to enable embodied agents to follow natural language instructions and reach target locations in real-world environments. While prior methods often rely on either global scene representations or object-level features, these approaches are insufficient for capturing the complex interactions across modalities required for accurate navigation. In this paper, we propose a Multi-level Fusion and Reasoning Architecture (MFRA) to enhance the agent's ability to reason over visual observations, language instructions and navigation history. Specifically, MFRA introduces a hierarchical fusion mechanism that aggregates multi-level features-ranging from low-level visual cues to high-level semantic concepts-across multiple modalities. We further design a reasoning module that leverages fused representations to infer navigation actions through instruction-guided attention and dynamic context integration. By selectively capturing and combining relevant visual, linguistic, and temporal signals, MFRA improves decision-making accuracy in complex navigation scenarios. Extensive experiments on benchmark VLN datasets including REVERIE, R2R, and SOON demonstrate that MFRA achieves superior performance compared to state-of-the-art methods, validating the effectiveness of multi-level modal fusion for embodied navigation.","authors":["Junrong Yue","Yifan Zhang","Chuan Qin","Bo Li","Xiaomin Lie","Xinlei Yu","Wenxin Zhang","Zhendong Zhao"],"url":"https://arxiv.org/abs/2504.16516"}
{"created":"2025-04-28","title":"Using Causal Inference to Test Systems with Hidden and Interacting Variables: An Evaluative Case Study","abstract":"Software systems with large parameter spaces, nondeterminism and high computational cost are challenging to test. Recently, software testing techniques based on causal inference have been successfully applied to systems that exhibit such characteristics, including scientific models and autonomous driving systems. One significant limitation is that these are restricted to test properties where all of the variables involved can be observed and where there are no interactions between variables. In practice, this is rarely guaranteed; the logging infrastructure may not be available to record all of the necessary runtime variable values, and it can often be the case that an output of the system can be affected by complex interactions between variables. To address this, we leverage two additional concepts from causal inference, namely effect modification and instrumental variable methods. We build these concepts into an existing causal testing tool and conduct an evaluative case study which uses the concepts to test three system-level requirements of CARLA, a high-fidelity driving simulator widely used in autonomous vehicle development and testing. The results show that we can obtain reliable test outcomes without requiring large amounts of highly controlled test data or instrumentation of the code, even when variables interact with each other and are not recorded in the test data.","authors":["Michael Foster","Robert M. Hierons","Donghwan Shin","Neil Walkinshaw","Christopher Wild"],"url":"https://arxiv.org/abs/2504.16526"}
{"created":"2025-04-28","title":"Skywork R1V2: Multimodal Hybrid Reinforcement Learning for Reasoning","abstract":"We present Skywork R1V2, a next-generation multimodal reasoning model and a major leap forward from its predecessor, Skywork R1V. At its core, R1V2 introduces a hybrid reinforcement learning paradigm that jointly leverages the Mixed Preference Optimization (MPO) and the Group Relative Policy Optimization (GRPO), which harmonizes reward-model guidance with rule-based strategies, thereby addressing the long-standing challenge of balancing sophisticated reasoning capabilities with broad generalization. To further enhance training efficiency, we introduce the Selective Sample Buffer (SSB) mechanism, which effectively counters the ``Vanishing Advantages'' dilemma inherent in GRPO by prioritizing high-value samples throughout the optimization process. Notably, we observe that excessive reinforcement signals can induce visual hallucinations--a phenomenon we systematically monitor and mitigate through calibrated reward thresholds throughout the training process. Empirical results affirm the exceptional capability of R1V2, with benchmark-leading performances such as 62.6 on OlympiadBench, 78.9 on AIME2024, 63.6 on LiveCodeBench, and 73.6 on MMMU. These results underscore R1V2's superiority over existing open-source models and demonstrate significant progress in closing the performance gap with premier proprietary systems, including Gemini 2.5 and OpenAI-o4-mini. The Skywork R1V2 model weights have been publicly released to promote openness and reproducibility https://huggingface.co/Skywork/Skywork-R1V2-38B.","authors":["Chris","Yichen Wei","Yi Peng","Xiaokun Wang","Weijie Qiu","Wei Shen","Tianyidan Xie","Jiangbo Pei","Jianhao Zhang","Yunzhuo Hao","Xuchen Song","Yang Liu","Yahui Zhou"],"url":"https://arxiv.org/abs/2504.16656"}
{"created":"2025-04-28","title":"Improving Significant Wave Height Prediction Using Chronos Models","abstract":"Accurate wave height prediction is critical for maritime safety and coastal resilience, yet conventional physics-based models and traditional machine learning methods face challenges in computational efficiency and nonlinear dynamics modeling. This study introduces Chronos, the first implementation of a large language model (LLM)-powered temporal architecture (Chronos) optimized for wave forecasting. Through advanced temporal pattern recognition applied to historical wave data from three strategically chosen marine zones in the Northwest Pacific basin, our framework achieves multimodal improvements: (1) 14.3% reduction in training time with 2.5x faster inference speed compared to PatchTST baselines, achieving 0.575 mean absolute scaled error (MASE) units; (2) superior short-term forecasting (1-24h) across comprehensive metrics; (3) sustained predictive leadership in extended-range forecasts (1-120h); and (4) demonstrated zero-shot capability maintaining median performance (rank 4/12) against specialized operational models. This LLM-enhanced temporal modeling paradigm establishes a new standard in wave prediction, offering both computationally efficient solutions and a transferable framework for complex geophysical systems modeling.","authors":["Yilin Zhai","Hongyuan Shi","Chao Zhan","Qing Wang","Zaijin You","Nan Wang"],"url":"https://arxiv.org/abs/2504.16834"}
{"created":"2025-04-28","title":"Memory-efficient Sketch Acceleration for Handling Large Network Flows on FPGAs","abstract":"Sketch-based algorithms for network traffic monitoring have drawn increasing interest in recent years due to their sub-linear memory efficiency and high accuracy. As the volume of network traffic grows, software-based sketch implementations cannot match the throughput of the incoming network flows. FPGA-based hardware sketch has shown better performance compared to software running on a CPU when handling these packets. Among the various sketch algorithms, Count-min sketch is one of the most popular and efficient. However, due to the limited amount of on-chip memory, the FPGA-based count-Min sketch accelerator suffers from performance drops as network traffic grows. In this work, we propose a hardware-friendly architecture with a variable width memory counter for count-min sketch. Our architecture provides a more compact design to store the sketch data structure effectively, allowing us to support larger hash tables and reduce overestimation errors. The design makes use of a P4-based programmable data plane and the AMD OpenNIC shell. The design is implemented and verified on the Open Cloud Testbed running on AMD Alveo U280s and can keep up with the 100 Gbit link speed.","authors":["Zhaoyang Han","Yicheng Qian","Michael Zink","Miriam Leeser"],"url":"https://arxiv.org/abs/2504.16896"}
{"created":"2025-04-28","title":"Assessing SSL/TLS Certificate Centralization: Implications for Digital Sovereignty","abstract":"SSL/TLS is a fundamental technology in the network protocol stack that enables encrypted data transmission and authentication of web domains. However, the current model relies on a small number of Certificate Authorities (CAs) to provide and validate certificates, thus creating a highly centralized ecosystem. In this paper, we analyze the degree of centralization of certificate provisioning from CAs in two major political groups: Brazil, Russia, India, China, and South Africa (BRICS) and the European Union (EU). We have found that over 75% of certificates for both BRICS and EU domains originate from CAs based in the United States, indicating possible risks to their digital sovereignty due to the high level of external dependency. This indicates the need for nations within those groups to research alternatives to reduce the high level of dependency on foreign CAs and increase their digital autonomy.","authors":["Andrei Cordova Azevedo","Eder John Scheid","Muriel Figueredo Franco","Lisandro Zambenedetti Granville"],"url":"https://arxiv.org/abs/2504.16897"}
{"created":"2025-04-28","title":"BackSlash: Rate Constrained Optimized Training of Large Language Models","abstract":"The rapid advancement of large-language models (LLMs) has driven extensive research into parameter compression after training has been completed, yet compression during the training phase remains largely unexplored. In this work, we introduce Rate-Constrained Training (BackSlash), a novel training-time compression approach based on rate-distortion optimization (RDO). BackSlash enables a flexible trade-off between model accuracy and complexity, significantly reducing parameter redundancy while preserving performance. Experiments in various architectures and tasks demonstrate that BackSlash can reduce memory usage by 60% - 90% without accuracy loss and provides significant compression gain compared to compression after training. Moreover, BackSlash proves to be highly versatile: it enhances generalization with small Lagrange multipliers, improves model robustness to pruning (maintaining accuracy even at 80% pruning rates), and enables network simplification for accelerated inference on edge devices.","authors":["Jun Wu","Jiangtao Wen","Yuxing Han"],"url":"https://arxiv.org/abs/2504.16968"}
{"created":"2025-04-28","title":"Deep Learning for Individual Heterogeneity","abstract":"This paper integrates deep neural networks (DNNs) into structural economic models to increase flexibility and capture rich heterogeneity while preserving interpretability. Economic structure and machine learning are complements in empirical modeling, not substitutes: DNNs provide the capacity to learn complex, non-linear heterogeneity patterns, while the structural model ensures the estimates remain interpretable and suitable for decision making and policy analysis. We start with a standard parametric structural model and then enrich its parameters into fully flexible functions of observables, which are estimated using a particular DNN architecture whose structure reflects the economic model. We illustrate our framework by studying demand estimation in consumer choice. We show that by enriching a standard demand model we can capture rich heterogeneity, and further, exploit this heterogeneity to create a personalized pricing strategy. This type of optimization is not possible without economic structure, but cannot be heterogeneous without machine learning. Finally, we provide theoretical justification of each step in our proposed methodology. We first establish non-asymptotic bounds and convergence rates of our structural deep learning approach. Next, a novel and quite general influence function calculation allows for feasible inference via double machine learning in a wide variety of contexts. These results may be of interest in many other contexts, as they generalize prior work.","authors":["Max H. Farrell","Tengyuan Liang","Sanjog Misra"],"url":"https://arxiv.org/abs/2010.14694"}
{"created":"2025-04-28","title":"Deniable Encryption in a Quantum World","abstract":"(Sender-)Deniable encryption provides a very strong privacy guarantee: a sender who is coerced by an attacker into \"opening\" their ciphertext after-the-fact is able to generate \"fake\" local random choices that are consistent with any plaintext of their choice. In this work, we study (sender-)deniable encryption in a setting where the encryption procedure is a quantum algorithm, but the ciphertext is classical. We show that quantum computation unlocks a fundamentally stronger form of deniable encryption, which we call perfect unexplainability. The primitive at the heart of unexplainability is a quantum computation for which there is provably no efficient way, such as exhibiting the \"history of the computation\", to establish that the output was indeed the result of the computation. We give a construction that is secure in the random oracle model, assuming the quantum hardness of LWE. Crucially, this notion implies a form of protection against coercion \"before-the-fact\", a property that is impossible to achieve classically.","authors":["Andrea Coladangelo","Shafi Goldwasser","Umesh Vazirani"],"url":"https://arxiv.org/abs/2112.14988"}
{"created":"2025-04-28","title":"Identifying Chemicals Through Dimensionality Reduction","abstract":"Civilizations have tried to make drinking water safe to consume for thousands of years. The process of determining water contaminants has evolved with the complexity of the contaminants due to pesticides and heavy metals. The routine procedure to determine water safety is to use targeted analysis which searches for specific substances from some known list; however, we do not explicitly know which substances should be on this list. Before experimentally determining which substances are contaminants, how do we answer the sampling problem of identifying all the substances in the water? Here, we present an approach that builds on the work of Jaanus Liigand et al., which used non-targeted analysis that conducts a broader search on the sample to develop a random-forest regression model, to predict the names of all the substances in a sample, as well as their respective concentrations[1]. This work utilizes techniques from dimensionality reduction and linear decompositions to present a more accurate model using data from the European Massbank Metabolome Library to produce a global list of chemicals that researchers can then identify and test for when purifying water.","authors":["Emile Anand","Charles Steinhardt","Martin Hansen"],"url":"https://arxiv.org/abs/2211.14708"}
{"created":"2025-04-28","title":"Reputation-Driven Adoption and Avoidance of Algorithmic Decision Aids in Credence Goods Markets","abstract":"In credence goods markets such as health care or repair services, consumers rely on experts with superior information to adequately diagnose and treat them. Experts, however, are constrained in their diagnostic abilities, which hurts market efficiency and consumer welfare. Technological breakthroughs that substitute or complement expert judgments have the potential to alleviate consumer mistreatment. This article studies how competitive experts adopt novel diagnostic technologies when skills are heterogeneously distributed and obfuscated to consumers. We differentiate between novel technologies that increase expert abilities, and algorithmic decision aids that complement expert judgments, but do not affect an expert's personal diagnostic precision. When consumers build up beliefs about an expert's type through repeated interactions, we show that high-ability experts may strategically forego the decision aid in order to escape a pooling equilibrium by differentiating themselves from low-ability experts. Without future visits, signaling concerns cause all experts to randomize their investment choice, leading to under-utilization from low-ability experts and over-utilization from high-ability experts. Results from two online experiments support our hypotheses. High-ability experts are significantly less likely than low-ability experts to invests into an algorithmic decision aid if reputation building is possible. Otherwise, there is no difference, and experts who believe that consumers play a signaling game randomize their investment choice.","authors":["Alexander Erlei","Lukas Meub"],"url":"https://arxiv.org/abs/2401.17929"}
{"created":"2025-04-28","title":"Prior-Dependent Allocations for Bayesian Fixed-Budget Best-Arm Identification in Structured Bandits","abstract":"We study the problem of Bayesian fixed-budget best-arm identification (BAI) in structured bandits. We propose an algorithm that uses fixed allocations based on the prior information and the structure of the environment. We provide theoretical bounds on its performance across diverse models, including the first prior-dependent upper bounds for linear and hierarchical BAI. Our key contribution is introducing new proof methods that result in tighter bounds for multi-armed BAI compared to existing methods. We extensively compare our approach to other fixed-budget BAI methods, demonstrating its consistent and robust performance in various settings. Our work improves our understanding of Bayesian fixed-budget BAI in structured bandits and highlights the effectiveness of our approach in practical scenarios.","authors":["Nicolas Nguyen","Imad Aouali","Andr\\'as Gy\\\"orgy","Claire Vernade"],"url":"https://arxiv.org/abs/2402.05878"}
{"created":"2025-04-28","title":"Continuum limit of $p$-biharmonic equations on graphs","abstract":"This paper studies the $p$-biharmonic equation on graphs, which arises in point cloud processing and can be interpreted as a natural extension of the graph $p$-Laplacian from the perspective of hypergraph. The asymptotic behavior of the solution is investigated when the random geometric graph is considered and the number of data points goes to infinity. We show that the continuum limit is an appropriately weighted $p$-biharmonic equation with homogeneous Neumann boundary conditions. The result relies on the uniform $L^p$ estimates for solutions and gradients of nonlocal and graph Poisson equations. The $L^\\infty$ estimates of solutions are also obtained as a byproduct.","authors":["Kehan Shi","Martin Burger"],"url":"https://arxiv.org/abs/2404.19689"}
{"created":"2025-04-28","title":"Certifying solutions of degenerate semidefinite programs","abstract":"This paper deals with the algorithmic aspects of solving feasibility problems of semidefinite programming (SDP), aka linear matrix inequalities (LMI). Since in some SDP instances all feasible solutions have irrational entries, numerical solvers that work with rational numbers can only find an approximate solution. We study the following question: is it possible to certify feasibility of a given SDP using an approximate solution that is sufficiently close to some exact solution? Existing approaches make the assumption that there exist rational feasible solutions (and use techniques such as rounding and lattice reduction algorithms). We propose an alternative approach that does not need this assumption. More specifically, we show how to construct a system of polynomial equations whose set of real solutions is guaranteed to have an isolated correct solution (assuming that the target exact solution is maximum-rank). This allows, in particular, to use algorithms from real algebraic geometry for solving systems of polynomial equations, yielding a hybrid (or symbolic-numerical) method for SDPs. We experimentally compare it with a pure symbolic method; the hybrid method was able to certify feasibility of many SDP instances on which the exact method failed. Our approach may have further applications, such as refining an approximate solution using methods of numerical algebraic geometry for systems of polynomial equations.","authors":["Vladimir Kolmogorov","Simone Naldi","Jeferson Zapata"],"url":"https://arxiv.org/abs/2405.13625"}
{"created":"2025-04-28","title":"Robust Kernel Hypothesis Testing under Data Corruption","abstract":"We propose a general method for constructing robust permutation tests under data corruption. The proposed tests effectively control the non-asymptotic type I error under data corruption, and we prove their consistency in power under minimal conditions. This contributes to the practical deployment of hypothesis tests for real-world applications with potential adversarial attacks. For the two-sample and independence settings, we show that our kernel robust tests are minimax optimal, in the sense that they are guaranteed to be non-asymptotically powerful against alternatives uniformly separated from the null in the kernel MMD and HSIC metrics at some optimal rate (tight with matching lower bound). We point out that existing differentially private tests can be adapted to be robust to data corruption, and we demonstrate in experiments that our proposed tests achieve much higher power than these private tests. Finally, we provide publicly available implementations and empirically illustrate the practicality of our robust tests.","authors":["Antonin Schrab","Ilmun Kim"],"url":"https://arxiv.org/abs/2405.19912"}
{"created":"2025-04-28","title":"Combining X-Vectors and Bayesian Batch Active Learning: Two-Stage Active Learning Pipeline for Speech Recognition","abstract":"This paper introduces a novel two-stage active learning (AL) pipeline for automatic speech recognition (ASR), combining unsupervised and supervised AL methods. The first stage utilizes unsupervised AL by using x-vectors clustering for diverse sample selection from unlabeled speech data, thus establishing a robust initial dataset for the subsequent supervised AL. The second stage incorporates a supervised AL strategy, with a batch AL method specifically developed for ASR, aimed at selecting diverse and informative batches of samples. Here, sample diversity is also achieved using x-vectors clustering, while the most informative samples are identified using a Bayesian AL method tailored for ASR with an adaptation of Monte Carlo dropout to approximate Bayesian inference. This approach enables precise uncertainty estimation, thereby enhancing ASR model training with significantly reduced data requirements. Our method has shown superior performance compared to competing methods on homogeneous, heterogeneous, and OOD test sets, demonstrating that strategic sample selection and innovative Bayesian modeling can substantially optimize both labeling effort and data utilization in deep learning-based ASR applications.","authors":["Ognjen Kundacina","Vladimir Vincan","Dragisa Miskovic"],"url":"https://arxiv.org/abs/2406.02566"}
{"created":"2025-04-28","title":"Symmetry-driven embedding of networks in hyperbolic space","abstract":"Hyperbolic models are known to produce networks with properties observed empirically in most network datasets, including heavy-tailed degree distribution, high clustering, and hierarchical structures. As a result, several embeddings algorithms have been proposed to invert these models and assign hyperbolic coordinates to network data. Current algorithms for finding these coordinates, however, do not quantify uncertainty in the inferred coordinates. We present BIGUE, a Markov chain Monte Carlo (MCMC) algorithm that samples the posterior distribution of a Bayesian hyperbolic random graph model. We show that the samples are consistent with current algorithms while providing added credible intervals for the coordinates and all network properties. We also show that some networks admit two or more plausible embeddings, a feature that an optimization algorithm can easily overlook.","authors":["Simon Lizotte","Jean-Gabriel Young","Antoine Allard"],"url":"https://arxiv.org/abs/2406.10711"}
{"created":"2025-04-28","title":"Mixed Convection and Entropy Generation Analysis of Carbon Nanotube-Water Nanofluid in a Square Cavity with Cylinders and Flow Deflectors","abstract":"This study explores the mixed convection of carbon nanotube (CNT)-water nanofluid within a square cavity containing heated cylinders under the influence of a magnetic field, focusing on three geometric configurations: a single heated cylinder, two heated cylinders, and two heated cylinders with a flow deflector. The impact of various parameters, including Reynolds number ($Re$), Richardson number ($Ri$), Hartmann number ($Ha$), wavy wall peaks ($n$), nanoparticle volume fraction ($\\phi$), Hartmann angle ($\\gamma$), rotational speed ($\\omega$), and inclination angle ($\\alpha$), on thermal and fluid dynamic behaviors is analyzed. MWCNT nanofluids exhibit up to a 19.1% increase in $Nu_{\\text{ave}}$ compared to SWCNT nanofluids, confirming their superior heat transfer performance. Adding a second heated cylinder increases $Nu_{\\text{ave}}$ by approximately 71.7% compared to a single-cylinder configuration, while the inclusion of a flow deflector modifies vortex structures, further enhancing convective transport. Increasing wavy wall peaks ($n$) enhances heat transfer by intensifying vortex formation and disrupting thermal boundary layers, leading to a more uniform temperature distribution. SWCNT nanofluids exhibit Bejan numbers up to 58.7% higher than MWCNT nanofluids, indicating greater thermal irreversibility. These findings provide valuable insights for optimizing thermal management systems in engineering applications, highlighting the importance of selecting appropriate nanofluids, geometric configurations, and magnetic field parameters to achieve optimal thermal performance and fluid stability.","authors":["Hashnayne Ahmed","Shashanka Biswas","Farzana Akter Tina"],"url":"https://arxiv.org/abs/2407.17625"}
{"created":"2025-04-28","title":"Adaptive Uncertainty Quantification for Generative AI","abstract":"This work is concerned with conformal prediction in contemporary applications (including generative AI) where a black-box model has been trained on data that are not accessible to the user. Mirroring split-conformal inference, we design a wrapper around a black-box algorithm which calibrates conformity scores. This calibration is local and proceeds in two stages by first adaptively partitioning the predictor space into groups and then calibrating sectionally group by group. Adaptive partitioning (self-grouping) is achieved by fitting a robust regression tree to the conformity scores on the calibration set. This new tree variant is designed in such a way that adding a single new observation does not change the tree fit with overwhelmingly large probability. This add-one-in robustness property allows us to conclude a finite sample group-conditional coverage guarantee, a refinement of the marginal guarantee. In addition, unlike traditional split-conformal inference, adaptive splitting and within-group calibration yields adaptive bands which can stretch and shrink locally. We demonstrate benefits of local tightening on several simulated as well as real examples using non-parametric regression. Finally, we consider two contemporary classification applications for obtaining uncertainty quantification around GPT-4o predictions. We conformalize skin disease diagnoses based on self-reported symptoms as well as predicted states of U.S. legislators based on summaries of their ideology. We demonstrate substantial local tightening of the uncertainty sets while attaining similar marginal coverage.","authors":["Jungeum Kim","Sean O'Hagan","Veronika Rockova"],"url":"https://arxiv.org/abs/2408.08990"}
{"created":"2025-04-28","title":"Efficient Budget Allocation for Large-Scale LLM-Enabled Virtual Screening","abstract":"Screening tasks that aim to identify a small subset of top alternatives from a large pool are common in business decision-making processes. These tasks often require substantial human effort to evaluate each alternative's performance, making them time-consuming and costly. Motivated by recent advances in large language models (LLMs), particularly their ability to generate outputs that align well with human evaluations, we consider an LLM-as-human-evaluator approach for conducting screening virtually, thereby reducing the cost burden. To achieve scalability and cost-effectiveness in virtual screening, we identify that the stochastic nature of LLM outputs and their cost structure necessitate efficient budget allocation across all alternatives. To address this, we propose using a top-$m$ greedy evaluation mechanism, a simple yet effective approach that keeps evaluating the current top-$m$ alternatives, and design the explore-first top-$m$ greedy (EFG-$m$) algorithm. We prove that EFG-$m$ is both sample-optimal and consistent in large-scale virtual screening. Surprisingly, we also uncover a bonus ranking effect, where the algorithm naturally induces an indifference-based ranking within the selected subset. To further enhance practicality, we design a suite of algorithm variants to improve screening performance and computational efficiency. Numerical experiments validate our results and demonstrate the effectiveness of our algorithms. Lastly, we conduct a case study on LLM-based virtual screening. The study shows that while LLMs alone may not provide meaningful screening and ranking results when directly queried, integrating them with our sample-optimal algorithms unlocks their potential for cost-effective, large-scale virtual screening.","authors":["Zaile Li","Weiwei Fan","L. Jeff Hong"],"url":"https://arxiv.org/abs/2408.09537"}
{"created":"2025-04-28","title":"RandALO: Out-of-sample risk estimation in no time flat","abstract":"Estimating out-of-sample risk for models trained on large high-dimensional datasets is an expensive but essential part of the machine learning process, enabling practitioners to optimally tune hyperparameters. Cross-validation (CV) serves as the de facto standard for risk estimation but poorly trades off high bias ($K$-fold CV) for computational cost (leave-one-out CV). We propose a randomized approximate leave-one-out (RandALO) risk estimator that is not only a consistent estimator of risk in high dimensions but also less computationally expensive than $K$-fold CV. We support our claims with extensive simulations on synthetic and real data and provide a user-friendly Python package implementing RandALO available on PyPI as randalo and at https://github.com/cvxgrp/randalo.","authors":["Parth Nobel","Daniel LeJeune","Emmanuel J. Cand\\`es"],"url":"https://arxiv.org/abs/2409.09781"}
{"created":"2025-04-28","title":"Inferring Thunderstorm Occurrence from Vertical Profiles of Convection-Permitting Simulations: Physical Insights from a Physical Deep Learning Model","abstract":"Thunderstorms have significant social and economic impacts due to heavy precipitation, hail, lightning, and strong winds, necessitating reliable forecasts. Thunderstorm forecasts based on numerical weather prediction (NWP) often rely on single-level surrogate predictors, like convective available potential energy and convective inhibition, derived from vertical profiles of three-dimensional atmospheric variables. In this study, we develop SALAMA 1D, a deep neural network which directly infers the probability of thunderstorm occurrence from vertical profiles of ten atmospheric variables, bypassing single-level predictors. By training the model on convection-permitting NWP forecasts, we allow SALAMA 1D to flexibly identify convective patterns, with the goal of enhancing forecast accuracy. The model's architecture is physically motivated: sparse connections encourage interactions at similar height levels while keeping model size and inference times computationally efficient, whereas a shuffling mechanism prevents the model from learning non-physical patterns tied to the vertical grid. SALAMA 1D is trained over Central Europe with lightning observations as the ground truth. Comparative analysis against a baseline machine learning model that uses single-level predictors shows SALAMA 1D's superior skill across various metrics and lead times of up to at least 11 hours. Moreover, expanding the archive of forecasts from which training examples are sampled improves skill, even when training set size remains constant. Finally, a sensitivity analysis using saliency maps indicates that our model relies on physically interpretable patterns consistent with established theoretical understanding, such as ice particle content near the tropopause, cloud cover, conditional instability, and low-level moisture.","authors":["Kianusch Vahid Yousefnia","Christoph Metzl","Tobias B\\\"olle"],"url":"https://arxiv.org/abs/2409.20087"}
{"created":"2025-04-28","title":"The Geometry of Fixed-Magnetization Spin Systems at Low Temperature","abstract":"Spin systems are fundamental models of statistical physics that provide insight into collective behavior across scientific domains. Their interest to computer science stems in part from the deep connection between the phase transitions they exhibit and the computational complexity of sampling from the probability distributions they describe. Our focus is on the geometry of spin configurations, motivated by applications to programmable matter and computational biology. Rigorous results in this vein are scarce because the natural setting of these applications is the low-temperature, fixed-magnetization regime. Recent progress in this regime is largely limited to spin systems under which magnetization concentrates, which enables the analysis to be reduced to that of the simpler, variable-magnetization case. More complicated models, like those that arise in applications, do not share this property.","authors":["Jacob Calvert","Shunhao Oh","Dana Randall"],"url":"https://arxiv.org/abs/2411.03643"}
{"created":"2025-04-28","title":"TDAvec: Computing Vector Summaries of Persistence Diagrams for Topological Data Analysis in R and Python","abstract":"Persistent homology is a widely-used tool in topological data analysis (TDA) for understanding the underlying shape of complex data. By constructing a filtration of simplicial complexes from data points, it captures topological features such as connected components, loops, and voids across multiple scales. These features are encoded in persistence diagrams (PDs), which provide a concise summary of the data's topological structure. However, the non-Hilbert nature of the space of PDs poses challenges for their direct use in machine learning applications. To address this, kernel methods and vectorization techniques have been developed to transform PDs into machine-learning-compatible formats. In this paper, we introduce a new software package designed to streamline the vectorization of PDs, offering an intuitive workflow and advanced functionalities. We demonstrate the necessity of the package through practical examples and provide a detailed discussion on its contributions to applied TDA. Definitions of all vectorization summaries used in the package are included in the appendix.","authors":["Aleksei Luchinsky","Umar Islambekov"],"url":"https://arxiv.org/abs/2411.17340"}
{"created":"2025-04-28","title":"Kernel-Based Optimal Control: An Infinitesimal Generator Approach","abstract":"This paper presents a novel operator-theoretic approach for optimal control of nonlinear stochastic systems within reproducing kernel Hilbert spaces. Our learning framework leverages data samples of system dynamics and stage cost functions, with only control penalties and constraints provided. The proposed method directly learns the infinitesimal generator of a controlled stochastic diffusion in an infinite-dimensional hypothesis space. We demonstrate that our approach seamlessly integrates with modern convex operator-theoretic Hamilton-Jacobi-Bellman recursions, enabling a data-driven solution to the optimal control problems. Furthermore, our learning framework includes nonparametric estimators for uncontrolled infinitesimal generators as a special case. Numerical experiments, ranging from synthetic differential equations to simulated robotic systems, showcase the advantages of our approach compared to both modern data-driven and classical nonlinear programming methods for optimal control.","authors":["Petar Bevanda","Nicolas Hoischen","Tobias Wittmann","Jan Br\\\"udigam","Sandra Hirche","Boris Houska"],"url":"https://arxiv.org/abs/2412.01591"}
{"created":"2025-04-28","title":"A Fourfold Pathogen Reference Ontology Suite","abstract":"Infectious diseases remain a critical global health challenge, and the integration of standardized ontologies plays a vital role in managing related data. The Infectious Disease Ontology (IDO) and its extensions, such as the Coronavirus Infectious Disease Ontology (CIDO), are essential for organizing and disseminating information related to infectious diseases. The COVID-19 pandemic highlighted the need for updating IDO and its virus-specific extensions. There is an additional need to update IDO extensions specific to bacteria, fungus, and parasite infectious diseases. We adopt the \"hub and spoke\" methodology to generate pathogen-specific extensions of IDO: Virus Infectious Disease Ontology (VIDO), Bacteria Infectious Disease Ontology (BIDO), Mycosis Infectious Disease Ontology (MIDO), and Parasite Infectious Disease Ontology (PIDO). The creation of pathogen-specific reference ontologies advances modularization and reusability of infectious disease data within the IDO ecosystem. Future work will focus on further refining these ontologies, creating new extensions, and developing application ontologies based on them, in line with ongoing efforts to standardize biological and biomedical terminologies for improved data sharing and analysis.","authors":["Shane Babcock","Carter Benson","Giacomo De Colle","Sydney Cohen","Alexander D. Diehl","Ram A. N. R. Challa","Ray Mavrovich","Joshua Billig","Anthony Huffman","Yongqun He","John Beverley"],"url":"https://arxiv.org/abs/2501.01454"}
{"created":"2025-04-28","title":"Impossibility of Quantum Private Queries","abstract":"Symmetric private information retrieval is a cryptographic task allowing a user to query a database and obtain exactly one entry without revealing to the owner of the database which element was accessed. The task is a variant of general two-party protocols called one-sided secure function evaluation and is closely related to oblivious transfer. Under the name quantum private queries, quantum protocols have been proposed to solve this problem in a cheat-sensitive way: In such protocols, it is not impossible for dishonest participants to cheat, but they risk detection [V. Giovannetti, S. Lloyd, and L. Maccone, Phys. Rev. Lett. 100, 230502 (2008)]. We give an explicit attack against any cheat-sensitive symmetric private information retrieval protocol, showing that any protocol that is secure for the user cannot have non-trivial security guarantees for the owner of the database.","authors":["Esther H\\\"anggi","Severin Winkler"],"url":"https://arxiv.org/abs/2501.12842"}
{"created":"2025-04-28","title":"Distributed Multiple Testing with False Discovery Rate Control in the Presence of Byzantines","abstract":"This work studies distributed multiple testing with false discovery rate (FDR) control in the presence of Byzantine attacks, where an adversary captures a fraction of the nodes and corrupts their reported p-values. We focus on two baseline attack models: an oracle model with the full knowledge of which hypotheses are true nulls, and a practical attack model that leverages the Benjamini-Hochberg (BH) procedure locally to classify which p-values follow the true null hypotheses. We provide a thorough characterization of how both attack models affect the global FDR, which in turn motivates counter-attack strategies and stronger attack models. Our extensive simulation studies confirm the theoretical results, highlight key design trade-offs under attacks and countermeasures, and provide insights into more sophisticated attacks.","authors":["Daofu Zhang","Mehrdad Pournaderi","Yu Xiang","Pramod Varshney"],"url":"https://arxiv.org/abs/2501.13242"}
{"created":"2025-04-28","title":"Function-coherent gambles","abstract":"The desirable gambles framework provides a foundational approach to imprecise probability theory but relies heavily on linear utility assumptions. This paper introduces function-coherent gambles, a generalization that accommodates non-linear utility while preserving essential rationality properties. We establish core axioms for function-coherence and prove a representation theorem that characterizes acceptable gambles through continuous linear functionals. The framework is then applied to analyze various forms of discounting in intertemporal choice, including hyperbolic, quasi-hyperbolic, scale-dependent, and state-dependent discounting. We demonstrate how these alternatives to constant-rate exponential discounting can be integrated within the function-coherent framework. This unified treatment provides theoretical foundations for modeling sophisticated patterns of time preference within the desirability paradigm, bridging a gap between normative theory and observed behavior in intertemporal decision-making under genuine uncertainty.","authors":["Gregory Wheeler"],"url":"https://arxiv.org/abs/2503.01855"}
{"created":"2025-04-28","title":"A Scalable Synthesis Algorithm for Reversible Functions","abstract":"Reversible computation is an emerging technology that has gained significant attention due to its critical role in quantum circuit synthesis and low-power design. This paper introduces a transformation-based method for exact synthesis of reversible circuits. The proposed approach utilizes a novel adaptation of the Quine-McCluskey algorithm to eliminate input-output discrepancies in the truth table, transforming the permutation matrix into an identity matrix. Furthermore, a novel search space reduction technique is presented which, combined with the primary method, enables the synthesis algorithm to handle high-input reversible functions. This approach combines the influence of multiple control qubits on a target qubit, evaluating their collective impact. This aggregation can decrease the control qubit count within quantum gates. Consequently, it proves beneficial for applications like surface code error correction architectures as well as current Noisy Intermediate-Scale Quantum (NISQ) hardwares. Experimental results demonstrate significant improvements over the state-of-the-art exact synthesis methods, achieving up to 99% improvements in terms of the number of levels of T-gates.","authors":["Moein Sarvaghad-Moghaddam","Morteza Saheb Zamani","Mehdi Sedighi"],"url":"https://arxiv.org/abs/2504.02632"}
{"created":"2025-04-28","title":"Generating ensembles of spatially-coherent in-situ forecasts using flow matching","abstract":"We propose a machine-learning-based methodology for in-situ weather forecast postprocessing that is both spatially coherent and multivariate. Compared to previous work, our Flow MAtching Postprocessing (FMAP) better represents the correlation structures of the observations distribution, while also improving marginal performance at the stations. FMAP generates forecasts that are not bound to what is already modeled by the underlying gridded prediction and can infer new correlation structures from data. The resulting model can generate an arbitrary number of forecasts from a limited number of numerical simulations, allowing for low-cost forecasting systems. A single training is sufficient to perform postprocessing at multiple lead times, in contrast with other methods which use multiple trained networks at generation time. This work details our methodology, including a spatial attention transformer backbone trained within a flow matching generative modeling framework. FMAP shows promising performance in experiments on the EUPPBench dataset, forecasting surface temperature and wind gust values at station locations in western Europe up to five-day lead times.","authors":["David Landry","Claire Monteleoni","Anastase Charantonis"],"url":"https://arxiv.org/abs/2504.03463"}
{"created":"2025-04-28","title":"EquiNO: A Physics-Informed Neural Operator for Multiscale Simulations","abstract":"Multiscale problems are ubiquitous in physics. Numerical simulations of such problems by solving partial differential equations (PDEs) at high resolution are computationally too expensive for many-query scenarios, e.g., uncertainty quantification, remeshing applications, topology optimization, and so forth. This limitation has motivated the application of data-driven surrogate models, where the microscale computations are $\\textit{substituted}$ with a surrogate, usually acting as a black-box mapping between macroscale quantities. These models offer significant speedups but struggle with incorporating microscale physical constraints, such as the balance of linear momentum and constitutive models. In this contribution, we propose Equilibrium Neural Operator (EquiNO) as a $\\textit{complementary}$ physics-informed PDE surrogate for predicting microscale physics and compare it with variational physics-informed neural and operator networks. Our framework, applicable to the so-called multiscale FE$^{\\,2}\\,$ computations, introduces the FE-OL approach by integrating the finite element (FE) method with operator learning (OL). We apply the proposed FE-OL approach to quasi-static problems of solid mechanics. The results demonstrate that FE-OL can yield accurate solutions even when confronted with a restricted dataset during model development. Our results show that EquiNO achieves speedup factors exceeding 8000-fold compared to traditional methods and offers an optimal balance between data-driven and physics-based strategies.","authors":["Hamidreza Eivazi","Jendrik-Alexander Tr\\\"oger","Stefan Wittek","Stefan Hartmann","Andreas Rausch"],"url":"https://arxiv.org/abs/2504.07976"}
{"created":"2025-04-28","title":"On graphs with a simple structure of maximal cliques","abstract":"We say that a hereditary graph class $\\mathcal{G}$ is \\emph{clique-sparse} if there is a constant $k=k(\\mathcal{G})$ such that for every graph $G\\in\\mathcal{G}$, every vertex of $G$ belongs to at most $k$ maximal cliques, and any maximal clique of $G$ can be intersected in at most $k$ different ways by other maximal cliques.","authors":["J. Pascal Gollin","Meike Hatzel","Sebastian Wiederrecht"],"url":"https://arxiv.org/abs/2504.16863"}
{"created":"2025-04-28","title":"Better artificial intelligence does not mean better models of biology","abstract":"Deep neural networks (DNNs) once showed increasing alignment with primate perception and neural responses as they improved on vision benchmarks, raising hopes that advances in AI would yield better models of biological vision. However, we show across three benchmarks that this alignment is now plateauing - and in some cases worsening - as DNNs scale to human or superhuman accuracy. This divergence may reflect the adoption of visual strategies that differ from those used by primates. These findings challenge the view that progress in artificial intelligence will naturally translate to neuroscience. We argue that vision science must chart its own course, developing algorithms grounded in biological visual systems rather than optimizing for benchmarks based on internet-scale datasets.","authors":["Drew Linsley","Pinyuan Feng","Thomas Serre"],"url":"https://arxiv.org/abs/2504.16940"}
