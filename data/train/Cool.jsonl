{"abstract": "To succeed in common digital tasks such as web navigation, agents must carry out a variety of specialized tasks such as searching for products or planning a travel route. To tackle these tasks, agents can bootstrap themselves by learning task-specific skills online through interaction with the web environment. In this work, we demonstrate that programs are an effective representation for skills. We propose agent skill induction (ASI), which allows agents to adapt themselves by inducing, verifying, and utilizing program-based skills on the fly. We start with an evaluation on the WebArena agent benchmark and show that ASI outperforms the static baseline agent and its text-skill counterpart by 23.5% and 11.3% in success rate, mainly thanks to the programmatic verification guarantee during the induction phase. ASI also improves efficiency by reducing 10.7-15.3% of the steps over baselines, by composing primitive actions (e.g., click) into higher-level skills (e.g., search product). We then highlight the efficacy of ASI in remaining efficient and accurate under scaled-up web activities. Finally, we examine the generalizability of induced skills when transferring between websites, and find that ASI can effectively reuse common skills, while also updating incompatible skills to versatile website changes.", "authors": ["Zora Zhiruo Wang", "Apurva Gandhi", "Graham Neubig", "Daniel Fried"], "created": "2025-04-11", "title": "Inducing Programmatic Skills for Agentic Tasks", "url": "https://arxiv.org/abs/2504.06821"}
{"abstract": "We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.", "authors": ["Jiacheng Liu", "Taylor Blanton", "Yanai Elazar", "Sewon Min", "YenSung Chen", "Arnavi Chheda-Kothary", "Huy Tran", "Byron Bischoff", "Eric Marsh", "Michael Schmitz", "Cassidy Trier", "Aaron Sarnat", "Jenna James", "Jon Borchardt", "Bailey Kuehl", "Evie Cheng", "Karen Farley", "Sruthi Sreeram", "Taira Anderson", "David Albright", "Carissa Schoenick", "Luca Soldaini", "Dirk Groeneveld", "Rock Yuren Pang", "Pang Wei Koh", "Noah A. Smith", "Sophie Lebrecht", "Yejin Choi", "Hannaneh Hajishirzi", "Ali Farhadi", "Jesse Dodge"], "created": "2025-04-11", "title": "OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens", "url": "https://arxiv.org/abs/2504.07096"}
