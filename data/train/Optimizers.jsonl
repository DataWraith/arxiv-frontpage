{"abstract": "We present a manifestly covariant formulation of the gradient descent method, ensuring consistency across arbitrary coordinate systems and general curved trainable spaces. The optimization dynamics is defined using a covariant force vector and a covariant metric tensor, both computed from the first and second statistical moments of the gradients. These moments are estimated through time-averaging with an exponential weight function, which preserves linear computational complexity. We show that commonly used optimization methods such as RMSProp, Adam and AdaBelief correspond to special limits of the covariant gradient descent (CGD) and demonstrate how these methods can be further generalized and improved.", "authors": ["Dmitry Guskov", "Vitaly Vanchurin"], "created": "2025-04-11", "title": "Covariant Gradient Descent", "url": "https://arxiv.org/abs/2504.05279"}
