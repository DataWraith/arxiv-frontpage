{"created": "2025-04-21", "title": "Beyond Grids: Multi-objective Bayesian Optimization With Adaptive Discretization", "abstract": "We consider the problem of optimizing a vector-valued objective function $\\boldsymbol{f}$ sampled from a Gaussian Process (GP) whose index set is a well-behaved, compact metric space $({\\cal X},d)$ of designs. We assume that $\\boldsymbol{f}$ is not known beforehand and that evaluating $\\boldsymbol{f}$ at design $x$ results in a noisy observation of $\\boldsymbol{f}(x)$. Since identifying the Pareto optimal designs via exhaustive search is infeasible when the cardinality of ${\\cal X}$ is large, we propose an algorithm, called Adaptive $\\boldsymbol{\\epsilon}$-PAL, that exploits the smoothness of the GP-sampled function and the structure of $({\\cal X},d)$ to learn fast. In essence, Adaptive $\\boldsymbol{\\epsilon}$-PAL employs a tree-based adaptive discretization technique to identify an $\\boldsymbol{\\epsilon}$-accurate Pareto set of designs in as few evaluations as possible. We provide both information-type and metric dimension-type bounds on the sample complexity of $\\boldsymbol{\\epsilon}$-accurate Pareto set identification. We also experimentally show that our algorithm outperforms other Pareto set identification methods.", "authors": ["Andi Nika", "Sepehr Elahi", "\\c{C}a\\u{g}{\\i}n Ararat", "Cem Tekin"], "url": "https://arxiv.org/abs/2006.14061"}
{"created": "2025-04-11", "title": "Scalable mixed-domain Gaussian process modeling and model reduction for longitudinal data", "abstract": "Gaussian process (GP) models that combine both categorical and continuous input variables have found use in analysis of longitudinal data and computer experiments. However, standard inference for these models has the typical cubic scaling, and common scalable approximation schemes for GPs cannot be applied since the covariance function is non-continuous. In this work, we derive a basis function approximation scheme for mixed-domain covariance functions, which scales linearly with respect to the number of observations and total number of basis functions. The proposed approach is naturally applicable to also Bayesian GP regression with discrete observation models. We demonstrate the scalability of the approach and compare model reduction techniques for additive GP models in a longitudinal data context. We confirm that we can approximate the exact GP model accurately in a fraction of the runtime compared to fitting the corresponding exact model. In addition, we demonstrate a scalable model reduction workflow for obtaining smaller and more interpretable models when dealing with a large number of candidate predictors.", "authors": ["Juho Timonen", "Harri L\\\"ahdesm\\\"aki"], "url": "https://arxiv.org/abs/2111.02019"}
{"created": "2025-04-18", "title": "Accelerating Non-Conjugate Gaussian Processes By Trading Off Computation For Uncertainty", "abstract": "Non-conjugate Gaussian processes (NCGPs) define a flexible probabilistic framework to model categorical, ordinal and continuous data, and are widely used in practice. However, exact inference in NCGPs is prohibitively expensive for large datasets, thus requiring approximations in practice. The approximation error adversely impacts the reliability of the model and is not accounted for in the uncertainty of the prediction. We introduce a family of iterative methods that explicitly model this error. They are uniquely suited to parallel modern computing hardware, efficiently recycle computations, and compress information to reduce both the time and memory requirements for NCGPs. As we demonstrate on large-scale classification problems, our method significantly accelerates posterior inference compared to competitive baselines by trading off reduced computation for increased uncertainty.", "authors": ["Lukas Tatzel", "Jonathan Wenger", "Frank Schneider", "Philipp Hennig"], "url": "https://arxiv.org/abs/2310.20285"}
{"created": "2025-04-15", "title": "Towards safe Bayesian optimization with Wiener kernel regression", "abstract": "Bayesian Optimization (BO) is a data-driven strategy for minimizing/maximizing black-box functions based on probabilistic surrogate models. In the presence of safety constraints, the performance of BO crucially relies on tight probabilistic error bounds related to the uncertainty surrounding the surrogate model. For the case of Gaussian Process surrogates and Gaussian measurement noise, we present a novel error bound based on the recently proposed Wiener kernel regression. We prove that under rather mild assumptions, the proposed error bound is tighter than bounds previously documented in the literature, leading to enlarged safety regions. We draw upon a numerical example to demonstrate the efficacy of the proposed error bound in safe BO.", "authors": ["Oleksii Molodchyk", "Johannes Teutsch", "Timm Faulwasser"], "url": "https://arxiv.org/abs/2411.02253"}
{"abstract": "Time-Varying Bayesian Optimization (TVBO) is the go-to framework for optimizing a time-varying, expensive, noisy black-box function. However, most of the solutions proposed so far either rely on unrealistic assumptions on the nature of the objective function or do not offer any theoretical guarantees. We propose the first analysis that asymptotically bounds the cumulative regret of TVBO algorithms under mild and realistic assumptions only. In particular, we provide an algorithm-independent lower regret bound and an upper regret bound that holds for a large class of TVBO algorithms. Based on this analysis, we formulate recommendations for TVBO algorithms and show how an algorithm (BOLT) that follows them performs better than the state-of-the-art of TVBO through experiments on synthetic and real-world problems.", "authors": ["Anthony Bardou", "Patrick Thiran"], "created": "2025-04-11", "title": "Optimizing Through Change: Bounds and Recommendations for Time-Varying Bayesian Optimization Algorithms", "url": "https://arxiv.org/abs/2501.18963"}
{"abstract": "We address the challenge of designing cellular networks for uncrewed aerial vehicles (UAVs) corridors through a novel data-driven approach. We assess multiple state-of-the-art high-dimensional Bayesian optimization (HD-BO) techniques to jointly optimize the cell antenna tilts and half-power beamwidth (HPBW). We find that some of these approaches achieve over 20dB gains in median SINR along UAV corridors, with negligible degradation to ground user performance. Furthermore, we explore the HD-BO's capabilities in terms of model generalization via transfer learning, where data from a previously observed scenario source is leveraged to predict the optimal solution for a new scenario target. We provide examples of scenarios where such transfer learning is successful and others where it fails. Moreover, we demonstrate that HD-BO enables multi-objective optimization, identifying optimal design trade-offs between data rates on the ground versus UAV coverage reliability. We observe that aiming to provide UAV coverage across the entire sky can lower the rates for ground users compared to setups specifically optimized for UAV corridors. Finally, we validate our approach through a case study in a real-world cellular network, where HD-BO identifies optimal and non-obvious antenna configurations that result in more than double the rates along 3D UAV corridors with negligible ground performance loss.", "authors": ["Mohamed Benzaghta", "Giovanni Geraci", "David L\\'opez-P\\'erez", "Alvaro Valcarce"], "created": "2025-04-11", "title": "Cellular Network Design for UAV Corridors via Data-driven High-dimensional Bayesian Optimization", "url": "https://arxiv.org/abs/2504.05176"}
{"created": "2025-04-14", "title": "Regularized infill criteria for multi-objective Bayesian optimization with application to aircraft design", "abstract": "Bayesian optimization is an advanced tool to perform ecient global optimization It consists on enriching iteratively surrogate Kriging models of the objective and the constraints both supposed to be computationally expensive of the targeted optimization problem Nowadays efficient extensions of Bayesian optimization to solve expensive multiobjective problems are of high interest The proposed method in this paper extends the super efficient global optimization with mixture of experts SEGOMOE to solve constrained multiobjective problems To cope with the illposedness of the multiobjective inll criteria different enrichment procedures using regularization techniques are proposed The merit of the proposed approaches are shown on known multiobjective benchmark problems with and without constraints The proposed methods are then used to solve a biobjective application related to conceptual aircraft design with ve unknown design variables and three nonlinear inequality constraints The preliminary results show a reduction of the total cost in terms of function evaluations by a factor of 20 compared to the evolutionary algorithm NSGA-II.", "authors": ["Robin Grapin", "Youssef Diouane", "Joseph Morlier", "Nathalie Bartoli", "Thierry Lefebvre", "Paul Saves", "Jasper Bussemaker"], "url": "https://arxiv.org/abs/2504.08671"}
{"created": "2025-04-14", "title": "Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design", "abstract": "Multidisciplinary design optimization methods aim at adapting numerical optimization techniques to the design of engineering systems involving multiple disciplines. In this context, a large number of mixed continuous, integer and categorical variables might arise during the optimization process and practical applications involve a large number of design variables. Recently, there has been a growing interest in mixed variables constrained Bayesian optimization but most existing approaches severely increase the number of the hyperparameters related to the surrogate model. In this paper, we address this issue by constructing surrogate models using less hyperparameters. The reduction process is based on the partial least squares method. An adaptive procedure for choosing the number of hyperparameters is proposed. The performance of the proposed approach is confirmed on analytical tests as well as two real applications related to aircraft design. A significant improvement is obtained compared to genetic algorithms.", "authors": ["Paul Saves", "Nathalie Bartoli", "Youssef Diouane", "Thierry Lefebvre", "Joseph Morlier", "Christophe David", "Eric Nguyen Van", "S\\'ebastien Defoort"], "url": "https://arxiv.org/abs/2504.08682"}
{"created": "2025-04-14", "title": "Surrogate-based optimization of system architectures subject to hidden constraints", "abstract": "The exploration of novel architectures requires physics-based simulation due to a lack of prior experience to start from, which introduces two specific challenges for optimization algorithms: evaluations become more expensive (in time) and evaluations might fail. The former challenge is addressed by Surrogate-Based Optimization (SBO) algorithms, in particular Bayesian Optimization (BO) using Gaussian Process (GP) models. An overview is provided of how BO can deal with challenges specific to architecture optimization, such as design variable hierarchy and multiple objectives: specific measures include ensemble infills and a hierarchical sampling algorithm. Evaluations might fail due to non-convergence of underlying solvers or infeasible geometry in certain areas of the design space. Such failed evaluations, also known as hidden constraints, pose a particular challenge to SBO/BO, as the surrogate model cannot be trained on empty results. This work investigates various strategies for satisfying hidden constraints in BO algorithms. Three high-level strategies are identified: rejection of failed points from the training set, replacing failed points based on viable (non-failed) points, and predicting the failure region. Through investigations on a set of test problems including a jet engine architecture optimization problem, it is shown that best performance is achieved with a mixed-discrete GP to predict the Probability of Viability (PoV), and by ensuring selected infill points satisfy some minimum PoV threshold. This strategy is demonstrated by solving a jet engine architecture problem that features at 50% failure rate and could not previously be solved by a BO algorithm. The developed BO algorithm and used test problems are available in the open-source Python library SBArchOpt.", "authors": ["Jasper Bussemaker", "Paul Saves", "Nathalie Bartoli", "Thierry Lefebvre", "Bj\\\"orn Nagel"], "url": "https://arxiv.org/abs/2504.08721"}
{"created": "2025-04-15", "title": "Distilling and exploiting quantitative insights from Large Language Models for enhanced Bayesian optimization of chemical reactions", "abstract": "Machine learning and Bayesian optimization (BO) algorithms can significantly accelerate the optimization of chemical reactions. Transfer learning can bolster the effectiveness of BO algorithms in low-data regimes by leveraging pre-existing chemical information or data outside the direct optimization task (i.e., source data). Large language models (LLMs) have demonstrated that chemical information present in foundation training data can give them utility for processing chemical data. Furthermore, they can be augmented with and help synthesize potentially multiple modalities of source chemical data germane to the optimization task. In this work, we examine how chemical information from LLMs can be elicited and used for transfer learning to accelerate the BO of reaction conditions to maximize yield. Specifically, we show that a survey-like prompting scheme and preference learning can be used to infer a utility function which models prior chemical information embedded in LLMs over a chemical parameter space; we find that the utility function shows modest correlation to true experimental measurements (yield) over the parameter space despite operating in a zero-shot setting. Furthermore, we show that the utility function can be leveraged to focus BO efforts in promising regions of the parameter space, improving the yield of the initial BO query and enhancing optimization in 4 of the 6 datasets studied. Overall, we view this work as a step towards bridging the gap between the chemistry knowledge embedded in LLMs and the capabilities of principled BO methods to accelerate reaction optimization.", "authors": ["Roshan Patel", "Saeed Moayedpour", "Louis De Lescure", "Lorenzo Kogler-Anele", "Alan Cherney", "Sven Jager", "Yasser Jangjou"], "url": "https://arxiv.org/abs/2504.08874"}
{"created": "2025-04-15", "title": "Multi-objective Bayesian Optimization With Mixed-categorical Design Variables for Expensive-to-evaluate Aeronautical Applications", "abstract": "This work aims at developing new methodologies to optimize computational costly complex systems (e.g., aeronautical engineering systems). The proposed surrogate-based method (often called Bayesian optimization) uses adaptive sampling to promote a trade-off between exploration and exploitation. Our in-house implementation, called SEGOMOE, handles a high number of design variables (continuous, discrete or categorical) and nonlinearities by combining mixtures of experts for the objective and/or the constraints. Additionally, the method handles multi-objective optimization settings, as it allows the construction of accurate Pareto fronts with a minimal number of function evaluations. Different infill criteria have been implemented to handle multiple objectives with or without constraints. The effectiveness of the proposed method was tested on practical aeronautical applications within the context of the European Project AGILE 4.0 and demonstrated favorable results. A first example concerns a retrofitting problem where a comparison between two optimizers have been made. A second example introduces hierarchical variables to deal with architecture system in order to design an aircraft family. The third example increases drastically the number of categorical variables as it combines aircraft design, supply chain and manufacturing process. In this article, we show, on three different realistic problems, various aspects of our optimization codes thanks to the diversity of the treated aircraft problems.", "authors": ["Nathalie Bartoli", "Thierry Lefebvre", "R\\'emi Lafage", "Paul Saves", "Youssef Diouane", "Joseph Morlier", "Jasper Bussemaker", "Giuseppa Donelli", "Joao Marcos Gomes de Mello", "Massimo Mandorino", "Pierluigi Della Vecchia"], "url": "https://arxiv.org/abs/2504.09930"}
{"created": "2025-04-15", "title": "Towards Scalable Bayesian Optimization via Gradient-Informed Bayesian Neural Networks", "abstract": "Bayesian optimization (BO) is a widely used method for data-driven optimization that generally relies on zeroth-order data of objective function to construct probabilistic surrogate models. These surrogates guide the exploration-exploitation process toward finding global optimum. While Gaussian processes (GPs) are commonly employed as surrogates of the unknown objective function, recent studies have highlighted the potential of Bayesian neural networks (BNNs) as scalable and flexible alternatives. Moreover, incorporating gradient observations into GPs, when available, has been shown to improve BO performance. However, the use of gradients within BNN surrogates remains unexplored. By leveraging automatic differentiation, gradient information can be seamlessly integrated into BNN training, resulting in more informative surrogates for BO. We propose a gradient-informed loss function for BNN training, effectively augmenting function observations with local gradient information. The effectiveness of this approach is demonstrated on well-known benchmarks in terms of improved BNN predictions and faster BO convergence as the number of decision variables increases.", "authors": ["Georgios Makrygiorgos", "Joshua Hang Sai Ip", "Ali Mesbah"], "url": "https://arxiv.org/abs/2504.10076"}
{"created": "2025-04-16", "title": "Collaborative Bayesian Optimization via Wasserstein Barycenters", "abstract": "Motivated by the growing need for black-box optimization and data privacy, we introduce a collaborative Bayesian optimization (BO) framework that addresses both of these challenges. In this framework agents work collaboratively to optimize a function they only have oracle access to. In order to mitigate against communication and privacy constraints, agents are not allowed to share their data but can share their Gaussian process (GP) surrogate models. To enable collaboration under these constraints, we construct a central model to approximate the objective function by leveraging the concept of Wasserstein barycenters of GPs. This central model integrates the shared models without accessing the underlying data. A key aspect of our approach is a collaborative acquisition function that balances exploration and exploitation, allowing for the optimization of decision variables collaboratively in each iteration. We prove that our proposed algorithm is asymptotically consistent and that its implementation via Monte Carlo methods is numerically accurate. Through numerical experiments, we demonstrate that our approach outperforms other baseline collaborative frameworks and is competitive with centralized approaches that do not consider data privacy.", "authors": ["Donglin Zhan", "Haoting Zhang", "Rhonda Righter", "Zeyu Zheng", "James Anderson"], "url": "https://arxiv.org/abs/2504.10770"}
{"created": "2025-04-16", "title": "An Adaptive Dropout Approach for High-Dimensional Bayesian Optimization", "abstract": "Bayesian optimization (BO) is a widely used algorithm for solving expensive black-box optimization problems. However, its performance decreases significantly on high-dimensional problems due to the inherent high-dimensionality of the acquisition function. In the proposed algorithm, we adaptively dropout the variables of the acquisition function along the iterations. By gradually reducing the dimension of the acquisition function, the proposed approach has less and less difficulty to optimize the acquisition function. Numerical experiments demonstrate that AdaDropout effectively tackle high-dimensional challenges and improve solution quality where standard Bayesian optimization methods often struggle. Moreover, it achieves superior results when compared with state-of-the-art high-dimensional Bayesian optimization approaches. This work provides a simple yet efficient solution for high-dimensional expensive optimization.", "authors": ["Jundi Huang", "Dawei Zhan"], "url": "https://arxiv.org/abs/2504.11353"}
{"created": "2025-04-21", "title": "Risk-aware black-box portfolio construction using Bayesian optimization with adaptive weighted Lagrangian estimator", "abstract": "Existing portfolio management approaches are often black-box models due to safety and commercial issues in the industry. However, their performance can vary considerably whenever market conditions or internal trading strategies change. Furthermore, evaluating these non-transparent systems is expensive, where certain budgets limit observations of the systems. Therefore, optimizing performance while controlling the potential risk of these financial systems has become a critical challenge. This work presents a novel Bayesian optimization framework to optimize black-box portfolio management models under limited observations. In conventional Bayesian optimization settings, the objective function is to maximize the expectation of performance metrics. However, simply maximizing performance expectations leads to erratic optimization trajectories, which exacerbate risk accumulation in portfolio management. Meanwhile, this can lead to misalignment between the target distribution and the actual distribution of the black-box model. To mitigate this problem, we propose an adaptive weight Lagrangian estimator considering dual objective, which incorporates maximizing model performance and minimizing variance of model observations. Extensive experiments demonstrate the superiority of our approach over five backtest settings with three black-box stock portfolio management models. Ablation studies further verify the effectiveness of the proposed estimator.", "authors": ["Zinuo You", "John Cartlidge", "Karen Elliott", "Menghan Ge", "Daniel Gold"], "url": "https://arxiv.org/abs/2504.13529"}
