{"abstract": "Manifold learning techniques play a pivotal role in machine learning by revealing lower-dimensional embeddings within high-dimensional data, thus enhancing both the efficiency and interpretability of data analysis by transforming the data into a lower-dimensional representation. However, a notable challenge with current manifold learning methods is their lack of explicit functional mappings, crucial for explainability in many real-world applications. Genetic programming, known for its interpretable functional tree-based models, has emerged as a promising approach to address this challenge. Previous research leveraged multi-objective GP to balance manifold quality against embedding dimensionality, producing functional mappings across a range of embedding sizes. Yet, these mapping trees often became complex, hindering explainability. In response, in this paper, we introduce Genetic Programming for Explainable Manifold Learning (GP-EMaL), a novel approach that directly penalises tree complexity. Our new method is able to maintain high manifold quality while significantly enhancing explainability and also allows customisation of complexity measures, such as symmetry balancing, scaling, and node complexity, catering to diverse application needs. Our experimental analysis demonstrates that GP-EMaL is able to match the performance of the existing approach in most cases, while using simpler, smaller, and more interpretable tree structures. This advancement marks a significant step towards achieving interpretable manifold learning.", "authors": ["Ben Cravens", "Andrew Lensen", "Paula Maddigan", "Bing Xue"], "created": "2025-04-11", "title": "Genetic Programming for Explainable Manifold Learning", "url": "https://arxiv.org/abs/2403.14139"}
{"title": "When to Truncate the Archive? On the Effect of the Truncation Frequency in Multi-Objective Optimisation", "url": "https://arxiv.org/abs/2504.01332", "authors": ["Zhiji Cui", "Zimin Liang", "Lie Meng Pang", "Hisao Ishibuchi", "Miqing Li"], "created": "2025-04-15", "abstract": "Using an archive to store nondominated solutions found during the search of a multi-objective evolutionary algorithm (MOEA) is a useful practice. However, as nondominated solutions of a multi-objective optimisation problem can be enormous or infinitely many, it is desirable to provide the decision-maker with only a small, representative portion of all the nondominated solutions in the archive, thus entailing a truncation operation. Then, an important issue is when to truncate the archive. This can be done once a new solution generated, a batch of new solutions generated, or even using an unbounded archive to keep all nondominated solutions generated and truncate it later. Intuitively, the last approach may lead to a better result since we have all the information in hand before performing the truncation. In this paper, we study this issue and investigate the effect of the timing of truncating the archive. We apply well-established truncation criteria that are commonly used in the population maintenance procedure of MOEAs (e.g., crowding distance, hypervolume indicator, and decomposition). We show that, interestingly, truncating the archive once a new solution generated tends to be the best, whereas considering an unbounded archive is often the worst. We analyse and discuss this phenomenon. Our results highlight the importance of developing effective subset selection techniques (rather than employing the population maintenance methods in MOEAs) when using a large archive."}
{"abstract": "Discovering efficient algorithms for solving complex problems has been an outstanding challenge in mathematics and computer science, requiring substantial human expertise over the years. Recent advancements in evolutionary search with large language models (LLMs) have shown promise in accelerating the discovery of algorithms across various domains, particularly in mathematics and optimization. However, existing approaches treat the LLM as a static generator, missing the opportunity to update the model with the signal obtained from evolutionary exploration. In this work, we propose to augment LLM-based evolutionary search by continuously refining the search operator - the LLM - through reinforcement learning (RL) fine-tuning. Our method leverages evolutionary search as an exploration strategy to discover improved algorithms, while RL optimizes the LLM policy based on these discoveries. Our experiments on three combinatorial optimization tasks - bin packing, traveling salesman, and the flatpack problem - show that combining RL and evolutionary search improves discovery efficiency of improved algorithms, showcasing the potential of RL-enhanced evolutionary strategies to assist computer scientists and mathematicians for more efficient algorithm design.", "authors": ["Anja Surina", "Amin Mansouri", "Lars Quaedvlieg", "Amal Seddas", "Maryna Viazovska", "Emmanuel Abbe", "Caglar Gulcehre"], "created": "2025-04-11", "title": "Algorithm Discovery With LLMs: Evolutionary Search Meets Reinforcement Learning", "url": "https://arxiv.org/abs/2504.05108"}
{"abstract": "Quantum computing leverages the unique properties of qubits and quantum parallelism to solve problems intractable for classical systems, offering unparalleled computational potential. However, the optimization of quantum circuits remains critical, especially for noisy intermediate-scale quantum (NISQ) devices with limited qubits and high error rates. Genetic algorithms (GAs) provide a promising approach for efficient quantum circuit synthesis by automating optimization tasks. This work examines the impact of various mutation strategies within a GA framework for quantum circuit synthesis. By analyzing how different mutations transform circuits, it identifies strategies that enhance efficiency and performance. Experiments utilized a fitness function emphasizing fidelity, while accounting for circuit depth and T operations, to optimize circuits with four to six qubits. Comprehensive hyperparameter testing revealed that combining delete and swap strategies outperformed other approaches, demonstrating their effectiveness in developing robust GA-based quantum circuit optimizers.", "authors": ["Michael K\\\"olle", "Tom Bintener", "Maximilian Zorn", "Gerhard Stenzel", "Leo S\\\"unkel", "Thomas Gabor", "Claudia Linnhoff-Popien"], "created": "2025-04-11", "title": "Evaluating Mutation Techniques in Genetic Algorithm-Based Quantum Circuit Synthesis", "url": "https://arxiv.org/abs/2504.06413"}
{"title": "Genetic Algorithm Design Exploration for On-Device Training on FPGAs", "url": "https://arxiv.org/abs/2504.08534", "authors": ["Alaa Mazouz", "Van-Tam Nguyen"], "created": "2025-04-14", "abstract": "We propose an automated Design Space Exploration (DSE) workflow for generating adaptive and reconfigurable deep learning models on FPGA hardware. The workflow consists of two main components: Offline Design Exploration (ODE) and Online Design Reconfiguration (ODR). ODE applies a multi-objective genetic algorithm to explore CNN-based hardware configurations, optimizing for latency and resource utilization by leveraging intra-layer parallelism. Given a CNN architecture and user-defined constraints, the hardware model is generated automatically. ODR enables runtime hardware adaptability by dynamically selecting between partial or full reconfigurable designs based on application requirements. This flexibility is essential for time-critical, autonomous onboard systems. We demonstrate the proposed workflow on the Xilinx Zynq-7100 FPGA operating at 200 MHz, using CNN models trained on MNIST, SVHN, and CIFAR-10. ODE-generated designs show latency improvements of up to 95 times for MNIST, 71 times for CIFAR-10, and 18 times for SVHN. Resource utilization in DSP slices was improved by up to 44 times for MNIST, 52 times for SVHN, and 24 times for CIFAR-10. The ODR approach achieved trade-offs between accuracy and performance, such as a 0.7 percent accuracy drop for a 13 times speedup and 25 percent power reduction on MNIST, a 2 percent drop for 14 times speedup and 28 percent power savings on SVHN, and a 4 percent drop for 50 times speedup with 32.5 percent power reduction on CIFAR-10."}
