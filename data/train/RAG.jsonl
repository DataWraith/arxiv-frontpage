{"abstract": "Processing long contexts presents a significant challenge for large language models (LLMs). While recent advancements allow LLMs to handle much longer contexts than before (e.g., 32K or 128K tokens), it is computationally expensive and can still be insufficient for many applications. Retrieval-Augmented Generation (RAG) is considered a promising strategy to address this problem. However, conventional RAG methods face inherent limitations because of two underlying requirements: 1) explicitly stated queries, and 2) well-structured knowledge. These conditions, however, do not hold in general long-context processing tasks.", "authors": ["Hongjin Qian", "Zheng Liu", "Peitian Zhang", "Kelong Mao", "Defu Lian", "Zhicheng Dou", "Tiejun Huang"], "created": "2025-04-11", "title": "MemoRAG: Boosting Long Context Processing with Global Memory-Enhanced Retrieval Augmentation", "url": "https://arxiv.org/abs/2409.05591"}
{"abstract": "Large language models (LLMs) excel in question-answering (QA) tasks, and retrieval-augmented generation (RAG) enhances their precision by incorporating external evidence from diverse sources like web pages, databases, and knowledge graphs. However, current RAG methods rely on agent-specific strategies for individual data sources, posing challenges low-resource or black-box environments and complicates operations when evidence is fragmented across sources. To address these limitations, we propose ER-RAG, a framework that unifies evidence integration across heterogeneous data sources using the Entity-Relationship (ER) model. ER-RAG standardizes entity retrieval and relationship querying through ER-based APIs with GET and JOIN operations. It employs a two-stage generation process: first, a preference optimization module selects optimal sources; second, another module constructs API chains based on source schemas. This unified approach allows efficient fine-tuning and seamless integration across diverse data sources. ER-RAG demonstrated its effectiveness by winning all three tracks of the 2024 KDDCup CRAG Challenge, achieving performance on par with commercial RAG pipelines using an 8B LLM backbone. It outperformed hybrid competitors by 3.1% in LLM score and accelerated retrieval by 5.5X.", "authors": ["Yikuan Xia", "Jiazun Chen", "Yirui Zhan", "Suifeng Zhao", "Weipeng Jiang", "Chaorui Zhang", "Wei Han", "Bo Bai", "Jun Gao"], "created": "2025-04-11", "title": "ER-RAG: Enhance RAG with ER-Based Unified Modeling of Heterogeneous Data Sources", "url": "https://arxiv.org/abs/2504.06271"}
{"title": "Out of Style: RAG's Fragility to Linguistic Variation", "url": "https://arxiv.org/abs/2504.08231", "authors": ["Tianyu Cao", "Neel Bhandari", "Akhila Yerukola", "Akari Asai", "Maarten Sap"], "created": "2025-04-14", "abstract": "Despite the impressive performance of Retrieval-augmented Generation (RAG) systems across various NLP benchmarks, their robustness in handling real-world user-LLM interaction queries remains largely underexplored. This presents a critical gap for practical deployment, where user queries exhibit greater linguistic variations and can trigger cascading errors across interdependent RAG components. In this work, we systematically analyze how varying four linguistic dimensions (formality, readability, politeness, and grammatical correctness) impact RAG performance. We evaluate two retrieval models and nine LLMs, ranging from 3 to 72 billion parameters, across four information-seeking Question Answering (QA) datasets. Our results reveal that linguistic reformulations significantly impact both retrieval and generation stages, leading to a relative performance drop of up to 40.41% in Recall@5 scores for less formal queries and 38.86% in answer match scores for queries containing grammatical errors. Notably, RAG systems exhibit greater sensitivity to such variations compared to LLM-only generations, highlighting their vulnerability to error propagation due to linguistic shifts. These findings highlight the need for improved robustness techniques to enhance reliability in diverse user interactions."}
{"title": "UltraRAG: A Modular and Automated Toolkit for Adaptive Retrieval-Augmented Generation", "url": "https://arxiv.org/abs/2504.08761", "authors": ["Yuxuan Chen", "Dewen Guo", "Sen Mei", "Xinze Li", "Hao Chen", "Yishan Li", "Yixuan Wang", "Chaoyue Tang", "Ruobing Wang", "Dingjun Wu", "Yukun Yan", "Zhenghao Liu", "Shi Yu", "Zhiyuan Liu", "Maosong Sun"], "created": "2025-04-15", "abstract": "Retrieval-Augmented Generation (RAG) significantly enhances the performance of large language models (LLMs) in downstream tasks by integrating external knowledge. To facilitate researchers in deploying RAG systems, various RAG toolkits have been introduced. However, many existing RAG toolkits lack support for knowledge adaptation tailored to specific application scenarios. To address this limitation, we propose UltraRAG, a RAG toolkit that automates knowledge adaptation throughout the entire workflow, from data construction and training to evaluation, while ensuring ease of use. UltraRAG features a user-friendly WebUI that streamlines the RAG process, allowing users to build and optimize systems without coding expertise. It supports multimodal input and provides comprehensive tools for managing the knowledge base. With its highly modular architecture, UltraRAG delivers an end-to-end development solution, enabling seamless knowledge adaptation across diverse user scenarios. The code, demonstration videos, and installable package for UltraRAG are publicly available at https://github.com/OpenBMB/UltraRAG."}
{"title": "A Survey of Personalization: From RAG to Agent", "url": "https://arxiv.org/abs/2504.10147", "authors": ["Xiaopeng Li", "Pengyue Jia", "Derong Xu", "Yi Wen", "Yingyi Zhang", "Wenlin Zhang", "Wanyu Wang", "Yichao Wang", "Zhaocheng Du", "Xiangyang Li", "Yong Liu", "Huifeng Guo", "Ruiming Tang", "Xiangyu Zhao"], "created": "2025-04-15", "abstract": "Personalization has become an essential capability in modern AI systems, enabling customized interactions that align with individual user preferences, contexts, and goals. Recent research has increasingly concentrated on Retrieval-Augmented Generation (RAG) frameworks and their evolution into more advanced agent-based architectures within personalized settings to enhance user satisfaction. Building on this foundation, this survey systematically examines personalization across the three core stages of RAG: pre-retrieval, retrieval, and generation. Beyond RAG, we further extend its capabilities into the realm of Personalized LLM-based Agents, which enhance traditional RAG systems with agentic functionalities, including user understanding, personalized planning and execution, and dynamic generation. For both personalization in RAG and agent-based personalization, we provide formal definitions, conduct a comprehensive review of recent literature, and summarize key datasets and evaluation metrics. Additionally, we discuss fundamental challenges, limitations, and promising research directions in this evolving field. Relevant papers and resources are continuously updated at https://github.com/Applied-Machine-Learning-Lab/Awesome-Personalized-RAG-Agent."}
