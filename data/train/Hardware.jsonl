{"created": "2025-04-25", "title": "THEMIS: Time, Heterogeneity, and Energy Minded Scheduling for Fair Multi-Tenant Use in FPGAs", "abstract": "Using correct design metrics and understanding the limitations of the underlying technology is critical to developing effective scheduling algorithms. Unfortunately, existing scheduling techniques used \\emph{incorrect} metrics and had \\emph{unrealistic} assumptions for fair scheduling of multi-tenant FPGAs where each tenant is aimed to share approximately the same number of resources both spatially and temporally.", "authors": ["Emre Karabulut", "Arsalan Ali Malik", "Amro Awad", "Aydin Aysu"], "url": "https://arxiv.org/abs/2404.00507"}
{"abstract": "Design space exploration (DSE) enables architects to systematically evaluate various design options, guiding decisions on the most suitable configurations to meet specific objectives such as optimizing performance, power, and area. However, the growing complexity of modern CPUs has dramatically increased the number of micro-architectural parameters and expanded the overall design space, making DSE more challenging and time-consuming. Existing DSE frameworks struggle in large-scale design spaces due to inaccurate models and limited insights into parameter impact, hindering efficient identification of optimal micro-architectures within tight timeframes.", "authors": ["Runzhen Xue", "Hao Wu", "Mingyu Yan", "Ziheng Xiao", "Xiaochun Ye", "Dongrui Fan"], "created": "2025-04-11", "title": "Multi-objective Optimization in CPU Design Space Exploration: Attention is All You Need", "url": "https://arxiv.org/abs/2410.18368"}
{"created": "2025-05-01", "title": "Comparative Analysis of FPGA and GPU Performance for Machine Learning-Based Track Reconstruction at LHCb", "abstract": "In high-energy physics, the increasing luminosity and detector granularity at the Large Hadron Collider are driving the need for more efficient data processing solutions. Machine Learning has emerged as a promising tool for reconstructing charged particle tracks, due to its potentially linear computational scaling with detector hits. The recent implementation of a graph neural network-based track reconstruction pipeline in the first level trigger of the LHCb experiment on GPUs serves as a platform for comparative studies between computational architectures in the context of high-energy physics. This paper presents a novel comparison of the throughput of ML model inference between FPGAs and GPUs, focusing on the first step of the track reconstruction pipeline$\\unicode{x2013}$an implementation of a multilayer perceptron. Using HLS4ML for FPGA deployment, we benchmark its performance against the GPU implementation and demonstrate the potential of FPGAs for high-throughput, low-latency inference without the need for an expertise in FPGA development and while consuming significantly less power.", "authors": ["Fotis I. Giasemis", "Vladimir Lon\\v{c}ar", "Bertrand Granado", "Vladimir Vava Gligorov"], "url": "https://arxiv.org/abs/2502.02304"}
{"abstract": "Testing Electronic Design Automation (EDA) tools rely on benchmarks -- designs written in Hardware Description Languages (HDLs) such as Verilog, SystemVerilog, or VHDL. Although collections of benchmarks for these languages exist, they are typically limited in size. This scarcity has recently drawn more attention due to the increasing need for training large language models in this domain. To deal with such limitation, this paper presents a methodology and a corresponding tool for generating realistic Verilog designs. The tool, ChiGen, was originally developed to test the Jasper\\textregistered\\ Formal Verification Platform, a product by Cadence Design Systems. Now, released as open-source software, ChiGen has been able to identify zero-day bugs in a range of tools, including Verible, Verilator, and Yosys. This paper outlines the principles behind ChiGen's design, focusing on three aspects of it: (i) generation guided by probabilistic grammars, (ii) type inference via the Hindley-Milner algorithm, and (iii) code injection enabled by data-flow analysis. Once deployed on standard hardware, ChiGen outperforms existing Verilog Fuzzers such as Verismith, TransFuzz, and VlogHammer regarding structural diversity, code coverage, and bug-finding ability.", "authors": ["Jo\\~ao Victor Amorim Vieira", "Luiza de Melo Gomes", "Rafael Sumitani", "Raissa Maciel", "Augusto Mafra", "Mirlaine Crepalde", "Fernando Magno Quint\\~ao Pereira"], "created": "2025-04-11", "title": "Bottom-Up Generation of Verilog Designs for Testing EDA Tools", "url": "https://arxiv.org/abs/2504.06295"}
{"abstract": "Application performance of modern day processors is often limited by the memory subsystem rather than actual compute capabilities. Therefore, data throughput specifications play a key role in modeling application performance and determining possible bottlenecks. However, while peak instruction throughputs and bandwidths for local caches are often documented, the achievable throughput can also depend on the relation between memory access and compute instructions. In this paper, we present an Arm version of the well established x86-membench throughput benchmark, which we have adapted to support all current SIMD extensions of the Armv8 instruction set architecture. We describe aspects of the Armv8 ISA that need to be considered in the portable design of this benchmark. We use the benchmark to analyze the memory subsystem at a fine spatial granularity and to unveil microarchitectural details of three processors: Fujitsu A64FX, Ampere Altra and Cavium ThunderX2. Based on the resulting performance information, we show that instruction fetch and decoder widths become a potential bottleneck for cache-bandwidth-sensitive workloads due to the load-store concept of the Arm ISA.", "authors": ["Cyrill Burth", "Markus Velten", "Robert Sch\\\"one"], "created": "2025-04-11", "title": "Introducing the Arm-membench Throughput Benchmark", "url": "https://arxiv.org/abs/2504.06813"}
{"created": "2025-04-15", "title": "Leveraging Application-Specific Knowledge for Energy-Efficient Deep Learning Accelerators on Resource-Constrained FPGAs", "abstract": "The growing adoption of Deep Learning (DL) applications in the Internet of Things has increased the demand for energy-efficient accelerators. Field Programmable Gate Arrays (FPGAs) offer a promising platform for such acceleration due to their flexibility and power efficiency. However, deploying DL models on resource-constrained FPGAs remains challenging because of limited resources, workload variability, and the need for energy-efficient operation. This paper presents a framework for generating energy-efficient DL accelerators on resource-constrained FPGAs. The framework systematically explores design configurations to enhance energy efficiency while meeting requirements for resource utilization and inference performance in diverse application scenarios. The contributions of this work include: (1) analyzing challenges in achieving energy efficiency on resource-constrained FPGAs; (2) proposing a methodology for designing DL accelerators with integrated Register Transfer Level (RTL) optimizations, workload-aware strategies, and application-specific knowledge; and (3) conducting a literature review to identify gaps and demonstrate the necessity of this work.", "authors": ["Chao Qian"], "url": "https://arxiv.org/abs/2504.09151"}
{"created": "2025-04-15", "title": "HBS -- Hardware Build System: A Tcl-based, minimal common abstraction approach for build system for hardware designs", "abstract": "Build systems become an indispensable part of the software implementation and deployment process. New programming languages are released with the build system integrated into the language tools, for example, Go, Rust, or Zig. However, in the hardware description domain, no official build systems have been released with the predominant Hardware Description Languages (HDL) such as VHDL or SystemVerilog. Moreover, hardware design projects are often multilanguage.", "authors": ["Micha{\\l} Kruszewski"], "url": "https://arxiv.org/abs/2504.09642"}
{"created": "2025-04-15", "title": "Towards Low-Latency Event-based Obstacle Avoidance on a FPGA-Drone", "abstract": "This work quantitatively evaluates the performance of event-based vision systems (EVS) against conventional RGB-based models for action prediction in collision avoidance on an FPGA accelerator. Our experiments demonstrate that the EVS model achieves a significantly higher effective frame rate (1 kHz) and lower temporal (-20 ms) and spatial prediction errors (-20 mm) compared to the RGB-based model, particularly when tested on out-of-distribution data. The EVS model also exhibits superior robustness in selecting optimal evasion maneuvers. In particular, in distinguishing between movement and stationary states, it achieves a 59 percentage point advantage in precision (78% vs. 19%) and a substantially higher F1 score (0.73 vs. 0.06), highlighting the susceptibility of the RGB model to overfitting. Further analysis in different combinations of spatial classes confirms the consistent performance of the EVS model in both test data sets. Finally, we evaluated the system end-to-end and achieved a latency of approximately 2.14 ms, with event aggregation (1 ms) and inference on the processing unit (0.94 ms) accounting for the largest components. These results underscore the advantages of event-based vision for real-time collision avoidance and demonstrate its potential for deployment in resource-constrained environments.", "authors": ["Pietro Bonazzi", "Christian Vogt", "Michael Jost", "Lyes Khacef", "Federico Paredes-Vall\\'es", "Michele Magno"], "url": "https://arxiv.org/abs/2504.10400"}
{"created": "2025-04-15", "title": "FPGA-Optimized Hardware Accelerator for Fast Fourier Transform and Singular Value Decomposition in AI", "abstract": "This research introduces an FPGA-based hardware accelerator to optimize the Singular Value Decomposition (SVD) and Fast Fourier transform (FFT) operations in AI models. The proposed design aims to improve processing speed and reduce computational latency. Through experiments, we validate the performance benefits of the hardware accelerator and show how well it handles FFT and SVD operations. With its strong security and durability, the accelerator design achieves significant speedups over software implementations, thanks to its modules for data flow control, watermark embedding, FFT, and SVD.", "authors": ["Hong Ding", "Chia Chao Kang", "SuYang Xi", "Zehang Liu", "Xuan Zhang", "Yi Ding"], "url": "https://arxiv.org/abs/2504.10411"}
{"created": "2025-04-21", "title": "Implementation of Field Programmable Gate Arrays (FPGAs) in Extremely Cold Environments for Space and Cryogenic Computing Applications", "abstract": "The operation of CMOS Field Programmable Gate Arrays (FPGAs) at extremely cold environments as low as 4 K is demonstrated. Various FPGA and periphery hardware design techniques spanning from HDL design to improvements of peripheral circuitry such as discrete voltage regulators are displayed, and their respective performances are reported. While general operating conditions for voltage regulators are widened, FPGAs see a broader temperature range with improved jitter performance, reduced LUT delays, and enhanced transceiver performance at extremely low temperatures.", "authors": ["Christopher Lewis", "Drew Sellers", "Michael Hamilton"], "url": "https://arxiv.org/abs/2504.13305"}
{"title": "EXAM: Exploiting Exclusive System-Level Cache in Apple M-Series SoCs for Enhanced Cache Occupancy Attacks", "url": "https://arxiv.org/abs/2504.13385", "authors": ["Tianhong Xu", "Aidong Adam Ding", "Yunsi Fei"], "created": "2025-04-21", "abstract": "Cache occupancy attacks exploit the shared nature of cache hierarchies to infer a victim's activities by monitoring overall cache usage, unlike access-driven cache attacks that focus on specific cache lines or sets. There exists some prior work that target the last-level cache (LLC) of Intel processors, which is inclusive of higher-level caches, and L2 caches of ARM systems. In this paper, we target the System-Level Cache (SLC) of Apple M-series SoCs, which is exclusive to higher-level CPU caches. We address the challenges of the exclusiveness and propose a suite of SLC-cache occupancy attacks, the first of its kind, where an adversary can monitor GPU and other CPU cluster activities from their own CPU cluster. We first discover the structure of SLC in Apple M1 SOC and various policies pertaining to access and sharing through reverse engineering. We propose two attacks against websites. One is a coarse-grained fingerprinting attack, recognizing which website is accessed based on their different GPU memory access patterns monitored through the SLC occupancy channel. The other attack is a fine-grained pixel stealing attack, which precisely monitors the GPU memory usage for rendering different pixels, through the SLC occupancy channel. Third, we introduce a novel screen capturing attack which works beyond webpages, with the monitoring granularity of 57 rows of pixels (there are 1600 rows for the screen). This significantly expands the attack surface, allowing the adversary to retrieve any screen display, posing a substantial new threat to system security. Our findings reveal critical vulnerabilities in Apple's M-series SoCs and emphasize the urgent need for effective countermeasures against cache occupancy attacks in heterogeneous computing environments."}
{"created": "2025-04-28", "title": "Memory-efficient Sketch Acceleration for Handling Large Network Flows on FPGAs", "abstract": "Sketch-based algorithms for network traffic monitoring have drawn increasing interest in recent years due to their sub-linear memory efficiency and high accuracy. As the volume of network traffic grows, software-based sketch implementations cannot match the throughput of the incoming network flows. FPGA-based hardware sketch has shown better performance compared to software running on a CPU when handling these packets. Among the various sketch algorithms, Count-min sketch is one of the most popular and efficient. However, due to the limited amount of on-chip memory, the FPGA-based count-Min sketch accelerator suffers from performance drops as network traffic grows. In this work, we propose a hardware-friendly architecture with a variable width memory counter for count-min sketch. Our architecture provides a more compact design to store the sketch data structure effectively, allowing us to support larger hash tables and reduce overestimation errors. The design makes use of a P4-based programmable data plane and the AMD OpenNIC shell. The design is implemented and verified on the Open Cloud Testbed running on AMD Alveo U280s and can keep up with the 100 Gbit link speed.", "authors": ["Zhaoyang Han", "Yicheng Qian", "Michael Zink", "Miriam Leeser"], "url": "https://arxiv.org/abs/2504.16896"}
{"created": "2025-04-29", "title": "NSFlow: An End-to-End FPGA Framework with Scalable Dataflow Architecture for Neuro-Symbolic AI", "abstract": "Neuro-Symbolic AI (NSAI) is an emerging paradigm that integrates neural networks with symbolic reasoning to enhance the transparency, reasoning capabilities, and data efficiency of AI systems. Recent NSAI systems have gained traction due to their exceptional performance in reasoning tasks and human-AI collaborative scenarios. Despite these algorithmic advancements, executing NSAI tasks on existing hardware (e.g., CPUs, GPUs, TPUs) remains challenging, due to their heterogeneous computing kernels, high memory intensity, and unique memory access patterns. Moreover, current NSAI algorithms exhibit significant variation in operation types and scales, making them incompatible with existing ML accelerators. These challenges highlight the need for a versatile and flexible acceleration framework tailored to NSAI workloads. In this paper, we propose NSFlow, an FPGA-based acceleration framework designed to achieve high efficiency, scalability, and versatility across NSAI systems. NSFlow features a design architecture generator that identifies workload data dependencies and creates optimized dataflow architectures, as well as a reconfigurable array with flexible compute units, re-organizable memory, and mixed-precision capabilities. Evaluating across NSAI workloads, NSFlow achieves 31x speedup over Jetson TX2, more than 2x over GPU, 8x speedup over TPU-like systolic array, and more than 3x over Xilinx DPU. NSFlow also demonstrates enhanced scalability, with only 4x runtime increase when symbolic workloads scale by 150x. To the best of our knowledge, NSFlow is the first framework to enable real-time generalizable NSAI algorithms acceleration, demonstrating a promising solution for next-generation cognitive systems.", "authors": ["Hanchen Yang", "Zishen Wan", "Ritik Raj", "Joongun Park", "Ziwei Li", "Ananda Samajdar", "Arijit Raychowdhury", "Tushar Krishna"], "url": "https://arxiv.org/abs/2504.19323"}
{"created": "2025-04-29", "title": "Hardware/Software Co-Design of RISC-V Extensions for Accelerating Sparse DNNs on FPGAs", "abstract": "The customizability of RISC-V makes it an attractive choice for accelerating deep neural networks (DNNs). It can be achieved through instruction set extensions and corresponding custom functional units. Yet, efficiently exploiting these opportunities requires a hardware/software co-design approach in which the DNN model, software, and hardware are designed together. In this paper, we propose novel RISC-V extensions for accelerating DNN models containing semi-structured and unstructured sparsity. While the idea of accelerating structured and unstructured pruning is not new, our novel design offers various advantages over other designs. To exploit semi-structured sparsity, we take advantage of the fine-grained (bit-level) configurability of FPGAs and suggest reserving a few bits in a block of DNN weights to encode the information about sparsity in the succeeding blocks. The proposed custom functional unit utilizes this information to skip computations. To exploit unstructured sparsity, we propose a variable cycle sequential multiply-and-accumulate unit that performs only as many multiplications as the non-zero weights. Our implementation of unstructured and semi-structured pruning accelerators can provide speedups of up to a factor of 3 and 4, respectively. We then propose a combined design that can accelerate both types of sparsities, providing speedups of up to a factor of 5. Our designs consume a small amount of additional FPGA resources such that the resulting co-designs enable the acceleration of DNNs even on small FPGAs. We benchmark our designs on standard TinyML applications such as keyword spotting, image classification, and person detection.", "authors": ["Muhammad Sabih", "Abrarul Karim", "Jakob Wittmann", "Frank Hannig", "J\\\"urgen Teich"], "url": "https://arxiv.org/abs/2504.19659"}
{"created": "2025-04-29", "title": "Dynamic Tsetlin Machine Accelerators for On-Chip Training at the Edge using FPGAs", "abstract": "The increased demand for data privacy and security in machine learning (ML) applications has put impetus on effective edge training on Internet-of-Things (IoT) nodes. Edge training aims to leverage speed, energy efficiency and adaptability within the resource constraints of the nodes. Deploying and training Deep Neural Networks (DNNs)-based models at the edge, although accurate, posit significant challenges from the back-propagation algorithm's complexity, bit precision trade-offs, and heterogeneity of DNN layers. This paper presents a Dynamic Tsetlin Machine (DTM) training accelerator as an alternative to DNN implementations. DTM utilizes logic-based on-chip inference with finite-state automata-driven learning within the same Field Programmable Gate Array (FPGA) package. Underpinned on the Vanilla and Coalesced Tsetlin Machine algorithms, the dynamic aspect of the accelerator design allows for a run-time reconfiguration targeting different datasets, model architectures, and model sizes without resynthesis. This makes the DTM suitable for targeting multivariate sensor-based edge tasks. Compared to DNNs, DTM trains with fewer multiply-accumulates, devoid of derivative computation. It is a data-centric ML algorithm that learns by aligning Tsetlin automata with input data to form logical propositions enabling efficient Look-up-Table (LUT) mapping and frugal Block RAM usage in FPGA training implementations. The proposed accelerator offers 2.54x more Giga operations per second per Watt (GOP/s per W) and uses 6x less power than the next-best comparable design.", "authors": ["Gang Mao", "Tousif Rahman", "Sidharth Maheshwari", "Bob Pattison", "Zhuang Shao", "Rishad Shafik", "Alex Yakovlev"], "url": "https://arxiv.org/abs/2504.19797"}
{"created": "2025-05-01", "title": "Coyote v2: Raising the Level of Abstraction for Data Center FPGAs", "abstract": "In the trend towards hardware specialization, FPGAs play a dual role as accelerators for offloading, e.g., network virtualization, and as a vehicle for prototyping and exploring hardware designs. While FPGAs offer versatility and performance, integrating them in larger systems remains challenging. Thus, recent efforts have focused on raising the level of abstraction through better interfaces and high-level programming languages. Yet, there is still quite some room for improvement. In this paper, we present Coyote v2, an open source FPGA shell built with a novel, three-layer hierarchical design supporting dynamic partial reconfiguration of services and user logic, with a unified logic interface, and high-level software abstractions such as support for multithreading and multitenancy. Experimental results indicate Coyote v2 reduces synthesis times between 15% and 20% and run-time reconfiguration times by an order of magnitude, when compared to existing systems. We also demonstrate the advantages of Coyote v2 by deploying several realistic applications, including HyperLogLog cardinality estimation, AES encryption, and neural network inference. Finally, Coyote v2 places a great deal of emphasis on integration with real systems through reusable and reconfigurable services, including a fully RoCE v2-compliant networking stack, a shared virtual memory model with the host, and a DMA engine between FPGAs and GPUs. We demonstrate these features by, e.g., seamlessly deploying an FPGA-accelerated neural network from Python.", "authors": ["Benjamin Ramhorst", "Dario Korolija", "Maximilian Jakob Heer", "Jonas Dann", "Luhao Liu", "Gustavo Alonso"], "url": "https://arxiv.org/abs/2504.21538"}
