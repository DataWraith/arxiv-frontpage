{"title": "WildlifeReID-10k: Wildlife re-identification dataset with 10k individual animals", "url": "https://arxiv.org/abs/2406.09211", "authors": ["Luk\\'a\\v{s} Adam", "Vojt\\v{e}ch \\v{C}erm\\'ak", "Kostas Papafitsoros", "Lukas Picek"], "created": "2025-04-16", "abstract": "This paper introduces WildlifeReID-10k, a new large-scale re-identification benchmark with more than 10k animal identities of around 33 species across more than 140k images, re-sampled from 37 existing datasets. WildlifeReID-10k covers diverse animal species and poses significant challenges for SoTA methods, ensuring fair and robust evaluation through its time-aware and similarity-aware split protocol. The latter is designed to address the common issue of training-to-test data leakage caused by visually similar images appearing in both training and test sets. The WildlifeReID-10k dataset and benchmark are publicly available on Kaggle, along with strong baselines for both closed-set and open-set evaluation, enabling fair, transparent, and standardized evaluation of not just multi-species animal re-identification models."}
{"abstract": "Sociality borne by language, as is the predominant digital trace on text-based social media platforms, harbours the raw material for exploring multiple social phenomena. Distinctively, the messaging service Telegram provides functionalities that allow for socially interactive as well as one-to-many communication. Our Telegram dataset contains over 6,000 groups and channels, 40 million text messages, and over 3 million transcribed audio files, originating from a data-hoarding initiative named the ``Schwurbelarchiv'' (from German schwurbeln: speaking nonsense). This dataset publication details the structure, scope, and methodological specifics of the Schwurbelarchiv, emphasising its relevance for further research on the German-language conspiracy theory discourse. We validate its predominantly German origin by linguistic and temporal markers and situate it within the context of similar datasets. We describe process and extent of the transcription of multimedia files. Thanks to this effort the dataset uniquely supports multimodal analysis of online social dynamics and content dissemination. Researchers can employ this resource to explore societal dynamics in misinformation, political extremism, opinion adaptation, and social network structures on Telegram. The Schwurbelarchiv thus offers unprecedented opportunities for investigations into digital communication and its societal implications.", "authors": ["Mathias Angermaier", "Joao Pinheiro-Neto", "Elisabeth Hoeldrich", "Jana Lasser"], "created": "2025-04-11", "title": "The Schwurbelarchiv: a German Language Telegram dataset for the Study of Conspiracy Theories", "url": "https://arxiv.org/abs/2504.06318"}
{"created": "2025-04-11", "title": "A Digital Twin of an Electrical Distribution Grid: SoCal 28-Bus Dataset", "abstract": "We provide an open-access dataset of phasor & waveform measurement units (PMUs/WMUs) of a real-world electrical distribution network. The network consists of diverse sets of generation resources (including solar panels, fuel cells, natural gas generators, and utility interconnections), loads (including large-scale electric vehicle charging, data centers, central cooling, offices), topology changes (such as line outages and load transfers), as well as a mixture of single- and three-phase networks. We describe a densely deployed PMU sensor network in a distribution grid, in which all buses with non-zero power injections are measured. This approach enables a range of applications such as state estimation, system identification, power flow optimization, and feedback control, several of which are discussed in this paper. Additionally, we provide a synchronized waveform dataset which allows the analysis of harmonics, transient events, dynamic grid impedance, and stability. Data collection started in 2023 while new data is generated continuously and made available online. A characterization of measurement error is provided. Finally, we provide circuit topology and parameters as a part of the dataset. Together, the circuit and timeseries data offer an opportunity for researchers to develop and test algorithms on a real-world system.", "authors": ["Yiheng Xie", "Lucien Werner", "Kaibo Chen", "Thuy-Linh Le", "Christine Ortega", "Steven Low"], "url": "https://arxiv.org/abs/2504.06588"}
{"abstract": "Large Language Models (LLMs) and Large Multimodal Models (LMMs) demonstrate impressive problem-solving skills in many tasks and domains. However, their ability to reason with complex images in academic domains has not been systematically investigated. To bridge this gap, we present SCI-Reason, a dataset for complex multimodel reasoning in academic areas. SCI-Reason aims to test and improve the reasoning ability of large multimodal models using real complex images in academic domains. The dataset contains 12,066 images and 12,626 question-answer pairs extracted from PubMed, divided into training, validation and test splits. Each question-answer pair also contains an accurate and efficient inference chain as a guide to improving the inference properties of the dataset. With SCI-Reason, we performed a comprehensive evaluation of 8 well-known models. The best performing model, Claude-3.7-Sonnet, only achieved an accuracy of 55.19%. Error analysis shows that more than half of the model failures are due to breakdowns in multi-step inference chains rather than errors in primary visual feature extraction. This finding underscores the inherent limitations in reasoning capabilities exhibited by current multimodal models when processing complex image analysis tasks within authentic academic contexts. Experiments on open-source models show that SCI-Reason not only enhances reasoning ability but also demonstrates cross-domain generalization in VQA tasks. We also explore future applications of model inference capabilities in this domain, highlighting its potential for future research.", "authors": ["Chenghao Ma", "Haihong E.", "Junpeng Ding", "Jun Zhang", "Ziyan Ma", "Huang Qing", "Bofei Gao", "Liang Chen", "Meina Song"], "created": "2025-04-11", "title": "SCI-Reason: A Dataset with Chain-of-Thought Rationales for Complex Multimodal Reasoning in Academic Areas", "url": "https://arxiv.org/abs/2504.06637"}
{"created": "2025-04-11", "title": "A Dataset of Software Bill of Materials for Evaluating SBOM Consumption Tools", "abstract": "A Software Bill of Materials (SBOM) is becoming an essential tool for effective software dependency management. An SBOM is a list of components used in software, including details such as component names, versions, and licenses. Using SBOMs, developers can quickly identify software components and assess whether their software depends on vulnerable libraries. Numerous tools support software dependency management through SBOMs, which can be broadly categorized into two types: tools that generate SBOMs and tools that utilize SBOMs. A substantial collection of accurate SBOMs is required to evaluate tools that utilize SBOMs. However, there is no publicly available dataset specifically designed for this purpose, and research on SBOM consumption tools remains limited. In this paper, we present a dataset of SBOMs to address this gap. The dataset we constructed comprises 46 SBOMs generated from real-world Java projects, with plans to expand it to include a broader range of projects across various programming languages. Accurate and well-structured SBOMs enable researchers to evaluate the functionality of SBOM consumption tools and identify potential issues. We collected 3,271 Java projects from GitHub and generated SBOMs for 798 of them using Maven with an open-source SBOM generation tool. These SBOMs were refined through both automatic and manual corrections to ensure accuracy, currently resulting in 46 SBOMs that comply with the SPDX Lite profile, which defines minimal requirements tailored to practical workflows in industries. This process also revealed issues with the SBOM generation tools themselves. The dataset is publicly available on Zenodo (DOI: 10.5281/zenodo.14233415).", "authors": ["Rio Kishimoto", "Tetsuya Kanda", "Yuki Manabe", "Katsuro Inoue", "Shi Qiu", "Yoshiki Higo"], "url": "https://arxiv.org/abs/2504.06880"}
{"created": "2025-04-11", "title": "S-EO: A Large-Scale Dataset for Geometry-Aware Shadow Detection in Remote Sensing Applications", "abstract": "We introduce the S-EO dataset: a large-scale, high-resolution dataset, designed to advance geometry-aware shadow detection. Collected from diverse public-domain sources, including challenge datasets and government providers such as USGS, our dataset comprises 702 georeferenced tiles across the USA, each covering 500x500 m. Each tile includes multi-date, multi-angle WorldView-3 pansharpened RGB images, panchromatic images, and a ground-truth DSM of the area obtained from LiDAR scans. For each image, we provide a shadow mask derived from geometry and sun position, a vegetation mask based on the NDVI index, and a bundle-adjusted RPC model. With approximately 20,000 images, the S-EO dataset establishes a new public resource for shadow detection in remote sensing imagery and its applications to 3D reconstruction. To demonstrate the dataset's impact, we train and evaluate a shadow detector, showcasing its ability to generalize, even to aerial images. Finally, we extend EO-NeRF - a state-of-the-art NeRF approach for satellite imagery - to leverage our shadow predictions for improved 3D reconstructions.", "authors": ["Masquil El\\'ias", "Mar\\'i Roger", "Ehret Thibaud", "Meinhardt-Llopis Enric", "Mus\\'e Pablo", "Facciolo Gabriele"], "url": "https://arxiv.org/abs/2504.06920"}
{"created": "2025-04-14", "title": "A Vulnerability Code Intent Summary Dataset", "abstract": "In the era of Large Language Models (LLMs), the code summarization technique boosts a lot, along with the emergence of many new significant works. However, the potential of code summarization in the Computer Security Area still remains explored. Can we generate a code summary of a code snippet for its security intention? Thus, this work proposes an innovative large-scale multi-perspective Code Intent Summary Dataset named BADS , aiming to increase the understanding of a given code snippet and reduce the risk in the code developing process. The procedure of establishing a dataset can be divided into four steps: First, we collect samples of codes with known vulnerabilities as well as code generated by AI from multiple sources. Second, we do the data clean and format unification, then do the data combination. Third, we utilize the LLM to automatically Annotate the code snippet. Last, We do the human evaluation to double-check. The dataset contains X code examples which cover Y categories of vulnerability. Our data are from Z open-source projects and CVE entries, and compared to existing work, our dataset not only contains original code but also code function summary and security intent summary, providing context information for research in code security analysis. All information is in CSV format. The contributions of this paper are four-fold: the establishment of a high-quality, multi-perspective Code Intent Summary Dataset; an innovative method in data collection and processing; A new multi-perspective code analysis framework that promotes cross-disciplinary research in the fields of software engineering and cybersecurity; improving the practicality and scalability of the research outcomes by considering the code length limitations in real-world applications. Our dataset and related tools have been publicly released on GitHub.", "authors": ["Yifan Huang", "Weisong Sun", "Yubin Qu"], "url": "https://arxiv.org/abs/2504.08180"}
{"created": "2025-04-14", "title": "RealCam-Vid: High-resolution Video Dataset with Dynamic Scenes and Metric-scale Camera Movements", "abstract": "Recent advances in camera-controllable video generation have been constrained by the reliance on static-scene datasets with relative-scale camera annotations, such as RealEstate10K. While these datasets enable basic viewpoint control, they fail to capture dynamic scene interactions and lack metric-scale geometric consistency-critical for synthesizing realistic object motions and precise camera trajectories in complex environments. To bridge this gap, we introduce the first fully open-source, high-resolution dynamic-scene dataset with metric-scale camera annotations in https://github.com/ZGCTroy/RealCam-Vid.", "authors": ["Guangcong Zheng", "Teng Li", "Xianpan Zhou", "Xi Li"], "url": "https://arxiv.org/abs/2504.08212"}
{"created": "2025-04-14", "title": "Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset", "abstract": "Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neural Architecture Search (NAS) method that directly focuses on identifying architectures that minimize energy consumption while maintaining acceptable accuracy. Unlike previous methods that primarily target vision and language tasks, the approach proposed here specifically addresses tabular datasets. Remarkably, the optimal architecture suggested by this method can reduce energy consumption by up to 92% compared to architectures recommended by conventional NAS.", "authors": ["Hoang-Loc La", "Phuong Hoai Ha"], "url": "https://arxiv.org/abs/2504.08359"}
{"created": "2025-04-14", "title": "Datasets for Lane Detection in Autonomous Driving: A Comprehensive Review", "abstract": "Accurate lane detection is essential for automated driving, enabling safe and reliable vehicle navigation in a variety of road scenarios. Numerous datasets have been introduced to support the development and evaluation of lane detection algorithms, each differing in terms of the amount of data, sensor types, annotation granularity, environmental conditions, and scenario diversity. This paper provides a comprehensive review of over 30 publicly available lane detection datasets, systematically analysing their characteristics, advantages and limitations. We classify these datasets based on key factors such as sensor resolution, annotation types and diversity of road and weather conditions. By identifying existing challenges and research gaps, we highlight opportunities for future dataset improvements that can further drive innovation in robust lane detection. This survey serves as a resource for researchers seeking appropriate datasets for lane detection, and contributes to the broader goal of advancing autonomous driving.", "authors": ["J\\\"org Gamerdinger", "Sven Teufel", "Oliver Bringmann"], "url": "https://arxiv.org/abs/2504.08540"}
{"created": "2025-04-14", "title": "Banana Ripeness Level Classification using a Simple CNN Model Trained with Real and Synthetic Datasets", "abstract": "The level of ripeness is essential in determining the quality of bananas. To correctly estimate banana maturity, the metrics of international marketing standards need to be considered. However, the process of assessing the maturity of bananas at an industrial level is still carried out using manual methods. The use of CNN models is an attractive tool to solve the problem, but there is a limitation regarding the availability of sufficient data to train these models reliably. On the other hand, in the state-of-the-art, existing CNN models and the available data have reported that the accuracy results are acceptable in identifying banana maturity. For this reason, this work presents the generation of a robust dataset that combines real and synthetic data for different levels of banana ripeness. In addition, it proposes a simple CNN architecture, which is trained with synthetic data and using the transfer learning technique, the model is improved to classify real data, managing to determine the level of maturity of the banana. The proposed CNN model is evaluated with several architectures, then hyper-parameter configurations are varied, and optimizers are used. The results show that the proposed CNN model reaches a high accuracy of 0.917 and a fast execution time.", "authors": ["Luis Chuquimarca", "Boris Vintimilla", "Sergio Velastin"], "url": "https://arxiv.org/abs/2504.08568"}
{"created": "2025-04-14", "title": "A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English", "abstract": "The dissemination of online hate speech can have serious negative consequences for individuals, online communities, and entire societies. This and the large volume of hateful online content prompted both practitioners', i.e., in content moderation or law enforcement, and researchers' interest in machine learning models to automatically classify instances of hate speech. Whereas most scientific works address hate speech classification as a binary task, practice often requires a differentiation into sub-types, e.g., according to target, severity, or legality, which may overlap for individual content. Hence, researchers created datasets and machine learning models that approach hate speech classification in textual data as a multi-label problem. This work presents the first systematic and comprehensive survey of scientific literature on this emerging research landscape in English (N=46). We contribute with a concise overview of 28 datasets suited for training multi-label classification models that reveals significant heterogeneity regarding label-set, size, meta-concept, annotation process, and inter-annotator agreement. Our analysis of 24 publications proposing suitable classification models further establishes inconsistency in evaluation and a preference for architectures based on Bidirectional Encoder Representation from Transformers (BERT) and Recurrent Neural Networks (RNNs). We identify imbalanced training data, reliance on crowdsourcing platforms, small and sparse datasets, and missing methodological alignment as critical open issues and formulate ten recommendations for research.", "authors": ["Julian B\\\"aumler", "Louis Bl\\\"ocher", "Lars-Joel Frey", "Xian Chen", "Markus Bayer", "Christian Reuter"], "url": "https://arxiv.org/abs/2504.08609"}
