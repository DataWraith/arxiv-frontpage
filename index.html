<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Frontpage</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .paper {
            margin-bottom: 30px;
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .paper-abstract {
            margin-bottom: 10px;
        }
        .abstract-short {
            display: inline;
        }
        .abstract-full {
            display: none;
        }
        .more-link {
            color: blue;
            cursor: pointer;
            text-decoration: underline;
        }
        .tag-badge {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 5px;
            margin-bottom: 5px;
            border-radius: 3px;
            font-size: 0.8em;
            color: white;
        }
        .tag-badge.high-confidence {
            opacity: 1;
        }
        .tag-badge.low-confidence {
            opacity: 0.6;
            display: none;
        }
        .interestingness-score {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 10px;
            color: white;
            border-radius: 3px;
            font-weight: bold;
        }
        .interestingness-positive {
            background-color: #4CAF50;
        }
        .interestingness-negative {
            background-color: #f44336;
        }
        .interestingness-neutral {
            background-color: #9e9e9e;
        }
        .last-updated {
            text-align: right;
            color: #666;
            font-size: 0.9em;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .intro {
            text-align: center;
            max-width: 60em;
            margin: 0 auto;
            color: #888;
        }
        .copy-icon {
            display: inline-block;
            width: 16px;
            height: 16px;
            cursor: pointer;
            margin-left: 5px;
            opacity: 0.5;
        }
        .copy-icon:hover {
            opacity: 1;
        }
        .json-popup {
            display: none;
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            max-width: 500px;
            max-height: 300px;
            overflow: auto;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        a {
            color: inherit;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1 {
            text-align: center;
        }
        h1 a {
            text-decoration: underline;
        }
        .date-section {
            margin-bottom: 40px;
        }
        .date-header {
            color: #666;
            font-size: 1.5em;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 2px solid #eee;
        }
    </style>
</head>
<body>
    <h1>
        <a href="https://github.com/DataWraith/arxiv-frontpage">DataWraith's</a> ArXiv Frontpage
    </h1>

    <div class="last-updated">
        Last updated: 2025-05-13
    </div>

    <p class="intro">
        This frontpage is made by scraping ArXiv's computer science RSS feed and tagging papers with a classifier.
    </p>

    <p class="intro">
        Each tag is weighted according to my preferences to compute a paper's <i>interestingness</i> score.
    </p>
    
    
    <div class="date-section">
        <h2 class="date-header">2025-05-13</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    1.186
                </span>
                <a href="https://arxiv.org/abs/2312.16928" target="_blank" rel="noopener noreferrer">Error Estimates for Systems of Nonlocal Balance Laws Modeling Dense Multilane Vehicular Traffic</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Aekta Aggarwal, Helge Holden, Ganesh Vaidya
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We discuss a class of coupled systems of nonlocal nonlinear balance laws modeling multilane traffic, with the nonlocality present in both convective and source terms. The uniqueness and existence of the entropy solution are proven via doubling of the variables arguments and convergent finite volume </span>
                
                <span class="abstract-full" style="display: none;">We discuss a class of coupled systems of nonlocal nonlinear balance laws modeling multilane traffic, with the nonlocality present in both convective and source terms. The uniqueness and existence of the entropy solution are proven via doubling of the variables arguments and convergent finite volume approximations, respectively. The primary goal is to establish that the finite volume numerical approximations of the system converge to the unique entropy solution at a rate of $\sqrt{\Delta t}$, even when using relatively less regular one-sided kernels, compared to the globally smooth kernels analyzed in [Num. Math., 156(1):237-271, 2024] and [IMA J. Numer. Anal., 44(6):3354-3392, 2024].The applicability of the proven theory to a general class of systems of nonlocal balance laws coupled strongly through the convective part and weakly through the source part, is indicated. As the support of the kernel tends to zero, the convergence of the entropy solutions of the proposed model to its local counterparts [SIAM J. Math. Anal., 51: 3694-3713, 2019] is also discussed. Numerical simulations illustrating the behavior of the entropy solutions of the coupled nonlocal systems are also shown.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 7.1 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- Math: 4.2 -->
                    
                <!-- Networks: 3.6 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    1.0571
                </span>
                <a href="https://arxiv.org/abs/2502.09517" target="_blank" rel="noopener noreferrer">Coupled Rendezvous and Docking Maneuver control of satellite using Reinforcement learning-based Adaptive Fixed-Time Sliding Mode Controller</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rakesh Kumar Sahoo, Manoranjan Sinha
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Satellite dynamics in unknown environments are inherently uncertain due to factors such as varying gravitational fields, atmospheric drag, and unpredictable interactions with space debris or other celestial bodies. Traditional sliding mode controllers with fixed parameters often struggle to maintain</span>
                
                <span class="abstract-full" style="display: none;">Satellite dynamics in unknown environments are inherently uncertain due to factors such as varying gravitational fields, atmospheric drag, and unpredictable interactions with space debris or other celestial bodies. Traditional sliding mode controllers with fixed parameters often struggle to maintain optimal performance under these fluctuating conditions. Therefore, an adaptive controller is essential to address these challenges by continuously tuning its gains in real-time. In this paper, we have tuned the slopes of the Fixed-time Sliding surface adaptively using reinforcement learning for coupled rendezvous and docking maneuver of chaser satellite with the target satellite in an unknown space environment. The neural network model is used to determine the optimal gains of reaching law of the fixed-time sliding surface. We have assumed that we don't have an accurate model of the system so we have added noise in the tangent space instead of directly on the manifold to preserve the geometric structure of the system while ensuring mathematically consistent uncertainty propagation. The reinforcement learning is used as an approximator to represent the value function of the agent to estimate the dynamical model of the system using the Actor-Critic method. The proposed control algorithm integrates a neural network and a sliding mode controller in a cascade loop architecture, where the tracking error dynamically tunes the sliding surface gains. Global fixed-time stability of the closed-loop feedback system is proved within the Lyapunov framework. This comprehensive approach of fixed-time sliding mode controller using a Reinforcement Learning based ensures the completion of the mission efficiently while addressing the critical challenges posed by the uncertain environment. The simulation results presented support the claims made.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 6.3 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- Robotics: 2.3 -->
                    
                <!-- LLMs: 2.0 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Pathfinding: 1.6 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    1.001
                </span>
                <a href="https://arxiv.org/abs/2505.07523" target="_blank" rel="noopener noreferrer">On rapid parallel tuning of controllers of a swarm of MAVs -- distribution strategies of the updated gains</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dariusz Horla, Wojciech Giernacki, V\'it Kr\'atk\'y, Petr \v{S}tibinger, Tom\'a\v{s} B\'a\v{c}a, Martin Saska
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we present a reliable, scalable, time deterministic, model-free procedure to tune swarms of Micro Aerial Vehicles (MAVs) using basic sensory data. Two approaches to taking advantage of parallel tuning are presented. First, the tuning with averaging of the results on the basis of perfo</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we present a reliable, scalable, time deterministic, model-free procedure to tune swarms of Micro Aerial Vehicles (MAVs) using basic sensory data. Two approaches to taking advantage of parallel tuning are presented. First, the tuning with averaging of the results on the basis of performance indices reported from the swarm with identical gains to decrease the negative effect of the noise in the measurements. Second, the tuning with parallel testing of varying set of gains across the swarm to reduce the tuning time. The presented methods were evaluated both in simulation and real-world experiments. The achieved results show the ability of the proposed approach to improve the results of the tuning while decreasing the tuning time, ensuring at the same time a reliable tuning mechanism.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 6.0 -->
                    
                <!-- LLMs: 4.2 -->
                    
                <!-- Networks: 4.0 -->
                    
                <!-- Math: 4.0 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Pathfinding: 2.0 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.9569
                </span>
                <a href="https://arxiv.org/abs/2410.13800" target="_blank" rel="noopener noreferrer">Discrete distributions are learnable from metastable samples</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Abhijith Jayakumar, Andrey Y. Lokhov, Sidhant Misra, Marc Vuffray
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Physically motivated stochastic dynamics are often used to sample from high-dimensional distributions. However such dynamics often get stuck in specific regions of their state space and mix very slowly to the desired stationary state. This causes such systems to approximately sample from a metastabl</span>
                
                <span class="abstract-full" style="display: none;">Physically motivated stochastic dynamics are often used to sample from high-dimensional distributions. However such dynamics often get stuck in specific regions of their state space and mix very slowly to the desired stationary state. This causes such systems to approximately sample from a metastable distribution which is usually quite different from the desired, stationary distribution of the dynamic. We rigorously show that, in the case of multi-variable discrete distributions, the true model describing the stationary distribution can be recovered from samples produced from a metastable distribution under minimal assumptions about the system. This follows from a fundamental observation that the single-variable conditionals of metastable distributions that satisfy a strong metastability condition are on average close to those of the stationary distribution. This holds even when the metastable distribution differs considerably from the true model in terms of global metrics like Kullback-Leibler divergence or total variation distance. This property allows us to learn the true model using a conditional likelihood based estimator, even when the samples come from a metastable distribution concentrated in a small region of the state space. Explicit examples of such metastable states can be constructed from regions that effectively bottleneck the probability flow and cause poor mixing of the Markov chain. For specific cases of binary pairwise undirected graphical models (i.e. Ising models), we extend our results to further rigorously show that data coming from metastable states can be used to learn the parameters of the energy function and recover the structure of the model.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.7 -->
                    
                <!-- Math: 4.1 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Medicine: 2.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.9185
                </span>
                <a href="https://arxiv.org/abs/2505.07182" target="_blank" rel="noopener noreferrer">Economic data-enabled predictive control using machine learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mingxue Yan, Xuewen Zhang, Kaixiang Zhang, Zhaojian Li, Xunyuan Yin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we propose a convex data-based economic predictive control method within the framework of data-enabled predictive control (DeePC). Specifically, we use a neural network to transform the system output into a new state space, where the nonlinear economic cost function of the underlying </span>
                
                <span class="abstract-full" style="display: none;">In this paper, we propose a convex data-based economic predictive control method within the framework of data-enabled predictive control (DeePC). Specifically, we use a neural network to transform the system output into a new state space, where the nonlinear economic cost function of the underlying nonlinear system is approximated using a quadratic function expressed by the transformed output in the new state space. Both the neural network parameters and the coefficients of the quadratic function are learned from open-loop data of the system. Additionally, we reconstruct constrained output variables from the transformed output through learning an output reconstruction matrix; this way, the proposed economic DeePC can handle output constraints explicitly. The performance of the proposed method is evaluated via a case study in a simulated chemical process.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 6.1 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- Networks: 3.9 -->
                    
                <!-- Math: 3.3 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.857
                </span>
                <a href="https://arxiv.org/abs/2505.06601" target="_blank" rel="noopener noreferrer">Learning Guarantee of Reward Modeling Using Deep Neural Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yuanhang Luo, Yeheng Ge, Ruijian Han, Guohao Shen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this work, we study the learning theory of reward modeling with pairwise comparison data using deep neural networks. We establish a novel non-asymptotic regret bound for deep reward estimators in a non-parametric setting, which depends explicitly on the network architecture. Furthermore, to under</span>
                
                <span class="abstract-full" style="display: none;">In this work, we study the learning theory of reward modeling with pairwise comparison data using deep neural networks. We establish a novel non-asymptotic regret bound for deep reward estimators in a non-parametric setting, which depends explicitly on the network architecture. Furthermore, to underscore the critical importance of clear human beliefs, we introduce a margin-type condition that assumes the conditional winning probability of the optimal action in pairwise comparisons is significantly distanced from 1/2. This condition enables a sharper regret bound, which substantiates the empirical efficiency of Reinforcement Learning from Human Feedback and highlights clear human beliefs in its success. Notably, this improvement stems from high-quality pairwise comparison data implied by the margin-type condition, is independent of the specific estimators used, and thus applies to various learning algorithms and models.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.1 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.6788
                </span>
                <a href="https://arxiv.org/abs/2505.07234" target="_blank" rel="noopener noreferrer">A Novel Online Pseudospectral Method for Approximation of Nonlinear Systems Dynamics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Arian Yousefian, Avimanyu Sahoo, Vignesh Narayanan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a novel online system identification approach utilizing the Chebyshev pseudospectral (PS) method to approximate the dynamics of a continuous-time nonlinear system. Unlike conventional periodic sampling, the proposed identification scheme employs aperiodic state sampling, leveragi</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a novel online system identification approach utilizing the Chebyshev pseudospectral (PS) method to approximate the dynamics of a continuous-time nonlinear system. Unlike conventional periodic sampling, the proposed identification scheme employs aperiodic state sampling, leveraging the Chebyshev nodes to achieve the desired approximation accuracy. Unlike traditional off-line PS approaches, the scheme utilizes a moving time-window strategy to compute the sampling instants (Chebyshev nodes) forward in time for state measurements. Within each time window, the number of sampling instants is adaptively determined to meet the specified approximation accuracy. The Chebyshev basis is also shifted for each time window to accommodate arbitrary approximation intervals. The least-squares approach is employed to estimate the coefficients of the shifted Chebyshev basis functions using the measured state and its derivatives at the end of each window, resulting in a piecewise approximation of the drift dynamics of the system. In the second step, the identified drift dynamics are utilized to design an adaptive state estimator to reconstruct the continuous system states from the aperiodic state measurements. To address the smoothness of the piecewise approximated system dynamics, the Chebyshev coefficients are recomputed at the window transition instants, between two consecutive time windows, by enforcing continuity of the approximated function and its derivatives. In addition, analytical results are provided to determine the number of sampling instants within each time window, which guarantees the desired approximation accuracy. The boundedness of function approximation and state estimation errors is also proved analytically. Finally, numerical simulation results are included to validate the proposed scheme.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 6.3 -->
                    
                <!-- Math: 5.5 -->
                    
                <!-- Networks: 4.8 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Pathfinding: 2.4 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Multi-armed Bandit: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.5007
                </span>
                <a href="https://arxiv.org/abs/2505.07124" target="_blank" rel="noopener noreferrer">Learning from Samples: Inverse Problems over measures via Sharpened Fenchel-Young Losses</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Francisco Andrade, Gabriel Peyr\'e, Clarice Poon
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Estimating parameters from samples of an optimal probability distribution is essential in applications ranging from socio-economic modeling to biological system analysis. In these settings, the probability distribution arises as the solution to an optimization problem that captures either static int</span>
                
                <span class="abstract-full" style="display: none;">Estimating parameters from samples of an optimal probability distribution is essential in applications ranging from socio-economic modeling to biological system analysis. In these settings, the probability distribution arises as the solution to an optimization problem that captures either static interactions among agents or the dynamic evolution of a system over time. Our approach relies on minimizing a new class of loss functions, called sharpened Fenchel-Young losses, which measure the sub-optimality gap of the optimization problem over the space of measures. We study the stability of this estimation method when only a finite number of sample is available. The parameters to be estimated typically correspond to a cost function in static problems and to a potential function in dynamic problems. To analyze stability, we introduce a general methodology that leverages the strong convexity of the loss function together with the sample complexity of the forward optimization problem. Our analysis emphasizes two specific settings in the context of optimal transport, where our method provides explicit stability guarantees: The first is inverse unbalanced optimal transport (iUOT) with entropic regularization, where the parameters to estimate are cost functions that govern transport computations; this method has applications such as link prediction in machine learning. The second is inverse gradient flow (iJKO), where the objective is to recover a potential function that drives the evolution of a probability distribution via the Jordan-Kinderlehrer-Otto (JKO) time-discretization scheme; this is particularly relevant for understanding cell population dynamics in single-cell genomics. Finally, we validate our approach through numerical experiments on Gaussian distributions, where closed-form solutions are available, to demonstrate the practical performance of our methods</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 5.1 -->
                    
                <!-- Reinforcement Learning: 5.0 -->
                    
                <!-- Math: 3.7 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Medicine: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Pathfinding: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06282" target="_blank" rel="noopener noreferrer">InfoNCE is a Free Lunch for Semantically guided Graph Contrastive Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zixu Wang, Bingbing Xu, Yige Yuan, Huawei Shen, Xueqi Cheng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As an important graph pre-training method, Graph Contrastive Learning (GCL) continues to play a crucial role in the ongoing surge of research on graph foundation models or LLM as enhancer for graphs. Traditional GCL optimizes InfoNCE by using augmentations to define self-supervised tasks, treating a</span>
                
                <span class="abstract-full" style="display: none;">As an important graph pre-training method, Graph Contrastive Learning (GCL) continues to play a crucial role in the ongoing surge of research on graph foundation models or LLM as enhancer for graphs. Traditional GCL optimizes InfoNCE by using augmentations to define self-supervised tasks, treating augmented pairs as positive samples and others as negative. However, this leads to semantically similar pairs being classified as negative, causing significant sampling bias and limiting performance. In this paper, we argue that GCL is essentially a Positive-Unlabeled (PU) learning problem, where the definition of self-supervised tasks should be semantically guided, i.e., augmented samples with similar semantics are considered positive, while others, with unknown semantics, are treated as unlabeled. From this perspective, the key lies in how to extract semantic information. To achieve this, we propose IFL-GCL, using InfoNCE as a "free lunch" to extract semantic information. Specifically, We first prove that under InfoNCE, the representation similarity of node pairs aligns with the probability that the corresponding contrastive sample is positive. Then we redefine the maximum likelihood objective based on the corrected samples, leading to a new InfoNCE loss function. Extensive experiments on both the graph pretraining framework and LLM as an enhancer show significantly improvements of IFL-GCL in both IID and OOD scenarios, achieving up to a 9.05% improvement, validating the effectiveness of semantically guided. Code for IFL-GCL is publicly available at: https://github.com/Camel-Prince/IFL-GCL.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.1 -->
                    
                <!-- Medicine: 3.4 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- GNN: 2.6 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06335" target="_blank" rel="noopener noreferrer">Remote Rowhammer Attack using Adversarial Observations on Federated Learning Clients</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jinsheng Yuan, Yuhang Hao, Weisi Guo, Yun Wu, Chongyan Gu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Federated Learning (FL) has the potential for simultaneous global learning amongst a large number of parallel agents, enabling emerging AI such as LLMs to be trained across demographically diverse data. Central to this being efficient is the ability for FL to perform sparse gradient updates and remo</span>
                
                <span class="abstract-full" style="display: none;">Federated Learning (FL) has the potential for simultaneous global learning amongst a large number of parallel agents, enabling emerging AI such as LLMs to be trained across demographically diverse data. Central to this being efficient is the ability for FL to perform sparse gradient updates and remote direct memory access at the central server. Most of the research in FL security focuses on protecting data privacy at the edge client or in the communication channels between the client and server. Client-facing attacks on the server are less well investigated as the assumption is that a large collective of clients offer resilience.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.2 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- LLMs: 2.8 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Hardware: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06389" target="_blank" rel="noopener noreferrer">Deep Learning-Based Robust Optical Guidance for Hypersonic Platforms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Adrien Chan-Hon-Tong, Aur\'elien Plyer, Baptiste Cadalen, Laurent Serre
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Sensor-based guidance is required for long-range platforms. To bypass the structural limitation of classical registration on reference image framework, we offer in this paper to encode a stack of images of the scene into a deep network. Relying on a stack is showed to be relevant on bimodal scene (e</span>
                
                <span class="abstract-full" style="display: none;">Sensor-based guidance is required for long-range platforms. To bypass the structural limitation of classical registration on reference image framework, we offer in this paper to encode a stack of images of the scene into a deep network. Relying on a stack is showed to be relevant on bimodal scene (e.g. when the scene can or can not be snowy).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06419" target="_blank" rel="noopener noreferrer">Initialization and training of matrix product state probabilistic models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xun Tang, Yuehaw Khoo, Lexing Ying
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modeling probability distributions via the wave function of a quantum state is central to quantum-inspired generative modeling and quantum state tomography (QST). We investigate a common failure mode in training randomly initialized matrix product states (MPS) using gradient descent. The results sho</span>
                
                <span class="abstract-full" style="display: none;">Modeling probability distributions via the wave function of a quantum state is central to quantum-inspired generative modeling and quantum state tomography (QST). We investigate a common failure mode in training randomly initialized matrix product states (MPS) using gradient descent. The results show that the trained MPS models do not accurately predict the strong interactions between boundary sites in periodic spin chain models. In the case of the Born machine algorithm, we further identify a causality trap, where the trained MPS models resemble causal models that ignore the non-local correlations in the true distribution. We propose two complementary strategies to overcome the training failure -- one through optimization and one through initialization. First, we develop a natural gradient descent (NGD) method, which approximately simulates the gradient flow on tensor manifolds and significantly enhances training efficiency. Numerical experiments show that NGD avoids local minima in both Born machines and in general MPS tomography. Remarkably, we show that NGD with line search can converge to the global minimum in only a few iterations. Second, for the BM algorithm, we introduce a warm-start initialization based on the TTNS-Sketch algorithm. We show that gradient descent under a warm initialization does not encounter the causality trap and admits rapid convergence to the ground truth.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 3.5 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- Math: 3.1 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06436" target="_blank" rel="noopener noreferrer">My Emotion on your face: The use of Facial Keypoint Detection to preserve Emotions in Latent Space Editing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jingrui He, Andrew Stephen McGough
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generative Adversarial Network approaches such as StyleGAN/2 provide two key benefits: the ability to generate photo-realistic face images and possessing a semantically structured latent space from which these images are created. Many approaches have emerged for editing images derived from vectors i</span>
                
                <span class="abstract-full" style="display: none;">Generative Adversarial Network approaches such as StyleGAN/2 provide two key benefits: the ability to generate photo-realistic face images and possessing a semantically structured latent space from which these images are created. Many approaches have emerged for editing images derived from vectors in the latent space of a pre-trained StyleGAN/2 models by identifying semantically meaningful directions (e.g., gender or age) in the latent space. By moving the vector in a specific direction, the ideal result would only change the target feature while preserving all the other features. Providing an ideal data augmentation approach for gesture research as it could be used to generate numerous image variations whilst keeping the facial expressions intact. However, entanglement issues, where changing one feature inevitably affects other features, impacts the ability to preserve facial expressions. To address this, we propose the use of an addition to the loss function of a Facial Keypoint Detection model to restrict changes to the facial expressions. Building on top of an existing model, adding the proposed Human Face Landmark Detection (HFLD) loss, provided by a pre-trained Facial Keypoint Detection model, to the original loss function. We quantitatively and qualitatively evaluate the existing and our extended model, showing the effectiveness of our approach in addressing the entanglement issue and maintaining the facial expression. Our approach achieves up to 49% reduction in the change of emotion in our experiments. Moreover, we show the benefit of our approach by comparing with state-of-the-art models. By increasing the ability to preserve the facial gesture and expression during facial transformation, we present a way to create human face images with fixed expression but different appearances, making it a reliable data augmentation approach for Facial Gesture and Expression research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.6 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06445" target="_blank" rel="noopener noreferrer">Tweedie Regression for Video Recommendation System</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yan Zheng, Qiang Chen, Chenglei Niu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modern recommendation systems aim to increase click-through rates (CTR) for better user experience, through commonly treating ranking as a classification task focused on predicting CTR. However, there is a gap between this method and the actual objectives of businesses across different sectors. In v</span>
                
                <span class="abstract-full" style="display: none;">Modern recommendation systems aim to increase click-through rates (CTR) for better user experience, through commonly treating ranking as a classification task focused on predicting CTR. However, there is a gap between this method and the actual objectives of businesses across different sectors. In video recommendation services, the objective of video on demand (VOD) extends beyond merely encouraging clicks, but also guiding users to discover their true interests, leading to increased watch time. And longer users watch time will leads to more revenue through increased chances of presenting online display advertisements. This research addresses the issue by redefining the problem from classification to regression, with a focus on maximizing revenue through user viewing time. Due to the lack of positive labels on recommendation, the study introduces Tweedie Loss Function, which is better suited in this scenario than the traditional mean square error loss. The paper also provides insights on how Tweedie process capture users diverse interests. Our offline simulation and online A/B test revealed that we can substantially enhance our core business objectives: user engagement in terms of viewing time and, consequently, revenue. Additionally, we provide a theoretical comparison between the Tweedie Loss and the commonly employed viewing time weighted Logloss, highlighting why Tweedie Regression stands out as an efficient solution. We further outline a framework for designing a loss function that focuses on a singular objective.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.4 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Networks: 3.3 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06477" target="_blank" rel="noopener noreferrer">Learning from the Good Ones: Risk Profiling-Based Defenses Against Evasion Attacks on DNNs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mohammed Elnawawy, Gargi Mitra, Shahrear Iqbal, Karthik Pattabiraman
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Safety-critical applications such as healthcare and autonomous vehicles use deep neural networks (DNN) to make predictions and infer decisions. DNNs are susceptible to evasion attacks, where an adversary crafts a malicious data instance to trick the DNN into making wrong decisions at inference time.</span>
                
                <span class="abstract-full" style="display: none;">Safety-critical applications such as healthcare and autonomous vehicles use deep neural networks (DNN) to make predictions and infer decisions. DNNs are susceptible to evasion attacks, where an adversary crafts a malicious data instance to trick the DNN into making wrong decisions at inference time. Existing defenses that protect DNNs against evasion attacks are either static or dynamic. Static defenses are computationally efficient but do not adapt to the evolving threat landscape, while dynamic defenses are adaptable but suffer from an increased computational overhead. To combine the best of both worlds, in this paper, we propose a novel risk profiling framework that uses a risk-aware strategy to selectively train static defenses using victim instances that exhibit the most resilient features and are hence more resilient against an evasion attack. We hypothesize that training existing defenses on instances that are less vulnerable to the attack enhances the adversarial detection rate by reducing false negatives. We evaluate the efficacy of our risk-aware selective training strategy on a blood glucose management system that demonstrates how training static anomaly detectors indiscriminately may result in an increased false negative rate, which could be life-threatening in safety-critical applications. Our experiments show that selective training on the less vulnerable patients achieves a recall increase of up to 27.5\% with minimal impact on precision compared to indiscriminate training.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.5 -->
                    
                <!-- Reinforcement Learning: 4.2 -->
                    
                <!-- GNN: 3.6 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06505" target="_blank" rel="noopener noreferrer">On Definite Iterated Belief Revision with Belief Algebras</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hua Meng, Zhiguo Long, Michael Sioutis, Zhengchun Zhou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Traditional logic-based belief revision research focuses on designing rules to constrain the behavior of revision operators. Frameworks have been proposed to characterize iterated revision rules, but they are often too loose, leading to multiple revision operators that all satisfy the rules under th</span>
                
                <span class="abstract-full" style="display: none;">Traditional logic-based belief revision research focuses on designing rules to constrain the behavior of revision operators. Frameworks have been proposed to characterize iterated revision rules, but they are often too loose, leading to multiple revision operators that all satisfy the rules under the same belief condition. In many practical applications, such as safety critical ones, it is important to specify a definite revision operator to enable agents to iteratively revise their beliefs in a deterministic way. In this paper, we propose a novel framework for iterated belief revision by characterizing belief information through preference relations. Semantically, both beliefs and new evidence are represented as belief algebras, which provide a rich and expressive foundation for belief revision. Building on traditional revision rules, we introduce additional postulates for revision with belief algebra, including an upper-bound constraint on the outcomes of revision. We prove that the revision result is uniquely determined given the current belief state and new evidence. Furthermore, to make the framework more useful in practice, we develop a particular algorithm for performing the proposed revision process. We argue that this approach may offer a more predictable and principled method for belief revision, making it suitable for real-world applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.9 -->
                    
                <!-- Medicine: 3.7 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06520" target="_blank" rel="noopener noreferrer">PRUNE: A Patching Based Repair Framework for Certiffable Unlearning of Neural Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xuran Li, Jingyi Wang, Xiaohan Yuan, Peixin Zhang, Zhan Qin, Zhibo Wang, Kui Ren
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">It is often desirable to remove (a.k.a. unlearn) a speciffc part of the training data from a trained neural network model. A typical application scenario is to protect the data holder's right to be forgotten, which has been promoted by many recent regulation rules. Existing unlearning methods involv</span>
                
                <span class="abstract-full" style="display: none;">It is often desirable to remove (a.k.a. unlearn) a speciffc part of the training data from a trained neural network model. A typical application scenario is to protect the data holder's right to be forgotten, which has been promoted by many recent regulation rules. Existing unlearning methods involve training alternative models with remaining data, which may be costly and challenging to verify from the data holder or a thirdparty auditor's perspective. In this work, we provide a new angle and propose a novel unlearning approach by imposing carefully crafted "patch" on the original neural network to achieve targeted "forgetting" of the requested data to delete. Speciffcally, inspired by the research line of neural network repair, we propose to strategically seek a lightweight minimum "patch" for unlearning a given data point with certiffable guarantee. Furthermore, to unlearn a considerable amount of data points (or an entire class), we propose to iteratively select a small subset of representative data points to unlearn, which achieves the effect of unlearning the whole set. Extensive experiments on multiple categorical datasets demonstrates our approach's effectiveness, achieving measurable unlearning while preserving the model's performance and being competitive in efffciency and memory consumption compared to various baseline methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.4 -->
                    
                <!-- Networks: 4.0 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Robotics: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06537" target="_blank" rel="noopener noreferrer">ProFashion: Prototype-guided Fashion Video Generation with Multiple Reference Images</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xianghao Kong, Qiaosong Qi, Yuanbin Wang, Anyi Rao, Biaolong Chen, Aixi Zhang, Si Liu, Hao Jiang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Fashion video generation aims to synthesize temporally consistent videos from reference images of a designated character. Despite significant progress, existing diffusion-based methods only support a single reference image as input, severely limiting their capability to generate view-consistent fash</span>
                
                <span class="abstract-full" style="display: none;">Fashion video generation aims to synthesize temporally consistent videos from reference images of a designated character. Despite significant progress, existing diffusion-based methods only support a single reference image as input, severely limiting their capability to generate view-consistent fashion videos, especially when there are different patterns on the clothes from different perspectives. Moreover, the widely adopted motion module does not sufficiently model human body movement, leading to sub-optimal spatiotemporal consistency. To address these issues, we propose ProFashion, a fashion video generation framework leveraging multiple reference images to achieve improved view consistency and temporal coherency. To effectively leverage features from multiple reference images while maintaining a reasonable computational cost, we devise a Pose-aware Prototype Aggregator, which selects and aggregates global and fine-grained reference features according to pose information to form frame-wise prototypes, which serve as guidance in the denoising process. To further enhance motion consistency, we introduce a Flow-enhanced Prototype Instantiator, which exploits the human keypoint motion flow to guide an extra spatiotemporal attention process in the denoiser. To demonstrate the effectiveness of ProFashion, we extensively evaluate our method on the MRFashion-7K dataset we collected from the Internet. ProFashion also outperforms previous methods on the UBC Fashion dataset.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.4 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Reinforcement Learning: 3.3 -->
                    
                <!-- 3D: 2.9 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06557" target="_blank" rel="noopener noreferrer">Weakly Supervised Temporal Sentence Grounding via Positive Sample Mining</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lu Dong, Haiyu Zhang, Hongjie Zhang, Yifei Huang, Zhen-Hua Ling, Yu Qiao, Limin Wang, Yali Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from </span>
                
                <span class="abstract-full" style="display: none;">The task of weakly supervised temporal sentence grounding (WSTSG) aims to detect temporal intervals corresponding to a language description from untrimmed videos with only video-level video-language correspondence. For an anchor sample, most existing approaches generate negative samples either from other videos or within the same video for contrastive learning. However, some training samples are highly similar to the anchor sample, directly regarding them as negative samples leads to difficulties for optimization and ignores the correlations between these similar samples and the anchor sample. To address this, we propose Positive Sample Mining (PSM), a novel framework that mines positive samples from the training set to provide more discriminative supervision. Specifically, for a given anchor sample, we partition the remaining training set into semantically similar and dissimilar subsets based on the similarity of their text queries. To effectively leverage these correlations, we introduce a PSM-guided contrastive loss to ensure that the anchor proposal is closer to similar samples and further from dissimilar ones. Additionally, we design a PSM-guided rank loss to ensure that similar samples are closer to the anchor proposal than to the negative intra-video proposal, aiming to distinguish the anchor proposal and the negative intra-video proposal. Experiments on the WSTSG and grounded VideoQA tasks demonstrate the effectiveness and superiority of our method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- Reinforcement Learning: 4.3 -->
                    
                <!-- LLMs: 2.8 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06625" target="_blank" rel="noopener noreferrer">CaMDN: Enhancing Cache Efficiency for Multi-tenant DNNs on Integrated NPUs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tianhao Cai, Liang Wang, Limin Xiao, Meng Han, Zeyu Wang, Lin Sun, Xiaojian Liao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">With the rapid development of DNN applications, multi-tenant execution, where multiple DNNs are co-located on a single SoC, is becoming a prevailing trend. Although many methods are proposed in prior works to improve multi-tenant performance, the impact of shared cache is not well studied. This pape</span>
                
                <span class="abstract-full" style="display: none;">With the rapid development of DNN applications, multi-tenant execution, where multiple DNNs are co-located on a single SoC, is becoming a prevailing trend. Although many methods are proposed in prior works to improve multi-tenant performance, the impact of shared cache is not well studied. This paper proposes CaMDN, an architecture-scheduling co-design to enhance cache efficiency for multi-tenant DNNs on integrated NPUs. Specifically, a lightweight architecture is proposed to support model-exclusive, NPU-controlled regions inside shared cache to eliminate unexpected cache contention. Moreover, a cache scheduling method is proposed to improve shared cache utilization. In particular, it includes a cache-aware mapping method for adaptability to the varying available cache capacity and a dynamic allocation algorithm to adjust the usage among co-located DNNs at runtime. Compared to prior works, CaMDN reduces the memory access by 33.4% on average and achieves a model speedup of up to 2.56$\times$ (1.88$\times$ on average).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 4.6 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- GNN: 2.9 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06645" target="_blank" rel="noopener noreferrer">RTOS Architectures that Solve the Diminishing Bandwidth Problem</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mazen Arakji
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The Diminishing Bandwidth Problem is a long standing, previously unidentified, extensibility problem of current real-time operating systems characterized by a superficial dependency between the number of tasks in a system and the maximum bandwidth associated with a peripheral device. In the worst ca</span>
                
                <span class="abstract-full" style="display: none;">The Diminishing Bandwidth Problem is a long standing, previously unidentified, extensibility problem of current real-time operating systems characterized by a superficial dependency between the number of tasks in a system and the maximum bandwidth associated with a peripheral device. In the worst case, this diabolical deficiency will continue to decrease the maximum bandwidth of a peripheral device as more tasks are added to the application. If this is not taken into account, a previously functional application may experience data loss if more tasks are added to it in order to, for example, implement new features. Three novel RTOS architectures that solve the Diminishing Bandwidth Problem are specified and discussed: the Defer Structure RTOS Architecture, the Barriers and Requests RTOS Architecture, and the Strictly Atomic RTOS Architecture. Finally, two hardware solutions to the Diminishing Bandwidth Problem are also presented.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.8 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Networks: 3.3 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06725" target="_blank" rel="noopener noreferrer">On Finding Randomly Planted Cliques in Arbitrary Graphs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Francesco Agrimonti, Marco Bressan, Tommaso d'Orsi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study a planted clique model introduced by Feige where a complete graph of size $c\cdot n$ is planted uniformly at random in an arbitrary $n$-vertex graph. We give a simple deterministic algorithm that, in almost linear time, recovers a clique of size $(c/3)^{O(1/c)} \cdot n$ as long as the origi</span>
                
                <span class="abstract-full" style="display: none;">We study a planted clique model introduced by Feige where a complete graph of size $c\cdot n$ is planted uniformly at random in an arbitrary $n$-vertex graph. We give a simple deterministic algorithm that, in almost linear time, recovers a clique of size $(c/3)^{O(1/c)} \cdot n$ as long as the original graph has maximum degree at most $(1-p)n$ for some fixed $p>0$. The proof hinges on showing that the degrees of the final graph are correlated with the planted clique, in a way similar to (but more intricate than) the classical $G(n,\frac{1}{2})+K_{\sqrt{n}}$ planted clique model. Our algorithm suggests a separation from the worst-case model, where, assuming the Unique Games Conjecture, no polynomial algorithm can find cliques of size $\Omega(n)$ for every fixed $c>0$, even if the input graph has maximum degree $(1-p)n$. Our techniques extend beyond the planted clique model. For example, when the planted graph is a balanced biclique, we recover a balanced biclique of size larger than the best guarantees known for the worst case.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Math: 4.0 -->
                    
                <!-- Networks: 3.9 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- LLMs: 2.2 -->
                    
                <!-- Pathfinding: 2.0 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06737" target="_blank" rel="noopener noreferrer">Balancing Progress and Safety: A Novel Risk-Aware Objective for RL in Autonomous Driving</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ahmed Abouelazm, Jonas Michel, Helen Gremmelmaier, Tim Joseph, Philip Sch\"orner, J. Marius Z\"ollner
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reinforcement Learning (RL) is a promising approach for achieving autonomous driving due to robust decision-making capabilities. RL learns a driving policy through trial and error in traffic scenarios, guided by a reward function that combines the driving objectives. The design of such reward functi</span>
                
                <span class="abstract-full" style="display: none;">Reinforcement Learning (RL) is a promising approach for achieving autonomous driving due to robust decision-making capabilities. RL learns a driving policy through trial and error in traffic scenarios, guided by a reward function that combines the driving objectives. The design of such reward function has received insufficient attention, yielding ill-defined rewards with various pitfalls. Safety, in particular, has long been regarded only as a penalty for collisions. This leaves the risks associated with actions leading up to a collision unaddressed, limiting the applicability of RL in real-world scenarios. To address these shortcomings, our work focuses on enhancing the reward formulation by defining a set of driving objectives and structuring them hierarchically. Furthermore, we discuss the formulation of these objectives in a normalized manner to transparently determine their contribution to the overall reward. Additionally, we introduce a novel risk-aware objective for various driving interactions based on a two-dimensional ellipsoid function and an extension of Responsibility-Sensitive Safety (RSS) concepts. We evaluate the efficacy of our proposed reward in unsignalized intersection scenarios with varying traffic densities. The approach decreases collision rates by 21\% on average compared to baseline rewards and consistently surpasses them in route progress and cumulative reward, demonstrating its capability to promote safer driving behaviors while maintaining high-performance levels.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.5 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06753" target="_blank" rel="noopener noreferrer">Boltzmann Classifier: A Thermodynamic-Inspired Approach to Supervised Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Muhamed Amin, Bernard R. Brooks
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a novel classification algorithm, the Boltzmann Classifier, inspired by the thermodynamic principles underlying the Boltzmann distribution. Our method computes a probabilistic estimate for each class based on an energy function derived from feature-wise deviations between input samples an</span>
                
                <span class="abstract-full" style="display: none;">We propose a novel classification algorithm, the Boltzmann Classifier, inspired by the thermodynamic principles underlying the Boltzmann distribution. Our method computes a probabilistic estimate for each class based on an energy function derived from feature-wise deviations between input samples and class-specific centroids. The resulting probabilities are proportional to the exponential negative energies, normalized across classes, analogous to the Boltzmann distribution used in statistical mechanics. In addition, the KT variable can be used to allow the high energy states to be more accessible, which allows the tuning of their probabilities as needed. We evaluate the model performance on several datasets from different applications. The model achieves a high accuracy, which indicates that the Boltzmann Classifier is competitive with standard models like logistic regression and k-nearest neighbors while offering a thermodynamically motivated probabilistic interpretation. our classifier does not require iterative optimization or backpropagation and is thus computationally efficient and easy to integrate into existing workflows. This work demonstrates how ideas from physics can inform new directions in machine learning, providing a foundation for interpretable, energy-based decision-making systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.0 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06758" target="_blank" rel="noopener noreferrer">8 Years of Optimizing Apache Otava: How disconnected open source developers took an algorithm from n3 to constant time</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Henrik Ingo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">As the project now known as Apache Otava (incubating) makes it first release, we look back over the past 8 years that the codebase was developed by a rather uncoordinated, loosely connected group of performance engineers at MongoDB, Datastax, Confluent, Nyrkio and others. Ever since the first public</span>
                
                <span class="abstract-full" style="display: none;">As the project now known as Apache Otava (incubating) makes it first release, we look back over the past 8 years that the codebase was developed by a rather uncoordinated, loosely connected group of performance engineers at MongoDB, Datastax, Confluent, Nyrkio and others. Ever since the first publication (Daly 2020), developers of the code base now known as Apache Otava (incubating), have continuosly improved its performance. Even when a contributor's primary motivation was to add functionality, it seems like they couldn't help themselves but to also make some performance optimizations while at it. When developing the Nyrkio web service to provide change detection for performance testing, we have observed that Otava had become fast enough that it was almost feasible to compute change points synchronously, as the user is browsing test results in a web browser. Inspired by this, we have developed and contributed a new optimization for the common case where new data points are appended to the end of the series. This is now the 7th generation of performance optimizations in Otava. These improvements have been done over the past 8 years of development, by disconnected individuals at different employees. Taken together, the historical optimizations and those published in this paper, represent a 18 000 to 300 000 speedup over the original by the book implementation of (Matteson and James 2014). In the language of computational complexity, an evolution from O (n3) to O (1) (constant time). The ability to compute and recompute change points in real-time unlocks new opportunities in the user experience.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.9 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06845" target="_blank" rel="noopener noreferrer">Secure Safety Filter: Towards Safe Flight Control under Sensor Attacks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiao Tan, Junior Sundar, Renzo Bruzzone, Pio Ong, Willian T. Lunardi, Martin Andreoni, Paulo Tabuada, Aaron D. Ames
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modern autopilot systems are prone to sensor attacks that can jeopardize flight safety. To mitigate this risk, we proposed a modular solution: the secure safety filter, which extends the well-established control barrier function (CBF)-based safety filter to account for, and mitigate, sensor attacks.</span>
                
                <span class="abstract-full" style="display: none;">Modern autopilot systems are prone to sensor attacks that can jeopardize flight safety. To mitigate this risk, we proposed a modular solution: the secure safety filter, which extends the well-established control barrier function (CBF)-based safety filter to account for, and mitigate, sensor attacks. This module consists of a secure state reconstructor (which generates plausible states) and a safety filter (which computes the safe control input that is closest to the nominal one). Differing from existing work focusing on linear, noise-free systems, the proposed secure safety filter handles bounded measurement noise and, by leveraging reduced-order model techniques, is applicable to the nonlinear dynamics of drones. Software-in-the-loop simulations and drone hardware experiments demonstrate the effectiveness of the secure safety filter in rendering the system safe in the presence of sensor attacks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.9 -->
                    
                <!-- Medicine: 4.0 -->
                    
                <!-- Reinforcement Learning: 3.7 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06852" target="_blank" rel="noopener noreferrer">Improving Random Forests by Smoothing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ziyi Liu, Phuc Luong, Mario Boley, Daniel F. Schmidt
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Gaussian process regression is a popular model in the small data regime due to its sound uncertainty quantification and the exploitation of the smoothness of the regression function that is encountered in a wide range of practical problems. However, Gaussian processes perform sub-optimally when the </span>
                
                <span class="abstract-full" style="display: none;">Gaussian process regression is a popular model in the small data regime due to its sound uncertainty quantification and the exploitation of the smoothness of the regression function that is encountered in a wide range of practical problems. However, Gaussian processes perform sub-optimally when the degree of smoothness is non-homogeneous across the input domain. Random forest regression partially addresses this issue by providing local basis functions of variable support set sizes that are chosen in a data-driven way. However, they do so at the expense of forgoing any degree of smoothness, which often results in poor performance in the small data regime. Here, we aim to combine the advantages of both models by applying a kernel-based smoothing mechanism to a learned random forest or any other piecewise constant prediction function. As we demonstrate empirically, the resulting model consistently improves the predictive performance of the underlying random forests and, in almost all test cases, also improves the log loss of the usual uncertainty quantification based on inter-tree variance. The latter advantage can be attributed to the ability of the smoothing model to take into account the uncertainty over the exact tree-splitting locations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.0 -->
                    
                <!-- Math: 4.2 -->
                    
                <!-- Reinforcement Learning: 3.2 -->
                    
                <!-- Federated Learning: 2.9 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Pathfinding: 1.9 -->
                    
                <!-- Robotics: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06863" target="_blank" rel="noopener noreferrer">Masked Subspace Clustering Methods</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jiebo Song, Huaming Ling
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">To further utilize the unsupervised features and pairwise information, we propose a general Bilevel Clustering Optimization (BCO) framework to improve the performance of clustering. And then we introduce three special cases on subspace clustering with two different types of masks. At first, we refor</span>
                
                <span class="abstract-full" style="display: none;">To further utilize the unsupervised features and pairwise information, we propose a general Bilevel Clustering Optimization (BCO) framework to improve the performance of clustering. And then we introduce three special cases on subspace clustering with two different types of masks. At first, we reformulate the original subspace clustering as a Basic Masked Subspace Clustering (BMSC), which reformulate the diagonal constraints to a hard mask. Then, we provide a General Masked Subspace Clustering (GMSC) method to integrate different clustering via a soft mask. Furthermore, based on BCO and GMSC, we induce a learnable soft mask and design a Recursive Masked Subspace Clustering (RMSC) method that can alternately update the affinity matrix and the soft mask. Numerical experiments show that our models obtain significant improvement compared with the baselines on several commonly used datasets, such as MNIST, USPS, ORL, COIL20 and COIL100.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06909" target="_blank" rel="noopener noreferrer">Time-Modulated EM Skins for Integrated Sensing and Communications</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lorenzo Poli, Aakash Bansal, Giacomo Oliveri, Aaron Angel Salas-Sanchez, Will Whittow, Andrea Massa
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">An innovative solution, based on the exploitation of the harmonic beams generated by time-modulated electromagnetic skins (TM-EMSs), is proposed for the implementation of integrated sensing and communication (ISAC) functionalities in a Smart Electromagnetic Environment (SEME) scenario. More in detai</span>
                
                <span class="abstract-full" style="display: none;">An innovative solution, based on the exploitation of the harmonic beams generated by time-modulated electromagnetic skins (TM-EMSs), is proposed for the implementation of integrated sensing and communication (ISAC) functionalities in a Smart Electromagnetic Environment (SEME) scenario. More in detail, the field radiated by a user terminal, located at an unknown position, is assumed to illuminate a passive TM-EMS that, thanks to a suitable modulation of the local reflection coefficients at the meta-atom level of the EMS surface, simultaneously reflects towards a receiving base station (BS) a "sum" beam and a "difference" one at slightly different frequencies. By processing the received signals and exploiting monopulse radar tracking concepts, the BS both localizes the user terminal and, as a by-product, establishes a communication link with it by leveraging on the "sum" reflected beam. Towards this purpose, the arising harmonic beam control problem is reformulated as a global optimization one, which is successively solved by means of an evolutionary iterative approach to determine the desired TM-EMS modulation sequence. The results from selected numerical and experimental tests are reported to assess the effectiveness and the reliability of the proposed approach.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 4.8 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- LLMs: 1.5 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06938" target="_blank" rel="noopener noreferrer">A digital perspective on the role of a stemma in material-philological transmission studies</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Katarzyna Anna Kapitan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Taking its point of departure in the recent developments in the field of digital humanities and the increasing automatisation of scholarly workflows, this study explores the implications of digital approaches to textual traditions for the broader field of textual scholarship. It argues that the rela</span>
                
                <span class="abstract-full" style="display: none;">Taking its point of departure in the recent developments in the field of digital humanities and the increasing automatisation of scholarly workflows, this study explores the implications of digital approaches to textual traditions for the broader field of textual scholarship. It argues that the relative simplicity of creating computergenerated stemmas allows us to view the stemma codicum as a research tool rather than the final product of our scholarly investigation. Using the Old Norse saga of Hr\'omundur as a case study, this article demonstrates that stemmas can serve as a starting point for exploring textual traditions further. In doing so, they enable us to address research questions that otherwise remain unanswered. The article is accompanied by datasets used to generate stemmas for the Hr\'omundar saga tradition as well as two custom Python scripts. The scripts are designed to convert XML-based textual data, encoded according to the TEI Guidelines, into the input format used for the analysis in the PHYLIP package to generate unrooted trees of relationships between texts.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 4.1 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Math: 2.8 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06958" target="_blank" rel="noopener noreferrer">A Formally Verified Robustness Certifier for Neural Networks (Extended Version)</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: James Tobler, Hira Taqdees Syeda, Toby Murray
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Neural networks are often susceptible to minor perturbations in input that cause them to misclassify. A recent solution to this problem is the use of globally-robust neural networks, which employ a function to certify that the classification of an input cannot be altered by such a perturbation. Outp</span>
                
                <span class="abstract-full" style="display: none;">Neural networks are often susceptible to minor perturbations in input that cause them to misclassify. A recent solution to this problem is the use of globally-robust neural networks, which employ a function to certify that the classification of an input cannot be altered by such a perturbation. Outputs that pass this test are called certified robust. However, to the authors' knowledge, these certification functions have not yet been verified at the implementation level. We demonstrate how previous unverified implementations are exploitably unsound in certain circumstances. Moreover, they often rely on approximation-based algorithms, such as power iteration, that (perhaps surprisingly) do not guarantee soundness. To provide assurance that a given output is robust, we implemented and formally verified a certification function for globally-robust neural networks in Dafny. We describe the program, its specifications, and the important design decisions taken for its implementation and verification, as well as our experience applying it in practice.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.6 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- GNN: 2.8 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07016" target="_blank" rel="noopener noreferrer">Multi-Terminal Remote Generation and Estimation Over a Broadcast Channel With Correlated Priors</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Maximilian Egger, Rawad Bitar, Antonia Wachter-Zeh, Nir Weinberger, Deniz G\"und\"uz
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study the multi-terminal remote estimation problem under a rate constraint, in which the goal of the encoder is to help each decoder estimate a function over a certain distribution -- while the distribution is known only to the encoder, the function to be estimated is known only to the decoders, </span>
                
                <span class="abstract-full" style="display: none;">We study the multi-terminal remote estimation problem under a rate constraint, in which the goal of the encoder is to help each decoder estimate a function over a certain distribution -- while the distribution is known only to the encoder, the function to be estimated is known only to the decoders, and can also be different for each decoder. The decoders can observe correlated samples from prior distributions, instantiated through shared randomness with the encoder. To achieve this, we employ remote generation, where the encoder helps decoders generate samples from the underlying distribution by using the samples from the prior through importance sampling. While methods such as minimal random coding can be used to efficiently transmit samples to each decoder individually using their importance scores, it is unknown if the correlation among the samples from the priors can reduce the communication cost using the availability of a broadcast link. We propose a hierarchical importance sampling strategy that facilitates, in the case of non-zero G\'acs-K\"orner common information among the priors of the decoders, a common sampling step leveraging the availability of a broadcast channel. This is followed by a refinement step for the individual decoders. We present upper bounds on the bias and the estimation error for unicast transmission, which is of independent interest. We then introduce a method that splits into two phases, dedicated to broadcast and unicast transmission, respectively, and show the reduction in communication cost.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 5.0 -->
                    
                <!-- Math: 4.9 -->
                    
                <!-- Reinforcement Learning: 4.2 -->
                    
                <!-- Medicine: 3.5 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Pathfinding: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Multi-armed Bandit: 1.5 -->
                    
                <!-- LLMs: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07034" target="_blank" rel="noopener noreferrer">NetSight: Graph Attention Based Traffic Forecasting in Computer Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jinming Xing, Guoheng Sun, Hui Sun, Linchao Pan, Shakir Mahmood, Xuanhao Luo, Muhammad Shahzad
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The traffic in today's networks is increasingly influenced by the interactions among network nodes as well as by the temporal fluctuations in the demands of the nodes. Traditional statistical prediction methods are becoming obsolete due to their inability to address the non-linear and dynamic spatio</span>
                
                <span class="abstract-full" style="display: none;">The traffic in today's networks is increasingly influenced by the interactions among network nodes as well as by the temporal fluctuations in the demands of the nodes. Traditional statistical prediction methods are becoming obsolete due to their inability to address the non-linear and dynamic spatio-temporal dependencies present in today's network traffic. The most promising direction of research today is graph neural networks (GNNs) based prediction approaches that are naturally suited to handle graph-structured data. Unfortunately, the state-of-the-art GNN approaches separate the modeling of spatial and temporal information, resulting in the loss of important information about joint dependencies. These GNN based approaches further do not model information at both local and global scales simultaneously, leaving significant room for improvement. To address these challenges, we propose NetSight. NetSight learns joint spatio-temporal dependencies simultaneously at both global and local scales from the time-series of measurements of any given network metric collected at various nodes in a network. Using the learned information, NetSight can then accurately predict the future values of the given network metric at those nodes in the network. We propose several new concepts and techniques in the design of NetSight, such as spatio-temporal adjacency matrix and node normalization. Through extensive evaluations and comparison with prior approaches using data from two large real-world networks, we show that NetSight significantly outperforms all prior state-of-the-art approaches. We will release the source code and data used in the evaluation of NetSight on the acceptance of this paper.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- Medicine: 4.3 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07042" target="_blank" rel="noopener noreferrer">A Reinforcement Learning Framework for Application-Specific TCP Congestion-Control</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jinming Xing, Muhammad Shahzad
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The Congestion Control (CC) module plays a critical role in the Transmission Control Protocol (TCP), ensuring the stability and efficiency of network data transmission. The CC approaches that are commonly used these days employ heuristics-based rules to adjust the sending rate. Due to their heuristi</span>
                
                <span class="abstract-full" style="display: none;">The Congestion Control (CC) module plays a critical role in the Transmission Control Protocol (TCP), ensuring the stability and efficiency of network data transmission. The CC approaches that are commonly used these days employ heuristics-based rules to adjust the sending rate. Due to their heuristics-based nature, these approaches are not only unable to adapt to changing network conditions but are also agnostic to the diverse requirements that different applications often have. Recently, several learning-based CC approaches have been proposed to adapt to changing network conditions. Unfortunately, they are not designed to take application requirements into account. Prior heuristics-based as well as learning-based CC approaches focus on achieving a singular objective, which is often to maximize throughput, even though a lot of applications care more about latency, packet losses, jitter, and different combinations of various network metrics. Motivated by this, we propose a Deep Reinforcement Learning (DRL) based CC framework, namely ASC, which allows any application to specify any arbitrary objectives that the network traffic of that application should achieve and is able to swiftly adapt to the changes in the objectives of the applications as well as to the changes in the network conditions. Our ASC framework further employs a client-server architecture that serves two purposes: 1) it makes ASC highly scalable in terms of the arrival and departure of TCP connections, and 2) it makes ASC very lightweight for the nodes maintaining the TCP connections. We implemented and extensively evaluated ASC in a variety of settings. Our results show that it can not only achieve various objectives but also outperforms prior approaches even in the specific objectives that those approaches were designed to achieve.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.3 -->
                    
                <!-- Reinforcement Learning: 3.7 -->
                    
                <!-- Networks: 3.5 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Medicine: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07081" target="_blank" rel="noopener noreferrer">COMRECGC: Global Graph Counterfactual Explainer through Common Recourse</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gregoire Fournier, Sourav Medya
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph neural networks (GNNs) have been widely used in various domains such as social networks, molecular biology, or recommendation systems. Concurrently, different explanations methods of GNNs have arisen to complement its black-box nature. Explanations of the GNNs' predictions can be categorized i</span>
                
                <span class="abstract-full" style="display: none;">Graph neural networks (GNNs) have been widely used in various domains such as social networks, molecular biology, or recommendation systems. Concurrently, different explanations methods of GNNs have arisen to complement its black-box nature. Explanations of the GNNs' predictions can be categorized into two types--factual and counterfactual. Given a GNN trained on binary classification into ''accept'' and ''reject'' classes, a global counterfactual explanation consists in generating a small set of ''accept'' graphs relevant to all of the input ''reject'' graphs. The transformation of a ''reject'' graph into an ''accept'' graph is called a recourse. A common recourse explanation is a small set of recourse, from which every ''reject'' graph can be turned into an ''accept'' graph. Although local counterfactual explanations have been studied extensively, the problem of finding common recourse for global counterfactual explanation remains unexplored, particularly for GNNs. In this paper, we formalize the common recourse explanation problem, and design an effective algorithm, COMRECGC, to solve it. We benchmark our algorithm against strong baselines on four different real-world graphs datasets and demonstrate the superior performance of COMRECGC against the competitors. We also compare the common recourse explanations to the graph counterfactual explanation, showing that common recourse explanations are either comparable or superior, making them worth considering for applications such as drug discovery or computational biology.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.0 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07126" target="_blank" rel="noopener noreferrer">AI-Driven Optimization of Wave-Controlled Reconfigurable Intelligent Surfaces</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Gal Ben Itzhak, Miguel Saavedra-Melo, Ender Ayanoglu, Filippo Capolino, A. Lee Swindlehurst
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A promising type of Reconfigurable Intelligent Surface (RIS) employs tunable control of its varactors using biasing transmission lines below the RIS reflecting elements. Biasing standing waves (BSWs) are excited by a time-periodic signal and sampled at each RIS element to create a desired biasing vo</span>
                
                <span class="abstract-full" style="display: none;">A promising type of Reconfigurable Intelligent Surface (RIS) employs tunable control of its varactors using biasing transmission lines below the RIS reflecting elements. Biasing standing waves (BSWs) are excited by a time-periodic signal and sampled at each RIS element to create a desired biasing voltage and control the reflection coefficients of the elements. A simple rectifier can be used to sample the voltages and capture the peaks of the BSWs over time. Like other types of RIS, attempting to model and accurately configure a wave-controlled RIS is extremely challenging due to factors such as device non-linearities, frequency dependence, element coupling, etc., and thus significant differences will arise between the actual and assumed performance. An alternative approach to solving this problem is data-driven: Using training data obtained by sampling the reflected radiation pattern of the RIS for a set of BSWs, a neural network (NN) is designed to create an input-output map between the BSW amplitudes and the resulting sampled radiation pattern. This is the approach discussed in this paper. In the proposed approach, the NN is optimized using a genetic algorithm (GA) to minimize the error between the predicted and measured radiation patterns. The BSW amplitudes are then designed via Simulated Annealing (SA) to optimize a signal-to-leakage-plus-noise ratio measure by iteratively forward-propagating the BSW amplitudes through the NN and using its output as feedback to determine convergence. The resulting optimal solutions are stored in a lookup table to be used both as settings to instantly configure the RIS and as a basis for determining more complex radiation patterns.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 4.0 -->
                    
                <!-- Medicine: 3.8 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Robotics: 2.5 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07130" target="_blank" rel="noopener noreferrer">Minimal Linear Codes Violating the Ashikhmin-Barg Condition from Arbitrary Projective Linear Codes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hao Chen, Yaqi Chen, Conghui Xie, Huimin Lao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In recent years, there have been many constructions of minimal linear codes violating the Ashikhmin-Barg condition from Boolean functions, linear codes with few nonzero weights or partial difference sets. In this paper, we first give a general method to transform a minimal code satisfying the Ashikh</span>
                
                <span class="abstract-full" style="display: none;">In recent years, there have been many constructions of minimal linear codes violating the Ashikhmin-Barg condition from Boolean functions, linear codes with few nonzero weights or partial difference sets. In this paper, we first give a general method to transform a minimal code satisfying the Ashikhmin-Barg condition to a minimal code violating the Ashikhmin-Barg condition. Then we give a construction of a minimal code satisfying the Ashikhmin-Barg condition from an arbitrary projective linear code. Hence an arbitrary projective linear code can be transformed to a minimal codes violating the Ashikhmin-Barg condition. Then we give infinite many families of minimal codes violating the Ashikhamin-Barg condition. Weight distributions of constructed minimal codes violating the Ashikhmin-Barg condition in this paper are determined. Many minimal linear codes violating the Ashikhmin-Barg condition with their minimum weights close to the optimal or the best known minimum weights of linear codes are constructed in this paper. Moreover, many infinite families of self-orthogonal binary minimal codes violating the Ashikhmin-Barg condition are also given.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.9 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Math: 3.1 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07259" target="_blank" rel="noopener noreferrer">A Framework for Joint Grasp and Motion Planning in Confined Spaces</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Martin Rudorfer, Ji\v{r}\'i Hartvich, Vojt\v{e}ch Von\'asek
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Robotic grasping is a fundamental skill across all domains of robot applications. There is a large body of research for grasping objects in table-top scenarios, where finding suitable grasps is the main challenge. In this work, we are interested in scenarios where the objects are in confined spaces </span>
                
                <span class="abstract-full" style="display: none;">Robotic grasping is a fundamental skill across all domains of robot applications. There is a large body of research for grasping objects in table-top scenarios, where finding suitable grasps is the main challenge. In this work, we are interested in scenarios where the objects are in confined spaces and hence particularly difficult to reach. Planning how the robot approaches the object becomes a major part of the challenge, giving rise to methods for joint grasp and motion planning. The framework proposed in this paper provides 20 benchmark scenarios with systematically increasing difficulty, realistic objects with precomputed grasp annotations, and tools to create and share more scenarios. We further provide two baseline planners and evaluate them on the scenarios, demonstrating that the proposed difficulty levels indeed offer a meaningful progression. We invite the research community to build upon this framework by making all components publicly available as open source.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.2 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Robotics: 2.7 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Math: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07315" target="_blank" rel="noopener noreferrer">FedIFL: A federated cross-domain diagnostic framework for motor-driven systems with inconsistent fault modes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zexiao Wang, Yankai Wang, Xiaoqiang Liao, Xinguo Ming, Weiming Shen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Due to the scarcity of industrial data, individual equipment users, particularly start-ups, struggle to independently train a comprehensive fault diagnosis model; federated learning enables collaborative training while ensuring data privacy, making it an ideal solution. However, the diversity of wor</span>
                
                <span class="abstract-full" style="display: none;">Due to the scarcity of industrial data, individual equipment users, particularly start-ups, struggle to independently train a comprehensive fault diagnosis model; federated learning enables collaborative training while ensuring data privacy, making it an ideal solution. However, the diversity of working conditions leads to variations in fault modes, resulting in inconsistent label spaces across different clients. In federated diagnostic scenarios, label space inconsistency leads to local models focus on client-specific fault modes and causes local models from different clients to map different failure modes to similar feature representations, which weakens the aggregated global model's generalization. To tackle this issue, this article proposed a federated cross-domain diagnostic framework termed Federated Invariant Features Learning (FedIFL). In intra-client training, prototype contrastive learning mitigates intra-client domain shifts, subsequently, feature generating ensures local models can access distributions of other clients in a privacy-friendly manner. Besides, in cross-client training, a feature disentanglement mechanism is introduced to mitigate cross-client domain shifts, specifically, an instance-level federated instance consistency loss is designed to ensure the instance-level consistency of invariant features between different clients, furthermore, a federated instance personalization loss and an orthogonal loss are constructed to distinguish specific features that from the invariant features. Eventually, the aggregated model achieves promising generalization among global label spaces, enabling accurate fault diagnosis for target clients' Motor Driven Systems (MDSs) with inconsistent label spaces. Experiments on real-world MDSs validate the effectiveness and superiority of FedIFL in federated cross-domain diagnosis with inconsistent fault modes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.8 -->
                    
                <!-- Medicine: 4.7 -->
                    
                <!-- Federated Learning: 3.3 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07348" target="_blank" rel="noopener noreferrer">Stiffness-based Analytic Centre Method for Cable-Driven Parallel Robots</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Domenico Dona', Vincenzo Di Paola, Matteo Zoppi, Alberto Trevisani
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Nowadays, being fast and precise are key requirements in Robotics. This work introduces a novel methodology to tune the stiffness of Cable-Driven Parallel Robots (CDPRs) while simultaneously addressing the tension distribution problem. In particular, the approach relies on the Analytic-Centre method</span>
                
                <span class="abstract-full" style="display: none;">Nowadays, being fast and precise are key requirements in Robotics. This work introduces a novel methodology to tune the stiffness of Cable-Driven Parallel Robots (CDPRs) while simultaneously addressing the tension distribution problem. In particular, the approach relies on the Analytic-Centre method. Indeed, weighting the barrier functions makes natural the stiffness adaptation. The intrinsic ability to adjust the stiffness during the execution of the task enables the CDPRs to effectively meet above-mentioned requirements. The capabilities of the method are demonstrated through simulations by comparing it with the existing approach.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.2 -->
                    
                <!-- Reinforcement Learning: 3.8 -->
                    
                <!-- Medicine: 3.6 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- Math: 3.1 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Pathfinding: 1.4 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07408" target="_blank" rel="noopener noreferrer">Energy personas in Danish households</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nadine Sandjo Tchatchoua (Roskilde University), Line Valdorff Madsen (Aalborg University), Anders Rhiger Hansen (Aalborg University)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Technologies to monitor the provision of renewable energy are part of emerging technologies to help address the discrepancy between renewable energy production and its related usage in households. This paper presents various ways householders use a technological artifact for the real-time monitoring</span>
                
                <span class="abstract-full" style="display: none;">Technologies to monitor the provision of renewable energy are part of emerging technologies to help address the discrepancy between renewable energy production and its related usage in households. This paper presents various ways householders use a technological artifact for the real-time monitoring of renewable energy provision. Such a monitoring thus affords householders with an opportunity to adjust their energy consumption according to renewable energy provision. In Denmark, Ewii, previously Barry, is a Danish energy supplier which provides householders with an opportunity to monitor energy sources in real time through a technological solution of the same name. This paper use provision afforded by Ewii as a case for exploring how householders organize themselves to use a technological artefact that supports the monitoring of energy and its related usage. This study aims to inform technology design through the derivation of four personas. The derived personas highlight the differences in energy monitoring practices for the householders and their engagement. These personas are characterised as dedicated, organised, sporadic, and convenient. Understanding these differences in energy monitoring practice using the technological artefact form a solid element in the design of future energy technologies that interfere with the everyday practices and energy consumption for households. This is paramount for future energy related technology design, and for the clarification of usage assumptions that are embedded in the rollout of energy related technology as a country such as Denmark moves through its green transition.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.6 -->
                    
                <!-- Medicine: 4.4 -->
                    
                <!-- Networks: 3.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07441" target="_blank" rel="noopener noreferrer">Cooperative Assembly with Autonomous Mobile Manipulators in an Underwater Scenario</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Davide Torielli
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">[...] Specifically, the problem addressed is an assembly one known as the peg-in-hole task. In this case, two autonomous manipulators must carry cooperatively (at kinematic level) a peg and must insert it into an hole fixed in the environment. Even if the peg-in-hole is a well-known problem, there a</span>
                
                <span class="abstract-full" style="display: none;">[...] Specifically, the problem addressed is an assembly one known as the peg-in-hole task. In this case, two autonomous manipulators must carry cooperatively (at kinematic level) a peg and must insert it into an hole fixed in the environment. Even if the peg-in-hole is a well-known problem, there are no specific studies related to the use of two different autonomous manipulators, especially in underwater scenarios. Among all the possible investigations towards the problem, this work focuses mainly on the kinematic control of the robots. The methods used are part of the Task Priority Inverse Kinematics (TPIK) approach, with a cooperation scheme that permits to exchange as less information as possible between the agents (that is really important being water a big impediment for communication). A force-torque sensor is exploited at kinematic level to help the insertion phase. The results show how the TPIK and the chosen cooperation scheme can be used for the stated problem. The simulated experiments done consider little errors in the hole's pose, that still permit to insert the peg but with a lot of frictions and possible stucks. It is shown how can be possible to improve (thanks to the data provided by the force-torque sensor) the insertion phase performed by the two manipulators in presence of these errors. [...]</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 4.1 -->
                    
                <!-- Math: 3.9 -->
                    
                <!-- Reinforcement Learning: 3.5 -->
                    
                <!-- Medicine: 2.9 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Robotics: 2.2 -->
                    
                <!-- LLMs: 1.9 -->
                    
                <!-- Pathfinding: 1.9 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- GNN: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07451" target="_blank" rel="noopener noreferrer">Optimal Low Emission Zones scheduling as an example of transport policy backcasting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Asmae Alami, Vinith Lakshmanan, Antonio Sciarretta
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This study presents a backcasting approach that considers the passenger car fleet dynamics to determine optimal policy roadmaps in transport systems. As opposed to the scenario-based approach, backcasting sets emission reduction targets first, then identifies policies that meet the constraint. The p</span>
                
                <span class="abstract-full" style="display: none;">This study presents a backcasting approach that considers the passenger car fleet dynamics to determine optimal policy roadmaps in transport systems. As opposed to the scenario-based approach, backcasting sets emission reduction targets first, then identifies policies that meet the constraint. The policy is the implementation of Low Emission Zones (LEZs), in the Ile-de-France region as a case study. The aim is to minimize the number of scrapped vehicles due to LEZs under CO2 emission targets and to deduce an interdiction schedule of polluting vehicles by 2050. To explore potential solutions, we use a genetic algorithm that provides a first insight into optimal policy pathways.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.6 -->
                    
                <!-- Networks: 4.3 -->
                    
                <!-- Reinforcement Learning: 3.9 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- Medicine: 2.5 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07503" target="_blank" rel="noopener noreferrer">Identifying Causal Direction via Variational Bayesian Compression</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Quang-Duy Tran, Bao Duong, Phuoc Nguyen, Thin Nguyen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Telling apart the cause and effect between two random variables with purely observational data is a challenging problem that finds applications in various scientific disciplines. A key principle utilized in this task is the algorithmic Markov condition, which postulates that the joint distribution, </span>
                
                <span class="abstract-full" style="display: none;">Telling apart the cause and effect between two random variables with purely observational data is a challenging problem that finds applications in various scientific disciplines. A key principle utilized in this task is the algorithmic Markov condition, which postulates that the joint distribution, when factorized according to the causal direction, yields a more succinct codelength compared to the anti-causal direction. Previous approaches approximate these codelengths by relying on simple functions or Gaussian processes (GPs) with easily evaluable complexity, compromising between model fitness and computational complexity. To overcome these limitations, we propose leveraging the variational Bayesian learning of neural networks as an interpretation of the codelengths. Consequently, we can enhance the model fitness while promoting the succinctness of the codelengths, while avoiding the significant computational complexity of the GP-based approaches. Extensive experiments on both synthetic and real-world benchmarks in cause-effect identification demonstrate the effectiveness of our proposed method, surpassing the overall performance of related complexity-based and structural causal model regression-based approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.9 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Math: 3.0 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Federated Learning: 2.1 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07515" target="_blank" rel="noopener noreferrer">Improved Mixing of Critical Hardcore Model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zongchen Chen, Tianhui Jiang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The hardcore model is one of the most classic and widely studied examples of undirected graphical models. Given a graph $G$, the hardcore model describes a Gibbs distribution of $\lambda$-weighted independent sets of $G$. In the last two decades, a beautiful computational phase transition has been e</span>
                
                <span class="abstract-full" style="display: none;">The hardcore model is one of the most classic and widely studied examples of undirected graphical models. Given a graph $G$, the hardcore model describes a Gibbs distribution of $\lambda$-weighted independent sets of $G$. In the last two decades, a beautiful computational phase transition has been established at a precise threshold $\lambda_c(\Delta)$ where $\Delta$ denotes the maximum degree, where the task of sampling independent sets transfers from polynomial-time solvable to computationally intractable. We study the critical hardcore model where $\lambda = \lambda_c(\Delta)$ and show that the Glauber dynamics, a simple yet popular Markov chain algorithm, mixes in $\tilde{O}(n^{7.44 + O(1/\Delta)})$ time on any $n$-vertex graph of maximum degree $\Delta\geq3$, significantly improving the previous upper bound $\tilde{O}(n^{12.88+O(1/\Delta)})$ by the recent work arXiv:2411.03413. The core property we establish in this work is that the critical hardcore model is $O(\sqrt{n})$-spectrally independent, improving the trivial bound of $n$ and matching the critical behavior of the Ising model. Our proof approach utilizes an online decision-making framework to study a site percolation model on the infinite $(\Delta-1)$-ary tree, which can be interesting by itself.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.8 -->
                    
                <!-- Reinforcement Learning: 3.9 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Math: 3.2 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Pathfinding: 1.4 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07575" target="_blank" rel="noopener noreferrer">Personalized Federated Learning under Model Dissimilarity Constraints</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Samuel Erickson, Mikael Johansson
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">One of the defining challenges in federated learning is that of statistical heterogeneity among clients. We address this problem with KARULA, a regularized strategy for personalized federated learning, which constrains the pairwise model dissimilarities between clients based on the difference in the</span>
                
                <span class="abstract-full" style="display: none;">One of the defining challenges in federated learning is that of statistical heterogeneity among clients. We address this problem with KARULA, a regularized strategy for personalized federated learning, which constrains the pairwise model dissimilarities between clients based on the difference in their distributions, as measured by a surrogate for the 1-Wasserstein distance adapted for the federated setting. This allows the strategy to adapt to highly complex interrelations between clients, that e.g., clustered approaches fail to capture. We propose an inexact projected stochastic gradient algorithm to solve the constrained problem that the strategy defines, and show theoretically that it converges with smooth, possibly non-convex losses to a neighborhood of a stationary point with rate O(1/K). We demonstrate the effectiveness of KARULA on synthetic and real federated data sets.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.0 -->
                    
                <!-- Medicine: 3.5 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07579" target="_blank" rel="noopener noreferrer">Dynamic Rental Games with Stagewise Individual Rationality</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Batya Berzack, Rotem Oshman, Inbal Talgam-Cohen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We study \emph{rental games} -- a single-parameter dynamic mechanism design problem, in which a designer rents out an indivisible asset over $n$ days. Each day, an agent arrives with a private valuation per day of rental, drawn from that day's (known) distribution. The designer can either rent out t</span>
                
                <span class="abstract-full" style="display: none;">We study \emph{rental games} -- a single-parameter dynamic mechanism design problem, in which a designer rents out an indivisible asset over $n$ days. Each day, an agent arrives with a private valuation per day of rental, drawn from that day's (known) distribution. The designer can either rent out the asset to the current agent for any number of remaining days, charging them a (possibly different) payment per day, or turn the agent away. Agents who arrive when the asset is not available are turned away. A defining feature of our dynamic model is that agents are \emph{stagewise-IR} (individually rational), meaning they reject any rental agreement that results in temporary negative utility, even if their final utility is positive. We ask whether and under which economic objectives it is useful for the designer to exploit the stagewise-IR nature of the agents.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.3 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Networks: 3.6 -->
                    
                <!-- Math: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07704" target="_blank" rel="noopener noreferrer">Through the Looking Glass: Common Sense Consistency Evaluation of Weird Images</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Elisei Rykov, Kseniia Petrushina, Kseniia Titova, Anton Razzhigaev, Alexander Panchenko, Vasily Konovalov
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Lar</span>
                
                <span class="abstract-full" style="display: none;">Measuring how real images look is a complex task in artificial intelligence research. For example, an image of a boy with a vacuum cleaner in a desert violates common sense. We introduce a novel method, which we call Through the Looking Glass (TLG), to assess image common sense consistency using Large Vision-Language Models (LVLMs) and Transformer-based encoder. By leveraging LVLMs to extract atomic facts from these images, we obtain a mix of accurate facts. We proceed by fine-tuning a compact attention-pooling classifier over encoded atomic facts. Our TLG has achieved a new state-of-the-art performance on the WHOOPS! and WEIRD datasets while leveraging a compact fine-tuning component.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.8 -->
                    
                <!-- LLMs: 4.6 -->
                    
                <!-- Networks: 3.7 -->
                    
                <!-- 3D: 3.2 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07750" target="_blank" rel="noopener noreferrer">The Pitfalls of Benchmarking in Algorithm Selection: What We Are Getting Wrong</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ga\v{s}per Petelin, Gjorgjina Cenikj
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Algorithm selection, aiming to identify the best algorithm for a given problem, plays a pivotal role in continuous black-box optimization. A common approach involves representing optimization functions using a set of features, which are then used to train a machine learning meta-model for selecting </span>
                
                <span class="abstract-full" style="display: none;">Algorithm selection, aiming to identify the best algorithm for a given problem, plays a pivotal role in continuous black-box optimization. A common approach involves representing optimization functions using a set of features, which are then used to train a machine learning meta-model for selecting suitable algorithms. Various approaches have demonstrated the effectiveness of these algorithm selection meta-models. However, not all evaluation approaches are equally valid for assessing the performance of meta-models. We highlight methodological issues that frequently occur in the community and should be addressed when evaluating algorithm selection approaches. First, we identify flaws with the "leave-instance-out" evaluation technique. We show that non-informative features and meta-models can achieve high accuracy, which should not be the case with a well-designed evaluation framework. Second, we demonstrate that measuring the performance of optimization algorithms with metrics sensitive to the scale of the objective function requires careful consideration of how this impacts the construction of the meta-model, its predictions, and the model's error. Such metrics can falsely present overly optimistic performance assessments of the meta-models. This paper emphasizes the importance of careful evaluation, as loosely defined methodologies can mislead researchers, divert efforts, and introduce noise into the field</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Math: 4.9 -->
                    
                <!-- LLMs: 4.8 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.8 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07780" target="_blank" rel="noopener noreferrer">Formal P-Category Theory and Normalization by Evaluation in Rocq</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: David G. Berry, Marcelo P. Fiore
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Traditional category theory is typically based on set-theoretic principles and ideas, which are often non-constructive. An alternative approach to formalizing category theory is to use E-category theory, where hom sets become setoids. Our work reconsiders a third approach - P-category theory - from </span>
                
                <span class="abstract-full" style="display: none;">Traditional category theory is typically based on set-theoretic principles and ideas, which are often non-constructive. An alternative approach to formalizing category theory is to use E-category theory, where hom sets become setoids. Our work reconsiders a third approach - P-category theory - from \v{C}ubri\'c et al. (1998) emphasizing a computational standpoint. We formalize in Rocq a modest library of P-category theory - where homs become subsetoids - and apply it to formalizing algorithms for normalization by evaluation which are purely categorical but, surprisingly, do not use neutral and normal terms. \v{C}ubri\'c et al. (1998) establish only a soundness correctness property by categorical means; here, we extend their work by providing a categorical proof also for a strong completeness property. For this we formalize the full universal property of the free Cartesian-closed category, which is not known to have been performed before. We further formalize a novel universal property of unquotiented simply typed lambda-calculus syntax and apply this to a proof of correctness of a categorical normalization by evaluation algorithm. We pair the overall mathematical development with a formalization in the Rocq proof assistant, following the principle that the formalization exists for practical computation. Indeed, it permits extraction of synthesized normalization programs that compute (long) beta-eta-normal forms of simply typed lambda-terms together with a derivation of beta-eta-conversion.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.6 -->
                    
                <!-- Networks: 3.3 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07812" target="_blank" rel="noopener noreferrer">Continuous Visual Autoregressive Generation via Score Maximization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chenze Shao, Fandong Meng, Jie Zhou
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce si</span>
                
                <span class="abstract-full" style="display: none;">Conventional wisdom suggests that autoregressive models are used to process discrete data. When applied to continuous modalities such as visual data, Visual AutoRegressive modeling (VAR) typically resorts to quantization-based approaches to cast the data into a discrete space, which can introduce significant information loss. To tackle this issue, we introduce a Continuous VAR framework that enables direct visual autoregressive generation without vector quantization. The underlying theoretical foundation is strictly proper scoring rules, which provide powerful statistical tools capable of evaluating how well a generative model approximates the true distribution. Within this framework, all we need is to select a strictly proper score and set it as the training objective to optimize. We primarily explore a class of training objectives based on the energy score, which is likelihood-free and thus overcomes the difficulty of making probabilistic predictions in the continuous space. Previous efforts on continuous autoregressive generation, such as GIVT and diffusion loss, can also be derived from our framework using other strictly proper scores. Source code: https://github.com/shaochenze/EAR.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.9 -->
                    
                <!-- Medicine: 3.1 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- GNN: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.6 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07816" target="_blank" rel="noopener noreferrer">A class of distributed automata that contains the modal mu-fragment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Veeti Ahvonen, Damian Heiman, Antti Kuusisto
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper gives a translation from the $\mu$-fragment of the graded modal $\mu$-calculus to a class of distributed message-passing automata. As a corollary, we obtain an alternative proof for a theorem from \cite{ahvonen_neurips} stating that recurrent graph neural networks working with reals and g</span>
                
                <span class="abstract-full" style="display: none;">This paper gives a translation from the $\mu$-fragment of the graded modal $\mu$-calculus to a class of distributed message-passing automata. As a corollary, we obtain an alternative proof for a theorem from \cite{ahvonen_neurips} stating that recurrent graph neural networks working with reals and graded modal substitution calculus have the same expressive power in restriction to the logic monadic second-order logic MSO.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.1 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Networks: 3.3 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2409.04300" target="_blank" rel="noopener noreferrer">Equivariant Machine Learning Decoder for 3D Toric Codes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Oliver Weissl, Evgenii Egorov
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Mitigating errors in computing and communication systems has seen a great deal of research since the beginning of the widespread use of these technologies. However, as we develop new methods to do computation or communication, we also need to reiterate the method used to deal with errors. Within the</span>
                
                <span class="abstract-full" style="display: none;">Mitigating errors in computing and communication systems has seen a great deal of research since the beginning of the widespread use of these technologies. However, as we develop new methods to do computation or communication, we also need to reiterate the method used to deal with errors. Within the field of quantum computing, error correction is getting a lot of attention since errors can propagate fast and invalidate results, which makes the theoretical exponential speed increase in computation time, compared to traditional systems, obsolete. To correct errors in quantum systems, error-correcting codes are used. A subgroup of codes, topological codes, is currently the focus of many research papers. Topological codes represent parity check matrices corresponding to graphs embedded on a $d$-dimensional surface. For our research, the focus lies on the toric code with a 3D square lattice. The goal of any decoder is robustness to noise, which can increase with code size. However, a reasonable decoder performance scales polynomially with lattice size. As error correction is a time-sensitive operation, we propose a neural network using an inductive bias: equivariance. This allows the network to learn from a rather small subset of the exponentially growing training space of possible inputs. In addition, we investigate how transformer networks can help in correction. These methods will be compared with various configurations and previously published methods of decoding errors in the 3D toric code.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.0 -->
                    
                <!-- Quantum Computing: 3.6 -->
                    
                <!-- Networks: 3.3 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06442" target="_blank" rel="noopener noreferrer">A linear-time algorithm to compute the conjugate of nonconvex bivariate piecewise linear-quadratic functions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tanmaya Karmarkar, Yves Lucet
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose the first linear-time algorithm to compute the conjugate of (nonconvex) bivariate piecewise linear-quadratic (PLQ) functions (bivariate quadratic functions defined on a polyhedral subdivision). Our algorithm starts with computing the convex envelope of each quadratic piece obtaining ratio</span>
                
                <span class="abstract-full" style="display: none;">We propose the first linear-time algorithm to compute the conjugate of (nonconvex) bivariate piecewise linear-quadratic (PLQ) functions (bivariate quadratic functions defined on a polyhedral subdivision). Our algorithm starts with computing the convex envelope of each quadratic piece obtaining rational functions (quadratic over linear) defined over a polyhedral subdivision. Then we compute the conjugate of each resulting piece to obtain piecewise quadratic functions defined over a parabolic subdivision. Finally we compute the maximum of all those functions to obtain the conjugate as a piecewise quadratic function defined on a parabolic subdivision. The resulting algorithm runs in linear time if the initial subdivision is a triangulation (or has a uniform upper bound on the number of vertexes for each piece).</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Networks: 4.3 -->
                    
                <!-- Reinforcement Learning: 4.2 -->
                    
                <!-- Math: 4.0 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Medicine: 2.4 -->
                    
                <!-- Pathfinding: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06756" target="_blank" rel="noopener noreferrer">Out-of-Sample Embedding with Proximity Data: Projection versus Restricted Reconstruction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Michael W. Trosset, Kaiyi Tan, Minh Tang, Carey E. Priebe
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The problem of using proximity (similarity or dissimilarity) data for the purpose of "adding a point to a vector diagram" was first studied by J.C. Gower in 1968. Since then, a number of methods -- mostly kernel methods -- have been proposed for solving what has come to be called the problem of *out</span>
                
                <span class="abstract-full" style="display: none;">The problem of using proximity (similarity or dissimilarity) data for the purpose of "adding a point to a vector diagram" was first studied by J.C. Gower in 1968. Since then, a number of methods -- mostly kernel methods -- have been proposed for solving what has come to be called the problem of *out-of-sample embedding*. We survey the various kernel methods that we have encountered and show that each can be derived from one or the other of two competing strategies: *projection* or *restricted reconstruction*. Projection can be analogized to a well-known formula for adding a point to a principal component analysis. Restricted reconstruction poses a different challenge: how to best approximate redoing the entire multivariate analysis while holding fixed the vector diagram that was previously obtained. This strategy results in a nonlinear optimization problem that can be simplified to a unidimensional search. Various circumstances may warrant either projection or restricted reconstruction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 4.6 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Medicine: 3.3 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06800" target="_blank" rel="noopener noreferrer">Reverse-BSDE Monte Carlo</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jairon H. N. Batista, Fl\'avio B. Gon\c{c}alves, Yuri F. Saporito, Rodrigo S. Targino
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recently, there has been a growing interest in generative models based on diffusions driven by the empirical robustness of these methods in generating high-dimensional photorealistic images and the possibility of using the vast existing toolbox of stochastic differential equations. %This remarkable </span>
                
                <span class="abstract-full" style="display: none;">Recently, there has been a growing interest in generative models based on diffusions driven by the empirical robustness of these methods in generating high-dimensional photorealistic images and the possibility of using the vast existing toolbox of stochastic differential equations. %This remarkable ability may stem from their capacity to model and generate multimodal distributions. In this work, we offer a novel perspective on the approach introduced in Song et al. (2021), shifting the focus from a "learning" problem to a "sampling" problem. To achieve this, we reformulate the equations governing diffusion-based generative models as a Forward-Backward Stochastic Differential Equation (FBSDE), which avoids the well-known issue of pre-estimating the gradient of the log target density. The solution of this FBSDE is proved to be unique using non-standard techniques. Additionally, we propose a numerical solution to this problem, leveraging on Deep Learning techniques. This reformulation opens new pathways for sampling multidimensional distributions with densities known up to a normalization constant, a problem frequently encountered in Bayesian statistics.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.3 -->
                    
                <!-- Medicine: 3.6 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.06934" target="_blank" rel="noopener noreferrer">Whitened CLIP as a Likelihood Surrogate of Images and Captions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Roy Betser, Meir Yossef Levi, Guy Gilboa
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Likelihood approximations for images are not trivial to compute and can be useful in many applications. We examine the use of Contrastive Language-Image Pre-training (CLIP) to assess the likelihood of images and captions. We introduce \textit{Whitened CLIP}, a novel transformation of the CLIP latent</span>
                
                <span class="abstract-full" style="display: none;">Likelihood approximations for images are not trivial to compute and can be useful in many applications. We examine the use of Contrastive Language-Image Pre-training (CLIP) to assess the likelihood of images and captions. We introduce \textit{Whitened CLIP}, a novel transformation of the CLIP latent space via an invertible linear operation. This transformation ensures that each feature in the embedding space has zero mean, unit standard deviation, and no correlation with all other features, resulting in an identity covariance matrix. We show that the whitened embeddings statistics can be well approximated as a standard normal distribution, thus, the log-likelihood is estimated simply by the square Euclidean norm in the whitened embedding space. The whitening procedure is completely training-free and performed using a pre-computed whitening matrix, hence, is very fast. We present several preliminary experiments demonstrating the properties and applicability of these likelihood scores to images and captions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.5 -->
                    
                <!-- Medicine: 4.2 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07014" target="_blank" rel="noopener noreferrer">When cardinals strategize: An agent-based model of influence and ideology for the papal conclave</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Nuno Crokidakis
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose and analyze two agent-based models to investigate the dynamics of papal conclaves, focusing on how social influence, strategic voting, and ideological alignment affect the time required to elect a pope. In the first model, cardinals interact through two mechanisms: with probability $p$, t</span>
                
                <span class="abstract-full" style="display: none;">We propose and analyze two agent-based models to investigate the dynamics of papal conclaves, focusing on how social influence, strategic voting, and ideological alignment affect the time required to elect a pope. In the first model, cardinals interact through two mechanisms: with probability $p$, they imitate the choice of a randomly selected peer, and with probability $q$, they shift support to the most voted candidate from the previous round. Additionally, strategic behavior is introduced via ``useful voting'', where agents abandon their preferred candidate if he receives less than a certain fraction of the votes, switching instead to the most viable alternative. A candidate must secure a qualified majority of two-thirds to be elected. After that, we extend the framework by incorporating ideological blocs, assigning each cardinal and candidate to one of two groups (e.g., progressives and conservatives). Cardinals initially vote for candidates from their own group, but may cross ideological lines due to strategic considerations. We initialize the population with $20\%$ conservative cardinals, reflecting the current composition shaped by papal appointments. Numerical simulations show that ideological polarization can delay the election, increasing the average number of voting rounds. Our results highlight the crucial role of both strategic flexibility and ideological structure in collective decision-making under conditions found in papal conclaves. The recent rapid outcome of the 2025 conclave, despite a polarized electorate, suggests that informal consensus-building - possibly prior to voting - may play a decisive role in accelerating convergence, complementing the mechanisms explored in our model.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.2 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- GNN: 1.0 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07068" target="_blank" rel="noopener noreferrer">A Sparse Bayesian Learning Algorithm for Estimation of Interaction Kernels in Motsch-Tadmor Model</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jinchao Feng, Sui Tang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we investigate the data-driven identification of asymmetric interaction kernels in the Motsch-Tadmor model based on observed trajectory data. The model under consideration is governed by a class of semilinear evolution equations, where the interaction kernel defines a normalized, stat</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we investigate the data-driven identification of asymmetric interaction kernels in the Motsch-Tadmor model based on observed trajectory data. The model under consideration is governed by a class of semilinear evolution equations, where the interaction kernel defines a normalized, state-dependent Laplacian operator that governs collective dynamics. To address the resulting nonlinear inverse problem, we propose a variational framework that reformulates kernel identification using the implicit form of the governing equations, reducing it to a subspace identification problem. We establish an identifiability result that characterizes conditions under which the interaction kernel can be uniquely recovered up to scale. To solve the inverse problem robustly, we develop a sparse Bayesian learning algorithm that incorporates informative priors for regularization, quantifies uncertainty, and enables principled model selection. Extensive numerical experiments on representative interacting particle systems demonstrate the accuracy, robustness, and interpretability of the proposed framework across a range of noise levels and data regimes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- Reinforcement Learning: 3.9 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Federated Learning: 2.0 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07287" target="_blank" rel="noopener noreferrer">Learning Quasi-LPV Models and Robust Control Invariant Sets with Reduced Conservativeness</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sampath Kumar Mulagaleti, Alberto Bemporad
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present an approach to identify a quasi Linear Parameter Varying (qLPV) model of a plant, with the qLPV model guaranteed to admit a robust control invariant (RCI) set. It builds upon the concurrent synthesis framework presented in [1], in which the requirement of existence of an RCI set is modele</span>
                
                <span class="abstract-full" style="display: none;">We present an approach to identify a quasi Linear Parameter Varying (qLPV) model of a plant, with the qLPV model guaranteed to admit a robust control invariant (RCI) set. It builds upon the concurrent synthesis framework presented in [1], in which the requirement of existence of an RCI set is modeled as a control-oriented regularization. Here, we reduce the conservativeness of the approach by bounding the qLPV system with an uncertain LTI system, which we derive using bound propagation approaches. The resulting regularization function is the optimal value of a nonlinear robust optimization problem that we solve via a differentiable algorithm. We numerically demonstrate the benefits of the proposed approach over two benchmark approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.7 -->
                    
                <!-- Networks: 4.6 -->
                    
                <!-- Medicine: 4.0 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Math: 3.1 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07434" target="_blank" rel="noopener noreferrer">Efficient Lifting of Discrete Logarithms Modulo Prime Powers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Giovanni Viglietta, Yasuyuki Kachi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a deterministic algorithm that, given a prime $p$ and a solution $x \in \mathbb Z$ to the discrete logarithm problem $a^x \equiv b \pmod p$ with $p\nmid a$, efficiently lifts it to a solution modulo $p^k$, i.e., $a^x \equiv b \pmod {p^k}$, for any fixed $k \geq 1$.</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- LLMs: 4.7 -->
                    
                <!-- Networks: 4.1 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- 3D: 3.0 -->
                    
                <!-- GNN: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07545" target="_blank" rel="noopener noreferrer">Congestion-Sensitive Grid Aggregation for DC Optimal Power Flow</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Benjamin St\"ockl, Yannick Werner, Sonja Wogrin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The vast spatial dimension of modern interconnected electricity grids challenges the tractability of the DC optimal power flow problem. Grid aggregation methods try to overcome this challenge by reducing the number of network elements. Many existing methods use Locational Marginal Prices as a distan</span>
                
                <span class="abstract-full" style="display: none;">The vast spatial dimension of modern interconnected electricity grids challenges the tractability of the DC optimal power flow problem. Grid aggregation methods try to overcome this challenge by reducing the number of network elements. Many existing methods use Locational Marginal Prices as a distance metric to cluster nodes. In this paper, we show that prevalent methods adopting this distance metric fail to adequately capture the impact of individual lines when there is more than one line congested. This leads to suboptimal outcomes for the optimization of the aggregated model. To overcome those issues, we propose two methods based on the novel Network Congestion Price metric, which preserves the impact of nodal power injections on individual line congestions. The proposed methods are compared to several existing aggregation methods based on Locational Marginal Prices. We demonstrate all methods on adapted versions of the IEEE RTS 24- and 300-Bus systems. We show that the proposed methods outperform existing approaches both in terms of objective function value error and maximum line limit violation, while exhibiting faster node clustering. We conclude that aggregation methods based on the novel Network Congestion Price metric are better at preserving the essential physical characteristics of the network topology in the grid aggregation process than methods based on Locational Marginal Prices.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.7 -->
                    
                <!-- LLMs: 3.9 -->
                    
                <!-- Math: 3.5 -->
                    
                <!-- Medicine: 3.0 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Federated Learning: 2.3 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- SpikingNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07640" target="_blank" rel="noopener noreferrer">Certified Data Removal Under High-dimensional Settings</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haolin Zou, Arnab Auddy, Yongchan Kwon, Kamiar Rahnama Rad, Arian Maleki
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Machine unlearning focuses on the computationally efficient removal of specific training data from trained models, ensuring that the influence of forgotten data is effectively eliminated without the need for full retraining. Despite advances in low-dimensional settings, where the number of parameter</span>
                
                <span class="abstract-full" style="display: none;">Machine unlearning focuses on the computationally efficient removal of specific training data from trained models, ensuring that the influence of forgotten data is effectively eliminated without the need for full retraining. Despite advances in low-dimensional settings, where the number of parameters \( p \) is much smaller than the sample size \( n \), extending similar theoretical guarantees to high-dimensional regimes remains challenging. We propose an unlearning algorithm that starts from the original model parameters and performs a theory-guided sequence of Newton steps \( T \in \{ 1,2\}\). After this update, carefully scaled isotropic Laplacian noise is added to the estimate to ensure that any (potential) residual influence of forget data is completely removed. We show that when both \( n, p \to \infty \) with a fixed ratio \( n/p \), significant theoretical and computational obstacles arise due to the interplay between the complexity of the model and the finite signal-to-noise ratio. Finally, we show that, unlike in low-dimensional settings, a single Newton step is insufficient for effective unlearning in high-dimensional problems -- however, two steps are enough to achieve the desired certifiebility. We provide numerical experiments to support the certifiability and accuracy claims of this approach.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.7 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Math: 3.1 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07754" target="_blank" rel="noopener noreferrer">Skeletonization of neuronal processes using Discrete Morse techniques from computational topology</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Samik Banerjee, Caleb Stam, Daniel J. Tward, Steven Savoia, Yusu Wang, Partha P. Mitra
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">To understand biological intelligence we need to map neuronal networks in vertebrate brains. Mapping mesoscale neural circuitry is done using injections of tracers that label groups of neurons whose axons project to different brain regions. Since many neurons are labeled, it is difficult to follow i</span>
                
                <span class="abstract-full" style="display: none;">To understand biological intelligence we need to map neuronal networks in vertebrate brains. Mapping mesoscale neural circuitry is done using injections of tracers that label groups of neurons whose axons project to different brain regions. Since many neurons are labeled, it is difficult to follow individual axons. Previous approaches have instead quantified the regional projections using the total label intensity within a region. However, such a quantification is not biologically meaningful. We propose a new approach better connected to the underlying neurons by skeletonizing labeled axon fragments and then estimating a volumetric length density. Our approach uses a combination of deep nets and the Discrete Morse (DM) technique from computational topology. This technique takes into account nonlocal connectivity information and therefore provides noise-robustness. We demonstrate the utility and scalability of the approach on whole-brain tracer injected data. We also define and illustrate an information theoretic measure that quantifies the additional information obtained, compared to the skeletonized tracer injection fragments, when individual axon morphologies are available. Our approach is the first application of the DM technique to computational neuroanatomy. It can help bridge between single-axon skeletons and tracer injections, two important data types in mapping neural networks in vertebrates.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 4.2 -->
                    
                <!-- Medicine: 3.6 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2505.07778" target="_blank" rel="noopener noreferrer">Schrijver's $\vartheta$-function is not an upper bound on the Shannon capacity of a graph: a counterexample</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Igal Sason
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This note addresses an open question concerning a variant of the Lov\'{a}sz $\vartheta$-function, introduced by Schrijver and independently by McEliece et al. (1978). It provides a complete and detailed presentation of a counterexample demonstrating that this variant does not universally upper bound</span>
                
                <span class="abstract-full" style="display: none;">This note addresses an open question concerning a variant of the Lov\'{a}sz $\vartheta$-function, introduced by Schrijver and independently by McEliece et al. (1978). It provides a complete and detailed presentation of a counterexample demonstrating that this variant does not universally upper bound the Shannon capacity of a graph, in contrast to the Lov\'{a}sz $\vartheta$-function. The counterexample, previously introduced in Example 5.24 of a recent paper by the author, entitled: Observations on graph invariants with the Lov\'{a}sz $\vartheta$-function (AIMS Mathematics, vol. 9, pp. 15385--15468, April 2024, https://doi.org/10.3934/math.2024747), is revisited and fully detailed here. By resolving this question, the note clarifies a subtle but significant distinction between these two closely related graph invariants.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 3.8 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Networks: 3.4 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2101.09306" target="_blank" rel="noopener noreferrer">Towards Optimal Branching of Linear and Semidefinite Relaxations for Neural Network Robustness Certification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Brendon G. Anderson, Ziye Ma, Jingqi Li, Somayeh Sojoudi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we study certifying the robustness of ReLU neural networks against adversarial input perturbations. To diminish the relaxation error suffered by the popular linear programming (LP) and semidefinite programming (SDP) certification methods, we take a branch-and-bound approach to propose</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we study certifying the robustness of ReLU neural networks against adversarial input perturbations. To diminish the relaxation error suffered by the popular linear programming (LP) and semidefinite programming (SDP) certification methods, we take a branch-and-bound approach to propose partitioning the input uncertainty set and solving the relaxations on each part separately. We show that this approach reduces relaxation error, and that the error is eliminated entirely upon performing an LP relaxation with a partition intelligently designed to exploit the nature of the ReLU activations. To scale this approach to large networks, we consider using a coarser partition whereby the number of parts in the partition is reduced. We prove that computing such a coarse partition that directly minimizes the LP relaxation error is NP-hard. By instead minimizing the worst-case LP relaxation error, we develop a closed-form branching scheme in the single-hidden layer case. We extend the analysis to the SDP, where the feasible set geometry is exploited to design a branching scheme that minimizes the worst-case SDP relaxation error. Experiments on MNIST, CIFAR-10, and Wisconsin breast cancer diagnosis classifiers demonstrate significant increases in the percentages of test samples certified. By independently increasing the input size and the number of layers, we empirically illustrate under which regimes the branched LP and branched SDP are best applied. Finally, we extend our LP branching method into a multi-layer branching heuristic, which attains comparable performance to prior state-of-the-art heuristics on large-scale, deep neural network certification benchmarks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 4.5 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Math: 3.2 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- LLMs: 2.4 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2102.12345" target="_blank" rel="noopener noreferrer">Automated Fuzzing of Automotive Control Units</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Timothy Werquin, Roos Hubrechtsen, Ashok Thangarajan, Frank Piessens, Jan Tobias Muehlberg
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modern vehicles are governed by a network of Electronic Control Units (ECUs), which are programmed to sense inputs from the driver and the environment, to process these inputs, and to control actuators that, e.g., regulate the engine or even control the steering system. ECUs within a vehicle communi</span>
                
                <span class="abstract-full" style="display: none;">Modern vehicles are governed by a network of Electronic Control Units (ECUs), which are programmed to sense inputs from the driver and the environment, to process these inputs, and to control actuators that, e.g., regulate the engine or even control the steering system. ECUs within a vehicle communicate via automotive bus systems such as the Controller Area Network (CAN), and beyond the vehicles boundaries through upcoming vehicle-to-vehicle and vehicle-to-infrastructure channels. Approaches to manipulate the communication between ECUs for the purpose of security testing and reverse-engineering of vehicular functions have been presented in the past, all of which struggle with automating the detection of system change in response to message injection. In this paper we present our findings with fuzzing CAN networks, in particular while observing individual ECUs with a sensor harness. The harness detects physical responses, which we then use in a oracle functions to inform the fuzzing process. We systematically define fuzzers, fuzzing configurations and oracle functions for testing ECUs. We evaluate our approach based on case studies of commercial instrument clusters and with an experimental framework for CAN authentication. Our results show that the approach is capable of identifying interesting ECU states with a high level of automation. Our approach is applicable in distributed cyber-physical systems beyond automotive computing.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.0 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2110.14734" target="_blank" rel="noopener noreferrer">Approximating 1-Wasserstein Distance between Persistence Diagrams by Graph Sparsification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tamal K. Dey, Simon Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Persistence diagrams (PD)s play a central role in topological data analysis. This analysis requires computing distances among such diagrams such as the $1$-Wasserstein distance. Accurate computation of these PD distances for large data sets that render large diagrams may not scale appropriately with</span>
                
                <span class="abstract-full" style="display: none;">Persistence diagrams (PD)s play a central role in topological data analysis. This analysis requires computing distances among such diagrams such as the $1$-Wasserstein distance. Accurate computation of these PD distances for large data sets that render large diagrams may not scale appropriately with the existing methods. The main source of difficulty ensues from the size of the bipartite graph on which a matching needs to be computed for determining these PD distances. We address this problem by making several algorithmic and computational observations in order to obtain, in theory, a near-linear fully polynomial-time approximation scheme. This is theoretically optimal assuming the $(1+\epsilon)$-approximate EMD conjecture in constant dimension, which is that the EMD problem on the plane cannot be approximated by a PTAS in time $O(\frac{1}{\epsilon^2}n)$ up to polylog factors. In our implementation, first, taking advantage of the distribution of PD points, we \emph{condense} them thereby decreasing the number of nodes in the graph for computation. The increase in point multiplicities is addressed by reducing the matching problem to a min-cost flow problem on a transshipment network. Second, we use Well Separated Pair Decomposition to sparsify the graph to a size that is linear in the number of points. Both node and arc sparsifications contribute to the approximation factor where we leverage a lower bound given by the Relaxed Word Mover's distance. Third, we eliminate bottlenecks during the sparsification procedure by introducing parallelism. Fourth, we develop an open source software called PDoptFlow based on our algorithm, exploiting parallelism by GPU and multicore. We perform extensive experiments and show that the actual empirical error is very low. We also show that we can achieve high performance at low guaranteed relative errors, improving upon the state of the arts.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Math: 3.2 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Medicine: 2.0 -->
                    
                <!-- Pathfinding: 1.9 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-neutral">
                    0.0
                </span>
                <a href="https://arxiv.org/abs/2304.03057" target="_blank" rel="noopener noreferrer">Distributed formation-enforcing control for UAVs robust to observation noise in relative pose measurements</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Viktor Walter, Matou\v{s} Vrba, Daniel Bonilla Licea, Matej Hilmer, Martin Saska
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">A technique that allows a Formation-Enforcing Control (FEC) derived from graph rigidity theory to interface with a realistic relative localization system onboard lightweight Unmanned Aerial Vehicles (UAVs) is proposed in this paper. The proposed methodology enables reliable real-world deployment of </span>
                
                <span class="abstract-full" style="display: none;">A technique that allows a Formation-Enforcing Control (FEC) derived from graph rigidity theory to interface with a realistic relative localization system onboard lightweight Unmanned Aerial Vehicles (UAVs) is proposed in this paper. The proposed methodology enables reliable real-world deployment of UAVs in tight formations using relative localization systems burdened by non-negligible sensory noise, which is typically not fully taken into account in FEC algorithms. The proposed solution is based on decomposition of the gradient descent-based FEC command into interpretable elements, and then modifying these individually based on the estimated distribution of sensory noise, such that the resulting action limits the probability of the desired formation. The behavior of the system was analyzed and the practicality of the proposed solution was compared to pure gradient-descent in real-world experiments where it presented significantly better performance in terms of oscillations, deviation from the desired state and convergence time.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 4.9 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -0.1366
                </span>
                <a href="https://arxiv.org/abs/2502.14908" target="_blank" rel="noopener noreferrer">SegSub: Evaluating Robustness to Knowledge Conflicts and Hallucinations in Vision-Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Peter Carragher, Nikitha Rao, Abhinand Jha, R Raghav, Kathleen M. Carley
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Vision language models (VLM) demonstrate sophisticated multimodal reasoning yet are prone to hallucination when confronted with knowledge conflicts, impeding their deployment in information-sensitive contexts. While existing research addresses robustness in unimodal models, the multimodal domain lac</span>
                
                <span class="abstract-full" style="display: none;">Vision language models (VLM) demonstrate sophisticated multimodal reasoning yet are prone to hallucination when confronted with knowledge conflicts, impeding their deployment in information-sensitive contexts. While existing research addresses robustness in unimodal models, the multimodal domain lacks systematic investigation of cross-modal knowledge conflicts. This research introduces \segsub, a framework for applying targeted image perturbations to investigate VLM resilience against knowledge conflicts. Our analysis reveals distinct vulnerability patterns: while VLMs are robust to parametric conflicts (20% adherence rates), they exhibit significant weaknesses in identifying counterfactual conditions (<30% accuracy) and resolving source conflicts (<1% accuracy). Correlations between contextual richness and hallucination rate (r = -0.368, p = 0.003) reveal the kinds of images that are likely to cause hallucinations. Through targeted fine-tuning on our benchmark dataset, we demonstrate improvements in VLM knowledge conflict detection, establishing a foundation for developing hallucination-resilient multimodal systems in information-sensitive environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 15.0 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- RAG: 2.4 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- T2I: 1.6 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3216
                </span>
                <a href="https://arxiv.org/abs/2505.05101" target="_blank" rel="noopener noreferrer">MDE-Edit: Masked Dual-Editing for Multi-Object Image Editing via Diffusion Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hongyang Zhu, Haipeng Liu, Bo Fu, Yang Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multi-object editing aims to modify multiple objects or regions in complex scenes while preserving structural coherence. This task faces significant challenges in scenarios involving overlapping or interacting objects: (1) Inaccurate localization of target objects due to attention misalignment, lead</span>
                
                <span class="abstract-full" style="display: none;">Multi-object editing aims to modify multiple objects or regions in complex scenes while preserving structural coherence. This task faces significant challenges in scenarios involving overlapping or interacting objects: (1) Inaccurate localization of target objects due to attention misalignment, leading to incomplete or misplaced edits; (2) Attribute-object mismatch, where color or texture changes fail to align with intended regions due to cross-attention leakage, creating semantic conflicts (\textit{e.g.}, color bleeding into non-target areas). Existing methods struggle with these challenges: approaches relying on global cross-attention mechanisms suffer from attention dilution and spatial interference between objects, while mask-based methods fail to bind attributes to geometrically accurate regions due to feature entanglement in multi-object scenarios. To address these limitations, we propose a training-free, inference-stage optimization approach that enables precise localized image manipulation in complex multi-object scenes, named MDE-Edit. MDE-Edit optimizes the noise latent feature in diffusion models via two key losses: Object Alignment Loss (OAL) aligns multi-layer cross-attention with segmentation masks for precise object positioning, and Color Consistency Loss (CCL) amplifies target attribute attention within masks while suppressing leakage to adjacent regions. This dual-loss design ensures localized and coherent multi-object edits. Extensive experiments demonstrate that MDE-Edit outperforms state-of-the-art methods in editing accuracy and visual quality, offering a robust solution for complex multi-object image manipulation tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.1 -->
                    
                <!-- Medicine: 5.0 -->
                    
                <!-- 3D: 3.6 -->
                    
                <!-- GNN: 3.4 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- T2I: 2.7 -->
                    
                <!-- RAG: 2.4 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Attention: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3625
                </span>
                <a href="https://arxiv.org/abs/2505.06628" target="_blank" rel="noopener noreferrer">ACORN: Adaptive Contrastive Optimization for Safe and Robust Fine-Grained Robotic Manipulation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhongquan Zhou, Shuhao Li, Zixian Yue
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Embodied AI research has traditionally emphasized performance metrics such as success rate and cumulative reward, overlooking critical robustness and safety considerations that emerge during real-world deployment. In actual environments, agents continuously encounter unpredicted situations and distr</span>
                
                <span class="abstract-full" style="display: none;">Embodied AI research has traditionally emphasized performance metrics such as success rate and cumulative reward, overlooking critical robustness and safety considerations that emerge during real-world deployment. In actual environments, agents continuously encounter unpredicted situations and distribution shifts, causing seemingly reliable policies to experience catastrophic failures, particularly in manipulation tasks. To address this gap, we introduce four novel safety-centric metrics that quantify an agent's resilience to environmental perturbations. Building on these metrics, we present Adaptive Contrastive Optimization for Robust Manipulation (ACORN), a plug-and-play algorithm that enhances policy robustness without sacrificing performance. ACORN leverages contrastive learning to simultaneously align trajectories with expert demonstrations while diverging from potentially unsafe behaviors. Our approach efficiently generates informative negative samples through structured Gaussian noise injection, employing a double perturbation technique that maintains sample diversity while minimizing computational overhead. Comprehensive experiments across diverse manipulation environments validate ACORN's effectiveness, yielding improvements of up to 23% in safety metrics under disturbance compared to baseline methods. These findings underscore ACORN's significant potential for enabling reliable deployment of embodied agents in safety-critical real-world applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.7 -->
                    
                <!-- Medicine: 5.1 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- 3D: 3.1 -->
                    
                <!-- GNN: 2.8 -->
                    
                <!-- RAG: 2.3 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- T2I: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.3885
                </span>
                <a href="https://arxiv.org/abs/2505.07141" target="_blank" rel="noopener noreferrer">Terrain-aware Low Altitude Path Planning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yixuan Jia, Andrea Tagliabue, Navid Dadkhah Tehrani, Jonathan P. How
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this paper, we study the problem of generating low altitude path plans for nap-of-the-earth (NOE) flight in real time with only RGB images from onboard cameras and the vehicle pose. We propose a novel training method that combines behavior cloning and self-supervised learning that enables the lea</span>
                
                <span class="abstract-full" style="display: none;">In this paper, we study the problem of generating low altitude path plans for nap-of-the-earth (NOE) flight in real time with only RGB images from onboard cameras and the vehicle pose. We propose a novel training method that combines behavior cloning and self-supervised learning that enables the learned policy to outperform the policy trained with standard behavior cloning approach on this task. Simulation studies are performed on a custom canyon terrain.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.2 -->
                    
                <!-- Medicine: 5.3 -->
                    
                <!-- Reinforcement Learning: 4.3 -->
                    
                <!-- GNN: 2.7 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4181
                </span>
                <a href="https://arxiv.org/abs/2505.07035" target="_blank" rel="noopener noreferrer">Robust Movable-Antenna Position Optimization with Imperfect CSI for MISO Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Haifeng Ma, Weidong Mei, Xin Wei, Boyu Ning, Zhi Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Movable antenna (MA) technology has emerged as a promising solution for reconfiguring wireless channel conditions through local antenna movement within confined regions. Unlike previous works assuming perfect channel state information (CSI), this letter addresses the robust MA position optimization </span>
                
                <span class="abstract-full" style="display: none;">Movable antenna (MA) technology has emerged as a promising solution for reconfiguring wireless channel conditions through local antenna movement within confined regions. Unlike previous works assuming perfect channel state information (CSI), this letter addresses the robust MA position optimization problem under imperfect CSI conditions for a multiple-input single-output (MISO) MA system. Specifically, we consider two types of CSI errors: norm-bounded and randomly distributed errors, aiming to maximize the worst-case and non-outage received signal power, respectively. For norm-bounded CSI errors, we derive the worst-case received signal power in closed-form. For randomly distributed CSI errors, due to the intractability of the probabilistic constraints, we apply the Bernstein-type inequality to obtain a closed-form lower bound for the non-outage received signal power. Based on these results, we show the optimality of the maximum-ratio transmission for imperfect CSI in both scenarios and employ a graph-based algorithm to obtain the optimal MA positions. Numerical results show that our proposed scheme can even outperform other benchmark schemes implemented under perfect CSI conditions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.2 -->
                    
                <!-- Networks: 3.9 -->
                    
                <!-- Reinforcement Learning: 3.3 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Math: 2.1 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4887
                </span>
                <a href="https://arxiv.org/abs/2505.06835" target="_blank" rel="noopener noreferrer">Streaming Sliced Optimal Transport</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Khai Nguyen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Sliced optimal transport (SOT) or sliced Wasserstein (SW) distance is widely recognized for its statistical and computational scalability. In this work, we further enhance the computational scalability by proposing the first method for computing SW from sample streams, called \emph{streaming sliced </span>
                
                <span class="abstract-full" style="display: none;">Sliced optimal transport (SOT) or sliced Wasserstein (SW) distance is widely recognized for its statistical and computational scalability. In this work, we further enhance the computational scalability by proposing the first method for computing SW from sample streams, called \emph{streaming sliced Wasserstein} (Stream-SW). To define Stream-SW, we first introduce the streaming computation of the one-dimensional Wasserstein distance. Since the one-dimensional Wasserstein (1DW) distance has a closed-form expression, given by the absolute difference between the quantile functions of the compared distributions, we leverage quantile approximation techniques for sample streams to define the streaming 1DW distance. By applying streaming 1DW to all projections, we obtain Stream-SW. The key advantage of Stream-SW is its low memory complexity while providing theoretical guarantees on the approximation error. We demonstrate that Stream-SW achieves a more accurate approximation of SW than random subsampling, with lower memory consumption, in comparing Gaussian distributions and mixtures of Gaussians from streaming samples. Additionally, we conduct experiments on point cloud classification, point cloud gradient flows, and streaming change point detection to further highlight the favorable performance of Stream-SW.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.0 -->
                    
                <!-- LLMs: 4.2 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.4938
                </span>
                <a href="https://arxiv.org/abs/2411.11904" target="_blank" rel="noopener noreferrer">GeoGround: A Unified Large Vision-Language Model for Remote Sensing Visual Grounding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yue Zhou, Mengcheng Lan, Xiang Li, Litong Feng, Yiping Ke, Xue Jiang, Qingyun Li, Xue Yang, Wayne Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on hori</span>
                
                <span class="abstract-full" style="display: none;">Remote sensing (RS) visual grounding aims to use natural language expression to locate specific objects (in the form of the bounding box or segmentation mask) in RS images, enhancing human interaction with intelligent RS interpretation systems. Early research in this area was primarily based on horizontal bounding boxes (HBBs), but as more diverse RS datasets have become available, tasks involving oriented bounding boxes (OBBs) and segmentation masks have emerged. In practical applications, different targets require different grounding types: HBB can localize an object's position, OBB provides its orientation, and mask depicts its shape. However, existing specialized methods are typically tailored to a single type of RS visual grounding task and are hard to generalize across tasks. In contrast, large vision-language models (VLMs) exhibit powerful multi-task learning capabilities but struggle to handle dense prediction tasks like segmentation. This paper proposes GeoGround, a novel framework that unifies support for HBB, OBB, and mask RS visual grounding tasks, allowing flexible output selection. Rather than customizing the architecture of VLM, our work aims to elegantly support pixel-level visual grounding output through the Text-Mask technique. We define prompt-assisted and geometry-guided learning to enhance consistency across different signals. Experimental results show that GeoGround demonstrates strong performance across four RS visual grounding tasks, matching the performance of specialized methods on multiple benchmarks. Code available at https://github.com/zytx121/GeoGround</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.8 -->
                    
                <!-- Medicine: 5.6 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5455
                </span>
                <a href="https://arxiv.org/abs/2503.15831" target="_blank" rel="noopener noreferrer">EDEN: Enhanced Diffusion for High-quality Large-motion Video Frame Interpolation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zihao Zhang, Haoran Chen, Haoyu Zhao, Guansong Lu, Yanwei Fu, Hang Xu, Zuxuan Wu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenari</span>
                
                <span class="abstract-full" style="display: none;">Handling complex or nonlinear motion patterns has long posed challenges for video frame interpolation. Although recent advances in diffusion-based methods offer improvements over traditional optical flow-based approaches, they still struggle to generate sharp, temporally consistent frames in scenarios with large motion. To address this limitation, we introduce EDEN, an Enhanced Diffusion for high-quality large-motion vidEo frame iNterpolation. Our approach first utilizes a transformer-based tokenizer to produce refined latent representations of the intermediate frames for diffusion models. We then enhance the diffusion transformer with temporal attention across the process and incorporate a start-end frame difference embedding to guide the generation of dynamic motion. Extensive experiments demonstrate that EDEN achieves state-of-the-art results across popular benchmarks, including nearly a 10% LPIPS reduction on DAVIS and SNU-FILM, and an 8% improvement on DAIN-HD.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.0 -->
                    
                <!-- LLMs: 6.3 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- T2I: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Attention: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5632
                </span>
                <a href="https://arxiv.org/abs/2405.15913" target="_blank" rel="noopener noreferrer">Scaling up the Banded Matrix Factorization Mechanism for Differentially Private ML</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ryan McKenna
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes. Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF, whi</span>
                
                <span class="abstract-full" style="display: none;">Correlated noise mechanisms such as DP Matrix Factorization (DP-MF) have proven to be effective alternatives to DP-SGD in large-epsilon few-epoch training regimes. Significant work has been done to find the best correlated noise strategies, and the current state-of-the-art approach is DP-BandMF, which optimally balances the benefits of privacy amplification and noise correlation. Despite it's utility advantages, severe scalability limitations prevent this mechanism from handling large-scale training scenarios where the number of training iterations may exceed $10^4$ and the number of model parameters may exceed $10^7$. In this work, we present techniques to scale up DP-BandMF along these two dimensions, significantly extending it's reach and enabling it to handle settings with virtually any number of model parameters and training iterations, with negligible utility degradation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.0 -->
                    
                <!-- Medicine: 6.5 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Hardware: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5965
                </span>
                <a href="https://arxiv.org/abs/2505.03420" target="_blank" rel="noopener noreferrer">Mitigating Image Captioning Hallucinations in Vision-Language Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fei Zhao, Chengcui Zhang, Runlin Zhang, Tianyang Wang, Xi Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Hallucinations in vision-language models (VLMs) hinder reliability and real-world applicability, usually stemming from distribution shifts between pretraining data and test samples. Existing solutions, such as retraining or fine-tuning on additional data, demand significant computational resources a</span>
                
                <span class="abstract-full" style="display: none;">Hallucinations in vision-language models (VLMs) hinder reliability and real-world applicability, usually stemming from distribution shifts between pretraining data and test samples. Existing solutions, such as retraining or fine-tuning on additional data, demand significant computational resources and labor-intensive data collection, while ensemble-based methods incur additional costs by introducing auxiliary VLMs. To address these challenges, we propose a novel test-time adaptation framework using reinforcement learning to mitigate hallucinations during inference without retraining or any auxiliary VLMs. By updating only the learnable parameters in the layer normalization of the language model (approximately 0.003% of the model parameters), our method reduces distribution shifts between test samples and pretraining samples. A CLIP-based hallucination evaluation model is proposed to provide dual rewards to VLMs. Experimental results demonstrate a 15.4% and 17.3% reduction in hallucination rates on LLaVA and InstructBLIP, respectively. Our approach outperforms state-of-the-art baselines with a 68.3% improvement in hallucination mitigation, demonstrating its effectiveness.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.1 -->
                    
                <!-- Medicine: 6.1 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6415
                </span>
                <a href="https://arxiv.org/abs/2505.00351" target="_blank" rel="noopener noreferrer">Integral Representations of Sobolev Spaces via ReLU$^k$ Activation Function and Optimal Error Estimates for Linearized Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xinliang Liu, Tong Mao, Jinchao Xu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents two main theoretical results concerning shallow neural networks with ReLU$^k$ activation functions. We establish a novel integral representation for Sobolev spaces, showing that every function in $H^{\frac{d+2k+1}{2}}(\Omega)$ can be expressed as an $L^2$-weighted integral of ReL</span>
                
                <span class="abstract-full" style="display: none;">This paper presents two main theoretical results concerning shallow neural networks with ReLU$^k$ activation functions. We establish a novel integral representation for Sobolev spaces, showing that every function in $H^{\frac{d+2k+1}{2}}(\Omega)$ can be expressed as an $L^2$-weighted integral of ReLU$^k$ ridge functions over the unit sphere. This result mirrors the known representation of Barron spaces and highlights a fundamental connection between Sobolev regularity and neural network representations. Moreover, we prove that linearized shallow networks -- constructed by fixed inner parameters and optimizing only the linear coefficients -- achieve optimal approximation rates $O(n^{-\frac{1}{2}-\frac{2k+1}{2d}})$ in Sobolev spaces.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.4 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Quantum Computing: 3.8 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6641
                </span>
                <a href="https://arxiv.org/abs/2504.20078" target="_blank" rel="noopener noreferrer">Low-Rank Matrix Approximation for Neural Network Compression</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Kalyan Cherukuri, Aarav Lala
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Deep Neural Networks (DNNs) have encountered an emerging deployment challenge due to large and expensive memory and computation requirements. In this paper, we present a new Adaptive-Rank Singular Value Decomposition (ARSVD) method that approximates the optimal rank for compressing weight matrices i</span>
                
                <span class="abstract-full" style="display: none;">Deep Neural Networks (DNNs) have encountered an emerging deployment challenge due to large and expensive memory and computation requirements. In this paper, we present a new Adaptive-Rank Singular Value Decomposition (ARSVD) method that approximates the optimal rank for compressing weight matrices in neural networks using spectral entropy. Unlike conventional SVD-based methods that apply a fixed-rank truncation across all layers, ARSVD uses an adaptive selection of the rank per layer through the entropy distribution of its singular values. This approach ensures that each layer will retain a certain amount of its informational content, thereby reducing redundancy. Our method enables efficient, layer-wise compression, yielding improved performance with reduced space and time complexity compared to static-rank reduction techniques.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.0 -->
                    
                <!-- Medicine: 6.9 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- GNN: 3.0 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                <!-- Robotics: 1.0 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7147
                </span>
                <a href="https://arxiv.org/abs/2411.12363" target="_blank" rel="noopener noreferrer">DGSNA: prompt-based Dynamic Generative Scene-based Noise Addition method</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zihao Chen, Zhentao Lin, Bi Zeng, Linyi Huang, Zhi Li, Jia Cai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">To ensure the reliable operation of speech systems across diverse environments, noise addition methods have emerged as the prevailing solution. However, existing methods offer limited coverage of real-world noisy scenes and depend on pre-existing scene-based information and noise. This paper present</span>
                
                <span class="abstract-full" style="display: none;">To ensure the reliable operation of speech systems across diverse environments, noise addition methods have emerged as the prevailing solution. However, existing methods offer limited coverage of real-world noisy scenes and depend on pre-existing scene-based information and noise. This paper presents prompt-based Dynamic Generative Scene-based Noise Addition (DGSNA), a novel noise addition methodology that integrates Dynamic Generation of Scene-based Information (DGSI) with Scene-based Noise Addition for Speech (SNAS). This integration facilitates automated scene-based noise addition by transforming clean speech into various noise environments, thereby providing a more comprehensive and realistic simulation of diverse noise conditions. Experimental results demonstrate that DGSNA significantly enhances the robustness of speech recognition and keyword spotting models across various noise conditions, achieving a relative improvement of up to 11.21%. Furthermore, DGSNA can be effectively integrated with other noise addition methods to enhance performance. Our implementation and demonstrations are available at https://dgsna.github.io.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.6 -->
                    
                <!-- Medicine: 7.1 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7671
                </span>
                <a href="https://arxiv.org/abs/2405.09915" target="_blank" rel="noopener noreferrer">Sparse Regression Codes for Non-coherent SIMO channels</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sai Dinesh Kancharana, Madhusudan Kumar Sinha, Arun Pachai Kannu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Motivated by hyper-reliable low-latency communication in 6G, we consider error control coding for short block lengths in multi-antenna fading channels. In general, the channel fading coefficients are unknown at both the transmitter and receiver, which is referred to as non-coherent channels. Convent</span>
                
                <span class="abstract-full" style="display: none;">Motivated by hyper-reliable low-latency communication in 6G, we consider error control coding for short block lengths in multi-antenna fading channels. In general, the channel fading coefficients are unknown at both the transmitter and receiver, which is referred to as non-coherent channels. Conventionally, pilot symbols are transmitted to facilitate channel estimation, causing power and bandwidth overhead. Our paper considers sparse regression codes (SPARCs) for non-coherent flat-fading channels without using pilots. We develop a novel greedy decoder for SPARC using maximum likelihood principles, referred to as maximum likelihood matching pursuit (MLMP). MLMP works based on successive combining principles as opposed to conventional greedy algorithms, which are based on successive cancellation. We also obtain the noiseless perfect recovery condition for our successive combining algorithm. In addition, we develop an approximate message passing (AMP) SPARC decoder for the non-coherent flat fading model. Using simulation studies, we show that the MLMP decoder for SPARC outperforms AMP and other greedy decoders. Also, SPARC with MLMP decoder outperforms polar codes employing pilot-based channel estimation and polar codes with non-coherent decoders.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.1 -->
                    
                <!-- Quantum Computing: 4.0 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7969
                </span>
                <a href="https://arxiv.org/abs/2505.07508" target="_blank" rel="noopener noreferrer">EAGLE: Contrastive Learning for Efficient Graph Anomaly Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jing Ren, Mingliang Hou, Zhixuan Liu, Xiaomei Bai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Graph anomaly detection is a popular and vital task in various real-world scenarios, which has been studied for several decades. Recently, many studies extending deep learning-based methods have shown preferable performance on graph anomaly detection. However, existing methods are lack of efficiency</span>
                
                <span class="abstract-full" style="display: none;">Graph anomaly detection is a popular and vital task in various real-world scenarios, which has been studied for several decades. Recently, many studies extending deep learning-based methods have shown preferable performance on graph anomaly detection. However, existing methods are lack of efficiency that is definitely necessary for embedded devices. Towards this end, we propose an Efficient Anomaly detection model on heterogeneous Graphs via contrastive LEarning (EAGLE) by contrasting abnormal nodes with normal ones in terms of their distances to the local context. The proposed method first samples instance pairs on meta path-level for contrastive learning. Then, a graph autoencoder-based model is applied to learn informative node embeddings in an unsupervised way, which will be further combined with the discriminator to predict the anomaly scores of nodes. Experimental results show that EAGLE outperforms the state-of-the-art methods on three heterogeneous network datasets.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.2 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- GNN: 3.9 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8475
                </span>
                <a href="https://arxiv.org/abs/2408.09701" target="_blank" rel="noopener noreferrer">Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mingda Li, Abhijit Mishra, Utkarsh Mujumdar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The use of Large Language Models (LLMs) for program code generation has gained substantial attention, but their biases and limitations with non-English prompts challenge global inclusivity. This paper investigates the complexities of multilingual prompt-based code generation. Our evaluations of LLMs</span>
                
                <span class="abstract-full" style="display: none;">The use of Large Language Models (LLMs) for program code generation has gained substantial attention, but their biases and limitations with non-English prompts challenge global inclusivity. This paper investigates the complexities of multilingual prompt-based code generation. Our evaluations of LLMs, including CODELLAMA and CODEGEMMA, reveal significant disparities in code quality for non-English prompts; we also demonstrate the inadequacy of simple approaches like prompt translation, bootstrapped data augmentation, and fine-tuning. To address this, we propose a zero-shot cross-lingual approach using a neural projection technique, integrating a cross-lingual encoder like LASER to map multilingual embeddings from it into the LLM's token space. This method requires training only on English data and scales effectively to other languages. Results on a translated and quality-checked MBPP dataset show substantial improvements in code quality. This research promotes a more inclusive code generation landscape by empowering LLMs with multilingual capabilities to support the diverse linguistic spectrum in programming.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 17.4 -->
                    
                <!-- Medicine: 6.8 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8663
                </span>
                <a href="https://arxiv.org/abs/2505.06933" target="_blank" rel="noopener noreferrer">Optimal pressure approximation for the nonstationary Stokes problem by a variational method in time with post-processing</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mathias Anselmann, Markus Bause, Gunar Matthies, Friedhelm Schieweck
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We provide an error analysis for the solution of the nonstationary Stokes problem by a variational method in space and time. We use finite elements of higher order for the approximation in space and a Galerkin-Petrov method with first order polynomials for the approximation in time. We require globa</span>
                
                <span class="abstract-full" style="display: none;">We provide an error analysis for the solution of the nonstationary Stokes problem by a variational method in space and time. We use finite elements of higher order for the approximation in space and a Galerkin-Petrov method with first order polynomials for the approximation in time. We require global continuity of the discrete velocity trajectory in time, while allowing the discrete pressure trajectory to be discontinuous at the endpoints of the time intervals. We show existence and uniqueness of the discrete velocity solution, characterize the set of all discrete pressure solutions and prove an optimal second order estimate in time for the pressure error in the midpoints of the time intervals. The key result and innovation is the construction of approximations to the pressure trajectory by means of post-processing together with the proof of optimal order error estimates. We propose two variants for a post-processed pressure within the set of pressure solutions based on collocation techniques or interpolation. Both variants guarantee that the pressure error measured in the L2-norm converges with optimal second order in time and optimal order in space. For the discrete velocity solution, we prove error estimates of optimal order in time and space. We present some numerical tests to support our theoretical results.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.6 -->
                    
                <!-- Math: 4.8 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Pathfinding: 3.4 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8755
                </span>
                <a href="https://arxiv.org/abs/2505.07744" target="_blank" rel="noopener noreferrer">BodyGPS: Anatomical Positioning System</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Halid Ziya Yerebakan, Kritika Iyer, Xueqi Guo, Yoshihisa Shinagawa, Gerardo Hermosillo Valadez
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce a new type of foundational model for parsing human anatomy in medical images that works for different modalities. It supports supervised or unsupervised training and can perform matching, registration, classification, or segmentation with or without user interaction. We achieve this by </span>
                
                <span class="abstract-full" style="display: none;">We introduce a new type of foundational model for parsing human anatomy in medical images that works for different modalities. It supports supervised or unsupervised training and can perform matching, registration, classification, or segmentation with or without user interaction. We achieve this by training a neural network estimator that maps query locations to atlas coordinates via regression. Efficiency is improved by sparsely sampling the input, enabling response times of less than 1 ms without additional accelerator hardware. We demonstrate the utility of the algorithm in both CT and MRI modalities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.5 -->
                    
                <!-- Medicine: 7.3 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- 3D: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.902
                </span>
                <a href="https://arxiv.org/abs/2505.06535" target="_blank" rel="noopener noreferrer">Online Feedback Efficient Active Target Discovery in Partially Observable Environments</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Anindya Sarkar, Binglin Ji, Yevgeniy Vorobeychik
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In various scientific and engineering domains, where data acquisition is costly, such as in medical imaging, environmental monitoring, or remote sensing, strategic sampling from unobserved regions, guided by prior observations, is essential to maximize target discovery within a limited sampling budg</span>
                
                <span class="abstract-full" style="display: none;">In various scientific and engineering domains, where data acquisition is costly, such as in medical imaging, environmental monitoring, or remote sensing, strategic sampling from unobserved regions, guided by prior observations, is essential to maximize target discovery within a limited sampling budget. In this work, we introduce Diffusion-guided Active Target Discovery (DiffATD), a novel method that leverages diffusion dynamics for active target discovery. DiffATD maintains a belief distribution over each unobserved state in the environment, using this distribution to dynamically balance exploration-exploitation. Exploration reduces uncertainty by sampling regions with the highest expected entropy, while exploitation targets areas with the highest likelihood of discovering the target, indicated by the belief distribution and an incrementally trained reward model designed to learn the characteristics of the target. DiffATD enables efficient target discovery in a partially observable environment within a fixed sampling budget, all without relying on any prior supervised training. Furthermore, DiffATD offers interpretability, unlike existing black-box policies that require extensive supervised training. Through extensive experiments and ablation studies across diverse domains, including medical imaging and remote sensing, we show that DiffATD performs significantly better than baselines and competitively with supervised methods that operate under full environmental observability.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.1 -->
                    
                <!-- Medicine: 6.7 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9094
                </span>
                <a href="https://arxiv.org/abs/2505.07600" target="_blank" rel="noopener noreferrer">Beyond Static Perception: Integrating Temporal Context into VLMs for Cloth Folding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Oriol Barbany, Adri\`a Colom\'e, Carme Torras
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Manipulating clothes is challenging due to their complex dynamics, high deformability, and frequent self-occlusions. Garments exhibit a nearly infinite number of configurations, making explicit state representations difficult to define. In this paper, we analyze BiFold, a model that predicts languag</span>
                
                <span class="abstract-full" style="display: none;">Manipulating clothes is challenging due to their complex dynamics, high deformability, and frequent self-occlusions. Garments exhibit a nearly infinite number of configurations, making explicit state representations difficult to define. In this paper, we analyze BiFold, a model that predicts language-conditioned pick-and-place actions from visual observations, while implicitly encoding garment state through end-to-end learning. To address scenarios such as crumpled garments or recovery from failed manipulations, BiFold leverages temporal context to improve state estimation. We examine the internal representations of the model and present evidence that its fine-tuning and temporal context enable effective alignment between text and image regions, as well as temporal consistency.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.5 -->
                    
                <!-- Medicine: 6.8 -->
                    
                <!-- Quantum Computing: 4.1 -->
                    
                <!-- 3D: 2.6 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.9127
                </span>
                <a href="https://arxiv.org/abs/2505.06782" target="_blank" rel="noopener noreferrer">Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Damian Curran, Brian Chapman, Mike Conway
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Australia and the UK have developed contrasting approaches to the regulation of electronic cigarettes, with - broadly speaking - Australia adopting a relatively restrictive approach and the UK adopting a more permissive approach. Notably, these divergent policies were developed from the same broad e</span>
                
                <span class="abstract-full" style="display: none;">Australia and the UK have developed contrasting approaches to the regulation of electronic cigarettes, with - broadly speaking - Australia adopting a relatively restrictive approach and the UK adopting a more permissive approach. Notably, these divergent policies were developed from the same broad evidence base. In this paper, to investigate differences in how the two jurisdictions manage and present evidence, we developed and evaluated a Large Language Model-based sentence classifier to perform automated analyses of electronic cigarette-related policy documents drawn from official Australian and UK legislative processes (109 documents in total). Specifically, we utilized GPT-4 to automatically classify sentences based on whether they contained claims that e-cigarettes were broadly helpful or harmful for public health. Our LLM-based classifier achieved an F-score of 0.9. Further, when applying the classifier to our entire sentence-level corpus, we found that Australian legislative documents show a much higher proportion of harmful statements, and a lower proportion of helpful statements compared to the expected values, with the opposite holding for the UK. In conclusion, this work utilized an LLM-based approach to provide evidence to support the contention that - drawing on the same evidence base - Australian ENDS-related policy documents emphasize the harms associated with ENDS products and UK policy documents emphasize the benefits. Further, our approach provides a starting point for using LLM-based methods to investigate the complex relationship between evidence and health policy formation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 5.7 -->
                    
                <!-- Medicine: 5.5 -->
                    
                <!-- Reinforcement Learning: 4.8 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Quantum Computing: 1.7 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.948
                </span>
                <a href="https://arxiv.org/abs/2505.05396" target="_blank" rel="noopener noreferrer">A Pain Assessment Framework based on multimodal data and Deep Machine Learning methods</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Stefanos Gkikas
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">From the original abstract: This thesis initially aims to study the pain assessment process from a clinical-theoretical perspective while exploring and examining existing automatic approaches. Building on this foundation, the primary objective of this Ph.D. project is to develop innovative computati</span>
                
                <span class="abstract-full" style="display: none;">From the original abstract: This thesis initially aims to study the pain assessment process from a clinical-theoretical perspective while exploring and examining existing automatic approaches. Building on this foundation, the primary objective of this Ph.D. project is to develop innovative computational methods for automatic pain assessment that achieve high performance and are applicable in real clinical settings. A primary goal is to thoroughly investigate and assess significant factors, including demographic elements that impact pain perception, as recognized in pain research, through a computational standpoint. Within the limits of the available data in this research area, our goal was to design, develop, propose, and offer automatic pain assessment pipelines for unimodal and multimodal configurations that are applicable to the specific requirements of different scenarios. The studies published in this Ph.D. thesis showcased the effectiveness of the proposed methods, achieving state-of-the-art results. Additionally, they paved the way for exploring new approaches in artificial intelligence, foundation models, and generative artificial intelligence.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.3 -->
                    
                <!-- LLMs: 6.0 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.066
                </span>
                <a href="https://arxiv.org/abs/2505.06531" target="_blank" rel="noopener noreferrer">High-Dimensional Importance-Weighted Information Criteria: Theory and Optimality</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yong-Syun Cao, Shinpei Imori, Ching-Kang Ing
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Imori and Ing (2025) proposed the importance-weighted orthogonal greedy algorithm (IWOGA) for model selection in high-dimensional misspecified regression models under covariate shift. To determine the number of IWOGA iterations, they introduced the high-dimensional importance-weighted information cr</span>
                
                <span class="abstract-full" style="display: none;">Imori and Ing (2025) proposed the importance-weighted orthogonal greedy algorithm (IWOGA) for model selection in high-dimensional misspecified regression models under covariate shift. To determine the number of IWOGA iterations, they introduced the high-dimensional importance-weighted information criterion (HDIWIC). They argued that the combined use of IWOGA and HDIWIC, IWOGA + HDIWIC, achieves an optimal trade-off between variance and squared bias, leading to optimal convergence rates in terms of conditional mean squared prediction error. In this article, we provide a theoretical justification for this claim by establishing the optimality of IWOGA + HDIWIC under a set of reasonable assumptions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.0 -->
                    
                <!-- LLMs: 7.8 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- T2I: 1.0 -->
                    
                <!-- 3D: 1.0 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1118
                </span>
                <a href="https://arxiv.org/abs/2505.07164" target="_blank" rel="noopener noreferrer">EmoVLM-KD: Fusing Distilled Expertise with Vision-Language Models for Visual Emotion Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: SangEun Lee, Yubeen Lee, Eunil Park
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Visual emotion analysis, which has gained considerable attention in the field of affective computing, aims to predict the dominant emotions conveyed by an image. Despite advancements in visual emotion analysis with the emergence of vision-language models, we observed that instruction-tuned vision-la</span>
                
                <span class="abstract-full" style="display: none;">Visual emotion analysis, which has gained considerable attention in the field of affective computing, aims to predict the dominant emotions conveyed by an image. Despite advancements in visual emotion analysis with the emergence of vision-language models, we observed that instruction-tuned vision-language models and conventional vision models exhibit complementary strengths in visual emotion analysis, as vision-language models excel in certain cases, whereas vision models perform better in others. This finding highlights the need to integrate these capabilities to enhance the performance of visual emotion analysis. To bridge this gap, we propose EmoVLM-KD, an instruction-tuned vision-language model augmented with a lightweight module distilled from conventional vision models. Instead of deploying both models simultaneously, which incurs high computational costs, we transfer the predictive patterns of a conventional vision model into the vision-language model using a knowledge distillation framework. Our approach first fine-tunes a vision-language model on emotion-specific instruction data and then attaches a distilled module to its visual encoder while keeping the vision-language model frozen. Predictions from the vision language model and the distillation module are effectively balanced by a gate module, which subsequently generates the final outcome. Extensive experiments show that EmoVLM-KD achieves state-of-the-art performance on multiple visual emotion analysis benchmark datasets, outperforming the existing methods while maintaining computational efficiency. The code is available in https://github.com/sange1104/EmoVLM-KD.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.7 -->
                    
                <!-- Medicine: 7.5 -->
                    
                <!-- Federated Learning: 2.6 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1376
                </span>
                <a href="https://arxiv.org/abs/2505.06762" target="_blank" rel="noopener noreferrer">Investigating Robotaxi Crash Severity Using Geographical Random Forest</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Junfeng Jiao, Seung Gyu Baik, Seung Jun Choi, Yiming Xu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper quantitatively investigates the crash severity of Autonomous Vehicles (AVs) with spatially localized machine learning and macroscopic measures of the urban built environment. We address spatial heterogeneity and spatial autocorrelation, while focusing on land use patterns and human behavi</span>
                
                <span class="abstract-full" style="display: none;">This paper quantitatively investigates the crash severity of Autonomous Vehicles (AVs) with spatially localized machine learning and macroscopic measures of the urban built environment. We address spatial heterogeneity and spatial autocorrelation, while focusing on land use patterns and human behavior. Our Geographical Random Forest (GRF) model, accompanied with a crash severity risk map of San Francisco, presents three findings that are useful for commercial operations of AVs and robotaxis. First, spatially localized machine learning performed better than regular machine learning, when predicting AV crash severity. Bias-variance tradeoff was evident as we adjust the localization weight hyperparameter. Second, land use was the most important built environment measure, compared to intersections, building footprints, public transit stops, and Points Of Interests (POIs). Third, it was predicted that city center areas with greater diversity and commercial activities were more likely to result in low-severity AV crashes, than residential neighborhoods. Residential land use may be associated with higher severity due to human behavior and less restrictive environment. This paper recommends to explicitly consider geographic locations, and to design safety measures specific to residential neighborhoods, when robotaxi operators train their AV systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.3 -->
                    
                <!-- LLMs: 7.1 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- GNN: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1824
                </span>
                <a href="https://arxiv.org/abs/2505.07020" target="_blank" rel="noopener noreferrer">R-CAGE: A Structural Model for Emotion Output Design in Human-AI Interaction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Suyeon Choi
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego), a theoretical framework for restructuring emotional output in long-term human-AI interaction. While prior affective computing approaches emphasized expressiveness, immersion, and responsiveness, they often neglected the cog</span>
                
                <span class="abstract-full" style="display: none;">This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego), a theoretical framework for restructuring emotional output in long-term human-AI interaction. While prior affective computing approaches emphasized expressiveness, immersion, and responsiveness, they often neglected the cognitive and structural consequences of repeated emotional engagement. R-CAGE instead conceptualizes emotional output not as reactive expression but as ethical design structure requiring architectural intervention. The model is grounded in experiential observations of subtle affective symptoms such as localized head tension, interpretive fixation, and emotional lag arising from prolonged interaction with affective AI systems. These indicate a mismatch between system-driven emotion and user interpretation that cannot be fully explained by biometric data or observable behavior. R-CAGE adopts a user-centered stance prioritizing psychological recovery, interpretive autonomy, and identity continuity. The framework consists of four control blocks: (1) Control of Rhythmic Expression regulates output pacing to reduce fatigue; (2) Architecture of Sensory Structuring adjusts intensity and timing of affective stimuli; (3) Guarding of Cognitive Framing reduces semantic pressure to allow flexible interpretation; (4) Ego-Aligned Response Design supports self-reference recovery during interpretive lag. By structurally regulating emotional rhythm, sensory intensity, and interpretive affordances, R-CAGE frames emotion not as performative output but as sustainable design unit. The goal is to protect users from oversaturation and cognitive overload while sustaining long-term interpretive agency in AI-mediated environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.8 -->
                    
                <!-- Medicine: 7.7 -->
                    
                <!-- Quantum Computing: 3.3 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Blockchain: 1.9 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.1829
                </span>
                <a href="https://arxiv.org/abs/2407.13120" target="_blank" rel="noopener noreferrer">HPPP: Halpern-type Preconditioned Proximal Point Algorithms and Applications to Image Restoration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shuchang Zhang, Hui Zhang, Hongxia Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recently, the degenerate preconditioned proximal point (PPP) method provides a unified and flexible framework for designing and analyzing operator-splitting algorithms such as Douglas-Rachford (DR). However, the degenerate PPP method exhibits weak convergence in the infinite-dimensional Hilbert spac</span>
                
                <span class="abstract-full" style="display: none;">Recently, the degenerate preconditioned proximal point (PPP) method provides a unified and flexible framework for designing and analyzing operator-splitting algorithms such as Douglas-Rachford (DR). However, the degenerate PPP method exhibits weak convergence in the infinite-dimensional Hilbert space and lacks accelerated variants. To address these issues, we propose a Halpern-type PPP (HPPP) algorithm, which leverages the strong convergence and acceleration properties of Halpern's iteration method. Moreover, we propose a novel algorithm for image restoration by combining HPPP with denoiser priors such as Plug-and-Play (PnP) prior, which can be viewed as an accelerated PnP method. Finally, numerical experiments including several toy examples and image restoration validate the effectiveness of our proposed algorithms.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.3 -->
                    
                <!-- LLMs: 5.5 -->
                    
                <!-- 3D: 3.1 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2164
                </span>
                <a href="https://arxiv.org/abs/2502.11013" target="_blank" rel="noopener noreferrer">Collaborative Deterministic-Diffusion Model for Probabilistic Spatiotemporal Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhi Sheng, Yuan Yuan, Yudi Zhang, Depeng Jin, Yong Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate prediction of urban spatiotemporal dynamics is essential for enhancing urban management and decision-making. Existing spatiotemporal prediction models are predominantly deterministic, focusing on primary spatiotemporal patterns. However, those dynamics are highly complex, exhibiting multi-m</span>
                
                <span class="abstract-full" style="display: none;">Accurate prediction of urban spatiotemporal dynamics is essential for enhancing urban management and decision-making. Existing spatiotemporal prediction models are predominantly deterministic, focusing on primary spatiotemporal patterns. However, those dynamics are highly complex, exhibiting multi-modal distributions that are challenging for deterministic models to capture. In this paper, we highlight the critical role of probabilistic prediction in capturing the uncertainties and complexities inherent in spatiotemporal data. While mainstream probabilistic models can capture uncertainty, they struggle with accurately learning primary patterns and often suffer from computational inefficiency. To address these challenges, we propose CoST, which collaborates deterministic and probabilistic models to improve both predictive accuracy and the ability to handle uncertainty. To achieve this, we design a mean-residual decomposition framework, where the mean value is modeled by a deterministic model, and the residual variations are learned by a probabilistic model, specifically diffusion models. Moreover, we introduce a scale-aware diffusion process, which better accounts for spatially heterogeneous dynamics across different regions. Extensive experiments on eight real-world datasets demonstrate that CoST significantly outperforms existing methods in both deterministic and probabilistic metrics, achieving a 20% improvement with low computational cost. CoST bridges the gap between deterministic precision and probabilistic uncertainty, making a significant advancement in the field of urban spatiotemporal prediction.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.0 -->
                    
                <!-- LLMs: 5.1 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2188
                </span>
                <a href="https://arxiv.org/abs/2505.07772" target="_blank" rel="noopener noreferrer">The Value of Disagreement in AI Design, Evaluation, and Alignment</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sina Fazelpour, Will Fleisher
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Disagreements are widespread across the design, evaluation, and alignment pipelines of artificial intelligence (AI) systems. Yet, standard practices in AI development often obscure or eliminate disagreement, resulting in an engineered homogenization that can be epistemically and ethically harmful, p</span>
                
                <span class="abstract-full" style="display: none;">Disagreements are widespread across the design, evaluation, and alignment pipelines of artificial intelligence (AI) systems. Yet, standard practices in AI development often obscure or eliminate disagreement, resulting in an engineered homogenization that can be epistemically and ethically harmful, particularly for marginalized groups. In this paper, we characterize this risk, and develop a normative framework to guide practical reasoning about disagreement in the AI lifecycle. Our contributions are two-fold. First, we introduce the notion of perspectival homogenization, characterizing it as a coupled ethical-epistemic risk that arises when an aspect of an AI system's development unjustifiably suppresses disagreement and diversity of perspectives. We argue that perspectival homogenization is best understood as a procedural risk, which calls for targeted interventions throughout the AI development pipeline. Second, we propose a normative framework to guide such interventions, grounded in lines of research that explain why disagreement can be epistemically beneficial, and how its benefits can be realized in practice. We apply this framework to key design questions across three stages of AI development tasks: when disagreement is epistemically valuable; whose perspectives should be included and preserved; how to structure tasks and navigate trade-offs; and how disagreement should be documented and communicated. In doing so, we challenge common assumptions in AI practice, offer a principled foundation for emerging participatory and pluralistic approaches, and identify actionable pathways for future work in AI design and governance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 14.5 -->
                    
                <!-- Medicine: 6.2 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- GNN: 1.0 -->
                    
                <!-- RAG: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2279
                </span>
                <a href="https://arxiv.org/abs/2409.10983" target="_blank" rel="noopener noreferrer">MoDex: Planning High-Dimensional Dexterous Control via Learning Neural Internal Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tong Wu, Shoujie Li, Chuqiao Lyu, Kit-Wa Sou, Wang-Sing Chan, Wenbo Ding
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Controlling hands in high-dimensional action space has been a longstanding challenge, yet humans naturally perform dexterous tasks with ease. In this paper, we draw inspiration from the concept of internal model exhibited in human behavior and reconsider dexterous hands as learnable systems. Specifi</span>
                
                <span class="abstract-full" style="display: none;">Controlling hands in high-dimensional action space has been a longstanding challenge, yet humans naturally perform dexterous tasks with ease. In this paper, we draw inspiration from the concept of internal model exhibited in human behavior and reconsider dexterous hands as learnable systems. Specifically, we introduce MoDex, a framework that includes a couple of neural networks (NNs) capturing the dynamical characteristics of hands and a bidirectional planning approach, which demonstrates both training and planning efficiency. To show the versatility of MoDex, we further integrate it with an external model to manipulate in-hand objects and a large language model (LLM) to generate various gestures in both simulation and real world. Extensive experiments on different dexterous hands address the data efficiency in learning a new task and the transferability between different tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 9.8 -->
                    
                <!-- Medicine: 7.1 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- T2I: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2281
                </span>
                <a href="https://arxiv.org/abs/2505.07245" target="_blank" rel="noopener noreferrer">REMEDI: Relative Feature Enhanced Meta-Learning with Distillation for Imbalanced Prediction</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fei Liu, Huanhuan Ren, Yu Guan, Xiuxu Wang, Wang Lv, Zhiqiang Hu, Yaxi Chen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Predicting future vehicle purchases among existing owners presents a critical challenge due to extreme class imbalance (<0.5% positive rate) and complex behavioral patterns. We propose REMEDI (Relative feature Enhanced Meta-learning with Distillation for Imbalanced prediction), a novel multi-stage f</span>
                
                <span class="abstract-full" style="display: none;">Predicting future vehicle purchases among existing owners presents a critical challenge due to extreme class imbalance (<0.5% positive rate) and complex behavioral patterns. We propose REMEDI (Relative feature Enhanced Meta-learning with Distillation for Imbalanced prediction), a novel multi-stage framework addressing these challenges. REMEDI first trains diverse base models to capture complementary aspects of user behavior. Second, inspired by comparative op-timization techniques, we introduce relative performance meta-features (deviation from ensemble mean, rank among peers) for effective model fusion through a hybrid-expert architecture. Third, we distill the ensemble's knowledge into a single efficient model via supervised fine-tuning with MSE loss, enabling practical deployment. Evaluated on approximately 800,000 vehicle owners, REMEDI significantly outperforms baseline approaches, achieving the business target of identifying ~50% of actual buyers within the top 60,000 recommendations at ~10% precision. The distilled model preserves the ensemble's predictive power while maintaining deployment efficiency, demonstrating REMEDI's effectiveness for imbalanced prediction in industry settings.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.5 -->
                    
                <!-- LLMs: 5.8 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2583
                </span>
                <a href="https://arxiv.org/abs/2505.07377" target="_blank" rel="noopener noreferrer">Examining the Role of LLM-Driven Interactions on Attention and Cognitive Engagement in Virtual Classrooms</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Suleyman Ozdel, Can Sarpkaya, Efe Bozkir, Hong Gao, Enkelejda Kasneci
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Transforming educational technologies through the integration of large language models (LLMs) and virtual reality (VR) offers the potential for immersive and interactive learning experiences. However, the effects of LLMs on user engagement and attention in educational environments remain open questi</span>
                
                <span class="abstract-full" style="display: none;">Transforming educational technologies through the integration of large language models (LLMs) and virtual reality (VR) offers the potential for immersive and interactive learning experiences. However, the effects of LLMs on user engagement and attention in educational environments remain open questions. In this study, we utilized a fully LLM-driven virtual learning environment, where peers and teachers were LLM-driven, to examine how students behaved in such settings. Specifically, we investigate how peer question-asking behaviors influenced student engagement, attention, cognitive load, and learning outcomes and found that, in conditions where LLM-driven peer learners asked questions, students exhibited more targeted visual scanpaths, with their attention directed toward the learning content, particularly in complex subjects. Our results suggest that peer questions did not introduce extraneous cognitive load directly, as the cognitive load is strongly correlated with increased attention to the learning material. Considering these findings, we provide design recommendations for optimizing VR learning spaces.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.1 -->
                    
                <!-- Medicine: 8.7 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3435
                </span>
                <a href="https://arxiv.org/abs/2407.19031" target="_blank" rel="noopener noreferrer">Artificial Neural Networks on Graded Vector Spaces</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tony Shaska
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a transformative framework for artificial neural networks over graded vector spaces, tailored to model hierarchical and structured data in fields like algebraic geometry and physics. By exploiting the algebraic properties of graded vector spaces, where features carry distinct wei</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a transformative framework for artificial neural networks over graded vector spaces, tailored to model hierarchical and structured data in fields like algebraic geometry and physics. By exploiting the algebraic properties of graded vector spaces, where features carry distinct weights, we extend classical neural networks with graded neurons, layers, and activation functions that preserve structural integrity. Grounded in group actions, representation theory, and graded algebra, our approach combines theoretical rigor with practical utility.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.9 -->
                    
                <!-- LLMs: 7.5 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- 3D: 3.1 -->
                    
                <!-- GNN: 2.6 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Attention: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3801
                </span>
                <a href="https://arxiv.org/abs/2503.18938" target="_blank" rel="noopener noreferrer">AdaWorld: Learning Adaptable World Models with Latent Actions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shenyuan Gao, Siyuan Zhou, Yilun Du, Jun Zhang, Chuang Gan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with hetero</span>
                
                <span class="abstract-full" style="display: none;">World models aim to learn action-controlled future prediction and have proven essential for the development of intelligent agents. However, most existing world models rely heavily on substantial action-labeled data and costly training, making it challenging to adapt to novel environments with heterogeneous actions through limited interactions. This limitation can hinder their applicability across broader domains. To overcome this limitation, we propose AdaWorld, an innovative world model learning approach that enables efficient adaptation. The key idea is to incorporate action information during the pretraining of world models. This is achieved by extracting latent actions from videos in a self-supervised manner, capturing the most critical transitions between frames. We then develop an autoregressive world model that conditions on these latent actions. This learning paradigm enables highly adaptable world models, facilitating efficient transfer and learning of new actions even with limited interactions and finetuning. Our comprehensive experiments across multiple environments demonstrate that AdaWorld achieves superior performance in both simulation quality and visual planning.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 10.4 -->
                    
                <!-- Medicine: 9.2 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- GNN: 2.4 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4252
                </span>
                <a href="https://arxiv.org/abs/2505.06906" target="_blank" rel="noopener noreferrer">Realistic Counterfactual Explanations for Machine Learning-Controlled Mobile Robots using 2D LiDAR</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Sindre Benjamin Remman, Anastasios M. Lekkas
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a novel method for generating realistic counterfactual explanations (CFEs) in machine learning (ML)-based control for mobile robots using 2D LiDAR. ML models, especially artificial neural networks (ANNs), can provide advanced decision-making and control capabilities by learning f</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a novel method for generating realistic counterfactual explanations (CFEs) in machine learning (ML)-based control for mobile robots using 2D LiDAR. ML models, especially artificial neural networks (ANNs), can provide advanced decision-making and control capabilities by learning from data. However, they often function as black boxes, making it challenging to interpret them. This is especially a problem in safety-critical control applications. To generate realistic CFEs, we parameterize the LiDAR space with simple shapes such as circles and rectangles, whose parameters are chosen by a genetic algorithm, and the configurations are transformed into LiDAR data by raycasting. Our model-agnostic approach generates CFEs in the form of synthetic LiDAR data that resembles a base LiDAR state but is modified to produce a pre-defined ML model control output based on a query from the user. We demonstrate our method on a mobile robot, the TurtleBot3, controlled using deep reinforcement learning (DRL) in real-world and simulated scenarios. Our method generates logical and realistic CFEs, which helps to interpret the DRL agent's decision making. This paper contributes towards advancing explainable AI in mobile robotics, and our method could be a tool for understanding, debugging, and improving ML-based autonomous control.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.5504
                </span>
                <a href="https://arxiv.org/abs/2505.07003" target="_blank" rel="noopener noreferrer">CMD: Controllable Multiview Diffusion for 3D Editing and Progressive Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Peng Li, Suizhi Ma, Jialiang Chen, Yuan Liu, Chongyi Zhang, Wei Xue, Wenhan Luo, Alla Sheffer, Wenping Wang, Yike Guo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the inpu</span>
                
                <span class="abstract-full" style="display: none;">Recently, 3D generation methods have shown their powerful ability to automate 3D model creation. However, most 3D generation methods only rely on an input image or a text prompt to generate a 3D model, which lacks the control of each component of the generated 3D model. Any modifications of the input image lead to an entire regeneration of the 3D models. In this paper, we introduce a new method called CMD that generates a 3D model from an input image while enabling flexible local editing of each component of the 3D model. In CMD, we formulate the 3D generation as a conditional multiview diffusion model, which takes the existing or known parts as conditions and generates the edited or added components. This conditional multiview diffusion model not only allows the generation of 3D models part by part but also enables local editing of 3D models according to the local revision of the input image without changing other 3D parts. Extensive experiments are conducted to demonstrate that CMD decomposes a complex 3D generation task into multiple components, improving the generation quality. Meanwhile, CMD enables efficient and flexible local editing of a 3D model by just editing one rendered image.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- 3D: 8.6 -->
                    
                <!-- Medicine: 5.7 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Federated Learning: 1.8 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Quantum Computing: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.5839
                </span>
                <a href="https://arxiv.org/abs/2505.06549" target="_blank" rel="noopener noreferrer">Good Things Come in Pairs: Paired Autoencoders for Inverse Problems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Matthias Chung, Bas Peters, Michael Solomon
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In this book chapter, we discuss recent advances in data-driven approaches for inverse problems. In particular, we focus on the \emph{paired autoencoder} framework, which has proven to be a powerful tool for solving inverse problems in scientific computing. The paired autoencoder framework is a nove</span>
                
                <span class="abstract-full" style="display: none;">In this book chapter, we discuss recent advances in data-driven approaches for inverse problems. In particular, we focus on the \emph{paired autoencoder} framework, which has proven to be a powerful tool for solving inverse problems in scientific computing. The paired autoencoder framework is a novel approach that leverages the strengths of both data-driven and model-based methods by projecting both the data and the quantity of interest into a latent space and mapping these latent spaces to provide surrogate forward and inverse mappings. We illustrate the advantages of this approach through numerical experiments, including seismic imaging and classical inpainting: nonlinear and linear inverse problems, respectively. Although the paired autoencoder framework is likelihood-free, it generates multiple data- and model-based reconstruction metrics that help assess whether examples are in or out of distribution. In addition to direct model estimates from data, the paired autoencoder enables latent-space refinement to fit the observed data accurately. Numerical experiments show that this procedure, combined with the latent-space initial guess, is essential for high-quality estimates, even when data noise exceeds the training regime. We also introduce two novel variants that combine variational and paired autoencoder ideas, maintaining the original benefits while enabling sampling for uncertainty analysis.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 8.4 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.669
                </span>
                <a href="https://arxiv.org/abs/2503.18376" target="_blank" rel="noopener noreferrer">Analysis of Forces Exerted by Shoulder and Elbow Fabric-based Pneumatic Actuators for Pediatric Exosuits</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mehrnoosh Ayazi, Ipsita Sahin, Caio Mucchiani, Elena Kokkoni, Konstantinos Karydis
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">To enhance pediatric exosuit design, it is crucial to assess the actuator-generated forces. This work evaluates the contact forces exerted by soft fabric-based pneumatic actuators in an upper extremity pediatric exosuit. Two actuators were examined: a single-cell bidirectional actuator for shoulder </span>
                
                <span class="abstract-full" style="display: none;">To enhance pediatric exosuit design, it is crucial to assess the actuator-generated forces. This work evaluates the contact forces exerted by soft fabric-based pneumatic actuators in an upper extremity pediatric exosuit. Two actuators were examined: a single-cell bidirectional actuator for shoulder abduction/adduction and a bellow-type actuator for elbow extension/flexion. Experiments assessed the impact of actuator anchoring points and the adjacent joint's angle on exerted forces and actuated joint range of motion (ROM). These were measured via load cells and encoders integrated into a custom infant-scale engineered apparatus with two degrees of freedom (two revolute joints). For the shoulder actuator, results show that anchoring it further from the shoulder joint center while the elbow is flexed at $90^\circ$ yields the highest ROM while minimizing the peak force exerted on the body. For the elbow actuator, anchoring it symmetrically while the shoulder joint is at $0^\circ$ optimizes actuator performance. These findings contribute a key step toward co-optimizing the considered exosuit design for functionality and wearability.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.8 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Math: 2.0 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7456
                </span>
                <a href="https://arxiv.org/abs/2505.07685" target="_blank" rel="noopener noreferrer">Periods of fibre products of elliptic surfaces and the Gamma conjecture</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Eric Pichon-Pharabod
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We provide an algorithm for computing a basis of homology of fibre products of elliptic surfaces over $\mathbb P^1$, along with the corresponding intersection product and period matrices. We use this data to investigate the Gamma conjecture for Calabi-Yau threefolds obtained in this manner. We find </span>
                
                <span class="abstract-full" style="display: none;">We provide an algorithm for computing a basis of homology of fibre products of elliptic surfaces over $\mathbb P^1$, along with the corresponding intersection product and period matrices. We use this data to investigate the Gamma conjecture for Calabi-Yau threefolds obtained in this manner. We find a formula that works for all operators of a list of 105 fibre products, as well as for fourth order operators of the Calabi-Yau database. This algorithm comes with a SageMath implementation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Quantum Computing: 3.9 -->
                    
                <!-- Math: 2.6 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7569
                </span>
                <a href="https://arxiv.org/abs/2309.17397" target="_blank" rel="noopener noreferrer">Analytic and Gevrey class regularity for parametric semilinear reaction-diffusion problems and applications in uncertainty quantification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alexey Chernov, Tung Le
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We investigate a class of parametric elliptic semilinear partial differential equations of second order with homogeneous essential boundary conditions, where the coefficients and the right-hand side (and hence the solution) may depend on a parameter. This model can be seen as a reaction-diffusion pr</span>
                
                <span class="abstract-full" style="display: none;">We investigate a class of parametric elliptic semilinear partial differential equations of second order with homogeneous essential boundary conditions, where the coefficients and the right-hand side (and hence the solution) may depend on a parameter. This model can be seen as a reaction-diffusion problem with a polynomial nonlinearity in the reaction term. The efficiency of various numerical approximations across the entire parameter space is closely related to the regularity of the solution with respect to the parameter. We show that if the coefficients and the right-hand side are analytic or Gevrey class regular with respect to the parameter, the same type of parametric regularity is valid for the solution. The key ingredient of the proof is the combination of the alternative-to-factorial technique from our previous work [1] with a novel argument for the treatment of the power-type nonlinearity in the reaction term. As an application of this abstract result, we obtain rigorous convergence estimates for numerical integration of semilinear reaction-diffusion problems with random coefficients using Gaussian and Quasi-Monte Carlo quadrature. Our theoretical findings are confirmed in numerical experiments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- Math: 5.7 -->
                    
                <!-- Reinforcement Learning: 3.2 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- LLMs: 2.0 -->
                    
                <!-- Quantum Computing: 1.9 -->
                    
                <!-- Pathfinding: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.7591
                </span>
                <a href="https://arxiv.org/abs/2505.07043" target="_blank" rel="noopener noreferrer">Design and Experimental Test of Datatic Approximate Optimal Filter in Nonlinear Dynamic Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Weixian He, Zeyu He, Wenhan Cao, Haoyu Gao, Tong Liu, Bin Shuai, Chang Liu, Shengbo Eben Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Filtering is crucial in engineering fields, providing vital state estimation for control systems. However, the nonlinear nature of complex systems and the presence of non-Gaussian noises pose significant challenges to the performance of conventional filtering methods in terms of estimation accuracy </span>
                
                <span class="abstract-full" style="display: none;">Filtering is crucial in engineering fields, providing vital state estimation for control systems. However, the nonlinear nature of complex systems and the presence of non-Gaussian noises pose significant challenges to the performance of conventional filtering methods in terms of estimation accuracy and computational efficiency. In this work, we present a data-driven closed-loop filter, termed datatic approximate optimal filter (DAOF), specifically designed for nonlinear systems under non-Gaussian conditions. We first formulate a Markovian filtering problem (MFP), which inherently shares a connection with reinforcement learning (RL) as it aims to compute the optimal state estimate by minimizing the accumulated error. To solve MFP, we propose DAOF, which primarily incorporates a trained RL policy and features two distinct structural designs: DAOF-v1 and DAOF-v2. Designed for systems with explicit models, DAOF-v1 combines prediction and update phases, with the RL policy generating the update value. Meanwhile, DAOF-v2 bypasses system modeling by directly outputting the state estimate. Then, we utilize an actor-critic algorithm to learn the parameterized policy for DAOF. Experimental results on a 2-degree-of-freedom (2-DOF) vehicle system, equipped with explicit system models, demonstrate the superior accuracy and computational efficiency of DAOF-v1 compared to existing nonlinear filters. Moreover, DAOF-v2 showcases its unique ability to perform filtering without requiring explicit system modeling, as validated by a 14-DOF vehicle system.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.2 -->
                    
                <!-- Reinforcement Learning: 3.8 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.819
                </span>
                <a href="https://arxiv.org/abs/2505.00945" target="_blank" rel="noopener noreferrer">SSRLBot: Designing and Developing a Large Language Model-based Agent using Socially Shared Regulated Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xiaoshan Huang, Jie Gao, Haolun Wu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Large language model (LLM)--based agents have emerged as pivotal tools in assisting human experts across various fields by transforming complex tasks into more efficient workflows and providing actionable stakeholder insights. Despite their potential, the application of LLM-based agents for medical </span>
                
                <span class="abstract-full" style="display: none;">Large language model (LLM)--based agents have emerged as pivotal tools in assisting human experts across various fields by transforming complex tasks into more efficient workflows and providing actionable stakeholder insights. Despite their potential, the application of LLM-based agents for medical education remains underexplored. The study aims to assist in evaluating the students' process and outcomes on medical case diagnosis and discussion while incorporating the theoretical framework of Socially Shared Regulation of Learning (SSRL) to assess student performance. SSRL emphasizes metacognitive, cognitive, motivational, and emotional interactions, highlighting the collaborative management of learning processes to improve decision-making outcomes. Grounded in SSRL theory, this tool paper introduces SSRLBot, an LLM-based agent designed to enable team members to reflect on their diagnostic performance and the key SSRL skills that foster team success. SSRLBot's core functions include summarizing dialogue content, analyzing participants' SSRL skills, and evaluating students' diagnostic results. Meanwhile, we evaluated SSRLBot through diagnostic conversation data collected from six groups (12 participants, 1926 conversational turns). Results showed that SSRLBot can deliver detailed, theory-aligned evaluations, link specific behaviors to SSRL dimensions, and offer actionable recommendations for improving teamwork. The findings address a critical gap in medical education, advancing the application of LLM agents to enhance team-based decision-making and collaboration in high-stakes environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 14.0 -->
                    
                <!-- Medicine: 9.9 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8325
                </span>
                <a href="https://arxiv.org/abs/2404.15731" target="_blank" rel="noopener noreferrer">MD-NOMAD: Mixture density nonlinear manifold decoder for emulating stochastic differential equations and uncertainty propagation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Akshay Thakur, Souvik Chakraborty
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a neural operator framework, termed mixture density nonlinear manifold decoder (MD-NOMAD), for stochastic simulators. Our approach leverages an amalgamation of the pointwise operator learning neural architecture nonlinear manifold decoder (NOMAD) with mixture density-based methods to esti</span>
                
                <span class="abstract-full" style="display: none;">We propose a neural operator framework, termed mixture density nonlinear manifold decoder (MD-NOMAD), for stochastic simulators. Our approach leverages an amalgamation of the pointwise operator learning neural architecture nonlinear manifold decoder (NOMAD) with mixture density-based methods to estimate conditional probability distributions for stochastic output functions. MD-NOMAD harnesses the ability of probabilistic mixture models to estimate complex probability and the high-dimensional scalability of pointwise neural operator NOMAD. We conduct empirical assessments on a wide array of stochastic ordinary and partial differential equations and present the corresponding results, which highlight the performance of the proposed framework.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.4 -->
                    
                <!-- LLMs: 4.7 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Reinforcement Learning: 2.7 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8447
                </span>
                <a href="https://arxiv.org/abs/2505.06927" target="_blank" rel="noopener noreferrer">Stability Regularized Cross-Validation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ryan Cory-Wright, Andr\'es G\'omez
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We revisit the problem of ensuring strong test-set performance via cross-validation. Motivated by the generalization theory literature, we propose a nested k-fold cross-validation scheme that selects hyperparameters by minimizing a weighted sum of the usual cross-validation metric and an empirical m</span>
                
                <span class="abstract-full" style="display: none;">We revisit the problem of ensuring strong test-set performance via cross-validation. Motivated by the generalization theory literature, we propose a nested k-fold cross-validation scheme that selects hyperparameters by minimizing a weighted sum of the usual cross-validation metric and an empirical model-stability measure. The weight on the stability term is itself chosen via a nested cross-validation procedure. This reduces the risk of strong validation set performance and poor test set performance due to instability. We benchmark our procedure on a suite of 13 real-world UCI datasets, and find that, compared to k-fold cross-validation over the same hyperparameters, it improves the out-of-sample MSE for sparse ridge regression and CART by 4% on average, but has no impact on XGBoost. This suggests that for interpretable and unstable models, such as sparse regression and CART, our approach is a viable and computationally affordable method for improving test-set performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.5 -->
                    
                <!-- LLMs: 3.7 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- GNN: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.9163
                </span>
                <a href="https://arxiv.org/abs/2410.03024" target="_blank" rel="noopener noreferrer">Flow Matching with Gaussian Process Priors for Probabilistic Time Series Forecasting</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Marcel Kollovieh, Marten Lienen, David L\"udke, Leo Schwinn, Stephan G\"unnemann
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advancements in generative modeling, particularly diffusion models, have opened new directions for time series modeling, achieving state-of-the-art performance in forecasting and synthesis. However, the reliance of diffusion-based models on a simple, fixed prior complicates the generative pro</span>
                
                <span class="abstract-full" style="display: none;">Recent advancements in generative modeling, particularly diffusion models, have opened new directions for time series modeling, achieving state-of-the-art performance in forecasting and synthesis. However, the reliance of diffusion-based models on a simple, fixed prior complicates the generative process since the data and prior distributions differ significantly. We introduce TSFlow, a conditional flow matching (CFM) model for time series combining Gaussian processes, optimal transport paths, and data-dependent prior distributions. By incorporating (conditional) Gaussian processes, TSFlow aligns the prior distribution more closely with the temporal structure of the data, enhancing both unconditional and conditional generation. Furthermore, we propose conditional prior sampling to enable probabilistic forecasting with an unconditionally trained model. In our experimental evaluation on eight real-world datasets, we demonstrate the generative capabilities of TSFlow, producing high-quality unconditional samples. Finally, we show that both conditionally and unconditionally trained models achieve competitive results across multiple forecasting benchmarks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.4 -->
                    
                <!-- LLMs: 7.0 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.0128
                </span>
                <a href="https://arxiv.org/abs/2401.02890" target="_blank" rel="noopener noreferrer">Nonlinear functional regression by functional deep neural network with kernel embedding</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhongjie Shi, Jun Fan, Linhao Song, Ding-Xuan Zhou, Johan A. K. Suykens
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recently, deep learning has been widely applied in functional data analysis (FDA) with notable empirical success. However, the infinite dimensionality of functional data necessitates an effective dimension reduction approach for functional learning tasks, particularly in nonlinear functional regress</span>
                
                <span class="abstract-full" style="display: none;">Recently, deep learning has been widely applied in functional data analysis (FDA) with notable empirical success. However, the infinite dimensionality of functional data necessitates an effective dimension reduction approach for functional learning tasks, particularly in nonlinear functional regression. In this paper, we introduce a functional deep neural network with an adaptive and discretization-invariant dimension reduction method. Our functional network architecture consists of three parts: first, a kernel embedding step that features an integral transformation with an adaptive smooth kernel; next, a projection step that utilizes eigenfunction bases based on a projection Mercer kernel for the dimension reduction; and finally, a deep ReLU neural network is employed for the prediction. Explicit rates of approximating nonlinear smooth functionals across various input function spaces by our proposed functional network are derived. Additionally, we conduct a generalization analysis for the empirical risk minimization (ERM) algorithm applied to our functional net, by employing a novel two-stage oracle inequality and the established functional approximation results. Ultimately, we conduct numerical experiments on both simulated and real datasets to demonstrate the effectiveness and benefits of our functional net.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.0 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1807
                </span>
                <a href="https://arxiv.org/abs/2505.06743" target="_blank" rel="noopener noreferrer">TPK: Trustworthy Trajectory Prediction Integrating Prior Knowledge For Interpretability and Kinematic Feasibility</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Marius Baden, Ahmed Abouelazm, Christian Hubschneider, Yin Wu, Daniel Slieter, J. Marius Z\"ollner
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Trajectory prediction is crucial for autonomous driving, enabling vehicles to navigate safely by anticipating the movements of surrounding road users. However, current deep learning models often lack trustworthiness as their predictions can be physically infeasible and illogical to humans. To make p</span>
                
                <span class="abstract-full" style="display: none;">Trajectory prediction is crucial for autonomous driving, enabling vehicles to navigate safely by anticipating the movements of surrounding road users. However, current deep learning models often lack trustworthiness as their predictions can be physically infeasible and illogical to humans. To make predictions more trustworthy, recent research has incorporated prior knowledge, like the social force model for modeling interactions and kinematic models for physical realism. However, these approaches focus on priors that suit either vehicles or pedestrians and do not generalize to traffic with mixed agent classes. We propose incorporating interaction and kinematic priors of all agent classes--vehicles, pedestrians, and cyclists with class-specific interaction layers to capture agent behavioral differences. To improve the interpretability of the agent interactions, we introduce DG-SFM, a rule-based interaction importance score that guides the interaction layer. To ensure physically feasible predictions, we proposed suitable kinematic models for all agent classes with a novel pedestrian kinematic model. We benchmark our approach on the Argoverse 2 dataset, using the state-of-the-art transformer HPTR as our baseline. Experiments demonstrate that our method improves interaction interpretability, revealing a correlation between incorrect predictions and divergence from our interaction prior. Even though incorporating the kinematic models causes a slight decrease in accuracy, they eliminate infeasible trajectories found in the dataset and the baseline model. Thus, our approach fosters trust in trajectory prediction as its interaction reasoning is interpretable, and its predictions adhere to physics.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.4 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1829
                </span>
                <a href="https://arxiv.org/abs/2505.06636" target="_blank" rel="noopener noreferrer">A Contrastive Federated Semi-Supervised Learning Intrusion Detection Framework for Internet of Robotic Things</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yifan Zeng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In intelligent industry, autonomous driving and other environments, the Internet of Things (IoT) highly integrated with robotic to form the Internet of Robotic Things (IoRT). However, network intrusion to IoRT can lead to data leakage, service interruption in IoRT and even physical damage by control</span>
                
                <span class="abstract-full" style="display: none;">In intelligent industry, autonomous driving and other environments, the Internet of Things (IoT) highly integrated with robotic to form the Internet of Robotic Things (IoRT). However, network intrusion to IoRT can lead to data leakage, service interruption in IoRT and even physical damage by controlling robots or vehicles. This paper proposes a Contrastive Federated Semi-Supervised Learning Network Intrusion Detection framework (CFedSSL-NID) for IoRT intrusion detection and defense, to address the practical scenario of IoRT where robots don't possess labeled data locally and the requirement for data privacy preserving. CFedSSL-NID integrates randomly weak and strong augmentation, latent contrastive learning, and EMA update to integrate supervised signals, thereby enhancing performance and robustness on robots' local unlabeled data. Extensive experiments demonstrate that CFedSSL-NID outperforms existing federated semi-supervised and fully supervised methods on benchmark dataset and has lower resource requirements.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.5 -->
                    
                <!-- LLMs: 6.8 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1907
                </span>
                <a href="https://arxiv.org/abs/2505.06263" target="_blank" rel="noopener noreferrer">From Biometrics to Environmental Control: AI-Enhanced Digital Twins for Personalized Health Interventions in Healing Landscapes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yiping Meng, Yiming Sun
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The dynamic nature of human health and comfort calls for adaptive systems that respond to individual physiological needs in real time. This paper presents an AI-enhanced digital twin framework that integrates biometric signals, specifically electrocardiogram (ECG) data, with environmental parameters</span>
                
                <span class="abstract-full" style="display: none;">The dynamic nature of human health and comfort calls for adaptive systems that respond to individual physiological needs in real time. This paper presents an AI-enhanced digital twin framework that integrates biometric signals, specifically electrocardiogram (ECG) data, with environmental parameters such as temperature, humidity, and ventilation. Leveraging IoT-enabled sensors and biometric monitoring devices, the system continuously acquires, synchronises, and preprocesses multimodal data streams to construct a responsive virtual replica of the physical environment. To validate this framework, a detailed case study is conducted using the MIT-BIH noise stress test dataset. ECG signals are filtered and segmented using dynamic sliding windows, followed by extracting heart rate variability (HRV) features such as SDNN, BPM, QTc, and LF/HF ratio. Relative deviation metrics are computed against clean baselines to quantify stress responses. A random forest classifier is trained to predict stress levels across five categories, and Shapley Additive exPlanations (SHAP) is used to interpret model behaviour and identify key contributing features. These predictions are mapped to a structured set of environmental interventions using a Five Level Stress Intervention Mapping, which activates multi-scale responses across personal, room, building, and landscape levels. This integration of physiological insight, explainable AI, and adaptive control establishes a new paradigm for health-responsive built environments. It lays the foundation for the future development of intelligent, personalised healing spaces.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.5 -->
                    
                <!-- LLMs: 5.4 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2709
                </span>
                <a href="https://arxiv.org/abs/2312.14999" target="_blank" rel="noopener noreferrer">Leveraging Habitat Information for Fine-grained Bird Identification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tin Nguyen, Peijie Chen, Anh Totti Nguyen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Traditional bird classifiers mostly rely on the visual characteristics of birds. Some prior works even train classifiers to be invariant to the background, completely discarding the living environment of birds. Instead, we are the first to explore integrating habitat information, one of the four maj</span>
                
                <span class="abstract-full" style="display: none;">Traditional bird classifiers mostly rely on the visual characteristics of birds. Some prior works even train classifiers to be invariant to the background, completely discarding the living environment of birds. Instead, we are the first to explore integrating habitat information, one of the four major cues for identifying birds by ornithologists, into modern bird classifiers. We focus on two leading model types: (1) CNNs and ViTs trained on the downstream bird datasets; and (2) original, multi-modal CLIP. Training CNNs and ViTs with habitat-augmented data results in an improvement of up to +0.83 and +0.23 points on NABirds and CUB-200, respectively. Similarly, adding habitat descriptors to the prompts for CLIP yields a substantial accuracy boost of up to +0.99 and +1.1 points on NABirds and CUB-200, respectively. We find consistent accuracy improvement after integrating habitat features into the image augmentation process and into the textual descriptors of vision-language CLIP classifiers. Code is available at: https://anonymous.4open.science/r/reasoning-8B7E/.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.9 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.48
                </span>
                <a href="https://arxiv.org/abs/2505.07170" target="_blank" rel="noopener noreferrer">Empowering the Grid: Collaborative Edge Artificial Intelligence for Decentralized Energy Systems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Eddie de Paula Jr, Niel Bunda, Hezerul Abdul Karim, Nouar AlDahoul, Myles Joshua Toledo Tan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper examines how decentralized energy systems can be enhanced using collaborative Edge Artificial Intelligence. Decentralized grids use local renewable sources to reduce transmission losses and improve energy security. Edge AI enables real-time, privacy-preserving data processing at the netwo</span>
                
                <span class="abstract-full" style="display: none;">This paper examines how decentralized energy systems can be enhanced using collaborative Edge Artificial Intelligence. Decentralized grids use local renewable sources to reduce transmission losses and improve energy security. Edge AI enables real-time, privacy-preserving data processing at the network edge. Techniques such as federated learning and distributed control improve demand response, equipment maintenance, and energy optimization. The paper discusses key challenges including data privacy, scalability, and interoperability, and suggests solutions such as blockchain integration and adaptive architectures. Examples from virtual power plants and smart grids highlight the potential of these technologies. The paper calls for increased investment, policy support, and collaboration to advance sustainable energy systems.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.5 -->
                    
                <!-- LLMs: 6.4 -->
                    
                <!-- Quantum Computing: 3.2 -->
                    
                <!-- 3D: 2.3 -->
                    
                <!-- Blockchain: 2.1 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.5929
                </span>
                <a href="https://arxiv.org/abs/2505.07747" target="_blank" rel="noopener noreferrer">Step1X-3D: Towards High-Fidelity and Controllable Generation of Textured 3D Assets</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Weiyu Li, Xuanyang Zhang, Zheng Sun, Di Qi, Hao Li, Wei Cheng, Weiwei Cai, Shihao Wu, Jiarui Liu, Zihao Wang, Xiao Chen, Feipeng Tian, Jianxiong Pan, Zeming Li, Gang Yu, Xiangyu Zhang, Daxin Jiang, Ping Tan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step</span>
                
                <span class="abstract-full" style="display: none;">While generative artificial intelligence has advanced significantly across text, image, audio, and video domains, 3D generation remains comparatively underdeveloped due to fundamental challenges such as data scarcity, algorithmic limitations, and ecosystem fragmentation. To this end, we present Step1X-3D, an open framework addressing these challenges through: (1) a rigorous data curation pipeline processing >5M assets to create a 2M high-quality dataset with standardized geometric and textural properties; (2) a two-stage 3D-native architecture combining a hybrid VAE-DiT geometry generator with an diffusion-based texture synthesis module; and (3) the full open-source release of models, training code, and adaptation modules. For geometry generation, the hybrid VAE-DiT component produces TSDF representations by employing perceiver-based latent encoding with sharp edge sampling for detail preservation. The diffusion-based texture synthesis module then ensures cross-view consistency through geometric conditioning and latent-space synchronization. Benchmark results demonstrate state-of-the-art performance that exceeds existing open-source methods, while also achieving competitive quality with proprietary solutions. Notably, the framework uniquely bridges the 2D and 3D generation paradigms by supporting direct transfer of 2D control techniques~(e.g., LoRA) to 3D synthesis. By simultaneously advancing data quality, algorithmic fidelity, and reproducibility, Step1X-3D aims to establish new standards for open research in controllable 3D asset generation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.8 -->
                    
                <!-- LLMs: 5.9 -->
                    
                <!-- 3D: 5.9 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- RAG: 1.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.6998
                </span>
                <a href="https://arxiv.org/abs/2505.07689" target="_blank" rel="noopener noreferrer">Anatomical Attention Alignment representation for Radiology Report Generation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Quang Vinh Nguyen, Minh Duc Nguyen, Thanh Hoang Son Vo, Hyung-Jeong Yang, Soo-Hyung Kim
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Automated Radiology report generation (RRG) aims at producing detailed descriptions of medical images, reducing radiologists' workload and improving access to high-quality diagnostic services. Existing encoder-decoder models only rely on visual features extracted from raw input images, which can lim</span>
                
                <span class="abstract-full" style="display: none;">Automated Radiology report generation (RRG) aims at producing detailed descriptions of medical images, reducing radiologists' workload and improving access to high-quality diagnostic services. Existing encoder-decoder models only rely on visual features extracted from raw input images, which can limit the understanding of spatial structures and semantic relationships, often resulting in suboptimal text generation. To address this, we propose Anatomical Attention Alignment Network (A3Net), a framework that enhance visual-textual understanding by constructing hyper-visual representations. Our approach integrates a knowledge dictionary of anatomical structures with patch-level visual features, enabling the model to effectively associate image regions with their corresponding anatomical entities. This structured representation improves semantic reasoning, interpretability, and cross-modal alignment, ultimately enhancing the accuracy and clinical relevance of generated reports. Experimental results on IU X-Ray and MIMIC-CXR datasets demonstrate that A3Net significantly improves both visual perception and text generation quality. Our code is available at \href{https://github.com/Vinh-AI/A3Net}{GitHub}.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.5 -->
                    
                <!-- LLMs: 8.3 -->
                    
                <!-- 3D: 3.2 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- T2I: 2.1 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7191
                </span>
                <a href="https://arxiv.org/abs/2505.06719" target="_blank" rel="noopener noreferrer">Modelling and Study of t , Peak and Effective Diameter in Temporal Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zahra Farahi, Ali Kamandi, Ali Moeini
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Understanding how information, diseases, or influence spread across networks is a fundamental challenge in complex systems. While network diameter has been extensively studied in static networks, its definition and behavior in temporal networks remain underexplored due to their dynamic nature. In th</span>
                
                <span class="abstract-full" style="display: none;">Understanding how information, diseases, or influence spread across networks is a fundamental challenge in complex systems. While network diameter has been extensively studied in static networks, its definition and behavior in temporal networks remain underexplored due to their dynamic nature. In this study, we present a formal mathematical framework for analyzing diameter in temporal networks and introduce three time-aware metrics: Effective Diameter , Peak Diameter (*D), and t-Diameter (tD), each capturing distinct temporal aspects of connectivity and diffusion. Our approach combines theoretical analysis with empirical validation using four real-world datasets: high school, hospital, conference, and workplace contact networks. We simulate flow propagation on temporal networks and compare the observed diameters with the proposed theoretical Equations. Across all datasets, our model demonstrates high accuracy, with low RMSE and absolute error values. Furthermore, we observe that the effective diameter decreases with increasing average degree and increases with network size. The results also show that tD and *D are more sensitive to node removal, highlighting their relevance for applications such as epidemic modeling. By bridging formal modeling and empirical data, our framework offers new insights into the temporal dynamics of networked systems and provides tools for assessing robustness, controlling information spread, and optimizing interventions in time-sensitive environments.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.3 -->
                    
                <!-- LLMs: 10.5 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.796
                </span>
                <a href="https://arxiv.org/abs/2505.07166" target="_blank" rel="noopener noreferrer">Pre-training vs. Fine-tuning: A Reproducibility Study on Dense Retrieval Knowledge Acquisition</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zheng Yao, Shuai Wang, Guido Zuccon
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Dense retrievers utilize pre-trained backbone language models (e.g., BERT, LLaMA) that are fine-tuned via contrastive learning to perform the task of encoding text into sense representations that can be then compared via a shallow similarity operation, e.g. inner product. Recent research has questio</span>
                
                <span class="abstract-full" style="display: none;">Dense retrievers utilize pre-trained backbone language models (e.g., BERT, LLaMA) that are fine-tuned via contrastive learning to perform the task of encoding text into sense representations that can be then compared via a shallow similarity operation, e.g. inner product. Recent research has questioned the role of fine-tuning vs. that of pre-training within dense retrievers, specifically arguing that retrieval knowledge is primarily gained during pre-training, meaning knowledge not acquired during pre-training cannot be sub-sequentially acquired via fine-tuning. We revisit this idea here as the claim was only studied in the context of a BERT-based encoder using DPR as representative dense retriever. We extend the previous analysis by testing other representation approaches (comparing the use of CLS tokens with that of mean pooling), backbone architectures (encoder-only BERT vs. decoder-only LLaMA), and additional datasets (MSMARCO in addition to Natural Questions). Our study confirms that in DPR tuning, pre-trained knowledge underpins retrieval performance, with fine-tuning primarily adjusting neuron activation rather than reorganizing knowledge. However, this pattern does not hold universally, such as in mean-pooled (Contriever) and decoder-based (LLaMA) models. We ensure full reproducibility and make our implementation publicly available at https://github.com/ielab/DenseRetriever-Knowledge-Acquisition.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 13.6 -->
                    
                <!-- Quantum Computing: 5.5 -->
                    
                <!-- Medicine: 3.3 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- 3D: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.8106
                </span>
                <a href="https://arxiv.org/abs/2503.00955" target="_blank" rel="noopener noreferrer">SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Dien X. Tran, Nam V. Nguyen, Thanh T. Tran, Anh T. Hoang, Tai V. Duong, Di T. Le, Phuc-Lu Le
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading ac</span>
                
                <span class="abstract-full" style="display: none;">The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\% strict accuracy on ISE-DSC01 and 80.82\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.9 -->
                    
                <!-- LLMs: 10.4 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- 3D: 2.6 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.0363
                </span>
                <a href="https://arxiv.org/abs/2505.07032" target="_blank" rel="noopener noreferrer">MarkMatch: Same-Hand Stuffing Detection</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Fei Zhao, Runlin Zhang, Chengcui Zhang, Nitesh Saxena
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present MarkMatch, a retrieval system for detecting whether two paper ballot marks were filled by the same hand. Unlike the previous SOTA method BubbleSig, which used binary classification on isolated mark pairs, MarkMatch ranks stylistic similarity between a query mark and a mark in the database</span>
                
                <span class="abstract-full" style="display: none;">We present MarkMatch, a retrieval system for detecting whether two paper ballot marks were filled by the same hand. Unlike the previous SOTA method BubbleSig, which used binary classification on isolated mark pairs, MarkMatch ranks stylistic similarity between a query mark and a mark in the database using contrastive learning. Our model is trained with a dense batch similarity matrix and a dual loss objective. Each sample is contrasted against many negatives within each batch, enabling the model to learn subtle handwriting difference and improve generalization under handwriting variation and visual noise, while diagonal supervision reinforces high confidence on true matches. The model achieves an F1 score of 0.943, surpassing BubbleSig's best performance. MarkMatch also integrates Segment Anything Model for flexible mark extraction via box- or point-based prompts. The system offers election auditors a practical tool for visual, non-biometric investigation of suspicious ballots.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.8 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Networks: 2.7 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.1244
                </span>
                <a href="https://arxiv.org/abs/2505.06592" target="_blank" rel="noopener noreferrer">Batch Augmentation with Unimodal Fine-tuning for Multimodal Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: H M Dipu Kabir, Subrota Kumar Mondal, Mohammad Ali Moni
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper proposes batch augmentation with unimodal fine-tuning to detect the fetus's organs from ultrasound images and associated clinical textual information. We also prescribe pre-training initial layers with investigated medical data before the multimodal training. At first, we apply a transfer</span>
                
                <span class="abstract-full" style="display: none;">This paper proposes batch augmentation with unimodal fine-tuning to detect the fetus's organs from ultrasound images and associated clinical textual information. We also prescribe pre-training initial layers with investigated medical data before the multimodal training. At first, we apply a transferred initialization with the unimodal image portion of the dataset with batch augmentation. This step adjusts the initial layer weights for medical data. Then, we apply neural networks (NNs) with fine-tuned initial layers to images in batches with batch augmentation to obtain features. We also extract information from descriptions of images. We combine this information with features obtained from images to train the head layer. We write a dataloader script to load the multimodal data and use existing unimodal image augmentation techniques with batch augmentation for the multimodal data. The dataloader brings a new random augmentation for each batch to get a good generalization. We investigate the FPU23 ultrasound and UPMC Food-101 multimodal datasets. The multimodal large language model (LLM) with the proposed training provides the best results among the investigated methods. We receive near state-of-the-art (SOTA) performance on the UPMC Food-101 dataset. We share the scripts of the proposed method with traditional counterparts at the following repository: github.com/dipuk0506/multimodal</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.4 -->
                    
                <!-- Reinforcement Learning: 4.8 -->
                    
                <!-- Networks: 4.4 -->
                    
                <!-- Math: 2.5 -->
                    
                <!-- Federated Learning: 1.9 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Quantum Computing: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- GNN: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.1386
                </span>
                <a href="https://arxiv.org/abs/2505.07019" target="_blank" rel="noopener noreferrer">A Vision-Language Foundation Model for Leaf Disease Identification</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Khang Nguyen Quoc, Lan Le Thi Thu, Luyl-Da Quach
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Leaf disease identification plays a pivotal role in smart agriculture. However, many existing studies still struggle to integrate image and textual modalities to compensate for each other's limitations. Furthermore, many of these approaches rely on pretraining with constrained datasets such as Image</span>
                
                <span class="abstract-full" style="display: none;">Leaf disease identification plays a pivotal role in smart agriculture. However, many existing studies still struggle to integrate image and textual modalities to compensate for each other's limitations. Furthermore, many of these approaches rely on pretraining with constrained datasets such as ImageNet, which lack domain-specific information. We propose SCOLD (Soft-target COntrastive learning for Leaf Disease identification), a context-aware vision-language foundation model tailored to address these challenges for agricultural tasks. SCOLD is developed using a diverse corpus of plant leaf images and corresponding symptom descriptions, comprising over 186,000 image-caption pairs aligned with 97 unique concepts. Through task-agnostic pretraining, SCOLD leverages contextual soft targets to mitigate overconfidence in contrastive learning by smoothing labels, thereby improving model generalization and robustness on fine-grained classification tasks. Experimental results demonstrate that SCOLD outperforms existing vision-language models such as OpenAI-CLIP-L, BioCLIP, and SigLIP2 across several benchmarks, including zero-shot and few-shot classification, image-text retrieval, and image classification, while maintaining a competitive parameter footprint. Ablation studies further highlight SCOLD's effectiveness in contrast to its counterparts. The proposed approach significantly advances the agricultural vision-language foundation model, offering strong performance with minimal or no supervised fine-tuning. This work lays a solid groundwork for future research on models trained with long-form and simplified contexts, tasks involving class ambiguity, and multi-modal systems for intelligent plant disease diagnostics. The code for this study is available at https://huggingface.co/enalis/scold</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.0 -->
                    
                <!-- LLMs: 5.7 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.1535
                </span>
                <a href="https://arxiv.org/abs/2505.07809" target="_blank" rel="noopener noreferrer">A Comparative Analysis of Static Word Embeddings for Hungarian</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: M\'at\'e Gedeon
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents a comprehensive analysis of various static word embeddings for Hungarian, including traditional models such as Word2Vec, FastText, as well as static embeddings derived from BERT-based models using different extraction methods. We evaluate these embeddings on both intrinsic and ex</span>
                
                <span class="abstract-full" style="display: none;">This paper presents a comprehensive analysis of various static word embeddings for Hungarian, including traditional models such as Word2Vec, FastText, as well as static embeddings derived from BERT-based models using different extraction methods. We evaluate these embeddings on both intrinsic and extrinsic tasks to provide a holistic view of their performance. For intrinsic evaluation, we employ a word analogy task, which assesses the embeddings ability to capture semantic and syntactic relationships. Our results indicate that traditional static embeddings, particularly FastText, excel in this task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among the BERT-based models, the X2Static method for extracting static embeddings demonstrates superior performance compared to decontextualized and aggregate methods, approaching the effectiveness of traditional static embeddings. For extrinsic evaluation, we utilize a bidirectional LSTM model to perform Named Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results reveal that embeddings derived from dynamic models, especially those extracted using the X2Static method, outperform purely static embeddings. Notably, ELMo embeddings achieve the highest accuracy in both NER and POS tagging tasks, underscoring the benefits of contextualized representations even when used in a static form. Our findings highlight the continued relevance of static word embeddings in NLP applications and the potential of advanced extraction methods to enhance the utility of BERT-based models. This piece of research contributes to the understanding of embedding performance in the Hungarian language and provides valuable insights for future developments in the field. The training scripts, evaluation codes, restricted vocabulary, and extracted embeddings will be made publicly available to support further research and reproducibility.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.6 -->
                    
                <!-- Medicine: 8.0 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.4349
                </span>
                <a href="https://arxiv.org/abs/2505.06515" target="_blank" rel="noopener noreferrer">RESAR-BEV: An Explainable Progressive Residual Autoregressive Approach for Camera-Radar Fusion in BEV Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhiwen Zeng, Yunfei Yin, Zheng Yuan, Argho Dey, Xianjian Bao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Bird's-Eye-View (BEV) semantic segmentation provides comprehensive environmental perception for autonomous driving but suffers multi-modal misalignment and sensor noise. We propose RESAR-BEV, a progressive refinement framework that advances beyond single-step end-to-end approaches: (1) progressive r</span>
                
                <span class="abstract-full" style="display: none;">Bird's-Eye-View (BEV) semantic segmentation provides comprehensive environmental perception for autonomous driving but suffers multi-modal misalignment and sensor noise. We propose RESAR-BEV, a progressive refinement framework that advances beyond single-step end-to-end approaches: (1) progressive refinement through residual autoregressive learning that decomposes BEV segmentation into interpretable coarse-to-fine stages via our Drive-Transformer and Modifier-Transformer residual prediction cascaded architecture, (2) robust BEV representation combining ground-proximity voxels with adaptive height offsets and dual-path voxel feature encoding (max+attention pooling) for efficient feature extraction, and (3) decoupled supervision with offline Ground Truth decomposition and online joint optimization to prevent overfitting while ensuring structural coherence. Experiments on nuScenes demonstrate RESAR-BEV achieves state-of-the-art performance with 54.0% mIoU across 7 essential driving-scene categories while maintaining real-time capability at 14.6 FPS. The framework exhibits robustness in challenging scenarios of long-range perception and adverse weather conditions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.0 -->
                    
                <!-- LLMs: 8.9 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- 3D: 3.6 -->
                    
                <!-- GNN: 2.1 -->
                    
                <!-- RAG: 2.0 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.5476
                </span>
                <a href="https://arxiv.org/abs/2505.07446" target="_blank" rel="noopener noreferrer">TPT-Bench: A Large-Scale, Long-Term and Robot-Egocentric Dataset for Benchmarking Target Person Tracking</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hanjing Ye, Yu Zhan, Weixi Situ, Guangcheng Chen, Jingwen Yu, Kuanqi Cai, Hong Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Tracking a target person from robot-egocentric views is crucial for developing autonomous robots that provide continuous personalized assistance or collaboration in Human-Robot Interaction (HRI) and Embodied AI. However, most existing target person tracking (TPT) benchmarks are limited to controlled</span>
                
                <span class="abstract-full" style="display: none;">Tracking a target person from robot-egocentric views is crucial for developing autonomous robots that provide continuous personalized assistance or collaboration in Human-Robot Interaction (HRI) and Embodied AI. However, most existing target person tracking (TPT) benchmarks are limited to controlled laboratory environments with few distractions, clean backgrounds, and short-term occlusions. In this paper, we introduce a large-scale dataset designed for TPT in crowded and unstructured environments, demonstrated through a robot-person following task. The dataset is collected by a human pushing a sensor-equipped cart while following a target person, capturing human-like following behavior and emphasizing long-term tracking challenges, including frequent occlusions and the need for re-identification from numerous pedestrians. It includes multi-modal data streams, including odometry, 3D LiDAR, IMU, panoptic, and RGB-D images, along with exhaustively annotated 2D bounding boxes of the target person across 35 sequences, both indoors and outdoors. Using this dataset and visual annotations, we perform extensive experiments with existing TPT methods, offering a thorough analysis of their limitations and suggesting future research directions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.5 -->
                    
                <!-- LLMs: 5.9 -->
                    
                <!-- 3D: 3.6 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.0621
                </span>
                <a href="https://arxiv.org/abs/2502.08282" target="_blank" rel="noopener noreferrer">Individualised Treatment Effects Estimation with Composite Treatments and Composite Outcomes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vinod Kumar Chauhan, Lei Clifton, Gaurav Nigam, David A. Clifton
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \textit{composite treatments}, on a set of outcome variables of interest, referred to as \textit{composite outc</span>
                
                <span class="abstract-full" style="display: none;">Estimating individualised treatment effect (ITE) -- that is the causal effect of a set of variables (also called exposures, treatments, actions, policies, or interventions), referred to as \textit{composite treatments}, on a set of outcome variables of interest, referred to as \textit{composite outcomes}, for a unit from observational data -- remains a fundamental problem in causal inference with applications across disciplines, such as healthcare, economics, education, social science, marketing, and computer science. Previous work in causal machine learning for ITE estimation is limited to simple settings, like single treatments and single outcomes. This hinders their use in complex real-world scenarios; for example, consider studying the effect of different ICU interventions, such as beta-blockers and statins for a patient admitted for heart surgery, on different outcomes of interest such as atrial fibrillation and in-hospital mortality. The limited research into composite treatments and outcomes is primarily due to data scarcity for all treatments and outcomes. To address the above challenges, we propose a novel and innovative hypernetwork-based approach, called \emph{H-Learner}, to solve ITE estimation under composite treatments and composite outcomes, which tackles the data scarcity issue by dynamically sharing information across treatments and outcomes. Our empirical analysis with binary and arbitrary composite treatments and outcomes demonstrates the effectiveness of the proposed approach compared to existing methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.5 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.1108
                </span>
                <a href="https://arxiv.org/abs/2412.15826" target="_blank" rel="noopener noreferrer">Using matrix-product states for time-series machine learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Joshua B. Moore, Hugo P. Stackhouse, Ben D. Fulcher, Sahand Mahmoodian
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Matrix-product states (MPS) have proven to be a versatile ansatz for modeling quantum many-body physics. For many applications, and particularly in one-dimension, they capture relevant quantum correlations in many-body wavefunctions while remaining tractable to store and manipulate on a classical co</span>
                
                <span class="abstract-full" style="display: none;">Matrix-product states (MPS) have proven to be a versatile ansatz for modeling quantum many-body physics. For many applications, and particularly in one-dimension, they capture relevant quantum correlations in many-body wavefunctions while remaining tractable to store and manipulate on a classical computer. This has motivated researchers to also apply the MPS ansatz to machine learning (ML) problems where capturing complex correlations in datasets is also a key requirement. Here, we develop and apply an MPS-based algorithm, MPSTime, for learning a joint probability distribution underlying an observed time-series dataset, and show how it can be used to tackle important time-series ML problems, including classification and imputation. MPSTime can efficiently learn complicated time-series probability distributions directly from data, requires only moderate maximum MPS bond dimension $\chi_{\rm max}$, with values for our applications ranging between $\chi_{\rm max} = 20-160$, and can be trained for both classification and imputation tasks under a single logarithmic loss function. Using synthetic and publicly available real-world datasets, spanning applications in medicine, energy, and astronomy, we demonstrate performance competitive with state-of-the-art ML approaches, but with the key advantage of encoding the full joint probability distribution learned from the data, which is useful for analyzing and interpreting its underlying structure. This manuscript is supplemented with the release of a publicly available code package MPSTime that implements our approach. The effectiveness of the MPS-based ansatz for capturing complex correlation structures in time-series data makes it a powerful foundation for tackling challenging time-series analysis problems across science, industry, and medicine.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.3 -->
                    
                <!-- Quantum Computing: 4.6 -->
                    
                <!-- LLMs: 2.7 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.3541
                </span>
                <a href="https://arxiv.org/abs/2505.06333" target="_blank" rel="noopener noreferrer">NSF-MAP: Neurosymbolic Multimodal Fusion for Robust and Interpretable Anomaly Prediction in Assembly Pipelines</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chathurangi Shyalika, Renjith Prasad, Fadi El Kalach, Revathy Venkataramanan, Ramtin Zand, Ramy Harik, Amit Sheth
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In modern assembly pipelines, identifying anomalies is crucial in ensuring product quality and operational efficiency. Conventional single-modality methods fail to capture the intricate relationships required for precise anomaly prediction in complex predictive environments with abundant data and mu</span>
                
                <span class="abstract-full" style="display: none;">In modern assembly pipelines, identifying anomalies is crucial in ensuring product quality and operational efficiency. Conventional single-modality methods fail to capture the intricate relationships required for precise anomaly prediction in complex predictive environments with abundant data and multiple modalities. This paper proposes a neurosymbolic AI and fusion-based approach for multimodal anomaly prediction in assembly pipelines. We introduce a time series and image-based fusion model that leverages decision-level fusion techniques. Our research builds upon three primary novel approaches in multimodal learning: time series and image-based decision-level fusion modeling, transfer learning for fusion, and knowledge-infused learning. We evaluate the novel method using our derived and publicly available multimodal dataset and conduct comprehensive ablation studies to assess the impact of our preprocessing techniques and fusion model compared to traditional baselines. The results demonstrate that a neurosymbolic AI-based fusion approach that uses transfer learning can effectively harness the complementary strengths of time series and image data, offering a robust and interpretable approach for anomaly prediction in assembly pipelines with enhanced performance. \noindent The datasets, codes to reproduce the results, supplementary materials, and demo are available at https://github.com/ChathurangiShyalika/NSF-MAP.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.6 -->
                    
                <!-- LLMs: 6.9 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- 3D: 2.0 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.4259
                </span>
                <a href="https://arxiv.org/abs/2505.06599" target="_blank" rel="noopener noreferrer">Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Abbas Bertina, Shahab Beirami, Hossein Biniazian, Elham Esmaeilnia, Soheil Shahi, Mahdi Pirnia
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges due to its complex phonological features, particularly homographs and Ezafe, which exist in formal and informal language contexts. This paper introduces an intermediate language specifically designed for Persian language pro</span>
                
                <span class="abstract-full" style="display: none;">Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges due to its complex phonological features, particularly homographs and Ezafe, which exist in formal and informal language contexts. This paper introduces an intermediate language specifically designed for Persian language processing that addresses these challenges through a multi-faceted approach. Our methodology combines two key components: Large Language Model (LLM) prompting techniques and a specialized sequence-to-sequence machine transliteration architecture. We developed and implemented a systematic approach for constructing a comprehensive lexical database for homographs with multiple pronunciations disambiguation often termed polyphones, utilizing formal concept analysis for semantic differentiation. We train our model using two distinct datasets: the LLM-generated dataset for formal and informal Persian and the B-Plus podcasts for informal language variants. The experimental results demonstrate superior performance compared to existing state-of-the-art approaches, particularly in handling the complexities of Persian phoneme conversion. Our model significantly improves Phoneme Error Rate (PER) metrics, establishing a new benchmark for Persian G2P conversion accuracy. This work contributes to the growing research in low-resource language processing and provides a robust solution for Persian text-to-speech systems and demonstrating its applicability beyond Persian. Specifically, the approach can extend to languages with rich homographic phenomena such as Chinese and Arabic</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.0 -->
                    
                <!-- LLMs: 7.1 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.6969
                </span>
                <a href="https://arxiv.org/abs/2505.06771" target="_blank" rel="noopener noreferrer">JaxRobotarium: Training and Deploying Multi-Robot Policies in 10 Minutes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shalin Anand Jain, Jiazhen Liu, Siva Kailas, Harish Ravichandar
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning resear</span>
                
                <span class="abstract-full" style="display: none;">Multi-agent reinforcement learning (MARL) has emerged as a promising solution for learning complex and scalable coordination behaviors in multi-robot systems. However, established MARL platforms (e.g., SMAC and MPE) lack robotics relevance and hardware deployment, leaving multi-robot learning researchers to develop bespoke environments and hardware testbeds dedicated to the development and evaluation of their individual contributions. The Multi-Agent RL Benchmark and Learning Environment for the Robotarium (MARBLER) is an exciting recent step in providing a standardized robotics-relevant platform for MARL, by bridging the Robotarium testbed with existing MARL software infrastructure. However, MARBLER lacks support for parallelization and GPU/TPU execution, making the platform prohibitively slow compared to modern MARL environments and hindering adoption. We contribute JaxRobotarium, a Jax-powered end-to-end simulation, learning, deployment, and benchmarking platform for the Robotarium. JaxRobotarium enables rapid training and deployment of multi-robot reinforcement learning (MRRL) policies with realistic robot dynamics and safety constraints, supporting both parallelization and hardware acceleration. Our generalizable learning interface provides an easy-to-use integration with SOTA MARL libraries (e.g., JaxMARL). In addition, JaxRobotarium includes eight standardized coordination scenarios, including four novel scenarios that bring established MARL benchmark tasks (e.g., RWARE and Level-Based Foraging) to a realistic robotics setting. We demonstrate that JaxRobotarium retains high simulation fidelity while achieving dramatic speedups over baseline (20x in training and 150x in simulation), and provides an open-access sim-to-real evaluation pipeline through the Robotarium testbed, accelerating and democratizing access to multi-robot learning research and evaluation.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.1 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.1073
                </span>
                <a href="https://arxiv.org/abs/2504.21415" target="_blank" rel="noopener noreferrer">Optimizing Mouse Dynamics for User Authentication by Machine Learning: Addressing Data Sufficiency, Accuracy-Practicality Trade-off, and Model Performance Challenges</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yi Wang, Chengyv Wu, Yang Liao, Maowei You
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">User authentication is essential to ensure secure access to computer systems, yet traditional methods face limitations in usability, cost, and security. Mouse dynamics authentication, based on the analysis of users' natural interaction behaviors with mouse devices, offers a cost-effective, non-intru</span>
                
                <span class="abstract-full" style="display: none;">User authentication is essential to ensure secure access to computer systems, yet traditional methods face limitations in usability, cost, and security. Mouse dynamics authentication, based on the analysis of users' natural interaction behaviors with mouse devices, offers a cost-effective, non-intrusive, and adaptable solution. However, challenges remain in determining the optimal data volume, balancing accuracy and practicality, and effectively capturing temporal behavioral patterns. In this study, we propose a statistical method using Gaussian kernel density estimate (KDE) and Kullback-Leibler (KL) divergence to estimate the sufficient data volume for training authentication models. We introduce the Mouse Authentication Unit (MAU), leveraging Approximate Entropy (ApEn) to optimize segment length for efficient and accurate behavioral representation. Furthermore, we design the Local-Time Mouse Authentication (LT-AMouse) framework, integrating 1D-ResNet for local feature extraction and GRU for modeling long-term temporal dependencies. Taking the Balabit and DFL datasets as examples, we significantly reduced the data scale, particularly by a factor of 10 for the DFL dataset, greatly alleviating the training burden. Additionally, we determined the optimal input recognition unit length for the user authentication system on different datasets based on the slope of Approximate Entropy. Training with imbalanced samples, our model achieved a successful defense AUC 98.52% for blind attack on the DFL dataset and 94.65% on the Balabit dataset, surpassing the current sota performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.3 -->
                    
                <!-- Reinforcement Learning: 2.4 -->
                    
                <!-- LLMs: 2.1 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Math: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.5512
                </span>
                <a href="https://arxiv.org/abs/2505.07534" target="_blank" rel="noopener noreferrer">The Human-Data-Model Interaction Canvas for Visual Analytics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: J\"urgen Bernard
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Visual Analytics (VA) integrates humans, data, and models as key actors in insight generation and data-driven decision-making. This position paper values and reflects on 16 VA process models and frameworks and makes nine high-level observations that motivate a fresh perspective on VA. The contributi</span>
                
                <span class="abstract-full" style="display: none;">Visual Analytics (VA) integrates humans, data, and models as key actors in insight generation and data-driven decision-making. This position paper values and reflects on 16 VA process models and frameworks and makes nine high-level observations that motivate a fresh perspective on VA. The contribution is the HDMI Canvas, a perspective to VA that complements the strengths of existing VA process models and frameworks. It systematically characterizes diverse roles of humans, data, and models, and how these actors benefit from and contribute to VA processes. The descriptive power of the HDMI Canvas eases the differentiation between a series of VA building blocks, rather than describing general VA principles only. The canvas includes modern human-centered methodologies, including human knowledge externalization and forms of feedback loops, while interpretable and explainable AI highlight model contributions beyond their conventional outputs. The HDMI Canvas has generative power, guiding the design of new VA processes and is optimized for external stakeholders, improving VA outreach, interdisciplinary collaboration, and user-centered design. The utility of the HDMI Canvas is demonstrated through two preliminary case studies.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 17.2 -->
                    
                <!-- LLMs: 7.9 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Blockchain: 1.6 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.8085
                </span>
                <a href="https://arxiv.org/abs/2404.17844" target="_blank" rel="noopener noreferrer">Towards Robust Recommendation: A Review and an Adversarial Robustness Evaluation Library</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lei Cheng, Xiaowen Huang, Jitao Sang, Jian Yu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recently, recommender system has achieved significant success. However, due to the openness of recommender systems, they remain vulnerable to malicious attacks. Additionally, natural noise in training data and issues such as data sparsity can also degrade the performance of recommender systems. Ther</span>
                
                <span class="abstract-full" style="display: none;">Recently, recommender system has achieved significant success. However, due to the openness of recommender systems, they remain vulnerable to malicious attacks. Additionally, natural noise in training data and issues such as data sparsity can also degrade the performance of recommender systems. Therefore, enhancing the robustness of recommender systems has become an increasingly important research topic. In this survey, we provide a comprehensive overview of the robustness of recommender systems. Based on our investigation, we categorize the robustness of recommender systems into adversarial robustness and non-adversarial robustness. In the adversarial robustness, we introduce the fundamental principles and classical methods of recommender system adversarial attacks and defenses. In the non-adversarial robustness, we analyze non-adversarial robustness from the perspectives of data sparsity, natural noise, and data imbalance. Additionally, we summarize commonly used datasets and evaluation metrics for evaluating the robustness of recommender systems. Finally, we also discuss the current challenges in the field of recommender system robustness and potential future research directions. Additionally, to facilitate fair and efficient evaluation of attack and defense methods in adversarial robustness, we propose an adversarial robustness evaluation library--ShillingREC, and we conduct evaluations of basic attack models and recommendation models. ShillingREC project is released at https://github.com/chengleileilei/ShillingREC.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.6 -->
                    
                <!-- LLMs: 9.9 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.0598
                </span>
                <a href="https://arxiv.org/abs/2505.07347" target="_blank" rel="noopener noreferrer">AI-Enabled Accurate Non-Invasive Assessment of Pulmonary Hypertension Progression via Multi-Modal Echocardiography</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jiewen Yang, Taoran Huang, Shangwei Ding, Xiaowei Xu, Qinhua Zhao, Yong Jiang, Jiarong Guo, Bin Pu, Jiexuan Zheng, Caojin Zhang, Hongwen Fei, Xiaomeng Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Echocardiographers can detect pulmonary hypertension using Doppler echocardiography; however, accurately assessing its progression often proves challenging. Right heart catheterization (RHC), the gold standard for precise evaluation, is invasive and unsuitable for routine use, limiting its practical</span>
                
                <span class="abstract-full" style="display: none;">Echocardiographers can detect pulmonary hypertension using Doppler echocardiography; however, accurately assessing its progression often proves challenging. Right heart catheterization (RHC), the gold standard for precise evaluation, is invasive and unsuitable for routine use, limiting its practicality for timely diagnosis and monitoring of pulmonary hypertension progression. Here, we propose MePH, a multi-view, multi-modal vision-language model to accurately assess pulmonary hypertension progression using non-invasive echocardiography. We constructed a large dataset comprising paired standardized echocardiogram videos, spectral images and RHC data, covering 1,237 patient cases from 12 medical centers. For the first time, MePH precisely models the correlation between non-invasive multi-view, multi-modal echocardiography and the pressure and resistance obtained via RHC. We show that MePH significantly outperforms echocardiographers' assessments using echocardiography, reducing the mean absolute error in estimating mean pulmonary arterial pressure (mPAP) and pulmonary vascular resistance (PVR) by 49.73% and 43.81%, respectively. In eight independent external hospitals, MePH achieved a mean absolute error of 3.147 for PVR assessment. Furthermore, MePH achieved an area under the curve of 0.921, surpassing echocardiographers (area under the curve of 0.842) in accurately predicting the severity of pulmonary hypertension, whether mild or severe. A prospective study demonstrated that MePH can predict treatment efficacy for patients. Our work provides pulmonary hypertension patients with a non-invasive and timely method for monitoring disease progression, improving the accuracy and efficiency of pulmonary hypertension management while enabling earlier interventions and more personalized treatment decisions.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 21.0 -->
                    
                <!-- LLMs: 6.7 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.9621
                </span>
                <a href="https://arxiv.org/abs/2505.06799" target="_blank" rel="noopener noreferrer">Quantum Observers: A NISQ Hardware Demonstration of Chaotic State Prediction Using Quantum Echo-state Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Erik L. Connerty, Ethan N. Evans, Gerasimos Angelatos, Vignesh Narayanan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recent advances in artificial intelligence have highlighted the remarkable capabilities of neural network (NN)-powered systems on classical computers. However, these systems face significant computational challenges that limit scalability and efficiency. Quantum computers hold the potential to overc</span>
                
                <span class="abstract-full" style="display: none;">Recent advances in artificial intelligence have highlighted the remarkable capabilities of neural network (NN)-powered systems on classical computers. However, these systems face significant computational challenges that limit scalability and efficiency. Quantum computers hold the potential to overcome these limitations and increase processing power beyond classical systems. Despite this, integrating quantum computing with NNs remains largely unrealized due to challenges posed by noise, decoherence, and high error rates in current quantum hardware. Here, we propose a novel quantum echo-state network (QESN) design and implementation algorithm that can operate within the presence of noise on current IBM hardware. We apply classical control-theoretic response analysis to characterize the QESN, emphasizing its rich nonlinear dynamics and memory, as well as its ability to be fine-tuned with sparsity and re-uploading blocks. We validate our approach through a comprehensive demonstration of QESNs functioning as quantum observers, applied in both high-fidelity simulations and hardware experiments utilizing data from a prototypical chaotic Lorenz system. Our results show that the QESN can predict long time-series with persistent memory, running over 100 times longer than the median T}1 and T2 of the IBM Marrakesh QPU, achieving state-of-the-art time-series performance on superconducting hardware.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 13.6 -->
                    
                <!-- LLMs: 5.2 -->
                    
                <!-- Medicine: 3.5 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.5182
                </span>
                <a href="https://arxiv.org/abs/2505.06300" target="_blank" rel="noopener noreferrer">ARDNS-FN-Quantum: A Quantum-Enhanced Reinforcement Learning Framework with Cognitive-Inspired Adaptive Exploration for Dynamic Environments</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Umberto Gon\c{c}alves de Sousa
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Reinforcement learning (RL) has transformed sequential decision making, yet traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy Optimization (PPO) often struggle with efficient exploration, stability, and adaptability in dynamic environments. This study presents ARDNS-FN-Quantum (</span>
                
                <span class="abstract-full" style="display: none;">Reinforcement learning (RL) has transformed sequential decision making, yet traditional algorithms like Deep Q-Networks (DQNs) and Proximal Policy Optimization (PPO) often struggle with efficient exploration, stability, and adaptability in dynamic environments. This study presents ARDNS-FN-Quantum (Adaptive Reward-Driven Neural Simulator with Quantum enhancement), a novel framework that integrates a 2-qubit quantum circuit for action selection, a dual-memory system inspired by human cognition, and adaptive exploration strategies modulated by reward variance and curiosity. Evaluated in a 10X10 grid-world over 20,000 episodes, ARDNS-FN-Quantum achieves a 99.5% success rate (versus 81.3% for DQN and 97.0% for PPO), a mean reward of 9.0528 across all episodes (versus 1.2941 for DQN and 7.6196 for PPO), and an average of 46.7 steps to goal (versus 135.9 for DQN and 62.5 for PPO). In the last 100 episodes, it records a mean reward of 9.1652 (versus 7.0916 for DQN and 9.0310 for PPO) and 37.2 steps to goal (versus 52.7 for DQN and 53.4 for PPO). Graphical analyses, including learning curves, steps-to-goal trends, reward variance, and reward distributions, demonstrate ARDNS-FN-Quantum's superior stability (reward variance 5.424 across all episodes versus 252.262 for DQN and 76.583 for PPO) and efficiency. By bridging quantum computing, cognitive science, and RL, ARDNS-FN-Quantum offers a scalable, human-like approach to adaptive learning in uncertain environments, with potential applications in robotics, autonomous systems, and decision-making under uncertainty.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 35.1 -->
                    
                <!-- Quantum Computing: 3.5 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- 3D: 2.8 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Evolutionary Algorithms: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.5376
                </span>
                <a href="https://arxiv.org/abs/2505.06898" target="_blank" rel="noopener noreferrer">Multi-Modal Explainable Medical AI Assistant for Trustworthy Human-AI Collaboration</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Honglong Yang, Shanshan Song, Yi Qin, Lehan Wang, Haonan Wang, Xinpeng Ding, Qixiang Zhang, Bodong Du, Xiaomeng Li
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generalist Medical AI (GMAI) systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI as</span>
                
                <span class="abstract-full" style="display: none;">Generalist Medical AI (GMAI) systems have demonstrated expert-level performance in biomedical perception tasks, yet their clinical utility remains limited by inadequate multi-modal explainability and suboptimal prognostic capabilities. Here, we present XMedGPT, a clinician-centric, multi-modal AI assistant that integrates textual and visual interpretability to support transparent and trustworthy medical decision-making. XMedGPT not only produces accurate diagnostic and descriptive outputs, but also grounds referenced anatomical sites within medical images, bridging critical gaps in interpretability and enhancing clinician usability. To support real-world deployment, we introduce a reliability indexing mechanism that quantifies uncertainty through consistency-based assessment via interactive question-answering. We validate XMedGPT across four pillars: multi-modal interpretability, uncertainty quantification, and prognostic modeling, and rigorous benchmarking. The model achieves an IoU of 0.703 across 141 anatomical regions, and a Kendall's tau-b of 0.479, demonstrating strong alignment between visual rationales and clinical outcomes. For uncertainty estimation, it attains an AUC of 0.862 on visual question answering and 0.764 on radiology report generation. In survival and recurrence prediction for lung and glioma cancers, it surpasses prior leading models by 26.9%, and outperforms GPT-4o by 25.0%. Rigorous benchmarking across 347 datasets covers 40 imaging modalities and external validation spans 4 anatomical systems confirming exceptional generalizability, with performance gains surpassing existing GMAI by 20.7% for in-domain evaluation and 16.7% on 11,530 in-house data evaluation. Together, XMedGPT represents a significant leap forward in clinician-centric AI integration, offering trustworthy and scalable support for diverse healthcare applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 31.3 -->
                    
                <!-- LLMs: 8.9 -->
                    
                <!-- 3D: 3.7 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- T2I: 1.9 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Datasets: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.7653
                </span>
                <a href="https://arxiv.org/abs/2505.07036" target="_blank" rel="noopener noreferrer">Predicting Diabetes Using Machine Learning: A Comparative Study of Classifiers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Mahade Hasan, Farhana Yasmin
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Diabetes remains a significant health challenge globally, contributing to severe complications like kidney disease, vision loss, and heart issues. The application of machine learning (ML) in healthcare enables efficient and accurate disease prediction, offering avenues for early intervention and pat</span>
                
                <span class="abstract-full" style="display: none;">Diabetes remains a significant health challenge globally, contributing to severe complications like kidney disease, vision loss, and heart issues. The application of machine learning (ML) in healthcare enables efficient and accurate disease prediction, offering avenues for early intervention and patient support. Our study introduces an innovative diabetes prediction framework, leveraging both traditional ML techniques such as Logistic Regression, SVM, Na\"ive Bayes, and Random Forest and advanced ensemble methods like AdaBoost, Gradient Boosting, Extra Trees, and XGBoost. Central to our approach is the development of a novel model, DNet, a hybrid architecture combining Convolutional Neural Network (CNN) and Long Short-Term Memory (LSTM) layers for effective feature extraction and sequential learning. The DNet model comprises an initial convolutional block for capturing essential features, followed by a residual block with skip connections to facilitate efficient information flow. Batch Normalization and Dropout are employed for robust regularization, and an LSTM layer captures temporal dependencies within the data. Using a Kaggle-sourced real-world diabetes dataset, our model evaluation spans cross-validation accuracy, precision, recall, F1 score, and ROC-AUC. Among the models, DNet demonstrates the highest efficacy with an accuracy of 99.79% and an AUC-ROC of 99.98%, establishing its potential for superior diabetes prediction. This robust hybrid architecture showcases the value of combining CNN and LSTM layers, emphasizing its applicability in medical diagnostics and disease prediction tasks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 32.1 -->
                    
                <!-- LLMs: 5.4 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -11.6174
                </span>
                <a href="https://arxiv.org/abs/2305.11471" target="_blank" rel="noopener noreferrer">Concrete Quantum Channels and Algebraic Structure of Abstract Quantum Channels</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: M. N. N. Namboodiri
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This article analyzes the algebraic structure of the set of all quantum channels and its subset consisting of quantum channels that have Holevo representation. The regularity of these semigroups under composition of mappings is analyzed. It is also known that these sets are compact convex sets and, </span>
                
                <span class="abstract-full" style="display: none;">This article analyzes the algebraic structure of the set of all quantum channels and its subset consisting of quantum channels that have Holevo representation. The regularity of these semigroups under composition of mappings is analyzed. It is also known that these sets are compact convex sets and, therefore, rich in geometry. An attempt is made to identify generalized invertible channels and also the idempotent channels. When channels are of the Holevo type, these two problems are fully studied in this article. The motivation behind this study is its applicability to the reversibility of channel transformations and recent developments in resource-destroying channels, which are idempotents. This is related to the coding-encoding problem in quantum information theory. Several examples are provided, with the main examples coming from pre-conditioner maps which assign preconditioners to matrices in numerical linear algebra. Thus, the known pre-conditioner maps are viewed as quantum channels in finite dimensions. In addition, the infinite-dimensional analogue of preconditioners is introduced and certain limit theorems are discussed; this is with an aim to analyze asymptotic methods in quantum channels analogous to problems in asymptotic linear algebra.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 14.5 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Math: 2.7 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -11.8348
                </span>
                <a href="https://arxiv.org/abs/2505.07625" target="_blank" rel="noopener noreferrer">QC-Adviser: Quantum Hardware Recommendations for Solving Industrial Optimization Problems</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Djamel Laps-Bouraba, Markus Zajac, Uta St\"orl
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The availability of quantum hardware via the cloud offers opportunities for new approaches to computing optimization problems in an industrial environment. However, selecting the right quantum hardware is difficult for non-experts due to its technical characteristics. In this paper, we present the Q</span>
                
                <span class="abstract-full" style="display: none;">The availability of quantum hardware via the cloud offers opportunities for new approaches to computing optimization problems in an industrial environment. However, selecting the right quantum hardware is difficult for non-experts due to its technical characteristics. In this paper, we present the QC-Adviser prototype, which supports users in selecting suitable quantum annealer hardware without requiring quantum computing knowledge.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 22.2 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Medicine: 3.3 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Hardware: 1.2 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -11.8453
                </span>
                <a href="https://arxiv.org/abs/2505.06866" target="_blank" rel="noopener noreferrer">Quantum preconditioning method for linear systems problems via Schr\"odingerization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Shi Jin, Nana Liu, Chuwen Ma, Yue Yu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a quantum computational framework that systematically converts classical linear iterative algorithms with fixed iteration operators into their quantum counterparts using the Schr\"odingerization technique [Shi Jin, Nana Liu and Yue Yu, Phys. Rev. Lett., vol. 133 No. 230602,2024]. This is </span>
                
                <span class="abstract-full" style="display: none;">We present a quantum computational framework that systematically converts classical linear iterative algorithms with fixed iteration operators into their quantum counterparts using the Schr\"odingerization technique [Shi Jin, Nana Liu and Yue Yu, Phys. Rev. Lett., vol. 133 No. 230602,2024]. This is achieved by capturing the steady state of the associated differential equations. The Schr\"odingerization technique transforms linear partial and ordinary differential equations into Schr\"odinger-type systems, making them suitable for quantum computing. This is accomplished through the so-called warped phase transformation, which maps the equation into a higher-dimensional space. Building on this framework, we develop a quantum preconditioning algorithm that leverages the well-known BPX multilevel preconditioner for the finite element discretization of the Poisson equation. The algorithm achieves a near-optimal dependence on the number of queries to our established input models, with a complexity of $\mathscr{O}(\text{polylog} \frac{1}{\varepsilon})$ for a target accuracy of $\varepsilon$ when the dimension $d\geq 2$. This improvement results from the Hamiltonian simulation strategy applied to the Schr\"odingerized preconditioning dynamics, coupled with the smoothing of initial data in the extended space.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 12.3 -->
                    
                <!-- Medicine: 6.7 -->
                    
                <!-- Reinforcement Learning: 3.6 -->
                    
                <!-- Math: 3.6 -->
                    
                <!-- Networks: 3.0 -->
                    
                <!-- LLMs: 1.5 -->
                    
                <!-- Pathfinding: 1.4 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -13.0003
                </span>
                <a href="https://arxiv.org/abs/2505.07804" target="_blank" rel="noopener noreferrer">Quon Classical Simulation: Unifying Clifford, Matchgates and Entanglement</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zixuan Feng, Zhengwei Liu, Fan Lu, Ningfeng Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose a unified classical simulation framework for quantum circuits, termed Quon Classical Simulation (QCS), built upon the diagrammatic formalism of the Quon language. Central to this framework is the introduction of magic holes, a topological feature that captures the global source of computa</span>
                
                <span class="abstract-full" style="display: none;">We propose a unified classical simulation framework for quantum circuits, termed Quon Classical Simulation (QCS), built upon the diagrammatic formalism of the Quon language. Central to this framework is the introduction of magic holes, a topological feature that captures the global source of computational hardness in simulating quantum systems. Unlike conventional measures, the complexity of QCS is governed by the topological entanglement entropy associated with these magic holes. We show that Clifford circuits and Matchgate circuits are free of magic holes and thus efficiently simulable within our model. To capture the interaction structure of magic holes, we define a topological tensor network representation and develop novel skein relations and reduction algorithms to simplify circuit representations. This approach significantly improves the efficiency of classical simulations and provides a unified explanation for the tractability of various known quantum circuit classes. Our work offers a new topological perspective on the classical simulability of quantum systems and topological complexity.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 10.9 -->
                    
                <!-- Medicine: 5.5 -->
                    
                <!-- LLMs: 3.5 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- Blockchain: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -13.4002
                </span>
                <a href="https://arxiv.org/abs/2505.06322" target="_blank" rel="noopener noreferrer">Quantum strategies, error bounds, optimality, and duality gaps for multiplayer XOR, $\mathrm{XOR}^{*}$, compiled XOR, $\mathrm{XOR}^{*}$, and strong parallel repetiton of XOR, $\mathrm{XOR}^{*}$, and FFL games</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pete Rigas
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We characterize exact, and approximate, optimality of games that players can interact with using quantum strategies. In comparison to a previous work of the author, arXiv: 2311.12887, which applied a 2016 framework due to Ostrev for constructing error bounds beyond CHSH and XOR games, in addition to</span>
                
                <span class="abstract-full" style="display: none;">We characterize exact, and approximate, optimality of games that players can interact with using quantum strategies. In comparison to a previous work of the author, arXiv: 2311.12887, which applied a 2016 framework due to Ostrev for constructing error bounds beyond CHSH and XOR games, in addition to the existence of well-posed semidefinite programs for determining primal feasible solutions, along with quantum-classical duality gaps, it continues to remain of interest to further develop the construction of error bounds, and related objects, to game-theoretic settings with several participants. In such settings, one encounters a rich information theoretic landscape, not only from the fact that there exists a significantly larger combinatorial space of possible strategies for each player, but also several opportunities for pronounced quantum advantage. We conclude this effort by describing other variants of other possible strategies, as proposed sources for quantum advantage, in $\mathrm{XOR}^{*}$, compiled $\mathrm{XOR}^{*}$, and strong parallel repetition variants of $\mathrm{XOR}^{*}$ games.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 10.2 -->
                    
                <!-- Medicine: 8.3 -->
                    
                <!-- LLMs: 6.3 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -13.8358
                </span>
                <a href="https://arxiv.org/abs/2212.13089" target="_blank" rel="noopener noreferrer">New protocols for quantum key distribution with explicit upper and lower bound on secret key rate</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Arindam Dutta, Anirban Pathak
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present two new schemes for quantum key distribution (QKD) that neither require entanglement nor an ideal single-photon source, making them implementable with commercially available single-photon sources. These protocols are shown to be secure against multiple attacks, including intercept-resend </span>
                
                <span class="abstract-full" style="display: none;">We present two new schemes for quantum key distribution (QKD) that neither require entanglement nor an ideal single-photon source, making them implementable with commercially available single-photon sources. These protocols are shown to be secure against multiple attacks, including intercept-resend and a class of collective attacks. We derive bounds on the key rate and demonstrate that a specific type of classical pre-processing can increase the tolerable error limit. A trade-off between quantum resources and information revealed to an eavesdropper (Eve) is observed, with higher efficiency achievable through the use of additional quantum resources. Specifically, our proposed protocols outperform the SARG04 protocol in terms of efficiency at the cost of more quantum resources.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 17.3 -->
                    
                <!-- Medicine: 4.8 -->
                    
                <!-- LLMs: 4.7 -->
                    
                <!-- Reinforcement Learning: 2.5 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -15.9797
                </span>
                <a href="https://arxiv.org/abs/2505.06347" target="_blank" rel="noopener noreferrer">Quantum State Preparation via Large-Language-Model-Driven Evolution</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Qing-Hong Cao, Zong-Yue Hou, Ying-Ying Li, Xiaohui Liu, Zhuo-Yang Song, Liang-Qi Zhang, Shutao Zhang, Ke Zhao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose an automated framework for quantum circuit design by integrating large-language models (LLMs) with evolutionary optimization to overcome the rigidity, scalability limitations, and expert dependence of traditional ones in variational quantum algorithms. Our approach (FunSearch) autonomousl</span>
                
                <span class="abstract-full" style="display: none;">We propose an automated framework for quantum circuit design by integrating large-language models (LLMs) with evolutionary optimization to overcome the rigidity, scalability limitations, and expert dependence of traditional ones in variational quantum algorithms. Our approach (FunSearch) autonomously discovers hardware-efficient ans\"atze with new features of scalability and system-size-independent number of variational parameters entirely from scratch. Demonstrations on the Ising and XY spin chains with n = 9 qubits yield circuits containing 4 parameters, achieving near-exact energy extrapolation across system sizes. Implementations on quantum hardware (Zuchongzhi chip) validate practicality, where two-qubit quantum gate noises can be effectively mitigated via zero-noise extrapolations for a spin chain system as large as 20 sites. This framework bridges algorithmic design and experimental constraints, complementing contemporary quantum architecture search frameworks to advance scalable quantum simulations.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 21.6 -->
                    
                <!-- LLMs: 6.4 -->
                    
                <!-- Medicine: 6.0 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -17.2592
                </span>
                <a href="https://arxiv.org/abs/2505.06928" target="_blank" rel="noopener noreferrer">Unraveling Quantum Environments: Transformer-Assisted Learning in Lindblad Dynamics</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Chi-Sheng Chen, En-Jui Kuo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Understanding dissipation in open quantum systems is crucial for the development of robust quantum technologies. In this work, we introduce a Transformer-based machine learning framework to infer time-dependent dissipation rates in quantum systems governed by the Lindblad master equation. Our approa</span>
                
                <span class="abstract-full" style="display: none;">Understanding dissipation in open quantum systems is crucial for the development of robust quantum technologies. In this work, we introduce a Transformer-based machine learning framework to infer time-dependent dissipation rates in quantum systems governed by the Lindblad master equation. Our approach uses time series of observable quantities, such as expectation values of single Pauli operators, as input to learn dissipation profiles without requiring knowledge of the initial quantum state or even the system Hamiltonian.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 21.5 -->
                    
                <!-- LLMs: 6.2 -->
                    
                <!-- Medicine: 2.6 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -19.2461
                </span>
                <a href="https://arxiv.org/abs/2505.07711" target="_blank" rel="noopener noreferrer">Circuit Partitioning Using Large Language Models for Quantum Compilation and Simulations</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Pranav Sinha, Sumit Kumar Jha, Sunny Raj
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where quantum computers are limited by noisy gates, some of which are more error-prone than others and can render the final computation incomprehensible. Quantum circuit compilation algorithms attempt to minimize these noisy gat</span>
                
                <span class="abstract-full" style="display: none;">We are in the midst of the noisy intermediate-scale quantum (NISQ) era, where quantum computers are limited by noisy gates, some of which are more error-prone than others and can render the final computation incomprehensible. Quantum circuit compilation algorithms attempt to minimize these noisy gates when mapping quantum algorithms onto quantum hardware but face computational challenges that restrict their application to circuits with no more than 5-6 qubits, necessitating the need to partition large circuits before the application of noisy quantum gate minimization algorithms. The existing generation of these algorithms is heuristic in nature and does not account for downstream gate minimization tasks. Large language models (LLMs) have the potential to change this and help improve quantum circuit partitions. This paper investigates the use of LLMs, such as Llama and Mistral, for partitioning quantum circuits by capitalizing on their abilities to understand and generate code, including QASM. Specifically, we teach LLMs to partition circuits using the quick partition approach of the Berkeley Quantum Synthesis Toolkit. Through experimental evaluations, we show that careful fine-tuning of open source LLMs enables us to obtain an accuracy of 53.4% for the partition task while over-the-shelf LLMs are unable to correctly partition circuits, using standard 1-shot and few-shot training approaches.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 16.6 -->
                    
                <!-- LLMs: 16.4 -->
                    
                <!-- Medicine: 2.8 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -21.5276
                </span>
                <a href="https://arxiv.org/abs/2505.06404" target="_blank" rel="noopener noreferrer">The Quantum Approximate Optimization Algorithm Can Require Exponential Time to Optimize Linear Functions</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Francisco Chicano, Zakaria Abdelmoiz Dahi, Gabiel Luque
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">QAOA is a hybrid quantum-classical algorithm to solve optimization problems in gate-based quantum computers. It is based on a variational quantum circuit that can be interpreted as a discretization of the annealing process that quantum annealers follow to find a minimum energy state of a given Hamil</span>
                
                <span class="abstract-full" style="display: none;">QAOA is a hybrid quantum-classical algorithm to solve optimization problems in gate-based quantum computers. It is based on a variational quantum circuit that can be interpreted as a discretization of the annealing process that quantum annealers follow to find a minimum energy state of a given Hamiltonian. This ensures that QAOA must find an optimal solution for any given optimization problem when the number of layers, $p$, used in the variational quantum circuit tends to infinity. In practice, the number of layers is usually bounded by a small number. This is a must in current quantum computers of the NISQ era, due to the depth limit of the circuits they can run to avoid problems with decoherence and noise. In this paper, we show mathematical evidence that QAOA requires exponential time to solve linear functions when the number of layers is less than the number of different coefficients of the linear function $n$. We conjecture that QAOA needs exponential time to find the global optimum of linear functions for any constant value of $p$, and that the runtime is linear only if $p \geq n$. We conclude that we need new quantum algorithms to reach quantum supremacy in quantum optimization.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 21.5 -->
                    
                <!-- Math: 3.7 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Networks: 3.1 -->
                    
                <!-- Reinforcement Learning: 2.9 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -23.9832
                </span>
                <a href="https://arxiv.org/abs/2505.06945" target="_blank" rel="noopener noreferrer">A systematic review of challenges and proposed solutions in modeling multimodal data</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Maryam Farhadizadeh (Institute of General Practice/Family Medicine, Faculty of Medicine and Medical Center - University of Freiburg, Germany, Freiburg Center for Data Analysis, Modeling and AI, University of Freiburg, Germany), Maria Weymann (Freiburg Center for Data Analysis, Modeling and AI, University of Freiburg, Germany, Institute of Medical Biometry and Statistics, Faculty of Medicine and Medical Center - University of Freiburg, Germany), Michael Bla{\ss} (Institute for Applied Medical Informatics, University Medical Center Hamburg-Eppendorf, Germany), Johann Kraus (Institute of Medical Systems Biology, Ulm University, Germany), Christopher Gundler (Institute for Applied Medical Informatics, University Medical Center Hamburg-Eppendorf, Germany), Sebastian Walter (Department of Computer Science, Faculty of Engineering - University of Freiburg, Germany), Noah Hempen (Institute of General Practice/Family Medicine, Faculty of Medicine and Medical Center - University of Freiburg, Germany), Harald Binde (Freiburg Center for Data Analysis, Modeling and AI, University of Freiburg, Germany, Institute of Medical Biometry and Statistics, Faculty of Medicine and Medical Center - University of Freiburg, Germany), Nadine Binder (Institute of General Practice/Family Medicine, Faculty of Medicine and Medical Center - University of Freiburg, Germany, Freiburg Center for Data Analysis, Modeling and AI, University of Freiburg, Germany)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Multimodal data modeling has emerged as a powerful approach in clinical research, enabling the integration of diverse data types such as imaging, genomics, wearable sensors, and electronic health records. Despite its potential to improve diagnostic accuracy and support personalized care, modeling su</span>
                
                <span class="abstract-full" style="display: none;">Multimodal data modeling has emerged as a powerful approach in clinical research, enabling the integration of diverse data types such as imaging, genomics, wearable sensors, and electronic health records. Despite its potential to improve diagnostic accuracy and support personalized care, modeling such heterogeneous data presents significant technical challenges. This systematic review synthesizes findings from 69 studies to identify common obstacles, including missing modalities, limited sample sizes, dimensionality imbalance, interpretability issues, and finding the optimal fusion techniques. We highlight recent methodological advances, such as transfer learning, generative models, attention mechanisms, and neural architecture search that offer promising solutions. By mapping current trends and innovations, this review provides a comprehensive overview of the field and offers practical insights to guide future research and development in multimodal modeling for medical applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 39.4 -->
                    
                <!-- LLMs: 17.3 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -41.3323
                </span>
                <a href="https://arxiv.org/abs/2505.07243" target="_blank" rel="noopener noreferrer">A Black-box Testing Framework for Oracle Quantum Programs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Peixun Long, Jianjun Zhao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Oracle quantum programs are a fundamental class of quantum programs that serve as a critical bridge between quantum computing and classical computing. Many important quantum algorithms are built upon oracle quantum programs, making it essential to ensure their correctness during development. While s</span>
                
                <span class="abstract-full" style="display: none;">Oracle quantum programs are a fundamental class of quantum programs that serve as a critical bridge between quantum computing and classical computing. Many important quantum algorithms are built upon oracle quantum programs, making it essential to ensure their correctness during development. While software testing is a well-established approach for improving program reliability, no systematic method has been developed to test oracle quantum programs. This paper proposes a black-box testing framework designed for general oracle quantum programs. We define these programs formally, establish the foundational theory for their testing, and propose a detailed testing framework. We develop a prototype tool and conduct extensive experimental evaluations to evaluate the framework's effectiveness. Our results demonstrate that the proposed framework significantly aids developers in testing oracle quantum programs, providing insights to enhance the reliability of quantum software.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 46.5 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Networks: 2.9 -->
                    
                <!-- Medicine: 2.1 -->
                    
                <!-- GNN: 2.0 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -64.5036
                </span>
                <a href="https://arxiv.org/abs/2504.03529" target="_blank" rel="noopener noreferrer">PHOENIX: Pauli-Based High-Level Optimization Engine for Instruction Execution on NISQ Devices</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhaohui Yang, Dawei Ding, Chenghong Zhu, Jianxin Chen, Yuan Xie
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Variational quantum algorithms (VQA) based on Hamiltonian simulation represent a specialized class of quantum programs well-suited for near-term quantum computing applications due to its modest resource requirements in terms of qubits and circuit depth. Unlike the conventional single-qubit (1Q) and </span>
                
                <span class="abstract-full" style="display: none;">Variational quantum algorithms (VQA) based on Hamiltonian simulation represent a specialized class of quantum programs well-suited for near-term quantum computing applications due to its modest resource requirements in terms of qubits and circuit depth. Unlike the conventional single-qubit (1Q) and two-qubit (2Q) gate sequence representation, Hamiltonian simulation programs are essentially composed of disciplined subroutines known as Pauli exponentiations (Pauli strings with coefficients) that are variably arranged. To capitalize on these distinct program features, this study introduces PHOENIX, a highly effective compilation framework that primarily operates at the high-level Pauli-based intermediate representation (IR) for generic Hamiltonian simulation programs. PHOENIX exploits global program optimization opportunities to the greatest extent, compared to existing SOTA methods despite some of them also utilizing similar IRs. PHOENIX employs the binary symplectic form (BSF) to formally describe Pauli strings and reformulates IR synthesis as reducing the column weights of BSF by appropriate Clifford transformations. It comes with a heuristic BSF simplification algorithm that searches for the most appropriate 2Q Clifford operators in sequence to maximally simplify the BSF at each step, until the BSF can be directly synthesized by basic 1Q and 2Q gates. PHOENIX further performs a global ordering strategy in a Tetris-like fashion for these simplified IR groups, carefully balancing optimization opportunities for gate cancellation, minimizing circuit depth, and managing qubit routing overhead. Experimental results demonstrate that PHOENIX outperforms SOTA VQA compilers across diverse program categories, backend ISAs, and hardware topologies.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #d37d97" title="Confidence: 80.6%">
                            Quantum Computing
                        </span>
                <!-- Medicine: 4.2 -->
                    
                <!-- LLMs: 3.3 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- 3D: 1.5 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                
            </div>
        </div>
        
    </div>
    
    <div class="date-section">
        <h2 class="date-header">2025-05-12</h2>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    1.2262
                </span>
                <a href="https://arxiv.org/abs/2505.05609" target="_blank" rel="noopener noreferrer">On Corruption-Robustness in Performative Reinforcement Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vasilis Pollatos, Debmalya Mandal, Goran Radanovic
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In performative Reinforcement Learning (RL), an agent faces a policy-dependent environment: the reward and transition functions depend on the agent's policy. Prior work on performative RL has studied the convergence of repeated retraining approaches to a performatively stable policy. In the finite s</span>
                
                <span class="abstract-full" style="display: none;">In performative Reinforcement Learning (RL), an agent faces a policy-dependent environment: the reward and transition functions depend on the agent's policy. Prior work on performative RL has studied the convergence of repeated retraining approaches to a performatively stable policy. In the finite sample regime, these approaches repeatedly solve for a saddle point of a convex-concave objective, which estimates the Lagrangian of a regularized version of the reinforcement learning problem. In this paper, we aim to extend such repeated retraining approaches, enabling them to operate under corrupted data. More specifically, we consider Huber's $\epsilon$-contamination model, where an $\epsilon$ fraction of data points is corrupted by arbitrary adversarial noise. We propose a repeated retraining approach based on convex-concave optimization under corrupted gradients and a novel problem-specific robust mean estimator for the gradients. We prove that our approach exhibits last-iterate convergence to an approximately stable policy, with the approximation error linear in $\sqrt{\epsilon}$. We experimentally demonstrate the importance of accounting for corruption in performative RL.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 7.4 -->
                    
                <!-- Medicine: 3.3 -->
                    
                <!-- LLMs: 3.1 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Robotics: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    1.0326
                </span>
                <a href="https://arxiv.org/abs/2312.08365" target="_blank" rel="noopener noreferrer">An Invitation to Deep Reinforcement Learning</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bernhard Jaeger, Andreas Geiger
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Training a deep neural network to maximize a target objective has become the standard recipe for successful machine learning over the last decade. These networks can be optimized with supervised learning, if the target objective is differentiable. For many interesting problems, this is however not t</span>
                
                <span class="abstract-full" style="display: none;">Training a deep neural network to maximize a target objective has become the standard recipe for successful machine learning over the last decade. These networks can be optimized with supervised learning, if the target objective is differentiable. For many interesting problems, this is however not the case. Common objectives like intersection over union (IoU), bilingual evaluation understudy (BLEU) score or rewards cannot be optimized with supervised learning. A common workaround is to define differentiable surrogate losses, leading to suboptimal solutions with respect to the actual objective. Reinforcement learning (RL) has emerged as a promising alternative for optimizing deep neural networks to maximize non-differentiable objectives in recent years. Examples include aligning large language models via human feedback, code generation, object detection or control problems. This makes RL techniques relevant to the larger machine learning audience. The subject is, however, time intensive to approach due to the large range of methods, as well as the often very theoretical presentation. In this introduction, we take an alternative approach, different from classic reinforcement learning textbooks. Rather than focusing on tabular problems, we introduce reinforcement learning as a generalization of supervised learning, which we first apply to non-differentiable objectives and later to temporal problems. Assuming only basic knowledge of supervised learning, the reader will be able to understand state-of-the-art deep RL algorithms like proximal policy optimization (PPO) after reading this tutorial.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 6.9 -->
                    
                <!-- Networks: 3.5 -->
                    
                <!-- GNN: 3.0 -->
                    
                <!-- LLMs: 2.9 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Medicine: 2.6 -->
                    
                <!-- Federated Learning: 1.7 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.9386
                </span>
                <a href="https://arxiv.org/abs/2504.06751" target="_blank" rel="noopener noreferrer">Visualization of a multidimensional point cloud as a 3D swarm of avatars</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Leszek Luchowski, Dariusz Pojda
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The article presents an innovative approach to the visualization of multidimensional data, using icons inspired by Chernoff faces. The approach merges classical projection techniques with the assignment of particular data dimensions to mimic features, capitalizing on the natural ability of the human</span>
                
                <span class="abstract-full" style="display: none;">The article presents an innovative approach to the visualization of multidimensional data, using icons inspired by Chernoff faces. The approach merges classical projection techniques with the assignment of particular data dimensions to mimic features, capitalizing on the natural ability of the human brain to interpret facial expressions. We introduce a semantic division of data dimensions into intuitive and technical categories, assigning the former to avatar features and projecting the latter into a hyperspace of four, or potentially more dimensions. The technique is implemented as a plugin to the dpVision open-source image handling platform. The plugin allows the data to be interactively explored in the form of a swarm of avatars whose position in hyperspace as well as facial features represent various aspects of the data. Sample visualizations, based on synthetic test data as well as the 12-dimensional database on Portuguese Vinho Verde wines, confirm the usefulness of our approach to the analysis of complex data structures.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.0 -->
                    
                <!-- Medicine: 4.1 -->
                    
                <!-- Networks: 3.7 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.8906
                </span>
                <a href="https://arxiv.org/abs/2505.05967" target="_blank" rel="noopener noreferrer">Learning Power Control Protocol for In-Factory 6G Subnetworks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Uyoata E. Uyoata, Gilberto Berardinelli, Ramoni Adeogun
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">In-X Subnetworks are envisioned to meet the stringent demands of short-range communication in diverse 6G use cases. In the context of In-Factory scenarios, effective power control is critical to mitigating the impact of interference resulting from potentially high subnetwork density. Existing approa</span>
                
                <span class="abstract-full" style="display: none;">In-X Subnetworks are envisioned to meet the stringent demands of short-range communication in diverse 6G use cases. In the context of In-Factory scenarios, effective power control is critical to mitigating the impact of interference resulting from potentially high subnetwork density. Existing approaches to power control in this domain have predominantly emphasized the data plane, often overlooking the impact of signaling overhead. Furthermore, prior work has typically adopted a network-centric perspective, relying on the assumption of complete and up-to-date channel state information (CSI) being readily available at the central controller. This paper introduces a novel multi-agent reinforcement learning (MARL) framework designed to enable access points to autonomously learn both signaling and power control protocols in an In-Factory Subnetwork environment. By formulating the problem as a partially observable Markov decision process (POMDP) and leveraging multi-agent proximal policy optimization (MAPPO), the proposed approach achieves significant advantages. The simulation results demonstrate that the learning-based method reduces signaling overhead by a factor of 8 while maintaining a buffer flush rate that lags the ideal "Genie" approach by only 5%.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.3 -->
                    
                <!-- LLMs: 4.3 -->
                    
                <!-- Networks: 3.2 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- Medicine: 2.7 -->
                    
                <!-- GNN: 1.6 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.7172
                </span>
                <a href="https://arxiv.org/abs/2405.21027" target="_blank" rel="noopener noreferrer">Fusion-PSRO: Nash Policy Fusion for Policy Space Response Oracles</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jiesong Lian, Yucong Huang, Chengdong Ma, Mingzhi Wang, Ying Wen, Long Hu, Yixue Hao
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">For solving zero-sum games involving non-transitivity, a useful approach is to maintain a policy population to approximate the Nash Equilibrium (NE). Previous studies have shown that the Policy Space Response Oracles (PSRO) algorithm is an effective framework for solving such games. However, current</span>
                
                <span class="abstract-full" style="display: none;">For solving zero-sum games involving non-transitivity, a useful approach is to maintain a policy population to approximate the Nash Equilibrium (NE). Previous studies have shown that the Policy Space Response Oracles (PSRO) algorithm is an effective framework for solving such games. However, current methods initialize a new policy from scratch or inherit a single historical policy in Best Response (BR), missing the opportunity to leverage past policies to generate a better BR. In this paper, we propose Fusion-PSRO, which employs Nash Policy Fusion to initialize a new policy for BR training. Nash Policy Fusion serves as an implicit guiding policy that starts exploration on the current Meta-NE, thus providing a closer approximation to BR. Moreover, it insightfully captures a weighted moving average of past policies, dynamically adjusting these weights based on the Meta-NE in each iteration. This cumulative process further enhances the policy population. Empirical results on classic benchmarks show that Fusion-PSRO achieves lower exploitability, thereby mitigating the shortcomings of previous research on policy initialization in BR.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 5.7 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Networks: 3.5 -->
                    
                <!-- Quantum Computing: 2.8 -->
                    
                <!-- GNN: 2.7 -->
                    
                <!-- Medicine: 2.3 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Evolutionary Algorithms: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-positive">
                    0.4654
                </span>
                <a href="https://arxiv.org/abs/2505.05724" target="_blank" rel="noopener noreferrer">Towards Secure Semantic Transmission In the Era of GenAI: A Diffusion-based Framework</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Boxiang He, Zihan Chen, Junshan Luo, Chuanhong Liu, Shilian Wang, Fanggang Wang, Tony Q. S. Quek
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Semantic communication, due to its focus on the transmitting meaning rather than the raw bit data, poses unique security challenges compared to the traditional communication systems. In particular, semantic communication systems are vulnerable to the malicious attacks that focus on the semantic laye</span>
                
                <span class="abstract-full" style="display: none;">Semantic communication, due to its focus on the transmitting meaning rather than the raw bit data, poses unique security challenges compared to the traditional communication systems. In particular, semantic communication systems are vulnerable to the malicious attacks that focus on the semantic layer, with the intention of understanding or distorting the intended meaning of the transmitted privacy data. Diffusion models, a class of generative artificial intelligence (GenAI), are well-suited for ensuring data security to attack. Through iteratively adding and then removing noise, diffusion models can generate meaningful information despite the presence of the unknown noise. This article proposes a diffusion-based framework to enhance the security of semantic transmission for the attacks including eavesdropping and jamming. Specifically, the proposed framework incorporates both the artificial noise and natural channel noise into the forward process of the diffusion models during the semantic transmission, with the reverse process used to remove noise at the legitimate receiver. In the eavesdropping scenarios, the artificial noise is the friendly noise designed to prevent semantic eavesdropping. In the jamming scenarios, the artificial noise is the malicious jamming generated by the jammer, which disrupts the semantic transmission. The case studies show that the proposed diffusion-based framework is promising in securing the semantic transmission. We also consolidate several broad research directions associated with the proposed framework.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Reinforcement Learning: 7.2 -->
                    
                <!-- Networks: 6.4 -->
                    
                <!-- Math: 5.9 -->
                    
                <!-- Medicine: 3.9 -->
                    
                <!-- Federated Learning: 2.8 -->
                    
                <!-- Pathfinding: 2.4 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Multi-armed Bandit: 1.1 -->
                    
                <!-- LLMs: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5681
                </span>
                <a href="https://arxiv.org/abs/2402.00455" target="_blank" rel="noopener noreferrer">Generalized Arlery-Tan-Rabaste-Levenshtein Lower Bounds on Ambiguity Function and Their Asymptotic Achievability</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Lingsheng Meng, Yong Liang Guan, Yao Ge, Zilong Liu, Pingzhi Fan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This paper presents generalized Arlery-Tan-Rabaste-Levenshtein lower bounds on the maximum aperiodic ambiguity function (AF) magnitude of unimodular sequences under certain delay-Doppler low ambiguity zones (LAZ). Our core idea is to explore the upper and lower bounds on the Frobenius norm of the we</span>
                
                <span class="abstract-full" style="display: none;">This paper presents generalized Arlery-Tan-Rabaste-Levenshtein lower bounds on the maximum aperiodic ambiguity function (AF) magnitude of unimodular sequences under certain delay-Doppler low ambiguity zones (LAZ). Our core idea is to explore the upper and lower bounds on the Frobenius norm of the weighted auto- and cross-AF matrices by introducing two weight vectors associated with the delay and Doppler shifts, respectively. As a second major contribution, we demonstrate that our derived lower bounds are asymptotically achievable with selected Chu sequence sets by analyzing their maximum auto- and cross- AF magnitudes within certain LAZ.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.8 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Math: 2.3 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Networks: 1.7 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- T2I: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.5967
                </span>
                <a href="https://arxiv.org/abs/2504.04202" target="_blank" rel="noopener noreferrer">Directional Sign Loss: A Topology-Preserving Loss Function that Approximates the Sign of Finite Differences</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Harvey Dam, Tripti Agarwal, Ganesh Gopalakrishnan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Preserving critical topological features in learned latent spaces is a fundamental challenge in representation learning, particularly for topology-sensitive data. This paper introduces directional sign loss (DSL), a novel loss function that approximates the number of mismatches in the signs of finit</span>
                
                <span class="abstract-full" style="display: none;">Preserving critical topological features in learned latent spaces is a fundamental challenge in representation learning, particularly for topology-sensitive data. This paper introduces directional sign loss (DSL), a novel loss function that approximates the number of mismatches in the signs of finite differences between corresponding elements of two arrays. By penalizing discrepancies in critical points between input and reconstructed data, DSL encourages autoencoders and other learnable compressors to retain the topological features of the original data. We present the mathematical formulation, complexity analysis, and practical implementation of DSL, comparing its behavior to its non-differentiable counterpart and to other topological measures. Experiments on one-, two-, and three-dimensional data show that combining DSL with traditional loss functions preserves topological features more effectively than traditional losses alone. Moreover, DSL serves as a differentiable, efficient proxy for common topology-based metrics, enabling its use in gradient-based optimization frameworks.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.1 -->
                    
                <!-- Medicine: 5.1 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Math: 1.6 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.6958
                </span>
                <a href="https://arxiv.org/abs/2505.05591" target="_blank" rel="noopener noreferrer">QuickSplat: Fast 3D Surface Reconstruction via Learned Gaussian Initialization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Yueh-Cheng Liu, Lukas H\"ollein, Matthias Nie{\ss}ner, Angela Dai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Surface reconstruction is fundamental to computer vision and graphics, enabling applications in 3D modeling, mixed reality, robotics, and more. Existing approaches based on volumetric rendering obtain promising results, but optimize on a per-scene basis, resulting in a slow optimization that can str</span>
                
                <span class="abstract-full" style="display: none;">Surface reconstruction is fundamental to computer vision and graphics, enabling applications in 3D modeling, mixed reality, robotics, and more. Existing approaches based on volumetric rendering obtain promising results, but optimize on a per-scene basis, resulting in a slow optimization that can struggle to model under-observed or textureless regions. We introduce QuickSplat, which learns data-driven priors to generate dense initializations for 2D gaussian splatting optimization of large-scale indoor scenes. This provides a strong starting point for the reconstruction, which accelerates the convergence of the optimization and improves the geometry of flat wall structures. We further learn to jointly estimate the densification and update of the scene parameters during each iteration; our proposed densifier network predicts new Gaussians based on the rendering gradients of existing ones, removing the needs of heuristics for densification. Extensive experiments on large-scale indoor scene reconstruction demonstrate the superiority of our data-driven optimization. Concretely, we accelerate runtime by 8x, while decreasing depth errors by up to 48% in comparison to state of the art methods.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.1 -->
                    
                <!-- LLMs: 4.4 -->
                    
                <!-- Reinforcement Learning: 3.1 -->
                    
                <!-- Quantum Computing: 2.5 -->
                    
                <!-- Networks: 2.3 -->
                    
                <!-- 3D: 1.8 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7256
                </span>
                <a href="https://arxiv.org/abs/2505.05880" target="_blank" rel="noopener noreferrer">Combining Abstract Argumentation and Machine Learning for Efficiently Analyzing Low-Level Process Event Streams</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bettina Fazzinga, Sergio Flesca, Filippo Furfaro, Luigi Pontieri, Francesco Scala
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Monitoring and analyzing process traces is a critical task for modern companies and organizations. In scenarios where there is a gap between trace events and reference business activities, this entails an interpretation problem, amounting to translating each event of any ongoing trace into the corre</span>
                
                <span class="abstract-full" style="display: none;">Monitoring and analyzing process traces is a critical task for modern companies and organizations. In scenarios where there is a gap between trace events and reference business activities, this entails an interpretation problem, amounting to translating each event of any ongoing trace into the corresponding step of the activity instance. Building on a recent approach that frames the interpretation problem as an acceptance problem within an Abstract Argumentation Framework (AAF), one can elegantly analyze plausible event interpretations (possibly in an aggregated form), as well as offer explanations for those that conflict with prior process knowledge. Since, in settings where event-to-activity mapping is highly uncertain (or simply under-specified) this reasoning-based approach may yield lowly-informative results and heavy computation, one can think of discovering a sequencetagging model, trained to suggest highly-probable candidate event interpretations in a context-aware way. However, training such a model optimally may require using a large amount of manually-annotated example traces. Considering the urgent need of developing Green AI solutions enabling environmental and societal sustainability (with reduced labor/computational costs and carbon footprint), we propose a data/computation-efficient neuro-symbolic approach to the problem, where the candidate interpretations returned by the example-driven sequence tagger is refined by the AAF-based reasoner. This allows us to also leverage prior knowledge to compensate for the scarcity of example data, as confirmed by experimental results; clearly, this property is particularly useful in settings where data annotation and model optimization costs are subject to stringent constraints.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 5.2 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Robotics: 2.0 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- 3D: 1.3 -->
                    
                <!-- Math: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.7298
                </span>
                <a href="https://arxiv.org/abs/2505.06090" target="_blank" rel="noopener noreferrer">Orthogonal Emptiness Queries for Random Points</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jonathan E. Dullerud, Sariel Har-Peled
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present a data-structure for orthogonal range searching for random points in the plane. The new data-structure uses (in expectation) $O\bigl(n \log n ( \log \log n)^2 \bigr)$ space, and answers emptiness queries in constant time. As a building block, we construct a data-structure of expected line</span>
                
                <span class="abstract-full" style="display: none;">We present a data-structure for orthogonal range searching for random points in the plane. The new data-structure uses (in expectation) $O\bigl(n \log n ( \log \log n)^2 \bigr)$ space, and answers emptiness queries in constant time. As a building block, we construct a data-structure of expected linear size, that can answer predecessor/rank queries, in constant time, for random numbers sampled uniformly from $[0,1]$.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 7.3 -->
                    
                <!-- LLMs: 5.5 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- Networks: 2.6 -->
                    
                <!-- 3D: 2.5 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- Math: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.769
                </span>
                <a href="https://arxiv.org/abs/2504.21619" target="_blank" rel="noopener noreferrer">3D Hand-Eye Calibration for Collaborative Robot Arm: Look at Robot Base Once</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Leihui Li, Lixuepiao Wan, Volker Krueger, Xuping Zhang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Hand-eye calibration is a common problem in the field of collaborative robotics, involving the determination of the transformation matrix between the visual sensor and the robot flange to enable vision-based robotic tasks. However, this process typically requires multiple movements of the robot arm </span>
                
                <span class="abstract-full" style="display: none;">Hand-eye calibration is a common problem in the field of collaborative robotics, involving the determination of the transformation matrix between the visual sensor and the robot flange to enable vision-based robotic tasks. However, this process typically requires multiple movements of the robot arm and an external calibration object, making it both time-consuming and inconvenient, especially in scenarios where frequent recalibration is necessary. In this work, we extend our previous method which eliminates the need for external calibration objects such as a chessboard. We propose a generic dataset generation approach for point cloud registration, focusing on aligning the robot base point cloud with the scanned data. Furthermore, a more detailed simulation study is conducted involving several different collaborative robot arms, followed by real-world experiments in an industrial setting. Our improved method is simulated and evaluated using a total of 14 robotic arms from 9 different brands, including KUKA, Universal Robots, UFACTORY, and Franka Emika, all of which are widely used in the field of collaborative robotics. Physical experiments demonstrate that our extended approach achieves performance comparable to existing commercial hand-eye calibration solutions, while completing the entire calibration procedure in just a few seconds. In addition, we provide a user-friendly hand-eye calibration solution, with the code publicly available at github.com/leihui6/LRBO.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.5 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Robotics: 2.1 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- Math: 1.7 -->
                    
                <!-- Pathfinding: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -1.8695
                </span>
                <a href="https://arxiv.org/abs/2505.04283" target="_blank" rel="noopener noreferrer">On multiplicities of interpoint distances</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Felix Christian Clemen, Adrian Dumitrescu, Dingyuan Liu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Given a set $X\subseteq\mathbb{R}^2$ of $n$ points and a distance $d>0$, the multiplicity of $d$ is the number of times the distance $d$ appears between points in $X$. Let $a_1(X) \geq a_2(X) \geq \cdots \geq a_m(X)$ denote the multiplicities of the $m$ distances determined by $X$ and let $a(X)=\lef</span>
                
                <span class="abstract-full" style="display: none;">Given a set $X\subseteq\mathbb{R}^2$ of $n$ points and a distance $d>0$, the multiplicity of $d$ is the number of times the distance $d$ appears between points in $X$. Let $a_1(X) \geq a_2(X) \geq \cdots \geq a_m(X)$ denote the multiplicities of the $m$ distances determined by $X$ and let $a(X)=\left(a_1(X),\dots,a_m(X)\right)$. In this paper, we study several questions from Erd\H{o}s's time regarding distance multiplicities. Among other results, we show that:</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 7.9 -->
                    
                <!-- Medicine: 6.6 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.2 -->
                    
                <!-- Math: 2.2 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Pathfinding: 1.3 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2163
                </span>
                <a href="https://arxiv.org/abs/2505.00384" target="_blank" rel="noopener noreferrer">Improving the scalability of a high-order atmospheric dynamics solver based on the deal.II library</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Giuseppe Orlando, Tommaso Benacchio, Luca Bonaventura
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We present recent advances on the massively parallel performance of a numerical scheme for atmosphere dynamics applications based on the deal.II library. The implicit-explicit discontinuous finite element scheme is based on a matrix-free approach, meaning that no global sparse matrix is built and on</span>
                
                <span class="abstract-full" style="display: none;">We present recent advances on the massively parallel performance of a numerical scheme for atmosphere dynamics applications based on the deal.II library. The implicit-explicit discontinuous finite element scheme is based on a matrix-free approach, meaning that no global sparse matrix is built and only the action of the linear operators on a vector is actually implemented. Following a profiling analysis, we focus on the performance optimization of the numerical method and describe the impact of different preconditioning and solving techniques in this framework. Moreover, we show how the use of the latest version of the deal.II library and of suitable execution flags can improve the parallel performance.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.5 -->
                    
                <!-- Math: 5.3 -->
                    
                <!-- Reinforcement Learning: 4.7 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- LLMs: 1.7 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.2696
                </span>
                <a href="https://arxiv.org/abs/2505.05723" target="_blank" rel="noopener noreferrer">Unfitted finite element modelling of surface-bulk viscous flows in animal cells</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Eric Neiva, Herv\'e Turlier
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">This work presents a novel unfitted finite element framework to simulate coupled surface-bulk problems in time-dependent domains, focusing on fluid-fluid interactions in animal cells between the actomyosin cortex and the cytoplasm. The cortex, a thin layer beneath the plasma membrane, provides struc</span>
                
                <span class="abstract-full" style="display: none;">This work presents a novel unfitted finite element framework to simulate coupled surface-bulk problems in time-dependent domains, focusing on fluid-fluid interactions in animal cells between the actomyosin cortex and the cytoplasm. The cortex, a thin layer beneath the plasma membrane, provides structural integrity and drives shape changes by generating surface contractile forces akin to tension. Cortical contractions generate Marangoni-like surface flows and induce intracellular cytoplasmic flows that are essential for processes such as cell division, migration, and polarization, particularly in large animal cells. Despite its importance, the spatiotemporal regulation of cortex-cytoplasm interactions remains poorly understood and computational modelling can be very challenging because surface-bulk dynamics often lead to large cell deformations. To address these challenges, we propose a sharp-interface framework that uniquely combines the trace finite element method for surface flows with the aggregated finite element method for bulk flows. This approach enables accurate and stable simulations on fixed Cartesian grids without remeshing. The model also incorporates mechanochemical feedback through the surface transport of a molecular regulator of active tension. We solve the resulting mixed-dimensional system on a fixed Cartesian grid using a level-set-based method to track the evolving surface. Numerical experiments validate the accuracy and stability of the method, capturing phenomena such as self-organised pattern formation, curvature-driven relaxation, and cell cleavage. This novel framework offers a powerful and extendable tool for investigating increasingly complex morphogenetic processes in animal cells.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 6.8 -->
                    
                <!-- LLMs: 3.8 -->
                    
                <!-- Networks: 2.5 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- 3D: 1.7 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.3452
                </span>
                <a href="https://arxiv.org/abs/2505.06150" target="_blank" rel="noopener noreferrer">A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ryan Lagasse, Aidan Kiernans, Avijit Ghosh, Shiri Dori-Hacohen
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We introduce a scaling law for fine-tuning large language models (LLMs) under fixed compute budgets that explicitly accounts for data composition. Conventional approaches measure training data solely by total tokens, yet the number of examples and their average token length -- what we term \emph{dat</span>
                
                <span class="abstract-full" style="display: none;">We introduce a scaling law for fine-tuning large language models (LLMs) under fixed compute budgets that explicitly accounts for data composition. Conventional approaches measure training data solely by total tokens, yet the number of examples and their average token length -- what we term \emph{dataset volume} -- play a decisive role in model performance. Our formulation is tuned following established procedures. Experiments on the BRICC dataset \cite{salavati2024reducing} and subsets of the MMLU dataset \cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple subsampling strategies, reveal that data composition significantly affects token efficiency. These results motivate refined scaling laws for practical LLM fine-tuning in resource-constrained settings.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 11.3 -->
                    
                <!-- Medicine: 9.0 -->
                    
                <!-- Quantum Computing: 3.4 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- 3D: 1.6 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Math: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.4004
                </span>
                <a href="https://arxiv.org/abs/2505.05846" target="_blank" rel="noopener noreferrer">Representation gaps of rigid planar diagram monoids</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Willow Stewart, Daniel Tubbenhauer
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We define non-pivotal analogs of the Temperley-Lieb, Motzkin, and planar rook monoids, and compute bounds for the sizes of their nontrivial simple representations. From this, we assess the two types of monoids in their relative suitability for use in cryptography by comparing their representation ga</span>
                
                <span class="abstract-full" style="display: none;">We define non-pivotal analogs of the Temperley-Lieb, Motzkin, and planar rook monoids, and compute bounds for the sizes of their nontrivial simple representations. From this, we assess the two types of monoids in their relative suitability for use in cryptography by comparing their representation gaps and gap ratios. We conclude that the non-pivotal monoids are generally worse for cryptographic purposes.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 8.6 -->
                    
                <!-- Medicine: 6.9 -->
                    
                <!-- Quantum Computing: 4.3 -->
                    
                <!-- Math: 2.5 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- 3D: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.5316
                </span>
                <a href="https://arxiv.org/abs/2505.05853" target="_blank" rel="noopener noreferrer">PICD: Versatile Perceptual Image Compression with Diffusion Rendering</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tongda Xu, Jiahao Li, Bin Li, Yan Wang, Ya-Qin Zhang, Yan Lu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Recently, perceptual image compression has achieved significant advancements, delivering high visual quality at low bitrates for natural images. However, for screen content, existing methods often produce noticeable artifacts when compressing text. To tackle this challenge, we propose versatile perc</span>
                
                <span class="abstract-full" style="display: none;">Recently, perceptual image compression has achieved significant advancements, delivering high visual quality at low bitrates for natural images. However, for screen content, existing methods often produce noticeable artifacts when compressing text. To tackle this challenge, we propose versatile perceptual screen image compression with diffusion rendering (PICD), a codec that works well for both screen and natural images. More specifically, we propose a compression framework that encodes the text and image separately, and renders them into one image using diffusion model. For this diffusion rendering, we integrate conditional information into diffusion models at three distinct levels: 1). Domain level: We fine-tune the base diffusion model using text content prompts with screen content. 2). Adaptor level: We develop an efficient adaptor to control the diffusion model using compressed image and text as input. 3). Instance level: We apply instance-wise guidance to further enhance the decoding process. Empirically, our PICD surpasses existing perceptual codecs in terms of both text accuracy and perceptual quality. Additionally, without text conditions, our approach serves effectively as a perceptual codec for natural images.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.9 -->
                    
                <!-- LLMs: 6.4 -->
                    
                <!-- T2I: 3.6 -->
                    
                <!-- Quantum Computing: 3.1 -->
                    
                <!-- 3D: 3.0 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Attention: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.6159
                </span>
                <a href="https://arxiv.org/abs/2505.05812" target="_blank" rel="noopener noreferrer">Towards order of magnitude X-ray dose reduction in breast cancer imaging using phase contrast and deep denoising</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ashkan Pakzad, Robert Turnbull, Simon J. Mutch, Thomas A. Leatham, Darren Lockie, Jane Fox, Beena Kumar, Daniel H\"asermann, Christopher J. Hall, Anton Maksimenko, Benedicta D. Arhatari, Yakov I. Nesterets, Amir Entezam, Seyedamir T. Taba, Patrick C. Brennan, Timur E. Gureyev, Harry M. Quiney
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Breast cancer is the most frequently diagnosed human cancer in the United States at present. Early detection is crucial for its successful treatment. X-ray mammography and digital breast tomosynthesis are currently the main methods for breast cancer screening. However, both have known limitations in</span>
                
                <span class="abstract-full" style="display: none;">Breast cancer is the most frequently diagnosed human cancer in the United States at present. Early detection is crucial for its successful treatment. X-ray mammography and digital breast tomosynthesis are currently the main methods for breast cancer screening. However, both have known limitations in terms of their sensitivity and specificity to breast cancers, while also frequently causing patient discomfort due to the requirement for breast compression. Breast computed tomography is a promising alternative, however, to obtain high-quality images, the X-ray dose needs to be sufficiently high. As the breast is highly radiosensitive, dose reduction is particularly important. Phase-contrast computed tomography (PCT) has been shown to produce higher-quality images at lower doses and has no need for breast compression. It is demonstrated in the present study that, when imaging full fresh mastectomy samples with PCT, deep learning-based image denoising can further reduce the radiation dose by a factor of 16 or more, without any loss of image quality. The image quality has been assessed both in terms of objective metrics, such as spatial resolution and contrast-to-noise ratio, as well as in an observer study by experienced medical imaging specialists and radiologists. This work was carried out in preparation for live patient PCT breast cancer imaging, initially at specialized synchrotron facilities.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.5 -->
                    
                <!-- LLMs: 4.5 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- Networks: 2.4 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Pathfinding: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.8405
                </span>
                <a href="https://arxiv.org/abs/2505.05796" target="_blank" rel="noopener noreferrer">Human-in-the-Loop AI for HVAC Management Enhancing Comfort and Energy Efficiency</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xinyu Liang, Frits de Nijs, Buser Say, Hao Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Heating, Ventilation, and Air Conditioning (HVAC) systems account for approximately 38% of building energy consumption globally, making them one of the most energy-intensive services. The increasing emphasis on energy efficiency and sustainability, combined with the need for enhanced occupant comfor</span>
                
                <span class="abstract-full" style="display: none;">Heating, Ventilation, and Air Conditioning (HVAC) systems account for approximately 38% of building energy consumption globally, making them one of the most energy-intensive services. The increasing emphasis on energy efficiency and sustainability, combined with the need for enhanced occupant comfort, presents a significant challenge for traditional HVAC systems. These systems often fail to dynamically adjust to real-time changes in electricity market rates or individual comfort preferences, leading to increased energy costs and reduced comfort. In response, we propose a Human-in-the-Loop (HITL) Artificial Intelligence framework that optimizes HVAC performance by incorporating real-time user feedback and responding to fluctuating electricity prices. Unlike conventional systems that require predefined information about occupancy or comfort levels, our approach learns and adapts based on ongoing user input. By integrating the occupancy prediction model with reinforcement learning, the system improves operational efficiency and reduces energy costs in line with electricity market dynamics, thereby contributing to demand response initiatives. Through simulations, we demonstrate that our method achieves significant cost reductions compared to baseline approaches while maintaining or enhancing occupant comfort. This feedback-driven approach ensures personalized comfort control without the need for predefined settings, offering a scalable solution that balances individual preferences with economic and environmental goals.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 10.2 -->
                    
                <!-- LLMs: 6.8 -->
                    
                <!-- Quantum Computing: 2.6 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- RAG: 1.4 -->
                    
                <!-- T2I: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -2.926
                </span>
                <a href="https://arxiv.org/abs/2312.08598" target="_blank" rel="noopener noreferrer">MotherNet: Fast Training and Inference via Hyper-Network Transformers</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Andreas M\"uller, Carlo Curino, Raghu Ramakrishnan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Foundation models are transforming machine learning across many modalities, with in-context learning replacing classical model training. Recent work on tabular data hints at a similar opportunity to build foundation models for classification for numerical data. However, existing meta-learning approa</span>
                
                <span class="abstract-full" style="display: none;">Foundation models are transforming machine learning across many modalities, with in-context learning replacing classical model training. Recent work on tabular data hints at a similar opportunity to build foundation models for classification for numerical data. However, existing meta-learning approaches can not compete with tree-based methods in terms of inference time. In this paper, we propose MotherNet, a hypernetwork architecture trained on synthetic classification tasks that, once prompted with a never-seen-before training set generates the weights of a trained ``child'' neural-network by in-context learning using a single forward pass. In contrast to most existing hypernetworks that are usually trained for relatively constrained multi-task settings, MotherNet can create models for multiclass classification on arbitrary tabular datasets without any dataset specific gradient descent. The child network generated by MotherNet outperforms neural networks trained using gradient descent on small datasets, and is comparable to predictions by TabPFN and standard ML methods like Gradient Boosting. Unlike a direct application of TabPFN, MotherNet generated networks are highly efficient at inference time. We also demonstrate that HyperFast is unable to perform effective in-context learning on small datasets, and heavily relies on dataset specific fine-tuning and hyper-parameter tuning, while MotherNet requires no fine-tuning or per-dataset hyper-parameters.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 11.7 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- GNN: 2.7 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- Reinforcement Learning: 1.9 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- T2I: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.1421
                </span>
                <a href="https://arxiv.org/abs/2505.05625" target="_blank" rel="noopener noreferrer">SPIN-ODE: Stiff Physics-Informed Neural ODE for Chemical Reaction Rate Estimation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wenqing Peng, Zhi-Song Liu, Michael Boy
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Estimating rate constants from complex chemical reactions is essential for advancing detailed chemistry. However, the stiffness inherent in real-world atmospheric chemistry systems poses severe challenges, leading to training instability and poor convergence that hinder effective rate constant estim</span>
                
                <span class="abstract-full" style="display: none;">Estimating rate constants from complex chemical reactions is essential for advancing detailed chemistry. However, the stiffness inherent in real-world atmospheric chemistry systems poses severe challenges, leading to training instability and poor convergence that hinder effective rate constant estimation using learning-based approaches. To address this, we propose a Stiff Physics-Informed Neural ODE framework (SPIN-ODE) for chemical reaction modelling. Our method introduces a three-stage optimisation process: first, a latent neural ODE learns the continuous and differentiable trajectory between chemical concentrations and their time derivatives; second, an explicit Chemical Reaction Neural Network (CRNN) extracts the underlying rate coefficients based on the learned dynamics; and third, fine-tune CRNN using a neural ODE solver to further improve rate coefficient estimation. Extensive experiments on both synthetic and newly proposed real-world datasets validate the effectiveness and robustness of our approach. As the first work on stiff Neural ODEs for chemical rate coefficient discovery, our study opens promising directions for integrating neural networks with detailed chemistry.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.6 -->
                    
                <!-- LLMs: 3.0 -->
                    
                <!-- 3D: 2.7 -->
                    
                <!-- GNN: 2.3 -->
                    
                <!-- Reinforcement Learning: 2.3 -->
                    
                <!-- Quantum Computing: 2.3 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2096
                </span>
                <a href="https://arxiv.org/abs/2502.10436" target="_blank" rel="noopener noreferrer">MERGE$^3$: Efficient Evolutionary Merging on Consumer-grade GPUs</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Tommaso Mencattini, Adrian Robert Minut, Donato Crisostomi, Andrea Santilli, Emanuele Rodol\`a
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Evolutionary model merging enables the creation of high-performing multi-task models but remains computationally prohibitive for consumer hardware. We introduce MERGE$^3$, an efficient framework that makes evolutionary merging feasible on a single GPU by reducing fitness computation costs 50$\times$</span>
                
                <span class="abstract-full" style="display: none;">Evolutionary model merging enables the creation of high-performing multi-task models but remains computationally prohibitive for consumer hardware. We introduce MERGE$^3$, an efficient framework that makes evolutionary merging feasible on a single GPU by reducing fitness computation costs 50$\times$ while preserving performance. MERGE$^3$ achieves this by Extracting a reduced dataset for evaluation, Estimating model abilities using Item Response Theory (IRT), and Evolving optimal merges via IRT-based performance estimators. Our method enables state-of-the-art multilingual and cross-lingual merging, transferring knowledge across languages with significantly lower computational overhead. We provide theoretical guarantees and an open-source library, democratizing high-quality model merging.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 12.5 -->
                    
                <!-- LLMs: 9.1 -->
                    
                <!-- Quantum Computing: 3.7 -->
                    
                <!-- 3D: 2.6 -->
                    
                <!-- GNN: 1.9 -->
                    
                <!-- Blockchain: 1.8 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.7 -->
                    
                <!-- RAG: 1.5 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2374
                </span>
                <a href="https://arxiv.org/abs/2505.05659" target="_blank" rel="noopener noreferrer">V-EfficientNets: Vector-Valued Efficiently Scaled Convolutional Neural Network Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Guilherme Vieira Neto, Marcos Eduardo Valle
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">EfficientNet models are convolutional neural networks optimized for parameter allocation by jointly balancing network width, depth, and resolution. Renowned for their exceptional accuracy, these models have become a standard for image classification tasks across diverse computer vision benchmarks. W</span>
                
                <span class="abstract-full" style="display: none;">EfficientNet models are convolutional neural networks optimized for parameter allocation by jointly balancing network width, depth, and resolution. Renowned for their exceptional accuracy, these models have become a standard for image classification tasks across diverse computer vision benchmarks. While traditional neural networks learn correlations between feature channels during training, vector-valued neural networks inherently treat multidimensional data as coherent entities, taking for granted the inter-channel relationships. This paper introduces vector-valued EfficientNets (V-EfficientNets), a novel extension of EfficientNet designed to process arbitrary vector-valued data. The proposed models are evaluated on a medical image classification task, achieving an average accuracy of 99.46% on the ALL-IDB2 dataset for detecting acute lymphoblastic leukemia. V-EfficientNets demonstrate remarkable efficiency, significantly reducing parameters while outperforming state-of-the-art models, including the original EfficientNet. The source code is available at https://github.com/mevalle/v-nets.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.1 -->
                    
                <!-- LLMs: 4.0 -->
                    
                <!-- Quantum Computing: 3.0 -->
                    
                <!-- GNN: 2.9 -->
                    
                <!-- 3D: 2.4 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- RAG: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- SpikingNN: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- T2I: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.2821
                </span>
                <a href="https://arxiv.org/abs/2505.05679" target="_blank" rel="noopener noreferrer">From Bias To Improved Prompts: A Case Study of Bias Mitigation of Clone Detection Models</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: QiHong Chen, Lianghao Jiang, Iftekhar Ahmed
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The issue of clone code has persisted in software engineering, primarily because developers often copy and paste code segments. This common practice has elevated the importance of clone code detection, garnering attention from both software engineering researchers and industry professionals. Their c</span>
                
                <span class="abstract-full" style="display: none;">The issue of clone code has persisted in software engineering, primarily because developers often copy and paste code segments. This common practice has elevated the importance of clone code detection, garnering attention from both software engineering researchers and industry professionals. Their collective concern arises from the potential negative impacts that clone code can have on software quality. The emergence of powerful Generative Large Language Models (LLMs) like ChatGPT has exacerbated the clone code problem. These advanced models possess code generation capabilities that can inadvertently create code clones. As a result, the need to detect clone code has become more critical than ever before. In this study, we assess the suitability of LLMs for clone code detection. Our results demonstrate that the Palm model achieved a high F1 score of 89.30 for the avatar dataset and 86.41 for the poolC dataset. A known issue with LLMs is their susceptibility to prompt bias, where the performance of these models fluctuates based on the input prompt provided. In our research, we delve deeper into the reasons behind these fluctuations and propose a framework to mitigate prompt bias for clone detection. Our analysis identifies eight distinct categories of prompt bias, and our devised approach leveraging these biases yields a significant improvement of up to 10.81% in the F1 score. These findings underscore the substantial impact of prompt bias on the performance of LLMs and highlight the potential for leveraging model errors to alleviate this bias.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 15.7 -->
                    
                <!-- Medicine: 7.1 -->
                    
                <!-- Quantum Computing: 2.1 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Math: 1.9 -->
                    
                <!-- Federated Learning: 1.6 -->
                    
                <!-- Networks: 1.4 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.3554
                </span>
                <a href="https://arxiv.org/abs/2505.05804" target="_blank" rel="noopener noreferrer">Describe Anything in Medical Images</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Xi Xiao, Yunbei Zhang, Thanh-Huy Nguyen, Ba-Thinh Lam, Janet Wang, Jihun Hamm, Tianyang Wang, Xingjian Li, Xiao Wang, Hao Xu, Tianming Liu, Min Xu
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Localized image captioning has made significant progress with models like the Describe Anything Model (DAM), which can generate detailed region-specific descriptions without explicit region-text supervision. However, such capabilities have yet to be widely applied to specialized domains like medical</span>
                
                <span class="abstract-full" style="display: none;">Localized image captioning has made significant progress with models like the Describe Anything Model (DAM), which can generate detailed region-specific descriptions without explicit region-text supervision. However, such capabilities have yet to be widely applied to specialized domains like medical imaging, where diagnostic interpretation relies on subtle regional findings rather than global understanding. To mitigate this gap, we propose MedDAM, the first comprehensive framework leveraging large vision-language models for region-specific captioning in medical images. MedDAM employs medical expert-designed prompts tailored to specific imaging modalities and establishes a robust evaluation benchmark comprising a customized assessment protocol, data pre-processing pipeline, and specialized QA template library. This benchmark evaluates both MedDAM and other adaptable large vision-language models, focusing on clinical factuality through attribute-level verification tasks, thereby circumventing the absence of ground-truth region-caption pairs in medical datasets. Extensive experiments on the VinDr-CXR, LIDC-IDRI, and SkinCon datasets demonstrate MedDAM's superiority over leading peers (including GPT-4o, Claude 3.7 Sonnet, LLaMA-3.2 Vision, Qwen2.5-VL, GPT-4Rol, and OMG-LLaVA) in the task, revealing the importance of region-level semantic alignment in medical image understanding and establishing MedDAM as a promising foundation for clinical vision-language integration.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 13.2 -->
                    
                <!-- LLMs: 7.8 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- RAG: 1.6 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Networks: 1.5 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Reinforcement Learning: 1.4 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Federated Learning: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -3.7012
                </span>
                <a href="https://arxiv.org/abs/2211.03040" target="_blank" rel="noopener noreferrer">Modelling of a DC-DC Buck Converter Using Long-Short-Term-Memory (LSTM)</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Muhy Eddin Za'ter
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Artificial neural networks make it possible to identify black-box models. Based on a recurrent nonlinear autoregressive exogenous neural network, this research provides a technique for simulating the static and dynamic behavior of a DC-DC power converter. This approach employs an algorithm for train</span>
                
                <span class="abstract-full" style="display: none;">Artificial neural networks make it possible to identify black-box models. Based on a recurrent nonlinear autoregressive exogenous neural network, this research provides a technique for simulating the static and dynamic behavior of a DC-DC power converter. This approach employs an algorithm for training a neural network using the inputs and outputs (currents and voltages) of a Buck converter. The technique is validated using simulated data of a realistic Simulink-programmed nonsynchronous Buck converter model and experimental findings. The correctness of the technique is determined by comparing the predicted outputs of the neural network to the actual outputs of the system, thereby confirming the suggested strategy. Simulation findings demonstrate the practicability and precision of the proposed black-box method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 9.9 -->
                    
                <!-- Reinforcement Learning: 3.7 -->
                    
                <!-- Networks: 3.6 -->
                    
                <!-- Math: 1.8 -->
                    
                <!-- LLMs: 1.8 -->
                    
                <!-- Robotics: 1.6 -->
                    
                <!-- Quantum Computing: 1.4 -->
                    
                <!-- Pathfinding: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.256
                </span>
                <a href="https://arxiv.org/abs/2505.05940" target="_blank" rel="noopener noreferrer">Fast Differentiable Modal Simulation of Non-linear Strings, Membranes, and Plates</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Rodrigo Diaz, Mark Sandler
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modal methods for simulating vibrations of strings, membranes, and plates are widely used in acoustics and physically informed audio synthesis. However, traditional implementations, particularly for non-linear models like the von K\'arm\'an plate, are computationally demanding and lack differentiabi</span>
                
                <span class="abstract-full" style="display: none;">Modal methods for simulating vibrations of strings, membranes, and plates are widely used in acoustics and physically informed audio synthesis. However, traditional implementations, particularly for non-linear models like the von K\'arm\'an plate, are computationally demanding and lack differentiability, limiting inverse modelling and real-time applications. We introduce a fast, differentiable, GPU-accelerated modal framework built with the JAX library, providing efficient simulations and enabling gradient-based inverse modelling. Benchmarks show that our approach significantly outperforms CPU and GPU-based implementations, particularly for simulations with many modes. Inverse modelling experiments demonstrate that our approach can recover physical parameters, including tension, stiffness, and geometry, from both synthetic and experimental data. Although fitting physical parameters is more sensitive to initialisation compared to other methods, it provides greater interpretability and more compact parameterisation. The code is released as open source to support future research and applications in differentiable physical modelling and sound synthesis.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 15.3 -->
                    
                <!-- LLMs: 10.6 -->
                    
                <!-- 3D: 3.0 -->
                    
                <!-- Quantum Computing: 2.9 -->
                    
                <!-- Robotics: 1.8 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Networks: 1.2 -->
                    
                <!-- GNN: 1.2 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.4775
                </span>
                <a href="https://arxiv.org/abs/2505.06133" target="_blank" rel="noopener noreferrer">BrainSegDMlF: A Dynamic Fusion-enhanced SAM for Brain Lesion Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Hongming Wang, Yifeng Wu, Huimin Huang, Hongtao Wu, Jia-Xuan Jiang, Xiaodong Zhang, Hao Zheng, Xian Wu, Yefeng Zheng, Jinping Xu, Jing Cheng
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The segmentation of substantial brain lesions is a significant and challenging task in the field of medical image segmentation. Substantial brain lesions in brain imaging exhibit high heterogeneity, with indistinct boundaries between lesion regions and normal brain tissue. Small lesions in single sl</span>
                
                <span class="abstract-full" style="display: none;">The segmentation of substantial brain lesions is a significant and challenging task in the field of medical image segmentation. Substantial brain lesions in brain imaging exhibit high heterogeneity, with indistinct boundaries between lesion regions and normal brain tissue. Small lesions in single slices are difficult to identify, making the accurate and reproducible segmentation of abnormal regions, as well as their feature description, highly complex. Existing methods have the following limitations: 1) They rely solely on single-modal information for learning, neglecting the multi-modal information commonly used in diagnosis. This hampers the ability to comprehensively acquire brain lesion information from multiple perspectives and prevents the effective integration and utilization of multi-modal data inputs, thereby limiting a holistic understanding of lesions. 2) They are constrained by the amount of data available, leading to low sensitivity to small lesions and difficulty in detecting subtle pathological changes. 3) Current SAM-based models rely on external prompts, which cannot achieve automatic segmentation and, to some extent, affect diagnostic efficiency.To address these issues, we have developed a large-scale fully automated segmentation model specifically designed for brain lesion segmentation, named BrainSegDMLF. This model has the following features: 1) Dynamic Modal Interactive Fusion (DMIF) module that processes and integrates multi-modal data during the encoding process, providing the SAM encoder with more comprehensive modal information. 2) Layer-by-Layer Upsampling Decoder, enabling the model to extract rich low-level and high-level features even with limited data, thereby detecting the presence of small lesions. 3) Automatic segmentation masks, allowing the model to generate lesion masks automatically without requiring manual prompts.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.9 -->
                    
                <!-- LLMs: 4.9 -->
                    
                <!-- Networks: 2.2 -->
                    
                <!-- Quantum Computing: 2.0 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Reinforcement Learning: 1.6 -->
                    
                <!-- Federated Learning: 1.5 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.7376
                </span>
                <a href="https://arxiv.org/abs/2505.05725" target="_blank" rel="noopener noreferrer">Quantitative Hardness Assessment with Vision-based Tactile Sensing for Fruit Classification and Grasping</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Zhongyuan Liao, Yipai Du, Jianghua Duan, Haobo Liang, Michael Yu Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Accurate estimation of fruit hardness is essential for automated classification and handling systems, particularly in determining fruit variety, assessing ripeness, and ensuring proper harvesting force. This study presents an innovative framework for quantitative hardness assessment utilizing vision</span>
                
                <span class="abstract-full" style="display: none;">Accurate estimation of fruit hardness is essential for automated classification and handling systems, particularly in determining fruit variety, assessing ripeness, and ensuring proper harvesting force. This study presents an innovative framework for quantitative hardness assessment utilizing vision-based tactile sensing, tailored explicitly for robotic applications in agriculture. The proposed methodology derives normal force estimation from a vision-based tactile sensor, and, based on the dynamics of this normal force, calculates the hardness. This approach offers a rapid, non-destructive evaluation through single-contact interaction. The integration of this framework into robotic systems enhances real-time adaptability of grasping forces, thereby reducing the likelihood of fruit damage. Moreover, the general applicability of this approach, through a universal criterion based on average normal force dynamics, ensures its effectiveness across a wide variety of fruit types and sizes. Extensive experimental validation conducted across different fruit types and ripeness-tracking studies demonstrates the efficacy and robustness of the framework, marking a significant advancement in the domain of automated fruit handling.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 14.0 -->
                    
                <!-- LLMs: 6.2 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.8 -->
                    
                <!-- Blockchain: 1.7 -->
                    
                <!-- Math: 1.5 -->
                    
                <!-- Robotics: 1.5 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -4.7785
                </span>
                <a href="https://arxiv.org/abs/2409.03935" target="_blank" rel="noopener noreferrer">Galled Perfect Transfer Networks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Alitzel L\'opez S\'anchez, Manuel Lafond
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Predicting horizontal gene transfers often requires comparative sequence data, but recent work has shown that character-based approaches could also be useful for this task. Notably, perfect transfer networks (PTN) explain the character diversity of a set of taxa for traits that are gained once, rare</span>
                
                <span class="abstract-full" style="display: none;">Predicting horizontal gene transfers often requires comparative sequence data, but recent work has shown that character-based approaches could also be useful for this task. Notably, perfect transfer networks (PTN) explain the character diversity of a set of taxa for traits that are gained once, rarely lost, but that can be transferred laterally. Characterizing the structure of such characters is an important step towards understanding more complex characters. Although efficient algorithms can infer such networks from character data, they can sometimes predict overly complicated transfer histories. With the goal of recovering the simplest possible scenarios in this model, we introduce galled perfect transfer networks, which are PTNs that are galled trees. Such networks are useful for characters that are incompatible in terms of tree-like evolution, but that do fit in an almost-tree scenario. We provide polynomial-time algorithms for two problems: deciding whether one can add transfer edges to a tree to transform it into a galled PTN, and deciding whether a set of characters are galled-compatible, that is, they can be explained by some galled PTN. We also analyze a real dataset comprising of a bacterial species trees and KEGG functions as characters, and derive several conclusions on the difficulty of explaining characters in a galled tree, which provide several directions for future research.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- LLMs: 6.0 -->
                    
                <!-- Quantum Computing: 5.9 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- Math: 2.5 -->
                    
                <!-- GNN: 2.2 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 2.0 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.2849
                </span>
                <a href="https://arxiv.org/abs/2505.06152" target="_blank" rel="noopener noreferrer">MM-Skin: Enhancing Dermatology Vision-Language Model with an Image-Text Dataset Derived from Textbooks</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Wenqi Zeng, Yuqi Sun, Chenxi Ma, Weimin Tan, Bo Yan
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Medical vision-language models (VLMs) have shown promise as clinical assistants across various medical fields. However, specialized dermatology VLM capable of delivering professional and detailed diagnostic analysis remains underdeveloped, primarily due to less specialized text descriptions in curre</span>
                
                <span class="abstract-full" style="display: none;">Medical vision-language models (VLMs) have shown promise as clinical assistants across various medical fields. However, specialized dermatology VLM capable of delivering professional and detailed diagnostic analysis remains underdeveloped, primarily due to less specialized text descriptions in current dermatology multimodal datasets. To address this issue, we propose MM-Skin, the first large-scale multimodal dermatology dataset that encompasses 3 imaging modalities, including clinical, dermoscopic, and pathological and nearly 10k high-quality image-text pairs collected from professional textbooks. In addition, we generate over 27k diverse, instruction-following vision question answering (VQA) samples (9 times the size of current largest dermatology VQA dataset). Leveraging public datasets and MM-Skin, we developed SkinVL, a dermatology-specific VLM designed for precise and nuanced skin disease interpretation. Comprehensive benchmark evaluations of SkinVL on VQA, supervised fine-tuning (SFT) and zero-shot classification tasks across 8 datasets, reveal its exceptional performance for skin diseases in comparison to both general and medical VLM models. The introduction of MM-Skin and SkinVL offers a meaningful contribution to advancing the development of clinical dermatology VLM assistants. MM-Skin is available at https://github.com/ZwQ803/MM-Skin</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 20.8 -->
                    
                <!-- LLMs: 10.0 -->
                    
                <!-- Quantum Computing: 2.4 -->
                    
                <!-- 3D: 2.1 -->
                    
                <!-- RAG: 1.7 -->
                    
                <!-- T2I: 1.5 -->
                    
                <!-- Blockchain: 1.5 -->
                    
                <!-- GNN: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.2 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Networks: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -5.4804
                </span>
                <a href="https://arxiv.org/abs/2505.00410" target="_blank" rel="noopener noreferrer">Machine Learning Meets Transparency in Osteoporosis Risk Assessment: A Comparative Study of ML and Explainability Analysis</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Farhana Elias, Md Shihab Reza, Muhammad Zawad Mahmud, Samiha Islam, Shahran Rahman Alve
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The present research tackles the difficulty of predicting osteoporosis risk via machine learning (ML) approaches, emphasizing the use of explainable artificial intelligence (XAI) to improve model transparency. Osteoporosis is a significant public health concern, sometimes remaining untreated owing t</span>
                
                <span class="abstract-full" style="display: none;">The present research tackles the difficulty of predicting osteoporosis risk via machine learning (ML) approaches, emphasizing the use of explainable artificial intelligence (XAI) to improve model transparency. Osteoporosis is a significant public health concern, sometimes remaining untreated owing to its asymptomatic characteristics, and early identification is essential to avert fractures. The research assesses six machine learning classifiers: Random Forest, Logistic Regression, XGBoost, AdaBoost, LightGBM, and Gradient Boosting and utilizes a dataset based on clinical, demographic, and lifestyle variables. The models are refined using GridSearchCV to calibrate hyperparameters, with the objective of enhancing predictive efficacy. XGBoost had the greatest accuracy (91%) among the evaluated models, surpassing others in precision (0.92), recall (0.91), and F1-score (0.90). The research further integrates XAI approaches, such as SHAP, LIME, and Permutation Feature Importance, to elucidate the decision-making process of the optimal model. The study indicates that age is the primary determinant in forecasting osteoporosis risk, followed by hormonal alterations and familial history. These results corroborate clinical knowledge and affirm the models' therapeutic significance. The research underscores the significance of explainability in machine learning models for healthcare applications, guaranteeing that physicians can rely on the system's predictions. The report ultimately proposes directions for further research, such as validation across varied populations and the integration of supplementary biomarkers for enhanced predictive accuracy.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.4 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Quantum Computing: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.9 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Federated Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -6.5735
                </span>
                <a href="https://arxiv.org/abs/2303.17051" target="_blank" rel="noopener noreferrer">Towards Foundation Models and Few-Shot Parameter-Efficient Fine-Tuning for Volumetric Organ Segmentation</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Julio Silva-Rodr\'iguez, Jose Dolz, Ismail Ben Ayed
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">The recent popularity of foundation models and the pre-train-and-adapt paradigm, where a large-scale model is transferred to downstream tasks, is gaining attention for volumetric medical image segmentation. However, current transfer learning strategies devoted to full fine-tuning for transfer learni</span>
                
                <span class="abstract-full" style="display: none;">The recent popularity of foundation models and the pre-train-and-adapt paradigm, where a large-scale model is transferred to downstream tasks, is gaining attention for volumetric medical image segmentation. However, current transfer learning strategies devoted to full fine-tuning for transfer learning may require significant resources and yield sub-optimal results when the labeled data of the target task is scarce. This makes its applicability in real clinical settings challenging since these institutions are usually constrained on data and computational resources to develop proprietary solutions. To address this challenge, we formalize Few-Shot Efficient Fine-Tuning (FSEFT), a novel and realistic scenario for adapting medical image segmentation foundation models. This setting considers the key role of both data- and parameter-efficiency during adaptation. Building on a foundation model pre-trained on open-access CT organ segmentation sources, we propose leveraging Parameter-Efficient Fine-Tuning and black-box Adapters to address such challenges. Furthermore, novel efficient adaptation methodologies are introduced in this work, which include Spatial black-box Adapters that are more appropriate for dense prediction tasks and constrained transductive inference, leveraging task-specific prior knowledge. Our comprehensive transfer learning experiments confirm the suitability of foundation models in medical image segmentation and unveil the limitations of popular fine-tuning strategies in few-shot scenarios.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 26.1 -->
                    
                <!-- LLMs: 5.5 -->
                    
                <!-- Quantum Computing: 1.8 -->
                    
                <!-- Reinforcement Learning: 1.5 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Robotics: 1.3 -->
                    
                <!-- T2I: 1.2 -->
                    
                <!-- 3D: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- RAG: 1.1 -->
                    
                <!-- Federated Learning: 1.1 -->
                    
                <!-- GNN: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -7.7668
                </span>
                <a href="https://arxiv.org/abs/2505.05845" target="_blank" rel="noopener noreferrer">Automated Knot Detection and Pairing for Wood Analysis in the Timber Industry</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Guohao Lin, Shidong Pan, Rasul Khanbayov, Changxi Yang, Ani Khaloian-Sarnaghi, Andriy Kovryga
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Knots in wood are critical to both aesthetics and structural integrity, making their detection and pairing essential in timber processing. However, traditional manual annotation was labor-intensive and inefficient, necessitating automation. This paper proposes a lightweight and fully automated pipel</span>
                
                <span class="abstract-full" style="display: none;">Knots in wood are critical to both aesthetics and structural integrity, making their detection and pairing essential in timber processing. However, traditional manual annotation was labor-intensive and inefficient, necessitating automation. This paper proposes a lightweight and fully automated pipeline for knot detection and pairing based on machine learning techniques. In the detection stage, high-resolution surface images of wooden boards were collected using industrial-grade cameras, and a large-scale dataset was manually annotated and preprocessed. After the transfer learning, the YOLOv8l achieves an mAP@0.5 of 0.887. In the pairing stage, detected knots were analyzed and paired based on multidimensional feature extraction. A triplet neural network was used to map the features into a latent space, enabling clustering algorithms to identify and pair corresponding knots. The triplet network with learnable weights achieved a pairing accuracy of 0.85. Further analysis revealed that he distances from the knot's start and end points to the bottom of the wooden board, and the longitudinal coordinates play crucial roles in achieving high pairing accuracy. Our experiments validate the effectiveness of the proposed solution, demonstrating the potential of AI in advancing wood science and industry.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 18.1 -->
                    
                <!-- LLMs: 4.1 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- Robotics: 1.9 -->
                    
                <!-- Quantum Computing: 1.5 -->
                    
                <!-- Networks: 1.4 -->
                    
                <!-- Math: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -8.8596
                </span>
                <a href="https://arxiv.org/abs/2505.05549" target="_blank" rel="noopener noreferrer">Machine learning automorphic forms for black holes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Vishnu Jejjala, Suresh Nampuri, Dumisani Nxumalo, Pratik Roy, Abinash Swain
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Modular, Jacobi, and mock-modular forms serve as generating functions for BPS black hole degeneracies. By training feed-forward neural networks on Fourier coefficients of automorphic forms derived from the Dedekind eta function, Eisenstein series, and Jacobi theta functions, we demonstrate that mach</span>
                
                <span class="abstract-full" style="display: none;">Modular, Jacobi, and mock-modular forms serve as generating functions for BPS black hole degeneracies. By training feed-forward neural networks on Fourier coefficients of automorphic forms derived from the Dedekind eta function, Eisenstein series, and Jacobi theta functions, we demonstrate that machine learning techniques can accurately predict modular weights from truncated expansions. Our results reveal strong performance for negative weight modular and quasi-modular forms, particularly those arising in exact black hole counting formulae, with lower accuracy for positive weights and more complicated combinations of Jacobi theta functions. This study establishes a proof of concept for using machine learning to identify how data is organized in terms of modular symmetries in gravitational systems and suggests a pathway toward automated detection and verification of symmetries in quantum gravity.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 16.0 -->
                    
                <!-- Quantum Computing: 6.1 -->
                    
                <!-- LLMs: 5.4 -->
                    
                <!-- 3D: 1.9 -->
                    
                <!-- GNN: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- T2I: 1.3 -->
                    
                <!-- RAG: 1.3 -->
                    
                <!-- Blockchain: 1.3 -->
                    
                <!-- Reinforcement Learning: 1.3 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.0216
                </span>
                <a href="https://arxiv.org/abs/2502.20364" target="_blank" rel="noopener noreferrer">Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ryan C. Barron, Maksim E. Eren, Olga M. Serafimova, Cynthia Matuszek, Boian S. Alexandrov
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and</span>
                
                <span class="abstract-full" style="display: none;">Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 23.7 -->
                    
                <!-- LLMs: 12.1 -->
                    
                <!-- 3D: 3.0 -->
                    
                <!-- RAG: 2.3 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- T2I: 1.7 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -9.7884
                </span>
                <a href="https://arxiv.org/abs/2407.20520" target="_blank" rel="noopener noreferrer">Optimization perspective on raking</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Ariane Ducellier (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Alexander Hsu (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA), Parkes Kendrick (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Bill Gustafson (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Laura Dwyer-Lindgren (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Christopher Murray (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Peng Zheng (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA), Aleksandr Aravkin (Institute for Health Metrics and Evaluation, University of Washington, Seattle, WA, Department of Applied Mathematics, University of Washington, Seattle, WA)
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Raking is widely used in survey inference and global health models to adjust the observations in contingency tables to given marginals, in the latter case reconciling estimates between models with different granularities. We review the convex optimization foundation of raking and focus on a dual per</span>
                
                <span class="abstract-full" style="display: none;">Raking is widely used in survey inference and global health models to adjust the observations in contingency tables to given marginals, in the latter case reconciling estimates between models with different granularities. We review the convex optimization foundation of raking and focus on a dual perspective that simplifies and streamlines prior raking extensions and provides new functionality, enabling a unified approach to n-dimensional raking, raking with differential weights, ensuring bounds on estimates are respected, raking to margins either as hard constraints or as aggregate observations, handling missing data, and allowing efficient uncertainty propagation. The dual perspective also enables a uniform fast and scalable matrix-free optimization approach for all of these extensions. All of the methods are implemented in an open source Python package with an intuitive user interface, installable from PyPi (https://pypi.org/project/raking/), and we illustrate the capabilities using synthetic data and real mortality estimates.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Medicine: 22.5 -->
                    
                <!-- LLMs: 9.3 -->
                    
                <!-- Quantum Computing: 2.2 -->
                    
                <!-- Robotics: 1.7 -->
                    
                <!-- Blockchain: 1.2 -->
                    
                <!-- 3D: 1.0 -->
                    
                <!-- Reinforcement Learning: 1.0 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -10.5481
                </span>
                <a href="https://arxiv.org/abs/2504.14002" target="_blank" rel="noopener noreferrer">Predicting fermionic densities using a Projected Quantum Kernel method</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Francesco Perciavalle, Francesco Plastina, Michele Pisarra, Nicola Lo Gullo
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We use a support vector regressor based on a projected quantum kernel method to predict the density structure of 1D fermionic systems of interest in quantum chemistry and quantum matter. The kernel is built on with the observables of a quantum reservoir implementable with interacting Rydberg atoms. </span>
                
                <span class="abstract-full" style="display: none;">We use a support vector regressor based on a projected quantum kernel method to predict the density structure of 1D fermionic systems of interest in quantum chemistry and quantum matter. The kernel is built on with the observables of a quantum reservoir implementable with interacting Rydberg atoms. Training and test data of the fermionic system are generated using a Density Functional Theory approach. We test the performance of the method for several Hamiltonian parameters, finding a general common behavior of the error as a function of measurement time. At sufficiently large measurement times, we find that the method outperforms the classical linear kernel method and can be competitive with the radial basis function method.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 9.2 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- Math: 3.7 -->
                    
                <!-- Reinforcement Learning: 3.4 -->
                    
                <!-- Networks: 2.8 -->
                    
                <!-- LLMs: 2.6 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -12.8993
                </span>
                <a href="https://arxiv.org/abs/2401.02940" target="_blank" rel="noopener noreferrer">Digital-analog quantum learning on Rydberg atom arrays</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Jonathan Z. Lu, Lucy Jiao, Kristina Wolinski, Milan Kornja\v{c}a, Hong-Ye Hu, Sergio Cantu, Fangli Liu, Susanne F. Yelin, Sheng-Tao Wang
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We propose hybrid digital-analog learning algorithms on Rydberg atom arrays, combining the potentially practical utility and near-term realizability of quantum learning with the rapidly scaling architectures of neutral atoms. Our construction requires only single-qubit operations in the digital sett</span>
                
                <span class="abstract-full" style="display: none;">We propose hybrid digital-analog learning algorithms on Rydberg atom arrays, combining the potentially practical utility and near-term realizability of quantum learning with the rapidly scaling architectures of neutral atoms. Our construction requires only single-qubit operations in the digital setting and global driving according to the Rydberg Hamiltonian in the analog setting. We perform a comprehensive numerical study of our algorithm on both classical and quantum data, given respectively by handwritten digit classification and unsupervised quantum phase boundary learning. We show in the two representative problems that digital-analog learning is not only feasible in the near term, but also requires shorter circuit depths and is more robust to realistic error models as compared to digital learning schemes. Our results suggest that digital-analog learning opens a promising path towards improved variational quantum learning experiments in the near term.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 14.5 -->
                    
                <!-- Medicine: 3.5 -->
                    
                <!-- LLMs: 3.2 -->
                    
                <!-- Reinforcement Learning: 3.0 -->
                    
                <!-- Networks: 2.1 -->
                    
                <!-- GNN: 1.5 -->
                    
                <!-- Federated Learning: 1.4 -->
                    
                <!-- Robotics: 1.4 -->
                    
                <!-- Math: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -14.8217
                </span>
                <a href="https://arxiv.org/abs/2505.06069" target="_blank" rel="noopener noreferrer">Operator Spaces, Linear Logic and the Heisenberg-Schr\"odinger Duality of Quantum Theory</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Bert Lindenhovius, Vladimir Zamdzhiev
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">We show that the category OS of operator spaces, with complete contractions as morphisms, is locally countably presentable and a model of Intuitionistic Linear Logic in the sense of Lafont. We then describe a model of Classical Linear Logic, based on OS, whose duality is compatible with the Heisenbe</span>
                
                <span class="abstract-full" style="display: none;">We show that the category OS of operator spaces, with complete contractions as morphisms, is locally countably presentable and a model of Intuitionistic Linear Logic in the sense of Lafont. We then describe a model of Classical Linear Logic, based on OS, whose duality is compatible with the Heisenberg-Schr\"odinger duality of quantum theory. We also show that OS provides a good setting for studying pure state and mixed state quantum information, the interaction between the two, and even higher-order quantum maps such as the quantum switch.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 14.8 -->
                    
                <!-- Medicine: 4.6 -->
                    
                <!-- LLMs: 3.4 -->
                    
                <!-- Math: 2.7 -->
                    
                <!-- Reinforcement Learning: 2.0 -->
                    
                <!-- Networks: 1.8 -->
                    
                <!-- Blockchain: 1.4 -->
                    
                <!-- Robotics: 1.2 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -16.6559
                </span>
                <a href="https://arxiv.org/abs/2505.06157" target="_blank" rel="noopener noreferrer">Advancing Finite-Length Quantum Error Correction Using Generalized Bicycle Codes</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Olai \AA. Mostad, Hsuan-Yin Lin, Eirik Rosnes, De-Shih Lee, Ching-Yi Lai
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">Generalized bicycle (GB) codes have emerged as a promising class of quantum error-correcting codes with practical decoding capabilities. While numerous asymptotically good quantum codes and quantum low-density parity-check code constructions have been proposed, their finite block-length performance </span>
                
                <span class="abstract-full" style="display: none;">Generalized bicycle (GB) codes have emerged as a promising class of quantum error-correcting codes with practical decoding capabilities. While numerous asymptotically good quantum codes and quantum low-density parity-check code constructions have been proposed, their finite block-length performance often remains unquantified. In this work, we demonstrate that GB codes exhibit comparable or superior error correction performance in finite-length settings, particularly when designed with higher or unrestricted row weights. Leveraging their flexible construction, GB codes can be tailored to achieve high rates while maintaining efficient decoding. We evaluate GB codes against other leading quantum code families, such as quantum Tanner codes and single-parity-check product codes, highlighting their versatility in practical finite-length applications.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 24.9 -->
                    
                <!-- LLMs: 8.7 -->
                    
                <!-- Medicine: 3.2 -->
                    
                <!-- GNN: 2.6 -->
                    
                <!-- 3D: 2.2 -->
                    
                <!-- RAG: 2.1 -->
                    
                <!-- Blockchain: 2.0 -->
                    
                <!-- Networks: 1.6 -->
                    
                <!-- T2I: 1.6 -->
                    
                <!-- Robotics: 1.2 -->
                    
                <!-- Math: 1.1 -->
                    
                <!-- Reinforcement Learning: 1.1 -->
                    
                
            </div>
        </div>
        
        <div class="paper">
            <div class="paper-title">
                <span class="interestingness-score interestingness-negative">
                    -19.0661
                </span>
                <a href="https://arxiv.org/abs/2505.05957" target="_blank" rel="noopener noreferrer">Efficient Quantum Convolutional Neural Networks for Image Classification: Overcoming Hardware Constraints</a>
                <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
            </div>
            <div class="paper-meta">
                Authors: Peter R\"oseler, Oliver Schaudt, Helmut Berg, Christian Bauckhage, Matthias Koch
            </div>
            <div class="paper-abstract">
                <span class="abstract-short">While classical convolutional neural networks (CNNs) have revolutionized image classification, the emergence of quantum computing presents new opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs) leverage quantum mechanical properties and hold potential to outperform classi</span>
                
                <span class="abstract-full" style="display: none;">While classical convolutional neural networks (CNNs) have revolutionized image classification, the emergence of quantum computing presents new opportunities for enhancing neural network architectures. Quantum CNNs (QCNNs) leverage quantum mechanical properties and hold potential to outperform classical approaches. However, their implementation on current noisy intermediate-scale quantum (NISQ) devices remains challenging due to hardware limitations. In our research, we address this challenge by introducing an encoding scheme that significantly reduces the input dimensionality. We demonstrate that a primitive QCNN architecture with 49 qubits is sufficient to directly process $28\times 28$ pixel MNIST images, eliminating the need for classical dimensionality reduction pre-processing. Additionally, we propose an automated framework based on expressibility, entanglement, and complexity characteristics to identify the building blocks of QCNNs, parameterized quantum circuits (PQCs). Our approach demonstrates advantages in accuracy and convergence speed with a similar parameter count compared to both hybrid QCNNs and classical CNNs. We validated our experiments on IBM's Heron r2 quantum processor, achieving $96.08\%$ classification accuracy, surpassing the $71.74\%$ benchmark of traditional approaches under identical training conditions. These results represent one of the first implementations of image classifications on real quantum hardware and validate the potential of quantum computing in this area.</span>
                <span class="more-link" onclick="toggleAbstract(this)">... more</span>
                
            </div>
            <div class="paper-tags"><!-- Quantum Computing: 24.0 -->
                    
                <!-- Medicine: 5.9 -->
                    
                <!-- LLMs: 3.6 -->
                    
                <!-- Reinforcement Learning: 2.1 -->
                    
                <!-- GNN: 1.7 -->
                    
                <!-- Networks: 1.3 -->
                    
                <!-- Federated Learning: 1.2 -->
                    
                <!-- Math: 1.2 -->
                    
                <!-- Blockchain: 1.1 -->
                    
                <!-- Robotics: 1.1 -->
                    
                <!-- 3D: 1.0 -->
                    
                
            </div>
        </div>
        
    </div>
    
    
    <div id="jsonPopup" class="json-popup">
        <pre id="jsonContent"></pre>
        <button onclick="copyJson()">Copy to Clipboard</button>
        <button onclick="closePopup()">Close</button>
    </div>

    <script>
        function extractPaperData(paperElement) {
            const titleElement = paperElement.querySelector('.paper-title a');
            const metaElement = paperElement.querySelector('.paper-meta');
            const abstractElement = paperElement.querySelector('.paper-abstract');
            const tagsElement = paperElement.querySelector('.paper-tags');
            
            // Get the date from the parent date-section header
            const dateSection = paperElement.closest('.date-section');
            const dateText = dateSection.querySelector('.date-header').textContent.trim();
            
            const authorsText = metaElement.textContent.replace('Authors:', '').trim();
            
            const paperData = {
                title: titleElement.textContent,
                url: titleElement.href,
                authors: authorsText.split(',').map(author => author.trim()),
                created: dateText,
                abstract: abstractElement.querySelector('.abstract-full').textContent
            };
            
            return paperData;
        }

        function showJson(paperElement) {
            const popup = document.getElementById('jsonPopup');
            const content = document.getElementById('jsonContent');
            const paperData = extractPaperData(paperElement);
            content.textContent = JSON.stringify(paperData, null, null);
            popup.style.display = 'block';
            document.addEventListener('click', function closePopupOnClick(event) {
                if (!popup.contains(event.target)) {
                    popup.style.display = 'none';
                    document.removeEventListener('click', closePopupOnClick);
                }
            });
        }
        function toggleAbstract(element) {
            const abstract = element.parentElement;
            const short = abstract.querySelector('.abstract-short');
            const full = abstract.querySelector('.abstract-full');
            const lowConfidenceTags = abstract.parentElement.querySelectorAll('.tag-badge.low-confidence');
            
            if (element.textContent === '... more') {
                short.style.display = 'none';
                full.style.display = 'inline';
                element.textContent = ' less';
                lowConfidenceTags.forEach(tag => tag.style.display = 'inline-block');
            } else {
                short.style.display = 'inline';
                full.style.display = 'none';
                element.textContent = '... more';
                lowConfidenceTags.forEach(tag => tag.style.display = 'none');
            }
        }

        function closePopup() {
            document.getElementById('jsonPopup').style.display = 'none';
        }

        function copyJson() {
            const content = document.getElementById('jsonContent').textContent;
            navigator.clipboard.writeText(content).catch(() => {
                // If clipboard API is not available, just show the popup
                alert('Could not copy to clipboard. JSON is displayed in the popup.');
            });
        }

        // Close popup when clicking outside
        window.onclick = function(event) {
            const popup = document.getElementById('jsonPopup');
            if (event.target === popup) {
                popup.style.display = 'none';
            }
        }
    </script>
</body>
</html> 