<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ArXiv Frontpage</title>
    <style>
        body {
            font-family: sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
        }
        .paper {
            margin-bottom: 30px;
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
        }
        .paper-title {
            font-size: 1.2em;
            font-weight: bold;
            margin-bottom: 10px;
        }
        .paper-meta {
            color: #666;
            font-size: 0.9em;
            margin-bottom: 10px;
        }
        .paper-abstract {
            margin-bottom: 10px;
        }
        .abstract-short {
            display: inline;
        }
        .abstract-full {
            display: none;
        }
        .more-link {
            color: blue;
            cursor: pointer;
            text-decoration: underline;
        }
        .tag-badge {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 5px;
            margin-bottom: 5px;
            border-radius: 3px;
            font-size: 0.8em;
            color: white;
        }
        .tag-badge.high-confidence {
            opacity: 1;
        }
        .tag-badge.low-confidence {
            opacity: 0.6;
            display: none;
        }
        .interestingness-score {
            display: inline-block;
            padding: 3px 8px;
            margin-right: 10px;
            color: white;
            border-radius: 3px;
            font-weight: bold;
        }
        .interestingness-positive {
            background-color: #4CAF50;
        }
        .interestingness-negative {
            background-color: #f44336;
        }
        .last-updated {
            text-align: right;
            color: #666;
            font-size: 0.9em;
            margin-top: 20px;
            margin-bottom: 20px;
        }
        .intro {
            text-align: center;
            max-width: 60em;
            margin: 0 auto;
            color: #888;
        }
        .copy-icon {
            display: inline-block;
            width: 16px;
            height: 16px;
            cursor: pointer;
            margin-left: 5px;
            opacity: 0.5;
        }
        .copy-icon:hover {
            opacity: 1;
        }
        .json-popup {
            display: none;
            position: fixed;
            top: 20px;
            right: 20px;
            background: white;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
            max-width: 500px;
            max-height: 300px;
            overflow: auto;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
        }
        a {
            color: inherit;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        h1 {
            text-align: center;
        }
        h1 a {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>
        <a href="https://github.com/DataWraith/arxiv-frontpage">DataWraith's</a> ArXiv Frontpage
    </h1>

    <div class="last-updated">
        Last updated: 2025-04-14
    </div>

    <p class="intro">
        This frontpage is made by scraping ArXiv's computer science RSS feed and tagging papers with a classifier.
    </p>

    <p class="intro">
        Each tag is weighted according to my preferences to compute a paper's <i>interestingness</i> score.
    </p>
    
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                45.5958
            </span>
            <a href="https://arxiv.org/abs/2504.08057" target="_blank" rel="noopener noreferrer">Vector Quantized-Elites: Unsupervised and Problem-Agnostic Quality-Diversity Optimization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Constantinos Tsakonas, Konstantinos Chatzilygeroudis | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavioral descriptors and complete prior knowle</span>
            
            <span class="abstract-full" style="display: none;">Quality-Diversity algorithms have transformed optimization by prioritizing the discovery of diverse, high-performing solutions over a single optimal result. However, traditional Quality-Diversity methods, such as MAP-Elites, rely heavily on predefined behavioral descriptors and complete prior knowledge of the task to define the behavioral space grid, limiting their flexibility and applicability. In this work, we introduce Vector Quantized-Elites (VQ-Elites), a novel Quality-Diversity algorithm that autonomously constructs a structured behavioral space grid using unsupervised learning, eliminating the need for prior task-specific knowledge. At the core of VQ-Elites is the integration of Vector Quantized Variational Autoencoders, which enables the dynamic learning of behavioral descriptors and the generation of a structured, rather than unstructured, behavioral space grid - a significant advancement over existing unsupervised Quality-Diversity approaches. This design establishes VQ-Elites as a flexible, robust, and task-agnostic optimization framework. To further enhance the performance of unsupervised Quality-Diversity algorithms, we introduce two key components: behavioral space bounding and cooperation mechanisms, which significantly improve convergence and performance. We validate VQ-Elites on robotic arm pose-reaching and mobile robot space-covering tasks. The results demonstrate its ability to efficiently generate diverse, high-quality solutions, emphasizing its adaptability, scalability, robustness to hyperparameters, and potential to extend Quality-Diversity optimization to complex, previously inaccessible domains.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #82c40e" title="Confidence: 66.4%">
                        Quality Diversity
                    </span>
            <!-- Medicine: 7.8 -->
                
            <!-- LLMs: 6.0 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                30.9692
            </span>
            <a href="https://arxiv.org/abs/2504.08359" target="_blank" rel="noopener noreferrer">Kernel-Level Energy-Efficient Neural Architecture Search for Tabular Dataset</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hoang-Loc La, Phuong Hoai Ha | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neur</span>
            
            <span class="abstract-full" style="display: none;">Many studies estimate energy consumption using proxy metrics like memory usage, FLOPs, and inference latency, with the assumption that reducing these metrics will also lower energy consumption in neural networks. This paper, however, takes a different approach by introducing an energy-efficient Neural Architecture Search (NAS) method that directly focuses on identifying architectures that minimize energy consumption while maintaining acceptable accuracy. Unlike previous methods that primarily target vision and language tasks, the approach proposed here specifically addresses tabular datasets. Remarkably, the optimal architecture suggested by this method can reduce energy consumption by up to 92% compared to architectures recommended by conventional NAS.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #19d3a1" title="Confidence: 59.2%">
                        HPO and AutoML
                    </span>
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- LLMs: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                1.1665
            </span>
            <a href="https://arxiv.org/abs/2412.14865" target="_blank" rel="noopener noreferrer">Hierarchical Subspaces of Policies for Continual Offline Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anthony Kobanda, R\'emy Portelas, Odalric-Ambrym Maillard, Ludovic Denoyer | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Su</span>
            
            <span class="abstract-full" style="display: none;">We consider a Continual Reinforcement Learning setup, where a learning agent must continuously adapt to new tasks while retaining previously acquired skill sets, with a focus on the challenge of avoiding forgetting past gathered knowledge and ensuring scalability with the growing number of tasks. Such issues prevail in autonomous robotics and video game simulations, notably for navigation tasks prone to topological or kinematic changes. To address these issues, we introduce HiSPO, a novel hierarchical framework designed specifically for continual learning in navigation settings from offline data. Our method leverages distinct policy subspaces of neural networks to enable flexible and efficient adaptation to new tasks while preserving existing knowledge. We demonstrate, through a careful experimental study, the effectiveness of our method in both classical MuJoCo maze environments and complex video game-like navigation simulations, showcasing competitive performances and satisfying adaptability with respect to classical continual learning metrics, in particular regarding the memory usage and efficiency.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #44f899" title="Confidence: 63.5%">
                        Reinforcement Learning
                    </span>
            <!-- Medicine: 8.3 -->
                
            <!-- LLMs: 7.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-positive">
                0.0674
            </span>
            <a href="https://arxiv.org/abs/2504.08667" target="_blank" rel="noopener noreferrer">Faster shortest-path algorithms using the acyclic-connected tree</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Elis Stefansson, Oliver Biggar, Karl H. Johansson | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper gives a fixed-parameter linear algorithm for the single-source shortest path problem (SSSP) on directed graphs. The parameter in question is the nesting width, a measure of the extent to which a graph can be represented as a nested collection of graphs. We present a novel directed graph d</span>
            
            <span class="abstract-full" style="display: none;">This paper gives a fixed-parameter linear algorithm for the single-source shortest path problem (SSSP) on directed graphs. The parameter in question is the nesting width, a measure of the extent to which a graph can be represented as a nested collection of graphs. We present a novel directed graph decomposition called the acyclic-connected tree (A-C tree), which breaks the graph into a recursively nested sequence of strongly connected components in topological order. We prove that the A-C tree is optimal in the sense that its width, the size of the largest nested graph, is equal to the nesting width of the graph. We then provide a linear-time algorithm for constructing the A-C tree of any graph. Finally, we show how the A-C tree allows us to construct a simple variant of Dijkstra's algorithm which achieves a time complexity of $O(e+n\log w)$, where $n$ ($e$) is the number of nodes (arcs) in the graph and $w$ is the nesting width. The idea is to apply the shortest path algorithm separately to each component in the order dictated by the A-C tree. We obtain an asymptotic improvement over Dijkstra's algorithm: when $w=n$, our algorithm reduces to Dijkstra's algorithm, but it is faster when $w \in o(n)$, and linear-time for classes of graphs with bounded width, such as directed acyclic graphs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5caaa5" title="Confidence: 69.6%">
                        Pathfinding
                    </span>
            <!-- Medicine: 5.8 -->
                
            <!-- LLMs: 3.9 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- Math: 2.9 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -2.1402
            </span>
            <a href="https://arxiv.org/abs/2504.08003" target="_blank" rel="noopener noreferrer">Have we unified image generation and understanding yet? An empirical study of GPT-4o's image generation ability</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ning Li, Jingran Zhang, Justin Cui | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we s</span>
            
            <span class="abstract-full" style="display: none;">OpenAI's multimodal GPT-4o has demonstrated remarkable capabilities in image generation and editing, yet its ability to achieve world knowledge-informed semantic synthesis--seamlessly integrating domain knowledge, contextual reasoning, and instruction adherence--remains unproven. In this study, we systematically evaluate these capabilities across three critical dimensions: (1) Global Instruction Adherence, (2) Fine-Grained Editing Precision, and (3) Post-Generation Reasoning. While existing benchmarks highlight GPT-4o's strong capabilities in image generation and editing, our evaluation reveals GPT-4o's persistent limitations: the model frequently defaults to literal interpretations of instructions, inconsistently applies knowledge constraints, and struggles with conditional reasoning tasks. These findings challenge prevailing assumptions about GPT-4o's unified understanding and generation capabilities, exposing significant gaps in its dynamic knowledge integration. Our study calls for the development of more robust benchmarks and training strategies that go beyond surface-level alignment, emphasizing context-aware and reasoning-grounded multimodal generation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #5f6936" title="Confidence: 82.0%">
                        LLMs
                    </span>
            <span class="tag-badge high-confidence" style="background-color: #827745" title="Confidence: 76.2%">
                        T2I
                    </span>
            <!-- GNN: 3.8 -->
                
            <!-- 3D: 3.2 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- RAG: 2.0 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Attention: 1.3 -->
                
            <!-- Bayesian Optimization: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Medicine: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.2406
            </span>
            <a href="https://arxiv.org/abs/2502.18791" target="_blank" rel="noopener noreferrer">Can LLMs Help Uncover Insights about LLMs? A Large-Scale, Evolving Literature Analysis of Frontier LLMs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jungsoo Park, Junmo Kang, Gabriel Stanovsky, Alan Ritter | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The surge of LLM studies makes synthesizing their findings challenging. Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use. Our study presents a semi-automated approach for literature an</span>
            
            <span class="abstract-full" style="display: none;">The surge of LLM studies makes synthesizing their findings challenging. Analysis of experimental results from literature can uncover important trends across studies, but the time-consuming nature of manual data extraction limits its use. Our study presents a semi-automated approach for literature analysis that accelerates data extraction using LLMs. It automatically identifies relevant arXiv papers, extracts experimental results and related attributes, and organizes them into a structured dataset, LLMEvalDB. We then conduct an automated literature analysis of frontier LLMs, reducing the effort of paper surveying and data extraction by more than 93% compared to manual approaches. We validate LLMEvalDB by showing that it reproduces key findings from a recent manual analysis of Chain-of-Thought (CoT) reasoning and also uncovers new insights that go beyond it, showing, for example, that in-context examples benefit coding and multimodal tasks but offer limited gains in math reasoning tasks compared to zero-shot CoT. Our automatically updatable dataset enables continuous tracking of target models by extracting evaluation studies as new data becomes available. Through LLMEvalDB and empirical analysis, we provide insights into LLMs while facilitating ongoing literature analyses of their behavior.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 44.8 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Medicine: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.4809
            </span>
            <a href="https://arxiv.org/abs/2503.17365" target="_blank" rel="noopener noreferrer">How Effective Is Constitutional AI in Small LLMs? A Study on DeepSeek-R1 and Its Peers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Antonio-Gabriel Chac\'on Menke (Shibaura Institute of Technology, Kempten University of Applied Sciences), Phan Xuan Tan (Shibaura Institute of Technology) | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent incidents highlight safety risks in Large Language Models (LLMs), motivating research into alignment methods like Constitutional AI (CAI). This paper explores CAI's self-critique mechanism on small, uncensored 7-9B parameter models: DeepSeek-R1-8B, Gemma-2-9B, Llama 3.1-8B, and Qwen2.5-7B. We</span>
            
            <span class="abstract-full" style="display: none;">Recent incidents highlight safety risks in Large Language Models (LLMs), motivating research into alignment methods like Constitutional AI (CAI). This paper explores CAI's self-critique mechanism on small, uncensored 7-9B parameter models: DeepSeek-R1-8B, Gemma-2-9B, Llama 3.1-8B, and Qwen2.5-7B. We show that while Llama-based models exhibited significant harm reduction through self-critique, other architectures demonstrated less improvement in harm detection after abliteration. These results suggest CAI's effectiveness may vary depending on model architecture and reasoning capabilities.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #5f6936" title="Confidence: 81.8%">
                        LLMs
                    </span>
            <!-- GNN: 3.8 -->
                
            <!-- 3D: 3.4 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- T2I: 2.3 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- Bayesian Optimization: 1.3 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Attention: 1.3 -->
                
            <!-- Medicine: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -3.4848
            </span>
            <a href="https://arxiv.org/abs/2504.08231" target="_blank" rel="noopener noreferrer">Out of Style: RAG's Fragility to Linguistic Variation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tianyu Cao, Neel Bhandari, Akhila Yerukola, Akari Asai, Maarten Sap | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Despite the impressive performance of Retrieval-augmented Generation (RAG) systems across various NLP benchmarks, their robustness in handling real-world user-LLM interaction queries remains largely underexplored. This presents a critical gap for practical deployment, where user queries exhibit grea</span>
            
            <span class="abstract-full" style="display: none;">Despite the impressive performance of Retrieval-augmented Generation (RAG) systems across various NLP benchmarks, their robustness in handling real-world user-LLM interaction queries remains largely underexplored. This presents a critical gap for practical deployment, where user queries exhibit greater linguistic variations and can trigger cascading errors across interdependent RAG components. In this work, we systematically analyze how varying four linguistic dimensions (formality, readability, politeness, and grammatical correctness) impact RAG performance. We evaluate two retrieval models and nine LLMs, ranging from 3 to 72 billion parameters, across four information-seeking Question Answering (QA) datasets. Our results reveal that linguistic reformulations significantly impact both retrieval and generation stages, leading to a relative performance drop of up to 40.41% in Recall@5 scores for less formal queries and 38.86% in answer match scores for queries containing grammatical errors. Notably, RAG systems exhibit greater sensitivity to such variations compared to LLM-only generations, highlighting their vulnerability to error propagation due to linguistic shifts. These findings highlight the need for improved robustness techniques to enhance reliability in diverse user interactions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- RAG: 43.3 -->
                
            <!-- LLMs: 11.3 -->
                
            <!-- GNN: 2.4 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Medicine: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.4269
            </span>
            <a href="https://arxiv.org/abs/2504.08098" target="_blank" rel="noopener noreferrer">Semicontinuity bounds for the von Neumann entropy and partial majorization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: M. E. Shirokov | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We consider families of tight upper bounds on the difference $S(\rho)-S(\sigma)$ with the rank/energy constraint imposed on the state $\rho$ which are valid provided that the state $\rho$ partially majorizes the state $\sigma$ and is close to the state $\sigma$ w.r.t. the trace norm.</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.7 -->
                
            <!-- LLMs: 5.9 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Math: 2.8 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Multi-armed Bandit: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.666
            </span>
            <a href="https://arxiv.org/abs/2409.16163" target="_blank" rel="noopener noreferrer">The anonymization problem in social networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rachel G. de Jong, Mark P. J. van der Loo, Frank W. Takes | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper we introduce a general version of the anonymization problem in social networks, in which the goal is to maximize the number of anonymous nodes by altering a given graph. We define three variants of this optimization problem being full, partial and budgeted anonymization. In each, the o</span>
            
            <span class="abstract-full" style="display: none;">In this paper we introduce a general version of the anonymization problem in social networks, in which the goal is to maximize the number of anonymous nodes by altering a given graph. We define three variants of this optimization problem being full, partial and budgeted anonymization. In each, the objective is to maximize the number of k-anonymous nodes, i.e., nodes for which there are at least k-1 equivalent nodes, according to a particular anonymity measure of structural node equivalence. We propose four new heuristic algorithms for solving the anonymization problem which we implement into a reusable computational framework. As a baseline, we use an edge sampling method introduced in previous work. Experiments on both graph models and 23 real-world network datasets result in three empirical findings. First, we demonstrate that edge deletion is the most effective graph alteration operation. Second, we compare four commonly used anonymity measures from the literature and highlight how the choice of anonymity measure has a tremendous effect on both the initial anonymity as well as the difficulty of solving the anonymization problem. Third, we find that the proposed algorithm that preferentially deletes edges with a larger effect on nodes at a structurally unique position consistently outperforms heuristics solely based on network structure. Our best performing algorithm retains on average 14 times more edges in full anonymization, and overall ensures a better trade-off between anonymity and data utility. In the budgeted variant, it achieves 4.8 times more anonymous nodes than the baseline. This work lays foundations for future development of algorithms for anonymizing social networks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.7 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.4 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.6978
            </span>
            <a href="https://arxiv.org/abs/2504.07596" target="_blank" rel="noopener noreferrer">Boosting Universal LLM Reward Design through Heuristic Reward Observation Space Evolution</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zen Kit Heng, Zimeng Zhao, Tianhao Wu, Yuanfei Wang, Mingdong Wu, Yangang Wang, Hao Dong | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) are emerging as promising tools for automated reinforcement learning (RL) reward design, owing to their robust capabilities in commonsense reasoning and code generation. By engaging in dialogues with RL agents, LLMs construct a Reward Observation Space (ROS) by selecting relevant environment states and defining their internal operations. However, existing frameworks have not effectively leveraged historical exploration data or manual task descriptions to iteratively evolve this space. In this paper, we propose a novel heuristic framework that enhances LLM-driven reward design by evolving the ROS through a table-based exploration caching mechanism and a text-code reconciliation strategy. Our framework introduces a state execution table, which tracks the historical usage and success rates of environment states, overcoming the Markovian constraint typically found in LLM dialogues and facilitating more effective exploration. Furthermore, we reconcile user-provided task descriptions with expert-defined success criteria using structured prompts, ensuring alignment in reward design objectives. Comprehensive evaluations on benchmark RL tasks demonstrate the effectiveness and stability of the proposed framework. Code and video demos are available at jingjjjjjie.github.io/LLM2Reward.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 18.2 -->
                
            <!-- Medicine: 6.3 -->
                
            <!-- Quantum Computing: 2.7 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Math: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.8448
            </span>
            <a href="https://arxiv.org/abs/2503.15854" target="_blank" rel="noopener noreferrer">Persistent Stiefel-Whitney Classes of Tangent Bundles</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dongwoo Gang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Stiefel-Whitney classes are invariants of the tangent bundle of a smooth manifold, represented as cohomology classes of the base manifold. These classes are essential in obstruction theory, embedding problems, and cobordism theory. In this work, we first reestablish an appropriate notion of vector b</span>
            
            <span class="abstract-full" style="display: none;">Stiefel-Whitney classes are invariants of the tangent bundle of a smooth manifold, represented as cohomology classes of the base manifold. These classes are essential in obstruction theory, embedding problems, and cobordism theory. In this work, we first reestablish an appropriate notion of vector bundles in a persistent setting, allowing characteristic classes to be interpreted through topological data analysis. Next, we propose a concrete algorithm to compute persistent cohomology classes that represent the Stiefel-Whitney classes of the tangent bundle of a smooth manifold. Given a point cloud, we construct a \v{C}ech or alpha filtration. By applying the Wu formula in this setting, we derive a sequence of persistent cohomology classes from the filtration. We show that if the filtration is homotopy equivalent to a smooth manifold, then one of these persistent cohomology classes corresponds to the $k$-th Stiefel-Whitney class of the tangent bundle of that manifold. To demonstrate the effectiveness of our approach, we present experiments on real-world datasets, including applications to complex manifolds, image patches, and molecular conformation space.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 6.4 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.939
            </span>
            <a href="https://arxiv.org/abs/2410.10370" target="_blank" rel="noopener noreferrer">Innovative Thinking, Infinite Humor: Humor Research of Large Language Models through Structured Thought Leaps</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Han Wang, Yilin Zhao, Dian Li, Xiaohan Wang, Gang Liu, Xuguang Lan, Hui Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Humor is previously regarded as a gift exclusive to humans for the following reasons. Humor is a culturally nuanced aspect of human language, presenting challenges for its understanding and generation. Humor generation necessitates a multi-hop reasoning process, with each hop founded on proper ratio</span>
            
            <span class="abstract-full" style="display: none;">Humor is previously regarded as a gift exclusive to humans for the following reasons. Humor is a culturally nuanced aspect of human language, presenting challenges for its understanding and generation. Humor generation necessitates a multi-hop reasoning process, with each hop founded on proper rationales. Although many studies, such as those related to GPT-o1, focus on logical reasoning with reflection and correction, they still fall short in humor generation. Due to the sparsity of the knowledge graph in creative thinking, it is arduous to achieve multi-hop reasoning. Consequently, in this paper, we propose a more robust framework for addressing the humor reasoning task, named LoL. LoL aims to inject external information to mitigate the sparsity of the knowledge graph, thereby enabling multi-hop reasoning. In the first stage of LoL, we put forward an automatic instruction-evolution method to incorporate the deeper and broader thinking processes underlying humor. Judgment-oriented instructions are devised to enhance the model's judgment capability, dynamically supplementing and updating the sparse knowledge graph. Subsequently, through reinforcement learning, the reasoning logic for each online-generated response is extracted using GPT-4o. In this process, external knowledge is re-introduced to aid the model in logical reasoning and the learning of human preferences. Finally, experimental results indicate that the combination of these two processes can enhance both the model's judgment ability and its generative capacity. These findings deepen our comprehension of the creative capabilities of large language models (LLMs) and offer approaches to boost LLMs' creative abilities for cross-domain innovative applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 24.8 -->
                
            <!-- Medicine: 5.0 -->
                
            <!-- Quantum Computing: 2.9 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Networks: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Federated Learning: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -4.9536
            </span>
            <a href="https://arxiv.org/abs/2504.08525" target="_blank" rel="noopener noreferrer">Task Memory Engine (TME): Enhancing State Awareness for Multi-Step LLM Agent Tasks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ye Ye | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, fr</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) are increasingly used as autonomous agents for multi-step tasks. However, most existing frameworks fail to maintain a structured understanding of the task state, often relying on linear prompt concatenation or shallow memory buffers. This leads to brittle performance, frequent hallucinations, and poor long-range coherence. In this work, we propose the Task Memory Engine (TME), a lightweight and structured memory module that tracks task execution using a hierarchical Task Memory Tree (TMT). Each node in the tree corresponds to a task step, storing relevant input, output, status, and sub-task relationships. We introduce a prompt synthesis method that dynamically generates LLM prompts based on the active node path, significantly improving execution consistency and contextual grounding. Through case studies and comparative experiments on multi-step agent tasks, we demonstrate that TME leads to better task completion accuracy and more interpretable behavior with minimal implementation overhead. The full implementation of TME is available at https://github.com/biubiutomato/TME-Agent.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #5f6936" title="Confidence: 55.8%">
                        LLMs
                    </span>
            <!-- Medicine: 5.1 -->
                
            <!-- GNN: 2.6 -->
                
            <!-- Quantum Computing: 2.2 -->
                
            <!-- Robotics: 2.1 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Networks: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- RAG: 1.3 -->
                
            <!-- Reinforcement Learning: 1.2 -->
                
            <!-- Federated Learning: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.0352
            </span>
            <a href="https://arxiv.org/abs/2504.07986" target="_blank" rel="noopener noreferrer">SEAL: Steerable Reasoning Calibration of Large Language Models for Free</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Runjin Chen, Zhenyu Zhang, Junyuan Hong, Souvik Kundu, Zhangyang Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated compelling capabilities for complex reasoning tasks via the extended chain-of-thought (CoT) reasoning mechanism. However, recent studies reveal substantial redundancy in the CoT reasoning traces, which not only increases infe</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs), such as OpenAI's o1-series have demonstrated compelling capabilities for complex reasoning tasks via the extended chain-of-thought (CoT) reasoning mechanism. However, recent studies reveal substantial redundancy in the CoT reasoning traces, which not only increases inference latency but also negatively impacts model performance by diverting attention to unnecessary reasoning paths. To address this issue, we investigate the internal reasoning structures of LLMs and categorize them into three primary thought types: execution, reflection, and transition thoughts. Moreover, our analysis reveals that excessive reflection and transition thoughts are strongly correlated with failure cases and these thought categories exhibit clear separation in the latent space. Based on these, we introduce SEAL (Steerable reasoning calibration), a training-free approach that seamlessly calibrates the CoT process, improving accuracy while demonstrating significant efficiency gains. SEAL consists of an offline stage for extracting the reasoning steering vector in the latent space, followed by an on-the-fly calibration of the reasoning trace through representation intervention using the steering vector. Notably, the steering vector exhibits strong transferability across various tasks. Extensive experiments across multiple models (DeepSeek-R1-Distill and QwQ-32B-Preview) and benchmarks (Math500, GSM8K, LiveCodeBench) validate the effectiveness of SEAL, up to a 11% improvement in accuracy while reducing reasoning tokens by 11.8% to 50.4%. Our code is publicly available at https://github.com/VITA-Group/SEAL.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.1 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.1342
            </span>
            <a href="https://arxiv.org/abs/2504.08682" target="_blank" rel="noopener noreferrer">Bayesian optimization for mixed variables using an adaptive dimension reduction process: applications to aircraft design</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Paul Saves, Nathalie Bartoli, Youssef Diouane, Thierry Lefebvre, Joseph Morlier, Christophe David, Eric Nguyen Van, S\'ebastien Defoort | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multidisciplinary design optimization methods aim at adapting numerical optimization techniques to the design of engineering systems involving multiple disciplines. In this context, a large number of mixed continuous, integer and categorical variables might arise during the optimization process and </span>
            
            <span class="abstract-full" style="display: none;">Multidisciplinary design optimization methods aim at adapting numerical optimization techniques to the design of engineering systems involving multiple disciplines. In this context, a large number of mixed continuous, integer and categorical variables might arise during the optimization process and practical applications involve a large number of design variables. Recently, there has been a growing interest in mixed variables constrained Bayesian optimization but most existing approaches severely increase the number of the hyperparameters related to the surrogate model. In this paper, we address this issue by constructing surrogate models using less hyperparameters. The reduction process is based on the partial least squares method. An adaptive procedure for choosing the number of hyperparameters is proposed. The performance of the proposed approach is confirmed on analytical tests as well as two real applications related to aircraft design. A significant improvement is obtained compared to genetic algorithms.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.6 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- Robotics: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.189
            </span>
            <a href="https://arxiv.org/abs/2504.08272" target="_blank" rel="noopener noreferrer">Palmprint De-Identification Using Diffusion Model for High-Quality and Diverse Synthesis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Licheng Yan, Bob Zhang, Andrew Beng Jin Teoh, Lu Leng, Shuyi Li, Yuqi Wang, Ziyuan Yang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Palmprint recognition techniques have advanced significantly in recent years, enabling reliable recognition even when palmprints are captured in uncontrolled or challenging environments. However, this strength also introduces new risks, as publicly available palmprint images can be misused by advers</span>
            
            <span class="abstract-full" style="display: none;">Palmprint recognition techniques have advanced significantly in recent years, enabling reliable recognition even when palmprints are captured in uncontrolled or challenging environments. However, this strength also introduces new risks, as publicly available palmprint images can be misused by adversaries for malicious activities. Despite this growing concern, research on methods to obscure or anonymize palmprints remains largely unexplored. Thus, it is essential to develop a palmprint de-identification technique capable of removing identity-revealing features while retaining the image's utility and preserving non-sensitive information. In this paper, we propose a training-free framework that utilizes pre-trained diffusion models to generate diverse, high-quality palmprint images that conceal identity features for de-identification purposes. To ensure greater stability and controllability in the synthesis process, we incorporate a semantic-guided embedding fusion alongside a prior interpolation mechanism. We further propose the de-identification ratio, a novel metric for intuitive de-identification assessment. Extensive experiments across multiple palmprint datasets and recognition methods demonstrate that our method effectively conceals identity-related traits with significant diversity across de-identified samples. The de-identified samples preserve high visual fidelity and maintain excellent usability, achieving a balance between de-identification and retaining non-identity information.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.8 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.2062
            </span>
            <a href="https://arxiv.org/abs/2503.17604" target="_blank" rel="noopener noreferrer">OmniScience: A Domain-Specialized LLM for Scientific Reasoning and Discovery</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vignesh Prabhakar, Md Amirul Islam, Adam Atanas, Yao-Ting Wang, Joah Han, Aastha Jhunjhunwala, Rucha Apte, Robert Clark, Kang Xu, Zihan Wang, Kai Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretra</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have demonstrated remarkable potential in advancing scientific knowledge and addressing complex challenges. In this work, we introduce OmniScience, a specialized large reasoning model for general science, developed through three key components: (1) domain adaptive pretraining on a carefully curated corpus of scientific literature, (2) instruction tuning on a specialized dataset to guide the model in following domain-specific tasks, and (3) reasoning-based knowledge distillation through fine-tuning to significantly enhance its ability to generate contextually relevant and logically sound responses. We demonstrate the versatility of OmniScience by developing a battery agent that efficiently ranks molecules as potential electrolyte solvents or additives. Comprehensive evaluations reveal that OmniScience is competitive with state-of-the-art large reasoning models on the GPQA Diamond and domain-specific battery benchmarks, while outperforming all public reasoning and non-reasoning models with similar parameter counts. We further demonstrate via ablation experiments that domain adaptive pretraining and reasoning-based knowledge distillation are critical to attain our performance levels, across benchmarks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 16.3 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- T2I: 1.5 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.3258
            </span>
            <a href="https://arxiv.org/abs/2502.03979" target="_blank" rel="noopener noreferrer">Towards Unified Music Emotion Recognition across Dimensional and Categorical Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jaeyong Kang, Dorien Herremans | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">One of the most significant challenges in Music Emotion Recognition (MER) comes from the fact that emotion labels can be heterogeneous across datasets with regard to the emotion representation, including categorical (e.g., happy, sad) versus dimensional labels (e.g., valence-arousal). In this paper,</span>
            
            <span class="abstract-full" style="display: none;">One of the most significant challenges in Music Emotion Recognition (MER) comes from the fact that emotion labels can be heterogeneous across datasets with regard to the emotion representation, including categorical (e.g., happy, sad) versus dimensional labels (e.g., valence-arousal). In this paper, we present a unified multitask learning framework that combines these two types of labels and is thus able to be trained on multiple datasets. This framework uses an effective input representation that combines musical features (i.e., key and chords) and MERT embeddings. Moreover, knowledge distillation is employed to transfer the knowledge of teacher models trained on individual datasets to a student model, enhancing its ability to generalize across multiple tasks. To validate our proposed framework, we conducted extensive experiments on a variety of datasets, including MTG-Jamendo, DEAM, PMEmo, and EmoMusic. According to our experimental results, the inclusion of musical features, multitask learning, and knowledge distillation significantly enhances performance. In particular, our model outperforms the state-of-the-art models, including the best-performing model from the MediaEval 2021 competition on the MTG-Jamendo dataset. Our work makes a significant contribution to MER by allowing the combination of categorical and dimensional emotion labels in one unified framework, thus enabling training across datasets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.8 -->
                
            <!-- LLMs: 8.1 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.336
            </span>
            <a href="https://arxiv.org/abs/2504.08048" target="_blank" rel="noopener noreferrer">On Quorum Sizes in DAG-Based BFT Protocols</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Razya Ladelsky, Roy Friedman | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Several prominent DAG-based blockchain protocols, such as DAG-Rider, Tusk, and Bullshark, completely separate between equivocation elimination and committing; equivocation is handled through the use of a reliable Byzantine broadcast black-box protocol, while committing is handled by an independent D</span>
            
            <span class="abstract-full" style="display: none;">Several prominent DAG-based blockchain protocols, such as DAG-Rider, Tusk, and Bullshark, completely separate between equivocation elimination and committing; equivocation is handled through the use of a reliable Byzantine broadcast black-box protocol, while committing is handled by an independent DAG-based protocol. With such an architecture, a natural question that we study in this paper is whether the DAG protocol would work when the number of nodes (or validators) is only $2f+1$ (when equivocation is eliminated), and whether there are benefits in working with larger number of nodes, i.e., a total of $kf+1$ nodes for $k > 3$.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.4 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.3406
            </span>
            <a href="https://arxiv.org/abs/2504.08399" target="_blank" rel="noopener noreferrer">Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yin Jou Huang, Rafik Hadfi | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">There is a growing interest in assessing the personality traits of Large language models (LLMs). However, traditional personality assessments based on self-report questionnaires may fail to capture their true behavioral nuances due to inherent biases and meta-knowledge contamination. This paper intr</span>
            
            <span class="abstract-full" style="display: none;">There is a growing interest in assessing the personality traits of Large language models (LLMs). However, traditional personality assessments based on self-report questionnaires may fail to capture their true behavioral nuances due to inherent biases and meta-knowledge contamination. This paper introduces a novel multi-observer framework for LLM personality assessment that draws inspiration from informant-report methods in psychology. Instead of relying solely on self-assessments, our approach employs multiple observer agents configured with a specific relationship context (e.g., family, friend, or workplace) to simulate interactive scenarios with a subject LLM. These observers engage in dialogues and subsequently provide ratings across the Big Five personality dimensions. Our experiments reveal that LLMs possess systematic biases in self-report personality ratings. Moreover, aggregating observer ratings effectively reduces non-systematic biases and achieves optimal reliability with 5-7 observers. The findings highlight the significant impact of relationship context on personality perception and demonstrate that a multi-observer paradigm yields a more robust and context-sensitive evaluation of LLM personality traits.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.2 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- 3D: 2.0 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.3558
            </span>
            <a href="https://arxiv.org/abs/2501.10261" target="_blank" rel="noopener noreferrer">Logarithmic Regret for Nonlinear Control</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: James Wang, Bruce D. Lee, Ingvar Ziemann, Nikolai Matni | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We address the problem of learning to control an unknown nonlinear dynamical system through sequential interactions. Motivated by high-stakes applications in which mistakes can be catastrophic, such as robotics and healthcare, we study situations where it is possible for fast sequential learning to </span>
            
            <span class="abstract-full" style="display: none;">We address the problem of learning to control an unknown nonlinear dynamical system through sequential interactions. Motivated by high-stakes applications in which mistakes can be catastrophic, such as robotics and healthcare, we study situations where it is possible for fast sequential learning to occur. Fast sequential learning is characterized by the ability of the learning agent to incur logarithmic regret relative to a fully-informed baseline. We demonstrate that fast sequential learning is achievable in a diverse class of continuous control problems where the system dynamics depend smoothly on unknown parameters, provided the optimal control policy is persistently exciting. Additionally, we derive a regret bound which grows with the square root of the number of interactions for cases where the optimal policy is not persistently exciting. Our results provide the first regret bounds for controlling nonlinear dynamical systems depending nonlinearly on unknown parameters. We validate the trends our theory predicts in simulation on a simple dynamical system.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.2 -->
                
            <!-- Medicine: 7.0 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.3905
            </span>
            <a href="https://arxiv.org/abs/2504.08438" target="_blank" rel="noopener noreferrer">Diffusion Models for Robotic Manipulation: A Survey</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rosa Wolf, Yitian Shi, Sheng Liu, Rania Rayyes | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Diffusion generative models have demonstrated remarkable success in visual domains such as image and video generation. They have also recently emerged as a promising approach in robotics, especially in robot manipulations. Diffusion models leverage a probabilistic framework, and they stand out with </span>
            
            <span class="abstract-full" style="display: none;">Diffusion generative models have demonstrated remarkable success in visual domains such as image and video generation. They have also recently emerged as a promising approach in robotics, especially in robot manipulations. Diffusion models leverage a probabilistic framework, and they stand out with their ability to model multi-modal distributions and their robustness to high-dimensional input and output spaces. This survey provides a comprehensive review of state-of-the-art diffusion models in robotic manipulation, including grasp learning, trajectory planning, and data augmentation. Diffusion models for scene and image augmentation lie at the intersection of robotics and computer vision for vision-based tasks to enhance generalizability and data scarcity. This paper also presents the two main frameworks of diffusion models and their integration with imitation learning and reinforcement learning. In addition, it discusses the common architectures and benchmarks and points out the challenges and advantages of current state-of-the-art diffusion-based methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4134
            </span>
            <a href="https://arxiv.org/abs/2504.08001" target="_blank" rel="noopener noreferrer">Linguistic Interpretability of Transformer-based Language Models: a systematic review</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Miguel L\'opez-Otal, Jorge Gracia, Jordi Bernad, Carlos Bobed, Luc\'ia Pitarch-Ballesteros, Emma Angl\'es-Herrero | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Language models based on the Transformer architecture achieve excellent results in many language-related tasks, such as text classification or sentiment analysis. However, despite the architecture of these models being well-defined, little is known about how their internal computations help them ach</span>
            
            <span class="abstract-full" style="display: none;">Language models based on the Transformer architecture achieve excellent results in many language-related tasks, such as text classification or sentiment analysis. However, despite the architecture of these models being well-defined, little is known about how their internal computations help them achieve their results. This renders these models, as of today, a type of 'black box' systems. There is, however, a line of research -- 'interpretability' -- aiming to learn how information is encoded inside these models. More specifically, there is work dedicated to studying whether Transformer-based models possess knowledge of linguistic phenomena similar to human speakers -- an area we call 'linguistic interpretability' of these models. In this survey we present a comprehensive analysis of 160 research works, spread across multiple languages and models -- including multilingual ones -- that attempt to discover linguistic information from the perspective of several traditional Linguistics disciplines: Syntax, Morphology, Lexico-Semantics and Discourse. Our survey fills a gap in the existing interpretability literature, which either not focus on linguistic knowledge in these models or present some limitations -- e.g. only studying English-based models. Our survey also focuses on Pre-trained Language Models not further specialized for a downstream task, with an emphasis on works that use interpretability techniques that explore models' internal representations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4151
            </span>
            <a href="https://arxiv.org/abs/2410.05852" target="_blank" rel="noopener noreferrer">A$^3$L-FEC: Age-Aware Application Layer Forward Error Correction Flow Control</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sajjad Baghaee, Elif Uysal | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Age of Information (AoI) is a metric and KPI that has been developed for measuring and controlling data freshness. Optimization of AoI in a real-life network requires adapting the rate and timing of transmissions to varying network conditions. The vast majority of previous research on the control of</span>
            
            <span class="abstract-full" style="display: none;">Age of Information (AoI) is a metric and KPI that has been developed for measuring and controlling data freshness. Optimization of AoI in a real-life network requires adapting the rate and timing of transmissions to varying network conditions. The vast majority of previous research on the control of AoI has been theoretical, using idealized models that ignored certain implementation aspects. As such, there is still a gap between the research on AoI and real-world protocols. In this paper we present an effort toward closing this gap by introducing an age-aware flow control algorithm. The algorithm, Age-Aware Application Layer Forward Error Correction (A$^3$L-FEC), is a packet generation mechanism operating on top of the User Datagram Protocol (UDP). The purpose is to control the peak Age of the end-to-end packet flow, specifically to reduce the rate of so-called "Age Violations," i.e., events where the peak age exceeds a given threshold. Evaluations in Mininet-WiFi and MATLAB indicate that A$3$L-FEC reduces age violations compared to two related protocols in the literature, namely TCP-BBR and ACP+.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.1 -->
                
            <!-- LLMs: 7.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.438
            </span>
            <a href="https://arxiv.org/abs/2408.13880" target="_blank" rel="noopener noreferrer">On classical advice, sampling advise and complexity assumptions for learning separations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jordi P\'erez-Guijarro | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we study the relationship between advice in the form of a training set and classical advice. We do this by analyzing the class \textbf{BPP/samp} and certain variants of it. Specifically, our main result demonstrates that \textbf{BPP/samp} is a proper subset of the class \textbf{P/poly</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we study the relationship between advice in the form of a training set and classical advice. We do this by analyzing the class \textbf{BPP/samp} and certain variants of it. Specifically, our main result demonstrates that \textbf{BPP/samp} is a proper subset of the class \textbf{P/poly}. This result remains valid when considering quantum advice and a quantum generalization of the training set. Finally, leveraging the insights gained from these proofs, we identify sufficient and necessary complexity assumptions for the existence of concept classes that exhibit a quantum learning speed-up in the worst-case scenario, i.e., when accurate results are required for all inputs, and in the average-case scenario.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.2 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- 3D: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4664
            </span>
            <a href="https://arxiv.org/abs/2504.08600" target="_blank" rel="noopener noreferrer">SQL-R1: Training Natural Language to SQL Reasoning Model By Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Peixian Ma, Xialie Zhuang, Chengjin Xu, Xuhui Jiang, Ran Chen, Jian Guo | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regar</span>
            
            <span class="abstract-full" style="display: none;">Natural Language to SQL (NL2SQL) enables intuitive interactions with databases by transforming natural language queries into structured SQL statements. Despite recent advancements in enhancing human-computer interaction within database applications, significant challenges persist, particularly regarding the inference performance in complex scenarios involving multi-table joins and nested queries. Current methodologies primarily utilize supervised fine-tuning (SFT) to train the NL2SQL model, which may limit adaptability and interpretability in new environments (e.g., finance and healthcare). In order to enhance the reasoning performance of the NL2SQL model in the above complex situations, we introduce SQL-R1, a novel NL2SQL reasoning model trained by the reinforcement learning (RL) algorithms. We design a specialized RL-based reward function tailored for NL2SQL tasks and discussed the impact of cold start on the effectiveness of intensive training. In addition, we achieve competitive accuracy using only a tiny amount of synthetic NL2SQL data for augmented training and further explore data engineering for RL. In existing experiments, SQL-R1 achieves execution accuracy of 88.6% and 66.6% on the benchmark Spider and BIRD, respectively, only using the 7B base model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.5 -->
                
            <!-- LLMs: 6.7 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.5 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4674
            </span>
            <a href="https://arxiv.org/abs/2501.01142" target="_blank" rel="noopener noreferrer">Adaptive Hardness-driven Augmentation and Alignment Strategies for Multi-Source Domain Adaptations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yang Yuxiang, Zeng Xinyi, Zeng Pinxian, Zu Chen, Yan Binyu, Zhou Jiliu, Wang Yan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multi-source Domain Adaptation (MDA) aims to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Nevertheless, traditional methods primarily focus on achieving inter-domain alignment through sample-level constraints, such as Maximum Mean Discrepancy (MMD), neglecti</span>
            
            <span class="abstract-full" style="display: none;">Multi-source Domain Adaptation (MDA) aims to transfer knowledge from multiple labeled source domains to an unlabeled target domain. Nevertheless, traditional methods primarily focus on achieving inter-domain alignment through sample-level constraints, such as Maximum Mean Discrepancy (MMD), neglecting three pivotal aspects: 1) the potential of data augmentation, 2) the significance of intra-domain alignment, and 3) the design of cluster-level constraints. In this paper, we introduce a novel hardness-driven strategy for MDA tasks, named "A3MDA" , which collectively considers these three aspects through Adaptive hardness quantification and utilization in both data Augmentation and domain Alignment.To achieve this, "A3MDA" progressively proposes three Adaptive Hardness Measurements (AHM), i.e., Basic, Smooth, and Comparative AHMs, each incorporating distinct mechanisms for diverse scenarios. Specifically, Basic AHM aims to gauge the instantaneous hardness for each source/target sample. Then, hardness values measured by Smooth AHM will adaptively adjust the intensity level of strong data augmentation to maintain compatibility with the model's generalization capacity.In contrast, Comparative AHM is designed to facilitate cluster-level constraints. By leveraging hardness values as sample-specific weights, the traditional MMD is enhanced into a weighted-clustered variant, strengthening the robustness and precision of inter-domain alignment. As for the often-neglected intra-domain alignment, we adaptively construct a pseudo-contrastive matrix by selecting harder samples based on the hardness rankings, enhancing the quality of pseudo-labels, and shaping a well-clustered target feature space. Experiments on multiple MDA benchmarks show that " A3MDA " outperforms other methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.7 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4803
            </span>
            <a href="https://arxiv.org/abs/2504.08063" target="_blank" rel="noopener noreferrer">Deterministic factorization of constant-depth algebraic circuits in subexponential time</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Somnath Bhattacharjee, Mrinal Kumar, Varun Ramanathan, Ramprasad Saptharishi, Shubhangi Saraf | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">While efficient randomized algorithms for factorization of polynomials given by algebraic circuits have been known for decades, obtaining an even slightly non-trivial deterministic algorithm for this problem has remained an open question of great interest. This is true even when the input algebraic </span>
            
            <span class="abstract-full" style="display: none;">While efficient randomized algorithms for factorization of polynomials given by algebraic circuits have been known for decades, obtaining an even slightly non-trivial deterministic algorithm for this problem has remained an open question of great interest. This is true even when the input algebraic circuit has additional structure, for instance, when it is a constant-depth circuit. Indeed, no efficient deterministic algorithms are known even for the seemingly easier problem of factoring sparse polynomials or even the problem of testing the irreducibility of sparse polynomials.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.9 -->
                
            <!-- LLMs: 8.7 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.4839
            </span>
            <a href="https://arxiv.org/abs/2410.01590" target="_blank" rel="noopener noreferrer">Active Learning of Deterministic Transducers with Outputs in Arbitrary Monoids</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Quentin Aristote (Universit\'e Paris Cit\'e, CNRS, Inria, IRIF, F-75013, Paris, France) | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We study monoidal transducers, transition systems arising as deterministic automata whose transitions also produce outputs in an arbitrary monoid, for instance allowing outputs to commute or to cancel out. We use the categorical framework for minimization and learning of Colcombet, Petri\c{s}an and </span>
            
            <span class="abstract-full" style="display: none;">We study monoidal transducers, transition systems arising as deterministic automata whose transitions also produce outputs in an arbitrary monoid, for instance allowing outputs to commute or to cancel out. We use the categorical framework for minimization and learning of Colcombet, Petri\c{s}an and Stabile to recover the notion of minimal transducer recognizing a language, and give necessary and sufficient conditions on the output monoid for this minimal transducer to exist and be unique (up to isomorphism). The categorical framework then provides an abstract algorithm for learning it using membership and equivalence queries, and we discuss practical aspects of this algorithm's implementation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.548
            </span>
            <a href="https://arxiv.org/abs/2504.08210" target="_blank" rel="noopener noreferrer">Optimizing Power Grid Topologies with Reinforcement Learning: A Survey of Methods and Challenges</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Erica van der Sar, Alessandro Zocca, Sandjai Bhulai | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Power grid operation is becoming increasingly complex due to the rising integration of renewable energy sources and the need for more adaptive control strategies. Reinforcement Learning (RL) has emerged as a promising approach to power network control (PNC), offering the potential to enhance decisio</span>
            
            <span class="abstract-full" style="display: none;">Power grid operation is becoming increasingly complex due to the rising integration of renewable energy sources and the need for more adaptive control strategies. Reinforcement Learning (RL) has emerged as a promising approach to power network control (PNC), offering the potential to enhance decision-making in dynamic and uncertain environments. The Learning To Run a Power Network (L2RPN) competitions have played a key role in accelerating research by providing standardized benchmarks and problem formulations, leading to rapid advancements in RL-based methods. This survey provides a comprehensive and structured overview of RL applications for power grid topology optimization, categorizing existing techniques, highlighting key design choices, and identifying gaps in current research. Additionally, we present a comparative numerical study evaluating the impact of commonly applied RL-based methods, offering insights into their practical effectiveness. By consolidating existing research and outlining open challenges, this survey aims to provide a foundation for future advancements in RL-driven power grid optimization.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 4.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.5951
            </span>
            <a href="https://arxiv.org/abs/2504.08201" target="_blank" rel="noopener noreferrer">Neural Encoding and Decoding at Scale</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yizi Zhang, Yanchen Wang, Mehdi Azabou, Alexandre Andre, Zixuan Wang, Hanrui Lyu, The International Brain Laboratory, Eva Dyer, Liam Paninski, Cole Hurwitz | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent work has demonstrated that large-scale, multi-animal models are powerful tools for characterizing the relationship between neural activity and behavior. Current large-scale approaches, however, focus exclusively on either predicting neural activity from behavior (encoding) or predicting behav</span>
            
            <span class="abstract-full" style="display: none;">Recent work has demonstrated that large-scale, multi-animal models are powerful tools for characterizing the relationship between neural activity and behavior. Current large-scale approaches, however, focus exclusively on either predicting neural activity from behavior (encoding) or predicting behavior from neural activity (decoding), limiting their ability to capture the bidirectional relationship between neural activity and behavior. To bridge this gap, we introduce a multimodal, multi-task model that enables simultaneous Neural Encoding and Decoding at Scale (NEDS). Central to our approach is a novel multi-task-masking strategy, which alternates between neural, behavioral, within-modality, and cross-modality masking. We pretrain our method on the International Brain Laboratory (IBL) repeated site dataset, which includes recordings from 83 animals performing the same visual decision-making task. In comparison to other large-scale models, we demonstrate that NEDS achieves state-of-the-art performance for both encoding and decoding when pretrained on multi-animal data and then fine-tuned on new animals. Surprisingly, NEDS's learned embeddings exhibit emergent properties: even without explicit training, they are highly predictive of the brain regions in each recording. Altogether, our approach is a step towards a foundation model of the brain that enables seamless translation between neural activity and behavior.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- GNN: 2.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.5956
            </span>
            <a href="https://arxiv.org/abs/2504.08613" target="_blank" rel="noopener noreferrer">Enhancing knowledge retention for continual learning with domain-specific adapters and features gating</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mohamed Abbas Hedjazi, Oussama Hadjerci, Adel Hafiane | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Continual learning empowers models to learn from a continuous stream of data while preserving previously acquired knowledge, effectively addressing the challenge of catastrophic forgetting. In this study, we propose a new approach that integrates adapters within the self-attention mechanisms of Visi</span>
            
            <span class="abstract-full" style="display: none;">Continual learning empowers models to learn from a continuous stream of data while preserving previously acquired knowledge, effectively addressing the challenge of catastrophic forgetting. In this study, we propose a new approach that integrates adapters within the self-attention mechanisms of Vision Transformers to enhance knowledge retention when sequentially adding datasets from different domains. Unlike previous methods that continue learning with only one dataset, our approach introduces domain-specific output heads and feature gating, allowing the model to maintain high accuracy on previously learned tasks while incorporating only the essential information from multiple domains. The proposed method is compared to prominent parameter-efficient fine-tuning methods in the current state of the art. The results provide evidence that our method effectively alleviates the limitations of previous works. Furthermore, we conduct a comparative analysis using three datasets, CIFAR-100, Flowers102, and DTD, each representing a distinct domain, to investigate the impact of task order on model performance. Our findings underscore the critical role of dataset sequencing in shaping learning outcomes, demonstrating that strategic ordering can significantly improve the model's ability to adapt to evolving data distributions over time while preserving the integrity of previously learned knowledge.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Blockchain: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6044
            </span>
            <a href="https://arxiv.org/abs/2409.19465" target="_blank" rel="noopener noreferrer">Construction of the Sparsest Maximally r-Robust Graphs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Haejoon Lee, Dimitra Panagou | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent years, the notion of r-robustness for the communication graph of the network has been introduced to address the challenge of achieving consensus in the presence of misbehaving agents. Higher r-robustness typically implies higher tolerance to malicious information towards achieving resilien</span>
            
            <span class="abstract-full" style="display: none;">In recent years, the notion of r-robustness for the communication graph of the network has been introduced to address the challenge of achieving consensus in the presence of misbehaving agents. Higher r-robustness typically implies higher tolerance to malicious information towards achieving resilient consensus, but it also implies more edges for the communication graph. This in turn conflicts with the need to minimize communication due to limited resources in real-world applications (e.g., multi-robot networks). In this paper, our contributions are twofold. (a) We provide the necessary subgraph structures and tight lower bounds on the number of edges required for graphs with a given number of nodes to achieve maximum robustness. (b) We then use the results of (a) to introduce two classes of graphs that maintain maximum robustness with the least number of edges. Our work is validated through a series of simulations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.7 -->
                
            <!-- LLMs: 6.2 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Math: 2.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.7 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6221
            </span>
            <a href="https://arxiv.org/abs/2504.08450" target="_blank" rel="noopener noreferrer">Well-Posedness of Discretizations for Fractional Elasto-Plasticity</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Michael Feischl, David Niederkofler, Barbara Wohlmuth | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We consider a fractional plasticity model based on linear isotropic and kinematic hardening as well as a standard von-Mises yield function, where the flow rule is replaced by a Riesz--Caputo fractional derivative. The resulting mathematical model is typically non-local and non-smooth. Our numerical </span>
            
            <span class="abstract-full" style="display: none;">We consider a fractional plasticity model based on linear isotropic and kinematic hardening as well as a standard von-Mises yield function, where the flow rule is replaced by a Riesz--Caputo fractional derivative. The resulting mathematical model is typically non-local and non-smooth. Our numerical algorithm is based on the well-known radial return mapping and exploits that the kernel is finitely supported. We propose explicit and implicit discretizations of the model and show the well-posedness of the explicit in time discretization in combination with a standard finite element approach in space. Our numerical results in 2D and 3D illustrate the performance of the algorithm and the influence of the fractional parameter.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.2 -->
                
            <!-- LLMs: 5.7 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Math: 2.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.4 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6362
            </span>
            <a href="https://arxiv.org/abs/2404.12481" target="_blank" rel="noopener noreferrer">Understanding Optimal Feature Transfer via a Fine-Grained Bias-Variance Analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yufan Li, Subhabrata Sen, Ben Adlam | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In the transfer learning paradigm models learn useful representations (or features) during a data-rich pretraining stage, and then use the pretrained representation to improve model performance on data-scarce downstream tasks. In this work, we explore transfer learning with the goal of optimizing do</span>
            
            <span class="abstract-full" style="display: none;">In the transfer learning paradigm models learn useful representations (or features) during a data-rich pretraining stage, and then use the pretrained representation to improve model performance on data-scarce downstream tasks. In this work, we explore transfer learning with the goal of optimizing downstream performance. We introduce a simple linear model that takes as input an arbitrary pretrained feature transform. We derive exact asymptotics of the downstream risk and its \textit{fine-grained} bias-variance decomposition. We then identify the pretrained representation that optimizes the asymptotic downstream bias and variance averaged over an ensemble of downstream tasks. Our theoretical and empirical analysis uncovers the surprising phenomenon that the optimal featurization is naturally sparse, even in the absence of explicit sparsity-inducing priors or penalties. Additionally, we identify a phase transition where the optimal pretrained representation shifts from hard selection to soft selection of relevant features.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6508
            </span>
            <a href="https://arxiv.org/abs/2412.04042" target="_blank" rel="noopener noreferrer">Recognizing 2-Layer and Outer $k$-Planar Graphs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yasuaki Kobayashi, Yuto Okada, Alexander Wolff | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The crossing number of a graph is the least number of crossings over all drawings of the graph in the plane. Computing the crossing number of a given graph is NP-hard, but fixed-parameter tractable (FPT) with respect to the natural parameter. Two well-known variants of the problem are 2-layer crossi</span>
            
            <span class="abstract-full" style="display: none;">The crossing number of a graph is the least number of crossings over all drawings of the graph in the plane. Computing the crossing number of a given graph is NP-hard, but fixed-parameter tractable (FPT) with respect to the natural parameter. Two well-known variants of the problem are 2-layer crossing minimization and circular crossing minimization, where every vertex must lie on one of two layers, namely two parallel lines, or a circle, respectively. Both variants are NP-hard, but FPT with respect to the natural parameter.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.8 -->
                
            <!-- Medicine: 6.3 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6524
            </span>
            <a href="https://arxiv.org/abs/2504.08608" target="_blank" rel="noopener noreferrer">Discretization Error Analysis of a High Order Unfitted Space-Time Method for moving domain problems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fabian Heimann, Christoph Lehrenfeld, Janosch Preu{\ss} | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a numerical analysis of a higher order unfitted space-time Finite Element method applied to a convection-diffusion model problem posed on a moving bulk domain. The method uses isoparametric space-time mappings for the geometry approximation of level set domains and has been presented and </span>
            
            <span class="abstract-full" style="display: none;">We present a numerical analysis of a higher order unfitted space-time Finite Element method applied to a convection-diffusion model problem posed on a moving bulk domain. The method uses isoparametric space-time mappings for the geometry approximation of level set domains and has been presented and investigated computationally in [Heimann, Lehrenfeld, Preu{\ss}, SIAM J. Sci. Comp. 45(2), 2023, B139 - B165]. Recently, in [Heimann, Lehrenfeld, IMA J. Numer. Anal., 2025] error bounds for the geometry approximation have been proven. In this paper we prove stability and accuracy including the influence of the geometry approximation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.7 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6678
            </span>
            <a href="https://arxiv.org/abs/2310.09638" target="_blank" rel="noopener noreferrer">A Combinatorial Algorithm for Weighted Correlation Clustering</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mojtaba Ostovari, Alireza Zarei | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This article introduces a quick and simple combinatorial approximation algorithm for the weighted correlation clustering problem. In this problem, we have a set of vertices and two weight values for each pair of vertices denoting their difference and similarity. The goal is to cluster the vertices w</span>
            
            <span class="abstract-full" style="display: none;">This article introduces a quick and simple combinatorial approximation algorithm for the weighted correlation clustering problem. In this problem, we have a set of vertices and two weight values for each pair of vertices denoting their difference and similarity. The goal is to cluster the vertices with minimum total intra-cluster difference weights plus inter-cluster similarity weights. Our algorithm is a randomized approximation algorithm with $O(n^2)$ running time where $n$ is the number of vertices. Its approximation factor is 3 when the instance satisfies probability constraints. If the instance satisfies triangle inequality in addition to probability constraints, the approximation factor is 1.6. Both algorithms are superior to the best known results in terms of running time and the second one is also superior in terms of the approximation factor.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.8 -->
                
            <!-- LLMs: 7.8 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6797
            </span>
            <a href="https://arxiv.org/abs/2504.08579" target="_blank" rel="noopener noreferrer">Analysis of the Unscented Transform Controller for Systems with Bounded Nonlinearities</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Siddharth A. Dinkar, Ram Padmanabhan, Anna Clarke, Per-Olof Gutman, Melkior Ornik | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we present an analysis of the Unscented Transform Controller (UTC), a technique to control nonlinear systems motivated as a dual to the Unscented Kalman Filter (UKF). We consider linear, discrete-time systems augmented by a bounded nonlinear function of the state. For such systems, we</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we present an analysis of the Unscented Transform Controller (UTC), a technique to control nonlinear systems motivated as a dual to the Unscented Kalman Filter (UKF). We consider linear, discrete-time systems augmented by a bounded nonlinear function of the state. For such systems, we review 1-step and N-step versions of the UTC. Using a Lyapunov-based analysis, we prove that the states and inputs converge to a bounded ball around the origin, whose radius depends on the bound on the nonlinearity. Using examples of a fighter jet model and a quadcopter, we demonstrate that the UTC achieves satisfactory regulation and tracking performance on these nonlinear models.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.1 -->
                
            <!-- Medicine: 6.7 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Hardware: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6839
            </span>
            <a href="https://arxiv.org/abs/2504.07968" target="_blank" rel="noopener noreferrer">Sensing for Communication: RIS-Assisted ISAC Coordination Gain Enhancement With Imperfect CSI</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xiaohui Li, Qi Zhu, Yunpei Chen, Chadi Assi, Yifei Yuan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Integrated sensing and communication (ISAC) has the potential to facilitate coordination gains from mutual assistance between sensing and communication (S&amp;C), especially sensing-aided communication enhancement (SACE). Reconfigurable intelligent surface (RIS) is another potential technique for ac</span>
            
            <span class="abstract-full" style="display: none;">Integrated sensing and communication (ISAC) has the potential to facilitate coordination gains from mutual assistance between sensing and communication (S&amp;C), especially sensing-aided communication enhancement (SACE). Reconfigurable intelligent surface (RIS) is another potential technique for achieving resource-efficient communication enhancement. Therefore, this paper proposes an innovative RIS-assisted SACE (R-SACE) mechanism with the goal of improving the systemic communication performance of the ISAC system in practical scenarios where the channel status information (CSI) is imperfectly known. In the proposed R-SACE mechanism, a dual-functional base station (BS) provides downlink communication services to both the communication user and the dynamically changing target that is detected using the communication signals. RIS assists in both sensing and communications of the BS. A typical scenario is investigated in which either or both the direct and RIS-assisted reflected communication links are available depending on sensing results. The average systemic throughput (AST) over the entire timeline of the R-SACE mechanism is maximized by jointly optimizing both temporal and spatial resources under the probabilistic constraint and the sensing performance, transmission power, and communication interference constraints. The non-convex probabilistic mixed optimization problem is transformed and then solved by the proposed fixed-point iterative (FPI) algorithm. Simulation results demonstrate that the proposed FPI algorithm and R-SACE mechanism outperform the baseline algorithms and communication enhancement mechanisms in achieving higher systemic communication performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.4 -->
                
            <!-- LLMs: 7.1 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6929
            </span>
            <a href="https://arxiv.org/abs/2401.10727" target="_blank" rel="noopener noreferrer">MLLM-Tool: A Multimodal Large Language Model For Tool Agent Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chenyu Wang, Weixin Luo, Sixun Dong, Xiaohua Xuan, Zhengxin Li, Lin Ma, Shenghua Gao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the applicat</span>
            
            <span class="abstract-full" style="display: none;">Recently, the astonishing performance of large language models (LLMs) in natural language comprehension and generation tasks triggered lots of exploration of using them as central controllers to build agent systems. Multiple studies focus on bridging the LLMs to external tools to extend the application scenarios. However, the current LLMs' ability to perceive tool use is limited to a single text query, which may result in ambiguity in understanding the users' real intentions. LLMs are expected to eliminate that by perceiving the information in the visual- or auditory-grounded instructions. Therefore, in this paper, we propose MLLM-Tool, a system incorporating open-source LLMs and multi-modal encoders so that the learned LLMs can be conscious of multi-modal input instruction and then select the function-matched tool correctly. To facilitate the evaluation of the model's capability, we collect a dataset featuring multi-modal input tools from HuggingFace. Another essential feature of our dataset is that it also contains multiple potential choices for the same instruction due to the existence of identical functions and synonymous functions, which provides more potential solutions for the same query. The experiments reveal that our MLLM-Tool is capable of recommending appropriate tools for multi-modal instructions. Codes and data are available at https://github.com/MLLM-Tool/MLLM-Tool.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.4 -->
                
            <!-- Medicine: 6.3 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.6964
            </span>
            <a href="https://arxiv.org/abs/2504.08505" target="_blank" rel="noopener noreferrer">POD-Based Sparse Stochastic Estimation of Wind Turbine Blade Vibrations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lorenzo Schena, Wim Munters, Jan Helsen, Miguel A. Mendez | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This study presents a framework for estimating the full vibrational state of wind turbine blades from sparse deflection measurements. The identification is performed in a reduced-order space obtained from a Proper Orthogonal Decomposition (POD) of high-fidelity aeroelastic simulations based on Geome</span>
            
            <span class="abstract-full" style="display: none;">This study presents a framework for estimating the full vibrational state of wind turbine blades from sparse deflection measurements. The identification is performed in a reduced-order space obtained from a Proper Orthogonal Decomposition (POD) of high-fidelity aeroelastic simulations based on Geometrically Exact Beam Theory (GEBT). In this space, a Reduced Order Model (ROM) is constructed using a linear stochastic estimator, and further enhanced through Kalman fusion with a quasi-steady model of azimuthal dynamics driven by measured wind speed. The performance of the proposed estimator is assessed in a synthetic environment replicating turbulent inflow and measurement noise over a wide range of operating conditions. Results demonstrate the method's ability to accurately reconstruct three-dimensional deformations and accelerations using noisy displacement and acceleration measurements at only four spatial locations. These findings highlight the potential of the proposed framework for real-time blade monitoring, optimal sensor placement, and active load control in wind turbine systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7004
            </span>
            <a href="https://arxiv.org/abs/2504.08421" target="_blank" rel="noopener noreferrer">Poisson multi-Bernoulli mixture filter for trajectory measurements</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Marco Fontana, \'Angel F. Garc\'ia-Fern\'andez, Simon Maskell | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for multi-target filtering based on sensor measurements that are sets of trajectories in the last two-time step window. The proposed filter, the trajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the set of t</span>
            
            <span class="abstract-full" style="display: none;">This paper presents a Poisson multi-Bernoulli mixture (PMBM) filter for multi-target filtering based on sensor measurements that are sets of trajectories in the last two-time step window. The proposed filter, the trajectory measurement PMBM (TM-PMBM) filter, propagates a PMBM density on the set of target states. In prediction, the filter obtains the PMBM density on the set of trajectories over the last two time steps. This density is then updated with the set of trajectory measurements. After the update step, the PMBM posterior on the set of two-step trajectories is marginalised to obtain a PMBM density on the set of target states. The filter provides a closed-form solution for multi-target filtering based on sets of trajectory measurements, estimating the set of target states at the end of each time window. Additionally, the paper proposes computationally lighter alternatives to the TM-PMBM filter by deriving a Poisson multi-Bernoulli (PMB) density through Kullback-Leibler divergence minimisation in an augmented space with auxiliary variables. The performance of the proposed filters are evaluated in a simulation study.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.8 -->
                
            <!-- LLMs: 5.4 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Math: 2.7 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Federated Learning: 2.1 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7384
            </span>
            <a href="https://arxiv.org/abs/2504.08247" target="_blank" rel="noopener noreferrer">Millions of States: Designing a Scalable MoE Architecture with RWKV-7 Meta-learner</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Liu Xiao, Li Zhiyuan, Lin Yueyu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">State-based sequence models like RWKV-7 offer a compelling alternative to Transformer architectures, achieving linear complexity while demonstrating greater expressive power in short-context scenarios and enabling state tracking beyond the \(\text{TC}^0\) complexity class. However, RWKV-7 lacks mech</span>
            
            <span class="abstract-full" style="display: none;">State-based sequence models like RWKV-7 offer a compelling alternative to Transformer architectures, achieving linear complexity while demonstrating greater expressive power in short-context scenarios and enabling state tracking beyond the \(\text{TC}^0\) complexity class. However, RWKV-7 lacks mechanisms for token-parameter interactions and native scalability, limiting its adaptability and growth without retraining. In this paper, we propose \textbf{Meta-State}, a novel extension to RWKV-7 that replaces attention mechanisms with a fully state-driven approach, integrating token-parameter interactions through a \textbf{Self-State Encoder} (SSE) mechanism. The SSE repurposes a portion of the RWKV-7 Weighted Key-Value (WKV) state as transformation weights to encode token-parameter interactions in a linear, state-driven manner without introducing new trainable matrices or softmax operations, while preserving the autoregressive property of token processing. Meta-State supports progressive model scaling by expanding the WKV state and parameter tokens, reusing existing parameters without retraining. Our approach bridges the gap between state-based modeling, token-parameter interactions, and scalable architectures, offering a flexible framework for efficient and adaptable sequence modeling with linear complexity and constant memory usage.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.5 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7456
            </span>
            <a href="https://arxiv.org/abs/2504.08100" target="_blank" rel="noopener noreferrer">ContrastiveGaussian: High-Fidelity 3D Generation with Contrastive Learning and Gaussian Splatting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junbang Liu, Enpei Huang, Dongxing Mao, Hui Zhang, Xinyuan Song, Yongxin Ni | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Creating 3D content from single-view images is a challenging problem that has attracted considerable attention in recent years. Current approaches typically utilize score distillation sampling (SDS) from pre-trained 2D diffusion models to generate multi-view 3D representations. Although some methods</span>
            
            <span class="abstract-full" style="display: none;">Creating 3D content from single-view images is a challenging problem that has attracted considerable attention in recent years. Current approaches typically utilize score distillation sampling (SDS) from pre-trained 2D diffusion models to generate multi-view 3D representations. Although some methods have made notable progress by balancing generation speed and model quality, their performance is often limited by the visual inconsistencies of the diffusion model outputs. In this work, we propose ContrastiveGaussian, which integrates contrastive learning into the generative process. By using a perceptual loss, we effectively differentiate between positive and negative samples, leveraging the visual inconsistencies to improve 3D generation quality. To further enhance sample differentiation and improve contrastive learning, we incorporate a super-resolution model and introduce another Quantity-Aware Triplet Loss to address varying sample distributions during training. Our experiments demonstrate that our approach achieves superior texture fidelity and improved geometric consistency.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.7 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Math: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7521
            </span>
            <a href="https://arxiv.org/abs/2504.08019" target="_blank" rel="noopener noreferrer">DGFamba: Learning Flow Factorized State Space for Visual Domain Generalization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qi Bi, Jingjun Yi, Hao Zheng, Haolan Zhan, Wei Ji, Yawen Huang, Yuexiang Li | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Domain generalization aims to learn a representation from the source domain, which can be generalized to arbitrary unseen target domains. A fundamental challenge for visual domain generalization is the domain gap caused by the dramatic style variation whereas the image content is stable. The realm o</span>
            
            <span class="abstract-full" style="display: none;">Domain generalization aims to learn a representation from the source domain, which can be generalized to arbitrary unseen target domains. A fundamental challenge for visual domain generalization is the domain gap caused by the dramatic style variation whereas the image content is stable. The realm of selective state space, exemplified by VMamba, demonstrates its global receptive field in representing the content. However, the way exploiting the domain-invariant property for selective state space is rarely explored. In this paper, we propose a novel Flow Factorized State Space model, dubbed as DG-Famba, for visual domain generalization. To maintain domain consistency, we innovatively map the style-augmented and the original state embeddings by flow factorization. In this latent flow space, each state embedding from a certain style is specified by a latent probability path. By aligning these probability paths in the latent space, the state embeddings are able to represent the same content distribution regardless of the style differences. Extensive experiments conducted on various visual domain generalization settings show its state-of-the-art performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 6.4 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7669
            </span>
            <a href="https://arxiv.org/abs/2405.15172" target="_blank" rel="noopener noreferrer">Learning the Distribution Map in Reverse Causal Performative Prediction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Daniele Bracale, Subha Maity, Moulinath Banerjee, Yuekai Sun | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In numerous predictive scenarios, the predictive model affects the sampling distribution; for example, job applicants often meticulously craft their resumes to navigate through a screening systems. Such shifts in distribution are particularly prevalent in the realm of social computing, yet, the stra</span>
            
            <span class="abstract-full" style="display: none;">In numerous predictive scenarios, the predictive model affects the sampling distribution; for example, job applicants often meticulously craft their resumes to navigate through a screening systems. Such shifts in distribution are particularly prevalent in the realm of social computing, yet, the strategies to learn these shifts from data remain remarkably limited. Inspired by a microeconomic model that adeptly characterizes agents' behavior within labor markets, we introduce a novel approach to learn the distribution shift. Our method is predicated on a reverse causal model, wherein the predictive model instigates a distribution shift exclusively through a finite set of agents' actions. Within this framework, we employ a microfoundation model for the agents' actions and develop a statistically justified methodology to learn the distribution shift map, which we demonstrate to be effective in minimizing the performative prediction risk.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.7703
            </span>
            <a href="https://arxiv.org/abs/2503.19163" target="_blank" rel="noopener noreferrer">Insights into the explainability of Lasso-based DeePC for nonlinear systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Gianluca Giacomelli, Simone Formentin, Victor G. Lopez, Matthias A. M\"uller, Valentina Breschi | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Data-enabled Predictive Control (DeePC) has recently gained the spotlight as an easy-to-use control technique that allows for constraint handling while relying on raw data only. Initially proposed for linear time-invariant systems, several DeePC extensions are now available to cope with nonlinear sy</span>
            
            <span class="abstract-full" style="display: none;">Data-enabled Predictive Control (DeePC) has recently gained the spotlight as an easy-to-use control technique that allows for constraint handling while relying on raw data only. Initially proposed for linear time-invariant systems, several DeePC extensions are now available to cope with nonlinear systems. Nonetheless, these solutions mainly focus on ensuring the controller's effectiveness, overlooking the explainability of the final result. As a step toward explaining the outcome of DeePC for the control of nonlinear systems, in this paper, we focus on analyzing the earliest and simplest DeePC approach proposed to cope with nonlinearities in the controlled system, using a Lasso regularization. Our theoretical analysis highlights that the decisions undertaken by DeePC with Lasso regularization are unexplainable, as control actions are determined by data incoherent with the system's local behavior. This result is true even when the available input/output samples are grouped according to the different operating conditions explored during data collection. Our numerical study confirms these findings, highlighting the benefits of data grouping in terms of performance while showing that explainability remains a challenge in control design via DeePC.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.8 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8069
            </span>
            <a href="https://arxiv.org/abs/2504.08626" target="_blank" rel="noopener noreferrer">Task-conditioned Ensemble of Expert Models for Continuous Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Renu Sharma, Debasmita Pal, Arun Ross | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed mode</span>
            
            <span class="abstract-full" style="display: none;">One of the major challenges in machine learning is maintaining the accuracy of the deployed model (e.g., a classifier) in a non-stationary environment. The non-stationary environment results in distribution shifts and, consequently, a degradation in accuracy. Continuous learning of the deployed model with new data could be one remedy. However, the question arises as to how we should update the model with new training data so that it retains its accuracy on the old data while adapting to the new data. In this work, we propose a task-conditioned ensemble of models to maintain the performance of the existing model. The method involves an ensemble of expert models based on task membership information. The in-domain models-based on the local outlier concept (different from the expert models) provide task membership information dynamically at run-time to each probe sample. To evaluate the proposed method, we experiment with three setups: the first represents distribution shift between tasks (LivDet-Iris-2017), the second represents distribution shift both between and within tasks (LivDet-Iris-2020), and the third represents disjoint distribution between tasks (Split MNIST). The experiments highlight the benefits of the proposed method. The source code is available at https://github.com/iPRoBe-lab/Continuous_Learning_FE_DM.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.5 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.843
            </span>
            <a href="https://arxiv.org/abs/2504.05963" target="_blank" rel="noopener noreferrer">Learning Verified Monitors for Hidden Markov Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Luko van der Maas, Sebastian Junges | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Runtime monitors assess whether a system is in an unsafe state based on a stream of observations. We study the problem where the system is subject to probabilistic uncertainty and described by a hidden Markov model. A stream of observations is then unsafe if the probability of being in an unsafe sta</span>
            
            <span class="abstract-full" style="display: none;">Runtime monitors assess whether a system is in an unsafe state based on a stream of observations. We study the problem where the system is subject to probabilistic uncertainty and described by a hidden Markov model. A stream of observations is then unsafe if the probability of being in an unsafe state is above a threshold. A correct monitor recognizes the set of unsafe observations. The key contribution of this paper is the first correct-by-construction synthesis method for such monitors, represented as finite automata. The contribution combines four ingredients: First, we establish the coNP-hardness of checking whether an automaton is a correct monitor, i.e., a monitor without misclassifications. Second, we provide a reduction that reformulates the search for misclassifications into a standard probabilistic system synthesis problem. Third, we integrate the verification routine into an active automata learning routine to synthesize correct monitors. Fourth, we provide a prototypical implementation that shows the feasibility and limitations of the approach on a series of benchmarks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.3 -->
                
            <!-- LLMs: 7.7 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8488
            </span>
            <a href="https://arxiv.org/abs/2504.08148" target="_blank" rel="noopener noreferrer">Orchestrating Agents and Data for Enterprise: A Blueprint Architecture for Compound AI</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Eser Kandogan, Nikita Bhutani, Dan Zhang, Rafael Li Chen, Sairam Gurajada, Estevam Hruschka | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have gained significant interest in industry due to their impressive capabilities across a wide range of tasks. However, the widespread adoption of LLMs presents several challenges, such as integration into existing applications and infrastructure, utilization of company</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have gained significant interest in industry due to their impressive capabilities across a wide range of tasks. However, the widespread adoption of LLMs presents several challenges, such as integration into existing applications and infrastructure, utilization of company proprietary data, models, and APIs, and meeting cost, quality, responsiveness, and other requirements. To address these challenges, there is a notable shift from monolithic models to compound AI systems, with the premise of more powerful, versatile, and reliable applications. However, progress thus far has been piecemeal, with proposals for agentic workflows, programming models, and extended LLM capabilities, without a clear vision of an overall architecture. In this paper, we propose a 'blueprint architecture' for compound AI systems for orchestrating agents and data for enterprise applications. In our proposed architecture the key orchestration concept is 'streams' to coordinate the flow of data and instructions among agents. Existing proprietary models and APIs in the enterprise are mapped to 'agents', defined in an 'agent registry' that serves agent metadata and learned representations for search and planning. Agents can utilize proprietary data through a 'data registry' that similarly registers enterprise data of various modalities. Tying it all together, data and task 'planners' break down, map, and optimize tasks and queries for given quality of service (QoS) requirements such as cost, accuracy, and latency. We illustrate an implementation of the architecture for a use-case in the HR domain and discuss opportunities and challenges for 'agentic AI' in the enterprise.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.8 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.857
            </span>
            <a href="https://arxiv.org/abs/2504.08639" target="_blank" rel="noopener noreferrer">Constructing Witnesses for Lower Bounds on Behavioural Distances</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ruben Turkenburg, Harsh Beohar, Franck van Breugel, Clemens Kupke, Jurriaan Rot | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Apartness of states in transition systems has seen growing interest recently as an inductive counterpart to many well-established bisimilarity notions. The constructive nature of apartness allows the definition of derivation systems for reasoning about apartness of states. It further corresponds clo</span>
            
            <span class="abstract-full" style="display: none;">Apartness of states in transition systems has seen growing interest recently as an inductive counterpart to many well-established bisimilarity notions. The constructive nature of apartness allows the definition of derivation systems for reasoning about apartness of states. It further corresponds closely to distinguishing formulas in suitable modal logics. Both the derivations and distinguishing formulas provide (finite) evidence for differences in the behaviour of states. The current paper provides a derivation system in the setting of behavioural distances on labelled Markov chains. Rather than showing pairs of states apart, the system allows the derivation of lower bounds on their distance, complementing existing work giving upper bounds. We further show the soundness and (approximate) completeness of the system with respect to the behavioural distance. We go on to give a constructive correspondence between our derivation system and formulas in a modal logic with quantitative semantics. This logic was used in recent work of Rady and van Breugel to construct evidence for lower bounds on behavioural distances. Our constructions will provide smaller witnessing formulas in many examples.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.2 -->
                
            <!-- LLMs: 8.1 -->
                
            <!-- Quantum Computing: 5.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8977
            </span>
            <a href="https://arxiv.org/abs/2411.06787" target="_blank" rel="noopener noreferrer">A System Parametrization for Direct Data-Driven Analysis and Control with Error-in-Variables</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Felix Br\"andle, Frank Allg\"ower | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we present a new parametrization to perform direct data-driven analysis and controller synthesis for the error-in-variables case. To achieve this, we employ the Sherman-Morrison-Woodbury formula to transform the problem into a linear fractional transformation (LFT) with unknown measur</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we present a new parametrization to perform direct data-driven analysis and controller synthesis for the error-in-variables case. To achieve this, we employ the Sherman-Morrison-Woodbury formula to transform the problem into a linear fractional transformation (LFT) with unknown measurement errors and disturbances as uncertainties. For bounded uncertainties, we apply robust control techniques to derive a guaranteed upper bound on the H2-norm of the unknown true system. To this end, a single semidefinite program (SDP) needs to be solved, with complexity that is independent of the amount of data. Furthermore, we exploit the signal-to-noise ratio to provide a data-dependent condition, that characterizes whether the proposed parametrization can be employed. The modular formulation allows to extend this framework to controller synthesis with different performance criteria, input-output settings, and various system properties. Finally, we validate the proposed approach through a numerical example.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 6.8 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.8989
            </span>
            <a href="https://arxiv.org/abs/2407.15317" target="_blank" rel="noopener noreferrer">Open-CD: A Comprehensive Toolbox for Change Detection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kaiyu Li, Jiawei Jiang, Andrea Codegoni, Chengxi Han, Yupeng Deng, Keyan Chen, Zhuo Zheng, Hao Chen, Ziyuan Liu, Yuantao Gu, Zhengxia Zou, Zhenwei Shi, Sheng Fang, Deyu Meng, Zhi Wang, Xiangyong Cao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present Open-CD, a change detection toolbox that contains a rich set of change detection methods as well as related components and modules. The toolbox started from a series of open source general vision task tools, including OpenMMLab Toolkits, PyTorch Image Models, etc. It gradually evolves int</span>
            
            <span class="abstract-full" style="display: none;">We present Open-CD, a change detection toolbox that contains a rich set of change detection methods as well as related components and modules. The toolbox started from a series of open source general vision task tools, including OpenMMLab Toolkits, PyTorch Image Models, etc. It gradually evolves into a unified platform that covers many popular change detection methods and contemporary modules. It not only includes training and inference codes, but also provides some useful scripts for data analysis. We believe this toolbox is by far the most complete change detection toolbox. In this report, we introduce the various features, supported methods and applications of Open-CD. In addition, we also conduct a benchmarking study on different methods and components. We wish that the toolbox and benchmark could serve the growing research community by providing a flexible toolkit to reimplement existing methods and develop their own new change detectors. Code and models are available at https://github.com/likyoo/open-cd. Pioneeringly, this report also includes brief descriptions of the algorithms supported in Open-CD, mainly contributed by their authors. We sincerely encourage researchers in this field to participate in this project and work together to create a more open community. This toolkit and report will be kept updated.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9317
            </span>
            <a href="https://arxiv.org/abs/2503.19609" target="_blank" rel="noopener noreferrer">Nanopass Back-Translation of Call-Return Trees for Mechanized Secure Compilation Proofs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: J\'er\'emy Thibault, Joseph Lenormand, Catalin Hritcu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Researchers aim to build secure compilation chains enforcing that if there is no attack a source context can mount against a source program then there is also no attack an adversarial target context can mount against the compiled program. Proving that these compilation chains are secure is, however,</span>
            
            <span class="abstract-full" style="display: none;">Researchers aim to build secure compilation chains enforcing that if there is no attack a source context can mount against a source program then there is also no attack an adversarial target context can mount against the compiled program. Proving that these compilation chains are secure is, however, challenging, and involves a non-trivial back-translation step: for any attack a target context mounts against the compiled program one has to exhibit a source context mounting the same attack against the source program. We describe a novel back-translation technique, which results in simpler proofs that can be more easily mechanized in a proof assistant. Given a finite set of finite trace prefixes, capturing the interaction recorded during an attack between a target context and the compiled program, we build a call-return tree that we back-translate into a source context producing the same trace prefixes. We use state in the generated source context to record the current location in the call-return tree. The back-translation is done in several small steps, each adding to the tree new information describing how the location should change depending on how the context regains control. To prove this back-translation correct we give semantics to every intermediate call-return tree language, using ghost state to store information and explicitly enforce execution invariants. We prove several small forward simulations, basically seeing the back-translation as a verified nanopass compiler. Thanks to this modular structure, we are able to mechanize this complex back-translation and its correctness proof in the Rocq prover without too much effort.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9337
            </span>
            <a href="https://arxiv.org/abs/2504.08703" target="_blank" rel="noopener noreferrer">SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Muhammad Shihab Rashid, Christian Bock, Yuan Zhuang, Alexander Buccholz, Tim Esler, Simon Valentin, Luca Franceschi, Martin Wistuba, Prabhu Teja Sivaprasad, Woo Jung Kim, Anoop Deoras, Giovanni Zappella, Laurent Callot | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for reposito</span>
            
            <span class="abstract-full" style="display: none;">Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at: https://github.com/amazon-science/SWE-PolyBench</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.8 -->
                
            <!-- Medicine: 6.8 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9352
            </span>
            <a href="https://arxiv.org/abs/2409.14111" target="_blank" rel="noopener noreferrer">Quantum Data Management in the NISQ Era: Extended Version</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rihan Hai, Shih-Han Hung, Tim Coopmans, Tim Littau, Floris Geerts | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quantum computing has emerged as a promising tool for transforming the landscape of computing technology. Recent efforts have applied quantum techniques to classical database challenges, such as query optimization, data integration, index selection, and transaction management. In this paper, we shif</span>
            
            <span class="abstract-full" style="display: none;">Quantum computing has emerged as a promising tool for transforming the landscape of computing technology. Recent efforts have applied quantum techniques to classical database challenges, such as query optimization, data integration, index selection, and transaction management. In this paper, we shift focus to a critical yet underexplored area: data management for quantum computing. We are currently in the noisy intermediate-scale quantum (NISQ) era, where qubits, while promising, are fragile and still limited in scale. After differentiating quantum data from classical data, we outline current and future data management paradigms in the NISQ era and beyond. We address the data management challenges arising from the emerging demands of near-term quantum computing. Our goal is to chart a clear course for future quantum-oriented data management research, establishing it as a cornerstone for the advancement of quantum computing in the NISQ era.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.5 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 5.3 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9567
            </span>
            <a href="https://arxiv.org/abs/2407.14367" target="_blank" rel="noopener noreferrer">Thinking Racial Bias in Fair Forgery Detection: Models, Datasets and Evaluations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Decheng Liu, Zongqi Wang, Chunlei Peng, Nannan Wang, Ruimin Hu, Xinbo Gao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Due to the successful development of deep image generation technology, forgery detection plays a more important role in social and economic security. Racial bias has not been explored thoroughly in the deep forgery detection field. In the paper, we first contribute a dedicated dataset called the Fai</span>
            
            <span class="abstract-full" style="display: none;">Due to the successful development of deep image generation technology, forgery detection plays a more important role in social and economic security. Racial bias has not been explored thoroughly in the deep forgery detection field. In the paper, we first contribute a dedicated dataset called the Fair Forgery Detection (FairFD) dataset, where we prove the racial bias of public state-of-the-art (SOTA) methods. Different from existing forgery detection datasets, the self-constructed FairFD dataset contains a balanced racial ratio and diverse forgery generation images with the largest-scale subjects. Additionally, we identify the problems with naive fairness metrics when benchmarking forgery detection models. To comprehensively evaluate fairness, we design novel metrics including Approach Averaged Metric and Utility Regularized Metric, which can avoid deceptive results. We also present an effective and robust post-processing technique, Bias Pruning with Fair Activations (BPFA), which improves fairness without requiring retraining or weight updates. Extensive experiments conducted with 12 representative forgery detection models demonstrate the value of the proposed dataset and the reasonability of the designed fairness metrics. By applying the BPFA to the existing fairest detector, we achieve a new SOTA. Furthermore, we conduct more in-depth analyses to offer more insights to inspire researchers in the community.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 6.8 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9809
            </span>
            <a href="https://arxiv.org/abs/2407.18271" target="_blank" rel="noopener noreferrer">Large Language Model for Verilog Generation with Code-Structure-Guided Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ning Wang, Bingkun Yao, Jie Zhou, Xi Wang, Zhe Jiang, Nan Guan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent advancements in large language models (LLMs) have sparked significant interest in the automatic generation of Register Transfer Level (RTL) designs, particularly using Verilog. Current research on this topic primarily focuses on pre-training and instruction tuning, but the effectiveness of th</span>
            
            <span class="abstract-full" style="display: none;">Recent advancements in large language models (LLMs) have sparked significant interest in the automatic generation of Register Transfer Level (RTL) designs, particularly using Verilog. Current research on this topic primarily focuses on pre-training and instruction tuning, but the effectiveness of these methods is constrained by the limited availability of training data, as public Verilog code is far less abundant than software code. In particular, these methods struggle to effectively capture Verilog parallel code structures, which fundamentally differ from the imperative, sequential control flow typical in most software programming languages. This paper introduces VeriSeek, an LLM enhanced by reinforcement learning using a limited amount of high-quality training data to achieve high Verilog code generation performance. Our reinforcement learning approach employs code structure information as feedback signals to refine the pre-trained model, enabling it to effectively learn important patterns from Verilog code with parallel structures. Experiments show that VeriSeek outperforms state-of-the-art methods across multiple benchmarks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.4 -->
                
            <!-- Medicine: 6.7 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -5.9859
            </span>
            <a href="https://arxiv.org/abs/2402.16170" target="_blank" rel="noopener noreferrer">Nonparametric Steady-state Learning for Robust Output Regulation of Nonlinear Output Feedback Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shimin Wang, Martin Guay, Richard D. Braatz | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This article addresses the nonadaptive and robust output regulation problem of the general nonlinear output feedback system with error output. The global robust output regulation problem for a class of general output feedback nonlinear systems with an uncertain exosystem and high relative degree can</span>
            
            <span class="abstract-full" style="display: none;">This article addresses the nonadaptive and robust output regulation problem of the general nonlinear output feedback system with error output. The global robust output regulation problem for a class of general output feedback nonlinear systems with an uncertain exosystem and high relative degree can be tackled by constructing a linear generic internal model provided that a continuous nonlinear mapping exists. Leveraging the presented nonadaptive framework facilitates the conversion of the nonlinear robust output regulation problem into a robust nonadaptive stabilization endeavour for the augmented system endowed with Input-to-State Stable dynamics, removing the need for constructing a specific Lyapunov function with positive semidefinite derivatives and the commmonly employed assumption that the nonlinear system should be linear-in-parameter(parameterized) condition. The nonadaptive approach is extended by incorporating the nonparametric learning framework to ensure the feasibility of the nonlinear mapping, which can be classified into a data-driven method. Moreover, the introduced nonparametric learning framework allows the controlled system to learn the dynamics of the steady-state/input behaviour from the signal generated from the internal model with the output error as the feedback. As a result, the nonadaptive/nonparametric approach can be advantageous by guaranteeing convergence of the estimation and tracking error even when the underlying controlled system dynamics are complex or poorly understood. The effectiveness of the theoretical results is illustrated for a benchmark example: a controlled duffing system and two practical examples: a continuously stirred tank reactor and a continuous bioreactor.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.7 -->
                
            <!-- LLMs: 5.3 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Math: 2.8 -->
                
            <!-- Reinforcement Learning: 2.4 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0019
            </span>
            <a href="https://arxiv.org/abs/2504.07994" target="_blank" rel="noopener noreferrer">Evaluating the Fitness of Ontologies for the Task of Question Generation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Samah Alkhuzaey, Floriana Grasso, Terry R. Payne, Valentina Tamma | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Ontology-based question generation is an important application of semantic-aware systems that enables the creation of large question banks for diverse learning environments. The effectiveness of these systems, both in terms of the calibre and cognitive difficulty of the resulting questions, depends </span>
            
            <span class="abstract-full" style="display: none;">Ontology-based question generation is an important application of semantic-aware systems that enables the creation of large question banks for diverse learning environments. The effectiveness of these systems, both in terms of the calibre and cognitive difficulty of the resulting questions, depends heavily on the quality and modelling approach of the underlying ontologies, making it crucial to assess their fitness for this task. To date, there has been no comprehensive investigation into the specific ontology aspects or characteristics that affect the question generation process. Therefore, this paper proposes a set of requirements and task-specific metrics for evaluating the fitness of ontologies for question generation tasks in pedagogical settings. Using the ROMEO methodology, a structured framework for deriving task-specific metrics, an expert-based approach is employed to assess the performance of various ontologies in Automatic Question Generation (AQG) tasks, which is then evaluated over a set of ontologies. Our results demonstrate that ontology characteristics significantly impact the effectiveness of question generation, with different ontologies exhibiting varying performance levels. This highlights the importance of assessing ontology quality with respect to AQG tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.7 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Quantum Computing: 5.0 -->
                
            <!-- Math: 2.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0104
            </span>
            <a href="https://arxiv.org/abs/2504.08550" target="_blank" rel="noopener noreferrer">Proxy-Anchor and EVT-Driven Continual Learning Method for Generalized Category Discovery</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alireza Fathalizadeh, Roozbeh Razavi-Far | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Continual generalized category discovery has been introduced and studied in the literature as a method that aims to continuously discover and learn novel categories in incoming data batches while avoiding catastrophic forgetting of previously learned categories. A key component in addressing this ch</span>
            
            <span class="abstract-full" style="display: none;">Continual generalized category discovery has been introduced and studied in the literature as a method that aims to continuously discover and learn novel categories in incoming data batches while avoiding catastrophic forgetting of previously learned categories. A key component in addressing this challenge is the model's ability to separate novel samples, where Extreme Value Theory (EVT) has been effectively employed. In this work, we propose a novel method that integrates EVT with proxy anchors to define boundaries around proxies using a probability of inclusion function, enabling the rejection of unknown samples. Additionally, we introduce a novel EVT-based loss function to enhance the learned representation, achieving superior performance compared to other deep-metric learning methods in similar settings. Using the derived probability functions, novel samples are effectively separated from previously known categories. However, category discovery within these novel samples can sometimes overestimate the number of new categories. To mitigate this issue, we propose a novel EVT-based approach to reduce the model size and discard redundant proxies. We also incorporate experience replay and knowledge distillation mechanisms during the continual learning stage to prevent catastrophic forgetting. Experimental results demonstrate that our proposed approach outperforms state-of-the-art methods in continual generalized category discovery scenarios.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0126
            </span>
            <a href="https://arxiv.org/abs/2504.08254" target="_blank" rel="noopener noreferrer">Understanding the Impact of Data Domain Extraction on Synthetic Data Privacy</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Georgi Ganev, Meenatchi Sundaram Muthu Selva Annamalai, Sofiane Mahiou, Emiliano De Cristofaro | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Privacy attacks, particularly membership inference attacks (MIAs), are widely used to assess the privacy of generative models for tabular synthetic data, including those with Differential Privacy (DP) guarantees. These attacks often exploit outliers, which are especially vulnerable due to their posi</span>
            
            <span class="abstract-full" style="display: none;">Privacy attacks, particularly membership inference attacks (MIAs), are widely used to assess the privacy of generative models for tabular synthetic data, including those with Differential Privacy (DP) guarantees. These attacks often exploit outliers, which are especially vulnerable due to their position at the boundaries of the data domain (e.g., at the minimum and maximum values). However, the role of data domain extraction in generative models and its impact on privacy attacks have been overlooked. In this paper, we examine three strategies for defining the data domain: assuming it is externally provided (ideally from public data), extracting it directly from the input data, and extracting it with DP mechanisms. While common in popular implementations and libraries, we show that the second approach breaks end-to-end DP guarantees and leaves models vulnerable. While using a provided domain (if representative) is preferable, extracting it with DP can also defend against popular MIAs, even at high privacy budgets.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.027
            </span>
            <a href="https://arxiv.org/abs/2504.08249" target="_blank" rel="noopener noreferrer">Neural Network-assisted Interval Reachability for Systems with Control Barrier Function-Based Safe Controllers</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Damola Ajeyemi, Saber Jafarpour, Emiliano Dall'Anese | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Control Barrier Functions (CBFs) have been widely utilized in the design of optimization-based controllers and filters for dynamical systems to ensure forward invariance of a given set of safe states. While CBF-based controllers offer safety guarantees, they can compromise the performance of the sys</span>
            
            <span class="abstract-full" style="display: none;">Control Barrier Functions (CBFs) have been widely utilized in the design of optimization-based controllers and filters for dynamical systems to ensure forward invariance of a given set of safe states. While CBF-based controllers offer safety guarantees, they can compromise the performance of the system, leading to undesirable behaviors such as unbounded trajectories and emergence of locally stable spurious equilibria. Computing reachable sets for systems with CBF-based controllers is an effective approach for runtime performance and stability verification, and can potentially serve as a tool for trajectory re-planning. In this paper, we propose a computationally efficient interval reachability method for performance verification of systems with optimization-based controllers by: (i) approximating the optimization-based controller by a pre-trained neural network to avoid solving optimization problems repeatedly, and (ii) using mixed monotone theory to construct an embedding system that leverages state-of-the-art neural network verification algorithms for bounding the output of the neural network. Results in terms of closeness of solutions of trajectories of the system with the optimization-based controller and the neural network are derived. Using a single trajectory of the embedding system along with our closeness of solutions result, we obtain an over-approximation of the reachable set of the system with optimization-based controllers. Numerical results are presented to corroborate the technical findings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.2 -->
                
            <!-- Medicine: 6.8 -->
                
            <!-- Quantum Computing: 3.0 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0339
            </span>
            <a href="https://arxiv.org/abs/2504.02281" target="_blank" rel="noopener noreferrer">Parallel Market Environments for FinRL Contests</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Keyi Wang, Kairong Xiao, Xiao-Yang Liu Yanglet | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Financial reinforcement learning has attracted lots of attention recently. From 2023 to 2025, we have organized three FinRL Contests featuring different financial tasks. Large language models have a strong capability to process financial documents. By integrating LLM-generated signals into the state</span>
            
            <span class="abstract-full" style="display: none;">Financial reinforcement learning has attracted lots of attention recently. From 2023 to 2025, we have organized three FinRL Contests featuring different financial tasks. Large language models have a strong capability to process financial documents. By integrating LLM-generated signals into the state, trading agents can take smarter actions based on both structured market data and unstructured financial documents. In this paper, we summarize the parallel market environments for tasks used in FinRL Contests 2023-2025. To address the sampling bottleneck during training, we introduce GPU-optimized parallel market environments to address the sampling bottleneck. In particular, two new tasks incorporate LLM-generated signals and all tasks support massively parallel simulation. Contestants have used these market environments to train robust and powerful trading agents for both stock and cryptocurrency trading tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.5 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.2 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0506
            </span>
            <a href="https://arxiv.org/abs/2503.11217" target="_blank" rel="noopener noreferrer">Deep Joint Distribution Optimal Transport for Universal Domain Adaptation on Time Series</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Romain Mussard, Fannia Pacheco, Maxime Berar, Gilles Gasso, Paul Honeine | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Universal Domain Adaptation (UniDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain, even when their classes are not fully shared. Few dedicated UniDA methods exist for Time Series (TS), which remains a challenging case. In general, UniDA approaches align common</span>
            
            <span class="abstract-full" style="display: none;">Universal Domain Adaptation (UniDA) aims to transfer knowledge from a labeled source domain to an unlabeled target domain, even when their classes are not fully shared. Few dedicated UniDA methods exist for Time Series (TS), which remains a challenging case. In general, UniDA approaches align common class samples and detect unknown target samples from emerging classes. Such detection often results from thresholding a discriminability metric. The threshold value is typically either a fine-tuned hyperparameter or a fixed value, which limits the ability of the model to adapt to new data. Furthermore, discriminability metrics exhibit overconfidence for unknown samples, leading to misclassifications. This paper introduces UniJDOT, an optimal-transport-based method that accounts for the unknown target samples in the transport cost. Our method also proposes a joint decision space to improve the discriminability of the detection module. In addition, we use an auto-thresholding algorithm to reduce the dependence on fixed or fine-tuned thresholds. Finally, we rely on a Fourier transform-based layer inspired by the Fourier Neural Operator for better TS representation. Experiments on TS benchmarks demonstrate the discriminability, robustness, and state-of-the-art performance of UniJDOT.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.9 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.0602
            </span>
            <a href="https://arxiv.org/abs/2504.08208" target="_blank" rel="noopener noreferrer">How Good Are Large Language Models for Course Recommendation in MOOCs?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Boxuan Ma, Md Akib Zabed Khan, Tianyuan Yang, Agoritsa Polyzou, Shin'ichi Konomi | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have made significant strides in natural language processing and are increasingly being integrated into recommendation systems. However, their potential in educational recommendation systems has yet to be fully explored. This paper investigates the use of LLMs as a gener</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have made significant strides in natural language processing and are increasingly being integrated into recommendation systems. However, their potential in educational recommendation systems has yet to be fully explored. This paper investigates the use of LLMs as a general-purpose recommendation model, leveraging their vast knowledge derived from large-scale corpora for course recommendation tasks. We explore a variety of approaches, ranging from prompt-based methods to more advanced fine-tuning techniques, and compare their performance against traditional recommendation models. Extensive experiments were conducted on a real-world MOOC dataset, evaluating using LLMs as course recommendation systems across key dimensions such as accuracy, diversity, and novelty. Our results demonstrate that LLMs can achieve good performance comparable to traditional models, highlighting their potential to enhance educational recommendation systems. These findings pave the way for further exploration and development of LLM-based approaches in the context of educational recommendations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 17.4 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1007
            </span>
            <a href="https://arxiv.org/abs/2411.10193" target="_blank" rel="noopener noreferrer">DiMoDif: Discourse Modality-information Differentiation for Audio-visual Deepfake Detection and Localization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Christos Koutlis, Symeon Papadopoulos | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Deepfake technology has rapidly advanced and poses significant threats to information integrity and trust in online multimedia. While significant progress has been made in detecting deepfakes, the simultaneous manipulation of audio and visual modalities, sometimes at small parts or in subtle ways, p</span>
            
            <span class="abstract-full" style="display: none;">Deepfake technology has rapidly advanced and poses significant threats to information integrity and trust in online multimedia. While significant progress has been made in detecting deepfakes, the simultaneous manipulation of audio and visual modalities, sometimes at small parts or in subtle ways, presents highly challenging detection scenarios. To address these challenges, we present DiMoDif, an audio-visual deepfake detection framework that leverages the inter-modality differences in machine perception of speech, based on the assumption that in real samples -- in contrast to deepfakes -- visual and audio signals coincide in terms of information. DiMoDif leverages features from deep networks that specialize in visual and audio speech recognition to spot frame-level cross-modal incongruities, and in that way to temporally localize the deepfake forgery. To this end, we devise a hierarchical cross-modal fusion network, integrating adaptive temporal alignment modules and a learned discrepancy mapping layer to explicitly model the subtle differences between visual and audio representations. Then, the detection model is optimized through a composite loss function accounting for frame-level detections and fake intervals localization. DiMoDif outperforms the state-of-the-art on the Deepfake Detection task by 30.5 AUC on the highly challenging AV-Deepfake1M, while it performs exceptionally on FakeAVCeleb and LAV-DF. On the Temporal Forgery Localization task, it outperforms the state-of-the-art by 47.88 AP@0.75 on AV-Deepfake1M, and performs on-par on LAV-DF. Code available at https://github.com/mever-team/dimodif.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.7 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1021
            </span>
            <a href="https://arxiv.org/abs/2501.07601" target="_blank" rel="noopener noreferrer">Real-Time Decision-Making for Digital Twin in Additive Manufacturing with Model Predictive Control using Time-Series Deep Neural Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yi-Ping Chen, Vispi Karkaria, Ying-Kuan Tsai, Faith Rolark, Daniel Quispe, Robert X. Gao, Jian Cao, Wei Chen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Digital Twin -- a virtual replica of a physical system enabling real-time monitoring, model updating, prediction, and decision-making -- combined with recent advances in machine learning, offers new opportunities for proactive control strategies in autonomous manufacturing. However, achieving real-t</span>
            
            <span class="abstract-full" style="display: none;">Digital Twin -- a virtual replica of a physical system enabling real-time monitoring, model updating, prediction, and decision-making -- combined with recent advances in machine learning, offers new opportunities for proactive control strategies in autonomous manufacturing. However, achieving real-time decision-making with Digital Twins requires efficient optimization driven by accurate predictions of highly nonlinear manufacturing systems. This paper presents a simultaneous multi-step Model Predictive Control (MPC) framework for real-time decision-making, using a multivariate deep neural network, named Time-Series Dense Encoder (TiDE), as the surrogate model. Unlike conventional MPC models which only provide one-step ahead prediction, TiDE is capable of predicting future states within the prediction horizon in one shot (multi-step), significantly accelerating the MPC. Using Directed Energy Deposition (DED) additive manufacturing as a case study, we demonstrate the effectiveness of the proposed MPC in achieving melt pool temperature tracking to ensure part quality, while reducing porosity defects by regulating laser power to maintain melt pool depth constraints. In this work, we first show that TiDE is capable of accurately predicting melt pool temperature and depth. Second, we demonstrate that the proposed MPC achieves precise temperature tracking while satisfying melt pool depth constraints within a targeted dilution range (10\%-30\%), reducing potential porosity defects. Compared to PID controller, the MPC results in smoother and less fluctuating laser power profiles with competitive or superior melt pool temperature control performance. This demonstrates the MPC's proactive control capabilities, leveraging time-series prediction and real-time optimization, positioning it as a powerful tool for future Digital Twin applications and real-time process optimization in manufacturing.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1309
            </span>
            <a href="https://arxiv.org/abs/2411.00928" target="_blank" rel="noopener noreferrer">A Bregman firmly nonexpansive proximal operator for baryconvex optimization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mastane Achab | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a generalization of the proximal operator defined through a convex combination of convex objectives, where the coefficients are updated in a minimax fashion. We prove that this new operator is Bregman firmly nonexpansive with respect to a Bregman divergence that combines Euclidean and inf</span>
            
            <span class="abstract-full" style="display: none;">We present a generalization of the proximal operator defined through a convex combination of convex objectives, where the coefficients are updated in a minimax fashion. We prove that this new operator is Bregman firmly nonexpansive with respect to a Bregman divergence that combines Euclidean and information geometries; and that its fixed points are given by the critical points of a certain nonconvex function. Finally, we derive the associated continuous flows.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 2.0 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1438
            </span>
            <a href="https://arxiv.org/abs/2504.08698" target="_blank" rel="noopener noreferrer">Performance Evaluation of Trajectory Tracking Controllers for a Quadruped Robot Leg</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hossein Shojaei, Hamid Rahmanei, Seyed Hossein Sadati | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The complexities in the dynamic model of the legged robots make it necessary to utilize model-free controllers in the task of trajectory tracking. In This paper, an adaptive transpose Jacobian approach is proposed to deal with the dynamic model complexity, which utilizes an adaptive PI-algorithm to </span>
            
            <span class="abstract-full" style="display: none;">The complexities in the dynamic model of the legged robots make it necessary to utilize model-free controllers in the task of trajectory tracking. In This paper, an adaptive transpose Jacobian approach is proposed to deal with the dynamic model complexity, which utilizes an adaptive PI-algorithm to adjust the control gains. The performance of the proposed control algorithm is compared with the conventional transpose Jacobian and sliding mode control algorithms and evaluated by the root mean square of the errors and control input energy criteria. In order to appraise the effectiveness of the proposed control system, simulations are carried out in MATLAB/Simulink software for a quadruped robot leg for semi-elliptical path tracking. The obtained results show that the proposed adaptive transpose Jacobian reduces the overshoot and root mean square of the errors and at the same time, decreases the control input energy. Moreover, transpose Jacobin and adaptive transpose Jacobian are more robust to changes in initial conditions compared to the conventional sliding mode control. Furthermore, sliding mode control performs well up to 20% uncertainties in the parameters due to its model-based nature, whereas the transpose Jacobin and the proposed adaptive transpose Jacobian algorithms show promising results even in higher mass uncertainties.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 6.8 -->
                
            <!-- LLMs: 6.3 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 3.2 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Hardware: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1486
            </span>
            <a href="https://arxiv.org/abs/2209.11450" target="_blank" rel="noopener noreferrer">A Second-Order TGV Discretization with $90^{\circ}$ Rotational Invariance Property</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alireza Hosseini, Kristian Bredies | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this work, we propose a new discretization for second-order total generalized variation (TGV) with some distinct properties compared to existing discrete formulations. The introduced model is based on same design principles as Condat's discrete total variation model (\textit{SIAM J. Imaging Sci}.</span>
            
            <span class="abstract-full" style="display: none;">In this work, we propose a new discretization for second-order total generalized variation (TGV) with some distinct properties compared to existing discrete formulations. The introduced model is based on same design principles as Condat's discrete total variation model (\textit{SIAM J. Imaging Sci}., 10(3), 1258--1290, 2017) and shares its benefits, in particular, improved quality for the solution of imaging problems. An algorithm for image denoising with second-order TGV using the new discretization is proposed. Numerical results obtained with this algorithm demonstrate the discretization's advantages. Moreover, in order to compare invariance properties of the new model, an algorithm for calculating the TGV value with respect to the new discretization model is given.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1535
            </span>
            <a href="https://arxiv.org/abs/2307.11170" target="_blank" rel="noopener noreferrer">UMLS-KGI-BERT: Data-Centric Knowledge Integration in Transformers for Biomedical Entity Recognition</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Aidan Mannion, Thierry Chevalier, Didier Schwab, Lorraine Geouriot | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the bio</span>
            
            <span class="abstract-full" style="display: none;">Pre-trained transformer language models (LMs) have in recent years become the dominant paradigm in applied NLP. These models have achieved state-of-the-art performance on tasks such as information extraction, question answering, sentiment analysis, document classification and many others. In the biomedical domain, significant progress has been made in adapting this paradigm to NLP tasks that require the integration of domain-specific knowledge as well as statistical modelling of language. In particular, research in this area has focused on the question of how best to construct LMs that take into account not only the patterns of token distribution in medical text, but also the wealth of structured information contained in terminology resources such as the UMLS. This work contributes a data-centric paradigm for enriching the language representations of biomedical transformer-encoder LMs by extracting text sequences from the UMLS. This allows for graph-based learning objectives to be combined with masked-language pre-training. Preliminary results from experiments in the extension of pre-trained LMs as well as training from scratch show that this framework improves downstream performance on multiple biomedical and clinical Named Entity Recognition (NER) tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1562
            </span>
            <a href="https://arxiv.org/abs/2402.15718" target="_blank" rel="noopener noreferrer">Optimal Rates and Saturation for Noiseless Kernel Ridge Regression</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jihao Long, Xiaojun Peng, Lei Wu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Kernel ridge regression (KRR), also known as the least-squares support vector machine, is a fundamental method for learning functions from finite samples. While most existing analyses focus on the noisy setting with constant-level label noise, we present a comprehensive study of KRR in the noiseless</span>
            
            <span class="abstract-full" style="display: none;">Kernel ridge regression (KRR), also known as the least-squares support vector machine, is a fundamental method for learning functions from finite samples. While most existing analyses focus on the noisy setting with constant-level label noise, we present a comprehensive study of KRR in the noiseless regime -- a critical setting in scientific computing where data are often generated via high-fidelity numerical simulations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1704
            </span>
            <a href="https://arxiv.org/abs/2504.08671" target="_blank" rel="noopener noreferrer">Regularized infill criteria for multi-objective Bayesian optimization with application to aircraft design</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Robin Grapin, Youssef Diouane, Joseph Morlier, Nathalie Bartoli, Thierry Lefebvre, Paul Saves, Jasper Bussemaker | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Bayesian optimization is an advanced tool to perform ecient global optimization It consists on enriching iteratively surrogate Kriging models of the objective and the constraints both supposed to be computationally expensive of the targeted optimization problem Nowadays efficient extensions of Bayes</span>
            
            <span class="abstract-full" style="display: none;">Bayesian optimization is an advanced tool to perform ecient global optimization It consists on enriching iteratively surrogate Kriging models of the objective and the constraints both supposed to be computationally expensive of the targeted optimization problem Nowadays efficient extensions of Bayesian optimization to solve expensive multiobjective problems are of high interest The proposed method in this paper extends the super efficient global optimization with mixture of experts SEGOMOE to solve constrained multiobjective problems To cope with the illposedness of the multiobjective inll criteria different enrichment procedures using regularization techniques are proposed The merit of the proposed approaches are shown on known multiobjective benchmark problems with and without constraints The proposed methods are then used to solve a biobjective application related to conceptual aircraft design with ve unknown design variables and three nonlinear inequality constraints The preliminary results show a reduction of the total cost in terms of function evaluations by a factor of 20 compared to the evolutionary algorithm NSGA-II.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.3 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1713
            </span>
            <a href="https://arxiv.org/abs/2412.00034" target="_blank" rel="noopener noreferrer">Data-Driven Prescriptive Analytics Applications: A Comprehensive Survey</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Martin Moesmann, Torben Bach Pedersen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Prescriptive Analytics (PSA), an emerging business analytics field suggesting concrete options for solving business problems, has seen an increasing amount of interest after more than a decade of multidisciplinary research. This paper is a comprehensive survey of existing applications within PSA in </span>
            
            <span class="abstract-full" style="display: none;">Prescriptive Analytics (PSA), an emerging business analytics field suggesting concrete options for solving business problems, has seen an increasing amount of interest after more than a decade of multidisciplinary research. This paper is a comprehensive survey of existing applications within PSA in terms of their use cases, methodologies, and possible future research directions. To ensure a manageable scope, we focus on PSA applications that develop data-driven, automatic workflows, i.e., Data-Driven PSA (DPSA). Following a systematic methodology, we identify and include 104 papers in our survey. As our key contributions, we derive a number of novel taxonomies of the field and use them to analyse the field's temporal development. In terms of use cases, we derive 10 application domains for DPSA, from Healthcare to Manufacturing, and subsumed problem types within each. In terms of individual method usage, we derive 5 method types and map them to a comprehensive taxonomy of method usage within DPSA applications, covering mathematical optimization, data mining and machine learning, probabilistic modelling, domain expertise, as well as simulations. As for combined method usage, we provide a statistical overview of how different method usage combinations are distributed and derive 2 generic workflow patterns along with subsumed workflow patterns, combining methods by either sequential or simultaneous relationships. Finally, we derive 4 possible research directions based on frequently recurring issues among surveyed papers, suggesting new frontiers in terms of methods, tools, and use cases.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1812
            </span>
            <a href="https://arxiv.org/abs/2504.08368" target="_blank" rel="noopener noreferrer">FocalLens: Instruction Tuning Enables Zero-Shot Conditional Image Representations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Cheng-Yu Hsieh, Pavan Kumar Anasosalu Vasu, Fartash Faghri, Raviteja Vemulapalli, Chun-Liang Li, Ranjay Krishna, Oncel Tuzel, Hadi Pouransari | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Visual understanding is inherently contextual -- what we focus on in an image depends on the task at hand. For instance, given an image of a person holding a bouquet of flowers, we may focus on either the person such as their clothing, or the type of flowers, depending on the context of interest. Ye</span>
            
            <span class="abstract-full" style="display: none;">Visual understanding is inherently contextual -- what we focus on in an image depends on the task at hand. For instance, given an image of a person holding a bouquet of flowers, we may focus on either the person such as their clothing, or the type of flowers, depending on the context of interest. Yet, most existing image encoding paradigms represent an image as a fixed, generic feature vector, overlooking the potential needs of prioritizing varying visual information for different downstream use cases. In this work, we introduce FocalLens, a conditional visual encoding method that produces different representations for the same image based on the context of interest, expressed flexibly through natural language. We leverage vision instruction tuning data and contrastively finetune a pretrained vision encoder to take natural language instructions as additional inputs for producing conditional image representations. Extensive experiments validate that conditional image representation from FocalLens better pronounce the visual features of interest compared to generic features produced by standard vision encoders like CLIP. In addition, we show FocalLens further leads to performance improvements on a range of downstream tasks including image-image retrieval, image classification, and image-text retrieval, with an average gain of 5 and 10 points on the challenging SugarCrepe and MMVP-VLM benchmarks, respectively.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.2 -->
                
            <!-- LLMs: 7.6 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- GNN: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1834
            </span>
            <a href="https://arxiv.org/abs/2504.08590" target="_blank" rel="noopener noreferrer">Playpen: An Environment for Exploring Learning Through Conversational Interaction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Nicola Horst, Davide Mazzaccara, Antonia Schmidt, Michael Sullivan, Filippo Moment\`e, Luca Franceschetti, Philipp Sadler, Sherzod Hakimov, Alberto Testoni, Raffaella Bernardi, Raquel Fern\'andez, Alexander Koller, Oliver Lemon, David Schlangen, Mario Giulianelli, Alessandro Suglia | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Are we running out of learning signal? Predicting the next word in an existing text has turned out to be a powerful signal, at least at scale. But there are signs that we are running out of this resource. In recent months, interaction between learner and feedback-giver has come into focus, both for </span>
            
            <span class="abstract-full" style="display: none;">Are we running out of learning signal? Predicting the next word in an existing text has turned out to be a powerful signal, at least at scale. But there are signs that we are running out of this resource. In recent months, interaction between learner and feedback-giver has come into focus, both for "alignment" (with a reward model judging the quality of instruction following attempts) and for improving "reasoning" (process- and outcome-based verifiers judging reasoning steps). In this paper, we explore to what extent synthetic interaction in what we call Dialogue Games -- goal-directed and rule-governed activities driven predominantly by verbal actions -- can provide a learning signal, and how this signal can be used. We introduce an environment for producing such interaction data (with the help of a Large Language Model as counterpart to the learner model), both offline and online. We investigate the effects of supervised fine-tuning on this data, as well as reinforcement learning setups such as DPO, and GRPO; showing that all of these approaches achieve some improvements in in-domain games, but only GRPO demonstrates the ability to generalise to out-of-domain games as well as retain competitive performance in reference-based tasks. We release the framework and the baseline training setups in the hope that this can foster research in this promising new direction.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.2 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1904
            </span>
            <a href="https://arxiv.org/abs/2501.18998" target="_blank" rel="noopener noreferrer">Adversarial Attacks on AI-Generated Text Detection Models: A Token Probability-Based Approach Using Embeddings</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ahmed K. Kadhim, Lei Jiao, Rishad Shafik, Ole-Christoffer Granmo | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent years, text generation tools utilizing Artificial Intelligence (AI) have occasionally been misused across various domains, such as generating student reports or creative writings. This issue prompts plagiarism detection services to enhance their capabilities in identifying AI-generated con</span>
            
            <span class="abstract-full" style="display: none;">In recent years, text generation tools utilizing Artificial Intelligence (AI) have occasionally been misused across various domains, such as generating student reports or creative writings. This issue prompts plagiarism detection services to enhance their capabilities in identifying AI-generated content. Adversarial attacks are often used to test the robustness of AI-text generated detectors. This work proposes a novel textual adversarial attack on the detection models such as Fast-DetectGPT. The method employs embedding models for data perturbation, aiming at reconstructing the AI generated texts to reduce the likelihood of detection of the true origin of the texts. Specifically, we employ different embedding techniques, including the Tsetlin Machine (TM), an interpretable approach in machine learning for this purpose. By combining synonyms and embedding similarity vectors, we demonstrates the state-of-the-art reduction in detection scores against Fast-DetectGPT. Particularly, in the XSum dataset, the detection score decreased from 0.4431 to 0.2744 AUROC, and in the SQuAD dataset, it dropped from 0.5068 to 0.3532 AUROC.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.1933
            </span>
            <a href="https://arxiv.org/abs/2504.08470" target="_blank" rel="noopener noreferrer">On the Design of Diffusion-based Neural Speech Codecs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pietro Foti, Andreas Brendel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recently, neural speech codecs (NSCs) trained as generative models have shown superior performance compared to conventional codecs at low bitrates. Although most state-of-the-art NSCs are trained as Generative Adversarial Networks (GANs), Diffusion Models (DMs), a recent class of generative models, </span>
            
            <span class="abstract-full" style="display: none;">Recently, neural speech codecs (NSCs) trained as generative models have shown superior performance compared to conventional codecs at low bitrates. Although most state-of-the-art NSCs are trained as Generative Adversarial Networks (GANs), Diffusion Models (DMs), a recent class of generative models, represent a promising alternative due to their superior performance in image generation relative to GANs. Consequently, DMs have been successfully applied for audio and speech coding among various other audio generation applications. However, the design of diffusion-based NSCs has not yet been explored in a systematic way. We address this by providing a comprehensive analysis of diffusion-based NSCs divided into three contributions. First, we propose a categorization based on the conditioning and output domains of the DM. This simple conceptual framework allows us to define a design space for diffusion-based NSCs and to assign a category to existing approaches in the literature. Second, we systematically investigate unexplored designs by creating and evaluating new diffusion-based NSCs within the conceptual framework. Finally, we compare the proposed models to existing GAN and DM baselines through objective metrics and subjective listening tests.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.9 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2002
            </span>
            <a href="https://arxiv.org/abs/2406.07278" target="_blank" rel="noopener noreferrer">On Kernel's Safety in the Spectre Era (Extended Version)</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Davide Davoli, Martin Avanzini, Tamara Rezk | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The efficacy of address space layout randomization has been formally demonstrated in a shared-memory model by Abadi et al., contingent on specific assumptions about victim programs. However, modern operating systems, implementing layout randomization in the kernel, diverge from these assumptions and</span>
            
            <span class="abstract-full" style="display: none;">The efficacy of address space layout randomization has been formally demonstrated in a shared-memory model by Abadi et al., contingent on specific assumptions about victim programs. However, modern operating systems, implementing layout randomization in the kernel, diverge from these assumptions and operate on a separate memory model with communication through system calls. In this work, we relax Abadi et al.'s language assumptions while demonstrating that layout randomization offers a comparable safety guarantee in a system with memory separation. However, in practice, speculative execution and side-channels are recognized threats to layout randomization. We show that kernel safety cannot be restored for attackers capable of using side-channels and speculative execution and introduce a new condition, that allows us to formally prove kernel safety in the Spectre era. Our research demonstrates that under this condition, the system remains safe without relying on layout randomization. We also demonstrate that our condition can be sensibly weakened, leading to enforcement mechanisms that can guarantee kernel safety for safe system calls in the Spectre era.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.0 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2003
            </span>
            <a href="https://arxiv.org/abs/2504.08113" target="_blank" rel="noopener noreferrer">Test Amplification for REST APIs via Single and Multi-Agent LLM Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Robbe Nooyens, Tolgahan Bardakci, Mutlu Beyazit, Serge Demeyer | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">REST APIs (Representational State Transfer Application Programming Interfaces) are essential to modern cloud-native applications. Strong and automated test cases are crucial to expose lurking bugs in the API. However, creating automated tests for REST APIs is difficult, and it requires test cases th</span>
            
            <span class="abstract-full" style="display: none;">REST APIs (Representational State Transfer Application Programming Interfaces) are essential to modern cloud-native applications. Strong and automated test cases are crucial to expose lurking bugs in the API. However, creating automated tests for REST APIs is difficult, and it requires test cases that explore the protocol's boundary conditions. In this paper, we investigate how single-agent and multi-agent LLM (Large Language Model) systems can amplify a REST API test suite. Our evaluation demonstrates increased API coverage, identification of numerous bugs in the API under test, and insights into the computational cost and energy consumption of both approaches.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.1 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2228
            </span>
            <a href="https://arxiv.org/abs/2504.08694" target="_blank" rel="noopener noreferrer">TP-RAG: Benchmarking Retrieval-Augmented Large Language Model Agents for Spatiotemporal-Aware Travel Planning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hang Ni, Fan Liu, Xinyu Ma, Lixin Su, Shuaiqiang Wang, Dawei Yin, Hui Xiong, Hao Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptab</span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) have shown promise in automating travel planning, yet they often fall short in addressing nuanced spatiotemporal rationality. While existing benchmarks focus on basic plan validity, they neglect critical aspects such as route efficiency, POI appeal, and real-time adaptability. This paper introduces TP-RAG, the first benchmark tailored for retrieval-augmented, spatiotemporal-aware travel planning. Our dataset includes 2,348 real-world travel queries, 85,575 fine-grain annotated POIs, and 18,784 high-quality travel trajectory references sourced from online tourist documents, enabling dynamic and context-aware planning. Through extensive experiments, we reveal that integrating reference trajectories significantly improves spatial efficiency and POI rationality of the travel plan, while challenges persist in universality and robustness due to conflicting references and noisy data. To address these issues, we propose EvoRAG, an evolutionary framework that potently synergizes diverse retrieved trajectories with LLMs' intrinsic reasoning. EvoRAG achieves state-of-the-art performance, improving spatiotemporal compliance and reducing commonsense violation compared to ground-up and retrieval-augmented baselines. Our work underscores the potential of hybridizing Web knowledge with LLM-driven optimization, paving the way for more reliable and adaptive travel planning agents.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.3 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- RAG: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2334
            </span>
            <a href="https://arxiv.org/abs/2411.14695" target="_blank" rel="noopener noreferrer">Anti-Forgetting Adaptation for Unsupervised Person Re-identification</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hao Chen, Francois Bremond, Nicu Sebe, Shiliang Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Regular unsupervised domain adaptive person re-identification (ReID) focuses on adapting a model from a source domain to a fixed target domain. However, an adapted ReID model can hardly retain previously-acquired knowledge and generalize to unseen data. In this paper, we propose a Dual-level Joint A</span>
            
            <span class="abstract-full" style="display: none;">Regular unsupervised domain adaptive person re-identification (ReID) focuses on adapting a model from a source domain to a fixed target domain. However, an adapted ReID model can hardly retain previously-acquired knowledge and generalize to unseen data. In this paper, we propose a Dual-level Joint Adaptation and Anti-forgetting (DJAA) framework, which incrementally adapts a model to new domains without forgetting source domain and each adapted target domain. We explore the possibility of using prototype and instance-level consistency to mitigate the forgetting during the adaptation. Specifically, we store a small number of representative image samples and corresponding cluster prototypes in a memory buffer, which is updated at each adaptation step. With the buffered images and prototypes, we regularize the image-to-image similarity and image-to-prototype similarity to rehearse old knowledge. After the multi-step adaptation, the model is tested on all seen domains and several unseen domains to validate the generalization ability of our method. Extensive experiments demonstrate that our proposed method significantly improves the anti-forgetting, generalization and backward-compatible ability of an unsupervised person ReID model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.8 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.1 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2341
            </span>
            <a href="https://arxiv.org/abs/2504.08137" target="_blank" rel="noopener noreferrer">Empowering Vector Architectures for ML: The CAMP Architecture for Matrix Multiplication</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mohammadreza Esmali Nojehdeh, Hossein Mokhtarnia, Julian Pavon Rivera, Narcis Rodas Quiroga, Roger Figueras Bagu\'e, Enrico Reggiani, Miquel Moreto, Osman Unsal, Adrian Cristal, Eduard Ayguade | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This study presents the Cartesian Accumulative Matrix Pipeline (CAMP) architecture, a novel approach designed to enhance matrix multiplication in Vector Architectures (VAs) and Single Instruction Multiple Data (SIMD) units. CAMP improves the processing efficiency of Quantized Neural Networks (QNNs).</span>
            
            <span class="abstract-full" style="display: none;">This study presents the Cartesian Accumulative Matrix Pipeline (CAMP) architecture, a novel approach designed to enhance matrix multiplication in Vector Architectures (VAs) and Single Instruction Multiple Data (SIMD) units. CAMP improves the processing efficiency of Quantized Neural Networks (QNNs). Matrix multiplication is a cornerstone of machine learning applications, and its quantized versions are increasingly popular for more efficient operations. Unfortunately, existing VAs and SIMD-support units struggle to efficiently handle these quantized formats. In this work, we propose CAMP, a simple yet effective architecture that leverages a hybrid multiplier. The CAMP architecture significantly advances the performance of vector architectures in handling quantized data, enabling more efficient execution of matrix multiplication across various platforms, specifically targeting the ARMv8 Scalable Vector Extension (SVE) and edge RISC-V SIMD-based architectures. In addition to increasing throughput, CAMP's architectural design also contributes to energy efficiency, making it an effective solution for low-power applications. Evaluations on a range of Large Language Models (LLMs) and Convolutional Neural Networks (CNNs) demonstrate that matrix multiplication operations using the proposed micro-architecture achieve up to 17$\times$ and 23$\times$ performance improvements compared to their respective baselines, the ARM A64FX core and a RISC-V-based edge System-on-Chip (SoC). Furthermore, synthesis and place-and-route (PnR) of the CAMP micro-architecture using Synopsys tools -- targeting ARM TSMC 7nm for A64FX and GlobalFoundries 22nm for the RISC-V SoC -- add only 1\% and 4\% area overhead, respectively, compared to the baseline designs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.4 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2394
            </span>
            <a href="https://arxiv.org/abs/2504.08086" target="_blank" rel="noopener noreferrer">Differentially Private Selection using Smooth Sensitivity</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Iago Chaves, Victor Farias, Amanda Perez, Diego Parente, Javam Machado | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Differentially private selection mechanisms offer strong privacy guarantees for queries aiming to identify the top-scoring element r from a finite set R, based on a dataset-dependent utility function. While selection queries are fundamental in data science, few mechanisms effectively ensure their pr</span>
            
            <span class="abstract-full" style="display: none;">Differentially private selection mechanisms offer strong privacy guarantees for queries aiming to identify the top-scoring element r from a finite set R, based on a dataset-dependent utility function. While selection queries are fundamental in data science, few mechanisms effectively ensure their privacy. Furthermore, most approaches rely on global sensitivity to achieve differential privacy (DP), which can introduce excessive noise and impair downstream inferences. To address this limitation, we propose the Smooth Noisy Max (SNM) mechanism, which leverages smooth sensitivity to yield provably tighter (upper bounds on) expected errors compared to global sensitivity-based methods. Empirical results demonstrate that SNM is more accurate than state-of-the-art differentially private selection methods in three applications: percentile selection, greedy decision trees, and random forests.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.9 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.0 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2588
            </span>
            <a href="https://arxiv.org/abs/2406.18892" target="_blank" rel="noopener noreferrer">LearnedKV: Integrating LSM and Learned Index for Superior Performance on Storage</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wenlong Wang, David Hung-Chang Du | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present LearnedKV, a novel tiered key-value store that seamlessly integrates a Log-Structured Merge (LSM) tree with a Learned Index to achieve superior read and write performance on storage systems. While existing approaches use learned indexes primarily as auxiliary components within LSM trees, </span>
            
            <span class="abstract-full" style="display: none;">We present LearnedKV, a novel tiered key-value store that seamlessly integrates a Log-Structured Merge (LSM) tree with a Learned Index to achieve superior read and write performance on storage systems. While existing approaches use learned indexes primarily as auxiliary components within LSM trees, LearnedKV employs a two-tier design where the LSM tree handles recent write operations while a separate Learned Index accelerates read performance. Our design includes a non-blocking conversion mechanism that efficiently transforms LSM data into a Learned Index during garbage collection, maintaining high performance without interrupting operations. LearnedKV dramatically reduces LSM size through this tiered approach, leading to significant performance gains in both reads and writes. Extensive evaluations across diverse workloads show that LearnedKV outperforms state-of-the-art LSM-based solutions by up to 4.32x for read operations and 1.43x for writes. The system demonstrates robust performance across different data distributions, access patterns, and storage media including both SSDs and HDDs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.9 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2636
            </span>
            <a href="https://arxiv.org/abs/2504.08300" target="_blank" rel="noopener noreferrer">Large language models could be rote learners</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuyang Xu, Renjun Hu, Haochao Ying, Jian Wu, Xing Shi, Wei Lin | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of learning and seek to disentangle genuine capability acquisition from</span>
            
            <span class="abstract-full" style="display: none;">Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of learning and seek to disentangle genuine capability acquisition from superficial memorization in LLM evaluation. First, by analyzing model performance under different memorization conditions, we uncover a counterintuitive trend: LLMs perform worse on memorized MCQs than on non-memorized ones, indicating the coexistence of two distinct learning phenomena, i.e., rote memorization and genuine capability learning. To disentangle them, we propose TrinEval, a novel evaluation framework that reformulates MCQs into an alternative trinity format, reducing memorization while preserving knowledge assessment. Experiments validate TrinEval's effectiveness in reformulation, and its evaluation reveals that common LLMs may memorize by rote 20.5% of knowledge points (in MMLU on average).</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.9 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- RAG: 1.0 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.28
            </span>
            <a href="https://arxiv.org/abs/2504.08222" target="_blank" rel="noopener noreferrer">F$^3$Set: Towards Analyzing Fast, Frequent, and Fine-grained Events from Videos</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhaoyu Liu, Kan Jiang, Murong Ma, Zhe Hou, Yun Lin, Jin Song Dong | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a significant challenge in video analytics and multi-modal LLMs. Current methods struggle to identify events that satisfy all the F$^3$ criteria with high accuracy due to challenges such as motion blur and subtle visual discrepancies</span>
            
            <span class="abstract-full" style="display: none;">Analyzing Fast, Frequent, and Fine-grained (F$^3$) events presents a significant challenge in video analytics and multi-modal LLMs. Current methods struggle to identify events that satisfy all the F$^3$ criteria with high accuracy due to challenges such as motion blur and subtle visual discrepancies. To advance research in video understanding, we introduce F$^3$Set, a benchmark that consists of video datasets for precise F$^3$ event detection. Datasets in F$^3$Set are characterized by their extensive scale and comprehensive detail, usually encompassing over 1,000 event types with precise timestamps and supporting multi-level granularity. Currently, F$^3$Set contains several sports datasets, and this framework may be extended to other applications as well. We evaluated popular temporal action understanding methods on F$^3$Set, revealing substantial challenges for existing techniques. Additionally, we propose a new method, F$^3$ED, for F$^3$ event detections, achieving superior performance. The dataset, model, and benchmark code are available at https://github.com/F3Set/F3Set.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2821
            </span>
            <a href="https://arxiv.org/abs/2502.07202" target="_blank" rel="noopener noreferrer">Monte Carlo Tree Diffusion for System 2 Planning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jaesik Yoon, Hyeonseo Cho, Doojin Baek, Yoshua Bengio, Sungjin Ahn | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally improves with additional test-time computation (TTC), standard diffusion-based planners offer only limited avenues for TTC scalability. In this paper, we</span>
            
            <span class="abstract-full" style="display: none;">Diffusion models have recently emerged as a powerful tool for planning. However, unlike Monte Carlo Tree Search (MCTS)-whose performance naturally improves with additional test-time computation (TTC), standard diffusion-based planners offer only limited avenues for TTC scalability. In this paper, we introduce Monte Carlo Tree Diffusion (MCTD), a novel framework that integrates the generative strength of diffusion models with the adaptive search capabilities of MCTS. Our method reconceptualizes denoising as a tree-structured process, allowing partially denoised plans to be iteratively evaluated, pruned, and refined. By selectively expanding promising trajectories while retaining the flexibility to revisit and improve suboptimal branches, MCTD achieves the benefits of MCTS such as controlling exploration-exploitation trade-offs within the diffusion framework. Empirical results on challenging long-horizon tasks show that MCTD outperforms diffusion baselines, yielding higher-quality solutions as TTC increases.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2827
            </span>
            <a href="https://arxiv.org/abs/2504.08471" target="_blank" rel="noopener noreferrer">Dark Haptics: Exploring Manipulative Haptic Design in Mobile User Interfaces</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chenge Tang, Karthikeya Puttur Venkatraj, Hongbo Liu, Christina Schneegass, Gijs Huisman, Abdallah El Ali | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Mobile user interfaces abundantly feature so-called 'dark patterns'. These deceptive design practices manipulate users' decision making to profit online service providers. While past research on dark patterns mainly focus on visual design, other sensory modalities such as audio and touch remain larg</span>
            
            <span class="abstract-full" style="display: none;">Mobile user interfaces abundantly feature so-called 'dark patterns'. These deceptive design practices manipulate users' decision making to profit online service providers. While past research on dark patterns mainly focus on visual design, other sensory modalities such as audio and touch remain largely unexplored. In this early work, we investigate the manipulative side of haptics, which we term as 'Dark Haptics', as a strategy to manipulate users. We designed a study to empirically showcase the potential of using a dark haptic pattern in a mobile device to manipulate user actions in a survey. Our findings indicate that our dark haptic design successfully influenced participants to forego their privacy after experiencing an alarming feedback for rejecting intrusive requests in the survey. As a first exploration of manipulative qualities of dark haptic designs, we attempt to lay the groundwork for future research and tools to mitigate harms and risks of dark haptics.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2958
            </span>
            <a href="https://arxiv.org/abs/2503.20505" target="_blank" rel="noopener noreferrer">Riemannian Optimization on Relaxed Indicator Matrix Manifold</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jinghui Yuan, Fangyuan Xie, Feiping Nie, Xuelong Li | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The indicator matrix plays an important role in machine learning, but optimizing it is an NP-hard problem. We propose a new relaxation of the indicator matrix and prove that this relaxation forms a manifold, which we call the Relaxed Indicator Matrix Manifold (RIM manifold). Based on Riemannian geom</span>
            
            <span class="abstract-full" style="display: none;">The indicator matrix plays an important role in machine learning, but optimizing it is an NP-hard problem. We propose a new relaxation of the indicator matrix and prove that this relaxation forms a manifold, which we call the Relaxed Indicator Matrix Manifold (RIM manifold). Based on Riemannian geometry, we develop a Riemannian toolbox for optimization on the RIM manifold. Specifically, we provide several methods of Retraction, including a fast Retraction method to obtain geodesics. We point out that the RIM manifold is a generalization of the double stochastic manifold, and it is much faster than existing methods on the double stochastic manifold, which has a complexity of \( \mathcal{O}(n^3) \), while RIM manifold optimization is \( \mathcal{O}(n) \) and often yields better results. We conducted extensive experiments, including image denoising, with millions of variables to support our conclusion, and applied the RIM manifold to Ratio Cut, we provide a rigorous convergence proof and achieve clustering results that outperform the state-of-the-art methods. Our Code in \href{https://github.com/Yuan-Jinghui/Riemannian-Optimization-on-Relaxed-Indicator-Matrix-Manifold}{here}.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.4 -->
                
            <!-- Medicine: 6.8 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.2968
            </span>
            <a href="https://arxiv.org/abs/2503.23655" target="_blank" rel="noopener noreferrer">Construction of Hyperchaotic Maps Based on 3D-CCC and its Applications in Image Encryption</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jilei Sun | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The security performance of chaos-based image encryption algorithms heavily depends on the complexity of the underlying chaotic system. To enhance encryption effectiveness, it is crucial to design chaotic systems with improved dynamic properties. This paper proposes a novel approach, the 3D Cascaded</span>
            
            <span class="abstract-full" style="display: none;">The security performance of chaos-based image encryption algorithms heavily depends on the complexity of the underlying chaotic system. To enhance encryption effectiveness, it is crucial to design chaotic systems with improved dynamic properties. This paper proposes a novel approach, the 3D Cascaded Cross-Coupling Method (3D-CCC), for constructing 3D hyperchaotic systems by combining three one-dimensional chaotic systems, which can be identical or different. Using this method, we develop a new 3D hyperchaotic map, 3D-ICCCLS, which exhibits superior chaotic characteristics, including good ergodicity, randomness, positive Lyapunov exponents, and high spectral entropy. Furthermore, we introduce a color image encryption algorithm based on 3D-ICCCLS. The proposed scheme treats the three color channels as an integrated unit, employing cross-channel bit mixing followed by simultaneous permutation and diffusion. This approach achieves a strong encryption effect in a single round. Experimental results demonstrate that the algorithm provides a large key space, high key sensitivity, and strong resistance against common attacks,</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3099
            </span>
            <a href="https://arxiv.org/abs/2504.08617" target="_blank" rel="noopener noreferrer">Counterexample-Guided Abstraction Refinement for Generalized Graph Transformation Systems (Full Version)</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Barbara K\"onig, Arend Rensink, Lara Stoltenow, Fabian Urrigshardt | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper addresses the following verification task: Given a graph transformation system and a class of initial graphs, can we guarantee (non-)reachability of a given other class of graphs that characterizes bad or erroneous states? Both initial and bad states are characterized by nested conditions</span>
            
            <span class="abstract-full" style="display: none;">This paper addresses the following verification task: Given a graph transformation system and a class of initial graphs, can we guarantee (non-)reachability of a given other class of graphs that characterizes bad or erroneous states? Both initial and bad states are characterized by nested conditions (having first-order expressive power). Such systems typically have an infinite state space, causing the problem to be undecidable. We use abstract interpretation to obtain a finite approximation of that state space, and employ counter-example guided abstraction refinement to iteratively obtain suitable predicates for automated verification. Although our primary application is the analysis of graph transformation systems, we state our result in the general setting of reactive systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.9 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3109
            </span>
            <a href="https://arxiv.org/abs/2504.08437" target="_blank" rel="noopener noreferrer">Customizing Spider Silk: Generative Models with Mechanical Property Conditioning for Protein Engineering</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Neeru Dubey, Elin Karlsson, Miguel Angel Redondo, Johan Reimeg{\aa}rd, Anna Rising, Hedvig Kjellstr\"om | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The remarkable mechanical properties of spider silk, including its tensile strength and extensibility, are primarily governed by the repetitive regions of the proteins that constitute the fiber, the major ampullate spidroins (MaSps). However, establishing correlations between mechanical characterist</span>
            
            <span class="abstract-full" style="display: none;">The remarkable mechanical properties of spider silk, including its tensile strength and extensibility, are primarily governed by the repetitive regions of the proteins that constitute the fiber, the major ampullate spidroins (MaSps). However, establishing correlations between mechanical characteristics and repeat sequences is challenging due to the intricate sequence-structure-function relationships of MaSps and the limited availability of annotated datasets. In this study, we present a novel computational framework for designing MaSp repeat sequences with customizable mechanical properties. To achieve this, we developed a lightweight GPT-based generative model by distilling the pre-trained ProtGPT2 protein language model. The distilled model was subjected to multilevel fine-tuning using curated subsets of the Spider Silkome dataset. Specifically, we adapt the model for MaSp repeat generation using 6,000 MaSp repeat sequences and further refine it with 572 repeats associated with experimentally determined fiber-level mechanical properties. Our model generates biologically plausible MaSp repeat regions tailored to specific mechanical properties while also predicting those properties for given sequences. Validation includes sequence-level analysis, assessing physicochemical attributes and expected distribution of key motifs as well as secondary structure compositions. A correlation study using BLAST on the Spider Silkome dataset and a test set of MaSp repeats with known mechanical properties further confirmed the predictive accuracy of the model. This framework advances the rational design of spider silk-inspired biomaterials, offering a versatile tool for engineering protein sequences with tailored mechanical attributes.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.0 -->
                
            <!-- LLMs: 7.3 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3113
            </span>
            <a href="https://arxiv.org/abs/2504.08253" target="_blank" rel="noopener noreferrer">Knowledge Distillation for Underwater Feature Extraction and Matching via GAN-synthesized Images</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jinghe Yang, Mingming Gong, Ye Pu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Autonomous Underwater Vehicles (AUVs) play a crucial role in underwater exploration. Vision-based methods offer cost-effective solutions for localization and mapping in the absence of conventional sensors like GPS and LIDAR. However, underwater environments present significant challenges for feature</span>
            
            <span class="abstract-full" style="display: none;">Autonomous Underwater Vehicles (AUVs) play a crucial role in underwater exploration. Vision-based methods offer cost-effective solutions for localization and mapping in the absence of conventional sensors like GPS and LIDAR. However, underwater environments present significant challenges for feature extraction and matching due to image blurring and noise caused by attenuation, scattering, and the interference of \textit{marine snow}. In this paper, we aim to improve the robustness of the feature extraction and matching in the turbid underwater environment using the cross-modal knowledge distillation method that transfers the in-air feature extraction models to underwater settings using synthetic underwater images as the medium. We first propose a novel adaptive GAN-synthesis method to estimate water parameters and underwater noise distribution, to generate environment-specific synthetic underwater images. We then introduce a general knowledge distillation framework compatible with different teacher models. The evaluation of GAN-based synthesis highlights the significance of the new components, i.e. GAN-synthesized noise and forward scattering, in the proposed model. Additionally, the downstream application of feature extraction and matching (VSLAM) on real underwater sequences validates the effectiveness of the transferred model.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Robotics: 1.0 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.317
            </span>
            <a href="https://arxiv.org/abs/2412.13478" target="_blank" rel="noopener noreferrer">Efficient Fine-Tuning of Single-Cell Foundation Models Enables Zero-Shot Molecular Perturbation Prediction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sepideh Maleki, Jan-Christian Huetter, Kangway V. Chuang, David Richmond, Gabriele Scalia, Tommaso Biancalani | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Predicting transcriptional responses to novel drugs provides a unique opportunity to accelerate biomedical research and advance drug discovery efforts. However, the inherent complexity and high dimensionality of cellular responses, combined with the extremely limited available experimental data, mak</span>
            
            <span class="abstract-full" style="display: none;">Predicting transcriptional responses to novel drugs provides a unique opportunity to accelerate biomedical research and advance drug discovery efforts. However, the inherent complexity and high dimensionality of cellular responses, combined with the extremely limited available experimental data, makes the task challenging. In this study, we leverage single-cell foundation models (FMs) pre-trained on tens of millions of single cells, encompassing multiple cell types, states, and disease annotations, to address molecular perturbation prediction. We introduce a drug-conditional adapter that allows efficient fine-tuning by training less than 1% of the original foundation model, thus enabling molecular conditioning while preserving the rich biological representation learned during pre-training. The proposed strategy allows not only the prediction of cellular responses to novel drugs, but also the zero-shot generalization to unseen cell lines. We establish a robust evaluation framework to assess model performance across different generalization tasks, demonstrating state-of-the-art results across all settings, with significant improvements in the few-shot and zero-shot generalization to new cell lines compared to existing baselines.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.1 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3174
            </span>
            <a href="https://arxiv.org/abs/2411.17729" target="_blank" rel="noopener noreferrer">Fast convolution algorithm for state space models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Gregory Beylkin | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present an unconditionally stable algorithm for applying matrix transfer function of a linear time invariant system (LTI) in time domain. The state matrix of an LTI system used for modeling long range dependencies in state space models (SSMs) has eigenvalues close to $1$. The standard recursion d</span>
            
            <span class="abstract-full" style="display: none;">We present an unconditionally stable algorithm for applying matrix transfer function of a linear time invariant system (LTI) in time domain. The state matrix of an LTI system used for modeling long range dependencies in state space models (SSMs) has eigenvalues close to $1$. The standard recursion defining LTI system becomes unstable if the $m\times m$ state matrix has just one eigenvalue with absolute value even slightly greater than 1. This may occur when approximating a state matrix by a structured matrix to reduce the cost of matrix-vector multiplication from $\mathcal{O}\left(m^{2}\right)$ to $\mathcal{O}\left(m\right)$ or $\mathcal{O}\left(m\log m\right).$ We introduce an unconditionally stable algorithm that uses an approximation of the rational transfer function in the z-domain by a matrix polynomial of degree $2^{N+1}-1$, where $N$ is chosen to achieve any user-selected accuracy. Using a cascade implementation in time domain, applying such transfer function to compute $L$ states requires no more than $2L$ matrix-vector multiplications (whereas the standard recursion requires $L$ matrix-vector multiplications). However, using unconditionally stable algorithm, it is not necessary to assure that an approximate state matrix has all eigenvalues with absolute values strictly less than 1 i.e., within the desired accuracy, the absolute value of some eigenvalues may possibly exceed $1$. Consequently, this algorithm allows one to use a wider variety of structured approximations to reduce the cost of matrix-vector multiplication and we briefly describe several of them to be used for this purpose.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3246
            </span>
            <a href="https://arxiv.org/abs/2504.08216" target="_blank" rel="noopener noreferrer">Local Distance-Preserving Node Embeddings and Their Performance on Random Graphs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: My Le, Luana Ruiz, Souvik Dhara | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Learning node representations is a fundamental problem in graph machine learning. While existing embedding methods effectively preserve local similarity measures, they often fail to capture global functions like graph distances. Inspired by Bourgain's seminal work on Hilbert space embeddings of metr</span>
            
            <span class="abstract-full" style="display: none;">Learning node representations is a fundamental problem in graph machine learning. While existing embedding methods effectively preserve local similarity measures, they often fail to capture global functions like graph distances. Inspired by Bourgain's seminal work on Hilbert space embeddings of metric spaces (1985), we study the performance of local distance-preserving node embeddings. Known as landmark-based algorithms, these embeddings approximate pairwise distances by computing shortest paths from a small subset of reference nodes (i.e., landmarks). Our main theoretical contribution shows that random graphs, such as Erd\H{o}s-R\'enyi random graphs, require lower dimensions in landmark-based embeddings compared to worst-case graphs. Empirically, we demonstrate that the GNN-based approximations for the distances to landmarks generalize well to larger networks, offering a scalable alternative for graph representation learning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3252
            </span>
            <a href="https://arxiv.org/abs/2211.10882" target="_blank" rel="noopener noreferrer">Multi-head Ensemble of Smoothed Classifiers for Certified Robustness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kun Fang, Qinghua Tao, Yingwen Wu, Tao Li, Xiaolin Huang, Jie Yang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Randomized Smoothing (RS) is a promising technique for certified robustness, and recently in RS the ensemble of multiple Deep Neural Networks (DNNs) has shown state-of-the-art performances due to its variance reduction effect over Gaussian noises. However, such an ensemble brings heavy computation b</span>
            
            <span class="abstract-full" style="display: none;">Randomized Smoothing (RS) is a promising technique for certified robustness, and recently in RS the ensemble of multiple Deep Neural Networks (DNNs) has shown state-of-the-art performances due to its variance reduction effect over Gaussian noises. However, such an ensemble brings heavy computation burdens in both training and certification, and yet under-exploits individual DNNs and their mutual effects, as the communication between these classifiers is commonly ignored in optimization. In this work, we consider a novel ensemble-based training way for a single DNN with multiple augmented heads, named as SmOothed Multi-head Ensemble (SOME). In SOME, similar to the pursuit of variance reduction via ensemble, an ensemble of multiple heads imposed with a cosine constraint inside a single DNN is employed with much cheaper training and certification computation overloads in RS. In such network structure, an associated training strategy is designed by introducing a circular communication flow among those augmented heads. That is, each head teaches its neighbor with the self-paced learning strategy using smoothed losses, which are specifically designed in relation to certified robustness. The deployed multi-head structure and the circular-teaching scheme in SOME jointly contribute to the diversities among multiple heads and benefit their ensemble, leading to a competitively stronger certifiably-robust RS-based defense than ensembling multiple DNNs (effectiveness) at the cost of much less computational expenses (efficiency), verified by extensive experiments and discussions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3454
            </span>
            <a href="https://arxiv.org/abs/2412.04332" target="_blank" rel="noopener noreferrer">Liquid: Language Models are Scalable and Unified Multi-modal Generators</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, Xiang Bai | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared feature space for both vision and language. Unlike previous multi</span>
            
            <span class="abstract-full" style="display: none;">We present Liquid, an auto-regressive generation paradigm that seamlessly integrates visual comprehension and generation by tokenizing images into discrete codes and learning these code embeddings alongside text tokens within a shared feature space for both vision and language. Unlike previous multimodal large language model (MLLM), Liquid achieves this integration using a single large language model (LLM), eliminating the need for external pretrained visual embeddings such as CLIP. For the first time, Liquid uncovers a scaling law that performance drop unavoidably brought by the unified training of visual and language tasks diminishes as the model size increases. Furthermore, the unified token space enables visual generation and comprehension tasks to mutually enhance each other, effectively removing the typical interference seen in earlier models. We show that existing LLMs can serve as strong foundations for Liquid, saving 100x in training costs while outperforming Chameleon in multimodal capabilities and maintaining language performance comparable to mainstream LLMs like LLAMA2. Liquid also outperforms models like SD v2.1 and SD-XL (FID of 5.47 on MJHQ-30K), excelling in both vision-language and text-only tasks. This work demonstrates that LLMs such as Qwen2.5 and GEMMA2 are powerful multimodal generators, offering a scalable solution for enhancing both vision-language understanding and generation. The code and models will be released at https://github.com/FoundationVision/Liquid.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.6 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.8 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3595
            </span>
            <a href="https://arxiv.org/abs/2403.10444" target="_blank" rel="noopener noreferrer">Block Verification Accelerates Speculative Decoding</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ziteng Sun, Uri Mendlovic, Yaniv Leviathan, Asaf Aharoni, Jae Hun Ro, Ahmad Beirami, Ananda Theertha Suresh | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Speculative decoding is an effective method for lossless acceleration of large language models during inference. It uses a fast model to draft a block of tokens which are then verified in parallel by the target model, and provides a guarantee that the output is distributed identically to a sample fr</span>
            
            <span class="abstract-full" style="display: none;">Speculative decoding is an effective method for lossless acceleration of large language models during inference. It uses a fast model to draft a block of tokens which are then verified in parallel by the target model, and provides a guarantee that the output is distributed identically to a sample from the target model. In prior works, draft verification is performed independently token-by-token. Surprisingly, we show that this approach is not optimal. We propose Block Verification, a simple draft verification algorithm that verifies the entire block jointly and provides additional wall-clock speedup. We prove that the proposed mechanism is optimal in the expected number of tokens produced each iteration and specifically is never worse than the standard token-level verification. Empirically, block verification provides modest but consistent wall-clock speedups over the standard token verification algorithm of 5%-8% in a range of tasks and datasets. Given that block verification does not increase code complexity, maintains the strong lossless guarantee of the standard speculative decoding verification algorithm, cannot deteriorate performance, and, in fact, consistently improves it, it can be used as a good default in speculative decoding implementations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3607
            </span>
            <a href="https://arxiv.org/abs/2504.08417" target="_blank" rel="noopener noreferrer">Belief States for Cooperative Multi-Agent Reinforcement Learning under Partial Observability</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Paul J. Pritz, Kin K. Leung | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Reinforcement learning in partially observable environments is typically challenging, as it requires agents to learn an estimate of the underlying system state. These challenges are exacerbated in multi-agent settings, where agents learn simultaneously and influence the underlying state as well as e</span>
            
            <span class="abstract-full" style="display: none;">Reinforcement learning in partially observable environments is typically challenging, as it requires agents to learn an estimate of the underlying system state. These challenges are exacerbated in multi-agent settings, where agents learn simultaneously and influence the underlying state as well as each others' observations. We propose the use of learned beliefs on the underlying state of the system to overcome these challenges and enable reinforcement learning with fully decentralized training and execution. Our approach leverages state information to pre-train a probabilistic belief model in a self-supervised fashion. The resulting belief states, which capture both inferred state information as well as uncertainty over this information, are then used in a state-based reinforcement learning algorithm to create an end-to-end model for cooperative multi-agent reinforcement learning under partial observability. By separating the belief and reinforcement learning tasks, we are able to significantly simplify the policy and value function learning tasks and improve both the convergence speed and the final performance. We evaluate our proposed method on diverse partially observable multi-agent tasks designed to exhibit different variants of partial observability.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.0 -->
                
            <!-- LLMs: 7.6 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.5 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.369
            </span>
            <a href="https://arxiv.org/abs/2504.08154" target="_blank" rel="noopener noreferrer">Investigating Vision-Language Model for Point Cloud-based Vehicle Classification</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yiqiao Li, Jie Wei, Camille Kamga | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Heavy-duty trucks pose significant safety challenges due to their large size and limited maneuverability compared to passenger vehicles. A deeper understanding of truck characteristics is essential for enhancing the safety perspective of cooperative autonomous driving. Traditional LiDAR-based truck </span>
            
            <span class="abstract-full" style="display: none;">Heavy-duty trucks pose significant safety challenges due to their large size and limited maneuverability compared to passenger vehicles. A deeper understanding of truck characteristics is essential for enhancing the safety perspective of cooperative autonomous driving. Traditional LiDAR-based truck classification methods rely on extensive manual annotations, which makes them labor-intensive and costly. The rapid advancement of large language models (LLMs) trained on massive datasets presents an opportunity to leverage their few-shot learning capabilities for truck classification. However, existing vision-language models (VLMs) are primarily trained on image datasets, which makes it challenging to directly process point cloud data. This study introduces a novel framework that integrates roadside LiDAR point cloud data with VLMs to facilitate efficient and accurate truck classification, which supports cooperative and safe driving environments. This study introduces three key innovations: (1) leveraging real-world LiDAR datasets for model development, (2) designing a preprocessing pipeline to adapt point cloud data for VLM input, including point cloud registration for dense 3D rendering and mathematical morphological techniques to enhance feature representation, and (3) utilizing in-context learning with few-shot prompting to enable vehicle classification with minimally labeled training data. Experimental results demonstrate encouraging performance of this method and present its potential to reduce annotation efforts while improving classification accuracy.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.8 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.378
            </span>
            <a href="https://arxiv.org/abs/2503.16678" target="_blank" rel="noopener noreferrer">QCPINN: Quantum-Classical Physics-Informed Neural Networks for Solving PDEs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Afrah Farea, Saiful Khan, Mustafa Serdar Celebi | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Physics-informed neural networks (PINNs) have emerged as promising methods for solving partial differential equations (PDEs) by embedding physical laws within neural architectures. However, these classical approaches often require a large number of parameters to achieve reasonable accuracy, particul</span>
            
            <span class="abstract-full" style="display: none;">Physics-informed neural networks (PINNs) have emerged as promising methods for solving partial differential equations (PDEs) by embedding physical laws within neural architectures. However, these classical approaches often require a large number of parameters to achieve reasonable accuracy, particularly for complex PDEs. In this paper, we present a quantum-classical physics-informed neural network (QCPINN) that combines quantum and classical components, allowing us to solve PDEs with significantly fewer parameters while maintaining comparable accuracy and convergence to classical PINNs. We systematically evaluated two quantum circuit architectures across various configurations on five benchmark PDEs to identify optimal QCPINN designs. Our results demonstrate that the QCPINN achieves stable convergence and comparable accuracy, while requiring approximately 10% of the trainable parameters used in classical approaches. It also results in a 40% reduction in the relative error L2 for the convection-diffusion equation. These findings demonstrate the potential of parameter efficiency as a measurable quantum advantage in physics-informed machine learning, significantly reducing model complexity while preserving solution quality. This approach presents a promising solution to the computational challenges associated with solving PDEs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3875
            </span>
            <a href="https://arxiv.org/abs/2504.08020" target="_blank" rel="noopener noreferrer">Learning Fine-grained Domain Generalization via Hyperbolic State Space Hallucination</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qi Bi, Jingjun Yi, Haolan Zhan, Wei Ji, Gui-Song Xia | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Fine-grained domain generalization (FGDG) aims to learn a fine-grained representation that can be well generalized to unseen target domains when only trained on the source domain data. Compared with generic domain generalization, FGDG is particularly challenging in that the fine-grained category can</span>
            
            <span class="abstract-full" style="display: none;">Fine-grained domain generalization (FGDG) aims to learn a fine-grained representation that can be well generalized to unseen target domains when only trained on the source domain data. Compared with generic domain generalization, FGDG is particularly challenging in that the fine-grained category can be only discerned by some subtle and tiny patterns. Such patterns are particularly fragile under the cross-domain style shifts caused by illumination, color and etc. To push this frontier, this paper presents a novel Hyperbolic State Space Hallucination (HSSH) method. It consists of two key components, namely, state space hallucination (SSH) and hyperbolic manifold consistency (HMC). SSH enriches the style diversity for the state embeddings by firstly extrapolating and then hallucinating the source images. Then, the pre- and post- style hallucinate state embeddings are projected into the hyperbolic manifold. The hyperbolic state space models the high-order statistics, and allows a better discernment of the fine-grained patterns. Finally, the hyperbolic distance is minimized, so that the impact of style variation on fine-grained patterns can be eliminated. Experiments on three FGDG benchmarks demonstrate its state-of-the-art performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.7 -->
                
            <!-- LLMs: 7.3 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3959
            </span>
            <a href="https://arxiv.org/abs/2504.08341" target="_blank" rel="noopener noreferrer">Deep learning-based moment closure for multi-phase computation of semiclassical limit of the Schr\"odinger equation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jin Woo Jang, Jae Yong Lee, Liu Liu, Zhenyi Zhu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a deep learning approach for computing multi-phase solutions to the semiclassical limit of the Schr\"odinger equation. Traditional methods require deriving a multi-phase ansatz to close the moment system of the Liouville equation, a process that is often computationally intensive and impr</span>
            
            <span class="abstract-full" style="display: none;">We present a deep learning approach for computing multi-phase solutions to the semiclassical limit of the Schr\"odinger equation. Traditional methods require deriving a multi-phase ansatz to close the moment system of the Liouville equation, a process that is often computationally intensive and impractical. Our method offers an efficient alternative by introducing a novel two-stage neural network framework to close the $2N\times 2N$ moment system, where $N$ represents the number of phases in the solution ansatz. In the first stage, we train neural networks to learn the mapping between higher-order moments and lower-order moments (along with their derivatives). The second stage incorporates physics-informed neural networks (PINNs), where we substitute the learned higher-order moments to systematically close the system. We provide theoretical guarantees for the convergence of both the loss functions and the neural network approximations. Numerical experiments demonstrate the effectiveness of our method for one- and two-dimensional problems with various phase numbers $N$ in the multi-phase solutions. The results confirm the accuracy and computational efficiency of the proposed approach compared to conventional techniques.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.9 -->
                
            <!-- LLMs: 6.3 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- Reinforcement Learning: 2.7 -->
                
            <!-- Math: 2.3 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- SpikingNN: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.3973
            </span>
            <a href="https://arxiv.org/abs/2504.08056" target="_blank" rel="noopener noreferrer">Brain Signatures of Time Perception in Virtual Reality</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sahar Niknam, Saravanakumar Duraisamy, Jean Botev, Luis A. Leiva | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Achieving a high level of immersion and adaptation in virtual reality (VR) requires precise measurement and representation of user state. While extrinsic physical characteristics such as locomotion and pose can be accurately tracked in real-time, reliably capturing mental states is more challenging.</span>
            
            <span class="abstract-full" style="display: none;">Achieving a high level of immersion and adaptation in virtual reality (VR) requires precise measurement and representation of user state. While extrinsic physical characteristics such as locomotion and pose can be accurately tracked in real-time, reliably capturing mental states is more challenging. Quantitative psychology allows considering more intrinsic features like emotion, attention, or cognitive load. Time perception, in particular, is strongly tied to users' mental states, including stress, focus, and boredom. However, research on objectively measuring the pace at which we perceive the passage of time is scarce. In this work, we investigate the potential of electroencephalography (EEG) as an objective measure of time perception in VR, exploring neural correlates with oscillatory responses and time-frequency analysis. To this end, we implemented a variety of time perception modulators in VR, collected EEG recordings, and labeled them with overestimation, correct estimation, and underestimation time perception states. We found clear EEG spectral signatures for these three states, that are persistent across individuals, modulators, and modulation duration. These signatures can be integrated and applied to monitor and actively influence time perception in VR, allowing the virtual environment to be purposefully adapted to the individual to increase immersion further and improve user experience. A free copy of this paper and all supplemental materials are available at https://vrarlab.uni.lu/pub/brain-signatures.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4002
            </span>
            <a href="https://arxiv.org/abs/2504.07993" target="_blank" rel="noopener noreferrer">Towards Simple Machine Learning Baselines for GNSS RFI Detection</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Viktor Ivanov, Richard C. Wilson, Maurizio Scaramuzza | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Machine learning research in GNSS radio frequency interference (RFI) detection often lacks a proper justification for the decisions made in deep learning-based model architectures. Our paper challenges the status quo in machine learning approaches for GNSS RFI detection, revealing the potentially mi</span>
            
            <span class="abstract-full" style="display: none;">Machine learning research in GNSS radio frequency interference (RFI) detection often lacks a proper justification for the decisions made in deep learning-based model architectures. Our paper challenges the status quo in machine learning approaches for GNSS RFI detection, revealing the potentially misleading track of current research and highlighting alternative directions. Our position advocates for a shift in focus from solely pursuing novel model designs to critically evaluating the utility of complex black box deep learning methods against simpler and more interpretable machine learning baselines. Our findings demonstrate the need for the creation of simple baselines and suggest the need for more exploration and development of simple and interpretable machine learning methods for the detection of GNSS RFIs. The increment of model complexity in the state-of-the-art deep learning-based models often provides very little improvement. Thanks to a unique dataset from Swiss Air Force and Swiss Air-Rescue (Rega), preprocessed by Swiss Air Navigation Services Ltd. (Skyguide), we demonstrate the effectiveness of a simple machine learning baseline for GNSS RFI detection on real-world large-scale aircraft data containing flight recordings impacted by real jamming. The experimental results indicate that our solution successfully detects potential GNSS RFI with 91% accuracy outperforming state-of-the-art deep learning architectures. We believe that our work offers insights and suggestions for the field to move forward.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 10.3 -->
                
            <!-- LLMs: 7.9 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4185
            </span>
            <a href="https://arxiv.org/abs/2504.07992" target="_blank" rel="noopener noreferrer">'Neural howlround' in large language models: a self-reinforcing bias phenomenon, and a dynamic attenuation solution</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Seth Drake | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language model (LLM)-driven AI systems may exhibit an inference failure mode we term `neural howlround,' a self-reinforcing cognitive loop where certain highly weighted inputs become dominant, leading to entrenched response patterns resistant to correction. This paper explores the mechanisms u</span>
            
            <span class="abstract-full" style="display: none;">Large language model (LLM)-driven AI systems may exhibit an inference failure mode we term `neural howlround,' a self-reinforcing cognitive loop where certain highly weighted inputs become dominant, leading to entrenched response patterns resistant to correction. This paper explores the mechanisms underlying this phenomenon, which is distinct from model collapse and biased salience weighting. We propose an attenuation-based correction mechanism that dynamically introduces counterbalancing adjustments and can restore adaptive reasoning, even in `locked-in' AI systems. Additionally, we discuss some other related effects arising from improperly managed reinforcement. Finally, we outline potential applications of this mitigation strategy for improving AI robustness in real-world decision-making tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.1 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.419
            </span>
            <a href="https://arxiv.org/abs/2411.17459" target="_blank" rel="noopener noreferrer">WF-VAE: Enhancing Video VAE by Wavelet-Driven Energy Flow for Latent Video Diffusion Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zongjian Li, Bin Lin, Yang Ye, Liuhan Chen, Xinhua Cheng, Shenghai Yuan, Li Yuan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes</span>
            
            <span class="abstract-full" style="display: none;">Video Variational Autoencoder (VAE) encodes videos into a low-dimensional latent space, becoming a key component of most Latent Video Diffusion Models (LVDMs) to reduce model training costs. However, as the resolution and duration of generated videos increase, the encoding cost of Video VAEs becomes a limiting bottleneck in training LVDMs. Moreover, the block-wise inference method adopted by most LVDMs can lead to discontinuities of latent space when processing long-duration videos. The key to addressing the computational bottleneck lies in decomposing videos into distinct components and efficiently encoding the critical information. Wavelet transform can decompose videos into multiple frequency-domain components and improve the efficiency significantly, we thus propose Wavelet Flow VAE (WF-VAE), an autoencoder that leverages multi-level wavelet transform to facilitate low-frequency energy flow into latent representation. Furthermore, we introduce a method called Causal Cache, which maintains the integrity of latent space during block-wise inference. Compared to state-of-the-art video VAEs, WF-VAE demonstrates superior performance in both PSNR and LPIPS metrics, achieving 2x higher throughput and 4x lower memory consumption while maintaining competitive reconstruction quality. Our code and models are available at https://github.com/PKU-YuanGroup/WF-VAE.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4192
            </span>
            <a href="https://arxiv.org/abs/2504.08372" target="_blank" rel="noopener noreferrer">eST$^2$ Miner -- Process Discovery Based on Firing Partial Orders</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sabine Folz-Weinstein, Christian Rennert, Lisa Luise Mannel, Robin Bergenthum, Wil van der Aalst | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Process discovery generates process models from event logs. Traditionally, an event log is defined as a multiset of traces, where each trace is a sequence of events. The total order of the events in a sequential trace is typically based on their temporal occurrence. However, real-life processes are </span>
            
            <span class="abstract-full" style="display: none;">Process discovery generates process models from event logs. Traditionally, an event log is defined as a multiset of traces, where each trace is a sequence of events. The total order of the events in a sequential trace is typically based on their temporal occurrence. However, real-life processes are partially ordered by nature. Different activities can occur in different parts of the process and, thus, independently of each other. Therefore, the temporal total order of events does not necessarily reflect their causal order, as also causally unrelated events may be ordered in time. Only partial orders allow to express concurrency, duration, overlap, and uncertainty of events. Consequently, there is a growing need for process mining algorithms that can directly handle partially ordered input. In this paper, we combine two well-established and efficient algorithms, the eST Miner from the process mining community and the Firing LPO algorithm from the Petri net community, to introduce the eST$^2$ Miner. The eST$^2$ Miner is a process discovery algorithm that can directly handle partially ordered input, gives strong formal guarantees, offers good runtime and excellent space complexity, and can, thus, be used in real-life applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.0 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4214
            </span>
            <a href="https://arxiv.org/abs/2504.08306" target="_blank" rel="noopener noreferrer">STSeg-Complex Video Object Segmentation: The 1st Solution for 4th PVUW MOSE Challenge</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kehuan Song, Xinglin Xie, Kexin Zhang, Licheng Jiao, Lingling Li, Shuyuan Yang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Segmentation of video objects in complex scenarios is highly challenging, and the MOSE dataset has significantly contributed to the development of this field. This technical report details the STSeg solution proposed by the "imaplus" team.By finetuning SAM2 and the unsupervised model TMO on the MOSE</span>
            
            <span class="abstract-full" style="display: none;">Segmentation of video objects in complex scenarios is highly challenging, and the MOSE dataset has significantly contributed to the development of this field. This technical report details the STSeg solution proposed by the "imaplus" team.By finetuning SAM2 and the unsupervised model TMO on the MOSE dataset, the STSeg solution demonstrates remarkable advantages in handling complex object motions and long-video sequences. In the inference phase, an Adaptive Pseudo-labels Guided Model Refinement Pipeline is adopted to intelligently select appropriate models for processing each video. Through finetuning the models and employing the Adaptive Pseudo-labels Guided Model Refinement Pipeline in the inference phase, the STSeg solution achieved a J&amp;F score of 87.26% on the test set of the 2025 4th PVUW Challenge MOSE Track, securing the 1st place and advancing the technology for video object segmentation in complex scenarios.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.9 -->
                
            <!-- LLMs: 7.7 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4292
            </span>
            <a href="https://arxiv.org/abs/2411.02702" target="_blank" rel="noopener noreferrer">Corners in Quasirandom Groups via Sparse Mixing</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Michael Jaber, Shachar Lovett, Anthony Ostuni | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We improve the best known upper bounds on the density of corner-free sets over quasirandom groups from inverse poly-logarithmic to quasi-polynomial. We make similarly substantial improvements to the best known lower bounds on the communication complexity of a large class of permutation functions in </span>
            
            <span class="abstract-full" style="display: none;">We improve the best known upper bounds on the density of corner-free sets over quasirandom groups from inverse poly-logarithmic to quasi-polynomial. We make similarly substantial improvements to the best known lower bounds on the communication complexity of a large class of permutation functions in the 3-player Number-on-Forehead model. Underpinning both results is a general combinatorial theorem that extends the recent work of Kelley, Lovett, and Meka (STOC'24), itself a development of ideas from the breakthrough result of Kelley and Meka on three-term arithmetic progressions (FOCS'23).</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.7 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Quantum Computing: 5.1 -->
                
            <!-- Math: 3.7 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4312
            </span>
            <a href="https://arxiv.org/abs/2408.11278" target="_blank" rel="noopener noreferrer">The Key of Parameter Skew in Federated Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junfeng Liao, Sifan Wang, Ye Yuan, Riquan Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Federated Learning (FL) has emerged as an excellent solution for performing deep learning on different data owners without exchanging raw data. However, statistical heterogeneity in FL presents a key challenge, leading to a phenomenon of skewness in local model parameter distributions that researche</span>
            
            <span class="abstract-full" style="display: none;">Federated Learning (FL) has emerged as an excellent solution for performing deep learning on different data owners without exchanging raw data. However, statistical heterogeneity in FL presents a key challenge, leading to a phenomenon of skewness in local model parameter distributions that researchers have largely overlooked. In this work, we propose the concept of parameter skew to describe the phenomenon that can substantially affect the accuracy of global model parameter estimation. Additionally, we introduce FedSA, an aggregation strategy to obtain a high-quality global model, to address the implication from parameter skew. Specifically, we categorize parameters into high-dispersion and low-dispersion groups based on the coefficient of variation. For high-dispersion parameters, Micro-Classes (MIC) and Macro-Classes (MAC) represent the dispersion at the micro and macro levels, respectively, forming the foundation of FedSA. To evaluate the effectiveness of FedSA, we conduct extensive experiments with different FL algorithms on three computer vision datasets. FedSA outperforms eight state-of-the-art baselines by about 4.7% in test accuracy.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #b3ebae" title="Confidence: 68.8%">
                        Federated Learning
                    </span>
            <!-- Medicine: 7.5 -->
                
            <!-- LLMs: 6.2 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.2 -->
                
            <!-- SpikingNN: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4468
            </span>
            <a href="https://arxiv.org/abs/2504.08716" target="_blank" rel="noopener noreferrer">ModernBERT or DeBERTaV3? Examining Architecture and Data Influence on Transformer Encoder Models Performance</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Wissam Antoun, Beno\^it Sagot, Djam\'e Seddah | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the abs</span>
            
            <span class="abstract-full" style="display: none;">Pretrained transformer-encoder models like DeBERTaV3 and ModernBERT introduce architectural advancements aimed at improving efficiency and performance. Although the authors of ModernBERT report improved performance over DeBERTaV3 on several benchmarks, the lack of disclosed training data and the absence of comparisons using a shared dataset make it difficult to determine whether these gains are due to architectural improvements or differences in training data. In this work, we conduct a controlled study by pretraining ModernBERT on the same dataset as CamemBERTaV2, a DeBERTaV3 French model, isolating the effect of model design. Our results show that the previous model generation remains superior in sample efficiency and overall benchmark performance, with ModernBERT's primary advantage being faster training and inference speed. However, the new proposed model still provides meaningful architectural improvements compared to earlier models such as BERT and RoBERTa. Additionally, we observe that high-quality pre-training data accelerates convergence but does not significantly improve final performance, suggesting potential benchmark saturation. These findings show the importance of disentangling pretraining data from architectural innovations when evaluating transformer models.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 4.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4476
            </span>
            <a href="https://arxiv.org/abs/2504.08678" target="_blank" rel="noopener noreferrer">From "Worse is Better" to Better: Lessons from a Mixed Methods Study of Ansible's Challenges</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Carolina Carreira, Nuno Saavedra, Alexandra Mendes, Jo\~ao F. Ferreira | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Infrastructure as Code (IaC) tools have transformed the way IT infrastructure is automated and managed, but their growing adoption has also exposed numerous challenges for practitioners. In this paper, we investigate these challenges through the lens of Ansible, a popular IaC tool. Using a mixed met</span>
            
            <span class="abstract-full" style="display: none;">Infrastructure as Code (IaC) tools have transformed the way IT infrastructure is automated and managed, but their growing adoption has also exposed numerous challenges for practitioners. In this paper, we investigate these challenges through the lens of Ansible, a popular IaC tool. Using a mixed methods approach, we investigate challenges, obstacles, and issues faced by practitioners. We analyze 59,157 posts from Stack Overflow, Reddit, and the Ansible Forum to identify common pain points, complemented by 16 semi-structured interviews with practitioners of varying expertise levels.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4507
            </span>
            <a href="https://arxiv.org/abs/2504.08361" target="_blank" rel="noopener noreferrer">SN-LiDAR: Semantic Neural Fields for Novel Space-time View LiDAR Synthesis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yi Chen, Tianchen Deng, Wentao Zhao, Xiaoning Wang, Wenqian Xi, Weidong Chen, Jingchuan Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent research has begun exploring novel view synthesis (NVS) for LiDAR point clouds, aiming to generate realistic LiDAR scans from unseen viewpoints. However, most existing approaches do not reconstruct semantic labels, which are crucial for many downstream applications such as autonomous driving </span>
            
            <span class="abstract-full" style="display: none;">Recent research has begun exploring novel view synthesis (NVS) for LiDAR point clouds, aiming to generate realistic LiDAR scans from unseen viewpoints. However, most existing approaches do not reconstruct semantic labels, which are crucial for many downstream applications such as autonomous driving and robotic perception. Unlike images, which benefit from powerful segmentation models, LiDAR point clouds lack such large-scale pre-trained models, making semantic annotation time-consuming and labor-intensive. To address this challenge, we propose SN-LiDAR, a method that jointly performs accurate semantic segmentation, high-quality geometric reconstruction, and realistic LiDAR synthesis. Specifically, we employ a coarse-to-fine planar-grid feature representation to extract global features from multi-frame point clouds and leverage a CNN-based encoder to extract local semantic features from the current frame point cloud. Extensive experiments on SemanticKITTI and KITTI-360 demonstrate the superiority of SN-LiDAR in both semantic and geometric reconstruction, effectively handling dynamic objects and large-scale scenes. Codes will be available on https://github.com/dtc111111/SN-Lidar.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.0 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- 3D: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4513
            </span>
            <a href="https://arxiv.org/abs/2504.08440" target="_blank" rel="noopener noreferrer">Speech Command + Speech Emotion: Exploring Emotional Speech Commands as a Compound and Playful Modality</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ilhan Aslan, Timothy Merritt, Stine S. Johansen, Niels van Berkel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In an era of human-computer interaction with increasingly agentic AI systems capable of connecting with users conversationally, speech is an important modality for commanding agents. By recognizing and using speech emotions (i.e., how a command is spoken), we can provide agents with the ability to e</span>
            
            <span class="abstract-full" style="display: none;">In an era of human-computer interaction with increasingly agentic AI systems capable of connecting with users conversationally, speech is an important modality for commanding agents. By recognizing and using speech emotions (i.e., how a command is spoken), we can provide agents with the ability to emotionally accentuate their responses and socially enrich users' perceptions and experiences. To explore the concept and impact of speech emotion commands on user perceptions, we realized a prototype and conducted a user study (N = 14) where speech commands are used to steer two vehicles in a minimalist and retro game style implementation. While both agents execute user commands, only one of the agents uses speech emotion information to adapt its execution behavior. We report on differences in how users perceived each agent, including significant differences in stimulation and dependability, outline implications for designing interactions with agents using emotional speech commands, and provide insights on how users consciously emote, which we describe as "voice acting".</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4525
            </span>
            <a href="https://arxiv.org/abs/2504.08666" target="_blank" rel="noopener noreferrer">Variability-Driven User-Story Generation using LLM and Triadic Concept Analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alexandre Bazin (Huaxi), Alain Gutierrez (Huaxi), Marianne Huchard (Huaxi), Pierre Martin (Huaxi), Yulin (Huaxi), Zhang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A widely used Agile practice for requirements is to produce a set of user stories (also called ``agile product backlog''), which roughly includes a list of pairs (role, feature), where the role handles the feature for a certain purpose. In the context of Software Product Lines, the requirements for </span>
            
            <span class="abstract-full" style="display: none;">A widely used Agile practice for requirements is to produce a set of user stories (also called ``agile product backlog''), which roughly includes a list of pairs (role, feature), where the role handles the feature for a certain purpose. In the context of Software Product Lines, the requirements for a family of similar systems is thus a family of user-story sets, one per system, leading to a 3-dimensional dataset composed of sets of triples (system, role, feature). In this paper, we combine Triadic Concept Analysis (TCA) and Large Language Model (LLM) prompting to suggest the user-story set required to develop a new system relying on the variability logic of an existing system family. This process consists in 1) computing 3-dimensional variability expressed as a set of TCA implications, 2) providing the designer with intelligible design options, 3) capturing the designer's selection of options, 4) proposing a first user-story set corresponding to this selection, 5) consolidating its validity according to the implications identified in step 1, while completing it if necessary, and 6) leveraging LLM to have a more comprehensive website. This process is evaluated with a dataset comprising the user-story sets of 67 similar-purpose websites.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.5 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4608
            </span>
            <a href="https://arxiv.org/abs/2412.16739" target="_blank" rel="noopener noreferrer">UNEM: UNrolled Generalized EM for Transductive Few-Shot Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Long Zhou, Fereshteh Shakeri, Aymen Sadraoui, Mounir Kaaniche, Jean-Christophe Pesquet, Ismail Ben Ayed | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Transductive few-shot learning has recently triggered wide attention in computer vision. Yet, current methods introduce key hyper-parameters, which control the prediction statistics of the test batches, such as the level of class balance, affecting performances significantly. Such hyper-parameters a</span>
            
            <span class="abstract-full" style="display: none;">Transductive few-shot learning has recently triggered wide attention in computer vision. Yet, current methods introduce key hyper-parameters, which control the prediction statistics of the test batches, such as the level of class balance, affecting performances significantly. Such hyper-parameters are empirically grid-searched over validation data, and their configurations may vary substantially with the target dataset and pre-training model, making such empirical searches both sub-optimal and computationally intractable. In this work, we advocate and introduce the unrolling paradigm, also referred to as "learning to optimize", in the context of few-shot learning, thereby learning efficiently and effectively a set of optimized hyper-parameters. Specifically, we unroll a generalization of the ubiquitous Expectation-Maximization (EM) optimizer into a neural network architecture, mapping each of its iterates to a layer and learning a set of key hyper-parameters over validation data. Our unrolling approach covers various statistical feature distributions and pre-training paradigms, including recent foundational vision-language models and standard vision-only classifiers. We report comprehensive experiments, which cover a breadth of fine-grained downstream image classification tasks, showing significant gains brought by the proposed unrolled EM algorithm over iterative variants. The achieved improvements reach up to 10% and 7.5% on vision-only and vision-language benchmarks, respectively.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.9 -->
                
            <!-- LLMs: 7.1 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4615
            </span>
            <a href="https://arxiv.org/abs/2504.04279" target="_blank" rel="noopener noreferrer">Could AI Trace and Explain the Origins of AI-Generated Images and Text?</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hongchao Fang, Yixin Liu, Jiangshu Du, Can Qin, Ran Xu, Feng Liu, Lichao Sun, Dongwon Lee, Lifu Huang, Wenpeng Yin | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">AI-generated content is becoming increasingly prevalent in the real world, leading to serious ethical and societal concerns. For instance, adversaries might exploit large multimodal models (LMMs) to create images that violate ethical or legal standards, while paper reviewers may misuse large languag</span>
            
            <span class="abstract-full" style="display: none;">AI-generated content is becoming increasingly prevalent in the real world, leading to serious ethical and societal concerns. For instance, adversaries might exploit large multimodal models (LMMs) to create images that violate ethical or legal standards, while paper reviewers may misuse large language models (LLMs) to generate reviews without genuine intellectual effort. While prior work has explored detecting AI-generated images and texts, and occasionally tracing their source models, there is a lack of a systematic and fine-grained comparative study. Important dimensions--such as AI-generated images vs. text, fully vs. partially AI-generated images, and general vs. malicious use cases--remain underexplored. Furthermore, whether AI systems like GPT-4o can explain why certain forged content is attributed to specific generative models is still an open question, with no existing benchmark addressing this. To fill this gap, we introduce AI-FAKER, a comprehensive multimodal dataset with over 280,000 samples spanning multiple LLMs and LMMs, covering both general and malicious use cases for AI-generated images and texts. Our experiments reveal two key findings: (i) AI authorship detection depends not only on the generated output but also on the model's original training intent; and (ii) GPT-4o provides highly consistent but less specific explanations when analyzing content produced by OpenAI's own models, such as DALL-E and GPT-4o itself.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.4 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4714
            </span>
            <a href="https://arxiv.org/abs/2504.07984" target="_blank" rel="noopener noreferrer">Topic mining based on fine-tuning Sentence-BERT and LDA</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jianheng Li, Lirong Chen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Research background: With the continuous development of society, consumers pay more attention to the key information of product fine-grained attributes when shopping. Research purposes: This study will fine tune the Sentence-BERT word embedding model and LDA model, mine the subject characteristics i</span>
            
            <span class="abstract-full" style="display: none;">Research background: With the continuous development of society, consumers pay more attention to the key information of product fine-grained attributes when shopping. Research purposes: This study will fine tune the Sentence-BERT word embedding model and LDA model, mine the subject characteristics in online reviews of goods, and show consumers the details of various aspects of goods. Research methods: First, the Sentence-BERT model was fine tuned in the field of e-commerce online reviews, and the online review text was converted into a word vector set with richer semantic information; Secondly, the vectorized word set is input into the LDA model for topic feature extraction; Finally, focus on the key functions of the product through keyword analysis under the theme. Results: This study compared this model with other word embedding models and LDA models, and compared it with common topic extraction methods. The theme consistency of this model is 0.5 higher than that of other models, which improves the accuracy of theme extraction</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.6 -->
                
            <!-- LLMs: 8.6 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Federated Learning: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4725
            </span>
            <a href="https://arxiv.org/abs/2504.08061" target="_blank" rel="noopener noreferrer">STEI-PCN: an efficient pure convolutional network for traffic prediction via spatial-temporal encoding and inferring</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kai Hu, Zhidan Zhao, Zhifeng Hao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Traffic data exhibits complex temporal, spatial, and spatial-temporal correlations. Most of models use either independent modules to separately extract temporal and spatial correlations or joint modules to synchronously extract them, without considering the spatial-temporal correlations. Moreover, m</span>
            
            <span class="abstract-full" style="display: none;">Traffic data exhibits complex temporal, spatial, and spatial-temporal correlations. Most of models use either independent modules to separately extract temporal and spatial correlations or joint modules to synchronously extract them, without considering the spatial-temporal correlations. Moreover, models that consider joint spatial-temporal correlations (temporal, spatial, and spatial-temporal correlations) often encounter significant challenges in accuracy and computational efficiency which prevent such models from demonstrating the expected advantages of a joint spatial-temporal correlations architecture. To address these issues, this paper proposes an efficient pure convolutional network for traffic prediction via spatial-temporal encoding and inferring (STEI-PCN). The model introduces and designs a dynamic adjacency matrix inferring module based on absolute spatial and temporal coordinates, as well as relative spatial and temporal distance encoding, using a graph convolutional network combined with gating mechanism to capture local synchronous joint spatial-temporal correlations. Additionally, three layers of temporal dilated causal convolutional network are used to capture long-range temporal correlations. Finally, through multi-view collaborative prediction module, the model integrates the gated-activated original, local synchronous joint spatial-temporal, and long-range temporal features to achieve comprehensive prediction. This study conducts extensive experiments on flow datasets (PeMS03/04/07/08) and speed dataset (PeMS-Bay), covering multiple prediction horizons. The results show that STEI-PCN demonstrates competitive computational efficiency in both training and inference speeds, and achieves superior or slightly inferior to state-of-the-art (SOTA) models on most evaluation metrics.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4773
            </span>
            <a href="https://arxiv.org/abs/2304.05229" target="_blank" rel="noopener noreferrer">The Big-O Problem for Max-Plus Automata is Decidable (PSPACE-Complete)</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Laure Daviaud, David Purser, Marie Tcheng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We show that the big-O problem for max-plus automata is decidable and PSPACE-complete. The big-O (or affine domination) problem asks whether, given two max-plus automata computing functions f and g, there exists a constant c such that f < cg+ c. This is a relaxation of the containment problem asking</span>
            
            <span class="abstract-full" style="display: none;">We show that the big-O problem for max-plus automata is decidable and PSPACE-complete. The big-O (or affine domination) problem asks whether, given two max-plus automata computing functions f and g, there exists a constant c such that f < cg+ c. This is a relaxation of the containment problem asking whether f < g, which is undecidable. Our decidability result uses Simon's forest factorisation theorem, and relies on detecting specific elements, that we call witnesses, in a finite semigroup closed under two special operations: stabilisation and flattening.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.0 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4819
            </span>
            <a href="https://arxiv.org/abs/2412.04447" target="_blank" rel="noopener noreferrer">EgoPlan-Bench2: A Benchmark for Multimodal Large Language Model Planning in Real-World Scenarios</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lu Qiu, Yi Chen, Yuying Ge, Yixiao Ge, Ying Shan, Xihui Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The advent of Multimodal Large Language Models, leveraging the power of Large Language Models, has recently demonstrated superior multimodal understanding and reasoning abilities, heralding a new era for artificial general intelligence. However, achieving AGI necessitates more than just comprehensio</span>
            
            <span class="abstract-full" style="display: none;">The advent of Multimodal Large Language Models, leveraging the power of Large Language Models, has recently demonstrated superior multimodal understanding and reasoning abilities, heralding a new era for artificial general intelligence. However, achieving AGI necessitates more than just comprehension and reasoning. A crucial capability required is effective planning in diverse scenarios, which involves making reasonable decisions based on complex environments to solve real-world problems. Despite its importance, the planning abilities of current MLLMs in varied scenarios remain underexplored. In this paper, we introduce EgoPlan-Bench2, a rigorous and comprehensive benchmark designed to assess the planning capabilities of MLLMs across a wide range of real-world scenarios. EgoPlan-Bench2 encompasses everyday tasks spanning 4 major domains and 24 detailed scenarios, closely aligned with human daily life. EgoPlan-Bench2 is constructed through a semi-automatic process utilizing egocentric videos, complemented by manual verification. Grounded in a first-person perspective, it mirrors the way humans approach problem-solving in everyday life. We evaluate 21 competitive MLLMs and provide an in-depth analysis of their limitations, revealing that they face significant challenges in real-world planning. To further improve the planning proficiency of current MLLMs, we propose a training-free approach using multimodal Chain-of-Thought (CoT) prompting through investigating the effectiveness of various multimodal prompts in complex planning. Our approach enhances the performance of GPT-4V by 10.24 on EgoPlan-Bench2 without additional training. Our work not only sheds light on the current limitations of MLLMs in planning, but also provides insights for future enhancements in this critical area. We have made data and code available at https://qiulu66.github.io/egoplanbench2/.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.4 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4862
            </span>
            <a href="https://arxiv.org/abs/2412.06845" target="_blank" rel="noopener noreferrer">7B Fully Open Source Moxin-LLM -- From Pretraining to GRPO-based Reinforcement Learning Enhancement</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pu Zhao, Xuan Shen, Zhenglun Kong, Yixin Shen, Sung-En Chang, Timothy Rupprecht, Lei Lu, Enfu Nan, Changdi Yang, Yumei He, Weiyan Shi, Xingchen Xu, Yu Huang, Wei Jiang, Wei Wang, Yue Chen, Yong He, Yanzhi Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkabl</span>
            
            <span class="abstract-full" style="display: none;">Recently, Large Language Models (LLMs) have undergone a significant transformation, marked by a rapid rise in both their popularity and capabilities. Leading this evolution are proprietary LLMs like GPT-4 and GPT-o1, which have captured widespread attention in the AI community due to their remarkable performance and versatility. Simultaneously, open-source LLMs, such as LLaMA, have made great contributions to the ever-increasing popularity of LLMs due to the ease to customize and deploy the models across diverse applications. Although open-source LLMs present unprecedented opportunities for innovation and research, the commercialization of LLMs has raised concerns about transparency, reproducibility, and safety. Many open-source LLMs fail to meet fundamental transparency requirements by withholding essential components like training code and data, which may hinder further innovations on LLMs. To mitigate this issue, we introduce Moxin 7B, a fully open-source LLM developed, adhering to principles of open science, open source, open data, and open access. We release the pre-training code and configurations, training and fine-tuning datasets, and intermediate and final checkpoints, aiming to make continuous commitments to fully open-source LLMs. After pre-training and obtaining the base model, we finetune the Moxin Base model with SOTA post-training framework and instruction data to obtain Moxin Instruct model. To improve the reasoning capability, we further finetune our Instruct model with chain-of-thought data distilled from DeepSeek R1, and then use Group Relative Policy Optimization (GRPO), an efficient and effective reinforcement learning algorithm following DeepSeek R1, to finetune our model, leading to the Moxin Reasoning model. Experiments show that our models achieve superior performance in various evaluations such as zero-shot evaluation, few-shot evaluation, and CoT evaluation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.487
            </span>
            <a href="https://arxiv.org/abs/2504.03120" target="_blank" rel="noopener noreferrer">Distributed Resilience-Aware Control in Multi-Robot Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Haejoon Lee, Dimitra Panagou | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Ensuring resilient consensus in multi-robot systems with misbehaving agents remains a challenge, as many existing network resilience properties are inherently combinatorial and globally defined. While previous works have proposed control laws to enhance or preserve resilience in multi-robot networks</span>
            
            <span class="abstract-full" style="display: none;">Ensuring resilient consensus in multi-robot systems with misbehaving agents remains a challenge, as many existing network resilience properties are inherently combinatorial and globally defined. While previous works have proposed control laws to enhance or preserve resilience in multi-robot networks, they often assume a fixed topology with known resilience properties, or require global state knowledge. These assumptions may be impractical in physically-constrained environments, where safety and resilience requirements are conflicting, or when misbehaving agents corrupt the shared information. In this work, we propose a distributed control law that enables each robot to guarantee resilient consensus and safety during its navigation without fixed topologies using only locally available information. To this end, we establish a new sufficient condition for resilient consensus in time-varying networks based on the degree of non-misbehaving or normal agents. Using this condition, we design a Control Barrier Function (CBF)-based controller that guarantees resilient consensus and collision avoidance without requiring estimates of global state and/or control actions of all other robots. Finally, we validate our method through simulations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.9 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.491
            </span>
            <a href="https://arxiv.org/abs/2504.08307" target="_blank" rel="noopener noreferrer">DSM: Building A Diverse Semantic Map for 3D Visual Grounding</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qinghongbing Xie, Zijian Liang, Long Zeng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent years, with the growing research and application of multimodal large language models (VLMs) in robotics, there has been an increasing trend of utilizing VLMs for robotic scene understanding tasks. Existing approaches that use VLMs for 3D Visual Grounding tasks often focus on obtaining scen</span>
            
            <span class="abstract-full" style="display: none;">In recent years, with the growing research and application of multimodal large language models (VLMs) in robotics, there has been an increasing trend of utilizing VLMs for robotic scene understanding tasks. Existing approaches that use VLMs for 3D Visual Grounding tasks often focus on obtaining scene information through geometric and visual information, overlooking the extraction of diverse semantic information from the scene and the understanding of rich implicit semantic attributes, such as appearance, physics, and affordance. The 3D scene graph, which combines geometry and language, is an ideal representation method for environmental perception and is an effective carrier for language models in 3D Visual Grounding tasks. To address these issues, we propose a diverse semantic map construction method specifically designed for robotic agents performing 3D Visual Grounding tasks. This method leverages VLMs to capture the latent semantic attributes and relations of objects within the scene and creates a Diverse Semantic Map (DSM) through a geometry sliding-window map construction strategy. We enhance the understanding of grounding information based on DSM and introduce a novel approach named DSM-Grounding. Experimental results show that our method outperforms current approaches in tasks like semantic segmentation and 3D Visual Grounding, particularly excelling in overall metrics compared to the state-of-the-art. In addition, we have deployed this method on robots to validate its effectiveness in navigation and grasping tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.1 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4913
            </span>
            <a href="https://arxiv.org/abs/2504.08618" target="_blank" rel="noopener noreferrer">A Hybrid Chaos-Based Cryptographic Framework for Post-Quantum Secure Communications</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kevin Song, Noorullah Imran, Jake Y. Chen, Allan C. Dobbins | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present CryptoChaos, a novel hybrid cryptographic framework that synergizes deterministic chaos theory with cutting-edge cryptographic primitives to achieve robust, post-quantum resilient encryption. CryptoChaos harnesses the intrinsic unpredictability of four discrete chaotic maps (Logistic, Che</span>
            
            <span class="abstract-full" style="display: none;">We present CryptoChaos, a novel hybrid cryptographic framework that synergizes deterministic chaos theory with cutting-edge cryptographic primitives to achieve robust, post-quantum resilient encryption. CryptoChaos harnesses the intrinsic unpredictability of four discrete chaotic maps (Logistic, Chebyshev, Tent, and Henon) to generate a high-entropy, multidimensional key from a unified entropy pool. This key is derived through a layered process that combines SHA3-256 hashing with an ephemeral X25519 Diffie-Hellman key exchange and is refined using an HMAC-based key derivation function (HKDF). The resulting encryption key powers AES-GCM, providing both confidentiality and integrity. Comprehensive benchmarking against established symmetric ciphers confirms that CryptoChaos attains near-maximal Shannon entropy (approximately 8 bits per byte) and exhibits negligible adjacent-byte correlations, while robust performance on the NIST SP 800-22 test suite underscores its statistical rigor. Moreover, quantum simulations demonstrate that the additional complexity inherent in chaotic key generation dramatically elevates the resource requirements for Grover-based quantum attacks, with an estimated T gate count of approximately 2.1 x 10^9. The modular and interoperable design of CryptoChaos positions it as a promising candidate for high-assurance applications, ranging from secure communications and financial transactions to IoT systems, paving the way for next-generation post-quantum encryption standards.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4921
            </span>
            <a href="https://arxiv.org/abs/2403.16760" target="_blank" rel="noopener noreferrer">As Good As A Coin Toss: Human detection of AI-generated images, videos, audio, and audiovisual stimuli</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Di Cooke, Abigail Edwards, Sophia Barkoff, Kathryn Kelly | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">One of the current principal defenses against weaponized synthetic media continues to be the ability of the targeted individual to visually or auditorily recognize AI-generated content when they encounter it. However, as the realism of synthetic media continues to rapidly improve, it is vital to hav</span>
            
            <span class="abstract-full" style="display: none;">One of the current principal defenses against weaponized synthetic media continues to be the ability of the targeted individual to visually or auditorily recognize AI-generated content when they encounter it. However, as the realism of synthetic media continues to rapidly improve, it is vital to have an accurate understanding of just how susceptible people currently are to potentially being misled by convincing but false AI generated content. We conducted a perceptual study with 1276 participants to assess how capable people were at distinguishing between authentic and synthetic images, audio, video, and audiovisual media. We find that on average, people struggled to distinguish between synthetic and authentic media, with the mean detection performance close to a chance level performance of 50%. We also find that accuracy rates worsen when the stimuli contain any degree of synthetic content, features foreign languages, and the media type is a single modality. People are also less accurate at identifying synthetic images when they feature human faces, and when audiovisual stimuli have heterogeneous authenticity. Finally, we find that higher degrees of prior knowledgeability about synthetic media does not significantly impact detection accuracy rates, but age does, with older individuals performing worse than their younger counterparts. Collectively, these results highlight that it is no longer feasible to rely on the perceptual capabilities of people to protect themselves against the growing threat of weaponized synthetic media, and that the need for alternative countermeasures is more critical than ever before.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.4944
            </span>
            <a href="https://arxiv.org/abs/2504.08553" target="_blank" rel="noopener noreferrer">Uncovering the Structure of Explanation Quality with Spectral Analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Johannes Mae{\ss}, Gr\'egoire Montavon, Shinichi Nakajima, Klaus-Robert M\"uller, Thomas Schnake | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As machine learning models are increasingly considered for high-stakes domains, effective explanation methods are crucial to ensure that their prediction strategies are transparent to the user. Over the years, numerous metrics have been proposed to assess quality of explanations. However, their prac</span>
            
            <span class="abstract-full" style="display: none;">As machine learning models are increasingly considered for high-stakes domains, effective explanation methods are crucial to ensure that their prediction strategies are transparent to the user. Over the years, numerous metrics have been proposed to assess quality of explanations. However, their practical applicability remains unclear, in particular due to a limited understanding of which specific aspects each metric rewards. In this paper we propose a new framework based on spectral analysis of explanation outcomes to systematically capture the multifaceted properties of different explanation techniques. Our analysis uncovers two distinct factors of explanation quality-stability and target sensitivity-that can be directly observed through spectral decomposition. Experiments on both MNIST and ImageNet show that popular evaluation techniques (e.g., pixel-flipping, entropy) partially capture the trade-offs between these factors. Overall, our framework provides a foundational basis for understanding explanation quality, guiding the development of more reliable techniques for evaluating explanations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.7 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5022
            </span>
            <a href="https://arxiv.org/abs/2410.06232" target="_blank" rel="noopener noreferrer">Range, not Independence, Drives Modularity in Biologically Inspired Representations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Will Dorrell, Kyle Hsu, Luke Hollingsworth, Jin Hwa Lee, Jiajun Wu, Chelsea Finn, Peter E Latham, Tim EJ Behrens, James CR Whittington | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Why do biological and artificial neurons sometimes modularise, each encoding a single meaningful variable, and sometimes entangle their representation of many variables? In this work, we develop a theory of when biologically inspired networks -- those that are nonnegative and energy efficient -- mod</span>
            
            <span class="abstract-full" style="display: none;">Why do biological and artificial neurons sometimes modularise, each encoding a single meaningful variable, and sometimes entangle their representation of many variables? In this work, we develop a theory of when biologically inspired networks -- those that are nonnegative and energy efficient -- modularise their representation of source variables (sources). We derive necessary and sufficient conditions on a sample of sources that determine whether the neurons in an optimal biologically-inspired linear autoencoder modularise. Our theory applies to any dataset, extending far beyond the case of statistical independence studied in previous work. Rather we show that sources modularise if their support is ``sufficiently spread''. From this theory, we extract and validate predictions in a variety of empirical studies on how data distribution affects modularisation in nonlinear feedforward and recurrent neural networks trained on supervised and unsupervised tasks. Furthermore, we apply these ideas to neuroscience data, showing that range independence can be used to understand the mixing or modularising of spatial and reward information in entorhinal recordings in seemingly conflicting experiments. Further, we use these results to suggest alternate origins of mixed-selectivity, beyond the predominant theory of flexible nonlinear classification. In sum, our theory prescribes precise conditions on when neural activities modularise, providing tools for inducing and elucidating modular representations in brains and machines.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5045
            </span>
            <a href="https://arxiv.org/abs/2504.08650" target="_blank" rel="noopener noreferrer">Quality evaluation of Tabby coding assistant using real source code snippets</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Marta Borek, Robert Nowak | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models have become a popular tool in software development, providing coding assistance. The proper measurement of the accuracy and reliability of the code produced by such tools is a challenge due to natural language prompts.</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.4 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.505
            </span>
            <a href="https://arxiv.org/abs/2504.08712" target="_blank" rel="noopener noreferrer">Beyond Black-Box Predictions: Identifying Marginal Feature Effects in Tabular Transformer Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anton Thielmann, Arik Reuter, Benjamin Saefken | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent years, deep neural networks have showcased their predictive power across a variety of tasks. Beyond natural language processing, the transformer architecture has proven efficient in addressing tabular data problems and challenges the previously dominant gradient-based decision trees in the</span>
            
            <span class="abstract-full" style="display: none;">In recent years, deep neural networks have showcased their predictive power across a variety of tasks. Beyond natural language processing, the transformer architecture has proven efficient in addressing tabular data problems and challenges the previously dominant gradient-based decision trees in these areas. However, this predictive power comes at the cost of intelligibility: Marginal feature effects are almost completely lost in the black-box nature of deep tabular transformer networks. Alternative architectures that use the additivity constraints of classical statistical regression models can maintain intelligible marginal feature effects, but often fall short in predictive power compared to their more complex counterparts. To bridge the gap between intelligibility and performance, we propose an adaptation of tabular transformer networks designed to identify marginal feature effects. We provide theoretical justifications that marginal feature effects can be accurately identified, and our ablation study demonstrates that the proposed model efficiently detects these effects, even amidst complex feature interactions. To demonstrate the model's predictive capabilities, we compare it to several interpretable as well as black-box models and find that it can match black-box performances while maintaining intelligibility. The source code is available at https://github.com/OpenTabular/NAMpy.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.506
            </span>
            <a href="https://arxiv.org/abs/2501.18563" target="_blank" rel="noopener noreferrer">No Equations Needed: Learning System Dynamics Without Relying on Closed-Form ODEs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Krzysztof Kacprzyk, Mihaela van der Schaar | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model's behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug developm</span>
            
            <span class="abstract-full" style="display: none;">Data-driven modeling of dynamical systems is a crucial area of machine learning. In many scenarios, a thorough understanding of the model's behavior becomes essential for practical applications. For instance, understanding the behavior of a pharmacokinetic model, constructed as part of drug development, may allow us to both verify its biological plausibility (e.g., the drug concentration curve is non-negative and decays to zero) and to design dosing guidelines. Discovery of closed-form ordinary differential equations (ODEs) can be employed to obtain such insights by finding a compact mathematical equation and then analyzing it (a two-step approach). However, its widespread use is currently hindered because the analysis process may be time-consuming, requiring substantial mathematical expertise, or even impossible if the equation is too complex. Moreover, if the found equation's behavior does not satisfy the requirements, editing it or influencing the discovery algorithms to rectify it is challenging as the link between the symbolic form of an ODE and its behavior can be elusive. This paper proposes a conceptual shift to modeling low-dimensional dynamical systems by departing from the traditional two-step modeling process. Instead of first discovering a closed-form equation and then analyzing it, our approach, direct semantic modeling, predicts the semantic representation of the dynamical system (i.e., description of its behavior) directly from data, bypassing the need for complex post-hoc analysis. This direct approach also allows the incorporation of intuitive inductive biases into the optimization algorithm and editing the model's behavior directly, ensuring that the model meets the desired specifications. Our approach not only simplifies the modeling pipeline but also enhances the transparency and flexibility of the resulting models compared to traditional closed-form ODEs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5141
            </span>
            <a href="https://arxiv.org/abs/2504.08381" target="_blank" rel="noopener noreferrer">An Empirical Investigation of Reconstruction-Based Models for Seizure Prediction from ECG Signals</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mohammad Reza Chopannavaz, Foad Ghaderi | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Epileptic seizures are sudden neurological disorders characterized by abnormal, excessive neuronal activity in the brain, which is often associated with changes in cardiovascular activity. These disruptions can pose significant physical and psychological challenges for patients. Therefore, accurate </span>
            
            <span class="abstract-full" style="display: none;">Epileptic seizures are sudden neurological disorders characterized by abnormal, excessive neuronal activity in the brain, which is often associated with changes in cardiovascular activity. These disruptions can pose significant physical and psychological challenges for patients. Therefore, accurate seizure prediction can help mitigate these risks by enabling timely interventions, ultimately improving patients' quality of life. Traditionally, EEG signals have been the primary standard for seizure prediction due to their precision in capturing brain activity. However, their high cost, susceptibility to noise, and logistical constraints limit their practicality, restricting their use to clinical settings. In order to overcome these limitations, this study focuses on leveraging ECG signals as an alternative for seizure prediction. In this paper, we present a novel method for predicting seizures based on detecting anomalies in ECG signals during their reconstruction. By extracting time-frequency features and leveraging various advanced deep learning architectures, the proposed method identifies deviations in heart rate dynamics associated with seizure onset. The proposed approach was evaluated using the Siena database and could achieve specificity of 99.16\%, accuracy of 76.05\%, and false positive rate (FPR) of 0.01/h, with an average prediction time of 45 minutes before seizure onset. These results highlight the potential of ECG-based seizure prediction as a patient-friendly alternative to traditional EEG-based methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5332
            </span>
            <a href="https://arxiv.org/abs/2504.08202" target="_blank" rel="noopener noreferrer">Harnessing the Unseen: The Hidden Influence of Intrinsic Knowledge in Long-Context Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yu Fu, Haz Sameen Shahgir, Hui Liu, Xianfeng Tang, Qi He, Yue Dong | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent advances in long-context models (LCMs), designed to handle extremely long input contexts, primarily focus on utilizing external contextual information, often leaving the influence of large language models' intrinsic knowledge underexplored. In this work, we investigate how this intrinsic know</span>
            
            <span class="abstract-full" style="display: none;">Recent advances in long-context models (LCMs), designed to handle extremely long input contexts, primarily focus on utilizing external contextual information, often leaving the influence of large language models' intrinsic knowledge underexplored. In this work, we investigate how this intrinsic knowledge affects content generation and demonstrate that its impact becomes increasingly pronounced as context length extends. Furthermore, we show that the model's ability to utilize intrinsic knowledge, which we call intrinsic retrieval ability, does not improve simultaneously with its ability to leverage contextual knowledge through extrinsic retrieval ability. Moreover, better extrinsic retrieval can interfere with the model's ability to use its own knowledge effectively, limiting its full potential. To bridge this gap, we design a simple yet effective Hybrid Needle-in-a-Haystack test that evaluates models based on their capabilities across both retrieval abilities, rather than solely emphasizing extrinsic retrieval ability. Our experimental results reveal that Qwen-2.5 models significantly outperform Llama-3.1 models, demonstrating superior intrinsic retrieval ability. Moreover, even the more powerful Llama-3.1-70B-Instruct model fails to exhibit better performance under LCM conditions, highlighting the importance of evaluating models from a dual-retrieval perspective.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.5 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.0 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.4 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5521
            </span>
            <a href="https://arxiv.org/abs/2504.08490" target="_blank" rel="noopener noreferrer">Adopting Large Language Models to Automated System Integration</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Robin D. Pesl | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Modern enterprise computing systems integrate numerous subsystems to resolve a common task by yielding emergent behavior. A widespread approach is using services implemented with Web technologies like REST or OpenAPI, which offer an interaction mechanism and service documentation standard, respectiv</span>
            
            <span class="abstract-full" style="display: none;">Modern enterprise computing systems integrate numerous subsystems to resolve a common task by yielding emergent behavior. A widespread approach is using services implemented with Web technologies like REST or OpenAPI, which offer an interaction mechanism and service documentation standard, respectively. Each service represents a specific business functionality, allowing encapsulation and easier maintenance. Despite the reduced maintenance costs on an individual service level, increased integration complexity arises. Consequently, automated service composition approaches have arisen to mitigate this issue. Nevertheless, these approaches have not achieved high acceptance in practice due to their reliance on complex formal modeling. Within this Ph.D. thesis, we analyze the application of Large Language Models (LLMs) to automatically integrate the services based on a natural language input. The result is a reusable service composition, e.g., as program code. While not always generating entirely correct results, the result can still be helpful by providing integration engineers with a close approximation of a suitable solution, which requires little effort to become operational. Our research involves (i) introducing a software architecture for automated service composition using LLMs, (ii) analyzing Retrieval Augmented Generation (RAG) for service discovery, (iii) proposing a novel natural language query-based benchmark for service discovery, and (iv) extending the benchmark to complete service composition scenarios. We have presented our software architecture as Compositio Prompto, the analysis of RAG for service discovery, and submitted a proposal for the service discovery benchmark. Open topics are primarily the extension of the service discovery benchmark to service composition scenarios and the improvements of the service composition generation, e.g., using fine-tuning or LLM agents.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.8 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5546
            </span>
            <a href="https://arxiv.org/abs/2504.08010" target="_blank" rel="noopener noreferrer">Self-Bootstrapping for Versatile Test-Time Adaptation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shuaicheng Niu, Guohao Chen, Peilin Zhao, Tianyi Wang, Pengcheng Wu, Zhiqi Shen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we seek to develop a versatile test-time adaptation (TTA) objective for a variety of tasks - classification and regression across image-, object-, and pixel-level predictions. We achieve this through a self-bootstrapping scheme that optimizes prediction consistency between the test im</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we seek to develop a versatile test-time adaptation (TTA) objective for a variety of tasks - classification and regression across image-, object-, and pixel-level predictions. We achieve this through a self-bootstrapping scheme that optimizes prediction consistency between the test image (as target) and its deteriorated view. The key challenge lies in devising effective augmentations/deteriorations that: i) preserve the image's geometric information, e.g., object sizes and locations, which is crucial for TTA on object/pixel-level tasks, and ii) provide sufficient learning signals for TTA. To this end, we analyze how common distribution shifts affect the image's information power across spatial frequencies in the Fourier domain, and reveal that low-frequency components carry high power and masking these components supplies more learning signals, while masking high-frequency components can not. In light of this, we randomly mask the low-frequency amplitude of an image in its Fourier domain for augmentation. Meanwhile, we also augment the image with noise injection to compensate for missing learning signals at high frequencies, by enhancing the information power there. Experiments show that, either independently or as a plug-and-play module, our method achieves superior results across classification, segmentation, and 3D monocular detection tasks with both transformer and CNN models.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.0 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5613
            </span>
            <a href="https://arxiv.org/abs/2504.08378" target="_blank" rel="noopener noreferrer">Scaling Up On-Device LLMs via Active-Weight Swapping Between DRAM and Flash</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fucheng Jia, Zewen Wu, Shiqi Jiang, Huiqiang Jiang, Qianxi Zhang, Yuqing Yang, Yunxin Liu, Ju Ren, Deyu Zhang, Ting Cao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large language models (LLMs) are increasingly being deployed on mobile devices, but the limited DRAM capacity constrains the deployable model size. This paper introduces ActiveFlow, the first LLM inference framework that can achieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the </span>
            
            <span class="abstract-full" style="display: none;">Large language models (LLMs) are increasingly being deployed on mobile devices, but the limited DRAM capacity constrains the deployable model size. This paper introduces ActiveFlow, the first LLM inference framework that can achieve adaptive DRAM usage for modern LLMs (not ReLU-based), enabling the scaling up of deployable model sizes. The framework is based on the novel concept of active weight DRAM-flash swapping and incorporates three novel techniques: (1) Cross-layer active weights preloading. It uses the activations from the current layer to predict the active weights of several subsequent layers, enabling computation and data loading to overlap, as well as facilitating large I/O transfers. (2) Sparsity-aware self-distillation. It adjusts the active weights to align with the dense-model output distribution, compensating for approximations introduced by contextual sparsity. (3) Active weight DRAM-flash swapping pipeline. It orchestrates the DRAM space allocation among the hot weight cache, preloaded active weights, and computation-involved weights based on available memory. Results show ActiveFlow achieves the performance-cost Pareto frontier compared to existing efficiency optimization methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.2 -->
                
            <!-- Medicine: 6.6 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5704
            </span>
            <a href="https://arxiv.org/abs/2504.08489" target="_blank" rel="noopener noreferrer">Statistically guided deep learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Michael Kohler, Adam Krzyzak | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a theoretically well-founded deep learning algorithm for nonparametric regression. It uses over-parametrized deep neural networks with logistic activation function, which are fitted to the given data via gradient descent. We propose a special topology of these networks, a special random i</span>
            
            <span class="abstract-full" style="display: none;">We present a theoretically well-founded deep learning algorithm for nonparametric regression. It uses over-parametrized deep neural networks with logistic activation function, which are fitted to the given data via gradient descent. We propose a special topology of these networks, a special random initialization of the weights, and a data-dependent choice of the learning rate and the number of gradient descent steps. We prove a theoretical bound on the expected $L_2$ error of this estimate, and illustrate its finite sample size performance by applying it to simulated data. Our results show that a theoretical analysis of deep learning which takes into account simultaneously optimization, generalization and approximation can result in a new deep learning estimate which has an improved finite sample performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.8 -->
                
            <!-- LLMs: 7.1 -->
                
            <!-- Quantum Computing: 3.3 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 2.4 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.5819
            </span>
            <a href="https://arxiv.org/abs/2411.06019" target="_blank" rel="noopener noreferrer">GaussianSpa: An "Optimizing-Sparsifying" Simplification Framework for Compact and High-Quality 3D Gaussian Splatting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yangming Zhang, Wenqi Jia, Wei Niu, Miao Yin | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view synthesis, leveraging continuous aggregations of Gaussian functions to model scene geometry. However, 3DGS suffers from substantial memory requirements to store the multitude of Gaussians, hindering its practicality. To address </span>
            
            <span class="abstract-full" style="display: none;">3D Gaussian Splatting (3DGS) has emerged as a mainstream for novel view synthesis, leveraging continuous aggregations of Gaussian functions to model scene geometry. However, 3DGS suffers from substantial memory requirements to store the multitude of Gaussians, hindering its practicality. To address this challenge, we introduce GaussianSpa, an optimization-based simplification framework for compact and high-quality 3DGS. Specifically, we formulate the simplification as an optimization problem associated with the 3DGS training. Correspondingly, we propose an efficient "optimizing-sparsifying" solution that alternately solves two independent sub-problems, gradually imposing strong sparsity onto the Gaussians in the training process. Our comprehensive evaluations on various datasets show the superiority of GaussianSpa over existing state-of-the-art approaches. Notably, GaussianSpa achieves an average PSNR improvement of 0.9 dB on the real-world Deep Blending dataset with 10$\times$ fewer Gaussians compared to the vanilla 3DGS. Our project page is available at https://noodle-lab.github.io/gaussianspa/.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.603
            </span>
            <a href="https://arxiv.org/abs/2504.08486" target="_blank" rel="noopener noreferrer">PlugSelect: Pruning Channels with Plug-and-Play Flexibility for Electroencephalography-based Brain Computer Interface</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xue Yuan, Keren Shi, Ning Jiang, Jiayuan He | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Automatic minimization and optimization of the number of the electrodes is essential for the practical application of electroencephalography (EEG)-based brain computer interface (BCI). Previous methods typically require additional training costs or rely on prior knowledge assumptions. This study pro</span>
            
            <span class="abstract-full" style="display: none;">Automatic minimization and optimization of the number of the electrodes is essential for the practical application of electroencephalography (EEG)-based brain computer interface (BCI). Previous methods typically require additional training costs or rely on prior knowledge assumptions. This study proposed a novel channel pruning model, plug-and-select (PlugSelect), applicable across a broad range of BCI paradigms with no additional training cost and plug-and-play functionality. It integrates gradients along the input path to globally infer the causal relationships between input channels and outputs, and ranks the contribution sequences to identify the most highly attributed channels. The results showed that for three BCI paradigms, i.e., auditory attention decoding (AAD), motor imagery (MI), affective computation (AC), PlugSelect could reduce the number of channels by at least half while effectively maintaining decoding performance and improving efficiency. The outcome benefits the design of wearable EEG-based devices, facilitating the practical application of BCI technology.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 7.8 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.607
            </span>
            <a href="https://arxiv.org/abs/2504.08002" target="_blank" rel="noopener noreferrer">More diverse more adaptive: Comprehensive Multi-task Learning for Improved LLM Domain Adaptation in E-commerce</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tong Piao, Pei Tang, Zhipeng Zhang, Jiaqi Li, Qiao Liu, Zufeng Wu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In recent years, Large Language Models (LLMs) have been widely applied across various domains due to their powerful domain adaptation capabilities. Previous studies have suggested that diverse, multi-modal data can enhance LLMs' domain adaptation performance. However, this hypothesis remains insuffi</span>
            
            <span class="abstract-full" style="display: none;">In recent years, Large Language Models (LLMs) have been widely applied across various domains due to their powerful domain adaptation capabilities. Previous studies have suggested that diverse, multi-modal data can enhance LLMs' domain adaptation performance. However, this hypothesis remains insufficiently validated in the e-commerce sector. To address this gap, we propose a comprehensive e-commerce multi-task framework and design empirical experiments to examine the impact of diverse data and tasks on LLMs from two perspectives: "capability comprehensiveness" and "task comprehensiveness." Specifically, we observe significant improvements in LLM performance by progressively introducing tasks related to new major capability areas and by continuously adding subtasks within different major capability domains. Furthermore, we observe that increasing model capacity amplifies the benefits of diversity, suggesting a synergistic relationship between model capacity and data diversity. Finally, we validate the best-performing model from our empirical experiments in the KDD Cup 2024, achieving a rank 5 in Task 1. This outcome demonstrates the significance of our research for advancing LLMs in the e-commerce domain.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 14.0 -->
                
            <!-- Medicine: 8.9 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6186
            </span>
            <a href="https://arxiv.org/abs/2504.08672" target="_blank" rel="noopener noreferrer">Genius: A Generalizable and Purely Unsupervised Self-Training Framework For Advanced Reasoning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fangzhi Xu, Hang Yan, Chang Ma, Haiteng Zhao, Qiushi Sun, Kanzhi Cheng, Junxian He, Jun Liu, Zhiyong Wu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reason</span>
            
            <span class="abstract-full" style="display: none;">Advancing LLM reasoning skills has captivated wide interest. However, current post-training techniques rely heavily on supervisory signals, such as outcome supervision or auxiliary reward models, which face the problem of scalability and high annotation costs. This motivates us to enhance LLM reasoning without the need for external supervision. We introduce a generalizable and purely unsupervised self-training framework, named Genius. Without external auxiliary, Genius requires to seek the optimal response sequence in a stepwise manner and optimize the LLM. To explore the potential steps and exploit the optimal ones, Genius introduces a stepwise foresight re-sampling strategy to sample and estimate the step value by simulating future outcomes. Further, we recognize that the unsupervised setting inevitably induces the intrinsic noise and uncertainty. To provide a robust optimization, we propose an advantage-calibrated optimization (ACO) loss function to mitigate estimation inconsistencies. Combining these techniques together, Genius provides an advanced initial step towards self-improve LLM reasoning with general queries and without supervision, revolutionizing reasoning scaling laws given the vast availability of general queries. The code will be released at https://github.com/xufangzhi/Genius.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.63
            </span>
            <a href="https://arxiv.org/abs/2504.08183" target="_blank" rel="noopener noreferrer">Detecting Credit Card Fraud via Heterogeneous Graph Neural Networks with Graph Attention</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qiuwu Sha, Tengda Tang, Xinyu Du, Jie Liu, Yixian Wang, Yuan Sheng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This study proposes a credit card fraud detection method based on Heterogeneous Graph Neural Network (HGNN) to address fraud in complex transaction networks. Unlike traditional machine learning methods that rely solely on numerical features of transaction records, this approach constructs heterogene</span>
            
            <span class="abstract-full" style="display: none;">This study proposes a credit card fraud detection method based on Heterogeneous Graph Neural Network (HGNN) to address fraud in complex transaction networks. Unlike traditional machine learning methods that rely solely on numerical features of transaction records, this approach constructs heterogeneous transaction graphs. These graphs incorporate multiple node types, including users, merchants, and transactions. By leveraging graph neural networks, the model captures higher-order transaction relationships. A Graph Attention Mechanism is employed to dynamically assign weights to different transaction relationships. Additionally, a Temporal Decay Mechanism is integrated to enhance the model's sensitivity to time-related fraud patterns. To address the scarcity of fraudulent transaction samples, this study applies SMOTE oversampling and Cost-sensitive Learning. These techniques strengthen the model's ability to identify fraudulent transactions. Experimental results demonstrate that the proposed method outperforms existing GNN models, including GCN, GAT, and GraphSAGE, on the IEEE-CIS Fraud Detection dataset. The model achieves notable improvements in both accuracy and OC-ROC. Future research may explore the integration of dynamic graph neural networks and reinforcement learning. Such advancements could enhance the real-time adaptability of fraud detection systems and provide more intelligent solutions for financial risk control.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6413
            </span>
            <a href="https://arxiv.org/abs/2504.07627" target="_blank" rel="noopener noreferrer">Robustness of Online Identification-based Policy Iteration to Noisy Data</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Bowen Song, Andrea Iannelli | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This article investigates the core mechanisms of indirect data-driven control for unknown systems, focusing on the application of policy iteration (PI) within the context of the linear quadratic regulator (LQR) optimal control problem. Specifically, we consider a setting where data is collected sequ</span>
            
            <span class="abstract-full" style="display: none;">This article investigates the core mechanisms of indirect data-driven control for unknown systems, focusing on the application of policy iteration (PI) within the context of the linear quadratic regulator (LQR) optimal control problem. Specifically, we consider a setting where data is collected sequentially from a linear system subject to exogenous process noise, and is then used to refine estimates of the optimal control policy. We integrate recursive least squares (RLS) for online model estimation within a certainty-equivalent framework, and employ PI to iteratively update the control policy. In this work, we investigate first the convergence behavior of RLS under two different models of adversarial noise, namely point-wise and energy bounded noise, and then we provide a closed-loop analysis of the combined model identification and control design process. This iterative scheme is formulated as an algorithmic dynamical system consisting of the feedback interconnection between two algorithms expressed as discrete-time systems. This system theoretic viewpoint on indirect data-driven control allows us to establish convergence guarantees to the optimal controller in the face of uncertainty caused by noisy data. Simulations illustrate the theoretical results.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.1 -->
                
            <!-- Medicine: 7.1 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6544
            </span>
            <a href="https://arxiv.org/abs/2504.07557" target="_blank" rel="noopener noreferrer">Using LLMs for Analyzing AIS Data</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Gaspard Merten, Gilles Dejaegere, Mahmoud Sakr | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent research in Large Language Models (LLMs), has had a profound impact across various fields, including mobility data science. This paper explores the and experiment with different approaches to using LLMs for analyzing AIS data. We propose a set of carefully designed queries to assess the reaso</span>
            
            <span class="abstract-full" style="display: none;">Recent research in Large Language Models (LLMs), has had a profound impact across various fields, including mobility data science. This paper explores the and experiment with different approaches to using LLMs for analyzing AIS data. We propose a set of carefully designed queries to assess the reasoning capabilities of LLMs in this kind of tasks. Further, we experiment with four different methods: (1) using LLMs as a natural language interface to a spatial database, (2) reasoning on raw data, (3) reasoning on compressed trajectories, and (4) reasoning on semantic trajectories. We investigate the strengths and weaknesses for the four methods, and discuss the findings. The goal is to provide valuable insights for both researchers and practitioners on selecting the most appropriate LLM-based method depending on their specific data analysis objectives.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.5 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6658
            </span>
            <a href="https://arxiv.org/abs/2504.08528" target="_blank" rel="noopener noreferrer">On The Landscape of Spoken Language Models: A Comprehensive Survey</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Siddhant Arora, Kai-Wei Chang, Chung-Ming Chien, Yifan Peng, Haibin Wu, Yossi Adi, Emmanuel Dupoux, Hung-Yi Lee, Karen Livescu, Shinji Watanabe | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that ha</span>
            
            <span class="abstract-full" style="display: none;">The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both "pure" language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6691
            </span>
            <a href="https://arxiv.org/abs/2409.05657" target="_blank" rel="noopener noreferrer">Adversarial Attacks on Data Attribution</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xinhe Wang, Pingbang Hu, Junwei Deng, Jiaqi W. Ma | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Data attribution aims to quantify the contribution of individual training data points to the outputs of an AI model, which has been used to measure the value of training data and compensate data providers. Given the impact on financial decisions and compensation mechanisms, a critical question arise</span>
            
            <span class="abstract-full" style="display: none;">Data attribution aims to quantify the contribution of individual training data points to the outputs of an AI model, which has been used to measure the value of training data and compensate data providers. Given the impact on financial decisions and compensation mechanisms, a critical question arises concerning the adversarial robustness of data attribution methods. However, there has been little to no systematic research addressing this issue. In this work, we aim to bridge this gap by detailing a threat model with clear assumptions about the adversary's goal and capabilities and proposing principled adversarial attack methods on data attribution. We present two methods, Shadow Attack and Outlier Attack, which generate manipulated datasets to inflate the compensation adversarially. The Shadow Attack leverages knowledge about the data distribution in the AI applications, and derives adversarial perturbations through "shadow training", a technique commonly used in membership inference attacks. In contrast, the Outlier Attack does not assume any knowledge about the data distribution and relies solely on black-box queries to the target model's predictions. It exploits an inductive bias present in many data attribution methods - outlier data points are more likely to be influential - and employs adversarial examples to generate manipulated datasets. Empirically, in image classification and text generation tasks, the Shadow Attack can inflate the data-attribution-based compensation by at least 200%, while the Outlier Attack achieves compensation inflation ranging from 185% to as much as 643%. Our implementation is ready at https://github.com/TRAIS-Lab/adversarial-attack-data-attribution.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.4 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6739
            </span>
            <a href="https://arxiv.org/abs/2504.08718" target="_blank" rel="noopener noreferrer">EMO-X: Efficient Multi-Person Pose and Shape Estimation in One-Stage</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Haohang Jian, Jinlu Zhang, Junyi Wu, Zhigang Tu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Expressive Human Pose and Shape Estimation (EHPS) aims to jointly estimate human pose, hand gesture, and facial expression from monocular images. Existing methods predominantly rely on Transformer-based architectures, which suffer from quadratic complexity in self-attention, leading to substantial c</span>
            
            <span class="abstract-full" style="display: none;">Expressive Human Pose and Shape Estimation (EHPS) aims to jointly estimate human pose, hand gesture, and facial expression from monocular images. Existing methods predominantly rely on Transformer-based architectures, which suffer from quadratic complexity in self-attention, leading to substantial computational overhead, especially in multi-person scenarios. Recently, Mamba has emerged as a promising alternative to Transformers due to its efficient global modeling capability. However, it remains limited in capturing fine-grained local dependencies, which are essential for precise EHPS. To address these issues, we propose EMO-X, the Efficient Multi-person One-stage model for multi-person EHPS. Specifically, we explore a Scan-based Global-Local Decoder (SGLD) that integrates global context with skeleton-aware local features to iteratively enhance human tokens. Our EMO-X leverages the superior global modeling capability of Mamba and designs a local bidirectional scan mechanism for skeleton-aware local refinement. Comprehensive experiments demonstrate that EMO-X strikes an excellent balance between efficiency and accuracy. Notably, it achieves a significant reduction in computational complexity, requiring 69.8% less inference time compared to state-of-the-art (SOTA) methods, while outperforming most of them in accuracy.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- 3D: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.6865
            </span>
            <a href="https://arxiv.org/abs/2504.08278" target="_blank" rel="noopener noreferrer">Interior Point Differential Dynamic Programming, Redux</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ming Xu, Stephen Gould, Iman Shames | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present IPDDP2, a structure-exploiting algorithm for solving discrete-time, finite horizon optimal control problems with nonlinear constraints. Inequality constraints are handled using a primal-dual interior point formulation and step acceptance for equality constraints follows a line-search filt</span>
            
            <span class="abstract-full" style="display: none;">We present IPDDP2, a structure-exploiting algorithm for solving discrete-time, finite horizon optimal control problems with nonlinear constraints. Inequality constraints are handled using a primal-dual interior point formulation and step acceptance for equality constraints follows a line-search filter approach. The iterates of the algorithm are derived under the Differential Dynamic Programming (DDP) framework. Our numerical experiments evaluate IPDDP2 on four robotic motion planning problems. IPDDP2 reliably converges to low optimality error and exhibits local quadratic and global convergence from remote starting points. Notably, we showcase the robustness of IPDDP2 by using it to solve a contact-implicit, joint limited acrobot swing-up problem involving complementarity constraints from a range of initial conditions. We provide a full implementation of IPDDP2 in the Julia programming language.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.7 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7041
            </span>
            <a href="https://arxiv.org/abs/2412.04156" target="_blank" rel="noopener noreferrer">WalkSAT is linear on random 2-SAT</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Petra Berenbrink, Amin Coja-Oghlan, Colin Cooper, Thorsten G\"otte, Lukas Hintze, Pavel Zakharov | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In an influential article Papadimitriou [FOCS 1991] proved that a local search algorithm called WalkSAT finds a satisfying assignment of a satisfiable 2-CNF with $n$ variables in $O(n^2)$ expected time. Variants of the WalkSAT algorithm have become a mainstay of practical SAT solving (e.g., [Hoos an</span>
            
            <span class="abstract-full" style="display: none;">In an influential article Papadimitriou [FOCS 1991] proved that a local search algorithm called WalkSAT finds a satisfying assignment of a satisfiable 2-CNF with $n$ variables in $O(n^2)$ expected time. Variants of the WalkSAT algorithm have become a mainstay of practical SAT solving (e.g., [Hoos and St\"utzle 2000]). In the present article we analyse the expected running time of WalkSAT on random 2-SAT instances. Answering a question raised by Alekhnovich and Ben-Sasson [SICOMP 2007], we show that WalkSAT runs in linear expected time for all clause/variable densities up to the random 2-SAT satisfiability threshold.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.7 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7247
            </span>
            <a href="https://arxiv.org/abs/2504.07982" target="_blank" rel="noopener noreferrer">Metamorphic Testing for Fairness Evaluation in Large Language Models: Identifying Intersectional Bias in LLaMA and GPT</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Harishwar Reddy, Madhusudan Srinivasan, Upulee Kanewala | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) have made significant strides in Natural Language Processing but remain vulnerable to fairness-related issues, often reflecting biases inherent in their training data. These biases pose risks, particularly when LLMs are deployed in sensitive areas such as healthcare, fin</span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) have made significant strides in Natural Language Processing but remain vulnerable to fairness-related issues, often reflecting biases inherent in their training data. These biases pose risks, particularly when LLMs are deployed in sensitive areas such as healthcare, finance, and law. This paper introduces a metamorphic testing approach to systematically identify fairness bugs in LLMs. We define and apply a set of fairness-oriented metamorphic relations (MRs) to assess the LLaMA and GPT model, a state-of-the-art LLM, across diverse demographic inputs. Our methodology includes generating source and follow-up test cases for each MR and analyzing model responses for fairness violations. The results demonstrate the effectiveness of MT in exposing bias patterns, especially in relation to tone and sentiment, and highlight specific intersections of sensitive attributes that frequently reveal fairness faults. This research improves fairness testing in LLMs, providing a structured approach to detect and mitigate biases and improve model robustness in fairness-sensitive applications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 15.1 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Reinforcement Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7338
            </span>
            <a href="https://arxiv.org/abs/2412.15655" target="_blank" rel="noopener noreferrer">MathSpeech: Leveraging Small LMs for Accurate Conversion in Mathematical Speech-to-Formula</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sieun Hyeon, Kyudan Jung, Jaehee Won, Nam-Joon Kim, Hyun Gon Ryu, Hyuk-Jae Lee, Jaeyoung Do | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In various academic and professional settings, such as mathematics lectures or research presentations, it is often necessary to convey mathematical expressions orally. However, reading mathematical expressions aloud without accompanying visuals can significantly hinder comprehension, especially for </span>
            
            <span class="abstract-full" style="display: none;">In various academic and professional settings, such as mathematics lectures or research presentations, it is often necessary to convey mathematical expressions orally. However, reading mathematical expressions aloud without accompanying visuals can significantly hinder comprehension, especially for those who are hearing-impaired or rely on subtitles due to language barriers. For instance, when a presenter reads Euler's Formula, current Automatic Speech Recognition (ASR) models often produce a verbose and error-prone textual description (e.g., e to the power of i x equals cosine of x plus i $\textit{side}$ of x), instead of the concise $\LaTeX{}$ format (i.e., $ e^{ix} = \cos(x) + i\sin(x) $), which hampers clear understanding and communication. To address this issue, we introduce MathSpeech, a novel pipeline that integrates ASR models with small Language Models (sLMs) to correct errors in mathematical expressions and accurately convert spoken expressions into structured $\LaTeX{}$ representations. Evaluated on a new dataset derived from lecture recordings, MathSpeech demonstrates $\LaTeX{}$ generation capabilities comparable to leading commercial Large Language Models (LLMs), while leveraging fine-tuned small language models of only 120M parameters. Specifically, in terms of CER, BLEU, and ROUGE scores for $\LaTeX{}$ translation, MathSpeech demonstrated significantly superior capabilities compared to GPT-4o. We observed a decrease in CER from 0.390 to 0.298, and higher ROUGE/BLEU scores compared to GPT-4o.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.6 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7406
            </span>
            <a href="https://arxiv.org/abs/2504.02790" target="_blank" rel="noopener noreferrer">Dynamic Treewidth in Logarithmic Time</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tuukka Korhonen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a dynamic data structure that maintains a tree decomposition of width at most $9k+8$ of a dynamic graph with treewidth at most $k$, which is updated by edge insertions and deletions. The amortized update time of our data structure is $2^{O(k)} \log n$, where $n$ is the number of vertices.</span>
            
            <span class="abstract-full" style="display: none;">We present a dynamic data structure that maintains a tree decomposition of width at most $9k+8$ of a dynamic graph with treewidth at most $k$, which is updated by edge insertions and deletions. The amortized update time of our data structure is $2^{O(k)} \log n$, where $n$ is the number of vertices. The data structure also supports maintaining any ``dynamic programming scheme'' on the tree decomposition, providing, for example, a dynamic version of Courcelle's theorem with $O_{k}(\log n)$ amortized update time; the $O_{k}(\cdot)$ notation hides factors that depend on $k$. This improves upon a result of Korhonen, Majewski, Nadara, Pilipczuk, and Soko{\l}owski [FOCS 2023], who gave a similar data structure but with amortized update time $2^{k^{O(1)}} n^{o(1)}$. Furthermore, our data structure is arguably simpler.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.8 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7499
            </span>
            <a href="https://arxiv.org/abs/2504.07326" target="_blank" rel="noopener noreferrer">MatBase Algorithm for Translating Entity-Relationship Data Models into (Elementary) Mathematical Data Model Schemes</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Christian Mancas, Diana Christina Mancas | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper presents a pseudocode algorithm for translating Entity-Relationship data models into (Elementary) Mathematical Data Model schemes. We prove that this algorithm is linear, solid, complete, and optimal. We apply this algorithm to an Entity-Relationship data model for a teaching sub-universe</span>
            
            <span class="abstract-full" style="display: none;">This paper presents a pseudocode algorithm for translating Entity-Relationship data models into (Elementary) Mathematical Data Model schemes. We prove that this algorithm is linear, solid, complete, and optimal. We apply this algorithm to an Entity-Relationship data model for a teaching sub-universe. We also provide the main additional features added to the implementation of this algorithm in MatBase, our intelligent knowledge and database management system prototype based on both the Entity-Relationship, (Elementary) Mathematical, and Relational Data Models.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.5 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7582
            </span>
            <a href="https://arxiv.org/abs/2504.08644" target="_blank" rel="noopener noreferrer">Reverberation-based Features for Sound Event Localization and Detection with Distance Estimation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Davide Berghi, Philip J. B. Jackson | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Sound event localization and detection (SELD) involves predicting active sound event classes over time while estimating their positions. The localization subtask in SELD is usually treated as a direction of arrival estimation problem, ignoring source distance. Only recently, SELD was extended to 3D </span>
            
            <span class="abstract-full" style="display: none;">Sound event localization and detection (SELD) involves predicting active sound event classes over time while estimating their positions. The localization subtask in SELD is usually treated as a direction of arrival estimation problem, ignoring source distance. Only recently, SELD was extended to 3D by incorporating distance estimation, enabling the prediction of sound event positions in 3D space (3D SELD). However, existing methods lack input features designed for distance estimation. We argue that reverberation encodes valuable information for this task. This paper introduces two novel feature formats for 3D SELD based on reverberation: one using direct-to-reverberant ratio (DRR) and another leveraging signal autocorrelation to provide the model with insights into early reflections. Pre-training on synthetic data improves relative distance error (RDE) and overall SELD score, with autocorrelation-based features reducing RDE by over 3 percentage points on the STARSS23 dataset. The code to extract the features is available at github.com/dberghi/SELD-distance-features.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7595
            </span>
            <a href="https://arxiv.org/abs/2504.08726" target="_blank" rel="noopener noreferrer">Interaction-Required Suggestions for Control, Ownership, and Awareness in Human-AI Co-Writing</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kenneth C. Arnold, Jiho Kim | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper explores interaction designs for generative AI interfaces that necessitate human involvement throughout the generation process. We argue that such interfaces can promote cognitive engagement, agency, and thoughtful decision-making. Through a case study in text revision, we present and ana</span>
            
            <span class="abstract-full" style="display: none;">This paper explores interaction designs for generative AI interfaces that necessitate human involvement throughout the generation process. We argue that such interfaces can promote cognitive engagement, agency, and thoughtful decision-making. Through a case study in text revision, we present and analyze two interaction techniques: (1) using a predictive-text interaction to type the assistant's response to a revision request, and (2) highlighting potential edit opportunities in a document. Our implementations demonstrate how these approaches reveal the landscape of writing possibilities and enable fine-grained control. We discuss implications for human-AI writing partnerships and future interaction design directions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7739
            </span>
            <a href="https://arxiv.org/abs/2504.08428" target="_blank" rel="noopener noreferrer">Standardization of Weighted Ranking Correlation Coefficients</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pierangelo Lombardo | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">A relevant problem in statistics is defining the correlation of two rankings of a list of items. Kendall's tau and Spearman's rho are two well established correlation coefficients, characterized by a symmetric form that ensures zero expected value between two pairs of rankings randomly chosen with u</span>
            
            <span class="abstract-full" style="display: none;">A relevant problem in statistics is defining the correlation of two rankings of a list of items. Kendall's tau and Spearman's rho are two well established correlation coefficients, characterized by a symmetric form that ensures zero expected value between two pairs of rankings randomly chosen with uniform probability. However, in recent years, several weighted versions of the original Spearman and Kendall coefficients have emerged that take into account the greater importance of top ranks compared to low ranks, which is common in many contexts. The weighting schemes break the symmetry, causing a non-zero expected value between two random rankings. This issue is very relevant, as it undermines the concept of uncorrelation between rankings. In this paper, we address this problem by proposing a standardization function $g(x)$ that maps a correlation ranking coefficient $\Gamma$ in a standard form $g(\Gamma)$ that has zero expected value, while maintaining the relevant statistical properties of $\Gamma$.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7816
            </span>
            <a href="https://arxiv.org/abs/2504.08205" target="_blank" rel="noopener noreferrer">EO-VLM: VLM-Guided Energy Overload Attacks on Vision Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Minjae Seo, Myoungsung You, Junhee Lee, Jaehan Kim, Hwanjo Heo, Jintae Oh, Jinwoo Kim | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Vision models are increasingly deployed in critical applications such as autonomous driving and CCTV monitoring, yet they remain susceptible to resource-consuming attacks. In this paper, we introduce a novel energy-overloading attack that leverages vision language model (VLM) prompts to generate adv</span>
            
            <span class="abstract-full" style="display: none;">Vision models are increasingly deployed in critical applications such as autonomous driving and CCTV monitoring, yet they remain susceptible to resource-consuming attacks. In this paper, we introduce a novel energy-overloading attack that leverages vision language model (VLM) prompts to generate adversarial images targeting vision models. These images, though imperceptible to the human eye, significantly increase GPU energy consumption across various vision models, threatening the availability of these systems. Our framework, EO-VLM (Energy Overload via VLM), is model-agnostic, meaning it is not limited by the architecture or type of the target vision model. By exploiting the lack of safety filters in VLMs like DALL-E 3, we create adversarial noise images without requiring prior knowledge or internal structure of the target vision models. Our experiments demonstrate up to a 50% increase in energy consumption, revealing a critical vulnerability in current vision models.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7826
            </span>
            <a href="https://arxiv.org/abs/2409.10410" target="_blank" rel="noopener noreferrer">Sharp Estimates for Optimal Multistage Group Partition Testing</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Guojiang Shao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In multistage group testing, the tests within the same stage are considered nonadaptive, while those conducted across different stages are adaptive. Specifically, when the pools within the same stage are disjoint, meaning that the entire set is divided into several disjoint subgroups, it is referred</span>
            
            <span class="abstract-full" style="display: none;">In multistage group testing, the tests within the same stage are considered nonadaptive, while those conducted across different stages are adaptive. Specifically, when the pools within the same stage are disjoint, meaning that the entire set is divided into several disjoint subgroups, it is referred to as a multistage group partition testing problem, denoted as the (n, d, s) problem, where n, d, and s represent the total number of items, defectives, and stages respectively. This paper presents exact solutions for the (n, 1, s) and (n, d, 2) problems for the first time. Additionally, a general dynamic programming approach is developed for the (n, d, s) problem. Significantly we give the sharp upper and lower bounds estimates. If the defective number in unknown but bounded, we can provide an algorithm with an optimal competitive ratio in the asymptotic sense. While assuming the prior distribution of the defective items, we also establish a well performing upper and lower bound estimate to the expectation of optimal strategy</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.2 -->
                
            <!-- Medicine: 6.5 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7861
            </span>
            <a href="https://arxiv.org/abs/2411.06736" target="_blank" rel="noopener noreferrer">MrSteve: Instruction-Following Agents in Minecraft with What-Where-When Memory</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junyeong Park, Junmo Cho, Sungjin Ahn | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers freque</span>
            
            <span class="abstract-full" style="display: none;">Significant advances have been made in developing general-purpose embodied AI in environments like Minecraft through the adoption of LLM-augmented hierarchical approaches. While these approaches, which combine high-level planners with low-level controllers, show promise, low-level controllers frequently become performance bottlenecks due to repeated failures. In this paper, we argue that the primary cause of failure in many low-level controllers is the absence of an episodic memory system. To address this, we introduce MrSteve (Memory Recall Steve), a novel low-level controller equipped with Place Event Memory (PEM), a form of episodic memory that captures what, where, and when information from episodes. This directly addresses the main limitation of the popular low-level controller, Steve-1. Unlike previous models that rely on short-term memory, PEM organizes spatial and event-based data, enabling efficient recall and navigation in long-horizon tasks. Additionally, we propose an Exploration Strategy and a Memory-Augmented Task Solving Framework, allowing agents to alternate between exploration and task-solving based on recalled events. Our approach significantly improves task-solving and exploration efficiency compared to existing methods. We will release our code and demos on the project page: https://sites.google.com/view/mr-steve.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7873
            </span>
            <a href="https://arxiv.org/abs/2504.08260" target="_blank" rel="noopener noreferrer">Evaluating the Bias in LLMs for Surveying Opinion and Decision Making in Healthcare</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yonchanok Khaokaew, Flora D. Salim, Andreas Z\"ufle, Hao Xue, Taylor Anderson, Matthew Scotch, David J Heslop | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent rea</span>
            
            <span class="abstract-full" style="display: none;">Generative agents have been increasingly used to simulate human behaviour in silico, driven by large language models (LLMs). These simulacra serve as sandboxes for studying human behaviour without compromising privacy or safety. However, it remains unclear whether such agents can truly represent real individuals. This work compares survey data from the Understanding America Study (UAS) on healthcare decision-making with simulated responses from generative agents. Using demographic-based prompt engineering, we create digital twins of survey respondents and analyse how well different LLMs reproduce real-world behaviours. Our findings show that some LLMs fail to reflect realistic decision-making, such as predicting universal vaccine acceptance. However, Llama 3 captures variations across race and Income more accurately but also introduces biases not present in the UAS data. This study highlights the potential of generative agents for behavioural research while underscoring the risks of bias from both LLMs and prompting strategies.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.3 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7907
            </span>
            <a href="https://arxiv.org/abs/2504.08661" target="_blank" rel="noopener noreferrer">Safe Flow Matching: Robot Motion Planning with Control Barrier Functions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xiaobing Dai, Dian Yu, Shanshan Zhang, Zewen Yang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent advances in generative modeling have led to promising results in robot motion planning, particularly through diffusion and flow-based models that capture complex, multimodal trajectory distributions. However, these methods are typically trained offline and remain limited when faced with unsee</span>
            
            <span class="abstract-full" style="display: none;">Recent advances in generative modeling have led to promising results in robot motion planning, particularly through diffusion and flow-based models that capture complex, multimodal trajectory distributions. However, these methods are typically trained offline and remain limited when faced with unseen environments or dynamic constraints, often lacking explicit mechanisms to ensure safety during deployment. In this work, we propose, Safe Flow Matching (SafeFM), a motion planning approach for trajectory generation that integrates flow matching with safety guarantees. By incorporating the proposed flow matching barrier functions, SafeFM ensures that generated trajectories remain within safe regions throughout the planning horizon, even in the presence of previously unseen obstacles or state-action constraints. Unlike diffusion-based approaches, our method allows for direct, efficient sampling of constraint-satisfying trajectories, making it well-suited for real-time motion planning. We evaluate SafeFM on a diverse set of tasks, including planar robot navigation and 7-DoF manipulation, demonstrating superior safety, generalization, and planning performance compared to state-of-the-art generative planners. Comprehensive resources are available on the project website: https://safeflowmatching.github.io/SafeFM/</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.1 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.796
            </span>
            <a href="https://arxiv.org/abs/2504.08184" target="_blank" rel="noopener noreferrer">Leveraging Passive Compliance of Soft Robotics for Physical Human-Robot Collaborative Manipulation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Dallin L. Cordon, Shaden Moss, Marc Killpack, John L. Salmon | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work represents an initial benchmark of a large-scale soft robot performing physical, collaborative manipulation of a long, extended object with a human partner. The robot consists of a pneumatically-actuated, three-link continuum soft manipulator mounted to an omni-directional mobile base. The</span>
            
            <span class="abstract-full" style="display: none;">This work represents an initial benchmark of a large-scale soft robot performing physical, collaborative manipulation of a long, extended object with a human partner. The robot consists of a pneumatically-actuated, three-link continuum soft manipulator mounted to an omni-directional mobile base. The system level configuration of the robot and design of the collaborative manipulation (co-manipulation) study are presented. The initial results, both quantitative and qualitative, are directly compared to previous similar human-human co-manipulation studies. These initial results show promise in the ability for large-scale soft robots to perform comparably to human partners acting as non-visual followers in a co-manipulation task. Furthermore, these results challenge traditional soft robot strength limitations and indicate potential for applications requiring strength and adaptability.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.7968
            </span>
            <a href="https://arxiv.org/abs/2305.14985" target="_blank" rel="noopener noreferrer">IdealGPT: Iteratively Decomposing Vision and Language Reasoning via Large Language Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Haoxuan You, Zhecan Wang, Rui Sun, Long Chen, Gengyu Wang, Hammad A. Ayyubi, Kai-Wei Chang, Shih-Fu Chang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer</span>
            
            <span class="abstract-full" style="display: none;">The field of vision-and-language (VL) understanding has made unprecedented progress with end-to-end large pre-trained VL models (VLMs). However, they still fall short in zero-shot reasoning tasks that require multi-step inferencing. To achieve this goal, previous works resort to a divide-and-conquer pipeline. In this paper, we argue that previous efforts have several inherent shortcomings: 1) They rely on domain-specific sub-question decomposing models. 2) They force models to predict the final answer even if the sub-questions or sub-answers provide insufficient information. We address these limitations via IdealGPT, a framework that iteratively decomposes VL reasoning using large language models (LLMs). Specifically, IdealGPT utilizes an LLM to generate sub-questions, a VLM to provide corresponding sub-answers, and another LLM to reason to achieve the final answer. These three modules perform the divide-and-conquer procedure iteratively until the model is confident about the final answer to the main question. We evaluate IdealGPT on multiple challenging VL reasoning tasks under a zero-shot setting. In particular, our IdealGPT outperforms the best existing GPT-4-like models by an absolute 10% on VCR and 15% on SNLI-VE. Code is available at https://github.com/Hxyou/IdealGPT</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.5 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8055
            </span>
            <a href="https://arxiv.org/abs/2308.09549" target="_blank" rel="noopener noreferrer">Quantum and Probabilistic Computers Rigorously Powerful than Traditional Computers, and Derandomization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tianrong Lin | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this paper, we extend the techniques used in our previous work to show that there exists a probabilistic Turing machine running within time $O(n^k)$ for all $k\in\mathbb{N}_1$ accepting a language $L_d$ which is different from any language in $\mathcal{P}$, and then further to prove that $L_d\in\</span>
            
            <span class="abstract-full" style="display: none;">In this paper, we extend the techniques used in our previous work to show that there exists a probabilistic Turing machine running within time $O(n^k)$ for all $k\in\mathbb{N}_1$ accepting a language $L_d$ which is different from any language in $\mathcal{P}$, and then further to prove that $L_d\in\mathcal{BPP}$, thus separating the complexity class $\mathcal{BPP}$ from the class $\mathcal{P}$ (i.e., $\mathcal{P}\subsetneq\mathcal{BPP}$).</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.6 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8108
            </span>
            <a href="https://arxiv.org/abs/2502.17793" target="_blank" rel="noopener noreferrer">SYNTHIA: Novel Concept Design with Affordance Composition</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hyeonjeong Ha, Xiaomeng Jin, Jeonghwan Kim, Jiateng Liu, Zhenhailong Wang, Khanh Duy Nguyen, Ansel Blume, Nanyun Peng, Kai-Wei Chang, Heng Ji | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, functional coherence--the integration of multiple affordances into a single coherent concept--rema</span>
            
            <span class="abstract-full" style="display: none;">Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, functional coherence--the integration of multiple affordances into a single coherent concept--remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, functionally coherent designs based on desired affordances. Our approach leverages a hierarchical concept ontology that decomposes concepts into parts and affordances, serving as a crucial building block for functionally coherent design. We also develop a curriculum learning scheme based on our ontology that contrastively fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. To elaborate, we (i) gradually increase affordance distance, guiding models from basic concept-affordance association to complex affordance compositions that integrate parts of distinct affordances into a single, coherent form, and (ii) enforce visual novelty by employing contrastive objectives to push learned representations away from existing concepts. Experimental results show that SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains of 25.1% and 14.7% for novelty and functional coherence in human evaluation, respectively.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.8 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8115
            </span>
            <a href="https://arxiv.org/abs/2504.07792" target="_blank" rel="noopener noreferrer">Breaking the Barriers: Video Vision Transformers for Word-Level Sign Language Recognition</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alexander Brettmann, Jakob Gr\"avinghoff, Marlene R\"uschoff, Marie Westhues | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Sign language is a fundamental means of communication for the deaf and hard-of-hearing (DHH) community, enabling nuanced expression through gestures, facial expressions, and body movements. Despite its critical role in facilitating interaction within the DHH population, significant barriers persist </span>
            
            <span class="abstract-full" style="display: none;">Sign language is a fundamental means of communication for the deaf and hard-of-hearing (DHH) community, enabling nuanced expression through gestures, facial expressions, and body movements. Despite its critical role in facilitating interaction within the DHH population, significant barriers persist due to the limited fluency in sign language among the hearing population. Overcoming this communication gap through automatic sign language recognition (SLR) remains a challenge, particularly at a dynamic word-level, where temporal and spatial dependencies must be effectively recognized. While Convolutional Neural Networks (CNNs) have shown potential in SLR, they are computationally intensive and have difficulties in capturing global temporal dependencies between video sequences. To address these limitations, we propose a Video Vision Transformer (ViViT) model for word-level American Sign Language (ASL) recognition. Transformer models make use of self-attention mechanisms to effectively capture global relationships across spatial and temporal dimensions, which makes them suitable for complex gesture recognition tasks. The VideoMAE model achieves a Top-1 accuracy of 75.58% on the WLASL100 dataset, highlighting its strong performance compared to traditional CNNs with 65.89%. Our study demonstrates that transformer-based architectures have great potential to advance SLR, overcome communication barriers and promote the inclusion of DHH individuals.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 9.2 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8166
            </span>
            <a href="https://arxiv.org/abs/2504.08364" target="_blank" rel="noopener noreferrer">DRIP: DRop unImportant data Points -- Enhancing Machine Learning Efficiency with Grad-CAM-Based Real-Time Data Prioritization for On-Device Training</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Marcus R\"ub, Daniel Konegen, Axel Sikora, Daniel Mueller-Gritschneder | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Selecting data points for model training is critical in machine learning. Effective selection methods can reduce the labeling effort, optimize on-device training for embedded systems with limited data storage, and enhance the model performance. This paper introduces a novel algorithm that uses Grad-</span>
            
            <span class="abstract-full" style="display: none;">Selecting data points for model training is critical in machine learning. Effective selection methods can reduce the labeling effort, optimize on-device training for embedded systems with limited data storage, and enhance the model performance. This paper introduces a novel algorithm that uses Grad-CAM to make online decisions about retaining or discarding data points. Optimized for embedded devices, the algorithm computes a unique DRIP Score to quantify the importance of each data point. This enables dynamic decision-making on whether a data point should be stored for potential retraining or discarded without compromising model performance. Experimental evaluations on four benchmark datasets demonstrate that our approach can match or even surpass the accuracy of models trained on the entire dataset, all while achieving storage savings of up to 39\%. To our knowledge, this is the first algorithm that makes online decisions about data point retention without requiring access to the entire dataset.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.3 -->
                
            <!-- LLMs: 8.7 -->
                
            <!-- Quantum Computing: 4.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.819
            </span>
            <a href="https://arxiv.org/abs/2504.08524" target="_blank" rel="noopener noreferrer">Mitigating Timbre Leakage with Universal Semantic Mapping Residual Block for Voice Conversion</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Na Li, Chuke Wang, Yu Gu, Zhifeng Li | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Voice conversion (VC) transforms source speech into a target voice by preserving the content. However, timbre information from the source speaker is inherently embedded in the content representations, causing significant timbre leakage and reducing similarity to the target speaker. To address this, </span>
            
            <span class="abstract-full" style="display: none;">Voice conversion (VC) transforms source speech into a target voice by preserving the content. However, timbre information from the source speaker is inherently embedded in the content representations, causing significant timbre leakage and reducing similarity to the target speaker. To address this, we introduce a residual block to a content extractor. The residual block consists of two weighted branches: 1) universal semantic dictionary based Content Feature Re-expression (CFR) module, supplying timbre-free content representation. 2) skip connection to the original content layer, providing complementary fine-grained information. In the CFR module, each dictionary entry in the universal semantic dictionary represents a phoneme class, computed statistically using speech from multiple speakers, creating a stable, speaker-independent semantic set. We introduce a CFR method to obtain timbre-free content representations by expressing each content frame as a weighted linear combination of dictionary entries using corresponding phoneme posteriors as weights. Extensive experiments across various VC frameworks demonstrate that our approach effectively mitigates timbre leakage and significantly improves similarity to the target speaker.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8242
            </span>
            <a href="https://arxiv.org/abs/2504.08129" target="_blank" rel="noopener noreferrer">Between Linear and Sinusoidal: Rethinking the Time Encoder in Dynamic Graph Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hsing-Huan Chung, Shravan Chaudhari, Xing Han, Yoav Wald, Suchi Saria, Joydeep Ghosh | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Dynamic graph learning is essential for applications involving temporal networks and requires effective modeling of temporal relationships. Seminal attention-based models like TGAT and DyGFormer rely on sinusoidal time encoders to capture temporal relationships between edge events. In this paper, we</span>
            
            <span class="abstract-full" style="display: none;">Dynamic graph learning is essential for applications involving temporal networks and requires effective modeling of temporal relationships. Seminal attention-based models like TGAT and DyGFormer rely on sinusoidal time encoders to capture temporal relationships between edge events. In this paper, we study a simpler alternative: the linear time encoder, which avoids temporal information loss caused by sinusoidal functions and reduces the need for high dimensional time encoders. We show that the self-attention mechanism can effectively learn to compute time spans from linear time encodings and extract relevant temporal patterns. Through extensive experiments on six dynamic graph datasets, we demonstrate that the linear time encoder improves the performance of TGAT and DyGFormer in most cases. Moreover, the linear time encoder can lead to significant savings in model parameters with minimal performance loss. For example, compared to a 100-dimensional sinusoidal time encoder, TGAT with a 2-dimensional linear time encoder saves 43% of parameters and achieves higher average precision on five datasets. These results can be readily used to positively impact the design choices of a wide variety of dynamic graph learning architectures. The experimental code is available at: https://github.com/hsinghuan/dg-linear-time.git.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8294
            </span>
            <a href="https://arxiv.org/abs/2504.08112" target="_blank" rel="noopener noreferrer">Scaling Laws of Graph Neural Networks for Atomistic Materials Modeling</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chaojian Li, Zhifan Ye, Massimiliano Lupo Pasini, Jong Youl Choi, Cheng Wan, Yingyan Celine Lin, Prasanna Balaprakash | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Atomistic materials modeling is a critical task with wide-ranging applications, from drug discovery to materials science, where accurate predictions of the target material property can lead to significant advancements in scientific discovery. Graph Neural Networks (GNNs) represent the state-of-the-a</span>
            
            <span class="abstract-full" style="display: none;">Atomistic materials modeling is a critical task with wide-ranging applications, from drug discovery to materials science, where accurate predictions of the target material property can lead to significant advancements in scientific discovery. Graph Neural Networks (GNNs) represent the state-of-the-art approach for modeling atomistic material data thanks to their capacity to capture complex relational structures. While machine learning performance has historically improved with larger models and datasets, GNNs for atomistic materials modeling remain relatively small compared to large language models (LLMs), which leverage billions of parameters and terabyte-scale datasets to achieve remarkable performance in their respective domains. To address this gap, we explore the scaling limits of GNNs for atomistic materials modeling by developing a foundational model with billions of parameters, trained on extensive datasets in terabyte-scale. Our approach incorporates techniques from LLM libraries to efficiently manage large-scale data and models, enabling both effective training and deployment of these large-scale GNN models. This work addresses three fundamental questions in scaling GNNs: the potential for scaling GNN model architectures, the effect of dataset size on model accuracy, and the applicability of LLM-inspired techniques to GNN architectures. Specifically, the outcomes of this study include (1) insights into the scaling laws for GNNs, highlighting the relationship between model size, dataset volume, and accuracy, (2) a foundational GNN model optimized for atomistic materials modeling, and (3) a GNN codebase enhanced with advanced LLM-based training techniques. Our findings lay the groundwork for large-scale GNNs with billions of parameters and terabyte-scale datasets, establishing a scalable pathway for future advancements in atomistic materials modeling.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.839
            </span>
            <a href="https://arxiv.org/abs/2504.08431" target="_blank" rel="noopener noreferrer">The Composite Visual-Laser Navigation Method Applied in Indoor Poultry Farming Environments</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiafan Lu, Dongcheng Hu, Yitian Ye, Anqi Liu, Zixian Zhang, Xin Peng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Indoor poultry farms require inspection robots to maintain precise environmental control, which is crucial for preventing the rapid spread of disease and large-scale bird mortality. However, the complex conditions within these facilities, characterized by areas of intense illumination and water accu</span>
            
            <span class="abstract-full" style="display: none;">Indoor poultry farms require inspection robots to maintain precise environmental control, which is crucial for preventing the rapid spread of disease and large-scale bird mortality. However, the complex conditions within these facilities, characterized by areas of intense illumination and water accumulation, pose significant challenges. Traditional navigation methods that rely on a single sensor often perform poorly in such environments, resulting in issues like laser drift and inaccuracies in visual navigation line extraction. To overcome these limitations, we propose a novel composite navigation method that integrates both laser and vision technologies. This approach dynamically computes a fused yaw angle based on the real-time reliability of each sensor modality, thereby eliminating the need for physical navigation lines. Experimental validation in actual poultry house environments demonstrates that our method not only resolves the inherent drawbacks of single-sensor systems, but also significantly enhances navigation precision and operational efficiency. As such, it presents a promising solution for improving the performance of inspection robots in complex indoor poultry farming settings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.848
            </span>
            <a href="https://arxiv.org/abs/2504.07989" target="_blank" rel="noopener noreferrer">Regional Tiny Stories: Using Small Models to Compare Language Learning and Tokenizer Performance</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Nirvan Patil, Malhar Abhay Inamdar, Agnivo Gosai, Guruprasad Pathak, Anish Joshi, Aryan Sagavekar, Anish Joshirao, Raj Dandekar, Rajat Dandekar, Sreedath Panat | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Small Language Models (SLMs) offer efficient alternatives to LLMs for specific domains. The 2023 TinyStories study developed an English dataset that allows SLMs with 1 to 10 million parameters to produce coherent outputs. Our research expands this framework by translating the original dataset into I</span>
            
            <span class="abstract-full" style="display: none;">Small Language Models (SLMs) offer efficient alternatives to LLMs for specific domains. The 2023 TinyStories study developed an English dataset that allows SLMs with 1 to 10 million parameters to produce coherent outputs. Our research expands this framework by translating the original dataset into Indian languages and creating synthetic data using LLMs. We focus on Hindi, Marathi, and Bengali, evaluating SLMs for regional language processing and understanding linguistic complexity. We show that SLMs efficiently process regional languages with significantly fewer parameters than LLMs, providing a complementary framework for ``inference based evaluation" of tokenization strategies and linguistic complexity. Our analysis shows that language-specific tokenizers outperform general-purpose ones for Indian languages. Empirical validations, supported by information-theoretic and morphological analyses, provides fundamental understanding behind the better performance of Hindi models over Marathi and Bengali. Additionally, we show that synthetic datasets outperform translated content for training SLMs. Correlation analyses reveal cross-linguistic patterns and language-specific relationships between creativity, grammatical precision, and narrative completeness. These findings advance both the practical application of SLMs to underserved languages and our theoretical understanding of neural language development.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.4 -->
                
            <!-- Medicine: 7.4 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8547
            </span>
            <a href="https://arxiv.org/abs/2504.08323" target="_blank" rel="noopener noreferrer">Academic Network Representation via Prediction-Sampling Incorporated Tensor Factorization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chunyang Zhang, Xin Liao, Hao Wu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Accurate representation to an academic network is of great significance to academic relationship mining like predicting scientific impact. A Latent Factorization of Tensors (LFT) model is one of the most effective models for learning the representation of a target network. However, an academic netwo</span>
            
            <span class="abstract-full" style="display: none;">Accurate representation to an academic network is of great significance to academic relationship mining like predicting scientific impact. A Latent Factorization of Tensors (LFT) model is one of the most effective models for learning the representation of a target network. However, an academic network is often High-Dimensional and Incomplete (HDI) because the relationships among numerous network entities are impossible to be fully explored, making it difficult for an LFT model to learn accurate representation of the academic network. To address this issue, this paper proposes a Prediction-sampling-based Latent Factorization of Tensors (PLFT) model with two ideas: 1) constructing a cascade LFT architecture to enhance model representation learning ability via learning academic network hierarchical features, and 2) introducing a nonlinear activation-incorporated predicting-sampling strategy to more accurately learn the network representation via generating new academic network data layer by layer. Experimental results from the three real-world academic network datasets show that the PLFT model outperforms existing models when predicting the unexplored relationships among network entities.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.6 -->
                
            <!-- LLMs: 8.3 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.8558
            </span>
            <a href="https://arxiv.org/abs/2503.20286" target="_blank" rel="noopener noreferrer">Bridging Evolutionary Multiobjective Optimization and GPU Acceleration via Tensorization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhenyu Liang, Hao Li, Naiwei Yu, Kebin Sun, Ran Cheng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focuse</span>
            
            <span class="abstract-full" style="display: none;">Evolutionary multiobjective optimization (EMO) has made significant strides over the past two decades. However, as problem scales and complexities increase, traditional EMO algorithms face substantial performance limitations due to insufficient parallelism and scalability. While most work has focused on algorithm design to address these challenges, little attention has been given to hardware acceleration, thereby leaving a clear gap between EMO algorithms and advanced computing devices, such as GPUs. To bridge the gap, we propose to parallelize EMO algorithms on GPUs via the tensorization methodology. By employing tensorization, the data structures and operations of EMO algorithms are transformed into concise tensor representations, which seamlessly enables automatic utilization of GPU computing. We demonstrate the effectiveness of our approach by applying it to three representative EMO algorithms: NSGA-III, MOEA/D, and HypE. To comprehensively assess our methodology, we introduce a multiobjective robot control benchmark using a GPU-accelerated physics engine. Our experiments show that the tensorized EMO algorithms achieve speedups of up to 1113x compared to their CPU-based counterparts, while maintaining solution quality and effectively scaling population sizes to hundreds of thousands. Furthermore, the tensorized EMO algorithms efficiently tackle complex multiobjective robot control tasks, producing high-quality solutions with diverse behaviors. Source codes are available at https://github.com/EMI-Group/evomo.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.5 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9004
            </span>
            <a href="https://arxiv.org/abs/2504.08360" target="_blank" rel="noopener noreferrer">Target Tracking With ISAC Using EMLSR in Next-Generation IEEE 802.11 WLANs: Non-Cooperative and Cooperative Approaches</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ching-Lun Tai, Jingyuan Zhang, Douglas M. Blough, Raghupathy Sivakumar | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">New amendments support Wi-Fi access points (APs) and stations (STAs) in next-generation IEEE 802.11 wireless local area networks (WLANs). IEEE 802.11be (Wi-Fi 7) features multi-link operation (MLO) with multi-link device (MLD) hosting multiple interfaces, highlighting enhanced multi-link single-radi</span>
            
            <span class="abstract-full" style="display: none;">New amendments support Wi-Fi access points (APs) and stations (STAs) in next-generation IEEE 802.11 wireless local area networks (WLANs). IEEE 802.11be (Wi-Fi 7) features multi-link operation (MLO) with multi-link device (MLD) hosting multiple interfaces, highlighting enhanced multi-link single-radio (EMLSR) operation. IEEE 802.11bf features Wi-Fi sensing, enabling integrated sensing and communications (ISAC) in Wi-Fi. In this paper, we pioneer an innovative combination of EMLSR operation and ISAC functionality, considering target tracking with ISAC using EMLSR in IEEE 802.11 WLANs. We establish a unique scenario where AP MLD needs to make ISAC decision and STA MLD selection when its interface gains a transmit opportunity (TXOP). Then, we present key design principles: ISAC decision involves the Kalman filter for target state and a developed time-based strategy for sensing/communications determination, while STA MLD selection involves a Cram\'er-Rao lower bound (CRLB)-based trilateration performance metric along with a developed candidate strategy for UL sensing and involves a developed weighted proportional fairness-aware heuristic strategy for DL communications. We propose novel non-cooperative and cooperative approaches, where each interface leverages its own information and aggregate information across all interfaces, respectively. For proposed non-cooperative and cooperative approaches, simulation results exhibit their tradeoff and superiority about sensing and communications.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Math: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9032
            </span>
            <a href="https://arxiv.org/abs/2504.08727" target="_blank" rel="noopener noreferrer">Visual Chronicles: Using Multimodal LLMs to Analyze Massive Collections of Images</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Boyang Deng, Songyou Peng, Kyle Genova, Gordon Wetzstein, Noah Snavely, Leonidas Guibas, Thomas Funkhouser | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes ("trends") across a city over a certain p</span>
            
            <span class="abstract-full" style="display: none;">We present a system using Multimodal LLMs (MLLMs) to analyze a large database with tens of millions of images captured at different times, with the aim of discovering patterns in temporal changes. Specifically, we aim to capture frequent co-occurring changes ("trends") across a city over a certain period. Unlike previous visual analyses, our analysis answers open-ended queries (e.g., "what are the frequent types of changes in the city?") without any predetermined target subjects or training labels. These properties cast prior learning-based or unsupervised visual analysis tools unsuitable. We identify MLLMs as a novel tool for their open-ended semantic understanding capabilities. Yet, our datasets are four orders of magnitude too large for an MLLM to ingest as context. So we introduce a bottom-up procedure that decomposes the massive visual analysis problem into more tractable sub-problems. We carefully design MLLM-based solutions to each sub-problem. During experiments and ablation studies with our system, we find it significantly outperforms baselines and is able to discover interesting trends from images captured in large cities (e.g., "addition of outdoor dining,", "overpass was painted blue," etc.). See more results and interactive demos at https://boyangdeng.com/visual-chronicles.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9074
            </span>
            <a href="https://arxiv.org/abs/2504.07583" target="_blank" rel="noopener noreferrer">Do LLMs Understand Your Translations? Evaluating Paragraph-level MT with Question Answering</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Patrick Fernandes, Sweta Agrawal, Emmanouil Zaranis, Andr\'e F. T. Martins, Graham Neubig | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Despite the steady progress in machine translation evaluation, existing automatic metrics struggle to capture how well meaning is preserved beyond sentence boundaries. We posit that reliance on a single intrinsic quality score, trained to mimic human judgments, might be insufficient for evaluating t</span>
            
            <span class="abstract-full" style="display: none;">Despite the steady progress in machine translation evaluation, existing automatic metrics struggle to capture how well meaning is preserved beyond sentence boundaries. We posit that reliance on a single intrinsic quality score, trained to mimic human judgments, might be insufficient for evaluating translations of long, complex passages, and a more ``pragmatic'' approach that assesses how accurately key information is conveyed by a translation in context is needed. We introduce TREQA (Translation Evaluation via Question-Answering), a framework that extrinsically evaluates translation quality by assessing how accurately candidate translations answer reading comprehension questions that target key information in the original source or reference texts. In challenging domains that require long-range understanding, such as literary texts, we show that TREQA is competitive with and, in some cases, outperforms state-of-the-art neural and LLM-based metrics in ranking alternative paragraph-level translations, despite never being explicitly optimized to correlate with human judgments. Furthermore, the generated questions and answers offer interpretability: empirical analysis shows that they effectively target translation errors identified by experts in evaluated datasets. Our code is available at https://github.com/deep-spin/treqa</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.2 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9083
            </span>
            <a href="https://arxiv.org/abs/2504.08647" target="_blank" rel="noopener noreferrer">Mind the Gap: The Missing Features of the Tools to Support User Studies in Software Engineering</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: L\'azaro Costa, Susana Barbosa, J\'acome Cunha | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">User studies are paramount for advancing science. However, researchers face several barriers when performing them despite the existence of supporting tools.</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9096
            </span>
            <a href="https://arxiv.org/abs/2504.08334" target="_blank" rel="noopener noreferrer">Efficient Architecture for RISC-V Vector Memory Access</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hongyi Guan, Yichuan Gao, Chenlu Miao, Haoyang Wu, Hang Zhu, Mingfeng Lin, Huayue Liang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Vector processors frequently suffer from inefficient memory accesses, particularly for strided and segment patterns. While coalescing strided accesses is a natural solution, effectively gathering or scattering elements at fixed strides remains challenging. Naive approaches rely on high-overhead cros</span>
            
            <span class="abstract-full" style="display: none;">Vector processors frequently suffer from inefficient memory accesses, particularly for strided and segment patterns. While coalescing strided accesses is a natural solution, effectively gathering or scattering elements at fixed strides remains challenging. Naive approaches rely on high-overhead crossbars that remap any byte between memory and registers, leading to physical design issues. Segment operations require row-column transpositions, typically handled using either element-level in-place transposition (degrading performance) or large buffer-based bulk transposition (incurring high area overhead). In this paper, we present EARTH, a novel vector memory access architecture designed to overcome these challenges through shifting-based optimizations. For strided accesses, EARTH integrates specialized shift networks for gathering and scattering elements. After coalescing multiple accesses within the same cache line, data is routed between memory and registers through the shifting network with minimal overhead. For segment operations, EARTH employs a shifted register bank enabling direct column-wise access, eliminating dedicated segment buffers while providing high-performance, in-place bulk transposition. Implemented on FPGA with Chisel HDL based on an open-source RISC-V vector unit, EARTH enhances performance for strided memory accesses, achieving 4x-8x speedups in benchmarks dominated by strided operations. Compared to conventional designs, EARTH reduces hardware area by 9% and power consumption by 41%, significantly advancing both performance and efficiency of vector processors.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.9 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9152
            </span>
            <a href="https://arxiv.org/abs/2504.07971" target="_blank" rel="noopener noreferrer">SPHERE: An Evaluation Card for Human-AI Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qianou Ma, Dora Zhao, Xinran Zhao, Chenglei Si, Chenyang Yang, Ryan Louie, Ehud Reiter, Diyi Yang, Tongshuang Wu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In the era of Large Language Models (LLMs), establishing effective evaluation methods and standards for diverse human-AI interaction systems is increasingly challenging. To encourage more transparent documentation and facilitate discussion on human-AI system evaluation design options, we present an </span>
            
            <span class="abstract-full" style="display: none;">In the era of Large Language Models (LLMs), establishing effective evaluation methods and standards for diverse human-AI interaction systems is increasingly challenging. To encourage more transparent documentation and facilitate discussion on human-AI system evaluation design options, we present an evaluation card SPHERE, which encompasses five key dimensions: 1) What is being evaluated?; 2) How is the evaluation conducted?; 3) Who is participating in the evaluation?; 4) When is evaluation conducted?; 5) How is evaluation validated? We conduct a review of 39 human-AI systems using SPHERE, outlining current evaluation practices and areas for improvement. We provide three recommendations for improving the validity and rigor of evaluation practices.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.2 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9154
            </span>
            <a href="https://arxiv.org/abs/2410.22816" target="_blank" rel="noopener noreferrer">Advancing Manipulation Capabilities of a UAV Featuring Dynamic Center-of-Mass Displacement</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tong Hui, Matteo Fumagalli | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As aerial robots gain traction in industrial applications, there is growing interest in enhancing their physical interaction capabilities. Pushing tasks performed by aerial manipulators have been successfully demonstrated in contact-based inspections. However, more complex industrial applications re</span>
            
            <span class="abstract-full" style="display: none;">As aerial robots gain traction in industrial applications, there is growing interest in enhancing their physical interaction capabilities. Pushing tasks performed by aerial manipulators have been successfully demonstrated in contact-based inspections. However, more complex industrial applications require these systems to support higher-DoF (Degree of Freedom) manipulators and generate larger forces while pushing (e.g., drilling, grinding). This paper builds on our previous work, where we introduced an aerial vehicle that can dynamically vary its CoM (Center of Mass) location to improve force exertion during interactions. We propose a novel approach to further enhance this system's force generation by optimizing its CoM location during interactions. Additionally, we study the case of this aerial vehicle equipped with a 2-DoF manipulation arm to extend the system's functionality in tool-based tasks. The effectiveness of the proposed methods is validated through simulations, demonstrating the potential of this system for advanced aerial manipulation in practical settings.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.6 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9162
            </span>
            <a href="https://arxiv.org/abs/2504.08134" target="_blank" rel="noopener noreferrer">Hybrid Reinforcement Learning-based Sustainable Multi-User Computation Offloading for Mobile Edge-Quantum Computing</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Minrui Xu (Sherman), Dusit Niyato (Sherman), Jiawen Kang (Sherman), Zehui Xiong (Sherman), Mingzhe Chen (Sherman), Dong In Kim (Sherman), Xuemin (Sherman), Shen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Exploiting quantum computing at the mobile edge holds immense potential for facilitating large-scale network design, processing multimodal data, optimizing resource management, and enhancing network security. In this paper, we propose a pioneering paradigm of mobile edge quantum computing (MEQC) tha</span>
            
            <span class="abstract-full" style="display: none;">Exploiting quantum computing at the mobile edge holds immense potential for facilitating large-scale network design, processing multimodal data, optimizing resource management, and enhancing network security. In this paper, we propose a pioneering paradigm of mobile edge quantum computing (MEQC) that integrates quantum computing capabilities into classical edge computing servers that are proximate to mobile devices. To conceptualize the MEQC, we first design an MEQC system, where mobile devices can offload classical and quantum computation tasks to edge servers equipped with classical and quantum computers. We then formulate the hybrid classical-quantum computation offloading problem whose goal is to minimize system cost in terms of latency and energy consumption. To solve the offloading problem efficiently, we propose a hybrid discrete-continuous multi-agent reinforcement learning algorithm to learn long-term sustainable offloading and partitioning strategies. Finally, numerical results demonstrate that the proposed algorithm can reduce the MEQC system cost by up to 30% compared to existing baselines.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.3 -->
                
            <!-- Medicine: 6.9 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.92
            </span>
            <a href="https://arxiv.org/abs/2504.03767" target="_blank" rel="noopener noreferrer">MCP Safety Audit: LLMs with the Model Context Protocol Allow Major Security Exploits</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Brandon Radosevich, John Halloran | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API</span>
            
            <span class="abstract-full" style="display: none;">To reduce development overhead and enable seamless integration between potential components comprising any given generative AI application, the Model Context Protocol (MCP) (Anthropic, 2024) has recently been released and subsequently widely adopted. The MCP is an open protocol that standardizes API calls to large language models (LLMs), data sources, and agentic tools. By connecting multiple MCP servers, each defined with a set of tools, resources, and prompts, users are able to define automated workflows fully driven by LLMs. However, we show that the current MCP design carries a wide range of security risks for end users. In particular, we demonstrate that industry-leading LLMs may be coerced into using MCP tools to compromise an AI developer's system through various attacks, such as malicious code execution, remote access control, and credential theft. To proactively mitigate these and related attacks, we introduce a safety auditing tool, MCPSafetyScanner, the first agentic tool to assess the security of an arbitrary MCP server. MCPScanner uses several agents to (a) automatically determine adversarial samples given an MCP server's tools and resources; (b) search for related vulnerabilities and remediations based on those samples; and (c) generate a security report detailing all findings. Our work highlights serious security issues with general-purpose agentic workflows while also providing a proactive tool to audit MCP server safety and address detected vulnerabilities before deployment.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9358
            </span>
            <a href="https://arxiv.org/abs/2504.08044" target="_blank" rel="noopener noreferrer">Large-Scale Analysis of Online Questions Related to Opioid Use Disorder on Reddit</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tanmay Laud, Akadia Kacha-Ochana, Steven A. Sumner, Vikram Krishnasamy, Royal Law, Lyna Schieber, Munmun De Choudhury, Mai ElSherief | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Opioid use disorder (OUD) is a leading health problem that affects individual well-being as well as general public health. Due to a variety of reasons, including the stigma faced by people using opioids, online communities for recovery and support were formed on different social media platforms. In </span>
            
            <span class="abstract-full" style="display: none;">Opioid use disorder (OUD) is a leading health problem that affects individual well-being as well as general public health. Due to a variety of reasons, including the stigma faced by people using opioids, online communities for recovery and support were formed on different social media platforms. In these communities, people share their experiences and solicit information by asking questions to learn about opioid use and recovery. However, these communities do not always contain clinically verified information. In this paper, we study natural language questions asked in the context of OUD-related discourse on Reddit. We adopt transformer-based question detection along with hierarchical clustering across 19 subreddits to identify six coarse-grained categories and 69 fine-grained categories of OUD-related questions. Our analysis uncovers ten areas of information seeking from Reddit users in the context of OUD: drug sales, specific drug-related questions, OUD treatment, drug uses, side effects, withdrawal, lifestyle, drug testing, pain management and others, during the study period of 2018-2021. Our work provides a major step in improving the understanding of OUD-related questions people ask unobtrusively on Reddit. We finally discuss technological interventions and public health harm reduction techniques based on the topics of these questions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9375
            </span>
            <a href="https://arxiv.org/abs/2410.03887" target="_blank" rel="noopener noreferrer">Solving Dual Sourcing Problems with Supply Mode Dependent Failure Rates</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fabian Akkerman, Nils Knofius, Matthieu van der Heijden, Martijn Mes | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper investigates dual sourcing problems with supply mode dependent failure rates, particularly relevant in managing spare parts for downtime-critical assets. To enhance resilience, businesses increasingly adopt dual sourcing strategies using both conventional and additive manufacturing techni</span>
            
            <span class="abstract-full" style="display: none;">This paper investigates dual sourcing problems with supply mode dependent failure rates, particularly relevant in managing spare parts for downtime-critical assets. To enhance resilience, businesses increasingly adopt dual sourcing strategies using both conventional and additive manufacturing techniques. This paper explores how these strategies can optimise sourcing by addressing variations in part properties and failure rates. A significant challenge is the distinct failure characteristics of parts produced by these methods, which influence future demand. To tackle this, we propose a new iterative heuristic and several reinforcement learning techniques combined with an endogenous parameterised learning (EPL) approach. This EPL approach - compatible with any learning method - allows a single policy to handle various input parameters for multiple items. In a stylised setting, our best policy achieves an average optimality gap of 0.4%. In a case study within the energy sector, our policies outperform the baseline in 91.1% of instances, yielding average cost savings up to 22.6%.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9454
            </span>
            <a href="https://arxiv.org/abs/2312.05114" target="_blank" rel="noopener noreferrer">The Inadequacy of Similarity-based Privacy Metrics: Privacy Attacks against "Truly Anonymous" Synthetic Datasets</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Georgi Ganev, Emiliano De Cristofaro | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative models producing synthetic data are meant to provide a privacy-friendly approach to releasing data. However, their privacy guarantees are only considered robust when models satisfy Differential Privacy (DP). Alas, this is not a ubiquitous standard, as many leading companies (and, in fact,</span>
            
            <span class="abstract-full" style="display: none;">Generative models producing synthetic data are meant to provide a privacy-friendly approach to releasing data. However, their privacy guarantees are only considered robust when models satisfy Differential Privacy (DP). Alas, this is not a ubiquitous standard, as many leading companies (and, in fact, research papers) use ad-hoc privacy metrics based on testing the statistical similarity between synthetic and real data.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9482
            </span>
            <a href="https://arxiv.org/abs/2504.08140" target="_blank" rel="noopener noreferrer">Impact of Language Guidance: A Reproducibility Study</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Cherish Puniani, Advika Sinha, Shree Singhi, Aayan Yadav | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Modern deep-learning architectures need large amounts of data to produce state-of-the-art results. Annotating such huge datasets is time-consuming, expensive, and prone to human error. Recent advances in self-supervised learning allow us to train huge models without explicit annotation. Contrastive </span>
            
            <span class="abstract-full" style="display: none;">Modern deep-learning architectures need large amounts of data to produce state-of-the-art results. Annotating such huge datasets is time-consuming, expensive, and prone to human error. Recent advances in self-supervised learning allow us to train huge models without explicit annotation. Contrastive learning is a popular paradigm in self-supervised learning. Recent works like SimCLR and CLIP rely on image augmentations or directly minimizing cross-modal loss between image and text. Banani et al. (2023) propose to use language guidance to sample view pairs. They claim that language enables better conceptual similarity, eliminating the effects of visual variability. We reproduce their experiments to verify their claims and find that their dataset, RedCaps, contains low-quality captions. We use an off-the-shelf image captioning model, BLIP-2, to replace the captions and improve performance, and we also devise a new metric to evaluate the semantic capabilities of self-supervised models based on interpretability methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.4 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9494
            </span>
            <a href="https://arxiv.org/abs/2504.08000" target="_blank" rel="noopener noreferrer">Neuron-level Balance between Stability and Plasticity in Deep Reinforcement Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiahua Lan, Sen Zhang, Haixia Pan, Ruijun Liu, Li Shen, Dacheng Tao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In contrast to the human ability to continuously acquire knowledge, agents struggle with the stability-plasticity dilemma in deep reinforcement learning (DRL), which refers to the trade-off between retaining existing skills (stability) and learning new knowledge (plasticity). Current methods focus o</span>
            
            <span class="abstract-full" style="display: none;">In contrast to the human ability to continuously acquire knowledge, agents struggle with the stability-plasticity dilemma in deep reinforcement learning (DRL), which refers to the trade-off between retaining existing skills (stability) and learning new knowledge (plasticity). Current methods focus on balancing these two aspects at the network level, lacking sufficient differentiation and fine-grained control of individual neurons. To overcome this limitation, we propose Neuron-level Balance between Stability and Plasticity (NBSP) method, by taking inspiration from the observation that specific neurons are strongly relevant to task-relevant skills. Specifically, NBSP first (1) defines and identifies RL skill neurons that are crucial for knowledge retention through a goal-oriented method, and then (2) introduces a framework by employing gradient masking and experience replay techniques targeting these neurons to preserve the encoded existing skills while enabling adaptation to new tasks. Numerous experimental results on the Meta-World and Atari benchmarks demonstrate that NBSP significantly outperforms existing approaches in balancing stability and plasticity.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9504
            </span>
            <a href="https://arxiv.org/abs/2503.15812" target="_blank" rel="noopener noreferrer">Data Spatial Programming</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jason Mars | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We introduce a novel programming model, Data Spatial Programming, which extends the semantics of Object-Oriented Programming (OOP) by introducing new class-like constructs called archetypes. These archetypes encapsulate the topological relationships between data entities and the execution flow in a </span>
            
            <span class="abstract-full" style="display: none;">We introduce a novel programming model, Data Spatial Programming, which extends the semantics of Object-Oriented Programming (OOP) by introducing new class-like constructs called archetypes. These archetypes encapsulate the topological relationships between data entities and the execution flow in a structured manner, enabling more expressive and semantically rich computations over interconnected data structures or finite states. By formalizing the relationships between data elements in this topological space, our approach allows for more intuitive modeling of complex systems where a topology of connections is formed for the underlying computational model. This paradigm addresses limitations in traditional OOP when representing a wide range of problems in computer science such as agent-based systems, social networks, processing on relational data, neural networks, distributed systems, finite state machines, and other spatially-oriented computational problems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9542
            </span>
            <a href="https://arxiv.org/abs/2504.08575" target="_blank" rel="noopener noreferrer">Prophecies all the Way: Game-based Model-Checking for HyperQPTL beyond $\forall^*\exists^*$</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sarah Winter, Martin Zimmermann | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Model-checking HyperLTL, a temporal logic expressing properties of sets of traces with applications to information-flow based security and privacy, has a decidable, but TOWER-complete, model-checking problem. In the classical algorithm, the complexity manifests itself with a need for the complementa</span>
            
            <span class="abstract-full" style="display: none;">Model-checking HyperLTL, a temporal logic expressing properties of sets of traces with applications to information-flow based security and privacy, has a decidable, but TOWER-complete, model-checking problem. In the classical algorithm, the complexity manifests itself with a need for the complementation of automata over infinite words. To overcome this aforementioned need, a game-based alternative for the $\forall^*\exists^*$-fragment was recently presented.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.5 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9567
            </span>
            <a href="https://arxiv.org/abs/2504.08240" target="_blank" rel="noopener noreferrer">InSPE: Rapid Evaluation of Heterogeneous Multi-Modal Infrastructure Sensor Placement</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhaoliang Zheng, Yun Zhang, Zongling Meng, Johnson Liu, Xin Xia, Jiaqi Ma | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Infrastructure sensing is vital for traffic monitoring at safety hotspots (e.g., intersections) and serves as the backbone of cooperative perception in autonomous driving. While vehicle sensing has been extensively studied, infrastructure sensing has received little attention, especially given the u</span>
            
            <span class="abstract-full" style="display: none;">Infrastructure sensing is vital for traffic monitoring at safety hotspots (e.g., intersections) and serves as the backbone of cooperative perception in autonomous driving. While vehicle sensing has been extensively studied, infrastructure sensing has received little attention, especially given the unique challenges of diverse intersection geometries, complex occlusions, varying traffic conditions, and ambient environments like lighting and weather. To address these issues and ensure cost-effective sensor placement, we propose Heterogeneous Multi-Modal Infrastructure Sensor Placement Evaluation (InSPE), a perception surrogate metric set that rapidly assesses perception effectiveness across diverse infrastructure and environmental scenarios with combinations of multi-modal sensors. InSPE systematically evaluates perception capabilities by integrating three carefully designed metrics, i.e., sensor coverage, perception occlusion, and information gain. To support large-scale evaluation, we develop a data generation tool within the CARLA simulator and also introduce Infra-Set, a dataset covering diverse intersection types and environmental conditions. Benchmarking experiments with state-of-the-art perception algorithms demonstrate that InSPE enables efficient and scalable sensor placement analysis, providing a robust solution for optimizing intelligent intersection infrastructure.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 7.6 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9581
            </span>
            <a href="https://arxiv.org/abs/2504.08166" target="_blank" rel="noopener noreferrer">Learning Object Focused Attention</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vivek Trivedy, Amani Almalki, Longin Jan Latecki | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose an adaptation to the training of Vision Transformers (ViTs) that allows for an explicit modeling of objects during the attention computation. This is achieved by adding a new branch to selected attention layers that computes an auxiliary loss which we call the object-focused attention (OF</span>
            
            <span class="abstract-full" style="display: none;">We propose an adaptation to the training of Vision Transformers (ViTs) that allows for an explicit modeling of objects during the attention computation. This is achieved by adding a new branch to selected attention layers that computes an auxiliary loss which we call the object-focused attention (OFA) loss. We restrict the attention to image patches that belong to the same object class, which allows ViTs to gain a better understanding of configural (or holistic) object shapes by focusing on intra-object patches instead of other patches such as those in the background. Our proposed inductive bias fits easily into the attention framework of transformers since it only adds an auxiliary loss over selected attention layers. Furthermore, our approach has no additional overhead during inference. We also experiment with multiscale masking to further improve the performance of our OFA model and give a path forward for self-supervised learning with our method. Our experimental results demonstrate that ViTs with OFA achieve better classification results than their base models, exhibit a stronger generalization ability to out-of-distribution (OOD) and adversarially corrupted images, and learn representations based on object shapes rather than spurious correlations via general textures. For our OOD setting, we generate a novel dataset using the COCO dataset and Stable Diffusion inpainting which we plan to share with the community.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9597
            </span>
            <a href="https://arxiv.org/abs/2504.08664" target="_blank" rel="noopener noreferrer">The Steenrod squares via unordered joins</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Axel Ljungstr\"om, David W\"arn | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The Steenrod squares are cohomology operations with important applications in algebraic topology. While these operations are well-understood classically, little is known about them in the setting of homotopy type theory. Although a definition of the Steenrod squares was put forward by Brunerie (2017</span>
            
            <span class="abstract-full" style="display: none;">The Steenrod squares are cohomology operations with important applications in algebraic topology. While these operations are well-understood classically, little is known about them in the setting of homotopy type theory. Although a definition of the Steenrod squares was put forward by Brunerie (2017), proofs of their characterising properties have remained elusive. In this paper, we revisit Brunerie's definition and provide proofs of these properties, including stability, Cartan's formula and the Adem relations. This is done by studying a higher inductive type called the unordered join. This approach is inherently synthetic and, consequently, many of our proofs differ significantly from their classical counterparts. Along the way, we discuss upshots and limitations of homotopy type theory as a synthetic language for homotopy theory. The paper is accompanied by a computer formalisation in Cubical Agda.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9618
            </span>
            <a href="https://arxiv.org/abs/2503.15711" target="_blank" rel="noopener noreferrer">VPAL: A novel method to reduce reconstruction time for 5D free-running imaging</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yitong Yang, Muhammad Naeem, Marly Van Assen, Jerome Yerly, Davide Piccini, Matthias Stuber, John Oshinski, Matthias Chung | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Purpose: Ferumoxytal-enhanced 5D free-running whole heart CMR provides image quality comparable to CTA, but requires hours-long reconstruction time, preventing clinical usage. This study developed a variable projection augmented Lagrangian (VPAL) method for 5D motion-resolved image reconstruction an</span>
            
            <span class="abstract-full" style="display: none;">Purpose: Ferumoxytal-enhanced 5D free-running whole heart CMR provides image quality comparable to CTA, but requires hours-long reconstruction time, preventing clinical usage. This study developed a variable projection augmented Lagrangian (VPAL) method for 5D motion-resolved image reconstruction and compared it with alternating direction method of multipliers (ADMM) in five numerical simulations and 15 in-vivo pediatric data set.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9647
            </span>
            <a href="https://arxiv.org/abs/2504.08169" target="_blank" rel="noopener noreferrer">On the Practice of Deep Hierarchical Ensemble Network for Ad Conversion Rate Prediction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jinfeng Zhuang, Yinrui Li, Runze Su, Ke Xu, Zhixuan Shao, Kungang Li, Ling Leng, Han Sun, Meng Qi, Yixiong Meng, Yang Tang, Zhifang Liu, Qifei Shen, Aayush Mudgal | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The predictions of click through rate (CTR) and conversion rate (CVR) play a crucial role in the success of ad-recommendation systems. A Deep Hierarchical Ensemble Network (DHEN) has been proposed to integrate multiple feature crossing modules and has achieved great success in CTR prediction. Howeve</span>
            
            <span class="abstract-full" style="display: none;">The predictions of click through rate (CTR) and conversion rate (CVR) play a crucial role in the success of ad-recommendation systems. A Deep Hierarchical Ensemble Network (DHEN) has been proposed to integrate multiple feature crossing modules and has achieved great success in CTR prediction. However, its performance for CVR prediction is unclear in the conversion ads setting, where an ad bids for the probability of a user's off-site actions on a third party website or app, including purchase, add to cart, sign up, etc. A few challenges in DHEN: 1) What feature-crossing modules (MLP, DCN, Transformer, to name a few) should be included in DHEN? 2) How deep and wide should DHEN be to achieve the best trade-off between efficiency and efficacy? 3) What hyper-parameters to choose in each feature-crossing module? Orthogonal to the model architecture, the input personalization features also significantly impact model performance with a high degree of freedom. In this paper, we attack this problem and present our contributions biased to the applied data science side, including:</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9653
            </span>
            <a href="https://arxiv.org/abs/2504.08685" target="_blank" rel="noopener noreferrer">Seaweed-7B: Cost-Effective Training of Video Generation Foundation Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, Feng Cheng, Feilong Zuo Xuejiao Zeng, Ziyan Yang, Fangyuan Kong, Zhiwu Qing, Fei Xiao, Meng Wei, Tuyen Hoang, Siyu Zhang, Peihao Zhu, Qi Zhao, Jiangqiao Yan, Liangke Gui, Sheng Bi, Jiashi Li, Yuxi Ren, Rui Wang, Huixia Li, Xuefeng Xiao, Shu Liu, Feng Ling, Heng Zhang, Houmin Wei, Huafeng Kuang, Jerry Duncan, Junda Zhang, Junru Zheng, Li Sun, Manlin Zhang, Renfei Sun, Xiaobin Zhuang, Xiaojie Li, Xin Xia, Xuyan Chi, Yanghua Peng, Yuping Wang, Yuxuan Wang, Zhongkai Zhao, Zhuo Chen, Zuquan Song, Zhenheng Yang, Jiashi Feng, Jianchao Yang, Lu Jiang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate compu</span>
            
            <span class="abstract-full" style="display: none;">This technical report presents a cost-efficient strategy for training a video generation foundation model. We present a mid-sized research model with approximately 7 billion parameters (7B) called Seaweed-7B trained from scratch using 665,000 H100 GPU hours. Despite being trained with moderate computational resources, Seaweed-7B demonstrates highly competitive performance compared to contemporary video generation models of much larger size. Design choices are especially crucial in a resource-constrained setting. This technical report highlights the key design decisions that enhance the performance of the medium-sized diffusion model. Empirically, we make two observations: (1) Seaweed-7B achieves performance comparable to, or even surpasses, larger models trained on substantially greater GPU resources, and (2) our model, which exhibits strong generalization ability, can be effectively adapted across a wide range of downstream applications either by lightweight fine-tuning or continue training. See the project page at https://seaweed.video/</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.9687
            </span>
            <a href="https://arxiv.org/abs/2504.08066" target="_blank" rel="noopener noreferrer">The AI Scientist-v2: Workshop-Level Automated Scientific Discovery via Agentic Tree Search</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yutaro Yamada, Robert Tjarko Lange, Cong Lu, Shengran Hu, Chris Lu, Jakob Foerster, Jeff Clune, David Ha | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypoth</span>
            
            <span class="abstract-full" style="display: none;">AI is increasingly playing a pivotal role in transforming how scientific discoveries are made. We introduce The AI Scientist-v2, an end-to-end agentic system capable of producing the first entirely AI generated peer-review-accepted workshop paper. This system iteratively formulates scientific hypotheses, designs and executes experiments, analyzes and visualizes data, and autonomously authors scientific manuscripts. Compared to its predecessor (v1, Lu et al., 2024 arXiv:2408.06292), The AI Scientist-v2 eliminates the reliance on human-authored code templates, generalizes effectively across diverse machine learning domains, and leverages a novel progressive agentic tree-search methodology managed by a dedicated experiment manager agent. Additionally, we enhance the AI reviewer component by integrating a Vision-Language Model (VLM) feedback loop for iterative refinement of content and aesthetics of the figures. We evaluated The AI Scientist-v2 by submitting three fully autonomous manuscripts to a peer-reviewed ICLR workshop. Notably, one manuscript achieved high enough scores to exceed the average human acceptance threshold, marking the first instance of a fully AI-generated paper successfully navigating a peer review. This accomplishment highlights the growing capability of AI in conducting all aspects of scientific research. We anticipate that further advancements in autonomous scientific discovery technologies will profoundly impact human knowledge generation, enabling unprecedented scalability in research productivity and significantly accelerating scientific breakthroughs, greatly benefiting society at large. We have open-sourced the code at https://github.com/SakanaAI/AI-Scientist-v2 to foster the future development of this transformative technology. We also discuss the role of AI in science, including AI safety.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.1 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.986
            </span>
            <a href="https://arxiv.org/abs/2504.08006" target="_blank" rel="noopener noreferrer">A Python toolkit for dealing with Petri nets over ontological graphs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Krzysztof Pancerz | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present theoretical rudiments of Petri nets over ontological graphs as well as the designed and implemented Python toolkit for dealing with such nets. In Petri nets over ontological graphs, the domain knowledge is enclosed in a form of ontologies. In this way, some valuable knowledge (especially </span>
            
            <span class="abstract-full" style="display: none;">We present theoretical rudiments of Petri nets over ontological graphs as well as the designed and implemented Python toolkit for dealing with such nets. In Petri nets over ontological graphs, the domain knowledge is enclosed in a form of ontologies. In this way, some valuable knowledge (especially in terms of semantic relations) can be added to model reasoning and control processes by means of Petri nets. In the implemented approach, ontological graphs are obtained from ontologies built in accordance with the OWL 2 Web Ontology Language. The implemented tool enables the users to define the structure and dynamics of Petri nets over ontological graphs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.6 -->
                
            <!-- LLMs: 8.4 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 1.9 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -6.996
            </span>
            <a href="https://arxiv.org/abs/2503.20102" target="_blank" rel="noopener noreferrer">Extendable Long-Horizon Planning via Hierarchical Multiscale Diffusion</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chang Chen, Hany Hamed, Doojin Baek, Taegu Kang, Yoshua Bengio, Sungjin Ahn | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper tackles a novel problem, extendable long-horizon planning-enabling agents to plan trajectories longer than those in training data without compounding errors. To tackle this, we propose the Hierarchical Multiscale Diffuser (HM-Diffuser) and Progressive Trajectory Extension (PTE), an augmen</span>
            
            <span class="abstract-full" style="display: none;">This paper tackles a novel problem, extendable long-horizon planning-enabling agents to plan trajectories longer than those in training data without compounding errors. To tackle this, we propose the Hierarchical Multiscale Diffuser (HM-Diffuser) and Progressive Trajectory Extension (PTE), an augmentation method that iteratively generates longer trajectories by stitching shorter ones. HM-Diffuser trains on these extended trajectories using a hierarchical structure, efficiently handling tasks across multiple temporal scales. Additionally, we introduce Adaptive Plan Pondering and the Recursive HM-Diffuser, which consolidate hierarchical layers into a single model to process temporal scales recursively. Experimental results demonstrate the effectiveness of our approach, advancing diffusion-based planners for scalable long-horizon planning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0056
            </span>
            <a href="https://arxiv.org/abs/2504.08452" target="_blank" rel="noopener noreferrer">Road Grip Uncertainty Estimation Through Surface State Segmentation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jyri Maanp\"a\"a, Julius Pesonen, Iaroslav Melekhov, Heikki Hyyti, Juha Hyypp\"a | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Slippery road conditions pose significant challenges for autonomous driving. Beyond predicting road grip, it is crucial to estimate its uncertainty reliably to ensure safe vehicle control. In this work, we benchmark several uncertainty prediction methods to assess their effectiveness for grip uncert</span>
            
            <span class="abstract-full" style="display: none;">Slippery road conditions pose significant challenges for autonomous driving. Beyond predicting road grip, it is crucial to estimate its uncertainty reliably to ensure safe vehicle control. In this work, we benchmark several uncertainty prediction methods to assess their effectiveness for grip uncertainty estimation. Additionally, we propose a novel approach that leverages road surface state segmentation to predict grip uncertainty. Our method estimates a pixel-wise grip probability distribution based on inferred road surface conditions. Experimental results indicate that the proposed approach enhances the robustness of grip uncertainty prediction.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0093
            </span>
            <a href="https://arxiv.org/abs/2504.07983" target="_blank" rel="noopener noreferrer">Psychological Health Knowledge-Enhanced LLM-based Social Network Crisis Intervention Text Transfer Recognition Method</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Shurui Wu, Xinyi Huang, Dingxin Lu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As the prevalence of mental health crises increases on social media platforms, identifying and preventing potential harm has become an urgent challenge. This study introduces a large language model (LLM)-based text transfer recognition method for social network crisis intervention, enhanced with dom</span>
            
            <span class="abstract-full" style="display: none;">As the prevalence of mental health crises increases on social media platforms, identifying and preventing potential harm has become an urgent challenge. This study introduces a large language model (LLM)-based text transfer recognition method for social network crisis intervention, enhanced with domain-specific mental health knowledge. We propose a multi-level framework that incorporates transfer learning using BERT, and integrates mental health knowledge, sentiment analysis, and behavior prediction techniques. The framework includes a crisis annotation tool trained on social media datasets from real-world events, enabling the model to detect nuanced emotional cues and identify psychological crises. Experimental results show that the proposed method outperforms traditional models in crisis detection accuracy and exhibits greater sensitivity to subtle emotional and contextual variations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.5 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0126
            </span>
            <a href="https://arxiv.org/abs/2504.08046" target="_blank" rel="noopener noreferrer">Teaching Humans Subtle Differences with DIFFusion</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mia Chiquier, Orr Avrech, Yossi Gandelsman, Berthy Feng, Katherine Bouman, Carl Vondrick | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Human expertise depends on the ability to recognize subtle visual differences, such as distinguishing diseases, species, or celestial phenomena. We propose a new method to teach novices how to differentiate between nuanced categories in specialized domains. Our method uses generative models to visua</span>
            
            <span class="abstract-full" style="display: none;">Human expertise depends on the ability to recognize subtle visual differences, such as distinguishing diseases, species, or celestial phenomena. We propose a new method to teach novices how to differentiate between nuanced categories in specialized domains. Our method uses generative models to visualize the minimal change in features to transition between classes, i.e., counterfactuals, and performs well even in domains where data is sparse, examples are unpaired, and category boundaries are not easily explained by text. By manipulating the conditioning space of diffusion models, our proposed method DIFFusion disentangles category structure from instance identity, enabling high-fidelity synthesis even in challenging domains. Experiments across six domains show accurate transitions even with limited and unpaired examples across categories. User studies confirm that our generated counterfactuals outperform unpaired examples in teaching perceptual expertise, showing the potential of generative models for specialized visual learning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.3 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0283
            </span>
            <a href="https://arxiv.org/abs/2504.04893" target="_blank" rel="noopener noreferrer">SCAM: A Real-World Typographic Robustness Evaluation for Multimodal Foundation Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Justus Westerhoff, Erblina Purelku, Jakob Hackstein, Leo Pinetzki, Lorenz Hufe | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Typographic attacks exploit the interplay between text and visual content in multimodal foundation models, causing misclassifications when misleading text is embedded within images. However, existing datasets are limited in size and diversity, making it difficult to study such vulnerabilities. In th</span>
            
            <span class="abstract-full" style="display: none;">Typographic attacks exploit the interplay between text and visual content in multimodal foundation models, causing misclassifications when misleading text is embedded within images. However, existing datasets are limited in size and diversity, making it difficult to study such vulnerabilities. In this paper, we introduce SCAM, the largest and most diverse dataset of real-world typographic attack images to date, containing 1,162 images across hundreds of object categories and attack words. Through extensive benchmarking of Vision-Language Models (VLMs) on SCAM, we demonstrate that typographic attacks significantly degrade performance, and identify that training data and model architecture influence the susceptibility to these attacks. Our findings reveal that typographic attacks persist in state-of-the-art Large Vision-Language Models (LVLMs) due to the choice of their vision encoder, though larger Large Language Models (LLMs) backbones help mitigate their vulnerability. Additionally, we demonstrate that synthetic attacks closely resemble real-world (handwritten) attacks, validating their use in research. Our work provides a comprehensive resource and empirical insights to facilitate future research toward robust and trustworthy multimodal AI systems. We publicly release the datasets introduced in this paper under https://huggingface.co/datasets/BLISS-e-V/SCAM, along with the code for evaluations at https://github.com/Bliss-e-V/SCAM.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 12.8 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0329
            </span>
            <a href="https://arxiv.org/abs/2504.08175" target="_blank" rel="noopener noreferrer">Multi-person Physics-based Pose Estimation for Combat Sports</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hossein Feiz, David Labb\'e, Thomas Romeas, Jocelyn Faubert, Sheldon Andrews | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose a novel framework for accurate 3D human pose estimation in combat sports using sparse multi-camera setups. Our method integrates robust multi-view 2D pose tracking via a transformer-based top-down approach, employing epipolar geometry constraints and long-term video object segmentation fo</span>
            
            <span class="abstract-full" style="display: none;">We propose a novel framework for accurate 3D human pose estimation in combat sports using sparse multi-camera setups. Our method integrates robust multi-view 2D pose tracking via a transformer-based top-down approach, employing epipolar geometry constraints and long-term video object segmentation for consistent identity tracking across views. Initial 3D poses are obtained through weighted triangulation and spline smoothing, followed by kinematic optimization to refine pose accuracy. We further enhance pose realism and robustness by introducing a multi-person physics-based trajectory optimization step, effectively addressing challenges such as rapid motions, occlusions, and close interactions. Experimental results on diverse datasets, including a new benchmark of elite boxing footage, demonstrate state-of-the-art performance. Additionally, we release comprehensive annotated video datasets to advance future research in multi-person pose estimation for combat sports.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.043
            </span>
            <a href="https://arxiv.org/abs/2504.08531" target="_blank" rel="noopener noreferrer">Embodied Image Captioning: Self-supervised Learning Agents for Spatially Coherent Image Descriptions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tommaso Galliena, Tommaso Apicella, Stefano Rosa, Pietro Morerio, Alessio Del Bue, Lorenzo Natale | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a</span>
            
            <span class="abstract-full" style="display: none;">We present a self-supervised method to improve an agent's abilities in describing arbitrary objects while actively exploring a generic environment. This is a challenging problem, as current models struggle to obtain coherent image captions due to different camera viewpoints and clutter. We propose a three-phase framework to fine-tune existing captioning models that enhances caption accuracy and consistency across views via a consensus mechanism. First, an agent explores the environment, collecting noisy image-caption pairs. Then, a consistent pseudo-caption for each object instance is distilled via consensus using a large language model. Finally, these pseudo-captions are used to fine-tune an off-the-shelf captioning model, with the addition of contrastive learning. We analyse the performance of the combination of captioning models, exploration policies, pseudo-labeling methods, and fine-tuning strategies, on our manually labeled test set. Results show that a policy can be trained to mine samples with higher disagreement compared to classical baselines. Our pseudo-captioning method, in combination with all policies, has a higher semantic similarity compared to other existing methods, and fine-tuning improves caption accuracy and consistency by a significant margin. Code and test set annotations available at https://hsp-iit.github.io/embodied-captioning/</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0439
            </span>
            <a href="https://arxiv.org/abs/2504.08446" target="_blank" rel="noopener noreferrer">Charting the Parrot's Song: A Maximum Mean Discrepancy Approach to Measuring AI Novelty, Originality, and Distinctiveness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Anirban Mukherjee, Hannah Hanwen Chang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Current intellectual property frameworks struggle to evaluate the novelty of AI-generated content, relying on subjective assessments ill-suited for comparing effectively infinite AI outputs against prior art. This paper introduces a robust, quantitative methodology grounded in Maximum Mean Discrepan</span>
            
            <span class="abstract-full" style="display: none;">Current intellectual property frameworks struggle to evaluate the novelty of AI-generated content, relying on subjective assessments ill-suited for comparing effectively infinite AI outputs against prior art. This paper introduces a robust, quantitative methodology grounded in Maximum Mean Discrepancy (MMD) to measure distributional differences between generative processes. By comparing entire output distributions rather than conducting pairwise similarity checks, our approach directly contrasts creative processes--overcoming the computational challenges inherent in evaluating AI outputs against unbounded prior art corpora. Through experiments combining kernel mean embeddings with domain-specific machine learning representations (LeNet-5 for MNIST digits, CLIP for art), we demonstrate exceptional sensitivity: our method distinguishes MNIST digit classes with 95% confidence using just 5-6 samples and differentiates AI-generated art from human art in the AI-ArtBench dataset (n=400 per category; p<0.0001) using as few as 7-10 samples per distribution despite human evaluators' limited discrimination ability (58% accuracy). These findings challenge the "stochastic parrot" hypothesis by providing empirical evidence that AI systems produce outputs from semantically distinct distributions rather than merely replicating training data. Our approach bridges technical capabilities with legal doctrine, offering a pathway to modernize originality assessments while preserving intellectual property law's core objectives. This research provides courts and policymakers with a computationally efficient, legally relevant tool to quantify AI novelty--a critical advancement as AI blurs traditional authorship and inventorship boundaries.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.8 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0478
            </span>
            <a href="https://arxiv.org/abs/2504.08325" target="_blank" rel="noopener noreferrer">Practical Secure Aggregation by Combining Cryptography and Trusted Execution Environments</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Romain de Laage, Peterson Yuhala, Fran\c{c}ois-Xavier Wicht, Pascal Felber, Christian Cachin, Valerio Schiavoni | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Secure aggregation enables a group of mutually distrustful parties, each holding private inputs, to collaboratively compute an aggregate value while preserving the privacy of their individual inputs. However, a major challenge in adopting secure aggregation approaches for practical applications is t</span>
            
            <span class="abstract-full" style="display: none;">Secure aggregation enables a group of mutually distrustful parties, each holding private inputs, to collaboratively compute an aggregate value while preserving the privacy of their individual inputs. However, a major challenge in adopting secure aggregation approaches for practical applications is the significant computational overhead of the underlying cryptographic protocols, e.g. fully homomorphic encryption. This overhead makes secure aggregation protocols impractical, especially for large datasets. In contrast, hardware-based security techniques such as trusted execution environments (TEEs) enable computation at near-native speeds, making them a promising alternative for reducing the computational burden typically associated with purely cryptographic techniques. Yet, in many scenarios, parties may opt for either cryptographic or hardware-based security mechanisms, highlighting the need for hybrid approaches. In this work, we introduce several secure aggregation architectures that integrate both cryptographic and TEE-based techniques, analyzing the trade-offs between security and performance.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 7.5 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0688
            </span>
            <a href="https://arxiv.org/abs/2503.12165" target="_blank" rel="noopener noreferrer">VTON 360: High-Fidelity Virtual Try-On from Any Viewing Direction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zijian He, Yuwei Ning, Yipeng Qin, Guangrun Wang, Sibei Yang, Liang Lin, Guanbin Li | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Virtual Try-On (VTON) is a transformative technology in e-commerce and fashion design, enabling realistic digital visualization of clothing on individuals. In this work, we propose VTON 360, a novel 3D VTON method that addresses the open challenge of achieving high-fidelity VTON that supports any-vi</span>
            
            <span class="abstract-full" style="display: none;">Virtual Try-On (VTON) is a transformative technology in e-commerce and fashion design, enabling realistic digital visualization of clothing on individuals. In this work, we propose VTON 360, a novel 3D VTON method that addresses the open challenge of achieving high-fidelity VTON that supports any-view rendering. Specifically, we leverage the equivalence between a 3D model and its rendered multi-view 2D images, and reformulate 3D VTON as an extension of 2D VTON that ensures 3D consistent results across multiple views. To achieve this, we extend 2D VTON models to include multi-view garments and clothing-agnostic human body images as input, and propose several novel techniques to enhance them, including: i) a pseudo-3D pose representation using normal maps derived from the SMPL-X 3D human model, ii) a multi-view spatial attention mechanism that models the correlations between features from different viewing angles, and iii) a multi-view CLIP embedding that enhances the garment CLIP features used in 2D VTON with camera information. Extensive experiments on large-scale real datasets and clothing images from e-commerce platforms demonstrate the effectiveness of our approach. Project page: https://scnuhealthy.github.io/VTON360.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.4 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Math: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.069
            </span>
            <a href="https://arxiv.org/abs/2504.08578" target="_blank" rel="noopener noreferrer">Knowledge Distillation for Multimodal Egocentric Action Recognition Robust to Missing Modalities</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Maria Santos-Villafranca, Dustin Carri\'on-Ojeda, Alejandro Perez-Yus, Jesus Bermudez-Cameo, Jose J. Guerrero, Simone Schaub-Meyer | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Action recognition is an essential task in egocentric vision due to its wide range of applications across many fields. While deep learning methods have been proposed to address this task, most rely on a single modality, typically video. However, including additional modalities may improve the robust</span>
            
            <span class="abstract-full" style="display: none;">Action recognition is an essential task in egocentric vision due to its wide range of applications across many fields. While deep learning methods have been proposed to address this task, most rely on a single modality, typically video. However, including additional modalities may improve the robustness of the approaches to common issues in egocentric videos, such as blurriness and occlusions. Recent efforts in multimodal egocentric action recognition often assume the availability of all modalities, leading to failures or performance drops when any modality is missing. To address this, we introduce an efficient multimodal knowledge distillation approach for egocentric action recognition that is robust to missing modalities (KARMMA) while still benefiting when multiple modalities are available. Our method focuses on resource-efficient development by leveraging pre-trained models as unimodal feature extractors in our teacher model, which distills knowledge into a much smaller and faster student model. Experiments on the Epic-Kitchens and Something-Something datasets demonstrate that our student model effectively handles missing modalities while reducing its accuracy drop in this scenario.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0692
            </span>
            <a href="https://arxiv.org/abs/2504.08012" target="_blank" rel="noopener noreferrer">SRVP: Strong Recollection Video Prediction Model Using Attention-Based Spatiotemporal Correlation Fusion</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuseon Kim, Kyongseok Park | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Video prediction (VP) generates future frames by leveraging spatial representations and temporal context from past frames. Traditional recurrent neural network (RNN)-based models enhance memory cell structures to capture spatiotemporal states over extended durations but suffer from gradual loss of o</span>
            
            <span class="abstract-full" style="display: none;">Video prediction (VP) generates future frames by leveraging spatial representations and temporal context from past frames. Traditional recurrent neural network (RNN)-based models enhance memory cell structures to capture spatiotemporal states over extended durations but suffer from gradual loss of object appearance details. To address this issue, we propose the strong recollection VP (SRVP) model, which integrates standard attention (SA) and reinforced feature attention (RFA) modules. Both modules employ scaled dot-product attention to extract temporal context and spatial correlations, which are then fused to enhance spatiotemporal representations. Experiments on three benchmark datasets demonstrate that SRVP mitigates image quality degradation in RNN-based models while achieving predictive performance comparable to RNN-free architectures.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 3.4 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 2.2 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.07
            </span>
            <a href="https://arxiv.org/abs/2504.08456" target="_blank" rel="noopener noreferrer">Generalization Bounds in Hybrid Quantum-Classical Machine Learning Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tongyan Wu, Amine Bentellis, Alona Sakhnenko, Jeanette Miriam Lorenz | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Hybrid classical-quantum models aim to harness the strengths of both quantum computing and classical machine learning, but their practical potential remains poorly understood. In this work, we develop a unified mathematical framework for analyzing generalization in hybrid models, offering insight in</span>
            
            <span class="abstract-full" style="display: none;">Hybrid classical-quantum models aim to harness the strengths of both quantum computing and classical machine learning, but their practical potential remains poorly understood. In this work, we develop a unified mathematical framework for analyzing generalization in hybrid models, offering insight into how these systems learn from data. We establish a novel generalization bound of the form $O\big( \sqrt{\frac{T\log{T}}{N}} + \frac{\alpha}{\sqrt{N}}\big)$ for $N$ training data points, $T$ trainable quantum gates, and bounded fully-connected layers $||F|| \leq \alpha$. This bound decomposes cleanly into quantum and classical contributions, extending prior work on both components and clarifying their interaction. We apply our results to the quantum-classical convolutional neural network (QCCNN), an architecture that integrates quantum convolutional layers with classical processing. Alongside the bound, we highlight conceptual limitations of applying classical statistical learning theory in the hybrid setting and suggest promising directions for future theoretical work.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.2 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 5.1 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.0884
            </span>
            <a href="https://arxiv.org/abs/2504.08340" target="_blank" rel="noopener noreferrer">All-in-Memory Stochastic Computing using ReRAM</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jo\~ao Paulo C. de Lima, Mehran Shoushtari Moghadam, Sercan Aygun, Jeronimo Castrillon, M. Hassan Najafi, Asif Ali Khan | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As the demand for efficient, low-power computing in embedded and edge devices grows, traditional computing methods are becoming less effective for handling complex tasks. Stochastic computing (SC) offers a promising alternative by approximating complex arithmetic operations, such as addition and mul</span>
            
            <span class="abstract-full" style="display: none;">As the demand for efficient, low-power computing in embedded and edge devices grows, traditional computing methods are becoming less effective for handling complex tasks. Stochastic computing (SC) offers a promising alternative by approximating complex arithmetic operations, such as addition and multiplication, using simple bitwise operations, like majority or AND, on random bit-streams. While SC operations are inherently fault-tolerant, their accuracy largely depends on the length and quality of the stochastic bit-streams (SBS). These bit-streams are typically generated by CMOS-based stochastic bit-stream generators that consume over 80% of the SC system's power and area. Current SC solutions focus on optimizing the logic gates but often neglect the high cost of moving the bit-streams between memory and processor. This work leverages the physics of emerging ReRAM devices to implement the entire SC flow in place: (1) generating low-cost true random numbers and SBSs, (2) conducting SC operations, and (3) converting SBSs back to binary. Considering the low reliability of ReRAM cells, we demonstrate how SC's robustness to errors copes with ReRAM's variability. Our evaluation shows significant improvements in throughput (1.39x, 2.16x) and energy consumption (1.15x, 2.8x) over state-of-the-art (CMOS- and ReRAM-based) solutions, respectively, with an average image quality drop of 5% across multiple SBS lengths and image processing tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1024
            </span>
            <a href="https://arxiv.org/abs/2406.09953" target="_blank" rel="noopener noreferrer">DAG-Plan: Generating Directed Acyclic Dependency Graphs for Dual-Arm Cooperative Planning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zeyu Gao, Yao Mu, Jinye Qu, Mengkang Hu, Shijia Peng, Chengkai Hou, Lingyue Guo, Ping Luo, Shanghang Zhang, Yanfeng Lu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Dual-arm robots offer enhanced versatility and efficiency over single-arm counterparts by enabling concurrent manipulation of multiple objects or cooperative execution of tasks using both arms. However, the coordination of dual-arm systems for long-horizon tasks continues to pose significant challen</span>
            
            <span class="abstract-full" style="display: none;">Dual-arm robots offer enhanced versatility and efficiency over single-arm counterparts by enabling concurrent manipulation of multiple objects or cooperative execution of tasks using both arms. However, the coordination of dual-arm systems for long-horizon tasks continues to pose significant challenges, stemming from the intricate temporal and spatial dependencies among sub-tasks, necessitating intelligent decisions regarding the allocation of actions between arms and their optimal execution order. Existing task planning methods predominantly focus on single-arm robots or rely on predefined bimanual operations to use large language models (LLMs) generate task sequence with linear temporal dependency, failing to fully leverage the capabilities of dual-arm systems. To address this limitation, we introduce DAG-Plan, a structured task planning framework tailored for dual-arm robots. DAG-Plan harnesses LLMs to decompose intricate tasks into actionable sub-tasks represented as nodes within a directed acyclic graph (DAG). Critically, DAG-Plan dynamically assigns these sub-tasks to the appropriate arm based on real-time environmental observations, enabling parallel and adaptive execution. We evaluate DAG-Plan on the Dual-Arm Kitchen Benchmark, comprising 5 sequential tasks with 44 sub-tasks. Extensive experiments demonstrate the superiority of DAG-Plan over directly using LLM to generate linear task sequence, achieving 52.8% higher efficiency compared to the single-arm task planning and 48% higher success rate of the dual-arm task planning. Compared to iterative methods, DAG-Plan improving execution efficiency 84.1% due to its fewer query time. More demos and information are available on https://sites.google.com/view/dag-plan.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.4 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1141
            </span>
            <a href="https://arxiv.org/abs/2504.08188" target="_blank" rel="noopener noreferrer">Safe Data-Driven Predictive Control</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Amin Vahidi-Moghaddam, Kaian Chen, Kaixiang Zhang, Zhaojian Li, Yan Wang, Kai Wu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In the realm of control systems, model predictive control (MPC) has exhibited remarkable potential; however, its reliance on accurate models and substantial computational resources has hindered its broader application, especially within real-time nonlinear systems. This study presents an innovative </span>
            
            <span class="abstract-full" style="display: none;">In the realm of control systems, model predictive control (MPC) has exhibited remarkable potential; however, its reliance on accurate models and substantial computational resources has hindered its broader application, especially within real-time nonlinear systems. This study presents an innovative control framework to enhance the practical viability of the MPC. The developed safe data-driven predictive control aims to eliminate the requirement for precise models and alleviate computational burdens in the nonlinear MPC (NMPC). This is achieved by learning both the system dynamics and the control policy, enabling efficient data-driven predictive control while ensuring system safety. The methodology involves a spatial temporal filter (STF)-based concurrent learning for system identification, a robust control barrier function (RCBF) to ensure the system safety amid model uncertainties, and a RCBF-based NMPC policy approximation. An online policy correction mechanism is also introduced to counteract performance degradation caused by the existing model uncertainties. Demonstrated through simulations on two applications, the proposed approach offers comparable performance to existing benchmarks with significantly reduced computational costs.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 8.9 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 3.2 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1259
            </span>
            <a href="https://arxiv.org/abs/2504.08219" target="_blank" rel="noopener noreferrer">VL-UR: Vision-Language-guided Universal Restoration of Images Degraded by Adverse Weather Conditions</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ziyan Liu, Yuxu Lu, Huashan Yu, Dong yang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Image restoration is critical for improving the quality of degraded images, which is vital for applications like autonomous driving, security surveillance, and digital content enhancement. However, existing methods are often tailored to specific degradation scenarios, limiting their adaptability to </span>
            
            <span class="abstract-full" style="display: none;">Image restoration is critical for improving the quality of degraded images, which is vital for applications like autonomous driving, security surveillance, and digital content enhancement. However, existing methods are often tailored to specific degradation scenarios, limiting their adaptability to the diverse and complex challenges in real-world environments. Moreover, real-world degradations are typically non-uniform, highlighting the need for adaptive and intelligent solutions. To address these issues, we propose a novel vision-language-guided universal restoration (VL-UR) framework. VL-UR leverages a zero-shot contrastive language-image pre-training (CLIP) model to enhance image restoration by integrating visual and semantic information. A scene classifier is introduced to adapt CLIP, generating high-quality language embeddings aligned with degraded images while predicting degraded types for complex scenarios. Extensive experiments across eleven diverse degradation settings demonstrate VL-UR's state-of-the-art performance, robustness, and adaptability. This positions VL-UR as a transformative solution for modern image restoration challenges in dynamic, real-world environments.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1296
            </span>
            <a href="https://arxiv.org/abs/2504.08526" target="_blank" rel="noopener noreferrer">Hallucination, reliability, and the role of generative AI in science</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Charles Rathkopf | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative AI is increasingly used in scientific domains, from protein folding to climate modeling. But these models produce distinctive errors known as hallucinations - outputs that are incorrect yet superficially plausible. Worse, some arguments suggest that hallucinations are an inevitable conseq</span>
            
            <span class="abstract-full" style="display: none;">Generative AI is increasingly used in scientific domains, from protein folding to climate modeling. But these models produce distinctive errors known as hallucinations - outputs that are incorrect yet superficially plausible. Worse, some arguments suggest that hallucinations are an inevitable consequence of the mechanisms underlying generative inference. Fortunately, such arguments rely on a conception of hallucination defined solely with respect to internal properties of the model, rather than in reference to the empirical target system. This conception fails to distinguish epistemically benign errors from those that threaten scientific inference. I introduce the concept of corrosive hallucination to capture the epistemically troubling subclass: misrepresentations that are substantively misleading and resistant to systematic anticipation. I argue that although corrosive hallucinations do pose a threat to scientific reliability, they are not inevitable. Scientific workflows such as those surrounding AlphaFold and GenCast, both of which serve as case studies, can neutralize their effects by imposing theoretical constraints during training, and by strategically screening for errors at inference time. When embedded in such workflows, generative AI can reliably contribute to scientific knowledge.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1378
            </span>
            <a href="https://arxiv.org/abs/2504.00614" target="_blank" rel="noopener noreferrer">Learning Bipedal Locomotion on Gear-Driven Humanoid Robot Using Foot-Mounted IMUs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sotaro Katayama, Yuta Koda, Norio Nagatsuka, Masaya Kinoshita | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Sim-to-real reinforcement learning (RL) for humanoid robots with high-gear ratio actuators remains challenging due to complex actuator dynamics and the absence of torque sensors. To address this, we propose a novel RL framework leveraging foot-mounted inertial measurement units (IMUs). Instead of pu</span>
            
            <span class="abstract-full" style="display: none;">Sim-to-real reinforcement learning (RL) for humanoid robots with high-gear ratio actuators remains challenging due to complex actuator dynamics and the absence of torque sensors. To address this, we propose a novel RL framework leveraging foot-mounted inertial measurement units (IMUs). Instead of pursuing detailed actuator modeling and system identification, we utilize foot-mounted IMU measurements to enhance rapid stabilization capabilities over challenging terrains. Additionally, we propose symmetric data augmentation dedicated to the proposed observation space and random network distillation to enhance bipedal locomotion learning over rough terrain. We validate our approach through hardware experiments on a miniature-sized humanoid EVAL-03 over a variety of environments. The experimental results demonstrate that our method improves rapid stabilization capabilities over non-rigid surfaces and sudden environmental transitions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- 3D: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1428
            </span>
            <a href="https://arxiv.org/abs/2309.11488" target="_blank" rel="noopener noreferrer">An Evaluation and Comparison of GPU Hardware and Solver Libraries for Accelerating the OPM Flow Reservoir Simulator</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tong Dong Qiu, Andreas Thune, Vinicius Oliveira Martins, Markus Blatt, Alf Birger Rustad, Razvan Nane | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Realistic reservoir simulation is known to be prohibitively expensive in terms of computation time when increasing the accuracy of the simulation or by enlarging the model grid size. One method to address this issue is to parallelize the computation by dividing the model in several partitions and us</span>
            
            <span class="abstract-full" style="display: none;">Realistic reservoir simulation is known to be prohibitively expensive in terms of computation time when increasing the accuracy of the simulation or by enlarging the model grid size. One method to address this issue is to parallelize the computation by dividing the model in several partitions and using multiple CPUs to compute the result using techniques such as MPI and multi-threading. Alternatively, GPUs are also a good candidate to accelerate the computation due to their massively parallel architecture that allows many floating point operations per second to be performed. The numerical iterative solver takes thus the most computational time and is challenging to solve efficiently due to the dependencies that exist in the model between cells. In this work, we evaluate the OPM Flow simulator and compare several state-of-the-art GPU solver libraries as well as custom developed solutions for a BiCGStab solver using an ILU0 preconditioner and benchmark their performance against the default DUNE library implementation running on multiple CPU processors using MPI. The evaluated GPU software libraries include a manual linear solver in OpenCL and the integration of several third party sparse linear algebra libraries, such as cuSparse, rocSparse, and amgcl. To perform our bench-marking, we use small, medium, and large use cases, starting with the public test case NORNE that includes approximately 50k active cells and ending with a large model that includes approximately 1 million active cells. We find that a GPU can accelerate a single dual-threaded MPI process up to 5.6 times, and that it can compare with around 8 dual-threaded MPI processes.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.5 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1472
            </span>
            <a href="https://arxiv.org/abs/2504.08430" target="_blank" rel="noopener noreferrer">A Hybrid ABM-PDE Framework for Real-World Infectious Disease Simulations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kristina Maier, Tim O. F. Conrad | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper presents a hybrid modeling approach that couples an Agent-Based Model (ABM) with a partial differential equation (PDE) model in an epidemic setting to simulate the spatial spread of infectious diseases using a compartmental structure with seven health states. The goal is to reduce the com</span>
            
            <span class="abstract-full" style="display: none;">This paper presents a hybrid modeling approach that couples an Agent-Based Model (ABM) with a partial differential equation (PDE) model in an epidemic setting to simulate the spatial spread of infectious diseases using a compartmental structure with seven health states. The goal is to reduce the computational complexity of a full-ABM by introducing a coupled ABM-PDE model that offers significantly faster simulations while maintaining comparable accuracy. Our results demonstrate that the hybrid model not only reduces the overall simulation runtime (defined as the number of runs required for stable results multiplied by the duration of a single run) but also achieves smaller errors across both 25% and 100% population samples. The coupling mechanism ensures consistency at the model interface: agents crossing from the ABM into the PDE domain are removed and represented as density contributions at the corresponding grid node, while surplus density in the PDE domain is used to generate agents with plausible trajectories derived from mobile phone data. We evaluate the hybrid model using real-world mobility and infection data for the Berlin-Brandenburg region in Germany, showing that it captures the core epidemiological dynamics while enabling efficient large-scale simulations.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.4 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 3.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1592
            </span>
            <a href="https://arxiv.org/abs/2504.08353" target="_blank" rel="noopener noreferrer">Single View Garment Reconstruction Using Diffusion Mapping Via Pattern Coordinates</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ren Li, Cong Cao, Corentin Dumery, Yingxuan You, Hao Li, Pascal Fua | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Reconstructing 3D clothed humans from images is fundamental to applications like virtual try-on, avatar creation, and mixed reality. While recent advances have enhanced human body recovery, accurate reconstruction of garment geometry -- especially for loose-fitting clothing -- remains an open challe</span>
            
            <span class="abstract-full" style="display: none;">Reconstructing 3D clothed humans from images is fundamental to applications like virtual try-on, avatar creation, and mixed reality. While recent advances have enhanced human body recovery, accurate reconstruction of garment geometry -- especially for loose-fitting clothing -- remains an open challenge. We present a novel method for high-fidelity 3D garment reconstruction from single images that bridges 2D and 3D representations. Our approach combines Implicit Sewing Patterns (ISP) with a generative diffusion model to learn rich garment shape priors in a 2D UV space. A key innovation is our mapping model that establishes correspondences between 2D image pixels, UV pattern coordinates, and 3D geometry, enabling joint optimization of both 3D garment meshes and the corresponding 2D patterns by aligning learned priors with image observations. Despite training exclusively on synthetically simulated cloth data, our method generalizes effectively to real-world images, outperforming existing approaches on both tight- and loose-fitting garments. The reconstructed garments maintain physical plausibility while capturing fine geometric details, enabling downstream applications including garment retargeting and texture manipulation.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.1 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1762
            </span>
            <a href="https://arxiv.org/abs/2504.08603" target="_blank" rel="noopener noreferrer">FindAnything: Open-Vocabulary and Object-Centric Mapping for Robot Exploration in Any Environment</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sebasti\'an Barbas Laina, Simon Boche, Sotiris Papatheodorou, Simon Schaefer, Jaehyung Jung, Stefan Leutenegger | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Geometrically accurate and semantically expressive map representations have proven invaluable to facilitate robust and safe mobile robot navigation and task planning. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments is still an open problem. In this</span>
            
            <span class="abstract-full" style="display: none;">Geometrically accurate and semantically expressive map representations have proven invaluable to facilitate robust and safe mobile robot navigation and task planning. Nevertheless, real-time, open-vocabulary semantic understanding of large-scale unknown environments is still an open problem. In this paper we present FindAnything, an open-world mapping and exploration framework that incorporates vision-language information into dense volumetric submaps. Thanks to the use of vision-language features, FindAnything bridges the gap between pure geometric and open-vocabulary semantic information for a higher level of understanding while allowing to explore any environment without the help of any external source of ground-truth pose information. We represent the environment as a series of volumetric occupancy submaps, resulting in a robust and accurate map representation that deforms upon pose updates when the underlying SLAM system corrects its drift, allowing for a locally consistent representation between submaps. Pixel-wise vision-language features are aggregated from efficient SAM (eSAM)-generated segments, which are in turn integrated into object-centric volumetric submaps, providing a mapping from open-vocabulary queries to 3D geometry that is scalable also in terms of memory usage. The open-vocabulary map representation of FindAnything achieves state-of-the-art semantic accuracy in closed-set evaluations on the Replica dataset. This level of scene understanding allows a robot to explore environments based on objects or areas of interest selected via natural language queries. Our system is the first of its kind to be deployed on resource-constrained devices, such as MAVs, leveraging vision-language information for real-world robotic tasks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.177
            </span>
            <a href="https://arxiv.org/abs/2504.08358" target="_blank" rel="noopener noreferrer">LMM4LMM: Benchmarking and Evaluating Large-multimodal Image Generation with LMMs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiarui Wang, Huiyu Duan, Yu Zhao, Juntong Wang, Guangtao Zhai, Xiongkuo Min | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Recent breakthroughs in large multimodal models (LMMs) have significantly advanced both text-to-image (T2I) generation and image-to-text (I2T) interpretation. However, many generated images still suffer from issues related to perceptual quality and text-image alignment. Given the high cost and ineff</span>
            
            <span class="abstract-full" style="display: none;">Recent breakthroughs in large multimodal models (LMMs) have significantly advanced both text-to-image (T2I) generation and image-to-text (I2T) interpretation. However, many generated images still suffer from issues related to perceptual quality and text-image alignment. Given the high cost and inefficiency of manual evaluation, an automatic metric that aligns with human preferences is desirable. To this end, we present EvalMi-50K, a comprehensive dataset and benchmark for evaluating large-multimodal image generation, which features (i) comprehensive tasks, encompassing 2,100 extensive prompts across 20 fine-grained task dimensions, and (ii) large-scale human-preference annotations, including 100K mean-opinion scores (MOSs) and 50K question-answering (QA) pairs annotated on 50,400 images generated from 24 T2I models. Based on EvalMi-50K, we propose LMM4LMM, an LMM-based metric for evaluating large multimodal T2I generation from multiple dimensions including perception, text-image correspondence, and task-specific accuracy. Extensive experimental results show that LMM4LMM achieves state-of-the-art performance on EvalMi-50K, and exhibits strong generalization ability on other AI-generated image evaluation benchmark datasets, manifesting the generality of both the EvalMi-50K dataset and LMM4LMM metric. Both EvalMi-50K and LMM4LMM will be released at https://github.com/IntMeGroup/LMM4LMM.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.6 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1834
            </span>
            <a href="https://arxiv.org/abs/2504.08534" target="_blank" rel="noopener noreferrer">Genetic Algorithm Design Exploration for On-Device Training on FPGAs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Alaa Mazouz, Van-Tam Nguyen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We propose an automated Design Space Exploration (DSE) workflow for generating adaptive and reconfigurable deep learning models on FPGA hardware. The workflow consists of two main components: Offline Design Exploration (ODE) and Online Design Reconfiguration (ODR). ODE applies a multi-objective gene</span>
            
            <span class="abstract-full" style="display: none;">We propose an automated Design Space Exploration (DSE) workflow for generating adaptive and reconfigurable deep learning models on FPGA hardware. The workflow consists of two main components: Offline Design Exploration (ODE) and Online Design Reconfiguration (ODR). ODE applies a multi-objective genetic algorithm to explore CNN-based hardware configurations, optimizing for latency and resource utilization by leveraging intra-layer parallelism. Given a CNN architecture and user-defined constraints, the hardware model is generated automatically. ODR enables runtime hardware adaptability by dynamically selecting between partial or full reconfigurable designs based on application requirements. This flexibility is essential for time-critical, autonomous onboard systems. We demonstrate the proposed workflow on the Xilinx Zynq-7100 FPGA operating at 200 MHz, using CNN models trained on MNIST, SVHN, and CIFAR-10. ODE-generated designs show latency improvements of up to 95 times for MNIST, 71 times for CIFAR-10, and 18 times for SVHN. Resource utilization in DSP slices was improved by up to 44 times for MNIST, 52 times for SVHN, and 24 times for CIFAR-10. The ODR approach achieved trade-offs between accuracy and performance, such as a 0.7 percent accuracy drop for a 13 times speedup and 25 percent power reduction on MNIST, a 2 percent drop for 14 times speedup and 28 percent power savings on SVHN, and a 4 percent drop for 50 times speedup with 32.5 percent power reduction on CIFAR-10.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.1 -->
                
            <!-- LLMs: 8.9 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1943
            </span>
            <a href="https://arxiv.org/abs/2504.08509" target="_blank" rel="noopener noreferrer">The Complexity of Generalized HyperLTL with Stuttering and Contexts</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ga\"etan Regaud, Martin Zimmermann | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We settle the complexity of satisfiability and model-checking for generalized HyperLTL with stuttering and contexts, an expressive logic for the specification of asynchronous hyperproperties. Such properties cannot be specified in HyperLTL, as it is restricted to synchronous hyperproperties.</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.6 -->
                
            <!-- LLMs: 8.5 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.1982
            </span>
            <a href="https://arxiv.org/abs/2504.08206" target="_blank" rel="noopener noreferrer">Advancing Autonomous Vehicle Safety: A Combined Fault Tree Analysis and Bayesian Network Approach</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lansu Dai, Burak Kantarci | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper integrates Fault Tree Analysis (FTA) and Bayesian Networks (BN) to assess collision risk and establish Automotive Safety Integrity Level (ASIL) B failure rate targets for critical autonomous vehicle (AV) components. The FTA-BN integration combines the systematic decomposition of failure e</span>
            
            <span class="abstract-full" style="display: none;">This paper integrates Fault Tree Analysis (FTA) and Bayesian Networks (BN) to assess collision risk and establish Automotive Safety Integrity Level (ASIL) B failure rate targets for critical autonomous vehicle (AV) components. The FTA-BN integration combines the systematic decomposition of failure events provided by FTA with the probabilistic reasoning capabilities of BN, which allow for dynamic updates in failure probabilities, enhancing the adaptability of risk assessment. A fault tree is constructed based on AV subsystem architecture, with collision as the top event, and failure rates are assigned while ensuring the total remains within 100 FIT. Bayesian inference is applied to update posterior probabilities, and the results indicate that perception system failures (46.06 FIT) are the most significant contributor, particularly failures to detect existing objects (PF5) and misclassification (PF6). Mitigation strategies are proposed for sensors, perception, decision-making, and motion control to reduce the collision risk. The FTA-BN integration approach provides dynamic risk quantification, offering system designers refined failure rate targets to improve AV safety.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2194
            </span>
            <a href="https://arxiv.org/abs/2504.08655" target="_blank" rel="noopener noreferrer">TinyCenterSpeed: Efficient Center-Based Object Detection for Autonomous Racing</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Neil Reichlin, Nicolas Baumann, Edoardo Ghignone, Michele Magno | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Perception within autonomous driving is nearly synonymous with Neural Networks (NNs). Yet, the domain of autonomous racing is often characterized by scaled, computationally limited robots used for cost-effectiveness and safety. For this reason, opponent detection and tracking systems typically resor</span>
            
            <span class="abstract-full" style="display: none;">Perception within autonomous driving is nearly synonymous with Neural Networks (NNs). Yet, the domain of autonomous racing is often characterized by scaled, computationally limited robots used for cost-effectiveness and safety. For this reason, opponent detection and tracking systems typically resort to traditional computer vision techniques due to computational constraints. This paper introduces TinyCenterSpeed, a streamlined adaptation of the seminal CenterPoint method, optimized for real-time performance on 1:10 scale autonomous racing platforms. This adaptation is viable even on OBCs powered solely by Central Processing Units (CPUs), as it incorporates the use of an external Tensor Processing Unit (TPU). We demonstrate that, compared to Adaptive Breakpoint Detector (ABD), the current State-of-the-Art (SotA) in scaled autonomous racing, TinyCenterSpeed not only improves detection and velocity estimation by up to 61.38% but also supports multi-opponent detection and estimation. It achieves real-time performance with an inference time of just 7.88 ms on the TPU, significantly reducing CPU utilization 8.3-fold.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2234
            </span>
            <a href="https://arxiv.org/abs/2407.16557" target="_blank" rel="noopener noreferrer">Patched RTC: evaluating LLMs for diverse software development tasks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Asankhaya Sharma | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel evaluation technique for Large Language Models (LLMs) applied to diverse software development tasks, particularly focusing on "outer loop" activities such as bug fixing, code review, and documentation updates. Patched RTC ex</span>
            
            <span class="abstract-full" style="display: none;">This paper introduces Patched Round-Trip Correctness (Patched RTC), a novel evaluation technique for Large Language Models (LLMs) applied to diverse software development tasks, particularly focusing on "outer loop" activities such as bug fixing, code review, and documentation updates. Patched RTC extends the original Round-Trip Correctness method to work with any LLM and downstream task, offering a self-evaluating framework that measures consistency and robustness of model responses without human intervention. The study demonstrates a correlation between Patched RTC scores and task-specific accuracy metrics, presenting it as an alternative to the LLM-as-Judge paradigm for open-domain task evaluation. We implement Patched RTC in an open-source framework called patchwork, allowing for transparent evaluation during inference across various patchflows. Experiments comparing GPT-3.5 and GPT-4 models across different software development tasks reveal that Patched RTC effectively distinguishes model performance and task difficulty. The paper also explores the impact of consistency prompts on improving model accuracy, suggesting that Patched RTC can guide prompt refinement and model selection for complex software development workflows.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.4 -->
                
            <!-- Medicine: 7.3 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.252
            </span>
            <a href="https://arxiv.org/abs/2504.08171" target="_blank" rel="noopener noreferrer">An open framework for archival, reproducible, and transparent science</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sabar Dasgupta, Paul Nuyujukian | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Digital computational outputs are now ubiquitous in the research workflow and the way in which these data are stored and cataloged is becoming more standardized across fields of research. However, even with accessible data and code, the barrier to recreating figures and reproducing scientific findin</span>
            
            <span class="abstract-full" style="display: none;">Digital computational outputs are now ubiquitous in the research workflow and the way in which these data are stored and cataloged is becoming more standardized across fields of research. However, even with accessible data and code, the barrier to recreating figures and reproducing scientific findings remains high. What is generally missing is the computational environment and associated pipelines in which the data and code are executed to generate figures. The archival, reproducible, and transparent science (ARTS) open framework incorporates containers, version control systems, and persistent archives through which all data, code, and figures related to a research project can be stored together, easily recreated, and serve as an accessible platform for long-term sharing and validation. If the underlying principles behind this framework are broadly adopted, it will improve the reproducibility and transparency of research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2564
            </span>
            <a href="https://arxiv.org/abs/2504.08234" target="_blank" rel="noopener noreferrer">Bringing Structure to Naturalness: On the Naturalness of ASTs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Profir-Petru P\^ar\c{t}achi, Mahito Sugiyama | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Source code comes in different shapes and forms. Previous research has already shown code to be more predictable than natural language as well as highlighted its statistical predictability at the token level: source code can be natural. More recently, the structure of code -- control flow, syntax gr</span>
            
            <span class="abstract-full" style="display: none;">Source code comes in different shapes and forms. Previous research has already shown code to be more predictable than natural language as well as highlighted its statistical predictability at the token level: source code can be natural. More recently, the structure of code -- control flow, syntax graphs, abstract syntax trees etc. -- has been successfully used to improve the state-of-the-art on numerous tasks: code suggestion, code summarisation, method naming etc. This body of work implicitly assumes that structured representations of code are similarly statistically predictable, i.e. that a structured view of code is also natural. We consider that this view should be made explicit and propose directly studying the Structured Naturalness Hypothesis. Beyond just naming existing research that assumes this hypothesis and formulating it, we also provide evidence in the case of trees: TreeLSTM models over ASTs for some languages, such as Ruby, are competitive with $n$-gram models while handling the syntax token issue highlighted by previous research 'for free'. For other languages, such as Java or Python, we find tree models to perform worse, suggesting that downstream task improvement is uncorrelated to the language modelling task. Further, we show how such naturalness signals can be employed for near state-of-the-art results on just-in-time defect prediction while forgoing manual feature engineering work.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2604
            </span>
            <a href="https://arxiv.org/abs/2504.08619" target="_blank" rel="noopener noreferrer">Analyzing 16,193 LLM Papers for Fun and Profits</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zhiqiu Xia, Lang Zhu, Bingzhe Li, Feng Chen, Qiannan Li, Hang Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Large Language Models (LLMs) are reshaping the landscape of computer science research, driving significant shifts in research priorities across diverse conferences and fields. This study provides a comprehensive analysis of the publication trend of LLM-related papers in 77 top-tier computer science </span>
            
            <span class="abstract-full" style="display: none;">Large Language Models (LLMs) are reshaping the landscape of computer science research, driving significant shifts in research priorities across diverse conferences and fields. This study provides a comprehensive analysis of the publication trend of LLM-related papers in 77 top-tier computer science conferences over the past six years (2019-2024). We approach this analysis from four distinct perspectives: (1) We investigate how LLM research is driving topic shifts within major conferences. (2) We adopt a topic modeling approach to identify various areas of LLM-related topic growth and reveal the topics of concern at different conferences. (3) We explore distinct contribution patterns of academic and industrial institutions. (4) We study the influence of national origins on LLM development trajectories. Synthesizing the findings from these diverse analytical angles, we derive ten key insights that illuminate the dynamics and evolution of the LLM research ecosystem.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 13.0 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2703
            </span>
            <a href="https://arxiv.org/abs/2311.10599" target="_blank" rel="noopener noreferrer">Chatbots as social companions: How people perceive consciousness, human likeness, and social health benefits in machines</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rose E. Guingrich, Michael S. A. Graziano | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As artificial intelligence (AI) becomes more widespread, one question that arises is how human-AI interaction might impact human-human interaction. Chatbots, for example, are increasingly used as social companions, and while much is speculated, little is known empirically about how their use impacts</span>
            
            <span class="abstract-full" style="display: none;">As artificial intelligence (AI) becomes more widespread, one question that arises is how human-AI interaction might impact human-human interaction. Chatbots, for example, are increasingly used as social companions, and while much is speculated, little is known empirically about how their use impacts human relationships. A common hypothesis is that relationships with companion chatbots are detrimental to social health by harming or replacing human interaction, but this hypothesis may be too simplistic, especially considering the social needs of users and the health of their preexisting human relationships. To understand how relationships with companion chatbots impact social health, we studied people who regularly used companion chatbots and people who did not use them. Contrary to expectations, companion chatbot users indicated that these relationships were beneficial to their social health, whereas non-users viewed them as harmful. Another common assumption is that people perceive conscious, humanlike AI as disturbing and threatening. Among both users and non-users, however, we found the opposite: perceiving companion chatbots as more conscious and humanlike correlated with more positive opinions and more pronounced social health benefits. Detailed accounts from users suggested that these humanlike chatbots may aid social health by supplying reliable and safe interactions, without necessarily harming human relationships, but this may depend on users' preexisting social needs and how they perceive both human likeness and mind in the chatbot.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.7 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2704
            </span>
            <a href="https://arxiv.org/abs/2504.08024" target="_blank" rel="noopener noreferrer">From Speech to Summary: A Comprehensive Survey of Speech Summarization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Fabian Retkowski, Maike Z\"ufle, Andreas Sudmann, Dinah Pfau, Jan Niehues, Alexander Waibel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Speech summarization has become an essential tool for efficiently managing and accessing the growing volume of spoken and audiovisual content. However, despite its increasing importance, speech summarization is still not clearly defined and intersects with several research areas, including speech re</span>
            
            <span class="abstract-full" style="display: none;">Speech summarization has become an essential tool for efficiently managing and accessing the growing volume of spoken and audiovisual content. However, despite its increasing importance, speech summarization is still not clearly defined and intersects with several research areas, including speech recognition, text summarization, and specific applications like meeting summarization. This survey not only examines existing datasets and evaluation methodologies, which are crucial for assessing the effectiveness of summarization approaches but also synthesizes recent developments in the field, highlighting the shift from traditional systems to advanced models like fine-tuned cascaded architectures and end-to-end solutions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2747
            </span>
            <a href="https://arxiv.org/abs/2504.08172" target="_blank" rel="noopener noreferrer">Enhanced Cooperative Perception Through Asynchronous Vehicle to Infrastructure Framework with Delay Mitigation for Connected and Automated Vehicles</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Nithish Kumar Saravanan, Varun Jammula, Yezhou Yang, Jeffrey Wishart, Junfeng Zhao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Perception is a key component of Automated vehicles (AVs). However, sensors mounted to the AVs often encounter blind spots due to obstructions from other vehicles, infrastructure, or objects in the surrounding area. While recent advancements in planning and control algorithms help AVs react to sudde</span>
            
            <span class="abstract-full" style="display: none;">Perception is a key component of Automated vehicles (AVs). However, sensors mounted to the AVs often encounter blind spots due to obstructions from other vehicles, infrastructure, or objects in the surrounding area. While recent advancements in planning and control algorithms help AVs react to sudden object appearances from blind spots at low speeds and less complex scenarios, challenges remain at high speeds and complex intersections. Vehicle to Infrastructure (V2I) technology promises to enhance scene representation for AVs in complex intersections, providing sufficient time and distance to react to adversary vehicles violating traffic rules. Most existing methods for infrastructure-based vehicle detection and tracking rely on LIDAR, RADAR or sensor fusion methods, such as LIDAR-Camera and RADAR-Camera. Although LIDAR and RADAR provide accurate spatial information, the sparsity of point cloud data limits its ability to capture detailed object contours of objects far away, resulting in inaccurate 3D object detection results. Furthermore, the absence of LIDAR or RADAR at every intersection increases the cost of implementing V2I technology. To address these challenges, this paper proposes a V2I framework that utilizes monocular traffic cameras at road intersections to detect 3D objects. The results from the roadside unit (RSU) are then combined with the on-board system using an asynchronous late fusion method to enhance scene representation. Additionally, the proposed framework provides a time delay compensation module to compensate for the processing and transmission delay from the RSU. Lastly, the V2I framework is tested by simulating and validating a scenario similar to the one described in an industry report by Waymo. The results show that the proposed method improves the scene representation and the AV's perception range, giving enough time and space to react to adversary vehicles.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.8 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 3.0 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2918
            </span>
            <a href="https://arxiv.org/abs/2501.01421" target="_blank" rel="noopener noreferrer">R-SCoRe: Revisiting Scene Coordinate Regression for Robust Large-Scale Visual Localization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xudong Jiang, Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Marc Pollefeys | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Learning-based visual localization methods that use scene coordinate regression (SCR) offer the advantage of smaller map sizes. However, on datasets with complex illumination changes or image-level ambiguities, it remains a less robust alternative to feature matching methods. This work aims to close</span>
            
            <span class="abstract-full" style="display: none;">Learning-based visual localization methods that use scene coordinate regression (SCR) offer the advantage of smaller map sizes. However, on datasets with complex illumination changes or image-level ambiguities, it remains a less robust alternative to feature matching methods. This work aims to close the gap. We introduce a covisibility graph-based global encoding learning and data augmentation strategy, along with a depth-adjusted reprojection loss to facilitate implicit triangulation. Additionally, we revisit the network architecture and local feature extraction module. Our method achieves state-of-the-art on challenging large-scale datasets without relying on network ensembles or 3D supervision. On Aachen Day-Night, we are 10$\times$ more accurate than previous SCR methods with similar map sizes and require at least 5$\times$ smaller map sizes than any other SCR method while still delivering superior accuracy. Code is available at: https://github.com/cvg/scrstudio .</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2938
            </span>
            <a href="https://arxiv.org/abs/2407.11930" target="_blank" rel="noopener noreferrer">Localizing and Mitigating Errors in Long-form Question Answering</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rachneet Sachdeva, Yixiao Song, Mohit Iyyer, Iryna Gurevych | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Long-form question answering (LFQA) aims to provide thorough and in-depth answers to complex questions, enhancing comprehension. However, such detailed responses are prone to hallucinations and factual inconsistencies, challenging their faithful evaluation. This work introduces HaluQuestQA, the firs</span>
            
            <span class="abstract-full" style="display: none;">Long-form question answering (LFQA) aims to provide thorough and in-depth answers to complex questions, enhancing comprehension. However, such detailed responses are prone to hallucinations and factual inconsistencies, challenging their faithful evaluation. This work introduces HaluQuestQA, the first hallucination dataset with localized error annotations for human-written and model-generated LFQA answers. HaluQuestQA comprises 698 QA pairs with 1.8k span-level error annotations for five different error types by expert annotators, along with preference judgments. Using our collected data, we thoroughly analyze the shortcomings of long-form answers and find that they lack comprehensiveness and provide unhelpful references. We train an automatic feedback model on this dataset that predicts error spans with incomplete information and provides associated explanations. Finally, we propose a prompt-based approach, Error-informed refinement, that uses signals from the learned feedback model to refine generated answers, which we show reduces errors and improves answer quality across multiple models. Furthermore, humans find answers generated by our approach comprehensive and highly prefer them (84%) over the baseline answers.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.2969
            </span>
            <a href="https://arxiv.org/abs/2504.08544" target="_blank" rel="noopener noreferrer">Slicing the Gaussian Mixture Wasserstein Distance</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Moritz Piening, Robert Beinert | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Gaussian mixture models (GMMs) are widely used in machine learning for tasks such as clustering, classification, image reconstruction, and generative modeling. A key challenge in working with GMMs is defining a computationally efficient and geometrically meaningful metric. The mixture Wasserstein (M</span>
            
            <span class="abstract-full" style="display: none;">Gaussian mixture models (GMMs) are widely used in machine learning for tasks such as clustering, classification, image reconstruction, and generative modeling. A key challenge in working with GMMs is defining a computationally efficient and geometrically meaningful metric. The mixture Wasserstein (MW) distance adapts the Wasserstein metric to GMMs and has been applied in various domains, including domain adaptation, dataset comparison, and reinforcement learning. However, its high computational cost -- arising from repeated Wasserstein distance computations involving matrix square root estimations and an expensive linear program -- limits its scalability to high-dimensional and large-scale problems. To address this, we propose multiple novel slicing-based approximations to the MW distance that significantly reduce computational complexity while preserving key optimal transport properties. From a theoretical viewpoint, we establish several weak and strong equivalences between the introduced metrics, and show the relations to the original MW distance and the well-established sliced Wasserstein distance. Furthermore, we validate the effectiveness of our approach through numerical experiments, demonstrating computational efficiency and applications in clustering, perceptual image comparison, and GMM minimization</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.0 -->
                
            <!-- Medicine: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.314
            </span>
            <a href="https://arxiv.org/abs/2504.08418" target="_blank" rel="noopener noreferrer">seeBias: A Comprehensive Tool for Assessing and Visualizing AI Fairness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yilin Ning, Yian Ma, Mingxuan Liu, Xin Li, Nan Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Fairness in artificial intelligence (AI) prediction models is increasingly emphasized to support responsible adoption in high-stakes domains such as health care and criminal justice. Guidelines and implementation frameworks highlight the importance of both predictive accuracy and equitable outcomes.</span>
            
            <span class="abstract-full" style="display: none;">Fairness in artificial intelligence (AI) prediction models is increasingly emphasized to support responsible adoption in high-stakes domains such as health care and criminal justice. Guidelines and implementation frameworks highlight the importance of both predictive accuracy and equitable outcomes. However, current fairness toolkits often evaluate classification performance disparities in isolation, with limited attention to other critical aspects such as calibration. To address these gaps, we present seeBias, an R package for comprehensive evaluation of model fairness and predictive performance. seeBias offers an integrated evaluation across classification, calibration, and other performance domains, providing a more complete view of model behavior. It includes customizable visualizations to support transparent reporting and responsible AI implementation. Using public datasets from criminal justice and healthcare, we demonstrate how seeBias supports fairness evaluations, and uncovers disparities that conventional fairness metrics may overlook. The R package is available on GitHub, and a Python version is under development.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.4 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Reinforcement Learning: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3223
            </span>
            <a href="https://arxiv.org/abs/2504.08609" target="_blank" rel="noopener noreferrer">A Survey of Machine Learning Models and Datasets for the Multi-label Classification of Textual Hate Speech in English</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Julian B\"aumler, Louis Bl\"ocher, Lars-Joel Frey, Xian Chen, Markus Bayer, Christian Reuter | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The dissemination of online hate speech can have serious negative consequences for individuals, online communities, and entire societies. This and the large volume of hateful online content prompted both practitioners', i.e., in content moderation or law enforcement, and researchers' interest in mac</span>
            
            <span class="abstract-full" style="display: none;">The dissemination of online hate speech can have serious negative consequences for individuals, online communities, and entire societies. This and the large volume of hateful online content prompted both practitioners', i.e., in content moderation or law enforcement, and researchers' interest in machine learning models to automatically classify instances of hate speech. Whereas most scientific works address hate speech classification as a binary task, practice often requires a differentiation into sub-types, e.g., according to target, severity, or legality, which may overlap for individual content. Hence, researchers created datasets and machine learning models that approach hate speech classification in textual data as a multi-label problem. This work presents the first systematic and comprehensive survey of scientific literature on this emerging research landscape in English (N=46). We contribute with a concise overview of 28 datasets suited for training multi-label classification models that reveals significant heterogeneity regarding label-set, size, meta-concept, annotation process, and inter-annotator agreement. Our analysis of 24 publications proposing suitable classification models further establishes inconsistency in evaluation and a preference for architectures based on Bidirectional Encoder Representation from Transformers (BERT) and Recurrent Neural Networks (RNNs). We identify imbalanced training data, reliance on crowdsourcing platforms, small and sparse datasets, and missing methodological alignment as critical open issues and formulate ten recommendations for research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.4 -->
                
            <!-- LLMs: 9.3 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3472
            </span>
            <a href="https://arxiv.org/abs/2504.08117" target="_blank" rel="noopener noreferrer">Design Activity for Robot Faces: Evaluating Child Responses To Expressive Faces</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Denielle Oliva, Joshua Knight, Tyler J Becker, Heather Amistani, Monica Nicolescu, David Feil-Seifer | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Facial expressiveness plays a crucial role in a robot's ability to engage and interact with children. Prior research has shown that expressive robots can enhance child engagement during human-robot interactions. However, many robots used in therapy settings feature non-personalized, static faces des</span>
            
            <span class="abstract-full" style="display: none;">Facial expressiveness plays a crucial role in a robot's ability to engage and interact with children. Prior research has shown that expressive robots can enhance child engagement during human-robot interactions. However, many robots used in therapy settings feature non-personalized, static faces designed with traditional facial feature considerations, which can limit the depth of interactions and emotional connections. Digital faces offer opportunities for personalization, yet the current landscape of robot face design lacks a dynamic, user-centered approach. Specifically, there is a significant research gap in designing robot faces based on child preferences. Instead, most robots in child-focused therapy spaces are developed from an adult-centric perspective. We present a novel study investigating the influence of child-drawn digital faces in child-robot interactions. This approach focuses on a design activity with children instructed to draw their own custom robot faces. We compare the perceptions of social intelligence (PSI) of two implementations: a generic digital face and a robot face, personalized using the user's drawn robot faces. The results of this study show the perceived social intelligence of a child-drawn robot was significantly higher compared to a generic face.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3816
            </span>
            <a href="https://arxiv.org/abs/2504.08484" target="_blank" rel="noopener noreferrer">Physics-informed data-driven control without persistence of excitation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Martina Vanelli, Julien M. Hendrickx | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We show that data that is not sufficiently informative to allow for system re-identification can still provide meaningful information when combined with external or physical knowledge of the system, such as bounded system matrix norms. We then illustrate how this information can be leveraged for saf</span>
            
            <span class="abstract-full" style="display: none;">We show that data that is not sufficiently informative to allow for system re-identification can still provide meaningful information when combined with external or physical knowledge of the system, such as bounded system matrix norms. We then illustrate how this information can be leveraged for safety and energy minimization problems and to enhance predictions in unmodelled dynamics. This preliminary work outlines key ideas toward using limited data for effective control by integrating physical knowledge of the system and exploiting interpolation conditions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 6.8 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 2.0 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3941
            </span>
            <a href="https://arxiv.org/abs/2408.01194" target="_blank" rel="noopener noreferrer">Frequency-Explicit Shape Holomorphy in Uncertainty Quantification for Acoustic Scattering</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ralf Hiptmair, Christoph Schwab, Euan A. Spence | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We consider frequency-domain acoustic scattering at a homogeneous star-shaped penetrable obstacle, whose shape is uncertain and modelled via a radial spectral parameterization with random coefficients. Using recent results on the stability of Helmholtz transmission problems with piecewise constant c</span>
            
            <span class="abstract-full" style="display: none;">We consider frequency-domain acoustic scattering at a homogeneous star-shaped penetrable obstacle, whose shape is uncertain and modelled via a radial spectral parameterization with random coefficients. Using recent results on the stability of Helmholtz transmission problems with piecewise constant coefficients from [A. Moiola and E. A. Spence, Acoustic transmission problems: wavenumber-explicit bounds and resonance-free regions, Mathematical Models and Methods in Applied Sciences, 29 (2019), pp. 317-354] we obtain frequency-explicit statements on the holomorphic dependence of the scattered field and the far-field pattern on the stochastic shape parameters. This paves the way for applying general results on the efficient construction of high-dimensional surrogate models. We also take into account the effect of domain truncation by means of perfectly matched layers (PML). In addition, spatial regularity estimates which are explicit in terms of the wavenumber $k$ permit us to quantify the impact of finite-element Galerkin discretization using high-order Lagrangian finite-element spaces.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.2 -->
                
            <!-- LLMs: 6.1 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Math: 2.4 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.3981
            </span>
            <a href="https://arxiv.org/abs/2409.16561" target="_blank" rel="noopener noreferrer">Supporting Co-Adaptive Machine Teaching through Human Concept Learning and Cognitive Theories</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Simret Araya Gebreegziabher, Yukun Yang, Elena L. Glassman, Toby Jia-Jun Li | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">An important challenge in interactive machine learning, particularly in subjective or ambiguous domains, is fostering bi-directional alignment between humans and models. Users teach models their concept definition through data labeling, while refining their own understandings throughout the process.</span>
            
            <span class="abstract-full" style="display: none;">An important challenge in interactive machine learning, particularly in subjective or ambiguous domains, is fostering bi-directional alignment between humans and models. Users teach models their concept definition through data labeling, while refining their own understandings throughout the process. To facilitate this, we introduce MOCHA, an interactive machine learning tool informed by two theories of human concept learning and cognition. First, it utilizes a neuro-symbolic pipeline to support Variation Theory-based counterfactual data generation. By asking users to annotate counterexamples that are syntactically and semantically similar to already-annotated data but predicted to have different labels, the system can learn more effectively while helping users understand the model and reflect on their own label definitions. Second, MOCHA uses Structural Alignment Theory to present groups of counterexamples, helping users comprehend alignable differences between data items and annotate them in batch. We validated MOCHA's effectiveness and usability through a lab study with 18 participants.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4127
            </span>
            <a href="https://arxiv.org/abs/2504.08382" target="_blank" rel="noopener noreferrer">An posteriori error estimator for discontinuous Galerkin discretisations of convection-diffusion problems with application to Earth's mantle convection simulations</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tiffany Barry, Andrea Cangiani, Samuel P. Cox, Emmanuil H. Georgoulis | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We present new aposteriori error estimates for the interior penalty discontinuous Galerkin method applied to non-stationary convection-diffusion equations. The focus is on strongly convection-dominated problems without zeroth-order reaction terms, which leads to the absence of positive L^2-like comp</span>
            
            <span class="abstract-full" style="display: none;">We present new aposteriori error estimates for the interior penalty discontinuous Galerkin method applied to non-stationary convection-diffusion equations. The focus is on strongly convection-dominated problems without zeroth-order reaction terms, which leads to the absence of positive L^2-like components. An important specific example is the energy/temperature equation of the Boussinesq system arising from the modelling of mantle convection of the Earth. The key mathematical challenge of mitigating the effects of exponential factors with respect to the final time, arising from the use of Gronwall-type arguments, is addressed by an exponential fitting technique. The latter results to a new class of aposteriori error estimates for the stationary problem, which are valid in cases of convection and reaction coefficient combinations not covered by the existing literature. This new class of estimators is combined with an elliptic reconstruction technique to derive new respective estimates for the non-stationary problem, exhibiting reduced dependence on Gronwall-type exponents and, thus, offer more accurate estimation for longer time intervals. We showcase the superior performance of the new class of aposteriori error estimators in driving mesh adaptivity in Earth's mantle convection simulations, in a setting where the energy/temperature equation is discretised by the discontinuous Galerkin method, coupled with the Taylor-Hood finite element for the momentum and mass conservation equations. We exploit the community code ASPECT, to present numerical examples showing the effectivity of the proposed approach.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 7.2 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Math: 3.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.3 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.5 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- GNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.414
            </span>
            <a href="https://arxiv.org/abs/2504.08480" target="_blank" rel="noopener noreferrer">Toward Realistic Adversarial Attacks in IDS: A Novel Feasibility Metric for Transferability</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sabrine Ennaji, Elhadj Benkhelifa, Luigi Vincenzo Mancini | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Transferability-based adversarial attacks exploit the ability of adversarial examples, crafted to deceive a specific source Intrusion Detection System (IDS) model, to also mislead a target IDS model without requiring access to the training data or any internal model parameters. These attacks exploit</span>
            
            <span class="abstract-full" style="display: none;">Transferability-based adversarial attacks exploit the ability of adversarial examples, crafted to deceive a specific source Intrusion Detection System (IDS) model, to also mislead a target IDS model without requiring access to the training data or any internal model parameters. These attacks exploit common vulnerabilities in machine learning models to bypass security measures and compromise systems. Although the transferability concept has been widely studied, its practical feasibility remains limited due to assumptions of high similarity between source and target models. This paper analyzes the core factors that contribute to transferability, including feature alignment, model architectural similarity, and overlap in the data distributions that each IDS examines. We propose a novel metric, the Transferability Feasibility Score (TFS), to assess the feasibility and reliability of such attacks based on these factors. Through experimental evidence, we demonstrate that TFS and actual attack success rates are highly correlated, addressing the gap between theoretical understanding and real-world impact. Our findings provide needed guidance for designing more realistic transferable adversarial attacks, developing robust defenses, and ultimately improving the security of machine learning-based IDS in critical systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.5 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4207
            </span>
            <a href="https://arxiv.org/abs/2504.08540" target="_blank" rel="noopener noreferrer">Datasets for Lane Detection in Autonomous Driving: A Comprehensive Review</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: J\"org Gamerdinger, Sven Teufel, Oliver Bringmann | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Accurate lane detection is essential for automated driving, enabling safe and reliable vehicle navigation in a variety of road scenarios. Numerous datasets have been introduced to support the development and evaluation of lane detection algorithms, each differing in terms of the amount of data, sens</span>
            
            <span class="abstract-full" style="display: none;">Accurate lane detection is essential for automated driving, enabling safe and reliable vehicle navigation in a variety of road scenarios. Numerous datasets have been introduced to support the development and evaluation of lane detection algorithms, each differing in terms of the amount of data, sensor types, annotation granularity, environmental conditions, and scenario diversity. This paper provides a comprehensive review of over 30 publicly available lane detection datasets, systematically analysing their characteristics, advantages and limitations. We classify these datasets based on key factors such as sensor resolution, annotation types and diversity of road and weather conditions. By identifying existing challenges and research gaps, we highlight opportunities for future dataset improvements that can further drive innovation in robust lane detection. This survey serves as a resource for researchers seeking appropriate datasets for lane detection, and contributes to the broader goal of advancing autonomous driving.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 8.4 -->
                
            <!-- LLMs: 8.3 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4303
            </span>
            <a href="https://arxiv.org/abs/2502.16701" target="_blank" rel="noopener noreferrer">Beyond Release: Access Considerations for Generative AI Systems</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Irene Solaiman, Rishi Bommasani, Dan Hendrycks, Ariel Herbert-Voss, Yacine Jernite, Aviya Skowron, Andrew Trask | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access r</span>
            
            <span class="abstract-full" style="display: none;">Generative AI release decisions determine whether system components are made available, but release does not address many other elements that change how users and stakeholders are able to engage with a system. Beyond release, access to system components informs potential risks and benefits. Access refers to practical needs, infrastructurally, technically, and societally, in order to use available components in some way. We deconstruct access along three axes: resourcing, technical usability, and utility. Within each category, a set of variables per system component clarify tradeoffs. For example, resourcing requires access to computing infrastructure to serve model weights. We also compare the accessibility of four high performance language models, two open-weight and two closed-weight, showing similar considerations for all based instead on access variables. Access variables set the foundation for being able to scale or increase access to users; we examine the scale of access and how scale affects ability to manage and intervene on risks. This framework better encompasses the landscape and risk-benefit tradeoffs of system releases to inform system release decisions, research, and policy.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.433
            </span>
            <a href="https://arxiv.org/abs/2411.00870" target="_blank" rel="noopener noreferrer">K-Means Clustering With Incomplete Data with the Use of Mahalanobis Distances</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lovis Kwasi Armah, Igor Melnykov | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Effectively applying the K-means algorithm to clustering tasks with incomplete features remains an important research area due to its impact on real-world applications. Recent work has shown that unifying K-means clustering and imputation into one single objective function and solving the resultant </span>
            
            <span class="abstract-full" style="display: none;">Effectively applying the K-means algorithm to clustering tasks with incomplete features remains an important research area due to its impact on real-world applications. Recent work has shown that unifying K-means clustering and imputation into one single objective function and solving the resultant optimization yield superior results compared to handling imputation and clustering separately.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4454
            </span>
            <a href="https://arxiv.org/abs/2310.08948" target="_blank" rel="noopener noreferrer">Federated Class-Incremental Learning with Prompting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xin Luo, Fang-Yi Liang, Jiale Liu, Yu-Wei Zhan, Zhen-Duo Chen, Xin-Shun Xu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As Web technology continues to develop, it has become increasingly common to use data stored on different clients. At the same time, federated learning has received widespread attention due to its ability to protect data privacy when let models learn from data which is distributed across various cli</span>
            
            <span class="abstract-full" style="display: none;">As Web technology continues to develop, it has become increasingly common to use data stored on different clients. At the same time, federated learning has received widespread attention due to its ability to protect data privacy when let models learn from data which is distributed across various clients. However, most existing works assume that the client's data are fixed. In real-world scenarios, such an assumption is most likely not true as data may be continuously generated and new classes may also appear. To this end, we focus on the practical and challenging federated class-incremental learning (FCIL) problem. For FCIL, the local and global models may suffer from catastrophic forgetting on old classes caused by the arrival of new classes and the data distributions of clients are non-independent and identically distributed (non-iid).</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4651
            </span>
            <a href="https://arxiv.org/abs/2504.08581" target="_blank" rel="noopener noreferrer">FMLGS: Fast Multilevel Language Embedded Gaussians for Part-level Interactive Agents</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Xin Tan, Yuzhou Ji, He Zhu, Yuan Xie | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality </span>
            
            <span class="abstract-full" style="display: none;">The semantically interactive radiance field has long been a promising backbone for 3D real-world applications, such as embodied AI to achieve scene understanding and manipulation. However, multi-granularity interaction remains a challenging task due to the ambiguity of language and degraded quality when it comes to queries upon object components. In this work, we present FMLGS, an approach that supports part-level open-vocabulary query within 3D Gaussian Splatting (3DGS). We propose an efficient pipeline for building and querying consistent object- and part-level semantics based on Segment Anything Model 2 (SAM2). We designed a semantic deviation strategy to solve the problem of language ambiguity among object parts, which interpolates the semantic features of fine-grained targets for enriched information. Once trained, we can query both objects and their describable parts using natural language. Comparisons with other state-of-the-art methods prove that our method can not only better locate specified part-level targets, but also achieve first-place performance concerning both speed and accuracy, where FMLGS is 98 x faster than LERF, 4 x faster than LangSplat and 2.5 x faster than LEGaussians. Meanwhile, we further integrate FMLGS as a virtual agent that can interactively navigate through 3D scenes, locate targets, and respond to user demands through a chat interface, which demonstrates the potential of our work to be further expanded and applied in the future.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 7.8 -->
                
            <!-- Quantum Computing: 4.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4676
            </span>
            <a href="https://arxiv.org/abs/2504.08713" target="_blank" rel="noopener noreferrer">ProtoECGNet: Case-Based Interpretable Deep Learning for Multi-Label ECG Classification with Contrastive Learning</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sahil Sethi, David Chen, Thomas Statchen, Michael C. Burkhart, Nipun Bhandari, Bashar Ramadan, Brett Beaulieu-Jones | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Deep learning-based electrocardiogram (ECG) classification has shown impressive performance but clinical adoption has been slowed by the lack of transparent and faithful explanations. Post hoc methods such as saliency maps may fail to reflect a model's true decision process. Prototype-based reasonin</span>
            
            <span class="abstract-full" style="display: none;">Deep learning-based electrocardiogram (ECG) classification has shown impressive performance but clinical adoption has been slowed by the lack of transparent and faithful explanations. Post hoc methods such as saliency maps may fail to reflect a model's true decision process. Prototype-based reasoning offers a more transparent alternative by grounding decisions in similarity to learned representations of real ECG segments, enabling faithful, case-based explanations. We introduce ProtoECGNet, a prototype-based deep learning model for interpretable, multi-label ECG classification. ProtoECGNet employs a structured, multi-branch architecture that reflects clinical interpretation workflows: it integrates a 1D CNN with global prototypes for rhythm classification, a 2D CNN with time-localized prototypes for morphology-based reasoning, and a 2D CNN with global prototypes for diffuse abnormalities. Each branch is trained with a prototype loss designed for multi-label learning, combining clustering, separation, diversity, and a novel contrastive loss that encourages appropriate separation between prototypes of unrelated classes while allowing clustering for frequently co-occurring diagnoses. We evaluate ProtoECGNet on all 71 diagnostic labels from the PTB-XL dataset, demonstrating competitive performance relative to state-of-the-art black-box models while providing structured, case-based explanations. To assess prototype quality, we conduct a structured clinician review of the final model's projected prototypes, finding that they are rated as representative and clear. ProtoECGNet shows that prototype learning can be effectively scaled to complex, multi-label time-series classification, offering a practical path toward transparent and trustworthy deep learning models for clinical decision support.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.9 -->
                
            <!-- LLMs: 9.3 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.472
            </span>
            <a href="https://arxiv.org/abs/2504.08277" target="_blank" rel="noopener noreferrer">Enabling Automatic Differentiation with Mollified Graph Neural Operators</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ryan Y. Lin, Julius Berner, Valentin Duruisseaux, David Pitt, Daniel Leibovici, Jean Kossaifi, Kamyar Azizzadenesheli, Anima Anandkumar | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Physics-informed neural operators offer a powerful framework for learning solution operators of partial differential equations (PDEs) by combining data and physics losses. However, these physics losses rely on derivatives. Computing these derivatives remains challenging, with spectral and finite dif</span>
            
            <span class="abstract-full" style="display: none;">Physics-informed neural operators offer a powerful framework for learning solution operators of partial differential equations (PDEs) by combining data and physics losses. However, these physics losses rely on derivatives. Computing these derivatives remains challenging, with spectral and finite difference methods introducing approximation errors due to finite resolution. Here, we propose the mollified graph neural operator (mGNO), the first method to leverage automatic differentiation and compute \emph{exact} gradients on arbitrary geometries. This enhancement enables efficient training on irregular grids and varying geometries while allowing seamless evaluation of physics losses at randomly sampled points for improved generalization. For a PDE example on regular grids, mGNO paired with autograd reduced the L2 relative data error by 20x compared to finite differences, although training was slower. It can also solve PDEs on unstructured point clouds seamlessly, using physics losses only, at resolutions vastly lower than those needed for finite differences to be accurate enough. On these unstructured point clouds, mGNO leads to errors that are consistently 2 orders of magnitude lower than machine learning baselines (Meta-PDE) for comparable runtimes, and also delivers speedups from 1 to 3 orders of magnitude compared to the numerical solver for similar accuracy. mGNOs can also be used to solve inverse design and shape optimization problems on complex geometries.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.5 -->
                
            <!-- Medicine: 8.4 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- GNN: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4743
            </span>
            <a href="https://arxiv.org/abs/2504.08192" target="_blank" rel="noopener noreferrer">SAEs $\textit{Can}$ Improve Unlearning: Dynamic Sparse Autoencoder Guardrails for Precision Unlearning in LLMs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Aashiq Muhamed, Jacopo Bonato, Mona Diab, Virginia Smith | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability </span>
            
            <span class="abstract-full" style="display: none;">Machine unlearning is a promising approach to improve LLM safety by removing unwanted knowledge from the model. However, prevailing gradient-based unlearning methods suffer from issues such as high computational costs, hyperparameter instability, poor sequential unlearning capability, vulnerability to relearning attacks, low data efficiency, and lack of interpretability. While Sparse Autoencoders are well-suited to improve these aspects by enabling targeted activation-based unlearning, prior approaches underperform gradient-based methods. This work demonstrates that, contrary to these earlier findings, SAEs can significantly improve unlearning when employed dynamically. We introduce $\textbf{Dynamic DAE Guardrails}$ (DSG), a novel method for precision unlearning that leverages principled feature selection and a dynamic classifier. Our experiments show DSG substantially outperforms leading unlearning methods, achieving superior forget-utility trade-offs. DSG addresses key drawbacks of gradient-based approaches for unlearning -- offering enhanced computational efficiency and stability, robust performance in sequential unlearning, stronger resistance to relearning attacks, better data efficiency including zero-shot settings, and more interpretable unlearning.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.0 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- 3D: 1.7 -->
                
            <!-- Math: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.4962
            </span>
            <a href="https://arxiv.org/abs/2504.08049" target="_blank" rel="noopener noreferrer">Patch distribution modeling framework adaptive cosine estimator (PaDiM-ACE) for anomaly detection and localization in synthetic aperture radar imagery</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Angelina Ibarra, Joshua Peeples | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work presents a new approach to anomaly detection and localization in synthetic aperture radar imagery (SAR), expanding upon the existing patch distribution modeling framework (PaDiM). We introduce the adaptive cosine estimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at i</span>
            
            <span class="abstract-full" style="display: none;">This work presents a new approach to anomaly detection and localization in synthetic aperture radar imagery (SAR), expanding upon the existing patch distribution modeling framework (PaDiM). We introduce the adaptive cosine estimator (ACE) detection statistic. PaDiM uses the Mahalanobis distance at inference, an unbounded metric. ACE instead uses the cosine similarity metric, providing bounded anomaly detection scores. The proposed method is evaluated across multiple SAR datasets, with performance metrics including the area under the receiver operating curve (AUROC) at the image and pixel level, aiming for increased performance in anomaly detection and localization of SAR imagery. The code is publicly available: https://github.com/Advanced-Vision-and-Learning-Lab/PaDiM-LACE.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.2 -->
                
            <!-- Medicine: 10.0 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- GNN: 2.0 -->
                
            <!-- 3D: 2.0 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Math: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- T2I: 1.1 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.499
            </span>
            <a href="https://arxiv.org/abs/2502.06674" target="_blank" rel="noopener noreferrer">RAILS: Risk-Aware Iterated Local Search for Joint SLA Decomposition and Service Provider Management in Multi-Domain Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Cyril Shih-Huan Hsu, Chrysa Papagianni, Paola Grosso | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The emergence of the fifth generation (5G) technology has transformed mobile networks into multi-service environments, necessitating efficient network slicing to meet diverse Service Level Agreements (SLAs). SLA decomposition across multiple network domains, each potentially managed by different ser</span>
            
            <span class="abstract-full" style="display: none;">The emergence of the fifth generation (5G) technology has transformed mobile networks into multi-service environments, necessitating efficient network slicing to meet diverse Service Level Agreements (SLAs). SLA decomposition across multiple network domains, each potentially managed by different service providers, poses a significant challenge due to limited visibility into real-time underlying domain conditions. This paper introduces Risk-Aware Iterated Local Search (RAILS), a novel risk model-driven meta-heuristic framework designed to jointly address SLA decomposition and service provider selection in multi-domain networks. By integrating online risk modeling with iterated local search principles, RAILS effectively navigates the complex optimization landscape, utilizing historical feedback from domain controllers. We formulate the joint problem as a Mixed-Integer Nonlinear Programming (MINLP) problem and prove its NP-hardness. Extensive simulations demonstrate that RAILS achieves near-optimal performance, offering an efficient, real-time solution for adaptive SLA management in modern multi-domain networks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- GNN: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5005
            </span>
            <a href="https://arxiv.org/abs/2504.08141" target="_blank" rel="noopener noreferrer">Variational quantum and neural quantum states algorithms for the linear complementarity problem</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Saibal De, Oliver Knitter, Rohan Kodati, Paramsothy Jayakumar, James Stokes, Shravan Veerapaneni | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of </span>
            
            <span class="abstract-full" style="display: none;">Variational quantum algorithms (VQAs) are promising hybrid quantum-classical methods designed to leverage the computational advantages of quantum computing while mitigating the limitations of current noisy intermediate-scale quantum (NISQ) hardware. Although VQAs have been demonstrated as proofs of concept, their practical utility in solving real-world problems -- and whether quantum-inspired classical algorithms can match their performance -- remains an open question. We present a novel application of the variational quantum linear solver (VQLS) and its classical neural quantum states-based counterpart, the variational neural linear solver (VNLS), as key components within a minimum map Newton solver for a complementarity-based rigid body contact model. We demonstrate using the VNLS that our solver accurately simulates the dynamics of rigid spherical bodies during collision events. These results suggest that quantum and quantum-inspired linear algebra algorithms can serve as viable alternatives to standard linear algebra solvers for modeling certain physical systems.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 4.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5172
            </span>
            <a href="https://arxiv.org/abs/2409.00536" target="_blank" rel="noopener noreferrer">Formal Verification and Control with Conformal Prediction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lars Lindemann, Yiqi Zhao, Xinyi Yu, George J. Pappas, Jyotirmoy V. Deshmukh | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In this survey, we design formal verification and control algorithms for autonomous systems with practical safety guarantees using conformal prediction (CP), a statistical tool for uncertainty quantification. We focus on learning-enabled autonomous systems (LEASs) in which the complexity of learning</span>
            
            <span class="abstract-full" style="display: none;">In this survey, we design formal verification and control algorithms for autonomous systems with practical safety guarantees using conformal prediction (CP), a statistical tool for uncertainty quantification. We focus on learning-enabled autonomous systems (LEASs) in which the complexity of learning-enabled components (LECs) is a major bottleneck that hampers the use of existing model-based verification and design techniques. Instead, we advocate for the use of CP, and we will demonstrate its use in formal verification, systems and control theory, and robotics. We argue that CP is specifically useful due to its simplicity (easy to understand, use, and modify), generality (requires no assumptions on learned models and data distributions, i.e., is distribution-free), and efficiency (real-time capable and accurate).</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.8 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5176
            </span>
            <a href="https://arxiv.org/abs/2504.07574" target="_blank" rel="noopener noreferrer">Malware analysis assisted by AI with R2AI</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Axelle Apvrille, Daniel Nakov | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This research studies the quality, speed and cost of malware analysis assisted by artificial intelligence. It focuses on Linux and IoT malware of 2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all malware and not all LLMs are equivalent but the study shows excellent result</span>
            
            <span class="abstract-full" style="display: none;">This research studies the quality, speed and cost of malware analysis assisted by artificial intelligence. It focuses on Linux and IoT malware of 2024-2025, and uses r2ai, the AI extension of Radare2's disassembler. Not all malware and not all LLMs are equivalent but the study shows excellent results with Claude 3.5 and 3.7 Sonnet. Despite a few errors, the quality of analysis is overall equal or better than without AI assistance. For good results, the AI cannot operate alone and must constantly be guided by an experienced analyst. The gain of speed is largely visible with AI assistance, even when taking account the time to understand AI's hallucinations, exaggerations and omissions. The cost is usually noticeably lower than the salary of a malware analyst, but attention and guidance is needed to keep it under control in cases where the AI would naturally loop without showing progress.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.4 -->
                
            <!-- Medicine: 8.7 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5304
            </span>
            <a href="https://arxiv.org/abs/2504.08386" target="_blank" rel="noopener noreferrer">PCA-RAG: Principal Component Analysis for Efficient Retrieval-Augmented Generation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Arman Khaledian, Amirreza Ghadiridehkordi, Nariman Khaledian | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for grounding large language models in external knowledge sources, improving the precision of agents responses. However, high-dimensional language model embeddings, often in the range of hundreds to thousands of dimensions, can </span>
            
            <span class="abstract-full" style="display: none;">Retrieval-Augmented Generation (RAG) has emerged as a powerful paradigm for grounding large language models in external knowledge sources, improving the precision of agents responses. However, high-dimensional language model embeddings, often in the range of hundreds to thousands of dimensions, can present scalability challenges in terms of storage and latency, especially when processing massive financial text corpora. This paper investigates the use of Principal Component Analysis (PCA) to reduce embedding dimensionality, thereby mitigating computational bottlenecks without incurring large accuracy losses. We experiment with a real-world dataset and compare different similarity and distance metrics under both full-dimensional and PCA-compressed embeddings. Our results show that reducing vectors from 3,072 to 110 dimensions provides a sizeable (up to $60\times$) speedup in retrieval operations and a $\sim 28.6\times$ reduction in index size, with only moderate declines in correlation metrics relative to human-annotated similarity scores. These findings demonstrate that PCA-based compression offers a viable balance between retrieval fidelity and resource efficiency, essential for real-time systems such as Zanista AI's \textit{Newswitch} platform. Ultimately, our study underscores the practicality of leveraging classical dimensionality reduction techniques to scale RAG architectures for knowledge-intensive applications in finance and trading, where speed, memory efficiency, and accuracy must jointly be optimized.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.7 -->
                
            <!-- Medicine: 8.0 -->
                
            <!-- Quantum Computing: 4.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5396
            </span>
            <a href="https://arxiv.org/abs/2504.08115" target="_blank" rel="noopener noreferrer">Benchmarking Suite for Synthetic Aperture Radar Imagery Anomaly Detection (SARIAD) Algorithms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lucian Chauvina, Somil Guptac, Angelina Ibarrac, Joshua Peeples | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Anomaly detection is a key research challenge in computer vision and machine learning with applications in many fields from quality control to radar imaging. In radar imaging, specifically synthetic aperture radar (SAR), anomaly detection can be used for the classification, detection, and segmentati</span>
            
            <span class="abstract-full" style="display: none;">Anomaly detection is a key research challenge in computer vision and machine learning with applications in many fields from quality control to radar imaging. In radar imaging, specifically synthetic aperture radar (SAR), anomaly detection can be used for the classification, detection, and segmentation of objects of interest. However, there is no method for developing and benchmarking these methods on SAR imagery. To address this issue, we introduce SAR imagery anomaly detection (SARIAD). In conjunction with Anomalib, a deep-learning library for anomaly detection, SARIAD provides a comprehensive suite of algorithms and datasets for assessing and developing anomaly detection approaches on SAR imagery. SARIAD specifically integrates multiple SAR datasets along with tools to effectively apply various anomaly detection algorithms to SAR imagery. Several anomaly detection metrics and visualizations are available. Overall, SARIAD acts as a central package for benchmarking SAR models and datasets to allow for reproducible research in the field of anomaly detection in SAR imagery. This package is publicly available: https://github.com/Advanced-Vision-and-Learning-Lab/SARIAD.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.5 -->
                
            <!-- LLMs: 8.8 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.576
            </span>
            <a href="https://arxiv.org/abs/2504.08537" target="_blank" rel="noopener noreferrer">Lexical Bundle Frequency as a Construct-Relevant Candidate Feature in Automated Scoring of L2 Academic Writing</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Burak Senel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Automated scoring (AS) systems are increasingly used for evaluating L2 writing, but require ongoing refinement for construct validity. While prior work suggested lexical bundles (LBs) - recurrent multi-word sequences satisfying certain frequency criteria - could inform assessment, their empirical in</span>
            
            <span class="abstract-full" style="display: none;">Automated scoring (AS) systems are increasingly used for evaluating L2 writing, but require ongoing refinement for construct validity. While prior work suggested lexical bundles (LBs) - recurrent multi-word sequences satisfying certain frequency criteria - could inform assessment, their empirical integration into AS models needs further investigation. This study tested the impact of incorporating LB frequency features into an AS model for TOEFL independent writing tasks. Analyzing a sampled subcorpus (N=1,225 essays, 9 L1s) from the TOEFL11 corpus, scored by ETS-trained raters (Low, Medium, High), 3- to 9-word LBs were extracted, distinguishing prompt-specific from non-prompt types. A baseline Support Vector Machine (SVM) scoring model using established linguistic features (e.g., mechanics, cohesion, sophistication) was compared against an extended model including three aggregate LB frequency features (total prompt, total non-prompt, overall total). Results revealed significant, though generally small-effect, relationships between LB frequency (especially non-prompt bundles) and proficiency (p < .05). Mean frequencies suggested lower proficiency essays used more LBs overall. Critically, the LB-enhanced model improved agreement with human raters (Quadratic Cohen's Kappa +2.05%, overall Cohen's Kappa +5.63%), with notable gains for low (+10.1% exact agreement) and medium (+14.3% Cohen's Kappa) proficiency essays. These findings demonstrate that integrating aggregate LB frequency offers potential for developing more linguistically informed and accurate AS systems, particularly for differentiating developing L2 writers.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- T2I: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.5852
            </span>
            <a href="https://arxiv.org/abs/2504.08389" target="_blank" rel="noopener noreferrer">Light-YOLOv8-Flame: A Lightweight High-Performance Flame Detection Algorithm</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiawei Lan, Zhibiao Wang, Haoyang Yu, Ye Tao, Wenhua Cui | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Fire detection algorithms, particularly those based on computer vision, encounter significant challenges such as high computational costs and delayed response times, which hinder their application in real-time systems. To address these limitations, this paper introduces Light-YOLOv8-Flame, a lightwe</span>
            
            <span class="abstract-full" style="display: none;">Fire detection algorithms, particularly those based on computer vision, encounter significant challenges such as high computational costs and delayed response times, which hinder their application in real-time systems. To address these limitations, this paper introduces Light-YOLOv8-Flame, a lightweight flame detection algorithm specifically designed for fast and efficient real-time deployment. The proposed model enhances the YOLOv8 architecture through the substitution of the original C2f module with the FasterNet Block module. This new block combines Partial Convolution (PConv) and Convolution (Conv) layers, reducing both computational complexity and model size. A dataset comprising 7,431 images, representing both flame and non-flame scenarios, was collected and augmented for training purposes. Experimental findings indicate that the modified YOLOv8 model achieves a 0.78% gain in mean average precision (mAP) and a 2.05% boost in recall, while reducing the parameter count by 25.34%, with only a marginal decrease in precision by 0.82%. These findings highlight that Light-YOLOv8-Flame offers enhanced detection performance and speed, making it well-suited for real-time fire detection on resource-constrained devices.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.3 -->
                
            <!-- LLMs: 8.9 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- 3D: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6122
            </span>
            <a href="https://arxiv.org/abs/2504.08207" target="_blank" rel="noopener noreferrer">DRAFT-ing Architectural Design Decisions using LLMs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Rudra Dhar, Adyansh Kakran, Amey Karan, Karthik Vaidhyanathan, Vasudeva Varma | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Architectural Knowledge Management (AKM) is crucial for software development but remains challenging due to the lack of standardization and high manual effort. Architecture Decision Records (ADRs) provide a structured approach to capture Architecture Design Decisions (ADDs), but their adoption is li</span>
            
            <span class="abstract-full" style="display: none;">Architectural Knowledge Management (AKM) is crucial for software development but remains challenging due to the lack of standardization and high manual effort. Architecture Decision Records (ADRs) provide a structured approach to capture Architecture Design Decisions (ADDs), but their adoption is limited due to the manual effort involved and insufficient tool support. Our previous work has shown that Large Language Models (LLMs) can assist in generating ADDs. However, simply prompting the LLM does not produce quality ADDs. Moreover, using third-party LLMs raises privacy concerns, while self-hosting them poses resource challenges.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 11.4 -->
                
            <!-- Medicine: 9.1 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6246
            </span>
            <a href="https://arxiv.org/abs/2502.08576" target="_blank" rel="noopener noreferrer">Mapping the Landscape of Generative AI in Network Monitoring and Management</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Giampaolo Bovenzi, Francesco Cerasuolo, Domenico Ciuonzo, Davide Di Monda, Idio Guarino, Antonio Montieri, Valerio Persico, Antonio Pescap\`e | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as</span>
            
            <span class="abstract-full" style="display: none;">Generative Artificial Intelligence (GenAI) models such as LLMs, GPTs, and Diffusion Models have recently gained widespread attention from both the research and the industrial communities. This survey explores their application in network monitoring and management, focusing on prominent use cases, as well as challenges and opportunities. We discuss how network traffic generation and classification, network intrusion detection, networked system log analysis, and network digital assistance can benefit from the use of GenAI models. Additionally, we provide an overview of the available GenAI models, datasets for large-scale training phases, and platforms for the development of such models. Finally, we discuss research directions that potentially mitigate the roadblocks to the adoption of GenAI for network monitoring and management. Our investigation aims to map the current landscape and pave the way for future research in leveraging GenAI for network monitoring and management.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 10.1 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.2 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6305
            </span>
            <a href="https://arxiv.org/abs/2504.08213" target="_blank" rel="noopener noreferrer">Big Meaning: Qualitative Analysis on Large Bodies of Data Using AI</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Samuel Flanders, Melati Nungsari, Mark Cheong Wing Loong | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This study introduces a framework that leverages AI-generated descriptive codes to indicate a text's fecundity--the density of unique human-generated codes--in thematic analysis. Rather than replacing human interpretation, AI-generated codes guide the selection of texts likely to yield richer qualit</span>
            
            <span class="abstract-full" style="display: none;">This study introduces a framework that leverages AI-generated descriptive codes to indicate a text's fecundity--the density of unique human-generated codes--in thematic analysis. Rather than replacing human interpretation, AI-generated codes guide the selection of texts likely to yield richer qualitative insights. Using a dataset of 2,530 Malaysian news articles on refugee attitudes, we compare AI-selected documents to randomly chosen ones by having three human coders independently derive codes. The results demonstrate that AI-selected texts exhibit approximately twice the fecundity. Our findings support the use of AI-generated codes as an effective proxy for identifying documents with a high potential for meaning-making in thematic analysis.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 8.9 -->
                
            <!-- Medicine: 8.5 -->
                
            <!-- Quantum Computing: 4.8 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6351
            </span>
            <a href="https://arxiv.org/abs/2504.08296" target="_blank" rel="noopener noreferrer">Generative AI for Film Creation: A Survey of Recent Advances</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ruihan Zhang, Borou Yu, Jiajian Min, Yetong Xin, Zheng Wei, Juncheng Nemo Shi, Mingzhen Huang, Xianghao Kong, Nix Liu Xin, Shanshan Jiang, Praagya Bahuguna, Mark Chan, Khushi Hora, Lijian Yang, Yongqi Liang, Runhe Bian, Yunlei Liu, Isabela Campillo Valencia, Patricia Morales Tredinick, Ilia Kozlov, Sijia Jiang, Peiwen Huang, Na Chen, Xuanxuan Liu, Anyi Rao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generative AI (GenAI) is transforming filmmaking, equipping artists with tools like text-to-image and image-to-video diffusion, neural radiance fields, avatar generation, and 3D synthesis. This paper examines the adoption of these technologies in filmmaking, analyzing workflows from recent AI-driven</span>
            
            <span class="abstract-full" style="display: none;">Generative AI (GenAI) is transforming filmmaking, equipping artists with tools like text-to-image and image-to-video diffusion, neural radiance fields, avatar generation, and 3D synthesis. This paper examines the adoption of these technologies in filmmaking, analyzing workflows from recent AI-driven films to understand how GenAI contributes to character creation, aesthetic styling, and narration. We explore key strategies for maintaining character consistency, achieving stylistic coherence, and ensuring motion continuity. Additionally, we highlight emerging trends such as the growing use of 3D generation and the integration of real footage with AI-generated elements.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.9 -->
                
            <!-- Medicine: 8.3 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.5 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6394
            </span>
            <a href="https://arxiv.org/abs/2504.08593" target="_blank" rel="noopener noreferrer">Hands-On: Segmenting Individual Signs from Continuous Sequences</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Low Jian He, Harry Walsh, Ozge Mercanoglu Sincan, Richard Bowden | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This work tackles the challenge of continuous sign language segmentation, a key task with huge implications for sign language translation and data annotation. We propose a transformer-based architecture that models the temporal dynamics of signing and frames segmentation as a sequence labeling probl</span>
            
            <span class="abstract-full" style="display: none;">This work tackles the challenge of continuous sign language segmentation, a key task with huge implications for sign language translation and data annotation. We propose a transformer-based architecture that models the temporal dynamics of signing and frames segmentation as a sequence labeling problem using the Begin-In-Out (BIO) tagging scheme. Our method leverages the HaMeR hand features, and is complemented with 3D Angles. Extensive experiments show that our model achieves state-of-the-art results on the DGS Corpus, while our features surpass prior benchmarks on BSLCorpus.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.9 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.7 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6464
            </span>
            <a href="https://arxiv.org/abs/2504.08305" target="_blank" rel="noopener noreferrer">A 55-nm SRAM Chip Scanning Errors Every 125 ns for Event-Wise Soft Error Measurement</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yuibi Gomi, Akira Sato, Waleed Madany, Kenichi Okada, Satoshi Adachi, Masatoshi Itoh, Masanori Hashimoto | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">We developed a 55 nm CMOS SRAM chip that scans all data every 125 ns and outputs timestamped soft error data via an SPI interface through a FIFO. The proposed system, consisting of the developed chip and particle detectors, enables event-wise soft error measurement and precise identification of SBUs</span>
            
            <span class="abstract-full" style="display: none;">We developed a 55 nm CMOS SRAM chip that scans all data every 125 ns and outputs timestamped soft error data via an SPI interface through a FIFO. The proposed system, consisting of the developed chip and particle detectors, enables event-wise soft error measurement and precise identification of SBUs and MCUs, thus resolving misclassifications such as Pseudo- and Distant MCUs that conventional methods cannot distinguish. An 80-MeV proton irradiation experiment at RASiS, Tohoku University verified the system operation. Timestamps between the SRAM chip and the particle detectors were successfully synchronized, accounting for PLL disturbances caused by radiation. Event building was achieved by determining a reset offset with sub-ns resolution, and spatial synchronization was maintained within several tens of micrometers.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 7.9 -->
                
            <!-- Quantum Computing: 3.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.6845
            </span>
            <a href="https://arxiv.org/abs/2504.08349" target="_blank" rel="noopener noreferrer">A Proof-Theoretic Approach to the Semantics of Classical Linear Logic</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Victor Barroso-Nascimento, Ekaterina Piotrovskaya, Elaine Pimentel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Linear logic (LL) is a resource-aware, abstract logic programming language that refines both classical and intuitionistic logic. Linear logic semantics is typically presented in one of two ways: by associating each formula with the set of all contexts that can be used to prove it (e.g. phase semanti</span>
            
            <span class="abstract-full" style="display: none;">Linear logic (LL) is a resource-aware, abstract logic programming language that refines both classical and intuitionistic logic. Linear logic semantics is typically presented in one of two ways: by associating each formula with the set of all contexts that can be used to prove it (e.g. phase semantics) or by assigning meaning directly to proofs (e.g. coherence spaces).</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.8 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.712
            </span>
            <a href="https://arxiv.org/abs/2504.08227" target="_blank" rel="noopener noreferrer">DaemonSec: Examining the Role of Machine Learning for Daemon Security in Linux Environments</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sheikh Muhammad Farjad | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">DaemonSec is an early-stage startup exploring machine learning (ML)-based security for Linux daemons, a critical yet often overlooked attack surface. While daemon security remains underexplored, conventional defenses struggle against adaptive threats and zero-day exploits. To assess the perspectives</span>
            
            <span class="abstract-full" style="display: none;">DaemonSec is an early-stage startup exploring machine learning (ML)-based security for Linux daemons, a critical yet often overlooked attack surface. While daemon security remains underexplored, conventional defenses struggle against adaptive threats and zero-day exploits. To assess the perspectives of IT professionals on ML-driven daemon protection, a systematic interview study based on semi-structured interviews was conducted with 22 professionals from industry and academia. The study evaluates adoption, feasibility, and trust in ML-based security solutions. While participants recognized the potential of ML for real-time anomaly detection, findings reveal skepticism toward full automation, limited security awareness among non-security roles, and concerns about patching delays creating attack windows. This paper presents the methods, key findings, and implications for advancing ML-driven daemon security in industry.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Quantum Computing: 4.7 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.805
            </span>
            <a href="https://arxiv.org/abs/2503.14681" target="_blank" rel="noopener noreferrer">DPImageBench: A Unified Benchmark for Differentially Private Image Synthesis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Chen Gong, Kecen Li, Zinan Lin, Tianhao Wang | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Differentially private (DP) image synthesis aims to generate artificial images that retain the properties of sensitive images while protecting the privacy of individual images within the dataset. Despite recent advancements, we find that inconsistent--and sometimes flawed--evaluation protocols have </span>
            
            <span class="abstract-full" style="display: none;">Differentially private (DP) image synthesis aims to generate artificial images that retain the properties of sensitive images while protecting the privacy of individual images within the dataset. Despite recent advancements, we find that inconsistent--and sometimes flawed--evaluation protocols have been applied across studies. This not only impedes the understanding of current methods but also hinders future advancements.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 9.0 -->
                
            <!-- Quantum Computing: 5.0 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Math: 1.9 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.8396
            </span>
            <a href="https://arxiv.org/abs/2504.08252" target="_blank" rel="noopener noreferrer">Stereophotoclinometry Revisited</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Travis Driver, Andrew Vaughan, Yang Cheng, Adnan Ansar, John Christian, Panagiotis Tsiotras | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Image-based surface reconstruction and characterization is crucial for missions to small celestial bodies, as it informs mission planning, navigation, and scientific analysis. However, current state-of-the-practice methods, such as stereophotoclinometry (SPC), rely heavily on human-in-the-loop verif</span>
            
            <span class="abstract-full" style="display: none;">Image-based surface reconstruction and characterization is crucial for missions to small celestial bodies, as it informs mission planning, navigation, and scientific analysis. However, current state-of-the-practice methods, such as stereophotoclinometry (SPC), rely heavily on human-in-the-loop verification and high-fidelity a priori information. This paper proposes Photoclinometry-from-Motion (PhoMo), a novel framework that incorporates photoclinometry techniques into a keypoint-based structure-from-motion (SfM) system to estimate the surface normal and albedo at detected landmarks to improve autonomous surface and shape characterization of small celestial bodies from in-situ imagery. In contrast to SPC, we forego the expensive maplet estimation step and instead use dense keypoint measurements and correspondences from an autonomous keypoint detection and matching method based on deep learning. Moreover, we develop a factor graph-based approach allowing for simultaneous optimization of the spacecraft's pose, landmark positions, Sun-relative direction, and surface normals and albedos via fusion of Sun vector measurements and image keypoint measurements. The proposed framework is validated on real imagery taken by the Dawn mission to the asteroid 4 Vesta and the minor planet 1 Ceres and compared against an SPC reconstruction, where we demonstrate superior rendering performance compared to an SPC solution and precise alignment to a stereophotogrammetry (SPG) solution without relying on any a priori camera pose and topography information or humans-in-the-loop.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.3 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Quantum Computing: 3.6 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -7.8982
            </span>
            <a href="https://arxiv.org/abs/2504.05636" target="_blank" rel="noopener noreferrer">A Multi-Modal AI System for Screening Mammography: Integrating 2D and 3D Imaging to Improve Breast Cancer Detection in a Prospective Clinical Study</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jungkyu Park, Jan Witowski, Yanqi Xu, Hari Trivedi, Judy Gichoya, Beatrice Brown-Mulry, Malte Westerhoff, Linda Moy, Laura Heacock, Alana Lewin, Krzysztof J. Geras | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Although digital breast tomosynthesis (DBT) improves diagnostic performance over full-field digital mammography (FFDM), false-positive recalls remain a concern in breast cancer screening. We developed a multi-modal artificial intelligence system integrating FFDM, synthetic mammography, and DBT to pr</span>
            
            <span class="abstract-full" style="display: none;">Although digital breast tomosynthesis (DBT) improves diagnostic performance over full-field digital mammography (FFDM), false-positive recalls remain a concern in breast cancer screening. We developed a multi-modal artificial intelligence system integrating FFDM, synthetic mammography, and DBT to provide breast-level predictions and bounding-box localizations of suspicious findings. Our AI system, trained on approximately 500,000 mammography exams, achieved 0.945 AUROC on an internal test set. It demonstrated capacity to reduce recalls by 31.7% and radiologist workload by 43.8% while maintaining 100% sensitivity, underscoring its potential to improve clinical workflows. External validation confirmed strong generalizability, reducing the gap to a perfect AUROC by 35.31%-69.14% relative to strong baselines. In prospective deployment across 18 sites, the system reduced recall rates for low-risk cases. An improved version, trained on over 750,000 exams with additional labels, further reduced the gap by 18.86%-56.62% across large external datasets. Overall, these results underscore the importance of utilizing all available imaging modalities, demonstrate the potential for clinical impact, and indicate feasibility of further reduction of the test error with increased training set when using large-capacity neural networks.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 9.0 -->
                
            <!-- Quantum Computing: 3.8 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.0338
            </span>
            <a href="https://arxiv.org/abs/2504.07099" target="_blank" rel="noopener noreferrer">Beyond the Time Domain: Recent Advances on Frequency Transforms in Time Series Analysis</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Qianru Zhang, Peng Yang, Honggang Wen, Xinzhu Li, Haixin Wang, Fang Sun, Zezheng Song, Zhichen Lai, Rui Ma, Ruihua Han, Tailin Wu, Siu-Ming Yiu, Yizhou Sun, Hongzhi Yin | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">The field of time series analysis has seen significant progress, yet traditional methods predominantly operate in temporal or spatial domains, overlooking the potential of frequency-based representations. This survey addresses this gap by providing the first comprehensive review of frequency transfo</span>
            
            <span class="abstract-full" style="display: none;">The field of time series analysis has seen significant progress, yet traditional methods predominantly operate in temporal or spatial domains, overlooking the potential of frequency-based representations. This survey addresses this gap by providing the first comprehensive review of frequency transform techniques-Fourier, Laplace, and Wavelet Transforms-in time series. We systematically explore their applications, strengths, and limitations, offering a comprehensive review and an up-to-date pipeline of recent advancements. By highlighting their transformative potential in time series applications including finance, molecular, weather, etc. This survey serves as a foundational resource for researchers, bridging theoretical insights with practical implementations. A curated GitHub repository further supports reproducibility and future research.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.5 -->
                
            <!-- Medicine: 8.8 -->
                
            <!-- Quantum Computing: 4.0 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.0718
            </span>
            <a href="https://arxiv.org/abs/2504.08150" target="_blank" rel="noopener noreferrer">Beyond Feature Importance: Feature Interactions in Predicting Post-Stroke Rigidity with Graph Explainable AI</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Jiawei Xu, Yonggeon Lee, Anthony Elkommos Youssef, Eunjin Yun, Tinglin Huang, Tianjian Guo, Hamidreza Saber, Rex Ying, Ying Ding | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This study addresses the challenge of predicting post-stroke rigidity by emphasizing feature interactions through graph-based explainable AI. Post-stroke rigidity, characterized by increased muscle tone and stiffness, significantly affects survivors' mobility and quality of life. Despite its prevale</span>
            
            <span class="abstract-full" style="display: none;">This study addresses the challenge of predicting post-stroke rigidity by emphasizing feature interactions through graph-based explainable AI. Post-stroke rigidity, characterized by increased muscle tone and stiffness, significantly affects survivors' mobility and quality of life. Despite its prevalence, early prediction remains limited, delaying intervention. We analyze 519K stroke hospitalization records from the Healthcare Cost and Utilization Project dataset, where 43% of patients exhibited rigidity. We compare traditional approaches such as Logistic Regression, XGBoost, and Transformer with graph-based models like Graphormer and Graph Attention Network. These graph models inherently capture feature interactions and incorporate intrinsic or post-hoc explainability. Our results show that graph-based methods outperform others (AUROC 0.75), identifying key predictors such as NIH Stroke Scale and APR-DRG mortality risk scores. They also uncover interactions missed by conventional models. This research provides a novel application of graph-based XAI in stroke prognosis, with potential to guide early identification and personalized rehabilitation strategies.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.2 -->
                
            <!-- Medicine: 8.1 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Math: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Robotics: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.082
            </span>
            <a href="https://arxiv.org/abs/2504.08555" target="_blank" rel="noopener noreferrer">Control Co-Design Under Uncertainty for Offshore Wind Farms: Optimizing Grid Integration, Energy Storage, and Market Participation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Himanshu Sharma, Wei Wang, Bowen Huang, Buxin She, Thiagarajan Ramachandaran | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Offshore wind farms (OWFs) are set to significantly contribute to global decarbonization efforts. Developers often use a sequential approach to optimize design variables and market participation for grid-integrated offshore wind farms. However, this method can lead to sub-optimal system performance,</span>
            
            <span class="abstract-full" style="display: none;">Offshore wind farms (OWFs) are set to significantly contribute to global decarbonization efforts. Developers often use a sequential approach to optimize design variables and market participation for grid-integrated offshore wind farms. However, this method can lead to sub-optimal system performance, and uncertainties associated with renewable resources are often overlooked in decision-making. This paper proposes a control co-design approach, optimizing design and control decisions for integrating OWFs into the power grid while considering energy market and primary frequency market participation. Additionally, we introduce optimal sizing solutions for energy storage systems deployed onshore to enhance revenue for OWF developers over time. This framework addresses uncertainties related to wind resources and energy prices. We analyze five U.S. west-coast offshore wind farm locations and potential interconnection points, as identified by the Bureau of Ocean Energy Management (BOEM). Results show that optimized control co-design solutions can increase market revenue by 3.2\% and provide flexibility in managing wind resource uncertainties.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.6 -->
                
            <!-- Medicine: 8.6 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.0888
            </span>
            <a href="https://arxiv.org/abs/2502.12615" target="_blank" rel="noopener noreferrer">Generalized Hofstadter functions $G, H$ and beyond: numeration systems and discrepancy</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Pierre Letouzey (IRIF, PICUBE) | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Hofstadter's $G$ function is recursively defined via $G(0)=0$ and then $G(n)=n-G(G(n-1))$. Following Hofstadter, a family $(F_k)$ of similar functions is obtained by varying the number $k$ of nested recursive calls in this equation. We study here some Fibonacci-like sequences that are deeply connect</span>
            
            <span class="abstract-full" style="display: none;">Hofstadter's $G$ function is recursively defined via $G(0)=0$ and then $G(n)=n-G(G(n-1))$. Following Hofstadter, a family $(F_k)$ of similar functions is obtained by varying the number $k$ of nested recursive calls in this equation. We study here some Fibonacci-like sequences that are deeply connected with these functions $F_k$. In particular, the Zeckendorf theorem can be adapted to provide digital expansions via sums of terms of these sequences. On these digital expansions, the functions $F_k$ are acting as right shifts of the digits. These Fibonacci-like sequences can be expressed in terms of zeros of the polynomial $X^k{-}X^{k-1}{-}1$. Considering now the discrepancy of each function $F_k$, i.e., the maximal distance between $F_k$ and its linear equivalent, we retrieve the fact that this discrepancy is finite exactly when $k \le 4$. Thanks to that, we solve two twenty-year-old OEIS conjectures stating how close the functions $F_3$ and $F_4$ are from the integer parts of their linear equivalents. Moreover we establish that $F_k$ can coincide exactly with such an integer part only when $k\le 2$, while $F_k$ is almost additive exactly when $k \le 4$. Finally, a nice fractal shape a la Rauzy has been encountered when investigating the discrepancy of $F_3$. Almost all this article has been formalized and verified in the Coq/Rocq proof assistant.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #a49950" title="Confidence: 62.6%">
                        Math
                    </span>
            <!-- LLMs: 7.5 -->
                
            <!-- Medicine: 7.2 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1098
            </span>
            <a href="https://arxiv.org/abs/2504.07967" target="_blank" rel="noopener noreferrer">Double Directional Wireless Channel Generation: A Statistics-Informed Generative Approach</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Md-Ferdous Pervej, Patel Pratik, Koushik Manjunatha, Prasad Shamain, Andreas F. Molisch | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Channel models that represent various operating conditions a communication system might experience are important for design and standardization of any communication system. While statistical channel models have long dominated this space, machine learning (ML) is becoming a popular alternative approa</span>
            
            <span class="abstract-full" style="display: none;">Channel models that represent various operating conditions a communication system might experience are important for design and standardization of any communication system. While statistical channel models have long dominated this space, machine learning (ML) is becoming a popular alternative approach. However, existing approaches have mostly focused on predictive solutions to match instantaneous channel realizations. Other solutions have focused on pathloss modeling, while double-directional (DD) channel representation is needed for a complete description. Motivated by this, we (a) develop a generative solution that uses a hybrid Transformer (hTransformer) model with a low-rank projected attention calculation mechanism and a bi-directional long short-term memory (BiLSTM) layer to generate complete DD channel information and (b) design a domain-knowledge-informed training method to match the generated and true channel realizations' statistics. Our extensive simulation results validate that the generated samples' statistics closely align with the true statistics while mostly outperforming the performance of existing predictive approaches.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.0 -->
                
            <!-- Medicine: 9.0 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.1856
            </span>
            <a href="https://arxiv.org/abs/2504.07995" target="_blank" rel="noopener noreferrer">SafeChat: A Framework for Building Trustworthy Collaborative Assistants and a Case Study of its Usefulness</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Biplav Srivastava, Kausik Lakkaraju, Nitin Gupta, Vansh Nagpal, Bharath C. Muppasani, Sara E. Jones | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Collaborative assistants, or chatbots, are data-driven decision support systems that enable natural interaction for task completion. While they can meet critical needs in modern society, concerns about their reliability and trustworthiness persist. In particular, Large Language Model (LLM)-based cha</span>
            
            <span class="abstract-full" style="display: none;">Collaborative assistants, or chatbots, are data-driven decision support systems that enable natural interaction for task completion. While they can meet critical needs in modern society, concerns about their reliability and trustworthiness persist. In particular, Large Language Model (LLM)-based chatbots like ChatGPT, Gemini, and DeepSeek are becoming more accessible. However, such chatbots have limitations, including their inability to explain response generation, the risk of generating problematic content, the lack of standardized testing for reliability, and the need for deep AI expertise and extended development times. These issues make chatbots unsuitable for trust-sensitive applications like elections or healthcare. To address these concerns, we introduce SafeChat, a general architecture for building safe and trustworthy chatbots, with a focus on information retrieval use cases. Key features of SafeChat include: (a) safety, with a domain-agnostic design where responses are grounded and traceable to approved sources (provenance), and 'do-not-respond' strategies to prevent harmful answers; (b) usability, with automatic extractive summarization of long responses, traceable to their sources, and automated trust assessments to communicate expected chatbot behavior, such as sentiment; and (c) fast, scalable development, including a CSV-driven workflow, automated testing, and integration with various devices. We implemented SafeChat in an executable framework using the open-source chatbot platform Rasa. A case study demonstrates its application in building ElectionBot-SC, a chatbot designed to safely disseminate official election information. SafeChat is being used in many domains, validating its potential, and is available at: https://github.com/ai4society/trustworthy-chatbot.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- LLMs: 9.7 -->
                
            <!-- Medicine: 9.5 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.6 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.4 -->
                
            <!-- Federated Learning: 1.4 -->
                
            <!-- Robotics: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -8.5181
            </span>
            <a href="https://arxiv.org/abs/2504.08660" target="_blank" rel="noopener noreferrer">Channel Estimation by Infinite Width Convolutional Networks</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mohammed Mallik, Guillaume Villemaud | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In wireless communications, estimation of channels in OFDM systems spans frequency and time, which relies on sparse collections of pilot data, posing an ill-posed inverse problem. Moreover, deep learning estimators require large amounts of training data, computational resources, and true channels to</span>
            
            <span class="abstract-full" style="display: none;">In wireless communications, estimation of channels in OFDM systems spans frequency and time, which relies on sparse collections of pilot data, posing an ill-posed inverse problem. Moreover, deep learning estimators require large amounts of training data, computational resources, and true channels to produce accurate channel estimates, which are not realistic. To address this, a convolutional neural tangent kernel (CNTK) is derived from an infinitely wide convolutional network whose training dynamics can be expressed by a closed-form equation. This CNTK is used to impute the target matrix and estimate the missing channel response using only the known values available at pilot locations. This is a promising solution for channel estimation that does not require a large training set. Numerical results on realistic channel datasets demonstrate that our strategy accurately estimates the channels without a large dataset and significantly outperforms deep learning methods in terms of speed, accuracy, and computational resources.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- Medicine: 9.6 -->
                
            <!-- LLMs: 8.9 -->
                
            <!-- Quantum Computing: 4.5 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.2 -->
                
            <!-- Math: 1.7 -->
                
            <!-- GNN: 1.6 -->
                
            <!-- 3D: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Robotics: 1.4 -->
                
            <!-- Blockchain: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -10.902
            </span>
            <a href="https://arxiv.org/abs/2504.08410" target="_blank" rel="noopener noreferrer">PMNI: Pose-free Multi-view Normal Integration for Reflective and Textureless Surface Reconstruction</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Mingzhi Pei, Xu Cao, Xiangyi Wang, Heng Guo, Zhanyu Ma | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Reflective and textureless surfaces remain a challenge in multi-view 3D reconstruction.Both camera pose calibration and shape reconstruction often fail due to insufficient or unreliable cross-view visual features. To address these issues, we present PMNI (Pose-free Multi-view Normal Integration), a </span>
            
            <span class="abstract-full" style="display: none;">Reflective and textureless surfaces remain a challenge in multi-view 3D reconstruction.Both camera pose calibration and shape reconstruction often fail due to insufficient or unreliable cross-view visual features. To address these issues, we present PMNI (Pose-free Multi-view Normal Integration), a neural surface reconstruction method that incorporates rich geometric information by leveraging surface normal maps instead of RGB images. By enforcing geometric constraints from surface normals and multi-view shape consistency within a neural signed distance function (SDF) optimization framework, PMNI simultaneously recovers accurate camera poses and high-fidelity surface geometry. Experimental results on synthetic and real-world datasets show that our method achieves state-of-the-art performance in the reconstruction of reflective surfaces, even without reliable initial camera poses.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #76aa96" title="Confidence: 75.6%">
                        3D
                    </span>
            <!-- LLMs: 19.8 -->
                
            <!-- GNN: 3.9 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- T2I: 2.3 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- Medicine: 1.4 -->
                
            <!-- Bayesian Optimization: 1.3 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Attention: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -10.9163
            </span>
            <a href="https://arxiv.org/abs/2503.16681" target="_blank" rel="noopener noreferrer">GauRast: Enhancing GPU Triangle Rasterizers to Accelerate 3D Gaussian Splatting</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Sixu Li, Ben Keller, Yingyan Celine Lin, Brucek Khailany | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">3D intelligence leverages rich 3D features and stands as a promising frontier in AI, with 3D rendering fundamental to many downstream applications. 3D Gaussian Splatting (3DGS), an emerging high-quality 3D rendering method, requires significant computation, making real-time execution on existing GPU</span>
            
            <span class="abstract-full" style="display: none;">3D intelligence leverages rich 3D features and stands as a promising frontier in AI, with 3D rendering fundamental to many downstream applications. 3D Gaussian Splatting (3DGS), an emerging high-quality 3D rendering method, requires significant computation, making real-time execution on existing GPU-equipped edge devices infeasible. Previous efforts to accelerate 3DGS rely on dedicated accelerators that require substantial integration overhead and hardware costs. This work proposes an acceleration strategy that leverages the similarities between the 3DGS pipeline and the highly optimized conventional graphics pipeline in modern GPUs. Instead of developing a dedicated accelerator, we enhance existing GPU rasterizer hardware to efficiently support 3DGS operations. Our results demonstrate a 23$\times$ increase in processing speed and a 24$\times$ reduction in energy consumption, with improvements yielding 6$\times$ faster end-to-end runtime for the original 3DGS algorithm and 4$\times$ for the latest efficiency-improved pipeline, achieving 24 FPS and 46 FPS respectively. These enhancements incur only a minimal area overhead of 0.2\% relative to the entire SoC chip area, underscoring the practicality and efficiency of our approach for enabling 3DGS rendering on resource-constrained platforms.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #76aa96" title="Confidence: 75.8%">
                        3D
                    </span>
            <!-- LLMs: 17.1 -->
                
            <!-- GNN: 4.0 -->
                
            <!-- Robotics: 3.1 -->
                
            <!-- T2I: 2.4 -->
                
            <!-- RAG: 1.9 -->
                
            <!-- Medicine: 1.3 -->
                
            <!-- Bayesian Optimization: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Attention: 1.3 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -10.9426
            </span>
            <a href="https://arxiv.org/abs/2409.16938" target="_blank" rel="noopener noreferrer">Generative Object Insertion in Gaussian Splatting with a Multi-View Diffusion Model</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Hongliang Zhong, Can Wang, Jingbo Zhang, Jing Liao | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Generating and inserting new objects into 3D content is a compelling approach for achieving versatile scene recreation. Existing methods, which rely on SDS optimization or single-view inpainting, often struggle to produce high-quality results. To address this, we propose a novel method for object in</span>
            
            <span class="abstract-full" style="display: none;">Generating and inserting new objects into 3D content is a compelling approach for achieving versatile scene recreation. Existing methods, which rely on SDS optimization or single-view inpainting, often struggle to produce high-quality results. To address this, we propose a novel method for object insertion in 3D content represented by Gaussian Splatting. Our approach introduces a multi-view diffusion model, dubbed MVInpainter, which is built upon a pre-trained stable video diffusion model to facilitate view-consistent object inpainting. Within MVInpainter, we incorporate a ControlNet-based conditional injection module to enable controlled and more predictable multi-view generation. After generating the multi-view inpainted results, we further propose a mask-aware 3D reconstruction technique to refine Gaussian Splatting reconstruction from these sparse inpainted views. By leveraging these fabricate techniques, our approach yields diverse results, ensures view-consistent and harmonious insertions, and produces better object quality. Extensive experiments demonstrate that our approach outperforms existing methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #76aa96" title="Confidence: 76.4%">
                        3D
                    </span>
            <!-- LLMs: 18.1 -->
                
            <!-- GNN: 3.9 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- T2I: 2.7 -->
                
            <!-- RAG: 2.0 -->
                
            <!-- Bayesian Optimization: 1.3 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Attention: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Medicine: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -11.03
            </span>
            <a href="https://arxiv.org/abs/2401.12452" target="_blank" rel="noopener noreferrer">Self-supervised Learning of LiDAR 3D Point Clouds via 2D-3D Neural Calibration</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Yifan Zhang, Siyu Ren, Junhui Hou, Jinjian Wu, Yixuan Yuan, Guangming Shi | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">This paper introduces a novel self-supervised learning framework for enhancing 3D perception in autonomous driving scenes. Specifically, our approach, namely NCLR, focuses on 2D-3D neural calibration, a novel pretext task that estimates the rigid pose aligning camera and LiDAR coordinate systems. Fi</span>
            
            <span class="abstract-full" style="display: none;">This paper introduces a novel self-supervised learning framework for enhancing 3D perception in autonomous driving scenes. Specifically, our approach, namely NCLR, focuses on 2D-3D neural calibration, a novel pretext task that estimates the rigid pose aligning camera and LiDAR coordinate systems. First, we propose the learnable transformation alignment to bridge the domain gap between image and point cloud data, converting features into a unified representation space for effective comparison and matching. Second, we identify the overlapping area between the image and point cloud with the fused features. Third, we establish dense 2D-3D correspondences to estimate the rigid pose. The framework not only learns fine-grained matching from points to pixels but also achieves alignment of the image and point cloud at a holistic level, understanding their relative pose. We demonstrate the efficacy of NCLR by applying the pre-trained backbone to downstream tasks, such as LiDAR-based 3D semantic segmentation, object detection, and panoptic segmentation. Comprehensive experiments on various datasets illustrate the superiority of NCLR over existing self-supervised methods. The results confirm that joint learning from different modalities significantly enhances the network's understanding abilities and effectiveness of learned representation. The code is publicly available at https://github.com/Eaphan/NCLR.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><!-- 3D: 41.4 -->
                
            <!-- LLMs: 8.6 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Robotics: 1.7 -->
                
            <!-- T2I: 1.3 -->
                
            <!-- Medicine: 1.2 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -16.8908
            </span>
            <a href="https://arxiv.org/abs/2411.03976" target="_blank" rel="noopener noreferrer">HRDecoder: High-Resolution Decoder Network for Fundus Image Lesion Segmentation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Ziyuan Ding, Yixiong Liang, Shichao Kan, Qing Liu | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">High resolution is crucial for precise segmentation in fundus images, yet handling high-resolution inputs incurs considerable GPU memory costs, with diminishing performance gains as overhead increases. To address this issue while tackling the challenge of segmenting tiny objects, recent studies have</span>
            
            <span class="abstract-full" style="display: none;">High resolution is crucial for precise segmentation in fundus images, yet handling high-resolution inputs incurs considerable GPU memory costs, with diminishing performance gains as overhead increases. To address this issue while tackling the challenge of segmenting tiny objects, recent studies have explored local-global fusion methods. These methods preserve fine details using local regions and capture long-range context information from downscaled global images. However, the necessity of multiple forward passes inevitably incurs significant computational overhead, adversely affecting inference speed. In this paper, we propose HRDecoder, a simple High-Resolution Decoder network for fundus lesion segmentation. It integrates a high-resolution representation learning module to capture fine-grained local features and a high-resolution fusion module to fuse multi-scale predictions. Our method effectively improves the overall segmentation accuracy of fundus lesions while consuming reasonable memory and computational overhead, and maintaining satisfying inference speed. Experimental results on the IDRiD and DDR datasets demonstrate the effectiveness of our method. Code is available at https://github.com/CVIU-CSU/HRDecoder.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 67.3%">
                        Medicine
                    </span>
            <!-- LLMs: 7.8 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.3 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Federated Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- GNN: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- 3D: 1.0 -->
                
            <!-- SpikingNN: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -18.2289
            </span>
            <a href="https://arxiv.org/abs/2504.08659" target="_blank" rel="noopener noreferrer">BowelRCNN: Region-based Convolutional Neural Network System for Bowel Sound Auscultation</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Igor Matynia, Robert Nowak | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Sound events representing intestinal activity detection is a diagnostic tool with potential to identify gastrointestinal conditions. This article introduces BowelRCNN, a novel bowel sound detection system that uses audio recording, spectrogram analysys and region-based convolutional neural network (</span>
            
            <span class="abstract-full" style="display: none;">Sound events representing intestinal activity detection is a diagnostic tool with potential to identify gastrointestinal conditions. This article introduces BowelRCNN, a novel bowel sound detection system that uses audio recording, spectrogram analysys and region-based convolutional neural network (RCNN) architecture. The system was trained and validated on a real recording dataset gathered from 19 patients, comprising 60 minutes of prepared and annotated audio data. BowelRCNN achieved a classification accuracy of 96% and an F1 score of 71%. This research highlights the feasibility of using CNN architectures for bowel sound auscultation, achieving results comparable to those of recurrent-convolutional methods.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 66.8%">
                        Medicine
                    </span>
            <!-- LLMs: 6.8 -->
                
            <!-- Quantum Computing: 4.6 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -18.5355
            </span>
            <a href="https://arxiv.org/abs/2504.08329" target="_blank" rel="noopener noreferrer">MedRep: Medical Concept Representation for General Electronic Health Record Foundation Models</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Junmo Kim, Namkyeong Lee, Jiwon Kim, Kwangsoo Kim | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generali</span>
            
            <span class="abstract-full" style="display: none;">Electronic health record (EHR) foundation models have been an area ripe for exploration with their improved performance in various medical tasks. Despite the rapid advances, there exists a fundamental limitation: Processing unseen medical codes out of the vocabulary. This problem limits the generality of EHR foundation models and the integration of models trained with different vocabularies. To deal with this problem, we propose MedRep for EHR foundation models based on the observational medical outcome partnership (OMOP) common data model (CDM), providing the integrated medical concept representations and the basic data augmentation strategy for patient trajectories. For concept representation learning, we enrich the information of each concept with a minimal definition through large language model (LLM) prompts and enhance the text-based representations through graph ontology of OMOP vocabulary. Trajectory augmentation randomly replaces selected concepts with other similar concepts that have closely related representations to let the model practice with the concepts out-of-vocabulary. Finally, we demonstrate that EHR foundation models trained with MedRep better maintain the prediction performance in external datasets. Our code implementation is publicly available at https://github.com/kicarussays/MedRep.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 68.9%">
                        Medicine
                    </span>
            <!-- LLMs: 7.3 -->
                
            <!-- Quantum Computing: 4.2 -->
                
            <!-- Networks: 2.7 -->
                
            <!-- Reinforcement Learning: 2.1 -->
                
            <!-- Math: 2.0 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Blockchain: 1.4 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            <!-- GNN: 1.0 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -19.1154
            </span>
            <a href="https://arxiv.org/abs/2504.08481" target="_blank" rel="noopener noreferrer">A Hybrid Fully Convolutional CNN-Transformer Model for Inherently Interpretable Medical Image Classification</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Kerol Djoumessi, Samuel Ofosu Mensah, Philipp Berens | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">In many medical imaging tasks, convolutional neural networks (CNNs) efficiently extract local features hierarchically. More recently, vision transformers (ViTs) have gained popularity, using self-attention mechanisms to capture global dependencies, but lacking the inherent spatial localization of co</span>
            
            <span class="abstract-full" style="display: none;">In many medical imaging tasks, convolutional neural networks (CNNs) efficiently extract local features hierarchically. More recently, vision transformers (ViTs) have gained popularity, using self-attention mechanisms to capture global dependencies, but lacking the inherent spatial localization of convolutions. Therefore, hybrid models combining CNNs and ViTs have been developed to combine the strengths of both architectures. However, such hybrid CNN-ViT models are difficult to interpret, which hinders their application in medical imaging. In this work, we introduce an interpretable-by-design hybrid fully convolutional CNN-Transformer architecture for medical image classification. Unlike widely used post-hoc saliency methods for ViTs, our approach generates faithful and localized evidence maps that directly reflect the model's decision process. We evaluated our method on two medical image classification tasks using color fundus images. Our model not only achieves state-of-the-art predictive performance compared to both black-box and interpretable models but also provides class-specific sparse evidence maps in a single forward pass. The code is available at: https://anonymous.4open.science/r/Expl-CNN-Transformer/.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 62.6%">
                        Medicine
                    </span>
            <!-- LLMs: 9.2 -->
                
            <!-- Quantum Computing: 4.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Math: 1.6 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- 3D: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -20.3415
            </span>
            <a href="https://arxiv.org/abs/2503.14049" target="_blank" rel="noopener noreferrer">A Modular Edge Device Network for Surgery Digitalization</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Vincent Schorp, Fr\'ed\'eric Giraud, Gianluca Parg\"atzi, Michael W\"aspe, Lorenzo von Ritter-Zahony, Marcel Wegmann, Nicola A. Cavalcanti, John Garcia Henao, Nicholas B\"unger, Dominique Cachin, Sebastiano Caprara, Philipp F\"urnstahl, Fabio Carrillo | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning technique</span>
            
            <span class="abstract-full" style="display: none;">Future surgical care demands real-time, integrated data to drive informed decision-making and improve patient outcomes. The pressing need for seamless and efficient data capture in the OR motivates our development of a modular solution that bridges the gap between emerging machine learning techniques and interventional medicine. We introduce a network of edge devices, called Data Hubs (DHs), that interconnect diverse medical sensors, imaging systems, and robotic tools via optical fiber and a centralized network switch. Built on the NVIDIA Jetson Orin NX, each DH supports multiple interfaces (HDMI, USB-C, Ethernet) and encapsulates device-specific drivers within Docker containers using the Isaac ROS framework and ROS2. A centralized user interface enables straightforward configuration and real-time monitoring, while an Nvidia DGX computer provides state-of-the-art data processing and storage. We validate our approach through an ultrasound-based 3D anatomical reconstruction experiment that combines medical imaging, pose tracking, and RGB-D data acquisition.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 61.4%">
                        Medicine
                    </span>
            <!-- LLMs: 8.2 -->
                
            <!-- Quantum Computing: 3.9 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Reinforcement Learning: 1.8 -->
                
            <!-- Math: 1.7 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- GNN: 1.5 -->
                
            <!-- 3D: 1.3 -->
                
            <!-- Blockchain: 1.2 -->
                
            <!-- Robotics: 1.2 -->
                
            <!-- Pathfinding: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -20.5423
            </span>
            <a href="https://arxiv.org/abs/2504.08675" target="_blank" rel="noopener noreferrer">X2BR: High-Fidelity 3D Bone Reconstruction from a Planar X-Ray Image with Hybrid Neural Implicit Methods</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Gokce Guven, H. Fatih Ugurdag, Hasan F. Ates | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Accurate 3D bone reconstruction from a single planar X-ray remains a challenge due to anatomical complexity and limited input data. We propose X2BR, a hybrid neural implicit framework that combines continuous volumetric reconstruction with template-guided non-rigid registration. The core network, X2</span>
            
            <span class="abstract-full" style="display: none;">Accurate 3D bone reconstruction from a single planar X-ray remains a challenge due to anatomical complexity and limited input data. We propose X2BR, a hybrid neural implicit framework that combines continuous volumetric reconstruction with template-guided non-rigid registration. The core network, X2B, employs a ConvNeXt-based encoder to extract spatial features from X-rays and predict high-fidelity 3D bone occupancy fields without relying on statistical shape models. To further refine anatomical accuracy, X2BR integrates a patient-specific template mesh, constructed using YOLOv9-based detection and the SKEL biomechanical skeleton model. The coarse reconstruction is aligned to the template using geodesic-based coherent point drift, enabling anatomically consistent 3D bone volumes. Experimental results on a clinical dataset show that X2B achieves the highest numerical accuracy, with an IoU of 0.952 and Chamfer-L1 distance of 0.005, outperforming recent baselines including X2V and D2IM-Net. Building on this, X2BR incorporates anatomical priors via YOLOv9-based bone detection and biomechanical template alignment, leading to reconstructions that, while slightly lower in IoU (0.875), offer superior anatomical realism, especially in rib curvature and vertebral alignment. This numerical accuracy vs. visual consistency trade-off between X2B and X2BR highlights the value of hybrid frameworks for clinically relevant 3D reconstructions.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 64.3%">
                        Medicine
                    </span>
            <!-- 3D: 40.2 -->
                
            <!-- LLMs: 10.9 -->
                
            <!-- Quantum Computing: 4.3 -->
                
            <!-- Networks: 2.8 -->
                
            <!-- GNN: 2.1 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Robotics: 1.5 -->
                
            <!-- Federated Learning: 1.5 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- T2I: 1.2 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- Hardware: 1.0 -->
                
            <!-- RAG: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -20.5694
            </span>
            <a href="https://arxiv.org/abs/2504.08469" target="_blank" rel="noopener noreferrer">Artifact detection and localization in single-channel mobile EEG for sleep research using deep learning and attention mechanisms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Khrystyna Semkiv, Jia Zhang, Maria Laura Ferster, Walter Karlen | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Artifacts in the electroencephalogram (EEG) degrade signal quality and impact the analysis of brain activity. Current methods for detecting artifacts in sleep EEG rely on simple threshold-based algorithms that require manual intervention, which is time-consuming and impractical due to the vast volum</span>
            
            <span class="abstract-full" style="display: none;">Artifacts in the electroencephalogram (EEG) degrade signal quality and impact the analysis of brain activity. Current methods for detecting artifacts in sleep EEG rely on simple threshold-based algorithms that require manual intervention, which is time-consuming and impractical due to the vast volume of data that novel mobile recording systems generate. We propose a convolutional neural network (CNN) model incorporating a convolutional block attention module (CNN-CBAM) to detect and identify the location of artifacts in the sleep EEG with attention maps. We benchmarked this model against six other machine learning and signal processing approaches. We trained/tuned all models on 72 manually annotated EEG recordings obtained during home-based monitoring from 18 healthy participants with a mean (SD) age of 68.05 y ($\pm$5.02). We tested them on 26 separate recordings from 6 healthy participants with a mean (SD) age of 68.33 y ($\pm$4.08), with contained artifacts in 4\% of epochs. CNN-CBAM achieved the highest area under the receiver operating characteristic curve (0.88), sensitivity (0.81), and specificity (0.86) when compared to the other approaches. The attention maps from CNN-CBAM localized artifacts within the epoch with a sensitivity of 0.71 and specificity of 0.67. This work demonstrates the feasibility of automating the detection and localization of artifacts in wearable sleep EEG.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #4ff278" title="Confidence: 72.3%">
                        Medicine
                    </span>
            <!-- Quantum Computing: 4.7 -->
                
            <!-- LLMs: 4.3 -->
                
            <!-- Networks: 2.9 -->
                
            <!-- Math: 2.2 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- SpikingNN: 1.2 -->
                
            <!-- HPO and AutoML: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- Hardware: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -35.9688
            </span>
            <a href="https://arxiv.org/abs/2412.09404" target="_blank" rel="noopener noreferrer">Opinion de-polarization of social networks with GNNs</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Konstantinos Mylonas, Thrasyvoulos Spyropoulos | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Nowadays, social media is the ground for political debate and exchange of opinions. There is a significant amount of research that suggests that social media are highly polarized. A phenomenon that is commonly observed is the echo chamber structure, where users are organized in polarized communities</span>
            
            <span class="abstract-full" style="display: none;">Nowadays, social media is the ground for political debate and exchange of opinions. There is a significant amount of research that suggests that social media are highly polarized. A phenomenon that is commonly observed is the echo chamber structure, where users are organized in polarized communities and form connections only with similar-minded individuals, limiting themselves to consume specific content. In this paper we explore a way to decrease the polarization of networks with two echo chambers. Particularly, we observe that if some users adopt a moderate opinion about a topic, the polarization of the network decreases. Based on this observation, we propose an efficient algorithm to identify a good set of K users, such that if they adopt a moderate stance around a topic, the polarization is minimized. Our algorithm employs a Graph Neural Network and thus it can handle large graphs more effectively than other approaches</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge high-confidence" style="background-color: #b243cd" title="Confidence: 76.8%">
                        GNN
                    </span>
            <!-- LLMs: 15.7 -->
                
            <!-- Robotics: 3.0 -->
                
            <!-- 3D: 2.7 -->
                
            <!-- T2I: 2.5 -->
                
            <!-- RAG: 1.7 -->
                
            <!-- Attention: 1.4 -->
                
            <!-- Bayesian Optimization: 1.4 -->
                
            <!-- Datasets: 1.3 -->
                
            <!-- Fuzzy Logic: 1.3 -->
                
            <!-- Medicine: 1.1 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -40.6141
            </span>
            <a href="https://arxiv.org/abs/2503.10790" target="_blank" rel="noopener noreferrer">Quantum Error Detection For Early Term Fault-Tolerant Quantum Algorithms</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Tom Ginsberg, Vyom Patel | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Quantum error detection (QED) offers a promising pathway to fault tolerance in near-term quantum devices by balancing error suppression with minimal resource overhead. However, its practical utility hinges on optimizing design parameters-such as syndrome measurement frequency-to avoid diminishing re</span>
            
            <span class="abstract-full" style="display: none;">Quantum error detection (QED) offers a promising pathway to fault tolerance in near-term quantum devices by balancing error suppression with minimal resource overhead. However, its practical utility hinges on optimizing design parameters-such as syndrome measurement frequency-to avoid diminishing returns from detection overhead. In this work, we present a comprehensive framework for fault-tolerant compilation and simulation of quantum algorithms using [[n, n-2, 2]] codes, which enable low-qubit-overhead error detection and a simple nearly fault-tolerant universal set of operations. We demonstrate and analyze our pipeline with a purely statistical interpretation and through the implementation of Grover's search algorithm. Our results are used to answer the question is quantum error detection a worthwhile avenue for early-term fault tolerance, and if so how can we get the most out of it? Simulations under the circuit-level noise model reveal that finding optimal syndrome schedules improves algorithm success probabilities by an average of 6.7x but eventual statistical limits from post-selection in noisy/resource-limited regimes constrain scalability. Furthermore, we propose a simple data-driven approach to predict fault tolerant compilation parameters, such as optimal syndrome schedules, and expected fault tolerant performance gains based on circuit and noise features. These results provide actionable guidelines for implementing QED in early-term quantum experiments and underscore its role as a pragmatic, constant-overhead error mitigation layer for shallow algorithms. To aid in further research, we release all simulation data computed for this work and provide an experimental QED compiler at https://codeqraft.xyz/qed.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 59.9%">
                        Quantum Computing
                    </span>
            <!-- LLMs: 8.3 -->
                
            <!-- Medicine: 7.7 -->
                
            <!-- Networks: 2.4 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.6 -->
                
            <!-- Math: 1.5 -->
                
            <!-- GNN: 1.4 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- 3D: 1.1 -->
                
            <!-- Robotics: 1.1 -->
                
            <!-- Pathfinding: 1.0 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -49.8057
            </span>
            <a href="https://arxiv.org/abs/2405.08190" target="_blank" rel="noopener noreferrer">Barren plateaus are amplified by the dimension of qudits</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Lucas Friedrich, Tiago de Souza Farias, Jonas Maziero | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for attaining quantum advantage in diverse scientific and technological domains, notably within Quantum Neural Networks. However, despite their potential, VQAs encounter significant obstacles, chief among them being the vanishi</span>
            
            <span class="abstract-full" style="display: none;">Variational Quantum Algorithms (VQAs) have emerged as pivotal strategies for attaining quantum advantage in diverse scientific and technological domains, notably within Quantum Neural Networks. However, despite their potential, VQAs encounter significant obstacles, chief among them being the vanishing gradient problem, commonly referred to as barren plateaus. In this article, through meticulous analysis, we demonstrate that existing literature implicitly suggests the intrinsic influence of qudit dimensionality on barren plateaus. To instantiate these findings, we present numerical results that exemplify the impact of qudit dimensionality on barren plateaus. Therefore, despite the proposition of various error mitigation techniques, our results call for further scrutiny about their efficacy in the context of VQAs with qudits.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #d37d97" title="Confidence: 70.2%">
                        Quantum Computing
                    </span>
            <!-- Medicine: 8.4 -->
                
            <!-- LLMs: 6.4 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Math: 2.1 -->
                
            <!-- Reinforcement Learning: 1.9 -->
                
            <!-- Federated Learning: 1.7 -->
                
            <!-- Pathfinding: 1.3 -->
                
            <!-- Blockchain: 1.3 -->
                
            <!-- HPO and AutoML: 1.2 -->
                
            <!-- Evolutionary Algorithms: 1.1 -->
                
            <!-- SpikingNN: 1.0 -->
                
            
        </div>
    </div>
    
    <div class="paper">
        <div class="paper-title">
            <span class="interestingness-score interestingness-negative">
                -71.1251
            </span>
            <a href="https://arxiv.org/abs/2504.07589" target="_blank" rel="noopener noreferrer">Copy-and-Paste? Identifying EVM-Inequivalent Code Smells in Multi-chain Reuse Contracts</a>
            <span class="copy-icon" onclick="event.stopPropagation(); showJson(this.closest('.paper'))">ðŸ“‹</span>
        </div>
        <div class="paper-meta">
            Authors: Zexu Wang, Jiachi Chen, Tao Zhang, Yu Zhang, Weizhe Zhang, Yuming Feng, Zibin Zheng | Date: 2025-04-14
        </div>
        <div class="paper-abstract">
            <span class="abstract-short">As the development of Solidity contracts on Ethereum, more developers are reusing them on other compatible blockchains. However, developers may overlook the differences between the designs of the blockchain system, such as the Gas Mechanism and Consensus Protocol, leading to the same contracts on di</span>
            
            <span class="abstract-full" style="display: none;">As the development of Solidity contracts on Ethereum, more developers are reusing them on other compatible blockchains. However, developers may overlook the differences between the designs of the blockchain system, such as the Gas Mechanism and Consensus Protocol, leading to the same contracts on different blockchains not being able to achieve consistent execution as on Ethereum. This inconsistency reveals design flaws in reused contracts, exposing code smells that hinder code reusability, and we define this inconsistency as EVM-Inequivalent Code Smells. In this paper, we conducted the first empirical study to reveal the causes and characteristics of EVM-Inequivalent Code Smells. To ensure the identified smells reflect real developer concerns, we collected and analyzed 1,379 security audit reports and 326 Stack Overflow posts related to reused contracts on EVM-compatible blockchains, such as Binance Smart Chain (BSC) and Polygon. Using the open card sorting method, we defined six types of EVM-Inequivalent Code Smells. For automated detection, we developed a tool named EquivGuard. It employs static taint analysis to identify key paths from different patterns and uses symbolic execution to verify path reachability. Our analysis of 905,948 contracts across six major blockchains shows that EVM-Inequivalent Code Smells are widespread, with an average prevalence of 17.70%. While contracts with code smells do not necessarily lead to financial loss and attacks, their high prevalence and significant asset management underscore the potential threats of reusing these smelly Ethereum contracts. Thus, developers are advised to abandon Copy-and-Paste programming practices and detect EVM-Inequivalent Code Smells before reusing Ethereum contracts.</span>
            <span class="more-link" onclick="toggleAbstract(this)">... more</span>
            
        </div>
        <div class="paper-tags"><span class="tag-badge low-confidence" style="background-color: #3cc377" title="Confidence: 62.8%">
                        Blockchain
                    </span>
            <!-- Medicine: 7.4 -->
                
            <!-- LLMs: 6.8 -->
                
            <!-- Quantum Computing: 4.1 -->
                
            <!-- Networks: 2.6 -->
                
            <!-- Reinforcement Learning: 2.0 -->
                
            <!-- Math: 1.8 -->
                
            <!-- Federated Learning: 1.8 -->
                
            <!-- Pathfinding: 1.2 -->
                
            <!-- GNN: 1.2 -->
                
            <!-- Hardware: 1.1 -->
                
            <!-- SpikingNN: 1.1 -->
                
            <!-- Evolutionary Algorithms: 1.0 -->
                
            <!-- HPO and AutoML: 1.0 -->
                
            
        </div>
    </div>
    
    
    <div id="jsonPopup" class="json-popup">
        <pre id="jsonContent"></pre>
        <button onclick="copyJson()">Copy to Clipboard</button>
        <button onclick="closePopup()">Close</button>
    </div>

    <script>
        function extractPaperData(paperElement) {
            const titleElement = paperElement.querySelector('.paper-title a');
            const metaElement = paperElement.querySelector('.paper-meta');
            const abstractElement = paperElement.querySelector('.paper-abstract');
            const tagsElement = paperElement.querySelector('.paper-tags');
            
            const authorsText = metaElement.textContent.split('|')[0].replace('Authors:', '').trim();
            const dateText = metaElement.textContent.split('|')[1].replace('Date:', '').trim();
            
            const paperData = {
                title: titleElement.textContent,
                url: titleElement.href,
                authors: authorsText.split(',').map(author => author.trim()),
                created: dateText,
                abstract: abstractElement.querySelector('.abstract-full').textContent
            };
            
            return paperData;
        }

        function showJson(paperElement) {
            const popup = document.getElementById('jsonPopup');
            const content = document.getElementById('jsonContent');
            const paperData = extractPaperData(paperElement);
            content.textContent = JSON.stringify(paperData, null, null);
            popup.style.display = 'block';
            document.addEventListener('click', function closePopupOnClick(event) {
                if (!popup.contains(event.target)) {
                    popup.style.display = 'none';
                    document.removeEventListener('click', closePopupOnClick);
                }
            });
        }
        function toggleAbstract(element) {
            const abstract = element.parentElement;
            const short = abstract.querySelector('.abstract-short');
            const full = abstract.querySelector('.abstract-full');
            const lowConfidenceTags = abstract.parentElement.querySelectorAll('.tag-badge.low-confidence');
            
            if (element.textContent === '... more') {
                short.style.display = 'none';
                full.style.display = 'inline';
                element.textContent = ' less';
                lowConfidenceTags.forEach(tag => tag.style.display = 'inline-block');
            } else {
                short.style.display = 'inline';
                full.style.display = 'none';
                element.textContent = '... more';
                lowConfidenceTags.forEach(tag => tag.style.display = 'none');
            }
        }

        function closePopup() {
            document.getElementById('jsonPopup').style.display = 'none';
        }

        function copyJson() {
            const content = document.getElementById('jsonContent').textContent;
            navigator.clipboard.writeText(content).catch(() => {
                // If clipboard API is not available, just show the popup
                alert('Could not copy to clipboard. JSON is displayed in the popup.');
            });
        }

        // Close popup when clicking outside
        window.onclick = function(event) {
            const popup = document.getElementById('jsonPopup');
            if (event.target === popup) {
                popup.style.display = 'none';
            }
        }
    </script>
</body>
</html> 